[{"file": "1601.00530.tex", "nexttext": "\nwhere $x\\in\\mathbb{R}^N$ is the signal, $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$ is the measurement matrix, $n\\in\\mathbb{R}^M$ is the measurement noise, $y\\in\\mathbb{R}^M$ is the set of 1-bit measurements, and function $\\rm sign(\\cdot)$ maps the signal from $\\mathbb{R}^N$ to the Boolean cube $\\mathcal{B}^M:=\\{-1,+1\\}^M$. Since signs of real-valued measurements are used, one loses the ability to recover the magnitude of $x$ and thus assumes that the signal has a unit norm, i.e., $\\|x\\|_2=1$. The 1-bit CS has been studied by many people and several algorithms have been developed to recover the sparse signals \\cite{boufounos20081,boufounos2009greedy,gupta2010sample,laska2011trust,zhou20121,jian2011investigation,biao2013fast}.\n\nDespite the attractive attributes of 1-bit CS, the major disadvantage is that measurements are susceptive to noise during both acquisition and transmission \\cite{gopi2013one,jacques2013robust,plan2013one}. In noisy scenario, the output bit is randomly perturbed from the sign of the real-valued measurement, and the so-called \\emph{sign flips} seriously degrade recovery performance. To date, researchers have developed numerous approaches for noisy 1-bit CS. Yan \\emph{et al.} \\cite{yan2012robust} proposed a greedy method which detect the positions of sign flips iteratively, and recover the signals using correct measurements. However, it requires the prior knowledge of noise level, which is often intractable in practical applications. Plan \\emph{et al.} \\cite{plan2013robust} proposed a constrained optimization method with a linear objective. This convex formulation can work with a general notion of noise and achieve error for both exactly and approximately sparse signals. Ai \\emph{et al.} \\cite{ai2014one} extends \\cite{plan2013robust} to sub-Gaussian measurements, and gets an irreducible component in the error and cannot be reduced by increasing the sample size or otherwise. However, they are computational inefficient and difficult for hardware implementation. Recently, Zhang \\emph{et al}. \\cite{zhang2014efficient} developed an efficient passive algorithm with closed-form solution, which improves the recovery performance for exactly $K$-sparse signal. Due to its high performance, robustness, and computational efficiency, they can be seen as the state-of-the-art algorithm for noisy 1-bit CS.\n\nThis study focuses on recovering exactly $K$-sparse signal in the noisy setting for 1-bit CS. A novel algorithm is proposed. Termed HISTORY, it consists of two key parts, namely \\emph{HammIng Support deTection}, and \\emph{cOefficients RecoverY}. The former aims to construct a candidate support set by detecting possible supports of nonzero entries. The latter aims to calculate the coefficients belonging to the candidate support set. Experimental results show that the proposed algorithm has high recovery performance than the state-of-the-art. In addition, due to the fact that containing no iterative step, it is computationally efficient and easy to implement.\n\n\\section{HISTORY Algorithm}\n\\label{sec:HISTORY_Algorithm}\nThe main objective of this section is to characterize the HISTORY algorithm. Notations used in this paper are first described, then the two key parts of HISTORY are introduced in sequence.\n\n\\subsection{Notations}\nBoldfaced capital letters such as $\\mathbf{A}$ are used for matrices. Italic capital letters such as $S$ denote sets. For a matrix $\\mathbf{A}$, $\\mathbf{A}_j$, $\\mathbf{A}_{i,j}$, $\\mathbf{A}^\\mathrm{T}$, and $\\mathbf{A}_S$ denote its $j^\\mathrm{th}$ column, $ij^\\mathrm{th}$ element, transpose, and sub-matrix which contains the columns with indices in $S$, respectively. Small letters such as $x$ are reserved for vectors and scalars. A vector $x$ is called $K$-sparse if at most $K$ of its coefficients are nonzero. For a vector $x$, $x_j$, $\\|x\\|_p$, and $x_S$ denote the $j^\\mathrm{th}$ element of the vector, its $p$-norm, and sub vector which contains the elements with indices in $S$, respectively. For two vectors $u\\in\\mathbb{R}^N$ and $v\\in\\mathbb{R}^N$, the notation $H(u,v)$ denotes the Hamming distance between them, which is defined as\n\n", "itemtype": "equation", "pos": 1778, "prevtext": "\n\\maketitle\n\n\\begin{summary}\nWe consider the problem of sparse signal recovery from 1-bit measurements. Due to the noise present in the acquisition and transmission process, some quantized bits may be flipped to their opposite states. These sign flips may result in severe performance degradation. In this study, a novel algorithm, termed HISTORY, is proposed. It consists of Hamming support detection and coefficients recovery. The HISTORY algorithm has high recovery accuracy and is robust to strong measurement noise. Numerical results are provided to demonstrate the effectiveness and superiority of the proposed algorithm.\n\\end{summary}\n\\begin{keywords}\n1-bit compressed sensing, sign flips, Hamming distance\n\\end{keywords}\n\n\\section{Introduction}\nCompressed sensing, as introduced in \\cite{candes2006near,donoho2006compressed,candes2006robust}, addresses the problem of estimating high dimensional signals from a set of relatively few linear measurements. It was demonstrated that a sparse signal can be reconstructed exactly if the measurement matrix satisfies the restricted isometric property (RIP) \\cite{candes2008restricted}. It was also shown that random matrices will satisfy the RIP with high probability if the entries are chosen according to independent and identically distributed (i.i.d.) Gaussian distribution.\n\nIn practical CS architectures, the measurements must be quantized to a finite number of bits. The extreme quantization setting where only the sign is acquired is known as 1-bit compressed sensing (1-bit CS) \\cite{boufounos20081}. It has become increasingly popular due to its low computational cost and  easy implementation for hardware \\cite{boufounos2010reconstruction}. In 1-bit CS, measurements of a signal $x\\in\\mathbb{R}^N$ are computed via\n\n", "index": 1, "text": "\\begin{equation}\n\t\\label{eq:1bitcs}\n\ty = {\\rm sign}(\\mathbf{A} x + n),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"y={\\rm sign}(\\mathbf{A}x+n),\" display=\"block\"><mrow><mrow><mi>y</mi><mo>=</mo><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>+</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nThe notation $\\mathbb{P}$ denotes probability of an event.\n\n\\subsection{Hamming support detection}\nTo detect possible supports of nonzero coefficients from noisy 1-bit measurements, a \\emph{Hamming support detection} method is developed based on Angle Proportional Probability (APP), which is outlined as follows,\n\\begin{theorem}[Angle Proportional Probability]\n\t\\label{Theorem:APP}\n\tLet $x\\in\\Sigma_K$ be a $K$-sparse signal with $\\|x\\|_2=1$. Let $\\phi$ be a Gaussian random vector which is drawn uniformly from the unit $\\ell_2$ sphere in $\\mathbb{R}^N$ (i.e., each element of $\\phi$ is firstly drawn i.i.d. from the standard Gaussian distribution $\\mathcal{N}(0,1)$. Define an event $E$ to be\n\t\n", "itemtype": "equation", "pos": 6000, "prevtext": "\nwhere $x\\in\\mathbb{R}^N$ is the signal, $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$ is the measurement matrix, $n\\in\\mathbb{R}^M$ is the measurement noise, $y\\in\\mathbb{R}^M$ is the set of 1-bit measurements, and function $\\rm sign(\\cdot)$ maps the signal from $\\mathbb{R}^N$ to the Boolean cube $\\mathcal{B}^M:=\\{-1,+1\\}^M$. Since signs of real-valued measurements are used, one loses the ability to recover the magnitude of $x$ and thus assumes that the signal has a unit norm, i.e., $\\|x\\|_2=1$. The 1-bit CS has been studied by many people and several algorithms have been developed to recover the sparse signals \\cite{boufounos20081,boufounos2009greedy,gupta2010sample,laska2011trust,zhou20121,jian2011investigation,biao2013fast}.\n\nDespite the attractive attributes of 1-bit CS, the major disadvantage is that measurements are susceptive to noise during both acquisition and transmission \\cite{gopi2013one,jacques2013robust,plan2013one}. In noisy scenario, the output bit is randomly perturbed from the sign of the real-valued measurement, and the so-called \\emph{sign flips} seriously degrade recovery performance. To date, researchers have developed numerous approaches for noisy 1-bit CS. Yan \\emph{et al.} \\cite{yan2012robust} proposed a greedy method which detect the positions of sign flips iteratively, and recover the signals using correct measurements. However, it requires the prior knowledge of noise level, which is often intractable in practical applications. Plan \\emph{et al.} \\cite{plan2013robust} proposed a constrained optimization method with a linear objective. This convex formulation can work with a general notion of noise and achieve error for both exactly and approximately sparse signals. Ai \\emph{et al.} \\cite{ai2014one} extends \\cite{plan2013robust} to sub-Gaussian measurements, and gets an irreducible component in the error and cannot be reduced by increasing the sample size or otherwise. However, they are computational inefficient and difficult for hardware implementation. Recently, Zhang \\emph{et al}. \\cite{zhang2014efficient} developed an efficient passive algorithm with closed-form solution, which improves the recovery performance for exactly $K$-sparse signal. Due to its high performance, robustness, and computational efficiency, they can be seen as the state-of-the-art algorithm for noisy 1-bit CS.\n\nThis study focuses on recovering exactly $K$-sparse signal in the noisy setting for 1-bit CS. A novel algorithm is proposed. Termed HISTORY, it consists of two key parts, namely \\emph{HammIng Support deTection}, and \\emph{cOefficients RecoverY}. The former aims to construct a candidate support set by detecting possible supports of nonzero entries. The latter aims to calculate the coefficients belonging to the candidate support set. Experimental results show that the proposed algorithm has high recovery performance than the state-of-the-art. In addition, due to the fact that containing no iterative step, it is computationally efficient and easy to implement.\n\n\\section{HISTORY Algorithm}\n\\label{sec:HISTORY_Algorithm}\nThe main objective of this section is to characterize the HISTORY algorithm. Notations used in this paper are first described, then the two key parts of HISTORY are introduced in sequence.\n\n\\subsection{Notations}\nBoldfaced capital letters such as $\\mathbf{A}$ are used for matrices. Italic capital letters such as $S$ denote sets. For a matrix $\\mathbf{A}$, $\\mathbf{A}_j$, $\\mathbf{A}_{i,j}$, $\\mathbf{A}^\\mathrm{T}$, and $\\mathbf{A}_S$ denote its $j^\\mathrm{th}$ column, $ij^\\mathrm{th}$ element, transpose, and sub-matrix which contains the columns with indices in $S$, respectively. Small letters such as $x$ are reserved for vectors and scalars. A vector $x$ is called $K$-sparse if at most $K$ of its coefficients are nonzero. For a vector $x$, $x_j$, $\\|x\\|_p$, and $x_S$ denote the $j^\\mathrm{th}$ element of the vector, its $p$-norm, and sub vector which contains the elements with indices in $S$, respectively. For two vectors $u\\in\\mathbb{R}^N$ and $v\\in\\mathbb{R}^N$, the notation $H(u,v)$ denotes the Hamming distance between them, which is defined as\n\n", "index": 3, "text": "\\begin{equation}\n\tH(u,v)\\overset{\\mathrm{def}}{=}\\#\\left(u_j\\neq v_j\\right),\\quad j\\in1,2,\\dots,N.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"H(u,v)\\overset{\\mathrm{def}}{=}\\#\\left(u_{j}\\neq v_{j}\\right),\\quad j\\in 1,2,%&#10;\\dots,N.\" display=\"block\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mover accent=\"true\"><mo>=</mo><mo>def</mo></mover><mi mathvariant=\"normal\">#</mi><mrow><mo>(</mo><msub><mi>u</mi><mi>j</mi></msub><mo>\u2260</mo><msub><mi>v</mi><mi>j</mi></msub><mo>)</mo></mrow><mo rspace=\"12.5pt\">,</mo><mi>j</mi><mo>\u2208</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n\tthen it holds,\n\t\n", "itemtype": "equation", "pos": 6811, "prevtext": "\nThe notation $\\mathbb{P}$ denotes probability of an event.\n\n\\subsection{Hamming support detection}\nTo detect possible supports of nonzero coefficients from noisy 1-bit measurements, a \\emph{Hamming support detection} method is developed based on Angle Proportional Probability (APP), which is outlined as follows,\n\\begin{theorem}[Angle Proportional Probability]\n\t\\label{Theorem:APP}\n\tLet $x\\in\\Sigma_K$ be a $K$-sparse signal with $\\|x\\|_2=1$. Let $\\phi$ be a Gaussian random vector which is drawn uniformly from the unit $\\ell_2$ sphere in $\\mathbb{R}^N$ (i.e., each element of $\\phi$ is firstly drawn i.i.d. from the standard Gaussian distribution $\\mathcal{N}(0,1)$. Define an event $E$ to be\n\t\n", "index": 5, "text": "\\begin{equation}\n\t\t\\label{eq:event}\n\t\tE: {\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign}(\\phi_j),\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"E:{\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign}(\\phi_{j}),\" display=\"block\"><mrow><mrow><mi>E</mi><mo>:</mo><mrow><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><mi>\u03d5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n\\end{theorem}\nThe proof can be found in Appendix A. In particular, it shows that $\\mathbb{P}(E)$ has a cosine function relationship with the $j$-th element of $\\phi$. Thus, $x_j$ can be uniquely identified by $\\mathbb{P}(E)$. In addition, the probability can be estimated from the instances of the random variable $\\mathrm{sign}(x^{\\rm T}\\phi)$, which are exactly the 1-bit measurement vector $y$ defined in (\\ref{eq:1bitcs}). Therefore, $y$ contains sufficient information to reconstruct $x_j$ from the estimation of $\\mathbb{P}(E)$.\n\nIn noisy setting, due to the fact that the signs of $y$ are randomly perturbed, $x_j$ cannot computed directly from (\\ref{eq:event}) and (\\ref{eq:APP}). However, given the noise level (sign flip ratio) as a prior knowledge, we have the following lemma,\n\\begin{lemma}\n\t\\label{Lemma:sign-flip}\n\tGiven a $K$-sparse signal $x$ with $\\|x\\|_2=1$, a standard Gaussian measurement matrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$, and a 1-bit measurements vector $y = {\\rm sign}(\\mathbf{A} x)$. In noisy setting, suppose the sign flip ratio $\\rho<0.5$, define $P\\in[0,1]^N$ as a probability vector with $P_j$ denoting its $j$-th element as\n\t\n", "itemtype": "equation", "pos": 6934, "prevtext": "\n\tthen it holds,\n\t\n", "index": 7, "text": "\\begin{equation}\n\t\t\\label{eq:APP}\n\t\t\\mathbb{P}(E) = \\frac{1}{\\pi}{\\rm arccos}(x_j).\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}(E)=\\frac{1}{\\pi}{\\rm arccos}(x_{j}).\" display=\"block\"><mrow><mrow><mrow><mi>\u2119</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mi>arccos</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n\tand it holds\n\t\n", "itemtype": "equation", "pos": 8199, "prevtext": "\n\\end{theorem}\nThe proof can be found in Appendix A. In particular, it shows that $\\mathbb{P}(E)$ has a cosine function relationship with the $j$-th element of $\\phi$. Thus, $x_j$ can be uniquely identified by $\\mathbb{P}(E)$. In addition, the probability can be estimated from the instances of the random variable $\\mathrm{sign}(x^{\\rm T}\\phi)$, which are exactly the 1-bit measurement vector $y$ defined in (\\ref{eq:1bitcs}). Therefore, $y$ contains sufficient information to reconstruct $x_j$ from the estimation of $\\mathbb{P}(E)$.\n\nIn noisy setting, due to the fact that the signs of $y$ are randomly perturbed, $x_j$ cannot computed directly from (\\ref{eq:event}) and (\\ref{eq:APP}). However, given the noise level (sign flip ratio) as a prior knowledge, we have the following lemma,\n\\begin{lemma}\n\t\\label{Lemma:sign-flip}\n\tGiven a $K$-sparse signal $x$ with $\\|x\\|_2=1$, a standard Gaussian measurement matrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$, and a 1-bit measurements vector $y = {\\rm sign}(\\mathbf{A} x)$. In noisy setting, suppose the sign flip ratio $\\rho<0.5$, define $P\\in[0,1]^N$ as a probability vector with $P_j$ denoting its $j$-th element as\n\t\n", "index": 9, "text": "\\begin{equation}\n\t\tP_j\\overset{\\mathrm{def}}{=}\\mathbb{P}\\big(\\mathrm{sign}\\left(y_i\\right)\\neq\\mathrm{sign}\\left(\\mathbf{A}_{ij}\\right)\\big),\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"P_{j}\\overset{\\mathrm{def}}{=}\\mathbb{P}\\big{(}\\mathrm{sign}\\left(y_{i}\\right)%&#10;\\neq\\mathrm{sign}\\left(\\mathbf{A}_{ij}\\right)\\big{)},\" display=\"block\"><mrow><msub><mi>P</mi><mi>j</mi></msub><mover accent=\"true\"><mo>=</mo><mo>def</mo></mover><mi>\u2119</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>sign</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>\u2260</mo><mi>sign</mi><mrow><mo>(</mo><msub><mi>\ud835\udc00</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n\\end{lemma}\nThe proof can be found in Appendix B. In the limit of large systems as the measurements dimension $M\\rightarrow\\infty$, $P_j$ is an asymptotically approximation with the \\emph{Hamming Distance} between $y$ and $\\mathbf{A}_j$, i.e., $H\\{y,\\mathbf{A}_j\\}/M = P_j$. Consequently, given the noise level and a relatively high measurements dimension, $P_j$ can be well estimated by computing the Hamming distance, then $x_j$ can be estimated accordingly. However, directly estimating $x_j$ from (\\ref{eq:hamming_rho}) is intractable. For one thing, with the decrease of measurements dimension, the coefficients estimation performance degrades significantly. For another, (\\ref{eq:hamming_rho}) requires the sign flip ratio $\\rho$ as prior knowledge, which is often unknown in practical applications. To address the first problem, we only detect possible supports in current part, and leave the coefficients estimation to the next one. To address the second problem, it is easy to verify that despite the value of $\\rho$, $P_j$ in (\\ref{eq:hamming_rho}) is a monotone decreasing function with respect to $x_j$, i.e., when $M\\rightarrow\\infty$, we have\n\n", "itemtype": "equation", "pos": 8373, "prevtext": "\n\tand it holds\n\t\n", "index": 11, "text": "\\begin{equation}\n\t\t\\label{eq:hamming_rho}\n\t\tP_j = \\frac{1-2\\rho}{\\pi}{\\rm arccos}(x_j)+\\rho.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"P_{j}=\\frac{1-2\\rho}{\\pi}{\\rm arccos}(x_{j})+\\rho.\" display=\"block\"><mrow><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>=</mo><mrow><mrow><mfrac><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c1</mi></mrow></mrow><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mi>arccos</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>\u03c1</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n\nThe main point is that despite the noise level, amplitude order of nonzero coefficients will be maintained while dependencies in $\\rho$ vanish in the corresponding Hamming distance. Therefore, we can set $\\rho$ to be an arbitrarily value (e.g. $\\rho = 0$) and compute approximate amplitudes of each coefficients via (\\ref{eq:hamming_rho}), then form the candidate support set by selecting the supports with largest amplitudes.\n\n\\subsection{Coefficients recovery}\nProviding the candidate support set, denoted by $S$, the next part is coefficients recovery, which aims to compute the amplitudes of nonzero coefficients. In this paper, we try to compute the coefficients vector $c$ by solving the following constrained least squares problem,\n\n", "itemtype": "equation", "pos": 9639, "prevtext": "\n\\end{lemma}\nThe proof can be found in Appendix B. In the limit of large systems as the measurements dimension $M\\rightarrow\\infty$, $P_j$ is an asymptotically approximation with the \\emph{Hamming Distance} between $y$ and $\\mathbf{A}_j$, i.e., $H\\{y,\\mathbf{A}_j\\}/M = P_j$. Consequently, given the noise level and a relatively high measurements dimension, $P_j$ can be well estimated by computing the Hamming distance, then $x_j$ can be estimated accordingly. However, directly estimating $x_j$ from (\\ref{eq:hamming_rho}) is intractable. For one thing, with the decrease of measurements dimension, the coefficients estimation performance degrades significantly. For another, (\\ref{eq:hamming_rho}) requires the sign flip ratio $\\rho$ as prior knowledge, which is often unknown in practical applications. To address the first problem, we only detect possible supports in current part, and leave the coefficients estimation to the next one. To address the second problem, it is easy to verify that despite the value of $\\rho$, $P_j$ in (\\ref{eq:hamming_rho}) is a monotone decreasing function with respect to $x_j$, i.e., when $M\\rightarrow\\infty$, we have\n\n", "index": 13, "text": "\\begin{equation}\n\tH\\{y,\\mathbf{A}_u\\}>H\\{y,\\mathbf{A}_v\\}, \\quad \\forall\\ x_u<x_v.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"H\\{y,\\mathbf{A}_{u}\\}&gt;H\\{y,\\mathbf{A}_{v}\\},\\quad\\forall\\ x_{u}&lt;x_{v}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>y</mi><mo>,</mo><msub><mi>\ud835\udc00</mi><mi>u</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>y</mi><mo>,</mo><msub><mi>\ud835\udc00</mi><mi>v</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mo rspace=\"7.5pt\">\u2200</mo><msub><mi>x</mi><mi>u</mi></msub></mrow><mo>&lt;</mo><msub><mi>x</mi><mi>v</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nwhere $\\|c\\|_0$ denotes the $0$-norm of $c$, i.e., counting the number of nonzero coefficients in $c$. It's worth noting that is a overdetermined system when $|S|_0<M$. Thus, the sparsest solution to (\\ref{eq:ls}) is given by\n\n", "itemtype": "equation", "pos": 10477, "prevtext": "\n\nThe main point is that despite the noise level, amplitude order of nonzero coefficients will be maintained while dependencies in $\\rho$ vanish in the corresponding Hamming distance. Therefore, we can set $\\rho$ to be an arbitrarily value (e.g. $\\rho = 0$) and compute approximate amplitudes of each coefficients via (\\ref{eq:hamming_rho}), then form the candidate support set by selecting the supports with largest amplitudes.\n\n\\subsection{Coefficients recovery}\nProviding the candidate support set, denoted by $S$, the next part is coefficients recovery, which aims to compute the amplitudes of nonzero coefficients. In this paper, we try to compute the coefficients vector $c$ by solving the following constrained least squares problem,\n\n", "index": 15, "text": "\\begin{equation}\n\t\\label{eq:ls}\n\tc^* = \\underset{c\\in\\mathbb{R}^{\\|S\\|_0}}{\\mathrm{minimize}}\\|y-\\mathbf{A}_S\\cdot c\\|_2\\quad \\mathrm{s.t.} \\quad \\|c\\|_0\\leq K,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"c^{*}=\\underset{c\\in\\mathbb{R}^{\\|S\\|_{0}}}{\\mathrm{minimize}}\\|y-\\mathbf{A}_{%&#10;S}\\cdot c\\|_{2}\\quad\\mathrm{s.t.}\\quad\\|c\\|_{0}\\leq K,\" display=\"block\"><mrow><msup><mi>c</mi><mo>*</mo></msup><mo>=</mo><munder accentunder=\"true\"><mi>minimize</mi><mrow><mi>c</mi><mo>\u2208</mo><msup><mi>\u211d</mi><msub><mrow><mo>\u2225</mo><mi>S</mi><mo>\u2225</mo></mrow><mn>0</mn></msub></msup></mrow></munder><mo>\u2225</mo><mi>y</mi><mo>-</mo><msub><mi>\ud835\udc00</mi><mi>S</mi></msub><mo>\u22c5</mo><mi>c</mi><msub><mo>\u2225</mo><mn>2</mn></msub><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mi mathvariant=\"normal\">s</mi><mo>.</mo><mi mathvariant=\"normal\">t</mi><mo>.</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mo>\u2225</mo><mi>c</mi><msub><mo>\u2225</mo><mn>0</mn></msub><mo>\u2264</mo><mi>K</mi><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nwhere $\\setminus$ denotes the \\emph{left matrix divide} operation. (\\ref{eq:mldivide}) can be solved via the QR decomposition \\cite{trefethen1997numerical} efficiently.\n\nBased on the two parts described above, the HISTORY algorithm is fully summarized in Algorithm \\ref{alg:HISTORY}, where FindSupp$(|h|,\\alpha K)$ returns the supports of the largest $\\alpha K$ elements in $|h|$, and $\\mathrm{H_K}(\\cdot)$ denotes the hard thresholding operator which only preserve the largest $K$ coefficients and set others to 0. It's worth noting that Algorithm \\ref{alg:HISTORY} is a nearly-linear time algorithm, with its computational complexity to be $O(MN)$. Therefore, the proposed algorithm runs significantly faster than iterative algorithms.\n\n\\begin{algorithm}[ht]\n\t\\caption{HISTORY}\n\t\\label{alg:HISTORY}\n\t\\begin{algorithmic}[1]\n\t\t\\Require\n\t\t$y,\\mathbf{A},K,\\alpha$\n\t\t\\State \\textbf{Initialize:} $x^* = \\mathrm{Zeros}(N)$\n\t\t\\For{each $j\\in 1,\\dots,N$}\n\t\t\\State $P_j = H\\{y,\\mathbf{A}_j\\}/M$\n\t\t\\State $h_j = \\text{cos}(\\pi P_j)$\n\t\t\\EndFor\n\t\t\\State $S = \\mathrm{FindSupp}(|h|,\\alpha K)$\n\t\t\\State $c^* = \\mathbf{A}_\\mathcal{S} \\setminus y$\n\t\t\\State $x^*_S = c^*$\n\t\t\\If{$\\alpha>1$}\n\t\t\\State $x^* = \\text{H}_\\text{K}(|x^*|)$\n\t\t\\EndIf\n\t\t\\State $x^* = x^*/\\|x^*\\|_2$\n\t\t\\Ensure\n\t\trecovered sparse signal $x^*$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Experiments}\n\\label{sec:Results}\n\\subsection{Experimental Setup}\nThe target vector $x\\in\\mathbb{R}^N$ is generated by drawing its nonzero elements from the standard Gaussian distribution, and then normalized to have unit norm. The locations of the $K$ nonzero coefficients of $x$ are randomly selected. The elements in the measurement matrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$ are also drawn from the standard Gaussian distribution. To generate sign flips, the measurement vector $y$ is firstly acquired as in (\\ref{eq:1bitcs}), then the sign of every element in $y$ is flipped with probability $\\rho$. For each setting of $M$, $N$, $K$ and $\\rho$, the recovery experiment is repeated for 100 trials, and the average recovery error, denoted by $\\|x-x^*\\|_2/\\|x\\|_2$, is reported. In all experiments, the parameter $\\alpha$ in Algorithm \\ref{alg:HISTORY} is set to 2.\n\nThe HISTORY algorithm is compared with the following three algorithms,\n\\begin{itemize}\n\t\\item BIHT-$\\ell_2$: a heuristic algorithm proposed in \\cite{jacques2013robust}, which has been proved to have better performance than BIHT in noisy setting. The maximum iterative number and step size are set to 200 and 1, respectively \\footnote{A matlab implementation of BIHT-$\\ell_2$ algorithm can be downloaded from http://perso.uclouvain.be/laurent.jacques\\\\\n\t\t/index.php/Main/BIHTDemo.}.\n\t\\item Convex: a provable algorithm proposed in \\cite{plan2013robust}, which solves a convex optimization problem to recover the sparse signal \\footnote{The CVX package is used to solve this optimization problem. The package can be downloaded from http://cvxr.com/cvx/.}.\n\t\\item Passive: an efficient optimization algorithm with closed-form solution proposed in \\cite{zhang2014efficient}, experimental results illustrated that their passive algorithm outperforms other baselines. The regularization parameter $\\gamma$ is set to $\\sqrt{\\frac{\\log N}{M}}$, which is the optimal choice in \\cite{zhang2014efficient}.\n\\end{itemize}\n\n\\subsection{Results}\n\\subsubsection{Recovery error versus measurement dimension} First the recovery error at different measurement dimension $M$ is studied. Parameters are set as $N=1000$, $K=10$, $\\rho=0.1$, and $M$ is varied from 200 to 4000. The recovery error curve is shown in Fig. \\ref{fig:Exp_M_noisy}. It's observed that with the increase of $M$, the recovery error of all algorithms decrease. In particular, BIHT-$\\ell_2$ has worst performance among these algorithms, that is because it is very sensitive to noise in the 1-bit measurements. In contrast, HISTORY has best performance, especially when $M$ is relatively large. The recovery error of Convex is similar to Passive as in noiseless scenario.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_M_noisy.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus measurement dimension $M$, when $N = 1000$, $K = 10$, and $\\rho=0.1$.}\n\t\\label{fig:Exp_M_noisy}\n\\end{figure}\n\n\\subsubsection{Recovery error versus sparsity} Then the recovery error at different sparsity $K$ is evaluated. Parameters are set as $N=1000$, $\\rho=0.1$, $M=4000$, and $K$ is varied from 10 to 200. The recovery error curves are shown in Fig. \\ref{fig:Exp_K}. Results show that with the increase of $K$, the recovery error of all algorithms increase. In particular, among these algorithms, HISTORY has best performance while BIHT-$\\ell_2$ has worst one. In addition, Passive and Convex almost have the same performance. Finally, we would like to emphasize that HISTORY increase its advantage with the increase of $K$, i.e., it is less sensitive to sparsity than other algorithms.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_K.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus sparsity $K$, when $N = 1000$, $\\rho = 0.1$, and $M = 4000$.}\n\t\\label{fig:Exp_K}\n\\end{figure}\n\n\\subsubsection{Recovery error versus noise level} Next the recovery error at different sign flip ratio $\\rho$ is evaluated. Parameters are set as $N=1000$, $K=10$, $M=4000$, and $\\rho$ is varied from 0 to 0.5. The recovery error curves are shown in Fig. \\ref{fig:Exp_rho}. Though BIHT-$\\ell_2$ had the minimum recovery error when $\\rho$ is small, with the increase of $\\rho$, its recovery error increased very quickly, making it to be the worst algorithm at high noise level. Passive and Convex had almost the same performance, which are better than that of BIHT-$\\ell_2$. HISTORY has best performance both at high and low noise levels. Thus, HISTORY has best noise robustness among these algorithms.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_rho.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus sign flip ratio $\\rho$, when $N = 1000$, $K = 10$, and $M = 4000$.}\n\t\\label{fig:Exp_rho}\n\\end{figure}\n\n\\subsubsection{Recovery error under misspecified model} Next we study the error of each algorithm under mis-specified model, i.e., the sparsity of original signal is unknown. Parameters are set as $N=1000$, $K=10$, $M=4000$, $\\rho=0.1$, and we select $K_\\mathrm{select}$ from 1 to 20 to evaluate the algorithms. The recovery error curves are shown in Fig. \\ref{fig:Exp_K_unknown}. Results show that the recovery error of HISTORY sharply drops at the correct $K_\\mathrm{select}=K$, moreover, HISTORY performs better than Passive and Convex in a neighborhood of $K$. Under mis-specification with $K_\\mathrm{select}<K$, the recovery error is large since the error from unrecovered coefficients is large. For $K_\\mathrm{select}>K$, the nonzero coefficients are correctly recovered so that the corresponding error is small, but there is some additional error due to noise.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_K_unknown.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm when $K$ is unknown. Parameters are set to $N = 1000$, $K = 10$, $M = 4000$, and $\\rho = 0.1$. $K$ is selected from 1 to 20.}\n\t\\label{fig:Exp_K_unknown}\n\\end{figure}\n\n\\subsubsection{Computational complexity} To evaluate the computational complexity of each algorithm, we study the running time of them. Parameters are set as $N=1000$, $K=10$, $M=4000$, and $\\rho=0.1$. The running time of those algorithms can be found in Table \\ref{table_runtime}. Results show that the running time of HISTORY and Passive are similar, while that of Convex and BIHT-$\\ell_2$ are significantly higher.\n\n\\begin{table}[ht]\n\t\\caption{Running time of each algorithm, when $N=1000$, $K=10$, $M=4000$, and $\\rho=0.1$. For BIHT-$\\ell_2$, there is no formal stoping criterion, and we report the running time after 100 iterations.}\n\t\\label{table_runtime}\n\t\\centering\n\t\\begin{tabular}{l|c|c|c|c}\n\t\t\\hline\n\t\t\\bfseries Algorithm & BIHT-$\\ell_2$ & Convex & Passive & HISTORY \\\\\n\t\t\\hline\n\t\t\\bfseries Time (s) & 325.82 & 163.54 & 2.95 & 3.51 \\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\n\\section{Conclusion}\n\\label{sec:Conclusion}\nIn this paper, we develop a efficient and robust algorithm for noisy 1-bit compressive sensing. Compared with the existing methods, the proposed algorithm have several important advantages: it is robust to noise, it is computationally efficient, it has lower sample complexity, and it is easy to be implement. Experimental results provide sound support to our theoretical development.\n\n\\section*{Acknowledgments}\nThis work was supported by the National Natural Science Foundation of China under Grants 61271321, 61473207 and 61401303, the Ph.D. Programs Foundation of the Ministry of Education of China under Grant 20120032110068, and Tianjin Key Technology Research and Development Program under Grant 14ZCZDS F00025.\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{99}\n\t\n\\bibitem{candes2006near}\nE. J. Candes and T. Tao, ``Near-optimal signal recovery from random projections: Universal encoding strategies?,\" IEEE T. Inform Theory, vol.52, no.12, pp. 5406-5425, 2006.\n\n\\bibitem{donoho2006compressed}\nD. L. Donoho, ``Compressed sensing,\" IEEE T. Inform Theory, vol. 52, no. 4, pp. 1289-1306, 2006.\n\t\n\\bibitem{candes2006robust}\nE. J. Cand{\\`e}s, J. Romberg, and T. Tao, ``Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,\" IEEE T. Inform Theory, vol. 52, no. 2, pp. 489-509, 2006.\n\t\n\\bibitem{candes2008restricted}\nE. J. Cand{\\`e}s, ``The restricted isometry property and its implications for compressed sensing,\" CR Math, vol. 346, no. 9, pp. 589-592, 2008.\n\t\n\\bibitem{boufounos20081}\nP. T. Boufounos and R. G. Baraniuk, ``1-bit compressive sensing,\" Proc. 42nd Annual Conf. on Information Sciences and Systems (CISS), Princeton, USA, pp. 16-21, 2008.\n\t\n\\bibitem{boufounos2010reconstruction}\nP. T. Boufounos, ``Reconstruction of sparse signals from distorted randomized measurements,\" Proc. IEEE International Conf. on Acoustics Speech and Signal Processing (ICASSP), Dallas, USA, pp. 3998-4001, 2010.\n\t\n\\bibitem{boufounos2009greedy}\nP. T. Boufounos, ``Greedy sparse signal reconstruction from sign measurements,\" Proc. 43rd Asilomar Conference on Signals, Systems and Computers, Asilomar, USA, pp. 1305-1309, 2009.\n\t\n\\bibitem{gupta2010sample}\nA. Gupta, R. Nowak, and B. Recht, ``Sample complexity for 1-bit compressed sensing and sparse classification,\" Proc. IEEE International Symposium on Information Theory (ISIT), Austin, USA, pp. 1553-1557, 2010.\n\t\n\\bibitem{laska2011trust}\nJ. N. Laska, Z. Wen, W. Yin, and R. G. Baraniuk, ``Trust, but verify: Fast and accurate signal recovery from 1-bit compressive measurements,\" IEEE T. Signal Proces, vol. 59, no. 11, pp. 5289-5301, 2011.\n\t\n\\bibitem{zhou20121}\nT. Zhou and D. Tao, ``1-bit hamming compressed sensing,\" Proc. IEEE International Symposium on Information Theory (ISIT), Cambridge, USA, pp. 1862-1866, 2012.\n\t\n\\bibitem{jian2011investigation}\nB. Sun and J. Jiang, ``Investigation of sign spectrum sensing method,\" Acta Phys Sin-ch Ed, vol. 11, no. 32, pp. 110701, 2011.\n\t\n\\bibitem{biao2013fast}\nB. Sun, Q. Chen, X. Xu, L. Zhang, and J. Jiang, ``A fast and accurate two-stage algorithm for 1-bit compressive sensing,\" IEICE Trans. Inf. \\& Syst., vol. 96, no. 1, pp. 120-123, 2013.\n\t\n\\bibitem{gopi2013one}\nS. Gopi, P. Netrapalli, P. Jain, and A. Nori, ``One-bit compressed sensing: Provable support and vector recovery,\" Proc. 30th International Conf. on Machine Learning (ICML), Atlanta, USA, pp. 154-162, 2013.\n\t\n\\bibitem{jacques2013robust}\nL. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, ``Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors,\" IEEE T. Inform Theory, vol. 59, no. 4, pp. 2082-2102, 2013.\n\t\n\\bibitem{plan2013one}\nY. Plan and R. Vershynin, ``One-bit compressed sensing by linear programming,\" Commun Pur Appl Math, vol. 66, no. 8, pp. 1275-1297, 2013.\n\t\n\\bibitem{yan2012robust}\nM. Yan, Y. Yang, and S. Osher, ``Robust 1-bit compressive sensing using adaptive outlier pursuit,\" IEEE T. Signal Proces,  vol. 60, no. 7, pp. 3868-3875, 2012.\n\t\n\\bibitem{plan2013robust}\nY. Plan and R. Vershynin, ``Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach,\" IEEE T. Inform Theory, vol. 59, no. 1, pp. 482-494, 2013.\n\t\n\\bibitem{ai2014one}\nA. Ai, A. Lapanowski, Y. Plan, and R. Vershynin, ``One-bit compressed sensing with non-gaussian measurements,\" Linear Algebra Appl, vol. 441, pp. 222-239, 2014.\n\t\n\\bibitem{zhang2014efficient}\nL. Zhang, J. Yi, and R. Jin, ``Efficient algorithms for robust one-bit compressive sensing,\" Proc. 31st International Conf. on Machine Learning (ICML), Beijing, China,  pp. 820-828, 2014.\n\t\n\\bibitem{trefethen1997numerical}\nL. N. Trefethen and D. Bau, Numerical linear algebra, SIAM, Philadelphia, 1997.\n\t\n\\end{thebibliography}\n\n\\appendix\n\\section{Proof of Theorem \\ref{Theorem:APP}}\nIt's worth noting that\n\n", "itemtype": "equation", "pos": 10879, "prevtext": "\nwhere $\\|c\\|_0$ denotes the $0$-norm of $c$, i.e., counting the number of nonzero coefficients in $c$. It's worth noting that is a overdetermined system when $|S|_0<M$. Thus, the sparsest solution to (\\ref{eq:ls}) is given by\n\n", "index": 17, "text": "\\begin{equation}\n\t\\label{eq:mldivide}\n\tc^* = \\mathbf{A}_\\mathcal{S} \\setminus y,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"c^{*}=\\mathbf{A}_{\\mathcal{S}}\\setminus y,\" display=\"block\"><mrow><mrow><msup><mi>c</mi><mo>*</mo></msup><mo>=</mo><mrow><msub><mi>\ud835\udc00</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2216</mo><mi>y</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nWe can divide $x^{\\rm T}\\phi$ into two parts as\n\n", "itemtype": "equation", "pos": 24123, "prevtext": "\nwhere $\\setminus$ denotes the \\emph{left matrix divide} operation. (\\ref{eq:mldivide}) can be solved via the QR decomposition \\cite{trefethen1997numerical} efficiently.\n\nBased on the two parts described above, the HISTORY algorithm is fully summarized in Algorithm \\ref{alg:HISTORY}, where FindSupp$(|h|,\\alpha K)$ returns the supports of the largest $\\alpha K$ elements in $|h|$, and $\\mathrm{H_K}(\\cdot)$ denotes the hard thresholding operator which only preserve the largest $K$ coefficients and set others to 0. It's worth noting that Algorithm \\ref{alg:HISTORY} is a nearly-linear time algorithm, with its computational complexity to be $O(MN)$. Therefore, the proposed algorithm runs significantly faster than iterative algorithms.\n\n\\begin{algorithm}[ht]\n\t\\caption{HISTORY}\n\t\\label{alg:HISTORY}\n\t\\begin{algorithmic}[1]\n\t\t\\Require\n\t\t$y,\\mathbf{A},K,\\alpha$\n\t\t\\State \\textbf{Initialize:} $x^* = \\mathrm{Zeros}(N)$\n\t\t\\For{each $j\\in 1,\\dots,N$}\n\t\t\\State $P_j = H\\{y,\\mathbf{A}_j\\}/M$\n\t\t\\State $h_j = \\text{cos}(\\pi P_j)$\n\t\t\\EndFor\n\t\t\\State $S = \\mathrm{FindSupp}(|h|,\\alpha K)$\n\t\t\\State $c^* = \\mathbf{A}_\\mathcal{S} \\setminus y$\n\t\t\\State $x^*_S = c^*$\n\t\t\\If{$\\alpha>1$}\n\t\t\\State $x^* = \\text{H}_\\text{K}(|x^*|)$\n\t\t\\EndIf\n\t\t\\State $x^* = x^*/\\|x^*\\|_2$\n\t\t\\Ensure\n\t\trecovered sparse signal $x^*$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\section{Experiments}\n\\label{sec:Results}\n\\subsection{Experimental Setup}\nThe target vector $x\\in\\mathbb{R}^N$ is generated by drawing its nonzero elements from the standard Gaussian distribution, and then normalized to have unit norm. The locations of the $K$ nonzero coefficients of $x$ are randomly selected. The elements in the measurement matrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$ are also drawn from the standard Gaussian distribution. To generate sign flips, the measurement vector $y$ is firstly acquired as in (\\ref{eq:1bitcs}), then the sign of every element in $y$ is flipped with probability $\\rho$. For each setting of $M$, $N$, $K$ and $\\rho$, the recovery experiment is repeated for 100 trials, and the average recovery error, denoted by $\\|x-x^*\\|_2/\\|x\\|_2$, is reported. In all experiments, the parameter $\\alpha$ in Algorithm \\ref{alg:HISTORY} is set to 2.\n\nThe HISTORY algorithm is compared with the following three algorithms,\n\\begin{itemize}\n\t\\item BIHT-$\\ell_2$: a heuristic algorithm proposed in \\cite{jacques2013robust}, which has been proved to have better performance than BIHT in noisy setting. The maximum iterative number and step size are set to 200 and 1, respectively \\footnote{A matlab implementation of BIHT-$\\ell_2$ algorithm can be downloaded from http://perso.uclouvain.be/laurent.jacques\\\\\n\t\t/index.php/Main/BIHTDemo.}.\n\t\\item Convex: a provable algorithm proposed in \\cite{plan2013robust}, which solves a convex optimization problem to recover the sparse signal \\footnote{The CVX package is used to solve this optimization problem. The package can be downloaded from http://cvxr.com/cvx/.}.\n\t\\item Passive: an efficient optimization algorithm with closed-form solution proposed in \\cite{zhang2014efficient}, experimental results illustrated that their passive algorithm outperforms other baselines. The regularization parameter $\\gamma$ is set to $\\sqrt{\\frac{\\log N}{M}}$, which is the optimal choice in \\cite{zhang2014efficient}.\n\\end{itemize}\n\n\\subsection{Results}\n\\subsubsection{Recovery error versus measurement dimension} First the recovery error at different measurement dimension $M$ is studied. Parameters are set as $N=1000$, $K=10$, $\\rho=0.1$, and $M$ is varied from 200 to 4000. The recovery error curve is shown in Fig. \\ref{fig:Exp_M_noisy}. It's observed that with the increase of $M$, the recovery error of all algorithms decrease. In particular, BIHT-$\\ell_2$ has worst performance among these algorithms, that is because it is very sensitive to noise in the 1-bit measurements. In contrast, HISTORY has best performance, especially when $M$ is relatively large. The recovery error of Convex is similar to Passive as in noiseless scenario.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_M_noisy.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus measurement dimension $M$, when $N = 1000$, $K = 10$, and $\\rho=0.1$.}\n\t\\label{fig:Exp_M_noisy}\n\\end{figure}\n\n\\subsubsection{Recovery error versus sparsity} Then the recovery error at different sparsity $K$ is evaluated. Parameters are set as $N=1000$, $\\rho=0.1$, $M=4000$, and $K$ is varied from 10 to 200. The recovery error curves are shown in Fig. \\ref{fig:Exp_K}. Results show that with the increase of $K$, the recovery error of all algorithms increase. In particular, among these algorithms, HISTORY has best performance while BIHT-$\\ell_2$ has worst one. In addition, Passive and Convex almost have the same performance. Finally, we would like to emphasize that HISTORY increase its advantage with the increase of $K$, i.e., it is less sensitive to sparsity than other algorithms.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_K.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus sparsity $K$, when $N = 1000$, $\\rho = 0.1$, and $M = 4000$.}\n\t\\label{fig:Exp_K}\n\\end{figure}\n\n\\subsubsection{Recovery error versus noise level} Next the recovery error at different sign flip ratio $\\rho$ is evaluated. Parameters are set as $N=1000$, $K=10$, $M=4000$, and $\\rho$ is varied from 0 to 0.5. The recovery error curves are shown in Fig. \\ref{fig:Exp_rho}. Though BIHT-$\\ell_2$ had the minimum recovery error when $\\rho$ is small, with the increase of $\\rho$, its recovery error increased very quickly, making it to be the worst algorithm at high noise level. Passive and Convex had almost the same performance, which are better than that of BIHT-$\\ell_2$. HISTORY has best performance both at high and low noise levels. Thus, HISTORY has best noise robustness among these algorithms.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_rho.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm versus sign flip ratio $\\rho$, when $N = 1000$, $K = 10$, and $M = 4000$.}\n\t\\label{fig:Exp_rho}\n\\end{figure}\n\n\\subsubsection{Recovery error under misspecified model} Next we study the error of each algorithm under mis-specified model, i.e., the sparsity of original signal is unknown. Parameters are set as $N=1000$, $K=10$, $M=4000$, $\\rho=0.1$, and we select $K_\\mathrm{select}$ from 1 to 20 to evaluate the algorithms. The recovery error curves are shown in Fig. \\ref{fig:Exp_K_unknown}. Results show that the recovery error of HISTORY sharply drops at the correct $K_\\mathrm{select}=K$, moreover, HISTORY performs better than Passive and Convex in a neighborhood of $K$. Under mis-specification with $K_\\mathrm{select}<K$, the recovery error is large since the error from unrecovered coefficients is large. For $K_\\mathrm{select}>K$, the nonzero coefficients are correctly recovered so that the corresponding error is small, but there is some additional error due to noise.\n\\begin{figure}[tb]\n\t\\begin{center}\n\t\t\\includegraphics[width=.5\\textwidth]{Exp_K_unknown.eps}\n\t\\end{center}\n\t\\caption{Evaluate recovery error of each algorithm when $K$ is unknown. Parameters are set to $N = 1000$, $K = 10$, $M = 4000$, and $\\rho = 0.1$. $K$ is selected from 1 to 20.}\n\t\\label{fig:Exp_K_unknown}\n\\end{figure}\n\n\\subsubsection{Computational complexity} To evaluate the computational complexity of each algorithm, we study the running time of them. Parameters are set as $N=1000$, $K=10$, $M=4000$, and $\\rho=0.1$. The running time of those algorithms can be found in Table \\ref{table_runtime}. Results show that the running time of HISTORY and Passive are similar, while that of Convex and BIHT-$\\ell_2$ are significantly higher.\n\n\\begin{table}[ht]\n\t\\caption{Running time of each algorithm, when $N=1000$, $K=10$, $M=4000$, and $\\rho=0.1$. For BIHT-$\\ell_2$, there is no formal stoping criterion, and we report the running time after 100 iterations.}\n\t\\label{table_runtime}\n\t\\centering\n\t\\begin{tabular}{l|c|c|c|c}\n\t\t\\hline\n\t\t\\bfseries Algorithm & BIHT-$\\ell_2$ & Convex & Passive & HISTORY \\\\\n\t\t\\hline\n\t\t\\bfseries Time (s) & 325.82 & 163.54 & 2.95 & 3.51 \\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\n\\section{Conclusion}\n\\label{sec:Conclusion}\nIn this paper, we develop a efficient and robust algorithm for noisy 1-bit compressive sensing. Compared with the existing methods, the proposed algorithm have several important advantages: it is robust to noise, it is computationally efficient, it has lower sample complexity, and it is easy to be implement. Experimental results provide sound support to our theoretical development.\n\n\\section*{Acknowledgments}\nThis work was supported by the National Natural Science Foundation of China under Grants 61271321, 61473207 and 61401303, the Ph.D. Programs Foundation of the Ministry of Education of China under Grant 20120032110068, and Tianjin Key Technology Research and Development Program under Grant 14ZCZDS F00025.\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{99}\n\t\n\\bibitem{candes2006near}\nE. J. Candes and T. Tao, ``Near-optimal signal recovery from random projections: Universal encoding strategies?,\" IEEE T. Inform Theory, vol.52, no.12, pp. 5406-5425, 2006.\n\n\\bibitem{donoho2006compressed}\nD. L. Donoho, ``Compressed sensing,\" IEEE T. Inform Theory, vol. 52, no. 4, pp. 1289-1306, 2006.\n\t\n\\bibitem{candes2006robust}\nE. J. Cand{\\`e}s, J. Romberg, and T. Tao, ``Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,\" IEEE T. Inform Theory, vol. 52, no. 2, pp. 489-509, 2006.\n\t\n\\bibitem{candes2008restricted}\nE. J. Cand{\\`e}s, ``The restricted isometry property and its implications for compressed sensing,\" CR Math, vol. 346, no. 9, pp. 589-592, 2008.\n\t\n\\bibitem{boufounos20081}\nP. T. Boufounos and R. G. Baraniuk, ``1-bit compressive sensing,\" Proc. 42nd Annual Conf. on Information Sciences and Systems (CISS), Princeton, USA, pp. 16-21, 2008.\n\t\n\\bibitem{boufounos2010reconstruction}\nP. T. Boufounos, ``Reconstruction of sparse signals from distorted randomized measurements,\" Proc. IEEE International Conf. on Acoustics Speech and Signal Processing (ICASSP), Dallas, USA, pp. 3998-4001, 2010.\n\t\n\\bibitem{boufounos2009greedy}\nP. T. Boufounos, ``Greedy sparse signal reconstruction from sign measurements,\" Proc. 43rd Asilomar Conference on Signals, Systems and Computers, Asilomar, USA, pp. 1305-1309, 2009.\n\t\n\\bibitem{gupta2010sample}\nA. Gupta, R. Nowak, and B. Recht, ``Sample complexity for 1-bit compressed sensing and sparse classification,\" Proc. IEEE International Symposium on Information Theory (ISIT), Austin, USA, pp. 1553-1557, 2010.\n\t\n\\bibitem{laska2011trust}\nJ. N. Laska, Z. Wen, W. Yin, and R. G. Baraniuk, ``Trust, but verify: Fast and accurate signal recovery from 1-bit compressive measurements,\" IEEE T. Signal Proces, vol. 59, no. 11, pp. 5289-5301, 2011.\n\t\n\\bibitem{zhou20121}\nT. Zhou and D. Tao, ``1-bit hamming compressed sensing,\" Proc. IEEE International Symposium on Information Theory (ISIT), Cambridge, USA, pp. 1862-1866, 2012.\n\t\n\\bibitem{jian2011investigation}\nB. Sun and J. Jiang, ``Investigation of sign spectrum sensing method,\" Acta Phys Sin-ch Ed, vol. 11, no. 32, pp. 110701, 2011.\n\t\n\\bibitem{biao2013fast}\nB. Sun, Q. Chen, X. Xu, L. Zhang, and J. Jiang, ``A fast and accurate two-stage algorithm for 1-bit compressive sensing,\" IEICE Trans. Inf. \\& Syst., vol. 96, no. 1, pp. 120-123, 2013.\n\t\n\\bibitem{gopi2013one}\nS. Gopi, P. Netrapalli, P. Jain, and A. Nori, ``One-bit compressed sensing: Provable support and vector recovery,\" Proc. 30th International Conf. on Machine Learning (ICML), Atlanta, USA, pp. 154-162, 2013.\n\t\n\\bibitem{jacques2013robust}\nL. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, ``Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors,\" IEEE T. Inform Theory, vol. 59, no. 4, pp. 2082-2102, 2013.\n\t\n\\bibitem{plan2013one}\nY. Plan and R. Vershynin, ``One-bit compressed sensing by linear programming,\" Commun Pur Appl Math, vol. 66, no. 8, pp. 1275-1297, 2013.\n\t\n\\bibitem{yan2012robust}\nM. Yan, Y. Yang, and S. Osher, ``Robust 1-bit compressive sensing using adaptive outlier pursuit,\" IEEE T. Signal Proces,  vol. 60, no. 7, pp. 3868-3875, 2012.\n\t\n\\bibitem{plan2013robust}\nY. Plan and R. Vershynin, ``Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach,\" IEEE T. Inform Theory, vol. 59, no. 1, pp. 482-494, 2013.\n\t\n\\bibitem{ai2014one}\nA. Ai, A. Lapanowski, Y. Plan, and R. Vershynin, ``One-bit compressed sensing with non-gaussian measurements,\" Linear Algebra Appl, vol. 441, pp. 222-239, 2014.\n\t\n\\bibitem{zhang2014efficient}\nL. Zhang, J. Yi, and R. Jin, ``Efficient algorithms for robust one-bit compressive sensing,\" Proc. 31st International Conf. on Machine Learning (ICML), Beijing, China,  pp. 820-828, 2014.\n\t\n\\bibitem{trefethen1997numerical}\nL. N. Trefethen and D. Bau, Numerical linear algebra, SIAM, Philadelphia, 1997.\n\t\n\\end{thebibliography}\n\n\\appendix\n\\section{Proof of Theorem \\ref{Theorem:APP}}\nIt's worth noting that\n\n", "index": 19, "text": "\\begin{equation*}\n\\begin{split}\n& \\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi) = {\\rm sign}(\\phi_j))\\\\\n& = \\mathbb{P}(x^{\\rm T}\\phi>0,\\phi_j>0) + \\mathbb{P}(x^{\\rm T}\\phi\\leq0,\\phi_j\\leq0).\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(%&#10;\\phi_{j}))\\\\&#10;&amp;\\displaystyle=\\mathbb{P}(x^{\\rm T}\\phi&gt;0,\\phi_{j}&gt;0)+\\mathbb{P}(x^{\\rm T}\\phi%&#10;\\leq 0,\\phi_{j}\\leq 0).\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>\u2264</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 24381, "prevtext": "\nWe can divide $x^{\\rm T}\\phi$ into two parts as\n\n", "index": 21, "text": "\\begin{equation*}\nm = x^{\\rm T}\\phi = m_j + m_c,\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"m=x^{\\rm T}\\phi=m_{j}+m_{c},\" display=\"block\"><mrow><mrow><mi>m</mi><mo>=</mo><mrow><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><mi>\u03d5</mi></mrow><mo>=</mo><mrow><msub><mi>m</mi><mi>j</mi></msub><mo>+</mo><msub><mi>m</mi><mi>c</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nIn addition, it can be easily verified both $m_j$ and $m_c$ satisfy Gaussian distribution, i.e.,\n\n", "itemtype": "equation", "pos": 24452, "prevtext": "\nwhere\n\n", "index": 23, "text": "\\begin{equation*}\n\\begin{split}\n\tm_j &= \\phi_jx_j,\\\\\n\tm_c &= x^{\\rm T}\\phi-\\phi_jx_j.\\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle m_{j}&amp;\\displaystyle=\\phi_{j}x_{j},\\\\&#10;\\displaystyle m_{c}&amp;\\displaystyle=x^{\\rm T}\\phi-\\phi_{j}x_{j}.\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>c</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><mi>\u03d5</mi></mrow><mo>-</mo><mrow><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nDepending on $x_j$, we have three situations as follows,\n(1) when $x_j=0$, we have\n\t\t\n", "itemtype": "equation", "pos": 24665, "prevtext": "\nIn addition, it can be easily verified both $m_j$ and $m_c$ satisfy Gaussian distribution, i.e.,\n\n", "index": 25, "text": "\\begin{equation*}\n\\begin{split}\nm_j &\\thicksim \\mathcal{N}(0,x_j^{2}),\\\\\nm_c &\\thicksim \\mathcal{N}(0,1-x_j^{2}).\\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle m_{j}&amp;\\displaystyle\\thicksim\\mathcal{N}(0,x_{j}^{2}%&#10;),\\\\&#10;\\displaystyle m_{c}&amp;\\displaystyle\\thicksim\\mathcal{N}(0,1-x_{j}^{2}).\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>c</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nIn the same way, we have\n\t\t\n", "itemtype": "equation", "pos": 24894, "prevtext": "\nDepending on $x_j$, we have three situations as follows,\n(1) when $x_j=0$, we have\n\t\t\n", "index": 27, "text": "\\begin{equation*}\n\t\t\\begin{split}\n\t\t&\\mathbb{P}(x^{\\rm T}\\phi>0,\\phi_j>0)\\\\\n\t\t&=\\mathbb{P}(m_c>0,m_j>0)\\\\\n\t\t&=\\mathbb{P}(m_c>0)P(m_j>0)\\\\\n\t\t&=\\frac{1}{4}.\\\\\n\t\t\\end{split}\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}(x^{\\rm T}\\phi&gt;0,\\phi_{j}&gt;0)\\\\&#10;&amp;\\displaystyle=\\mathbb{P}(m_{c}&gt;0,m_{j}&gt;0)\\\\&#10;&amp;\\displaystyle=\\mathbb{P}(m_{c}&gt;0)P(m_{j}&gt;0)\\\\&#10;&amp;\\displaystyle=\\frac{1}{4}.\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nTherefore,\n\t\t\n", "itemtype": "equation", "pos": 25110, "prevtext": "\nIn the same way, we have\n\t\t\n", "index": 29, "text": "\\begin{equation*}\n\t\t\t\\mathbb{P}(x^{\\rm T}\\phi\\leqslant0,\\phi_j\\leqslant0)=\\frac{1}{4}.\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}(x^{\\rm T}\\phi\\leqslant 0,\\phi_{j}\\leqslant 0)=\\frac{1}{4}.\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>\u2a7d</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2a7d</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n(2) when $x_j>0$, we have\n\t\t\n", "itemtype": "equation", "pos": 25228, "prevtext": "\nTherefore,\n\t\t\n", "index": 31, "text": "\\begin{equation*}\n\t\t\t\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j))= \\frac{1}{2}.\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_{j}))=\\frac{1}{2}.\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nThe joint probability density function of $m_c$ and $m_j$ is\\\\\n\t\t\n", "itemtype": "equation", "pos": 25366, "prevtext": "\n(2) when $x_j>0$, we have\n\t\t\n", "index": 33, "text": "\\begin{equation*}\n\t\t\t\\mathbb{P}(x^{\\rm T}\\phi>0,\\phi_j>0) = \\mathbb{P}(m_c+m_j>0,m_j>0)\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}(x^{\\rm T}\\phi&gt;0,\\phi_{j}&gt;0)=\\mathbb{P}(m_{c}+m_{j}&gt;0,m_{j}&gt;0)\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nAssume that\n\t\t\n", "itemtype": "equation", "pos": 25537, "prevtext": "\nThe joint probability density function of $m_c$ and $m_j$ is\\\\\n\t\t\n", "index": 35, "text": "\\begin{equation*}\n\t\t\tp(m_c,m_j)=\\frac{1}{2\\pi x_j \\sqrt{1-x_j^{2}}} \\exp \\left(-\\frac{1}{2}\\Big(\\frac{m_j^{2}}{x_j^{2}}+\\frac{m_c^{2}}{1-x_j^{2}}\\Big)\\right)\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"p(m_{c},m_{j})=\\frac{1}{2\\pi x_{j}\\sqrt{1-x_{j}^{2}}}\\exp\\left(-\\frac{1}{2}%&#10;\\Big{(}\\frac{m_{j}^{2}}{x_{j}^{2}}+\\frac{m_{c}^{2}}{1-x_{j}^{2}}\\Big{)}\\right)\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><msqrt><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mfrac><msubsup><mi>m</mi><mi>j</mi><mn>2</mn></msubsup><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mfrac><mo>+</mo><mfrac><msubsup><mi>m</mi><mi>c</mi><mn>2</mn></msubsup><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nthen we have,\n\n", "itemtype": "equation", "pos": 25727, "prevtext": "\nAssume that\n\t\t\n", "index": 37, "text": "\\begin{equation*}\n\t\t\t\\begin{split}\n\t\t\t\tm_c &= r\\cos\\theta,  \\\\\n\t\t\t\tm_j & =r\\sin\\theta, \\\\\n\t\t\t\\end{split}\n\t\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle m_{c}&amp;\\displaystyle=r\\cos\\theta,\\\\&#10;\\displaystyle m_{j}&amp;\\displaystyle=r\\sin\\theta,\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>c</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mi>cos</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>m</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nIn the same way, we have\n\n", "itemtype": "equation", "pos": 25864, "prevtext": "\nthen we have,\n\n", "index": 39, "text": "\\begin{equation*}\n\\begin{split}\n& \\mathbb{P}(m_c+m_j>0,m_j>0) \\\\\n&= \\frac{1}{2\\pi x_j\\sqrt{1-x_j^{2}}}\\int^{\\frac{3}{4}\\pi}_{0}\\,d\\theta \\\\\n&\\int^{\\infty}_{0}\\exp\\left(-\\frac{1}{2}\\Big(\\frac{r^{2}\\cos\\theta^{2}}{1-x_j^{2}}+\\frac{r^{2}\\sin\\theta^{2}}{x_j^{2}}\\Big)\\right)rdr \\\\\n&= \\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_j). \\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}(m_{c}+m_{j}&gt;0,m_{j}&gt;0)\\\\&#10;&amp;\\displaystyle=\\frac{1}{2\\pi x_{j}\\sqrt{1-x_{j}^{2}}}\\int^{\\frac{3}{4}\\pi}_{0}%&#10;\\,d\\theta\\\\&#10;&amp;\\displaystyle\\int^{\\infty}_{0}\\exp\\left(-\\frac{1}{2}\\Big{(}\\frac{r^{2}\\cos%&#10;\\theta^{2}}{1-x_{j}^{2}}+\\frac{r^{2}\\sin\\theta^{2}}{x_{j}^{2}}\\Big{)}\\right)%&#10;rdr\\\\&#10;&amp;\\displaystyle=\\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_{j}).\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><msqrt><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><mrow><mpadded width=\"+1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mrow><mfrac><mn>3</mn><mn>4</mn></mfrac><mo>\u2062</mo><mi>\u03c0</mi></mrow></msubsup></mpadded><mrow><mo>\ud835\udc51</mo><mi>\u03b8</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mfrac><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mi>cos</mi><mo>\u2061</mo><msup><mi>\u03b8</mi><mn>2</mn></msup></mrow></mrow><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac><mo>+</mo><mfrac><mrow><msup><mi>r</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><msup><mi>\u03b8</mi><mn>2</mn></msup></mrow></mrow><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>r</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nTherefore, we have\n\n", "itemtype": "equation", "pos": 26240, "prevtext": "\nIn the same way, we have\n\n", "index": 41, "text": "\\begin{equation*}\n\\begin{split}\n&\\mathbb{P}(x^{\\rm T}\\phi\\leq0,\\phi_j\\leq0) \\\\\n&= P(m_c+m_j\\leq0,m_j\\leq0) \\\\\n&= \\frac{1}{2}-\\frac{1}{2\\pi}\\arccos (x_j). \\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}(x^{\\rm T}\\phi\\leq 0,\\phi_{j}\\leq 0)\\\\&#10;&amp;\\displaystyle=P(m_{c}+m_{j}\\leq 0,m_{j}\\leq 0)\\\\&#10;&amp;\\displaystyle=\\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_{j}).\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>\u2264</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n(3) when $x_j<0$,\n\n", "itemtype": "equation", "pos": 26444, "prevtext": "\nTherefore, we have\n\n", "index": 43, "text": "\\begin{equation*}\n\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j))= 1-\\frac{1}{\\pi}{\\rm arccos}(x_j).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_{j}))=1-\\frac{1}{\\pi}{\\rm&#10;arccos%&#10;}(x_{j}).\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac><mi>arccos</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nThe first part can be computed via\n\n", "itemtype": "equation", "pos": 26588, "prevtext": "\n(3) when $x_j<0$,\n\n", "index": 45, "text": "\\begin{equation*}\n\\begin{split}\n&\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j)) \\\\\n& =\\mathbb{P}(x^{\\rm T}\\phi>0,\\phi_j>0)+\\mathbb{P}(x^{\\rm T}\\phi\\leq0,\\phi_j\\leq0) \\\\\n& =\\mathbb{P}(m_c+m_j>0,m_j<0)+\\mathbb{P}(m_c+m_j\\leq0,m_j\\geq0).\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(%&#10;\\phi_{j}))\\\\&#10;&amp;\\displaystyle=\\mathbb{P}(x^{\\rm T}\\phi&gt;0,\\phi_{j}&gt;0)+\\mathbb{P}(x^{\\rm T}\\phi%&#10;\\leq 0,\\phi_{j}\\leq 0)\\\\&#10;&amp;\\displaystyle=\\mathbb{P}(m_{c}+m_{j}&gt;0,m_{j}&lt;0)+\\mathbb{P}(m_{c}+m_{j}\\leq 0,%&#10;m_{j}\\geq 0).\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo>\u2264</mo><mn>0</mn><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&lt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2264</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>\u2265</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nIn the same way, we calculate the second part as\n\n", "itemtype": "equation", "pos": 26896, "prevtext": "\nThe first part can be computed via\n\n", "index": 47, "text": "\\begin{equation*}\n\\begin{split}\n& \\mathbb{P}(m_c+m_j>0,m_j<0) \\\\\n&= \\frac{1}{2\\pi x_j\\sqrt{1-x_j^{2}}}\\int^{0}_{-\\frac{1}{4}\\pi}\\,d\\theta \\\\\n& \\int^{\\infty}_{0}\\exp\\left(-\\frac{1}{2}\\Big(\\frac{r^{2}}{1-x_j^{2}}\\cos\\theta^{2}+\\frac{r^{2}}{x_j^{2}}\\sin\\theta^{2}\\Big)\\right)rdr \\\\\n&= \\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_j).\\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\mathbb{P}(m_{c}+m_{j}&gt;0,m_{j}&lt;0)\\\\&#10;&amp;\\displaystyle=\\frac{1}{2\\pi x_{j}\\sqrt{1-x_{j}^{2}}}\\int^{0}_{-\\frac{1}{4}\\pi%&#10;}\\,d\\theta\\\\&#10;&amp;\\displaystyle\\int^{\\infty}_{0}\\exp\\left(-\\frac{1}{2}\\Big{(}\\frac{r^{2}}{1-x_{%&#10;j}^{2}}\\cos\\theta^{2}+\\frac{r^{2}}{x_{j}^{2}}\\sin\\theta^{2}\\Big{)}\\right)rdr\\\\&#10;&amp;\\displaystyle=\\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_{j}).\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&lt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub><mo>\u2062</mo><msqrt><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><mrow><mpadded width=\"+1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo>\u2062</mo><mi>\u03c0</mi></mrow></mrow><mn>0</mn></msubsup></mpadded><mrow><mo>\ud835\udc51</mo><mi>\u03b8</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><mfrac><msup><mi>r</mi><mn>2</mn></msup><mrow><mn>1</mn><mo>-</mo><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac><mo>\u2062</mo><mrow><mi>cos</mi><mo>\u2061</mo><msup><mi>\u03b8</mi><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mfrac><msup><mi>r</mi><mn>2</mn></msup><msubsup><mi>x</mi><mi>j</mi><mn>2</mn></msubsup></mfrac><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><msup><mi>\u03b8</mi><mn>2</mn></msup></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>r</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nTherefore, we have\n\n", "itemtype": "equation", "pos": 27297, "prevtext": "\nIn the same way, we calculate the second part as\n\n", "index": 49, "text": "\\begin{equation*}\n\\mathbb{P}(m_c+m_j<0,m_j>0)=\\frac{1}{2}-\\frac{1}{2\\pi}\\arccos (x_j).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}(m_{c}+m_{j}&lt;0,m_{j}&gt;0)=\\frac{1}{2}-\\frac{1}{2\\pi}\\arccos(x_{j}).\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>c</mi></msub><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&lt;</mo><mn>0</mn><mo>,</mo><msub><mi>m</mi><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></mfrac><mi>arccos</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\n(3) when $x_j<0$,\n\n", "itemtype": "equation", "pos": 26444, "prevtext": "\nTherefore, we have\n\n", "index": 43, "text": "\\begin{equation*}\n\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j))= 1-\\frac{1}{\\pi}{\\rm arccos}(x_j).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_{j}))=1-\\frac{1}{\\pi}{\\rm&#10;arccos%&#10;}(x_{j}).\" display=\"block\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac><mi>arccos</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nThis concludes the proof.\n\n\\section{Proof of Lemma \\ref{Lemma:sign-flip}}\nProviding $M\\rightarrow\\infty$ with sign flip ratio $\\rho$, the Hamming distance between $y$ and $\\phi_{j}$ can be computed via\n\n", "itemtype": "equation", "pos": 27592, "prevtext": "\nSynthesize the above three situations, we have\n\n", "index": 53, "text": "\\begin{align*}\n\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j)) & = 1-\\frac{1}{\\pi}{\\rm arccos}(x_j) \\\\\n\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign}(\\phi_j)) & =\\frac{1}{\\pi}{\\rm arccos}(x_j).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_{j}))\" display=\"inline\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=1-\\frac{1}{\\pi}{\\rm arccos}(x_{j})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac></mstyle><mo>\u2062</mo><mi>arccos</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign}(\\phi_{j}))\" display=\"inline\"><mrow><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2260</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{\\pi}{\\rm arccos}(x_{j}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac></mstyle><mo>\u2062</mo><mi>arccos</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nwhich yields\n\n", "itemtype": "equation", "pos": 28014, "prevtext": "\nThis concludes the proof.\n\n\\section{Proof of Lemma \\ref{Lemma:sign-flip}}\nProviding $M\\rightarrow\\infty$ with sign flip ratio $\\rho$, the Hamming distance between $y$ and $\\phi_{j}$ can be computed via\n\n", "index": 55, "text": "\\begin{equation*}\n\\begin{split}\n& H\\{y,\\phi_{j}\\} \\\\\n&= (1-\\rho)M\\cdot\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign}(\\phi_j)) \\\\\n&+ \\rho M\\cdot\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi_j)) \\\\\n&= \\frac{(1-\\rho)M}{\\pi}\\arccos(x_{j})+\\rho M\\left(1-\\frac{1}{\\pi}\\arccos(x_{j})\\right) \\\\\n&= \\left(\\frac{1-2\\rho}{\\pi}\\arccos(x_{j})+\\rho\\right)M,\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle H\\{y,\\phi_{j}\\}\\\\&#10;&amp;\\displaystyle=(1-\\rho)M\\cdot\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)\\neq{\\rm sign%&#10;}(\\phi_{j}))\\\\&#10;&amp;\\displaystyle+\\rho M\\cdot\\mathbb{P}({\\rm sign}(x^{\\rm T}\\phi)={\\rm sign}(\\phi%&#10;_{j}))\\\\&#10;&amp;\\displaystyle=\\frac{(1-\\rho)M}{\\pi}\\arccos(x_{j})+\\rho M\\left(1-\\frac{1}{\\pi}%&#10;\\arccos(x_{j})\\right)\\\\&#10;&amp;\\displaystyle=\\left(\\frac{1-2\\rho}{\\pi}\\arccos(x_{j})+\\rho\\right)M,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>y</mi><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03c1</mi><mo stretchy=\"false\">)</mo></mrow><mi>M</mi><mo>\u22c5</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2260</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>+</mo><mi>\u03c1</mi><mi>M</mi><mo>\u22c5</mo><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi mathvariant=\"normal\">T</mi></msup><mi>\u03d5</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>sign</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03c1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>M</mi></mrow><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mrow><mfrac><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c1</mi></mrow></mrow><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mi>\u03c1</mi></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>M</mi></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00530.tex", "nexttext": "\nThen the proof completes.\n\n\\profile[BiaoSun]{Biao Sun}{received the Ph.D. degree in electrical science \\& technology from Huazhong University of Science and Technology in 2013. He is currently an assistant professor with the Department of Electrical Engineering \\& Automation, Tianjin University, China. His research interests include neural information processing, compressed sensing, and machine learning.\n}\n\n\n\\profile[HuiFeng]{Hui Feng}{is a master's degree in the Department of Electrical Engineering \\& Automation, Tianjin University, China. She is interested in compressed sensing.\n\n\n\n\n\n}\n\\profile[XinxinXu]{Xinxin Xu}{\nreceived the Ph.D. degree in electrical science \\& technology from Huazhong University of Science and Technology in 2013. She is currently an engineer with the Microsystems Technology Center, Information Science Academy of China Electronics Technology Group Corporation. Her research interests include meta material, frequency selective surfaces,electromagnetic protection and electromagnetic measurement.\n}\n\n\n", "itemtype": "equation", "pos": 28410, "prevtext": "\nwhich yields\n\n", "index": 57, "text": "\\begin{equation*}\n\\frac{H\\{y,\\phi_{j}\\}}{M}=\\frac{1-2\\rho}{\\pi}\\arccos(x_{j})+\\rho.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\frac{H\\{y,\\phi_{j}\\}}{M}=\\frac{1-2\\rho}{\\pi}\\arccos(x_{j})+\\rho.\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>y</mi><mo>,</mo><msub><mi>\u03d5</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mi>M</mi></mfrac><mo>=</mo><mrow><mrow><mfrac><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c1</mi></mrow></mrow><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mrow><mi>arccos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mi>\u03c1</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]