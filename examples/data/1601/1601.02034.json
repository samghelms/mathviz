[{"file": "1601.02034.tex", "nexttext": "\nIt is easy to show that the maximum likelihood solution is simply $p_i = \\frac{|M_i|}{m}$. Since $\\mathcal{T}_{\\mathrm{max}}$ corresponds to the maximum clique, its prior probability will be maximum \\emph{i.e.} workers have greatest tendency to pick this hierarchy.\n\nNow on the contrary assume that the maximum likelihood hierarchy has gone undiscovered. Since there are $k$ latent hierarchies, the maximum likelihood hierarchy must have probability atleast $\\frac{1}{k}$ of being discovered, since otherwise it could not be the maximum likelihood hierarchy. The probability that the maximum likelihood hierarchy goes undiscovered after $m$ worker responses is upper bounded by $(1 - \\frac{1}{k})^m$, and the result follows.\n\\end{proof}\n}\n\n\n\n\n\\noindent Figure~\\ref{fig:figB} shows the maximum likelihood hierarchy corresponding to the maximal clique $3,4,5$ in the clustering graph of Figure~\\ref{fig:figC}. Notice that this hierarchy corresponds to organizing the dataset on {\\sc shape}. \n\nIdentifying the most probable hierarchy is thus equivalent to finding the maximum sized clique in $G_{\\mathfrak{C}}$. The size of $G_{\\mathfrak{C}}$ is atmost the number of workers $m$, since we can combine identical worker clusterings into a single node. Since $m$ is typically small $(\\le 15)$, solving {\\sc Max-Clique} is quite tractable.\n\nWe note that we do not provide an explicit mechanism for worker mistakes. In our experiments, we find that workers made no errors with respect to the organization they had in mind. Even if some workers do make errors, our maximum likelihood hierarchy only contains those workers who clustered items consistently, and so either these errors would not be incorporated, or a large number of workers would have to make these errors in the same way, which is unlikely. In the future, we plan to relax our definition of consistency of clusterings, to admit a small tolerance threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\input{sampling.tex}\n\\input{generate_sample.tex}\n\\input{merging.tex}\n\\input{categorization.tex}\n\n\\label{sec:algorithm}\n\\section{Experiments}\n\n\n\n\\label{sec:exp}\nIn our experiments, our goal is to qualitatively \nand quantitatively compare {{\\sc Orchestra}\\xspace} to other \ncrowd-powered clustering algorithms.\n\n{\\vspace{0.3em}\\noindent\\textbf{{Datasets.}}} We used the following datasets\nin our experiments. \n\\begin{denselist}\n\\item \nOur first dataset, titled {\\em shapes}, \nis a synthetic, stylized dataset\nconsisting of shapes, our running\nexample in Section~\\ref{sec:prelim}.\nAs described, each item is a random \n({\\sc shape}, {\\sc size}, {\\sc color}) tuple.\nThe images from this dataset are also the ones\ndisplayed in Figure~\\ref{fig:clusterinterface}.\nWe use this dataset because we can control \nthe organizational hierarchies in this dataset,\nallowing us to evaluate the performance of\nalgorithms on recovering clusterings across\none or more hierarchies in the dataset.\n\\item \nThe second dataset, titled {\\em scenes}, contains images\nfrom 13 categories~\\cite{fei2005bayesian}\nin natural and man-made surroundings.\n\n\nThis dataset was also used in prior work on\ncrowd clustering~\\cite{gomes2011crowdclustering,yi2012crowdclustering}.\n\\item \nThe third dataset, titled {\\em imagenet},\ncontains images from ImageNet\\cite{deng2009imagenet}. \nImages are sampled randomly from 20 categories:\n\\{buildings, cars, parrots, vulture, fruit, flower, vegetable, fighter, commercial, helicopter, ship, seahorse, whale, cheetah, lion, elephant, tiger, jellyfish, sparrow, leaves\\}.\n\\end{denselist}\nAcross all datasets, we conduct multiple runs \nof different algorithms on random subsets of the datasets.\n\n\n{\\vspace{0.3em}\\noindent\\textbf{{Algorithms.}}}\nWe compare the following state-of-the-art crowd-powered\nclustering algorithms with {{\\sc Orchestra}\\xspace}:\n\\begin{denselist}\n\\item \\texttt{CrowdClust}: This is the algorithm from Gomes et al.~\\cite{gomes2011crowdclustering}. \n\\item \\texttt{MatComp}: This is the algorithm from Yi et al.~\\cite{yi2012crowdclustering}. \n\\end{denselist}\nCode for both these algorithms was provided by the authors;\nwe faithfully set the parameters as described in the papers.\nBoth these algorithms require workers to cluster random samples\nof items repeatedly (recall Figure~\\ref{fig:prior-work}), while ensuring that each item is assigned\nthe same number of times across various samples.\n\n\nNote that since these algorithms do not use categorization,\nwe only compare the clustering phase of {{\\sc Orchestra}\\xspace} with\nthese algorithms, enabling us to compare them on an equal footing.\nIn all executions of these algorithms, we ensured that \nthe algorithms had the exact same cost as {{\\sc Orchestra}\\xspace},\ni.e., the same number of clustering tasks assigned to workers.\nWe note that employing categorization would\nonly lead to further reduction in cost---we study the benefits\nof categorization later on in this section. \n\n\n\n{\\vspace{0.3em}\\noindent\\textbf{{Evaluated Aspects.}}}\nWe evaluate the following aspects of {{\\sc Orchestra}\\xspace}:\n\\begin{denselist}\n\\item How do the eventual clusterings provided by {{\\sc Orchestra}\\xspace}\ncompare with the clusterings provided by other algorithms\nboth {\\em qualitatively and quantitatively, on real-world datasets}?\n\\item How do the eventual clusterings provided by {{\\sc Orchestra}\\xspace}\ncompare with the clusterings provided by other algorithms\nboth {\\em qualitatively and quantitatively}, on datasets where\nwe can {\\em control} the organizational hierarchies?\n\\item What is the impact, on cost and accuracy, of \nthe {\\em categorization} interface relative to the clustering interface?\n\\item What is the benefit of {\\em intelligently chosen samples} \nover randomly chosen ones for {{\\sc Orchestra}\\xspace}?\n\\item How does the quality of clustering vary with\nthe {\\em number of workers}\nproviding clusters?\n\\end{denselist}\n\n{\\vspace{0.3em}\\noindent\\textbf{{Metrics.}}}\nTo quantitatively compare {{\\sc Orchestra}\\xspace} with prior work, we adopt\ntwo metrics that are also used in prior work:\n{\\em (a) Variation of Information (VI)}~\\cite{meilua2003comparing} is an information theoretic, true distance metric used\nfor comparing clusterings. A VI of 0 indicates a perfect match. \n{\\em (b) Normalized Mutual Information (NMI)}~\\cite{cover2012elements} is a metric on $[0,1]$--- a perfect match gets a score of 1.\nIn addition, for our stylized dataset, we\nintroduce a new metric:\n{\\em (c) Clustering Hierarchies}, the number of hierarchies\nthat explain the resulting clustering returned by the algorithms.\nThis is calculated as the minimum number of organizational hierarchies used in assigning all items to its cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\input{results}\n\n\n\n\n\\section{Related Work}\\label{sec:related}\n\nOur work is related to prior work on crowd clustering, taxonomy generation,\nas well as other work on crowdsourced algorithms.\n\n{\\vspace{0.3em}\\noindent\\textbf{{Crowd-Based Clustering.}}}\nOur work is most closely related to the prior work on crowd-powered clustering\nvia a matrix completion approach, including~\\cite{gomes2011crowdclustering,yi2012semi,yi2012crowdclustering}.\nIn all these papers, worker clusterings are performed on randomly selected \nsets of items. \nThen, the results of worker clusterings are interpreted as pairwise comparisons:\nfor example, if a worker placed items a, b, c in one cluster, \nthen this is interpreted as three pairwise comparisons,\nbetween a and b, b and c, and c and a. \nSubsequently, matrix completion techniques are applied to\ninfer the missing entries in the matrix.\nWe identify multiple ways this line of work fails to take into account the complexity\nof crowd-powered organization:\n{\\em (a)} Mixing of hierarchies and frontiers: since these papers do\nnot interpret different worker responses as being derived from different\nhierarchies or frontiers within a hierarchy, they tend to provide\nclusterings that mix hierarchies and mix frontiers within a hierarchy,\nleading to poor organization.\n{{\\sc Orchestra}\\xspace}, on the other hand, carefully treats distinct hierarchies\nas well as frontiers within a hierarchy. \n\n\n\n\n{\\em (b)} \nRandom samples of items: unlike {{\\sc Orchestra}\\xspace}, which uses \nintelligently chosen samples of items, these papers use random samples\nof items.\n{\\em (c)} Loss of information: since these papers interpret worker clusterings as pairwise information within\nthe matrix, they lose valuable information,\nas opposed to {{\\sc Orchestra}\\xspace}, which operates on clusters as a whole. \n{\\em (d)}  No categorization: since the {{\\sc Orchestra}\\xspace} approach identifies the \nconsensus hierarchy, this hierarchy can be leveraged to subsequently categorize the remaining\nitems, providing further cost savings. \nNone of these papers perform categorization to further save costs.\n\n\n\n\n\nThere has been other work on variants of clustering:\nHeikinheimo and Ukkonen~\\cite{heikinheimo2013crowd} describe the {\\sc Crowd-Median} algorithm\nwhose goal is to compute centroids, as opposed to identifying clusterings.\nFor example, as soon as they locate {\\em some} representative object, they\ncan stop, instead of having to organize all the objects, like in our case. \nFurther, they do not explicitly capture different perspectives of workers, limiting\nthe applicability in practice.\n\nDavidson et al~\\cite{davidson2013using}\nprovide theoretical guarantees for aggregation ({\\sf GROUP BY}) queries, \nwhere workers are asked to answer questions of the form \\emph{\"are a and b of the same type\"}. \nThis paper makes a simplifying assumption that there is a correct answer\n(\\emph{i.e.}, there is a ground truth collection of types),\nwith workers answering incorrectly with a fixed error probability.\n{{\\sc Orchestra}\\xspace} uses a more general question type (i.e., cluster a collection of objects)\nsince it provides more context,\nand also does not make the same assumptions about worker answer correctness. \n\n\\cite{yue2014personalized} propose a \\emph{collaborative clustering} scheme where they discover user preferences for clustering as opposed to identifying a consensus clustering of the data, as we do.\n\n{\\vspace{0.3em}\\noindent\\textbf{{Crowd-Based Hierarchy Building.}}}\nA variety of papers use the crowd for hierarchy construction:\nChilton et al.~and Bragg et al.~\\cite{chilton2013cascade,bragg2013crowdsourcing} \nuse text labels and filtering on the labels to create a hierarchy\nwhile Sun et al.~and Karampinas et al.~\\cite{sun2015building,karampinas2012crowdsourcing}\nask the crowd for pairwise ancestor descendant relationships, \nalso demonstrating that identifying the optimal set of ancestor\ndescendant questions is {\\sc NP-Hard}. \nWhile hierarchy construction could, in principle, be used\nas a precursor to clustering or organization, \nnone of these papers take into account different organizational\nprinciples (i.e., the existence of many hierarchies); it remains to be seen if\nhierarchy construction can be improved by taking into account our techniques\nfor identifying organizational principles. \nAt the same time, it would be interesting to extend our algorithms\nto generate a complete hierarchy on the set of clustered items.\n\n\n\n\n\n\n{\\vspace{0.3em}\\noindent\\textbf{{Other Crowdsourcing or Active Learning Work.}}}\nPast work on active learning has utilized human workers to provide constraints for automated clustering algorithms. This work relies on human competence in making judgments for ambiguous image pairs, rather than using humans expertise in organizing data into clusters. Biswas et al.~\\cite{biswas2014active} obtain hard pairwise constraints in a crowdsourced setting by asking targeted questions related to an item pair. Lad et al.~\\cite{lad2014interactively} ask humans to provide attribute-based explanations, rather than pairwise constraints, and opt to use these as soft constraints. Neither work allows humans to explicitly cluster data.\n\nOther work focuses on learning a embedding of the data using crowd workers, and then clustering in this latent space using a standard clustering algorithm. The disadvantage of this approach is that it mashes together worker responses, and loses rich information that can be extracted from workers. Wilber et al.~\\cite{wilber2015learning} create concept embeddings by combining human experts with automation. Tamuz et al.~\\cite{tamuz2011adaptively} learn a `crowd kernel', which embeds items into a Euclidean space. Neither work explores how to cluster this embedding effectively, to extract different organizational hierarchies. \n\nPrior work on categorization does not attempt to discover organizational principles, instead presenting a predefined organization to workers, and asking them to assign items into categories. \nBoth papers in this space~\\cite{parameswaran2011human,fan2015icrowd} use graph-based approaches to carry out categorization into a taxonomy of concepts. Our work can be considered a precursor to the algorithms described in these papers, which can be integrated into our categorization step.\n\nWork on entity resolution (ER) can be regarded as clustering with a different objective: find clusters of homogenous (identical) items, in contrast to our setting, where the organizing principle is not clear. \\cite{lee2013hybrid,whangcompare,whang2013question,vesdapunt2014crowdsourcing,wang2012crowder} are all examples of work that rely on human judgments to carry out ER, all using pairwise comparisons. \n\n\\section{Conclusions and Future Work}\nWe described {{\\sc Orchestra}\\xspace}, our approach to perform \nconsensus organization of corpora using the crowd.\nWe developed techniques for identifying \nmaximum likelihood frontiers, for issuing\nadditional questions from the crowd, ensuring\nthat the eventual frontiers have high coverage,\nand combining information across different crowd answers.\nWe demonstrated the benefits of {{\\sc Orchestra}\\xspace} versus other\ncrowd-clustering schemes on three datasets\nwith different characteristics.\n\nThe organizations returned by {{\\sc Orchestra}\\xspace} are higher\nquality (up to 24\\% improvement on VI) and are more cost effective \n(up to a reduction of $6\\times$) than other\nschemes.\n\nWe believe our paper raises a number of interesting\nunanswered questions:\n{\\em (a)} Would it help to ask workers to describe, in words,\nthe clustering that they are using, and combine that information\nwith the hierarchy construction or merging algorithm?\nOnce we identify the maximum likelihood hierarchy, could\nwe ask workers to cluster on that hierarchy (in words)?\n{\\em (b)} Would it help at all to drill-down on certain nodes\nin a given hierarchy by asking workers to only organize objects\nthat are known to be part of the concept corresponding to that node?\nOne drawback of this is that we may end up mixing hierarchies:\nif we apply drill-down to a node containing triangles, we may\nend up introducing size or color as the organizational principle \nat that point.\n{\\em (c)} We observed that often the hierarchies that we obtain (corresponding\nto the cliques) may in fact share many clusterings: in such cases, we \nstill just end up picking the largest clique. Would it be possible\nto merge these hierarchies together, despite not being part of a single clique, by\nusing a more tolerant merging criteria---would that lead to any\nbenefits?\n{\\em (d)} Can we combine our algorithm with an automated scheme\nthat provides prior assessments of similarity using automatically\nextracted features?\nWe plan to address these, and other questions in follow-up work.\n{\n\\bibliographystyle{abbrv}\n\\bibliography{biblio}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 44387, "prevtext": "\n\n\n\\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}\n\n\n\n\n\n\n\\title{{\\ttlit It's just a matter of perspective(s)}: \\\\ Crowd-Powered Consensus Organization of Corpora}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\numberofauthors{1}\n\\author{\n\\alignauthor Ayush Jain, Joon Young Seo, $^\\dagger$Karan Goel, Andrew Kuznetsov \\\\ Aditya Parameswaran, Hari Sundaram\\\\\n\\affaddr{University of Illinois, Urbana Champaign, $^\\dagger$IIT Delhi} \\\\\n\\affaddr{\\{ajain42, jmseo2, akuznet2, adityagp, hs1\\}@illinois.edu; $^\\dagger$kgoel93@gmail.com}\n}\n\n\\maketitle\n\\begin{abstract}\nWe study the problem of \norganizing a collection of objects---images, videos---into clusters,\nusing crowdsourcing. \nTh\\-is problem is notoriously hard for computers to do automatically,\nand even with crowd workers, is challenging to orchestrate:\n{\\em (a)} workers may cluster based on different latent hierarchies or perspectives;\n{\\em (b)} workers may cluster\nat different granularities even when clustering using the same perspective; \nand {\\em (c)} workers may only see a small portion of the objects when deciding\nhow to cluster them (and therefore have limited understanding\nof the ``big picture'').\nWe develop cost-efficient, accurate algorithms for identifying \nthe consensus organization (i.e., the organizing perspective most\nworkers prefer to employ), and incorporate these algorithms\ninto a cost-effective workflow for organizing a collection of objects,\ntermed {{\\sc Orchestra}\\xspace}.\nWe compare our algorithms with other algorithms for clustering,\non a variety of real-world datasets, and demonstrate that\n{{\\sc Orchestra}\\xspace} organizes items better and at significantly lower costs.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\\label{sec:intro}\n\n\\vspace{-5pt}\n\\begin{quote}\n{\\em ``Everything we hear is an opinion, not a fact. \\\\ Everything we see is a perspective, not the truth.''} \\\\ --- Marcus Aurelius, ca.~150 AD.\n\\end{quote}\n\\vspace{-5pt}\n\n\nWith the costs of storage rapidly decreasing, \n\n\nwe have been amassing large volumes of images and\nvideos within our personal computers\nand within shared file systems in organizations.\nTo be able to make effective use of these images and videos, \nit is essential to {\\em organize}\nthem into clusters.\nUnfortunately, automated schemes perform poorly at  \norganization since they are not able to interpret\nor understand content adequately.\nHuman beings, on the other hand, can easily organize\nsuch content, but it is often impossible for\nany single human worker to organize a large corpus. \nSo we turn to crowdsourcing for organizing content.\n\n\nUnfortunately, employing crowdsourcing\nis rife with several issues, stemming from the fact that\nthere are often many correct ways of organizing complex content such as images.\nTo illustrate these issues (listed below), we asked 20 workers\non Amazon's Mechanical Turk to cluster\na stylized set of 25 images, \nwhere each image is a random combination\nof (\\textsc{shape}, \\textsc{color}, \\textsc{size}).\nWorkers were allowed to create as many clusters\nas they wanted, and populate these\nclusters with the 25 images.\nWe note that this is a simple experiment---we expect\nreal world corpora to be more complex.\n\n\\begin{denselist}\n\\item {\\em Issue 1: Perspectives.} Human workers often organize items using \ndistinct organizational perspectives,\nrendering the answers or clusters obtained from different\nworkers incomparable, making it hard to \ncombine opinions across workers.\nFor example,  in our experiment,\n85\\% of the workers chose to organize by \n\\textsc{shape}, 10\\% by \\textsc{color}, \nand 5\\% by \\textsc{size}.\n \n\n\n\\item {\\em Issue 2: Granularities.} Even within a single organizational perspective, \nworkers often organize at \ndifferent ``granularities''.\nFor instance, for workers that chose \nto organize based on \\textsc{shape}, \nsome chose to create the following clusters: \\{{\\tt Polygons}, {\\tt Ellip\\-ses}\\}, \nwhile others chose to split the {\\tt Polygons} cluster, giving \nus\n\\{{\\tt Rectang\\-les}, {\\tt Trian\\-gles}, {\\tt Ellipses}\\}. \nConsequently, the number of clusters given by the workers \nalso varied drastically.\n\n\\item {\\em Issue 3: Limited Understanding of the ``Big Picture''.} \nTo limit cognitive load, workers \ncan only cluster or organize a\nsmall number of items at once, making it hard for them\nto understand how the small set of items fits in with  \nthe rest.\nFor instance, if there were no triangles in the\nset of 25 items given to a worker, they would\norganize the items assuming that\ntriangles did not exist in the dataset, \nwhile that might not actually be true.\n\n\n\n\n\n\\end{denselist}\n{\\em  We \nfocus on the problem of developing a cost-efficient robust workflow to\nperform consensus organization\nof large corpora}, one\nthat majority of the workers agree with.\nIn our experiment above, we found that majority\nof the workers clustered on  {\\sc shape},\nand that would represent our consensus organizational\nperspective.\nWork from behavioral psychology\non {\\em free classification}\nhas similarly demonstrated that humans\nhave a tendency to pick a specific (likely) organizational perspective,\nwhile at the same time humans do adopt different \nperspectives~\\cite{imai1965discriminability,regehr1995category,handel1972free,medin1987family,milton2004influence}.\n\n\nPrior work has considered the problem of crowd clustering~\\cite{gomes2011crowdclustering,yi2012semi,yi2012crowdclustering},\nfalling short in three ways:\n{\\em (a)} These papers do not take into account\nthe fact that different workers may organize using\ndifferent perspectives and at different granularities, \nleading to an organization that is sub-optimal\nwith mixed organizational perspectives.\n{\\em (b)} Prior work emphasizes the use of random sampling; \nhowever in the absence of any relationship between the subsets of samples \nthat the workers see, randomized sampling is costly. \nIndeed, \\cite{gomes2011crowdclustering} report in their paper that\nthey require each item to appear in 6 \nrandom samples to ensure goodness of clustering,\nmaking it impractical in terms of cost.\n{\\em (c)} These papers transform the clusters provided\nby workers into votes on the similarity or dissimilarity of \npairs of items, losing out on the overall clustering structure. \nThis is because\nthe eventual goal of these papers is to \nrecover pairwise similarity or dissimilarity information,\nas opposed to finding a consensus organization. \nDue to these limitations, prior work can only organize\nitems appropriately if there is a single perspective\nwith no variable granularities\n(which is not true even in our stylized example above\nand certainly not true in real datasets).\nIndeed, we find that on real datasets, \ntheir results are much worse.\nWe describe related work in more detail in Section~\\ref{sec:related}.\n\n\n\n\n\n\n\n\nOur workflow, termed {{\\sc Orchestra}\\xspace}, instead uses workers to\nrepeatedly organize carefully selected groups of items.\nInstead of decomposing the responses from workers\ninto pairwise comparisons, we\noperate on them directly.\nWe develop algorithms to infer not just \nwhich organizational perspective a worker is clustering\nusing but also the granularity within.\nWe use these algorithms in conjunction with techniques\nto identify the maximum likelihood granularity\nin the maximum likelihood perspective,\nassembled into a workflow \nfor organization.\n\nThere are several challenges in assembling {{\\sc Orchestra}\\xspace}. \nFirst, \nensuring adequate coverage is hard---all clusters need to be well \nrepresented, even when individual workers may not see representatives from all clusters.\nSecond, \nit is not easy to identify if workers are clustering\non the same organizational perspective,\nespecially if they are using different granularities,\nor combining granularities.\nFor instance, a worker may provide\ntriangles, squares, non-polygons as three clusters,\nwhile another worker may provide\npolygons, ellipses, circles as three clusters;\nboth these workers are using different granularities\non the same perspective.\nThird, \nonce we identify that workers are indeed clustering using\nthe same perspective, \nit is not trivial to combine information across workers.\nIn our example given previously, no two clusters provided\nby workers are alike, making it challenging to combine information across them.\nFourth, \ncombining or relating information across workers is exacerbated by the\nfact that different workers may be clustering different sets of items;\nwe need to identify common ``pivots'' that can help us relate\nclusters across workers on different sets of items. \nLast,\nassembling repeated worker clusterings into a cost-effective\nworkflow, while setting the parameters\nthat control the workflow in a principled manner,\nis yet another challenge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a list of technical contributions in this paper:\n\\begin{denselist}\n\\item We model the problem formally using {\\em graph hierarchies} to capture\nthe notion of perspectives, and {\\em frontiers} on the hierarchies to capture the \nnotion of granularities. (Section~\\ref{sec:prelim})\n\\item We design, {{\\sc Orchestra}\\xspace}, a {\\em robust, low-cost workflow for organization}\ncomprising the following algorithmic components:\n\\begin{denselist}\n\\item We develop techniques to map worker clusterings to hierarchies\n(to identify worker perspectives), \nand formalize the identification of the consensus or the maximum likelihood hierarchy\nas a {\\sc Max-Clique} problem. (Section~\\ref{sec:hierarchyConstruc})\n\\item We develop probabilistic techniques to ensure that our maximum likelihood hierarchy\nhas {\\em adequate coverage} of the space of all concepts in the dataset. (Section~\\ref{sec:Sampling Guarantee})\n\\item We develop the notion of a {\\em kernel} to relate worker clusterings\non different samples of items to the maximum likelihood hierarchy. (Section~\\ref{sec:generateSample})\n\\item We design techniques to {\\em extend} the current maximum likelihood hierarchy by merging\nworker responses on new items to the existing hierarchy. (Section~\\ref{sec:mergingHierarchies})\n\\item We develop algorithms that operate {\\em bottom-up} \nto identify the maximum likelihood frontier on the maximum likelihood \nhierarchy, which can then be leveraged for {\\em categorization}, \nproviding further savings on cost and improved accuracies. (Section~\\ref{sec:categorize})\n\\end{denselist}\n\\item We further couple these algorithmic contributions with experiments on three real datasets\non Amazon's Mechanical Turk (Section~\\ref{sec:exp}), \nand demonstrate that our techniques lead\nto better quality clusterings, when compared both to prior work in this space, as well\nmore primitive versions of {{\\sc Orchestra}\\xspace}.\n\\end{denselist}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nIn this section we discuss some essential concepts and ideas. \n\n\n\nIn Section~\\ref{sec:datamodel}, \nwe present a sequence of definitions that helps formalize \nthe problem we address in this paper. \nIn Section~\\ref{sec:workerBehavior}, we describe our\nmodel for worker behavior and our interfaces, \nand in Section~\\ref{sec:flow}, we describe \nthe {{\\sc Orchestra}\\xspace} workflow at a high level.\nFinally, in Section~\\ref{sec:high-level}, we provide a \nbreakdown of the clustering phase of {{\\sc Orchestra}\\xspace} that\nwill be our focus in the next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}\n\\centering\n\\subfigure[][\\label{fig:Hier1}Hierarchy]{\n        \n\t\\tiny{\\begin{tikzpicture}[\n\t\t->,\n\t\t>=stealth,\n\t\tnode distance=0.3cm,\n\t\tpil/.style={\n\t\t\t->,\n\t\t\tthick,\n\t\t\tshorten =2pt,}\n\t\t]\n\t\t\\node (univ) {Universe};\n\t\t\\node[below left=of univ] (poly) {Polygons};\n\t\t\\node[below left=of poly] (quad) {Quadrilaterals};\n\t\t\\node[below right=of univ] (nonPoly){Round};\n\t\t\\node[right=of quad] (tri) {Triangles};\n\t\t\\node[below left=of quad] (rect) {Rectangles};\n\t\t\\node[right=of rect] (sq) {Squares};\n\t\t\\node[right=of sq] (eq) {Equilateral};\n\t\t\\node[right=of eq] (scalene) {Scalene};\n\t\t\\node[right=of tri] (circle) {Circles};\n\t\t\\node[right=of circle] (ell) {Ellipses};\n\t\t\\draw [->] (univ) to (poly);\n\t\t\\draw [->] (univ) to (nonPoly);\n\t\t\\draw [->] (poly) to (quad);\n\t\t\\draw [->] (poly) to (tri);\n\t\t\\draw [->] (nonPoly) to (circle);\n\t\t\\draw [->] (nonPoly) to (ell);\n\t\t\\draw [->] (quad) to (rect);\n\t\t\\draw [->] (quad) to (sq);\n\t\t\\draw [->] (tri) to (eq);\n\t\t\\draw [->] (tri) to (scalene);\n\t\\end{tikzpicture}}\n\t\n\t   }\n\t   \\qquad\n\t   \\subfigure[][\\label{fig:Hier2}Hierarchy]{\n        \n\t\\tiny{\\begin{tikzpicture}[\n\t\t->,\n\t\t>=stealth,\n\t\tnode distance=0.3cm,\n\t\tpil/.style={\n\t\t\t->,\n\t\t\tthick,\n\t\t\tshorten =2pt,}\n\t\t]\n\t\t\\node (univ) {Universe};\n\t\t\\node[below right=of univ] (cyan) {Cyan};\n\t\t\\node[below left=of univ] (green){Green};\n\t\t\\node[left=of green] (blue) {Blue};\t\t\n\t\t\\node[left=of blue] (red) {Red};\n\t\t\\node[right=of cyan] (pink) {Pink};\n\t\t\\node[right=of pink] (yellow) {Yellow};\n\t\t\\draw [->] (univ) to (red);\n\t\t\\draw [->] (univ) to (blue);\n\t\t\\draw [->] (univ) to (green);\n\t\t\\draw [->] (univ) to (cyan);\n\t\t\\draw [->] (univ) to (pink);\n\t\t\\draw [->] (univ) to (yellow);\n\t\\end{tikzpicture}}\n\t\n\t   }\n\t   \t\t   \\qquad\n\t   \\subfigure[][\\label{fig:notHier2}Not a hierarchy]{\n\t\\tiny{\\begin{tikzpicture}[\n\t\t->,\n\t\t>=stealth,\n\t\tnode distance=0.3cm,\n\t\tpil/.style={\n\t\t\t->,\n\t\t\tthick,\n\t\t\tshorten =2pt,}\n\t\t]\n\t\t\\node (univ) {Universe};\n\t\t\\node[below left=of univ] (poly) {Polygons};\n\t\t\\node[below right=of univ] (nonTri) {Non-Triangles};\n\t\t\\draw [->] (univ) to (poly);\n\t\t\\draw [->] (univ) to (nonTri);\n\t\\end{tikzpicture}}\n\t}\n\t   \\qquad\n\n\t   \\subfigure[][\\label{fig:notHier1}Not a hierarchy]{\n\t\\tiny{\\begin{tikzpicture}[\n\t\t->,\n\t\t>=stealth,\n\t\tnode distance=0.3cm,\n\t\tpil/.style={\n\t\t\t->,\n\t\t\tthick,\n\t\t\tshorten =2pt,}\n\t\t]\n\t\t\\node (univ) {Universe};\n\t\t\\node[below left=of univ] (tri) {Triangles};\n\t\t\\node[right=of tri] (nonTri) {Non-Triangles};\n\t\t\\node[below=of nonTri] (nonPoly){Round};\n\t\t\\node[below left=of tri] (eq) {Equilateral};\n\t\t\\node[right=of eq] (scalene) {Scalene};\n\t\t\\node[below left=of nonPoly] (circle) {Circles};\n\t\t\\node[right=of circle] (ell) {Ellipses};\n\t\t\\draw [->] (univ) to (poly);\n\t\t\\draw [->] (univ) to (nonTri);\n\t\t\\draw [->] (nonTri) to (nonPoly);\n\t\t\n\t\t\\draw [->] (nonPoly) to (circle);\n\t\t\\draw [->] (nonPoly) to (ell);\n\t\t\\draw [->] (tri) to (eq);\n\t\t\\draw [->] (tri) to (scalene);\n\t\\end{tikzpicture}}\n\t}\n\t\\qquad\n\t\\subfigure[][\\label{fig:notHier3}Not a hierarchy]{\n\t\\tiny{\\begin{tikzpicture}[\n\t\t->,\n\t\t>=stealth,\n\t\tnode distance=0.3cm,\n\t\tpil/.style={\n\t\t\t->,\n\t\t\tthick,\n\t\t\tshorten =2pt,}\n\t\t]\n\t\t\\node (univ) {Universe};\n\t\t\\node[below left=of univ] (poly) {Polygons};\n\t\t\\node[below left=of poly] (quad) {Qudrilaterals};\n\t\t\\node[below right=of univ] (nonPoly){Round};\n\t\t\\node[right=of quad] (tri) {Triangles};\n\t\t\\node[right=of tri] (hexagons) {Hexagons};\n\t\t\\node[below left=of quad] (rect) {Rectangles};\n\t\t\\node[right=of rect] (sq) {Squares};\n\t\t\\node[right=of sq] (eq) {Equilateral};\n\t\t\\node[right=of eq] (scalene) {Scalene};\n\t\t\\node[below=of nonPoly] (circle) {Circles};\n\t\t\\node[right=of circle] (ell) {Ellipses};\n\t\t\\draw [->] (univ) to (poly);\n\t\t\\draw [->] (univ) to (nonPoly);\n\t\t\\draw [->] (poly) to (quad);\n\t\t\\draw [->] (poly) to (tri);\n\t\t\\draw [->] (poly) to (hexagons);\n\t\t\\draw [->] (nonPoly) to (circle);\n\t\t\\draw [->] (nonPoly) to (ell);\n\t\t\\draw [->] (quad) to (rect);\n\t\t\\draw [->] (quad) to (sq);\n\t\t\\draw [->] (tri) to (eq);\n\t\t\\draw [->] (tri) to (scalene);\n\t\\end{tikzpicture}}\n\t   }\n\t   \t   \\qquad\n\t\\subfigure[][Shapes]{\\label{fig:shapes}\\includegraphics[scale=0.2]{figs/shapes_data.png}}\n\t\\vspace{-10pt}\n\t\\caption{\\label{fig:hierarchies}(a) -- (e): Concept trees for the clustering example shown in Figure~\\ref{fig:clusterinterface} --- (a) and (b) are hierarchies; (c) is not a hierarchy since it violates (3) in Definition~\\ref{def:hierarchy} --- quadrilaterals in the dataset are instances of both children of {\\tt Universe}; (d) is not a hierarchy since it violates (2) --- quadrilaterals in the dataset are instances of {\\tt Non-Triangles} but not of any children;  (e) is not a hierarchy --- {\\tt Hexagons} is a superfluous concept for this dataset. (f) Some examples of items in our Shapes dataset, which we use as a running example in this paper}\n\t\\vspace{-10pt}\n\\end{figure*}\n\n\n\n\\subsection{Data Model}\\label{sec:datamodel}\nIn this subsection, we provide a series of definitions related to four ideas: clusterings, hierarchies, frontiers and complete frontiers. First, we begin with a formal definition of clustering. \n\n\n\n\n\n\n\n\n\n\\begin{definition}[{\\bf Clustering}]\nGiven a set of items $\\mathcal{D}$, a clustering is a partitioning of $\\mathcal{D}$ into clusters $C_{1},\\dots, C_{k}$ such that,\n\\vspace{-5pt}\n\\begin{multicols}{2}\n\\begin{enumerate}\n\\item[(1)] $C_{i} \\cap C_{j} = \\emptyset \\\\ \\forall \\, i \\neq j \\, \\in \\, \\{1,\\dots,k\\}$\n\\item[(2)] $\\bigcup_{i = 1} ^{k} C_{i} = \\mathcal{D}$\n\\end{enumerate}\n\\end{multicols}\n\n\n\n\n\n\\end{definition}\n\\vspace{-5pt}\nEvery cluster in a clustering (and by consequence any set of items) can be associated with an {\\it underlying latent concept}. Intuitively, a concept is a description that is satisfied by each item in a cluster. For example, in Figure~\\ref{fig:shapes}, the clusters---from top to bottom, one corresponding to each row---represent the concepts {\\tt Triangles}, {\\tt Quadrilaterals} and {\\tt Ellipses}. Formally, a concept describes the set of common attributes shared by all items in a cluster. We say that the items in a cluster are {\\it instances} of its latent concept. Anything that holds true for a concept, also holds true for the cluster that it represents. \n\n\n\n\nConcepts may have subset-superset relationships among them\n. Formally, we say that concept $B$ {\\it generalizes} concept $A$ (denoted $B \\succ A$) if every item in $\\mathcal{D}$ that is an instance of $A$ is also an instance of $B$. For example, the concept {\\tt Quadrilaterals} generalizes {\\tt Rectangles}. We introduce the concept {\\tt Universe}, which describes any item in the corpus $\\mathcal{D}$. By definition, {\\tt Universe} generalizes every concept associated with any subset of $\\mathcal{D}$.\n\nWe can organize concepts based on the {\\it generalize} relationship into a rooted concept tree.  We call this concept tree a {\\it hierarchy}.\n\n\\begin{definition}[{\\bf Hierarchy}]\n\\label{def:hierarchy}\nFor the set of items $\\mathcal{D}$, a {hierarchy} $\\mathcal{T}(\\mathcal{D})$ is a rooted concept tree where\n\\vspace{-2pt}\n\\begin{enumerate}\\itemsep -2pt\n\\item[(1)] A concept $A \\in \\mathcal{T}(\\mathcal{D})$ is a parent of another concept $B \\in \\mathcal{T}(\\mathcal{D})$ if $A \\succ B$ and there exists no $C \\in \\mathcal{T}(\\mathcal{D})$ such that $A \\succ C$ and $C \\succ B$\n\\item[(2)] {\\tt Universe} is the root node of $\\mathcal{T}$\n\\item[(3)] Every instance of $C \\in \\mathcal{T}(\\mathcal{D})$ is also an instance of exactly one of its children in $\\mathcal{T}$ \n\\item[(4)] For every $C \\in \\mathcal{T}(\\mathcal{D})$, at least one item in $\\mathcal{D}$ is an instance of $C$.\n\\end{enumerate}\n\\end{definition}\n\n\n\n\\noindent Intuitively, a hierarchy is a concept tree in which every item of $\\mathcal{D}$ can be assigned to exactly one of the leaf nodes (and consequently all of its ancestors), and no leaf node is empty. Multiple datasets may have the same hierarchy, and a dataset may be representable by multiple hierarchies.  \n\nNote that while a hierarchy is defined in terms of concepts, each concept can be replaced by the cluster that it describes, to get a hierarchy of clusters, built on the subset relation. We will treat these hierarchies as equivalent.\n\nFigure~\\ref{fig:hierarchies} shows some concept trees for the Shapes dataset items shown in Figure~\\ref{fig:clusterinterface}. Figures~\\ref{fig:Hier1} and~\\ref{fig:Hier2} are hierarchies as every item in the dataset can be assigned to one of the leaf nodes. Other trees, shown in Figure~\\ref{fig:notHier2}, \\ref{fig:notHier1} and \\ref{fig:notHier3}, are not hierarchies. Figure~\\ref{fig:notHier2} is not a hierarchy because the concepts {\\tt Polygons} and {\\tt Non-Triangles} are not disjoint. Rectangles in the dataset are instances of both concepts and cannot lie in exactly one of them. In~\\ref{fig:notHier1}, the concept {\\tt Round} does not cover all instances of its parent concept {\\tt Non-Triangles}. The dataset has a {\\tt Quadrilaterals} concept in addition to {\\tt Round}. Figure~\\ref{fig:notHier3} is also not a hierarchy as there are no instances of {\\tt Hexagons} in the dataset.\n\nWe now describe a method to find the hierarchy corresponding to any subset of $\\mathcal{D}$, when $\\mathcal{T}(\\mathcal{D})$ is given. Let there be some set of items $\\mathcal{S} \\subseteq \\mathcal{D}$ associated with $C \\in \\mathcal{T}(\\mathcal{D})$ such that every item in $\\mathcal{S}$ is an instance of $C$. $\\mathcal{S}$ may or may not contain every instance of $C$. Consider the subtree of $\\mathcal{T}(\\mathcal{D})$ rooted at $C$. If we enforce condition (2) and (4) in our definition of a hierarchy --- replacing $C$ by the {\\tt Universe} placeholder, and dropping superfluous concept nodes in this subtree --- the resulting tree will be a hierarchy $\\mathcal{T}(\\mathcal{S})$. For instance, in Figure~\\ref{fig:Hier1}, the subtree rooted at {\\tt Polygons} is a hierarchy if $S$ is the set of all polygons in the dataset. If $S$ only contains squares and all triangles, then we would remove {\\tt Rectangles} as it is now a superfluous concept, and the leftover tree would be a hierarchy. \nWe now define the concept of a frontier.\n\n\n\n\n\n\n\n\n\n\n\n\\begin{definition}[{\\bf Frontier}]\nA frontier $F$ is a set of disjoint concepts $\\{C_{1},\\dots,C_{k}\\}$ in a hierarchy $\\mathcal{T}(\\mathcal{D})$ such that: \\\\\n$\n\\hphantom{fun stuff} \\nexists \\ i, j \\ \\in \\ \\{1,\\dots , k\\} :  C_{i} \\succ C_{j}\n$\n\\end{definition}\n\\noindent In words, a frontier is a set of disjoint concepts such that no two concepts in a frontier are connected by the \\textit{generalizes} relationship. For the hierarchy shown in Figure~\\ref{fig:Hier2}, \\{{\\tt Red}, {\\tt Green}, {\\tt Blue}\\} forms a valid frontier. Since concepts in $F$ are disjoint, an item in $\\mathcal{D}$ can be an instance of {\\it atmost} one concept in $F$. \n\n\n\\begin{definition}[{\\bf Complete Frontier}]\nA frontier $F$ in $\\mathcal{T}(\\mathcal{D})$ is said to be complete if \\ \\ \n$\\bigcup_{i= 1}^k C_{i} = \\text{{\\tt Universe}}$\n\\end{definition}\n\n\n\n\n\n\n\n\\noindent In other words, $F$, it is said to be a complete frontier if every item in $\\mathcal{D}$ is an instance of {\\it exactly} one concept in $F$. For the hierarchy of Figure~\\ref{fig:Hier2}, the frontier \\{\\texttt{Red}, \\texttt{Blue}, \\texttt{Green}\\}, when expanded to \\{\\texttt{Red}, \\texttt{Blue}, \\texttt{Green}, \\texttt{Cyan}, \\texttt{Pink}, \\texttt{Yellow}\\} becomes complete as every item in the dataset is an instance of exactly one of these concepts. Similarly, for Figure~\\ref{fig:Hier1}, \\{{\\tt Polygons},  {\\tt Circles},  {\\tt Ellipses}\\}, \\{{\\tt Quadrilaterals},  {\\tt Triangles},  {\\tt Round}\\}, \\{{\\tt Rectangles},  {\\tt Squares},  {\\tt Equilateral},  {\\tt Scalene},  {\\tt Circles},  \\\\{\\tt Ellipses}\\} are all complete frontiers. \n\nNotice the similarities in the definition of clustering and that of a complete frontier.  Just as a cluster operationalizes a concept, a clustering can be viewed as an operationalization of a complete frontier on a set of items. Thus, a complete frontier is associated with a clustering of the dataset.\n\n\n\n\n\\subsection{Interacting with Workers}\n\\label{sec:workerBehavior}\n\n\n\nWe use two interfaces to interact with workers. \nThe first interface is a {\\em clustering interface}.\nHere, workers are presented with a carousel of items, which they can drag into as many clusters as they like. This interface allows us to generate partial clusterings for a small set of items.  See Figure~\\ref{fig:clusterinterface} for an example worker session.\n\n\\begin{figure}[htbp]\n\\begin{center}\n\\includegraphics[width=0.7\\columnwidth]{figs/img2.png}\n\\caption{Our clustering interface. In this example, workers are asked to organize shapes into multiple clusters. They can determine the number of clusters by using the `+' and the `-' buttons seen on the right.}\n\\label{fig:clusterinterface}\n\\end{center}\n\\vspace{-20pt}\n\\end{figure}\n\nWe model the response to this interface, resulting in a clustering, as a frontier in some latent, underlying hierarchy.\n\nDifferent workers may have completely different latent hierarchies in mind; for instance, Figures~\\ref{fig:Hier1} and~\\ref{fig:Hier2} are both valid hierarchies for the data shown in Figure~\\ref{fig:clusterinterface}.  Thus, the worker clustering process can be modeled as follows. First, given a subset $\\mathcal{S} \\in \\mathcal{D}$, a worker picks some latent hierarchy $\\mathcal{T}(\\mathcal{S})$.  Then, the worker chooses  a complete frontier $F$ in $\\mathcal{T}(\\mathcal{S})$.  Notice that while $F$ is complete in $\\mathcal{T}(\\mathcal{S})$, it will not in general be complete in $\\mathcal{T}(\\mathcal{D})$. Finally, the output of the worker is the clustering of $\\mathcal{S}$ associated with $F$. \n\n\nWe also use a {\\em categorization interface}, which is similar to the clustering\ninterface except that a fixed number of clusters are shown, and each cluster is pre-populated\nwith a fixed set of items. Workers are asked to drag the new items into one of these existing clusters,\nthereby categorizing them. \nIn this case, workers no longer have the freedom to select their own latent\nhierarchy for organization and must instead use the clustering already provided.\n\n\n\n\n\n\n\\subsection{Overall Workflow for {\\large {{\\sc Orchestra}\\xspace}}}\\label{sec:flow}\nOur overall workflow comprises of two phases: the clustering phase and the categorization phase.\nThe clustering phase discovers a consensus organization of the data using just a small fraction of items from the corpus. \nOnce the consensus set of clusters are determined, most of the items are then organized in the categorization phase, where we place items into clusters with which they share greatest similarity. Unlike previous work~\\cite{gomes2011crowdclustering,yi2012crowdclustering}, we \\emph{don't} make workers cluster every item in the dataset, which allows us to cut costs significantly. \nAlso unlike previous work, \\emph{we do not randomly sample} items in each iteration. Instead, we systematically pick some items that are already part of the hierarchy, so that new clusterings can be easily integrated into it.\n See Figures~\\ref{fig:our-work} and~\\ref{fig:prior-work}  for a graphical comparison between \\textsc{Orchestra} and prior work. \nIn Figure~\\ref{fig:our-work}, the first three boxes refer to the clustering phase, \nwhile the last one refers to the categorization phase. \n\nThe categorization phase is straightforward, with the only goal being to categorize\nthe remaining items in the dataset; categorization will be applied to the majority of the items. \nThe transition from the clustering to the categorization phase will depend on the dataset complexity.\nOur primary focus will be the clustering phase; we describe how it is broken down, next.\n\n\n\n\n\n\n\n\n\n\\begin{figure}[t!]\n\\centering\n\\vspace{-10pt}\n\\hspace{-10pt}\n\\subfigure[Our Workflow]{\\includegraphics[scale=0.3,valign=m]{figs/our-work.pdf} \n\\label{fig:our-work}}\n\\subfigure[Prior Work Workflow]{\\vspace{-40pt}\\includegraphics[scale=0.3,valign=m]{figs/prior-work.pdf}\n\\label{fig:prior-work}}\n\\vspace{-10pt}\n\\caption{Comparison of Workflows}\n\\vspace{-20pt}\n\n\\end{figure}\n\n\n\n\n\n\n\\subsection{Clustering Phase for {\\large {{\\sc Orchestra}\\xspace}}}\\label{sec:high-level}\n\n\n\n\nGiven a dataset $\\mathcal{D}$, the goal of the clustering phase is to recover the maximum likelihood latent hierarchy $\\mathcal{T}_{ML}(\\mathcal{D})$. This hierarchy has maximum likelihood in the sense that a worker clustering the entire dataset  would pick $\\mathcal{T}_{ML}(\\mathcal{D})$ as the latent organizational hierarchy with the highest probability.\n\nWe need to generate $\\mathcal{T}_{ML}(\\mathcal{D})$ across multiple samples of the dataset. This is because in any realistic setting with large datasets, workers  will cluster a dataset $\\mathcal{S}$ where $\\mathcal{S} \\subset \\mathcal{D}$, and indeed, in general it is likely that $|\\mathcal{S}| \\ll |\\mathcal{D}|$. Given this, we must find $\\mathcal{T}_{ML}(\\mathcal{D})$ by generating multiple samples and aggregating worker responses across them. \n\n\n\n\n\nTo find $\\mathcal{T}_{ML}(\\mathcal{D})$, \\textsc{Orchestra} has an iterative refinement procedure that performs repeated iterations of (\\textsc{GenerateSample} $\\to$ \\textsc{ConstructHierarchy} $\\to$ \\textsc{MergeHierarchies} $\\to \\dots$), to generate a final hierarchy. At the end of each iteration, we generate a new estimate for $\\mathcal{T}_{ML}(\\mathcal{D})$. We give an intuitive explanation for these algorithms below; a detailed description is given in the next section. \n\\begin{denselist}\n\\item {\\sc GenerateSample.} Any sample of items that we generate must contain some item overlap with previously generated samples, as well as contain new items that explore the dataset. The overlap helps us locate worker frontiers on this sample within the current estimate of $\\mathcal{T}_{ML}(\\mathcal{D})$, while the new items allow us to expand \n$\\mathcal{T}_{ML}(\\mathcal{D})$ by finding new concepts. We provide a procedure to check if two workers---working on different samples---are providing frontiers on the same latent hierarchy.\n\\item {\\sc HierarchyConstruction.} The construction algorithm takes as input multiple worker frontiers collected for a single sample, and outputs the dominant hierarchy. To separate the dominant hierarchy, \\textsc{HierarchyConstruction} infers whether these \n frontiers are chosen from the same hierarchy, or different ones.\n\\item {\\sc MergingHierarchies.} To combine hierarchies across multiple samples, the merging algorithm takes as input two hierarchies --- the current estimate of $\\mathcal{T}_{ML}(\\mathcal{D})$, and the hierarchy constructed on the current sample. The output is a new estimate of $\\mathcal{T}_{ML}(\\mathcal{D})$, and is calculated by augmenting the current estimate of $\\mathcal{T}_{ML}(\\mathcal{D})$. The merging exploits the location of the overlap items in the current estimate of $\\mathcal{T}_{ML}(\\mathcal{D})$.\n\\end{denselist}\nAt the end of this iterative procedure, we return the maximum likelihood frontier in $\\mathcal{T}_{ML}(\\mathcal{D})$ as the consensus clustering. The quality of the consensus clustering depends on whether the number of iterations were sufficient to ensure that most items in $\\mathcal{D}$ can be categorized into this consensus clustering. \n\nIn the next section we provide the details of the workflow for \\textsc{Orchestra} --- including a sampling guarantee that gives a lower bound on the size of the samples needed to cover \\emph{atleast} some fraction of items in $\\mathcal{D}$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Orchestra Workflow}\n\\label{sec:3}\nAs noted in the previous section, workers may choose different frontiers in different latent hierarchies when asked to cluster a set of items. The problem of finding the maximum likelihood hierarchy is then equivalent to finding a hierarchy that {\\it best explains} the most worker clusterings. In this section, we provide algorithms to find this hierarchy, as well as the consensus clustering within that hierarchy. We also give theoretical results that allow us to limit the number of iterations in the {{\\sc Orchestra}\\xspace} workflow. First, in Section~\\ref{sec:hierarchyConstruc}, we describe the {\\sc HierarchyConstruction} algorithm that finds the most likely hierarchy under the assumption that all workers cluster the same set of $n$ items. Then, in Section~\\ref{sec:Sampling Guarantee}, we provide a guarantee that helps us fix a reasonable value for $n$. Section~\\ref{sec:generateSample} lays out the {\\sc GenerateSample} algorithm, and in Section~\\ref{sec:mergingHierarchies}, we generalize our setting with the {\\sc MergingHierarchies} algorithm, allowing workers to cluster different subsets of items, and aggregating their clusterings to get a single hierarchy. Finally, in Section~\\ref{sec:categorize}, we present a procedure to find the consensus clustering from the final hierarchy that our iterative workflow generates, as well as describing how we categorize items. \n\n\n{{Due to space limitations, we omit all proofs and pseudocode; they can be found in our extended technical report~\\cite{orchestra2015}.}}\n\n\\begin{figure*}[t!]\n\\subfigure[Examples of real worker clusterings for the dataset in Figure~\\ref{fig:clusterinterface}.]{\\includegraphics[width=0.48\\linewidth]{figs/FigureA.png}\\label{fig:figA}}\n\\subfigure[The hierarchy $\\mathcal{T}$ corresponding to the maximum sized clique $3,4,5$ in (c) using {\\sc ConstructHierarchy}.]{\\includegraphics[width=0.42\\linewidth]{figs/FigureC.png}\\label{fig:figC}}\n\\subfigure[The clustering graph for the worker clusterings shown in (a).]{\\includegraphics[width=0.18\\linewidth]{figs/FigureB.png}\\label{fig:figB}}\\qquad\n\\subfigure[A hypothetical hierarchy $\\mathcal{T}(\\mathcal{S})$ constructed in the 2nd iteration of our workflow, which contains an extra {\\tt Hexagons} concept.]{\\includegraphics[width=0.33\\linewidth]{figs/FigureE.png}\\label{fig:figD}}\\qquad\n\\subfigure[The hierarchy $\\mathcal{T}'$ constructed by merging (b) and (d) using {\\sc MergingHierarchies} after 2 iterations.]{\\includegraphics[width=0.42\\linewidth]{figs/FigureD.png}\\label{fig:figE}}\n\\vspace{-10pt}\n\\caption{An example demonstrating our iterative workflow approach on the Shapes dataset of Figure~\\ref{fig:shapes}.}\n\\label{fig:workflowexample}\n\\vspace{-10pt}\n\\end{figure*}\n\n\n\\subsection{\\label{sec:hierarchyConstruc}The \\textbf{\\sc \\large HierarchyConstruction} Algorithm}\nGiven a set of items $\\mathcal{S} = \\{x_{1},\\dots, x_{n}\\} \\subseteq \\mathcal{D}$, we ask $m$ workers to cluster the items in $\\mathcal{S}$. We denote the set of worker clusterings by $\\mathfrak{C} = \\{\\mathbb{C}_1, \\dots, \\mathbb{C}_m\\}$, where $\\mathbb{C}_i = \\{C_{i, 1}, ..., C_{i, k_i}\\}$ is the set of clusters proposed by worker $i$. Note that workers can give as many clusters as they like, but no cluster is allowed to be empty. Figure~\\ref{fig:figA} shows some clusterings proposed by workers on the sample of items shown in Figure~\\ref{fig:clusterinterface}.\n\n\n\n\n\n\n\n\n\n\\vspace{-2pt}\n\\begin{problem}[\\textbf{Hierarchy Construction}]\\label{prob:hierarchy}\nGiven the clusterings $\\mathfrak{C}$ on a set of items $\\mathcal{S}$, find a hierarchy $\\mathcal{T}(\\mathcal{S})$ such that the number of clusterings from $\\mathfrak{C}$ that can be associated with complete frontiers in $\\mathcal{T}(\\mathcal{S})$ is maximum.  \n\\end{problem}\n\\vspace{-4pt}\n\\noindent Intuitively, we would like to find the maximum likelihood hierarchy, \\emph{i.e.}, one that contains the maximum number of clusterings as complete frontiers. For instance,  clustering $5$ in Figure~\\ref{fig:figA} can be associated with Figure~\\ref{fig:figC} as a complete frontier, covering all items in the dataset.\n\n\nWe will show that Problem~\\ref{prob:hierarchy} is equivalent to the {\\sc Max-Clique} problem. {\\sc Max-Clique} refers to the problem of finding the maximum sized clique in a graph $G$, and is a well-known {\\sc np-hard} problem. Consequently, the optimal solution takes exponential time to compute. However, in our case, the graph for which {\\sc Max-Clique} must be solved is small, so the computation is still feasible. We will prove the equivalence to {\\sc Max-Clique} via a constructive proof. We first provide some definitions that will help us carry out the construction.\n\n\n\n\n\n\n\n\\begin{definition}[\\textbf{Consistency of Clusterings}]\n\\label{def:Consistency}\nClusterings $\\mathbb{C}_i = \\{C_{i,1}, \\dots, C_{i, k_i}\\}$ and $\\mathbb{C}_j = \\{C_{j, 1}, \\dots, C_{j, k_j}\\}$ are said to be consistent if and only if for every $(s,t) \\in \\{1, \\dots, k_i\\} \\times \\{1, \\dots, k_j\\}$, one of the following holds:\n\\vspace{-7pt}\n\\begin{multicols}{2}\n\\begin{enumerate}\n\\item[(1)] $C_{i, s} \\cap C_{j, t} = \\phi$\n\\item[(2)] $C_{i, s} \\subset C_{j, t}$\n\\item[(3)] $C_{i, s} \\supset C_{j, t}$\n\\item[(4)] $C_{i, s} = C_{j, t}$\n\\end{enumerate}\n\\end{multicols}\n\\end{definition}\n\\vspace{-7pt}\n\\noindent \nIn Figure~\\ref{fig:figA}, the worker clusterings $1$ \\& $2$ are consistent --- {\\tt Blue Shades} decomposes perfectly into {\\tt Azure Blue} and {\\tt Dark Blue}, as does {\\tt Green Shades} --- while $1$ is inconsistent with $3,4,5$.\nSince every clustering is associated with a frontier, we can also define a corresponding notion of \\emph{consistent frontiers}: we simply replace $\\supset$ by $\\succ$ in Definition~\\ref{def:Consistency}. It is useful to note that any two complete frontiers in the same hierarchy will always be consistent.  In Figure~\\ref{fig:Hier1}, the complete frontiers \\{{\\tt Quadrilaterals}, {\\tt Triangles}, {\\tt Ellipses}, {\\tt Circles}\\} and \\{{\\tt Squares}, {\\tt Rectangles}, {\\tt Triangles}, {\\tt Round}\\} are consistent. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-2pt}\n\\begin{definition}[\\textbf{Clustering Graph}]\nClustering graph $G_{\\mathfrak{C}} = (\\mathfrak{C}, E)$ is an undirected graph, where each clustering in $\\mathfrak{C}$ corresponds to a unique vertex in $G$ and there is an edge between $\\mathbb{C}_i$ and $\\mathbb{C}_j$ $\\forall \\, i,j \\in \\{1,\\dots,m\\}$ if and only if $\\mathbb{C}_i$ and $\\mathbb{C}_j$ are consistent.\n\\end{definition}\n\\vspace{-2pt}\n\n\\noindent Figure~\\ref{fig:figC} depicts the clustering graph for the clusterings shown in Figure~\\ref{fig:figA}. Each worker clustering corresponds to a node in the graph. Notice how there is no edge from $1$ to any of $3,4,5$, since they are mutually inconsistent.\n\nLet $\\mathfrak{C}_{\\mathrm{CLIQUE}} \\subseteq \\mathfrak{C}$ be a clique in $G_{\\mathfrak{C}}$. Let the set of all \\emph{unique} clusters in $\\mathfrak{C}_{\\mathrm{CLIQUE}}$ be $\\mathcal{H} = \\{C_{i, j} \\mid C_{i, j} \\in \\mathbb{C}_i, \\forall \\, \\mathbb{C}_i \\in \\mathfrak{C}_{\\mathrm{CLIQUE}}\\}$. $\\mathcal{H}$ can be organized into a hierarchy $\\mathcal{T}_{\\mathcal{H}}$ as follows: for every cluster $ C_{i, j} \\ \\in \\ \\mathcal{H}$, find the smallest cluster in $\\mathcal{H} \\cup \\text{{\\tt Universe}}$ that is a superset of $ C_{i, j}$ and mark that as the parent of $ C_{i, j}$ in $\\mathcal{T}_{\\mathcal{H}}$. \n\\techreport{Algorithm~\\ref{alg1} shows the pseudocode for this {\\sc HierarchyConstruction} algorithm.} \n\nConsider the clique $3,4,5$ in the clustering graph of Figure~\\ref{fig:figC}. $\\mathcal{H}$ contains a total of 14 clusters as shown in Figure~\\ref{fig:figA}. Suppose we wanted to find the parent of {\\tt Rectangles}; the smallest cluster in $\\mathcal{H} \\ \\cup$ {\\tt Universe} containing {\\tt Rectangles} is {\\tt Quadrilaterals}. The cluster {\\tt Universe} also contains {\\tt Rectangles} but it is not the smallest such cluster. Thus, we make {\\tt Quadrilaterals} the parent of {\\tt Rectangles}, as shown in Figure~\\ref{fig:figB}. Similarly, {\\tt Universe} becomes the parent of {\\tt Quadrilaterals}. The hierarchy after this construction is shown in Figure~\\ref{fig:figB}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\techreport{\n\\vspace{-4pt}\n\\begin{algorithm}                      \n\\caption{\\texttt{HierarchyConstruction}($\\mathbf{\\mathcal{H}})$}          \n\\label{alg1}                           \n\\begin{algorithmic}                    \n    \\REQUIRE Set of clusters $\\mathcal{H}$\n    \\ENSURE Hierarchy $\\mathcal{T}_{\\mathcal{H}}$\n    \n\t\\STATE $\\mathcal{T_{\\mathcal{H}}(V)} \\leftarrow$ $\\{{\\tt Universe}\\} \\cup \\mathcal{H}$\n    \\FOR{each $H_i \\in \\mathcal{H}$}\n    \\STATE $P \\leftarrow $ smallest sized $H_j \\in \\mathcal{H}$ that is superset of $H_i$\n    \t\\IF{$P$ is null}\n    \t\t\\STATE Parent($H_i$) $\\leftarrow $ {\\tt Universe}\n    \t\t\\ELSE\n    \t\t\\STATE Parent($H_i$) $\\leftarrow P$\n    \t\\ENDIF\n    \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\\vspace{-6pt}\n}\nWe state the following lemma and theorem which show that our construction is valid, and omit the proof. As mentioned earlier, all proofs can be found\nin our extended technical report~\\cite{orchestra2015}.\n\\vspace{-4pt}\n\\begin{lemma}\n\\label{lemma:first}\nFor any $ C_{i, j} \\in \\mathcal{H}$, the smallest cluster in $\\mathcal{H} \\, \\cup \\, \\text{{\\tt Universe}}$ that is a superset of $ C_{i, j}$, is unique.\n\\vspace{-5pt}\n\\end{lemma}\n\\techreport{\n\\begin{proof}\nSince {\\tt Universe} is the superset of all clusters in $\\mathcal{H}$, every $ C_{i, j}$ has at least one superset in $\\mathcal{H} \\cup \\ \\text{{\\tt Universe}}$. Assume that there are two distinct smallest clusters ${C}_{1,x}$ and $C_{2,y}$ that are both supersets of $ C_{i, j}$. It follows that the clusterings to which ${C}_{1,x}$ and $C_{2,y}$ belong \\emph{i.e.} $\\mathbb{C}_{1}$ and $\\mathbb{C}_{2}$ cannot be consistent. This can be seen by noting that ${C}_{1,x}$ and $C_{2,y}$ do not satisfy any of the four conditions of Definition~\\ref{def:Consistency}. This contradicts the fact that $\\mathbb{C}_{1}$ and $\\mathbb{C}_{2}$ are part of the same clique in the clustering graph, and the result follows.\n\\end{proof}\n}\n\\begin{theorem}\n$\\mathcal{T}_{\\mathcal{H}}$ is a hierarchy.\n\\end{theorem}\n\\techreport{\n\\begin{proof}\nBy Lemma~\\ref{lemma:first}, it is easy to see that $\\mathcal{T}_{\\mathcal{H}}$ is a tree with {\\tt Universe} as its root. Let $C$ be a cluster in $\\mathcal{T}_{\\mathcal{H}}$, and denote by $\\{C_{1}, \\dots, C_{k}\\}$ the children of $C$ in $\\mathcal{T}_{\\mathcal{H}}$. To prove that $\\mathcal{T}_{\\mathcal{H}}$ is a hierarchy, we must show that for every such $C$, (i) $C_i \\cap C_j = \\phi \\ \\ \\forall \\, i\\ne j \\in \\{1,\\dots,k\\}$ and (ii) $\\bigcup_{i=1}^{k} C_i = C$. \n\nFor (i), 2 cases arise: either $C_i$ and $C_j$ are both from the same clustering and are disjoint by definition, or they come from different clusterings, in which case their corresponding clusterings would not be consistent if $C_i \\cap C_j \\ne \\phi$.\n\nFor (ii), we know that $\\bigcup_{i=1}^{k} C_i \\subseteq C$ by construction. Now suppose $\\bigcup_{i=1}^{k} C_i \\ne C$ and let $X = C \\setminus \\bigcup_{i=1}^{k} C_i$. Items in $X$ are not see in any child of $C$. \n\nLet $\\mathbb{C}_1,\\dots,\\mathbb{C}_k$ be clusterings that contain $C_1,\\dots,C_k$ respectively. Each $\\mathbb{C}_i$ contains atleast $C_i$. $C$ cannot be in any $\\mathbb{C}_i$, since that $\\mathbb{C}_i$ would no longer remain disjoint. Every $\\mathbb{C}_i$ is a clustering on $\\mathcal{S}$ and therefore cluster all items in $X$. \n\nFor every $\\mathbb{C}_i$, items in $X$ cannot lie in $C_i$ and must lie in other clusters that are not children of $C$. For any item $x \\in X$, consider the largest cluster $C_{\\mathrm{large}}$ that contains $x$ across $\\mathbb{C}_1,\\dots,\\mathbb{C}_k$. Since $C_{\\mathrm{large}}$ is the largest such cluster, its parent --- from our construction --- cannot lie in $\\mathbb{C}_1,\\dots,\\mathbb{C}_k$. It is easy to see that the smallest sized cluster that contains it must be $C$. Therefore, $C_{\\mathrm{large}}$ is a child of $C$ which leads us to a contradiction.\n\n\n\n\n\n\\end{proof}\n}\n\nSuppose we pick $\\mathfrak{C}_{\\mathrm{CLIQUE}}$ to be the maximum sized clique in $G_{\\mathfrak{C}}$, and let $\\mathcal{T}_{\\mathrm{max}}$ be the hierarchy that is generated using this clique. We now state an important result.\n\\begin{theorem}\nSuppose every clustering $\\mathbb{C} \\in \\mathfrak{C}$ lies in exactly one maximal clique.\n\n\n\n\n\nAlso suppose the total number of latent hierarchies is $k$. Then, $\\mathcal{T}_{\\mathrm{max}}$ is the maximum-likelihood hierarchy with probability atleast $\\left[1 - \\left(1 - \\frac{1}{k}\\right)^m\\right]$, where $m$ is the number of workers. \n\n\\end{theorem}\n\\techreport{\n\\begin{proof}\nFirst assume that the maximum likelihood hierarchy has not gone undiscovered. Notice that every maximal clique in $G_{\\mathfrak{C}}$ will correspond to a single, distinct hierarchy. Denote by $M_1,\\dots,M_k$, the maximal cliques in $G_{\\mathfrak{C}}$, where $M_i \\subseteq {\\mathfrak{C}}$. Let $\\mathcal{T}_1,\\dots,\\mathcal{T}_k$ be the set of hierarchies corresponding to these maximal cliques \\emph{i.e.} $\\mathcal{T}_i$ corresponds to $M_i$.\n\nSince every clustering lies in exactly one maximal clique $M_i$, each worker's clustering can be viewed as a vote for that hierarchy $\\mathcal{T}_i$. We can then define a multinomial distribution $(m,p_1,\\dots,p_k)$ that captures worker tendency to pick a particular latent hierarchy - $p_i$ is the probability that a worker picks the hierarchy $\\mathcal{T}_i$.\n\nThe likelihood function corresponding to the $m$ trials (clusterings) can be written as,\n\n", "index": 1, "text": "\\begin{equation*}\n\\mathcal{L} \\propto p_1^{|M_1|}p_2^{|M_2|}\\dots p_k^{|M_k|}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}\\propto p_{1}^{|M_{1}|}p_{2}^{|M_{2}|}\\dots p_{k}^{|M_{k}|}\" display=\"block\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u221d</mo><mrow><msubsup><mi>p</mi><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mn>1</mn></msub><mo stretchy=\"false\">|</mo></mrow></msubsup><mo>\u2062</mo><msubsup><mi>p</mi><mn>2</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><msubsup><mi>p</mi><mi>k</mi><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow></msubsup></mrow></mrow></math>", "type": "latex"}]