[{"file": "1601.04331.tex", "nexttext": "\nThe weights $(p_1,\\dots,p_L)$ are determined through stick-breaking from latent beta$(1,\\alpha)$ random variables\n(where $p_{L}=$ $1-\\sum_{l=1}^{L-1} p_{l}$), and the $(\\mu_l,\\Sigma_l)$ are independent and identically distributed \n(i.i.d.) from some base distribution $G_0$. Partitioning $\\mu_l$ and $\\Sigma_l$ with superscripts $x$ and $y$ corresponding to $z_{t-1}$ and $z_t$, respectively, the conditional transition density implied by \n(\\ref{eqn:joint_dens}) can be written as\n\n", "itemtype": "equation", "pos": 9743, "prevtext": "\n\n\\doublespacing\n\n\\title{A Bayesian Nonparametric Markovian Model for Nonstationary Time Series}\n\\author{Maria DeYoreo and Athanasios Kottas\n\\thanks{\nM. DeYoreo (maria.deyoreo@stat.duke.edu) is Postdoctoral Researcher, Department of Statistical Science, Duke University, and A. Kottas\n(thanos@ams.ucsc.edu) is Professor, Department of Applied Mathematics\nand Statistics, University of California, Santa Cruz. The work of the first author was supported \nby the National Science Foundation under award SES 1131897. The work of the second author \nwas supported in part by the National Science Foundation under awards DMS 1310438\nand DMS 1407838.}}\n\n\\date{}\n\\maketitle\n\n\n\\begin{abstract}\n\\noindent\nStationary time series models built from parametric distributions are, in general, limited in scope due \nto the assumptions imposed on the residual distribution and autoregression relationship. We present\na modeling approach for univariate time series data, which makes no assumptions of stationarity, and \ncan accommodate complex dynamics and capture nonstandard distributions. The model arises from \na Bayesian nonparametric mixture of normals specification for the joint distribution of successive \nobservations in time. This implies a flexible autoregressive form for the conditional transition density, \ndefining a time-homogeneous, nonstationary, Markovian model for real-valued data indexed in \ndiscrete-time. To obtain a more computationally tractable algorithm for posterior inference, we\nutilize a square-root-free Cholesky decomposition of the mixture kernel covariance matrix.\nResults from simulated data suggest the model is able to recover challenging transition and \npredictive densities. We also illustrate the model on time intervals between eruptions of the \nOld Faithful geyser. Extensions to accommodate higher order structure and to develop a \nstate-space model are also discussed.\n\\end{abstract}\n\n\\noindent\nKEY WORDS: Autoregressive models; Bayesian nonparametrics; Dirichlet process mixtures; \nMarkov chain Monte Carlo; nonstationarity; time series.\n\n\n\\newpage\n\n\n\\section{Introduction}\n\\label{sec:intro}\n\nConsider a time series of continuous random variables $(Z_1,\\dots,Z_n)$ observed at equally spaced time points $t=1,\\dots,n$.  It is common to assume dependence on lagged terms, or that $Z_t$ depends on \n$(Z_{t-1},\\dots,Z_{t-p})$, for some $p\\geq1$. The relationship between $Z_t$ and $(Z_{t-1},\\dots,Z_{t-p})$ \nis generally assumed to be linear, with error terms arising from a given parametric distribution. \nThe simplest scenario involves $p=1$ and normally distributed errors, referred to as a first-order \nGaussian autoregression.\n\nStationary time series models built from parametric distributions are not appropriate for many applications.  \nStochastic systems may go through structural changes, and as a consequence, the data they produce may require models which change across time. While stationarity is a convenient property, stationary models are often \nrestrictive in terms of the transition and marginal densities they imply. Time series may exhibit marginal \ndistributions which are asymmetric, and predictive distributions which are nonlinear in the effects of past \nobservations on the mean, and heteroskedasticity. Economic time series are generally believed to be \nnonstationary, often exhibiting distinct periods of high and low volatility, motivating the development of \nstochastic volatility and autoregressive (AR) conditional heteroskedasticity models, among others \\citep{fruwirth}.\n\nVarious parametric models have been developed to capture nonlinear AR behavior and/or relax the \nstationarity assumption. Time-varying autoregressions (TVAR) naturally extend \nAR models, by allowing the parameters to evolve in time, and thus can be used to describe nonstationary \ntime series. TVAR models have a dynamic linear model (DLM) representation and belong to the larger class \nof Markovian state-space models. Such models require specification of an observation density and a state \nevolution density, which need not rely on normality or linearity, though these are common assumptions. \n\nThe DLM framework can be made more flexible by combining multiple DLMs, referred to as multiprocess models \\citep{westharrison}. Mixture models of various forms have been used to move away from parametric assumptions, and capture changes over time in a series which may not be described well by a single model. The threshold autoregressive (TAR) model \\citep{tong,geweke} describes an AR process whose parameters switch according to the value of a previous observation, and is a special case of the Markov switching autoregressive model. \nWe refer to \\citet{tong1990} for a review of nonlinear time series, and \\citet{fruwirth} for a thorough review \nof mixture models for time series. Mixture autoregressive models \\citep{juangrabiner,wongli} are also special \ncases of Markov switching AR models, in which the parameters of the autoregression change according to \na hidden Markov process. \n\n \nThe models discussed above generally achieve nonstationarity or nonlinearity by allowing parameters to switch or evolve in time. These models are naturally suited to problems in which a single parametric model holds in a given interval of time. For instance, the TAR structure assumes only one linear submodel applies at any particular time, with abrupt changes at the thresholds. In contrast, mixture models can be obtained by introducing hierarchical priors on model parameters, to yield a set of parametric models which are favored with different probabilities across time. These models possess the ability to capture features which could not be accommodated under the assumption of a single parametric distribution at a particular point in time. To this end, a mixture modeling approach involving Bayesian nonparametric techniques was first proposed by \\citet{mullerwestmac}. More recently, \\citet{dilucca} have utilized dependent Dirichlet process priors to build countable mixtures of AR models as well as variations of this model. \n\\citet{antoniano} developed stationary time series models which contain flexible transition and invariant densities. \nExisting mixture models for time series are discussed further in Section \\ref{sec:background}, relative to our \nproposed model.\n\n\nThe restrictions commonly imposed on the residual distribution and autoregression relationship limit the scope \nof parametric AR models. Here, we present a general framework for modeling univariate time series data, which \nmakes no assumptions of stationarity, and can accommodate complex dynamics and capture non-standard distributions. The model arises from a Bayesian nonparametric mixture of normals specification for the joint distribution of successive observations in time. This implies a flexible AR model structure for the conditional \ntransition density. In particular, the transition density takes the form of a location-scale mixture of normal \ndensities, with means and mixture weights which depend on the previous state(s). Key to the posterior \nsimulation method is a square-root-free Cholesky decomposition of the mixture kernel covariance matrix.\nAs demonstrated with synthetic and real data, the model enables general inference for time-homogeneous, nonstationary, Markovian processes indexed in discrete time.\n\n\nThe rest of the paper is organized as follows. The methodology is presented in Section \\ref{sec:methods},\nincluding the model formulation for the transition density, and methods for posterior simulation and prior\nspecification. To place our contribution within the relevant literature, we also discuss certain classes of \nmixture models for discrete-time Markovian processes. In Section \\ref{sec:applications}, the modeling \napproach is illustrated with simulated data examples, and it is also applied to a standard data set \non waiting times between successive eruptions of the Old Faithful geyser. While the model presented \nand all data illustrations are focused on univariate time series data with first-order dependence, we \ndiscuss possible extensions to accommodate higher order structure, and to develop a state-space \nmodel in Section \\ref{sec:extensions}. Finally, Section \\ref{sec:conclusion} concludes with a summary.\n\n\n\n\n\\section{Methodology}\n\\label{sec:methods}\n\n\\subsection{Model Formulation}\n\\label{sec:model}\n\nHere, we present the model for nonstationary time series. We focus on the case with first-order\nMarkovian dependence, discussing the extension to modeling higher order time series in \nSection \\ref{sec:extensions}. Hence, the observed time series, $(z_{1},\\dots,z_{n})$, is assumed to \nbe a realization from a time-homogeneous, real-valued, first-order Markov chain, and thus the \nlikelihood, conditional on $z_{1}$, is given by $\\prod_{t=2}^{n} f(z_{t} \\mid z_{t-1})$.\nThe model for the transition density, $f(z_{t} \\mid z_{t-1})$, is induced by a nonparametric \nmixture of bivariate normal distributions for $f(z_{t-1},z_{t})$, which can accommodate \na wide range of density shapes and complex dependencies between $Z_t$ and $Z_{t-1}$.\n\n\nMore specifically, let $f(z_{t-1},z_{t}) \\equiv$ $f(z_{t-1},z_{t};G)=$\n$\\int \\text{N}(z_{t-1},z_{t};\\mu,\\Sigma) \\, \\text{d}G(\\mu,\\Sigma)$, with a Dirichlet process (DP) \nprior \\citep{ferguson} placed on the random mixing distribution $G$. Thus, any two successive \nobservations in time are distributed as a DP mixture of bivariate normals. \nIn the ensuing model expressions, we work with a truncated version of $G$ motivated by the \nDP constructive definition \\citep{sethuraman}, which is also the approach we follow for \nposterior simulation \\citep{ishjames}. Under a truncated DP at level $L$, the joint density \ncan be expressed as \n\n", "index": 1, "text": "\\begin{equation}\n\\label{eqn:joint_dens}\nf(z_{t-1},z_{t};G)\\approx \\sum_{l=1}^L p_l\\text{N}(z_{t-1},z_t;\\mu_l,\\Sigma_l).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"f(z_{t-1},z_{t};G)\\approx\\sum_{l=1}^{L}p_{l}\\text{N}(z_{t-1},z_{t};\\mu_{l},%&#10;\\Sigma_{l}).\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><msub><mi>p</mi><mi>l</mi></msub><mo>\u2062</mo><mtext>N</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><msub><mi>\u03bc</mi><mi>l</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nwith \n\n", "itemtype": "equation", "pos": 10359, "prevtext": "\nThe weights $(p_1,\\dots,p_L)$ are determined through stick-breaking from latent beta$(1,\\alpha)$ random variables\n(where $p_{L}=$ $1-\\sum_{l=1}^{L-1} p_{l}$), and the $(\\mu_l,\\Sigma_l)$ are independent and identically distributed \n(i.i.d.) from some base distribution $G_0$. Partitioning $\\mu_l$ and $\\Sigma_l$ with superscripts $x$ and $y$ corresponding to $z_{t-1}$ and $z_t$, respectively, the conditional transition density implied by \n(\\ref{eqn:joint_dens}) can be written as\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eqn:trans_dens}\nf(z_t \\mid z_{t-1};G) = \\sum_{l=1}^L q_l(z_{t-1})\\mathrm{N}\\left(z_t; \\mu_l^y+\\Sigma_l^{yx}(\\Sigma_l^{xx})^{-1}(z_{t-1}-\\mu_l^x),\\Sigma_l^{yy}-(\\Sigma_l^{yx})^2(\\Sigma_l^{xx})^{-1}\\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"f(z_{t}\\mid z_{t-1};G)=\\sum_{l=1}^{L}q_{l}(z_{t-1})\\mathrm{N}\\left(z_{t};\\mu_{%&#10;l}^{y}+\\Sigma_{l}^{yx}(\\Sigma_{l}^{xx})^{-1}(z_{t-1}-\\mu_{l}^{x}),\\Sigma_{l}^{%&#10;yy}-(\\Sigma_{l}^{yx})^{2}(\\Sigma_{l}^{xx})^{-1}\\right)\" display=\"block\"><mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>\u2223</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>q</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">N</mi><mrow><mo>(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>y</mi></msubsup><mo>+</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>y</mi></mrow></msubsup><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>y</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nThis transition density is therefore a location-scale mixture of normal transition densities, \nwith means which depend on the previous state in a linear fashion, and weights which favor \nmixture component $l$ if $z_{t-1}$ is near $\\mu_l^x$. This defines a general time-homogeneous \nMarkovian model which can handle nonstationary time series.\n\n\nAs  discussed above, the transition density in (\\ref{eqn:trans_dens}) arises from the flexible and well-studied DP mixture of normals model for two successive observations in time. Conditional on an initial value $z_1$, the likelihood $\\prod_{t=2}^n f(z_t \\mid z_{t-1};G)$ is a product of conditional densities, each being a mixture of normals. \nThe associated mixture weights, given by (\\ref{eqn:weights}), contain $\\{\\mu_l^x\\}$ and $\\{\\Sigma_l^{xx}\\}$ \nin the denominator, and each mixture component variance in (\\ref{eqn:trans_dens}) contains a complex function \nof the elements of $\\Sigma_l$. Hence, with respect to posterior simulation, there does not exist a choice \nof $G_0$ which allows the full conditional distributions for $\\mu_l^x$, $\\Sigma_l^{xx}$, $\\Sigma_l^{yy}$, or $\\Sigma_l^{yx}$ to be recognizable as standard distributions. \n\n\nThese difficulties are alleviated to some extent by employing a square-root-free Cholesky decomposition \nof the covariance matrix $\\Sigma$ \\citep{daniels,webb}, which expresses $\\Sigma$ in terms of a unit lower \ntriangular matrix $\\beta$ \nand a  diagonal matrix $\\Delta$ with positive elements, such that $\\Sigma=\\beta^{-1}\\Delta(\\beta^{-1})^T$. The utility of this parametrization lies in the following property. If $(Y_1,\\dots,Y_m)\\sim \\mathrm{N}(\\mu,\\beta^{-1}\\Delta(\\beta^{-1})^T)$, with $(\\delta_1,\\dots,\\delta_m)$ on the diagonal of $\\Delta$, then the joint distribution of $Y$ can be expressed in a recursive form: $Y_1\\sim \\mathrm{N}(\\mu_1,\\delta_1)$, and \n$(Y_k \\mid Y_1,\\dots,Y_{k-1})\\sim \\mathrm{N}(\\mu_k-\\sum_{j=1}^{k-1}\\beta_{k,j}(y_j-\\mu_j),\\delta_k)$, \nfor $k=2,\\dots,m$. \nWith this parameterization of the mixture kernel covariance matrix, the mixture transition density  \n(\\ref{eqn:trans_dens}) admits the form \n\n", "itemtype": "equation", "pos": 10608, "prevtext": "\nwith \n\n", "index": 5, "text": "\\begin{equation}\n\\label{eqn:weights}\nq_l(z_{t-1}) = p_l\\mathrm{N}(z_{t-1};\\mu_l^x,\\Sigma_l^{xx}) /\n\\left\\{ \\sum\\nolimits_{m=1}^L p_{m} \\mathrm{N}(z_{t-1};\\mu_m^x,\\Sigma_m^{xx}) \\right\\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"q_{l}(z_{t-1})=p_{l}\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\Sigma_{l}^{xx})/\\left\\{%&#10;\\sum\\nolimits_{m=1}^{L}p_{m}\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\Sigma_{m}^{xx})%&#10;\\right\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>q</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>p</mi><mi>l</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>l</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mrow><mo>{</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mrow><msub><mi>p</mi><mi>m</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi mathvariant=\"normal\">\u03a3</mi><mi>m</mi><mrow><mi>x</mi><mo>\u2062</mo><mi>x</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": " \nwith \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThis transition density is therefore a location-scale mixture of normal transition densities, \nwith means which depend on the previous state in a linear fashion, and weights which favor \nmixture component $l$ if $z_{t-1}$ is near $\\mu_l^x$. This defines a general time-homogeneous \nMarkovian model which can handle nonstationary time series.\n\n\nAs  discussed above, the transition density in (\\ref{eqn:trans_dens}) arises from the flexible and well-studied DP mixture of normals model for two successive observations in time. Conditional on an initial value $z_1$, the likelihood $\\prod_{t=2}^n f(z_t \\mid z_{t-1};G)$ is a product of conditional densities, each being a mixture of normals. \nThe associated mixture weights, given by (\\ref{eqn:weights}), contain $\\{\\mu_l^x\\}$ and $\\{\\Sigma_l^{xx}\\}$ \nin the denominator, and each mixture component variance in (\\ref{eqn:trans_dens}) contains a complex function \nof the elements of $\\Sigma_l$. Hence, with respect to posterior simulation, there does not exist a choice \nof $G_0$ which allows the full conditional distributions for $\\mu_l^x$, $\\Sigma_l^{xx}$, $\\Sigma_l^{yy}$, or $\\Sigma_l^{yx}$ to be recognizable as standard distributions. \n\n\nThese difficulties are alleviated to some extent by employing a square-root-free Cholesky decomposition \nof the covariance matrix $\\Sigma$ \\citep{daniels,webb}, which expresses $\\Sigma$ in terms of a unit lower \ntriangular matrix $\\beta$ \nand a  diagonal matrix $\\Delta$ with positive elements, such that $\\Sigma=\\beta^{-1}\\Delta(\\beta^{-1})^T$. The utility of this parametrization lies in the following property. If $(Y_1,\\dots,Y_m)\\sim \\mathrm{N}(\\mu,\\beta^{-1}\\Delta(\\beta^{-1})^T)$, with $(\\delta_1,\\dots,\\delta_m)$ on the diagonal of $\\Delta$, then the joint distribution of $Y$ can be expressed in a recursive form: $Y_1\\sim \\mathrm{N}(\\mu_1,\\delta_1)$, and \n$(Y_k \\mid Y_1,\\dots,Y_{k-1})\\sim \\mathrm{N}(\\mu_k-\\sum_{j=1}^{k-1}\\beta_{k,j}(y_j-\\mu_j),\\delta_k)$, \nfor $k=2,\\dots,m$. \nWith this parameterization of the mixture kernel covariance matrix, the mixture transition density  \n(\\ref{eqn:trans_dens}) admits the form \n\n", "index": 7, "text": "\\begin{equation}\n\\label{eqn:trans_dens_reparam}\nf(z_t \\mid z_{t-1};G) = \\sum_{l=1}^Lq_l(z_{t-1}) \\mathrm{N}(z_t;\\mu_l^y-\\beta_l(z_{t-1}-\\mu_l^x),\\delta_l^y)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"f(z_{t}\\mid z_{t-1};G)=\\sum_{l=1}^{L}q_{l}(z_{t-1})\\mathrm{N}(z_{t};\\mu_{l}^{y%&#10;}-\\beta_{l}(z_{t-1}-\\mu_{l}^{x}),\\delta_{l}^{y})\" display=\"block\"><mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>\u2223</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>q</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>y</mi></msubsup><mo>-</mo><msub><mi>\u03b2</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>y</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nwhere, in the case of the $2 \\times 2$ covariance matrix $\\Sigma$, $\\beta$ represents the only \nfree element of the lower triangular matrix, and $\\Delta$ has diagonal elements $(\\delta^x,\\delta^y)$. \n\n\nLet $\\eta_l=(\\mu_l^x,\\mu_l^y,\\beta_l,\\delta_l^x,\\delta_l^y)$, for $l=1,\\dots,L$, denote the mixing parameters.  \nThe mixture transition density can be broken by introducing latent configuration variables $\\{U_2,\\dots,U_n\\}$ \ntaking values in $\\{1,\\dots,L\\}$, with $\\mathrm{Pr}(U_t=l)=q_l(z_{t-1})$, such that the augmented hierarchical \nmodel for the data becomes:\n\\begin{eqnarray}\n\\label{eqn:hier_model}\nz_t \\mid z_{t-1},U_t,\\{\\eta_l\\}  \\stackrel{ind.}{\\sim} \n\\mathrm{N}(\\mu_{U_t}^y-\\beta_{U_t}(z_{t-1}-\\mu_{U_t}^x),\\delta_{U_t}^y), \\,\\,\\,\\,\\, t=2,\\dots,n \\notag \\\\\nU_t \\mid z_{t-1},p,\\mu^x,\\delta^x \\stackrel{ind.}{\\sim} \\sum_{l=1}^L\n\\frac{p_l\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)}{\\sum_{m=1}^Lp_m \\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)}\nI(U_{t} = l),   \\,\\,\\,\\,\\,  t=2,\\dots,n \\notag \\\\\n\\eta_l \\mid \\psi \\stackrel{i.i.d.}{\\sim} G_0(\\eta_l \\mid \\psi), \\,\\,\\,\\,\\,  l=1,\\dots,L\n\\end{eqnarray}\nand the prior density for $p=(p_1,\\dots p_L)$ is given by a special case of the generalized \nDirichlet distribution: $f(p \\mid \\alpha)=$\n$\\alpha^{L-1}p_L^{\\alpha-1}(1-p_1)^{-1}(1-(p_1+p_2))^{-1}\\times\\dots\\times(1-\\sum_{l=1}^{L-2}p_l)^{-1}$\n\\citep{connor}. The base distribution $G_0$ comprises independent components:  \n$\\mathrm{N}(m^x,v^x)$ and $\\mathrm{N}(m^y,v^y)$ for $\\mu_{l}^{x}$ and $\\mu_{l}^{y}$;\n$\\mathrm{IG}(\\nu^x,s^x)$ and $\\mathrm{IG}(\\nu^y,s^y)$ for $\\delta_l^x$ and $\\delta_l^y$;\nand $\\mathrm{N}(\\theta,c)$ for $\\beta_{l}$.\nThis choice is conjugate for $\\{\\delta_l^y\\}$, $\\{\\beta_l\\}$, and $\\{\\mu_l^y\\}$. The full Bayesian model \nis completed with conditionally conjugate priors on $\\psi=$ $(m^x,v^x,m^y,v^y,s^x,s^y,\\theta,c)$, \nthe hyperparameters of $G_{0}$:\n\\begin{eqnarray}\n\\label{eqn:priors}\nm^x\\sim \\mathrm{N}(a_m^x,b_m^x), \\; m^y\\sim \\mathrm{N}(a_m^y,b_m^y), \\; v^x\\sim \\mathrm{IG}(a_v^x,b_v^x), \\; v^y\\sim \\mathrm{IG}(a_v^y,b_v^y), \\notag \\\\\ns^x\\sim \\mathrm{Ga}(a_s^x,b_s^x), \\; s^y\\sim\\mathrm{Ga}(a_s^y,b_s^y), \\; \\theta\\sim \\mathrm{N}(a_\\theta,b_\\theta), \\; c\\sim\\mathrm{IG}(a_c,b_c)\n\\end{eqnarray}\nand a gamma prior for the DP precision parameter, $\\alpha\\sim \\mathrm{Ga}(a_{\\alpha},b_{\\alpha})$.\n\n\n\n\\subsection{Posterior Inference}\n\\label{sec:MCMC}\n\nSamples from the full posterior distribution \n\nof the model are obtained using a combination of Gibbs sampling and Metropolis-Hastings \nsteps. Here, we describe posterior simulation details for all model parameters, focusing \nparticular attention on the vector $(p_1,\\dots,p_L)$, which requires the most care in \ndeveloping an effective updating strategy. \n\n\nThe full conditional distributions for $\\alpha$ and the components of vector $\\psi$ \nare standard as they are assigned conditionally conjugate priors. \n\n Each $U_t$, $t=2,\\dots,n$ is sampled from a discrete distribution on $\\{1,\\dots,L\\}$, with probabilities $(\\tilde{p}_{1,t},\\dots,\\tilde{p}_{L,t})$, where $\\tilde{p}_{l,t}\\propto p_l\\mathrm{N}(z_t;\\mu_{l}^y-\\beta_{l}(z_{t-1}-\\mu_{l}^x),\\delta_{l}^y)\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)$, for $l=1,\\dots,L$.\n\nNext, consider the mixing parameters. Letting $\\{U_j^*:j=1,\\dots,n^*\\}$ be the $n^*$ distinct values of $(U_2,\\dots,U_n)$, and $M_l=|\\{U_t:U_t=l\\}|$, we obtain the full conditional \n", "itemtype": "equation", "pos": 13109, "prevtext": " \nwith \n\n", "index": 9, "text": "\\begin{equation}\n\\label{eqn:weights_reparam}\nq_l(z_{t-1}) = p_l\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x) /\n\\left\\{ \\sum\\nolimits_{m=1}^L p_{m} \\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_{m}^{x}) \\right\\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"q_{l}(z_{t-1})=p_{l}\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})/\\left\\{\\sum%&#10;\\nolimits_{m=1}^{L}p_{m}\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\delta_{m}^{x})\\right\\}\" display=\"block\"><mrow><mrow><msub><mi>q</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>p</mi><mi>l</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mrow><mo>{</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mrow><msub><mi>p</mi><mi>m</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>m</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\n Therefore, if $l\\in \\{U_j^*\\}$, $\\mu_l^y$ is sampled from a normal distribution with variance $(v^y)^*=[(\\nu^y)^{-1}+M_l(\\delta_l^y)^{-1}]^{-1}$, and mean $(v^y)^*[(\\nu^y)^{-1}m^y+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=U_j^*\\}}(z_t+\\beta_l(z_{t-1}-\\mu_l^x))]$. If component $l$ is empty, that is, $l\\notin \\{U_j^*\\}$, then $\\mu_l^y\\sim \\mathrm{N}(m^y,v^y)$. The updates for $\\delta_l^y$ and $\\beta_l$ also require only Gibbs sampling. If $l\\in \\{U_j^*\\}$, then $\\delta_l^y\\sim \\mathrm{IG}(\\nu^y+0.5M_l,s^y+0.5\\sum_{\\{t:U_t=l\\}}(z_t-\\mu_l^y+\\beta_l(z_{t-1}-\\mu_l^x))^2)$ and $\\beta_l$ is sampled from a normal with variance $c^*=[c^{-1}+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=l\\}}(z_{t-1}-\\mu_l^x)^2]^{-1}$ and mean $c^*[c^{-1}\\theta+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=l\\}}(z_{t-1}-\\mu_l^x)(\\mu_l^y-z_t)]$. \nIf $l\\notin \\{U_j^*\\}$, then we sample from $G_0$: $\\delta_l^y\\sim \\mathrm{IG}(\\nu^y,s^y)$ and \n$\\beta_l\\sim \\mathrm{N}(\\theta,c)$.\n\n\nNo matter the choice of $G_0$, the full conditionals for $\\mu_l^x$ and $\\delta_l^x$ are not proportional \nto any standard distribution, as these parameters are contained in the sum of $L$ terms in the \ndenominator of $q_l(z_{t-1})$. The posterior full conditional $p(\\mu_l^x \\mid \\dots,\\mathrm{data})$, \nwhen $l\\in\\{U_j^*\\}$, is given by \n", "itemtype": "equation", "pos": 16692, "prevtext": "\nwhere, in the case of the $2 \\times 2$ covariance matrix $\\Sigma$, $\\beta$ represents the only \nfree element of the lower triangular matrix, and $\\Delta$ has diagonal elements $(\\delta^x,\\delta^y)$. \n\n\nLet $\\eta_l=(\\mu_l^x,\\mu_l^y,\\beta_l,\\delta_l^x,\\delta_l^y)$, for $l=1,\\dots,L$, denote the mixing parameters.  \nThe mixture transition density can be broken by introducing latent configuration variables $\\{U_2,\\dots,U_n\\}$ \ntaking values in $\\{1,\\dots,L\\}$, with $\\mathrm{Pr}(U_t=l)=q_l(z_{t-1})$, such that the augmented hierarchical \nmodel for the data becomes:\n\\begin{eqnarray}\n\\label{eqn:hier_model}\nz_t \\mid z_{t-1},U_t,\\{\\eta_l\\}  \\stackrel{ind.}{\\sim} \n\\mathrm{N}(\\mu_{U_t}^y-\\beta_{U_t}(z_{t-1}-\\mu_{U_t}^x),\\delta_{U_t}^y), \\,\\,\\,\\,\\, t=2,\\dots,n \\notag \\\\\nU_t \\mid z_{t-1},p,\\mu^x,\\delta^x \\stackrel{ind.}{\\sim} \\sum_{l=1}^L\n\\frac{p_l\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)}{\\sum_{m=1}^Lp_m \\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)}\nI(U_{t} = l),   \\,\\,\\,\\,\\,  t=2,\\dots,n \\notag \\\\\n\\eta_l \\mid \\psi \\stackrel{i.i.d.}{\\sim} G_0(\\eta_l \\mid \\psi), \\,\\,\\,\\,\\,  l=1,\\dots,L\n\\end{eqnarray}\nand the prior density for $p=(p_1,\\dots p_L)$ is given by a special case of the generalized \nDirichlet distribution: $f(p \\mid \\alpha)=$\n$\\alpha^{L-1}p_L^{\\alpha-1}(1-p_1)^{-1}(1-(p_1+p_2))^{-1}\\times\\dots\\times(1-\\sum_{l=1}^{L-2}p_l)^{-1}$\n\\citep{connor}. The base distribution $G_0$ comprises independent components:  \n$\\mathrm{N}(m^x,v^x)$ and $\\mathrm{N}(m^y,v^y)$ for $\\mu_{l}^{x}$ and $\\mu_{l}^{y}$;\n$\\mathrm{IG}(\\nu^x,s^x)$ and $\\mathrm{IG}(\\nu^y,s^y)$ for $\\delta_l^x$ and $\\delta_l^y$;\nand $\\mathrm{N}(\\theta,c)$ for $\\beta_{l}$.\nThis choice is conjugate for $\\{\\delta_l^y\\}$, $\\{\\beta_l\\}$, and $\\{\\mu_l^y\\}$. The full Bayesian model \nis completed with conditionally conjugate priors on $\\psi=$ $(m^x,v^x,m^y,v^y,s^x,s^y,\\theta,c)$, \nthe hyperparameters of $G_{0}$:\n\\begin{eqnarray}\n\\label{eqn:priors}\nm^x\\sim \\mathrm{N}(a_m^x,b_m^x), \\; m^y\\sim \\mathrm{N}(a_m^y,b_m^y), \\; v^x\\sim \\mathrm{IG}(a_v^x,b_v^x), \\; v^y\\sim \\mathrm{IG}(a_v^y,b_v^y), \\notag \\\\\ns^x\\sim \\mathrm{Ga}(a_s^x,b_s^x), \\; s^y\\sim\\mathrm{Ga}(a_s^y,b_s^y), \\; \\theta\\sim \\mathrm{N}(a_\\theta,b_\\theta), \\; c\\sim\\mathrm{IG}(a_c,b_c)\n\\end{eqnarray}\nand a gamma prior for the DP precision parameter, $\\alpha\\sim \\mathrm{Ga}(a_{\\alpha},b_{\\alpha})$.\n\n\n\n\\subsection{Posterior Inference}\n\\label{sec:MCMC}\n\nSamples from the full posterior distribution \n\nof the model are obtained using a combination of Gibbs sampling and Metropolis-Hastings \nsteps. Here, we describe posterior simulation details for all model parameters, focusing \nparticular attention on the vector $(p_1,\\dots,p_L)$, which requires the most care in \ndeveloping an effective updating strategy. \n\n\nThe full conditional distributions for $\\alpha$ and the components of vector $\\psi$ \nare standard as they are assigned conditionally conjugate priors. \n\n Each $U_t$, $t=2,\\dots,n$ is sampled from a discrete distribution on $\\{1,\\dots,L\\}$, with probabilities $(\\tilde{p}_{1,t},\\dots,\\tilde{p}_{L,t})$, where $\\tilde{p}_{l,t}\\propto p_l\\mathrm{N}(z_t;\\mu_{l}^y-\\beta_{l}(z_{t-1}-\\mu_{l}^x),\\delta_{l}^y)\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)$, for $l=1,\\dots,L$.\n\nNext, consider the mixing parameters. Letting $\\{U_j^*:j=1,\\dots,n^*\\}$ be the $n^*$ distinct values of $(U_2,\\dots,U_n)$, and $M_l=|\\{U_t:U_t=l\\}|$, we obtain the full conditional \n", "index": 11, "text": "\n\\[\np(\\eta_l \\mid \\dots,\\mathrm{data}) \\propto G_0(\\eta_l \\mid \\psi)\n\\left\\{\n \\prod_{j=1}^{n^*}\\prod_{\\{t:U_t=U_j^*\\}}\\mathrm{N}(z_t;\\mu_{l}^y-\\beta_{l}(z_{t-1}-\\mu_{l}^x),\\delta_{l}^y) \\right\\}\n\\left\\{  \\prod_{l=1}^L \\prod_{\\{t:U_t=l\\}}q_l(z_{t-1}) \\right\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"p(\\eta_{l}\\mid\\dots,\\mathrm{data})\\propto G_{0}(\\eta_{l}\\mid\\psi)\\left\\{\\prod_%&#10;{j=1}^{n^{*}}\\prod_{\\{t:U_{t}=U_{j}^{*}\\}}\\mathrm{N}(z_{t};\\mu_{l}^{y}-\\beta_{%&#10;l}(z_{t-1}-\\mu_{l}^{x}),\\delta_{l}^{y})\\right\\}\\left\\{\\prod_{l=1}^{L}\\prod_{\\{%&#10;t:U_{t}=l\\}}q_{l}(z_{t-1})\\right\\}.\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b7</mi><mi>l</mi></msub><mo>\u2223</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>data</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><msub><mi>G</mi><mn>0</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b7</mi><mi>l</mi></msub><mo>\u2223</mo><mi>\u03c8</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>{</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>n</mi><mo>*</mo></msup></munderover><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo>:</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><msubsup><mi>U</mi><mi>j</mi><mo>*</mo></msubsup></mrow><mo stretchy=\"false\">}</mo></mrow></munder><mi mathvariant=\"normal\">N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>y</mi></msubsup><mo>-</mo><msub><mi>\u03b2</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>y</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>}</mo></mrow><mrow><mo>{</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo>:</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><mi>l</mi></mrow><mo stretchy=\"false\">}</mo></mrow></munder><msub><mi>q</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nThis can be written as $p(\\mu_l^x|\\dots,\\mathrm{data})\\propto \\mathrm{N}(\\mu_l^x;(m^x)^*,(v^x)^*)(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x))^{-1}$, with $(v^x)^*=((v^x)^{-1}+M_l(\\delta_l^x)^{-1}+M_l\\beta_l^2(\\delta_l^y)^{-1})$ and $(m^x)^*=(v^x)^*((v^x)^{-1}m^x+(\\delta_l^x)^{-1}\\sum_{\\{t:U_t=l\\}}z_{t-1}+(\\delta_l^y)^{-1}\\beta_l^2\\sum_{\\{t:U_t=l\\}}(z_{t-1}+(z_t-\\mu_l^y)/\\beta_l))$. We use a random-walk Metropolis step to update $\\mu_l^x$. For $l\\notin\\{U_j^*\\}$, \n$p(\\mu_l^x \\mid \\dots,\\mathrm{data})$ is proportional to $\\mathrm{N}(\\mu_l^x;m^x,v^x)[\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)]^{-1}$, and in this case we use a Metropolis-Hastings algorithm, proposing  a candidate value $\\mu_l^x$ from the base distribution $\\mathrm{N}(m^x,v^x)$. \n\nThe full conditional and sampling strategy for $\\delta_l^x$ are similar to those for $\\mu_l^x$. We have \n", "itemtype": "equation", "pos": 18215, "prevtext": "\n Therefore, if $l\\in \\{U_j^*\\}$, $\\mu_l^y$ is sampled from a normal distribution with variance $(v^y)^*=[(\\nu^y)^{-1}+M_l(\\delta_l^y)^{-1}]^{-1}$, and mean $(v^y)^*[(\\nu^y)^{-1}m^y+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=U_j^*\\}}(z_t+\\beta_l(z_{t-1}-\\mu_l^x))]$. If component $l$ is empty, that is, $l\\notin \\{U_j^*\\}$, then $\\mu_l^y\\sim \\mathrm{N}(m^y,v^y)$. The updates for $\\delta_l^y$ and $\\beta_l$ also require only Gibbs sampling. If $l\\in \\{U_j^*\\}$, then $\\delta_l^y\\sim \\mathrm{IG}(\\nu^y+0.5M_l,s^y+0.5\\sum_{\\{t:U_t=l\\}}(z_t-\\mu_l^y+\\beta_l(z_{t-1}-\\mu_l^x))^2)$ and $\\beta_l$ is sampled from a normal with variance $c^*=[c^{-1}+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=l\\}}(z_{t-1}-\\mu_l^x)^2]^{-1}$ and mean $c^*[c^{-1}\\theta+(\\delta_l^y)^{-1}\\sum_{\\{t:U_t=l\\}}(z_{t-1}-\\mu_l^x)(\\mu_l^y-z_t)]$. \nIf $l\\notin \\{U_j^*\\}$, then we sample from $G_0$: $\\delta_l^y\\sim \\mathrm{IG}(\\nu^y,s^y)$ and \n$\\beta_l\\sim \\mathrm{N}(\\theta,c)$.\n\n\nNo matter the choice of $G_0$, the full conditionals for $\\mu_l^x$ and $\\delta_l^x$ are not proportional \nto any standard distribution, as these parameters are contained in the sum of $L$ terms in the \ndenominator of $q_l(z_{t-1})$. The posterior full conditional $p(\\mu_l^x \\mid \\dots,\\mathrm{data})$, \nwhen $l\\in\\{U_j^*\\}$, is given by \n", "index": 13, "text": "\n\\[\n\\mathrm{N}\\left(\\mu_l^x;m^x,v^x\\right)\n\\prod_{\\{t:U_t=l\\}}\\mathrm{N}\\left(z_t;\\mu_{l}^y-\\beta_{l}(z_{t-1}-\\mu_{l}^x),\\delta_{l}^y\\right)\n\\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)\\left(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)\\right)^{-1}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{N}\\left(\\mu_{l}^{x};m^{x},v^{x}\\right)\\prod_{\\{t:U_{t}=l\\}}\\mathrm{N}%&#10;\\left(z_{t};\\mu_{l}^{y}-\\beta_{l}(z_{t-1}-\\mu_{l}^{x}),\\delta_{l}^{y}\\right)%&#10;\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})\\left(\\prod_{t=2}^{n}\\sum_{m=1}^%&#10;{L}p_{m}\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\delta_{m}^{x})\\right)^{-1}.\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>;</mo><msup><mi>m</mi><mi>x</mi></msup><mo>,</mo><msup><mi>v</mi><mi>x</mi></msup><mo>)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo>:</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><mi>l</mi></mrow><mo stretchy=\"false\">}</mo></mrow></munder><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>z</mi><mi>t</mi></msub><mo>;</mo><mrow><msubsup><mi>\u03bc</mi><mi>l</mi><mi>y</mi></msubsup><mo>-</mo><mrow><msub><mi>\u03b2</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>y</mi></msubsup><mo>)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><msub><mi>p</mi><mi>m</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>m</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nwhich for an active component, is written as proportional to \n", "itemtype": "equation", "pos": 19394, "prevtext": "\nThis can be written as $p(\\mu_l^x|\\dots,\\mathrm{data})\\propto \\mathrm{N}(\\mu_l^x;(m^x)^*,(v^x)^*)(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x))^{-1}$, with $(v^x)^*=((v^x)^{-1}+M_l(\\delta_l^x)^{-1}+M_l\\beta_l^2(\\delta_l^y)^{-1})$ and $(m^x)^*=(v^x)^*((v^x)^{-1}m^x+(\\delta_l^x)^{-1}\\sum_{\\{t:U_t=l\\}}z_{t-1}+(\\delta_l^y)^{-1}\\beta_l^2\\sum_{\\{t:U_t=l\\}}(z_{t-1}+(z_t-\\mu_l^y)/\\beta_l))$. We use a random-walk Metropolis step to update $\\mu_l^x$. For $l\\notin\\{U_j^*\\}$, \n$p(\\mu_l^x \\mid \\dots,\\mathrm{data})$ is proportional to $\\mathrm{N}(\\mu_l^x;m^x,v^x)[\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)]^{-1}$, and in this case we use a Metropolis-Hastings algorithm, proposing  a candidate value $\\mu_l^x$ from the base distribution $\\mathrm{N}(m^x,v^x)$. \n\nThe full conditional and sampling strategy for $\\delta_l^x$ are similar to those for $\\mu_l^x$. We have \n", "index": 15, "text": "\n\\[\np(\\delta_l^x \\mid \\dots,\\mathrm{data})\\propto\\mathrm{IG}(\\delta_l^x;\\nu^x,s^x)\n\\prod_{\\{t:U_t=l\\}} \\mathrm{N}(z_{t-1};\\mu_l^x,\\delta_l^x)\n\\left(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)\\right)^{-1},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"p(\\delta_{l}^{x}\\mid\\dots,\\mathrm{data})\\propto\\mathrm{IG}(\\delta_{l}^{x};\\nu^%&#10;{x},s^{x})\\prod_{\\{t:U_{t}=l\\}}\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})%&#10;\\left(\\prod_{t=2}^{n}\\sum_{m=1}^{L}p_{m}\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\delta_%&#10;{m}^{x})\\right)^{-1},\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo>\u2223</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>data</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>IG</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo>;</mo><msup><mi>\u03bd</mi><mi>x</mi></msup><mo>,</mo><msup><mi>s</mi><mi>x</mi></msup><mo stretchy=\"false\">)</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo>:</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><mi>l</mi></mrow><mo stretchy=\"false\">}</mo></mrow></munder><mi mathvariant=\"normal\">N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><msup><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>p</mi><mi>m</mi></msub><mi mathvariant=\"normal\">N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>m</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\n For non-active components, the full conditional is $\\mathrm{IG}(\\delta_l^x;\\nu^x,s^x)(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x))^{-1}$. We use a similar strategy for sampling $\\delta_l^x$ as we did with $\\mu_l^x$, using a random-walk Metropolis algorithm for the active components of $\\delta_l^x$, working on the log-scale and sampling $\\log(\\delta_l^x)$, and proposing the non-active components from $G_0(\\delta_l^x)=$ $\\mathrm{IG}(\\nu^x,s^x)$.\n\n\nWe now discuss the updating scheme for the vector $p=(p_1,\\dots,p_L)$, which poses the main\nchallenge for posterior simulation. The full conditional for $p$ has the form \n\n", "itemtype": "equation", "pos": 19688, "prevtext": "\nwhich for an active component, is written as proportional to \n", "index": 17, "text": "\n\\[\\mathrm{IG}\\left(\\delta_l^x;\\nu^x+0.5M_l,s^x+0.5\\sum_{\\{t:U_t=l\\}}(z_{t-1}-\\mu_l^x)^2\\right)\\left(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x)\\right)^{-1}.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{IG}\\left(\\delta_{l}^{x};\\nu^{x}+0.5M_{l},s^{x}+0.5\\sum_{\\{t:U_{t}=l\\}}%&#10;(z_{t-1}-\\mu_{l}^{x})^{2}\\right)\\left(\\prod_{t=2}^{n}\\sum_{m=1}^{L}p_{m}%&#10;\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\delta_{m}^{x})\\right)^{-1}.\" display=\"block\"><mrow><mrow><mi>IG</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo>;</mo><mrow><msup><mi>\u03bd</mi><mi>x</mi></msup><mo>+</mo><mrow><mn>0.5</mn><mo>\u2062</mo><msub><mi>M</mi><mi>l</mi></msub></mrow></mrow><mo>,</mo><mrow><msup><mi>s</mi><mi>x</mi></msup><mo>+</mo><mrow><mn>0.5</mn><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo>:</mo><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><mi>l</mi></mrow><mo stretchy=\"false\">}</mo></mrow></munder><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><msub><mi>p</mi><mi>m</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>m</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nIn standard DP mixture models, the implied generalized Dirichlet prior for $f(p \\mid \\alpha)$ combines with $\\prod_{l=1}^L p_l^{M_l}$ to form another generalized Dirichlet distribution. However, in this case there is an additional term. Metropolis--Hastings algorithms with various proposal distributions were explored to sample the vector $p$,  resulting in very low acceptance rates. We instead devise an alternative sampling scheme, in which we work directly with the latent beta-distributed random variables which determine the probability vector $p$ arising from the DP truncation approximation.\n\nRecall that the joint prior for $p$ corresponds to a generalized Dirichlet distribution, which can be constructed from latent beta random variables through stick-breaking. Let $v_1,\\dots,v_{L-1}\\stackrel{i.i.d.}{\\sim}\\mathrm{beta}(1,\\alpha)$, and define $p_1=v_1$, $p_l=v_l\\prod_{r=1}^{l-1}(1-v_r)$, for $l=2,\\dots,L-1$, and $p_L=\\prod_{r=1}^{L-1}(1-v_r)$. Equivalently, let $\\zeta_1,\\dots,\\zeta_{L-1}\\stackrel{i.i.d.}{\\sim}\\mathrm{beta}(\\alpha,1)$, and define $p_1=1-\\zeta_1$, $p_l=(1-\\zeta_l)\\prod_{r=1}^{l-1}\\zeta_r$, and $p_L=\\prod_{r=1}^{L-1}\\zeta_r$. \nRather than updating directly $p$, we work with the $\\zeta_{l}$, \na sample for which implies a particular probability vector $p$.\n\nThe full conditional for $\\zeta_l$, $l=1,\\dots,L-1$, has the form\n\n", "itemtype": "equation", "pos": 20520, "prevtext": "\n For non-active components, the full conditional is $\\mathrm{IG}(\\delta_l^x;\\nu^x,s^x)(\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x))^{-1}$. We use a similar strategy for sampling $\\delta_l^x$ as we did with $\\mu_l^x$, using a random-walk Metropolis algorithm for the active components of $\\delta_l^x$, working on the log-scale and sampling $\\log(\\delta_l^x)$, and proposing the non-active components from $G_0(\\delta_l^x)=$ $\\mathrm{IG}(\\nu^x,s^x)$.\n\n\nWe now discuss the updating scheme for the vector $p=(p_1,\\dots,p_L)$, which poses the main\nchallenge for posterior simulation. The full conditional for $p$ has the form \n\n", "index": 19, "text": "$$\nf(p \\mid \\alpha)\\prod_{l=1}^L p_l^{M_l}\n\\left(\n\\prod_{t=2}^n \\sum_{m=1}^L p_m\\mathrm{N}(z_{t-1};\\mu_m^x,\\delta_m^x) \\right)^{-1}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"f(p\\mid\\alpha)\\prod_{l=1}^{L}p_{l}^{M_{l}}\\left(\\prod_{t=2}^{n}\\sum_{m=1}^{L}p%&#10;_{m}\\mathrm{N}(z_{t-1};\\mu_{m}^{x},\\delta_{m}^{x})\\right)^{-1}.\" display=\"block\"><mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>\u2223</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msubsup><mi>p</mi><mi>l</mi><msub><mi>M</mi><mi>l</mi></msub></msubsup><msup><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>p</mi><mi>m</mi></msub><mi mathvariant=\"normal\">N</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>m</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>m</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nwhere \n", "itemtype": "equation", "pos": 22013, "prevtext": "\nIn standard DP mixture models, the implied generalized Dirichlet prior for $f(p \\mid \\alpha)$ combines with $\\prod_{l=1}^L p_l^{M_l}$ to form another generalized Dirichlet distribution. However, in this case there is an additional term. Metropolis--Hastings algorithms with various proposal distributions were explored to sample the vector $p$,  resulting in very low acceptance rates. We instead devise an alternative sampling scheme, in which we work directly with the latent beta-distributed random variables which determine the probability vector $p$ arising from the DP truncation approximation.\n\nRecall that the joint prior for $p$ corresponds to a generalized Dirichlet distribution, which can be constructed from latent beta random variables through stick-breaking. Let $v_1,\\dots,v_{L-1}\\stackrel{i.i.d.}{\\sim}\\mathrm{beta}(1,\\alpha)$, and define $p_1=v_1$, $p_l=v_l\\prod_{r=1}^{l-1}(1-v_r)$, for $l=2,\\dots,L-1$, and $p_L=\\prod_{r=1}^{L-1}(1-v_r)$. Equivalently, let $\\zeta_1,\\dots,\\zeta_{L-1}\\stackrel{i.i.d.}{\\sim}\\mathrm{beta}(\\alpha,1)$, and define $p_1=1-\\zeta_1$, $p_l=(1-\\zeta_l)\\prod_{r=1}^{l-1}\\zeta_r$, and $p_L=\\prod_{r=1}^{L-1}\\zeta_r$. \nRather than updating directly $p$, we work with the $\\zeta_{l}$, \na sample for which implies a particular probability vector $p$.\n\nThe full conditional for $\\zeta_l$, $l=1,\\dots,L-1$, has the form\n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:zeta_full}\np(\\zeta_{l} \\mid \\dots,\\mathrm{data}) \\propto \\mathrm{beta}\\left(\\zeta_{l};\\alpha+\\sum_{r=l+1}^{L}M_{r},M_{l}+1\\right)\\left(\\prod_{t=2}^{n}d(z_{t-1})\\right)^{-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"p(\\zeta_{l}\\mid\\dots,\\mathrm{data})\\propto\\mathrm{beta}\\left(\\zeta_{l};\\alpha+%&#10;\\sum_{r=l+1}^{L}M_{r},M_{l}+1\\right)\\left(\\prod_{t=2}^{n}d(z_{t-1})\\right)^{-1}\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b6</mi><mi>l</mi></msub><mo>\u2223</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>data</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>beta</mi><mrow><mo>(</mo><msub><mi>\u03b6</mi><mi>l</mi></msub><mo>;</mo><mi>\u03b1</mi><mo>+</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>L</mi></munderover><msub><mi>M</mi><mi>r</mi></msub><mo>,</mo><msub><mi>M</mi><mi>l</mi></msub><mo>+</mo><mn>1</mn><mo>)</mo></mrow><msup><mrow><mo>(</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\nAlso, let $c_{t,l}=\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})$, which is constant with respect \nto each $\\zeta_l$. The form of the full conditional in (\\ref{eq:zeta_full}) suggests the use of \na slice sampler to update each $\\zeta_l$ one at a time. The slice sampler is implemented by \ndrawing auxiliary random variables $u_{t}\\sim \\mathrm{uniform}(0,(d(z_{t-1}))^{-1}),$ $t=2,...,n,$ \nand then sampling $\\zeta_{l}\\sim \\mathrm{beta}(\\alpha+\\sum_{r=l+1}^{L}M_{r},M_{l}+1)$, but \nrestricted to the set $\\{\\zeta_{l}:u_{t}<(d(z_{t-1}))^{-1},t=2,...,n\\}$. The term $d(z_{t-1})$\ncan be expressed as \n\n\n\n\n\n$d(z_{t-1})=$ $\\zeta_{l} w_{1t}+w_{0t}$, for any $l=1,...,L-1$, where \n", "itemtype": "equation", "pos": 22234, "prevtext": "\nwhere \n", "index": 23, "text": "\n\\[\nd(z_{t-1})=\\mathrm{N}(z_{t-1};\\mu_{1}^{x},\\delta_{1}^{x})(1-\\zeta_{1})+\\sum_{l=2}^{L-1}\n\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})(1-\\zeta_{l})\\prod_{s=1}^{l-1}\\zeta_{s}+\n\\mathrm{N}(z_{t-1};\\mu_{L}^{x},\\delta_{L}^{x})\\prod_{s=1}^{L-1}\\zeta_{s}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"d(z_{t-1})=\\mathrm{N}(z_{t-1};\\mu_{1}^{x},\\delta_{1}^{x})(1-\\zeta_{1})+\\sum_{l%&#10;=2}^{L-1}\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})(1-\\zeta_{l})\\prod_{s=1%&#10;}^{l-1}\\zeta_{s}+\\mathrm{N}(z_{t-1};\\mu_{L}^{x},\\delta_{L}^{x})\\prod_{s=1}^{L-%&#10;1}\\zeta_{s}.\" display=\"block\"><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mn>1</mn><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mn>1</mn><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03b6</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>2</mn></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>l</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>l</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03b6</mi><mi>l</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03b6</mi><mi>s</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><msubsup><mi>\u03bc</mi><mi>L</mi><mi>x</mi></msubsup><mo>,</mo><msubsup><mi>\u03b4</mi><mi>L</mi><mi>x</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03b6</mi><mi>s</mi></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04331.tex", "nexttext": "\n and, if $l=1,$ $w_{0t}=c_{t,1}$, otherwise $w_{0t}=c_{t,1}(1-\\zeta_{1})+\\sum_{s=2}^{l-1}c_{t,s}(1-\\zeta_{s})\\prod_{r=1}^{s-1}\\zeta_{r}+c_{t,l}\\prod_{s=1}^{l-1}\\zeta_{s}$. \nThen, the set $\\{\\zeta_{l}:d(z_{t-1})<u_{t}^{-1}\\}$ is $\\{\\zeta_{l}:\\zeta_{l}w_{1t}<u_{t}^{-1}-w_{0t}\\}.$ This takes the form of $\\{\\zeta_{l}:\\zeta_{l}<(u_{t}w_{1t})^{-1}-w_{0t}(w_{1t})^{-1}\\}$ when $w_{1t}$ is positive, and has the form $\\{\\zeta_{l}:\\zeta_{l}>(u_{t}w_{1t})^{-1}-w_{0t}(w_{1t})^{-1}\\}$ otherwise. Therefore, the truncated--beta random draw for $\\zeta_l$ must lie in the interval $(\\max_{\\{t:w_{1t}<0\\}}[(u_{t}w_{1t})^{-1}-w_{0t}(w_{1t})^{-1}],\\min_{\\{t:w_{1t}>0\\}}[(u_{t}w_{1t})^{-1}-w_{0t}(w_{1t})^{-1}])$. \nThe inverse CDF random variate generation method can be used to sample from these truncated beta \nrandom variables. This strategy results in direct draws for the $\\zeta_l$, which implies a corresponding \nprobability vector $p$.\n\n\nAt any time point $t$, an entire distribution can be obtained for $f(z_{t+1} \\mid Z_t=z_t;G)$,\nfor any $z_{t}$, providing full inference for the transition density. This conditional distribution can \nbe evaluated at the last time point, conditional on $Z_n$, to give a forecasting distribution, $f(z_{n+1}\\mid Z_n=z_n;G)=\\sum_{l=1}^{L}q_l(z_n)\\text{N}(z_{n+1};\\mu_l^y-\\beta_l(z_n-\\mu_l^x),\\delta^{y}_l)$. \nFull inference is readily available for any $z_{n+1}$, yielding an entire forecasting distribution. The point estimate of this distribution is the posterior predictive density for the next observation, since it can be shown that $p(z_{n+1}\\mid Z_n=z_n;\\mathrm{data})=\\mathrm{E}(f(z_{n+1}|Z_n=z_n;G) \\mid \\mathrm{data})$. \nPoint estimates for forecasts further than one step ahead may be obtained fairly easily, and entire \ndistributions are also available, albeit at somewhat greater computational expense.  \n\n\n\n\n\\subsection{Prior Specification}\n\\label{sec:priors}\n\nWe now discuss prior specification for the hyperparameters $\\psi$ of $G_0$, aiming to \nspecify appropriately diffuse priors which use only a small amount of prior information.\nRecall that the model for the transition density $f(z_t \\mid z_{t-1})$ was motivated by a DP mixture of \nbivariate normals, that is $f(z_{t-1},z_{t};G)=\\int \\mathrm{N}(z_{t-1},z_{t};\\mu,\\Sigma) \\text{d}G(\\mu,\\Sigma)$, \nwith $G$ having a DP prior. In the limit, as $\\alpha\\rightarrow 0^+$, this model consists \nof a single mixture component, $\\mathrm{N}(z_{t-1},z_{t};\\mu,\\Sigma)$. An approximate \ncenter $d$ and range $r$ of the data are used to center and scale the mixture kernel appropriately. \n\n\nBased on the form of $G_0$, we obtain $\\mathrm{E}(z_{t-1})=\\mathrm{E}(\\mu^x)=a_m^x$ and $\\mathrm{E}(z_{t})=\\mathrm{E}(\\mu^y)=a_m^y$, and therefore set $a_m^x=a_m^y=d$. We find $\\mathrm{Cov}(z_{t-1},z_t)=\\mathrm{E}(\\Sigma)+\\mathrm{Cov}(\\mu)$, and use this expression to scale the prior for $\\mu$. The marginal prior variances for the components of $\\mu$ are $\\mathrm{var}(\\mu^x)=b_m^x+(a_v^x-1)^{-1}b_v^x$ and $\\mathrm{var}(\\mu^y)=b_m^y+(a_v^y-1)^{-1}b_v^y$. Fixing small values for $a_v^x$ and $a_v^y$ to ensure large variance for $v^x$ and $v^y$, and assuming $\\mathrm{var}(\\mu^x)\\approx (r/4)^2$ and $\\mathrm{var}(\\mu^y)\\approx (r/4)^2$, one can then obtain reasonable values for $b_m^x$, $b_v^x$, $b_m^y$, and $b_v^y$. This completes specification of the parameters associated with $\\mu^x$ and $\\mu^y$.\n\n\nWe now discuss two approaches to prior specification for the hyperparameters associated with $\\Sigma$. One approach involves obtaining the prior expectation of the diagonal elements of $\\Sigma$, and setting each of these equal $(r/4)^2$, while also ensuring that the implied prior on the correlation $-\\beta\\delta^x((\\beta^2\\delta^x+\\delta^y)\\delta^x)^{-1/2}$ is approximately uniform on $(-1,1)$. We find that $\\mathrm{E}(\\Sigma^{xx})=\\mathrm{E}(\\delta^{x})=(b_s^x(\\nu^x-1))^{-1}a_s^x$, and fixing $a_s^x$ and $\\nu^x$, determine $b_s^x$ so that $\\mathrm{E}(\\Sigma^{xx})=(r/4)^2$. The prior on $\\beta$ should be centered around $0$, supporting independence in the normal kernel, that is, $a_\\theta=0$. Then, again taking prior expectations, $\\mathrm{E}(\\Sigma^{yy})=(b_\\theta+b_c(a_c-1)^{-1})\\mathrm{E}(\\delta^x)+(b_s^y(\\nu^y-1))^{-1}a_s^y$. Fixing $\\nu^y$, $a_c$, and $a_s^y$, this sum can be set equal to $(r/4)^2$, where the particular values of $b_\\theta$, $b_c$, and $b_s^y$ are determined so that the induced prior on the correlation is approximately uniform on $(-1,1)$, as verified through prior simulation.\n\nAn alternative, more automatic, strategy arises from considering the distributions implied on $\\beta$, $\\delta^x$, and $\\delta^y$ if $\\Sigma$ is inverse-Wishart distributed, a setting under which we are accustomed to specifying priors for covariance matrices. A common noninformative $\\mathrm{IW}(a,B)$ specification for $\\Sigma$ involves fixing a small value for the degrees of freedom parameter $a$, and assuming $B$ to be a diagonal matrix with diagonal $(B_1,B_2)$. We can, for instance, fix $a=4$, the smallest possible integer value such that $\\Sigma$ has finite expectation, and assume $\\mathrm{E}(\\Sigma)=\\mathrm{diag}\\{(r/4)^2,(r/4)^2\\}$, that is, $(B_1,B_2)=((r/4)^2,(r/4)^2)$.  If $\\Sigma \\sim \\mathrm{IW}(a,B)$, then as a consequence, $\\delta^x\\sim \\mathrm{IG}(0.5(a-1),0.5 B_1)$, $\\delta^y\\sim \\mathrm{IG}(0.5a,0.5 B_2)$, and $\\beta \\mid \\delta^{y} \\sim \\mathrm{N}(0,\\delta^yB_1^{-1})$ \\citep{deyoreokottas}. Therefore, we set $\\nu^x=0.5(a-1)$ and $\\nu^y=0.5a$, and let $\\mathrm{E}(s^x)=\\mathrm{E}(s^y)=0.5(r/4)^2$. Exponential priors for $s^y$ and $s^x$ yield $b_s^x=b_s^y=2(r/4)^{-2}$. After marginalizing out $\\theta$, the $G_0$ component for $\\beta$ becomes $\\mathrm{N}(a_\\theta,b_\\theta+c)$, so we let $a_\\theta=0$, and $b_\\theta+\\mathrm{E}(c)=B_1^{-1}\\mathrm{E}(\\delta^y)=0.5(\\nu^y-1)^{-1}$. Assuming $b_\\theta=\\mathrm{E}(c)$, and fixing $a_c$, $b_\\theta$ and $b_c$ can be determined accordingly.\n\n\n\n\n\\subsection{Related Mixture Models for Time Series}\n\\label{sec:background}\n\n\\citet{carvalho,carvalho2006} model nonlinear time series through finite mixtures of generalized linear models, or experts, resulting in time series models with transition densities similar to \n(\\ref{eqn:trans_dens}). However, they approach the problem from a maximum likelihood perspective, and require the use of model selection criteria to determine the optimal size of the mixture. \\citet{wood} consider parametric mixture modeling for time series in which the weights are time-dependent and the lag is unknown.\n\nWhile Bayesian nonparametric techniques have become extremely popular in density estimation, regression, and other applications, they have been used to a lesser extent in the context of time series. \\citet{mullerwestmac} first made use of the DP to build a model for nonstationary time series. They propose a finite mixture of AR models with local weights, where the parameters of the autoregressions and the parameters of the mixture weights are assumed to be i.i.d from some random distribution which is assigned a DP prior. \\citet{tang:planning} establish posterior consistency for transition densities which can be expressed as DP mixtures of normal kernels, with means given by functions of previous observations. \\citet{tang:cs} consider a particular version of this class of models, involving a hyperbolic tangent transformation of lagged terms, which can approximate any linear autoregressive model arbitrarily closely. \\citet{dilucca} apply a dependent DP (DDP) mixture \\citep{maceachern} for the transition densities, focusing mainly on the common weights version of the DDP. The DP atoms arise from a normal distribution with means linear on the previous observation. Their primary model is then a countable location mixture of AR models, with mixing taking place on the AR parameters.  \\citet{mena} construct transition densities nonparametrically, but restrict the transition densities further to obtain strongly stationary AR models. \\citet{lauso} also considered DP mixtures of AR processes. \\citet{caron} and \\citet{fox} assume DP mixture errors within a DLM framework.   \n\n\nStationarity arises as a special case of (\\ref{eqn:trans_dens}), occurring when the two marginal densities are identical. To preserve stationarity, one can set $\\mu_l^x=\\mu_l^y$ and $\\Sigma_l^{xx}=\\Sigma_l^{yy}$. This is the version of the model studied by \\citet{antoniano}, who focus on building flexible stationary models, as stationarity is desirable in some settings, but achieving both stationarity and flexibility in the transition and invariant densities is a challenge. Their modeling framework begins much like ours, in that a transition mechanism is obtained as the conditional density from a bivariate distribution. The authors do not apply a truncation approximation to $G$, thus inference under this model requires the introduction of multiple sets of latent variables and a trans-dimensional MCMC algorithm for posterior simulation. The model developed by \\citet{antoniano} was previously proposed by \\citet{martinez}, however it was then thought to be intractable due to the infinite sum appearing in the denominator of the transition density mixture weights. Note that under our parameterization, constraints to stationary yield a transition density $\\sum_{l=1}^Lq_l(z_{t-1})\\mathrm{N}(z_t;\\mu_l-\\beta_l(z_{t-1}-\\mu_l),\\sigma^2_l(1-\\beta_l^2))$ with \n$q_l(z_{t-1}) \\propto p_{l} \\mathrm{N}(z_{t-1};\\mu_l,\\sigma^2_l)$, and $\\beta_l\\in (-1,1)$. \n\nAlthough we utilize a truncation approximation to the DP from the outset, the sum in the denominator of the weights in (\\ref{eqn:weights}) still presents challenges in terms of posterior simulation. We developed a tractable MCMC \nalgorithm by reparameterizing the covariance matrices in (\\ref{eqn:joint_dens}) and, rather than working directly with the probability vector $p$ which is difficult to update efficiently, working with the stick-breaking weights to develop a slice sampler which indirectly provides samples for $p$. \n\n\n\n\n\\section{Data Illustrations}\n\\label{sec:applications}\n\nWe now illustrate the proposed model on two simulated data sets (Section \\ref{simulated-data})\nand apply it to the waiting times between eruptions of the Old Faithful geyser\n(Section \\ref{oldfaithful-data}). In all cases, MCMC inference was implemented in R, \nsaving every $20$-th iteration after burn-in, and a Monte Carlo sample size of $5,000$ was used for inference. We follow the approach to prior specification described in Section \\ref{sec:priors}, using the second method for fixing hyperparameters associated with $\\beta$, $\\delta^x$, and $\\delta^y$, which follows from considering the distributions implied under an inverse-Wishart specification for $\\Sigma$.  \n\nThe DP truncation level $L$ was chosen using standard DP properties: under a gamma$(0.5,0.5)$ prior \non $\\alpha$, the expectation of the partial sum of the original DP weights, $\\mathrm{E}(\\sum_{l=1}^{L} p_l)$, \nis $0.9997$ with $L=30$ and $0.99999$ with $L=50$. We used a value of $L$ in this range for all \ndata examples, and monitored the number of effective components to ensure it never reached the \nupper bound. \n\n\n\n\n\n\\subsection{Simulated Data}\n\\label{simulated-data}\n\n\\subsubsection{Skew-normal Transition Densities}\n\nTo generate a time series that exhibits challenging transition densities which evolve over time in a \nplausible fashion, we assume each observation is generated from a skew-normal distribution \\citep{azzalini}, \nwith scale and skewness parameters which are functions of the previous observation. In particular, \nwe generate $z_{t} \\mid z_{t-1} \\sim$ $\\mathrm{SN}(z_t;0,1+0.7|z_{t-1}|,0.1+4\\sin(z_{t-1}))$, for $t=2,\\dots,n$. \nHere, $\\mathrm{SN}(y;\\xi,\\omega,\\alpha)$ denotes a skew-normal distribution with density $(\\omega\\pi)^{-1}\\exp(-(y-\\xi)^2/(2\\omega^2))\\Phi(\\alpha(x-\\xi)/\\omega)$, where\n$\\Phi(\\cdot)$ denotes the standard normal cumulative distribution function. The sinusoidal or periodic \ntrend in skewness parameter $\\alpha$ yields conditional distributions with various directions and \ndegrees of skewness, and the decreasing followed by increasing linear trend in scale parameter $\\omega$ \nleads to distributions which are more peaked when $z_{t-1}$ is near $0$. \n\nA time series $(z_2,\\dots,z_{500})$ was simulated from this model assuming an initial value $z_1=0$. Figure \\ref{fig:SN_lagged} (left panel) shows the simulated data $\\{(z_{t-1},z_t), \\, t=2,\\dots,500\\}$. Notice the oscillating \ntrend in location, and the larger variation in $z_t$ for $z_{t-1}$ far from $0$. We estimate $\\mathrm{E}(Z_{t}\\mid Z_{t-1}=z_{t-1})$ by evaluating $\\sum_{l=1}^L q_l(z_{t-1}) \\{ \\mu_l^y-\\beta_l(z_{t-1}-\\mu_l^x) \\}$ over a grid in $z_{t-1}$, providing point estimates and uncertainty quantification for the expectation of the next observation in a series given that the previous observation was $z_{t-1}$. Figure \\ref{fig:SN_lagged} (right panel) displays these results \nalong with the data-generating expectation trend. The estimates generally match fairly closely and the $95\\%$ \ncredible intervals contain the truth everywhere except for a small region around $z_{t-1}=10$, where there is \nvery little data.\n\n\\begin{figure}[t!]\n\\centering\n\\begin{tabular}{cc}\n\\includegraphics[height=3in,width=2.9in]{SN_lagged_data_eps}&\n\\includegraphics[height=3in,width=2.9in]{exp_SN_sim_eps}\n\\end{tabular}\n\\caption{Skew-normal simulation. The left panel plots the simulated data as pairs of points $(z_{t-1},z_t)$. \nThe right panel shows the posterior mean (solid line) and $95\\%$ credible intervals (gray shaded region)\nfor $\\mathrm{E}(Z_{t}\\mid Z_{t-1}=z_{t-1})$ plotted over a grid in $z_{t-1}$; the true expectation is shown \nas a dotted line.}\n\\label{fig:SN_lagged}\n\\end{figure}\n\n\nWe also compute posterior predictive densities $p(z_t\\mid z_{t-1};G)$, for $t=2,\\dots,n$. These densities are displayed in Figure \\ref{fig:cond_all_SN} (top panel). We plot each index $t$ on the horizontal axis, and the corresponding predictive density for $z_t$ on the vertical axis, using darker colors to represent larger values. The true predictive densities are given also in Figure \\ref{fig:cond_all_SN} (bottom panel) and the data is shown in each plot.  While these inferences are based on only a posterior point estimate for $f(z_t\\mid z_{t-1})$, we also have available full inference which we display in the form of point estimates and $95\\%$ uncertainty bands for $f(z_t\\mid Z_{t-1}=z_{t-1})$ at four particular values of $z_{t-1}$ in Figure \\ref{fig:cond_dens_SN}.  Notice the wide uncertainty bands for the density at $z_{t-1}=8.85$ (bottom right panel) and the narrow uncertainty bands when $z_{t-1}=-0.5$ (top right panel), \nwhich reflects the lack of data above 5 or 6 and the large amount of data in the region near 0. \n\n\n\n\n\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[height=2.9in,width=5.5in]{SN_cond_dens_all}\n\\includegraphics[height=2.9in,width=5.5in]{SN_cond_dens_all_true}\n\\caption{Skew-normal simulation. Predictive densities $p(z_t\\mid z_{t-1})$ for each $t=2,\\dots,n$. \nThe top panel displays the estimates from the model and the bottom panel shows the true densities. \nDarker colors indicate larger values. The data is also included in each plot.}\n\\label{fig:cond_all_SN}\n\\end{figure}\n\n\\begin{figure}[t!]\n\\centering\n\\begin{tabular}{cc}\n\\includegraphics[height=2.3in,width=2.5in]{SN_cond_dens_neg2p85_eps}&\n\\includegraphics[height=2.3in,width=2.5in]{SN_cond_dens_negp5_eps}\\\\\n\\includegraphics[height=2.3in,width=2.5in]{SN_cond_dens_4p2_eps}&\n\\includegraphics[height=2.3in,width=2.5in]{SN_cond_dens_8p85_eps}\n\\end{tabular}\n\\caption{Skew-normal simulation. Posterior mean (solid line) and $95\\%$ credible intervals \n(gray shaded region) for transition densities $f(z_t\\mid z_{t-1})$, for $z_{t-1}=-2.85$ (top left), \n$z_{t-1}=-0.5$ (top right), $z_{t-1}=4.2$ (bottom left), and $z_{t-1}=8.85$ (bottom right). \nThe corresponding true densities are plotted as dotted lines.}\n\\label{fig:cond_dens_SN}\n\\end{figure}\n\n\n\n\n\\subsubsection{Brownian Motion}\n\nStandard Brownian motion is a nonstationary process defined by the transition density $f(z_t\\mid z_{t-1})=\\mathrm{N}(z_{t-1},1)$. A standard Brownian motion path is generated assuming $n=500$. \nTrivially, $\\mathrm{E}(Z_t\\mid Z_{t-1}=z_{t-1})=z_{t-1}$ in this model. The inference from the model \nindicates it is detecting this trend with little uncertainty (Figure \\ref{fig:exp_brownian}, left panel). \nThe value of the last observation is approximately $-14.2$, one of the smallest values in the entire \nseries. The forecast distribution for the next observation is displayed in Figure \\ref{fig:exp_brownian} \n(right panel). While the $95\\%$ posterior credible intervals contain the true density, the mode of the \npoint estimate favors slightly larger values, likely due to the fact that $-14.2$ is an extreme value in \nthis series. \n\nPosterior predictive densities $p(z_t\\mid z_{t-1};G)$, for $t=2,\\dots,n$, are displayed in Figure \\ref{fig:cond_all_brownian} (top panel). For each index $t$ on the horizontal axis, the corresponding \npredictive density for $z_t$ is plotted on the vertical axis, where darker colors represent larger values. \nThe true predictive densities are also plotted in Figure \\ref{fig:cond_all_brownian} (bottom panel). \nIn summary, all visual displays indicate that the estimation from the model is of very good quality, \ncapturing the dynamics in the data exceedingly well. \n\n\\begin{figure}[t!]\n\\centering\n\\begin{tabular}{cc}\n\\includegraphics[height=3in,width=3in]{exp_brownian_eps}&\n\\includegraphics[height=3in,width=3in]{forecast_brownian_eps}\n\\end{tabular}\n\\caption{Brownian motion simulation. The left panel plots the posterior mean estimate (solid line) \nand $95\\%$ credible intervals (gray shaded region) for $\\mathrm{E}(Z_{t}\\mid Z_{t-1}=z_{t-1})$  \nplotted over a grid in $z_{t-1}$. The true expectation is indistinguishable from the model's estimate. \nThe right panel shows the posterior mean (solid line) and $95\\%$ credible intervals (gray shaded \nregion) for the forecast density, $f(z_{n+1}\\mid z_{n} = -14.2)$, compared to the truth \n(dotted line).}\n\\label{fig:exp_brownian}\n\\end{figure}\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[height=2.7in,width=5.5in]{pred_all_brownian_eps}\n\\includegraphics[height=2.7in,width=5.5in]{true_all_brownian_eps}\n\\caption{Brownian motion simulation. Predictive densities $p(z_t\\mid z_{t-1})$ for each $t=2,\\dots,n$. \nThe top panel displays the estimates from the model and the bottom panel shows the true densities. \nDarker colors indicate larger values. The data is also included in each plot.}\n\\label{fig:cond_all_brownian}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Waiting Times Between Eruptions of the Old Faithful Geyser}\n\\label{oldfaithful-data}\n\nWe illustrate the proposed model on the time intervals between successive eruptions of the Old Faithful geyser, \nwhich are available through R under the dataset {\\tt faithful}. The data set consists of $272$ measurements \n$\\{z_t, \\, t=1,\\dots,272\\}$, where $z_t$ represents the waiting time in minutes before eruption $t$. The data are displayed in Figure \\ref{fig:faithful_exp} in the form of a plot of $y_t$ versus $y_{t-1}$, for $t=2,\\dots,272$.  \nAlso plotted in Figure \\ref{fig:faithful_exp} are the posterior mean estimate and $95\\%$ credible intervals for $\\mathrm{E}(Z_t\\mid Z_{t-1}=z_{t-1})$. \n\nThe model required an average of $8$ mixture components. Standard MCMC diagnostics suggest convergence \nhas been reached. For instance, trace plots corresponding to $5,000$ posterior samples are shown for two \nquantities in Figure \\ref{fig:trace}. The plot on the right panel of Figure \\ref{fig:trace} monitors the average \nof the standard deviations of the conditional densities $f(z_t\\mid z_{t-1};G)$, that is, \n$\\sum_{t=2}^n (\\delta^y_{U_t})^{0.5}/(n-1)$, while the left panel plot monitors $\\sum_{t=2}^n \\beta_{U_t}/(n-1)$, \nwhich refers to an important quantity since the $\\beta_{l}$ control the strength and direction of the \nautoregressions. While each $\\beta_l$ was centered at $0$ in the prior, the posterior favors slightly \npositive values. \n\nThere are some interesting features present in the data. When $z_{t-1}$ is below $60$, there is a large \ncluster of points around $z_t=80$, and a small number of points extending down below $z_t=50$, indicating a distribution with a mode near $80$ but with a heavy left tail or a small additional mode near $50$. Moving to \nlarger values of $z_{t-1}$, there are two clusters of points, one centered around $55$ and one around $80$. \nThese features are captured by the estimated transition densities at $f(z_t\\mid z_{t-1}=50)$ and \n$f(z_t\\mid z_{t-1}=80)$, which are displayed in Figure \\ref{fig:faithful_cond}. One may be interested in \npredicting the next value $z_{n+1}$ in the series. It is important to provide estimates of uncertainty \nin this context. The posterior mean estimate and $95\\%$ credible intervals for the forecast density \nare given in Figure \\ref{fig:faithful_forecast}. This density is bimodal with the larger mode near $50$ \nand another mode near $80$, which is reasonable given the cross-section of data around $z_n=74$.\n\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[height=3in,width=3in]{exp_faithful_eps}\n\\caption{Old Faithful data. Posterior mean (solid line) and $95\\%$ credible intervals \n(gray shaded region) for $\\mathrm{E}(Z_{t}\\mid Z_{t-1}=z_{t-1})$ plotted over a grid in $z_{t-1}$,\nand overlaid on the data shown as pairs of points $(z_{t-1},z_{t})$, for $t=2,\\dots,272$.}\n\\label{fig:faithful_exp}\n\\end{figure}\n\n \\begin{figure}[t!]\n \\centering\n \\begin{tabular}{cc}\n \\includegraphics[height=2in,width=2.8in]{trace_beta_eps}&\n \\includegraphics[height=2in,width=2.8in]{trace_delta2_eps}\n \\end{tabular}\n\\caption{Old Faithful data. Trace plots of averages of $\\{\\beta_{U_t}, \\, t=2,\\dots,272\\}$ \n(left panel) and $\\{(\\delta_{U_t}^y)^{0.5}, \\, t=2,\\dots,272\\}$ (right panel) over \n$5,000$ MCMC iterations.}\n\\label{fig:trace}\n\\end{figure}\n\n\\begin{figure}[t!]\n\\centering\n\\begin{tabular}{cc}\n\\includegraphics[height=2.5in,width=2.5in]{trans_dens_50_faithful_eps}&\n\\includegraphics[height=2.5in,width=2.5in]{trans_dens_80_faithful_eps}\n\n\\end{tabular}\n\\caption{Old Faithful data. Posterior mean (solid line) and $95\\%$ credible intervals \n(gray shaded region) for transition densities $f(z_t\\mid z_{t-1})$, for $z_{t-1}=50$ \n(left panel) and $z_{t-1}=80$ (right panel).}\n\\label{fig:faithful_cond}\n\\end{figure}\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[height=3in,width=3in]{forecast_faithful_eps}\n\\caption{Old Faithful data. Posterior mean (solid line) and $95\\%$ credible intervals \n(gray shaded region) for the forecast density, $f(z_{n+1}\\mid z_{n}=74)$.}\n\\label{fig:faithful_forecast}\n\\end{figure}\n\n\n\n\n\\section{Extensions}\n\\label{sec:extensions}\n\nThe data illustrations suggest the ability of the first-order model to uncover a variety of conditional \ndensity shapes, and approximate well the truth contained in simulated data. However, some applications \nmay require additional features in the model formulation. \n\nAlthough the flexibility which is induced by the joint DP mixture modeling framework allows the \nfirst-order Markovian model to capture more complex features than many parametric models, \n\nthere are scenarios in which a higher-order structure is required. The first-order model can \nbe extended to incorporate higher order Markovian processes, by assuming \n$f(z_{t-r},\\dots,z_{t-1},z_t;G)\\sim \\int \\mathrm{N}(\\mu,\\Sigma) \\text{d}G(\\mu,\\Sigma)$. \nThen $f(z_t\\mid z_{t-1};G)$ is replaced by the transition density of order $r$, \n$f(z_t\\mid z_{t-1},\\dots,z_{t-r};G)$. This transition density has a similar form to \n(\\ref{eqn:trans_dens}), but now the means of the normal mixture components and the mixture\nweights depend on the previous $r$ states. Let superscript $y$ correspond to $Z_t$ and $x$ \nto $(Z_{t-r},\\dots,Z_{t-1})$ in the \nvector $\\mu$ of length $r+1$ and the $(r+1)\\times(r+1)$ matrix $\\Sigma$. Under the \nreparameterization of $\\Sigma$ used in the first-order case, the normal kernels have the form $\\mathrm{N}(z_{t};\\mu_l^y-\\sum_{j=1}^r\\beta_{l,(r+1,j)}(z_{t+j-r-1}-\\mu_{l,j}^x),\\delta_l^y)$, for \n$l=1,\\dots,L$. Gibbs sampling steps are thus preserved for $\\mu_l^y$ and $\\delta_l^y$, as well \nas the last row of the matrix $\\beta$. However, more care is needed in devising an MCMC algorithm \nto sample $\\delta_l^x$, $\\mu_l^x$ (each vectors of length $r$) and the first $r$ rows of $\\beta$, \nparticularly when $r$ is of order larger than $2$ or $3$.\n\n\n\n\n\nTurning to an application oriented extension, in population biology, the size of a wild population is often \nmonitored over time. Yearly estimated biomass may be recorded for a specific species, and the trend in \npopulation size indicates how the species is faring, and is indicative of greater environmental conditions. \nA state-space modeling framework is suitable for such applications, since the observed biomass is not \nan exact measurement of population size. Rather, biomass is viewed as a noisy version of the underlying \npopulation size, and a key goal is to forecast population size in the future. \n\nThe proposed model can be incorporated into a state-space framework, with the addition of an observation \nequation. The observations are now viewed as arising from latent unobserved states, which evolve in time \naccording to the flexible Markovian model. Denote the observed data by $(y_1,\\dots,y_n)$, and the underlying \nlatent states by $(z_1,\\dots,z_n)$. Assume $y_t\\mid z_t,\\theta \\sim$ $f(y_t\\mid z_t;\\theta)$, for some \nparametric distribution $f(y_t\\mid z_t;\\theta)$, and assume the latent states evolve according to the \nnonparametric Markovian model for $f(z_t \\mid z_{t-1};G)$ in (\\ref{eqn:trans_dens_reparam}). \nIn the population dynamics example, environmental covariates may also be available. \nThese can be treated as random, and modeled jointly with $y_t$ at the observation level, or incorporated \nat the state level.\n\n\n\nThe introduction of latent states is also useful in modeling ordinal time series data, as it is often \nassumed that $Y_t=j$ if and only if $Z_t\\in (\\gamma_{j-1},\\gamma_j)$, for $j=1,\\dots,C$. \nHowever, rather than working with a restrictive parametric distribution for the latent continuous \nresponses, they can be modeled with the proposed nonparametric Markovian model.\n\n\n\n\\section{Summary}\n\\label{sec:conclusion}\n\nWe have proposed a modeling approach for nonstationary time series which allows for\nnonstandard transition densities and nonlinear autoregressions. The conditional transition\ndensity of the Markovian model admits a representation as a location-scale mixture of normal\ndensities, with means and mixture weights that depend on observations from previous time\npoints. This structure is induced from a Dirichlet process mixture of normals specification for \nthe joint distribution of successive observations in time. We have discussed methods for \nposterior inference and prior specification, and illustrated the model with synthetic and \nreal data. Although the methodology has been developed and applied for directly observable\ntime series with first-order dependence, we have discussed possible extensions to model \nhigher order Markov chains, and to expand the model structure to a state-space setting.\n\n\n\\singlespacing\n\n\\bibliographystyle{asa}\n\n\\bibliography{DPM_MC_bib}\n\n\n", "itemtype": "equation", "pos": 23163, "prevtext": "\nAlso, let $c_{t,l}=\\mathrm{N}(z_{t-1};\\mu_{l}^{x},\\delta_{l}^{x})$, which is constant with respect \nto each $\\zeta_l$. The form of the full conditional in (\\ref{eq:zeta_full}) suggests the use of \na slice sampler to update each $\\zeta_l$ one at a time. The slice sampler is implemented by \ndrawing auxiliary random variables $u_{t}\\sim \\mathrm{uniform}(0,(d(z_{t-1}))^{-1}),$ $t=2,...,n,$ \nand then sampling $\\zeta_{l}\\sim \\mathrm{beta}(\\alpha+\\sum_{r=l+1}^{L}M_{r},M_{l}+1)$, but \nrestricted to the set $\\{\\zeta_{l}:u_{t}<(d(z_{t-1}))^{-1},t=2,...,n\\}$. The term $d(z_{t-1})$\ncan be expressed as \n\n\n\n\n\n$d(z_{t-1})=$ $\\zeta_{l} w_{1t}+w_{0t}$, for any $l=1,...,L-1$, where \n", "index": 25, "text": "\n\\[w_{1t}=-c_{t,l}\\prod_{s=1}^{l-1}\\zeta_{s}+\\left(\\sum_{m=l+1}^{L-1}c_{t,m}(1-\\zeta_{m})\\prod_{s=1,s\\neq l}^{m-1}\\zeta_{s}\\right)+c_{t,L}\\prod_{s=1,s\\neq l}^{L-1}\\zeta_{s}\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"w_{1t}=-c_{t,l}\\prod_{s=1}^{l-1}\\zeta_{s}+\\left(\\sum_{m=l+1}^{L-1}c_{t,m}(1-%&#10;\\zeta_{m})\\prod_{s=1,s\\neq l}^{m-1}\\zeta_{s}\\right)+c_{t,L}\\prod_{s=1,s\\neq l}%&#10;^{L-1}\\zeta_{s}\" display=\"block\"><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03b6</mi><mi>s</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>m</mi><mo>=</mo><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>,</mo><mi>m</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03b6</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>s</mi><mo>\u2260</mo><mi>l</mi></mrow></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03b6</mi><mi>s</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow><mo>+</mo><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>,</mo><mi>L</mi></mrow></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>s</mi><mo>\u2260</mo><mi>l</mi></mrow></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><msub><mi>\u03b6</mi><mi>s</mi></msub></mrow></mrow></mrow></mrow></math>", "type": "latex"}]