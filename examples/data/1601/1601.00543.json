[{"file": "1601.00543.tex", "nexttext": "\nwhere $\\mathbf{x}\\in\\mathbb{R}^{N}$ is the unknown signal, $\\mathbf{y}\\in\\mathbb{R}^{M}$\nis the available measurements, $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$\nis the known measurement matrix, and $\\mathbf{w}\\in\\mathbb{R}^{M}\\sim\\mathcal{N}\\bigl(\\mathbf{w};0,\\Delta_{0}\\mathbf{I}\\bigr)$\nis the additive noise. $\\mathcal{N}\\bigl(\\mathbf{x};\\mathbf{m},\\mathbf{C}\\bigr)$\ndenotes a Gaussian distribution of $\\mathbf{x}$ with mean $\\mathbf{m}$\nand covariance $\\mathbf{C}$ and $\\mathbf{I}$ denotes the identity\nmatrix. Our goal is to estimate $\\mathbf{x}$ from $\\mathbf{y}$ when\n$M\\ll N$ and $\\mathbf{x}$ is clustered sparse while its specific\nsparsity pattern is unknown a priori. \n\nTo enforce sparsity, from a Bayesian perspective, the signals are\nassumed to follow sparsity-promoting prior distributions, e.g., Laplace\nprior\\cite{park2008bayesian}, automatic relevance determination \\cite{tipping2001sparse},\nand spike and slab prior\\cite{vila2013expectation,krzakala2012probabilistic}.\nIn this letter we consider a flexible spike and slab prior of the\nform\n\n", "itemtype": "equation", "pos": 5510, "prevtext": "\n\n\\title{Approximate Message Passing with Nearest Neighbor  Sparsity Pattern\nLearning }\n\n\n\\author{Xiangming Meng, Sheng Wu, Linling Kuang, Defeng (David) Huang, and\nJianhua Lu, \\IEEEmembership{Fellow, IEEE} \n\\thanks{This work was partially supported by the National Nature Science Foundation\nof China (Grant Nos. 91338101, 91438206, and 61231011), the National\nBasic Research Program of China (Grant No. 2013CB329001).\n\nX. Meng and J. Lu are with the Department of Electronic Engineering,\nTsinghua University, Beijing, China. (e-mail: \n\\mbox{\nmengxm11\n}@mails.tsinghua.edu.cn; \n\\mbox{\nlhh-dee\n}@mail.tsinghua.edu.cn).\n\nS. Wu and L. Kuang are with the Tsinghua Space Center, Tsinghua University,\nBeijing, China. (e-mail: \n\\mbox{\nthuraya\n}@mail.tsinghua.edu.cn; \n\\mbox{\nkll\n}@mail.tsinghua.edu.cn).\n\nDefeng (David) Huang is with the School of Electrical, Electronic\nand Computer Engineering, The University of Western Australia, Australia\n(e- mail: huangdf@ee.uwa.edu.au).\n}}\n\\maketitle\n\\begin{abstract}\nWe consider the problem of recovering clustered sparse signals with\nno prior knowledge of the sparsity pattern. Beyond simple sparsity,\nsignals of interest often exhibits an underlying sparsity pattern\nwhich, if leveraged, can improve the reconstruction performance. However,\nthe sparsity pattern is usually \\textit{unknown} a priori. Inspired\nby the idea of k-nearest neighbor (k-NN) algorithm, we propose an\nefficient algorithm termed approximate message passing with nearest\nneighbor sparsity pattern learning (AMP-NNSPL), which learns the sparsity\npattern adaptively. AMP-NNSPL specifies a flexible spike and slab\nprior on the unknown signal and, after each AMP iteration, sets the\nsparse ratios as the average of the nearest neighbor estimates via\nexpectation maximization (EM). Experimental results on both synthetic\nand real data demonstrate the superiority of our proposed algorithm\nboth in terms of reconstruction performance and computational complexity.\\end{abstract}\n\n\\begin{IEEEkeywords}\nCompressed sensing, structured sparsity, approximate message passing,\nk-nearest neighbor. \n\\end{IEEEkeywords}\n\n\n\\section{Introduction}\n\nCompressed sensing (CS) aims to accurately reconstruct sparse signals\nfrom undersampled linear measurements\\cite{Donoho-CompressiveSensing,Candes-Introduction-to-CS,eldar2012compressed}.\nTo this end, a plethora of methods have been studied in the past years.\nAmong others, approximate message passing (AMP) \\cite{donoho2009message}\nproposed by Donoho \\textit{et al.} is one state-of-the-art algorithm\nto address sparse signal reconstruction in CS. Moreover, AMP has been\nextended to Bayesian AMP (B-AMP) \\cite{donoho2010message,krzakala2012probabilistic}\nand general linear mixing problems\\cite{rangan2011generalized,schniter2011message,Rangan-AMP-Learning}.\nWhile many practical signals can be described as sparse, they often\nexhibit an underlying structure, e.g., the nonzero coefficients occur\nin clusters\\cite{yuan2006model,cevher2009recovery,stojnic2009reconstruction,baraniuk2010model,huang2010benefit,eldar2009robust,eldar2010block}.\nExploiting such intrinsic structure beyond simple sparsity can significantly\nboost the reconstruction performance\\cite{huang2010benefit,eldar2009robust,eldar2010block}.\nTo this end, various algorithms have been proposed, e.g., group LASSO\\cite{yuan2006model},\nStructOMP\\cite{huang2011learning}, Graph-CoSaMP\\cite{hegde2015nearly},\nand block sparse Bayesian learning (B-SBL)\\cite{wipf2007empirical,zhang2011sparse,zhang2013extension},\netc. However, these algorithms require knowledge of sparsity pattern\nwhich is usually \\textit{unknown} a priori. To reconstruct sparse\nsignals with unknown structure, a number of methods\\cite{he2009exploiting,som2012compressive,yu2012bayesian,andersen2015spatio,fang2015pattern,fang2015two,Yu2015model}\nhave been developed to use various structured priors to encourage\nboth sparsity and cluster patterns simultaneously. The main effort\nof these algorithms lies in constructing a hierarchical prior model,\ne.g., Markov tree\\cite{som2012compressive}, structured spike and\nslab\\cite{yu2012bayesian,andersen2015spatio}, hierarchical Gamma-Gaussian\\cite{fang2015pattern,fang2015two,Yu2015model}\nto encode the structured sparsity pattern. \n\nIn this letter, we take an alternative approach and propose an efficient\nmessage passing algorithm, termed AMP with nearest neighbor sparsity\npattern learning (AMP-NNSPL), to recover clustered sparse signals\nadaptively, i.e., without any prior knowledge of the sparsity pattern.\nFor clustered sparse signals, if the nearest neighbors of one element\nare zeros (nonzeros), it will tend to be zero (nonzero) with high\nprobability, a similar idea of k-nearest neighbor (k-NN) algorithm\nwhich assumes that data close together more likely belong to the same\ncategory\\cite{fix1951discriminatory,cover1967nearest}. Therefore,\ninstead of explicitly modeling the sophisticated sparsity pattern,\nAMP-NNSPL specifies a flexible spike and slab prior on the unknown\nsignal and, after each AMP iteration, updates the sparse ratios as\nthe average of their nearest neighbor estimates via expectation maximization\n(EM)\\cite{dempster1977maximum}. In this way, the sparsity pattern\nis learned adaptively. Simulations results on both synthetic and real\ndata demonstrate the superiority of our proposed algorithm both in\nterms of reconstruction performance and computational efficiency.\n\n\n\\section{\\label{sec:System-Model}System Model }\n\nConsider the following linear Gaussian model\n\n", "index": 1, "text": "\\begin{equation}\n\\mathbf{y}=\\mathbf{Ax}+\\mathbf{w},\\label{eq:linear_model}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}=\\mathbf{Ax}+\\mathbf{w},\" display=\"block\"><mrow><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mi>\ud835\udc00\ud835\udc31</mi><mo>+</mo><mi>\ud835\udc30</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $\\lambda_{i}\\in(0,1)$ is the sparse ratio, i.e., the probability\nof $x_{i}$ being nonzero, $\\delta(x_{i})$ is the Dirac delta function,\n$f(x_{i})$ is the distribution of the nonzero entries in $\\mathbf{x}$,\ne.g., $f(x_{i})=\\mathcal{N}(x_{i};\\mu_{0},\\tau_{0})$ for sparse Gaussian\nsignals and $f(x_{i})=\\delta(x_{i}-1)$ for sparse binary signals,\netc.\n\nIt is important to note that in (\\ref{eq:common_BG_prior}) we specify\nan individual $\\lambda_{i}$ for each entry, as opposed to a common\nvalue in \\cite{vila2013expectation,krzakala2012probabilistic}. This\nis a key feature that will be exploited by the proposed algorithm\nfor reconstruction of structured sparse signals. Up to now, it seems\nthat no structure is ever introduced to enforce the underlying sparsity\npattern. Indeed, if the sparse ratios $\\lambda_{i},i=1,\\ldots,N$\nare learned independently, we will not benefit from the potential\nstructure. The main contribution of this letter is a novel adaptive\nlearning method which encourages clustered sparsity, as descried in\nSection \\ref{sec:Adaptive-Sparse-Reconstruction}.\n\n\n\\section{\\label{sec:Adaptive-Sparse-Reconstruction}Proposed Algorithm}\n\nIn this section, inspired by the idea of k-NN, we propose an adaptive\nreconstruction algorithm to recover clustered sparse signals without\nany prior knowledge of the sparsity pattern, e.g., structure and sparse\nratio. \n\nBefore proceeding, we first give a brief description of AMP. Generally,\nAMP decouples the vector estimation problem (\\ref{eq:linear_model})\ninto $N$ scalar problems in the asymptotic regime\\cite{montanari2012graphical,bayati2011dynamics}\n\n\n", "itemtype": "equation", "pos": 6653, "prevtext": "\nwhere $\\mathbf{x}\\in\\mathbb{R}^{N}$ is the unknown signal, $\\mathbf{y}\\in\\mathbb{R}^{M}$\nis the available measurements, $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$\nis the known measurement matrix, and $\\mathbf{w}\\in\\mathbb{R}^{M}\\sim\\mathcal{N}\\bigl(\\mathbf{w};0,\\Delta_{0}\\mathbf{I}\\bigr)$\nis the additive noise. $\\mathcal{N}\\bigl(\\mathbf{x};\\mathbf{m},\\mathbf{C}\\bigr)$\ndenotes a Gaussian distribution of $\\mathbf{x}$ with mean $\\mathbf{m}$\nand covariance $\\mathbf{C}$ and $\\mathbf{I}$ denotes the identity\nmatrix. Our goal is to estimate $\\mathbf{x}$ from $\\mathbf{y}$ when\n$M\\ll N$ and $\\mathbf{x}$ is clustered sparse while its specific\nsparsity pattern is unknown a priori. \n\nTo enforce sparsity, from a Bayesian perspective, the signals are\nassumed to follow sparsity-promoting prior distributions, e.g., Laplace\nprior\\cite{park2008bayesian}, automatic relevance determination \\cite{tipping2001sparse},\nand spike and slab prior\\cite{vila2013expectation,krzakala2012probabilistic}.\nIn this letter we consider a flexible spike and slab prior of the\nform\n\n", "index": 3, "text": "\\begin{equation}\np_{0}(\\mathbf{x})=\\stackrel[i=1]{N}{\\prod}p_{0}(x_{i})=\\stackrel[i=1]{N}{\\prod}\\bigl[\\bigl(1-\\lambda_{i}\\bigr)\\delta(x_{i})+\\lambda_{i}f(x_{i})\\bigr],\\label{eq:common_BG_prior}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"p_{0}(\\mathbf{x})=\\stackrel{[}{i}=1]{N}{\\prod}p_{0}(x_{i})=\\stackrel{[}{i}=1]{%&#10;N}{\\prod}\\bigl{[}\\bigl{(}1-\\lambda_{i}\\bigr{)}\\delta(x_{i})+\\lambda_{i}f(x_{i}%&#10;)\\bigr{]},\" display=\"block\"><mrow><mrow><msub><mi>p</mi><mn>0</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mover><mi>i</mi><mo maxsize=\"142%\" minsize=\"142%\">[</mo></mover><mo>=</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>N</mi><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><msub><mi>p</mi><mn>0</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mover><mi>i</mi><mo maxsize=\"142%\" minsize=\"142%\">[</mo></mover><mo>=</mo><mn>1</mn><mo stretchy=\"false\">]</mo><mi>N</mi><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mn>1</mn><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi>\u03b4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere the effective noise $\\tilde{w}_{i}$ asymptotically follows\n$\\mathcal{N}\\bigl(\\tilde{w}_{i};0,\\Sigma_{i}\\bigr)$. The values of\n$R_{i},\\Sigma_{i}$ are updated iteratively in each AMP iteration\n(see Algorithm \\ref{AMP-NNSPL Algorithm}) and the posterior distribution\nof $x_{i}$ is estimated as\n\n", "itemtype": "equation", "pos": 8482, "prevtext": "\nwhere $\\lambda_{i}\\in(0,1)$ is the sparse ratio, i.e., the probability\nof $x_{i}$ being nonzero, $\\delta(x_{i})$ is the Dirac delta function,\n$f(x_{i})$ is the distribution of the nonzero entries in $\\mathbf{x}$,\ne.g., $f(x_{i})=\\mathcal{N}(x_{i};\\mu_{0},\\tau_{0})$ for sparse Gaussian\nsignals and $f(x_{i})=\\delta(x_{i}-1)$ for sparse binary signals,\netc.\n\nIt is important to note that in (\\ref{eq:common_BG_prior}) we specify\nan individual $\\lambda_{i}$ for each entry, as opposed to a common\nvalue in \\cite{vila2013expectation,krzakala2012probabilistic}. This\nis a key feature that will be exploited by the proposed algorithm\nfor reconstruction of structured sparse signals. Up to now, it seems\nthat no structure is ever introduced to enforce the underlying sparsity\npattern. Indeed, if the sparse ratios $\\lambda_{i},i=1,\\ldots,N$\nare learned independently, we will not benefit from the potential\nstructure. The main contribution of this letter is a novel adaptive\nlearning method which encourages clustered sparsity, as descried in\nSection \\ref{sec:Adaptive-Sparse-Reconstruction}.\n\n\n\\section{\\label{sec:Adaptive-Sparse-Reconstruction}Proposed Algorithm}\n\nIn this section, inspired by the idea of k-NN, we propose an adaptive\nreconstruction algorithm to recover clustered sparse signals without\nany prior knowledge of the sparsity pattern, e.g., structure and sparse\nratio. \n\nBefore proceeding, we first give a brief description of AMP. Generally,\nAMP decouples the vector estimation problem (\\ref{eq:linear_model})\ninto $N$ scalar problems in the asymptotic regime\\cite{montanari2012graphical,bayati2011dynamics}\n\n\n", "index": 5, "text": "\\begin{equation}\n\\mathbf{y}=\\mathbf{Ax}+\\mathbf{w}\\longrightarrow\\begin{cases}\nR_{1}=x_{1}+\\tilde{w}_{1}\\\\\n\\vdots & ,\\\\\nR_{N}=x_{N}+\\tilde{w}_{N}\n\\end{cases}\\label{eq:scalar_system_AMP}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{y}=\\mathbf{Ax}+\\mathbf{w}\\longrightarrow\\begin{cases}R_{1}=x_{1}+%&#10;\\tilde{w}_{1}\\\\&#10;\\vdots&amp;,\\\\&#10;R_{N}=x_{N}+\\tilde{w}_{N}\\end{cases}\" display=\"block\"><mrow><mi>\ud835\udc32</mi><mo>=</mo><mrow><mi>\ud835\udc00\ud835\udc31</mi><mo>+</mo><mi>\ud835\udc30</mi></mrow><mo>\u27f6</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">~</mo></mover><mn>1</mn></msub></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>R</mi><mi>N</mi></msub><mo>=</mo><mrow><msub><mi>x</mi><mi>N</mi></msub><mo>+</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">~</mo></mover><mi>N</mi></msub></mrow></mrow></mtd><mtd/></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $Z(R_{i},\\Sigma_{i})$ is the normalization constant. From (\\ref{eq:post_dist_of_xi}),\nthe estimates of the mean and variance of $x_{i}$ are\n\n", "itemtype": "equation", "pos": 8980, "prevtext": "\nwhere the effective noise $\\tilde{w}_{i}$ asymptotically follows\n$\\mathcal{N}\\bigl(\\tilde{w}_{i};0,\\Sigma_{i}\\bigr)$. The values of\n$R_{i},\\Sigma_{i}$ are updated iteratively in each AMP iteration\n(see Algorithm \\ref{AMP-NNSPL Algorithm}) and the posterior distribution\nof $x_{i}$ is estimated as\n\n", "index": 7, "text": "\\begin{equation}\nq\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)=\\frac{1}{Z(R_{i},\\Sigma_{i})}p_{0}\\bigl(x_{i}\\bigr)\\mathcal{N}\\bigl(x_{i};R_{i},\\Sigma_{i}\\bigr),\\label{eq:post_dist_of_xi}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"q\\bigl{(}x_{i}|R_{i},\\Sigma_{i}\\bigr{)}=\\frac{1}{Z(R_{i},\\Sigma_{i})}p_{0}%&#10;\\bigl{(}x_{i}\\bigr{)}\\mathcal{N}\\bigl{(}x_{i};R_{i},\\Sigma_{i}\\bigr{)},\" display=\"block\"><mrow><mi>q</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><msub><mi>p</mi><mn>0</mn></msub><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\n\n\nFor more details of AMP and its extensions, the readers are referred\nto \\cite{donoho2009message,donoho2010message,montanari2012graphical,krzakala2012probabilistic}.\nTwo problems arise in traditional AMP. First, it assumes full knowledge\nof the prior distribution and noise variance, which is an impractical\nassumption. Second, it does not account for the potential structure\nof sparsity. In the sequel, we resort to expectation maximization\n(EM) to learn the unknown hyperparameters. Further, to encourage structured\nsparsity, we develop a nearest neighbor sparsity pattern learning\nrule motivated by the idea of k-NN algorithm. For lack of space, we\nonly consider the sparse Gaussian case, $f\\bigl(x_{i}\\bigr)=\\mathcal{N}\\bigl(x_{i};\\mu_{0},\\tau_{0}\\bigr),$\nwhile generalization to other settings is possible.\n\nThe hidden variables are chosen as the unknown signal vector $\\mathbf{x}$\nand the hyperparameters are denoted by $\\boldsymbol{\\theta}$. The\nspecific definition of $\\boldsymbol{\\theta}$ depends on the choice\nof distribution $f\\bigl(x\\bigr)$ in (\\ref{eq:common_BG_prior}).\nIn the Gaussian case, $\\boldsymbol{\\theta}=\\bigl\\{\\mu_{0},\\tau_{0},\\Delta_{0},\\lambda_{i},i=1,\\ldots,N\\bigr\\}$\nwhile in the binary case, $\\boldsymbol{\\theta}=\\bigl\\{\\Delta_{0},\\lambda_{i},i=1,\\ldots,N\\bigr\\}$.\nDenote by $\\boldsymbol{\\theta}^{t}$ the estimate of hyperparameters\nat the $t$th EM iteration, then EM alternates between the following\ntwo steps\\cite{dempster1977maximum} \n\n", "itemtype": "equation", "pos": 9318, "prevtext": "\nwhere $Z(R_{i},\\Sigma_{i})$ is the normalization constant. From (\\ref{eq:post_dist_of_xi}),\nthe estimates of the mean and variance of $x_{i}$ are\n\n", "index": 9, "text": "\\begin{align}\ng_{a}(R_{i},\\Sigma_{i}) & =\\int x_{i}q\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)dx_{i},\\label{eq:mean_def}\\\\\ng_{c}(R_{i},\\Sigma_{i}) & =\\int x_{i}^{2}q\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)dx_{i}-g_{a}^{2}(R_{i},\\Sigma_{i}).\\label{eq:var_def}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g_{a}(R_{i},\\Sigma_{i})\" display=\"inline\"><mrow><msub><mi>g</mi><mi>a</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int x_{i}q\\bigl{(}x_{i}|R_{i},\\Sigma_{i}\\bigr{)}dx_{i},\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><mo largeop=\"true\" symmetric=\"true\">\u222b</mo></mstyle><msub><mi>x</mi><mi>i</mi></msub><mi>q</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi>d</mi><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g_{c}(R_{i},\\Sigma_{i})\" display=\"inline\"><mrow><msub><mi>g</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int x_{i}^{2}q\\bigl{(}x_{i}|R_{i},\\Sigma_{i}\\bigr{)}dx_{i}-g_{a%&#10;}^{2}(R_{i},\\Sigma_{i}).\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><mo largeop=\"true\" symmetric=\"true\">\u222b</mo></mstyle><msubsup><mi>x</mi><mi>i</mi><mn>2</mn></msubsup><mi>q</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi>d</mi><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msubsup><mi>g</mi><mi>a</mi><mn>2</mn></msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $\\mathsf{E}\\bigl\\{\\cdot|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr\\}$\ndenotes expectation conditioned on observations $\\mathbf{y}$ with\nparameters $\\boldsymbol{\\theta}^{t}$, i.e., the expectation is with\nrespect to the posterior distribution $p\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)$.\nFrom (\\ref{eq:linear_model}), (\\ref{eq:common_BG_prior}), the joint\ndistribution $p(\\mathbf{\\mathbf{x}},\\mathbf{y})$ in (\\ref{eq:E_step_def-1})\nis defined as \n\n", "itemtype": "equation", "pos": 11043, "prevtext": "\n\n\nFor more details of AMP and its extensions, the readers are referred\nto \\cite{donoho2009message,donoho2010message,montanari2012graphical,krzakala2012probabilistic}.\nTwo problems arise in traditional AMP. First, it assumes full knowledge\nof the prior distribution and noise variance, which is an impractical\nassumption. Second, it does not account for the potential structure\nof sparsity. In the sequel, we resort to expectation maximization\n(EM) to learn the unknown hyperparameters. Further, to encourage structured\nsparsity, we develop a nearest neighbor sparsity pattern learning\nrule motivated by the idea of k-NN algorithm. For lack of space, we\nonly consider the sparse Gaussian case, $f\\bigl(x_{i}\\bigr)=\\mathcal{N}\\bigl(x_{i};\\mu_{0},\\tau_{0}\\bigr),$\nwhile generalization to other settings is possible.\n\nThe hidden variables are chosen as the unknown signal vector $\\mathbf{x}$\nand the hyperparameters are denoted by $\\boldsymbol{\\theta}$. The\nspecific definition of $\\boldsymbol{\\theta}$ depends on the choice\nof distribution $f\\bigl(x\\bigr)$ in (\\ref{eq:common_BG_prior}).\nIn the Gaussian case, $\\boldsymbol{\\theta}=\\bigl\\{\\mu_{0},\\tau_{0},\\Delta_{0},\\lambda_{i},i=1,\\ldots,N\\bigr\\}$\nwhile in the binary case, $\\boldsymbol{\\theta}=\\bigl\\{\\Delta_{0},\\lambda_{i},i=1,\\ldots,N\\bigr\\}$.\nDenote by $\\boldsymbol{\\theta}^{t}$ the estimate of hyperparameters\nat the $t$th EM iteration, then EM alternates between the following\ntwo steps\\cite{dempster1977maximum} \n\n", "index": 11, "text": "\\begin{align}\nQ\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr) & =\\mathsf{E}\\Bigl\\{\\ln p\\bigl(\\mathbf{\\mathbf{x}},\\mathbf{y}\\bigr)|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\Bigr\\},\\label{eq:E_step_def-1}\\\\\n\\boldsymbol{\\theta}^{t+1} & =\\arg\\underset{\\boldsymbol{\\theta}}{\\max}Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr),\\label{eq:max_step_def-1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Q\\bigl{(}\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr{)}\" display=\"inline\"><mrow><mi>Q</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\ud835\udf3d</mi><mo>,</mo><msup><mi>\ud835\udf3d</mi><mi>t</mi></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathsf{E}\\Bigl{\\{}\\ln p\\bigl{(}\\mathbf{\\mathbf{x}},\\mathbf{y}%&#10;\\bigr{)}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\Bigr{\\}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udda4</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">{</mo><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo><mrow><mi>\ud835\udc32</mi><mo>;</mo><msup><mi>\ud835\udf3d</mi><mi>t</mi></msup></mrow><mo maxsize=\"160%\" minsize=\"160%\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\boldsymbol{\\theta}^{t+1}\" display=\"inline\"><msup><mi>\ud835\udf3d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\arg\\underset{\\boldsymbol{\\theta}}{\\max}Q\\bigl{(}\\boldsymbol{%&#10;\\theta},\\boldsymbol{\\theta}^{t}\\bigr{)},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder accentunder=\"true\"><mi>max</mi><mo>\ud835\udf3d</mo></munder><mo>\u2062</mo><mi>Q</mi></mrow></mrow><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\ud835\udf3d</mi><mo>,</mo><msup><mi>\ud835\udf3d</mi><mi>t</mi></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $p(\\mathbf{y}|\\mathbf{\\mathbf{x}})=\\mathcal{N}\\bigl(\\mathbf{y};\\mathbf{Ax},\\Delta_{0}\\mathbf{I}\\bigr)$.\nAMP offers an efficient approximation of $p\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)$,\ndenoted as $q\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)=\\prod_{i}q\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)$,\nwhereby the E step (\\ref{eq:E_step_def-1}) can be efficiently calculated.\nSince joint optimization of $\\mathbf{\\boldsymbol{\\theta}}$ is difficult,\nwe adopt the incremental EM update rule proposed in \\cite{neal1998view},\ni.e., we update one or partial elements at a time while holding the\nother parameters fixed. \n\nAfter some algebra, the marginal posterior distribution of $x_{i}$\nin (\\ref{eq:post_dist_of_xi}) can be written as \n\n", "itemtype": "equation", "pos": 11889, "prevtext": "\nwhere $\\mathsf{E}\\bigl\\{\\cdot|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr\\}$\ndenotes expectation conditioned on observations $\\mathbf{y}$ with\nparameters $\\boldsymbol{\\theta}^{t}$, i.e., the expectation is with\nrespect to the posterior distribution $p\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)$.\nFrom (\\ref{eq:linear_model}), (\\ref{eq:common_BG_prior}), the joint\ndistribution $p(\\mathbf{\\mathbf{x}},\\mathbf{y})$ in (\\ref{eq:E_step_def-1})\nis defined as \n\n", "index": 13, "text": "\\begin{equation}\np(\\mathbf{\\mathbf{x}},\\mathbf{y})=p\\bigl(\\mathbf{y}|\\mathbf{\\mathbf{x}}\\bigr)\\prod_{i}(1-\\lambda_{i})\\delta(x_{i})+\\lambda_{i}f(x_{i}),\\label{eq:joint_dist}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"p(\\mathbf{\\mathbf{x}},\\mathbf{y})=p\\bigl{(}\\mathbf{y}|\\mathbf{\\mathbf{x}}\\bigr%&#10;{)}\\prod_{i}(1-\\lambda_{i})\\delta(x_{i})+\\lambda_{i}f(x_{i}),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>i</mi></munder><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>\u03b4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mi>\u03bb</mi><mi>i</mi></msub><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere \n\n", "itemtype": "equation", "pos": 12857, "prevtext": "\nwhere $p(\\mathbf{y}|\\mathbf{\\mathbf{x}})=\\mathcal{N}\\bigl(\\mathbf{y};\\mathbf{Ax},\\Delta_{0}\\mathbf{I}\\bigr)$.\nAMP offers an efficient approximation of $p\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)$,\ndenoted as $q\\bigl(\\mathbf{\\mathbf{x}}|\\mathbf{y};\\boldsymbol{\\theta}^{t}\\bigr)=\\prod_{i}q\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)$,\nwhereby the E step (\\ref{eq:E_step_def-1}) can be efficiently calculated.\nSince joint optimization of $\\mathbf{\\boldsymbol{\\theta}}$ is difficult,\nwe adopt the incremental EM update rule proposed in \\cite{neal1998view},\ni.e., we update one or partial elements at a time while holding the\nother parameters fixed. \n\nAfter some algebra, the marginal posterior distribution of $x_{i}$\nin (\\ref{eq:post_dist_of_xi}) can be written as \n\n", "index": 15, "text": "\\begin{equation}\nq\\bigl(x_{i}|R_{i},\\Sigma_{i}\\bigr)=\\bigl(1-\\pi_{i}\\bigr)\\delta\\bigl(x_{i}\\bigr)+\\pi_{i}\\mathcal{N}\\bigl(x_{i};m_{i},V_{i}\\bigr),\\label{eq:marginal_post_x}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"q\\bigl{(}x_{i}|R_{i},\\Sigma_{i}\\bigr{)}=\\bigl{(}1-\\pi_{i}\\bigr{)}\\delta\\bigl{(%&#10;}x_{i}\\bigr{)}+\\pi_{i}\\mathcal{N}\\bigl{(}x_{i};m_{i},V_{i}\\bigr{)},\" display=\"block\"><mrow><mi>q</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi>\u03b4</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>+</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><msub><mi>m</mi><mi>i</mi></msub><mo>,</mo><msub><mi>V</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nNote that for notational brevity, we have omitted the iteration index\n$t$. The mean and variance defined in (\\ref{eq:mean_def}) and (\\ref{eq:var_def})\ncan now be explicitly calculated as \n\n", "itemtype": "equation", "pos": 13052, "prevtext": "\nwhere \n\n", "index": 17, "text": "\\begin{align}\nV_{i} & =\\frac{\\tau_{0}\\Sigma_{i}}{\\Sigma_{i}+\\tau_{0}},\\label{eq:var1}\\\\\nm_{i} & =\\frac{\\tau_{0}R_{i}+\\Sigma_{i}\\mu_{0}}{\\Sigma_{i}+\\tau_{0}},\\label{eq:mean1}\\\\\n\\pi_{i} & =\\frac{\\lambda_{i}}{\\lambda_{i}+\\bigl(1-\\lambda_{i}\\bigr)\\exp\\bigl(-\\mathcal{L}\\bigr)},\\label{eq:pi_calculate_formula}\\\\\n\\mathcal{L} & =\\frac{1}{2}\\ln\\frac{\\Sigma_{i}}{\\Sigma_{i}+\\tau_{0}}+\\frac{R_{i}^{2}}{2\\Sigma_{i}}-\\frac{\\bigl(R_{i}-\\mu_{0}\\bigr)^{2}}{2\\bigl(\\Sigma_{i}+\\tau_{0}\\bigr)}.\\label{eq:log_znz_2_zz}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V_{i}\" display=\"inline\"><msub><mi>V</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\tau_{0}\\Sigma_{i}}{\\Sigma_{i}+\\tau_{0}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c4</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m_{i}\" display=\"inline\"><msub><mi>m</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\tau_{0}R_{i}+\\Sigma_{i}\\mu_{0}}{\\Sigma_{i}+\\tau_{0}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>\u03c4</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>R</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03bc</mi><mn>0</mn></msub></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\pi_{i}\" display=\"inline\"><msub><mi>\u03c0</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\lambda_{i}}{\\lambda_{i}+\\bigl{(}1-\\lambda_{i}\\bigr{)}\\exp%&#10;\\bigl{(}-\\mathcal{L}\\bigr{)}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03bb</mi><mi>i</mi></msub><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>+</mo><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mo>-</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{L}\" display=\"inline\"><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{2}\\ln\\frac{\\Sigma_{i}}{\\Sigma_{i}+\\tau_{0}}+\\frac{R_{i}%&#10;^{2}}{2\\Sigma_{i}}-\\frac{\\bigl{(}R_{i}-\\mu_{0}\\bigr{)}^{2}}{2\\bigl{(}\\Sigma_{i%&#10;}+\\tau_{0}\\bigr{)}}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow></mfrac></mstyle></mrow></mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>R</mi><mi>i</mi><mn>2</mn></msubsup><mrow><mn>2</mn><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub></mrow></mfrac></mstyle></mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03bc</mi><mn>0</mn></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\n\n\nTo learn the sparse ratios $\\lambda_{i},i=1,\\ldots,N$, we need to\nmaximize $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\lambda_{i}$. After some algebra, we obtain the\nstandard EM update equation as $\\lambda_{i}^{t+1}=\\pi_{i}^{t}$, which,\nalbeit simple, fails to capture the inherent structure in the sparsity\npattern. To address this problem, a novel learning rule is proposed\nas follows\n\n", "itemtype": "equation", "pos": 13752, "prevtext": "\nNote that for notational brevity, we have omitted the iteration index\n$t$. The mean and variance defined in (\\ref{eq:mean_def}) and (\\ref{eq:var_def})\ncan now be explicitly calculated as \n\n", "index": 19, "text": "\\begin{align}\ng_{a}\\bigl(R_{i},\\Sigma_{i}\\bigr) & =\\pi_{i}m_{i},\\label{eq:post_mean-1-1}\\\\\ng_{c}\\bigl(R_{i},\\Sigma_{i}\\bigr) & =\\pi_{i}\\bigl(m_{i}^{2}+V_{i}\\bigr)-g_{a}^{2}\\bigl(R_{i},\\Sigma_{i}\\bigr).\\label{eq:post_var-1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g_{a}\\bigl{(}R_{i},\\Sigma_{i}\\bigr{)}\" display=\"inline\"><mrow><msub><mi>g</mi><mi>a</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\pi_{i}m_{i},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>\u03c0</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g_{c}\\bigl{(}R_{i},\\Sigma_{i}\\bigr{)}\" display=\"inline\"><mrow><msub><mi>g</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\pi_{i}\\bigl{(}m_{i}^{2}+V_{i}\\bigr{)}-g_{a}^{2}\\bigl{(}R_{i},%&#10;\\Sigma_{i}\\bigr{)}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\u03c0</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msubsup><mi>m</mi><mi>i</mi><mn>2</mn></msubsup><mo>+</mo><msub><mi>V</mi><mi>i</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>-</mo><mrow><msubsup><mi>g</mi><mi>a</mi><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">\u03a3</mi><mi>i</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $\\mathcal{N}\\bigl(i\\bigr)$ denotes the set of nearest neighbor\nindexes of element $x_{i}$ in $\\mathbf{x}$ (\\ref{eq:linear_model})\nand $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|$ denotes the cardinality\nof $\\mathcal{N}\\bigl(i\\bigr)$. For one dimensional (1D) data , $\\mathcal{N}\\bigl(i\\bigr)=\\bigl\\{ i-1,i+1\\bigr\\}$\n\\footnote{For end points of 1D data, the nearest neighbor set has only one element.\nFor edge points of 2D data, the nearest neighbor set has only two\nor three elements.\n} and $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|=2$, while for two dimensional\n(2D) data, $\\mathcal{N}\\bigl(i\\bigr)=\\bigl\\{(q,l-1),(q,l+1),(q-1,l),(q+1,l)\\bigr\\}$\nand $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|=4$, where $(q,l)$ indicates\nthe coordinates of $x_{i}$ in the 2D space. Generalizations to other\ncases can be made.\n\nNote that in (\\ref{eq:Novel_update_lamda}), we have chosen the nearest\nneighbor of each element, excluding itself, as the neighboring set.\nThe estimate of one sparse ratio is not determined by its own estimate,\nbut rather the average of its nearest neighbor estimates. The insight\nfor this choice is that, for clustered sparse signals, if the nearest\nneighbors of one element are zero (nonzero), it will be zero (nonzero)\nwith high probability, a similar idea to k-NN. If the neighboring\nset is chosen as the whole elements, the proposed algorithm reduces\nto EM-BG-GAMP\\cite{vila2013expectation,krzakala2012probabilistic}.\n\nThe leaning of other hyperparameters follows the standard rule of\nEM algorithm. Maximizing $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\Delta_{0}$ and after some algebra, we obtain \n\n", "itemtype": "equation", "pos": 14409, "prevtext": "\n\n\nTo learn the sparse ratios $\\lambda_{i},i=1,\\ldots,N$, we need to\nmaximize $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\lambda_{i}$. After some algebra, we obtain the\nstandard EM update equation as $\\lambda_{i}^{t+1}=\\pi_{i}^{t}$, which,\nalbeit simple, fails to capture the inherent structure in the sparsity\npattern. To address this problem, a novel learning rule is proposed\nas follows\n\n", "index": 21, "text": "\\begin{equation}\n\\lambda_{i}^{t+1}=\\frac{1}{\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|}\\underset{j\\in\\mathcal{N}(i)}{\\sum}\\pi_{j}^{t},\\label{eq:Novel_update_lamda}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\lambda_{i}^{t+1}=\\frac{1}{\\bigl{|}\\mathcal{N}\\bigl{(}i\\bigr{)}\\bigr{|}}%&#10;\\underset{j\\in\\mathcal{N}(i)}{\\sum}\\pi_{j}^{t},\" display=\"block\"><mrow><mrow><msubsup><mi>\u03bb</mi><mi>i</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>i</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo fence=\"true\" maxsize=\"120%\" minsize=\"120%\">|</mo></mrow></mfrac><mo>\u2062</mo><munder accentunder=\"true\"><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2062</mo><msubsup><mi>\u03c0</mi><mi>j</mi><mi>t</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\nwhere $Z_{a}^{t}$ and $V_{a}^{t}$ are obtained within the AMP iteration\nand are defined in Algorithm \\ref{AMP-NNSPL Algorithm}. Similarly,\nmaximizing $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\mu_{0}$ and $\\tau_{0}$ results in the update equations\n\n", "itemtype": "equation", "pos": 16222, "prevtext": "\nwhere $\\mathcal{N}\\bigl(i\\bigr)$ denotes the set of nearest neighbor\nindexes of element $x_{i}$ in $\\mathbf{x}$ (\\ref{eq:linear_model})\nand $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|$ denotes the cardinality\nof $\\mathcal{N}\\bigl(i\\bigr)$. For one dimensional (1D) data , $\\mathcal{N}\\bigl(i\\bigr)=\\bigl\\{ i-1,i+1\\bigr\\}$\n\\footnote{For end points of 1D data, the nearest neighbor set has only one element.\nFor edge points of 2D data, the nearest neighbor set has only two\nor three elements.\n} and $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|=2$, while for two dimensional\n(2D) data, $\\mathcal{N}\\bigl(i\\bigr)=\\bigl\\{(q,l-1),(q,l+1),(q-1,l),(q+1,l)\\bigr\\}$\nand $\\bigl|\\mathcal{N}\\bigl(i\\bigr)\\bigr|=4$, where $(q,l)$ indicates\nthe coordinates of $x_{i}$ in the 2D space. Generalizations to other\ncases can be made.\n\nNote that in (\\ref{eq:Novel_update_lamda}), we have chosen the nearest\nneighbor of each element, excluding itself, as the neighboring set.\nThe estimate of one sparse ratio is not determined by its own estimate,\nbut rather the average of its nearest neighbor estimates. The insight\nfor this choice is that, for clustered sparse signals, if the nearest\nneighbors of one element are zero (nonzero), it will be zero (nonzero)\nwith high probability, a similar idea to k-NN. If the neighboring\nset is chosen as the whole elements, the proposed algorithm reduces\nto EM-BG-GAMP\\cite{vila2013expectation,krzakala2012probabilistic}.\n\nThe leaning of other hyperparameters follows the standard rule of\nEM algorithm. Maximizing $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\Delta_{0}$ and after some algebra, we obtain \n\n", "index": 23, "text": "\\begin{equation}\n\\Delta_{0}^{t+1}=\\frac{1}{M}\\sum_{a}\\Bigl[\\frac{\\bigl(y_{a}-Z_{a}^{t}\\bigr)^{2}}{\\bigl(1+V_{a}^{t}/\\Delta_{0}^{t}\\bigr)^{2}}+\\frac{\\Delta_{0}^{t}V_{a}^{t}}{\\Delta_{0}^{t}+V_{a}^{t}}\\Bigr],\\label{eq:noise_learning_free_energy}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\Delta_{0}^{t+1}=\\frac{1}{M}\\sum_{a}\\Bigl{[}\\frac{\\bigl{(}y_{a}-Z_{a}^{t}\\bigr%&#10;{)}^{2}}{\\bigl{(}1+V_{a}^{t}/\\Delta_{0}^{t}\\bigr{)}^{2}}+\\frac{\\Delta_{0}^{t}V%&#10;_{a}^{t}}{\\Delta_{0}^{t}+V_{a}^{t}}\\Bigr{]},\" display=\"block\"><mrow><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mn>0</mn><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><mfrac><mn>1</mn><mi>M</mi></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>a</mi></munder><mrow><mo maxsize=\"160%\" minsize=\"160%\">[</mo><mrow><mfrac><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>y</mi><mi>a</mi></msub><mo>-</mo><msubsup><mi>Z</mi><mi>a</mi><mi>t</mi></msubsup></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><msubsup><mi>V</mi><mi>a</mi><mi>t</mi></msubsup><mo>/</mo><msubsup><mi mathvariant=\"normal\">\u0394</mi><mn>0</mn><mi>t</mi></msubsup></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup></mfrac><mo>+</mo><mfrac><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mn>0</mn><mi>t</mi></msubsup><mo>\u2062</mo><msubsup><mi>V</mi><mi>a</mi><mi>t</mi></msubsup></mrow><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mn>0</mn><mi>t</mi></msubsup><mo>+</mo><msubsup><mi>V</mi><mi>a</mi><mi>t</mi></msubsup></mrow></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\n\n\nValid initialization of the unknown hyperparameters is essential since\nEM algorithm may converge to a local maximum or a saddle point of\nthe likelihood function\\cite{dempster1977maximum}. The sparse ratios\n$\\lambda_{i}$ and noise variance $\\Delta_{0}$ are initialized as\n$\\lambda_{i}^{1}=0.5$ and $\\Delta_{0}^{1}=\\bigl\\Vert\\mathbf{y}\\bigr\\Vert_{2}^{2}/M\\bigl(\\textrm{SNR}^{0}+1\\bigr)$,\nrespectively, where $\\textrm{SNR}^{0}$ is suggested to be 100 \\cite{vila2013expectation}.\nFor the sparse Gaussian case, active mean $\\mu_{0}$ and variance\n$\\tau_{0}$ are initialized as $\\mu_{0}^{1}=0$, and $\\tau_{0}^{1}=\\bigl(\\bigl\\Vert\\mathbf{y}\\bigr\\Vert_{2}^{2}-M\\Delta_{0}^{1}\\bigr)/\\lambda_{i}^{1}\\bigl\\Vert\\mathbf{A}\\bigr\\Vert_{F}^{2}$,\nrespectively, where$\\bigl\\Vert\\mathbf{\\cdot}\\bigr\\Vert_{2}$, $\\bigl\\Vert\\mathbf{\\cdot}\\bigr\\Vert_{F}$\nare the $l_{2}$ norm and Frobenius norm, respectively.\n\nThe proposed approximate message passing with nearest neighbor sparsity\npattern learning (AMP-NNSPL) is summarized in Algorithm \\ref{AMP-NNSPL Algorithm}.\nThe complexity of AMP-NNSPL is dominated by matrix-vector multiplications\nin the original AMP and thus only scales as $\\mathcal{O}(MN)$, i.e.,\nthe proposed algorithm is computationally efficient.\n\n\\begin{algorithm}\n\\protect\\caption{AMP-NNSPL Algorithm }\n\n\n\\textbf{Input}: $\\mathbf{y}$ $\\mathbf{A}$.\n\n\\begin{raggedright}\n\\textbf{Initialization}: Set $t=1$ and $T_{max},\\epsilon_{toc}$.\nInitialize $\\mu_{0},\\tau_{0},\\Delta_{0}$ and $\\lambda_{i},i=1,\\ldots,N$\nas in Section \\ref{sec:Adaptive-Sparse-Reconstruction}. $\\hat{x}_{i}^{1}=\\int x_{i}p_{0}(x_{i})dx_{i},\\nu_{i}^{1}=\\int|x_{i}-\\hat{x}_{i}^{1}|^{2}p_{0}(x_{i})dx_{i},i=1,\\ldots,N$,\n$V_{a}^{0}=1,Z_{a}^{0}=y_{a},a=1,\\ldots,M.$\n\\par\\end{raggedright}\n\n1) Factor node update: For $a=1,\\ldots,M$\n\n", "itemtype": "equation", "pos": 16762, "prevtext": "\nwhere $Z_{a}^{t}$ and $V_{a}^{t}$ are obtained within the AMP iteration\nand are defined in Algorithm \\ref{AMP-NNSPL Algorithm}. Similarly,\nmaximizing $Q\\bigl(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{t}\\bigr)$\nwith respect to $\\mu_{0}$ and $\\tau_{0}$ results in the update equations\n\n", "index": 25, "text": "\\begin{align}\n\\mu_{0}^{t+1} & =\\frac{\\sum_{i}\\pi_{i}^{t}m_{i}^{t}}{\\sum_{i}\\pi_{i}^{t}},\\label{eq:mu_0_update}\\\\\n\\tau_{0}^{t+1} & =\\frac{1}{\\sum_{i}\\pi_{i}^{t}}\\underset{i}{\\sum}\\pi_{i}^{t}\\bigl[\\bigl(\\mu_{0}^{t}-m_{i}^{t}\\bigr)^{2}+V_{i}\\bigr].\\label{eq:tao_0_update}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu_{0}^{t+1}\" display=\"inline\"><msubsup><mi>\u03bc</mi><mn>0</mn><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\sum_{i}\\pi_{i}^{t}m_{i}^{t}}{\\sum_{i}\\pi_{i}^{t}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><mrow><msubsup><mi>\u03c0</mi><mi>i</mi><mi>t</mi></msubsup><mo>\u2062</mo><msubsup><mi>m</mi><mi>i</mi><mi>t</mi></msubsup></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msubsup><mi>\u03c0</mi><mi>i</mi><mi>t</mi></msubsup></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tau_{0}^{t+1}\" display=\"inline\"><msubsup><mi>\u03c4</mi><mn>0</mn><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{\\sum_{i}\\pi_{i}^{t}}\\underset{i}{\\sum}\\pi_{i}^{t}\\bigl{%&#10;[}\\bigl{(}\\mu_{0}^{t}-m_{i}^{t}\\bigr{)}^{2}+V_{i}\\bigr{]}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msubsup><mi>\u03c0</mi><mi>i</mi><mi>t</mi></msubsup></mrow></mfrac></mstyle><mo>\u2062</mo><munder accentunder=\"true\"><mstyle displaystyle=\"true\"><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo></mstyle><mo>\ud835\udc56</mo></munder><mo>\u2062</mo><msubsup><mi>\u03c0</mi><mi>i</mi><mi>t</mi></msubsup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msubsup><mi>\u03bc</mi><mn>0</mn><mi>t</mi></msubsup><mo>-</mo><msubsup><mi>m</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msub><mi>V</mi><mi>i</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00543.tex", "nexttext": "\n\n\n2) Variable node update: For $i=1,\\ldots,N$\n\\begin{alignat*}{1}\n\\Sigma_{i}^{t} & =\\Bigl[\\sum_{a}\\frac{|A_{ai}|^{2}}{\\Delta_{0}^{t}+V_{a}^{t}}\\Bigr]^{-1},\\\\\nR_{i}^{t} & =\\hat{x}_{i}^{t}+\\Sigma_{i}^{t}\\sum_{a}\\frac{A_{ai}\\bigl(y_{a}-Z_{a}^{t}\\bigr)}{\\Delta_{0}^{t}+V_{a}^{t}},\\\\\n\\hat{x}_{i}^{t+1} & =g_{a}\\left(R_{i}^{t},\\Sigma_{i}^{t}\\right),\\\\\n\\hat{\\nu}_{i}^{t+1} & =g_{c}\\left(R_{i}^{t},\\Sigma_{i}^{t}\\right).\n\\end{alignat*}\n\n\n3) Update $\\lambda_{i}^{t+1},i=1,\\ldots N$, as (\\ref{eq:Novel_update_lamda});\n\n4) Update $\\mu_{0}^{t+1},\\tau_{0}^{t+1},\\Delta_{0}^{t+1}$ as (\\ref{eq:mu_0_update}),\n(\\ref{eq:tao_0_update}), and (\\ref{eq:noise_learning_free_energy});\n\n5) Set $t\\leftarrow t+1$ and proceed to step 1) until $T_{max}$\niterations or $\\bigl\\Vert\\mathbf{\\hat{x}}^{t+1}-\\mathbf{\\hat{x}}^{t}\\bigr\\Vert_{2}<\\epsilon_{toc}\\bigl\\Vert\\hat{\\mathbf{x}}^{t}\\bigr\\Vert_{2}$\n. \\label{AMP-NNSPL Algorithm}\n\\end{algorithm}\n\n\n\n\\section{\\label{sec:Numerical-Experiments}Numerical Experiments}\n\nIn this section, a series of numerical experiments are performed to\ndemonstrate the performance of the proposed algorithm under various\nsettings. Comparisons are made to some state-of-the-art methods which\nneed no prior information of the sparstiy pattern, e.g., PC-SBL\\cite{fang2015pattern}\nand its AMP version PCSBL-GAMP\\cite{fang2015two}, MBCS-LBP\\cite{Yu2015model},\nand EM-BG-GAMP\\cite{vila2013expectation}. The performance of Basis\nPursuit (BP) \\cite{chen1998atomic,candes2005decoding,candes2006robust}\nis also evaluated. Throughout the experiments, we set the maximum\nnumber of iterations for AMP-NNSPL, PCSBL-GAMP, and EM-BG-GAMP to\nbe $T_{max}=200$, and the tolerance value of termination to be $\\epsilon_{toc}=10^{-6}.$\nFor other algorithms, we use the defaut setting. The elements of measurement\nmatrix $\\mathbf{A}\\in\\mathbb{R}^{M\\times N}$ are independently generated\nfollowing standard Gaussian distribution and the columns are normalized\nto unit norm. The success rate is defined as the ratio of the number\nof successful trials to the total number of experiments, where a trial\nis successful if the normalized mean square error (NMSE) is less than\n-60 dB, where $\\mathsf{NMSE}=20\\log_{10}(\\bigl\\Vert\\mathbf{\\hat{x}}-\\mathbf{x}\\bigr\\Vert_{2}/\\bigl\\Vert\\mathbf{x}\\bigr\\Vert_{2})$.\nThe pattern recovery success rate is defined as the ratio of the number\nof successful trials to the total number of experiments, where a trial\nis successful if the support is exactly recovered. A coefficient whose\nmagnitude is less than $10^{-4}$ is deemed as a zero coefficient.\n\n\n\\subsection{Synthetic Data}\n\nWe generate synthetic block-sparse signals in a similar way as \\cite{zhang2013extension,fang2015pattern},\nwhere $K$ nonzero elements are partitioned into $L$ blocks with\nrandom sizes and random locations. Set $N=100,K=25,L=4$ and the nonzero\nelements are generated independently following Gaussian distribution\nwith mean $\\mu_{0}=3$ and variance $\\tau_{0}=1$. The results are\naveraged over 1000 independent runs. Fig. \\ref{Success_rate_noiseless}\ndepicts the success rate and pattern recovery success rate. It can\nbe seen that AMP-NNSPL achieves the highest success rate and pattern\nrecovery rate at various measurement ratios. In the noisy setting,\nFig. \\ref{Success_rate_noiseless-1} shows the average NMSE and runtime\nof different algorithms when the signal to noise ratio (SNR) is 50\ndB, where $SNR=20\\log_{10}(\\bigl\\Vert\\mathbf{A}\\mathbf{x}\\bigr\\Vert_{2}/\\bigl\\Vert\\mathbf{w}\\bigr\\Vert_{2})$.\nWe see that AMP-NNSPL outperforms other methods both in terms of NMSE\nand computational efficiency. \n\n\\begin{figure}\n\\includegraphics[width=3.85cm]{1D_synthetic_success_rate}\\includegraphics[width=3.85cm]{1D_synthetic_pattern_success_rate}\n\n\\protect\\caption{Success rate (left) and pattern success rate (right) vs. $M/N$ for\nblock-sparse signals $N=100$, $K=25$, $L=4$, noiseless case. }\n\n\n\\label{Success_rate_noiseless}\n\\end{figure}\n\n\n\\begin{figure}\n\\includegraphics[width=3.85cm]{1D_synthetic_NMSE_SNR50}\\includegraphics[width=3.85cm]{1D_synthetic_runtime_SNR50}\n\n\\protect\\caption{NMSE (left) and recovery time (right) vs. $M/N$ for block-sparse\nsignals $N=100$, $K=25$, $L=4$, SNR = 50 dB.}\n\n\n\\label{Success_rate_noiseless-1}\n\\end{figure}\n\n\n\n\\subsection{Real Data}\n\nTo evaluate the performance on real data, we consider a real angiogram\nimage\\cite{hegde2015nearly} of 100$\\times$100 pixels with sparsity\naround 0.12. Fig. \\ref{2D_image} depicts the success rate in noiseless\ncase and NMSE at $SNR=50$ dB, respectively. The MBCS-LBP and PC-SBL\nalgorithms are not included due to their high computational complexity.\nIt can be seen that AMP-NNSPL significantly outperforms other methods\nboth in terms of success rate and NMSE. In particular, when $M/N=0.12$\nand $SNR=50$ dB, typical recovery results are illustrated in Fig.\n\\ref{2D_image-1}, which shows that AMP-NNSPL achives the best reconstruction\nperformance.\n\n\\begin{figure}\n\\includegraphics[width=4cm]{2D_real_image_sucess_rate}\\includegraphics[width=4cm]{2D_real_image_NMSE_SNR50}\n\n\\protect\\caption{Success rate (left) in noiseless case and NMSE (right) at $SNR=50\\textrm{dB}$\nvs. $M/N$ for real 2D angiogram image. }\n\\label{2D_image}\n\\end{figure}\n\n\n\\begin{figure}[H]\n\\begin{centering}\n\\includegraphics[width=7cm]{Visio-recover_image}\n\\par\\end{centering}\n\n\\protect\\caption{Recovery results of real 2D angiogram image in noisy setting when\n$M/N=0.12$ and $SNR=50$ dB.}\n\n\n\\label{2D_image-1}\n\\end{figure}\n\n\n\n\\section{\\label{sec:Conclusion}Conclusion}\n\nIn this lettter, we propose an efficient algorithm termed AMP-NNSPL\nto recover clustered sparse signals when the sparsity pattern is unknown.\nInspired by the k-NN algorithm, AMP-NNSPL learns the sparse ratios\nin each AMP iteration as the average of their nearest neighbor estimates\nusing EM, thereby the sparsity pattern is learned adaptively. Experimental\nresults on both synthetic and real data demonstrate the state-of-the-art\nperformance of AMP-NNSPL.\n\n\\newpage{}\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{IEEEabrv,Mybib}\n\n\n", "itemtype": "equation", "pos": 18832, "prevtext": "\n\n\nValid initialization of the unknown hyperparameters is essential since\nEM algorithm may converge to a local maximum or a saddle point of\nthe likelihood function\\cite{dempster1977maximum}. The sparse ratios\n$\\lambda_{i}$ and noise variance $\\Delta_{0}$ are initialized as\n$\\lambda_{i}^{1}=0.5$ and $\\Delta_{0}^{1}=\\bigl\\Vert\\mathbf{y}\\bigr\\Vert_{2}^{2}/M\\bigl(\\textrm{SNR}^{0}+1\\bigr)$,\nrespectively, where $\\textrm{SNR}^{0}$ is suggested to be 100 \\cite{vila2013expectation}.\nFor the sparse Gaussian case, active mean $\\mu_{0}$ and variance\n$\\tau_{0}$ are initialized as $\\mu_{0}^{1}=0$, and $\\tau_{0}^{1}=\\bigl(\\bigl\\Vert\\mathbf{y}\\bigr\\Vert_{2}^{2}-M\\Delta_{0}^{1}\\bigr)/\\lambda_{i}^{1}\\bigl\\Vert\\mathbf{A}\\bigr\\Vert_{F}^{2}$,\nrespectively, where$\\bigl\\Vert\\mathbf{\\cdot}\\bigr\\Vert_{2}$, $\\bigl\\Vert\\mathbf{\\cdot}\\bigr\\Vert_{F}$\nare the $l_{2}$ norm and Frobenius norm, respectively.\n\nThe proposed approximate message passing with nearest neighbor sparsity\npattern learning (AMP-NNSPL) is summarized in Algorithm \\ref{AMP-NNSPL Algorithm}.\nThe complexity of AMP-NNSPL is dominated by matrix-vector multiplications\nin the original AMP and thus only scales as $\\mathcal{O}(MN)$, i.e.,\nthe proposed algorithm is computationally efficient.\n\n\\begin{algorithm}\n\\protect\\caption{AMP-NNSPL Algorithm }\n\n\n\\textbf{Input}: $\\mathbf{y}$ $\\mathbf{A}$.\n\n\\begin{raggedright}\n\\textbf{Initialization}: Set $t=1$ and $T_{max},\\epsilon_{toc}$.\nInitialize $\\mu_{0},\\tau_{0},\\Delta_{0}$ and $\\lambda_{i},i=1,\\ldots,N$\nas in Section \\ref{sec:Adaptive-Sparse-Reconstruction}. $\\hat{x}_{i}^{1}=\\int x_{i}p_{0}(x_{i})dx_{i},\\nu_{i}^{1}=\\int|x_{i}-\\hat{x}_{i}^{1}|^{2}p_{0}(x_{i})dx_{i},i=1,\\ldots,N$,\n$V_{a}^{0}=1,Z_{a}^{0}=y_{a},a=1,\\ldots,M.$\n\\par\\end{raggedright}\n\n1) Factor node update: For $a=1,\\ldots,M$\n\n", "index": 27, "text": "\\begin{align*}\nV_{a}^{t} & =\\sum_{i}|A_{ai}|^{2}\\nu_{i}^{t},\\\\\nZ_{a}^{t} & =\\sum_{i}A_{ai}\\hat{x}_{i}^{t}-\\frac{V_{a}^{t}}{\\Delta_{0}^{t}+V_{a}^{t-1}}\\bigl(y_{a}-Z_{a}^{t-1}\\bigr).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V_{a}^{t}\" display=\"inline\"><msubsup><mi>V</mi><mi>a</mi><mi>t</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i}|A_{ai}|^{2}\\nu_{i}^{t},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><mrow><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>A</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msubsup><mi>\u03bd</mi><mi>i</mi><mi>t</mi></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Z_{a}^{t}\" display=\"inline\"><msubsup><mi>Z</mi><mi>a</mi><mi>t</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i}A_{ai}\\hat{x}_{i}^{t}-\\frac{V_{a}^{t}}{\\Delta_{0}^{t}+V_%&#10;{a}^{t-1}}\\bigl{(}y_{a}-Z_{a}^{t-1}\\bigr{)}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>A</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mi>t</mi></msubsup></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>V</mi><mi>a</mi><mi>t</mi></msubsup><mrow><msubsup><mi mathvariant=\"normal\">\u0394</mi><mn>0</mn><mi>t</mi></msubsup><mo>+</mo><msubsup><mi>V</mi><mi>a</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>y</mi><mi>a</mi></msub><mo>-</mo><msubsup><mi>Z</mi><mi>a</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]