[{"file": "1601.01142.tex", "nexttext": "\nwhere $\\bm{\\theta}$ denotes all the topic mixing proportions, likewise for $\\bm{\\phi}$, $\\bm{Z}$ and $\\bm{W}$.  Given a set of documents $\\bm{W}$, we aim to infer the posterior distribution $p(\\bm{\\theta}, \\bm{\\phi}, \\bm{Z} | \\bm{W})$, which is generally intractable. We must resort to either variational approximation or Monte Carlo simulation methods. \n\\fi\n\nLet $D, K, V$ be the number of documents, topic and unique words respectively. $\\vec{\\phi}_k$ is a $V$ dimensional categorical distribution over words with symmetric Dirichlet prior $\\beta$, $\\vec{\\theta}_d$ is the topic proportion for document $d$ with Dirichlet prior $\\alpha$. The generative process of LDA is as follows\n\n", "itemtype": "equation", "pos": 6444, "prevtext": "\n\n\n\\maketitle\n\n\\begin{abstract}\n\\iffalse\nWe present the Streaming Gibbs Sampling (SGS) method to learn LDA model in an online manner.\nThe SGS can be viewed as an online extension of the classical Collapsed Gibbs Sampling.\nWe demonstrate the advantages of our algorithm by comparing it to Streaming Variational Bayes (SVB).\nIn a real-world scenario, SGS is much more accurate than SVB. Further, we have shown that the sparse and distributed implementation of SGS is more scalable than SVB.\n\\fi\n\nStreaming variational Bayes (SVB) is successful in learning LDA models in an online manner. However previous attempts toward developing online Monte-Carlo methods for LDA have little success, often by having much worse perplexity than their batch counterparts. We present a streaming Gibbs sampling (SGS) method, an online extension of the collapsed Gibbs sampling (CGS). Our empirical study shows that SGS can reach similar perplexity as CGS, much better than SVB. Our distributed version of SGS, DSGS, is much more scalable than SVB mainly because the updates' communication complexity is small.\n\\end{abstract}\n\n\n\\section{Introduction}\n\\label{intro}\nTopic models such as Latent Dirichlet Allocation (LDA) ~\\cite{blei2003latent} have gained increasing attention. LDA provides interpretable low dimensional representation of documents, uncovering the latent topics of the corpus. The model has been proven to be useful in many fields, such as natural language processing, information retrieval and recommendation systems~\\cite{mitchell2008vector, naveed2011bad}. Companies such as Google~\\cite{liu2011plda+}, Yahoo!~\\cite{ahmed2012scalable}, and Tencent~\\cite{wang2014peacock} have taken advantage of the model extensively. LDA has gradually become a standard tool to analyze documents in a semantic perspective.\n\nWith the Internet generating a huge amount of data each day, accommodating new data requires periodic retraining using traditional batch inference algorithms such as variational Bayes (VB) ~\\cite{blei2003latent}, collapsed Gibbs sampling (CGS) ~\\cite{griffiths2004finding} or their variants ~\\cite{hoffman2013stochastic, foulds2013stochastic, patterson2013stochastic}, which might be a waste of both computational and storage resources, due to the need of recomputing and storing all historical data. It is better to learn the model \\emph{incrementally}, with online (streaming) algorithms. An online algorithm is defined as the method that learns the model in a single pass of the data and can analyze a test document at any time during learning. \n\nStochastic variational inference (SVI) is an offline stochastic learning algorithm that has enjoyed great experimental success on LDA~\\cite{broderick2013streaming}. Although online methods often perform worse than offline ones, streaming variational Bayes (SVB) \\cite{broderick2013streaming} achieves the same performance as the offline SVI, which is impressive. However, these variational methods need to make unwarranted mean-field assumptions and require model-specific derivations. In contrast, Monte Carlo methods (e.g., CGS) are generally applicable and asymptotically converge to the target posterior. By exploring the sparsity structure, CGS has been adopted in many scalable algorithms for LDA~\\cite{yuan2014lightlda,ahmed2012scalable}.\nHowever, the attempts towards developing streaming Monte Carlo methods for LDA have little success, often achieving worse performance than CGS. \n\nFor instance, the perplexity of OLDA~\\cite{alsumait2008line} on NIPS dataset is much worse than CGS; and the particle filter approach \\cite{canini2009online} could  only achieve 50\\% performance of CGS, in terms of the normalized mutual information on labeled corpus. \n\n\n\n\n\n\n\n\nIn this paper, we fill up this gap by presenting a streaming Gibbs sampling (SGS) algorithm. SGS naturally extends the collapsed Gibbs sampling to the online learning setting. We empirically verify that, with weight decay, SGS can reach similar inference quality as CGS. \n\nWe further present a distributed version of SGS (DSGS) in order to deal with large-scale datasets on multiple compute nodes. Empirical results demonstrate that the distributed SGS achieves comparable inference quality as the non-distributed one, while with dramatic scaling-up. \n\nSince DSGS can be implemented with sparse data structures, demanding much lower communication bandwidth, it is more scalable than SVB. Note that the SGS without weight decay is the same as OLDA without ``topic mixing\", where ``topic mixing\" is the key component of OLDA \\cite{alsumait2008line}. Moreover, OLDA attempts to solve a different generative model, instead of the usual LDA model. \n\n\n\nThe rest of the paper is structured as follows. Section 2 presents the streaming Gibbs sampling (SGS) algorithm for LDA and shows its relationship with Conditional Density Filtering (CDF) \\cite{guhaniyogi2014bayesian}.\nWe also propose the Distributed SGS (DSGS) which can take advantage of sparsity in sampling to handle very large datasets.\nSection 3 presents experimental settings of SGS and DSGS to demonstrate their inference quality and speed. Section 4 concludes with discussion on future work. \n\n\\section{Streaming Gibbs Sampling for LDA} \n\\subsection{Basics of LDA} \nLDA\\cite{blei2003latent} is a hierarchical Bayesian model that describes a generative process of topic proportions for a document and topic assignments for every position in documents, words in the documents are sampled from distributions specified by topic assignments.\n\n\\iffalse\nFirst, we have $K$ topics that are shared by all documents, each topic $\\vec{\\phi}_k$ \\footnote{With a little abuse of notation, we will also use the parameter as a distribution of categorical outcomes.} is a categorical distribution over the $V$ words in a given vocabulary. For the simplicity of posterior inference, the topics are assumed to follow a symmetric Dirichlet prior $Dir(\\beta)$, which is conjugate to the multinomial word likelihood model. Then, for each document $d$, a topic mixing proportion vector $\\vec{\\theta}_d$ is drawn from a symmetric Dirichlet prior $Dir(\\alpha)$, again for the simplicity of inference. The last step is to generative words. Assume each document has $N_d$ tokens. The $i$-th token has a hidden topic $z_{di}$ that is drawn from $\\vec{\\theta}_d$ and the corresponding word $w_{di}$ is drawn from $\\vec{\\phi}_{z_{di}}$. \nThe generative process defines a joint distribution \n", "index": 1, "text": "$$p(\\bm{\\theta}, \\bm{Z}, \\bm{\\phi}, \\bm{W}) = \\prod_{k=1}^K p(\\vec{\\phi}_k | \\beta) \\prod_{d=1}^D p(\\vec{\\theta}_d | \\alpha) \\prod_{n=1}^{N_d} p(z_{dn} | \\vec{\\theta}_d) p(w_{dn} | \\vec{\\phi}_{z_{dn}}),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"p(\\bm{\\theta},\\bm{Z},\\bm{\\phi},\\bm{W})=\\prod_{k=1}^{K}p(\\vec{\\phi}_{k}|\\beta)%&#10;\\prod_{d=1}^{D}p(\\vec{\\theta}_{d}|\\alpha)\\prod_{n=1}^{N_{d}}p(z_{dn}|\\vec{%&#10;\\theta}_{d})p(w_{dn}|\\vec{\\phi}_{z_{dn}}),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf3d</mi><mo>,</mo><mi>\ud835\udc81</mi><mo>,</mo><mi mathvariant=\"bold-italic\">\u03d5</mi><mo>,</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>d</mi></msub></munderover><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">\u2192</mo></mover><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhere $L_d$ is the length of document $d$, $i \\in \\{1, \\dots, L_d\\}$, $z_{di}$, $w_{di}$ is the assignment and word on position $i$ of document $d$. $Dir(\\cdot)$, $Mult(\\cdot)$ is the Dirichlet distribution and Multinomial distribution. Denote $\\bm{\\theta}=(\\vec{\\theta}_1, \\dots, \\vec{\\theta}_D)$, the matrix formed by all topic proportions, likewise for $\\bm{\\Phi}, \\bm{Z}, \\bm{W}$.\n\nGiven a set of documents $\\bm{W}$, inferring the exact posterior distribution $p(\\bm{\\theta}, \\bm{\\Phi}, \\bm{Z} | \\bm{W})$ is intractable. We must resort to either variational approximation or Monte Carlo simulation methods. Among the various algorithms, collapsed Gibbs sampling (CGS)~\\cite{griffiths2004finding} is of particular interest due to its simplicity and sparsity. CGS exploits the conjugacy properties of Dirichlet and Multinomial to integrate out $(\\bm{\\theta}, \\bm{\\Phi})$ from the posterior: \n\\vspace{-1em}\n\n", "itemtype": "equation", "pos": 7333, "prevtext": "\nwhere $\\bm{\\theta}$ denotes all the topic mixing proportions, likewise for $\\bm{\\phi}$, $\\bm{Z}$ and $\\bm{W}$.  Given a set of documents $\\bm{W}$, we aim to infer the posterior distribution $p(\\bm{\\theta}, \\bm{\\phi}, \\bm{Z} | \\bm{W})$, which is generally intractable. We must resort to either variational approximation or Monte Carlo simulation methods. \n\\fi\n\nLet $D, K, V$ be the number of documents, topic and unique words respectively. $\\vec{\\phi}_k$ is a $V$ dimensional categorical distribution over words with symmetric Dirichlet prior $\\beta$, $\\vec{\\theta}_d$ is the topic proportion for document $d$ with Dirichlet prior $\\alpha$. The generative process of LDA is as follows\n\n", "index": 3, "text": "\\begin{align*}\n\\vec{\\phi}_k \\sim Dir(\\beta), \\forall d \\in \\{1, \\dots, D\\}:  \\vec{\\theta}_d \\sim Dir(\\alpha), z_{di} \\sim Mult(\\vec{\\theta}_d), w_{di} \\sim Mult(\\vec{\\phi}_{z_{di}}),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\vec{\\phi}_{k}\\sim Dir(\\beta),\\forall d\\in\\{1,\\dots,D\\}:\\vec{%&#10;\\theta}_{d}\\sim Dir(\\alpha),z_{di}\\sim Mult(\\vec{\\theta}_{d}),w_{di}\\sim Mult(%&#10;\\vec{\\phi}_{z_{di}}),\" display=\"inline\"><mrow><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>k</mi></msub><mo>\u223c</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mo>\u2200</mo><mi>d</mi></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>D</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo>\u223c</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u223c</mo><mrow><mi>M</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03b8</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><msub><mi>w</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u223c</mo><mrow><mi>M</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">\u2192</mo></mover><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhere $N_{kd}, N_{kv}$ are sufficient statistics (counts) for the Dirichlet-Multinomial distribution: $N_{kd}=\\sum_{i=1}^{L_d} \\mathbb{I}(z_{di}=k), N_{kv}=\\sum_d\\sum_{i=1}^{L_d} \\mathbb{I}(w_{di}=v, z_{di}=k), N_k=\\sum_d N_{kd}=\\sum_v N_{kv}$. The superscript $^{-di}$ stands for excluding the token at position $i$ of document $d$. $\\bm{N}_{kv}, \\bm{N}_{kd}, \\bm{N}_k$ are the matrices or vector formed by all corresponding counts. A pseudocode of CGS is depicted as Alg.~\\ref{alg:CGS}. The sparsity of sufficient statistic matrices $\\bm{N}_{kw}, \\bm{N}_{kd}$ leads to many fast sparsity aware algorithms \\cite{yao2009efficient, li2014reducing, yuan2014lightlda}. The sparsity is also an important factor that makes our algorithm more scalable than SVB.\n\\vspace{-1em}\n\n\\begin{figure}[H]\n\\begin{minipage}[t]{0.5\\columnwidth}\n\\begin{algorithm}[H]\n   \\caption{Collapsed Gibbs Sampling}\n   \\label{alg:CGS}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Input:} data $\\bm{W}$, iterations $N$\n   \\STATE Initialize $\\bm{Z}, N_{kv}, N_{k}, N_{dk}$\n\t\n   \\FOR{$iter=1$ {\\bfseries to} $N$}\n      \\FOR{each token $z_{di}$ in the documents}\n\t  \t\t\t\\STATE Sample $z_{di}\\sim p(z_{di}|\\bm{Z}^{-di},\\bm{W})$\n  \t\t\t\t\\STATE Update $N_{kv}, N_k, N_{dk}$\n  \t\t\\ENDFOR\n   \\ENDFOR\n   \\STATE {\\bfseries Output posterior mean:} \\\\\n   \t\t$\\phi_{kv}=\\frac{N_{kv}+\\beta}{N_{k}+V\\beta}$,\n\t\t$\\theta_{dk}=\\frac{N_{dk}+\\alpha}{N_{d}+K\\alpha}$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[t]{0.5\\columnwidth}\n\\begin{algorithm}[H]\n   \\caption{Streaming Gibbs Sampling} \n   \\label{alg:SGS}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Input:} iterations $N$, decay factor $\\lambda$\n   \\FOR{$t=1$ {\\bfseries to} $\\infty$}   \t\t\n\t   \\STATE {\\bfseries Input:} data $\\bm{W}^t$  \n\t   \\STATE Initialize $\\bm{Z}^t$ and update $N_{kv}^t, N_k^t, N_{dk}^t$ \n\t   \\FOR{$iter=1$ {\\bfseries to} $N$}\n\t        \\FOR{each token $z_{di}$ in the mini-batch}\n\t            \\STATE Sample $z_{di}\\sim p(z_{di}|\\bm{Z}_{-di}^{1:t},\\bm{W}^{1:t})$\n\t            \\STATE Update $N_{kv}^{t}, N_k^{t}, N_{dk}^t$\n  \t\t\t \\ENDFOR\n\t   \\ENDFOR\n\t   \\STATE Decay: $\\bm{N_{kv}^{t}}=\\lambda \\bm{N_{kv}^{t}}$\n\t   \\STATE {\\bfseries Output posterior mean:} \\\\\n \t   \t\t$\\phi_{kv}^t=\\frac{N_{kv}^t+\\beta}{N_{k}^t+V\\beta}$,\n\t\t\t$\\theta_{dk}^t=\\frac{N_{dk}^t+\\alpha}{N_{d}^t+K\\alpha}$  \t\t   \t\t\n   \\ENDFOR   \n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\end{figure}\n\n\n\n\\subsection{Streaming Gibbs Sampling}\n\n\nGiven a Bayesian model $P(x|\\bm{\\Theta})$ with prior $P(\\bm{\\Theta})$, and incoming data mini-batches $\\bm{X}^1, \\bm{X}^2, \\cdots, \\bm{X}^t, \\cdots$, let $\\bm{X}^{1:t}=\\{\\bm{X}^1,\\dots, \\bm{X}^t\\}$. \nBayesian streaming learning is the process of getting a series of posterior distributions $P(\\bm{\\Theta}|\\bm{X}^{1:t})$ by the recurrence relation:\n\n", "itemtype": "equation", "pos": 8437, "prevtext": "\nwhere $L_d$ is the length of document $d$, $i \\in \\{1, \\dots, L_d\\}$, $z_{di}$, $w_{di}$ is the assignment and word on position $i$ of document $d$. $Dir(\\cdot)$, $Mult(\\cdot)$ is the Dirichlet distribution and Multinomial distribution. Denote $\\bm{\\theta}=(\\vec{\\theta}_1, \\dots, \\vec{\\theta}_D)$, the matrix formed by all topic proportions, likewise for $\\bm{\\Phi}, \\bm{Z}, \\bm{W}$.\n\nGiven a set of documents $\\bm{W}$, inferring the exact posterior distribution $p(\\bm{\\theta}, \\bm{\\Phi}, \\bm{Z} | \\bm{W})$ is intractable. We must resort to either variational approximation or Monte Carlo simulation methods. Among the various algorithms, collapsed Gibbs sampling (CGS)~\\cite{griffiths2004finding} is of particular interest due to its simplicity and sparsity. CGS exploits the conjugacy properties of Dirichlet and Multinomial to integrate out $(\\bm{\\theta}, \\bm{\\Phi})$ from the posterior: \n\\vspace{-1em}\n\n", "index": 5, "text": "\\begin{equation}\np(z_{di}=k|\\bm{Z}^{-di},\\bm{W})\\propto(N_{kd}^{-di}+\\alpha) \\frac{N_{kv_{di}}^{-di}+\\beta}{N_k^{-di}+V\\beta},\n\\label{eq:CGS_cond}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"p(z_{di}=k|\\bm{Z}^{-di},\\bm{W})\\propto(N_{kd}^{-di}+\\alpha)\\frac{N_{kv_{di}}^{%&#10;-di}+\\beta}{N_{k}^{-di}+V\\beta},\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>k</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc81</mi><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msup><mo>,</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>N</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>d</mi></mrow><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msubsup><mo>+</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mfrac><mrow><msubsup><mi>N</mi><mrow><mi>k</mi><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msubsup><mo>+</mo><mi>\u03b2</mi></mrow><mrow><msubsup><mi>N</mi><mi>k</mi><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msubsup><mo>+</mo><mrow><mi>V</mi><mo>\u2062</mo><mi>\u03b2</mi></mrow></mrow></mfrac><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nTherefore, the posterior learnt from $\\bm{X}^{1:t-1}$ is used as the prior when learning from $\\bm{X}^t$. Note that the amount of data in a stream might be infinite, so a streaming learning algorithm can neither store all previous data, nor update the model at time $t$ with a time complexity even linear of $t$. Ideally, the algorithm should only have constant storage and constant update complexity at each time step.\n\n\n\nAs depicted in Alg.~\\ref{alg:SGS}, we propose a streaming Gibbs sampling (SGS) algorithm for LDA. SGS is an online extension of CGS, which fixes the topics $\\bm{Z}^{1:t-1}$ of the previous arrived document mini-batch, and then samples $\\bm{Z}^t$ of the current mini-batch using the normal CGS update. This is in contrast with CGS, which can come back and refine some $\\bm{Z}^{t}$ after it is first sampled. Actually $\\bm{Z}^{1:t-1}$ is not even stored in CGS, we store only the sufficient statistic $\\bm{N}_{kv}$. \n\nOne can understand SGS using the recurrence relation (\\ref{eqn:online}): without any data, the initial $\\vec{\\phi}_k$ have parameters $\\vec{\\beta}_k^0 = \\beta$, after incorporating mini-batch $\\bm{W}^1$, the parameters is updated to $\\vec{\\beta}^1_k = \\vec{\\beta}^0_k + \\bm{N_{kv}^1}[k]$, which is used as the prior of consequent mini-batches, where $\\bm{N_{kv}^t}[k]$ is the $k$-th row of matrix $\\bm{N_{kv}^t}$. In general, SGS updates the prior using the recurrence relation $\\vec{\\beta}^t_k = \\vec{\\beta}^{t-1}_k + \\bm{N_{kv}^t}[k]$, where $\\vec{\\beta}^t_k$ is the prior for $\\vec{\\phi}_k$ at time $t$. \n\nThe decay factor $\\lambda$ serves to forget the history. When plugging in the decay factor, the update equation becomes $\\vec{\\beta}^t_k = \\lambda (\\vec{\\beta}^{t-1}_k+\\bm{N_{kv}^t}[k])$. \n$\\lambda$ can then be understood as weakening the posterior caused by the previous data. This decay factor would improve the performance of SGS, especially when the topic-word distribution is evolving along the time.\n\n\\iffalse\nThe decay factor $\\lambda$ serves to forget the history. SGS only updates the sufficient statistics $\\bm{N}_{kv}$, which is shared among documents. Without the decay factor $\\lambda$, SGS actually updates the distribution of $\\vec{\\phi}_k$ from the prior at time $t$ (or the posterior at time $t-1$): $Dir(\\vec{\\beta}_{t-1})$ to the posterior $Dir(\\vec{\\beta}_t\\equiv\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k])$, where $\\bm{N_{kv}^t}[k]$ is the $k$-th row of matrix $\\bm{N_{kv}^t}$. In short, the learning procedure is just updating the parameter of $\\vec{\\phi}_k$'s Dirichlet Distribution: $\\vec{\\beta}_t\\equiv\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k]$. As a property of the exponential family distributions that $\\beta$ can be explained as pseudo-observations, so the update equation can  also be explained as: getting the posterior at time $t$ by adding more real-observations ($\\bm{N_{kv}^t}$) to the posterior at time $t-1$. When plugging in the decay factor, the update equation becomes $\\vec{\\beta}_t\\equiv \\lambda (\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k])$. \n$\\lambda$ can then be understood as weakening the posterior caused by the previous data. This decay factor would improve the performance of SGS, especially when the topic-word distribution is evolving along the time.\n\\fi\n\n\nSGS only requires constant memory: it only stores the current mini-batch $\\bm{W}^t, \\bm{Z}^t$, but not $\\bm{W}^{1:t-1}, \\bm{Z}^{1:t-1}, \\bm{W}^{t+1:\\infty}, \\bm{Z}^{t+1:\\infty}$. The total time complexity is the same as CGS, which is $O(KN|\\bm{W}^{1:t}|)$, where $|\\bm{W}^{1:t}|$ is number of tokens in mini-batch $1$ to $t$. In practice, we use a smaller number of iterations than that of CGS, because SGS iterates over a smaller number of documents (mini-batch) and thus it converges faster. \n\n\n\\subsection{Relation to Conditional Density Filtering}\n\n\n\nIn this section, we consider a special case of SGS, where the decay factor $\\lambda$ is set to 1.0. We relate this special case to Conditional Density Filtering (CDF) \\cite{guhaniyogi2014bayesian} and show that SGS can be seen as an improved version of CDF framework when applied to the LDA model.\n\n\\subsubsection{Conditional Density Filtering}\nCDF is an algorithm that can sample from a sequence of gradually evolving distributions. Given a probabilistic model $P(\\bm{D^{1:t}}|\\bm{\\Theta})$, where $\\bm{\\Theta}=(\\theta_1,\\cdots, \\theta_k)$ is a $k$-dimensional parameter vector and $\\bm{D^{1:t}}$ is the data until now, we define the Surrogate Conditional Sufficient Statistics (SCSS) as follows:\\\n\\begin{mydef}\n\\label{def:SCSS}\n[SCSS] Assume $p(\\theta_j|\\theta_{-j},D_t)$ can be written as $p(\\theta_j|\\theta_{-j,1},h(D_t, \\theta_{-j,2}))$, where $\\theta_{-j}=\\bm{\\Theta} \\backslash \\theta_j$, $\\theta_{-j,1}$ and $\\theta_{-j,2}$ are a partition of $\\theta_{-j}$ and $h$ is some known function. If $\\hat{\\theta}_{-j,2}^t$ is a consistent estimator of $\\theta_{-j,2}$ at time t, then $C^t=g(C^{t-1}, h(D_t, \\hat{\\theta}_{-j,2}^t))$ is defined as the SCSS of $\\theta_j$ at time $t$ , for some known function $g$. We use $p(\\theta_j|\\theta_{-j,1}, C^t)$ to approximate $p(\\theta_j|\\theta_{-j},D^{1:t})$.\n\\end{mydef}\n\n\nSCSS is an extension of Sufficient Statistics (SS), in the sense that both of them summarize the historical observations sampled from a class of distributions. SS is accurate and summarizes a whole distribution, but SCSS is approximate and only summarizes conditional distributions. \n\nIf the parameter set of a probabilistic model can be partitioned into two sets $I_1$ and $I_2$, where each parameter's SCSS only depends on the parameters in the other set, then we can use the CDF algorithm (\\ref{alg:CDF}) to infer the posterior of the parameters.\n\n\n\\begin{minipage}[t]{0.5\\columnwidth} \\vskip -0.7cm\n    \\begin{algorithm}[H]\n       \\caption{Conditional Density Filtering}\n       \\label{alg:CDF}\n    \\begin{algorithmic}\n       \\FOR{$t=1$ {\\bfseries to} $\\infty$}\n       \t\t\\FOR{$s \\in \\{1,2\\}$}\n       \t\t\t\\FOR{$j \\in I_s$}\n       \t\t\t\t\\STATE $C_{js}^t=g(C_{js}^{t-1}, h(D_t, \\hat{\\bm{\\Theta}}_{-s}))$\n       \t\t\t\t\\STATE Sample $\\theta_j \\sim p(\\theta_j | \\theta_{-js}, C_{js}^t)$\n       \t\t\t\\ENDFOR\n       \t\t\\ENDFOR\n       \\ENDFOR\n    \\end{algorithmic}\n    \\end{algorithm}\n    \\vskip -0.2in\n\n\n    \\begin{algorithm}[H]\n       \\caption{Distributed SGS (DSGS)}\n       \\label{alg:DSGS}\n    \\begin{algorithmic}\n       \\STATE {\\bfseries Input:} iterations $N$, decay factor $\\lambda$\n       \\STATE Initialize $\\bm{N}_{kv}=0$\n       \\FOR{each mini-batch $\\bm{W}^t$ at some worker}\n       \t\t\\STATE Copy global $\\bm{N}_{kv}$ to local $\\bm{N}_{kv}^{local}$\n       \t\t\\STATE $\\Delta \\bm{N}_{kv}^{local}=CGS(\\alpha, \\beta+\\bm{N}_{kv}^{local}, \\bm{W}^t)$\n       \t\t\\STATE Update global $\\bm{N}_{kv}=\\lambda (\\bm{N}_{kv} + \\Delta \\bm{N}_{kv}^{local})$\n       \\ENDFOR   \n    \\end{algorithmic}\n    \\end{algorithm}\n\\end{minipage}\n\\hskip 0.1in\n\\begin{minipage}[t]{0.5\\columnwidth} \\vskip -0.7cm \n\\begin{algorithm}[H]\n   \\caption{CDF-LDA}\n   \\label{alg:CDF-LDA}\n\\begin{algorithmic}\n   \\STATE Initialize: $\\bm{\\Phi}=\\mbox{Uniform}(0, 1), \\hat{\\bm{N}}_{kv}^0=0$\n   \\FOR{$t=1$ {\\bfseries to} $\\infty$}\n   \t\t   \\STATE {\\bfseries Input:} a single document $\\vec{w}_{t }$\n   \t\t   \\STATE Initialize $\\vec{z}_{t }$\n   \t\t   \n   \t\t   \\STATE SCSS of $z$: $\\hat{\\bm{\\Phi}}^t=\\bm{\\Phi}$\t\t   \n   \t\t   \\FOR{each token $i$ in doc $t$}   \n\t   \t\t   \\STATE Sample $z_{t i} \\sim p(z_{ti}|\\vec{z}_t^{-ti}, \\hat{\\bm{\\Phi}}^t)$\n   \t\t   \\ENDFOR\n   \t\t   \n           \\STATE SCSS of $\\bm{\\Phi}$:  $\\hat{N}_{kv}^t=\\hat{N}_{kv}^{t-1}+\\sum_i I(z_{ti}=k \\wedge w_{ti}=v)$\n   \t\t   \\FOR{$k=1$ {\\bfseries to} $K$}\n   \t\t   \t\t\\STATE Sample ${\\vec{\\phi}}_k \\sim Dir(\\hat{\\bm{N}}_{kv}^t+\\beta)$\n   \t\t   \\ENDFOR\n   \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\n\nUnder a semi-collapsed representation of LDA, where $\\vec{\\theta}_d$ is collapsed, we can partition the parameters into two sets: $I_1=\\{\\phi_{kv}\\}; I_2=\\{z_{di}\\}$. The conditional distributions are: \n\n", "itemtype": "equation", "pos": 11383, "prevtext": "\nwhere $N_{kd}, N_{kv}$ are sufficient statistics (counts) for the Dirichlet-Multinomial distribution: $N_{kd}=\\sum_{i=1}^{L_d} \\mathbb{I}(z_{di}=k), N_{kv}=\\sum_d\\sum_{i=1}^{L_d} \\mathbb{I}(w_{di}=v, z_{di}=k), N_k=\\sum_d N_{kd}=\\sum_v N_{kv}$. The superscript $^{-di}$ stands for excluding the token at position $i$ of document $d$. $\\bm{N}_{kv}, \\bm{N}_{kd}, \\bm{N}_k$ are the matrices or vector formed by all corresponding counts. A pseudocode of CGS is depicted as Alg.~\\ref{alg:CGS}. The sparsity of sufficient statistic matrices $\\bm{N}_{kw}, \\bm{N}_{kd}$ leads to many fast sparsity aware algorithms \\cite{yao2009efficient, li2014reducing, yuan2014lightlda}. The sparsity is also an important factor that makes our algorithm more scalable than SVB.\n\\vspace{-1em}\n\n\\begin{figure}[H]\n\\begin{minipage}[t]{0.5\\columnwidth}\n\\begin{algorithm}[H]\n   \\caption{Collapsed Gibbs Sampling}\n   \\label{alg:CGS}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Input:} data $\\bm{W}$, iterations $N$\n   \\STATE Initialize $\\bm{Z}, N_{kv}, N_{k}, N_{dk}$\n\t\n   \\FOR{$iter=1$ {\\bfseries to} $N$}\n      \\FOR{each token $z_{di}$ in the documents}\n\t  \t\t\t\\STATE Sample $z_{di}\\sim p(z_{di}|\\bm{Z}^{-di},\\bm{W})$\n  \t\t\t\t\\STATE Update $N_{kv}, N_k, N_{dk}$\n  \t\t\\ENDFOR\n   \\ENDFOR\n   \\STATE {\\bfseries Output posterior mean:} \\\\\n   \t\t$\\phi_{kv}=\\frac{N_{kv}+\\beta}{N_{k}+V\\beta}$,\n\t\t$\\theta_{dk}=\\frac{N_{dk}+\\alpha}{N_{d}+K\\alpha}$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[t]{0.5\\columnwidth}\n\\begin{algorithm}[H]\n   \\caption{Streaming Gibbs Sampling} \n   \\label{alg:SGS}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Input:} iterations $N$, decay factor $\\lambda$\n   \\FOR{$t=1$ {\\bfseries to} $\\infty$}   \t\t\n\t   \\STATE {\\bfseries Input:} data $\\bm{W}^t$  \n\t   \\STATE Initialize $\\bm{Z}^t$ and update $N_{kv}^t, N_k^t, N_{dk}^t$ \n\t   \\FOR{$iter=1$ {\\bfseries to} $N$}\n\t        \\FOR{each token $z_{di}$ in the mini-batch}\n\t            \\STATE Sample $z_{di}\\sim p(z_{di}|\\bm{Z}_{-di}^{1:t},\\bm{W}^{1:t})$\n\t            \\STATE Update $N_{kv}^{t}, N_k^{t}, N_{dk}^t$\n  \t\t\t \\ENDFOR\n\t   \\ENDFOR\n\t   \\STATE Decay: $\\bm{N_{kv}^{t}}=\\lambda \\bm{N_{kv}^{t}}$\n\t   \\STATE {\\bfseries Output posterior mean:} \\\\\n \t   \t\t$\\phi_{kv}^t=\\frac{N_{kv}^t+\\beta}{N_{k}^t+V\\beta}$,\n\t\t\t$\\theta_{dk}^t=\\frac{N_{dk}^t+\\alpha}{N_{d}^t+K\\alpha}$  \t\t   \t\t\n   \\ENDFOR   \n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\end{figure}\n\n\n\n\\subsection{Streaming Gibbs Sampling}\n\n\nGiven a Bayesian model $P(x|\\bm{\\Theta})$ with prior $P(\\bm{\\Theta})$, and incoming data mini-batches $\\bm{X}^1, \\bm{X}^2, \\cdots, \\bm{X}^t, \\cdots$, let $\\bm{X}^{1:t}=\\{\\bm{X}^1,\\dots, \\bm{X}^t\\}$. \nBayesian streaming learning is the process of getting a series of posterior distributions $P(\\bm{\\Theta}|\\bm{X}^{1:t})$ by the recurrence relation:\n\n", "index": 7, "text": "\\begin{align}\nP(\\bm{\\Theta}|\\bm{X}^{1:t})\\propto P(\\bm{\\Theta}|\\bm{X}^{1:t-1})P(\\bm{X}^t|\\bm{\\Theta}).\\label{eqn:online}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P(\\bm{\\Theta}|\\bm{X}^{1:t})\\propto P(\\bm{\\Theta}|\\bm{X}^{1:t-1})%&#10;P(\\bm{X}^{t}|\\bm{\\Theta}).\" display=\"inline\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc7f</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc7f</mi><mrow><mn>1</mn><mo>:</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc7f</mi><mi>t</mi></msup><mo stretchy=\"false\">|</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": " \n\n\nBy definition (\\ref{def:SCSS}), we can verify that the SCSS of $\\bm{\\Phi}$ and $\\vec{z}_d$ at time $t$ are $\\bm{N}_{kv}$ and $\\bm{\\Phi}$ respectively. Thus we have the CDF solution of LDA as shown in Algorithm (\\ref{alg:CDF-LDA}).\n\n\\iffalse\nThe update formula of SCSS of $\\bm{\\Phi}$ is: \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nTherefore, the posterior learnt from $\\bm{X}^{1:t-1}$ is used as the prior when learning from $\\bm{X}^t$. Note that the amount of data in a stream might be infinite, so a streaming learning algorithm can neither store all previous data, nor update the model at time $t$ with a time complexity even linear of $t$. Ideally, the algorithm should only have constant storage and constant update complexity at each time step.\n\n\n\nAs depicted in Alg.~\\ref{alg:SGS}, we propose a streaming Gibbs sampling (SGS) algorithm for LDA. SGS is an online extension of CGS, which fixes the topics $\\bm{Z}^{1:t-1}$ of the previous arrived document mini-batch, and then samples $\\bm{Z}^t$ of the current mini-batch using the normal CGS update. This is in contrast with CGS, which can come back and refine some $\\bm{Z}^{t}$ after it is first sampled. Actually $\\bm{Z}^{1:t-1}$ is not even stored in CGS, we store only the sufficient statistic $\\bm{N}_{kv}$. \n\nOne can understand SGS using the recurrence relation (\\ref{eqn:online}): without any data, the initial $\\vec{\\phi}_k$ have parameters $\\vec{\\beta}_k^0 = \\beta$, after incorporating mini-batch $\\bm{W}^1$, the parameters is updated to $\\vec{\\beta}^1_k = \\vec{\\beta}^0_k + \\bm{N_{kv}^1}[k]$, which is used as the prior of consequent mini-batches, where $\\bm{N_{kv}^t}[k]$ is the $k$-th row of matrix $\\bm{N_{kv}^t}$. In general, SGS updates the prior using the recurrence relation $\\vec{\\beta}^t_k = \\vec{\\beta}^{t-1}_k + \\bm{N_{kv}^t}[k]$, where $\\vec{\\beta}^t_k$ is the prior for $\\vec{\\phi}_k$ at time $t$. \n\nThe decay factor $\\lambda$ serves to forget the history. When plugging in the decay factor, the update equation becomes $\\vec{\\beta}^t_k = \\lambda (\\vec{\\beta}^{t-1}_k+\\bm{N_{kv}^t}[k])$. \n$\\lambda$ can then be understood as weakening the posterior caused by the previous data. This decay factor would improve the performance of SGS, especially when the topic-word distribution is evolving along the time.\n\n\\iffalse\nThe decay factor $\\lambda$ serves to forget the history. SGS only updates the sufficient statistics $\\bm{N}_{kv}$, which is shared among documents. Without the decay factor $\\lambda$, SGS actually updates the distribution of $\\vec{\\phi}_k$ from the prior at time $t$ (or the posterior at time $t-1$): $Dir(\\vec{\\beta}_{t-1})$ to the posterior $Dir(\\vec{\\beta}_t\\equiv\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k])$, where $\\bm{N_{kv}^t}[k]$ is the $k$-th row of matrix $\\bm{N_{kv}^t}$. In short, the learning procedure is just updating the parameter of $\\vec{\\phi}_k$'s Dirichlet Distribution: $\\vec{\\beta}_t\\equiv\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k]$. As a property of the exponential family distributions that $\\beta$ can be explained as pseudo-observations, so the update equation can  also be explained as: getting the posterior at time $t$ by adding more real-observations ($\\bm{N_{kv}^t}$) to the posterior at time $t-1$. When plugging in the decay factor, the update equation becomes $\\vec{\\beta}_t\\equiv \\lambda (\\vec{\\beta}_{t-1}+\\bm{N_{kv}^t}[k])$. \n$\\lambda$ can then be understood as weakening the posterior caused by the previous data. This decay factor would improve the performance of SGS, especially when the topic-word distribution is evolving along the time.\n\\fi\n\n\nSGS only requires constant memory: it only stores the current mini-batch $\\bm{W}^t, \\bm{Z}^t$, but not $\\bm{W}^{1:t-1}, \\bm{Z}^{1:t-1}, \\bm{W}^{t+1:\\infty}, \\bm{Z}^{t+1:\\infty}$. The total time complexity is the same as CGS, which is $O(KN|\\bm{W}^{1:t}|)$, where $|\\bm{W}^{1:t}|$ is number of tokens in mini-batch $1$ to $t$. In practice, we use a smaller number of iterations than that of CGS, because SGS iterates over a smaller number of documents (mini-batch) and thus it converges faster. \n\n\n\\subsection{Relation to Conditional Density Filtering}\n\n\n\nIn this section, we consider a special case of SGS, where the decay factor $\\lambda$ is set to 1.0. We relate this special case to Conditional Density Filtering (CDF) \\cite{guhaniyogi2014bayesian} and show that SGS can be seen as an improved version of CDF framework when applied to the LDA model.\n\n\\subsubsection{Conditional Density Filtering}\nCDF is an algorithm that can sample from a sequence of gradually evolving distributions. Given a probabilistic model $P(\\bm{D^{1:t}}|\\bm{\\Theta})$, where $\\bm{\\Theta}=(\\theta_1,\\cdots, \\theta_k)$ is a $k$-dimensional parameter vector and $\\bm{D^{1:t}}$ is the data until now, we define the Surrogate Conditional Sufficient Statistics (SCSS) as follows:\\\n\\begin{mydef}\n\\label{def:SCSS}\n[SCSS] Assume $p(\\theta_j|\\theta_{-j},D_t)$ can be written as $p(\\theta_j|\\theta_{-j,1},h(D_t, \\theta_{-j,2}))$, where $\\theta_{-j}=\\bm{\\Theta} \\backslash \\theta_j$, $\\theta_{-j,1}$ and $\\theta_{-j,2}$ are a partition of $\\theta_{-j}$ and $h$ is some known function. If $\\hat{\\theta}_{-j,2}^t$ is a consistent estimator of $\\theta_{-j,2}$ at time t, then $C^t=g(C^{t-1}, h(D_t, \\hat{\\theta}_{-j,2}^t))$ is defined as the SCSS of $\\theta_j$ at time $t$ , for some known function $g$. We use $p(\\theta_j|\\theta_{-j,1}, C^t)$ to approximate $p(\\theta_j|\\theta_{-j},D^{1:t})$.\n\\end{mydef}\n\n\nSCSS is an extension of Sufficient Statistics (SS), in the sense that both of them summarize the historical observations sampled from a class of distributions. SS is accurate and summarizes a whole distribution, but SCSS is approximate and only summarizes conditional distributions. \n\nIf the parameter set of a probabilistic model can be partitioned into two sets $I_1$ and $I_2$, where each parameter's SCSS only depends on the parameters in the other set, then we can use the CDF algorithm (\\ref{alg:CDF}) to infer the posterior of the parameters.\n\n\n\\begin{minipage}[t]{0.5\\columnwidth} \\vskip -0.7cm\n    \\begin{algorithm}[H]\n       \\caption{Conditional Density Filtering}\n       \\label{alg:CDF}\n    \\begin{algorithmic}\n       \\FOR{$t=1$ {\\bfseries to} $\\infty$}\n       \t\t\\FOR{$s \\in \\{1,2\\}$}\n       \t\t\t\\FOR{$j \\in I_s$}\n       \t\t\t\t\\STATE $C_{js}^t=g(C_{js}^{t-1}, h(D_t, \\hat{\\bm{\\Theta}}_{-s}))$\n       \t\t\t\t\\STATE Sample $\\theta_j \\sim p(\\theta_j | \\theta_{-js}, C_{js}^t)$\n       \t\t\t\\ENDFOR\n       \t\t\\ENDFOR\n       \\ENDFOR\n    \\end{algorithmic}\n    \\end{algorithm}\n    \\vskip -0.2in\n\n\n    \\begin{algorithm}[H]\n       \\caption{Distributed SGS (DSGS)}\n       \\label{alg:DSGS}\n    \\begin{algorithmic}\n       \\STATE {\\bfseries Input:} iterations $N$, decay factor $\\lambda$\n       \\STATE Initialize $\\bm{N}_{kv}=0$\n       \\FOR{each mini-batch $\\bm{W}^t$ at some worker}\n       \t\t\\STATE Copy global $\\bm{N}_{kv}$ to local $\\bm{N}_{kv}^{local}$\n       \t\t\\STATE $\\Delta \\bm{N}_{kv}^{local}=CGS(\\alpha, \\beta+\\bm{N}_{kv}^{local}, \\bm{W}^t)$\n       \t\t\\STATE Update global $\\bm{N}_{kv}=\\lambda (\\bm{N}_{kv} + \\Delta \\bm{N}_{kv}^{local})$\n       \\ENDFOR   \n    \\end{algorithmic}\n    \\end{algorithm}\n\\end{minipage}\n\\hskip 0.1in\n\\begin{minipage}[t]{0.5\\columnwidth} \\vskip -0.7cm \n\\begin{algorithm}[H]\n   \\caption{CDF-LDA}\n   \\label{alg:CDF-LDA}\n\\begin{algorithmic}\n   \\STATE Initialize: $\\bm{\\Phi}=\\mbox{Uniform}(0, 1), \\hat{\\bm{N}}_{kv}^0=0$\n   \\FOR{$t=1$ {\\bfseries to} $\\infty$}\n   \t\t   \\STATE {\\bfseries Input:} a single document $\\vec{w}_{t }$\n   \t\t   \\STATE Initialize $\\vec{z}_{t }$\n   \t\t   \n   \t\t   \\STATE SCSS of $z$: $\\hat{\\bm{\\Phi}}^t=\\bm{\\Phi}$\t\t   \n   \t\t   \\FOR{each token $i$ in doc $t$}   \n\t   \t\t   \\STATE Sample $z_{t i} \\sim p(z_{ti}|\\vec{z}_t^{-ti}, \\hat{\\bm{\\Phi}}^t)$\n   \t\t   \\ENDFOR\n   \t\t   \n           \\STATE SCSS of $\\bm{\\Phi}$:  $\\hat{N}_{kv}^t=\\hat{N}_{kv}^{t-1}+\\sum_i I(z_{ti}=k \\wedge w_{ti}=v)$\n   \t\t   \\FOR{$k=1$ {\\bfseries to} $K$}\n   \t\t   \t\t\\STATE Sample ${\\vec{\\phi}}_k \\sim Dir(\\hat{\\bm{N}}_{kv}^t+\\beta)$\n   \t\t   \\ENDFOR\n   \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\n\nUnder a semi-collapsed representation of LDA, where $\\vec{\\theta}_d$ is collapsed, we can partition the parameters into two sets: $I_1=\\{\\phi_{kv}\\}; I_2=\\{z_{di}\\}$. The conditional distributions are: \n\n", "index": 9, "text": "$$p(\\bm{\\Phi}|\\bm{Z}, \\bm{W})=\\prod_{k=1}^K Dir(\\vec{\\phi}_k| \\bm{N}_{kv}[k]+\\beta), \\quad p(z_{di}=k| \\vec{z}_d^{-di}, \\bm{\\Phi}, \\bm{W}) \\propto (N_{kd}^{-di}+\\alpha) {\\phi}_{kv_{di}}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"p(\\bm{\\Phi}|\\bm{Z},\\bm{W})=\\prod_{k=1}^{K}Dir(\\vec{\\phi}_{k}|\\bm{N}_{kv}[k]+%&#10;\\beta),\\quad p(z_{di}=k|\\vec{z}_{d}^{-di},\\bm{\\Phi},\\bm{W})\\propto(N_{kd}^{-di%&#10;}+\\alpha){\\phi}_{kv_{di}}.\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udebd</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc81</mi><mo>,</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>D</mi><mi>i</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03d5</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>k</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow><mo>+</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow><mo rspace=\"12.5pt\">,</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>k</mi><mo stretchy=\"false\">|</mo><msubsup><mover accent=\"true\"><mi>z</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msubsup><mo>,</mo><mi>\ud835\udebd</mi><mo>,</mo><mi>\ud835\udc7e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>N</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>d</mi></mrow><mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow></msubsup><mo>+</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03d5</mi><mrow><mi>k</mi><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow></msub><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhile $\\vec{z}_{d}$'s SCSS is simply $\\bm{\\Phi}^t$. \n\\fi\n\n\\subsubsection{Relationship between SGS and CDF-LDA}\n\nOur SGS method can be viewed as an improved version of CDF-LDA in the following aspects:\n\\begin{itemize}\n\\item In CDF-LDA, SCSS $\\bm{\\Phi}^t$ is directly sampled from a Dirichlet distribution, which unnecessarily introduces extra source of randomness. Replacing the sampling with the expectation $\\hat{\\phi}_{kv}^t=\\frac{\\hat{N}_{kv}^{t-1}+\\beta} {\\hat{N}_k^{t-1}+V\\beta}$ gives better performance in practice. This corresponds to a fully collapsed representation of LDA. It's more statistical efficient due to the Rao-Blackwell Theorem. \n\\item The CDF-LDA's sampling update of $z_{ti}$ does not include other tokens in the current document $t$, which could be improved by taking the current document into account. This is especially useful for the beginning iterations, because it enables the topics' preference of doc $t$ to be propagated immediately. \n\\item It is hard to say how a single document can be decomposed into topics without looking at the other documents, but this is the case that occurrs at the beginning of CDF-LDA. This would result in inaccurate $z_{ti}$ assignments and therefore polluting $\\bm{N}_{kv}$, and finally resulting in low convergence rate on the whole. Our method avoids this problem by processing a mini-batch of documents at a time and allows for multiple iterations over the mini-batch. This would enable topic-assignments to be propagated locally. \n\\end{itemize}\n\nTo sum up, SGS without weight decay can be seen as an improvement over CDF-LDA not only by adapting a fully collapsed representation, but also by enabling a timely and repeatedly cross-document information flow.\n\n\n\n\\subsection{Distributed SGS}\nMany distributed CGS samplers exist for LDA, including divide-and-conquer~\\cite{newman2007distributed}, parameter server~\\cite{ahmed2012scalable, li2014scaling} and model parallel~\\cite{guhaniyogi2014bayesian} approaches. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. \n\nHere, we adopt the parameter server approach and present a distributed SGS sampler. \n\n\\iffalse\nMany attempts have been initiated to make LDA's inference a distributed procedure, such as AD-LDA, HD-LDA \\cite{newman2007distributed} and SVB \\cite{broderick2013streaming}. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. We claim that SGS can be well paralleled without much loss of precision by adopting a similar scheme as Asynchronous SVB does. \n\\fi\n\nSame as CGS, the global parameter in SGS is the topic word count matrix $\\bm{N}_{kv}$. SGS can be viewed as a sequence of calls to the CGS procedure\n\n", "itemtype": "equation", "pos": 19917, "prevtext": " \n\n\nBy definition (\\ref{def:SCSS}), we can verify that the SCSS of $\\bm{\\Phi}$ and $\\vec{z}_d$ at time $t$ are $\\bm{N}_{kv}$ and $\\bm{\\Phi}$ respectively. Thus we have the CDF solution of LDA as shown in Algorithm (\\ref{alg:CDF-LDA}).\n\n\\iffalse\nThe update formula of SCSS of $\\bm{\\Phi}$ is: \n\n", "index": 11, "text": "$$\\bm{N}_{kv}^t=\\bm{N}_{kv}^{t-1}+\\sum_i \\mathbb{I}(z_{ti}=k \\wedge w_{ti}=v)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\bm{N}_{kv}^{t}=\\bm{N}_{kv}^{t-1}+\\sum_{i}\\mathbb{I}(z_{ti}=k\\wedge w_{ti}=v)\" display=\"block\"><mrow><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mi>t</mi></msubsup><mo>=</mo><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mi>\ud835\udd40</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>k</mi><mo>\u2227</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhere the $k$th row of $\\beta + \\bm{N}^{t-1}_{kv}$ is the prior of $\\vec{\\phi}_k$. Then the topic word count matrix can be updated by $\\bm{N}_{kv}^t=\\lambda(\\bm{N}_{kv}^{t-1}+\\Delta \\bm{N}_{kv}^t)$.\n\nIn the parameter server architecture, we store $\\bm{N}_{kv}$ in a central server. Each worker fetches the most recent parameter $\\bm{N}_{kv}$, runs $CGS(\\alpha, \\beta +\\bm{N}_{kv}^{t-1}, \\bm{W}^t)$ to get the updates $\\Delta \\bm{N}_{kv}^t$, and pushes back the update to the server. Upon receiving an update, the server updates its parameters. In our implementation, the workers run in a fully (hogwild) asynchronous fashion. Although workers may have slightly stale parameters compared to the serial version, affecting the convergence, this hogwild approach is shown to work well in~\\cite{newman2007distributed,ahmed2012scalable} as well as our experiments, based on the fact that $\\bm{N}_{kv}$ changes slowly. Better schemes such as stale synchronous parallel~\\cite{ho2013more} might be used, but it is just a matter of choosing parameter server implementations. A pseudo-code of is given as Alg.~\\ref{alg:DSGS}.\n\n\\iffalse\nSGS can be viewed as a sequence of calls to CGS procedure: \n\n", "itemtype": "equation", "pos": 22729, "prevtext": "\nwhile $\\vec{z}_{d}$'s SCSS is simply $\\bm{\\Phi}^t$. \n\\fi\n\n\\subsubsection{Relationship between SGS and CDF-LDA}\n\nOur SGS method can be viewed as an improved version of CDF-LDA in the following aspects:\n\\begin{itemize}\n\\item In CDF-LDA, SCSS $\\bm{\\Phi}^t$ is directly sampled from a Dirichlet distribution, which unnecessarily introduces extra source of randomness. Replacing the sampling with the expectation $\\hat{\\phi}_{kv}^t=\\frac{\\hat{N}_{kv}^{t-1}+\\beta} {\\hat{N}_k^{t-1}+V\\beta}$ gives better performance in practice. This corresponds to a fully collapsed representation of LDA. It's more statistical efficient due to the Rao-Blackwell Theorem. \n\\item The CDF-LDA's sampling update of $z_{ti}$ does not include other tokens in the current document $t$, which could be improved by taking the current document into account. This is especially useful for the beginning iterations, because it enables the topics' preference of doc $t$ to be propagated immediately. \n\\item It is hard to say how a single document can be decomposed into topics without looking at the other documents, but this is the case that occurrs at the beginning of CDF-LDA. This would result in inaccurate $z_{ti}$ assignments and therefore polluting $\\bm{N}_{kv}$, and finally resulting in low convergence rate on the whole. Our method avoids this problem by processing a mini-batch of documents at a time and allows for multiple iterations over the mini-batch. This would enable topic-assignments to be propagated locally. \n\\end{itemize}\n\nTo sum up, SGS without weight decay can be seen as an improvement over CDF-LDA not only by adapting a fully collapsed representation, but also by enabling a timely and repeatedly cross-document information flow.\n\n\n\n\\subsection{Distributed SGS}\nMany distributed CGS samplers exist for LDA, including divide-and-conquer~\\cite{newman2007distributed}, parameter server~\\cite{ahmed2012scalable, li2014scaling} and model parallel~\\cite{guhaniyogi2014bayesian} approaches. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. \n\nHere, we adopt the parameter server approach and present a distributed SGS sampler. \n\n\\iffalse\nMany attempts have been initiated to make LDA's inference a distributed procedure, such as AD-LDA, HD-LDA \\cite{newman2007distributed} and SVB \\cite{broderick2013streaming}. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. We claim that SGS can be well paralleled without much loss of precision by adopting a similar scheme as Asynchronous SVB does. \n\\fi\n\nSame as CGS, the global parameter in SGS is the topic word count matrix $\\bm{N}_{kv}$. SGS can be viewed as a sequence of calls to the CGS procedure\n\n", "index": 13, "text": "$$\\Delta \\bm{N}_{kv}^t=CGS(\\alpha, \\beta +\\bm{N}_{kv}^{t-1}, \\bm{W}^t),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\Delta\\bm{N}_{kv}^{t}=CGS(\\alpha,\\beta+\\bm{N}_{kv}^{t-1},\\bm{W}^{t}),\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mi>t</mi></msubsup></mrow><mo>=</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mrow><mi>\u03b2</mi><mo>+</mo><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo>,</mo><msup><mi>\ud835\udc7e</mi><mi>t</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhere the $k$th row of $\\beta + \\bm{N}^{t-1}_{kv}$ is the prior of $\\vec{\\phi}_k$. Then the topic word count matrix can be updated by $\\bm{N}_{kv}^t=\\lambda(\\bm{N}_{kv}^{t-1}+\\Delta \\bm{N}_{kv}^t)$.\n\nIn the parameter server architecture, we store $\\bm{N}_{kv}$ in a central server. Each worker fetches the most recent parameter $\\bm{N}_{kv}$, runs $CGS(\\alpha, \\beta +\\bm{N}_{kv}^{t-1}, \\bm{W}^t)$ to get the updates $\\Delta \\bm{N}_{kv}^t$, and pushes back the update to the server. Upon receiving an update, the server updates its parameters. In our implementation, the workers run in a fully (hogwild) asynchronous fashion. Although workers may have slightly stale parameters compared to the serial version, affecting the convergence, this hogwild approach is shown to work well in~\\cite{newman2007distributed,ahmed2012scalable} as well as our experiments, based on the fact that $\\bm{N}_{kv}$ changes slowly. Better schemes such as stale synchronous parallel~\\cite{ho2013more} might be used, but it is just a matter of choosing parameter server implementations. A pseudo-code of is given as Alg.~\\ref{alg:DSGS}.\n\n\\iffalse\nSGS can be viewed as a sequence of calls to CGS procedure: \n\n", "itemtype": "equation", "pos": 22729, "prevtext": "\nwhile $\\vec{z}_{d}$'s SCSS is simply $\\bm{\\Phi}^t$. \n\\fi\n\n\\subsubsection{Relationship between SGS and CDF-LDA}\n\nOur SGS method can be viewed as an improved version of CDF-LDA in the following aspects:\n\\begin{itemize}\n\\item In CDF-LDA, SCSS $\\bm{\\Phi}^t$ is directly sampled from a Dirichlet distribution, which unnecessarily introduces extra source of randomness. Replacing the sampling with the expectation $\\hat{\\phi}_{kv}^t=\\frac{\\hat{N}_{kv}^{t-1}+\\beta} {\\hat{N}_k^{t-1}+V\\beta}$ gives better performance in practice. This corresponds to a fully collapsed representation of LDA. It's more statistical efficient due to the Rao-Blackwell Theorem. \n\\item The CDF-LDA's sampling update of $z_{ti}$ does not include other tokens in the current document $t$, which could be improved by taking the current document into account. This is especially useful for the beginning iterations, because it enables the topics' preference of doc $t$ to be propagated immediately. \n\\item It is hard to say how a single document can be decomposed into topics without looking at the other documents, but this is the case that occurrs at the beginning of CDF-LDA. This would result in inaccurate $z_{ti}$ assignments and therefore polluting $\\bm{N}_{kv}$, and finally resulting in low convergence rate on the whole. Our method avoids this problem by processing a mini-batch of documents at a time and allows for multiple iterations over the mini-batch. This would enable topic-assignments to be propagated locally. \n\\end{itemize}\n\nTo sum up, SGS without weight decay can be seen as an improvement over CDF-LDA not only by adapting a fully collapsed representation, but also by enabling a timely and repeatedly cross-document information flow.\n\n\n\n\\subsection{Distributed SGS}\nMany distributed CGS samplers exist for LDA, including divide-and-conquer~\\cite{newman2007distributed}, parameter server~\\cite{ahmed2012scalable, li2014scaling} and model parallel~\\cite{guhaniyogi2014bayesian} approaches. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. \n\nHere, we adopt the parameter server approach and present a distributed SGS sampler. \n\n\\iffalse\nMany attempts have been initiated to make LDA's inference a distributed procedure, such as AD-LDA, HD-LDA \\cite{newman2007distributed} and SVB \\cite{broderick2013streaming}. Although some of them do not have theoretical guarantees, they all perform pretty well in practice. We claim that SGS can be well paralleled without much loss of precision by adopting a similar scheme as Asynchronous SVB does. \n\\fi\n\nSame as CGS, the global parameter in SGS is the topic word count matrix $\\bm{N}_{kv}$. SGS can be viewed as a sequence of calls to the CGS procedure\n\n", "index": 13, "text": "$$\\Delta \\bm{N}_{kv}^t=CGS(\\alpha, \\beta +\\bm{N}_{kv}^{t-1}, \\bm{W}^t),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\Delta\\bm{N}_{kv}^{t}=CGS(\\alpha,\\beta+\\bm{N}_{kv}^{t-1},\\bm{W}^{t}),\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mi>t</mi></msubsup></mrow><mo>=</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>G</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mrow><mi>\u03b2</mi><mo>+</mo><msubsup><mi>\ud835\udc75</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo>,</mo><msup><mi>\ud835\udc7e</mi><mi>t</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01142.tex", "nexttext": "\nwhere $\\log p(w_{di}|\\mathcal{M})=\\log {\\sum_k \\phi_{kv_{di}}\\theta_{dk}}$. \n\n\n\\begin{wraptable}{r}{0.5\\textwidth}\\vspace{-.5cm}\n\\vspace{-0.5cm}\n\\caption{Data Statistics, where $K$ and $M$ stand for thousand and million respectively. }\n\\label{tb:datasets}\n\\begin{tabular}{l | lll}\n\\hline\n       & \\# Docs  & \\# Token & Vocab-Size \\\\ \n       \\hline\nNIPS   & 1740 & 2M    & 13K   \\\\\nNYT    & 300K & 100M  & 100K  \\\\\nPubMed & 8.2M & 730M  & 141K  \\\\\n\\hline \n\\end{tabular}\n\\vskip -0.5cm\n\\end{wraptable}\n\nTo compare the performance of the algorithms in various settings, we test them on three datasets \\footnote{All the three datasets can be downloaded from UCI Machine Learning Repository: \\url{https://archive.ics.uci.edu/ml/datasets/Bag+of+Words}}. The small NIPS dataset has the articles from 1988 to 2000 published by Neural Information Processing Systems (NIPS) Conferences. A larger dataset is the news from New York Times (NYT) and the largest one is the abstracts of the publications on PubMed~\\cite{UCI+Bache+Lichman:2013}. Table~\\ref{tb:datasets} shows the detailed information of each dataset.\n\n\n\\subsection{Results for various mini-batch sizes}\n\n\\begin{figure}[t]\\vspace{-.4cm}\n\\centering \n    \\subfigure[Perplexities of SVB and SGS on NIPS.] { \n    \\label{fig:batch_nips} \n    \\includegraphics[width=0.45\\columnwidth, height=4cm]{./pics/batch/batch3-NIPS.eps}\n    }\n    \\subfigure[Perplexities of SVB and SGS on NYT.] { \n    \\label{fig:batch_nyt} \n    \\includegraphics[width=0.45\\columnwidth, height=4cm]{./pics/batch/batch3-NYT.eps} \n    }\\vspace{-.3cm}\n\\caption{The perplexities of SVB, SGS and CGS on the small NIPS (a) and large NYT (b) datasets. }\n\\label{fig:batch_small} \\vspace{-.4cm}\n\\end{figure}\nWe run SGS and SVB on NIPS and NYT datasets, varying mini-batch sizes. To simplify our comparison, we set the decay factor for SGS to $\\lambda=1$. Fig.  \\ref{fig:batch_small}(a) and Fig.  \\ref{fig:batch_small}(b) show\nthat SGS consistently performs better than SVB, especially in the cases of bigger dataset and smaller mini-batch sizes. \n\nThe difference in the performance gap between SGS and SVB on NYT and NIPS datasets could be understood as different levels of redundancy. In order learn effectively in a streaming setting, it is required to have a redundant dataset. There are only 1740 documents in the NIPS corpus, which is far from being redundant and thus different streaming algorithms performs alike on this dataset. From the trend of the dataset size, we can expect that SGS would perform even better than SVB on larger datasets. Another interesting phenomenon is that SVB is more sensitive to mini-batch sizes than SGS. The inherent ability of SGS to perform much better than SVB on smaller mini-batches have important advantages in practice.\n\n\n\n\nNote that SGS is equivalent to CGS when the mini-batch size equals to the whole training set size. The green horizontal dashed lines in Fig. \\ref{fig:batch_nips} \\ref{fig:batch_nyt} mark the perplexity of CGS. We can see that on the large NYT dataset, SGS has a huge improvement over SVB when compared to the best-achievable perplexity. \n\n\n\n\n\n\n\n\\subsection{Results for different decay factor}\nWe also investigate how the decay factor $\\lambda$ affects the performance of SGS on different datasets. The results are similar as above, a bigger dataset has more obvious trends. As shown in Fig.s~\\ref{fig:decay_nips} and \\ref{fig:decay_nyt}, when the mini-batch is too small, the decay factor has a negative effect. \nIt is probably because a small mini-batch can only learn a limited amount of knowledge in a single round and it is thus not preferable to forget the knowledge.\nWhen the mini-batch size gets bigger, the decay factor improves the performance, and the optimal value for $\\lambda$ gets smaller. This can be explained in a similar manner as  above, where bigger mini-batches will learn more and the next mini-batch would have greater discrepancy with the current one. \n\n\\begin{figure}[t]\\vspace{-.3cm}\n\\begin{center}\n    \\subfigure[Effect of decay factors (NIPS).] { \n    \\label{fig:decay_nips} \n    \\includegraphics[width=0.3\\columnwidth] {./pics/decay/decay-nips.eps}\n    }\n    \\hspace{-0.1in}\n    \\subfigure[Effect of decay factors (NYT).] { \n    \\label{fig:decay_nyt} \n    \\includegraphics[width=0.3\\columnwidth] {./pics/decay/decay-nyt.eps}\n    }\n    \\hspace{-0.1in}\n    \\subfigure[Learning process.] { \n    \\label{fig:pro} \n    \\includegraphics[width=0.3\\columnwidth] {./pics/process/pro4.eps}\n    }\\vspace{-.3cm}\n\\caption{(a,b) The change of SGS's perplexity w.r.t the decay factor $\\lambda$ on NIPS and NYT datasets; and (c) The trend of changing perplexities as new mini-batches arrive, mini-batch size = 3200. }\n\\end{center}\\vspace{-0.5cm}\n\\end{figure} \n\nLet us examine a specific setting where the batch size of NYT dataset is set to $3200$. SVB yields a perplexity $6511$, while SGS without decay can reach $5240$. After applying decay factor of $0.7$, SGS yields a perplexity of $4640$, which is pretty close to the batch perplexity of $4300$. We can conclude that, if the decay factor is set properly, SGS is much better than SVB and it can almost reach the same precision as its batch counterpart.\n\n\n\n\\subsection{Learning Process}\nHowever, the mean perplexity of each mini-batch is not a full description of the learning process. We should also take the trend of the inference quality as new mini-batches arrive into account. In Fig.~\\ref{fig:pro}, we partition each mini-batch into training and testing tests, and plot the testing perplexity of each mini-batch of SVB and SGS. We can clearly see that the performance of the decayed SGS is strictly better than the non-decayed version, and the latter one outperforms SVB. In other words, SGS can consistently learn a better model every day. All three models have perplexity bursts at a few initial mini-batches because the models over-fits the first few mini-batches. \n\n\\subsection{Computational Efficiency}\n\\begin{figure}[ht] \\vskip -0.2cm\n\\begin{center}\n    \\subfigure[Convergence of SGS and SVB on a mini-batch.] { \n    \\label{fig:test_conv} \n    \\includegraphics[width=0.45\\columnwidth]{./pics/test_conv/test-conv.eps}\n    }\n    \\hspace{-0.1in}\n    \\subfigure[Time consumption of SGS, SVB and CGS.] { \n    \\label{fig:time_effi} \n    \\includegraphics[width=0.45\\columnwidth]{./pics/time/time-NYT-batch3.eps}\n    }\n\\vskip -0.2cm\n\\caption{\nComputational Efficiency of SGS, SVB, and CGS\n\\iffalse(a) Testing perplexity-time plot of SGS and SVB on a middle(9th) mini-batch on NIPS dataset. Mini-batch size is 100. The perplexities of SVB in the first second are missing because it hasn't finished the first outer loop and thus we are unable to evaluate the performance. The SVB curve has a step shape because VB update topic-word variational parameter only after a whole sweep of the current mini-batch. \n(b) Total time consumption of SGS, SVB and CGS on the NYT dataset.\n\\fi}\n\\end{center}\n\\vskip -0.4cm\n\\end{figure} \n\n\nSince online algorithms usually run for a longer time span, months or years, online algorithms face less computational challenges than the offline versions. However, we would still like the online inference method to not use excessive computational resources. In this section, we compare the computational efficiencies of SGS, SVB and CGS. Since SGS and SVB have outer loops that process arriving mini-batches and inner loops (iterations), we investigate the time per mini-batch and on the whole dataset separately.\n\nIn Fig.~\\ref{fig:test_conv}, we run SGS and SVB through some initial mini-batches and then investigate their convergences on the same intermediate mini-batch. The perplexity on held-out documents are plotted against time. SGS starts from a better point than SVB because its better result on previous iterations. Furthermore, SGS also converges faster than SVB. \n\nIn Fig.~\\ref{fig:time_effi}, both SGS and SVB are run until convergence, where the criterion for convergence is stated in Section \\ref{sec:exp_setting}. We can see that SVB and SGS converge within similar time. Also, since the online version is searching over smaller number of configurations, we can observe that the smaller the mini-batch size is, the faster it converges. SGS can be faster than CGS with smaller mini-batch sizes.\n\n\\subsection{Distributed Experiments}\nIn this section we compare distributed SGS and SVB\\footnote{SVB refers to both distributed and single-threaded variants.} on the NYT dataset, and the scalability of DSGS is examined on the larger PubMed dataset. \nWhen we compare DSGS with SVB in Fig.~\\ref{fig:nyt_pers}, we can conclude that although the perplexity of DSGS gets worse as the number of cores increases, it still consistently outperforms SVB. Fig.~\\ref{fig:nyt_time} shows the throughput of DSGS and SVB, in tokens per second.\nSince the topic-word assignment update of DSGS is sparse and the corresponding variational parameter update of SVB is dense, the speedup of DSGS is much better than SVB. Fig.~\\ref{fig:dis_precision},~\\ref{fig:dis_time} show the scalability result on the larger PubMed dataset. In general we can conclude DSGS enjoys nice speedup while retaining a similar level of perplexity.\n\n\\iffalse\nWe have tested DSGS's performance with different mini-batch sizes and number of cores. In Fig.~\\ref{fig:dis_precision}, the distributed version has shown some interesting patterns in the precision of the inference. When the mini-batch size is relatively big, such as 50k, the inference quality get worse as the number of threads going up, which conforms with the expectation. But things are going in the opposite direction when the mini-batch size is small. Surprisingly, the bigger level of the parallelism is, the better the performance is. This is consistent with some previous work in paralleling LDA \\cite{broderick2013streaming}. On the other hand, the speed up is almost linear of the number of cores (Fig. \\ref{fig:dis_time}).\n\\fi\n\n\n\\begin{figure}[t] \\vskip -0.5cm\n\\centering \n    \\subfigure[Perplexity of DSGS and SVB on NYT.]{\n    \\label{fig:nyt_pers} \n    \\includegraphics[width=0.45\\columnwidth, height=3.5cm]{./pics/distribute_nyt/nyt_per.eps}\n    }\n    \\subfigure[Tokens per second of DSGS and SVB.]{\n    \\label{fig:nyt_time} \n    \\includegraphics[width=0.45\\columnwidth, height=3.5cm]{./pics/distribute_nyt/nyt_time.eps} \n    }\n    \n    \\subfigure[Perplexity of DSGS on PubMed dataset.] { \n    \\label{fig:dis_precision} \n    \\includegraphics[width=0.45\\columnwidth, height=4cm]{./pics/distribute/distributed_pubmed_pers.eps} \n    }\n    \\subfigure[Time consumption of DSGS on PubMed.] { \n    \\label{fig:dis_time} \n    \\includegraphics[width=0.45\\columnwidth, height=4cm]{./pics/distribute/distributed_pubmed_times.eps} \n    }\n\\vskip -0.2cm\n\\caption{Scalability results \\iffalse(a, b) Compare DSGS and SVB in perplexity and tokens per second on the NYT dataset. Mini-batch size is 3200.  The ideal curve in (b) is calculated by $single\\_thread\\_tokens/sec \\times num\\_threads$. \n(c) DSGS's experimental precision results on PubMed data, with various mini-batch sizes and the number of threads. \n(d) DSGS's time consumption under various number of threads as well as mini-batch sizes. \\fi}\n\\label{fig:batch} \n\\vskip -0.5cm\n\\end{figure}\n\n\n\\section{Discussion}\nWe have developed a streaming Gibbs sampling algorithm (SGS) for the LDA model. Our method can be seen as an online extension of the collapsed Gibbs sampling approach. \nOur experimental results demonstrate that SGS improves perplexity over previous online methods, while maintaining similar computational burden. We have also shown that SGS can be well parallelized using similar techniques as those adopted in SVB. \n\nIn the future, SGS can be further improved by making the decay factor $\\lambda$, the mini-batch size and the number of iterations for each document self-evolving, as more data is fed into the system. Intuitively, the algorithm learns fast at the beginning and slows down later on. Thus for example, it might be tempting to decrease the iteration counts for each document to some constant over time. The scheme for the evolution deserves future research and needs strong theoretical guidance. \n\n\n\n\n{\\small\n\\printbibliography\n}\n\n\n\n", "itemtype": "equation", "pos": 29849, "prevtext": "\nwhere $\\bm{N}_{kv}$ should be updated as $\\bm{N}_{kv}^t=\\bm{N}_{kv}^{t-1}+\\Delta \\bm{N}_{kv}^t$\n{\\color{red}{\\bf\\sf [CJF: {, The $k$th row of $\\beta + \\bm{N}^{t-1}_{kv}$ is the prior of $\\vec{\\phi}_k$.}]}} \nThen we can empirically design a parallel version of SGS as Alg.~\\ref{alg:DSGS}. We assume that the documents are stored in a distributed file system, such as the Hadoop Distributed File System (HDFS). Each worker only access a mini-batch of its own portion of documents in each iteration, represented by $\\bm{W}^t$ in Alg.~\\ref{alg:DSGS}. \n\\fi\n\n\\iffalse\nAt the beginning of each iteration, the worker receives the current $\\bm{N}_{kv}$ from the central master node. After finishing calculating the $\\bm{\\Delta N}_{kv}^{local}$, he sends the (sparse) delta matrix back to the master node. The master node opens $2\\times P$ threads, where $P$ is the number of workers. Among the $2\\times P$ threads, $P$ of them serve to update the master's global $\\bm{N}_{kv}$ and the other $P$ send the current $\\bm{N}_{kv}$ to each worker on request. The master node update $\\bm{N}_{kv}$ using an atomic fetch and add operation. The update operation might block itself, but the master node use another thread to send the most recent completely updated $\\bm{N}_{kv}$ to each worker. This ensures that all workers can operate at full speed, even if the master worker is busy performing the update. \n\\fi\n\n\n\n\nIn experiments, we can see that this empirical parallel framework can almost linearly scale up SGS with neglectable precision loss. \nDue to the sparseness of $\\bm{N}_{kv}$, Distributed SGS (DSGS) has much less communication overhead between the master and workers, hence more scalable than SVB. \n\n\n\n\n\n\\section{Experiments}\n\nWe evaluate the inference quality and computational efficiency of SGS. We also assess how the parameters such as mini-batch size, decay factor and the number of iterations affect the performance. We compare with the online variational Bayes approach SVB~\\cite{broderick2013streaming}, which has proven to have high inference quality that is similar to offline  stochastic methods like SVI~\\cite{hoffman2013stochastic}. \n\n\n\\subsection{Implementation Details}\\label{sec:initialization}\n\nAs Canini et al.~\\cite{canini2009online} mentioned, initializations might have big impacts on the solution qualities of the inference algorithms, and hence, using randomized initialization for $\\bm{Z}$ is often not good. Thus, we use a kind of ``progressive online\" initialization for SGS, CGS and SVB. Specifically, taking SGS as an example, at the first iteration for each mini-batch, for each document we sample $\\vec{z_d}$ from the posterior distribution up to current $t$. Then we update the posterior distribution using the current document and proceed to the next one. Such a ``sampling from posterior\" initialization technique ensures that our initialization is reasonably good. We use a similar initialization method for SVB and CGS. \n\n\n\n\n\nAll the core implementations (sampling and calculating a variational approximation) are in C++. We also use Python and MATLAB wrappers for computationally inexpensive operations, such as measuring time. Our implementation of SVB has been made as similar as possible with SGS for fair speed comparison. The speed of our SVB implementation is similar to the implementation in \\cite{broderick2013streaming}. The experiments are done on a 3 node cluster, with each node equipped with 12 cores of Intel Xeon E5645@2.4GHz, 24GB memory and 1Gbps ethernet. \n\nIn the distributed experiments, data $\\bm{W}^t$ is pre-partitioned and stored separately on each node, but in practice it can be stored in a distributed file system such as HDFS, or a publish-subscribe pattern can be used for handling streaming data. For the sake of simplicity, we implement our own parameter server using pyRpc, a simple remote procedure call (RPC) module that uses ZeroMQ. \n\nWe use a pipeline on worker nodes to hide the communication overhead. Parameters on master server is stored using \\texttt{atomic}, and there are model replicas on master server to ensure high availability while performing updates. \nFor production usage, existing high performance parameter servers, such as \\cite{li2014scaling} might be used to achieve better performance and scalability. Again, system is orthogonal with the algorithm in our case and is not the main focus in this paper.\n\n\n\\subsection{Setups} \\label{sec:exp_setting}\n\nIn the following experiments, hyper-parameters $\\alpha$ and $\\beta$ are all set to $0.1$ and $0.03$, number of topics $K$ is set to $50$. Different settings yield same conclusions. Thus we stick to this setting for simplicity. Multiple random starts of SGS and SVB don't result in significant difference. Without special remarks, for each mini-batch, both SGS and SVB run the sampler until convergence. To be specific, SGS stops the iteration when the training perplexity on the current mini-batch stops improving for 10 consecutive iterations, or when it reaches a maximum of 400 iterations. SVB stops the inner iteration when $\\frac{||\\vec{\\theta}_d^{old} - \\vec{\\theta}_d^{new}||_1}{K} < 10^{-5}$ or when it reaches a maximum of 100 iterations, and stops the outer iteration if\n$\\frac{||\\bm{\\phi}^{new}-\\bm{\\phi}^{old}||_1}{KV} < 10^{-3}$.\n\n\n\n\nThe predictive performance of LDA can be measured by the probability it assigns to the held-out documents. This metric is formalized by perplexity. First we partition the data-set and use 80\\% for training and 20\\% for testing. Let $\\mathcal{M}$ denote the model trained on the training data. Given a held-out document $\\vec{w}_{d}$, we can infer $\\vec{\\theta}_d$ from the first half of the tokens in $d$, and then calculate the exponentiated negative average of the log probability. Formally, we have:\n\n", "index": 17, "text": "$$per(\\vec{w}_{d}|\\mathcal{M})=\\exp\\left\\{ -\\frac{\\sum_i \\log p(w_{di}|\\mathcal{M})}{|\\vec{w}_{d}|} \\right\\}, $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"per(\\vec{w}_{d}|\\mathcal{M})=\\exp\\left\\{-\\frac{\\sum_{i}\\log p(w_{di}|\\mathcal{%&#10;M})}{|\\vec{w}_{d}|}\\right\\},\" display=\"block\"><mrow><mi>p</mi><mi>e</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>exp</mi><mrow><mo>{</mo><mo>-</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u2192</mo></mover><mi>d</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>}</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}]