[{"file": "1601.08245.tex", "nexttext": "\nThe $B$ parameter can be extracted from the data by fitting to this\nfunction, and results are shown for different versions of our code.\nIn the Figure, a big improvement was found by optimizing how data\nstructures are re-initialized on each event, leading a reduction of\nthe serial fraction from 26\\% to 9\\%.  There is a significant residual\ncontribution to non-ideal scaling due to variation of occupancy within\nthreads: some threads simply take longer than others. In our group\nthere is work ongoing to define strategies for an efficient `next in\nline' approach or a dynamic reallocation of thread resources to even\nout timing across threads.\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\nWe have made significant progress in parallelized and vectorized\nKalman-filter-based tracking R\\&D on Xeon and Xeon Phi architectures.\nWe have developed a good understanding of bottlenecks and limitations\nof our implementation.  New versions of the code are faster and\nexhibit scaling closer to ideal performance. We are continuing to\npursue new ideas to further improve performance\n\nThough it was not discussed in the talk, we have also developed tools\nto process fully realistic data, with encouraging preliminary results.\n\nThe project is solid and promising; however, much work remains.\n\n\n\\section*{Acknowledgment}\nThis work was supported by the\nU.S.  National Science Foundation. \n\n\\begin{thebibliography}{99}\n\\bibitem{Fruhwirth:1987fm}\nR.~Fruehwirth, ``{Application of Kalman filtering to track and vertex\n  fitting},''\n\\href{http://dx.doi.org/10.1016/0168-9002(87)90887-4}{{\\em Nucl.~Instrum.~Meth.\n  A} {\\bfseries 262} (1987) 444--450}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibitem{HALYO1}\nV.~Halyo, P.~LeGresley, P.~Lujan, V.~Karpusenko, and A.~Vladimirov, ``{First\n  evaluation of the CPU, GPGPU and MIC architectures for real time particle\n  tracking based on Hough transform at the LHC},'' {\\em Journal of\n  Instrumentation} {\\bfseries 9} no.~04, (2014) P04005.\n  \\url{http://stacks.iop.org/1748-0221/9/i=04/a=P04005}.\n\n\n\\bibitem{acat2014} G.~Cerati \\emph{et al.}, Traditional Track with Kalman Filter on Parallel Architectures, Proceedings of ACAT 2014, arXiv:1409.8213 (2014)\n\\bibitem{chep2015} G.~Cerati \\emph{et al.}, Kalman Filter Tracking on\n  Parallel Architectures, Proceedings of CHEP 2015, arXiv:1505.04540\n  (2015).\n\n\\bibitem{vtune} See \\url{https://software.intel.com/en-us/intel-vtune-amplifier-xe}.\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 16330, "prevtext": "\n\n\\title{Kalman-Filter-Based Particle Tracking on Parallel\nArchitectures at Hadron Colliders}\n\n\\author{G.~Cerati, M.~Tadel, F.~W\\\"urthwein, A.~Yagil, S.~Lantz,\n  K.~McDermott, D.~Riley, P.~Wittich and  P.~Elmer.\n\\thanks{Manuscript received December 7, 2015.\nThis work was supported by the National Science Foundation.}\n\\thanks{G.~Cerati, M.~Tadel, F.~W\\\"urthwein, A.~Yagil: UC San Diego.}\n\\thanks{S.~Lantz, K.~McDermott, D.~Riley and P.~Wittich: Cornell University.}\n\\thanks{P.~Elmer: Princeton University.}\n}\n\n\\maketitle\n\n\\thispagestyle{empty}\n\n\\begin{abstract}\n  Power density constraints are limiting the performance improvements\n  of modern CPUs. To address this we have seen the introduction of\n  lower-power, multi-core processors such as GPGPU, ARM and Intel\n  MIC. To stay within the power density limits but still obtain\n  Moore's Law performance/price gains, it will be necessary to\n  parallelize algorithms to exploit larger numbers of lightweight\n  cores and specialized functions like large vector units. Track\n  finding and fitting is one of the most computationally challenging\n  problems for event reconstruction in particle physics. At the\n  High-Luminosity Large Hadron Collider (HL-LHC), for example, this\n  will be by far the dominant problem. The need for greater\n  parallelism has driven investigations of very different track\n  finding techniques such as Cellular Automata or Hough\n  Transforms. The most common track finding techniques in use today,\n  however, are those based on the Kalman Filter. Significant\n  experience has been accumulated with these techniques on real\n  tracking detector systems, both in the trigger and offline. They are\n  known to provide high physics performance, are robust, and are in\n  use today at the LHC. We report on porting these algorithms to new\n  parallel architectures.  Our previous investigations showed that,\n  using optimized data structures, track fitting with Kalman Filter\n  can achieve large speedups both with Intel Xeon and Xeon Phi. We\n  report here our progress towards an end-to-end track reconstruction\n  algorithm fully exploiting vectorization and parallelization\n  techniques in a realistic experimental environment.\n\\end{abstract}\n\n\\section{Introduction}\n\\IEEEPARstart{T}{he} Large Hadron Collider (LHC) at CERN is the\nhighest energy collider ever constructed. It consists of two\ncounter-circulating proton beams made to interact in four locations\naround a 17 mile ring straddling the border between Switzerland and\nFrance. It is by some measures the largest man-made scientific device\non the planet. The goal of the LHC is to probe the basic building\nblocks of matter and their interactions. In 2012, the Higgs boson was\ndiscovered by the CMS and ATLAS collaborations.  Experimentally, we\ncollide proton beams at the center of our detectors and, by measuring\nthe energy and momentum of the escaping particles, infer the existence\nof massive particles that were created and decayed in the pp collision\nand measure those massive particles' properties.  In all cases, track\nreconstruction, i.e., the determination of the trajectories of charged\nparticles, plays a key role in identifying particles and measuring\ntheir charge and momentum. The track reconstruction as a whole is the\nmost computationally complex and time consuming, sensitive to\nincreased activity in the detector, and traditionally, least amenable\nto parallelization.  The speed of reconstruction has a direct impact\non how much data can be stored from the 40 MHz collisions rate. The\nmost CPU-intensive part of the event selection in the online trigger\nprocess is track reconstruction, to the point that it can only be\napplied to a few \\% of the input event rate. At the same time, the\ntracker is the most precise instrument in CMS. Tracking is thus the\nessential tool in making surgical decisions in the online selection to\noptimally use the limited output bandwidth for interesting physics\nevents. The speed of track reconstruction software thus limits both\nthe total output rate of the experiments via the first cap, and the\nsurgical precision with which interesting events can be selected via\nthe second cap. Our research aims to lift those caps by vastly\nspeeding up the online tracking reconstruction.\n\nThe large time spent in track reconstruction will become even more\nimportant in the HL-LHC era of the Large Hadron Collider, where the\nincrease in event rate will lead to an increase in detector occupancy\n(``pile-up'', PU), leading to an exponential gain in time taken to do\ntrack reconstruction, as can be seen in Fig.~\\ref{fig:pileup}. In the\nFigure, PU25 corresponds to the data taken during 2012, and PU140\ncorresponds to the low end of estimates for the HL-LHC era.\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{RecoTimeLUMI_25_BX_25ns.png}\n  \\caption{CPU time per event versus instantaneous luminosity, for\n    both full reconstruction and the dominant tracking portion. PU25\n    corresponds to the data taken during 2012, and PU140 corresponds to\n    the HL-LHC era. The time of the reconstruction is dominated by\n    track reconstruction. }\n  \\label{fig:pileup}\n\\end{figure}\nClearly this research will become increasingly important during this era.\n\nA correlated issue is the change in computing architectures in the\nlast decade. Around 2005, the computing processor market reached a\nturning point: power density limitations in chips ended the long-time\ntrend of ever-increasing clock speeds, and our applications no longer\nimmediately run exponentially faster on subsequent generations of\nprocessors. This is true even though the underlying transistor count\ncontinues to increase per Moore's law. Increases in processing speed\nsuch as those required by the time increases in Fig.~\\ref{fig:pileup}\nwill no longer come `for free' from faster processors. New processors\ninstead are aggregates of `small cores' that in toto still show large\nperformance gains from generation to generation, but their usage\nrequires a re-work of our software to exploit. The processors in question\ninclude ARM, GPGPU and the Intel Xeon Phi; in this work we target the\nXeon Phi architecture.\n\n\\section{Kalman Filter Tracking}\nTo realize the new performance gains, a change is required to move\nfrom the sequential applications of today to vectorized, parallelized\napplications of tomorrow.  The algorithm we are targeting for\nparallelization is a Kalman Filter (KF) based\nalgorithm~\\cite{Fruhwirth:1987fm}. KF-based tracking algorithms are\nwidely used since they naturally include estimates of scattering along\nthe trajectory of the particle due to multiple scattering off massive\ndetectors. Other algorithms, more naturally suited to parallelization\nand coming from the image processing community, have been explored by\nothers. These include Hough Transforms and Cellular Automata, among\nothers (see, for instance, ~\\cite{HALYO1}.)\nHowever, these are not the main algorithms in use at the LHC today,\nwhereas there is extensive understanding how KF algorithms perform. KF\nalgorithms have proven to be robust and perform well in the difficult\nexperimental environment of the LHC. By porting these algorithms to\nparallel architectures, we aim to port this robust tool to new\narchitectures.  Past work by our group has shown progress in\nsub-stages of the KF algorithm in simplified detectors (see, e.g. our\npresentations at ACAT2014~\\cite{acat2014} and\nCHEP2015~\\cite{chep2015}).\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{vectorunits1}\n  \\caption{Track building part of the problem, where we decide which\n    hits to group together as coming from the passage of a single\n    charged particle, as a function of the increased usage of the\n    processors' vector registers. }\n  \\label{fig:scaling}\n\\end{figure}\nFig.~\\ref{fig:scaling} shows a result from\nthe track building part of the problem, where we decide which hits to\ngroup together as coming from the passage of a single charged\nparticle, as a function of the increased usage of the processors'\nvector registers. The black line shows ``ideal behavior'' - perfect\nscaling, and the blue line shows our measured results. We observe a\nsignificant speedup compared to the baseline, but still room for\nimprovement with respect to the ideal behavior. All results are for\nthe Xeon Phi. We have now implemented an end-to-end algorithm with a\nsemi-realistic detector model. This talk represents a status report.\n\n\n\\subsection{Optimized Matrix Library {\\textsc{Matriplex}\\xspace}}\nThe computational problem of Kalman Filter-based tracking consists of\na sequence of matrix operations on matrices of sizes from\n$N\\times{}N = 3\\times{}3$ up to $N\\times{}N = 6\\times{}6$. To allow\nmaximum flexibility for exploring SIMD operations on small-dimensional\nmatrices, and to decouple the specific computations from the high\nlevel algorithm, we have developed a new matrix library,\n{\\textsc{Matriplex}\\xspace}. The {\\textsc{Matriplex}\\xspace} memory layout is optimized for the loading\nof vector registers for SIMD operations on a set of\nmatrices. {\\textsc{Matriplex}\\xspace} includes a code generator for generation of\noptimized matrix operations supporting symmetric matrices and\non-the-fly matrix transposition. Patterns of elements which are known\nby construction to be zero or one can be specified, and the resulting\ngenerated code will be optimized accordingly to reduce unnecessary\nregister loads and arithmetic operations. The generated code can be\neither standard {{\\small\\textsc{C++}}\\xspace} or simple intrinsic macros that can be\neasily mapped to architecture-specific intrinsic functions.\n\n\n\n\n\n\n\n\n\n\n\\section{Current Status}\nWe have performed extensive studies of the performance of our\nsoftware. Both the physics performance and the code performance were\nexamined. For the latter, we used the \\textsc{Intel\n  VTune}~\\cite{vtune} suite of tools to identify bottlenecks and\nunderstand the impacts of our optimization attempts. In particular, as\ncan be expected, we determined that memory management is of critical\nimportance.  To this effect we describe below several studies to\noptimize memory performance, and discuss the results of these studies.\n\\subsection{Cloning Engine Studies}\n\\begin{table*}[!t]\n  \\centering\n  \\begin{tabular}{|l|r|r|r|r|r|r|r|}\n    \\hline\n&       VU01 Xeon &   VU08 Xeon &    speedup [\\%]& VU01\n                                                   Xeon Phi\n    & VU16 Xeon Phi &      speedup [\\%] \\\\\n   & \\multicolumn{2}{|c|}{s/10 events} & &\\multicolumn{2}{|c|}{s/10\n                                         events} & \\\\\n    \\hline\noriginal &      17.4    &12.4&  29.0&   94.31&  70.76&  25 \\\\\ncloning engine  & 17.87 &8.2   &  54  &    93   &   50 &     46 \\\\\nthreaded cloning engine&    11.6  &  6.13  &  47   &   64.5   & 38& 41 \\\\\n\\hline\nspeedup [\\%]&   35    &  25 &     -      & 31 &     24 &      - \\\\\n\\hline\n  \\end{tabular}\n  \\caption{Performance Improvements from the cloning engine (CE) studies.  The\n    first set of columns refers to traditional Xeon processors. The\n    second set refers to Xeon Phi accelerators. VUXX refers to the\n    vector unit size; maximum size is 8 in Xeon and 16 in Xeon\n    Phi. The last row tabulates the speedup between the two CE models. }\n  \\label{tab:cloning}\n\\end{table*}\nIn Tab.~\\ref{tab:cloning} we present results from our ``cloning\nengine'' (CE) studies. In profiling our application it was observed\nthat a significant amount of time was spent in operations associated\nwith memory management. In our tracking algorithm, new data structures\nhave to be created when new track candidates are examined. Significant\ntime is also spent loading data into our local caches. In the CE\nstudies, we examine offloading the memory management work to a worker\nprocess. The first strategy, labeled ``cloning engine'' in the Table,\ngangs all memory operations together; speedup is observed by removing\nredundant operations.  In the second strategy, labeled ``threaded\ncloning engine'', the memory management task is delegated to a second\nthread. We observe increased performance and vector unit efficiency\nwith CE studies compared to the original model and increased\nperformance for both Xeon and Xeon Phi using the threaded CE compared\nto the non-threaded CE.\n\n\\subsection{Reduced Data Structures}\n\\begin{table}[!t]\n  \\centering\n  \\begin{tabular}{|l|r|r|}\n    \\hline\n    &VU01 \t&VU08  \\\\\n    & [s/10 evt]\t&  [s/10 evt] \\\\\n    \\hline\n    original\t& 17.6& \t12.3 \\\\\n    original (r.d.f) &\t13.9\t &8.7 \\\\\n    cloning engine\t& 19.3\t& 9.7 \\\\\n    cloning engine (r.d.f) &\t18.3\t&8.5 \\\\\n    threaded cloning engine & 13.0\t& 8.3 \\\\ \n    threaded cloning engine (r.d.f)\t&13.8&\t8.1 \\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Performance scaling for various improvements. Measurements\n    were performed on a Xeon CPU. VU01 and V08 refer to usage of\n    vector unit of one and eight elements, respectively.  The acronym\n    ``r.d.f'' refers to measurements with reduced data\n    formats. Significant performance gains are seen by using reduced\n    data formats. }\n  \\label{tab:scaling}\n\\end{table}\nThe size of the data structures used in our algorithm have a crucial\nimpact on the timing performance. In particular, the data structures\nrepresenting the energy deposits in the detector (the ``hits'') and\nthe reconstructed particle trajectories (the ``tracks'') must be\noptimized in size to fit into the fastest cache memory.\nObject-oriented data structures require more memory than arrays.\nTherefore, we replaced the Kalman covariance matrices with basic\n\\textsc{C}-style arrays rather than \\textsc{C++}-style classes. These\nchanges allowed us to reduce the size in memory of the track objects\nby 20\\% and the hit object by 40\\%.  Tab.~\\ref{tab:scaling} shows the\nresult of these studies.  The table only shows Xeon numbers; Xeon Phi\nperformance is similar.\n\nIn addition to moving to \\textsc{C}-style data structures, we\noptimized the contents of the data structures. Our original\nimplementation of these objects were designed for algorithmic ease of\nuse and contained data members not strictly needed for the track\nreconstruction. For instance, Monte Carlo truth information,\n\\emph{i.e.}, information about how the trajectories were generated,\nwere stored in the hit objects. The track objects contained\n(duplicate) copies of the hits themselves, rather than smaller\nreferences to the hits in their original locations.  We rewrote the\ndata structures to optimize performance and maintain ease of use by\ncarefully considering what was needed for physics output only and\nkeeping auxiliary information in associated data structures.\n\nWe find significant speed-ups from the reduced data formated (labeled\n``r.d.f'' in the table). We also find that with these changes, the\nimprovement of the CE studies is reduced; presumably since the amount\nof memory churn overall, which the CE improves, has now been globally\nreduced.\n\n\\subsection{Performance Scaling - parallelization}\n\\begin{figure*}[!t]\n  \\centering\n  \\includegraphics[width=1.0\\textwidth]{amdahl_plots}\n  \\caption{Performance scaling of the tracking code as a function of\n    the number of threads, on the Xeon architecture. The plot on the\n    right show the time to process 20,000 tracks as a function of the\n    number of threads. The curve in green represent ideal scaling from\n  the first data point. The blue and red curves show two different\n  ways of distributing the data across threads. On the right the same\n  data is plotted but now as a function of $1/n_\\text{threads}$; this\n  figure can be used to extract the serial fraction of the code.  }\n  \\label{fig:amdahl}\n\\end{figure*}\nFigure~\\ref{fig:amdahl} shows the parallelization performance scaling of\nour code on the Xeon architecture. The plot on the right show the time\nto process 20,000 tracks as a function of the number of threads. The\ncurve in green represent ideal scaling from the first data point. The\nblue and red curves show two different ways of distributing the data\nacross threads. On the right the same data is plotted but now as a\nfunction of $1/n_\\text{threads}$; this figure can be used to extract\nthe serial fraction of the code. According to Amdahl's law, the time\nspent on $n$ threads consists of a serial fraction $B$ and a parallel\nfraction $(1-B)/n_\\text{threads}$ multiplied by the time per thread, $T(1)$.\n\n", "index": 1, "text": "$$\nT(n) = T(1)\\left[ B + \\dfrac{1}{n_\\text{threads}} (1-B)\\right]\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"T(n)=T(1)\\left[B+\\dfrac{1}{n_{\\text{threads}}}(1-B)\\right]\" display=\"block\"><mrow><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mrow><mi>B</mi><mo>+</mo><mrow><mfrac><mn>1</mn><msub><mi>n</mi><mtext>threads</mtext></msub></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>B</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}]