[{"file": "1601.05883.tex", "nexttext": "\nwhere ${ \\mathbf{{M}} } = \\diag{(\\mu_i)}$ is the matrix of generalized eigenvalues (we assume\ndiagonalizability).\nSubstituting (\\ref{eq:genEigs}) into (\\ref{eq:map}), and recalling the form of our shifted matrices (\\ref{eq:shiftMat}), we have\n\\begin{eqnarray*}\n  (s_k { \\mathbf{{AV}} }{ \\mathbf{{M}} }{ \\mathbf{{V}} }^{-1} + { \\mathbf{{A}} }) {\\widehat}{{ \\mathbf{{N}} }}_k  = (s_0 { \\mathbf{{AV}} }{ \\mathbf{{M}} }{ \\mathbf{{V}} }^{-1} + { \\mathbf{{A}} }) \\iff\n\\end{eqnarray*}\n\\begin{eqnarray}\\label{eq:simTrans}\n  {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{V}} }(s_k { \\mathbf{{M}} } + { \\mathbf{{I}} })^{-1}(s_0 { \\mathbf{{M}} } + { \\mathbf{{I}} }){ \\mathbf{{V}} }^{-1} = { \\mathbf{{VDV}} }^{-1}\n\\end{eqnarray}\nand\n\\begin{eqnarray}\\label{eq:diag}\n  { \\mathbf{{D}} } = \\diag{ \\left(\\frac{s_0{\\mu}_i +1}{s_k{\\mu}_i +1}\\right)}.\n\\end{eqnarray}\nThe assumption that all matrices $s_j{ \\mathbf{{E}} } + { \\mathbf{{A}} }$ in a given sequence are invertible implies\nthat the diagonal matrices $s_j { \\mathbf{{M}} } + { \\mathbf{{I}} }$ are invertible, and hence\nthat $s_k{\\mu}_i + 1 \\neq 0$ and $s_0{\\mu}_i + 1 \\neq 0$ for $i = 1, \\ldots, n$ in (\\ref{eq:diag}).\nWith (\\ref{eq:simTrans}) a similarity transformation, the eigenvalues\nof ${\\widehat}{{ \\mathbf{{N}} }}_k$ are the (diagonal) entries of ${ \\mathbf{{D}} }$, $d_i$. Equation\n(\\ref{eq:diag}) suggests clustering of the eigenvalues if\n$|{\\mu}_i| \\gg |s_0|, |s_k|$ for most of the eigenvalues,\nor if $s_k$ is relatively close to $s_0$\nwith respect to most $|{\\mu}_i|$. We will see that, if the condition number of ${ \\mathbf{{V}} }$,\n${\\kappa}_{F,{ \\mathbf{{A}} }_k}({ \\mathbf{{V}} })$, is modest, clustering leads to\na good approximation of the ideal map by the\nLS map.\n\n\nIn two of the applications discussed in this paper,\nwe can expect the diagonal entries of (\\ref{eq:diag}) - and therefore the\neigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ - to be clustered (possibly with some outliers).\nFor stable dynamical systems, all eigenvalues have\nnegative real part. Therefore, in model reduction for such\nsystems, the shifts are generally computed\n(for example, by IRKA \\cite{GugeAnth08}),\nsuch that ${\\mathrm{Re}}(s_i)$ is close to zero and the shifts are often relatively close to one another.\nThis reflects the fact that the reduced model needs to represent\nmost accurately the modes of the\nsystem that decay slowest (corresponding to the eigenvalues with the smallest absolute\nreal part). In addition, a stable dynamical system may have many eigenvalues with\nlarge absolute real part (corresponding to modes that decay very rapidly).\nTherefore, $s_0{\\mu}_j \\approx s_k{\\mu}_j$ and $d_j \\approx 1$, or $|{\\mathrm{Re}}({\\mu}_j)|$ is very\nlarge compared to ${\\mathrm{Re}}(s_i)$, and\n$\\frac{s_0{\\mu}_j+1}{s_k{\\mu}_j+1}\\approx \\frac{{\\mu}_j}{{\\mu}_j} = 1$.\n\nWe also expect clustering of the eigenvalues of the ideal map for the THT matrices.\nIn this application, the shifts come from a modified Talbot contour and tend to be\nquite small, particularly for larger values of time, as shown in\nFigure \\ref{fig:contours}.  For more information on how these contours, and the\nparameters which define them, are determined, we refer to \\cite{Weid06}.  When\nthese shifts are small and relatively close to one another, as is the case with\nthe middle and late THT matrices, $s_0{\\mu}_j$ and $s_k{\\mu}_j$ are both small enough\nthat $d_j \\approx 1$.  More information on the THT matrices is provided later in this section as well as in Section \\ref{sec:THT}, and  more detail on the\nmodel reduction matrices is given in Section \\ref{sec:ModRed}.\n\n\nClustering implies a small ${\\epsilon}_D$ such that, for all $i$,\n\\begin{eqnarray*}\n  |\\bar{d}-d_i| < \\frac{{\\epsilon}_D}{\\sqrt{n}}.\n\\end{eqnarray*}\nwhere $\\bar{d}$ is the average of all $d_i$ or another appropriate\ncenter for the cluster, such as,\n$\\bar{d} = \\arg \\min_{{\\mathbb{C}}} \\max_i |\\bar{d} - d_i|$.\nNote that the average minimizes $\\|{ \\mathbf{{D}} } - \\bar{d}{ \\mathbf{{I}} }\\|_F$, whereas\nthe minimax solution minimizes $\\|{ \\mathbf{{D}} } - \\bar{d}{ \\mathbf{{I}} }\\|_2$.\nWriting ${ \\mathbf{{D}} } = \\bar{d} { \\mathbf{{I}} } + {\\widehat}{{ \\mathbf{{F}} }}$,  with ${\\widehat}{{ \\mathbf{{F}} }} = ({ \\mathbf{{D}} }-\\bar{d} { \\mathbf{{I}} })$\nand therefore ${\\|{{\\widehat}{{ \\mathbf{{F}} }}}\\|_F} < {\\epsilon}_D$, we have\n\\begin{eqnarray} \\label{eq:NkhatDec}\n  {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{VDV}} }^{-1} = \\bar{d}{ \\mathbf{{I}} } + { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\nNow assume that the chosen sparsity pattern, $S$, contains the\ndiagonal. This is often the case and can easily be ensured.\nThen\n\\begin{eqnarray*}\n  S \\supseteq S_0 = \\{(1,1), (2,2),\\dots, (n,n)\\}.\n\\end{eqnarray*}\nSince $\\bar{d}{ \\mathbf{{I}} } \\in {\\mathcal S}_0$, the subspace corresponding to $S_0$, by Theorem \\ref{teo:seqNestPatt},\n\\begin{eqnarray*}\n {\\|{{ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{\\bar{d}{ \\mathbf{{I}} } - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} =\n   {\\|{\\bar{d}{ \\mathbf{{I}} } -\\bar{d}{ \\mathbf{{I}} } - { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}} =\n   {\\|{{ \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}}.\n\\end{eqnarray*}\nTherefore,\n\\begin{eqnarray}\\label{eq:mapBound}\n{\\|{{ \\mathbf{{R}} }_k}\\|_F} = {\\|{{ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{{ \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}}.\n\\end{eqnarray}\nHence, for modest ${\\kappa}_{F,{ \\mathbf{{A}} }_k}({ \\mathbf{{V}} })$, ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$ will also be small,\nand we can expect good convergence.\nNote that, in general, this bound is rather pessimistic, as\n${ \\mathbf{{N}} }_k$ can provide a much better approximation than $\\bar{d} { \\mathbf{{I}} }$,\nand a further analysis of these approximation problems is a topic\nof further research.\n\n\nSince the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are not always perfectly clustered,\nwe also consider ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$ when ${\\widehat}{{ \\mathbf{{N}} }}_k$ has\nclustered eigenvalues with a few outliers.\nWe expect that, in that case,\n\\begin{eqnarray}\\label{eq:smallLowRank}\n  { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 = { \\mathbf{{I}} } + {\\widetilde}{{ \\mathbf{{K}} }} + { \\mathbf{{H}} },\n\\end{eqnarray}\nwhere ${\\|{{\\widetilde}{{ \\mathbf{{K}} }}}\\|_F}$ is small and $\\rank{({ \\mathbf{{H}} })} = p \\ll n$, but\n${\\|{{ \\mathbf{{H}} }}\\|_F}$ is not small.\nWe can still consider (\\ref{eq:NkhatDec}) for some\nappropriate $\\bar{d}$; however,\nsome of the (diagonal) coefficients of\n${\\widehat}{{ \\mathbf{{F}} }}$, $d_i - \\bar{d}$, will not be\nsmall, and ${ \\mathbf{{V}} } {\\widehat}{{ \\mathbf{{F}} }} { \\mathbf{{V}} }^{-1}$ will be\nthe sum of a matrix with small norm\nand a low rank matrix with (typically) larger norm.\nWriting\n\\begin{eqnarray*}\n  { \\mathbf{{N}} }_k = \\bar{d}{ \\mathbf{{I}} } + {\\widetilde}{{ \\mathbf{{N}} }},\n\\end{eqnarray*}\nwhere ${\\widetilde}{{ \\mathbf{{N}} }}$ has the same sparsity pattern as ${ \\mathbf{{N}} }_k$, gives\n\\begin{eqnarray}\\label{eq:bestApproxNTilde}\n { \\mathbf{{R}} }_k = { \\mathbf{{A}} }_k({ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k) = { \\mathbf{{A}} }_k( {\\widetilde}{{ \\mathbf{{N}} }} - { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}).\n\\end{eqnarray}\nFrom (\\ref{eq:bestApproxNTilde}) and Theorem \\ref{teo:mapBestApprox} follows\nthat ${\\widetilde}{{ \\mathbf{{N}} }}$ is the best approximation of ${ \\mathbf{{V{\\widehat}{F}V}} }^{-1}$ in the\nFrobenius ${ \\mathbf{{A}} }_k$-norm. Although a formal proof appears complicated,\nwe expect ${ \\mathbf{{R}} }_k$, and hence ${ \\mathbf{{R}} }_k { \\mathbf{{P}} }_0$, to also be the sum of a\nmatrix with small norm and a low rank matrix with (typically) larger norm.\nWe numerically verify this for the THT matrices below.\nFuture work will focus on the conditions under which we can prove this to be true.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyIdealMapEigs2.pdf}\n\t\t\\subcaption{Early}\n\t\t\\label{fig:Early}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{MiddleIdealMapEigs2.pdf}\n\t\t\\subcaption{Middle}\n\t\t\\label{fig:Middle}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateIdealMapEigs2.pdf}\n\t\t\\subcaption{Late}\n\t\t\\label{fig:Late}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the ideal map ${\\widehat}{{ \\mathbf{{N}} }}_k$ for selected shifts\n      (2-5, 10, 15, 20) of the THT matrices. Note the clustering for the first few\n      shifts for each sequence (early, middle, and late). Note the clustering with\n      relatively few outliers for the middle and late sequences ($n = 10\\,201$).}\n\t  \\label{fig:THT_eigs}\n\\end{center}\n\\end{figure}\nFigure \\ref{fig:THT_eigs} shows the eigenvalues of the ideal maps for the THT matrices.\\footnote{Figures \\ref{fig:THT_eigs} and \\ref{fig:THT_eigs_comp} show the eigenvalues of the maps beginning at shift two, since an ILUTP factorization of ${ \\mathbf{{A}} }_0$ is computed for the first shift and the SAMs are applied at subsequent shifts.}   Note that they are clustered for the first several shifts.  This corresponds to when the\nresidual of the SAMs is small, as shown in Figure \\ref{fig:residual}.  In Tables \\ref{table:THTearlySAMall} and \\ref{table:THTmiddleSAMall}, it can be seen that the number of GMRES iterations for these shifts is low.  Also in the case of the LS map, we observe clustering of the eigenvalues, as shown in Figure \\ref{fig:THT_eigs_comp}.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n\n\t\t\\includegraphics[width=\\linewidth]{EarlyCompMapEigs2.pdf}\n\t\t\\subcaption{Early}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{MiddleCompMapEigs2.pdf}\n\t\t\\subcaption{Middle}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateCompMapEigs2.pdf}\n\t\t\\subcaption{Late}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the LS map, ${ \\mathbf{{N}} }_k$, for selected shifts\n      (2-5, 10, 15, 20) of the THT Matrices. }\n\t  \\label{fig:THT_eigs_comp}\n\\end{center}\n\\end{figure}\n\nFigure \\ref{fig:THT_eigs} also shows that for the middle and late THT matrices, the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are mostly clustered with relatively few outliers.  We show the eigenvalues of\nthe ideal map for shifts 11 through 15 of the late THT matrices in Figure \\ref{fig:eig_vals_late}.\nExamining the $60$ largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for these same\nmatrices and shifts in Figure \\ref{fig:sing_vals_late},\nwe see that there are a few singular values larger than $1$ and only about ten larger than $0.5$.\\footnote{Our use of the word \"small\" is relative to Theorem \\ref{teo:smallNorm}.}\nThis shows that, for the THT matrices, it is reasonable to represent ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ as a small perturbation of a low rank matrix  (\\ref{eq:smallLowRank})\nwhen the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are clustered with few outliers.\n\nFigures \\ref{fig:eig_vals_early} and \\ref{fig:sing_vals_early} show that when the\neigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are not clustered, there are many more singular values\nof ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ larger than $1$.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateEigsClust.pdf}\n\t\t\\subcaption{Eigenvalues of the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$, for the late THT matrices for shifts 11 through 15.}\n\t\t\\label{fig:eig_vals_late}\n        \\end{subfigure}\n        \\hspace{2mm}\n        \\begin{subfigure}{.45\\textwidth}\n\t\t\\includegraphics[width=\\linewidth]{SingLate.pdf}\n\t\t\\subcaption{The 60 largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for the late THT matrices for shifts 11 through 15.}\n\t\t\\label{fig:sing_vals_late}\n        \\end{subfigure}\n\n         \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyEigsNotClust.pdf}\n\t\t\\subcaption{Eigenvalues of the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$, for the early THT matrices for shifts 16 through 20.}\n\t\t\\label{fig:eig_vals_early}\n        \\end{subfigure}\n        \\hspace{2mm}\n        \\begin{subfigure}{.45\\textwidth}\n\t\t\\includegraphics[width=\\linewidth]{SingEarly.pdf}\n\t\t\\subcaption{The 60 largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for the early THT matrices for shifts 16 through 20.}\n\t\t\\label{fig:sing_vals_early}\n        \\end{subfigure}\n\\end{center}\n\\caption{Comparison of the singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ with the eigenvalues of the late and early THT matrices when the eigenvalues are clustered with few outliers and when the eigenvalues are not clustered.}\n\\end{figure}\n\n\\section{Implementation}\\label{sec:impl}\nTo efficiently compute the SAM updates given a sparsity pattern $S$,\nthe solution of (\\ref{eq:introSAM}) must be implemented in sparse-sparse fashion.\nFurthermore, the nonzero pattern of the matrices\noften does not change, as is the case in the applications in this paper. Then the structure of the small least squares (LS) problems\nwill be the same for every update. This makes it efficient to setup\nthe data structures for the small LS problems just once, in advance.\n\n\nFor ease of notation, we drop the indices and consider the problem\n\\begin{eqnarray}\\label{eq:argSAM}\n{ \\mathbf{{N}} } = \\arg \\min_{{\\widetilde}{{ \\mathbf{{N}} }}\\in{\\mathcal S}}{\\|{{ \\mathbf{{A}} } {\\widetilde}{{ \\mathbf{{N}} }} - {\\widehat}{{ \\mathbf{{A}} }}}\\|_F}.\n\\end{eqnarray}\nGiven the pattern $S$, let $s_k$ be the set of indices of the (potential)\nnonzeros in column $k$ of ${ \\mathbf{{N}} }$: $s_k = \\{i \\,|\\, (i,k) \\in S\\}$\nand let\n${\\mathcal S}_k = \\{{ \\mathbf{{x}} } \\in {\\mathbb{C}^{n}} \\;|\\; { \\mathbf{{x}} }_i = 0 \\textrm{ if } i \\not\\in s_k \\}$.\nThus, for computing ${ \\mathbf{{n}} }_k$ (the $k$th column of ${ \\mathbf{{N}} }$) only the\ncolumns ${ \\mathbf{{a}} }_j$ with $j \\in s_k$ of ${ \\mathbf{{A}} }$ matter. These columns\nthemselves are sparse, and for the small LS problem defining\n${ \\mathbf{{n}} }_k$ we need only consider rows $i$ such that\n$a_{i,j} \\neq 0$ for some $j \\in s_k$.\nNote that if $\\hat{a}_{i,k} \\neq 0$ but $a_{i,j} = 0$\nfor all $j \\in s_k$, row $i$ is irrelevant for\ncomputing ${ \\mathbf{{n}} }_k$ since ${ \\mathbf{{e}} }_k \\perp\n{\\mathrm{Span}({\\{{ \\mathbf{{a}} }_j \\,|\\, j \\in s_k\\}})}$. However, if we wish\nto compute the residual (${ \\mathbf{{R}} } = { \\mathbf{{A}} } { \\mathbf{{N}} } - {\\widehat}{ { \\mathbf{{A}} } }$) or its norm,\nin addition to ${ \\mathbf{{N}} }$, we need to include such rows as well.\nIf the matrices ${ \\mathbf{{A}} }$ and ${\\widehat}{{ \\mathbf{{A}} }}$ have the same sparsity\npattern and the pattern of ${ \\mathbf{{N}} }$ includes at least the\ndiagonal, this is not an issue.\nLet $r_k$ be the set of indices of rows in ${ \\mathbf{{A}} }$ that\nare relevant for the $k$th small LS problem.\nThen the least squares problem for ${ \\mathbf{{n}} }_k$ is defined as\n\\begin{eqnarray*}\n  { \\mathbf{{n}} }_k & = & \\arg \\min_{{\\widetilde}{{ \\mathbf{{n}} }}_k \\in {\\mathcal S}_k }\n    \\|{ \\mathbf{{A}} }(r_k,s_k) {\\widetilde}{{ \\mathbf{{n}} }}_k(r_k) - {\\widehat}{{ \\mathbf{{A}} }}(r_k,k) \\|_2 ,\n\\end{eqnarray*}\nwhere ${ \\mathbf{{A}} }(r_k,s_k)$ is the block submatrix of ${ \\mathbf{{A}} }$ indexed\nby $r_k \\times s_k$, ${\\widetilde}{{ \\mathbf{{n}} }}_k(r_k)$\nis the subvector of ${\\widetilde}{{ \\mathbf{{n}} }}_k$ indexed by $r_k$,\nand ${\\widehat}{{ \\mathbf{{A}} }}(r_k,k)$ is the corresponding\nsubvector of the $k$th columns of ${\\widehat}{{ \\mathbf{{A}} }}$.\n\nWe preprocess the matrices or their\nsparsity patterns to be able to efficiently select\nthe relevant rows, $r_k$, and columns, $s_k$, of the small LS problem\nfor each column $k$. It may also be efficient to (only once)\nallocate memory space for solving these least squares problems.\nThis can be a single memory allocation sufficiently large\nfor each of the problems or multiple allocations to\nallow for parallelism.\nFor matrices that derive from some discretization,\nthe size of these least squares problems typically depends only on the\nsparsity pattern, not on the size of the matrix.\nSo, while $n$ may be large, each of the least squares problems solved\nis very small (and most are about the same size).\nFor example, the average size of these least squares problems for the\nTHT matrices is $18\\times 7$.\nIn general, the matrices or the underlying problems have structure\nthat should be exploited. For example, if a matrix derives from\ndiscretization on some mesh or grid, finding the nonzero\npatterns of powers of the matrix can be done very\nefficiently using the information defining the mesh or grid.\n\n\nFinally, it\nis essential to store the matrices in an appropriate\n(sparse) format and to generate ${ \\mathbf{{N}} }$ in an appropriate format.\nFor example, in MATLAB{\\textsuperscript{\\textregistered}} it is inefficient to generate\na sparse matrix one column at a time, even if the\ntotal space is allocated in advance (presumably because of the\nrequired manipulation of sparse matrix data structures\nfor each column). Therefore,\nwe generate ${ \\mathbf{{N}} }$ first in coordinate format (COO) \\cite{Saad09,Saad03},\nand after the whole matrix has been computed we\nconvert this temporary data structure into a MATLAB{\\textsuperscript{\\textregistered}}\nsparse matrix using the command {\\tt sparse}.\nIn Algorithm~\\ref{alg:SAMpre}, the statement\n$t = \\texttt{find}({ \\mathbf{{a}} }_j)$ (with reference to the MATLAB{\\textsuperscript{\\textregistered}}\ncommand {\\tt find}), it is important for efficiency\nthat ${ \\mathbf{{A}} }$ is stored as a sparse matrix, and that its\ncolumns are easily accessible.\n\n\nThe algorithm for preprocessing is given in\nAlgorithm~\\ref{alg:SAMpre}; the algorithm for\ncomputing the SAM itself is given in Algorithm \\ref{alg:SAM}.\n\n\\begin{algorithm}\n  \\caption{Preprocessing for Computing Sparse Approximate Maps }\n\t\\label{alg:SAMpre}\n  \\begin{algorithmic}\n  \\State Given sparsity pattern $S$, and matrix ${ \\mathbf{{A}} }$\n  \\State $maxSk = 0$; $maxRk = 0$; \\Comment{initialize max num of columns, max num of rows}\n  \\For{$k = 1:n$} \\Comment{for each column do}\n\t\\State $s_k = \\{i \\,|\\, (i,k) \\in S\\}$ \\Comment{get indices; typically defined in advance}\n    \\State $r_k = \\emptyset$ \\Comment{Initialize set of rows for $k$th LS problem}\n    \\ForAll {$j \\in s_k$}\n      \\State $t = \\texttt{find}({ \\mathbf{{a}} }_j)$ \\Comment{find indices of nonzeros in column ${ \\mathbf{{a}} }_j$}\n\t  \\State $r_k = r_k \\cup t$\n    \\EndFor\n    \\State $nnz_k = \\#( s_k )$ \\Comment{\\#() gives number of elements in a set}\n    \\If {$nnz_k > maxSk$}\n      \\State $maxSk = nnz_k$\n    \\EndIf\n    \\If {$\\#( r_k ) > maxRk$}\n      \\State $maxRk = \\#( r_k )$\n    \\EndIf\n  \\EndFor\n  \\State Allocate $maxRk \\times maxSk$ array for storing the LS matrices,\n    $maxRk$ vector for storing the right hand side, and $maxSk$ vector for\n    storing the solution.\n  \\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n  \\caption{Computing ${ \\mathbf{{N}} } = \\arg \\min_{{\\widetilde}{{ \\mathbf{{N}} }} \\in {\\mathcal S}}{\\|{{ \\mathbf{{A}} }{\\widetilde}{{ \\mathbf{{N}} }} - {\\widehat}{{ \\mathbf{{A}} }}}\\|_F}$}\n\t\\label{alg:SAM}\n  \\begin{algorithmic}\n  \\State $cnt = 0$ \\Comment{counts number of nonzeros in preconditioner}\n  \\For{$k = 1:n$}\n\t\\State ${ \\mathbf{{A}} }_\\mathrm{tmp} = { \\mathbf{{A}} }(r_k,s_k)$ \\Comment{get submatrix indexed by $r_k$ and $s_k$ for LS problem}\n\t\\State ${ \\mathbf{{f}} } = {\\widehat}{{ \\mathbf{{A}} }}(r_k)$ \\Comment{get rhs for LS problem}\n    \\State Solve LS ${ \\mathbf{{A}} }_\\textrm{tmp} { \\mathbf{{z}} } = { \\mathbf{{f}} }$\n\t\\State (possibly save residual, norm of residual, etc.)\n    \\State $rowN[cnt+1:cnt+nnz_k] = s_k$ \\Comment{assign indices in the order of values in ${ \\mathbf{{z}} }$}\n\t\\State $colN[cnt+1:cnt+nnz_k] = j$\n\t\\State $valN[cnt+1:cnt+nnz_k] = { \\mathbf{{z}} }$\n\t\\EndFor\n\t\\State ${ \\mathbf{{N}} } = $ sparse$(rowN,colN,valN)$ \\Comment{convert into sparse matrix}\n  \\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\\comment{\nWe refer to ${ \\mathbf{{S}} }$ as the matrix representation of $S$.  Some preprocessing should be done in order to make the computation of the map as cheap as possible, in particular, determining the indices and number of nonzeros in each column of the map as well as in each column of the least squares problem ${\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} }-{ \\mathbf{{A}} }_0}\\|_F}$.  For example, in each of our applications, ${ \\mathbf{{A}} }_0$ and ${ \\mathbf{{A}} }_k$ have the same nonzero pattern, which may not always be the case.  When we choose the sparsity pattern to be that of ${ \\mathbf{{A}} }_0$, we can use Matlab's `find' function to efficiently determine the nonzero pattern in each column of ${ \\mathbf{{A}} }_0$ and in each column of ${ \\mathbf{{A}} }_0^2$, the latter representing the nonzero pattern of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }$.\n\nThe user should also take advantage of the structure of the matrix, if possible.  For instance, when dealing with a mesh, the pattern of ${ \\mathbf{{A}} }_0$ (and powers of ${ \\mathbf{{A}} }_0$) can be easily determined prior to computing the map.  While our matrices are not defined by a mesh, when taking the sparsity pattern to be the nonzero pattern of ${ \\mathbf{{A}} }_0$ or ${ \\mathbf{{A}} }_0^2$, we are dealing with very sparse matrices.  Any preprocessing that can be done to take advantage of the sparsity and/or structure of the matrix should be performed.\n\nPreprocessing allows us to compute the sparse approximate map by solving $n$ small least squares problems,  where $n$ is the size of the matrix.  This is represented in the for-loop of Algorithm \\ref{alg:SAM}.  Note the size of these least squares problems depends on the sparsity pattern, and is  therefore independent of the size of the matrix.  While $n$ may be large, each of the least squares problems solved is very small and more or less constant.  For example, the average size of the least squares problems computed for the THT matrices is $18\\times 7$ and is easily computed using Matlab's `backslash' operator.\n\nSince our implementation is done in Matlab,  we take advantage of Matlab's sparse matrix functionality when solving each of the small least squares problems.  However, building a sparse matrix as we solve each of these problems is very expensive - potentially $O(n^2)$ work.  Therefore, we store values in compressed sparse row (CSR) format within the loop, using the `sparse' command at the very end to build the sparse approximate map.  Clearly, this will be done differently depending on the language used.\n\n}\n\n\n\\section{Numerical Experiments} \\label{sec:results}\nWe apply the strategies of reusing and recycling preconditioners to several applications, focusing both on total computation time as well as total GMRES iterations. We define total computation time to be the time to compute the preconditioner or SAM update plus the time for GMRES to converge for all shifts. We also report the times for the computation of individual preconditioners and SAMs as well as the number of iterations and runtime per system solve. These numbers provide good insight into the merits of reusing a preconditioner, possibly including a previous SAM update, computing a new preconditioner, or computing a new SAM update. Of course, the actual costs of these computations, the number of iterations saved, and the cost per iteration are all problem dependent. We compare the results of computing a new ILUTP preconditioner for each shift, reusing the initial ${ \\mathbf{{P}} }_0$ for all shifts, updating ${ \\mathbf{{P}} }_0$ with a new SAM update for all shifts, and updating ${ \\mathbf{{P}} }_0$ with a SAM update only at selected shifts. The first approach, a new ILUTP preconditioner for every shift, is always the most expensive option in runtime, but it provides a useful benchmark in terms of the number of iterations. The last approach, to compute a SAM update only at selected shifts, is usually the winner in runtime. The exceptions are the linear systems for the late THT matrices and the matrices from the\nmodel reduction test problem Rail. For these problems reusing the initial ILUTP for all systems leads\nto the lowest runtime. For brevity, we do not provide data for recomputing\nthe ILUTP at selected shifts. This, of course, can be better than computing\nthe ILUTP for all shifts, but it was never the fastest - this can easily be\nderived directly from the high cost of computing the preconditioner.\n\nFinally, for longer sequences of systems and other problems, many other variations of\ncomputing preconditioners and updates may be effective. Although we experimented with\nseveral {\\em indicator functions} to decide when to do\na SAM update and the results were encouraging, we did not find a\nsingle good indicator. Hence we leave a further analysis and discussion of these\nfor future work. A simple and effective strategy is to compute a new SAM or\npreconditioner based on (1) the time for this computation and (2)\nthe (relative) increase in the number\nof iterations or the solution time for a single system.\n\nFor the THT matrices, we also compare with the AINV preconditioner\nand update. The algorithms for calculating both the AINV preconditioner as well as the AINV updates can be found in \\cite{BellBert11,BenzBert03,BenzCull00,BenzHaws00,BenzKouh01,BenzTuma98,Rafi14}.  The implementation of the ILUTP preconditioner is computed using an implementation based on that in \\cite{Saad09}.\n\n\n\\subsection{Transient Hydraulic Tomography\\protect\\footnote{We would like to thank to Tania Bakhos, Arvind Saibaba, and Peter Kitanidis for providing the description of THT as well as the matrices used.}}\\label{sec:THT}\nTransient Hydraulic Tomography (THT) is a method for imaging the earth's subsurface (see \\cite{CardBarr11} for a detailed description of THT). Water is pumped at a constant rate in pumping wells and the measured drawdown curves of pressure response at the observation wells is recorded. A subset of this data is used in a nonlinear inversion to recover the parameters of interest, namely hydraulic conductivity and specific storage.  In the example described later in this section, we choose three key time points (corresponding to {\\em early}, {\\em middle}, and {\\em late} times).  The governing equations of groundwater flow through an aquifer with domain $\\Omega$ are given by,\n\n\n", "itemtype": "equation", "pos": 27543, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nPreconditioners are generally essential for fast convergence in the iterative solution\nof linear systems of equations. However, the computation of a good preconditioner\ncan be expensive. So, while solving a sequence of many linear systems, it\nis advantageous to recycle preconditioners, that is, update a previous\npreconditioner and reuse the updated version. In this paper, we introduce a\nsimple and effective method for doing this. Although our approach can be used\nfor matrices changing slowly in any way, we focus on the important\ncase of sequences\nof the type $(s_k{ \\mathbf{{E}} }({ \\mathbf{{p}} }) + { \\mathbf{{A}} }({ \\mathbf{{p}} })){ \\mathbf{{x}} }_k = { \\mathbf{{b}} }_k$, where the right hand side\nmay or may not change. More general changes in matrices will be discussed\nin a future paper.\n\nWe update preconditioners by defining a map from a new matrix to a\nprevious matrix, for example the first matrix in the sequence,\nand combine the preconditioner for this previous matrix with the map\nto define the new preconditioner. This approach has several advantages.\n{\\em The update is entirely independent from the original preconditioner, so\nit can be applied to any preconditioner.}\nThe possibly high cost of an\ninitial preconditioner can be amortized over many linear solves.\nThe cost of updating the preconditioner is more or less constant and\nindependent of the original preconditioner. There is flexibility in balancing\nthe quality of the map with the computational cost.\n\nIn the numerical experiments section we demonstrate good results for several\napplications.\n\\end{abstract}\n\n\\begin{keywords}\nPreconditioning, Updating Preconditioners, Krylov Subspace Methods, Sparse Approximate Inverse, Parameterized Systems, Model Reduction, Transient Hydraulic Tomography, Diffuse Optical Tomography\n\\end{keywords}\n\n\\begin{AMS}\n65F10\n\\end{AMS}\n\n\\pagestyle{myheadings} \\thispagestyle{plain} \\markboth{A. Grim-McNally, E. de Sturler,\nand S. Gugercin}{Preconditioning Parameterized Linear Systems}\n\n\n\\section{Introduction}\\label{sec:intro}\nWe consider the efficient computation of preconditioners for sequences of\nsystems which change slowly.  Such sequences can often be represented as\n\\begin{eqnarray}\\label{eq:SeqShiftSys}\n(s_k{ \\mathbf{{E}} }({ \\mathbf{{p}} }) + { \\mathbf{{A}} }({ \\mathbf{{p}} })){ \\mathbf{{x}} }_k = { \\mathbf{{b}} }_k ,\n\\end{eqnarray}\nwhere the right hand side may or may not change.  Here, $s_k$ is a shift\n(often related to a frequency), and the matrices ${ \\mathbf{{E}} }$ and ${ \\mathbf{{A}} }$ are\nfunctions of a parameter vector ${ \\mathbf{{p}} }$. For example, this is implicitly\nthe case for the Transient Hydraulic Tomography application (THT)\ndiscussed later in this paper and for diffuse optical\ntomography \\cite{deStGuge15}.\nPreconditioners are often essential for fast iterative solutions\nof linear systems of equations, but the computation of a good preconditioner\ncan be expensive. Therefore, we consider {\\em recycling preconditioners},\nthat is, updating a previous\npreconditioner and reusing the updated version for solving a new linear\nsystem.\nFor a sequence of linear systems, this may provide a substantial reduction\nin cost compared with computing a new preconditioner for each system or\nperiodically computing a new preconditioner from scratch.\nThe latter approach includes the important case of solving all systems with a single preconditioner, which we refer to as {\\em reusing} the initial preconditioner.\n\nThe main idea underlying our approach comes from \\cite{AhujClar11}.\nGiven a sequence of matrices, ${ \\mathbf{{A}} }_k$, for $k = 0, 1, 2, \\ldots$, and\na good preconditioner ${ \\mathbf{{P}} }_0$ for ${ \\mathbf{{A}} }_0$ such that\n${ \\mathbf{{A}} }_0 { \\mathbf{{P}} }_0$ (or ${ \\mathbf{{P}} }_0 { \\mathbf{{A}} }_0$) yields fast convergence, we could\ncompute for each system the {\\em ideal\nmap} ${\\widehat}{{ \\mathbf{{N}} }}_k$ such that\n\\begin{eqnarray}\\label{eq:map}\n  { \\mathbf{{A}} }_k {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{A}} }_0.\n\\end{eqnarray}\nIf we define the updated preconditioner as\n\\begin{eqnarray}\\label{eq:updP0}\n  { \\mathbf{{P}} }_k = {\\widehat}{{ \\mathbf{{N}} }}_k{ \\mathbf{{P}} }_0,\n\\end{eqnarray}\nthen, as a result, ${ \\mathbf{{A}} }_0{ \\mathbf{{P}} }_0 = { \\mathbf{{A}} }_1{ \\mathbf{{P}} }_1 = \\dots = { \\mathbf{{A}} }_k{ \\mathbf{{P}} }_k$.\nTherefore, ${ \\mathbf{{A}} }_k {\\widehat}{{ \\mathbf{{N}} }}_k { \\mathbf{{P}} }_0 = { \\mathbf{{A}} }_0 { \\mathbf{{P}} }_0$ will yield\nthe same fast convergence as\nthe original preconditioned system, for each $k$.\nNote that (in general) the matrix ${\\widehat}{{ \\mathbf{{N}} }}_k { \\mathbf{{P}} }_0$ is never\ncomputed; in an iterative method, we can just multiply vectors\nsuccessively by these two matrices\n(which does lead to some overhead).\nIf computing these maps\ncan be made cheap and the initial preconditioner is very good,\nwe obtain fast convergence for all systems at low cost.\n\nIn \\cite{AhujClar11}, we dealt with a very long sequence of matrices in a Markov chain Monte Carlo (MCMC) process.  These matrices change by one row at a time, ${ \\mathbf{{A}} }_{k+1} = { \\mathbf{{A}} }_k + { \\mathbf{{e}} }_{i_k}{ \\mathbf{{u}} }_k^T$,\nwhere $i_k$ indicates which row changes and ${ \\mathbf{{u}} }_k$ is the change\nin the row. In this case, computing the ideal map comes more or\nless for free, as we already need to compute\n${ \\mathbf{{u}} }_k^T { \\mathbf{{A}} }^{-1}_k { \\mathbf{{e}} }_{i_k}$ for the transition probability\nin the MCMC process.  While this update is specific to the particular\napplication, the approach proposed in this paper generalizes\nthe idea of recycling preconditioners to any set of closely related matrices.\n\nOur preconditioner update is advantageous in several ways.\nTo compute the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$, or an approximation,\nknowledge of the original preconditioner, ${ \\mathbf{{P}} }_0$, is not required.\nTherefore, the map is independent of ${ \\mathbf{{P}} }_0$ and can be applied\nto any type of preconditioner. Further, the cost of updating ${ \\mathbf{{P}} }_0$\nis more or less constant and the potentially high cost of computing a good ${ \\mathbf{{P}} }_0$ can be amortized over many linear solves.  In practice, we do not\ncalculate the ideal map as in (\\ref{eq:map}), but rather\nan approximation, ${ \\mathbf{{N}} }_k$ such that\n\\begin{eqnarray*} \n  { \\mathbf{{A}} }_k { \\mathbf{{N}} }_k \\approx { \\mathbf{{A}} }_0.\n\\end{eqnarray*}\nDepending on how good we want our approximation to be,\nthere is also flexibility in balancing the quality of ${ \\mathbf{{N}} }_k$\nwith the cost of computing it.  While there is the additional\ncost of applying the map in the matrix-vector product, usually this\ndoes not outweigh the cost of computing another preconditioner from scratch.\nWe demonstrate this for several applications in Section~\\ref{sec:results}.\n\nOur update scheme is motivated by the Sparse Approximate Inverse (SAI),\nand so we refer to it as a Sparse Approximate Map, or SAM update.\nThe SAI was proposed in \\cite{Bens73} and further developed in \\cite{BensFred82, ChowSaad98,GrotHuck97, HollWath05} and references therein.\nTo define SAIs and SAMs we need the following definitions.\n\\begin{definition} \\label{defn:pattern}\nA sparsity pattern for ${\\mathbb{C}^{n \\times n}}$ is any subset of $\\{1,2,\\dots,n\\}\\times\\{1,2,\\dots,n\\}$.\n\\end{definition}\n\n\\begin{definition} \\label{defn:subSpace}\nLet $S$ be a sparsity pattern for ${\\mathbb{C}^{n \\times n}}$.  We define the subspace ${\\mathcal S} \\subseteq {\\mathbb{C}^{n \\times n}}$ as ${\\mathcal S} = \\{X\\in {\\mathbb{C}^{n \\times n}}$ $|$ $X_{ij} = 0$ if $(i,j) \\not \\in S\\}$.\n\\end{definition}\n\\begin{definition}\nFor ${ \\mathbf{{P}} }, { \\mathbf{{A}} } \\in {\\mathbb{C}^{n \\times n}}$ and ${ \\mathbf{{I}} }$ the identity matrix in ${\\mathbb{C}^{n \\times n}}$, the Sparse Approximate Inverse, ${ \\mathbf{{P}} }$, for a matrix, ${ \\mathbf{{A}} }$, is defined as the minimizer of\n\\begin{eqnarray}\\label{eq:SAI}\n\\min_{{ \\mathbf{{P}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{I}} }-{ \\mathbf{{AP}} }}\\|_F}.\n\\end{eqnarray}\n\\end{definition}\n\n\nThe computation of a SAI is easily parallelized as $n$ independent\nsmall least squares problems, as discussed in \\cite{GrotHuck97}.\nWhile our preconditioner update (or SAM) can also be computed in\nparallel, we do not discuss this here.\n\nRather than considering the identity matrix in (\\ref{eq:SAI}),\nother work has focused on replacing it with another matrix, sometimes referred to as a target matrix \\cite{HollWath05}.  The problem then becomes\n\\begin{eqnarray}\\label{eq:targetSAI}\n  \\min_{{ \\mathbf{{P}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{B}} }-{ \\mathbf{{AP}} }}\\|_F},\n\\end{eqnarray}\nwhere ${ \\mathbf{{P}} }$ is such that ${ \\mathbf{{AP}} }$ targets ${ \\mathbf{{B}} }$.  In \\cite{ChowSaad98, HollWath05}, (\\ref{eq:targetSAI}) is solved in order to improve a\npreconditioner, ${ \\mathbf{{B}} }^{-1}$, such that the preconditioned\nsystem ${ \\mathbf{{APB}} }^{-1}$ is closer to the identity matrix\nthan ${ \\mathbf{{A}} }{ \\mathbf{{B}} }^{-1}$.  As a preconditioner, ${ \\mathbf{{B}} }^{-1}$ is generally\navailable through an approximate factorization of ${ \\mathbf{{A}} }$ (or of ${ \\mathbf{{A}} }^{-1}$).\nHowever, the columns of ${ \\mathbf{{B}} }$ must be computed in order to solve (\\ref{eq:targetSAI}),\nand the cost of constructing these columns can be relatively high.\nIn special cases, the structure or type of matrix can be exploited.\nIn an example using the advection-diffusion equation and\ntargeting the Laplacian, the authors in \\cite{HollWath05} are able\nto use a fast solver for the action of ${ \\mathbf{{B}} }^{-1}$ with good results.\nIn order to reduce the cost of explicitly constructing ${ \\mathbf{{B}} }$, iterative methods with numerical dropping are used to approximate the columns of ${ \\mathbf{{B}} }^{-1}$ in \\cite{ChowSaad98}.\n\nOur update scheme involves solving\n\\begin{eqnarray}\\label{eq:introSAM}\n  { \\mathbf{{N}} }_k & = & \\arg \\min_{{ \\mathbf{{N}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} } - { \\mathbf{{A}} }_0}\\|_F},\n\\end{eqnarray}\nwhere ${\\mathcal S}$ is the subspace defined by a chosen sparsity pattern $S$,\nas given in Definition \\ref{defn:subSpace}, and ${ \\mathbf{{A}} }_0$ and ${ \\mathbf{{A}} }_k$\nare matrices from a given sequence.\nThis paper focuses on solving (\\ref{eq:introSAM}) for each\nor selected $k$, but we can also\nincrementally apply such a map where, at shift $k$, we solve\n\\begin{eqnarray*}\n  { \\mathbf{{N}} }_k & = & \\arg \\min_{{ \\mathbf{{N}} }\\in{\\mathcal S}} \\|{ \\mathbf{{A}} }_k{ \\mathbf{{N}} } - { \\mathbf{{A}} }_{k-1}\\|_F\n\\end{eqnarray*}\nand define\n\\begin{eqnarray} \\label{eq:incrementSAM2}\n  { \\mathbf{{P}} }_k & = & { \\mathbf{{N}} }_k { \\mathbf{{P}} }_{k-1} = { \\mathbf{{N}} }_k{ \\mathbf{{N}} }_{k-1}\\cdots { \\mathbf{{N}} }_{1}{ \\mathbf{{P}} }_0 ,\n\\end{eqnarray}\nor let\n\\begin{eqnarray*}\n  { \\mathbf{{N}} }_k & = & \\arg \\min_{{ \\mathbf{{N}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} } - { \\mathbf{{A}} }_j}\\|_F},\n\\end{eqnarray*}\nwith\n\\begin{eqnarray}\n\\label{eq:jkSAM2}\n  { \\mathbf{{P}} }_k & = & { \\mathbf{{N}} }_k { \\mathbf{{P}} }_j ,\n\\end{eqnarray}\nfor some $j$ such that $0 < j < k$.  Applying the maps such as in (\\ref{eq:incrementSAM2}) and (\\ref{eq:jkSAM2}) is the focus of future research.\n\nWhile the minimization in (\\ref{eq:introSAM}) has a form very similar to (\\ref{eq:targetSAI}), there are fundamental differences.  Computing (\\ref{eq:targetSAI}) involves improving an existing preconditioner, ${ \\mathbf{{B}} }$, for a fixed matrix, ${ \\mathbf{{A}} }$, where for most preconditioners, ${\\|{{ \\mathbf{{B}} }-{ \\mathbf{{A}} }}\\|_F}$ is quite large, and so\nan accurate solution cannot be expected. Of course, if an accurate\nsolution would be obtained, the benefit would be\nfaster convergence rather than maintaining the same convergence.\nOur approach seeks to map one matrix to another\n{\\em closely related one}, so we often\ncan expect a relatively accurate solution.\nThe high cost of computing the columns of ${ \\mathbf{{B}} }$ when solving (\\ref{eq:targetSAI})\nis avoided when solving (\\ref{eq:introSAM}),\nsince the columns of ${ \\mathbf{{A}} }_0$ are readily available,\nas ${ \\mathbf{{A}} }_0$ is a previous matrix in the sequence of linear systems.\n\nOther update schemes for sequences of matrices have been proposed.  A cheap update to the factorized approximate inverse (AINV) preconditioner is discussed in \\cite{BenzBert03}.  However, this update requires that ${ \\mathbf{{P}} }_0$ is itself of AINV type.\nSeveral incremental, or iterative, update techniques to an ILU factorization\nare described in \\cite{CalgCheh10}.  But again, these updates require the initial preconditioner to be itself an ILU preconditioner.\nMoreover, these update techniques seem relatively expensive; they were\nnot competitive for the problems we considered.\n\nAlthough the SAM updates discussed in this paper can be used for any\nset of closely related matrices,\nhere we focus on shifted matrices of the form\n\\begin{eqnarray}\\label{eq:shiftMat}\n  { \\mathbf{{A}} }_k = s_k{ \\mathbf{{E}} } + { \\mathbf{{A}} } , \\,\\footnotemark\n\\end{eqnarray}\nwhich arise in model reduction \\cite{AnthBeat10,BeatGuge09,BeatGuge14,GugeAnth04,GugeAnth08},\noscillatory and transient hydraulic tomography (OHT/THT)  \\cite{CardBarr11},\nand diffuse optical tomography (DOT) \\cite{AghaKilm11,deStKilm11,KilmdeSt06,SaibBakh13}.\n\\footnotetext{Analogous results are obtained\nfor ${ \\mathbf{{A}} }_k = s_k{ \\mathbf{{E}} } - { \\mathbf{{A}} } = s_k{ \\mathbf{{E}} } + {\\widetilde}{{ \\mathbf{{A}} }}$ with ${\\widetilde}{{ \\mathbf{{A}} }}=-{ \\mathbf{{A}} }$.}\nNote that for a parameterized medium (subsurface or tissue) the\nlatter two problems result in sequences of type (\\ref{eq:SeqShiftSys}).\nWhile not explicitly considered in this paper, other work has focused on shifted systems where ${ \\mathbf{{E}} } = { \\mathbf{{I}} }$.  Flexible preconditioning is used for problems of this form in \\cite{BaumVanG15,GuZhou07}.  In \\cite{AhmaSzyl15}, the authors take\nadvantage of the form of the shifted systems in certain model\nreduction applications and the shift invariance of Krylov subspaces.\nIn Section~\\ref{sec:results}, we demonstrate the effectiveness of\nSAM updates to applications from THT and model reduction.\nFor THT, we consider three sequences of\nmatrices, referred to as {\\em early}, {\\em middle}, and {\\em late}, where\neach sequence includes twenty shifts.\nFor model reduction, we examine two applications, Rail and Flow,\nwith multiple sequences of six shifts each.\nWe also demonstrate the effectiveness of our approach\nfor a discretization of the Helmholtz equation\n$f = -{\\Delta} u - k^2 u$, where $u$ is an amplitude,\n$\\Delta$ is the Laplacian, and $k$ is the wave number.  Indefinite systems, such as the discretized Helmholtz equation, can also arise in flow control where the systems can be unstable, resulting in eigenvalues that are in both the right- and left-half planes \\cite{BorgGuge14}.\n\n\nThe shifts in these sequences can be real, as with the\nRail and Flow matrices, or complex, as with the THT matrices.\nOften, the magnitude of the shifts is not large, and the\nshifted matrices are closely related.  However, in model\nreduction applications the magnitude of the shifts can be quite large.\nMore detail for each application will be provided in Section \\ref{sec:results}.\n\nIn Section~\\ref{sec:theo}, we analyze SAM preconditioner updates\nand their effect on the convergence of GMRES.\nIn Section \\ref{sec:impl}, we discuss how to\nimplement SAMs.\nWe present the results of the SAM preconditioner updates for\nthe applications described above in Section~\\ref{sec:results}.\nFinally, we provide some conclusions in Section~\\ref{sec:concl}.\n\n\n\n\n\n\n\n\n\\section{Analysis of Sparse Approximate Maps and their\nEffect on Convergence}\\label{sec:theo}\nWe assume that our matrices take the form\n(\\ref{eq:shiftMat}). We define the ideal map,\n${\\widehat}{{ \\mathbf{{N}} }}_k$, such that (\\ref{eq:map}) holds.\nAs we consider applications in which a sequence of\nlinear systems must be solved, we assume that\nthe matrices ${ \\mathbf{{A}} }_k$ are invertible for all $k$.\nHence, we can also write ${\\widehat}{{ \\mathbf{{N}} }}_k$ as\n\\begin{eqnarray} \\label{eq:idealMapwithInv}\n  {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{A}} }_k^{-1}{ \\mathbf{{A}} }_0 ,\n\\end{eqnarray}\nand, clearly, ${\\widehat}{{ \\mathbf{{N}} }}_k$ is invertible (for all $k$).\nFor a given subspace, ${\\mathcal S}$, as defined in Definition~\\ref{defn:subSpace},\nthe {\\em least squares (LS) map}, ${ \\mathbf{{N}} }_k$,\nis the solution to (\\ref{eq:introSAM}).\nWe define ${ \\mathbf{{R}} }_k$, the residual of the LS map at shift $k$, as\n\\begin{eqnarray} \\label{eq:res}\n  { \\mathbf{{R}} }_k = { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k - { \\mathbf{{A}} }_0.\n\\end{eqnarray}\n\nWe would like to analyze the convergence of GMRES\nwhen using these maps. The following theorem\nbounds the convergence of GMRES for matrices that can\nbe expressed as a small (in norm) perturbation of the identity.\nThis result is well known and given here for ease of reference.\n\\begin{theorem}\\label{teo:smallNorm}\nLet ${ \\mathbf{{I}} }, { \\mathbf{{C}} } \\in {\\mathbb{C}^{n \\times n}}$ and ${ \\mathbf{{A}} } = { \\mathbf{{I}} } + { \\mathbf{{C}} }$,\nwhere ${ \\mathbf{{I}} }$ denotes the identity and ${\\|{C}\\|_2} \\leq {\\rho} < 1$.\nLet ${ \\mathbf{{r}} }_m = { \\mathbf{{b}} }-{ \\mathbf{{A}} }{ \\mathbf{{x}} }_m$ be the GMRES residual,\nwith ${ \\mathbf{{x}} }_m\\in {\\mathcal K}^m({ \\mathbf{{A}} };{ \\mathbf{{r}} }_0) =\n{\\mathrm{Span}({ \\{{ \\mathbf{{r}} }_0, { \\mathbf{{A}} }{ \\mathbf{{r}} }_0,\\dots,{ \\mathbf{{A}} }^{m-1}{ \\mathbf{{r}} }_0\\}})}$.\nThen ${\\|{{ \\mathbf{{r}} }_m}\\|_2} \\leq {\\rho}^m{\\|{{ \\mathbf{{r}} }_0}\\|_2}$, and ${\\rho}_m \\rightarrow 0$ for $m \\rightarrow \\infty$.\n\\end{theorem}\n\\begin{proof}\nWe refer to \\cite{CampIpse96,GmatPhil08,MingRao15} for a proof of this theorem and related discussion.\n\\end{proof}\n\nNote that in exact arithmetic ${ \\mathbf{{r}} }_n = 0$, but in practice $n$ is very large,\nand we are interested in good convergences for $m \\ll n$.  \nClearly, in Theorem \\ref{teo:smallNorm}, the smaller ${\\rho}$ is\n($0 \\leq {\\rho} <1$), the faster GMRES will converge.\nHowever, ${\\rho}$ need not be very small in order for GMRES to converge\nrapidly (consider, for example, ${\\rho} = 1/3$).  When the residual ${ \\mathbf{{R}} }_k$ is sufficiently small\nand the initial preconditioner is good,\nwe can use Theorem \\ref{teo:smallNorm} to guarantee rapid\nconvergence of GMRES for the preconditioned matrix\n${ \\mathbf{{A}} }_k{ \\mathbf{{P}} }_k = { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0$.  Since ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k = { \\mathbf{{A}} }_0+{ \\mathbf{{R}} }_k$, we have\n\\begin{eqnarray} \\label{eq:ResPrec}\n  { \\mathbf{{A}} }_k{ \\mathbf{{P}} }_k & = & { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 = { \\mathbf{{A}} }_0{ \\mathbf{{P}} }_0+{ \\mathbf{{R}} }_k{ \\mathbf{{P}} }_0.\n\\end{eqnarray}\nWe assume that the initial preconditioner ${ \\mathbf{{P}} }_0$ is\na good approximation to ${ \\mathbf{{A}} }_0^{-1}$ such that\n${ \\mathbf{{A}} }_0{ \\mathbf{{P}} }_0 = { \\mathbf{{I}} } + { \\mathbf{{K}} }$, with ${\\|{{ \\mathbf{{K}} }}\\|_2} \\leq {\\delta} < 1$.\nFrom (\\ref{eq:ResPrec}), we get\n\\begin{eqnarray}\\label{eq:CorrP}\n  { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 = { \\mathbf{{I}} } + { \\mathbf{{K}} } +{ \\mathbf{{R}} }_k{ \\mathbf{{P}} }_0.\n\\end{eqnarray}\n\\begin{corollary} \\label{cor:residual}\nLet the preconditioned system ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0$ be as in (\\ref{eq:CorrP})\nwith with ${ \\mathbf{{K}} }$ as above.\nThen GMRES will converge if\n${\\|{{ \\mathbf{{R}} }_k { \\mathbf{{P}} }_0}\\|_2} < (1-{\\delta})$ or\n${\\|{{ \\mathbf{{R}} }_k}\\|_2} < (1-{\\delta}){\\|{{ \\mathbf{{P}} }_0}\\|_2}^{-1}$.\n\\end{corollary}\n\\vspace{3mm}\n\n\nFrom (\\ref{eq:ResPrec}) we see that\n${ \\mathbf{{R}} }_k { \\mathbf{{P}} }_0 = { \\mathbf{{A}} }_k { \\mathbf{{P}} }_k - { \\mathbf{{A}} }_0 { \\mathbf{{P}} }_0$ represents the\n`deterioration' of the preconditioned system from\nthe original preconditioned system.\n\nIn Section \\ref{sec:impl} we show how to compute SAMs\nefficiently, and in Section~\\ref{sec:results} we show\nthat these maps are (relatively) cheap to compute.\nTherefore, computing a very good preconditioner such that we\nsatisfy the assumptions of Corollary \\ref{cor:residual} is reasonable.\n\nWhile our convergence results have been defined in terms\nof ${\\|{\\cdot}\\|_2}$, we note that ${\\|{{ \\mathbf{{R}} }_k}\\|_2} \\leq {\\|{{ \\mathbf{{R}} }_k}\\|_F}$,\nwhich is available more or less for free\nwhile computing (\\ref{eq:introSAM}).\n${\\|{{ \\mathbf{{P}} }_0}\\|_F}$ (or ${\\|{{ \\mathbf{{P}} }_0}\\|_2}$) can also be estimated. If ${ \\mathbf{{P}} }_0$\nis a sparse approximate inverse itself, then computing ${\\|{{ \\mathbf{{P}} }_0}\\|_F}$\nis trivial. Often, ${ \\mathbf{{P}} }_0$ is available in a factorized form, such as\nthe incomplete LU factorization used in this paper.\nIn this case, norm estimators can be used \\cite[Chapters~8,~14]{High02}.\nIf necessary or cost effective, we can make ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$\nsmaller by extending the sparsity pattern of ${ \\mathbf{{N}} }_k$\nguided by Corollary~\\ref{cor:residual}. In practice, it may be\nsufficient for fast convergence to satisfy\nCorollary~\\ref{cor:residual} only approximately, as a clustered spectrum with a few outliers\ngenerally leads to fast convergence as well \\cite[Chapter~3]{Gree97} and \\cite[Chapter~6]{Vand03}.\n  Several strategies for choosing and extending the nonzero patterns\nof SAIs can be adapted to achieve a good map, ${ \\mathbf{{N}} }_k$.\nAdaptive strategies for computing the SAI are discussed in\n\\cite{GrotHuck97}. However, fixing the sparsity pattern greatly reduces\nthe cost of computing the SAI. In our numerical experiments,\nwe choose the nonzero pattern of ${ \\mathbf{{N}} }_k$\nto be that of ${ \\mathbf{{A}} }_0$ or ${ \\mathbf{{A}} }_0^2$, though higher powers of\n${ \\mathbf{{A}} }_0$ can also be chosen. Other a priori and adaptive choices are\ndiscussed in \\cite{Chow00,Huck99} and the references therein.\n\nRegardless of how the sparsity pattern is chosen,\nfor a sequence of nested patterns with increasing numbers of nonzero\nentries, the following theorem guarantees a monotonic decrease in the\nsize of ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$.\n\n\\begin{theorem}\\label{teo:seqNestPatt}\nFor a sequence of nested patterns,\n\\begin{eqnarray*}\n  S_1 \\subseteq S_2 \\subseteq \\dots \\subseteq S_t,\n\\end{eqnarray*}\nand the corresponding subspaces ${\\mathcal S}_j$,\nlet ${ \\mathbf{{R}} }_k^{(j)} = { \\mathbf{{A}} }_k{ \\mathbf{{N}} }^{(j)} - { \\mathbf{{A}} }_0$ with\n${ \\mathbf{{N}} }^{(j)}$ the minimizer of (\\ref{eq:introSAM})\nfor ${\\mathcal S} = {\\mathcal S}_j$. Then\n\\begin{eqnarray}\\label{eq:monoDecRes}\n  {\\|{{ \\mathbf{{R}} }_k^{(t)} }\\|_F} \\leq {\\|{{ \\mathbf{{R}} }_k^{(t-1)} }\\|_F}\n  \\leq \\dots \\leq {\\|{{ \\mathbf{{R}} }_k^{(1)} }\\|_F}.\n\\end{eqnarray}\n\\end{theorem}\n\\begin{proof}\nSince the minimization problem (\\ref{eq:introSAM}) has a solution\nfor any sparsity pattern, the proof follows directly from the\nfact that the sequence of nested patterns leads\nto a sequence of nested (sub)spaces.\n\\end{proof}\n\n\nIn the THT application, we observe that both the relative and absolute\nresiduals tend to be small (in norm), with the exception of the larger shifts for the\nmatrices from the `early' sequence. We demonstrate this in Figure~\\ref{fig:residual},\nwhich shows the relative and absolute residuals of the early, middle, and late\nTHT matrices.  When these residuals are small, GMRES converges rapidly\nand GMRES convergence deteriorates for the larger residuals;\nsee Section \\ref{sec:THT}.\n\n\\begin{figure}\n\\begin{center}\n  \\scalebox{.6}{\\includegraphics[width=\\linewidth]{residual_THT.pdf}}\n  \\caption{The absolute residual ${\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k-{ \\mathbf{{A}} }_0}\\|_F}$ (dotted lines)\n  and relative residual $\\frac{{\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k-{ \\mathbf{{A}} }_0}\\|_F}}{{\\|{{ \\mathbf{{A}} }_0}\\|_F}}$\n  (solid lines) of the early (`$\\Diamond$'), middle (`$\\times$'), and\n  late (`$\\circ$') THT matrices.}\n\\label{fig:residual}\n\\end{center}\n\\end{figure}\n\n\nTo understand under which conditions we can expect small residuals for\npractical sparsity patterns (patterns that are not much denser than the\nsystem), we analyze the ideal map\nand its distance to the LS map, the solution to (\\ref{eq:introSAM}).\n\nWe first show that, in an appropriate norm, ${ \\mathbf{{N}} }_k$ is the best\napproximation to ${\\widehat}{{ \\mathbf{{N}} }}_k$.\n\\begin{definition}\nFor any nonsingular matrix ${ \\mathbf{{B}} } \\in {\\mathbb{C}^{n \\times n}}$,  we define the Frobenius ${ \\mathbf{{B}} }$-norm of ${ \\mathbf{{X}} } \\in {\\mathbb{C}^{n \\times n}}$ as\n\\begin{eqnarray*}\n  {\\|{{ \\mathbf{{X}} }}\\|_{F,{ \\mathbf{{B}} }}} = {\\|{{ \\mathbf{{B}} }{ \\mathbf{{X}} }}\\|_F}.\n\\end{eqnarray*}\n\\end{definition}\nIt is easy to verify that the Frobenius ${ \\mathbf{{B}} }$-norm satisfies the properties\nof a norm.\\footnote{In general, ${\\|{{ \\mathbf{{X}} }{ \\mathbf{{Y}} }}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{{ \\mathbf{{X}} }}\\|_{F,{ \\mathbf{{A}} }_k}}{\\|{{ \\mathbf{{Y}} }}\\|_{F,{ \\mathbf{{A}} }_k}}$ does\nnot hold.  While this property is occasionally considered part of the\ndefinition of a norm for matrices, we consider only the four standard\nproperties of a norm, omitting the submultiplicative property as a requirement.}\n\n\\begin{theorem}\\label{teo:mapBestApprox}\nLet $S$ be a sparsity pattern, then ${ \\mathbf{{N}} }_k$ is the best approximation to ${\\widehat}{{ \\mathbf{{N}} }}_k$ in the Frobenius ${ \\mathbf{{A}} }_k$-norm over the space ${\\mathcal S}$.\n\\end{theorem}\n\n\\begin{proof}\nWe can represent the residual in terms of the ideal and LS maps,\n\\begin{eqnarray}\\label{eq:froNormEq}\n  { \\mathbf{{R}} }_k & = & { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k - { \\mathbf{{A}} }_0 = { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k - { \\mathbf{{A}} }_k{\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{A}} }_k({ \\mathbf{{N}} }_k -{\\widehat}{{ \\mathbf{{N}} }}_k).\n\\end{eqnarray}\nHence, solving (\\ref{eq:introSAM}) is equivalent to solving\n\\begin{eqnarray} \\label{eq:minFroAk}\n  { \\mathbf{{N}} }_k = \\arg \\hspace{1mm} \\min_{{ \\mathbf{{N}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{A}} }_k({ \\mathbf{{N}} } - {\\widehat}{{ \\mathbf{{N}} }}_k)}\\|_F} = \\arg \\hspace{1mm} \\min_{{ \\mathbf{{N}} }\\in{\\mathcal S}}{\\|{{ \\mathbf{{N}} } - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}}.\n\\end{eqnarray}\n\\end{proof}\n\n\\noindent\nFrom (\\ref{eq:froNormEq})-(\\ref{eq:minFroAk}) we have\n${\\|{{ \\mathbf{{R}} }_k}\\|_F} = {\\|{{ \\mathbf{{N}} }_k - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}}$, which immediately leads to the following\nCorollary (to Theorem~\\ref{teo:seqNestPatt})\n\\begin{corollary}\\label{cor:seqNestPatt}\nFor a sequence of nested patterns,\n\\begin{eqnarray*}\nS_1 \\subseteq S_2 \\subseteq \\dots \\subseteq S_t,\n\\end{eqnarray*}\nand the corresponding subspaces ${\\mathcal S}_j$,\nlet ${ \\mathbf{{N}} }^{(j)}$ be the minimizer of (\\ref{eq:introSAM})\nfor ${\\mathcal S} = {\\mathcal S}_j$. Then\n\\begin{eqnarray}\\label{eq:monoConvFAkN}\n  {\\|{{ \\mathbf{{N}} }^{(t)} - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{{ \\mathbf{{N}} }^{(t-1)} - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq \\cdots\n  \\leq {\\|{{ \\mathbf{{N}} }^{(1)} - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}}\n\\end{eqnarray}\n\\end{corollary}\n\n\nNext, we examine the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$.\nFollowing the analysis in \\cite{BaiMeer10}, we consider the generalized\neigenvalues and eigenvectors of ${ \\mathbf{{A}} }$ and ${ \\mathbf{{E}} }$ in (\\ref{eq:shiftMat}).\n\n", "index": 1, "text": "\\begin{equation}\n  { \\mathbf{{AVM}} } = { \\mathbf{{EV}} } \\Leftrightarrow { \\mathbf{{AV}} }{ \\mathbf{{M}} }{ \\mathbf{{V}} }^{-1} = { \\mathbf{{E}} },\n\\label{eq:genEigs}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{{AVM}}}={\\mathbf{{EV}}}\\Leftrightarrow{\\mathbf{{AV}}}{\\mathbf{{M}}}{%&#10;\\mathbf{{V}}}^{-1}={\\mathbf{{E}}},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc00\ud835\udc15\ud835\udc0c</mi><mo>=</mo><mi>\ud835\udc04\ud835\udc15</mi></mrow><mo>\u21d4</mo><mrow><msup><mi>\ud835\udc00\ud835\udc15\ud835\udc0c\ud835\udc15</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>\ud835\udc04</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05883.tex", "nexttext": "\nwhere $x_s$ denotes the location of the pumping well, $q(t)$ is the pumping rate, $\\kappa(x)$ is the hydraulic conductivity, $S_s$ is the specific storage, $q(t){\\delta}(x-x_s)$  is the pumping source, and $\\phi(x,t)$ is the hydraulic head (pressure). $\\Omega_D$ and $\\Omega_N$ denote the parts of the boundary where the Dirichlet and Neumann boundary conditions are defined, respectively.\nThe differential equation \\eqref{eqn:timedomain} and its corresponding boundary conditions are discretized by standard linear finite elements using FEnICS \\cite{LoggMardalEtAl2012a,LoggWells2010a, LoggWellsEtAl2012a}. We obtain the semi-discrete system of equations,\n\n", "itemtype": "equation", "pos": 54677, "prevtext": "\nwhere ${ \\mathbf{{M}} } = \\diag{(\\mu_i)}$ is the matrix of generalized eigenvalues (we assume\ndiagonalizability).\nSubstituting (\\ref{eq:genEigs}) into (\\ref{eq:map}), and recalling the form of our shifted matrices (\\ref{eq:shiftMat}), we have\n\\begin{eqnarray*}\n  (s_k { \\mathbf{{AV}} }{ \\mathbf{{M}} }{ \\mathbf{{V}} }^{-1} + { \\mathbf{{A}} }) {\\widehat}{{ \\mathbf{{N}} }}_k  = (s_0 { \\mathbf{{AV}} }{ \\mathbf{{M}} }{ \\mathbf{{V}} }^{-1} + { \\mathbf{{A}} }) \\iff\n\\end{eqnarray*}\n\\begin{eqnarray}\\label{eq:simTrans}\n  {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{V}} }(s_k { \\mathbf{{M}} } + { \\mathbf{{I}} })^{-1}(s_0 { \\mathbf{{M}} } + { \\mathbf{{I}} }){ \\mathbf{{V}} }^{-1} = { \\mathbf{{VDV}} }^{-1}\n\\end{eqnarray}\nand\n\\begin{eqnarray}\\label{eq:diag}\n  { \\mathbf{{D}} } = \\diag{ \\left(\\frac{s_0{\\mu}_i +1}{s_k{\\mu}_i +1}\\right)}.\n\\end{eqnarray}\nThe assumption that all matrices $s_j{ \\mathbf{{E}} } + { \\mathbf{{A}} }$ in a given sequence are invertible implies\nthat the diagonal matrices $s_j { \\mathbf{{M}} } + { \\mathbf{{I}} }$ are invertible, and hence\nthat $s_k{\\mu}_i + 1 \\neq 0$ and $s_0{\\mu}_i + 1 \\neq 0$ for $i = 1, \\ldots, n$ in (\\ref{eq:diag}).\nWith (\\ref{eq:simTrans}) a similarity transformation, the eigenvalues\nof ${\\widehat}{{ \\mathbf{{N}} }}_k$ are the (diagonal) entries of ${ \\mathbf{{D}} }$, $d_i$. Equation\n(\\ref{eq:diag}) suggests clustering of the eigenvalues if\n$|{\\mu}_i| \\gg |s_0|, |s_k|$ for most of the eigenvalues,\nor if $s_k$ is relatively close to $s_0$\nwith respect to most $|{\\mu}_i|$. We will see that, if the condition number of ${ \\mathbf{{V}} }$,\n${\\kappa}_{F,{ \\mathbf{{A}} }_k}({ \\mathbf{{V}} })$, is modest, clustering leads to\na good approximation of the ideal map by the\nLS map.\n\n\nIn two of the applications discussed in this paper,\nwe can expect the diagonal entries of (\\ref{eq:diag}) - and therefore the\neigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ - to be clustered (possibly with some outliers).\nFor stable dynamical systems, all eigenvalues have\nnegative real part. Therefore, in model reduction for such\nsystems, the shifts are generally computed\n(for example, by IRKA \\cite{GugeAnth08}),\nsuch that ${\\mathrm{Re}}(s_i)$ is close to zero and the shifts are often relatively close to one another.\nThis reflects the fact that the reduced model needs to represent\nmost accurately the modes of the\nsystem that decay slowest (corresponding to the eigenvalues with the smallest absolute\nreal part). In addition, a stable dynamical system may have many eigenvalues with\nlarge absolute real part (corresponding to modes that decay very rapidly).\nTherefore, $s_0{\\mu}_j \\approx s_k{\\mu}_j$ and $d_j \\approx 1$, or $|{\\mathrm{Re}}({\\mu}_j)|$ is very\nlarge compared to ${\\mathrm{Re}}(s_i)$, and\n$\\frac{s_0{\\mu}_j+1}{s_k{\\mu}_j+1}\\approx \\frac{{\\mu}_j}{{\\mu}_j} = 1$.\n\nWe also expect clustering of the eigenvalues of the ideal map for the THT matrices.\nIn this application, the shifts come from a modified Talbot contour and tend to be\nquite small, particularly for larger values of time, as shown in\nFigure \\ref{fig:contours}.  For more information on how these contours, and the\nparameters which define them, are determined, we refer to \\cite{Weid06}.  When\nthese shifts are small and relatively close to one another, as is the case with\nthe middle and late THT matrices, $s_0{\\mu}_j$ and $s_k{\\mu}_j$ are both small enough\nthat $d_j \\approx 1$.  More information on the THT matrices is provided later in this section as well as in Section \\ref{sec:THT}, and  more detail on the\nmodel reduction matrices is given in Section \\ref{sec:ModRed}.\n\n\nClustering implies a small ${\\epsilon}_D$ such that, for all $i$,\n\\begin{eqnarray*}\n  |\\bar{d}-d_i| < \\frac{{\\epsilon}_D}{\\sqrt{n}}.\n\\end{eqnarray*}\nwhere $\\bar{d}$ is the average of all $d_i$ or another appropriate\ncenter for the cluster, such as,\n$\\bar{d} = \\arg \\min_{{\\mathbb{C}}} \\max_i |\\bar{d} - d_i|$.\nNote that the average minimizes $\\|{ \\mathbf{{D}} } - \\bar{d}{ \\mathbf{{I}} }\\|_F$, whereas\nthe minimax solution minimizes $\\|{ \\mathbf{{D}} } - \\bar{d}{ \\mathbf{{I}} }\\|_2$.\nWriting ${ \\mathbf{{D}} } = \\bar{d} { \\mathbf{{I}} } + {\\widehat}{{ \\mathbf{{F}} }}$,  with ${\\widehat}{{ \\mathbf{{F}} }} = ({ \\mathbf{{D}} }-\\bar{d} { \\mathbf{{I}} })$\nand therefore ${\\|{{\\widehat}{{ \\mathbf{{F}} }}}\\|_F} < {\\epsilon}_D$, we have\n\\begin{eqnarray} \\label{eq:NkhatDec}\n  {\\widehat}{{ \\mathbf{{N}} }}_k = { \\mathbf{{VDV}} }^{-1} = \\bar{d}{ \\mathbf{{I}} } + { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\n\n\n\n\nNow assume that the chosen sparsity pattern, $S$, contains the\ndiagonal. This is often the case and can easily be ensured.\nThen\n\\begin{eqnarray*}\n  S \\supseteq S_0 = \\{(1,1), (2,2),\\dots, (n,n)\\}.\n\\end{eqnarray*}\nSince $\\bar{d}{ \\mathbf{{I}} } \\in {\\mathcal S}_0$, the subspace corresponding to $S_0$, by Theorem \\ref{teo:seqNestPatt},\n\\begin{eqnarray*}\n {\\|{{ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{\\bar{d}{ \\mathbf{{I}} } - {\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} =\n   {\\|{\\bar{d}{ \\mathbf{{I}} } -\\bar{d}{ \\mathbf{{I}} } - { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}} =\n   {\\|{{ \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}}.\n\\end{eqnarray*}\nTherefore,\n\\begin{eqnarray}\\label{eq:mapBound}\n{\\|{{ \\mathbf{{R}} }_k}\\|_F} = {\\|{{ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k}\\|_{F,{ \\mathbf{{A}} }_k}} \\leq {\\|{{ \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}}\\|_{F,{ \\mathbf{{A}} }_k}}.\n\\end{eqnarray}\nHence, for modest ${\\kappa}_{F,{ \\mathbf{{A}} }_k}({ \\mathbf{{V}} })$, ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$ will also be small,\nand we can expect good convergence.\nNote that, in general, this bound is rather pessimistic, as\n${ \\mathbf{{N}} }_k$ can provide a much better approximation than $\\bar{d} { \\mathbf{{I}} }$,\nand a further analysis of these approximation problems is a topic\nof further research.\n\n\nSince the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are not always perfectly clustered,\nwe also consider ${\\|{{ \\mathbf{{R}} }_k}\\|_F}$ when ${\\widehat}{{ \\mathbf{{N}} }}_k$ has\nclustered eigenvalues with a few outliers.\nWe expect that, in that case,\n\\begin{eqnarray}\\label{eq:smallLowRank}\n  { \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 = { \\mathbf{{I}} } + {\\widetilde}{{ \\mathbf{{K}} }} + { \\mathbf{{H}} },\n\\end{eqnarray}\nwhere ${\\|{{\\widetilde}{{ \\mathbf{{K}} }}}\\|_F}$ is small and $\\rank{({ \\mathbf{{H}} })} = p \\ll n$, but\n${\\|{{ \\mathbf{{H}} }}\\|_F}$ is not small.\nWe can still consider (\\ref{eq:NkhatDec}) for some\nappropriate $\\bar{d}$; however,\nsome of the (diagonal) coefficients of\n${\\widehat}{{ \\mathbf{{F}} }}$, $d_i - \\bar{d}$, will not be\nsmall, and ${ \\mathbf{{V}} } {\\widehat}{{ \\mathbf{{F}} }} { \\mathbf{{V}} }^{-1}$ will be\nthe sum of a matrix with small norm\nand a low rank matrix with (typically) larger norm.\nWriting\n\\begin{eqnarray*}\n  { \\mathbf{{N}} }_k = \\bar{d}{ \\mathbf{{I}} } + {\\widetilde}{{ \\mathbf{{N}} }},\n\\end{eqnarray*}\nwhere ${\\widetilde}{{ \\mathbf{{N}} }}$ has the same sparsity pattern as ${ \\mathbf{{N}} }_k$, gives\n\\begin{eqnarray}\\label{eq:bestApproxNTilde}\n { \\mathbf{{R}} }_k = { \\mathbf{{A}} }_k({ \\mathbf{{N}} }_k-{\\widehat}{{ \\mathbf{{N}} }}_k) = { \\mathbf{{A}} }_k( {\\widetilde}{{ \\mathbf{{N}} }} - { \\mathbf{{V}} }{\\widehat}{{ \\mathbf{{F}} }}{ \\mathbf{{V}} }^{-1}).\n\\end{eqnarray}\nFrom (\\ref{eq:bestApproxNTilde}) and Theorem \\ref{teo:mapBestApprox} follows\nthat ${\\widetilde}{{ \\mathbf{{N}} }}$ is the best approximation of ${ \\mathbf{{V{\\widehat}{F}V}} }^{-1}$ in the\nFrobenius ${ \\mathbf{{A}} }_k$-norm. Although a formal proof appears complicated,\nwe expect ${ \\mathbf{{R}} }_k$, and hence ${ \\mathbf{{R}} }_k { \\mathbf{{P}} }_0$, to also be the sum of a\nmatrix with small norm and a low rank matrix with (typically) larger norm.\nWe numerically verify this for the THT matrices below.\nFuture work will focus on the conditions under which we can prove this to be true.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyIdealMapEigs2.pdf}\n\t\t\\subcaption{Early}\n\t\t\\label{fig:Early}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{MiddleIdealMapEigs2.pdf}\n\t\t\\subcaption{Middle}\n\t\t\\label{fig:Middle}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateIdealMapEigs2.pdf}\n\t\t\\subcaption{Late}\n\t\t\\label{fig:Late}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the ideal map ${\\widehat}{{ \\mathbf{{N}} }}_k$ for selected shifts\n      (2-5, 10, 15, 20) of the THT matrices. Note the clustering for the first few\n      shifts for each sequence (early, middle, and late). Note the clustering with\n      relatively few outliers for the middle and late sequences ($n = 10\\,201$).}\n\t  \\label{fig:THT_eigs}\n\\end{center}\n\\end{figure}\nFigure \\ref{fig:THT_eigs} shows the eigenvalues of the ideal maps for the THT matrices.\\footnote{Figures \\ref{fig:THT_eigs} and \\ref{fig:THT_eigs_comp} show the eigenvalues of the maps beginning at shift two, since an ILUTP factorization of ${ \\mathbf{{A}} }_0$ is computed for the first shift and the SAMs are applied at subsequent shifts.}   Note that they are clustered for the first several shifts.  This corresponds to when the\nresidual of the SAMs is small, as shown in Figure \\ref{fig:residual}.  In Tables \\ref{table:THTearlySAMall} and \\ref{table:THTmiddleSAMall}, it can be seen that the number of GMRES iterations for these shifts is low.  Also in the case of the LS map, we observe clustering of the eigenvalues, as shown in Figure \\ref{fig:THT_eigs_comp}.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n\n\t\t\\includegraphics[width=\\linewidth]{EarlyCompMapEigs2.pdf}\n\t\t\\subcaption{Early}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{MiddleCompMapEigs2.pdf}\n\t\t\\subcaption{Middle}\n        \\end{subfigure}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateCompMapEigs2.pdf}\n\t\t\\subcaption{Late}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the LS map, ${ \\mathbf{{N}} }_k$, for selected shifts\n      (2-5, 10, 15, 20) of the THT Matrices. }\n\t  \\label{fig:THT_eigs_comp}\n\\end{center}\n\\end{figure}\n\nFigure \\ref{fig:THT_eigs} also shows that for the middle and late THT matrices, the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are mostly clustered with relatively few outliers.  We show the eigenvalues of\nthe ideal map for shifts 11 through 15 of the late THT matrices in Figure \\ref{fig:eig_vals_late}.\nExamining the $60$ largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for these same\nmatrices and shifts in Figure \\ref{fig:sing_vals_late},\nwe see that there are a few singular values larger than $1$ and only about ten larger than $0.5$.\\footnote{Our use of the word \"small\" is relative to Theorem \\ref{teo:smallNorm}.}\nThis shows that, for the THT matrices, it is reasonable to represent ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ as a small perturbation of a low rank matrix  (\\ref{eq:smallLowRank})\nwhen the eigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are clustered with few outliers.\n\nFigures \\ref{fig:eig_vals_early} and \\ref{fig:sing_vals_early} show that when the\neigenvalues of ${\\widehat}{{ \\mathbf{{N}} }}_k$ are not clustered, there are many more singular values\nof ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ larger than $1$.\n\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{LateEigsClust.pdf}\n\t\t\\subcaption{Eigenvalues of the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$, for the late THT matrices for shifts 11 through 15.}\n\t\t\\label{fig:eig_vals_late}\n        \\end{subfigure}\n        \\hspace{2mm}\n        \\begin{subfigure}{.45\\textwidth}\n\t\t\\includegraphics[width=\\linewidth]{SingLate.pdf}\n\t\t\\subcaption{The 60 largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for the late THT matrices for shifts 11 through 15.}\n\t\t\\label{fig:sing_vals_late}\n        \\end{subfigure}\n\n         \\begin{subfigure}{.45\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyEigsNotClust.pdf}\n\t\t\\subcaption{Eigenvalues of the ideal map, ${\\widehat}{{ \\mathbf{{N}} }}_k$, for the early THT matrices for shifts 16 through 20.}\n\t\t\\label{fig:eig_vals_early}\n        \\end{subfigure}\n        \\hspace{2mm}\n        \\begin{subfigure}{.45\\textwidth}\n\t\t\\includegraphics[width=\\linewidth]{SingEarly.pdf}\n\t\t\\subcaption{The 60 largest singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ for the early THT matrices for shifts 16 through 20.}\n\t\t\\label{fig:sing_vals_early}\n        \\end{subfigure}\n\\end{center}\n\\caption{Comparison of the singular values of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 - { \\mathbf{{I}} }$ with the eigenvalues of the late and early THT matrices when the eigenvalues are clustered with few outliers and when the eigenvalues are not clustered.}\n\\end{figure}\n\n\\section{Implementation}\\label{sec:impl}\nTo efficiently compute the SAM updates given a sparsity pattern $S$,\nthe solution of (\\ref{eq:introSAM}) must be implemented in sparse-sparse fashion.\nFurthermore, the nonzero pattern of the matrices\noften does not change, as is the case in the applications in this paper. Then the structure of the small least squares (LS) problems\nwill be the same for every update. This makes it efficient to setup\nthe data structures for the small LS problems just once, in advance.\n\n\nFor ease of notation, we drop the indices and consider the problem\n\\begin{eqnarray}\\label{eq:argSAM}\n{ \\mathbf{{N}} } = \\arg \\min_{{\\widetilde}{{ \\mathbf{{N}} }}\\in{\\mathcal S}}{\\|{{ \\mathbf{{A}} } {\\widetilde}{{ \\mathbf{{N}} }} - {\\widehat}{{ \\mathbf{{A}} }}}\\|_F}.\n\\end{eqnarray}\nGiven the pattern $S$, let $s_k$ be the set of indices of the (potential)\nnonzeros in column $k$ of ${ \\mathbf{{N}} }$: $s_k = \\{i \\,|\\, (i,k) \\in S\\}$\nand let\n${\\mathcal S}_k = \\{{ \\mathbf{{x}} } \\in {\\mathbb{C}^{n}} \\;|\\; { \\mathbf{{x}} }_i = 0 \\textrm{ if } i \\not\\in s_k \\}$.\nThus, for computing ${ \\mathbf{{n}} }_k$ (the $k$th column of ${ \\mathbf{{N}} }$) only the\ncolumns ${ \\mathbf{{a}} }_j$ with $j \\in s_k$ of ${ \\mathbf{{A}} }$ matter. These columns\nthemselves are sparse, and for the small LS problem defining\n${ \\mathbf{{n}} }_k$ we need only consider rows $i$ such that\n$a_{i,j} \\neq 0$ for some $j \\in s_k$.\nNote that if $\\hat{a}_{i,k} \\neq 0$ but $a_{i,j} = 0$\nfor all $j \\in s_k$, row $i$ is irrelevant for\ncomputing ${ \\mathbf{{n}} }_k$ since ${ \\mathbf{{e}} }_k \\perp\n{\\mathrm{Span}({\\{{ \\mathbf{{a}} }_j \\,|\\, j \\in s_k\\}})}$. However, if we wish\nto compute the residual (${ \\mathbf{{R}} } = { \\mathbf{{A}} } { \\mathbf{{N}} } - {\\widehat}{ { \\mathbf{{A}} } }$) or its norm,\nin addition to ${ \\mathbf{{N}} }$, we need to include such rows as well.\nIf the matrices ${ \\mathbf{{A}} }$ and ${\\widehat}{{ \\mathbf{{A}} }}$ have the same sparsity\npattern and the pattern of ${ \\mathbf{{N}} }$ includes at least the\ndiagonal, this is not an issue.\nLet $r_k$ be the set of indices of rows in ${ \\mathbf{{A}} }$ that\nare relevant for the $k$th small LS problem.\nThen the least squares problem for ${ \\mathbf{{n}} }_k$ is defined as\n\\begin{eqnarray*}\n  { \\mathbf{{n}} }_k & = & \\arg \\min_{{\\widetilde}{{ \\mathbf{{n}} }}_k \\in {\\mathcal S}_k }\n    \\|{ \\mathbf{{A}} }(r_k,s_k) {\\widetilde}{{ \\mathbf{{n}} }}_k(r_k) - {\\widehat}{{ \\mathbf{{A}} }}(r_k,k) \\|_2 ,\n\\end{eqnarray*}\nwhere ${ \\mathbf{{A}} }(r_k,s_k)$ is the block submatrix of ${ \\mathbf{{A}} }$ indexed\nby $r_k \\times s_k$, ${\\widetilde}{{ \\mathbf{{n}} }}_k(r_k)$\nis the subvector of ${\\widetilde}{{ \\mathbf{{n}} }}_k$ indexed by $r_k$,\nand ${\\widehat}{{ \\mathbf{{A}} }}(r_k,k)$ is the corresponding\nsubvector of the $k$th columns of ${\\widehat}{{ \\mathbf{{A}} }}$.\n\nWe preprocess the matrices or their\nsparsity patterns to be able to efficiently select\nthe relevant rows, $r_k$, and columns, $s_k$, of the small LS problem\nfor each column $k$. It may also be efficient to (only once)\nallocate memory space for solving these least squares problems.\nThis can be a single memory allocation sufficiently large\nfor each of the problems or multiple allocations to\nallow for parallelism.\nFor matrices that derive from some discretization,\nthe size of these least squares problems typically depends only on the\nsparsity pattern, not on the size of the matrix.\nSo, while $n$ may be large, each of the least squares problems solved\nis very small (and most are about the same size).\nFor example, the average size of these least squares problems for the\nTHT matrices is $18\\times 7$.\nIn general, the matrices or the underlying problems have structure\nthat should be exploited. For example, if a matrix derives from\ndiscretization on some mesh or grid, finding the nonzero\npatterns of powers of the matrix can be done very\nefficiently using the information defining the mesh or grid.\n\n\nFinally, it\nis essential to store the matrices in an appropriate\n(sparse) format and to generate ${ \\mathbf{{N}} }$ in an appropriate format.\nFor example, in MATLAB{\\textsuperscript{\\textregistered}} it is inefficient to generate\na sparse matrix one column at a time, even if the\ntotal space is allocated in advance (presumably because of the\nrequired manipulation of sparse matrix data structures\nfor each column). Therefore,\nwe generate ${ \\mathbf{{N}} }$ first in coordinate format (COO) \\cite{Saad09,Saad03},\nand after the whole matrix has been computed we\nconvert this temporary data structure into a MATLAB{\\textsuperscript{\\textregistered}}\nsparse matrix using the command {\\tt sparse}.\nIn Algorithm~\\ref{alg:SAMpre}, the statement\n$t = \\texttt{find}({ \\mathbf{{a}} }_j)$ (with reference to the MATLAB{\\textsuperscript{\\textregistered}}\ncommand {\\tt find}), it is important for efficiency\nthat ${ \\mathbf{{A}} }$ is stored as a sparse matrix, and that its\ncolumns are easily accessible.\n\n\nThe algorithm for preprocessing is given in\nAlgorithm~\\ref{alg:SAMpre}; the algorithm for\ncomputing the SAM itself is given in Algorithm \\ref{alg:SAM}.\n\n\\begin{algorithm}\n  \\caption{Preprocessing for Computing Sparse Approximate Maps }\n\t\\label{alg:SAMpre}\n  \\begin{algorithmic}\n  \\State Given sparsity pattern $S$, and matrix ${ \\mathbf{{A}} }$\n  \\State $maxSk = 0$; $maxRk = 0$; \\Comment{initialize max num of columns, max num of rows}\n  \\For{$k = 1:n$} \\Comment{for each column do}\n\t\\State $s_k = \\{i \\,|\\, (i,k) \\in S\\}$ \\Comment{get indices; typically defined in advance}\n    \\State $r_k = \\emptyset$ \\Comment{Initialize set of rows for $k$th LS problem}\n    \\ForAll {$j \\in s_k$}\n      \\State $t = \\texttt{find}({ \\mathbf{{a}} }_j)$ \\Comment{find indices of nonzeros in column ${ \\mathbf{{a}} }_j$}\n\t  \\State $r_k = r_k \\cup t$\n    \\EndFor\n    \\State $nnz_k = \\#( s_k )$ \\Comment{\\#() gives number of elements in a set}\n    \\If {$nnz_k > maxSk$}\n      \\State $maxSk = nnz_k$\n    \\EndIf\n    \\If {$\\#( r_k ) > maxRk$}\n      \\State $maxRk = \\#( r_k )$\n    \\EndIf\n  \\EndFor\n  \\State Allocate $maxRk \\times maxSk$ array for storing the LS matrices,\n    $maxRk$ vector for storing the right hand side, and $maxSk$ vector for\n    storing the solution.\n  \\end{algorithmic}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n  \\caption{Computing ${ \\mathbf{{N}} } = \\arg \\min_{{\\widetilde}{{ \\mathbf{{N}} }} \\in {\\mathcal S}}{\\|{{ \\mathbf{{A}} }{\\widetilde}{{ \\mathbf{{N}} }} - {\\widehat}{{ \\mathbf{{A}} }}}\\|_F}$}\n\t\\label{alg:SAM}\n  \\begin{algorithmic}\n  \\State $cnt = 0$ \\Comment{counts number of nonzeros in preconditioner}\n  \\For{$k = 1:n$}\n\t\\State ${ \\mathbf{{A}} }_\\mathrm{tmp} = { \\mathbf{{A}} }(r_k,s_k)$ \\Comment{get submatrix indexed by $r_k$ and $s_k$ for LS problem}\n\t\\State ${ \\mathbf{{f}} } = {\\widehat}{{ \\mathbf{{A}} }}(r_k)$ \\Comment{get rhs for LS problem}\n    \\State Solve LS ${ \\mathbf{{A}} }_\\textrm{tmp} { \\mathbf{{z}} } = { \\mathbf{{f}} }$\n\t\\State (possibly save residual, norm of residual, etc.)\n    \\State $rowN[cnt+1:cnt+nnz_k] = s_k$ \\Comment{assign indices in the order of values in ${ \\mathbf{{z}} }$}\n\t\\State $colN[cnt+1:cnt+nnz_k] = j$\n\t\\State $valN[cnt+1:cnt+nnz_k] = { \\mathbf{{z}} }$\n\t\\EndFor\n\t\\State ${ \\mathbf{{N}} } = $ sparse$(rowN,colN,valN)$ \\Comment{convert into sparse matrix}\n  \\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\\comment{\nWe refer to ${ \\mathbf{{S}} }$ as the matrix representation of $S$.  Some preprocessing should be done in order to make the computation of the map as cheap as possible, in particular, determining the indices and number of nonzeros in each column of the map as well as in each column of the least squares problem ${\\|{{ \\mathbf{{A}} }_k{ \\mathbf{{N}} }-{ \\mathbf{{A}} }_0}\\|_F}$.  For example, in each of our applications, ${ \\mathbf{{A}} }_0$ and ${ \\mathbf{{A}} }_k$ have the same nonzero pattern, which may not always be the case.  When we choose the sparsity pattern to be that of ${ \\mathbf{{A}} }_0$, we can use Matlab's `find' function to efficiently determine the nonzero pattern in each column of ${ \\mathbf{{A}} }_0$ and in each column of ${ \\mathbf{{A}} }_0^2$, the latter representing the nonzero pattern of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }$.\n\nThe user should also take advantage of the structure of the matrix, if possible.  For instance, when dealing with a mesh, the pattern of ${ \\mathbf{{A}} }_0$ (and powers of ${ \\mathbf{{A}} }_0$) can be easily determined prior to computing the map.  While our matrices are not defined by a mesh, when taking the sparsity pattern to be the nonzero pattern of ${ \\mathbf{{A}} }_0$ or ${ \\mathbf{{A}} }_0^2$, we are dealing with very sparse matrices.  Any preprocessing that can be done to take advantage of the sparsity and/or structure of the matrix should be performed.\n\nPreprocessing allows us to compute the sparse approximate map by solving $n$ small least squares problems,  where $n$ is the size of the matrix.  This is represented in the for-loop of Algorithm \\ref{alg:SAM}.  Note the size of these least squares problems depends on the sparsity pattern, and is  therefore independent of the size of the matrix.  While $n$ may be large, each of the least squares problems solved is very small and more or less constant.  For example, the average size of the least squares problems computed for the THT matrices is $18\\times 7$ and is easily computed using Matlab's `backslash' operator.\n\nSince our implementation is done in Matlab,  we take advantage of Matlab's sparse matrix functionality when solving each of the small least squares problems.  However, building a sparse matrix as we solve each of these problems is very expensive - potentially $O(n^2)$ work.  Therefore, we store values in compressed sparse row (CSR) format within the loop, using the `sparse' command at the very end to build the sparse approximate map.  Clearly, this will be done differently depending on the language used.\n\n}\n\n\n\\section{Numerical Experiments} \\label{sec:results}\nWe apply the strategies of reusing and recycling preconditioners to several applications, focusing both on total computation time as well as total GMRES iterations. We define total computation time to be the time to compute the preconditioner or SAM update plus the time for GMRES to converge for all shifts. We also report the times for the computation of individual preconditioners and SAMs as well as the number of iterations and runtime per system solve. These numbers provide good insight into the merits of reusing a preconditioner, possibly including a previous SAM update, computing a new preconditioner, or computing a new SAM update. Of course, the actual costs of these computations, the number of iterations saved, and the cost per iteration are all problem dependent. We compare the results of computing a new ILUTP preconditioner for each shift, reusing the initial ${ \\mathbf{{P}} }_0$ for all shifts, updating ${ \\mathbf{{P}} }_0$ with a new SAM update for all shifts, and updating ${ \\mathbf{{P}} }_0$ with a SAM update only at selected shifts. The first approach, a new ILUTP preconditioner for every shift, is always the most expensive option in runtime, but it provides a useful benchmark in terms of the number of iterations. The last approach, to compute a SAM update only at selected shifts, is usually the winner in runtime. The exceptions are the linear systems for the late THT matrices and the matrices from the\nmodel reduction test problem Rail. For these problems reusing the initial ILUTP for all systems leads\nto the lowest runtime. For brevity, we do not provide data for recomputing\nthe ILUTP at selected shifts. This, of course, can be better than computing\nthe ILUTP for all shifts, but it was never the fastest - this can easily be\nderived directly from the high cost of computing the preconditioner.\n\nFinally, for longer sequences of systems and other problems, many other variations of\ncomputing preconditioners and updates may be effective. Although we experimented with\nseveral {\\em indicator functions} to decide when to do\na SAM update and the results were encouraging, we did not find a\nsingle good indicator. Hence we leave a further analysis and discussion of these\nfor future work. A simple and effective strategy is to compute a new SAM or\npreconditioner based on (1) the time for this computation and (2)\nthe (relative) increase in the number\nof iterations or the solution time for a single system.\n\nFor the THT matrices, we also compare with the AINV preconditioner\nand update. The algorithms for calculating both the AINV preconditioner as well as the AINV updates can be found in \\cite{BellBert11,BenzBert03,BenzCull00,BenzHaws00,BenzKouh01,BenzTuma98,Rafi14}.  The implementation of the ILUTP preconditioner is computed using an implementation based on that in \\cite{Saad09}.\n\n\n\\subsection{Transient Hydraulic Tomography\\protect\\footnote{We would like to thank to Tania Bakhos, Arvind Saibaba, and Peter Kitanidis for providing the description of THT as well as the matrices used.}}\\label{sec:THT}\nTransient Hydraulic Tomography (THT) is a method for imaging the earth's subsurface (see \\cite{CardBarr11} for a detailed description of THT). Water is pumped at a constant rate in pumping wells and the measured drawdown curves of pressure response at the observation wells is recorded. A subset of this data is used in a nonlinear inversion to recover the parameters of interest, namely hydraulic conductivity and specific storage.  In the example described later in this section, we choose three key time points (corresponding to {\\em early}, {\\em middle}, and {\\em late} times).  The governing equations of groundwater flow through an aquifer with domain $\\Omega$ are given by,\n\n\n", "index": 3, "text": "\\begin{align} \\label{eqn:timedomain}\nS_s(x) \\frac{\\partial \\phi(x,t)}{\\partial t} - \\nabla \\cdot \\left(\\kappa(x) \\nabla \\phi(x,t)\\right) & =  \\quad  q(t)\\delta(x-x_s), & x &\\in \\Omega \\\\ \\nonumber\n\\phi(x,t) & = \\quad 0, & x & \\in \\partial \\Omega_D  \\\\ \\nonumber\n\\nabla \\phi(x,t) \\cdot {n} & = \\quad 0, & x &\\in \\partial \\Omega_N\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{s}(x)\\frac{\\partial\\phi(x,t)}{\\partial t}-\\nabla\\cdot\\left(%&#10;\\kappa(x)\\nabla\\phi(x,t)\\right)\" display=\"inline\"><mrow><mrow><msub><mi>S</mi><mi>s</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03d5</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac></mstyle></mrow><mo>-</mo><mrow><mo>\u2207</mo><mo>\u22c5</mo><mrow><mo>(</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>\u03d5</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\quad q(t)\\delta(x-x_{s}),\" display=\"inline\"><mrow><mrow><mo>=</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle x\" display=\"inline\"><mi>x</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\in\\Omega\" display=\"inline\"><mrow><mi/><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\phi(x,t)\" display=\"inline\"><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\quad 0,\" display=\"inline\"><mrow><mrow><mo>=</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle x\" display=\"inline\"><mi>x</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\in\\partial\\Omega_{D}\" display=\"inline\"><mrow><mi/><mo>\u2208</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mi>D</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\nabla\\phi(x,t)\\cdot{n}\" display=\"inline\"><mrow><mrow><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>\u03d5</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>n</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\quad 0,\" display=\"inline\"><mrow><mrow><mo>=</mo><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle x\" display=\"inline\"><mi>x</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\in\\partial\\Omega_{N}\" display=\"inline\"><mrow><mi/><mo>\u2208</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi mathvariant=\"normal\">\u03a9</mi><mi>N</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05883.tex", "nexttext": "\nwhere ${ \\mathbf{{K}} }$ and ${ \\mathbf{{M}} }$ denote the stiffness and mass matrices respectively. Instead of using a traditional time-stepping scheme to solve these equations, the Laplace transform-based exponential time integrator is used, as described in \\cite{Bakhos2015940}. The main idea is that a contour integral representation of the inverse Laplace transform chosen on the modified Talbot contour can be used to efficiently solve the groundwater equations \\cite{Weid06}.  The solution at a given time $t$ is given by,\n\n", "itemtype": "equation", "pos": 55675, "prevtext": "\nwhere $x_s$ denotes the location of the pumping well, $q(t)$ is the pumping rate, $\\kappa(x)$ is the hydraulic conductivity, $S_s$ is the specific storage, $q(t){\\delta}(x-x_s)$  is the pumping source, and $\\phi(x,t)$ is the hydraulic head (pressure). $\\Omega_D$ and $\\Omega_N$ denote the parts of the boundary where the Dirichlet and Neumann boundary conditions are defined, respectively.\nThe differential equation \\eqref{eqn:timedomain} and its corresponding boundary conditions are discretized by standard linear finite elements using FEnICS \\cite{LoggMardalEtAl2012a,LoggWells2010a, LoggWellsEtAl2012a}. We obtain the semi-discrete system of equations,\n\n", "index": 5, "text": "\\begin{equation}\n { \\mathbf{{M}} }\\partial_t\\phi_h  + { \\mathbf{{K}} }\\phi_h =  q(t){ \\mathbf{{b}} }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{{M}}}\\partial_{t}\\phi_{h}+{\\mathbf{{K}}}\\phi_{h}=q(t){\\mathbf{{b}}}\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc0c</mi><mo>\u2062</mo><mrow><msub><mo>\u2202</mo><mi>t</mi></msub><mo>\u2061</mo><msub><mi>\u03d5</mi><mi>h</mi></msub></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><msub><mi>\u03d5</mi><mi>h</mi></msub></mrow></mrow><mo>=</mo><mrow><mi>q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1b</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05883.tex", "nexttext": "\nwith $w_k$ and $z_k$ being the weights and nodes of the quadrature scheme, respectively. Then \\eqref{eqn:quadrature} amounts to solving a shifted system of equations for each time point,\n\n", "itemtype": "equation", "pos": 56321, "prevtext": "\nwhere ${ \\mathbf{{K}} }$ and ${ \\mathbf{{M}} }$ denote the stiffness and mass matrices respectively. Instead of using a traditional time-stepping scheme to solve these equations, the Laplace transform-based exponential time integrator is used, as described in \\cite{Bakhos2015940}. The main idea is that a contour integral representation of the inverse Laplace transform chosen on the modified Talbot contour can be used to efficiently solve the groundwater equations \\cite{Weid06}.  The solution at a given time $t$ is given by,\n\n", "index": 7, "text": "\\begin{equation}  \\label{eqn:quadrature} \\phi_h(t)  \\approx \\sum_{k = 1}^{N_z} w_k({ \\mathbf{{K}} }+z_k{ \\mathbf{{M}} })^{-1}\\left({ \\mathbf{{M}} }\\phi_0 + \\hat{q}(z_k){ \\mathbf{{b}} } \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\phi_{h}(t)\\approx\\sum_{k=1}^{N_{z}}w_{k}({\\mathbf{{K}}}+z_{k}{\\mathbf{{M}}})^%&#10;{-1}\\left({\\mathbf{{M}}}\\phi_{0}+\\hat{q}(z_{k}){\\mathbf{{b}}}\\right)\" display=\"block\"><mrow><mrow><msub><mi>\u03d5</mi><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>z</mi></msub></munderover><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>+</mo><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\ud835\udc0c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mi>\ud835\udc0c</mi><mo>\u2062</mo><msub><mi>\u03d5</mi><mn>0</mn></msub></mrow><mo>+</mo><mrow><mover accent=\"true\"><mi>q</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc1b</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05883.tex", "nexttext": "\nIn the experiments presented later in this section, we solve for ${ \\mathbf{{b}} }$.\n\nWe consider a $2$D depth-averaged aquifer with zero Dirichlet boundary conditions on all boundaries. The domain size is square of size $100$m $\\times$ $100$m. For the log conductivity field, we use a randomly generated field from the exponential covariance kernel,\n\n", "itemtype": "equation", "pos": 56716, "prevtext": "\nwith $w_k$ and $z_k$ being the weights and nodes of the quadrature scheme, respectively. Then \\eqref{eqn:quadrature} amounts to solving a shifted system of equations for each time point,\n\n", "index": 9, "text": "\\begin{equation}\n\\left( { \\mathbf{{K}} } + z_k { \\mathbf{{M}} } \\right) X_k = [{ \\mathbf{{b}} },\\quad { \\mathbf{{M}} } \\phi_0], \\qquad k = 1, \\dots, N_z/2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\left({\\mathbf{{K}}}+z_{k}{\\mathbf{{M}}}\\right)X_{k}=[{\\mathbf{{b}}},\\quad{%&#10;\\mathbf{{M}}}\\phi_{0}],\\qquad k=1,\\dots,N_{z}/2\" display=\"block\"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><mi>\ud835\udc0a</mi><mo>+</mo><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\ud835\udc0c</mi></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>X</mi><mi>k</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mi>\ud835\udc1b</mi><mo rspace=\"12.5pt\">,</mo><mrow><mi>\ud835\udc0c</mi><mo>\u2062</mo><msub><mi>\u03d5</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><msub><mi>N</mi><mi>z</mi></msub><mo>/</mo><mn>2</mn></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05883.tex", "nexttext": "\nThe mean conductivity was chosen to be $\\mu_{K} = 10^{-3.5}$ [m$^2$/s] and the variance was chosen to be $\\sigma^2_K = 1.6$. The specific storage in this example was chosen to be constant with $S_s = 10^{-5}$.  There is one pumping source located at $(50,50)$, pumping at a constant rate of $0.85$ L/s. We are interested in the solution at three time instances which we will refer to as {\\em early} ($1$ min), {\\em middle} ($15$ min) and {\\em late} ($40$ min) - the shifts $z_k$ are shown in Figure \\ref{fig:contours}. Note that for the middle and late matrices, the shifts are clustered together more tightly. The matrices have system size $10201 \\times 10201$.\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[scale = 0.35]{contours_n40.pdf}\n\\caption{Plot of contours corresponding to $N_z = 40$. Because of symmetry, only half the contour plot is shown. Here $N_z$ is the number of quadrature points in Equation~\\eqref{eqn:quadrature}.  $t = 1$ min corresponds to the shifts for the early matrices, $t=15$ min to those for the middle matrices, and $t=40$ min to those for the late matrices.}\n\\label{fig:contours}\n\\end{figure}\n\nWe provide results for the early and middle THT matrices.\\footnote{For the late THT matrices, the magnitude of each shift is small enough that reusing ${ \\mathbf{{P}} }_0$ leads to the lowest \ntotal runtime.}  Tables \\ref{table:THTearlyILUTPall} and \\ref{table:THTmiddleILUTPall} show that computing a new ILUTP preconditioner for every system results in the lowest number of GMRES iterations, but the longest overall time.  Tables \\ref{table:THTearlyILUTPonce}, \\ref{table:THTearlySAMall},  \\ref{table:THTmiddleILUTPonce}, and \\ref{table:THTmiddleSAMall} show that when we apply the SAM update at each shift, we achieve fewer total GMRES iterations as compared with just reusing ${ \\mathbf{{P}} }_0$ for all shifts.\n\nWhile the computation of a SAM update is a factor ten cheaper than \nthat of an ILUTP, computing an update at every shift is still too expensive.  \nTherefore, it makes sense to do an update at selected shifts.  A\nsimple choice is to do a single SAM update at shift 10 (halfway), and \nreuse that update for all subsequent systems. For comparison,\nwe also try single SAM updates for shifts 5 or 15. \nWe present the results for the early and middle THT matrices in Tables \\ref{table:THTearlySAM5}-\\ref{table:THTearlySAM15} and \\ref{table:THTmiddleSAM5}-\\ref{table:THTmiddleSAM15}.  We experimented with indicators for updating, but although results were good we did not find a single indicator that was best in all cases.  However, our results demonstrate the potential of a SAM update at selected shifts.\n\nNote that when applying the SAM update at each shift, the number of GMRES iterations increases as the norm of the residual of the LS map increases.  GMRES iterations are also fewer when the eigenvalues of the ideal map are clustered, potentially with some outliers (see Figures \\ref{fig:Early} and \\ref{fig:Middle}), and when the eigenvalues of the preconditioned matrix are clustered near one.  For the early THT matrices, Figures \\ref{fig:EarlyReuse} and \\ref{fig:EarlySAM} show that the eigenvalues of ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0$ are more clustered than those of ${ \\mathbf{{A}} }_k{ \\mathbf{{P}} }_0$, especially for the later shifts.\n\nColumn pivoting in computing the ILUTP results in a matrix ${ \\mathbf{{Q}} }$ such that ${ \\mathbf{{A}} }_0{ \\mathbf{{Q}} } \\simeq { \\mathbf{{LU}} }$ and ${ \\mathbf{{A}} }_0^{-1} \\simeq { \\mathbf{{Q}} }{ \\mathbf{{U}} }^{-1}{ \\mathbf{{L}} }^{-1} = { \\mathbf{{P}} }_0$.\\footnote{In practice, we never invert these matrices but rather use forward/backward solves for ${ \\mathbf{{L}} }$ and ${ \\mathbf{{U}} }$.  Applying ${ \\mathbf{{Q}} }$ (or ${ \\mathbf{{Q}} }^{-1}$) amounts to reordering the components of a vector.}  When applying the SAM update to the ILUTP preconditioner, we get\n\\begin{eqnarray*}\n  { \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0 = ({ \\mathbf{{N}} }_k{ \\mathbf{{Q}} }) { \\mathbf{{U}} }^{-1}{ \\mathbf{{L}} }^{-1} = {\\widetilde}{{ \\mathbf{{N}} }}_k{ \\mathbf{{U}} }^{-1}{ \\mathbf{{L}} }^{-1} ,\n\\end{eqnarray*}\nwith ${\\widetilde}{{ \\mathbf{{N}} }}_k = { \\mathbf{{N}} }_k { \\mathbf{{Q}} }$. So, an additional advantage (in this case)\nof the SAM is that it absorbs this permutation,\nwhich saves a bit of time in the preconditioned matrix-vector product.  \nWhen reusing the initial ILUTP, however, the ${ \\mathbf{{U}} }$ factor must be permuted. \nIn MATLAB{\\textsuperscript{\\textregistered}} this leads to a slightly higher runtime for \nthe back-substitution with ${ \\mathbf{{U}} }$. This explains the slightly increased \ntime for GMRES iterations when reusing the ILUTP preconditioner compared \nwith using a new ILUTP preconditioner for a similar number of iterations.\n\nWe also apply the AINV updates to the THT matrices.  This update scheme specifies that we compute an AINV preconditioner for ${ \\mathbf{{P}} }_0 = { \\mathbf{{Z}} }{ \\mathbf{{D}} }^{-1}{ \\mathbf{{W}} }^T  \\approx { \\mathbf{{A}} }^{-1}$, with ${ \\mathbf{{Z}} }$, ${ \\mathbf{{W}} }$ unit upper triangular and ${ \\mathbf{{D}} }$ diagonal.  For this application, it turns out that this type of preconditioner is expensive to compute and substantially less effective than ILUTP.  While computation of the updates is inexpensive (and the updated preconditioner preserves GMRES iterations for the first few shifts), this preconditioner type and update are not effective for this class of problems.  Results are provided in Tables \\ref{table:THTearlyAINV}-\\ref{table:THTlateAINV}.\n\n\\begin{table}[h]\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&8.89&0.076&22\\\\\\hline\n\\textbf{2}&8.88&0.081&22\\\\\\hline\n\\textbf{3}&8.89&0.078&22\\\\\\hline\n\\textbf{4}&8.88&0.082&23\\\\\\hline\n\\textbf{5}&8.90&0.086&23\\\\\\hline\n\\textbf{10}&8.98&0.095&24\\\\\\hline\n\\textbf{15}&9.10&0.098&27\\\\\\hline\n\\textbf{20}&9.29&0.11&28\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with ILUTP computed at each shift (total time 182.23 s, total iterations 498).}\n\\label{table:THTearlyILUTPall}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&9.01&0.14&22\\\\\\hline\n\\textbf{2}&0&0.19&23\\\\\\hline\n\\textbf{3}&0&0.18&25\\\\\\hline\n\\textbf{4}&0&0.19&27\\\\\\hline\n\\textbf{5}&0&0.23&30\\\\\\hline\n\\textbf{10}&0&0.47&54\\\\\\hline\n\\textbf{15}&0&1.11&129\\\\\\hline\n\\textbf{20}&0&2.93&325\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with initial ILUTP reused (total time 25.69 s, total iterations 1975).}\n\\label{table:THTearlyILUTPonce}\n}\n\n\n\\vspace{4mm}\n\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&8.84&0.068&22\\\\\\hline\n\\textbf{2}&0.95&0.087&23\\\\\\hline\n\\textbf{3}&0.88&0.085&24\\\\\\hline\n\\textbf{4}&0.89&0.089&25\\\\\\hline\n\\textbf{5}&0.77&0.10&27\\\\\\hline\n\\textbf{10}&0.78&0.17&41\\\\\\hline\n\\textbf{15}&0.87&0.45&87\\\\\\hline\n\\textbf{20}&0.85&0.90&190\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with ILUTP computed at first shift and SAM updates computed for remaining shifts (total time 30.38 s, total iterations 1312).}\n\\label{table:THTearlySAMall}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{Shift 1}&8.88&0.072&22\\\\\\hline\n\\textbf{2}&0&0.14&23\\\\\\hline\n\\textbf{3}&0&0.15&25\\\\\\hline\n\\textbf{4}&0&0.17&27\\\\\\hline\n\\textbf{5}&0.93&0.099&27\\\\\\hline\n\\textbf{10}&0&0.22&47\\\\\\hline\n\\textbf{15}&0&0.53&115\\\\\\hline\n\\textbf{20}&0&1.43&301\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with ILUTP computed at first shift and SAM update only computed at shift 5 and reused for subsequent shifts (total time 18.085 s, total iterations 1763).}\n\\label{table:THTearlySAM5}\n}\n\n\n\\vspace{4mm}\n\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{Shift 1}&8.94&0.073&22\\\\\\hline\n\\textbf{2}&0&0.14&23\\\\\\hline\n\\textbf{3}&0&0.15&25\\\\\\hline\n\\textbf{4}&0&0.17&27\\\\\\hline\n\\textbf{5}&0&0.19&30\\\\\\hline\n\\textbf{10}&0.94&0.18&41\\\\\\hline\n\\textbf{15}&0&0.48&100\\\\\\hline\n\\textbf{20}&0&1.54&282\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with ILUTP computed at first shift and SAM update only computed at shift 10 and reused for subsequent shifts (total time 18.68 s, total iterations 1645).}\n\\label{table:THTearlySAM10}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{Shift 1}&9.015&0.075&22\\\\\\hline\n\\textbf{2}&0&0.14&23\\\\\\hline\n\\textbf{3}&0&0.15&25\\\\\\hline\n\\textbf{4}&0&0.17&27\\\\\\hline\n\\textbf{5}&0&0.19&30\\\\\\hline\n\\textbf{10}&0&0.40&54\\\\\\hline\n\\textbf{15}&0.98&0.40&87\\\\\\hline\n\\textbf{20}&0&1.055&226\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with ILUTP computed at first shift and SAM update only computed at shift 15 and reused for subsequent shifts (total time 18.80 s, total iterations 1564).}\n\\label{table:THTearlySAM15}\n}\n\\end{table}\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&9.057&0.085&24\\\\\\hline\n\\textbf{2}&9.032&0.082&24\\\\\\hline\n\\textbf{3}&8.99&0.08&24\\\\\\hline\n\\textbf{4}&8.99&0.080&24\\\\\\hline\n\\textbf{5}&8.94&0.080&24\\\\\\hline\n\\textbf{10}&9.049&0.091&26\\\\\\hline\n\\textbf{15}&9.008&0.096&27\\\\\\hline\n\\textbf{20}&9.098&0.10&28\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP computed at each shift (total time 182.023 s, total iterations 514).}\n\\label{table:THTmiddleILUTPall}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&8.95&0.083&24\\\\\\hline\n\\textbf{2}&0&0.20&24\\\\\\hline\n\\textbf{3}&0&0.21&25\\\\\\hline\n\\textbf{4}&0&0.20&25\\\\\\hline\n\\textbf{5}&0&0.22&26\\\\\\hline\n\\textbf{10}&0&0.31&35\\\\\\hline\n\\textbf{15}&0&0.44&47\\\\\\hline\n\\textbf{20}&0&0.99&111\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP reused (total time 16.80 s, total iterations 880).}\n\\label{table:THTmiddleILUTPonce}\n}\n\n\n\\vspace{4mm}\n\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&9.00&0.081&24\\\\\\hline\n\\textbf{2}&1.026&0.086&24\\\\\\hline\n\\textbf{3}&0.86&0.091&24\\\\\\hline\n\\textbf{4}&0.83&0.091&25\\\\\\hline\n\\textbf{5}&0.83&0.095&26\\\\\\hline\n\\textbf{10}&0.86&0.13&32\\\\\\hline\n\\textbf{15}&0.84&0.19&42\\\\\\hline\n\\textbf{20}&0.81&0.39&79\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP computed at first shift and SAM update computed for remaining shifts (total time 28.50 s, total iterations 736).}\n\\label{table:THTmiddleSAMall}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&8.97&0.081&24\\\\\\hline\n\\textbf{2}&0&0.21&24\\\\\\hline\n\\textbf{3}&0&0.22&25\\\\\\hline\n\\textbf{4}&0&0.22&25\\\\\\hline\n\\textbf{5}&1.069&0.11&26\\\\\\hline\n\\textbf{10}&0&0.19&34\\\\\\hline\n\\textbf{15}&0&0.28&45\\\\\\hline\n\\textbf{20}&0&0.59&102\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP computed at first shift and SAM update only applied at shift 5 (total time 15.084 s, total iterations 846).}\n\\label{table:THTmiddleSAM5}\n}\n\n\n\\vspace{4mm}\n\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&8.95&0.078&24\\\\\\hline\n\\textbf{2}&0&0.21&24\\\\\\hline\n\\textbf{3}&0&0.22&25\\\\\\hline\n\\textbf{4}&0&0.22&25\\\\\\hline\n\\textbf{5}&0&0.23&26\\\\\\hline\n\\textbf{10}&1.06&0.13&32\\\\\\hline\n\\textbf{15}&0&0.24&44\\\\\\hline\n\\textbf{20}&0&0.62&98\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP computed at first shift and SAM update only applied at shift 10 (total time 15.24 s, total iterations 814).}\n\\label{table:THTmiddleSAM10}\n\n\n}\n\\hfill\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&9.00&0.082&24\\\\\\hline\n\\textbf{2}&0&0.21&24\\\\\\hline\n\\textbf{3}&0&0.22&25\\\\\\hline\n\\textbf{4}&0&0.21&25\\\\\\hline\n\\textbf{5}&0&0.23&26\\\\\\hline\n\\textbf{10}&0&0.25&35\\\\\\hline\n\\textbf{15}&0.98&0.20&42\\\\\\hline\n\\textbf{20}&0&0.46&91\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT middle matrices with ILUTP computed at first shift and SAM update only applied at shift 15 (total time 15.14 s, total iterations 797).}\n\\label{table:THTmiddleSAM15}\n}\n\\end{table}\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&156.18&1.19&241\\\\\\hline\n\\textbf{2}&0.22&1.21&246\\\\\\hline\n\\textbf{3}&0.23&1.29&262\\\\\\hline\n\\textbf{4}&0.23&1.45&285\\\\\\hline\n\\textbf{5}&0.23&1.73&346\\\\\\hline\n\\textbf{10}&0.23&6.13&1239\\\\\\hline\n\\textbf{15}&0.23&20.84&4150\\\\\\hline\n\\textbf{20}&0.23&50.24&10202\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for selected shifts for THT early matrices with (full) robust AINV computed once with AINV updates applied to remaining shifts (total time 449.88 s, total iterations 57951).}\n\\label{table:THTearlyAINV}\n}\n\\hfil\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&161.25&1.31&265\\\\\\hline\n\\textbf{2}&0.22&1.34&277\\\\\\hline\n\\textbf{3}&0.22&1.38&282\\\\\\hline\n\\textbf{4}&0.22&1.47&298\\\\\\hline\n\\textbf{5}&0.22&1.49&298\\\\\\hline\n\\textbf{10}&0.22&2.54&513\\\\\\hline\n\\textbf{15}&0.22&5.67&1147\\\\\\hline\n\\textbf{20}&0.22&25.38&5101\\\\\\hline\n\n\n\n\\end{tabular}\n\n}\n\\caption{Timings for selected shifts for THT middle matrices with (full) robust AINV computed once with AINV updates applied to remaining shifts (total time 278.31 s, total iterations 22879).}\n\\label{table:THTmiddleAINV}\n}\n\\vspace{4mm}\n\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Shift}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1}&149.55&1.24&247\\\\\\hline\n\\textbf{2}&0.22&1.26&250\\\\\\hline\n\\textbf{3}&0.22&1.26&251\\\\\\hline\n\\textbf{4}&0.22&1.29&261\\\\\\hline\n\\textbf{5}&0.22&1.36&281\\\\\\hline\n\\textbf{10}&0.22&1.75&356\\\\\\hline\n\\textbf{15}&0.22&2.73&560\\\\\\hline\n\\textbf{20}&0.23&7.24&1467\\\\\\hline\n\n\n\n\\end{tabular}\n\n}\n\\caption{Timings for selected shifts for THT late matrices with (full) robust AINV computed once with AINV updates applied to remaining shifts (total time 204.085 s, total iterations 10251).}\n\\label{table:THTlateAINV}\n}\n\n\\end{table}\n\n\\begin{figure}\n\\begin{center}\n\n\n\n\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyReuse5.pdf}\n\t\t\\subcaption{Shift 5}\n        \\end{subfigure}\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyReuse10.pdf}\n\t\t\\subcaption{Shift 10}\n        \\end{subfigure}\n\n                \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyReuse15.pdf}\n\t\t\\subcaption{Shift 15}\n        \\end{subfigure}\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlyReuse20.pdf}\n\t\t\\subcaption{Shift 20 (eigenvalue at $(5.72,-2.478)$ omitted).}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the preconditioned early THT matrices, ${ \\mathbf{{A}} }_k{ \\mathbf{{P}} }_0$, for selected shifts with the ILUTP preconditioner for ${ \\mathbf{{A}} }_0$ reused for all shifts.}\n\t  \\label{fig:EarlyReuse}\n\\end{center}\n\\end{figure}\n\\begin{figure}\n\\begin{center}\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlySAM5.pdf}\n\t\t\\subcaption{Shift 5}\n        \\end{subfigure}\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlySAM10.pdf}\n\t\t\\subcaption{Shift 10}\n        \\end{subfigure}\n\n                \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlySAM15.pdf}\n\t\t\\subcaption{Shift 15}\n        \\end{subfigure}\n        \\begin{subfigure}{.4\\textwidth}\n        \t\t\\includegraphics[width=\\linewidth]{EarlySAM20.pdf}\n\t\t\\subcaption{Shift 20}\n        \\end{subfigure}\n\t  \\caption{Eigenvalues of the preconditioned early THT matrices, ${ \\mathbf{{A}} }_k{ \\mathbf{{N}} }_k{ \\mathbf{{P}} }_0$, for selected shifts with SAM updates applied to shifts 2 through 20.}\n\t  \\label{fig:EarlySAM}\n\\end{center}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Model Reduction}\\label{sec:ModRed}\nAnother set of linear systems arise in interpolatory model reduction, in particular, in the Iterative Rational Krylov Algorithm (IRKA)  \\cite{GugeAnth08}.  The linear dynamical system\n\\begin{eqnarray*}\n{ \\mathbf{{E}} }\\dot{{ \\mathbf{{x}} }}(t) + { \\mathbf{{A}} }{ \\mathbf{{x}} }(t) = { \\mathbf{{b}} } u(t), \\hspace{4mm} y(t) = { \\mathbf{{c}} }^T{ \\mathbf{{x}} }(t),\n\\end{eqnarray*}\nwith ${ \\mathbf{{A}} } \\in {\\mathbb{R}^{n \\times n}}$, ${ \\mathbf{{b}} }, { \\mathbf{{c}} } \\in {\\mathbb{R}^n}$, and $u(t), y(t) \\in {\\mathbb{R}}$, has the transfer function\n\\begin{eqnarray*}\nH(s) = { \\mathbf{{c}} }^T(s{ \\mathbf{{E}} }-{ \\mathbf{{A}} })^{-1}{ \\mathbf{{b}} }.\n\\end{eqnarray*}\nIRKA is a fixed point iteration that aims to find a reduced order transfer function\n\\begin{eqnarray*}\n  H_r(s) = { \\mathbf{{c}} }_r^T(s{ \\mathbf{{E}} }_r-{ \\mathbf{{A}} }_r)^{-1}{ \\mathbf{{b}} }_r,\n\\end{eqnarray*}\nwith $H_r(s_j) = H(s_j)$ for a set of interpolation points, $s_j$, and $r \\ll n$, such that\n\\begin{eqnarray*}\n  {\\|{H_r(s)-H(s)}\\|_{\\mathcal{H}_2}}\n\\end{eqnarray*}\nis minimized.  For ${ \\mathbf{{V}} }_r, { \\mathbf{{W}} }_r \\in {\\mathbb{R}^{n \\times r}}$,\n\\begin{eqnarray} \\label{eq:NewMatsVecs}\n{ \\mathbf{{A}} }_r = { \\mathbf{{W}} }_r^T{ \\mathbf{{A}} }{ \\mathbf{{V}} }_r, \\hspace{4mm} { \\mathbf{{E}} }_r = { \\mathbf{{W}} }_r^T{ \\mathbf{{E}} }{ \\mathbf{{V}} }_r, \\hspace{4mm} { \\mathbf{{b}} }_r = { \\mathbf{{W}} }_r^T{ \\mathbf{{b}} }, \\hspace{4mm} { \\mathbf{{c}} }_r = { \\mathbf{{V}} }_r^T{ \\mathbf{{c}} }.\n\\end{eqnarray}\nThe interpolation points are initialized by the user, but within the algorithm \nthe $s_j$ are updated to be the mirror images of the generalized \neigenvalues of ${ \\mathbf{{A}} }_r$ and ${ \\mathbf{{E}} }_r$ \\cite{GugeAnth08}.  Until convergence \n(on the fixed points, $s_j$) is achieved, the algorithm given in \n\\cite{GugeAnth08} iteratively updates the reduced order model by building the matrices\n\\begin{eqnarray}\\label{eq:V}\n{ \\mathbf{{V}} }_r = [(s_1{ \\mathbf{{E}} }-{ \\mathbf{{A}} })^{-1}{ \\mathbf{{b}} },\\dots,(s_r{ \\mathbf{{E}} }-{ \\mathbf{{A}} })^{-1}{ \\mathbf{{b}} }]\n\\end{eqnarray}\nand\n\\begin{eqnarray}\\label{eq:W}\n{ \\mathbf{{W}} }_r = [(s_1{ \\mathbf{{E}} }^T-{ \\mathbf{{A}} }^T)^{-1}{ \\mathbf{{c}} },\\dots,(s_r{ \\mathbf{{E}} }^T-{ \\mathbf{{A}} }^T)^{-1}{ \\mathbf{{c}} }].\n\\end{eqnarray}\nThe matrices in (\\ref{eq:V}) and (\\ref{eq:W}) give the new ${ \\mathbf{{A}} }_r$, ${ \\mathbf{{E}} }_r$, ${ \\mathbf{{b}} }_r$, and ${ \\mathbf{{c}} }_r$ as in (\\ref{eq:NewMatsVecs}).  Since it may take many iterations until IRKA converges to the final set of $s_j$, many shifted systems must be solved in computing (\\ref{eq:V}) and (\\ref{eq:W}).  We refer to the set of shifts for an IRKA iteration as a {\\em batch}.  For more detail on interpolatory model reduction and IRKA see \\cite{AnthBeat10,BeatGuge09,BeatGuge14,GugeAnth04,GugeAnth08}.\n\nOther approaches have been used to reduce the cost of the linear solves in (\\ref{eq:V}) and (\\ref{eq:W}).  In \\cite{BeatGuge12}, inexact solves within a Petrov-Galerkin framework are used to reduce this cost.  In \\cite{AhujdeSt12}, the recycling BiCG algorithm is proposed and applied effectively to a parametric model order reduction example, while recycling BiCGSTAB is used \n(also for parametric model reduction) in \\cite{AhujBenn15}.  More discussion of recycling Krylov subspace methods specifically applied to model reduction applications can \nbe found in  \\cite{FengBenn09, FengBenn13}.\n\nWe give results for two sets of matrices, Rail and Flow, which can be found in \\cite{repo15} along with additional information.  The Rail matrices come from a semi-discretized heat transfer equation for the cooling of steel beams. The matrices ${ \\mathbf{{A}} }$ and ${ \\mathbf{{E}} }$ are very sparse with ${ \\mathbf{{A}} }$ symmetric negative definite, ${ \\mathbf{{E}} }$ positive definite, and $n \\approx 80$ $000$.  We give results for three batches of six shifts, which are real and range from $O(10^{-5})$ to $O(10)$.  The shifts are provided in Table \\ref{table:RailShifts}.\n\nFor the Rail matrices, the number of nonzeros in ${ \\mathbf{{A}} }$ is 553921, that in ${ \\mathbf{{E}} }$ is 554913, making the matrices very sparse and therefore the matrix-vector product very cheap. \nIn addition, the ILUTP preconditioner for these matrices is remarkably good.  \nAs a result, the best choice is to reuse the initial preconditioner for all shifted matrices, which achieves the fastest overall computation time.  However, we do see that applying the SAM updates at each shift substantially reduces the total number of GMRES iterations as shown in Tables \\ref{table:Rail_ILUTP_reuse} and \\ref{table:Rail_ILUTPSAM_all}.  We omit the results for computing the preconditioner at each shift.  While the total GMRES iterations were the fewest when computing the ILUTP for each shift, the overall computation time was very high (over two hours).\n\nThe Flow matrices arise in a simulation of the heat exchange between a solid body and a fluid flow. Rather than using computational fluid dynamics, which is quite expensive,\nan alternative approach is to include a flow region with a given velocity profile \\cite{MoosRudn04}. However, this requires that the number of elements is\ndrastically increased. To deal with this, model reduction is used to effectively describe the system \\cite{MoosRudn04}. For further description of the problem as well as how model reduction is applied, we refer to \\cite{MoosRudn04, RudnKorv02}.\nThe model reduction involves the sparse matrices ${ \\mathbf{{A}} }$ and ${ \\mathbf{{E}} }$, where $n \\approx 10$ $000$.\nWe use three batches of six shifts, which are real and range from $O(1)$ to $O(10^4)$.  The shifts for the Flow matrices are provided in Table \\ref{table:FlowShifts}.\n\nAlthough ${ \\mathbf{{A}} }$ is not symmetric, and therefore the generalized eigenvalues \nof ${ \\mathbf{{A}} }$ and ${ \\mathbf{{E}} }$ may be complex, \n\nit turns out that for the three steps of IRKA that we used for \nour experiments, the shifts remain real. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable \\ref{table:Flow_ILUTPall} shows that computing a new ILUTP preconditioner \nat each shift results in the the lowest total number of iterations \nbut the highest overall computation time.  When we apply the SAM updates \nat each shift, the number of iterations increases, but this still yields \na lower total computation time compared with reusing the initial ILUTP \nfor all shifts, as shown in \nTables \\ref{table:Flow_ILUTPSAM_all} and \\ref{table:Flow_ILUTPonce}.   \nIn fact, at the first shift of batch three, \nthe initial ILUTP is not a good preconditioner and GMRES fails to converge. \nHowever, computing a SAM update at this shift produces a preconditioner \nfor which GMRES does converge.\\footnote{The maximum number of iterations \nis set to 5000; (5001) in Table \\ref{table:Flow_ILUTPonce} \nindicates GMRES does not converge.}   Here we see that reusing \nthe ILUTP cannot be done for all shifts.  Again note that when reusing \nthe ILUTP, the ${ \\mathbf{{U}} }$ factor must absorb any permutation that results \nfrom the initial factorization of ${ \\mathbf{{A}} }_0$.  However, when using the SAM \nupdates, the map absorbs the permutation and ${ \\mathbf{{U}} }$ remains upper triangular.  \nThis is why GMRES takes longer for a similar number of iterations.\n\nAs with the THT matrices, we also apply the SAM update at selected shifts of the Flow matrices.  We compute an ILUTP preconditioner for the first shift of the first batch.  We apply a SAM update at the sixth and largest shift of each batch and again at the start of each new batch.  It is at these shifts that we see some of the largest numbers of GMRES iterations when reusing the ILUTP for all shifts.  As seen in Table \\ref{table:Flow_ILUTPSAM_select}, we achieve the lowest overall computation time when we apply the SAM updates at these selected shifts.\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Batch 1}&\\textbf{Batch 2}&\\textbf{Batch 3}\\\\\\hline\n\\textbf{Shift 1}&1e-05&1.8347e-05&1.8447e-05\\\\\\hline\n\\textbf{2}&0.00013804&0.00032778&0.0003295\\\\\\hline\n\\textbf{3}&0.0019055&0.0046185&0.0046991\\\\\\hline\n\\textbf{4}&0.026303&0.056949&0.067322\\\\\\hline\n\\textbf{5}&0.36308&0.52928&0.72306\\\\\\hline\n\\textbf{6}&5.0119&8.4359&10.9255\\\\\\hline\n\\end{tabular}\n}\n\\caption{Shifts for the Rail Matrices.}\n\\label{table:RailShifts}\n}\n\\hfil \\hspace{12mm}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Batch 1}&\\textbf{Batch 2}&\\textbf{Batch 3}\\\\\\hline\n\\textbf{Shift 1}&1.4091&1.4106&1.411\\\\\\hline\n\\textbf{2}&28.1234&29.3905&29.7024\\\\\\hline\n\\textbf{3}&150.6975&158.8259&160.8\\\\\\hline\n\\textbf{4}&669.2639&683.6133&687.1313\\\\\\hline\n\\textbf{5}&3536.6535&3555.1646&3559.7271\\\\\\hline\n\\textbf{6}&17329.4291&17344.2626&17347.9311\\\\\\hline\n\\end{tabular}\n}\n\\caption{Shifts for the Flow Matrices.}\n\\label{table:FlowShifts}\n}\n\\end{table}\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{B/S}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&437.30&1.14&52\\\\\\hline\n\\textbf{1/2}&0&0.49&30\\\\\\hline\n\\textbf{1/3}&0&0.28&15\\\\\\hline\n\\textbf{1/4}&0&0.19&8\\\\\\hline\n\\textbf{1/5}&0&0.36&20\\\\\\hline\n\\textbf{1/6}&0&1.59&77\\\\\\hline\n\\textbf{2/1}&0&0.89&48\\\\\\hline\n\\textbf{2/2}&0&0.42&25\\\\\\hline\n\\textbf{2/3}&0&0.24&12\\\\\\hline\n\\textbf{2/4}&0&0.20&9\\\\\\hline\n\\textbf{2/5}&0&0.41&24\\\\\\hline\n\\textbf{2/6}&0&2.39&97\\\\\\hline\n\\textbf{3/1}&0&0.87&48\\\\\\hline\n\\textbf{3/2}&0&0.42&25\\\\\\hline\n\\textbf{3/3}&0&0.24&12\\\\\\hline\n\\textbf{3/4}&0&0.20&9\\\\\\hline\n\\textbf{3/5}&0&0.48&27\\\\\\hline\n\\textbf{3/6}&0&2.67&109\\\\\\hline\n\n\n\n\\end{tabular}\n}\n\\caption{Timings for Rail matrices with ILUTP for the first system reused for all shifts (total time 450.77 s, total iterations 647), B/S = Batch Number/Shift Number.}\n\\label{table:Rail_ILUTP_reuse}\n}\n\\hfil \\hspace{12mm}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{B/S}&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&438.38&1.075&52\\\\\\hline\n\\textbf{1/2}&29.96&0.59&30\\\\\\hline\n\\textbf{1/3}&10.36&0.32&15\\\\\\hline\n\\textbf{1/4}&10.13&0.22&8\\\\\\hline\n\\textbf{1/5}&10.40&0.35&17\\\\\\hline\n\\textbf{1/6}&10.13&0.51&26\\\\\\hline\n\\textbf{2/1}&11.74&0.99&48\\\\\\hline\n\\textbf{2/2}&10.36&0.49&25\\\\\\hline\n\\textbf{2/3}&10.43&0.27&12\\\\\\hline\n\\textbf{2/4}&10.47&0.23&9\\\\\\hline\n\\textbf{2/5}&10.19&0.39&20\\\\\\hline\n\\textbf{2/6}&10.48&0.74&38\\\\\\hline\n\\textbf{3/1}&11.51&0.96&48\\\\\\hline\n\\textbf{3/2}&10.24&0.48&25\\\\\\hline\n\\textbf{3/3}&10.29&0.27&12\\\\\\hline\n\\textbf{3/4}&10.25&0.23&9\\\\\\hline\n\\textbf{3/5}&10.29&0.43&22\\\\\\hline\n\\textbf{3/6}&10.12&2.32&93\\\\\\hline\n\n\n\n\\end{tabular}\n\n}\n\\caption{Timings for Rail matrices with ILUTP computed once and SAM updates applied for all other shifts (total time 646.60 s, total iterations 509).  B/S = Batch Number/Shift Number.}\n\\label{table:Rail_ILUTPSAM_all}\n}\n\n\\end{table}\n\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&4.47&4.90&3591\\\\\\hline\n\\textbf{1/2}&4.23&0.0254&13\\\\\\hline\n\\textbf{1/3}&4.087&0.029&11\\\\\\hline\n\\textbf{1/4}&4.18&0.023&10\\\\\\hline\n\\textbf{1/5}&3.95&0.020&7\\\\\\hline\n\\textbf{1/6}&3.61&0.021&7\\\\\\hline\n\\textbf{2/1}&4.20&0.80&579\\\\\\hline\n\\textbf{2/2}&4.23&0.026&13\\\\\\hline\n\\textbf{2/3}&4.33&0.026&11\\\\\\hline\n\\textbf{2/4}&4.22&0.025&10\\\\\\hline\n\\textbf{2/5}&4.059&0.020&7\\\\\\hline\n\\textbf{2/6}&3.74&0.020&7\\\\\\hline\n\\textbf{3/1}&4.34&0.81&584\\\\\\hline\n\\textbf{3/2}&4.39&0.026&13\\\\\\hline\n\\textbf{3/3}&4.35&0.026&11\\\\\\hline\n\\textbf{3/4}&4.25&0.022&10\\\\\\hline\n\\textbf{3/5}&4.011&0.021&7\\\\\\hline\n\\textbf{3/6}&3.74&0.021&7\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for Flow matrices with ILUTP recomputed for each shift (total time 81.25 s, total iterations 4898). B/S = Batch Number/Shift Number.}\n\\label{table:Flow_ILUTPall}\n}\n\\hfil \\hspace{12mm}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&4.17&4.88&3591\\\\\\hline\n\\textbf{1/2}&0.44&0.028&18\\\\\\hline\n\\textbf{1/3}&0.35&0.034&24\\\\\\hline\n\\textbf{1/4}&0.34&0.14&78\\\\\\hline\n\\textbf{1/5}&0.35&0.19&98\\\\\\hline\n\\textbf{1/6}&0.34&0.61&207\\\\\\hline\n\\textbf{2/1}&0.35&0.94&596\\\\\\hline\n\\textbf{2/2}&0.36&0.028&18\\\\\\hline\n\\textbf{2/3}&0.38&0.037&24\\\\\\hline\n\\textbf{2/4}&0.36&0.32&139\\\\\\hline\n\\textbf{2/5}&0.36&0.19&98\\\\\\hline\n\\textbf{2/6}&0.37&0.62&205\\\\\\hline\n\\textbf{3/1}&0.34&1.89&1204\\\\\\hline\n\\textbf{3/2}&0.36&0.027&18\\\\\\hline\n\\textbf{3/3}&0.35&0.034&24\\\\\\hline\n\\textbf{3/4}&0.37&0.26&123\\\\\\hline\n\\textbf{3/5}&0.35&0.090&59\\\\\\hline\n\\textbf{3/6}&0.36&0.63&207\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for Flow matrices with ILUTP computed once and SAM updates applied for all other shifts (total time 21.23 s, total iterations 6731).  B/S = Batch Number/Shift Number.}\n\\label{table:Flow_ILUTPSAM_all}\n}\n\\end{table}\n\n\\begin{table}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&4.39&4.98&3591\\\\\\hline\n\\textbf{1/2}&0&0.061&18\\\\\\hline\n\\textbf{1/3}&0&0.13&41\\\\\\hline\n\\textbf{1/4}&0&0.97&205\\\\\\hline\n\\textbf{1/5}&0&1.00&211\\\\\\hline\n\\textbf{1/6}&0&1.066&229\\\\\\hline\n\\textbf{2/1}&0&7.47&1623\\\\\\hline\n\\textbf{2/2}&0&0.056&18\\\\\\hline\n\\textbf{2/3}&0&0.13&41\\\\\\hline\n\\textbf{2/4}&0&0.97&202\\\\\\hline\n\\textbf{2/5}&0&0.99&211\\\\\\hline\n\\textbf{2/6}&0&1.14&239\\\\\\hline\n\\textbf{3/1}&0&22.63&(5001)\\\\\\hline\n\\textbf{3/2}&0&0.056&18\\\\\\hline\n\\textbf{3/3}&0&0.13&41\\\\\\hline\n\\textbf{3/4}&0&0.96&207\\\\\\hline\n\\textbf{3/5}&0&0.98&211\\\\\\hline\n\\textbf{3/6}&0&1.029&232\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for Flow matrices with ILUTP for the first system reused for all shifts (total time 49.12 s, total iterations 12339). B/S = Batch Number/Shift Number.}\n\\label{table:Flow_ILUTPonce}\n}\n\\hfil \\hspace{12mm}\n\\parbox{0.45\\linewidth}{\n\\captionsetup{font=scriptsize}\n\n\\centering\n\\scalebox{0.7}{\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n&\\textbf{Prec Time}&\\textbf{GMRES Time}&\\textbf{Iter}\\\\\\hline\n\\textbf{1/1}&4.10&4.85&3591\\\\\\hline\n\\textbf{1/2}&0&0.059&18\\\\\\hline\n\\textbf{1/3}&0&0.13&41\\\\\\hline\n\\textbf{1/4}&0&0.96&205\\\\\\hline\n\\textbf{1/5}&0&0.97&211\\\\\\hline\n\\textbf{1/6}&0.42&0.62&207\\\\\\hline\n\\textbf{2/1}&0.35&0.92&596\\\\\\hline\n\\textbf{2/2}&0&0.028&19\\\\\\hline\n\\textbf{2/3}&0&0.30&134\\\\\\hline\n\\textbf{2/4}&0&0.62&207\\\\\\hline\n\\textbf{2/5}&0&0.62&211\\\\\\hline\n\\textbf{2/6}&0.34&0.63&205\\\\\\hline\n\\textbf{3/1}&0.36&1.82&1204\\\\\\hline\n\\textbf{3/2}&0&0.027&19\\\\\\hline\n\\textbf{3/3}&0&0.058&42\\\\\\hline\n\\textbf{3/4}&0&0.62&206\\\\\\hline\n\\textbf{3/5}&0&0.64&212\\\\\\hline\n\\textbf{3/6}&0.34&0.62&207\\\\\\hline\n\\end{tabular}\n}\n\\caption{Timings for Flow matrices with ILUTP computed once and SAM updates applied at selected shifts and reused until next SAM update computed (total time 20.40 s, total iterations 7535).  B/S = Batch Number/Shift Number.}\n\\label{table:Flow_ILUTPSAM_select}\n}\n\\end{table}\n\n\n\n\\subsection{Indefinite Matrices}\nIn our previous applications, computing a new ILUTP for each system \nis better in terms of keeping \nGMRES iterations low, but it is, in general,\ntoo expensive in terms of time.  Here, we consider linear\nsystems of equations where the computation of the ILUTP preconditioner\nmay fail or may be unstable (resulting in poor preconditioners).\nThis is the case, for example, for indefinite systems \\cite[Chapter~10]{Saad03}.\nIndefinite matrices may arise when discretizing the 2D\nHelmholtz equation for a wave problem in an inhomogeneous\nmedium \\cite{ErlaNabb08,ErlaVuik06}.\nIn such cases, we can select a matrix from\nthe set, or choose an additional matrix, \nfor which the ILUTP algorithm computes an effective preconditioner.\nThen we use SAM updates to\n(approximately) map matrices for which ILUTP may fail to this matrix.\n\n\nThis same idea has also been applied to the Helmholtz equation using other preconditioning approaches. Previous work has successfully used operator-based preconditioners in order to achieve fast convergence of Krylov methods. The shifted Laplace preconditioner (SLP) is used along with multilevel Krylov methods in  \\cite{ErlaNabb08,ErlaVuik06,SheiLaha13}, while a  sweeping preconditioner is constructed layer-by-layer in \\cite{EngqYing11}.  Preconditioning by replacing a subset of the Sommerfield-like boundary conditions of the discretized Helmholtz equation with either Dirichlet or Neumann boundary conditions is examined in \\cite{ElmaOlea98,ElmaOlea99}.\n\n\nOur purpose for this section is to demonstrate another possible use of\nSAM updates; we do not mean to suggest it is a more effective approach than\nthe ones just mentioned.\n\n\nLet ${ \\mathbf{{K}} }_0$ and right hand side ${ \\mathbf{{b}} }$ be generated using a vertex-based \nfinite volume discretization for the 2D Laplacian \non the unit square, with the Dirichlet boundary conditions\n\\begin{eqnarray*}\n  u(x,0) = 1, \\hspace{4mm} u(0,y) = 1, \\hspace{4mm} u(x,1) = 0, \\hspace{4mm} u(1,y) = 0.\n\\end{eqnarray*}\n${ \\mathbf{{K}} }_0$ is\nsymmetric, positive definite and of size $100 \\times 100$.\nWe compute an ILUTP preconditioner, ${ \\mathbf{{P}} }_0$, for ${ \\mathbf{{K}} }_0$,\nand use GMRES to solve the preconditioned system.  We set the\nmaximum number of GMRES iterations to $100$, the relative convergence\ntolerance to $10^{-10}$, and we take the zero vector as the initial\nguess. Next, we consider the systems\n\\begin{eqnarray}\\label{eq:helm}\n  { \\mathbf{{K}} }_i = { \\mathbf{{K}} }_0-s_i{ \\mathbf{{I}} },\n\\end{eqnarray}\nwith ${ \\mathbf{{I}} }$ the identity matrix. We take \n$s_i = i{\\Delta} s$ with ${\\Delta} s = 0.01$, for $i=1,2,\\dots,200$. \nWe solve these systems with preconditioned\nGMRES as above, computing either a new ILUTP preconditioner\nat every shift or updating ${ \\mathbf{{P}} }_0$ with a SAM for each system.\n\n\nFor these small problems, we are not concerned with runtime;\nwe only demonstrate the superior convergence behavior obtained\nwith the updated preconditioners compared with ILUTP.\nAfter some number of shifts, the ILUTP\npreconditioners deteriorate due to instability and the\nnumber of iterations to convergence rapidly increases.\nTo show that the problem is not due to choices in our\nimplementation of the ILUTP preconditioner,\nwe also give results for MATLAB's\n{\\tt ilu}.\nFigure \\ref{fig:defToIndef_eigs} shows the eigenvalues for\nselected ${ \\mathbf{{K}} }_i$; ${ \\mathbf{{K}} }_i$ becomes indefinite by the twentieth shift.\n\n\nFigures \\ref{fig:indefMine} and \\ref{fig:indefMatlab} show that\nwhile ${ \\mathbf{{K}} }_i$ becomes indefinite around the twentieth shift,\nILUTP produces good preconditioners until approximately\nthe $125^{th}$ shift.  At this shift and subsequent shifts, both our\nimplementation of ILUTP and MATLAB's $\\tt ilu$ fail to produce a\ngood preconditioner, and the number of GMRES iterations increases\nsubstantially (or GMRES fails to converge).  However, the SAM updates\ncombined with ${ \\mathbf{{P}} }_0$ are successful in keeping the GMRES iterations\nlow for almost all shifts.\n\n\n\\begin{figure}\n\\begin{center}\n\n  \\begin{subfigure}{0.45\\textwidth}\n    \\includegraphics[width=\\linewidth]{defToIndefMine.pdf}\n\t\\caption{The number of GMRES iterations to converge for\n    the discretized Helmholtz problem computing a new ILUTP preconditioner\n    for each ${ \\mathbf{{K}} }_i$ (blue line) vs. computing an ILUTP preconditioner\n    for ${ \\mathbf{{K}} }_0$ and computing SAM updates for all subsequent ${ \\mathbf{{K}} }_i$ (red line).\n    These results are based on our implementation of the ILUTP preconditioner.}\n    \\label{fig:indefMine}\n  \\end{subfigure}\n  \\hfil\n  \\begin{subfigure}{0.45\\textwidth}\n    \\includegraphics[width=\\linewidth]{defToIndef.pdf}\n\t\\caption{The number of GMRES iterations to converge for\n    the discretized Helmholtz problem computing a new ILUTP preconditioner\n    for each ${ \\mathbf{{K}} }_i$ (blue line) vs. computing an ILUTP preconditioner\n    for ${ \\mathbf{{K}} }_0$ and computing SAM updates for all subsequent ${ \\mathbf{{K}} }_i$ (red line).\n    These results are based on the MATLAB{\\textsuperscript{\\textregistered}} \\ ILUTP preconditioner, {\\tt ilu}.}\n    \\label{fig:indefMatlab}\n  \\end{subfigure}\n\n  \\begin{subfigure}{0.45\\textwidth}\n    \\includegraphics[width=\\linewidth]{defToIndefEigs.pdf}\n    \\caption{Eigenvalues of every tenth matrix, ${ \\mathbf{{K}} }_i$. At\n    the twentieth shift, the matrices become indefinite.}\n\t\\label{fig:defToIndef_eigs}\n  \\end{subfigure}\n\\end{center}\n\\caption{GMRES convergence and selected eigenvalues for a\n  discretized Helmholtz problem.}\n\\label{fig:helm}\n\\end{figure}\n\n\n\\section{Conclusions and Future Work} \\label{sec:concl}\nIn applications that involve many linear systems, recycling a preconditioner \ncan be advantageous.  We develop a flexible update to arbitrary preconditioners \nthat we call the Sparse Approximate Map, or SAM update.  This map is \nmotivated by the Sparse Approximate Inverse; however, rather \nthan approximately inverting a matrix, the SAM update approximately maps one \nmatrix to another nearby matrix for which a good preconditioner is \nalready available.  In this paper, we discuss several applications \nwith systems defined by a shift, but the SAM update can be applied to \nany set of closely related systems. The cost of computing a very good \npreconditioner for a chosen matrix can be amortized over many \nmatrices in a sequence of systems, since our map is cheap to compute.  \nFurther, the map is independent of preconditioner type and quality.\n\nWhen the residual of the LS map is small (in norm) and the initial \npreconditioner is sufficiently good, we can guarantee rapid convergence of \nGMRES for the preconditioned system. We show that the LS map is the best \napproximation to the ideal map in the Frobenius ${ \\mathbf{{A}} }_k$-norm.  This \nresults in a monotonic decrease in the Frobenius-norm of the residual \nfor a sequence of nested sparsity patterns.  We show for several applications \nthat the SAM update leads to a good (updated) preconditioner.\n\n\nFuture work will consider applying the SAM update incrementally as \nin (\\ref{eq:incrementSAM2}) and (\\ref{eq:jkSAM2}).  We also plan to further \nanalyze how well the LS map approximates the ideal map under more general \nconditions as well as determine the conditions under which  our preconditioned \nmatrices - using the SAM updates - take the form (\\ref{eq:smallLowRank}).  \nWe also plan to analyze the SAM updates for more general sequences of \nmatrices.  Finally, we will further analyze and develop good indicators for \nwhen a new map should be computed.\n\n\\section{Acknowledgements} We would like to thank Tania Bakhos,  Arvind Saibaba, and Peter Kitanidis for providing the description of the Transient Hydraulic Tomography method as well as the THT matrices.\n\n\n\\clearpage\n\\bibliographystyle{siam}\n\n\\bibliography{paper_v02_bib}\n\n", "itemtype": "equation", "pos": 57237, "prevtext": "\nIn the experiments presented later in this section, we solve for ${ \\mathbf{{b}} }$.\n\nWe consider a $2$D depth-averaged aquifer with zero Dirichlet boundary conditions on all boundaries. The domain size is square of size $100$m $\\times$ $100$m. For the log conductivity field, we use a randomly generated field from the exponential covariance kernel,\n\n", "index": 11, "text": "\\begin{equation}\n\\kappa(x,y) = 4 \\exp( -2{\\|{x-y}\\|_2}/100)\n \\label{eqn:exp}\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\kappa(x,y)=4\\exp(-2{\\|{x-y}\\|_{2}}/100)\" display=\"block\"><mrow><mrow><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>4</mn><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>/</mo><mn>100</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]