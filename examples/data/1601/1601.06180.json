[{"file": "1601.06180.tex", "nexttext": "\nwhere $\\mathcal{N}(\\cdot {\\,|\\,} \\cdot)$ is the Gaussian PDF, $\\bm{\\mu}_k$ and $\\bm{\\Sigma}_k$ are the means and covariances of the $k^\\text{th}$ component, and ${w}_k$ are the mixture weights with ${w}_k \\geq 0$, $\\sum {w}_k = 1$.\nThe GMM can be interpreted in two ways:\ni) It is a convex combination of PDFs and thus itself a PDF, or\nii) it is a marginal distribution of a distribution ${p}({\\ensuremath{\\mathbf{X}}},Z)$ over ${\\ensuremath{\\mathbf{X}}}$ and a latent, marginalized variable $Z$, where ${p}({\\ensuremath{\\mathbf{X}}} {\\,|\\,} Z = k) = \\mathcal{N}({\\ensuremath{\\mathbf{X}}} {\\,|\\,} \\bm{\\mu}_k, \\bm{\\Sigma}_k)$ and ${p}(Z = k) = {w}_k$.\nThe second interpretation, the LV interpretation, yields a syntactically well-structured model.\nFor example, following the LV interpretation, it is clear how to draw samples from ${p}({\\ensuremath{\\mathbf{X}}})$ by using ancestral sampling.\nThis structure can also be of semantic nature, for instance when $Z$ represents a clustering of ${\\ensuremath{\\mathbf{X}}}$ or when $Z$ is a class variable.\nFurthermore, using the LV interpretation allows the application of the EM algorithm, which is essentially maximum-likelihood learning under missing data\n\\cite{Dempster1977, Ghahramani1994}.\n\n\nMixture models can be seen as a special case of SPNs with a single sum node, corresponding to a single LV.\nMore generally, SPNs can have arbitrarily many sum nodes, each corresponding to its own LV, leading to a hierarchical structured LV model.\nIn \\cite{Poon2011}, the LV interpretation in SPNs was justified by explicitly introducing the LVs in the SPN model, using the so-called \\emph{indicator variables} corresponding to the LVs' states.\nHowever, as shown in this paper, this justification is actually too simplistic, since it is potentially in conflict with the \\emph{completeness} condition \\cite{Poon2011}, leading to an incompletely specified model.\nAs a remedy we propose the \\emph{augmentation} of an SPN, which additionally to the IVs also introduces the so-called \\emph{twin sum nodes}, in order to completely specify the LV model.\nWe further investigate the independency structure of the LV model resulting from augmentation and find a parallel to the local independence assertions in Bayesian networks (BNs) \\cite{Pearl1988, Koller2009}.\nThis allows us to define a BN representation of the augmented SPN which serves as independence map (IMAP).\nUsing our BN interpretation of augmented SPNs, we give a sound derivation of the (soft) EM algorithm for SPNs.\n\n\nClosely related to the LV interpretation is the inference scenario of finding the most-probable-explanation (MPE), i.e.~finding a probability maximizing assignment for all RVs.\nUsing results form \\cite{deCampos2011, Peharz2015b}, we first point out that that this problem is generally NP-hard for SPNs.\nIn \\cite{Poon2011} it was proposed that an MPE solution can be found efficiently\nwhen maximizing over model RVs (i.e. non-latent RVs)\nand LVs. \nThe proposed algorithm replaces sum nodes by max nodes and recovers the solution by using Viterbi-style backtracking. \nHowever, it was not shown that this is a correct algorithm. \nIn this paper, we show that this algorithm is indeed correct, \\emph{when applied to augmented SPNs}. \nWhen applied to \\emph{non-augmented} SPNs, the algorithm still returns an MPE solution of the augmented SPN, but implicitly \nassumes that the weights for each twin sum are all 0 except a single 1. \nThis leads to a phenomenon in MPE inference which we call \\emph{low-depth bias}, i.e. more shallow parts of the SPN are \npreferred during backtracking.\n\n\nThe main contribution in this paper is to provide a sound theoretical foundation for the LV interpretation in SPNs and\nrelated concepts, i.e. the EM algorithm and MPE inference.\nOur theoretical findings are confirmed in experiments on synthetic data and 103 real-world datasets.\n\n\nThe paper is organized as follows: In the remainder of this section we introduce notation, review SPNs and discuss \nrelated work. \nIn Section 2 we propose the augmentation of SPNs, show its soundness as hierarchical LV model and give an interpretation \nas BN. \nFurthermore, we discuss its independency properties and the interpretation of sum-weights as conditional probabilities. \nThe EM algorithm for SPNs is derived in Section 3. In Section 4 we discuss MPE inference for SPNs. \nExperiments are presented in Section 5 and Section 6 concludes the paper. \nProofs for our theoretical results are deferred to the Appendix.\n\n\n\n\n\\subsection{Background and Notation}   \\label{sec:backgroundNotation}\nRVs are denoted by upper-case letters $W$, $X$, $Y$ and $Z$.\nThe set of values of an RV $X$ is denoted by ${\\ensuremath{\\mathbf{val}}}(X)$, where corresponding lower-case letters denote elements of ${\\ensuremath{\\mathbf{val}}}(X)$, e.g.~$x$ is an element of ${\\ensuremath{\\mathbf{val}}}(X)$.\nSets of RVs are denoted by boldface letters $\\mathbf{W}$, ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}}$ and ${\\ensuremath{\\mathbf{Z}}}$.\nFor RV set ${\\ensuremath{\\mathbf{X}}} = \\{X_1,\\dots,X_N\\}$, we define ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}}) = \\bigtimes_{n=1}^N {\\ensuremath{\\mathbf{val}}}(X_n)$ and use corresponding lower-case boldface letters for elements of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$, e.g.~${\\ensuremath{\\mathbf{x}}}$ is an element of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$.\nFor a sub-set ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$, ${{{\\ensuremath{\\mathbf{x}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}}$ denotes the projection of ${\\ensuremath{\\mathbf{x}}}$ onto ${\\ensuremath{\\mathbf{Y}}}$.\n\n\nThe elements of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$ can be interpreted as \\emph{complete} evidence, assigning each RV in ${\\ensuremath{\\mathbf{X}}}$ a fixed value.\n\\emph{Partial} evidence about $X$ is represented as a subset ${\\mathcal{X}} \\subseteq {\\ensuremath{\\mathbf{val}}}(X)$, which is an element of the sigma-algebra ${\\ensuremath{\\mathcal{A}}}_X$ induced by RV $X$.\nFor discrete RVs, we assume ${\\ensuremath{\\mathcal{A}}}_X = 2^{{\\ensuremath{\\mathbf{val}}}(X)}$, i.e.~the power-set of ${\\ensuremath{\\mathbf{val}}}(X)$.\nFor continuous RVs, we use ${\\ensuremath{\\mathcal{A}}}_X = \\{ {\\mathcal{X}} \\in \\bm{\\mathcal{B}} ~|~ {\\mathcal{X}} \\subseteq {\\ensuremath{\\mathbf{val}}}(X) \\}$, where $\\bm{\\mathcal{B}}$ are the Borel-sets over $\\mathbb{R}$.\nFor example, partial evidence ${\\mathcal{X}} = \\{1,3,5\\}$ for a discrete RV $X$ with ${\\ensuremath{\\mathbf{val}}}(X) = \\{1,\\dots,6\\}$ represents evidence\nthat $X$ takes one of the states $1$, $2$ or $3$, and ${\\mathcal{Y}} = [-\\infty,\\pi]$ for a real-valued RV $Y$ represents evidence that $Y$ takes a value\nsmaller than $\\pi$. \nFormally speaking, partial evidence is used to express the domain of \\emph{marginalization or maximization} for a particular RV.\n\n\nFor sets of RVs ${\\ensuremath{\\mathbf{X}}} = \\{X_1, \\dots, X_{\\ensuremath{N}}\\}$, we use the product sets\n${\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}} := \\{ \\bigtimes_{n=1}^{\\ensuremath{N}} {\\mathcal{X}}_{n} ~|~ {\\mathcal{X}}_{n} \\in {\\ensuremath{\\mathcal{A}}}_{X_n} \\}$ to represent partial evidence about ${\\ensuremath{\\mathbf{X}}}$.\nElements of ${\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}}$ are denoted using boldface notation, e.g.~${\\bm{{\\mathcal{X}}}}$.\nWhen ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$ and ${\\bm{{\\mathcal{X}}}} \\in \\mathcal{H}_{\\ensuremath{\\mathbf{X}}}$, we define ${{\\bm{\\mathcal{X}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}} := \\{{{{\\ensuremath{\\mathbf{x}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}} ~|~ {\\ensuremath{\\mathbf{x}}} \\in \\bm{\\mathcal{X}}\\}$.\nFurthermore, we use ${\\ensuremath{\\mathbf{e}}}$ to symbolize any combination of complete and partial evidence, i.e.~for RVs ${\\ensuremath{\\mathbf{X}}}$ we have some complete evidence ${\\ensuremath{\\mathbf{x}}}'$ for ${\\ensuremath{\\mathbf{X}}}' \\subseteq {\\ensuremath{\\mathbf{X}}}$ and some partial evidence ${\\bm{{\\mathcal{X}}}}'' \\in {\\ensuremath{\\mathcal{H}}}_{{\\ensuremath{\\mathbf{X}}}''}$ for ${\\ensuremath{\\mathbf{X}}}'' = {\\ensuremath{\\mathbf{X}}} \\setminus {\\ensuremath{\\mathbf{X}}}'$.\n\n\nGiven a node ${\\mathsf{N}}$ in some directed graph ${\\ensuremath{\\mathcal{G}}}$, let ${\\ensuremath{\\mathbf{ch}}}({\\mathsf{N}})$ and ${\\ensuremath{\\mathbf{pa}}}({\\mathsf{N}})$ be the set of children and parents of ${\\mathsf{N}}$, respectively.\nFurthermore, let ${\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$ be the set of descendants of ${\\mathsf{N}}$, recursively defined as the set containing ${\\mathsf{N}}$ itself and any child of a descendant.\nSimilarly, we define ${\\ensuremath{\\mathbf{anc}}}({\\mathsf{N}})$ as the ancestors of ${\\mathsf{N}}$, recursively defined as the set containing ${\\mathsf{N}}$ itself and any parent of an ancestor.\nSPNs are defined as follows.\n\n\n\n\\begin{definition}[Sum-Product Network]\n\\label{def:SPN}\nA Sum-Product network (SPN) ${\\ensuremath{\\mathcal{S}}}$ over a set of RVs ${\\ensuremath{\\mathbf{X}}}$ is a tuple $({\\ensuremath{\\mathcal{G}}}, \\bm{{w}})$ where ${\\ensuremath{\\mathcal{G}}}$ is a rooted acyclic directed graph and $\\bm{{w}}$ is a set of non-negative parameters.\nThe graph ${\\ensuremath{\\mathcal{G}}}$ contains three types of nodes: distributions, sums and products.\nAll \\emph{leaves} of ${\\ensuremath{\\mathcal{G}}}$ are distributions and all \\emph{internal} nodes are either sums or products.\nA distribution node (also called input distribution or simply distribution) ${\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}}\\colon {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}}) \\mapsto [0,\\infty]$ is a distribution function over a subset of RVs ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$, i.e.~either a PMF (discrete RVs), a PDF (continuous RVs), or a mixed distribution function (discrete and continuous RVs mixed).\nA sum node ${\\mathsf{S}}$ computes a weighted sum of its children, i.e.\n${\\mathsf{S}} = \\sum_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} {w}_{{\\mathsf{S}}, {\\mathsf{C}}} \\, {\\mathsf{C}}$, \nwhere ${w}_{{\\mathsf{S}}, {\\mathsf{C}}}$ is a non-negative weight associated with edge ${\\mathsf{S}} \\rightarrow {\\mathsf{C}}$, and $\\bm{{w}}$ contains the weights for all outgoing sum-edges.\nA product node ${\\mathsf{P}}$ computes the product over its children, i.e.\n${\\mathsf{P}} = \\prod_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{P}})} {\\mathsf{C}}$.\nThe sets ${\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$ and ${\\bm{\\mathsf{P}}}({\\ensuremath{\\mathcal{S}}})$ contain all sum nodes and all product nodes in ${\\ensuremath{\\mathcal{S}}}$, respectively.\n\n\nFor some node ${\\mathsf{N}}$ in ${\\ensuremath{\\mathcal{G}}}$, the \\emph{scope} of ${\\mathsf{N}}$ is defined as\n\n", "itemtype": "equation", "pos": 3737, "prevtext": "\n\\title{On the Latent Variable Interpretation in Sum-Product Networks}\n\n\n\\author{Robert Peharz, Robert Gens, Franz Pernkopf,~\\IEEEmembership{Senior Member,~IEEE,} and Pedro Domingos\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem R.~Peharz is with iDN -- \nInstitute of Physiology, Medical University of Graz, working in the Brain Ears and Eyes -- Pattern Recognition Initiative, \nBioTechMed--Graz.\\protect\\\\\nE-mail: robert.peharz@medunigraz.at\n\\IEEEcompsocthanksitem R.~Gens and P.~Domingos are with the Department of Computer Science and Engineering, \nUniversity of Washington.\\protect\\\\\nE-mail: rcg@cs.washington.edu, pedrod@cs.washington.edu\n\\IEEEcompsocthanksitem F.~Pernkopf is with the Signal Processing and Speech Communication Lab, \nGraz University of Technology.\\protect\\\\\nE-mail: pernkopf@tugraz.at}\n\\thanks{November 2015; }}\n\n\n\n\\markboth{November 2015}\n{Peharz \\MakeLowercase{\\textit{et al.}}: On the Latent Variable Interpretation in Sum-Product Networks}\n\n\n\n\n\\IEEEtitleabstractindextext{\n\\begin{abstract}\nOne of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent\nvariables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and\nto efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables\ncorresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in\nSPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for\nintroducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish\nthe probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on\nthese results, we find a sound derivation of the EM algorithm for SPNs, which was presented mistaken in literature. Furthermore, the\nViterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm,\nwhen applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.\n\\end{abstract}\n\n\n\\begin{IEEEkeywords}\nSum-Product Networks, Latent Variables, Mixture Models, Expectation-Maximization, MPE inference\n\\end{IEEEkeywords}}\n\n\n\n\\maketitle\n\n\n\\IEEEdisplaynontitleabstractindextext\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\\ifCLASSOPTIONcompsoc\n\\IEEEraisesectionheading{\\section{Introduction}\\label{sec:introduction}}\n\\else\n\\section{Introduction}\n\\label{sec:introduction}\n\\fi\n\n\n\n\\IEEEPARstart{S}{um-Product Networks} are a promising type of probabilistic model, combining the domains of\ndeep learning and graphical models. \nOne of their main advantages is that many interesting inference scenarios are expressed as single forward \nand/or backward passes, i.e. these inference scenarios have a computational cost linear in the SPN\u00e2\u0080\u0099s \nrepresentation size. \nSPNs have shown convincing performance in applications such as image completion \\cite{Poon2011, Dennis2012, Peharz2013}, \ncomputer vision \\cite{Amer2012}, classification \\cite{Gens2012} and speech and language modeling \\cite{Peharz2014, Cheng2014}. \nSince their proposition \\cite{Poon2011}, one of the central themes in SPNs has been their interpretation as \nhierarchically structured latent variable (LV) models. \nThis is essentially the same approach as the LV interpretation in mixture models. \nConsider for example a Gaussian mixture model with K components over a set of random variables (RVs) ${\\ensuremath{\\mathbf{X}}}$:\n\n", "index": 1, "text": "\\begin{equation}\n{p}({\\ensuremath{\\mathbf{X}}}) = \\sum_{k=1}^K {w}_k \\, \\mathcal{N}({\\ensuremath{\\mathbf{X}}} {\\,|\\,} \\bm{\\mu}_k, \\bm{\\Sigma}_k), \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{p}({\\mathbf{X}})=\\sum_{k=1}^{K}{w}_{k}\\,\\mathcal{N}({\\mathbf{X}}{\\,|\\,}\\bm{%&#10;\\mu}_{k},\\bm{\\Sigma}_{k}),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc17</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>\ud835\udf41</mi><mi>k</mi></msub><mo>,</mo><msub><mi>\ud835\udeba</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nThe function computed by ${\\ensuremath{\\mathcal{S}}}$ is the function computed by its root and denoted as ${\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}})$, where without loss of generality we assume that the scope of the root is ${\\ensuremath{\\mathbf{X}}}$.\n\\end{definition}\nWe use symbols ${\\mathsf{D}}$, ${\\mathsf{S}}$, ${\\mathsf{P}}$, ${\\mathsf{N}}$, ${\\mathsf{C}}$ and ${\\mathsf{F}}$ for nodes in SPNs, where ${\\mathsf{D}}$ denotes a distribution, ${\\mathsf{S}}$ denotes a sum, and ${\\mathsf{P}}$ denotes a product.\nSymbols ${\\mathsf{N}}$, ${\\mathsf{C}}$ and ${\\mathsf{F}}$ denote generic nodes, where ${\\mathsf{C}}$ and ${\\mathsf{F}}$ indicate a child or parent relationship to another node, respectively.\nThe \\emph{distribution} ${p}_{\\ensuremath{\\mathcal{S}}}$ of an SPN ${\\ensuremath{\\mathcal{S}}}$ is defined as the normalized output of ${\\ensuremath{\\mathcal{S}}}$, i.e.~${p}_{\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}}) \\propto {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}}) $. \nFor each node ${\\mathsf{N}}$, we define the \\emph{sub-SPN} ${\\ensuremath{\\mathcal{S}}}_{\\mathsf{N}}$ rooted at ${\\mathsf{N}}$ as the SPN defined by the graph induced by the descendants of ${\\mathsf{N}}$ and the corresponding parameters.\n\n\nInference in unconstrained SPNs is generally intractable.\nEfficient inference in SPNs is enabled by two structural constraints, \\emph{completeness} and \\emph{decomposability} \\cite{Poon2011}.\nAn SPN is \n\\emph{complete} if for all sums ${\\mathsf{S}}$ it holds that \n\n", "itemtype": "equation", "pos": 14640, "prevtext": "\nwhere $\\mathcal{N}(\\cdot {\\,|\\,} \\cdot)$ is the Gaussian PDF, $\\bm{\\mu}_k$ and $\\bm{\\Sigma}_k$ are the means and covariances of the $k^\\text{th}$ component, and ${w}_k$ are the mixture weights with ${w}_k \\geq 0$, $\\sum {w}_k = 1$.\nThe GMM can be interpreted in two ways:\ni) It is a convex combination of PDFs and thus itself a PDF, or\nii) it is a marginal distribution of a distribution ${p}({\\ensuremath{\\mathbf{X}}},Z)$ over ${\\ensuremath{\\mathbf{X}}}$ and a latent, marginalized variable $Z$, where ${p}({\\ensuremath{\\mathbf{X}}} {\\,|\\,} Z = k) = \\mathcal{N}({\\ensuremath{\\mathbf{X}}} {\\,|\\,} \\bm{\\mu}_k, \\bm{\\Sigma}_k)$ and ${p}(Z = k) = {w}_k$.\nThe second interpretation, the LV interpretation, yields a syntactically well-structured model.\nFor example, following the LV interpretation, it is clear how to draw samples from ${p}({\\ensuremath{\\mathbf{X}}})$ by using ancestral sampling.\nThis structure can also be of semantic nature, for instance when $Z$ represents a clustering of ${\\ensuremath{\\mathbf{X}}}$ or when $Z$ is a class variable.\nFurthermore, using the LV interpretation allows the application of the EM algorithm, which is essentially maximum-likelihood learning under missing data\n\\cite{Dempster1977, Ghahramani1994}.\n\n\nMixture models can be seen as a special case of SPNs with a single sum node, corresponding to a single LV.\nMore generally, SPNs can have arbitrarily many sum nodes, each corresponding to its own LV, leading to a hierarchical structured LV model.\nIn \\cite{Poon2011}, the LV interpretation in SPNs was justified by explicitly introducing the LVs in the SPN model, using the so-called \\emph{indicator variables} corresponding to the LVs' states.\nHowever, as shown in this paper, this justification is actually too simplistic, since it is potentially in conflict with the \\emph{completeness} condition \\cite{Poon2011}, leading to an incompletely specified model.\nAs a remedy we propose the \\emph{augmentation} of an SPN, which additionally to the IVs also introduces the so-called \\emph{twin sum nodes}, in order to completely specify the LV model.\nWe further investigate the independency structure of the LV model resulting from augmentation and find a parallel to the local independence assertions in Bayesian networks (BNs) \\cite{Pearl1988, Koller2009}.\nThis allows us to define a BN representation of the augmented SPN which serves as independence map (IMAP).\nUsing our BN interpretation of augmented SPNs, we give a sound derivation of the (soft) EM algorithm for SPNs.\n\n\nClosely related to the LV interpretation is the inference scenario of finding the most-probable-explanation (MPE), i.e.~finding a probability maximizing assignment for all RVs.\nUsing results form \\cite{deCampos2011, Peharz2015b}, we first point out that that this problem is generally NP-hard for SPNs.\nIn \\cite{Poon2011} it was proposed that an MPE solution can be found efficiently\nwhen maximizing over model RVs (i.e. non-latent RVs)\nand LVs. \nThe proposed algorithm replaces sum nodes by max nodes and recovers the solution by using Viterbi-style backtracking. \nHowever, it was not shown that this is a correct algorithm. \nIn this paper, we show that this algorithm is indeed correct, \\emph{when applied to augmented SPNs}. \nWhen applied to \\emph{non-augmented} SPNs, the algorithm still returns an MPE solution of the augmented SPN, but implicitly \nassumes that the weights for each twin sum are all 0 except a single 1. \nThis leads to a phenomenon in MPE inference which we call \\emph{low-depth bias}, i.e. more shallow parts of the SPN are \npreferred during backtracking.\n\n\nThe main contribution in this paper is to provide a sound theoretical foundation for the LV interpretation in SPNs and\nrelated concepts, i.e. the EM algorithm and MPE inference.\nOur theoretical findings are confirmed in experiments on synthetic data and 103 real-world datasets.\n\n\nThe paper is organized as follows: In the remainder of this section we introduce notation, review SPNs and discuss \nrelated work. \nIn Section 2 we propose the augmentation of SPNs, show its soundness as hierarchical LV model and give an interpretation \nas BN. \nFurthermore, we discuss its independency properties and the interpretation of sum-weights as conditional probabilities. \nThe EM algorithm for SPNs is derived in Section 3. In Section 4 we discuss MPE inference for SPNs. \nExperiments are presented in Section 5 and Section 6 concludes the paper. \nProofs for our theoretical results are deferred to the Appendix.\n\n\n\n\n\\subsection{Background and Notation}   \\label{sec:backgroundNotation}\nRVs are denoted by upper-case letters $W$, $X$, $Y$ and $Z$.\nThe set of values of an RV $X$ is denoted by ${\\ensuremath{\\mathbf{val}}}(X)$, where corresponding lower-case letters denote elements of ${\\ensuremath{\\mathbf{val}}}(X)$, e.g.~$x$ is an element of ${\\ensuremath{\\mathbf{val}}}(X)$.\nSets of RVs are denoted by boldface letters $\\mathbf{W}$, ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}}$ and ${\\ensuremath{\\mathbf{Z}}}$.\nFor RV set ${\\ensuremath{\\mathbf{X}}} = \\{X_1,\\dots,X_N\\}$, we define ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}}) = \\bigtimes_{n=1}^N {\\ensuremath{\\mathbf{val}}}(X_n)$ and use corresponding lower-case boldface letters for elements of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$, e.g.~${\\ensuremath{\\mathbf{x}}}$ is an element of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$.\nFor a sub-set ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$, ${{{\\ensuremath{\\mathbf{x}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}}$ denotes the projection of ${\\ensuremath{\\mathbf{x}}}$ onto ${\\ensuremath{\\mathbf{Y}}}$.\n\n\nThe elements of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$ can be interpreted as \\emph{complete} evidence, assigning each RV in ${\\ensuremath{\\mathbf{X}}}$ a fixed value.\n\\emph{Partial} evidence about $X$ is represented as a subset ${\\mathcal{X}} \\subseteq {\\ensuremath{\\mathbf{val}}}(X)$, which is an element of the sigma-algebra ${\\ensuremath{\\mathcal{A}}}_X$ induced by RV $X$.\nFor discrete RVs, we assume ${\\ensuremath{\\mathcal{A}}}_X = 2^{{\\ensuremath{\\mathbf{val}}}(X)}$, i.e.~the power-set of ${\\ensuremath{\\mathbf{val}}}(X)$.\nFor continuous RVs, we use ${\\ensuremath{\\mathcal{A}}}_X = \\{ {\\mathcal{X}} \\in \\bm{\\mathcal{B}} ~|~ {\\mathcal{X}} \\subseteq {\\ensuremath{\\mathbf{val}}}(X) \\}$, where $\\bm{\\mathcal{B}}$ are the Borel-sets over $\\mathbb{R}$.\nFor example, partial evidence ${\\mathcal{X}} = \\{1,3,5\\}$ for a discrete RV $X$ with ${\\ensuremath{\\mathbf{val}}}(X) = \\{1,\\dots,6\\}$ represents evidence\nthat $X$ takes one of the states $1$, $2$ or $3$, and ${\\mathcal{Y}} = [-\\infty,\\pi]$ for a real-valued RV $Y$ represents evidence that $Y$ takes a value\nsmaller than $\\pi$. \nFormally speaking, partial evidence is used to express the domain of \\emph{marginalization or maximization} for a particular RV.\n\n\nFor sets of RVs ${\\ensuremath{\\mathbf{X}}} = \\{X_1, \\dots, X_{\\ensuremath{N}}\\}$, we use the product sets\n${\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}} := \\{ \\bigtimes_{n=1}^{\\ensuremath{N}} {\\mathcal{X}}_{n} ~|~ {\\mathcal{X}}_{n} \\in {\\ensuremath{\\mathcal{A}}}_{X_n} \\}$ to represent partial evidence about ${\\ensuremath{\\mathbf{X}}}$.\nElements of ${\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}}$ are denoted using boldface notation, e.g.~${\\bm{{\\mathcal{X}}}}$.\nWhen ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$ and ${\\bm{{\\mathcal{X}}}} \\in \\mathcal{H}_{\\ensuremath{\\mathbf{X}}}$, we define ${{\\bm{\\mathcal{X}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}} := \\{{{{\\ensuremath{\\mathbf{x}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}} ~|~ {\\ensuremath{\\mathbf{x}}} \\in \\bm{\\mathcal{X}}\\}$.\nFurthermore, we use ${\\ensuremath{\\mathbf{e}}}$ to symbolize any combination of complete and partial evidence, i.e.~for RVs ${\\ensuremath{\\mathbf{X}}}$ we have some complete evidence ${\\ensuremath{\\mathbf{x}}}'$ for ${\\ensuremath{\\mathbf{X}}}' \\subseteq {\\ensuremath{\\mathbf{X}}}$ and some partial evidence ${\\bm{{\\mathcal{X}}}}'' \\in {\\ensuremath{\\mathcal{H}}}_{{\\ensuremath{\\mathbf{X}}}''}$ for ${\\ensuremath{\\mathbf{X}}}'' = {\\ensuremath{\\mathbf{X}}} \\setminus {\\ensuremath{\\mathbf{X}}}'$.\n\n\nGiven a node ${\\mathsf{N}}$ in some directed graph ${\\ensuremath{\\mathcal{G}}}$, let ${\\ensuremath{\\mathbf{ch}}}({\\mathsf{N}})$ and ${\\ensuremath{\\mathbf{pa}}}({\\mathsf{N}})$ be the set of children and parents of ${\\mathsf{N}}$, respectively.\nFurthermore, let ${\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$ be the set of descendants of ${\\mathsf{N}}$, recursively defined as the set containing ${\\mathsf{N}}$ itself and any child of a descendant.\nSimilarly, we define ${\\ensuremath{\\mathbf{anc}}}({\\mathsf{N}})$ as the ancestors of ${\\mathsf{N}}$, recursively defined as the set containing ${\\mathsf{N}}$ itself and any parent of an ancestor.\nSPNs are defined as follows.\n\n\n\n\\begin{definition}[Sum-Product Network]\n\\label{def:SPN}\nA Sum-Product network (SPN) ${\\ensuremath{\\mathcal{S}}}$ over a set of RVs ${\\ensuremath{\\mathbf{X}}}$ is a tuple $({\\ensuremath{\\mathcal{G}}}, \\bm{{w}})$ where ${\\ensuremath{\\mathcal{G}}}$ is a rooted acyclic directed graph and $\\bm{{w}}$ is a set of non-negative parameters.\nThe graph ${\\ensuremath{\\mathcal{G}}}$ contains three types of nodes: distributions, sums and products.\nAll \\emph{leaves} of ${\\ensuremath{\\mathcal{G}}}$ are distributions and all \\emph{internal} nodes are either sums or products.\nA distribution node (also called input distribution or simply distribution) ${\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}}\\colon {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}}) \\mapsto [0,\\infty]$ is a distribution function over a subset of RVs ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{X}}}$, i.e.~either a PMF (discrete RVs), a PDF (continuous RVs), or a mixed distribution function (discrete and continuous RVs mixed).\nA sum node ${\\mathsf{S}}$ computes a weighted sum of its children, i.e.\n${\\mathsf{S}} = \\sum_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} {w}_{{\\mathsf{S}}, {\\mathsf{C}}} \\, {\\mathsf{C}}$, \nwhere ${w}_{{\\mathsf{S}}, {\\mathsf{C}}}$ is a non-negative weight associated with edge ${\\mathsf{S}} \\rightarrow {\\mathsf{C}}$, and $\\bm{{w}}$ contains the weights for all outgoing sum-edges.\nA product node ${\\mathsf{P}}$ computes the product over its children, i.e.\n${\\mathsf{P}} = \\prod_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{P}})} {\\mathsf{C}}$.\nThe sets ${\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$ and ${\\bm{\\mathsf{P}}}({\\ensuremath{\\mathcal{S}}})$ contain all sum nodes and all product nodes in ${\\ensuremath{\\mathcal{S}}}$, respectively.\n\n\nFor some node ${\\mathsf{N}}$ in ${\\ensuremath{\\mathcal{G}}}$, the \\emph{scope} of ${\\mathsf{N}}$ is defined as\n\n", "index": 3, "text": "\\begin{equation}\n{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) = \n\\begin{cases}\n{\\ensuremath{\\mathbf{Y}}} & \\text{if } {\\mathsf{N}} \\text{ is a distribution } {\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}} \\\\\n\\bigcup_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{N}})} {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}}) & \\text{otherwise.}\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{sc}}({\\mathsf{N}})=\\begin{cases}{\\mathbf{Y}}&amp;\\text{if }{\\mathsf{N}}%&#10;\\text{ is a distribution }{\\mathsf{D}}_{\\mathbf{Y}}\\\\&#10;\\bigcup_{{\\mathsf{C}}\\in{\\mathbf{ch}}({\\mathsf{N}})}{\\mathbf{sc}}({\\mathsf{C}}%&#10;)&amp;\\text{otherwise.}\\end{cases}\" display=\"block\"><mrow><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi>\ud835\udc18</mi></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\ud835\uddad</mi><mo>\u2062</mo><mtext>\u00a0is a distribution\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udda3</mi><mi>\ud835\udc18</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mstyle displaystyle=\"false\"><msub><mo largeop=\"true\" mathsize=\"160%\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mi>\ud835\udda2</mi><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub></mstyle><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udda2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise.</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nAn SPN is \\emph{decomposable} if for all products ${\\mathsf{P}}$ it holds that \n\n", "itemtype": "equation", "pos": 16509, "prevtext": "\nThe function computed by ${\\ensuremath{\\mathcal{S}}}$ is the function computed by its root and denoted as ${\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}})$, where without loss of generality we assume that the scope of the root is ${\\ensuremath{\\mathbf{X}}}$.\n\\end{definition}\nWe use symbols ${\\mathsf{D}}$, ${\\mathsf{S}}$, ${\\mathsf{P}}$, ${\\mathsf{N}}$, ${\\mathsf{C}}$ and ${\\mathsf{F}}$ for nodes in SPNs, where ${\\mathsf{D}}$ denotes a distribution, ${\\mathsf{S}}$ denotes a sum, and ${\\mathsf{P}}$ denotes a product.\nSymbols ${\\mathsf{N}}$, ${\\mathsf{C}}$ and ${\\mathsf{F}}$ denote generic nodes, where ${\\mathsf{C}}$ and ${\\mathsf{F}}$ indicate a child or parent relationship to another node, respectively.\nThe \\emph{distribution} ${p}_{\\ensuremath{\\mathcal{S}}}$ of an SPN ${\\ensuremath{\\mathcal{S}}}$ is defined as the normalized output of ${\\ensuremath{\\mathcal{S}}}$, i.e.~${p}_{\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}}) \\propto {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}}) $. \nFor each node ${\\mathsf{N}}$, we define the \\emph{sub-SPN} ${\\ensuremath{\\mathcal{S}}}_{\\mathsf{N}}$ rooted at ${\\mathsf{N}}$ as the SPN defined by the graph induced by the descendants of ${\\mathsf{N}}$ and the corresponding parameters.\n\n\nInference in unconstrained SPNs is generally intractable.\nEfficient inference in SPNs is enabled by two structural constraints, \\emph{completeness} and \\emph{decomposability} \\cite{Poon2011}.\nAn SPN is \n\\emph{complete} if for all sums ${\\mathsf{S}}$ it holds that \n\n", "index": 5, "text": "\\begin{equation}\n\\forall {\\mathsf{C}}',{\\mathsf{C}}'' \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}) \\colon {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}}') = {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}}'').\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\forall{\\mathsf{C}}^{\\prime},{\\mathsf{C}}^{\\prime\\prime}\\in{\\mathbf{ch}}({%&#10;\\mathsf{S}})\\colon{\\mathbf{sc}}({\\mathsf{C}}^{\\prime})={\\mathbf{sc}}({\\mathsf{%&#10;C}}^{\\prime\\prime}).\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mo>\u2200</mo><msup><mi>\ud835\udda2</mi><mo>\u2032</mo></msup></mrow><mo>,</mo><msup><mi>\ud835\udda2</mi><mi>\u2032\u2032</mi></msup></mrow><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddb2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udda2</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udda2</mi><mi>\u2032\u2032</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nAs shown in \\cite{Peharz2015,Peharz2015b}, integrating ${\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}})$ over arbitrary sets ${\\bm{{\\mathcal{X}}}} \\in {\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}}$, i.e.~\\emph{marginalization} over ${\\bm{{\\mathcal{X}}}}$, reduces to the corresponding integrals at the input distributions and evaluating sums and products in the usual way.\nThis property is known as \\emph{validity} of the SPNs \\cite{Poon2011}, and key for efficient inference.\nIn this paper we only consider complete and decomposable SPNs.\nWithout loss of generality \\cite{Peharz2015}, we assume \\emph{locally normalized} sum weights, i.e.~for each sum node ${\\mathsf{S}}$ we have $\\sum_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} {w}_{{\\mathsf{S}}, {\\mathsf{C}}} = 1$, and thus ${p}_{\\ensuremath{\\mathcal{S}}} \\equiv {\\ensuremath{\\mathcal{S}}}$, i.e.~the SPN's normalization constant is $1$.\n\n\nFor RVs with finitely many states, we will use so-called \\emph{indicator variables} (IVs) as input distributions \\cite{Poon2011}.\nFor a finite-state RV $X$ and state $x \\in {\\ensuremath{\\mathbf{val}}}(X)$, we introduce the IV ${\\lambda_{{X}={x}}}(x') := \\mathds{1}(x = x')$, assigning all probability mass to $x$.\nA complete and decomposable SPN represents the \\emph{(extended) network polynomial} of ${p}_{\\ensuremath{\\mathcal{S}}}$, which can be used in the \\emph{differential approach to inference} \\cite{Darwiche2003,Poon2011,Peharz2015}.\nAssume any evidence ${\\ensuremath{\\mathbf{e}}}$ which is evaluated in the SPN.\nThe derivatives of the SPN function with respect to the IVs (by interpreting the IVs as real-valued variables, see \\cite{Darwiche2003,Peharz2015} for details) yield\n\n", "itemtype": "equation", "pos": 16798, "prevtext": "\nAn SPN is \\emph{decomposable} if for all products ${\\mathsf{P}}$ it holds that \n\n", "index": 7, "text": "\\begin{equation}\n\\forall {\\mathsf{C}}',{\\mathsf{C}}'' \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{P}}), {\\mathsf{C}}' \\not= {\\mathsf{C}}''\\colon {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}}') \\cap {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}}'') = \\emptyset.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\forall{\\mathsf{C}}^{\\prime},{\\mathsf{C}}^{\\prime\\prime}\\in{\\mathbf{ch}}({%&#10;\\mathsf{P}}),{\\mathsf{C}}^{\\prime}\\not={\\mathsf{C}}^{\\prime\\prime}\\colon{%&#10;\\mathbf{sc}}({\\mathsf{C}}^{\\prime})\\cap{\\mathbf{sc}}({\\mathsf{C}}^{\\prime%&#10;\\prime})=\\emptyset.\" display=\"block\"><mrow><mo>\u2200</mo><msup><mi>\ud835\udda2</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>\ud835\udda2</mi><mi>\u2032\u2032</mi></msup><mo>\u2208</mo><mi>\ud835\udc1c\ud835\udc21</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddaf</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><msup><mi>\ud835\udda2</mi><mo>\u2032</mo></msup><mo>\u2260</mo><msup><mi>\ud835\udda2</mi><mi>\u2032\u2032</mi></msup><mo>:</mo><mi>\ud835\udc2c\ud835\udc1c</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udda2</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><mi>\ud835\udc2c\ud835\udc1c</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udda2</mi><mi>\u2032\u2032</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi mathvariant=\"normal\">\u2205</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nrepresenting the inference scenario of \\emph{modified evidence}, i.e.~evidence $\\mathbf{e}$ is modified such that $X$ is set to $x$.\nThe computationally attractive feature of the differential approach is that \\eqref{eq:diffapproach} can be evaluated for \\emph{all} $X\\in{\\ensuremath{\\mathbf{X}}}$ and \\emph{all} $x\\in {\\ensuremath{\\mathbf{val}}}(X)$ simultaneously using a \\emph{single} back-propagation pass in the SPN.\nSimilarly, for the second (and higher) derivatives, we get\n\n", "itemtype": "equation", "pos": 18768, "prevtext": "\nAs shown in \\cite{Peharz2015,Peharz2015b}, integrating ${\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{x}}})$ over arbitrary sets ${\\bm{{\\mathcal{X}}}} \\in {\\ensuremath{\\mathcal{H}}}_{\\ensuremath{\\mathbf{X}}}$, i.e.~\\emph{marginalization} over ${\\bm{{\\mathcal{X}}}}$, reduces to the corresponding integrals at the input distributions and evaluating sums and products in the usual way.\nThis property is known as \\emph{validity} of the SPNs \\cite{Poon2011}, and key for efficient inference.\nIn this paper we only consider complete and decomposable SPNs.\nWithout loss of generality \\cite{Peharz2015}, we assume \\emph{locally normalized} sum weights, i.e.~for each sum node ${\\mathsf{S}}$ we have $\\sum_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} {w}_{{\\mathsf{S}}, {\\mathsf{C}}} = 1$, and thus ${p}_{\\ensuremath{\\mathcal{S}}} \\equiv {\\ensuremath{\\mathcal{S}}}$, i.e.~the SPN's normalization constant is $1$.\n\n\nFor RVs with finitely many states, we will use so-called \\emph{indicator variables} (IVs) as input distributions \\cite{Poon2011}.\nFor a finite-state RV $X$ and state $x \\in {\\ensuremath{\\mathbf{val}}}(X)$, we introduce the IV ${\\lambda_{{X}={x}}}(x') := \\mathds{1}(x = x')$, assigning all probability mass to $x$.\nA complete and decomposable SPN represents the \\emph{(extended) network polynomial} of ${p}_{\\ensuremath{\\mathcal{S}}}$, which can be used in the \\emph{differential approach to inference} \\cite{Darwiche2003,Poon2011,Peharz2015}.\nAssume any evidence ${\\ensuremath{\\mathbf{e}}}$ which is evaluated in the SPN.\nThe derivatives of the SPN function with respect to the IVs (by interpreting the IVs as real-valued variables, see \\cite{Darwiche2003,Peharz2015} for details) yield\n\n", "index": 9, "text": "\\begin{equation}\n\\label{eq:diffapproach}\n\\frac{\\partial {{\\ensuremath{\\mathcal{S}}}(\\mathbf{e})}}{\\partial {\\lambda_{{X}={x}}}} = {\\ensuremath{\\mathcal{S}}}(X=x, \\mathbf{e} \\setminus X),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{{\\mathcal{S}}(\\mathbf{e})}}{\\partial{\\lambda_{{X}={x}}}}={%&#10;\\mathcal{S}}(X=x,\\mathbf{e}\\setminus X),\" display=\"block\"><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow></msub></mrow></mfrac><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>\ud835\udc1e</mi><mo>\u2216</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nFurthermore, the differential approach can be generalized to SPNs with arbitrary input distributions, \ni.e. SPNs over RVs with countably infinite or uncountably many states (cf.~\\cite{Peharz2015} for details).\n\n\n\n\n\\subsection{Related Work}\nSPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional\ntheories \\cite{Darwiche1999, Darwiche2001, Darwiche2002}. \nLike in SPNs, structural constraints in NNFs enable certain polynomial-time queries in the represented theory. \nIn particular, the notions of smoothness and decomposability in NNFs translate to the notions of completeness and decomposability in SPNs, respectively.\nFurthermore, the notion of determinism in NNFs translates to selectivity in SPNs, which allows closed form maximum-likelihood estimates for sum-weights \\cite{Peharz2014b}. \nThe work on NNFs led to the concept of network polynomials as a multilinear representation of BNs over finitely many states \n\\cite{Darwiche2003}, \\cite{Darwiche2002b}. \nBNs were cast into an intermediate d-DNNF (deterministic decomposable NNF) representation in order to generate an arithmetic circuit (ACs), representing the\nBN\u00e2\u0080\u0099s network polynomial. ACs, when restricted to sums and products, are equivalent to SPNs but have a slightly different syntax. \nIn \\cite{Lowd2008}, ACs were learned by optimizing an objective trading off the log-likelihood on the training set and the inference cost of the AC, measured as the worst-case number of arithmetic operations required for inference (i.e. the number of edges in the AC). \nThe learned models still represent BNs with context-specific independencies \\cite{Boutilier1996, Chickering1997}. \nA similar approach learning Markov networks represented by ACs is followed in \\cite{Lowd2013}. \nSPNs were the first time proposed in \\cite{Poon2011}, where the represented distribution was not defined via a background graphical model any more, \nbut directly as the normalized output of the network. \nIn this work, SPNs were applied to image data, where a generic architecture reminiscent to convolutional neural networks was proposed. \nStructure learning algorithms not restricted to the image domain were proposed in \\cite{Dennis2012, Gens2013, Peharz2013, Rooshenas2014, Adel2015}.\nDiscriminative learning of SPNs, optimizing conditional likelihood, was proposed in \\cite{Gens2012}.\nFurthermore, there is a growing body of literature on theoretical aspects of SPNs and their relationship to other types of probabilistic models. \nIn \\cite{Delalleau2011} two families of functions were identified which are efficiently representable by deep, but not by shallow SPNs, where an SPN is considered as shallow if it has no more than three layers. \nIn \\cite{Peharz2015} it was shown that SPNs can w.l.o.g.~be assumed to be locally normalized and that the notion of consistency does not allow exponentially more compact models than decomposability. \nThese results were independently found in \\cite{Zhao2015}. \nFurthermore, in \\cite{Peharz2015}, a sound derivation of inference mechanisms for generalized SPNs was given, i.e. SPNs over RVs with (uncountably)\ninfinitely many states. \nIn \\cite{Zhao2015}, a BN representation of SPNs was found, where LVs associated the sum nodes and the model RVs are organized in a two layer bipartite structure.\nThe actual structure is captured in structured conditional probability tables (CPTs) using algebraic decision diagrams.\nThe LV interpretation discussed in this paper is rather complementary: the hierarchy of the LVs is more closely reflected in the corresponding BN structure, while the CPTs have a two-partition of the parents\u00e2\u0080\u0099 state space. \n\n\n\n\n\n\\section{Latent Variable Interpretation}\nAs pointed out in \\cite{Poon2011}, each sum node in an SPN can be\ninterpreted as a marginalized LV, similar as in the GMM\nexample in Section 1. \nFor each sum node ${\\mathsf{S}}$, one postulates a discrete LV $Z$ whose states correspond to ${\\mathsf{S}}$'s children. \nFor each state, an IV and a product is introduced, such that ${\\mathsf{S}}$'s children are switched on/off by the corresponding IVs, \nas illustrated in Fig.~\\ref{fig:problemAugmentSPNnaiv}.\\footnote{In graphical representations of SPNs, IVs are depicted as nodes\ncontaining a small circle, general distributions as nodes containing a Gaussian-like PDF, and sum and products as nodes with $+$ and $\\times$ symbols. \nEmpty nodes are of arbitrary type.} \nWhen all IVs in Fig.~\\ref{fig:problemAugmentSPNnaiv_2} are set to 1, ${\\mathsf{S}}$ still computes the same value as in Fig.~\\ref{fig:problemAugmentSPNnaiv_1}. \nSince setting all IVs of $Z$ to 1 corresponds to marginalizing $Z$, the sum ${\\mathsf{S}}$ should be interpreted as a latent, marginalized RV.\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\\includegraphics[scale=0.25]{augmentNaivProblem_1}\n\\label{fig:problemAugmentSPNnaiv_1} \n  }\\qquad\n\\subfloat[]{\n  \\includegraphics[scale=0.25]{augmentNaivProblem_2}\n\t\\label{fig:problemAugmentSPNnaiv_2}\n }\\qquad\n\\subfloat[]{\n  \\includegraphics[scale=0.25]{augmentRemedy}\n\t\\label{fig:problemAugmentSPNnaiv_3} \n }\n \\caption{Problems occurring when IVs of LVs are introduced. \n\\protect\\subref{fig:problemAugmentSPNnaiv_1}: Excerpt of an SPN containing a sum ${\\mathsf{S}}$, corresponding to an LV $Z$. \n\\protect\\subref{fig:problemAugmentSPNnaiv_2}: Introducing IVs for $Z$ renders ${\\mathsf{S}}'$ incomplete, assuming that ${\\mathsf{S}} \\notin {\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$.\n\\protect\\subref{fig:problemAugmentSPNnaiv_3}: Remedy by extending SPN structure further, introducing twin sum node $\\bar{{\\mathsf{S}}}$.}\n\\label{fig:problemAugmentSPNnaiv}\n\\end{figure}\n\n\nHowever, when we regard a larger structural context\nin Fig.~\\ref{fig:problemAugmentSPNnaiv_2}, we recognize that this justification is actually\ntoo simplistic. \nExplicitly introducing the IVs renders the ancestor ${\\mathsf{S}}'$ incomplete, since ${\\mathsf{S}}$ is no descendant of ${\\mathsf{N}}$ and\n$Z$ is thus not in the scope of ${\\mathsf{N}}$. \nNote that setting all IVs to 1 in an \\emph{incomplete} SPN generally does \\emph{not} correspond to marginalization. \nFurthermore, note that also ${\\mathsf{S}}'$ corresponds to an LV, say $Z'$. \nWhile we know the probability distribution of $Z$ if $Z'$ is in the state corresponding to ${\\mathsf{P}}$, namely the weights of ${\\mathsf{S}}$, we do not know this distribution when $Z'$ is in the state corresponding to ${\\mathsf{N}}$. \nIntuitively, we recognize that the state of $Z$ is \u00e2\u0080\u009cirrelevant\u00e2\u0080\u009d in this case, since it does not influence the resulting distribution over the model RVs ${\\ensuremath{\\mathbf{X}}}$.\nNevertheless, the probabilistic model is not completely specified, which is unsatisfying.\n\n\nA remedy for these problems is shown in Fig.~\\ref{fig:problemAugmentSPNnaiv_3}.\nWe introduce the twin sum node $\\bar {\\mathsf{S}}$ whose children are the IVs corresponding to $Z$. \nThe twin $\\bar {\\mathsf{S}}$ is connected as child of an additional product node, which is interconnected between ${\\mathsf{S}}'$ and ${\\mathsf{N}}$. \nSince this new product node has scope ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cup \\{Z\\}$, ${\\mathsf{S}}'$ is rendered complete now. \nFurthermore, if $Z'$ takes the state corresponding to ${\\mathsf{N}}$ (or actually the state corresponding to the new product node), we now have a specified conditional distribution for $Z$, namely the weights of the twin sum node. \nClearly, given that all IVs of $Z$ are set to 1, the network depicted in Fig.~\\ref{fig:problemAugmentSPNnaiv_3} still computes the same function as the network in Fig.~\\ref{fig:problemAugmentSPNnaiv_1} (or Fig.~\\ref{fig:problemAugmentSPNnaiv_2}), since $\\bar {\\mathsf{S}}$ constantly outputs 1, as long as we use normalized weights for it. \nWhich weights should be used for the twin sum node $\\bar {\\mathsf{S}}$? \nBasically, we can assume arbitrary normalized weights, which will cause $\\bar {\\mathsf{S}}$ to constantly output 1. \nHowever, following an esthetic and maximum entropy argument, we suggest to use uniform weights for $\\bar {\\mathsf{S}}$. \nAlthough the choice of weights is not crucial for \\emph{evaluating evidence} in the SPN, it plays a role in \\emph{MPE inference}, see Section \\ref{sec:MPE}.\nFor now, let us formalize the explicit introduction of LVs, denoted as \\emph{augmentation}.\n\n\n\\subsection{Augmentation of SPNs}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$.\nFor each ${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$ we assume an arbitrary but fixed ordering of its children ${\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}) = \\{{\\mathsf{C}}_{\\mathsf{S}}^1,\\dots,{\\mathsf{C}}_{\\mathsf{S}}^{K_{\\mathsf{S}}}\\}$, where $K_{\\mathsf{S}} = |{\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})|$.\nLet $Z_{\\mathsf{S}}$  be an RV on the same probability space as ${\\ensuremath{\\mathbf{X}}}$, with ${\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}}) = \\{1,\\dots,K_{\\mathsf{S}}\\}$, where state $k$ corresponds to child ${\\mathsf{C}}^k_{\\mathsf{S}}$.\nWe call $Z_{\\mathsf{S}}$ the \\emph{LV associated with} ${\\mathsf{S}}$.\nFor sets of sum nodes $\\bm{{\\mathsf{S}}}$ we define ${\\ensuremath{\\mathbf{Z}}}_{\\bm{{\\mathsf{S}}}} = \\{ Z_{\\mathsf{S}} {\\ensuremath{\\,|\\,}} {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}} \\}$.\nTo distinguish ${\\ensuremath{\\mathbf{X}}}$ from the LVs, we will refer to the former as \\emph{model RVs}.\nFor node ${\\mathsf{N}}$, we define the \\emph{sum ancestors/descendants} as\n\n", "itemtype": "equation", "pos": 19450, "prevtext": "\nrepresenting the inference scenario of \\emph{modified evidence}, i.e.~evidence $\\mathbf{e}$ is modified such that $X$ is set to $x$.\nThe computationally attractive feature of the differential approach is that \\eqref{eq:diffapproach} can be evaluated for \\emph{all} $X\\in{\\ensuremath{\\mathbf{X}}}$ and \\emph{all} $x\\in {\\ensuremath{\\mathbf{val}}}(X)$ simultaneously using a \\emph{single} back-propagation pass in the SPN.\nSimilarly, for the second (and higher) derivatives, we get\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:diffapproach2}\n\\frac{\\partial^2 {{\\ensuremath{\\mathcal{S}}}(\\mathbf{e})}}{\\partial {\\lambda_{{X}={x}}} {\\lambda_{{Y}={y}}}} = \n\\begin{cases}\n{\\ensuremath{\\mathcal{S}}}(X=x, Y=y, \\mathbf{e} \\setminus \\{X,Y\\}) & \\text{if } X \\not= Y \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial^{2}{{\\mathcal{S}}(\\mathbf{e})}}{\\partial{\\lambda_{{X}={x}}}{%&#10;\\lambda_{{Y}={y}}}}=\\begin{cases}{\\mathcal{S}}(X=x,Y=y,\\mathbf{e}\\setminus\\{X,%&#10;Y\\})&amp;\\text{if }X\\not=Y\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><msub><mi>\u03bb</mi><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow></msub><mo>\u2062</mo><msub><mi>\u03bb</mi><mrow><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></msub></mrow></mrow></mfrac><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>,</mo><mi>\ud835\udc1e</mi><mo>\u2216</mo><mrow><mo stretchy=\"false\">{</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>X</mi></mrow><mo>\u2260</mo><mi>Y</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nFor each sum node ${\\mathsf{S}}$ we define the \\emph{conditioning sums} as\n\n", "itemtype": "equation", "pos": 29187, "prevtext": "\nFurthermore, the differential approach can be generalized to SPNs with arbitrary input distributions, \ni.e. SPNs over RVs with countably infinite or uncountably many states (cf.~\\cite{Peharz2015} for details).\n\n\n\n\n\\subsection{Related Work}\nSPNs are related to negation normal forms (NNFs), a potential deep network representation of propositional\ntheories \\cite{Darwiche1999, Darwiche2001, Darwiche2002}. \nLike in SPNs, structural constraints in NNFs enable certain polynomial-time queries in the represented theory. \nIn particular, the notions of smoothness and decomposability in NNFs translate to the notions of completeness and decomposability in SPNs, respectively.\nFurthermore, the notion of determinism in NNFs translates to selectivity in SPNs, which allows closed form maximum-likelihood estimates for sum-weights \\cite{Peharz2014b}. \nThe work on NNFs led to the concept of network polynomials as a multilinear representation of BNs over finitely many states \n\\cite{Darwiche2003}, \\cite{Darwiche2002b}. \nBNs were cast into an intermediate d-DNNF (deterministic decomposable NNF) representation in order to generate an arithmetic circuit (ACs), representing the\nBN\u00e2\u0080\u0099s network polynomial. ACs, when restricted to sums and products, are equivalent to SPNs but have a slightly different syntax. \nIn \\cite{Lowd2008}, ACs were learned by optimizing an objective trading off the log-likelihood on the training set and the inference cost of the AC, measured as the worst-case number of arithmetic operations required for inference (i.e. the number of edges in the AC). \nThe learned models still represent BNs with context-specific independencies \\cite{Boutilier1996, Chickering1997}. \nA similar approach learning Markov networks represented by ACs is followed in \\cite{Lowd2013}. \nSPNs were the first time proposed in \\cite{Poon2011}, where the represented distribution was not defined via a background graphical model any more, \nbut directly as the normalized output of the network. \nIn this work, SPNs were applied to image data, where a generic architecture reminiscent to convolutional neural networks was proposed. \nStructure learning algorithms not restricted to the image domain were proposed in \\cite{Dennis2012, Gens2013, Peharz2013, Rooshenas2014, Adel2015}.\nDiscriminative learning of SPNs, optimizing conditional likelihood, was proposed in \\cite{Gens2012}.\nFurthermore, there is a growing body of literature on theoretical aspects of SPNs and their relationship to other types of probabilistic models. \nIn \\cite{Delalleau2011} two families of functions were identified which are efficiently representable by deep, but not by shallow SPNs, where an SPN is considered as shallow if it has no more than three layers. \nIn \\cite{Peharz2015} it was shown that SPNs can w.l.o.g.~be assumed to be locally normalized and that the notion of consistency does not allow exponentially more compact models than decomposability. \nThese results were independently found in \\cite{Zhao2015}. \nFurthermore, in \\cite{Peharz2015}, a sound derivation of inference mechanisms for generalized SPNs was given, i.e. SPNs over RVs with (uncountably)\ninfinitely many states. \nIn \\cite{Zhao2015}, a BN representation of SPNs was found, where LVs associated the sum nodes and the model RVs are organized in a two layer bipartite structure.\nThe actual structure is captured in structured conditional probability tables (CPTs) using algebraic decision diagrams.\nThe LV interpretation discussed in this paper is rather complementary: the hierarchy of the LVs is more closely reflected in the corresponding BN structure, while the CPTs have a two-partition of the parents\u00e2\u0080\u0099 state space. \n\n\n\n\n\n\\section{Latent Variable Interpretation}\nAs pointed out in \\cite{Poon2011}, each sum node in an SPN can be\ninterpreted as a marginalized LV, similar as in the GMM\nexample in Section 1. \nFor each sum node ${\\mathsf{S}}$, one postulates a discrete LV $Z$ whose states correspond to ${\\mathsf{S}}$'s children. \nFor each state, an IV and a product is introduced, such that ${\\mathsf{S}}$'s children are switched on/off by the corresponding IVs, \nas illustrated in Fig.~\\ref{fig:problemAugmentSPNnaiv}.\\footnote{In graphical representations of SPNs, IVs are depicted as nodes\ncontaining a small circle, general distributions as nodes containing a Gaussian-like PDF, and sum and products as nodes with $+$ and $\\times$ symbols. \nEmpty nodes are of arbitrary type.} \nWhen all IVs in Fig.~\\ref{fig:problemAugmentSPNnaiv_2} are set to 1, ${\\mathsf{S}}$ still computes the same value as in Fig.~\\ref{fig:problemAugmentSPNnaiv_1}. \nSince setting all IVs of $Z$ to 1 corresponds to marginalizing $Z$, the sum ${\\mathsf{S}}$ should be interpreted as a latent, marginalized RV.\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\\includegraphics[scale=0.25]{augmentNaivProblem_1}\n\\label{fig:problemAugmentSPNnaiv_1} \n  }\\qquad\n\\subfloat[]{\n  \\includegraphics[scale=0.25]{augmentNaivProblem_2}\n\t\\label{fig:problemAugmentSPNnaiv_2}\n }\\qquad\n\\subfloat[]{\n  \\includegraphics[scale=0.25]{augmentRemedy}\n\t\\label{fig:problemAugmentSPNnaiv_3} \n }\n \\caption{Problems occurring when IVs of LVs are introduced. \n\\protect\\subref{fig:problemAugmentSPNnaiv_1}: Excerpt of an SPN containing a sum ${\\mathsf{S}}$, corresponding to an LV $Z$. \n\\protect\\subref{fig:problemAugmentSPNnaiv_2}: Introducing IVs for $Z$ renders ${\\mathsf{S}}'$ incomplete, assuming that ${\\mathsf{S}} \\notin {\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$.\n\\protect\\subref{fig:problemAugmentSPNnaiv_3}: Remedy by extending SPN structure further, introducing twin sum node $\\bar{{\\mathsf{S}}}$.}\n\\label{fig:problemAugmentSPNnaiv}\n\\end{figure}\n\n\nHowever, when we regard a larger structural context\nin Fig.~\\ref{fig:problemAugmentSPNnaiv_2}, we recognize that this justification is actually\ntoo simplistic. \nExplicitly introducing the IVs renders the ancestor ${\\mathsf{S}}'$ incomplete, since ${\\mathsf{S}}$ is no descendant of ${\\mathsf{N}}$ and\n$Z$ is thus not in the scope of ${\\mathsf{N}}$. \nNote that setting all IVs to 1 in an \\emph{incomplete} SPN generally does \\emph{not} correspond to marginalization. \nFurthermore, note that also ${\\mathsf{S}}'$ corresponds to an LV, say $Z'$. \nWhile we know the probability distribution of $Z$ if $Z'$ is in the state corresponding to ${\\mathsf{P}}$, namely the weights of ${\\mathsf{S}}$, we do not know this distribution when $Z'$ is in the state corresponding to ${\\mathsf{N}}$. \nIntuitively, we recognize that the state of $Z$ is \u00e2\u0080\u009cirrelevant\u00e2\u0080\u009d in this case, since it does not influence the resulting distribution over the model RVs ${\\ensuremath{\\mathbf{X}}}$.\nNevertheless, the probabilistic model is not completely specified, which is unsatisfying.\n\n\nA remedy for these problems is shown in Fig.~\\ref{fig:problemAugmentSPNnaiv_3}.\nWe introduce the twin sum node $\\bar {\\mathsf{S}}$ whose children are the IVs corresponding to $Z$. \nThe twin $\\bar {\\mathsf{S}}$ is connected as child of an additional product node, which is interconnected between ${\\mathsf{S}}'$ and ${\\mathsf{N}}$. \nSince this new product node has scope ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cup \\{Z\\}$, ${\\mathsf{S}}'$ is rendered complete now. \nFurthermore, if $Z'$ takes the state corresponding to ${\\mathsf{N}}$ (or actually the state corresponding to the new product node), we now have a specified conditional distribution for $Z$, namely the weights of the twin sum node. \nClearly, given that all IVs of $Z$ are set to 1, the network depicted in Fig.~\\ref{fig:problemAugmentSPNnaiv_3} still computes the same function as the network in Fig.~\\ref{fig:problemAugmentSPNnaiv_1} (or Fig.~\\ref{fig:problemAugmentSPNnaiv_2}), since $\\bar {\\mathsf{S}}$ constantly outputs 1, as long as we use normalized weights for it. \nWhich weights should be used for the twin sum node $\\bar {\\mathsf{S}}$? \nBasically, we can assume arbitrary normalized weights, which will cause $\\bar {\\mathsf{S}}$ to constantly output 1. \nHowever, following an esthetic and maximum entropy argument, we suggest to use uniform weights for $\\bar {\\mathsf{S}}$. \nAlthough the choice of weights is not crucial for \\emph{evaluating evidence} in the SPN, it plays a role in \\emph{MPE inference}, see Section \\ref{sec:MPE}.\nFor now, let us formalize the explicit introduction of LVs, denoted as \\emph{augmentation}.\n\n\n\\subsection{Augmentation of SPNs}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$.\nFor each ${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$ we assume an arbitrary but fixed ordering of its children ${\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}) = \\{{\\mathsf{C}}_{\\mathsf{S}}^1,\\dots,{\\mathsf{C}}_{\\mathsf{S}}^{K_{\\mathsf{S}}}\\}$, where $K_{\\mathsf{S}} = |{\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})|$.\nLet $Z_{\\mathsf{S}}$  be an RV on the same probability space as ${\\ensuremath{\\mathbf{X}}}$, with ${\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}}) = \\{1,\\dots,K_{\\mathsf{S}}\\}$, where state $k$ corresponds to child ${\\mathsf{C}}^k_{\\mathsf{S}}$.\nWe call $Z_{\\mathsf{S}}$ the \\emph{LV associated with} ${\\mathsf{S}}$.\nFor sets of sum nodes $\\bm{{\\mathsf{S}}}$ we define ${\\ensuremath{\\mathbf{Z}}}_{\\bm{{\\mathsf{S}}}} = \\{ Z_{\\mathsf{S}} {\\ensuremath{\\,|\\,}} {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}} \\}$.\nTo distinguish ${\\ensuremath{\\mathbf{X}}}$ from the LVs, we will refer to the former as \\emph{model RVs}.\nFor node ${\\mathsf{N}}$, we define the \\emph{sum ancestors/descendants} as\n\n", "index": 13, "text": "\\begin{align}\n{\\ensuremath{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{N}}) &:= {\\ensuremath{\\mathbf{anc}}}({\\mathsf{N}}) \\cap {\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}),  \\\\\n{\\ensuremath{{\\mathbf{desc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{N}}) &:= {\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}}) \\cap {\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}({\\mathsf{N}})\" display=\"inline\"><mrow><msub><mi>\ud835\udc1a\ud835\udc27\ud835\udc1c</mi><mi>\ud835\udde6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:={\\mathbf{anc}}({\\mathsf{N}})\\cap{\\bm{\\mathsf{S}}}({\\mathcal{S}}),\" display=\"inline\"><mrow><mrow><mi/><mo>:=</mo><mrow><mrow><mi>\ud835\udc1a\ud835\udc27\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2229</mo><mrow><mi>\ud835\udde6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{{\\mathbf{desc}_{\\bm{\\mathsf{S}}}}}({\\mathsf{N}})\" display=\"inline\"><mrow><msub><mi>\ud835\udc1d\ud835\udc1e\ud835\udc2c\ud835\udc1c</mi><mi>\ud835\udde6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:={\\mathbf{desc}}({\\mathsf{N}})\\cap{\\bm{\\mathsf{S}}}({\\mathcal{S}%&#10;}).\" display=\"inline\"><mrow><mrow><mi/><mo>:=</mo><mrow><mrow><mi>\ud835\udc1d\ud835\udc1e\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddad</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2229</mo><mrow><mi>\ud835\udde6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nFurthermore, we assume a set of locally normalized \\emph{twin-weights} $\\bm{\\bar{{w}}}$, containing a twin-weight $\\bar{{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$ for each weight ${w}_{{\\mathsf{S}},{\\mathsf{C}}}$ in the SPN.\nWe are now ready to define the \\emph{augmentation} of an SPN. \n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{AugmentSPN}{${\\ensuremath{\\mathcal{S}}}$}\n\\State ${\\ensuremath{\\mathcal{S}}}' \\leftarrow {\\ensuremath{\\mathcal{S}}}$ \n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}'),~\\forall k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}\\colon$\n\\Statex \\hskip\\algorithmicindent \\hskip\\algorithmicindent  \nlet ${w}_{{\\mathsf{S}},k} = {w}_{{\\mathsf{S}},{\\mathsf{C}}_{\\mathsf{S}}^k}$, $\\bar{w}_{{\\mathsf{S}},k} = \\bar{w}_{{\\mathsf{S}},{\\mathsf{C}}_{\\mathsf{S}}^k}$\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$}      \\label{as:introLinksStart}\n\\For{$k = 1 \\dots K_{\\mathsf{S}}$}\n\\State Introduce a new product node ${\\mathsf{P}}_{\\mathsf{S}}^k$ in ${\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$\n\\State Disconnect ${\\mathsf{C}}_{\\mathsf{S}}^k$ from ${\\mathsf{S}}$\n\\State Connect ${\\mathsf{C}}_{\\mathsf{S}}^k$ as child of ${\\mathsf{P}}_{\\mathsf{S}}^k$\n\\State Connect ${\\mathsf{P}}_{\\mathsf{S}}^k$ as child of ${\\mathsf{S}}$ with weight ${w}_{{\\mathsf{S}},k}$\n\\EndFor\n\\EndFor                                        \\label{as:introLinksEnd}\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$}\n\\For {$k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}$}            \\label{as:introIVStart}\n\\State Connect new IV ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as child of ${\\mathsf{P}}_{\\mathsf{S}}^k$\n\\EndFor                                           \\label{as:introIVEnd}\n\\If {$\\bm{{\\mathsf{S}}}^c({\\mathsf{S}}) \\not= \\emptyset$}   \n\\State Introduce a twin sum node $\\bar{{\\mathsf{S}}}$ in ${\\ensuremath{\\mathcal{S}}}'$             \\label{as:augmentRemedyStart}\n\\State $\\forall k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}$: connect ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as child of $\\bar{{\\mathsf{S}}}$, \n\\Statex \\hskip\\algorithmicindent \\hskip\\algorithmicindent \\hskip\\algorithmicindent \\hskip\\algorithmicindent\nand let ${w}_{\\bar{{\\mathsf{S}}}, {\\lambda_{{Z_{\\mathsf{S}}}={k}}}} = \\bar{w}_{{{\\mathsf{S}}}, k}$\n\\For{${\\mathsf{S}}^c \\in \\bm{{\\mathsf{S}}}^c({\\mathsf{S}})$} \n\\For {$k \\in \\{k ~|~ {\\mathsf{S}} \\not\\in {\\ensuremath{\\mathbf{desc}}}({\\mathsf{P}}^k_{{\\mathsf{S}}^c})\\}$}\n\\State Connect $\\bar{{\\mathsf{S}}}$ as child of ${\\mathsf{P}}^k_{{\\mathsf{S}}^c}$     \\label{as:connectTwin}\n\\EndFor\n\\EndFor                                                              \\label{as:augmentRemedyEnd}\n\\EndIf                      \n\\EndFor\n\\State \\Return ${\\ensuremath{\\mathcal{S}}}'$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for augmentation of an SPN.}\n\\label{algo:augmentSPN}\n\\end{figure}\n\n\n\n\\begin{definition}[Augmentation of SPN]\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, $\\bar{\\bm{{w}}}$ be a set of twin-weights and ${\\ensuremath{\\mathcal{S}}}'$ be the result of algortihm \\textproc{AugmentSPN}, shown in \nFig.~\\ref{algo:augmentSPN}.\n${\\ensuremath{\\mathcal{S}}}'$ is called the \\emph{augmented} SPN of ${\\ensuremath{\\mathcal{S}}}$, denoted as ${\\ensuremath{\\mathcal{S}}}' =: {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.\nWithin the context of ${\\ensuremath{\\mathcal{S}}}'$, ${\\mathsf{C}}_{\\mathsf{S}}^k$ is called the $k^\\text{th}$ \\emph{former child} of ${\\mathsf{S}}$.\nThe introduced product node ${\\mathsf{P}}_{\\mathsf{S}}^k$ is called \\emph{link} of ${\\mathsf{S}}$, ${\\mathsf{C}}^k_{\\mathsf{S}}$ and ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$, respectively.\nThe sum node $\\bar{{\\mathsf{S}}}$, if introduced, is called the \\emph{twin} sum node of ${\\mathsf{S}}$.\nWith respect to ${\\ensuremath{\\mathcal{S}}}'$, we denote ${\\ensuremath{\\mathcal{S}}}$ as the original SPN.\n\\end{definition}\n\n\n\nIn steps \\ref{as:introLinksStart}--\\ref{as:introLinksEnd} of \\textproc{AugmentSPN} we introduce the links ${\\mathsf{P}}_{\\mathsf{S}}^k$ which are interconnected between sum node ${\\mathsf{S}}$ and its $k^\\text{th}$ child.\nEach link ${\\mathsf{P}}_{\\mathsf{S}}^k$ has a single parent, namely ${\\mathsf{S}}$, and simply copies the former child ${\\mathsf{C}}_{\\mathsf{S}}^k$.\nIn steps \\ref{as:introIVStart}--\\ref{as:introIVEnd}, we introduce IVs corresponding to the associated LV $Z_{\\mathsf{S}}$, as proposed in \\cite{Poon2011}.\nAs we saw in Figure~\\ref{fig:problemAugmentSPNnaiv} and the discussion above, this can render other sum nodes incomplete.\nThese sums are by definition the conditioning sums $\\bm{{\\mathsf{S}}}^c({\\mathsf{S}})$.\nThus, when necessary, we introduce a twin sum node in steps \\ref{as:augmentRemedyStart}--\\ref{as:augmentRemedyEnd}, to treat this problem.\nThe following proposition states the soundness of augmentation.\n\n\n\n\\begin{proposition}\n\\label{prop:augmentationSound}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$ and ${\\ensuremath{\\mathbf{Z}}} := {\\ensuremath{\\mathbf{Z}}}_{\\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$.\nThen ${\\ensuremath{\\mathcal{S}}}'$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$ with ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$.\n\\end{proposition}\n\n\n\nProposition \\ref{prop:augmentationSound} states that the marginal distribution of the augmented SPN is the same distribution over ${\\ensuremath{\\mathbf{X}}}$ as\nrepresented by the original SPN, while being a \\emph{completely specified probabilistic model} over ${\\ensuremath{\\mathbf{X}}}$ \\emph{and} ${\\ensuremath{\\mathbf{Z}}}$. \nThus, augmentation provides a way to generalize the LV interpretation from mixture models to more general SPNs. \nAn example of augmentation is shown in Fig.~\\ref{fig:exaugmentSPN}. \nIn the next section, we closer investigate the probabilistic interpretation of sum-weights in augmented SPNs.\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{exampleSPN}\n  \\label{fig:exampleaugpre}\t\n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{exampleSPN_Augment}\n  \\label{fig:exampleaugpost}\t\n }\n \\caption{Augmentation of an SPN. \n\\protect\\subref{fig:exampleaugpre}: Example SPN over ${\\ensuremath{\\mathbf{X}}} = \\{X_1, X_2, X_3\\}$, containing sum nodes ${\\mathsf{S}}^1$, ${\\mathsf{S}}^2$, ${\\mathsf{S}}^3$ and ${\\mathsf{S}}^4$. \n\\protect\\subref{fig:exampleaugpost}: Augmented SPN, containing IVs corresponding to $Z_{{\\mathsf{S}}^1}$, $Z_{{\\mathsf{S}}^2}$, $Z_{{\\mathsf{S}}^3}$, $Z_{{\\mathsf{S}}^4}$, links and twin sum nodes $\\bar{{\\mathsf{S}}}^2$, $\\bar{{\\mathsf{S}}}^3$, $\\bar{{\\mathsf{S}}}^4$.\nFor nodes introduced by augmentation, smaller circles are used. \n}\n\\label{fig:exaugmentSPN}\n\\end{figure}\n\n\n\\subsection{Conditional Independencies in Augmented SPNs and Probabilistic Interpretation of Sum-Weights}\nIt is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs \\cite{Darwiche1999, Darwiche2001, Darwiche2002}.\n\n\n\n\\begin{definition}[Configured SPN]\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ and ${\\ensuremath{\\mathbf{y}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$.\nThe \\emph{configured SPN} ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is obtained by deleting the IVs ${\\lambda_{{y}={y}}}$ and their corresponding link for each $Y \\in {\\ensuremath{\\mathbf{Y}}}$, $y \\not= {{{\\ensuremath{\\mathbf{y}}}}{[{Y}]}}$ from \n${\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$, and further deleting all nodes which are rendered unreachable from the root.\n\\end{definition}\n\n\n\nIntuitively, the configured SPN isolates the computational structure selected by ${\\ensuremath{\\mathbf{y}}}$. \nAll sum edges which \"survive\" in the configured SPN are equipped with the same weights as in the augmented SPN. \nTherefore, a configured SPN is in general not locally normalized. \nWe note the following properties of configured SPNs.\n\n\n\n\\begin{proposition}\n\\label{prop:confspn}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ and ${\\ensuremath{\\mathbf{Z}}} = {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})} \\setminus {\\ensuremath{\\mathbf{Y}}}$.\nLet ${\\ensuremath{\\mathbf{y}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$ and let ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.  \nIt holds that \n\\begin{enumerate}\n \\item Each node in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ has the same scope as its corresponding node in ${\\ensuremath{\\mathcal{S}}}'$.\n \\item ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}$.\n \\item For any node ${\\mathsf{N}}$ in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ with ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cap {\\ensuremath{\\mathbf{Y}}} = \\emptyset$, we have that ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}_{\\mathsf{N}} = {\\ensuremath{\\mathcal{S}}}'_{\\mathsf{N}}$.\n \\item For ${\\ensuremath{\\mathbf{y}}}' \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$ it holds that \n\n", "itemtype": "equation", "pos": 29614, "prevtext": "\nFor each sum node ${\\mathsf{S}}$ we define the \\emph{conditioning sums} as\n\n", "index": 15, "text": "\\begin{equation}\n \\bm{{\\mathsf{S}}}^c({\\mathsf{S}}) := \\{{\\mathsf{S}}^c \\in {\\ensuremath{\\mathbf{anc}}}_{\\bm{{\\mathsf{S}}}}({\\mathsf{S}}) \\setminus \\{{\\mathsf{S}}\\} {\\ensuremath{\\,|\\,}} \\exists {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}^c) \\colon {\\mathsf{S}} \\notin {\\ensuremath{\\mathbf{desc}}}({\\mathsf{C}}) \\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\bm{{\\mathsf{S}}}^{c}({\\mathsf{S}}):=\\{{\\mathsf{S}}^{c}\\in{\\mathbf{anc}}_{\\bm{%&#10;{\\mathsf{S}}}}({\\mathsf{S}})\\setminus\\{{\\mathsf{S}}\\}{\\,|\\,}\\exists{\\mathsf{C}%&#10;}\\in{\\mathbf{ch}}({\\mathsf{S}}^{c})\\colon{\\mathsf{S}}\\notin{\\mathbf{desc}}({%&#10;\\mathsf{C}})\\}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>\ud835\udde6</mi><mi>c</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddb2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>\ud835\uddb2</mi><mi>c</mi></msup><mo>\u2208</mo><mrow><mrow><msub><mi>\ud835\udc1a\ud835\udc27\ud835\udc1c</mi><mi>\ud835\udde6</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddb2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2216</mo><mrow><mo stretchy=\"false\">{</mo><mi>\ud835\uddb2</mi><mo rspace=\"4.2pt\" stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mrow><mrow><mrow><mo>\u2203</mo><mi>\ud835\udda2</mi></mrow><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\uddb2</mi><mi>c</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mi>\ud835\uddb2</mi><mo>\u2209</mo><mrow><mi>\ud835\udc1d\ud835\udc1e\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udda2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\n\\end{enumerate}\n\\end{proposition}\n\n\n\nThe next theorem shows certain conditional independencies in the augmented SPN. \nFor ease of discussion, we make the following definitions.\n\n\n\n\\begin{definition}\nLet ${\\mathsf{S}}$ be a sum node in an augmented SPN and $Z_{\\mathsf{S}}$ its associated LV.\nAll other RVs (model RVs and LVs) are divided into three sets:\n\\begin{itemize}\n\\item\n\\emph{Parents} ${\\ensuremath{\\mathbf{Z}}}_p$, which are all LVs \"above\" ${\\mathsf{S}}$, i.e.~${\\ensuremath{\\mathbf{Z}}}_p = {\\ensuremath{\\mathbf{Z}}}_{{\\ensuremath{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}})} \\setminus Z_{\\mathsf{S}}$.\n\\item \n\\emph{Children} ${\\ensuremath{\\mathbf{Y}}}_c$, which are all model RVs and LVs \"below\" ${\\mathsf{S}}$, i.e.~${\\ensuremath{\\mathbf{Y}}}_c = {\\ensuremath{\\mathbf{sc}}}({\\mathsf{S}}) \\cup {\\ensuremath{\\mathbf{Z}}}_{{\\ensuremath{{\\mathbf{desc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}})} \\setminus Z_{\\mathsf{S}}$.\n\\item\n\\emph{Non-descendants} ${\\ensuremath{\\mathbf{Y}}}_n$, which are the remaining RVs, i.e.~${\\ensuremath{\\mathbf{Y}}}_n = {\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})} \\setminus ({\\ensuremath{\\mathbf{Z}}}_p \\cup {\\ensuremath{\\mathbf{Y}}}_c \\cup Z_{\\mathsf{S}})$.\n\\end{itemize}\n\\end{definition}\nWe will show that the \\emph{parents}, \\emph{children} and \\emph{non-descendants}\nplay the likewise role as for independencies in BNs \\cite{Pearl1988, Koller2009},\ni.e. $Z_{\\mathsf{S}}$ \\emph{is independent of} ${\\ensuremath{\\mathbf{Y}}}_n$ \\emph{given} ${\\ensuremath{\\mathbf{Z}}}_p$. \nIn certain cases, there can be further independence relations, depending on the\nstructure of the SPN. \nHowever, the independence relations shown in the following hold in any case. \nWe will further show that the sum weights of ${\\mathsf{S}}$ are the conditional distribution\nof $Z_{\\mathsf{S}}$, conditioned on the event that \"${\\ensuremath{\\mathbf{Z}}}_p$ select a path to ${\\mathsf{S}}$\". \nOne problem in the LV interpretation presented in \\cite{Poon2011} was that no conditional distribution of $Z_{\\mathsf{S}}$ was specified for\nthe complementary event.\nHere, we will show that the twin-weights are precisely this conditional distribution. \nThis requires that the event ``${\\ensuremath{\\mathbf{Z}}}_p$ select a path to the twin $\\bar{\\mathsf{S}}$'' is indeed the complementary event to ``${\\ensuremath{\\mathbf{Z}}}_p$ select a path to ${\\mathsf{S}}$''.\nThis is shown in following lemma.\n\n\n\n\\begin{lemma}\n\\label{lem:singlepath}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, let ${\\mathsf{S}}$ be a sum node in ${\\ensuremath{\\mathcal{S}}}$ and ${\\ensuremath{\\mathbf{Z}}}_p$ be the parents of $Z_{\\mathsf{S}}$.\nFor any ${\\ensuremath{\\mathbf{z}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, the configured SPN ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains either ${\\mathsf{S}}$ or its twin $\\bar{{\\mathsf{S}}}$, but not both.\n\\end{lemma}\n\n\n\nThe following theorem establishes certain conditional independence relations in augmented SPNs.\n\n\n\n\\begin{theorem}\n\\label{theo:conditionalIndependence}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$ and ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.\nLet ${\\mathsf{S}}$ be an arbitrary sum in ${\\ensuremath{\\mathcal{S}}}$ and ${w}_k = {w}_{{\\mathsf{S}},{\\mathsf{C}}^k_{\\mathsf{S}}}$, $\\bar{w}_k = \\bar{w}_{{\\mathsf{S}},{\\mathsf{C}}^k_{\\mathsf{S}}}$, \n$k=1,\\dots,K_{\\mathsf{S}}$.\nWith respect to ${\\mathsf{S}}$, let ${\\ensuremath{\\mathbf{Z}}}_p$ be the parents, ${\\ensuremath{\\mathbf{Y}}}_c$ be the children and ${\\ensuremath{\\mathbf{Y}}}_n$ be the \nnon-descendants, respectively.\nThen there exists a two-partition of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, i.e.~${\\bm{{\\mathcal{Z}}}}, \\bar{\\bm{{\\mathcal{Z}}}} \\colon {\\bm{{\\mathcal{Z}}}} \\cup \\bar{\\bm{{\\mathcal{Z}}}} = {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, ${\\bm{{\\mathcal{Z}}}} \\cap \\bar{\\bm{{\\mathcal{Z}}}} = \\emptyset$, such that\n\n", "itemtype": "equation", "pos": 39569, "prevtext": "\nFurthermore, we assume a set of locally normalized \\emph{twin-weights} $\\bm{\\bar{{w}}}$, containing a twin-weight $\\bar{{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$ for each weight ${w}_{{\\mathsf{S}},{\\mathsf{C}}}$ in the SPN.\nWe are now ready to define the \\emph{augmentation} of an SPN. \n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{AugmentSPN}{${\\ensuremath{\\mathcal{S}}}$}\n\\State ${\\ensuremath{\\mathcal{S}}}' \\leftarrow {\\ensuremath{\\mathcal{S}}}$ \n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}'),~\\forall k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}\\colon$\n\\Statex \\hskip\\algorithmicindent \\hskip\\algorithmicindent  \nlet ${w}_{{\\mathsf{S}},k} = {w}_{{\\mathsf{S}},{\\mathsf{C}}_{\\mathsf{S}}^k}$, $\\bar{w}_{{\\mathsf{S}},k} = \\bar{w}_{{\\mathsf{S}},{\\mathsf{C}}_{\\mathsf{S}}^k}$\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$}      \\label{as:introLinksStart}\n\\For{$k = 1 \\dots K_{\\mathsf{S}}$}\n\\State Introduce a new product node ${\\mathsf{P}}_{\\mathsf{S}}^k$ in ${\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$\n\\State Disconnect ${\\mathsf{C}}_{\\mathsf{S}}^k$ from ${\\mathsf{S}}$\n\\State Connect ${\\mathsf{C}}_{\\mathsf{S}}^k$ as child of ${\\mathsf{P}}_{\\mathsf{S}}^k$\n\\State Connect ${\\mathsf{P}}_{\\mathsf{S}}^k$ as child of ${\\mathsf{S}}$ with weight ${w}_{{\\mathsf{S}},k}$\n\\EndFor\n\\EndFor                                        \\label{as:introLinksEnd}\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}')$}\n\\For {$k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}$}            \\label{as:introIVStart}\n\\State Connect new IV ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as child of ${\\mathsf{P}}_{\\mathsf{S}}^k$\n\\EndFor                                           \\label{as:introIVEnd}\n\\If {$\\bm{{\\mathsf{S}}}^c({\\mathsf{S}}) \\not= \\emptyset$}   \n\\State Introduce a twin sum node $\\bar{{\\mathsf{S}}}$ in ${\\ensuremath{\\mathcal{S}}}'$             \\label{as:augmentRemedyStart}\n\\State $\\forall k \\in \\{1,\\dots,K_{\\mathsf{S}}\\}$: connect ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as child of $\\bar{{\\mathsf{S}}}$, \n\\Statex \\hskip\\algorithmicindent \\hskip\\algorithmicindent \\hskip\\algorithmicindent \\hskip\\algorithmicindent\nand let ${w}_{\\bar{{\\mathsf{S}}}, {\\lambda_{{Z_{\\mathsf{S}}}={k}}}} = \\bar{w}_{{{\\mathsf{S}}}, k}$\n\\For{${\\mathsf{S}}^c \\in \\bm{{\\mathsf{S}}}^c({\\mathsf{S}})$} \n\\For {$k \\in \\{k ~|~ {\\mathsf{S}} \\not\\in {\\ensuremath{\\mathbf{desc}}}({\\mathsf{P}}^k_{{\\mathsf{S}}^c})\\}$}\n\\State Connect $\\bar{{\\mathsf{S}}}$ as child of ${\\mathsf{P}}^k_{{\\mathsf{S}}^c}$     \\label{as:connectTwin}\n\\EndFor\n\\EndFor                                                              \\label{as:augmentRemedyEnd}\n\\EndIf                      \n\\EndFor\n\\State \\Return ${\\ensuremath{\\mathcal{S}}}'$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for augmentation of an SPN.}\n\\label{algo:augmentSPN}\n\\end{figure}\n\n\n\n\\begin{definition}[Augmentation of SPN]\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, $\\bar{\\bm{{w}}}$ be a set of twin-weights and ${\\ensuremath{\\mathcal{S}}}'$ be the result of algortihm \\textproc{AugmentSPN}, shown in \nFig.~\\ref{algo:augmentSPN}.\n${\\ensuremath{\\mathcal{S}}}'$ is called the \\emph{augmented} SPN of ${\\ensuremath{\\mathcal{S}}}$, denoted as ${\\ensuremath{\\mathcal{S}}}' =: {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.\nWithin the context of ${\\ensuremath{\\mathcal{S}}}'$, ${\\mathsf{C}}_{\\mathsf{S}}^k$ is called the $k^\\text{th}$ \\emph{former child} of ${\\mathsf{S}}$.\nThe introduced product node ${\\mathsf{P}}_{\\mathsf{S}}^k$ is called \\emph{link} of ${\\mathsf{S}}$, ${\\mathsf{C}}^k_{\\mathsf{S}}$ and ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$, respectively.\nThe sum node $\\bar{{\\mathsf{S}}}$, if introduced, is called the \\emph{twin} sum node of ${\\mathsf{S}}$.\nWith respect to ${\\ensuremath{\\mathcal{S}}}'$, we denote ${\\ensuremath{\\mathcal{S}}}$ as the original SPN.\n\\end{definition}\n\n\n\nIn steps \\ref{as:introLinksStart}--\\ref{as:introLinksEnd} of \\textproc{AugmentSPN} we introduce the links ${\\mathsf{P}}_{\\mathsf{S}}^k$ which are interconnected between sum node ${\\mathsf{S}}$ and its $k^\\text{th}$ child.\nEach link ${\\mathsf{P}}_{\\mathsf{S}}^k$ has a single parent, namely ${\\mathsf{S}}$, and simply copies the former child ${\\mathsf{C}}_{\\mathsf{S}}^k$.\nIn steps \\ref{as:introIVStart}--\\ref{as:introIVEnd}, we introduce IVs corresponding to the associated LV $Z_{\\mathsf{S}}$, as proposed in \\cite{Poon2011}.\nAs we saw in Figure~\\ref{fig:problemAugmentSPNnaiv} and the discussion above, this can render other sum nodes incomplete.\nThese sums are by definition the conditioning sums $\\bm{{\\mathsf{S}}}^c({\\mathsf{S}})$.\nThus, when necessary, we introduce a twin sum node in steps \\ref{as:augmentRemedyStart}--\\ref{as:augmentRemedyEnd}, to treat this problem.\nThe following proposition states the soundness of augmentation.\n\n\n\n\\begin{proposition}\n\\label{prop:augmentationSound}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$ and ${\\ensuremath{\\mathbf{Z}}} := {\\ensuremath{\\mathbf{Z}}}_{\\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$.\nThen ${\\ensuremath{\\mathcal{S}}}'$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$ with ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$.\n\\end{proposition}\n\n\n\nProposition \\ref{prop:augmentationSound} states that the marginal distribution of the augmented SPN is the same distribution over ${\\ensuremath{\\mathbf{X}}}$ as\nrepresented by the original SPN, while being a \\emph{completely specified probabilistic model} over ${\\ensuremath{\\mathbf{X}}}$ \\emph{and} ${\\ensuremath{\\mathbf{Z}}}$. \nThus, augmentation provides a way to generalize the LV interpretation from mixture models to more general SPNs. \nAn example of augmentation is shown in Fig.~\\ref{fig:exaugmentSPN}. \nIn the next section, we closer investigate the probabilistic interpretation of sum-weights in augmented SPNs.\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{exampleSPN}\n  \\label{fig:exampleaugpre}\t\n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{exampleSPN_Augment}\n  \\label{fig:exampleaugpost}\t\n }\n \\caption{Augmentation of an SPN. \n\\protect\\subref{fig:exampleaugpre}: Example SPN over ${\\ensuremath{\\mathbf{X}}} = \\{X_1, X_2, X_3\\}$, containing sum nodes ${\\mathsf{S}}^1$, ${\\mathsf{S}}^2$, ${\\mathsf{S}}^3$ and ${\\mathsf{S}}^4$. \n\\protect\\subref{fig:exampleaugpost}: Augmented SPN, containing IVs corresponding to $Z_{{\\mathsf{S}}^1}$, $Z_{{\\mathsf{S}}^2}$, $Z_{{\\mathsf{S}}^3}$, $Z_{{\\mathsf{S}}^4}$, links and twin sum nodes $\\bar{{\\mathsf{S}}}^2$, $\\bar{{\\mathsf{S}}}^3$, $\\bar{{\\mathsf{S}}}^4$.\nFor nodes introduced by augmentation, smaller circles are used. \n}\n\\label{fig:exaugmentSPN}\n\\end{figure}\n\n\n\\subsection{Conditional Independencies in Augmented SPNs and Probabilistic Interpretation of Sum-Weights}\nIt is helpful to introduce the notion of configured SPNs, which takes a similar role as conditioning in the literature on DNNFs \\cite{Darwiche1999, Darwiche2001, Darwiche2002}.\n\n\n\n\\begin{definition}[Configured SPN]\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ and ${\\ensuremath{\\mathbf{y}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$.\nThe \\emph{configured SPN} ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is obtained by deleting the IVs ${\\lambda_{{y}={y}}}$ and their corresponding link for each $Y \\in {\\ensuremath{\\mathbf{Y}}}$, $y \\not= {{{\\ensuremath{\\mathbf{y}}}}{[{Y}]}}$ from \n${\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$, and further deleting all nodes which are rendered unreachable from the root.\n\\end{definition}\n\n\n\nIntuitively, the configured SPN isolates the computational structure selected by ${\\ensuremath{\\mathbf{y}}}$. \nAll sum edges which \"survive\" in the configured SPN are equipped with the same weights as in the augmented SPN. \nTherefore, a configured SPN is in general not locally normalized. \nWe note the following properties of configured SPNs.\n\n\n\n\\begin{proposition}\n\\label{prop:confspn}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, ${\\ensuremath{\\mathbf{Y}}} \\subseteq {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ and ${\\ensuremath{\\mathbf{Z}}} = {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})} \\setminus {\\ensuremath{\\mathbf{Y}}}$.\nLet ${\\ensuremath{\\mathbf{y}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$ and let ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.  \nIt holds that \n\\begin{enumerate}\n \\item Each node in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ has the same scope as its corresponding node in ${\\ensuremath{\\mathcal{S}}}'$.\n \\item ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}$.\n \\item For any node ${\\mathsf{N}}$ in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ with ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cap {\\ensuremath{\\mathbf{Y}}} = \\emptyset$, we have that ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}_{\\mathsf{N}} = {\\ensuremath{\\mathcal{S}}}'_{\\mathsf{N}}$.\n \\item For ${\\ensuremath{\\mathbf{y}}}' \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Y}}})$ it holds that \n\n", "index": 17, "text": "\\begin{equation}\n {\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}({\\ensuremath{\\mathbf{X}}},{\\ensuremath{\\mathbf{Z}}},{\\ensuremath{\\mathbf{y}}}') = \n\\begin{cases}\n {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{X}}},{\\ensuremath{\\mathbf{Z}}},{\\ensuremath{\\mathbf{y}}}') & \\text{if } {\\ensuremath{\\mathbf{y}}}' = {\\ensuremath{\\mathbf{y}}} \\\\\n0                 & \\text{otherwise}\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\mathbf{y}}({\\mathbf{X}},{\\mathbf{Z}},{\\mathbf{y}}^{\\prime})=%&#10;\\begin{cases}{\\mathcal{S}}^{\\prime}({\\mathbf{X}},{\\mathbf{Z}},{\\mathbf{y}}^{%&#10;\\prime})&amp;\\text{if }{\\mathbf{y}}^{\\prime}={\\mathbf{y}}\\\\&#10;0&amp;\\text{otherwise}\\end{cases}\" display=\"block\"><mrow><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>\ud835\udc32</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc19</mi><mo>,</mo><msup><mi>\ud835\udc32</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc19</mi><mo>,</mo><msup><mi>\ud835\udc32</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msup><mi>\ud835\udc32</mi><mo>\u2032</mo></msup></mrow><mo>=</mo><mi>\ud835\udc32</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\n\\end{theorem}\n\n\n\nFrom Theorem~\\ref{theo:conditionalIndependence} it follows that the weights and twin-weights of a sum node ${\\mathsf{S}}$ \ncan be interpreted as \\emph{conditional probability tables} (CPTs) of $Z_{\\mathsf{S}}$, conditioned on ${\\ensuremath{\\mathbf{Z}}}_p$ and that \n$Z_{\\mathsf{S}}$ is conditionally independent of ${\\ensuremath{\\mathbf{Y}}}_n$ given ${\\ensuremath{\\mathbf{Z}}}_p$, i.e.\n\n", "itemtype": "equation", "pos": 44055, "prevtext": "\n\\end{enumerate}\n\\end{proposition}\n\n\n\nThe next theorem shows certain conditional independencies in the augmented SPN. \nFor ease of discussion, we make the following definitions.\n\n\n\n\\begin{definition}\nLet ${\\mathsf{S}}$ be a sum node in an augmented SPN and $Z_{\\mathsf{S}}$ its associated LV.\nAll other RVs (model RVs and LVs) are divided into three sets:\n\\begin{itemize}\n\\item\n\\emph{Parents} ${\\ensuremath{\\mathbf{Z}}}_p$, which are all LVs \"above\" ${\\mathsf{S}}$, i.e.~${\\ensuremath{\\mathbf{Z}}}_p = {\\ensuremath{\\mathbf{Z}}}_{{\\ensuremath{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}})} \\setminus Z_{\\mathsf{S}}$.\n\\item \n\\emph{Children} ${\\ensuremath{\\mathbf{Y}}}_c$, which are all model RVs and LVs \"below\" ${\\mathsf{S}}$, i.e.~${\\ensuremath{\\mathbf{Y}}}_c = {\\ensuremath{\\mathbf{sc}}}({\\mathsf{S}}) \\cup {\\ensuremath{\\mathbf{Z}}}_{{\\ensuremath{{\\mathbf{desc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}})} \\setminus Z_{\\mathsf{S}}$.\n\\item\n\\emph{Non-descendants} ${\\ensuremath{\\mathbf{Y}}}_n$, which are the remaining RVs, i.e.~${\\ensuremath{\\mathbf{Y}}}_n = {\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})} \\setminus ({\\ensuremath{\\mathbf{Z}}}_p \\cup {\\ensuremath{\\mathbf{Y}}}_c \\cup Z_{\\mathsf{S}})$.\n\\end{itemize}\n\\end{definition}\nWe will show that the \\emph{parents}, \\emph{children} and \\emph{non-descendants}\nplay the likewise role as for independencies in BNs \\cite{Pearl1988, Koller2009},\ni.e. $Z_{\\mathsf{S}}$ \\emph{is independent of} ${\\ensuremath{\\mathbf{Y}}}_n$ \\emph{given} ${\\ensuremath{\\mathbf{Z}}}_p$. \nIn certain cases, there can be further independence relations, depending on the\nstructure of the SPN. \nHowever, the independence relations shown in the following hold in any case. \nWe will further show that the sum weights of ${\\mathsf{S}}$ are the conditional distribution\nof $Z_{\\mathsf{S}}$, conditioned on the event that \"${\\ensuremath{\\mathbf{Z}}}_p$ select a path to ${\\mathsf{S}}$\". \nOne problem in the LV interpretation presented in \\cite{Poon2011} was that no conditional distribution of $Z_{\\mathsf{S}}$ was specified for\nthe complementary event.\nHere, we will show that the twin-weights are precisely this conditional distribution. \nThis requires that the event ``${\\ensuremath{\\mathbf{Z}}}_p$ select a path to the twin $\\bar{\\mathsf{S}}$'' is indeed the complementary event to ``${\\ensuremath{\\mathbf{Z}}}_p$ select a path to ${\\mathsf{S}}$''.\nThis is shown in following lemma.\n\n\n\n\\begin{lemma}\n\\label{lem:singlepath}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$, let ${\\mathsf{S}}$ be a sum node in ${\\ensuremath{\\mathcal{S}}}$ and ${\\ensuremath{\\mathbf{Z}}}_p$ be the parents of $Z_{\\mathsf{S}}$.\nFor any ${\\ensuremath{\\mathbf{z}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, the configured SPN ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains either ${\\mathsf{S}}$ or its twin $\\bar{{\\mathsf{S}}}$, but not both.\n\\end{lemma}\n\n\n\nThe following theorem establishes certain conditional independence relations in augmented SPNs.\n\n\n\n\\begin{theorem}\n\\label{theo:conditionalIndependence}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$ and ${\\ensuremath{\\mathcal{S}}}' = {\\ensuremath{\\mathbf{aug}}}({\\ensuremath{\\mathcal{S}}})$.\nLet ${\\mathsf{S}}$ be an arbitrary sum in ${\\ensuremath{\\mathcal{S}}}$ and ${w}_k = {w}_{{\\mathsf{S}},{\\mathsf{C}}^k_{\\mathsf{S}}}$, $\\bar{w}_k = \\bar{w}_{{\\mathsf{S}},{\\mathsf{C}}^k_{\\mathsf{S}}}$, \n$k=1,\\dots,K_{\\mathsf{S}}$.\nWith respect to ${\\mathsf{S}}$, let ${\\ensuremath{\\mathbf{Z}}}_p$ be the parents, ${\\ensuremath{\\mathbf{Y}}}_c$ be the children and ${\\ensuremath{\\mathbf{Y}}}_n$ be the \nnon-descendants, respectively.\nThen there exists a two-partition of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, i.e.~${\\bm{{\\mathcal{Z}}}}, \\bar{\\bm{{\\mathcal{Z}}}} \\colon {\\bm{{\\mathcal{Z}}}} \\cup \\bar{\\bm{{\\mathcal{Z}}}} = {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$, ${\\bm{{\\mathcal{Z}}}} \\cap \\bar{\\bm{{\\mathcal{Z}}}} = \\emptyset$, such that\n\n", "index": 19, "text": "\\begin{align}\n\\forall {\\ensuremath{\\mathbf{z}}} \\in {\\bm{{\\mathcal{Z}}}} \\colon {\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}}) & = {w}_k {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{Y}}}_n,{\\ensuremath{\\mathbf{z}}}), and   \\label{eq:CIaugSum}\\\\\n\\forall {\\ensuremath{\\mathbf{z}}} \\in \\bar {\\bm{{\\mathcal{Z}}}} \\colon {\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}}) & = \\bar{w}_k {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{Y}}}_n,{\\ensuremath{\\mathbf{z}}}).  \\label{eq:CIaugTwinSum}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall{\\mathbf{z}}\\in{\\bm{{\\mathcal{Z}}}}\\colon{\\mathcal{S}}^{%&#10;\\prime}(Z_{\\mathsf{S}}=k,{\\mathbf{Y}}_{n},{\\mathbf{z}})\" display=\"inline\"><mrow><mo>\u2200</mo><mi>\ud835\udc33</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi><mo>:</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc18</mi><mi>n</mi></msub><mo>,</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={w}_{k}{\\mathcal{S}}^{\\prime}({\\mathbf{Y}}_{n},{\\mathbf{z}}),and\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>w</mi><mi>k</mi></msub><mo>\u2062</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mi>n</mi></msub><mo>,</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>d</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\forall{\\mathbf{z}}\\in\\bar{\\bm{{\\mathcal{Z}}}}\\colon{\\mathcal{S}}%&#10;^{\\prime}(Z_{\\mathsf{S}}=k,{\\mathbf{Y}}_{n},{\\mathbf{z}})\" display=\"inline\"><mrow><mo>\u2200</mo><mi>\ud835\udc33</mi><mo>\u2208</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>:</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc18</mi><mi>n</mi></msub><mo>,</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\bar{w}_{k}{\\mathcal{S}}^{\\prime}({\\mathbf{Y}}_{n},{\\mathbf{z}}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>k</mi></msub><mo>\u2062</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mi>n</mi></msub><mo>,</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nUsing this result, we can define a BN representing the\naugmented SPN as follows: For each sum node ${\\mathsf{S}}$, connect\n${\\ensuremath{\\mathbf{Z}}}_p$ as parents of $Z_{\\mathsf{S}}$, and all RVs ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{S}})$ as children of $Z_{\\mathsf{S}}$. \nBy doing this for each LV, we obtain our BN representation of the augmented SPN. \nAn example of the BN interpretation is shown in Fig. 4. \nNote that for constructing the BN, it is generally not enough to consider only the LVs of \nthe conditioning sums as parents of each $Z_{\\mathsf{S}}$: consider a case where the root is a sum node, \nwhich is not a conditioning sum of ${\\mathsf{S}}$, but has conditioning sums as children. \nIn this case, $Z_{\\mathsf{S}}$ is generally not conditionally independent of the root's LV given the LVs of the root's children.\n\n\n\n\\begin{figure}\n\\centering\n\n  \n  \n\t\n\t\n  \n\t\n\t\n\t\n\t\n\t\n  \n\t\n\t\n\t\n\t\n\t\n\t\n \n\n\n \\includegraphics[width=0.21\\textwidth]{BNAugmented}\n  \\label{fig:reaugspn_BN}\n\n\n\n\n\n\n\n\n\\caption{Dependency structure of augmented SPN from Fig.~\\ref{fig:exaugmentSPN}, represented as BN.\n}\n\\label{fig:dependencystructBN}\n\\end{figure}\n\n\n\n\nWe see that in principle any augmented SPN can be represented as a BN. \nThis serves as a useful tool to understand SPNs in the context of classical probabilistic models. \nNote that our BN representation is distinct from the one presented by Zhao et al. \\cite{Zhao2015}. \nThey proposed a BN representation of SPNs using a bipartite structure: every LV is a parent of each model RV \nand the model RVs and LVs are unconnected among each other, respectively. \nThe actual independencies in the SPNs are represented in structured CPT using algebraic decision diagrams (ADDs). \nThe BN representation used here is more closely related to the original SPN structure and also reflects \nindependencies among the LVs. \nIn the next section, we use the LV interpretation to derive the EM algorithm for SPNs.\n\n\n\n\\section{EM Algorithm}\n\\label{sec:EM}\nThe EM algorithm is a general scheme for maximum likelihood\nlearning, when for some RVs complete evidence is absent \\cite{Dempster1977, Ghahramani1994}. \nThus, augmented SPNs are amenable for EM due to the LVs associated with sum nodes. \nMoreover, since the twin-weights are kept fixed and are not subject to optimization, EM optimizes the weights of the original SPN. \nThis approach was already pointed out in \\cite{Poon2011}, where it was suggested that for evidence ${\\ensuremath{\\mathbf{e}}}$ and for any LV $Z_{\\mathsf{S}}$, the marginal posteriors should be given as $p(Z_{\\mathsf{S}} = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}) \\propto {w}_{{\\mathsf{S}}, {\\mathsf{C}}^k_{\\mathsf{S}}} \\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}})}{\\partial {\\mathsf{S}}({\\ensuremath{\\mathbf{e}}})}$, which should be used for EM updates.\nThis, however, was an unfortunate type in the seminal SPN paper, as it can be seen that these updates actually \\emph{leave the weights unchanged}.\nHere, we formally derive the standard EM updates for sum-weights via the LV interpretation in augmented SPNs.\nFurthermore, we also derive EM updates for the input distributions, when they are chosen from an exponential family.\n\n\n\\subsection{Updates for Weights}\n\\label{sec:MPE_weights}\nAssume a dataset $\\mathcal{D} = \\{{\\ensuremath{\\mathbf{e}}}^{(1)}, \\dots, {\\ensuremath{\\mathbf{e}}}^{(L)}\\}$ of $L$ i.i.d.~samples, where each ${\\ensuremath{\\mathbf{e}}}^{(l)}$\nis any combination of complete and partial evidence for the model RVs ${\\ensuremath{\\mathbf{X}}}$, cf.~Section~\\ref{sec:backgroundNotation}.\nLet ${\\ensuremath{\\mathbf{Z}}} = {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ be the set of all LVs and consider an arbitrary sum node ${\\mathsf{S}}$.\nTo derive EM updates for the sum-weights we use the BN interpretation of augmented SPNs.\nEq.~\\eqref{eq:weightsAsCondProbs} shows that the weights can be interpreted as conditional probabilities\nin this BN, where\n\n", "itemtype": "equation", "pos": 45075, "prevtext": "\n\\end{theorem}\n\n\n\nFrom Theorem~\\ref{theo:conditionalIndependence} it follows that the weights and twin-weights of a sum node ${\\mathsf{S}}$ \ncan be interpreted as \\emph{conditional probability tables} (CPTs) of $Z_{\\mathsf{S}}$, conditioned on ${\\ensuremath{\\mathbf{Z}}}_p$ and that \n$Z_{\\mathsf{S}}$ is conditionally independent of ${\\ensuremath{\\mathbf{Y}}}_n$ given ${\\ensuremath{\\mathbf{Z}}}_p$, i.e.\n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:weightsAsCondProbs}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k {\\,|\\,} {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}}) = {\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k {\\,|\\,} {\\ensuremath{\\mathbf{z}}}) = \n\\begin{cases}\n{w}_k & \\text{if } {\\ensuremath{\\mathbf{z}}} \\in {\\bm{{\\mathcal{Z}}}} \\\\\n\\bar{w}_k & \\text{if } {\\ensuremath{\\mathbf{z}}} \\in \\bar {\\bm{{\\mathcal{Z}}}}.\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k{\\,|\\,}{\\mathbf{Y}}_{n},{\\mathbf{z}})={%&#10;\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k{\\,|\\,}{\\mathbf{z}})=\\begin{cases}{w}_{k%&#10;}&amp;\\text{if }{\\mathbf{z}}\\in{\\bm{{\\mathcal{Z}}}}\\\\&#10;\\bar{w}_{k}&amp;\\text{if }{\\mathbf{z}}\\in\\bar{\\bm{{\\mathcal{Z}}}}.\\end{cases}\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>\ud835\udc18</mi><mi>n</mi></msub><mo>,</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>w</mi><mi>k</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>k</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow><mo>\u2208</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nAs mentioned above, the twin-weights $\\bar{w}_k$ are kept fixed. \nThe EM-updates for sum-weight ${w}_k$ are given by summing over the expected statistics\n\n", "itemtype": "equation", "pos": 49477, "prevtext": "\nUsing this result, we can define a BN representing the\naugmented SPN as follows: For each sum node ${\\mathsf{S}}$, connect\n${\\ensuremath{\\mathbf{Z}}}_p$ as parents of $Z_{\\mathsf{S}}$, and all RVs ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{S}})$ as children of $Z_{\\mathsf{S}}$. \nBy doing this for each LV, we obtain our BN representation of the augmented SPN. \nAn example of the BN interpretation is shown in Fig. 4. \nNote that for constructing the BN, it is generally not enough to consider only the LVs of \nthe conditioning sums as parents of each $Z_{\\mathsf{S}}$: consider a case where the root is a sum node, \nwhich is not a conditioning sum of ${\\mathsf{S}}$, but has conditioning sums as children. \nIn this case, $Z_{\\mathsf{S}}$ is generally not conditionally independent of the root's LV given the LVs of the root's children.\n\n\n\n\\begin{figure}\n\\centering\n\n  \n  \n\t\n\t\n  \n\t\n\t\n\t\n\t\n\t\n  \n\t\n\t\n\t\n\t\n\t\n\t\n \n\n\n \\includegraphics[width=0.21\\textwidth]{BNAugmented}\n  \\label{fig:reaugspn_BN}\n\n\n\n\n\n\n\n\n\\caption{Dependency structure of augmented SPN from Fig.~\\ref{fig:exaugmentSPN}, represented as BN.\n}\n\\label{fig:dependencystructBN}\n\\end{figure}\n\n\n\n\nWe see that in principle any augmented SPN can be represented as a BN. \nThis serves as a useful tool to understand SPNs in the context of classical probabilistic models. \nNote that our BN representation is distinct from the one presented by Zhao et al. \\cite{Zhao2015}. \nThey proposed a BN representation of SPNs using a bipartite structure: every LV is a parent of each model RV \nand the model RVs and LVs are unconnected among each other, respectively. \nThe actual independencies in the SPNs are represented in structured CPT using algebraic decision diagrams (ADDs). \nThe BN representation used here is more closely related to the original SPN structure and also reflects \nindependencies among the LVs. \nIn the next section, we use the LV interpretation to derive the EM algorithm for SPNs.\n\n\n\n\\section{EM Algorithm}\n\\label{sec:EM}\nThe EM algorithm is a general scheme for maximum likelihood\nlearning, when for some RVs complete evidence is absent \\cite{Dempster1977, Ghahramani1994}. \nThus, augmented SPNs are amenable for EM due to the LVs associated with sum nodes. \nMoreover, since the twin-weights are kept fixed and are not subject to optimization, EM optimizes the weights of the original SPN. \nThis approach was already pointed out in \\cite{Poon2011}, where it was suggested that for evidence ${\\ensuremath{\\mathbf{e}}}$ and for any LV $Z_{\\mathsf{S}}$, the marginal posteriors should be given as $p(Z_{\\mathsf{S}} = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}) \\propto {w}_{{\\mathsf{S}}, {\\mathsf{C}}^k_{\\mathsf{S}}} \\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}})}{\\partial {\\mathsf{S}}({\\ensuremath{\\mathbf{e}}})}$, which should be used for EM updates.\nThis, however, was an unfortunate type in the seminal SPN paper, as it can be seen that these updates actually \\emph{leave the weights unchanged}.\nHere, we formally derive the standard EM updates for sum-weights via the LV interpretation in augmented SPNs.\nFurthermore, we also derive EM updates for the input distributions, when they are chosen from an exponential family.\n\n\n\\subsection{Updates for Weights}\n\\label{sec:MPE_weights}\nAssume a dataset $\\mathcal{D} = \\{{\\ensuremath{\\mathbf{e}}}^{(1)}, \\dots, {\\ensuremath{\\mathbf{e}}}^{(L)}\\}$ of $L$ i.i.d.~samples, where each ${\\ensuremath{\\mathbf{e}}}^{(l)}$\nis any combination of complete and partial evidence for the model RVs ${\\ensuremath{\\mathbf{X}}}$, cf.~Section~\\ref{sec:backgroundNotation}.\nLet ${\\ensuremath{\\mathbf{Z}}} = {\\ensuremath{\\mathbf{Z}}}_{{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})}$ be the set of all LVs and consider an arbitrary sum node ${\\mathsf{S}}$.\nTo derive EM updates for the sum-weights we use the BN interpretation of augmented SPNs.\nEq.~\\eqref{eq:weightsAsCondProbs} shows that the weights can be interpreted as conditional probabilities\nin this BN, where\n\n", "index": 23, "text": "\\begin{equation}\n\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k {\\,|\\,} {\\ensuremath{\\mathbf{Z}}}_p = {\\ensuremath{\\mathbf{z}}}) = \n\\begin{cases}\n{w}_k & \\text{if } {\\ensuremath{\\mathbf{z}}} \\in {\\bm{{\\mathcal{Z}}}} \\\\\n\\bar{w}_k & \\text{if } {\\ensuremath{\\mathbf{z}}} \\in \\bar {\\bm{{\\mathcal{Z}}}}.\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k{\\,|\\,}{\\mathbf{Z}}_{p}={\\mathbf{z%&#10;}})=\\begin{cases}{w}_{k}&amp;\\text{if }{\\mathbf{z}}\\in{\\bm{{\\mathcal{Z}}}}\\\\&#10;\\bar{w}_{k}&amp;\\text{if }{\\mathbf{z}}\\in\\bar{\\bm{{\\mathcal{Z}}}}.\\end{cases}\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msub><mi>\ud835\udc19</mi><mi>p</mi></msub><mo>=</mo><mi>\ud835\udc33</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>w</mi><mi>k</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>k</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow><mo>\u2208</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nand renormalization \\cite{Koller2009, Dempster1977}.\nWe make the event ${\\ensuremath{\\mathbf{Z}}}_p \\in {\\bm{{\\mathcal{Z}}}}$ explicit, by introducing a \\emph{switching parent} $Y_{\\mathsf{S}}$ of $Z_{\\mathsf{S}}$:\nWhen the twin sum of ${\\mathsf{S}}$ exists, $Y_{\\mathsf{S}}$ assumes the two states ${\\ensuremath{\\mathbf{val}}}(Y_{\\mathsf{S}})=\\{y_{\\mathsf{S}}, y_{\\bar {\\mathsf{S}}}\\}$, where \n$Y_{\\mathsf{S}} = y_{\\mathsf{S}} \\Leftrightarrow {\\ensuremath{\\mathbf{Z}}}_p \\in {\\bm{{\\mathcal{Z}}}}$ and \n$Y_{\\mathsf{S}} = y_{\\bar{\\mathsf{S}}} \\Leftrightarrow {\\ensuremath{\\mathbf{Z}}}_p \\in \\bar{\\bm{{\\mathcal{Z}}}}$.\nWhen the twin sum does not exist, $Y_{\\mathsf{S}}$ just takes the single value ${\\ensuremath{\\mathbf{val}}}(Y_{\\mathsf{S}})=\\{y_{\\mathsf{S}}\\}$. \nClearly, when observed, $Y_{\\mathsf{S}}$ renders $Z_{\\mathsf{S}}$ independent from ${\\ensuremath{\\mathbf{Z}}}_p$. \nThe switching parent can be explicitly introduced in the augmented SPN, as depicted in Fig.~\\ref{fig:augmentSPN_switchingparent}. \n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{SumNodeExcerpt_Twin}\n  \\label{fig:augSPN_switchpar1}\n  \\put(-55,72){${\\mathsf{S}}$}\n  \\put(-130,72){$\\bar{{\\mathsf{S}}}$}\n  \\put(-150,5){$\\underbrace{~~~~~~~~~~~~~~~~~~}_{{\\lambda_{{Z_{{\\mathsf{S}}}}={1}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={2}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={3}}}}$}\n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{SumNodeExcerpt_Twin_Y}\n   \\label{fig:augSPN_switchpar2}\n\t \\put(-55,72){${\\mathsf{S}}$}\n   \\put(-130,72){$\\bar{{\\mathsf{S}}}$}\n   \\put(-150,5){$\\underbrace{~~~~~~~~~~~~~~~~~~}_{{\\lambda_{{Z_{{\\mathsf{S}}}}={1}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={2}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={3}}}}$}\n   \\put(-78,87){${\\lambda_{{Y_{\\mathsf{S}}}={y_{{\\mathsf{S}}}}}}$}\n   \\put(-153,87){${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{{\\mathsf{S}}}}}}}$}\n   \\put(-30,97){${\\mathsf{P}}$}\n}\n\\caption{Explicitly introducing a switching parent $Y_{\\mathsf{S}}$ in an augmented SPN. \n\\protect\\subref{fig:augSPN_switchpar1}: Part of an augmented SPN containing a sum node with three children and its twin. \n\\protect\\subref{fig:augSPN_switchpar2}: Explicitly introduced switching parent $Y_{\\mathsf{S}}$ using IVs ${\\lambda_{{Y_{\\mathsf{S}}}={y_{{\\mathsf{S}}}}}}$ and ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{{\\mathsf{S}}}}}}}$.}\n\\label{fig:augmentSPN_switchingparent}\n\\end{figure}\n\n\n\n\nHere we simply introduce two new IVs ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}$ and ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{\\mathsf{S}}}}}}$, \nwhich switch on/off the output of ${\\mathsf{S}}$ and $\\bar{\\mathsf{S}}$, respectively. \nIt is easy to see that when these IV are constantly set to 1, i.e. when $Y_{\\mathsf{S}}$ is marginalized, \nthe augmented SPN performs exactly the same computations as before. \nIt is furthermore easy to see that completeness and decomposability of the augmented SPN are maintained\nwhen the switching parent is introduced. \nUsing the switching parent, the required expected statistics \\eqref{eq:expectedStatisticsWeights} translate to\n\n", "itemtype": "equation", "pos": 49958, "prevtext": "\nAs mentioned above, the twin-weights $\\bar{w}_k$ are kept fixed. \nThe EM-updates for sum-weight ${w}_k$ are given by summing over the expected statistics\n\n", "index": 25, "text": "\\begin{equation}\n\\label{eq:expectedStatisticsWeights}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Z}}}_p \\in {\\bm{{\\mathcal{Z}}}} {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k,{\\mathbf{Z}}_{p}\\in{\\bm{{\\mathcal{Z}}}%&#10;}{\\,|\\,}{\\mathbf{e}}^{(l)})\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc19</mi><mi>p</mi></msub><mo>\u2208</mo><mpadded width=\"+1.7pt\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce9</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nTo compute \\eqref{eq:expectedStatisticsWeights2}, we use the differential approach, \\cite{Darwiche2003, Peharz2015, Peharz2015b}, cf.~also Section~\\ref{sec:backgroundNotation}.\nFirst note that\n\n", "itemtype": "equation", "pos": 53209, "prevtext": "\nand renormalization \\cite{Koller2009, Dempster1977}.\nWe make the event ${\\ensuremath{\\mathbf{Z}}}_p \\in {\\bm{{\\mathcal{Z}}}}$ explicit, by introducing a \\emph{switching parent} $Y_{\\mathsf{S}}$ of $Z_{\\mathsf{S}}$:\nWhen the twin sum of ${\\mathsf{S}}$ exists, $Y_{\\mathsf{S}}$ assumes the two states ${\\ensuremath{\\mathbf{val}}}(Y_{\\mathsf{S}})=\\{y_{\\mathsf{S}}, y_{\\bar {\\mathsf{S}}}\\}$, where \n$Y_{\\mathsf{S}} = y_{\\mathsf{S}} \\Leftrightarrow {\\ensuremath{\\mathbf{Z}}}_p \\in {\\bm{{\\mathcal{Z}}}}$ and \n$Y_{\\mathsf{S}} = y_{\\bar{\\mathsf{S}}} \\Leftrightarrow {\\ensuremath{\\mathbf{Z}}}_p \\in \\bar{\\bm{{\\mathcal{Z}}}}$.\nWhen the twin sum does not exist, $Y_{\\mathsf{S}}$ just takes the single value ${\\ensuremath{\\mathbf{val}}}(Y_{\\mathsf{S}})=\\{y_{\\mathsf{S}}\\}$. \nClearly, when observed, $Y_{\\mathsf{S}}$ renders $Z_{\\mathsf{S}}$ independent from ${\\ensuremath{\\mathbf{Z}}}_p$. \nThe switching parent can be explicitly introduced in the augmented SPN, as depicted in Fig.~\\ref{fig:augmentSPN_switchingparent}. \n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{SumNodeExcerpt_Twin}\n  \\label{fig:augSPN_switchpar1}\n  \\put(-55,72){${\\mathsf{S}}$}\n  \\put(-130,72){$\\bar{{\\mathsf{S}}}$}\n  \\put(-150,5){$\\underbrace{~~~~~~~~~~~~~~~~~~}_{{\\lambda_{{Z_{{\\mathsf{S}}}}={1}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={2}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={3}}}}$}\n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[scale=0.35]{SumNodeExcerpt_Twin_Y}\n   \\label{fig:augSPN_switchpar2}\n\t \\put(-55,72){${\\mathsf{S}}$}\n   \\put(-130,72){$\\bar{{\\mathsf{S}}}$}\n   \\put(-150,5){$\\underbrace{~~~~~~~~~~~~~~~~~~}_{{\\lambda_{{Z_{{\\mathsf{S}}}}={1}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={2}}} \\, {\\lambda_{{Z_{{\\mathsf{S}}}}={3}}}}$}\n   \\put(-78,87){${\\lambda_{{Y_{\\mathsf{S}}}={y_{{\\mathsf{S}}}}}}$}\n   \\put(-153,87){${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{{\\mathsf{S}}}}}}}$}\n   \\put(-30,97){${\\mathsf{P}}$}\n}\n\\caption{Explicitly introducing a switching parent $Y_{\\mathsf{S}}$ in an augmented SPN. \n\\protect\\subref{fig:augSPN_switchpar1}: Part of an augmented SPN containing a sum node with three children and its twin. \n\\protect\\subref{fig:augSPN_switchpar2}: Explicitly introduced switching parent $Y_{\\mathsf{S}}$ using IVs ${\\lambda_{{Y_{\\mathsf{S}}}={y_{{\\mathsf{S}}}}}}$ and ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{{\\mathsf{S}}}}}}}$.}\n\\label{fig:augmentSPN_switchingparent}\n\\end{figure}\n\n\n\n\nHere we simply introduce two new IVs ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}$ and ${\\lambda_{{Y_{\\mathsf{S}}}={y_{\\bar{\\mathsf{S}}}}}}$, \nwhich switch on/off the output of ${\\mathsf{S}}$ and $\\bar{\\mathsf{S}}$, respectively. \nIt is easy to see that when these IV are constantly set to 1, i.e. when $Y_{\\mathsf{S}}$ is marginalized, \nthe augmented SPN performs exactly the same computations as before. \nIt is furthermore easy to see that completeness and decomposability of the augmented SPN are maintained\nwhen the switching parent is introduced. \nUsing the switching parent, the required expected statistics \\eqref{eq:expectedStatisticsWeights} translate to\n\n", "index": 27, "text": "\\begin{equation}\n\\label{eq:expectedStatisticsWeights2}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, Y_{\\mathsf{S}} = y_{\\mathsf{S}} {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k,Y_{\\mathsf{S}}=y_{\\mathsf{S}}{\\,|\\,}{%&#10;\\mathbf{e}}^{(l)}).\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nThe first derivative is given as\n\n", "itemtype": "equation", "pos": 53594, "prevtext": "\nTo compute \\eqref{eq:expectedStatisticsWeights2}, we use the differential approach, \\cite{Darwiche2003, Peharz2015, Peharz2015b}, cf.~also Section~\\ref{sec:backgroundNotation}.\nFirst note that\n\n", "index": 29, "text": "\\begin{equation}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, Y_{\\mathsf{S}} = y_{\\mathsf{S}}, {\\ensuremath{\\mathbf{e}}}^{(l)}) = \n\\frac\n{\\partial^2 {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}\n{\\partial {\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}} \\partial {\\lambda_{{Z_{\\mathsf{S}}}={k}}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k,Y_{\\mathsf{S}}=y_{\\mathsf{S}},{\\mathbf%&#10;{e}}^{(l)})=\\frac{\\partial^{2}{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{%&#10;\\partial{\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}\\partial{\\lambda_{{Z_{%&#10;\\mathsf{S}}}={k}}}}.\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub><mo>,</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mrow></msub></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nwhere ${\\mathsf{P}}$ is the common product parent of ${\\mathsf{S}}$ and $ {\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}$ in the augmented SPN (see Fig.~\\ref{fig:augSPN_switchpar2}).\nDifferentiating \\eqref{eq:firstDerivativeWeights} after ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ yields the second derivative \n\n", "itemtype": "equation", "pos": 53956, "prevtext": "\nThe first derivative is given as\n\n", "index": 31, "text": "\\begin{align}\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}} \n&= \\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} \\, {\\mathsf{S}}({\\ensuremath{\\mathbf{e}}}^{(l)}) \\\\\n&=  \\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} \\, \\sum_{k=1}^{K_{\\mathsf{S}}} {\\lambda_{{Z_{\\mathsf{S}}}={k}}} \\, {w}_k \\, {\\mathsf{C}}_{\\mathsf{S}}^k({\\ensuremath{\\mathbf{e}}}^{(l)}), \n\\label{eq:firstDerivativeWeights}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{%&#10;\\partial{\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mrow></msub></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{%&#10;\\partial{\\mathsf{P}}}\\,{\\mathsf{S}}({\\mathbf{e}}^{(l)})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddaf</mi></mrow></mfrac></mstyle></mpadded><mo>\u2062</mo><mi>\ud835\uddb2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{%&#10;\\partial{\\mathsf{P}}}\\,\\sum_{k=1}^{K_{\\mathsf{S}}}{\\lambda_{{Z_{\\mathsf{S}}}={%&#10;k}}}\\,{w}_{k}\\,{\\mathsf{C}}_{\\mathsf{S}}^{k}({\\mathbf{e}}^{(l)}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddaf</mi></mrow></mfrac></mstyle></mpadded><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>K</mi><mi>\ud835\uddb2</mi></msub></munderover></mstyle><mrow><mpadded width=\"+1.7pt\"><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ndelivering the required posteriors\n\n", "itemtype": "equation", "pos": 54851, "prevtext": "\nwhere ${\\mathsf{P}}$ is the common product parent of ${\\mathsf{S}}$ and $ {\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}$ in the augmented SPN (see Fig.~\\ref{fig:augSPN_switchpar2}).\nDifferentiating \\eqref{eq:firstDerivativeWeights} after ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ yields the second derivative \n\n", "index": 33, "text": "\\begin{equation}\n\\frac\n{\\partial^2 {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}\n{\\partial {\\lambda_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}} \\partial {\\lambda_{{Z_{\\mathsf{S}}}={k}}}}\n=\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} \\, {w}_k \\, {\\mathsf{C}}_{\\mathsf{S}}^k({\\ensuremath{\\mathbf{e}}}^{(l)}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial^{2}{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{\\partial{\\lambda%&#10;_{{Y_{\\mathsf{S}}}={y_{\\mathsf{S}}}}}\\partial{\\lambda_{{Z_{\\mathsf{S}}}={k}}}}%&#10;=\\frac{\\partial{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{\\partial{\\mathsf{P}%&#10;}}\\,{w}_{k}\\,{\\mathsf{C}}_{\\mathsf{S}}^{k}({\\mathbf{e}}^{(l)}),\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mrow></msub></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub></mrow></mrow></mfrac><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddaf</mi></mrow></mfrac></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nWe do not want to construct the augmented SPN explicitly, so we express \\eqref{eq:EMposteriorsWeight} in terms of the original SPN.\nSince all LVs are marginalized, it holds that ${\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)}) = {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})$ and $\\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} = \\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{S}}}$, yielding\n\n", "itemtype": "equation", "pos": 55273, "prevtext": "\ndelivering the required posteriors\n\n", "index": 35, "text": "\\begin{equation}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, Y_{\\mathsf{S}} = y_{\\mathsf{S}} {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) = \n\\frac{1}{{\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} \\, {w}_k \\, {\\mathsf{C}}_{\\mathsf{S}}^k({\\ensuremath{\\mathbf{e}}}^{(l)}).\n\\label{eq:EMposteriorsWeight}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k,Y_{\\mathsf{S}}=y_{\\mathsf{S}}{\\,|\\,}{%&#10;\\mathbf{e}}^{(l)})=\\frac{1}{{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}\\frac{%&#10;\\partial{\\mathcal{S}}^{\\prime}({\\mathbf{e}}^{(l)})}{\\partial{\\mathsf{P}}}\\,{w}%&#10;_{k}\\,{\\mathsf{C}}_{\\mathsf{S}}^{k}({\\mathbf{e}}^{(l)}).\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddaf</mi></mrow></mfrac></mpadded><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ndelivering the required statistics for updating the sum-weights.\nWe now turn to the updates of the input distributions.\n\n\n\n\\subsection{Updates for Input Distributions}\n\\label{sec:MPE_dist}\nFor simplicity, we derive updates for univariate input distributions,\ni.e. for all distributions ${\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}}$ we have $|{\\ensuremath{\\mathbf{sc}}}({\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}})| = 1$.\nSimilar updates can rather easily be derived also for multivariate\ninput distributions. \nIn \\cite{Peharz2015}, the so-called distribution selectors (DSs) were introduced \nto derive the differential approach for generalized SPNs. \nSimilar as the switching parents for (twin) sum nodes, the DSs are RVs which render\nthe respective model RVs independent from the remaining RVs. \nMore formally, for each $X \\in {\\ensuremath{\\mathbf{X}}}$, let ${\\bm{\\mathsf{D}}}_X$ be the set of\nall input distributions which have scope $\\{X\\}$. \nAssume an arbitrary but fixed ordering of ${\\bm{\\mathsf{D}}}_X$ and let $[{\\bm{\\mathsf{D}}}_X]$ be the index\nof distribution ${\\bm{\\mathsf{D}}}_X$ in this ordering. \nLet the DS $W_X$ be a discrete RV with $|{\\bm{\\mathsf{D}}}_X|$ states. \nThe so-called gated SPN ${\\ensuremath{\\mathcal{S}}}^g$ is obtained by replacing each distribution by the product node\n\n", "itemtype": "equation", "pos": 56224, "prevtext": "\nWe do not want to construct the augmented SPN explicitly, so we express \\eqref{eq:EMposteriorsWeight} in terms of the original SPN.\nSince all LVs are marginalized, it holds that ${\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)}) = {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})$ and $\\frac{\\partial {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}} = \\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{S}}}$, yielding\n\n", "index": 37, "text": "\\begin{equation}\n{\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, Y_{\\mathsf{S}} = y_{\\mathsf{S}} {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) = \n\\frac{1}{{\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{S}}} \\, {w}_k \\, {\\mathsf{C}}_{\\mathsf{S}}^k({\\ensuremath{\\mathbf{e}}}^{(l)}),\n\\label{eq:EMposteriorsWeight2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{\\prime}(Z_{\\mathsf{S}}=k,Y_{\\mathsf{S}}=y_{\\mathsf{S}}{\\,|\\,}{%&#10;\\mathbf{e}}^{(l)})=\\frac{1}{{\\mathcal{S}}({\\mathbf{e}}^{(l)})}\\frac{\\partial{%&#10;\\mathcal{S}}({\\mathbf{e}}^{(l)})}{\\partial{\\mathsf{S}}}\\,{w}_{k}\\,{\\mathsf{C}}%&#10;_{\\mathsf{S}}^{k}({\\mathbf{e}}^{(l)}),\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>Y</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>y</mi><mi>\ud835\uddb2</mi></msub></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddb2</mi></mrow></mfrac></mpadded><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nThe introduced product is denoted as gate. \nAs shown in \\cite{Peharz2015}, $X$ is rendered independent from all other RVs in the SPN\nwhen conditioned on $W_X$. \nMoreover, ${\\mathsf{D}}_X$ is the conditional distribution of $X$ given \n$W_X = [{\\mathsf{D}}_X]$. \nTherefore, each $X$ and its DS $W_X$ can be incorporated as a two RV family in our BN\ninterpretation. \nWhen each input distribution ${\\mathsf{D}}_X$ is chosen from an exponential family with natural parameters \n$\\theta_{{\\mathsf{D}}_X}$, the M-step is given by the expected sufficient statistics\n\n", "itemtype": "equation", "pos": 57947, "prevtext": "\ndelivering the required statistics for updating the sum-weights.\nWe now turn to the updates of the input distributions.\n\n\n\n\\subsection{Updates for Input Distributions}\n\\label{sec:MPE_dist}\nFor simplicity, we derive updates for univariate input distributions,\ni.e. for all distributions ${\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}}$ we have $|{\\ensuremath{\\mathbf{sc}}}({\\mathsf{D}}_{\\ensuremath{\\mathbf{Y}}})| = 1$.\nSimilar updates can rather easily be derived also for multivariate\ninput distributions. \nIn \\cite{Peharz2015}, the so-called distribution selectors (DSs) were introduced \nto derive the differential approach for generalized SPNs. \nSimilar as the switching parents for (twin) sum nodes, the DSs are RVs which render\nthe respective model RVs independent from the remaining RVs. \nMore formally, for each $X \\in {\\ensuremath{\\mathbf{X}}}$, let ${\\bm{\\mathsf{D}}}_X$ be the set of\nall input distributions which have scope $\\{X\\}$. \nAssume an arbitrary but fixed ordering of ${\\bm{\\mathsf{D}}}_X$ and let $[{\\bm{\\mathsf{D}}}_X]$ be the index\nof distribution ${\\bm{\\mathsf{D}}}_X$ in this ordering. \nLet the DS $W_X$ be a discrete RV with $|{\\bm{\\mathsf{D}}}_X|$ states. \nThe so-called gated SPN ${\\ensuremath{\\mathcal{S}}}^g$ is obtained by replacing each distribution by the product node\n\n", "index": 39, "text": "\\begin{equation}   \\label{eq:introduceGate}\n{\\mathsf{D}}_X \\rightarrow {\\mathsf{D}}_X \\times {\\lambda_{{W_X}={[{\\mathsf{D}}_X]}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"{\\mathsf{D}}_{X}\\rightarrow{\\mathsf{D}}_{X}\\times{\\lambda_{{W_{X}}={[{\\mathsf{%&#10;D}}_{X}]}}}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mo>\u2192</mo><mrow><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mo>\u00d7</mo><msub><mi>\u03bb</mi><mrow><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nwhere $k=[{\\mathsf{D}}_X]$.\nWhen ${\\ensuremath{\\mathbf{e}}}^{(l)}$ contains complete evidence $x'$ for $X$, then the integral \n$\\int {\\mathsf{D}}_X(x {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) \\theta_{{\\mathsf{D}}_X}(x) \\mathrm{d}x$\nreduces to $\\theta_{{\\mathsf{D}}_X}(x')$.\nWhen ${\\ensuremath{\\mathbf{e}}}^{(l)}$ contains partial evidence ${\\mathcal{X}}$, then \n\n", "itemtype": "equation", "pos": 58650, "prevtext": "\nThe introduced product is denoted as gate. \nAs shown in \\cite{Peharz2015}, $X$ is rendered independent from all other RVs in the SPN\nwhen conditioned on $W_X$. \nMoreover, ${\\mathsf{D}}_X$ is the conditional distribution of $X$ given \n$W_X = [{\\mathsf{D}}_X]$. \nTherefore, each $X$ and its DS $W_X$ can be incorporated as a two RV family in our BN\ninterpretation. \nWhen each input distribution ${\\mathsf{D}}_X$ is chosen from an exponential family with natural parameters \n$\\theta_{{\\mathsf{D}}_X}$, the M-step is given by the expected sufficient statistics\n\n", "index": 41, "text": "\\begin{equation}   \\label{eq:MstepDistnodes}\n\\theta_{{\\mathsf{D}}_X} \\leftarrow \n\\frac\n{\\sum_l {\\ensuremath{\\mathcal{S}}}^g(W_X = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) \\int {\\mathsf{D}}_X(x {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) \\theta_{{\\mathsf{D}}_X}(x) \\mathrm{d}x}\n{\\sum_l {\\ensuremath{\\mathcal{S}}}^g(W_X = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)})},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\theta_{{\\mathsf{D}}_{X}}\\leftarrow\\frac{\\sum_{l}{\\mathcal{S}}^{g}(W_{X}=k{\\,|%&#10;\\,}{\\mathbf{e}}^{(l)})\\int{\\mathsf{D}}_{X}(x{\\,|\\,}{\\mathbf{e}}^{(l)})\\theta_{%&#10;{\\mathsf{D}}_{X}}(x)\\mathrm{d}x}{\\sum_{l}{\\mathcal{S}}^{g}(W_{X}=k{\\,|\\,}{%&#10;\\mathbf{e}}^{(l)})},\" display=\"block\"><mrow><mrow><msub><mi>\u03b8</mi><msub><mi>\ud835\udda3</mi><mi>X</mi></msub></msub><mo>\u2190</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>l</mi></msub><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03b8</mi><msub><mi>\ud835\udda3</mi><mi>X</mi></msub></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>x</mi></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>l</mi></msub><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nDepending on $X$ and the the type of ${\\mathsf{D}}_X$, evaluating \\eqref{eq:integralDist} can be more\nor less demanding. \nA simple but practical case is when ${\\mathsf{D}}_X$ is Gaussian and ${\\mathcal{X}}$ is some interval, permitting a\nclosed form solution for integrating the Gaussian's statistics $\\theta(x) = (x,x^2)$, using truncated Gaussians \\cite{Johnson1994}.\n\nTo obtain the posteriors ${\\ensuremath{\\mathcal{S}}}^g(W_X = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)})$ required in \\eqref{eq:MstepDistnodes}, we again use the differential approach.\nNote that\n\n", "itemtype": "equation", "pos": 59390, "prevtext": "\nwhere $k=[{\\mathsf{D}}_X]$.\nWhen ${\\ensuremath{\\mathbf{e}}}^{(l)}$ contains complete evidence $x'$ for $X$, then the integral \n$\\int {\\mathsf{D}}_X(x {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) \\theta_{{\\mathsf{D}}_X}(x) \\mathrm{d}x$\nreduces to $\\theta_{{\\mathsf{D}}_X}(x')$.\nWhen ${\\ensuremath{\\mathbf{e}}}^{(l)}$ contains partial evidence ${\\mathcal{X}}$, then \n\n", "index": 43, "text": "\\begin{equation} \\label{eq:integralDist}\n\\int {\\mathsf{D}}_X(x {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) \\theta_{{\\mathsf{D}}_X}(x) \\mathrm{d}x =\n\\frac\n{ \\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\theta_{{\\mathsf{D}}_X}(x) \\mathrm{d}x }\n{ \\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\mathrm{d}x }.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\int{\\mathsf{D}}_{X}(x{\\,|\\,}{\\mathbf{e}}^{(l)})\\theta_{{\\mathsf{D}}_{X}}(x)%&#10;\\mathrm{d}x=\\frac{\\int_{\\mathcal{X}}{\\mathsf{D}}_{X}(x)\\theta_{{\\mathsf{D}}_{X%&#10;}}(x)\\mathrm{d}x}{\\int_{\\mathcal{X}}{\\mathsf{D}}_{X}(x)\\mathrm{d}x}.\" display=\"block\"><mrow><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>x</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><msub><mi>\u03b8</mi><msub><mi>\ud835\udda3</mi><mi>X</mi></msub></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><mi>x</mi><mo>=</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></msub><mrow><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03b8</mi><msub><mi>\ud835\udda3</mi><mi>X</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>x</mi></mrow></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></msub><mrow><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>x</mi></mrow></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nwhere $k=[{\\mathsf{D}}_X]$ and ${\\mathsf{P}}$ is the gate of ${\\mathsf{D}}_X$, cf.~\\eqref{eq:introduceGate}.\nIf we do not want to construct the gated SPN explicitly, we can use the identity\n$\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}^g({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}}\n=\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{D}}_X}\n$.\nThus the required posteriors are given as\n\n", "itemtype": "equation", "pos": 60255, "prevtext": "\nDepending on $X$ and the the type of ${\\mathsf{D}}_X$, evaluating \\eqref{eq:integralDist} can be more\nor less demanding. \nA simple but practical case is when ${\\mathsf{D}}_X$ is Gaussian and ${\\mathcal{X}}$ is some interval, permitting a\nclosed form solution for integrating the Gaussian's statistics $\\theta(x) = (x,x^2)$, using truncated Gaussians \\cite{Johnson1994}.\n\nTo obtain the posteriors ${\\ensuremath{\\mathcal{S}}}^g(W_X = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)})$ required in \\eqref{eq:MstepDistnodes}, we again use the differential approach.\nNote that\n\n", "index": 45, "text": "\\begin{equation}\n{\\ensuremath{\\mathcal{S}}}^g(W_X = k, {\\ensuremath{\\mathbf{e}}}^{(l)}) = \n\\frac\n{\\partial {\\ensuremath{\\mathcal{S}}}^g({\\ensuremath{\\mathbf{e}}}^{(l)})}\n{\\partial {\\lambda_{{W_X}={k}}}} =\n\\frac\n{\\partial {\\ensuremath{\\mathcal{S}}}^g({\\ensuremath{\\mathbf{e}}}^{(l)})}\n{\\partial {\\mathsf{P}}}\n{\\mathsf{D}}_X({\\ensuremath{\\mathbf{e}}}^{(l)}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{g}(W_{X}=k,{\\mathbf{e}}^{(l)})=\\frac{\\partial{\\mathcal{S}}^{g}(%&#10;{\\mathbf{e}}^{(l)})}{\\partial{\\lambda_{{W_{X}}={k}}}}=\\frac{\\partial{\\mathcal{%&#10;S}}^{g}({\\mathbf{e}}^{(l)})}{\\partial{\\mathsf{P}}}{\\mathsf{D}}_{X}({\\mathbf{e}%&#10;}^{(l)}),\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03bb</mi><mrow><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mi>k</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\ud835\uddaf</mi></mrow></mfrac><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nThe EM algorithm for SPNs, both for sum-weights and input distributions, is summarized in Fig.~\\ref{algo:EM}.\nIn Section \\ref{sec:experimentsEM} we empirically verify our derivation of EM and show that standard EM successfully \ntrains SPNs when a suitable structure is at hand.\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{Expectation-Maximization}{{\\ensuremath{\\mathcal{S}}}}\n\\State Initialize $\\bm{{w}}$ and input distributions\n\\While {not converged} \n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})\\colon n_{{\\mathsf{S}},{\\mathsf{C}}} \\leftarrow 0$\n\\State $\\forall X \\in {\\ensuremath{\\mathbf{X}}}$, $\\forall {\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X \\colon \\theta_{{\\mathsf{D}}_X} \\leftarrow 0$, $n_{{\\mathsf{D}}_X} \\leftarrow 0$\n\\For{${\\ensuremath{l}} = 1 \\dots {\\ensuremath{L}}$}\n\\State Input $\\mathbf{e}^{(l)}$ to ${\\ensuremath{\\mathcal{S}}}$\n\\State Evaluate ${\\ensuremath{\\mathcal{S}}}$ (upward-pass)\n\\State Backprop ${\\ensuremath{\\mathcal{S}}}$ (backward-pass)\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})$}\n\\State  $n_{{\\mathsf{S}},{\\mathsf{C}}} \\leftarrow n_{{\\mathsf{S}},{\\mathsf{C}}} + \n\\frac{1}{{\\ensuremath{\\mathcal{S}}}} \\,\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}}{\\partial {\\mathsf{S}}} \\, \n{\\mathsf{C}}  \\, \n{w}_{{\\mathsf{S}}, {\\mathsf{C}}}$\n\\EndFor\n\\For{$X \\in {\\ensuremath{\\mathbf{X}}}$, ${\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X$}\n\\If {${\\ensuremath{\\mathbf{e}}}^{(l)}$ is complete w.r.t.~$X$}\n\\State $x \\leftarrow$ complete evidence for $X$\n\\State $\\theta \\leftarrow \\theta(x)$\n\\Else\n\\State ${\\mathcal{X}} \\leftarrow $ partial evidence for $X$\n\\State $\\theta \\leftarrow \\frac{\\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\theta(x) \\mathrm{d}x}{\\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\mathrm{d}x}$\n\\EndIf\n\\State $p \\leftarrow \\frac{1}{{\\ensuremath{\\mathcal{S}}}}\\frac{\\partial {\\ensuremath{\\mathcal{S}}}}{\\partial {\\mathsf{D}}_X} {\\mathsf{D}}_X$\n\\State $\\theta_{{\\mathsf{D}}_X} \\leftarrow \\theta_{{\\mathsf{D}}_X} + p\\,\\theta$ \n\\State $n_{{\\mathsf{D}}_X} \\leftarrow n_{{\\mathsf{D}}_X} + p$ \n\\EndFor\n\\EndFor\n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}) \\colon$ \n${w}_{{\\mathsf{S}}, {\\mathsf{C}}} \\leftarrow \\frac{n_{{\\mathsf{S}}, {\\mathsf{C}}}}{\\sum_{{\\mathsf{C}}' \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} n_{{\\mathsf{S}}, {\\mathsf{C}}'}}$\n\\State $\\forall X \\in {\\ensuremath{\\mathbf{X}}}, \\forall {\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X \\colon$ \nset parameters to $\\frac{\\theta_{{\\mathsf{D}}_X}}{n_{{\\mathsf{D}}_X}}$\n\\EndWhile \n\\State \\Return ${\\ensuremath{\\mathcal{S}}}$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for EM algorithm in SPNs.}\n\\label{algo:EM}\n\\end{figure}\n\n\n\n\n\n\\section{Most Probable Explanation}\n\\label{sec:MPE}\nIn \\cite{Poon2011,Peharz2013,Peharz2014}, SPNs were applied for reconstructing data using MPE inference.\nGiven some distribution ${p}$ over ${\\ensuremath{\\mathbf{X}}}$ and evidence ${\\ensuremath{\\mathbf{e}}}$, MPE can be formalized as finding\n$\\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\ensuremath{\\mathbf{e}}}}{\\arg\\max} ~ {p}({\\ensuremath{\\mathbf{x}}}),$\nwhere we assume here that ${p}$ actually has a maximum in ${\\ensuremath{\\mathbf{e}}}$.\nMPE is a special case of MAP, defined as finding\n$ \\underset{{\\ensuremath{\\mathbf{y}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}}}{\\arg\\max} ~ \\int_{{{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{Z}}}}]}}} {p}({\\ensuremath{\\mathbf{y}}},{\\ensuremath{\\mathbf{z}}}) \\, \\mathrm{d}{\\ensuremath{\\mathbf{z}}},$\nfor some two-partition of ${\\ensuremath{\\mathbf{X}}}$, i.e.~${\\ensuremath{\\mathbf{X}}} ={\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}, {\\ensuremath{\\mathbf{Y}}} \\cap {\\ensuremath{\\mathbf{Z}}} = \\emptyset$.\nBoth MPE and MAP are generally NP-hard in BNs \\cite{Bodlaender2002, Park2002, Park2004, Kwisthout2011}, and MAP is inherently harder than MPE\n\\cite{Park2004, Kwisthout2011}. \nUsing the result in \\cite{deCampos2011}, it can be shown that MPE inference (and thus MAP inference) is NP-hard also in SPNs. \nIn particular, Theorem 5 in \\cite{deCampos2011} shows that the decision version of MAP is NP-complete for a naive Bayes structure, when the class variable is marginalized. \nNaive Bayes can be represented by the augmentation of an SPN with a single sum node. \nThus, MAP in augmented SPNs with the LVs marginalized is NP-hard. \nEquivalently, MPE is generally NP-hard in SPNs. \nA similar proof taylored to SPNs can be found in \\cite{Peharz2015b}.\n\nWhile MPE is generally NP-hard in SPNs, it is stated in \\cite{Poon2011} that MPE over the model RVs ${\\ensuremath{\\mathbf{X}}}$ and the LVs ${\\ensuremath{\\mathbf{Z}}}$ can be\nefficiently solved using a Viterbi-style algorithm. \nHowever, it was never proved that this algorithm yields an MPE solution. \nHere we show that this algorithm indeed is correct, when applied to \\emph{augmented} SPNs.\n\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$. \nWe define a \\emph{max-product network} (MPN) $\\hat {\\ensuremath{\\mathcal{S}}}$, by replacing each distribution node ${\\mathsf{D}}$ by a \\emph{maximizing} distribution node\n\n", "itemtype": "equation", "pos": 61068, "prevtext": "\nwhere $k=[{\\mathsf{D}}_X]$ and ${\\mathsf{P}}$ is the gate of ${\\mathsf{D}}_X$, cf.~\\eqref{eq:introduceGate}.\nIf we do not want to construct the gated SPN explicitly, we can use the identity\n$\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}^g({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{P}}}\n=\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{D}}_X}\n$.\nThus the required posteriors are given as\n\n", "index": 47, "text": "\\begin{equation}\n{\\ensuremath{\\mathcal{S}}}^g(W_X = k {\\,|\\,} {\\ensuremath{\\mathbf{e}}}^{(l)}) = \n\\frac{1}{{\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{e}}}^{(l)})}{\\partial {\\mathsf{D}}_X} {\\mathsf{D}}_X({\\ensuremath{\\mathbf{e}}}^{(l)}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}^{g}(W_{X}=k{\\,|\\,}{\\mathbf{e}}^{(l)})=\\frac{1}{{\\mathcal{S}}({%&#10;\\mathbf{e}}^{(l)})}\\frac{\\partial{\\mathcal{S}}({\\mathbf{e}}^{(l)})}{\\partial{%&#10;\\mathsf{D}}_{X}}{\\mathsf{D}}_{X}({\\mathbf{e}}^{(l)}).\" display=\"block\"><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>g</mi></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>X</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mi>k</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udda3</mi><mi>X</mi></msub></mrow></mfrac><msub><mi>\ud835\udda3</mi><mi>X</mi></msub><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc1e</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nand each sum node ${\\mathsf{S}}$ by a \\emph{max node} $\\hat {\\mathsf{S}} := \\underset{\\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})}{\\max} {w}_{\\hat {\\mathsf{S}}, \\hat {\\mathsf{C}}} \\hat {\\mathsf{C}}$.\nA product node in ${\\ensuremath{\\mathcal{S}}}$ corresponds to a product node also in $\\hat {\\ensuremath{\\mathcal{S}}}$.\n\n\n\n\\begin{theorem}\n\\label{theo:MPEaug}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an \\emph{augmented} SPN over ${\\ensuremath{\\mathbf{X}}}$, where ${\\ensuremath{\\mathbf{X}}}$ already comprises the model RVs of the original SPN and LVs introduced by augmentation, and let $\\hat{{\\ensuremath{\\mathcal{S}}}}$ the corresponding MPN.\nLet ${\\mathsf{N}}$ be some node in ${\\ensuremath{\\mathcal{S}}}$ and $\\hat{{\\mathsf{N}}}$ its corresponding node in $\\hat{{\\ensuremath{\\mathcal{S}}}}$.\nThen, for every ${\\bm{{\\mathcal{X}}}} \\in {\\ensuremath{\\mathcal{H}}}_{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}$ we have $\\hat{{\\mathsf{N}}}({\\bm{{\\mathcal{X}}}}) = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}}{\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$.\n\\end{theorem}\n\n\n\nTheorem \\ref{theo:MPEaug} shows that the MPN maximizes the probability in the augmented SPN over ${\\bm{{\\mathcal{X}}}}$. \nThe proof (see appendix) also shows how to actually \\emph{find} a maximizing assignment in an augmented SPN. \nA product is maximized by independently maximizing each of its children, and a (twin) sum node is maximized by maximizing the weighted\nmaxima of its children. \nTherefore, the Viterbi-style algorithm proposed in \\cite{Poon2011} is indeed a correct algorithm, when applied to \\emph{augmented} SPNs. \nThis algorithm, denoted as \\textproc{MPEAugmented}, is shown in Fig.~\\ref{algo:MPEaugmented}. \nHere $Q$ denotes a queue of nodes, where $Q \\curvearrowleft {\\mathsf{N}}$ and ${\\mathsf{N}} \\curvearrowleft Q$ denote the en-queue and de-queue operations, respectively.\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{MPEAugmented}{${\\ensuremath{\\mathcal{S}}}, {\\ensuremath{\\mathbf{e}}}$}\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{x}}}^*$ of length $|{\\ensuremath{\\mathbf{X}}}|$\n\n\\State Evaluate ${\\ensuremath{\\mathbf{e}}}$ in corresponding MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ (upwards pass)\n\\State $Q \\curvearrowleft $ root node of MPN\n\\While {$Q$ not empty} \n\\State $\\hat {\\mathsf{N}} \\curvearrowleft Q$\n\\If {$\\hat {\\mathsf{N}}$ is a max node}\n\\State $Q \\curvearrowleft \\underset{\\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}})}{\\arg\\max} ~ \\left \\{{w}_{\\hat{\\mathsf{N}},\\hat{\\mathsf{C}}} \\, \\hat{\\mathsf{C}} \\right \\}$\n\\ElsIf {$\\hat {\\mathsf{N}}$ is a product node}\n\\State $\\forall \\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}}): Q \\curvearrowleft \\hat {\\mathsf{C}}$\n\\ElsIf {$\\hat {\\mathsf{N}}$ is a maximizing distribution node}\n\\State ${\\mathsf{N}} \\leftarrow$ corresponding distribution node\n\\State ${{{\\ensuremath{\\mathbf{x}}}^*}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}} = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}}}{\\arg\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$\n\\EndIf\n\\EndWhile \n\\State \\Return ${\\ensuremath{\\mathbf{x}}}^*$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for MPE inference in augmented SPNs.}\n\\label{algo:MPEaugmented}\n\\end{figure}\n\n\n\n\nIn \\cite{Poon2011}, however, \\textproc{MPEAugmented} was applied to \\emph{original} SPNs, not to \\emph{augmented} SPNs.\nDoes this still correspond to an MPE solution? \nAs we will see, this is indeed the case, but implicitly deterministic weights for the twin sum nodes are used in the \ncorresponding augmented SPN, i.e. weights which are all 0, except a single 1. \nAlthough this choice of twin-weights is allowed and the marginal distribution over model RVs is still the distribution\nof the original SPN, the use of deterministic twin-weights significantly influences the behavior of MPE inference.\n\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{MPEOriginal}{${\\ensuremath{\\mathcal{S}}}, {\\ensuremath{\\mathbf{e}}}$}\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{x}}}^*$ of length $|{\\ensuremath{\\mathbf{X}}}|$\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{z}}}^*$ of length $|{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})|$\n\n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}): \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}): ~ \\tilde{{w}}_{{\\mathsf{S}}, {\\mathsf{C}}} \\leftarrow 1$\n\\label{as:MPEinference_prepstart}\n\\For {${\\mathsf{S}} \\in {\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$, ${\\mathsf{S}}^c \\in {\\bm{\\mathsf{S}}}^c({\\mathsf{S}})$}\n\\For {${\\mathsf{C}} \\in \\{ {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}^c) {\\ensuremath{\\,|\\,}} {\\mathsf{S}} \\not \\in {\\ensuremath{\\mathbf{desc}}}({\\mathsf{C}}) \\}$}\n\\State $\\tilde{{w}}_{{\\mathsf{S}}^c, {\\mathsf{C}}} \\leftarrow \\tilde{{w}}_{{\\mathsf{S}}^c,{\\mathsf{C}}} \\times \n\\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\mathsf{S}}}]}}}{\\max} ~ \\bar{{w}}_{{\\mathsf{S}}, {\\mathsf{C}}^{k}_{\\mathsf{S}}}$\n\\EndFor\n\\EndFor\n\\label{as:MPEinference_prepend}\n\\State Equip MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ with weights\n${w}_{\\hat{\\mathsf{S}},\\hat{\\mathsf{C}}} \\leftarrow {\\tilde{w}}_{{\\mathsf{S}},{\\mathsf{C}}} \\times {{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$\n\\State Evaluate ${\\ensuremath{\\mathbf{e}}}$ in MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ (upwards pass)\n \\State $\\bm{\\hat {\\mathsf{S}}} \\leftarrow$ set of all max nodes\n\\label{as:MPEinference_mainstart}\n \\State $Q \\curvearrowleft $ root node of MPN\n \\While {$Q$ not empty} \n \\State $\\hat{\\mathsf{N}} \\curvearrowleft Q$\n \\If {$\\hat{\\mathsf{N}}$ is a max node}\n \\State ${{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{N}}}}]}} \\leftarrow \\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\hat{\\mathsf{N}}}}]}}}{\\arg\\max} ~ \n\\left \\{{w}_{\\hat{\\mathsf{N}}, \\hat{\\mathsf{C}}^k_{\\hat{\\mathsf{N}}}} \\, \\hat{\\mathsf{C}}^k_{\\hat{\\mathsf{N}}} \\right \\}$\n\\label{as:MPEinference_diff1}\n\\State $Q \\leftarrow {\\mathsf{C}}^{{{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{N}}}}]}}}_{\\hat{\\mathsf{N}}}$\n\\label{as:MPEinference_diff2}\n \\State $\\bm{\\hat{\\mathsf{S}}} \\leftarrow \\bm{\\hat{\\mathsf{S}}} \\setminus \\{\\hat{\\mathsf{N}}\\}$\n\\label{as:MPEinference_diff3}\n \\ElsIf {$\\hat {\\mathsf{N}}$ is a product node}\n \\State $\\forall \\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}}): Q \\curvearrowleft \\hat{\\mathsf{C}}$\n \\ElsIf {$\\hat {\\mathsf{N}}$ is a distribution node}\n\\State ${\\mathsf{N}} \\leftarrow$ corresponding distribution node\n\\State ${{{\\ensuremath{\\mathbf{x}}}^*}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}} = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}}}{\\arg\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$\n\n \\EndIf\n \\EndWhile \n\\label{as:MPEinference_mainend}\n\\For {$\\hat {\\mathsf{S}} \\in \\bm{\\hat{\\mathsf{S}}}$}\n\\label{as:MPEinference_assignreststart}\n\\State ${{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{S}}}}]}} = \\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\hat{\\mathsf{S}}}}]}}}{\\arg\\max} ~ \\bar{{w}}_{\\hat{\\mathsf{S}}, {\\mathsf{C}}^{k}_{\\hat{\\mathsf{S}}}}$\n\\EndFor\n\\label{as:MPEinference_assignrestend}\n\\State \\Return ${\\ensuremath{\\mathbf{x}}}^*, {\\ensuremath{\\mathbf{z}}}^*$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for MPE inference in augmented SPNs, performed using the original SPN structure.}\n\\label{algo:MPE}\n\\end{figure}\n\n\n\nTo see this, let us modify \\textproc{MPEAugmented}, such that it can be applied to an original SPN, \nbut returning an MPE solution for the corresponding augmented SPN. \n\\textproc{MPEOriginal} shown in Fig.~\\ref{algo:MPE} is such a modification, simulating the additional \nstructure introduced by augmentation. \nIn steps \\ref{as:MPEinference_prepstart}-\u00e2\u0080\u0093\\ref{as:MPEinference_prepend} we first construct the correction weights $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}}$. \nFor a particular sum ${\\mathsf{S}}$ and child ${\\mathsf{C}}$,  $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}}$ is the product over the maxima of all twin\nsum nodes which would be connected to the link of ${\\mathsf{C}}$ in\nthe augmented SPN. \nBy equipping the corresponding MPN with weights \n${w}_{\\hat{\\mathsf{S}},\\hat{\\mathsf{C}}} \\leftarrow {\\tilde{w}}_{{\\mathsf{S}},{\\mathsf{C}}} \\times {{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$, \nthe effect of the twin sum nodes is simulated.\n\n\nSince the LVs corresponding to the sum nodes are not explicitly introduced, we need to assign their MPE state\n\u00e2\u0080\u009cby hand\u00e2\u0080\u009d. \nIn step \\ref{as:MPEinference_diff1}, the maximizing state of an LV corresponding to a ``visited'' sum ${\\mathsf{S}}$ is assigned. \nIn steps \\ref{as:MPEinference_assignreststart}-\u00e2\u0080\u0093\\ref{as:MPEinference_assignrestend}, the maximizing states of those LVs are assigned, \nwhich were not visited during back-tracking.\n\n\n\n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=0.45\\textwidth]{lowdepth}\n \\caption{Illustration of the low-depth bias using an SPN over RVs $\\{X_1,X_2,X_3\\}$. The structure introduced by augmentation is depicted by small nodes and edges. When deterministic twin-weights are used, the state of $Z_{{\\mathsf{S}}^1}$ corresponding to ${\\mathsf{P}}^1$ is preferred over ${\\mathsf{P}}^2$ and ${\\mathsf{P}}^3$, since their probabilities are ``dampened'' by the weights of ${\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$.}\n\\label{fig:problemMPE}\n\\end{figure}\n\n\n\n\\textproc{MPEOriginal} is essentially equivalent to \\textproc{MPEAugmented}\nwhen all $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}} = 1$, which is only the\ncase when all twin-weights are \\emph{deterministic}, i.e. for each sum\nnode, one twin-weight is 1 and all others are 0. \nWhen deterministic twin-weights are used, we refer to \\textproc{MPEOriginal}\nas \\textproc{MPEDet}, and when uniform twin-weights are used, we refer to it as\n \\textproc{MPEUni}.\nUsing deterministic twin-weights is a rather unnatural choice, since this\nprefers one arbitrary state over the others in cases where this LV is actually ``rendered irrelevant''. \n\n\\textproc{MPEDet} also has a bias towards less structured sub-models, which we call \\emph{low-depth bias}.\nThis is illustrated in Fig.~\\ref{fig:problemMPE}, which shows an SPN over\nthree RVs $X_1, X_2, X_3$. \nThe augmented SPN has two twin sum nodes, corresponding to ${\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$. \nWhen their twin-weights are deterministic, the selection of the state of $Z_{{\\mathsf{S}}^1}$\nis \\emph{biased} towards the state corresponding to ${\\mathsf{P}}^1$, which is a distribution assuming \nindependence among $X_1$, $X_2$ and $X_3$. \nThis comes from the fact, that the values of ${\\mathsf{P}}^2$ and ${\\mathsf{P}}^3$ are dampened by the weights of \n${\\mathsf{S}}_2$ and ${\\mathsf{S}}_3$, respectively, which are generally smaller than 1. \nTherefore, when using deterministic weights for twin sum nodes, we introduce a bias towards the \nselection of sub-SPNs that are less deep and less structured. \nUsing uniform weights for twin sum nodes is ``fairer'', \nsince in this case \n${\\mathsf{P}}^1$ gets dampened by $\\bar{\\mathsf{S}}^2$ and $\\bar{\\mathsf{S}}^3$,\n${\\mathsf{P}}^2$ by ${\\mathsf{S}}^2$ and $\\bar{\\mathsf{S}}^3$, and \n${\\mathsf{P}}^3$ by $\\bar{\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$. \nUniform weights are actually the opposite choice to deterministic twin-weights: the\nformer represent the strongest dampening via twin-weights\nand therefore actually \\emph{penalize} less structured distributions.\n\nMPE inference in LV models can be used for hard EM,\ni.e.~to treat LVs as observed using the MPE solution and\nusing discrete winner-take-all counts for EM updates. \nNote that in \\cite{Poon2011} hard EM was used, but it was noted that \n\\emph{\n``The best results were obtained using sums on the upward pass and\nmaxes on the downward pass (i.e.,~the MPE value of each hidden\nvariable is computed conditioning on the MPE values of the\nhidden variables above it and summing out the ones below).''\n}\nUsing summation in the upwards pass and maximization\nin the downward pass does generally \\emph{not} deliver an MPE\nsolution. \nIt is, however, an arguable approximation to the\nMAP problem, when maximizing over model RVs and summing\nout LVs. \nDue to this behavior, we refer to this method\nas \\textproc{IterativeMap}. \nPresumably, \\textproc{IterativeMap} performed better during learning since \nit does not suffer from the low-depth bias, since the ``damping factor'' for each sum node is 1. \nInvestigating this effect further is subject to future work.\n\n\n\n\\section{Experiments}\n\n\\subsection{Experiments with EM Algorithm}   \\label{sec:experimentsEM}\nIn \\cite{Poon2011, PoonSPNCode} SPNs were applied to image data, where a generic\narchitecture reminiscent to convolutional neural networks was proposed. \nWe refer to this architecture as PD architecture. \nStandard EM was not used in experiments for two reasons: \nFirst, explicitly constructing the proposed structure\nand to train it with standard EM is hardly possible with\ncurrent hardware, since the number of nodes grows $\\mathcal{O}(l^3)$,\nwhere $l$ is the square-length of the modeled image domain in pixels \\cite{Peharz2015b}. \nInstead, a sparse hard EM algorithm was used, which virtualizes the PD structure, i.e.~sum and products\nare generated on the fly (see \\cite{PoonSPNCode} for details). \nSecond, using standard EM seemed unsuited to train large and dense SPNs,\neither because it is trapped in local optima or due to the\ngradient vanishing phenomenon.\n\n\nIn our experiments, we investigated three questions:\n\\begin{enumerate}\n\\item\nIs our derivation of EM correct, both for complete\nand missing data?\n\\item \nCan the result of hard EM  \\cite{Poon2011} be improved by\nstandard EM?\n\\item\nGiven a suited sparse structure, does EM yield a\ngood solution for parameters?\n\\end{enumerate}\nQuestion 1) is important since the original derivation contained\nan error. Questions 2) and 3) are concerned with the\ngeneral applicability of EM for training SPN.\n\n\nWe used the same datasets and SPN structures as used\nin \\cite{Poon2011}, which can be obtained from \\cite{PoonSPNCode}. \nThe used datasets comprise Caltech-101 (inclusive background class) \\cite{FeiFei2004} and\nthe ORL face images \\cite{Samaria1994}, i.e. in total 103 datasets. \nThe input distributions in these models are single-dimensional\nGaussians (4 for each pixel), where the means were set to the\naverage of the 4-quantiles and the variances were constantly set to 1. \nWe ran EM (Fig.~\\ref{algo:EM}) for 30 iterations, with various settings:\n\n\n\n\\begin{itemize}\n\\item \nUpdate any combination of the three different types\nof parameters, i.e. sum-weights, Gaussian means and\nGaussian variances. \nEach set of parameters types is encoded by a string of \nletters W (weights), M (means) and V (variances). \n(7 combinations)\n\\item\nUse original parameters for initialization, obtained\nfrom \\cite{PoonSPNCode}, or use 3 random initialization, where sum-weights\nare drawn from a Dirichlet distribution with uniform $\\alpha=1$ hyper-parameter \n(i.e. uniform distribution on the standard simplex), Gaussian means\nare uniformly drawn from $[-1,1]$ and Gaussian variances from $[0.01,1]$. \nOnly parameters which are actually updated are initialized randomly; otherwise\nthe original parameters \\cite{Poon2011} are used and kept fixed.\n(4 combinations)\n\\item\nUse complete data or simulate missing training data\nby randomly discarding $33\\%$ or $66\\%$ of the models\nRVs, independently for each sample. \n(3 combinations)\n\\end{itemize}\n\n\n\nThus, in total we ran EM $7 \\times 4 \\times 3 \\times 103 = 8652$ times yielding\n$259560$ EM-iterations. \nTo avoid pathological solutions we used a lower bound of $0.01$ for the Gaussian variances. \nIn \\emph{no iteration} we observed a decreasing likelihood on the training \nset, i.e. our derived EM algorithm showed monotonicity in our experiments. \nMoreover, as can be seen in the performance curves in Fig.~\\ref{fig:EMtrain}, the log-likelihood on the training set actually increased over\niterations. \nThe curves for the missing data scenarios are similar. \nThis gives affirmative evidence for question 1).\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[width=0.5 \\textwidth]{EMtrain}\n  \\label{fig:EMtrain}  \n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[width=0.5 \\textwidth]{EMtest}\n   \\label{fig:EMtest}\t\n}\n\\caption{Log-likelihood over EM-iterations, normalized by the number\nof samples and averaged over all 103 datasets and the 3 random\ninitializations.\n\\protect\\subref{fig:EMtrain}: Training set.\n\\protect\\subref{fig:EMtest}: Test set; Curves for V and WV are\noutside the displayed region, for better readability of the other curves.\nThey start at approximately $-8000$ nats and decreased to approximately\n$-11000$ nats. }\n\\label{fig:EMresult}\n\\end{figure}\n\n\n\n\nFig.~\\ref{fig:EMtest} shows the log-likelihood on the test set. \nNote that optimizing the parameter sets V and WV led to severe overfitting: \nwhile achieving extremely high likelihoods on the training set, \nthey achieved extremely poor likelihoods on the test set. \nAlso the parameter sets MV and WMV tend to overfit, although not as strong as V and WV.\n\n\nRegarding question 2), we closer inspected the test loglikelihood\nwhen the original parameters are used for initialization,\ni.e. when the parameters obtained by \\cite{PoonSPNCode} are post-trained\nusing EM. \nTable~\\ref{tab:posttrainEM} summarizes the results. \nWhen parameter sets not including Gaussian variances are optimized\n(i.e.~W, M, and WM), the test log-likelihood increased\nmost of the time, i.e. for $83.5\\%$ (M) to up to $92.23\\%$ (WM) of the datasets.\nFurthermore, having oracle knowledge about the ideal number of iterations \n(i.e. column best), the average log-likelihood increased by $0.58\\%$ (M) to up to\n$1.39\\%$ (WM) relative to the original parameters. \nMost of this improvement happens in the first iteration, yielding $0.52\\%$\n(M) up to $1.05\\%$ (WM) improvement. \nThese results indicate that the parameters obtained by \\cite{PoonSPNCode} slightly underfit the\ngiven datasets. \nSimilar as in Fig.~\\ref{fig:EMresult}, we see that parameter sets including the Gaussian variances \n(V, WV, MV, WMV) are prone to overfitting: more than $60\\%$ of the datasets\ndecreased their test log-likelihood during EM. \nHowever, in the remaining $40\\%$ of the datasets, the test log-likelihood can\nbe improved \\emph{substantially} by at least $14\\%$ on average.\n\n\n\n\\begin{table}\n\\caption{\nChanges in test log-likelihoods when original parameters are\npost-trained using EM. \n\\% inc.: percentage of datasets where log-likelihood increased in the first iteration. \n\\% all, \\% pos., \\% neg.: relative change of log-likelihood, averaged over all datasets, \ndatasets with positive change, datasets with negative change, respectively.\n}\n\\label{tab:posttrainEM}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c| c c c | c c c|}\n\\hline \n &       &  \\multicolumn{3}{|c|}{after 1st iteration}  & \\multicolumn{3}{|c|}{best} \\\\ \n\\hline \n &  \\% inc. &  \\% all. & \\% pos. & \\% neg.  &  \\% all & \\% pos. & \\% neg. \\\\ \n\\hline \nW    &  91.26  &  0.55 & 0.61 & -0.03  &  0.87 & 0.96 & -0.03 \\\\ \nM    &  83.50  &  0.52 & 0.67 & -0.21  &  0.58 & 0.73 & -0.21 \\\\ \nWM   &  92.23  &  1.06 & 1.18 & -0.30  &  1.39 & 1.53 & -0.30 \\\\ \nV    &  39.81  &  -13.47 & 14.44 & -31.93  &  -13.45 & 14.51 & -31.93 \\\\ \nWV   &  39.81  &  -13.41 & 14.79 & -32.06  &  -13.33 & 14.98 & -32.06 \\\\ \nMV   &  38.83  &  -17.24 & 14.27 & -37.25  &  -17.21 & 14.35 & -37.25 \\\\ \nWMV  &  38.83  &  -17.18 & 14.63 & -37.37  &  -17.12 & 14.78 & -37.37 \\\\ \n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\n\n\nWe now turn to question 3). \nAs pointed out above, a hard EM variant was used in \\cite{Poon2011, PoonSPNCode} which at the same time\nfinds the effective SPN structure. \nOptimizing W using the 3 random initialization amounts to using the oracle structure\nobtained by \\cite{Poon2011, PoonSPNCode}, discarding the learned parameters. \nFor each dataset we selected the random initialization which yielded the highest \nlikelihood on the training set in iteration 30. \nFor this run, we compared the log-likelihoods with the log-likelihoods obtained \nby the original parameters. \nThe results are summarized in Table~\\ref{tab:randinitEM}. \n\n\n\n\\begin{table}\n\\caption{\nLog-likelihoods when sum-weights (W) are trained, using random\ninitialization. \n$\\%>$: percentage of data sets, where log-likelihood is larger than for original parameters. \n$\\%$ all, $\\%$ pos., $\\%$ neg.: relative log-likelihood w.r.t.~original parameters, \nfor all data sets, data sets where relative log-likelihood is positive/negative, respectively.\n}\n\\label{tab:randinitEM}\n\\tabcolsep=0.12cm\n\\begin{tabular}{|l | c c c c | c c c c|}\n\\hline \n &      \\multicolumn{4}{|c|}{after 1st iteration}  & \\multicolumn{4}{|c|}{best} \\\\ \n\\hline \n &  \\%$>$ &  \\% all. & \\% pos. & \\% neg.  & \\%$>$ & \\% all & \\% pos. & \\% neg. \\\\ \n\\hline \ntrain & 70.87 & 0.68 & 1.38 & -1.00 & 100.00 & 3.97 & 3.97 & - \\\\ \ntest & 41.75 & -0.11 & 0.40 & -0.48 & 67.96 & 0.46 & 0.76 & -0.18 \\\\ \n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\nWe see that on all data\nsets the log-likelihood on the training set is larger than\nfor the original parameters. This is also the case for each\nindividual random start (not just best one) -- every random\nrestart always yielded a higher training log-likelihood than\nthe original parameters. \nThus, by considering the actual optimization objective \n-- the likelihood on the training set -- EM successfully trains SPNs, \ngiven a suited oracle structure.\nFurthermore, as can be seen in Table~\\ref{tab:randinitEM}, \nEM is also not more prone to overfitting than the algorithm in \\cite{Poon2011}: \non $67.96\\%$ of the datasets, EM delivered a higher test log-likelihood\nthan the original parameters, when using oracle knowledge\nabout the ideal number of iterations (column best).\n\n\n\\subsection{Experiments with MPE Inference}\nTo illustrate correctness of \\textproc{MPEOriginal} (Fig.~\\ref{algo:MPE}), we\ngenerated SPNs using the PD architecture \\cite{Poon2011}, arranging\n$4$, $9$ and $16$ binary RVs in a $2 \\times 2$, $3 \\times 3$ and $4 \\times 4$ grid,\nrespectively. \nAs inputs we used two indicator variables for each RV representing their two states. \nThe sum-weights were drawn from a Dirichlet distribution with uniform $\\alpha$-parameters, \nwhere $\\alpha \\in \\{0.5,1,2\\}$. \nFor all networks we drew $100$ independent parameters sets. \nFor each structure and parameter set we ran \\textproc{IterativeMap}, \n\\textproc{MPEDet} and \\textproc{MPEUni} to obtain an assignment for all RVs. \nFor each assignment the log-likelihoods were evaluated in the augmented SPN with\ndeterministic weights, the augmented SPN with uniform weights and in \nthe original SPN (discarding the states of the LVs). \nAdditionally, we found ground truth MPE assignments using exhaustive enumeration. \nThe results relative to the ground truth MPE solutions are shown in \nTables~\\ref{tab:MPEaugmented}, \\ref{tab:MPEaugmentedDet}, and \\ref{tab:MPEoriginal}. \nAs can be seen, \\textproc{MPEUni} always finds an MPE solution in the augmented SPN \nwith uniform twin-weights and \\textproc{MPEDet} always finds an MPE solution\nin augmented SPNs with deterministic twin-weights. \nThis gives empirical evidence for the correctness of \\textproc{MPEUni}\nfor MPE inference in augmented SPNs. \nFurthermore, we wanted to investigate the quality of the three algorithms\nwhen serving as approximation for MPE inference in the original SPNs. \nAs can be seen in Table~\\ref{tab:MPEoriginal}, all three algorithms\ndeliver arguable approximations. \nFor the SPNs considered here, \\textproc{MPEDet} delivered on average slightly\nbetter approximations than \\textproc{IterativeMap} and \\textproc{MPEUni}.\nHowever, these results should be interpreted with\ncaution, due to the rather similar nature of the distributions\nconsidered here. Closer investigating approximate MPE for\n(original) SPNs is an interesting direction and will be subject\nto future research.\n\n\n\n\\begin{table}\n\\center\n\\caption{\nDifferences of log-likelihood to the ground-truth MPE solution found by\nexhaustive enumeration, averaged over 100 independent draws of\nsum-weights. Numbers in parentheses are the number of times where\nan MPE solution was found. Results for augmented SPNs using\nuniform twin-weights.\n}\n\\label{tab:MPEaugmented}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n       & $\\alpha=0.5$ & 0.00 (100) & -0.04 (82) & 0.00 (100) \\\\ \n4 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.08 (77) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.05 (79) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.10 (70) & -0.44 (37) & 0.00 (100) \\\\ \n9 RVs & $\\alpha=1.0$ & -0.10 (68) & -0.48 (22) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & -0.11 (62) & -0.53 (19) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.63 (19) & -1.16 (10) & 0.00 (100) \\\\ \n16 RVs & $\\alpha=1.0$ & -0.85 (12) & -1.25 (8) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & -0.82 (12) & -1.25 (0) & 0.00 (100) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\\begin{table}\n\\center\n\\caption{\nSimilar as in Table~\\ref{tab:MPEaugmented}. Results for augmented SPNs using deterministic\ntwin-weights.\n}\n\\label{tab:MPEaugmentedDet}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n        & $\\alpha=0.5$ & 0.00 (100) & -0.04 (82) & 0.00 (100) \\\\ \n4 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.08 (77) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.05 (79) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & 0.00 (100) & -0.43 (36) & -0.10 (70) \\\\ \n9 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.46 (25) & -0.12 (68) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.57 (20) & -0.15 (62) \\\\ \n\\hline\n       & $\\alpha=0.5$ & 0.00 (100) & -1.19 (5) & -0.89 (19) \\\\ \n16 RVs & $\\alpha=1.0$ & 0.00 (100) & -1.36 (8) & -1.11 (12) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -1.47 (2) & -1.01 (12) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\\begin{table}\n\\center\n\\caption{\nSimilar as in Table~\\ref{tab:MPEaugmented}. Results for original SPNs.\n}\n\\label{tab:MPEoriginal}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n      & $\\alpha=0.5$ & -0.06 (72) & -0.09 (63) & -0.06 (72) \\\\ \n4 RVs & $\\alpha=1.0$ & -0.09 (59) & -0.14 (51) & -0.09 (59) \\\\ \n       & $\\alpha=2.0$ & -0.10 (52) & -0.15 (44) & -0.10 (52) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.31 (32) & -0.68 (13) & -0.38 (27) \\\\ \n9 RVs & $\\alpha=1.0$ & -0.47 (12) & -0.67 (5) & -0.48 (12) \\\\ \n       & $\\alpha=2.0$ & -0.40 (6) & -0.61 (2) & -0.37 (7) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.76 (5) & -1.42 (5) & -1.04 (4) \\\\ \n16 RVs & $\\alpha=1.0$ & -0.76 (3) & -1.29 (1) & -1.18 (2) \\\\ \n       & $\\alpha=2.0$ & -0.67 (1) & -1.07 (0) & -0.92 (0) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\n\\section{Conclusion}\nIn this paper we revisited the interpretation of SPNs as\nhierarchically structured LV model. \nWe pointed out that the original approach to explicitly incorporate LVs does not\nproduce a sound probabilistic model. \nAs a remedy we proposed the augmentation of SPNs and proved its soundness as LV model.\n\nWithin augmented SPNs, we investigated the independency\nstructure represented as BN, and showed that the sum-weights\ncan be interpreted as structured CPTs within this BN. \nUsing augmented SPNs, we derived the EM algorithm for sum-weights\nand single-dimensional input distributions from\nexponential families. \nWhile MPE-inference is generally NP-hard in SPNs, we showed that the Viterbi-style algorithm proposed in\n \\cite{Poon2011} indeed recovers an MPE solution in augmented SPNs.\nIn experiments we give empirical evidence supporting our theoretical results. \nWe furthermore showed that standard EM can successfully train generative SPNs, given a suitable\nnetwork structure.\n\n\n\n\n\n\n\\appendices\n\\section{Proofs}\n\n\n\\subsection{Proof of Proposition \\ref{prop:augmentationSound}}\nIf ${\\ensuremath{\\mathcal{S}}}'$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$, then ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$ is immediate: \nComputing ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{x}}})$ for any ${\\ensuremath{\\mathbf{x}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$ is done by marginalizing ${\\ensuremath{\\mathbf{Z}}}$, i.e.~setting all ${\\lambda_{{Z_{\\mathsf{S}}}={k}}} = 1$.\nIn this case, it is easy to see that none of the structural changes modifies the output of the SPN, i.e.~the outputs of ${\\ensuremath{\\mathcal{S}}}$ and ${\\ensuremath{\\mathcal{S}}}'$ agree for each ${\\ensuremath{\\mathbf{x}}}$, i.e.~${\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$.\n\n\nIt remains to show that ${\\ensuremath{\\mathcal{S}}}'$ is complete and decomposable, and that the root's scope is ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$.\nSteps \\ref{as:introLinksStart}--\\ref{as:introLinksEnd} in \\textproc{AugmentSPN} introduce the links, representing \"private copies\" of the sum's children, and clearly leave the SPN complete and decomposable.\nIn steps \\ref{as:introIVStart}--\\ref{as:introIVEnd} the LV $Z_{\\mathsf{S}}$ is introduced in the scope of ${\\mathsf{S}}$ and thus in the scope of the root.\nSince this is done for all sum nodes, all ${\\ensuremath{\\mathbf{Z}}}$ are introduced in the root's scope.\nSteps \\ref{as:introIVStart}--\\ref{as:introIVEnd} cannot render products non-decomposable, since this would imply that ${\\mathsf{S}}$ is reachable by two distinct children of this product -- a contradiction that the SPN was decomposable before.\nHowever, as shown in Fig.~\\ref{fig:problemAugmentSPNnaiv}, steps \\ref{as:introIVStart}--\\ref{as:introIVEnd} can render ancestor sums incomplete.\nThese are treated in steps \\ref{as:augmentRemedyStart}--\\ref{as:augmentRemedyEnd}.\nThe twin sum $\\bar{{\\mathsf{S}}}$, if introduced, is clearly complete and has scope $\\{Z\\}$.\nFurthermore, incompleteness of any conditioning sum ${\\mathsf{S}}^c$ can only be caused by links not having $Z_{\\mathsf{S}}$ in their scope.\nThe scope of these links is augmented by $Z_{\\mathsf{S}}$ in step \\ref{as:connectTwin}.\nThese links remain decomposable and moreover, ${\\mathsf{S}}^c$ is rendered complete now.\n\n\n\n\\hfill $\\qed$\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:confspn}}\n\\textbf{ad \\emph{1.)}}\nWhen deleting the IVs and their links, the scopes of any (twin) sum remains the same, since it is complete and is left with one child. \nThus also the scope of any ancestor remains the same. \\\\\n\\textbf{ad \\emph{2.)}}\nThe graph of ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is rooted and acyclic, since the root cannot be a link and deleting nodes and edges cannot introduce cycles.\nWhen an IV ${\\lambda_{{Y}={y}}}$ is deleted, also the link ${\\mathsf{P}}_{{\\mathsf{S}}_Y}^{y}$ is deleted, so no internal nodes are left as leaves.\nThe roots in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ and ${\\ensuremath{\\mathcal{S}}}'$ are the same, and by point \\emph{1.}, ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}$ is the scope of the root.\n${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is also complete and decomposable:\nWhenever an IV and its link are deleted, the corresponding sum node and twin sum node remain trivially complete, since they are left with a single child.\nThe link of this single child is clearly decomposable.\nFurthermore, completeness and decomposability of any ancestor of ${\\mathsf{S}}_Y$ or $\\bar {\\mathsf{S}}_Y$ is left intact, since neither ${\\mathsf{S}}_Y$ nor $\\bar {\\mathsf{S}}_Y$ changes its scope. \\\\\n\\textbf{ad \\emph{3.)}}\nAccording to point \\emph{1.}, the scope of ${\\mathsf{N}}$ is the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$.\nSince ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cap {\\ensuremath{\\mathbf{Y}}} = \\emptyset$, the disconnected IVs and deleted links are no descendants of ${\\mathsf{N}}$, i.e.~no descendants of ${\\mathsf{N}}$ are disconnected during configuration.\nSince ${\\mathsf{N}}$ is present in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$, it must still be reachable from the root.\nTherefore also all descendants of ${\\mathsf{N}}$ are reachable, i.e. ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}_{\\mathsf{N}} = {\\ensuremath{\\mathcal{S}}}'_{\\mathsf{N}}$. \\\\\n\\textbf{ad \\emph{4.)}}\nWhen the input is fixed to ${\\ensuremath{\\mathbf{x}}}, {\\ensuremath{\\mathbf{z}}}, {\\ensuremath{\\mathbf{y}}}$, all IVs and links which are deleted from the configured SPN ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ evaluate to zero in the augmented SPN ${\\ensuremath{\\mathcal{S}}}'$.\nThe outputs of all sums and twin sums are therefore the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$. \nTherefore, also the output of all other nodes remains the same.\nThis includes the root and therefore ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}},{\\ensuremath{\\mathbf{y}}}) = {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}}, {\\ensuremath{\\mathbf{y}}})$, for any ${\\ensuremath{\\mathbf{x}}}, {\\ensuremath{\\mathbf{z}}}$.\n\nWhen ${\\ensuremath{\\mathbf{y}}}' \\not= {\\ensuremath{\\mathbf{y}}}$, then there must be a $Y \\in {\\ensuremath{\\mathbf{Y}}}$ such that the IV ${\\lambda_{{Y}={{{{\\ensuremath{\\mathbf{y}}}'}{[{Y}]}}}}}$ has been deleted, i.e.~${\\lambda_{{Y}={{{{\\ensuremath{\\mathbf{y}}}'}{[{Y}]}}}}} \\notin {\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$, where ${\\mathsf{N}}$ is the root of ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$.\nUsing Lemma~1 in \\cite{Peharz2015}, it follows that ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}},{\\ensuremath{\\mathbf{y}}}') = 0$.\n \\hfill $\\qed$\n\n\n\n\\subsection{Proof of Lemma \\ref{lem:singlepath}}\n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ must contain either ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$, since $Z_{\\mathsf{S}}$ is in the scope of the root by Proposition~\\ref{prop:confspn}.\nTo show that \\emph{not both} are in ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$, let $\\bm{\\Pi}_k$ denote the set of paths of length $k$ from the root to any node ${\\mathsf{N}}$ with $Z_{\\mathsf{S}} \\in {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})$.\nFor $k > 1$, all paths in $\\bm{\\Pi}_k$ can be constructed by extending all paths in $\\bm{\\Pi}_{k-1}$ using all children which have $Z_{\\mathsf{S}}$ in their scopes.\nLet $K$ be the smallest number such that there is a path in $\\bm{\\Pi}_k$ containing ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$.\n\nWe show by induction, that $|\\bm{\\Pi}_k| = 1$, $k=1,\\dots,K$.\nNote that $\\bm{\\Pi}_1$ contains a single path $({\\mathsf{N}})$, where ${\\mathsf{N}}$ is the root, therefore the induction basis holds.\n\nFor the induction step, we show that given $|\\bm{\\Pi}_{k-1}| = 1$, then also $|\\bm{\\Pi}_{k}| = 1$.\nLet $({\\mathsf{N}}_1, \\dots, {\\mathsf{N}}_{k-1})$ be the single path in $\\bm{\\Pi}_{k-1}$.\nIf ${\\mathsf{N}}_{k-1}$ is a product node, then it has a single child ${\\mathsf{C}}$ with $Z_{\\mathsf{S}} \\in {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}})$, due to decomposability.\nIf ${\\mathsf{N}}_{k-1}$ is a sum node, then it must be in ${\\ensuremath{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}}) \\setminus \\{{\\mathsf{S}}\\}$, and therefore has a single child in the configured SPN. \nTherefore, there is a single way to extend the path and therefore $|\\bm{\\Pi}_k|=1, k=1,\\dots,K$.\nThis single path does either lead to ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$.\nSince ${\\mathsf{S}} \\notin {\\ensuremath{\\mathbf{desc}}}(\\bar{{\\mathsf{S}}})$ and $\\bar{{\\mathsf{S}}} \\notin {\\ensuremath{\\mathbf{desc}}}({{\\mathsf{S}}})$, ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains a single path to one of them, but not to both.\n \\hfill $\\qed$\n\n\n\n\\subsection{Proof of Theorem \\ref{theo:conditionalIndependence}}\nBy Lemma~\\ref{lem:singlepath}, for each ${\\ensuremath{\\mathbf{z}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ the configured SPN \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains either ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$, but not both.\nLet ${\\bm{{\\mathcal{Z}}}}$ be the subset of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ such that ${\\mathsf{S}}$ is in ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ \nand $\\bar {\\bm{{\\mathcal{Z}}}}$ be the subset of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ such that $\\bar{{\\mathsf{S}}}$ is in \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\n\n\nFix $Z_{\\mathsf{S}} = k$ and ${\\ensuremath{\\mathbf{z}}} \\in \\bm{\\mathcal{Z}}$.\nWe want to compute ${\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}})$, i.e.~we marginalize ${\\ensuremath{\\mathbf{Y}}}_c$.\nAccording to Proposition~\\ref{prop:confspn} (\\emph{4.}), this equals \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}})$.\nAccording to Proposition~\\ref{prop:confspn} (\\emph{3.}), the sub-SPN rooted at \nformer child ${\\mathsf{C}}^k_{\\mathsf{S}}$ is the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\nSince ${\\ensuremath{\\mathcal{S}}}'$ is locally normalized, this sub-SPN is also locally normalized in \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\nSince the scope of the former child ${\\mathsf{C}}^k_{\\mathsf{S}}$ is a sub-set of ${\\ensuremath{\\mathbf{Y}}}_c$, \nwhich is marginalized, and ${\\lambda_{{Z_{\\mathsf{S}}}={k}}} = 1$, the link  ${\\mathsf{P}}^{k}_{{\\mathsf{S}}}$ \noutputs $1$.\nSince ${\\lambda_{{Z_{\\mathsf{S}}}={k'}}} = 0$ for $k' \\not= k$, the sum ${\\mathsf{S}}$ outputs ${w}_k$.\n\n\nNow consider the set of nodes in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{z}}}$ which have $Z_{\\mathsf{S}}$ in their scope, \nnot including ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ and ${\\mathsf{P}}^k_{\\mathsf{S}}$.\nClearly, since $\\bar{{\\mathsf{S}}}$ is not in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{z}}}$, this set must be \n${\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$.\nLet ${\\mathsf{N}}_1,\\dots,{\\mathsf{N}}_L$ be a topologically ordered list of \n${\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$, where ${\\mathsf{S}}$ is ${\\mathsf{N}}_1$ and ${\\mathsf{N}}_L$ is the root.\nLet ${\\ensuremath{\\mathbf{Y}}}_{n,l} := {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}_l) \\cap {\\ensuremath{\\mathbf{Y}}}_n$ and \n${\\ensuremath{\\mathbf{Z}}}_l := {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}_l) \\cap {\\ensuremath{\\mathbf{Z}}}_p$.\nWe show by induction that for $l=1,\\dots,L$, we have\n\n", "itemtype": "equation", "pos": 66719, "prevtext": "\nThe EM algorithm for SPNs, both for sum-weights and input distributions, is summarized in Fig.~\\ref{algo:EM}.\nIn Section \\ref{sec:experimentsEM} we empirically verify our derivation of EM and show that standard EM successfully \ntrains SPNs when a suitable structure is at hand.\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{Expectation-Maximization}{{\\ensuremath{\\mathcal{S}}}}\n\\State Initialize $\\bm{{w}}$ and input distributions\n\\While {not converged} \n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})\\colon n_{{\\mathsf{S}},{\\mathsf{C}}} \\leftarrow 0$\n\\State $\\forall X \\in {\\ensuremath{\\mathbf{X}}}$, $\\forall {\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X \\colon \\theta_{{\\mathsf{D}}_X} \\leftarrow 0$, $n_{{\\mathsf{D}}_X} \\leftarrow 0$\n\\For{${\\ensuremath{l}} = 1 \\dots {\\ensuremath{L}}$}\n\\State Input $\\mathbf{e}^{(l)}$ to ${\\ensuremath{\\mathcal{S}}}$\n\\State Evaluate ${\\ensuremath{\\mathcal{S}}}$ (upward-pass)\n\\State Backprop ${\\ensuremath{\\mathcal{S}}}$ (backward-pass)\n\\For{${\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})$}\n\\State  $n_{{\\mathsf{S}},{\\mathsf{C}}} \\leftarrow n_{{\\mathsf{S}},{\\mathsf{C}}} + \n\\frac{1}{{\\ensuremath{\\mathcal{S}}}} \\,\n\\frac{\\partial {\\ensuremath{\\mathcal{S}}}}{\\partial {\\mathsf{S}}} \\, \n{\\mathsf{C}}  \\, \n{w}_{{\\mathsf{S}}, {\\mathsf{C}}}$\n\\EndFor\n\\For{$X \\in {\\ensuremath{\\mathbf{X}}}$, ${\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X$}\n\\If {${\\ensuremath{\\mathbf{e}}}^{(l)}$ is complete w.r.t.~$X$}\n\\State $x \\leftarrow$ complete evidence for $X$\n\\State $\\theta \\leftarrow \\theta(x)$\n\\Else\n\\State ${\\mathcal{X}} \\leftarrow $ partial evidence for $X$\n\\State $\\theta \\leftarrow \\frac{\\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\theta(x) \\mathrm{d}x}{\\int_{\\mathcal{X}} {\\mathsf{D}}_X(x) \\mathrm{d}x}$\n\\EndIf\n\\State $p \\leftarrow \\frac{1}{{\\ensuremath{\\mathcal{S}}}}\\frac{\\partial {\\ensuremath{\\mathcal{S}}}}{\\partial {\\mathsf{D}}_X} {\\mathsf{D}}_X$\n\\State $\\theta_{{\\mathsf{D}}_X} \\leftarrow \\theta_{{\\mathsf{D}}_X} + p\\,\\theta$ \n\\State $n_{{\\mathsf{D}}_X} \\leftarrow n_{{\\mathsf{D}}_X} + p$ \n\\EndFor\n\\EndFor\n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}), \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}) \\colon$ \n${w}_{{\\mathsf{S}}, {\\mathsf{C}}} \\leftarrow \\frac{n_{{\\mathsf{S}}, {\\mathsf{C}}}}{\\sum_{{\\mathsf{C}}' \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})} n_{{\\mathsf{S}}, {\\mathsf{C}}'}}$\n\\State $\\forall X \\in {\\ensuremath{\\mathbf{X}}}, \\forall {\\mathsf{D}}_X \\in {\\bm{\\mathsf{D}}}_X \\colon$ \nset parameters to $\\frac{\\theta_{{\\mathsf{D}}_X}}{n_{{\\mathsf{D}}_X}}$\n\\EndWhile \n\\State \\Return ${\\ensuremath{\\mathcal{S}}}$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for EM algorithm in SPNs.}\n\\label{algo:EM}\n\\end{figure}\n\n\n\n\n\n\\section{Most Probable Explanation}\n\\label{sec:MPE}\nIn \\cite{Poon2011,Peharz2013,Peharz2014}, SPNs were applied for reconstructing data using MPE inference.\nGiven some distribution ${p}$ over ${\\ensuremath{\\mathbf{X}}}$ and evidence ${\\ensuremath{\\mathbf{e}}}$, MPE can be formalized as finding\n$\\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\ensuremath{\\mathbf{e}}}}{\\arg\\max} ~ {p}({\\ensuremath{\\mathbf{x}}}),$\nwhere we assume here that ${p}$ actually has a maximum in ${\\ensuremath{\\mathbf{e}}}$.\nMPE is a special case of MAP, defined as finding\n$ \\underset{{\\ensuremath{\\mathbf{y}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{Y}}}}]}}}{\\arg\\max} ~ \\int_{{{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{Z}}}}]}}} {p}({\\ensuremath{\\mathbf{y}}},{\\ensuremath{\\mathbf{z}}}) \\, \\mathrm{d}{\\ensuremath{\\mathbf{z}}},$\nfor some two-partition of ${\\ensuremath{\\mathbf{X}}}$, i.e.~${\\ensuremath{\\mathbf{X}}} ={\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}, {\\ensuremath{\\mathbf{Y}}} \\cap {\\ensuremath{\\mathbf{Z}}} = \\emptyset$.\nBoth MPE and MAP are generally NP-hard in BNs \\cite{Bodlaender2002, Park2002, Park2004, Kwisthout2011}, and MAP is inherently harder than MPE\n\\cite{Park2004, Kwisthout2011}. \nUsing the result in \\cite{deCampos2011}, it can be shown that MPE inference (and thus MAP inference) is NP-hard also in SPNs. \nIn particular, Theorem 5 in \\cite{deCampos2011} shows that the decision version of MAP is NP-complete for a naive Bayes structure, when the class variable is marginalized. \nNaive Bayes can be represented by the augmentation of an SPN with a single sum node. \nThus, MAP in augmented SPNs with the LVs marginalized is NP-hard. \nEquivalently, MPE is generally NP-hard in SPNs. \nA similar proof taylored to SPNs can be found in \\cite{Peharz2015b}.\n\nWhile MPE is generally NP-hard in SPNs, it is stated in \\cite{Poon2011} that MPE over the model RVs ${\\ensuremath{\\mathbf{X}}}$ and the LVs ${\\ensuremath{\\mathbf{Z}}}$ can be\nefficiently solved using a Viterbi-style algorithm. \nHowever, it was never proved that this algorithm yields an MPE solution. \nHere we show that this algorithm indeed is correct, when applied to \\emph{augmented} SPNs.\n\nLet ${\\ensuremath{\\mathcal{S}}}$ be an SPN over ${\\ensuremath{\\mathbf{X}}}$. \nWe define a \\emph{max-product network} (MPN) $\\hat {\\ensuremath{\\mathcal{S}}}$, by replacing each distribution node ${\\mathsf{D}}$ by a \\emph{maximizing} distribution node\n\n", "index": 49, "text": "\\begin{equation}\n\\hat{{\\mathsf{D}}} \\colon {\\ensuremath{\\mathcal{H}}}_{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{D}})} \\mapsto [0,\\infty], \\hat{{\\mathsf{D}}}({\\bm{{\\mathcal{Y}}}}) := \\underset{y \\in {\\bm{{\\mathcal{Y}}}}}{\\max} \\, {\\mathsf{D}}({\\ensuremath{\\mathbf{y}}}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\hat{{\\mathsf{D}}}\\colon{\\mathcal{H}}_{{\\mathbf{sc}}({\\mathsf{D}})}\\mapsto[0,%&#10;\\infty],\\hat{{\\mathsf{D}}}({\\bm{{\\mathcal{Y}}}}):=\\underset{y\\in{\\bm{{\\mathcal%&#10;{Y}}}}}{\\max}\\,{\\mathsf{D}}({\\mathbf{y}}),\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udda3</mi><mo stretchy=\"false\">^</mo></mover><mo>:</mo><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udda3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u21a6</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">\u221e</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo><mrow><mrow><mover accent=\"true\"><mi>\ud835\udda3</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce8</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mpadded width=\"+1.7pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce8</mi></mrow></munder></mpadded><mo>\u2062</mo><mi>\ud835\udda3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nSince ${\\ensuremath{\\mathbf{Y}}}_{n,1} = \\emptyset$ and ${{\\ensuremath{\\mathbf{Z}}}_1} = \\emptyset$, and \n${\\mathsf{N}}_1(Z_{\\mathsf{S}} = k) = {w}_k$, the induction basis holds.\nAssume that \\eqref{eq:factor_induct} holds for all ${\\mathsf{N}}_1,\\dots,{\\mathsf{N}}_{l-1}$.\nIf ${\\mathsf{N}}_l$ is a sum, we have due to completeness\n\n", "itemtype": "equation", "pos": 105855, "prevtext": "\nand each sum node ${\\mathsf{S}}$ by a \\emph{max node} $\\hat {\\mathsf{S}} := \\underset{\\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}})}{\\max} {w}_{\\hat {\\mathsf{S}}, \\hat {\\mathsf{C}}} \\hat {\\mathsf{C}}$.\nA product node in ${\\ensuremath{\\mathcal{S}}}$ corresponds to a product node also in $\\hat {\\ensuremath{\\mathcal{S}}}$.\n\n\n\n\\begin{theorem}\n\\label{theo:MPEaug}\nLet ${\\ensuremath{\\mathcal{S}}}$ be an \\emph{augmented} SPN over ${\\ensuremath{\\mathbf{X}}}$, where ${\\ensuremath{\\mathbf{X}}}$ already comprises the model RVs of the original SPN and LVs introduced by augmentation, and let $\\hat{{\\ensuremath{\\mathcal{S}}}}$ the corresponding MPN.\nLet ${\\mathsf{N}}$ be some node in ${\\ensuremath{\\mathcal{S}}}$ and $\\hat{{\\mathsf{N}}}$ its corresponding node in $\\hat{{\\ensuremath{\\mathcal{S}}}}$.\nThen, for every ${\\bm{{\\mathcal{X}}}} \\in {\\ensuremath{\\mathcal{H}}}_{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}$ we have $\\hat{{\\mathsf{N}}}({\\bm{{\\mathcal{X}}}}) = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}}{\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$.\n\\end{theorem}\n\n\n\nTheorem \\ref{theo:MPEaug} shows that the MPN maximizes the probability in the augmented SPN over ${\\bm{{\\mathcal{X}}}}$. \nThe proof (see appendix) also shows how to actually \\emph{find} a maximizing assignment in an augmented SPN. \nA product is maximized by independently maximizing each of its children, and a (twin) sum node is maximized by maximizing the weighted\nmaxima of its children. \nTherefore, the Viterbi-style algorithm proposed in \\cite{Poon2011} is indeed a correct algorithm, when applied to \\emph{augmented} SPNs. \nThis algorithm, denoted as \\textproc{MPEAugmented}, is shown in Fig.~\\ref{algo:MPEaugmented}. \nHere $Q$ denotes a queue of nodes, where $Q \\curvearrowleft {\\mathsf{N}}$ and ${\\mathsf{N}} \\curvearrowleft Q$ denote the en-queue and de-queue operations, respectively.\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{MPEAugmented}{${\\ensuremath{\\mathcal{S}}}, {\\ensuremath{\\mathbf{e}}}$}\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{x}}}^*$ of length $|{\\ensuremath{\\mathbf{X}}}|$\n\n\\State Evaluate ${\\ensuremath{\\mathbf{e}}}$ in corresponding MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ (upwards pass)\n\\State $Q \\curvearrowleft $ root node of MPN\n\\While {$Q$ not empty} \n\\State $\\hat {\\mathsf{N}} \\curvearrowleft Q$\n\\If {$\\hat {\\mathsf{N}}$ is a max node}\n\\State $Q \\curvearrowleft \\underset{\\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}})}{\\arg\\max} ~ \\left \\{{w}_{\\hat{\\mathsf{N}},\\hat{\\mathsf{C}}} \\, \\hat{\\mathsf{C}} \\right \\}$\n\\ElsIf {$\\hat {\\mathsf{N}}$ is a product node}\n\\State $\\forall \\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}}): Q \\curvearrowleft \\hat {\\mathsf{C}}$\n\\ElsIf {$\\hat {\\mathsf{N}}$ is a maximizing distribution node}\n\\State ${\\mathsf{N}} \\leftarrow$ corresponding distribution node\n\\State ${{{\\ensuremath{\\mathbf{x}}}^*}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}} = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}}}{\\arg\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$\n\\EndIf\n\\EndWhile \n\\State \\Return ${\\ensuremath{\\mathbf{x}}}^*$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for MPE inference in augmented SPNs.}\n\\label{algo:MPEaugmented}\n\\end{figure}\n\n\n\n\nIn \\cite{Poon2011}, however, \\textproc{MPEAugmented} was applied to \\emph{original} SPNs, not to \\emph{augmented} SPNs.\nDoes this still correspond to an MPE solution? \nAs we will see, this is indeed the case, but implicitly deterministic weights for the twin sum nodes are used in the \ncorresponding augmented SPN, i.e. weights which are all 0, except a single 1. \nAlthough this choice of twin-weights is allowed and the marginal distribution over model RVs is still the distribution\nof the original SPN, the use of deterministic twin-weights significantly influences the behavior of MPE inference.\n\n\n\n\n\\begin{figure}\n\\begin{algorithmic}[1]\n\\Procedure{MPEOriginal}{${\\ensuremath{\\mathcal{S}}}, {\\ensuremath{\\mathbf{e}}}$}\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{x}}}^*$ of length $|{\\ensuremath{\\mathbf{X}}}|$\n\\State Initialize zero-vector ${\\ensuremath{\\mathbf{z}}}^*$ of length $|{\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})|$\n\n\\State $\\forall {\\mathsf{S}} \\in \\bm{{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}}): \\forall {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}): ~ \\tilde{{w}}_{{\\mathsf{S}}, {\\mathsf{C}}} \\leftarrow 1$\n\\label{as:MPEinference_prepstart}\n\\For {${\\mathsf{S}} \\in {\\bm{\\mathsf{S}}}({\\ensuremath{\\mathcal{S}}})$, ${\\mathsf{S}}^c \\in {\\bm{\\mathsf{S}}}^c({\\mathsf{S}})$}\n\\For {${\\mathsf{C}} \\in \\{ {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{S}}^c) {\\ensuremath{\\,|\\,}} {\\mathsf{S}} \\not \\in {\\ensuremath{\\mathbf{desc}}}({\\mathsf{C}}) \\}$}\n\\State $\\tilde{{w}}_{{\\mathsf{S}}^c, {\\mathsf{C}}} \\leftarrow \\tilde{{w}}_{{\\mathsf{S}}^c,{\\mathsf{C}}} \\times \n\\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\mathsf{S}}}]}}}{\\max} ~ \\bar{{w}}_{{\\mathsf{S}}, {\\mathsf{C}}^{k}_{\\mathsf{S}}}$\n\\EndFor\n\\EndFor\n\\label{as:MPEinference_prepend}\n\\State Equip MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ with weights\n${w}_{\\hat{\\mathsf{S}},\\hat{\\mathsf{C}}} \\leftarrow {\\tilde{w}}_{{\\mathsf{S}},{\\mathsf{C}}} \\times {{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$\n\\State Evaluate ${\\ensuremath{\\mathbf{e}}}$ in MPN $\\hat{\\ensuremath{\\mathcal{S}}}$ (upwards pass)\n \\State $\\bm{\\hat {\\mathsf{S}}} \\leftarrow$ set of all max nodes\n\\label{as:MPEinference_mainstart}\n \\State $Q \\curvearrowleft $ root node of MPN\n \\While {$Q$ not empty} \n \\State $\\hat{\\mathsf{N}} \\curvearrowleft Q$\n \\If {$\\hat{\\mathsf{N}}$ is a max node}\n \\State ${{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{N}}}}]}} \\leftarrow \\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\hat{\\mathsf{N}}}}]}}}{\\arg\\max} ~ \n\\left \\{{w}_{\\hat{\\mathsf{N}}, \\hat{\\mathsf{C}}^k_{\\hat{\\mathsf{N}}}} \\, \\hat{\\mathsf{C}}^k_{\\hat{\\mathsf{N}}} \\right \\}$\n\\label{as:MPEinference_diff1}\n\\State $Q \\leftarrow {\\mathsf{C}}^{{{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{N}}}}]}}}_{\\hat{\\mathsf{N}}}$\n\\label{as:MPEinference_diff2}\n \\State $\\bm{\\hat{\\mathsf{S}}} \\leftarrow \\bm{\\hat{\\mathsf{S}}} \\setminus \\{\\hat{\\mathsf{N}}\\}$\n\\label{as:MPEinference_diff3}\n \\ElsIf {$\\hat {\\mathsf{N}}$ is a product node}\n \\State $\\forall \\hat {\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat {\\mathsf{N}}): Q \\curvearrowleft \\hat{\\mathsf{C}}$\n \\ElsIf {$\\hat {\\mathsf{N}}$ is a distribution node}\n\\State ${\\mathsf{N}} \\leftarrow$ corresponding distribution node\n\\State ${{{\\ensuremath{\\mathbf{x}}}^*}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}} = \\underset{{\\ensuremath{\\mathbf{x}}} \\in {{{\\ensuremath{\\mathbf{e}}}}{[{{\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})}]}}}{\\arg\\max} ~ {\\mathsf{N}}({\\ensuremath{\\mathbf{x}}})$\n\n \\EndIf\n \\EndWhile \n\\label{as:MPEinference_mainend}\n\\For {$\\hat {\\mathsf{S}} \\in \\bm{\\hat{\\mathsf{S}}}$}\n\\label{as:MPEinference_assignreststart}\n\\State ${{{\\ensuremath{\\mathbf{z}}}^*}{[{Z_{\\hat{\\mathsf{S}}}}]}} = \\underset{k \\in {{{\\ensuremath{\\mathbf{e}}}}{[{Z_{\\hat{\\mathsf{S}}}}]}}}{\\arg\\max} ~ \\bar{{w}}_{\\hat{\\mathsf{S}}, {\\mathsf{C}}^{k}_{\\hat{\\mathsf{S}}}}$\n\\EndFor\n\\label{as:MPEinference_assignrestend}\n\\State \\Return ${\\ensuremath{\\mathbf{x}}}^*, {\\ensuremath{\\mathbf{z}}}^*$\n\\EndProcedure\n\\end{algorithmic}\n\\caption{Pseudo-code for MPE inference in augmented SPNs, performed using the original SPN structure.}\n\\label{algo:MPE}\n\\end{figure}\n\n\n\nTo see this, let us modify \\textproc{MPEAugmented}, such that it can be applied to an original SPN, \nbut returning an MPE solution for the corresponding augmented SPN. \n\\textproc{MPEOriginal} shown in Fig.~\\ref{algo:MPE} is such a modification, simulating the additional \nstructure introduced by augmentation. \nIn steps \\ref{as:MPEinference_prepstart}-\u00e2\u0080\u0093\\ref{as:MPEinference_prepend} we first construct the correction weights $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}}$. \nFor a particular sum ${\\mathsf{S}}$ and child ${\\mathsf{C}}$,  $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}}$ is the product over the maxima of all twin\nsum nodes which would be connected to the link of ${\\mathsf{C}}$ in\nthe augmented SPN. \nBy equipping the corresponding MPN with weights \n${w}_{\\hat{\\mathsf{S}},\\hat{\\mathsf{C}}} \\leftarrow {\\tilde{w}}_{{\\mathsf{S}},{\\mathsf{C}}} \\times {{w}}_{{\\mathsf{S}},{\\mathsf{C}}}$, \nthe effect of the twin sum nodes is simulated.\n\n\nSince the LVs corresponding to the sum nodes are not explicitly introduced, we need to assign their MPE state\n\u00e2\u0080\u009cby hand\u00e2\u0080\u009d. \nIn step \\ref{as:MPEinference_diff1}, the maximizing state of an LV corresponding to a ``visited'' sum ${\\mathsf{S}}$ is assigned. \nIn steps \\ref{as:MPEinference_assignreststart}-\u00e2\u0080\u0093\\ref{as:MPEinference_assignrestend}, the maximizing states of those LVs are assigned, \nwhich were not visited during back-tracking.\n\n\n\n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=0.45\\textwidth]{lowdepth}\n \\caption{Illustration of the low-depth bias using an SPN over RVs $\\{X_1,X_2,X_3\\}$. The structure introduced by augmentation is depicted by small nodes and edges. When deterministic twin-weights are used, the state of $Z_{{\\mathsf{S}}^1}$ corresponding to ${\\mathsf{P}}^1$ is preferred over ${\\mathsf{P}}^2$ and ${\\mathsf{P}}^3$, since their probabilities are ``dampened'' by the weights of ${\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$.}\n\\label{fig:problemMPE}\n\\end{figure}\n\n\n\n\\textproc{MPEOriginal} is essentially equivalent to \\textproc{MPEAugmented}\nwhen all $\\tilde {w}_{{\\mathsf{S}},{\\mathsf{C}}} = 1$, which is only the\ncase when all twin-weights are \\emph{deterministic}, i.e. for each sum\nnode, one twin-weight is 1 and all others are 0. \nWhen deterministic twin-weights are used, we refer to \\textproc{MPEOriginal}\nas \\textproc{MPEDet}, and when uniform twin-weights are used, we refer to it as\n \\textproc{MPEUni}.\nUsing deterministic twin-weights is a rather unnatural choice, since this\nprefers one arbitrary state over the others in cases where this LV is actually ``rendered irrelevant''. \n\n\\textproc{MPEDet} also has a bias towards less structured sub-models, which we call \\emph{low-depth bias}.\nThis is illustrated in Fig.~\\ref{fig:problemMPE}, which shows an SPN over\nthree RVs $X_1, X_2, X_3$. \nThe augmented SPN has two twin sum nodes, corresponding to ${\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$. \nWhen their twin-weights are deterministic, the selection of the state of $Z_{{\\mathsf{S}}^1}$\nis \\emph{biased} towards the state corresponding to ${\\mathsf{P}}^1$, which is a distribution assuming \nindependence among $X_1$, $X_2$ and $X_3$. \nThis comes from the fact, that the values of ${\\mathsf{P}}^2$ and ${\\mathsf{P}}^3$ are dampened by the weights of \n${\\mathsf{S}}_2$ and ${\\mathsf{S}}_3$, respectively, which are generally smaller than 1. \nTherefore, when using deterministic weights for twin sum nodes, we introduce a bias towards the \nselection of sub-SPNs that are less deep and less structured. \nUsing uniform weights for twin sum nodes is ``fairer'', \nsince in this case \n${\\mathsf{P}}^1$ gets dampened by $\\bar{\\mathsf{S}}^2$ and $\\bar{\\mathsf{S}}^3$,\n${\\mathsf{P}}^2$ by ${\\mathsf{S}}^2$ and $\\bar{\\mathsf{S}}^3$, and \n${\\mathsf{P}}^3$ by $\\bar{\\mathsf{S}}^2$ and ${\\mathsf{S}}^3$. \nUniform weights are actually the opposite choice to deterministic twin-weights: the\nformer represent the strongest dampening via twin-weights\nand therefore actually \\emph{penalize} less structured distributions.\n\nMPE inference in LV models can be used for hard EM,\ni.e.~to treat LVs as observed using the MPE solution and\nusing discrete winner-take-all counts for EM updates. \nNote that in \\cite{Poon2011} hard EM was used, but it was noted that \n\\emph{\n``The best results were obtained using sums on the upward pass and\nmaxes on the downward pass (i.e.,~the MPE value of each hidden\nvariable is computed conditioning on the MPE values of the\nhidden variables above it and summing out the ones below).''\n}\nUsing summation in the upwards pass and maximization\nin the downward pass does generally \\emph{not} deliver an MPE\nsolution. \nIt is, however, an arguable approximation to the\nMAP problem, when maximizing over model RVs and summing\nout LVs. \nDue to this behavior, we refer to this method\nas \\textproc{IterativeMap}. \nPresumably, \\textproc{IterativeMap} performed better during learning since \nit does not suffer from the low-depth bias, since the ``damping factor'' for each sum node is 1. \nInvestigating this effect further is subject to future work.\n\n\n\n\\section{Experiments}\n\n\\subsection{Experiments with EM Algorithm}   \\label{sec:experimentsEM}\nIn \\cite{Poon2011, PoonSPNCode} SPNs were applied to image data, where a generic\narchitecture reminiscent to convolutional neural networks was proposed. \nWe refer to this architecture as PD architecture. \nStandard EM was not used in experiments for two reasons: \nFirst, explicitly constructing the proposed structure\nand to train it with standard EM is hardly possible with\ncurrent hardware, since the number of nodes grows $\\mathcal{O}(l^3)$,\nwhere $l$ is the square-length of the modeled image domain in pixels \\cite{Peharz2015b}. \nInstead, a sparse hard EM algorithm was used, which virtualizes the PD structure, i.e.~sum and products\nare generated on the fly (see \\cite{PoonSPNCode} for details). \nSecond, using standard EM seemed unsuited to train large and dense SPNs,\neither because it is trapped in local optima or due to the\ngradient vanishing phenomenon.\n\n\nIn our experiments, we investigated three questions:\n\\begin{enumerate}\n\\item\nIs our derivation of EM correct, both for complete\nand missing data?\n\\item \nCan the result of hard EM  \\cite{Poon2011} be improved by\nstandard EM?\n\\item\nGiven a suited sparse structure, does EM yield a\ngood solution for parameters?\n\\end{enumerate}\nQuestion 1) is important since the original derivation contained\nan error. Questions 2) and 3) are concerned with the\ngeneral applicability of EM for training SPN.\n\n\nWe used the same datasets and SPN structures as used\nin \\cite{Poon2011}, which can be obtained from \\cite{PoonSPNCode}. \nThe used datasets comprise Caltech-101 (inclusive background class) \\cite{FeiFei2004} and\nthe ORL face images \\cite{Samaria1994}, i.e. in total 103 datasets. \nThe input distributions in these models are single-dimensional\nGaussians (4 for each pixel), where the means were set to the\naverage of the 4-quantiles and the variances were constantly set to 1. \nWe ran EM (Fig.~\\ref{algo:EM}) for 30 iterations, with various settings:\n\n\n\n\\begin{itemize}\n\\item \nUpdate any combination of the three different types\nof parameters, i.e. sum-weights, Gaussian means and\nGaussian variances. \nEach set of parameters types is encoded by a string of \nletters W (weights), M (means) and V (variances). \n(7 combinations)\n\\item\nUse original parameters for initialization, obtained\nfrom \\cite{PoonSPNCode}, or use 3 random initialization, where sum-weights\nare drawn from a Dirichlet distribution with uniform $\\alpha=1$ hyper-parameter \n(i.e. uniform distribution on the standard simplex), Gaussian means\nare uniformly drawn from $[-1,1]$ and Gaussian variances from $[0.01,1]$. \nOnly parameters which are actually updated are initialized randomly; otherwise\nthe original parameters \\cite{Poon2011} are used and kept fixed.\n(4 combinations)\n\\item\nUse complete data or simulate missing training data\nby randomly discarding $33\\%$ or $66\\%$ of the models\nRVs, independently for each sample. \n(3 combinations)\n\\end{itemize}\n\n\n\nThus, in total we ran EM $7 \\times 4 \\times 3 \\times 103 = 8652$ times yielding\n$259560$ EM-iterations. \nTo avoid pathological solutions we used a lower bound of $0.01$ for the Gaussian variances. \nIn \\emph{no iteration} we observed a decreasing likelihood on the training \nset, i.e. our derived EM algorithm showed monotonicity in our experiments. \nMoreover, as can be seen in the performance curves in Fig.~\\ref{fig:EMtrain}, the log-likelihood on the training set actually increased over\niterations. \nThe curves for the missing data scenarios are similar. \nThis gives affirmative evidence for question 1).\n\n\n\n\\begin{figure}\n\\centering\n \\subfloat[]{\n  \\includegraphics[width=0.5 \\textwidth]{EMtrain}\n  \\label{fig:EMtrain}  \n  }\\qquad\n \\subfloat[]{\n  \\includegraphics[width=0.5 \\textwidth]{EMtest}\n   \\label{fig:EMtest}\t\n}\n\\caption{Log-likelihood over EM-iterations, normalized by the number\nof samples and averaged over all 103 datasets and the 3 random\ninitializations.\n\\protect\\subref{fig:EMtrain}: Training set.\n\\protect\\subref{fig:EMtest}: Test set; Curves for V and WV are\noutside the displayed region, for better readability of the other curves.\nThey start at approximately $-8000$ nats and decreased to approximately\n$-11000$ nats. }\n\\label{fig:EMresult}\n\\end{figure}\n\n\n\n\nFig.~\\ref{fig:EMtest} shows the log-likelihood on the test set. \nNote that optimizing the parameter sets V and WV led to severe overfitting: \nwhile achieving extremely high likelihoods on the training set, \nthey achieved extremely poor likelihoods on the test set. \nAlso the parameter sets MV and WMV tend to overfit, although not as strong as V and WV.\n\n\nRegarding question 2), we closer inspected the test loglikelihood\nwhen the original parameters are used for initialization,\ni.e. when the parameters obtained by \\cite{PoonSPNCode} are post-trained\nusing EM. \nTable~\\ref{tab:posttrainEM} summarizes the results. \nWhen parameter sets not including Gaussian variances are optimized\n(i.e.~W, M, and WM), the test log-likelihood increased\nmost of the time, i.e. for $83.5\\%$ (M) to up to $92.23\\%$ (WM) of the datasets.\nFurthermore, having oracle knowledge about the ideal number of iterations \n(i.e. column best), the average log-likelihood increased by $0.58\\%$ (M) to up to\n$1.39\\%$ (WM) relative to the original parameters. \nMost of this improvement happens in the first iteration, yielding $0.52\\%$\n(M) up to $1.05\\%$ (WM) improvement. \nThese results indicate that the parameters obtained by \\cite{PoonSPNCode} slightly underfit the\ngiven datasets. \nSimilar as in Fig.~\\ref{fig:EMresult}, we see that parameter sets including the Gaussian variances \n(V, WV, MV, WMV) are prone to overfitting: more than $60\\%$ of the datasets\ndecreased their test log-likelihood during EM. \nHowever, in the remaining $40\\%$ of the datasets, the test log-likelihood can\nbe improved \\emph{substantially} by at least $14\\%$ on average.\n\n\n\n\\begin{table}\n\\caption{\nChanges in test log-likelihoods when original parameters are\npost-trained using EM. \n\\% inc.: percentage of datasets where log-likelihood increased in the first iteration. \n\\% all, \\% pos., \\% neg.: relative change of log-likelihood, averaged over all datasets, \ndatasets with positive change, datasets with negative change, respectively.\n}\n\\label{tab:posttrainEM}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c| c c c | c c c|}\n\\hline \n &       &  \\multicolumn{3}{|c|}{after 1st iteration}  & \\multicolumn{3}{|c|}{best} \\\\ \n\\hline \n &  \\% inc. &  \\% all. & \\% pos. & \\% neg.  &  \\% all & \\% pos. & \\% neg. \\\\ \n\\hline \nW    &  91.26  &  0.55 & 0.61 & -0.03  &  0.87 & 0.96 & -0.03 \\\\ \nM    &  83.50  &  0.52 & 0.67 & -0.21  &  0.58 & 0.73 & -0.21 \\\\ \nWM   &  92.23  &  1.06 & 1.18 & -0.30  &  1.39 & 1.53 & -0.30 \\\\ \nV    &  39.81  &  -13.47 & 14.44 & -31.93  &  -13.45 & 14.51 & -31.93 \\\\ \nWV   &  39.81  &  -13.41 & 14.79 & -32.06  &  -13.33 & 14.98 & -32.06 \\\\ \nMV   &  38.83  &  -17.24 & 14.27 & -37.25  &  -17.21 & 14.35 & -37.25 \\\\ \nWMV  &  38.83  &  -17.18 & 14.63 & -37.37  &  -17.12 & 14.78 & -37.37 \\\\ \n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\n\n\nWe now turn to question 3). \nAs pointed out above, a hard EM variant was used in \\cite{Poon2011, PoonSPNCode} which at the same time\nfinds the effective SPN structure. \nOptimizing W using the 3 random initialization amounts to using the oracle structure\nobtained by \\cite{Poon2011, PoonSPNCode}, discarding the learned parameters. \nFor each dataset we selected the random initialization which yielded the highest \nlikelihood on the training set in iteration 30. \nFor this run, we compared the log-likelihoods with the log-likelihoods obtained \nby the original parameters. \nThe results are summarized in Table~\\ref{tab:randinitEM}. \n\n\n\n\\begin{table}\n\\caption{\nLog-likelihoods when sum-weights (W) are trained, using random\ninitialization. \n$\\%>$: percentage of data sets, where log-likelihood is larger than for original parameters. \n$\\%$ all, $\\%$ pos., $\\%$ neg.: relative log-likelihood w.r.t.~original parameters, \nfor all data sets, data sets where relative log-likelihood is positive/negative, respectively.\n}\n\\label{tab:randinitEM}\n\\tabcolsep=0.12cm\n\\begin{tabular}{|l | c c c c | c c c c|}\n\\hline \n &      \\multicolumn{4}{|c|}{after 1st iteration}  & \\multicolumn{4}{|c|}{best} \\\\ \n\\hline \n &  \\%$>$ &  \\% all. & \\% pos. & \\% neg.  & \\%$>$ & \\% all & \\% pos. & \\% neg. \\\\ \n\\hline \ntrain & 70.87 & 0.68 & 1.38 & -1.00 & 100.00 & 3.97 & 3.97 & - \\\\ \ntest & 41.75 & -0.11 & 0.40 & -0.48 & 67.96 & 0.46 & 0.76 & -0.18 \\\\ \n\\hline \n\\end{tabular}\n\\end{table}\n\n\n\nWe see that on all data\nsets the log-likelihood on the training set is larger than\nfor the original parameters. This is also the case for each\nindividual random start (not just best one) -- every random\nrestart always yielded a higher training log-likelihood than\nthe original parameters. \nThus, by considering the actual optimization objective \n-- the likelihood on the training set -- EM successfully trains SPNs, \ngiven a suited oracle structure.\nFurthermore, as can be seen in Table~\\ref{tab:randinitEM}, \nEM is also not more prone to overfitting than the algorithm in \\cite{Poon2011}: \non $67.96\\%$ of the datasets, EM delivered a higher test log-likelihood\nthan the original parameters, when using oracle knowledge\nabout the ideal number of iterations (column best).\n\n\n\\subsection{Experiments with MPE Inference}\nTo illustrate correctness of \\textproc{MPEOriginal} (Fig.~\\ref{algo:MPE}), we\ngenerated SPNs using the PD architecture \\cite{Poon2011}, arranging\n$4$, $9$ and $16$ binary RVs in a $2 \\times 2$, $3 \\times 3$ and $4 \\times 4$ grid,\nrespectively. \nAs inputs we used two indicator variables for each RV representing their two states. \nThe sum-weights were drawn from a Dirichlet distribution with uniform $\\alpha$-parameters, \nwhere $\\alpha \\in \\{0.5,1,2\\}$. \nFor all networks we drew $100$ independent parameters sets. \nFor each structure and parameter set we ran \\textproc{IterativeMap}, \n\\textproc{MPEDet} and \\textproc{MPEUni} to obtain an assignment for all RVs. \nFor each assignment the log-likelihoods were evaluated in the augmented SPN with\ndeterministic weights, the augmented SPN with uniform weights and in \nthe original SPN (discarding the states of the LVs). \nAdditionally, we found ground truth MPE assignments using exhaustive enumeration. \nThe results relative to the ground truth MPE solutions are shown in \nTables~\\ref{tab:MPEaugmented}, \\ref{tab:MPEaugmentedDet}, and \\ref{tab:MPEoriginal}. \nAs can be seen, \\textproc{MPEUni} always finds an MPE solution in the augmented SPN \nwith uniform twin-weights and \\textproc{MPEDet} always finds an MPE solution\nin augmented SPNs with deterministic twin-weights. \nThis gives empirical evidence for the correctness of \\textproc{MPEUni}\nfor MPE inference in augmented SPNs. \nFurthermore, we wanted to investigate the quality of the three algorithms\nwhen serving as approximation for MPE inference in the original SPNs. \nAs can be seen in Table~\\ref{tab:MPEoriginal}, all three algorithms\ndeliver arguable approximations. \nFor the SPNs considered here, \\textproc{MPEDet} delivered on average slightly\nbetter approximations than \\textproc{IterativeMap} and \\textproc{MPEUni}.\nHowever, these results should be interpreted with\ncaution, due to the rather similar nature of the distributions\nconsidered here. Closer investigating approximate MPE for\n(original) SPNs is an interesting direction and will be subject\nto future research.\n\n\n\n\\begin{table}\n\\center\n\\caption{\nDifferences of log-likelihood to the ground-truth MPE solution found by\nexhaustive enumeration, averaged over 100 independent draws of\nsum-weights. Numbers in parentheses are the number of times where\nan MPE solution was found. Results for augmented SPNs using\nuniform twin-weights.\n}\n\\label{tab:MPEaugmented}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n       & $\\alpha=0.5$ & 0.00 (100) & -0.04 (82) & 0.00 (100) \\\\ \n4 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.08 (77) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.05 (79) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.10 (70) & -0.44 (37) & 0.00 (100) \\\\ \n9 RVs & $\\alpha=1.0$ & -0.10 (68) & -0.48 (22) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & -0.11 (62) & -0.53 (19) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.63 (19) & -1.16 (10) & 0.00 (100) \\\\ \n16 RVs & $\\alpha=1.0$ & -0.85 (12) & -1.25 (8) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & -0.82 (12) & -1.25 (0) & 0.00 (100) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\\begin{table}\n\\center\n\\caption{\nSimilar as in Table~\\ref{tab:MPEaugmented}. Results for augmented SPNs using deterministic\ntwin-weights.\n}\n\\label{tab:MPEaugmentedDet}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n        & $\\alpha=0.5$ & 0.00 (100) & -0.04 (82) & 0.00 (100) \\\\ \n4 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.08 (77) & 0.00 (100) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.05 (79) & 0.00 (100) \\\\ \n\\hline\n       & $\\alpha=0.5$ & 0.00 (100) & -0.43 (36) & -0.10 (70) \\\\ \n9 RVs & $\\alpha=1.0$ & 0.00 (100) & -0.46 (25) & -0.12 (68) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -0.57 (20) & -0.15 (62) \\\\ \n\\hline\n       & $\\alpha=0.5$ & 0.00 (100) & -1.19 (5) & -0.89 (19) \\\\ \n16 RVs & $\\alpha=1.0$ & 0.00 (100) & -1.36 (8) & -1.11 (12) \\\\ \n       & $\\alpha=2.0$ & 0.00 (100) & -1.47 (2) & -1.01 (12) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\\begin{table}\n\\center\n\\caption{\nSimilar as in Table~\\ref{tab:MPEaugmented}. Results for original SPNs.\n}\n\\label{tab:MPEoriginal}\n\\tabcolsep=0.17cm\n\\begin{tabular}{|l c c c c|}\n\\hline \n &  & \\textproc{MPEDet} & \\textproc{IterativeMap} & \\textproc{MPEUni}  \\\\ \n\\hline \n      & $\\alpha=0.5$ & -0.06 (72) & -0.09 (63) & -0.06 (72) \\\\ \n4 RVs & $\\alpha=1.0$ & -0.09 (59) & -0.14 (51) & -0.09 (59) \\\\ \n       & $\\alpha=2.0$ & -0.10 (52) & -0.15 (44) & -0.10 (52) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.31 (32) & -0.68 (13) & -0.38 (27) \\\\ \n9 RVs & $\\alpha=1.0$ & -0.47 (12) & -0.67 (5) & -0.48 (12) \\\\ \n       & $\\alpha=2.0$ & -0.40 (6) & -0.61 (2) & -0.37 (7) \\\\ \n\\hline\n       & $\\alpha=0.5$ & -0.76 (5) & -1.42 (5) & -1.04 (4) \\\\ \n16 RVs & $\\alpha=1.0$ & -0.76 (3) & -1.29 (1) & -1.18 (2) \\\\ \n       & $\\alpha=2.0$ & -0.67 (1) & -1.07 (0) & -0.92 (0) \\\\ \n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\n\n\\section{Conclusion}\nIn this paper we revisited the interpretation of SPNs as\nhierarchically structured LV model. \nWe pointed out that the original approach to explicitly incorporate LVs does not\nproduce a sound probabilistic model. \nAs a remedy we proposed the augmentation of SPNs and proved its soundness as LV model.\n\nWithin augmented SPNs, we investigated the independency\nstructure represented as BN, and showed that the sum-weights\ncan be interpreted as structured CPTs within this BN. \nUsing augmented SPNs, we derived the EM algorithm for sum-weights\nand single-dimensional input distributions from\nexponential families. \nWhile MPE-inference is generally NP-hard in SPNs, we showed that the Viterbi-style algorithm proposed in\n \\cite{Poon2011} indeed recovers an MPE solution in augmented SPNs.\nIn experiments we give empirical evidence supporting our theoretical results. \nWe furthermore showed that standard EM can successfully train generative SPNs, given a suitable\nnetwork structure.\n\n\n\n\n\n\n\\appendices\n\\section{Proofs}\n\n\n\\subsection{Proof of Proposition \\ref{prop:augmentationSound}}\nIf ${\\ensuremath{\\mathcal{S}}}'$ is a complete and decomposable SPN over ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$, then ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$ is immediate: \nComputing ${{\\ensuremath{\\mathcal{S}}}'}({\\ensuremath{\\mathbf{x}}})$ for any ${\\ensuremath{\\mathbf{x}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{X}}})$ is done by marginalizing ${\\ensuremath{\\mathbf{Z}}}$, i.e.~setting all ${\\lambda_{{Z_{\\mathsf{S}}}={k}}} = 1$.\nIn this case, it is easy to see that none of the structural changes modifies the output of the SPN, i.e.~the outputs of ${\\ensuremath{\\mathcal{S}}}$ and ${\\ensuremath{\\mathcal{S}}}'$ agree for each ${\\ensuremath{\\mathbf{x}}}$, i.e.~${\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{X}}}) \\equiv {\\ensuremath{\\mathcal{S}}}({\\ensuremath{\\mathbf{X}}})$.\n\n\nIt remains to show that ${\\ensuremath{\\mathcal{S}}}'$ is complete and decomposable, and that the root's scope is ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Z}}}$.\nSteps \\ref{as:introLinksStart}--\\ref{as:introLinksEnd} in \\textproc{AugmentSPN} introduce the links, representing \"private copies\" of the sum's children, and clearly leave the SPN complete and decomposable.\nIn steps \\ref{as:introIVStart}--\\ref{as:introIVEnd} the LV $Z_{\\mathsf{S}}$ is introduced in the scope of ${\\mathsf{S}}$ and thus in the scope of the root.\nSince this is done for all sum nodes, all ${\\ensuremath{\\mathbf{Z}}}$ are introduced in the root's scope.\nSteps \\ref{as:introIVStart}--\\ref{as:introIVEnd} cannot render products non-decomposable, since this would imply that ${\\mathsf{S}}$ is reachable by two distinct children of this product -- a contradiction that the SPN was decomposable before.\nHowever, as shown in Fig.~\\ref{fig:problemAugmentSPNnaiv}, steps \\ref{as:introIVStart}--\\ref{as:introIVEnd} can render ancestor sums incomplete.\nThese are treated in steps \\ref{as:augmentRemedyStart}--\\ref{as:augmentRemedyEnd}.\nThe twin sum $\\bar{{\\mathsf{S}}}$, if introduced, is clearly complete and has scope $\\{Z\\}$.\nFurthermore, incompleteness of any conditioning sum ${\\mathsf{S}}^c$ can only be caused by links not having $Z_{\\mathsf{S}}$ in their scope.\nThe scope of these links is augmented by $Z_{\\mathsf{S}}$ in step \\ref{as:connectTwin}.\nThese links remain decomposable and moreover, ${\\mathsf{S}}^c$ is rendered complete now.\n\n\n\n\\hfill $\\qed$\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:confspn}}\n\\textbf{ad \\emph{1.)}}\nWhen deleting the IVs and their links, the scopes of any (twin) sum remains the same, since it is complete and is left with one child. \nThus also the scope of any ancestor remains the same. \\\\\n\\textbf{ad \\emph{2.)}}\nThe graph of ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is rooted and acyclic, since the root cannot be a link and deleting nodes and edges cannot introduce cycles.\nWhen an IV ${\\lambda_{{Y}={y}}}$ is deleted, also the link ${\\mathsf{P}}_{{\\mathsf{S}}_Y}^{y}$ is deleted, so no internal nodes are left as leaves.\nThe roots in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ and ${\\ensuremath{\\mathcal{S}}}'$ are the same, and by point \\emph{1.}, ${\\ensuremath{\\mathbf{X}}} \\cup {\\ensuremath{\\mathbf{Y}}} \\cup {\\ensuremath{\\mathbf{Z}}}$ is the scope of the root.\n${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ is also complete and decomposable:\nWhenever an IV and its link are deleted, the corresponding sum node and twin sum node remain trivially complete, since they are left with a single child.\nThe link of this single child is clearly decomposable.\nFurthermore, completeness and decomposability of any ancestor of ${\\mathsf{S}}_Y$ or $\\bar {\\mathsf{S}}_Y$ is left intact, since neither ${\\mathsf{S}}_Y$ nor $\\bar {\\mathsf{S}}_Y$ changes its scope. \\\\\n\\textbf{ad \\emph{3.)}}\nAccording to point \\emph{1.}, the scope of ${\\mathsf{N}}$ is the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$.\nSince ${\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}) \\cap {\\ensuremath{\\mathbf{Y}}} = \\emptyset$, the disconnected IVs and deleted links are no descendants of ${\\mathsf{N}}$, i.e.~no descendants of ${\\mathsf{N}}$ are disconnected during configuration.\nSince ${\\mathsf{N}}$ is present in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$, it must still be reachable from the root.\nTherefore also all descendants of ${\\mathsf{N}}$ are reachable, i.e. ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}_{\\mathsf{N}} = {\\ensuremath{\\mathcal{S}}}'_{\\mathsf{N}}$. \\\\\n\\textbf{ad \\emph{4.)}}\nWhen the input is fixed to ${\\ensuremath{\\mathbf{x}}}, {\\ensuremath{\\mathbf{z}}}, {\\ensuremath{\\mathbf{y}}}$, all IVs and links which are deleted from the configured SPN ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$ evaluate to zero in the augmented SPN ${\\ensuremath{\\mathcal{S}}}'$.\nThe outputs of all sums and twin sums are therefore the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$. \nTherefore, also the output of all other nodes remains the same.\nThis includes the root and therefore ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}},{\\ensuremath{\\mathbf{y}}}) = {\\ensuremath{\\mathcal{S}}}'({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}}, {\\ensuremath{\\mathbf{y}}})$, for any ${\\ensuremath{\\mathbf{x}}}, {\\ensuremath{\\mathbf{z}}}$.\n\nWhen ${\\ensuremath{\\mathbf{y}}}' \\not= {\\ensuremath{\\mathbf{y}}}$, then there must be a $Y \\in {\\ensuremath{\\mathbf{Y}}}$ such that the IV ${\\lambda_{{Y}={{{{\\ensuremath{\\mathbf{y}}}'}{[{Y}]}}}}}$ has been deleted, i.e.~${\\lambda_{{Y}={{{{\\ensuremath{\\mathbf{y}}}'}{[{Y}]}}}}} \\notin {\\ensuremath{\\mathbf{desc}}}({\\mathsf{N}})$, where ${\\mathsf{N}}$ is the root of ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}$.\nUsing Lemma~1 in \\cite{Peharz2015}, it follows that ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{y}}}({\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{z}}},{\\ensuremath{\\mathbf{y}}}') = 0$.\n \\hfill $\\qed$\n\n\n\n\\subsection{Proof of Lemma \\ref{lem:singlepath}}\n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ must contain either ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$, since $Z_{\\mathsf{S}}$ is in the scope of the root by Proposition~\\ref{prop:confspn}.\nTo show that \\emph{not both} are in ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$, let $\\bm{\\Pi}_k$ denote the set of paths of length $k$ from the root to any node ${\\mathsf{N}}$ with $Z_{\\mathsf{S}} \\in {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}})$.\nFor $k > 1$, all paths in $\\bm{\\Pi}_k$ can be constructed by extending all paths in $\\bm{\\Pi}_{k-1}$ using all children which have $Z_{\\mathsf{S}}$ in their scopes.\nLet $K$ be the smallest number such that there is a path in $\\bm{\\Pi}_k$ containing ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$.\n\nWe show by induction, that $|\\bm{\\Pi}_k| = 1$, $k=1,\\dots,K$.\nNote that $\\bm{\\Pi}_1$ contains a single path $({\\mathsf{N}})$, where ${\\mathsf{N}}$ is the root, therefore the induction basis holds.\n\nFor the induction step, we show that given $|\\bm{\\Pi}_{k-1}| = 1$, then also $|\\bm{\\Pi}_{k}| = 1$.\nLet $({\\mathsf{N}}_1, \\dots, {\\mathsf{N}}_{k-1})$ be the single path in $\\bm{\\Pi}_{k-1}$.\nIf ${\\mathsf{N}}_{k-1}$ is a product node, then it has a single child ${\\mathsf{C}}$ with $Z_{\\mathsf{S}} \\in {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}})$, due to decomposability.\nIf ${\\mathsf{N}}_{k-1}$ is a sum node, then it must be in ${\\ensuremath{{\\mathbf{anc}_{\\bm{\\mathsf{S}}}}}}({\\mathsf{S}}) \\setminus \\{{\\mathsf{S}}\\}$, and therefore has a single child in the configured SPN. \nTherefore, there is a single way to extend the path and therefore $|\\bm{\\Pi}_k|=1, k=1,\\dots,K$.\nThis single path does either lead to ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$.\nSince ${\\mathsf{S}} \\notin {\\ensuremath{\\mathbf{desc}}}(\\bar{{\\mathsf{S}}})$ and $\\bar{{\\mathsf{S}}} \\notin {\\ensuremath{\\mathbf{desc}}}({{\\mathsf{S}}})$, ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains a single path to one of them, but not to both.\n \\hfill $\\qed$\n\n\n\n\\subsection{Proof of Theorem \\ref{theo:conditionalIndependence}}\nBy Lemma~\\ref{lem:singlepath}, for each ${\\ensuremath{\\mathbf{z}}} \\in {\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ the configured SPN \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ contains either ${\\mathsf{S}}$ or $\\bar{{\\mathsf{S}}}$, but not both.\nLet ${\\bm{{\\mathcal{Z}}}}$ be the subset of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ such that ${\\mathsf{S}}$ is in ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$ \nand $\\bar {\\bm{{\\mathcal{Z}}}}$ be the subset of ${\\ensuremath{\\mathbf{val}}}({\\ensuremath{\\mathbf{Z}}}_p)$ such that $\\bar{{\\mathsf{S}}}$ is in \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\n\n\nFix $Z_{\\mathsf{S}} = k$ and ${\\ensuremath{\\mathbf{z}}} \\in \\bm{\\mathcal{Z}}$.\nWe want to compute ${\\ensuremath{\\mathcal{S}}}'(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}})$, i.e.~we marginalize ${\\ensuremath{\\mathbf{Y}}}_c$.\nAccording to Proposition~\\ref{prop:confspn} (\\emph{4.}), this equals \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_n, {\\ensuremath{\\mathbf{z}}})$.\nAccording to Proposition~\\ref{prop:confspn} (\\emph{3.}), the sub-SPN rooted at \nformer child ${\\mathsf{C}}^k_{\\mathsf{S}}$ is the same in ${\\ensuremath{\\mathcal{S}}}'$ and ${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\nSince ${\\ensuremath{\\mathcal{S}}}'$ is locally normalized, this sub-SPN is also locally normalized in \n${\\ensuremath{\\mathcal{S}}}^{{\\ensuremath{\\mathbf{z}}}}$.\nSince the scope of the former child ${\\mathsf{C}}^k_{\\mathsf{S}}$ is a sub-set of ${\\ensuremath{\\mathbf{Y}}}_c$, \nwhich is marginalized, and ${\\lambda_{{Z_{\\mathsf{S}}}={k}}} = 1$, the link  ${\\mathsf{P}}^{k}_{{\\mathsf{S}}}$ \noutputs $1$.\nSince ${\\lambda_{{Z_{\\mathsf{S}}}={k'}}} = 0$ for $k' \\not= k$, the sum ${\\mathsf{S}}$ outputs ${w}_k$.\n\n\nNow consider the set of nodes in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{z}}}$ which have $Z_{\\mathsf{S}}$ in their scope, \nnot including ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ and ${\\mathsf{P}}^k_{\\mathsf{S}}$.\nClearly, since $\\bar{{\\mathsf{S}}}$ is not in ${\\ensuremath{\\mathcal{S}}}^{\\ensuremath{\\mathbf{z}}}$, this set must be \n${\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$.\nLet ${\\mathsf{N}}_1,\\dots,{\\mathsf{N}}_L$ be a topologically ordered list of \n${\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$, where ${\\mathsf{S}}$ is ${\\mathsf{N}}_1$ and ${\\mathsf{N}}_L$ is the root.\nLet ${\\ensuremath{\\mathbf{Y}}}_{n,l} := {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}_l) \\cap {\\ensuremath{\\mathbf{Y}}}_n$ and \n${\\ensuremath{\\mathbf{Z}}}_l := {\\ensuremath{\\mathbf{sc}}}({\\mathsf{N}}_l) \\cap {\\ensuremath{\\mathbf{Z}}}_p$.\nWe show by induction that for $l=1,\\dots,L$, we have\n\n", "index": 51, "text": "\\begin{equation}\n\\label{eq:factor_induct}\n {\\mathsf{N}}_l(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_{n,l}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}) = {w}_k \\, {\\mathsf{N}}_l({\\ensuremath{\\mathbf{Y}}}_{n,l}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"{\\mathsf{N}}_{l}(Z_{\\mathsf{S}}=k,{\\mathbf{Y}}_{n,l},{{{\\mathbf{z}}}{[{{%&#10;\\mathbf{Z}}_{l}}]}})={w}_{k}\\,{\\mathsf{N}}_{l}({\\mathbf{Y}}_{n,l},{{{\\mathbf{z%&#10;}}}{[{{\\mathbf{Z}}_{l}}]}}).\" display=\"block\"><mrow><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mi>\ud835\udc33</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mi>\ud835\udc33</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ni.e.~the induction step holds for sums.\nWhen ${\\mathsf{N}}_l$ is a product, due to decomposability, it must have a \\emph{single} \nchild with $Z_{\\mathsf{S}}$ in its scope.\nHence, this child must be a node ${\\mathsf{N}}_m \\in {\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$\nWe have\n\n", "itemtype": "equation", "pos": 106500, "prevtext": "\nSince ${\\ensuremath{\\mathbf{Y}}}_{n,1} = \\emptyset$ and ${{\\ensuremath{\\mathbf{Z}}}_1} = \\emptyset$, and \n${\\mathsf{N}}_1(Z_{\\mathsf{S}} = k) = {w}_k$, the induction basis holds.\nAssume that \\eqref{eq:factor_induct} holds for all ${\\mathsf{N}}_1,\\dots,{\\mathsf{N}}_{l-1}$.\nIf ${\\mathsf{N}}_l$ is a sum, we have due to completeness\n\n", "index": 53, "text": "\\begin{align}\n {\\mathsf{N}}_l(Z_{\\mathsf{S}} = k, {{\\ensuremath{\\mathbf{Y}}}_{n,l}}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}) \n&= \\sum_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{N}}_l)} {w}_{{\\mathsf{N}}_l, {\\mathsf{C}}} \\, {w}_k \\, {\\mathsf{C}}({{\\ensuremath{\\mathbf{Y}}}_{n,l}}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}) \\\\\n &= {w}_k \\, {\\mathsf{N}}_l({{\\ensuremath{\\mathbf{Y}}}_{n,l}}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathsf{N}}_{l}(Z_{\\mathsf{S}}=k,{{\\mathbf{Y}}_{n,l}},{{{\\mathbf%&#10;{z}}}{[{{\\mathbf{Z}}_{l}}]}})\" display=\"inline\"><mrow><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mi>\ud835\udc33</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{{\\mathsf{C}}\\in{\\mathbf{ch}}({\\mathsf{N}}_{l})}{w}_{{%&#10;\\mathsf{N}}_{l},{\\mathsf{C}}}\\,{w}_{k}\\,{\\mathsf{C}}({{\\mathbf{Y}}_{n,l}},{{{%&#10;\\mathbf{z}}}{[{{\\mathbf{Z}}_{l}}]}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udda2</mi><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mrow><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mo>,</mo><mi>\ud835\udda2</mi></mrow></msub></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><mi>\ud835\udda2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mrow><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={w}_{k}\\,{\\mathsf{N}}_{l}({{\\mathbf{Y}}_{n,l}},{{{\\mathbf{z}}}{[%&#10;{{\\mathbf{Z}}_{l}}]}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mrow><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ni.e.~the induction step holds for products.\nTherefore, by induction, \\eqref{eq:factor_induct} also holds for the root, \nand \\eqref{eq:CIaugSum} follows.\n\n\nNow we show \\eqref{eq:CIaugTwinSum}.\nIf the twin sum $\\bar {\\mathsf{S}}$ does not exist, $\\bar {\\bm{{\\mathcal{Z}}}}$ is empty and \\eqref{eq:CIaugTwinSum} holds trivially.\nOtherwise, fix the input to $Z_{\\mathsf{S}} = k$ and ${\\ensuremath{\\mathbf{z}}} \\in \\bar {\\bm{{\\mathcal{Z}}}}$.\nClearly, $\\bar{{\\mathsf{S}}}$ outputs $\\bar{{w}}_k$ and \\eqref{eq:CIaugTwinSum} can be shown in similar way as \n\\eqref{eq:CIaugSum}.\n\\hfill $\\qed$\n\n\n\\subsection{Proof of Theorem \\ref{theo:MPEaug}}\nWe proof the theorem using an inductive argument.\nThe theorem clearly holds for any $\\hat{{\\mathsf{D}}}$ by definition.\nConsider a product $\\hat{{\\mathsf{P}}}$ and assume the theorem holds for all ${\\ensuremath{\\mathbf{ch}}}(\\hat{{\\mathsf{P}}})$.\nThen the theorem also holds for $\\hat{{\\mathsf{P}}}$, since \n\n", "itemtype": "equation", "pos": 107298, "prevtext": "\ni.e.~the induction step holds for sums.\nWhen ${\\mathsf{N}}_l$ is a product, due to decomposability, it must have a \\emph{single} \nchild with $Z_{\\mathsf{S}}$ in its scope.\nHence, this child must be a node ${\\mathsf{N}}_m \\in {\\ensuremath{\\mathbf{anc}}}({\\mathsf{S}})$\nWe have\n\n", "index": 55, "text": "\\begin{align}\n& {\\mathsf{N}}_l(Z_{\\mathsf{S}} = k, {\\ensuremath{\\mathbf{Y}}}_{n,l}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}) \\\\\n&= {w}_k \\, {\\mathsf{N}}_m({\\ensuremath{\\mathbf{Y}}}_{n,m}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_m}]}})  \\prod_{{\\mathsf{C}} \\in {\\ensuremath{\\mathbf{ch}}}({\\mathsf{N}}_l) \\setminus {\\mathsf{N}}_m} {\\mathsf{C}}({\\ensuremath{\\mathbf{Y}}}_{n,l} \\cap {\\ensuremath{\\mathbf{sc}}}({\\mathsf{C}})) \\\\\n&= {w}_k \\, {\\mathsf{N}}_l({{\\ensuremath{\\mathbf{Y}}}_{n,l}}, {{{\\ensuremath{\\mathbf{z}}}}{[{{\\ensuremath{\\mathbf{Z}}}_l}]}}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathsf{N}}_{l}(Z_{\\mathsf{S}}=k,{\\mathbf{Y}}_{n,l},{{{\\mathbf{z%&#10;}}}{[{{\\mathbf{Z}}_{l}}]}})\" display=\"inline\"><mrow><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi><mo>,</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mi>\ud835\udc33</mi><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle={w}_{k}\\,{\\mathsf{N}}_{m}({\\mathbf{Y}}_{n,m},{{{\\mathbf{z}}}{[{{%&#10;\\mathbf{Z}}_{m}}]}})\\prod_{{\\mathsf{C}}\\in{\\mathbf{ch}}({\\mathsf{N}}_{l})%&#10;\\setminus{\\mathsf{N}}_{m}}{\\mathsf{C}}({\\mathbf{Y}}_{n,l}\\cap{\\mathbf{sc}}({%&#10;\\mathsf{C}}))\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msub><mi>\ud835\uddad</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>m</mi></mrow></msub><mo>,</mo><mrow><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>\ud835\udda2</mi><mo>\u2208</mo><mrow><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2216</mo><msub><mi>\ud835\uddad</mi><mi>m</mi></msub></mrow></mrow></munder></mstyle><mrow><mi>\ud835\udda2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>\u2229</mo><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udda2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle={w}_{k}\\,{\\mathsf{N}}_{l}({{\\mathbf{Y}}_{n,l}},{{{\\mathbf{z}}}{[%&#10;{{\\mathbf{Z}}_{l}}]}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msub><mi>\ud835\uddad</mi><mi>l</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>,</mo><mi>l</mi></mrow></msub><mo>,</mo><mrow><mi>\ud835\udc33</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>\ud835\udc19</mi><mi>l</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nwhere the max and the product can be switched due to decomposability.\nNow consider a max node $\\hat{{\\mathsf{S}}}$.\nSince ${\\ensuremath{\\mathcal{S}}}$ is an augmented SPN, $\\hat{{\\mathsf{S}}}$ corresponds to either a sum node ${\\mathsf{S}}$ or to a twin sum node $\\bar{{\\mathsf{S}}}$.\nIn either case we have an associated RV $Z_{\\mathsf{S}}$ with IVs ${\\lambda_{{Z_{\\mathsf{S}}}={1}}}, \\dots, {\\lambda_{{Z_{\\mathsf{S}}}={K}}}$, which are connected directly (twin) or via the links (original sum) to the sum node.\nIn the MPN, the IVs are replaced by maximizing distributions ${\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={1}}}, \\dots, {\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={K}}}$, where\n\n", "itemtype": "equation", "pos": 108844, "prevtext": "\ni.e.~the induction step holds for products.\nTherefore, by induction, \\eqref{eq:factor_induct} also holds for the root, \nand \\eqref{eq:CIaugSum} follows.\n\n\nNow we show \\eqref{eq:CIaugTwinSum}.\nIf the twin sum $\\bar {\\mathsf{S}}$ does not exist, $\\bar {\\bm{{\\mathcal{Z}}}}$ is empty and \\eqref{eq:CIaugTwinSum} holds trivially.\nOtherwise, fix the input to $Z_{\\mathsf{S}} = k$ and ${\\ensuremath{\\mathbf{z}}} \\in \\bar {\\bm{{\\mathcal{Z}}}}$.\nClearly, $\\bar{{\\mathsf{S}}}$ outputs $\\bar{{w}}_k$ and \\eqref{eq:CIaugTwinSum} can be shown in similar way as \n\\eqref{eq:CIaugSum}.\n\\hfill $\\qed$\n\n\n\\subsection{Proof of Theorem \\ref{theo:MPEaug}}\nWe proof the theorem using an inductive argument.\nThe theorem clearly holds for any $\\hat{{\\mathsf{D}}}$ by definition.\nConsider a product $\\hat{{\\mathsf{P}}}$ and assume the theorem holds for all ${\\ensuremath{\\mathbf{ch}}}(\\hat{{\\mathsf{P}}})$.\nThen the theorem also holds for $\\hat{{\\mathsf{P}}}$, since \n\n", "index": 57, "text": "\\begin{align}\n \\hat{{\\mathsf{P}}}({\\bm{{\\mathcal{X}}}})  & = \\prod_{\\hat{{\\mathsf{C}}} \\in {\\ensuremath{\\mathbf{ch}}}(\\hat{{\\mathsf{P}}})} \\hat{{\\mathsf{C}}}({\\bm{{\\mathcal{X}}}}) \\\\\n& = \\prod_{{{\\mathsf{C}}} \\in {\\ensuremath{\\mathbf{ch}}}({{\\mathsf{P}}})} \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}}{\\max}~{\\mathsf{C}}({\\ensuremath{\\mathbf{x}}})  \n= \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}} \\max  \\prod_ {{{\\mathsf{C}}} \\in {\\ensuremath{\\mathbf{ch}}}({{\\mathsf{P}}})} {\\mathsf{C}}({\\ensuremath{\\mathbf{x}}})  \\\\\n&= \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}} \\max ~ {{\\mathsf{P}}}({\\ensuremath{\\mathbf{x}}}), \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathsf{P}}}({\\bm{{\\mathcal{X}}}})\" display=\"inline\"><mrow><mover accent=\"true\"><mi>\ud835\uddaf</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\prod_{\\hat{{\\mathsf{C}}}\\in{\\mathbf{ch}}(\\hat{{\\mathsf{P}}})}%&#10;\\hat{{\\mathsf{C}}}({\\bm{{\\mathcal{X}}}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mover accent=\"true\"><mi>\ud835\udda2</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\uddaf</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mover accent=\"true\"><mi>\ud835\udda2</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\prod_{{{\\mathsf{C}}}\\in{\\mathbf{ch}}({{\\mathsf{P}}})}\\underset{%&#10;{\\mathbf{x}}\\in{\\bm{{\\mathcal{X}}}}}{\\max}~{}{\\mathsf{C}}({\\mathbf{x}})=%&#10;\\underset{{\\mathbf{x}}\\in{\\bm{{\\mathcal{X}}}}}{\\max}\\prod_{{{\\mathsf{C}}}\\in{%&#10;\\mathbf{ch}}({{\\mathsf{P}}})}{\\mathsf{C}}({\\mathbf{x}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>\ud835\udda2</mi><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi></mrow></munder></mpadded><mo>\u2062</mo><mi>\ud835\udda2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi></mrow></munder><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>\ud835\udda2</mi><mo>\u2208</mo><mrow><mi>\ud835\udc1c\ud835\udc21</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\uddaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mi>\ud835\udda2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{{\\mathbf{x}}\\in{\\bm{{\\mathcal{X}}}}}{\\max}~{}{{\\mathsf%&#10;{P}}}({\\mathbf{x}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi></mrow></munder></mpadded><mo>\u2062</mo><mi>\ud835\uddaf</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\nAssume first that $\\hat{{\\mathsf{S}}}$ corresponds to a twin $\\bar{{\\mathsf{S}}}$, having ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as children.\nLet $\\bar{w}_k = {w}_{\\bar{{\\mathsf{S}}}, {\\lambda_{{Z_{{\\mathsf{S}}}}={k}}}}$.\nWe have\n\n", "itemtype": "equation", "pos": 110193, "prevtext": "\nwhere the max and the product can be switched due to decomposability.\nNow consider a max node $\\hat{{\\mathsf{S}}}$.\nSince ${\\ensuremath{\\mathcal{S}}}$ is an augmented SPN, $\\hat{{\\mathsf{S}}}$ corresponds to either a sum node ${\\mathsf{S}}$ or to a twin sum node $\\bar{{\\mathsf{S}}}$.\nIn either case we have an associated RV $Z_{\\mathsf{S}}$ with IVs ${\\lambda_{{Z_{\\mathsf{S}}}={1}}}, \\dots, {\\lambda_{{Z_{\\mathsf{S}}}={K}}}$, which are connected directly (twin) or via the links (original sum) to the sum node.\nIn the MPN, the IVs are replaced by maximizing distributions ${\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={1}}}, \\dots, {\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={K}}}$, where\n\n", "index": 59, "text": "\\begin{equation}\n {\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={k}}}({\\mathcal{Z}}) = \\underset{z_{\\mathsf{S}} \\in {\\mathcal{Z}}} \\max ~ {\\lambda_{{Z_{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}}) = \n\\begin{cases}\n 1 & \\text{if } k \\in {\\mathcal{Z}} \\\\\n 0 & \\text{otherwise}.\n\\end{cases}\n\\label{eq:maximizeInd}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E38.m1\" class=\"ltx_Math\" alttext=\"{\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={k}}}({\\mathcal{Z}})=\\underset{z_{\\mathsf{S}}%&#10;\\in{\\mathcal{Z}}}{\\max}~{}{\\lambda_{{Z_{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}})=%&#10;\\begin{cases}1&amp;\\text{if }k\\in{\\mathcal{Z}}\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">^</mo></mover><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></munder></mpadded><mo>\u2062</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>k</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ni.e.~the theorem holds for twin sum nodes.\nNow assume that $\\hat{{\\mathsf{S}}}$ corresponds to a (non-twin) sum node ${\\mathsf{S}}$.\nThe children of ${\\mathsf{S}}$ are the links ${\\mathsf{P}}^k_{\\mathsf{S}} = {\\lambda_{{Z_{\\mathsf{S}}}={k}}} \\times {\\mathsf{C}}^k_{\\mathsf{S}}$, $k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})$.\n${\\mathsf{P}}^k_{\\mathsf{S}}$ and ${\\mathsf{C}}^k_{\\mathsf{S}}$ correspond to $\\hat{{\\mathsf{P}}}^k_{\\hat{{\\mathsf{S}}}}$ and $\\hat{{\\mathsf{C}}}^k_{\\hat{{\\mathsf{S}}}}$ in $\\hat{{\\ensuremath{\\mathcal{S}}}}$, respectively.\nLet ${w}_k := {w}_{{{\\mathsf{S}}}, {{\\mathsf{P}}}^k_{{{\\mathsf{S}}}}}$.\nWhen we assume that the theorem holds for all $\\hat{{\\mathsf{C}}}^k_{\\hat{{\\mathsf{S}}}}$, then it follows that\n\n", "itemtype": "equation", "pos": 110722, "prevtext": "\nAssume first that $\\hat{{\\mathsf{S}}}$ corresponds to a twin $\\bar{{\\mathsf{S}}}$, having ${\\lambda_{{Z_{\\mathsf{S}}}={k}}}$ as children.\nLet $\\bar{w}_k = {w}_{\\bar{{\\mathsf{S}}}, {\\lambda_{{Z_{{\\mathsf{S}}}}={k}}}}$.\nWe have\n\n", "index": 61, "text": "\\begin{align}\n\\hat{{\\mathsf{S}}}({\\mathcal{Z}}) \n&= \\underset{k\\in{\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})} {\\max} ~ \\bar{w}_k \\, {\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={k}}}({\\mathcal{Z}}) \\\\\n&= \\underset{k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})} {\\max} ~ \\bar{w}_k \\, \n\\underbrace{\\underset{z_{\\mathsf{S}} \\in {\\mathcal{Z}}} {\\max} ~ {\\lambda_{{Z_{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}})}_{=0, \\text{ for } k \\notin {\\mathcal{Z}},~ \\eqref{eq:maximizeInd}} \n= \\underset{z_{\\mathsf{S}} \\in {\\mathcal{Z}}} {\\max} ~ \\bar{{\\mathsf{S}}}(z_{\\mathsf{S}}),\n\\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathsf{S}}}({\\mathcal{Z}})\" display=\"inline\"><mrow><mover accent=\"true\"><mi>\ud835\uddb2</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{\\mathbf{val}}(Z_{\\mathsf{S}})}{\\max}~{}\\bar{w}_{k%&#10;}\\,{\\hat{\\lambda}_{{Z_{\\mathsf{S}}}={k}}}({\\mathcal{Z}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi>\ud835\udc2f\ud835\udc1a\ud835\udc25</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>k</mi></msub></mpadded><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">^</mo></mover><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo>=</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{\\mathbf{val}}(Z_{\\mathsf{S}})}{\\max}~{}\\bar{w}_{k%&#10;}\\,\\underbrace{\\underset{z_{\\mathsf{S}}\\in{\\mathcal{Z}}}{\\max}~{}{\\lambda_{{Z_%&#10;{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}})}_{=0,\\text{ for }k\\notin{\\mathcal{Z}},~{}%&#10;\\eqref{eq:maximizeInd}}=\\underset{z_{\\mathsf{S}}\\in{\\mathcal{Z}}}{\\max}~{}\\bar%&#10;{{\\mathsf{S}}}(z_{\\mathsf{S}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi>\ud835\udc2f\ud835\udc1a\ud835\udc25</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>k</mi></msub></mpadded><mo>\u2062</mo><munder><munder accentunder=\"true\"><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\">\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></munder></mpadded><mo movablelimits=\"false\">\u2062</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\">=</mo><mi>k</mi></mrow></msub><mo movablelimits=\"false\">\u2062</mo><mrow><mo movablelimits=\"false\" stretchy=\"false\">(</mo><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">)</mo></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mrow><mi/><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>k</mi></mrow><mo>\u2209</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi><mo rspace=\"5.8pt\">,</mo><mrow><mi>(</mi><mo>\u2062</mo><mtext mathvariant=\"italic\"><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_font_italic ltx_ref_self\" style=\"font-size:70%;\"><span class=\"ltx_text ltx_ref_tag\">38</span></span></mtext><mo>\u2062</mo><mi>)</mi></mrow></mrow></mrow></mrow></munder></mrow><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></munder></mpadded><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\uddb2</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06180.tex", "nexttext": "\ni.e.~the theorem holds for ${\\mathsf{S}}$.\nBy induction, the theorem holds for all nodes. \n \\hfill $\\qed$\n\n\n\n\n\\ifCLASSOPTIONcompsoc\n  \n  \\section*{Acknowledgments}\n\\else\n  \n  \\section*{Acknowledgment}\n\\fi\n\nThe authors would like to thank...\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{bibliography}\n\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{RobertPeharz}}]{Robert Peharz}\nreceived his MSc degree in Computer Engineering and his Ph.D degree in Electrical Engineering from Graz University of Technology.\nHis main research interest lies in machine learning, in particular probabilistic modeling, with applications to signal processing,\nspeech and audio processing, and computer vision.\nCurrently, he is with the research unit iDN -- interdisciplinary developmental neuroscience -- at the Medical University of \nGraz, applying machine learning techniques to detect early markers of neurological conditions in infants and adults. \nHe is funded by the BioTechMed-Graz cooperation, an interdisciplinary network of the 3 major universities in Graz with a focus \non basic bio-medical research, technological development and medical applications.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{RobGens}}]{Robert Gens}\n received the S.B. degree in electrical engineering and computer science from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 2009 and the M.Sc. degree in computer science and engineering from the University of Washington, Seattle, WA, USA, in 2012.\nDuring the Summer of 2014, he was a Research Intern at Microsoft Research, Redmond, WA, USA.  \nHe is currently a Ph.D. student of computer science and engineering at the University of Washington, Seattle, WA, USA.  \nHe is supported by the 2014 Google Ph.D. Fellowship in Deep Learning.\nMr. Gens was the recipient of an Outstanding Student Paper Award at the Neural Information Processing Systems conference in 2012.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=2cm]{FranzPernkopf}}]{Franz Pernkopf}\nreceived his MSc (Dipl. Ing.) degree in Electrical Engineering at Graz\nUniversity of Technology, Austria, in summer\n1999. He earned a Ph.D degree from the University of Leoben, Austria, in\n2002. In 2002 he was awarded the Erwin Schr\\\"{o}dinger Fellowship. He\nwas a Research Associate in the Department of Electrical Engineering at\nthe University of Washington, Seattle, from\n2004 to 2006. Currently, he is Associate Professor at the Laboratory of\nSignal Processing and Speech Communication, Graz University\nof Technology, Austria. His research interests include machine learning,\ndiscriminative learning, graphical models, feature selection, finite\nmixture models, and image- and speech processing applications.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=2cm]{PedroDomingos}}]{Pedro Domingos}\nis a professor of computer science at the University of Washington and the author of \u00e2\u0080\u009cThe Master Algorithm\u00e2\u0080\u009d. He is a winner of the SIGKDD Innovation Award, the highest honor in data science. He is a Fellow of the Association for the Advancement of Artificial Intelligence, and has received a Fulbright Scholarship, a Sloan Fellowship, the National Science Foundation\u00e2\u0080\u0099s CAREER Award, and numerous best paper awards. He received his Ph.D. from the University of California at Irvine and is the author or co-author of over 200 technical publications. He has held visiting positions at Stanford, Carnegie Mellon, and MIT. He co-founded the International Machine Learning Society in 2001. His research spans a wide variety of topics in machine learning, artificial intelligence, and data science, including scaling learning algorithms to big data, maximizing word of mouth in social networks, unifying logic and probability, and deep learning.\n\\end{IEEEbiography}\n\n\n\n\n", "itemtype": "equation", "pos": 112034, "prevtext": "\ni.e.~the theorem holds for twin sum nodes.\nNow assume that $\\hat{{\\mathsf{S}}}$ corresponds to a (non-twin) sum node ${\\mathsf{S}}$.\nThe children of ${\\mathsf{S}}$ are the links ${\\mathsf{P}}^k_{\\mathsf{S}} = {\\lambda_{{Z_{\\mathsf{S}}}={k}}} \\times {\\mathsf{C}}^k_{\\mathsf{S}}$, $k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})$.\n${\\mathsf{P}}^k_{\\mathsf{S}}$ and ${\\mathsf{C}}^k_{\\mathsf{S}}$ correspond to $\\hat{{\\mathsf{P}}}^k_{\\hat{{\\mathsf{S}}}}$ and $\\hat{{\\mathsf{C}}}^k_{\\hat{{\\mathsf{S}}}}$ in $\\hat{{\\ensuremath{\\mathcal{S}}}}$, respectively.\nLet ${w}_k := {w}_{{{\\mathsf{S}}}, {{\\mathsf{P}}}^k_{{{\\mathsf{S}}}}}$.\nWhen we assume that the theorem holds for all $\\hat{{\\mathsf{C}}}^k_{\\hat{{\\mathsf{S}}}}$, then it follows that\n\n", "index": 63, "text": "\\begin{align*}\n\\label{eq:maximizingsumstart}\n \\hat{{\\mathsf{S}}}({\\bm{{\\mathcal{X}}}}) \n&= \\underset{k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})} \\max ~ {w}_k \\, \\hat{{\\mathsf{P}}}^k_{\\hat{{\\mathsf{S}}}}({\\bm{{\\mathcal{X}}}}) \\\\\n&= \\underset{k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})} \\max ~ {w}_k \\, \\max_{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}} \\, {\\mathsf{P}}^k_{{{\\mathsf{S}}}}({\\ensuremath{\\mathbf{x}}}) \\\\\n&= \\underset{k \\in {\\ensuremath{\\mathbf{val}}}(Z_{\\mathsf{S}})} \\max ~ {w}_{k} \\, \n\\underbrace{ \\underset{z_{\\mathsf{S}} \\in {{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}}}]}}} {\\max} ~ {\\lambda_{{Z_{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}}) }_{=0, \\text{ for } k \\notin {{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}}}]}},~ \\eqref{eq:maximizeInd}}\n\\underset{{\\ensuremath{\\mathbf{y}}} \\in {{{\\bm{{\\mathcal{X}}}}}{[{{\\ensuremath{\\mathbf{sc}}} \\left({{\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}} \\right)}]}}} \\max {{\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}}({\\ensuremath{\\mathbf{y}}}) \n\\\\\n&=\n\\underset{k \\in {{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}}}]}}} {\\max} \\,\n\\underset{{\\ensuremath{\\mathbf{y}}} \\in {{{\\bm{{\\mathcal{X}}}}}{[{{\\ensuremath{\\mathbf{sc}}} \\left({{\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}} \\right)}]}}}{\\max}\n{w}_{k} \\,\n{{\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}}({\\ensuremath{\\mathbf{y}}}) \n= \\underset{{\\ensuremath{\\mathbf{x}}} \\in {\\bm{{\\mathcal{X}}}}} \\max ~ {\\mathsf{S}}({\\ensuremath{\\mathbf{x}}}),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\mathsf{S}}}({\\bm{{\\mathcal{X}}}})\" display=\"inline\"><mrow><mover accent=\"true\"><mi>\ud835\uddb2</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{\\mathbf{val}}(Z_{\\mathsf{S}})}{\\max}~{}{w}_{k}\\,%&#10;\\hat{{\\mathsf{P}}}^{k}_{\\hat{{\\mathsf{S}}}}({\\bm{{\\mathcal{X}}}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi>\ud835\udc2f\ud835\udc1a\ud835\udc25</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>\ud835\uddaf</mi><mo stretchy=\"false\">^</mo></mover><mover accent=\"true\"><mi>\ud835\uddb2</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{\\mathbf{val}}(Z_{\\mathsf{S}})}{\\max}~{}{w}_{k}\\,%&#10;\\max_{{\\mathbf{x}}\\in{\\bm{{\\mathcal{X}}}}}\\,{\\mathsf{P}}^{k}_{{{\\mathsf{S}}}}(%&#10;{\\mathbf{x}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi>\ud835\udc2f\ud835\udc1a\ud835\udc25</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi></mrow></munder></mpadded><mo>\u2061</mo><msubsup><mi>\ud835\uddaf</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{\\mathbf{val}}(Z_{\\mathsf{S}})}{\\max}~{}{w}_{k}\\,%&#10;\\underbrace{\\underset{z_{\\mathsf{S}}\\in{{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}%&#10;}}]}}}{\\max}~{}{\\lambda_{{Z_{\\mathsf{S}}}={k}}}(z_{\\mathsf{S}})}_{=0,\\text{ %&#10;for }k\\notin{{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}}}]}},~{}\\eqref{eq:%&#10;maximizeInd}}\\underset{{\\mathbf{y}}\\in{{{\\bm{{\\mathcal{X}}}}}{[{{\\mathbf{sc}}%&#10;\\left({{\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}}\\right)}]}}}{\\max}{{\\mathsf{C}}}^{k}_%&#10;{{{\\mathsf{S}}}}({\\mathbf{y}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi>\ud835\udc2f\ud835\udc1a\ud835\udc25</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><munder><munder accentunder=\"true\"><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\">\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo movablelimits=\"false\">\u2062</mo><mrow><mo movablelimits=\"false\" stretchy=\"false\">[</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">]</mo></mrow></mrow></mrow></munder></mpadded><mo movablelimits=\"false\">\u2062</mo><msub><mi>\u03bb</mi><mrow><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\">=</mo><mi>k</mi></mrow></msub><mo movablelimits=\"false\">\u2062</mo><mrow><mo movablelimits=\"false\" stretchy=\"false\">(</mo><msub><mi>z</mi><mi>\ud835\uddb2</mi></msub><mo movablelimits=\"false\" stretchy=\"false\">)</mo></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mrow><mi/><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>k</mi></mrow><mo>\u2209</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mi>(</mi><mo>\u2062</mo><mtext mathvariant=\"italic\"><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_font_italic ltx_ref_self\" style=\"font-size:70%;\"><span class=\"ltx_text ltx_ref_tag\">38</span></span></mtext><mo>\u2062</mo><mi>)</mi></mrow></mrow></mrow></mrow></munder><mo>\u2062</mo><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc32</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></munder><mo>\u2062</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\underset{k\\in{{{\\bm{{\\mathcal{X}}}}}{[{Z_{\\mathsf{S}}}]}}}{\\max%&#10;}\\,\\underset{{\\mathbf{y}}\\in{{{\\bm{{\\mathcal{X}}}}}{[{{\\mathbf{sc}}\\left({{%&#10;\\mathsf{C}}}^{k}_{{{\\mathsf{S}}}}\\right)}]}}}{\\max}{w}_{k}\\,{{\\mathsf{C}}}^{k}%&#10;_{{{\\mathsf{S}}}}({\\mathbf{y}})=\\underset{{\\mathbf{x}}\\in{\\bm{{\\mathcal{X}}}}}%&#10;{\\max}~{}{\\mathsf{S}}({\\mathbf{x}}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>k</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Z</mi><mi>\ud835\uddb2</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></munder></mpadded><mo>\u2062</mo><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc32</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>\ud835\udc2c\ud835\udc1c</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></munder><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msub><mi>w</mi><mi>k</mi></msub></mpadded><mo>\u2062</mo><msubsup><mi>\ud835\udda2</mi><mi>\ud835\uddb2</mi><mi>k</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+3.3pt\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>\ud835\udc31</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udce7</mi></mrow></munder></mpadded><mo>\u2062</mo><mi>\ud835\uddb2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]