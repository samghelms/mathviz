[{"file": "1601.03073.tex", "nexttext": "\nwhere $r_{t+1+k}$ are the future rewards that one would obtain by choosing to play uniquely the $i$-th arm up to the stopping time $t+\\tau$. \nThe brackets in \\eqref{eq:Gittins} denote the expectations of future success based on the posterior distribution defined by the past outcomes $w_i$ and $n_i$ (see \\eqref{eq:posterior_beta}).  Finally, the sup in \\eqref{eq:Gittins} is taken over future stopping times, i.e. decisions that interrupt the game based only on information obtained up to the stopping time. In other words, the Gittins index \\eqref{eq:Gittins} yields the expected rate of future rewards for the $i$-th arm, given \nits past number of plays $n_i$ and wins $w_i$.\nWhile \\eqref{eq:Gittins} is the only expression consistent with an index policy (see Chap. 2 in \\cite{gittins}), the existence of an index policy itself is remarkable, and it is specific to the discounted formulation. The calculation of the Gittins index is usually done via dynamic programming \\cite{gittins}. However, the exponentially growing \nnumber of possible paths makes the problem intractable as the discount factor $\\gamma$ approaches unity.\n\nThe Lai-Robbins \\cite{lairob} lower bound on the expected number of plays of suboptimal arms reads\\,: \n\n", "itemtype": "equation", "pos": 8536, "prevtext": "\n\\nocite{*}\n\n\n\n\n\n\n\n\n\\title{Infomax strategies for an optimal balance between exploration and exploitation}\n\\author{Gautam Reddy}\n\\affiliation{University of California San Diego, Department of Physics, La Jolla, CA 92093 USA}\n\\author{Antonio Celani}\n\\affiliation{The Abdus Salam International Centre for Theoretical Physics (ICTP), Strada Costiera 11, I-34014 - Trieste, Italy}\n\\author{Massimo Vergassola}\n\\affiliation{University of California San Diego, Department of Physics, La Jolla, CA 92093 USA}\n\n\\date{\\today}\n\n\\begin{abstract}\nProper balance between exploitation and exploration is what makes good decisions, which achieve high rewards like payoff or evolutionary fitness. The Infomax principle postulates that maximization of information directs the function of diverse systems, from living systems to artificial neural networks. While specific applications are successful, the validity of information as a proxy for reward remains unclear. Here, we consider the multi-armed bandit decision problem, which features arms (slot-machines) of unknown probabilities of success and a player trying to maximize cumulative payoff by choosing the sequence of arms to play. We show that an Infomax strategy (Info-p) which optimally gathers information on the highest mean reward among the arms, saturates known optimal bounds and compares favorably to existing policies. The highest mean reward considered by Info-p is not the quantity actually needed for the choice of the arm to play, yet it allows for optimal tradeoffs between exploration and exploitation. \n\\end{abstract}\n\n\n\\pacs{}\n\n\n\n\n\\maketitle\n\n\n\n\\section{Introduction}\nShannon's theory of information deliberately leaves aside the meaning of \nmessages and focuses on their statistical properties \\cite{Shannon48}. \n\n\n\n\nThis standpoint is crucial for the universality of the theory, as witnessed by its wide \nrange of applications in communication, computation and learning \\cite{Gallager,Mezard,MacKay}. \n\nBiological and economic sciences feature \nnatural measures of ``meaning'', i.e. evolutionary fitness and payoffs. The relation between payoffs and information was first addressed by Kelly for his model of horse race gambling \\cite{Kelly}, where information on the outcome \nof the race provides\n a bound on the increment in the doubling rate of returns. \n \n\nThe question was further developed and applied to portfolio management in Refs.~\\cite{Howard,Barron,Cover}. Kelly's horse race appears in some aspects of evolutionary biology as well. There, the reward function is the population growth rate and information \nrefers to the state of the environment \\cite{Bergstrom,Kussell,Donaldson-Matasci,Rivoire,Bialek}. \n\n\nNeurobiology is the field where information theory is arguably the most popular in biological sciences. Barlow's efficient coding  \\cite{Barlow61} postulated that \nearly neural sensory layers efficiently represent environmental information, i.e. their evolutionary fitness is proportional to \ntheir efficiency in the transmission of information from the environment to higher parts of the brain.  The hypothesis was \nspectacularly confirmed in the visual system \\cite{Laughlin89,Atick92,Rieke}, see also \\cite{Bialek,Dayan}. Similar ideas were recently introduced \nin cellular biology, namely to transduction pathways \\cite{Cheong}, their computational inference \\cite{Margolin} and evolution \\cite{Francois}, adaptation \\cite{Nemenman,Sharpee} and transcription regulation \\cite{Tkacik,Walczak}. \n\nThe catchy name Infomax for the maximization of information was introduced in \\cite{Linsker88}, where it was \napplied to the training of perceptual networks. Infomax was also later applied for blind separation and deconvolution \\cite{Bell95}. Infotaxis \\cite{infotaxis} used information as an orientation cue for searches aimed at locating sources of chemicals transported in a turbulent environment. For a recent review of information theory for decisions and actions, see \\cite{Tishby}. \n\n\nIt is usually the case that the more information is available, the better decisions or performances are, e.g. for the evolutionary model  discussed in \\cite{Bialek} the fitness increases with available information on the state of the environment. However, acquiring information has costs so that maximizing information does not generally lead to the best decisions. A first reason is the direct cost of acquiring and processing information, e.g. energy consumption costs\\,:\nrandom strategies can obviously be the most effective if those costs are too high. \nThe second, more subtle cost is that the choice of an action entails the exclusion of other possibilities. That calls for a balance between exploration and exploitation \\cite{Sutton}, which is what we shall discuss in the sequel. \n\nDecisions in fluctuating and unknown environments require a balance between two extremes\\,: exhaustive exploration of all available options {\\it vs} greedy exploitation of available information to maximize short-term return. \nWhile the first option seems wiser, it can still performs poorly as harvesting information does not coincide with maximizing reward. For instance, for the search problem of a source of chemicals discussed in \\cite{infotaxis}, the actual quantity to be minimized is the time of completion of the search. The information on the location of the source was found to be an efficient proxy, which replaces the daunting estimation of completion times by a much simpler statistic. When is such a replacement possible? More generally, when is the Infomax principle applicable and what are the situations, if any, where it is optimal? \n\nHere, we address the previous questions by considering a classical problem in statistical decision \ntheory: the multi-armed bandits. The model is the prototype of a broad class of sequential allocation problems that aim at optimally dividing resources to projects which yield benefits at a rate that depends on their degree of development. Among its many applications, we mention clinical trials, adaptive routing, job-scheduling, portfolio design and military logistics (see \\cite{berry,gittins} and references therein). Beside practical applications, the multi-armed bandit problem embodies the dilemma between exploitation and exploration mentioned above~\\cite{whittle}. The additional appeal of the model\nis that optimal strategies of decision are known, the so-called Gittins index \\cite{Gittins_paper}, as well as asymptotic bounds on maximal gains \\cite{lairob}. That allows to gauge the performance of the Infomax strategies developed below, Info-p and Info-id, and to provide a systematic assessment of cost and value of information. Finally, optimal bounds for the multi-armed bandit can be generalized to the broader class of problems encompassing Markov Decision Processes \\cite{Burnetas97}, suggesting that methods developed for the multi-armed bandit problem can have general relevance. To facilitate reading, we shall first briefly review known relevant results and then present our  own.\n\n\n\n\n\\section{The multi-armed bandit problem in a nutshell} At each discrete time, an agent chooses to pull one arm among $K$ available. The agent receives a reward for the chosen action, according either to some unknown distribution or to a known distribution  with unknown parameters. \nWe shall consider for concreteness the case of $K$ Bernoulli arms whose (unknown) probabilities of success are $p_1, p_2,\\dots ,p_K$, which are ordered for future convenience as $p_1 > p_2>\\dots >p_K$. After each play, a reward is paid, which is (rescaled to) unity upon winning and zero otherwise. The long-term goal is to find a strategy that maximizes the average cumulated reward or, equivalently, minimize the expected regret $R$\\,:\n\n$R(p_1,p_2,\\dots,p_k) = \\sum  \\overline{n}_i (p_1 - p_i)$,\n\nwhere $\\overline{n}_i$ is the expected number of plays of the $i$th arm. \n\nGittins index policy \\cite{Gittins_paper} applies to discounted rewards, i.e. maximizes the expected value of the sum $r_0 + \\gamma r_1  + \\gamma^2 r_2 + \\dots$ where $\\gamma$ is a discount factor between zero and one. Even though the total number of steps is infinite, the discount factor introduces an effective horizon $\\propto (1 - \\gamma)^{-1}$.  For this formulation, Gittins \\cite{Gittins_paper} showed that the optimal strategy is an index policy, i.e. for each arm $i$, one computes an index independent of all other arms, and then plays the arm with the highest index. The expression of the Gittins \nindex $\\nu_i$ for the $i$-th arm at time $t$ is\n\n", "index": 1, "text": "\\begin{equation}\n\\nu_{i}(w_i,n_i)={\\rm sup}_{\\tau>0}\\frac{\\langle\\sum_{k=0}^{\\tau-1} \\gamma^k r_{t+k+1}\\rangle}\n{\\langle\\sum_{k=0}^{\\tau-1} \\gamma^k\\rangle}\\,,\n\\label{eq:Gittins}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\nu_{i}(w_{i},n_{i})={\\rm sup}_{\\tau&gt;0}\\frac{\\langle\\sum_{k=0}^{\\tau-1}\\gamma^%&#10;{k}r_{t+k+1}\\rangle}{\\langle\\sum_{k=0}^{\\tau-1}\\gamma^{k}\\rangle}\\,,\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03bd</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo><msub><mi>n</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>sup</mi><mrow><mi>\u03c4</mi><mo>&gt;</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>\u03c4</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><msup><mi>\u03b3</mi><mi>k</mi></msup><mo>\u2062</mo><msub><mi>r</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>\u03c4</mi><mo>-</mo><mn>1</mn></mrow></msubsup><msup><mi>\u03b3</mi><mi>k</mi></msup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mfrac></mpadded></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe bound is generally valid when the number of plays $n$ is large and it does not involve any discount. In \\eqref{eq:lairob}, $i \\ne 1$ and $D(p, q)$ is the Kullback-Leibler relative entropy, that is the standard measure of divergence between two probability \ndistributions \\cite{Cover}. Specifically, $D(p,q) = p\\ln \\frac{p}{q} + (1-p)\\ln \\frac{1-p}{1-q}$ for two Bernoulli distributions parameterized by $p$ and $q$. \nThe closer the two probabilities $p_1$ and $p_i$ are, the larger is the constant in \\eqref{eq:lairob} and $ \\overline{n}_i \\propto \\ln n/\\left(p_1-p_i\\right)^2$ as $p_i\\to p_1$. Strategies that attain the bound \\eqref{eq:lairob} are called asymptotically optimal.\n\n\\section{Results}\n\nHereafter, we introduce two Infomax strategies, Info-p and Info-id. \n\nInfo-p greedily acquires information on the estimated highest success probability among the arms of the bandit. We show below that Info-p saturates the bound \\eqref{eq:lairob}, i.e. it is asymptotically optimal. Conversely, Info-id gathers information about the identity of the best arm. While Info-p leads to optimal payoffs,  Info-id is shown below to yield an optimal rate of acquisition of information on the identity of the best arm but suboptimal payoffs.\n\n\\subsection{Info-p}Unless specified otherwise, we discuss for simplicity a two-armed bandit with success probabilities $p_1>p_2$. Results are easily \ngeneralized to $K$ arms. The probability of success for the $i$th arm, as estimated from a sample of plays,\nis denoted by $\\pi_i$. \n\nIts posterior distribution $P_i(\\pi_i)$ after $n_i$ plays and $w_i$ wins reads (see, e.g., \\cite{MacKay})\\,:\n\n", "itemtype": "equation", "pos": 9965, "prevtext": "\nwhere $r_{t+1+k}$ are the future rewards that one would obtain by choosing to play uniquely the $i$-th arm up to the stopping time $t+\\tau$. \nThe brackets in \\eqref{eq:Gittins} denote the expectations of future success based on the posterior distribution defined by the past outcomes $w_i$ and $n_i$ (see \\eqref{eq:posterior_beta}).  Finally, the sup in \\eqref{eq:Gittins} is taken over future stopping times, i.e. decisions that interrupt the game based only on information obtained up to the stopping time. In other words, the Gittins index \\eqref{eq:Gittins} yields the expected rate of future rewards for the $i$-th arm, given \nits past number of plays $n_i$ and wins $w_i$.\nWhile \\eqref{eq:Gittins} is the only expression consistent with an index policy (see Chap. 2 in \\cite{gittins}), the existence of an index policy itself is remarkable, and it is specific to the discounted formulation. The calculation of the Gittins index is usually done via dynamic programming \\cite{gittins}. However, the exponentially growing \nnumber of possible paths makes the problem intractable as the discount factor $\\gamma$ approaches unity.\n\nThe Lai-Robbins \\cite{lairob} lower bound on the expected number of plays of suboptimal arms reads\\,: \n\n", "index": 3, "text": "\\begin{equation}\\label{eq:lairob}\n\\overline{n}_i \\ge \\frac{\\ln n }{D(p_i, p_1)} + \\text{terms of lower order in}\\,\\, n\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\overline{n}_{i}\\geq\\frac{\\ln n}{D(p_{i},p_{1})}+\\text{terms of lower order in%&#10;}\\,\\,n\\,.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>n</mi><mo>\u00af</mo></mover><mi>i</mi></msub><mo>\u2265</mo><mrow><mfrac><mrow><mi>ln</mi><mo>\u2061</mo><mi>n</mi></mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>+</mo><mrow><mpadded width=\"+3.4pt\"><mtext>terms of lower order in</mtext></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>n</mi></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $B$ is the Euler $\\beta$-function. In \\eqref{eq:posterior_beta} we assumed a uniform prior\\,; a different prior requires minor modifications and does not affect subsequent results.  \nWe are interested in the distribution of $\\pi_{\\max} = \\max_i \\pi_i$, i.e. the largest success probability among the arms of the bandit. The probability density $\\rho(\\pi_{\\max})$ is the sum of the contributions by each arm, weighted by the probability for that arm to be the best:\n\n", "itemtype": "equation", "pos": 11732, "prevtext": "\nThe bound is generally valid when the number of plays $n$ is large and it does not involve any discount. In \\eqref{eq:lairob}, $i \\ne 1$ and $D(p, q)$ is the Kullback-Leibler relative entropy, that is the standard measure of divergence between two probability \ndistributions \\cite{Cover}. Specifically, $D(p,q) = p\\ln \\frac{p}{q} + (1-p)\\ln \\frac{1-p}{1-q}$ for two Bernoulli distributions parameterized by $p$ and $q$. \nThe closer the two probabilities $p_1$ and $p_i$ are, the larger is the constant in \\eqref{eq:lairob} and $ \\overline{n}_i \\propto \\ln n/\\left(p_1-p_i\\right)^2$ as $p_i\\to p_1$. Strategies that attain the bound \\eqref{eq:lairob} are called asymptotically optimal.\n\n\\section{Results}\n\nHereafter, we introduce two Infomax strategies, Info-p and Info-id. \n\nInfo-p greedily acquires information on the estimated highest success probability among the arms of the bandit. We show below that Info-p saturates the bound \\eqref{eq:lairob}, i.e. it is asymptotically optimal. Conversely, Info-id gathers information about the identity of the best arm. While Info-p leads to optimal payoffs,  Info-id is shown below to yield an optimal rate of acquisition of information on the identity of the best arm but suboptimal payoffs.\n\n\\subsection{Info-p}Unless specified otherwise, we discuss for simplicity a two-armed bandit with success probabilities $p_1>p_2$. Results are easily \ngeneralized to $K$ arms. The probability of success for the $i$th arm, as estimated from a sample of plays,\nis denoted by $\\pi_i$. \n\nIts posterior distribution $P_i(\\pi_i)$ after $n_i$ plays and $w_i$ wins reads (see, e.g., \\cite{MacKay})\\,:\n\n", "index": 5, "text": "\\begin{equation}\nP_i(\\pi_i) = \\frac{\\pi_i^{w_i} (1-\\pi_i)^{n_i-w_i}}{B(w_i + 1, n_i- w_i + 1)}\\,,\n\\label{eq:posterior_beta}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"P_{i}(\\pi_{i})=\\frac{\\pi_{i}^{w_{i}}(1-\\pi_{i})^{n_{i}-w_{i}}}{B(w_{i}+1,n_{i}%&#10;-w_{i}+1)}\\,,\" display=\"block\"><mrow><mrow><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><msubsup><mi>\u03c0</mi><mi>i</mi><msub><mi>w</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>-</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><mi>B</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>-</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nFig.~\\ref{fig:infop} shows the posterior distributions $P_1(\\pi_1)$ and $P_2(\\pi_2)$ when \nthe number of plays $n$ is large and $n \\simeq n_1 \\gg n_2$. By the law of large numbers, the sample means $\\hat{\\pi}_i=w_i/n_i$ \nof the $\\pi_i$'s converge to their respective values $p_i$\nin the limit of large $n$. It follows that typically $\\hat{\\pi}_1 > \\hat{\\pi}_2$, as in Fig.~\\ref{fig:infop}. The distribution $\\rho(\\pi_{\\max})$ matches to a large extent the first term in the right hand side of \\eqref{eq:rho} except at the right tail, where  the contribution by $\\pi_2$ dominates as $n_2\\ll n_1$. The right tail corresponds to the unlikely event that \nthe inferior sample mean $\\hat{\\pi}_2$ is due to bad luck. Large deviations theory (see \\cite{Cover}) ensures that the probability for $\\hat\\pi_2$ to be generated by a true probability of success $>p_1$, is exponentially small in $n_2$, as we discuss below. \n\nThe differential entropy of the continuous distribution $\\rho(\\pi_{\\max})$ is $H(\\pi_{\\max}) = -\\int \\rho(p) \\ln \\rho(p) dp$ -- we shall be interested in the increments of the entropy so that normalization (see Chap. 8 in \\cite{Cover}) is not an issue here. \nThe Info-p strategy chooses the arm which maximizes the expected reduction of entropy  $H$. Specifically, \nthe expected reduction $\\langle \\Delta H \\rangle_i$ upon playing the $i$th arm with the posterior $P_i$ given by \\eqref{eq:posterior_beta} is\\,: \n\\begin{eqnarray}\n&\\langle \\Delta H \\rangle_i = \\text{Pr}(\\text{0 observed} |P_i) \\times \\Delta H(\\pi_{\\max}|\\text{0 observed})+\\nonumber \\\\ & \\text{Pr}(\\text{1 observed} | P_i) \\times \\Delta H(\\pi_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltaH}\n\\end{eqnarray}\nwhere $\\text{Pr}(X\\, \\text{observed} | P_i) = \\int P_i(p) \\Pr(X|p_i = p)dp$ is the likelihood of $X=1/0$, which denote win/loss, respectively. The increments $\\Delta H(\\pi_{\\max}|\\text{X observed})$ are calculated by updating the posterior \\eqref{eq:posterior_beta} appropriately, e.g. if $X=1$ then $n_i\\mapsto n_i+1$ and $w_i\\mapsto w_i+1$. The corresponding distribution $\\rho(\\pi_{\\max})$ is then obtained using \\eqref{eq:rho} and the increment of the entropy is finally calculated using the definition of $H(\\pi_{\\max})$  above.\n\nThe first arm of the bandit typically gives the dominant contribution to the entropy and Info-p plays it most frequently. However, as $n_1$ increases, the expected variation \\eqref{eq:DeltaH} of the first arm diminishes and the second arm is eventually played, as we proceed to discuss analytically and numerically.\n\n\\begin{figure}\n\\begin{center}\t\n\\includegraphics[width=.4\\textwidth]{betapdf_jsp.eps}\n\\caption{The posterior distributions $P$ for the estimated probabilities of success $\\pi_1$, $\\pi_2$ of a two-armed bandit and the corresponding distribution of $\\pi_{\\max}={\\max}_i\\pi_i$. The total number of plays is $n = 1000$ and the number of plays of the suboptimal arm $n_2 = 20$. By the law of large numbers, the two distributions (in red for $\\pi_1$ and blue for $\\pi_2$) \nare typically centered around their respective true values of the probabilities $p_1=0.9$ and $p_2=0.8$. \nThe core of the two distributions is Gaussian by the central limit theorem, while far tails are controlled by large deviations theory \\cite{Cover}. \nThe resulting distribution of $\\pi_{\\max}$ (in cyan) has a Gaussian bulk and a right tail that is controlled by the suboptimal arm. The tail captures the probability of misclassifying the order of the arms, as explained in the text.}\\label{fig:infop}\n\\end{center}\n\\end{figure}\n\n\\subsection{Optimality of Info-p}  In the region around the sample mean $\\hat{\\pi}_1\\simeq p_1$, the distribution $\\rho(\\pi_{\\max})$ in \\eqref{eq:rho} can be written as $\\rho(\\pi_{\\max}) \\simeq P_1(\\pi_{\\max}) \\text{Pr}(\\pi_2 < \\pi_{\\max}) \\approx  P_1(\\pi_{\\max})$ where  $P_1$ is approximately normal due to the central limit theorem, and its variance $\\sigma_1^2 = \\hat{\\pi}_1(1 - \\hat{\\pi}_1)/n_1$. \n\nThe right tail of $\\rho$ away from $\\hat{\\pi}_1$ is controlled by the theory of large deviations \\cite{Cover}. Specifically, the probability that a sequence of outcomes with sample mean $\\hat{\\pi}_2$ is generated by a distribution with parameter $p$ is $e^{-n_2D(\\hat{\\pi}_2,p)}$, where the Kullback-Leibler divergence $D$ was defined above, see \\eqref{eq:lairob}.  It follows from \\eqref{eq:rho} that \nthe right tail of $\\rho(\\pi_{\\max}) \\propto e^{-n_2D(\\hat{\\pi}_2, \\pi_{\\max})}\\text{Pr}(\\pi_1 < \\pi_{\\max}) \\approx e^{-n_2D(\\hat{\\pi}_2, \\pi_{\\max})}$ where the second approximation holds for $\\pi_{\\max}> \\hat{\\pi}_1$ as the distribution of $\\pi_1$ is strongly localized around its sample mean $ \\hat{\\pi}_1$. Ignoring subdominant terms, the contribution to the entropy $H(\\pi_{\\max})$ is \n$\\propto \\int_{\\hat{\\pi}_1}^1 n_2D(\\hat{\\pi}_2, p)e^{-n_2D(\\hat{\\pi}_2, p)}dp$. For moderately large $n_2$, the integral is dominated by the maximum of the exponential term and Laplace method gives\n$\\simeq n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)e^{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}$. \n\nAdding up the two previous contributions, we conclude that\n\n", "itemtype": "equation", "pos": 12342, "prevtext": "\nwhere $B$ is the Euler $\\beta$-function. In \\eqref{eq:posterior_beta} we assumed a uniform prior\\,; a different prior requires minor modifications and does not affect subsequent results.  \nWe are interested in the distribution of $\\pi_{\\max} = \\max_i \\pi_i$, i.e. the largest success probability among the arms of the bandit. The probability density $\\rho(\\pi_{\\max})$ is the sum of the contributions by each arm, weighted by the probability for that arm to be the best:\n\n", "index": 7, "text": "\\begin{equation}\n\\label{eq:rho}\n\\rho(\\pi_{\\max}) = P_1(\\pi_{\\max}) \\int_0^{\\pi_{\\max}} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!P_2(p) dp + P_2(\\pi_{\\max}) \\int_0^{\\pi_{\\max}} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!P_1(p) dp\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\rho(\\pi_{\\max})=P_{1}(\\pi_{\\max})\\int_{0}^{\\pi_{\\max}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!P%&#10;_{2}(p)dp+P_{2}(\\pi_{\\max})\\int_{0}^{\\pi_{\\max}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!P_{1}(p)%&#10;dp\\,.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mpadded width=\"-18.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><msub><mi>\u03c0</mi><mi>max</mi></msub></msubsup></mpadded><mrow><msub><mi>P</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>p</mi></mrow></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>P</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mpadded width=\"-18.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><msub><mi>\u03c0</mi><mi>max</mi></msub></msubsup></mpadded><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+1.7pt\"><mi>p</mi></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nwhere $A$ is a subdominant prefactor. The first term on the right-hand side of \\eqref{eq:infopent} becomes smaller as the first arm is played due to $\\sigma_1^2\\propto 1/n_1$, i.e. it is the exploitative term that selects the arm with the highest sample mean. The second, exploratory term in the right-hand side of \\eqref{eq:infopent} \naccounts for the probability of misclassification, and it reduces as $n_2$ increases.  \n\n\nThe neutral decision boundary, i.e. the boundary where the expected reduction of entropy on playing either arm is equal, is calculated for large $n$ by equating the variation of the two contributions in \\eqref{eq:infopent}. By using \n$n_1 \\simeq n$ and neglecting subdominant prefactors, we find\n\n", "itemtype": "equation", "pos": 17616, "prevtext": "\nFig.~\\ref{fig:infop} shows the posterior distributions $P_1(\\pi_1)$ and $P_2(\\pi_2)$ when \nthe number of plays $n$ is large and $n \\simeq n_1 \\gg n_2$. By the law of large numbers, the sample means $\\hat{\\pi}_i=w_i/n_i$ \nof the $\\pi_i$'s converge to their respective values $p_i$\nin the limit of large $n$. It follows that typically $\\hat{\\pi}_1 > \\hat{\\pi}_2$, as in Fig.~\\ref{fig:infop}. The distribution $\\rho(\\pi_{\\max})$ matches to a large extent the first term in the right hand side of \\eqref{eq:rho} except at the right tail, where  the contribution by $\\pi_2$ dominates as $n_2\\ll n_1$. The right tail corresponds to the unlikely event that \nthe inferior sample mean $\\hat{\\pi}_2$ is due to bad luck. Large deviations theory (see \\cite{Cover}) ensures that the probability for $\\hat\\pi_2$ to be generated by a true probability of success $>p_1$, is exponentially small in $n_2$, as we discuss below. \n\nThe differential entropy of the continuous distribution $\\rho(\\pi_{\\max})$ is $H(\\pi_{\\max}) = -\\int \\rho(p) \\ln \\rho(p) dp$ -- we shall be interested in the increments of the entropy so that normalization (see Chap. 8 in \\cite{Cover}) is not an issue here. \nThe Info-p strategy chooses the arm which maximizes the expected reduction of entropy  $H$. Specifically, \nthe expected reduction $\\langle \\Delta H \\rangle_i$ upon playing the $i$th arm with the posterior $P_i$ given by \\eqref{eq:posterior_beta} is\\,: \n\\begin{eqnarray}\n&\\langle \\Delta H \\rangle_i = \\text{Pr}(\\text{0 observed} |P_i) \\times \\Delta H(\\pi_{\\max}|\\text{0 observed})+\\nonumber \\\\ & \\text{Pr}(\\text{1 observed} | P_i) \\times \\Delta H(\\pi_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltaH}\n\\end{eqnarray}\nwhere $\\text{Pr}(X\\, \\text{observed} | P_i) = \\int P_i(p) \\Pr(X|p_i = p)dp$ is the likelihood of $X=1/0$, which denote win/loss, respectively. The increments $\\Delta H(\\pi_{\\max}|\\text{X observed})$ are calculated by updating the posterior \\eqref{eq:posterior_beta} appropriately, e.g. if $X=1$ then $n_i\\mapsto n_i+1$ and $w_i\\mapsto w_i+1$. The corresponding distribution $\\rho(\\pi_{\\max})$ is then obtained using \\eqref{eq:rho} and the increment of the entropy is finally calculated using the definition of $H(\\pi_{\\max})$  above.\n\nThe first arm of the bandit typically gives the dominant contribution to the entropy and Info-p plays it most frequently. However, as $n_1$ increases, the expected variation \\eqref{eq:DeltaH} of the first arm diminishes and the second arm is eventually played, as we proceed to discuss analytically and numerically.\n\n\\begin{figure}\n\\begin{center}\t\n\\includegraphics[width=.4\\textwidth]{betapdf_jsp.eps}\n\\caption{The posterior distributions $P$ for the estimated probabilities of success $\\pi_1$, $\\pi_2$ of a two-armed bandit and the corresponding distribution of $\\pi_{\\max}={\\max}_i\\pi_i$. The total number of plays is $n = 1000$ and the number of plays of the suboptimal arm $n_2 = 20$. By the law of large numbers, the two distributions (in red for $\\pi_1$ and blue for $\\pi_2$) \nare typically centered around their respective true values of the probabilities $p_1=0.9$ and $p_2=0.8$. \nThe core of the two distributions is Gaussian by the central limit theorem, while far tails are controlled by large deviations theory \\cite{Cover}. \nThe resulting distribution of $\\pi_{\\max}$ (in cyan) has a Gaussian bulk and a right tail that is controlled by the suboptimal arm. The tail captures the probability of misclassifying the order of the arms, as explained in the text.}\\label{fig:infop}\n\\end{center}\n\\end{figure}\n\n\\subsection{Optimality of Info-p}  In the region around the sample mean $\\hat{\\pi}_1\\simeq p_1$, the distribution $\\rho(\\pi_{\\max})$ in \\eqref{eq:rho} can be written as $\\rho(\\pi_{\\max}) \\simeq P_1(\\pi_{\\max}) \\text{Pr}(\\pi_2 < \\pi_{\\max}) \\approx  P_1(\\pi_{\\max})$ where  $P_1$ is approximately normal due to the central limit theorem, and its variance $\\sigma_1^2 = \\hat{\\pi}_1(1 - \\hat{\\pi}_1)/n_1$. \n\nThe right tail of $\\rho$ away from $\\hat{\\pi}_1$ is controlled by the theory of large deviations \\cite{Cover}. Specifically, the probability that a sequence of outcomes with sample mean $\\hat{\\pi}_2$ is generated by a distribution with parameter $p$ is $e^{-n_2D(\\hat{\\pi}_2,p)}$, where the Kullback-Leibler divergence $D$ was defined above, see \\eqref{eq:lairob}.  It follows from \\eqref{eq:rho} that \nthe right tail of $\\rho(\\pi_{\\max}) \\propto e^{-n_2D(\\hat{\\pi}_2, \\pi_{\\max})}\\text{Pr}(\\pi_1 < \\pi_{\\max}) \\approx e^{-n_2D(\\hat{\\pi}_2, \\pi_{\\max})}$ where the second approximation holds for $\\pi_{\\max}> \\hat{\\pi}_1$ as the distribution of $\\pi_1$ is strongly localized around its sample mean $ \\hat{\\pi}_1$. Ignoring subdominant terms, the contribution to the entropy $H(\\pi_{\\max})$ is \n$\\propto \\int_{\\hat{\\pi}_1}^1 n_2D(\\hat{\\pi}_2, p)e^{-n_2D(\\hat{\\pi}_2, p)}dp$. For moderately large $n_2$, the integral is dominated by the maximum of the exponential term and Laplace method gives\n$\\simeq n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)e^{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}$. \n\nAdding up the two previous contributions, we conclude that\n\n", "index": 9, "text": "\\begin{equation}\n\\label{eq:infopent}\nH(\\pi_{\\max}) \\approx \\frac{1}{2}\\ln 2\\pi e \\sigma_1^2 +  Ae^{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"H(\\pi_{\\max})\\approx\\frac{1}{2}\\ln 2\\pi e\\sigma_{1}^{2}+Ae^{-n_{2}D(\\hat{\\pi}_%&#10;{2},\\hat{\\pi}_{1})}\\,,\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>+</mo><mrow><mi>A</mi><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nIn the limit of large $n$, the sample means tend to their respective values $p$'s and \\eqref{eq:decisionbound} \ncoincides with the Lai-Robbins bound \\eqref{eq:lairob}. This establishes the optimality of Info-p, which we shall also verify numerically in the next Section.\n\nAsymptotic optimality is intuited as follows. The order between sample means, say $\\hat{\\pi}_2<\\hat{\\pi}_1$, might be due to fluctuations and  we ought to make sure that the true probabilities of success are not inverted, i.e. that $p_2<p_1$. The probability of inversion is $\\exp\\left[-n_2D(\\hat{\\pi_2},\\hat{\\pi}_1)\\right]$ by large-deviations theory \\cite{Cover}. The \nexponential dependence on $n_2$ pushes toward $n_2\\propto n$ whilst short-term reward pushes to play greedily the arm with the highest sample mean. The optimal trade-off is \ndictated by marginality of sampling\\,: \nthe number $n/n_2$ of possible stretches of size $n_2$ times the probability of inversion \nshould satisfy $n/n_2\\times \\exp\\left[-n_2D(\\hat{\\pi_2},\\hat{\\pi}_1)\\right]\\lesssim 1$. The dominant order of this expression yields the Lai-Robbins inequality \\eqref{eq:lairob} and the marginal case defines the Info-p decision boundary \\eqref{eq:decisionbound}.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{regret_jsp.eps}\n\\caption{Performance of Info-p and comparison with other strategies of decision. The two-armed bandit has $p_1=0.9$, $p_2=0.8$ as in Fig.~1.\nThe upper left panel shows points (blue dots) when Info-p played the second suboptimal arm. The plot  shows about 60,000 points cumulated over 250 realizations, each one of them lasting $n=10^9$ plays. The red line shows \na line of slope one, which corresponds to the Info-p decision boundary \\eqref{eq:decisionbound} in the asymptotic regime of large $n$. The upper right panel shows the comparison between the average regret obtained by Info-p and by the three Upper-Confidence Bound (UCB) strategies UCB-Tuned \\cite{auer}, UCB2 \\cite{auer}, KL-UCB \\cite{cappe}.  The \nUCB strategies exhibit logarithmic but suboptimal regrets, which are manifestly asymptotically bigger as compared to Info-p. The lower panels show a comparison between the average regret obtained by Info-p and known asymptotically optimal decision strategies (DMED  \\cite{honda}, Kelly's proportional betting \\cite{Kelly} and UCB index policies defined in \\cite{changlai}) discussed in the text.  Asymptotic optimality of all the strategies is visible in the left panel (the black line has the optimal slope $\\ln n/D(p_2,p_1)$ \nin \\eqref{eq:lairob}). In the right panel, we averaged over 25,000 statistical realizations and subtracted the dominant logarithmic term to evidence subdominant contributions\\,: Info-p compares favorably with other algorithms and even features the smallest regret at intermediate times.} \n\\label{fig:regret}\n\\end{center}\n\\end{figure*}\n\n\\subsection{Numerical simulations of Info-p}For our simulations, we chose a two-armed bandit with $p_1 = 0.9$, $p_2 = 0.8$. At every decision event, \nwe compute the expected variation in entropy and choose the arm to play as described in \\eqref{eq:DeltaH}. \n\n\n\n\n\n\nSubdominant corrections to the regret are $O(\\ln \\ln n)$ \\cite{lairob}. Consequently, clean data for the asymptotic regime require $n \\gtrsim 10^6$, which is computationally demanding due to the updates of the posteriors at every step. \n\nTo simulate the asymptotic regime, we developed an {\\em exact} numerical technique (see Appendix \\ref{a1}) that dramatically speeds up simulations. The logarithmic dependence in \\eqref{eq:lairob} implies that asymptotically optimal strategies play long stretches of the estimated best arm, punctuated by short stretches of suboptimal arms. We derive then a rigorous lower bound for the duration of the long stretches and generate a single random variable for the cumulated reward over the entire stretch. \n\nUsing the technique above, we verified that the decision boundary is indeed consistent with the optimality of Info-p\\,: Fig.~\\ref{fig:regret}A confirms that the points where Info-p chose the subdominant arm are below the predicted decision boundary  \\eqref{eq:decisionbound} and approach it as $n$ increases. \n\n\\subsection{Comparison between Info-p and other strategies of decision} The goal of this Section is to first briefly introduce state-of-the-art decision strategies whose regret increases logarithmically with $n$, and then compare them with Info-p.\n\nKelly's proportional betting \\cite{Kelly} (known as Thompson sampling \\cite{Kaufmann} in the machine learning community) is a randomized Bayesian strategy that plays arms with a probability proportional to their respective probability to be the best. Its asymptotic optimality was  recently proved in Ref.~\\cite{Kaufmann} (see also Appendix \\ref{a4}). \n\nUpper Confidence Bound (UCB) strategies are based on an index policy, like Gittins' index \\eqref{eq:Gittins}, yet the calculation of the index is vastly simplified. Specifically, UCBs are formed by inflating the sample mean estimate of the probability of success of an arm with an additional positive term that accounts for the uncertainty in that estimate.  \nA notable example is the UCB index $\\chi_i$ introduced in Ref.~\\cite{changlai}\\,: if the $i$th arm was played $n_i$ times and its sample mean is $\\hat{\\pi}_i$, the index $\\chi_i$ is defined via\\,:  \n$n_i D(\\hat{\\pi}_i,\\chi_i) = \\ln n/n_i + \\xi \\ln \\ln n/n_i$, with $\\chi_i > \\hat{\\pi}_i$. The constant $\\xi$ \ngeneralizes the value $\\xi=-1/2$ found by considering the Gittins' index \\eqref{eq:Gittins} for Gaussian rewards in the limit $\\gamma\\to 1$  \\cite{lai}. The class of models above is asymptotically optimal \\cite{lai,changlai}. The value of $\\xi$ is chosen empirically and controls subdominant terms. \n\nFigure~\\ref{fig:regret}B shows the comparison between Info-p and UCB-Tuned \\cite{auer}, UCB2 \\cite{auer}, KL-UCB \\cite{cappe}, which all \nexhibit logarithmic regret. However, their prefactor does not saturate the Lai-Robbins bound \\eqref{eq:lairob} and UCB regrets are  asymptotically bigger as compared to Info-p.\n\nFigures~\\ref{fig:regret}C-D present a comparison of the regret {\\it vs} $n$ for Info-p,  Kelly's proportional betting \\cite{Kelly}, the UCB strategy DMED \\cite{honda} and the UCBLai index policy \\cite{changlai} for various values of its free parameter $\\xi$. All the algorithms are asymptotically optimal and Info-p compares quite favorably with the others, especially at early and intermediate times when its regret remains below other curves. \n\n\\subsection{Information about the identity of the best arm}\n\nInfomax approaches can pursue information about diverse quantities. For multi-armed bandits, choosing which arm to play requires {\\it a priori} only the identity of the best arm and not its probability of success. It is then natural to investigate the alternative Infomax approach that maximizes the information gain about the \nidentity $b_{\\text{max}}$ of the best arm. This possibility, which was previously mentioned in Ref.~\\cite{Wyatt},  \nis analyzed in detail in the next Section. Here, we determine the maximum\npossible rate of information gain on $b_{\\text{max}}$. \n\nThe estimated probability for the  $i$-arm to be the best is denoted $q_i$. For two-armed bandits, $q_2+q_1=1$ and\n\n", "itemtype": "equation", "pos": 18489, "prevtext": "\n\nwhere $A$ is a subdominant prefactor. The first term on the right-hand side of \\eqref{eq:infopent} becomes smaller as the first arm is played due to $\\sigma_1^2\\propto 1/n_1$, i.e. it is the exploitative term that selects the arm with the highest sample mean. The second, exploratory term in the right-hand side of \\eqref{eq:infopent} \naccounts for the probability of misclassification, and it reduces as $n_2$ increases.  \n\n\nThe neutral decision boundary, i.e. the boundary where the expected reduction of entropy on playing either arm is equal, is calculated for large $n$ by equating the variation of the two contributions in \\eqref{eq:infopent}. By using \n$n_1 \\simeq n$ and neglecting subdominant prefactors, we find\n\n", "index": 11, "text": "\\begin{equation}\n\\ln n \\simeq n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1) + O(\\ln n_2)\\,.\n\\label{eq:decisionbound}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\ln n\\simeq n_{2}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})+O(\\ln n_{2})\\,.\" display=\"block\"><mrow><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>n</mi></mrow><mo>\u2243</mo><mrow><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe posterior distributions $P_i$ are given by \\eqref{eq:posterior_beta}. \n\nThe entropy of the unknown identity $b_{\\max}$ of the best arm is\n$H(b_{\\text{max}}) = -q_1\\ln q_1 - q_2\\ln q_2$. \nWe are interested in the asymptotic limit of $n_1$ and $n_2$ large. Sample means $\\hat{\\pi}_i = \\frac{w_i}{n_i} $ are then close to their true values $p_i$ and typically satisfy $\\hat{\\pi}_1 >  \\hat{\\pi}_2$. It follows that $q_1$ is close to unity and \n\n", "itemtype": "equation", "pos": 25885, "prevtext": "\nIn the limit of large $n$, the sample means tend to their respective values $p$'s and \\eqref{eq:decisionbound} \ncoincides with the Lai-Robbins bound \\eqref{eq:lairob}. This establishes the optimality of Info-p, which we shall also verify numerically in the next Section.\n\nAsymptotic optimality is intuited as follows. The order between sample means, say $\\hat{\\pi}_2<\\hat{\\pi}_1$, might be due to fluctuations and  we ought to make sure that the true probabilities of success are not inverted, i.e. that $p_2<p_1$. The probability of inversion is $\\exp\\left[-n_2D(\\hat{\\pi_2},\\hat{\\pi}_1)\\right]$ by large-deviations theory \\cite{Cover}. The \nexponential dependence on $n_2$ pushes toward $n_2\\propto n$ whilst short-term reward pushes to play greedily the arm with the highest sample mean. The optimal trade-off is \ndictated by marginality of sampling\\,: \nthe number $n/n_2$ of possible stretches of size $n_2$ times the probability of inversion \nshould satisfy $n/n_2\\times \\exp\\left[-n_2D(\\hat{\\pi_2},\\hat{\\pi}_1)\\right]\\lesssim 1$. The dominant order of this expression yields the Lai-Robbins inequality \\eqref{eq:lairob} and the marginal case defines the Info-p decision boundary \\eqref{eq:decisionbound}.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{regret_jsp.eps}\n\\caption{Performance of Info-p and comparison with other strategies of decision. The two-armed bandit has $p_1=0.9$, $p_2=0.8$ as in Fig.~1.\nThe upper left panel shows points (blue dots) when Info-p played the second suboptimal arm. The plot  shows about 60,000 points cumulated over 250 realizations, each one of them lasting $n=10^9$ plays. The red line shows \na line of slope one, which corresponds to the Info-p decision boundary \\eqref{eq:decisionbound} in the asymptotic regime of large $n$. The upper right panel shows the comparison between the average regret obtained by Info-p and by the three Upper-Confidence Bound (UCB) strategies UCB-Tuned \\cite{auer}, UCB2 \\cite{auer}, KL-UCB \\cite{cappe}.  The \nUCB strategies exhibit logarithmic but suboptimal regrets, which are manifestly asymptotically bigger as compared to Info-p. The lower panels show a comparison between the average regret obtained by Info-p and known asymptotically optimal decision strategies (DMED  \\cite{honda}, Kelly's proportional betting \\cite{Kelly} and UCB index policies defined in \\cite{changlai}) discussed in the text.  Asymptotic optimality of all the strategies is visible in the left panel (the black line has the optimal slope $\\ln n/D(p_2,p_1)$ \nin \\eqref{eq:lairob}). In the right panel, we averaged over 25,000 statistical realizations and subtracted the dominant logarithmic term to evidence subdominant contributions\\,: Info-p compares favorably with other algorithms and even features the smallest regret at intermediate times.} \n\\label{fig:regret}\n\\end{center}\n\\end{figure*}\n\n\\subsection{Numerical simulations of Info-p}For our simulations, we chose a two-armed bandit with $p_1 = 0.9$, $p_2 = 0.8$. At every decision event, \nwe compute the expected variation in entropy and choose the arm to play as described in \\eqref{eq:DeltaH}. \n\n\n\n\n\n\nSubdominant corrections to the regret are $O(\\ln \\ln n)$ \\cite{lairob}. Consequently, clean data for the asymptotic regime require $n \\gtrsim 10^6$, which is computationally demanding due to the updates of the posteriors at every step. \n\nTo simulate the asymptotic regime, we developed an {\\em exact} numerical technique (see Appendix \\ref{a1}) that dramatically speeds up simulations. The logarithmic dependence in \\eqref{eq:lairob} implies that asymptotically optimal strategies play long stretches of the estimated best arm, punctuated by short stretches of suboptimal arms. We derive then a rigorous lower bound for the duration of the long stretches and generate a single random variable for the cumulated reward over the entire stretch. \n\nUsing the technique above, we verified that the decision boundary is indeed consistent with the optimality of Info-p\\,: Fig.~\\ref{fig:regret}A confirms that the points where Info-p chose the subdominant arm are below the predicted decision boundary  \\eqref{eq:decisionbound} and approach it as $n$ increases. \n\n\\subsection{Comparison between Info-p and other strategies of decision} The goal of this Section is to first briefly introduce state-of-the-art decision strategies whose regret increases logarithmically with $n$, and then compare them with Info-p.\n\nKelly's proportional betting \\cite{Kelly} (known as Thompson sampling \\cite{Kaufmann} in the machine learning community) is a randomized Bayesian strategy that plays arms with a probability proportional to their respective probability to be the best. Its asymptotic optimality was  recently proved in Ref.~\\cite{Kaufmann} (see also Appendix \\ref{a4}). \n\nUpper Confidence Bound (UCB) strategies are based on an index policy, like Gittins' index \\eqref{eq:Gittins}, yet the calculation of the index is vastly simplified. Specifically, UCBs are formed by inflating the sample mean estimate of the probability of success of an arm with an additional positive term that accounts for the uncertainty in that estimate.  \nA notable example is the UCB index $\\chi_i$ introduced in Ref.~\\cite{changlai}\\,: if the $i$th arm was played $n_i$ times and its sample mean is $\\hat{\\pi}_i$, the index $\\chi_i$ is defined via\\,:  \n$n_i D(\\hat{\\pi}_i,\\chi_i) = \\ln n/n_i + \\xi \\ln \\ln n/n_i$, with $\\chi_i > \\hat{\\pi}_i$. The constant $\\xi$ \ngeneralizes the value $\\xi=-1/2$ found by considering the Gittins' index \\eqref{eq:Gittins} for Gaussian rewards in the limit $\\gamma\\to 1$  \\cite{lai}. The class of models above is asymptotically optimal \\cite{lai,changlai}. The value of $\\xi$ is chosen empirically and controls subdominant terms. \n\nFigure~\\ref{fig:regret}B shows the comparison between Info-p and UCB-Tuned \\cite{auer}, UCB2 \\cite{auer}, KL-UCB \\cite{cappe}, which all \nexhibit logarithmic regret. However, their prefactor does not saturate the Lai-Robbins bound \\eqref{eq:lairob} and UCB regrets are  asymptotically bigger as compared to Info-p.\n\nFigures~\\ref{fig:regret}C-D present a comparison of the regret {\\it vs} $n$ for Info-p,  Kelly's proportional betting \\cite{Kelly}, the UCB strategy DMED \\cite{honda} and the UCBLai index policy \\cite{changlai} for various values of its free parameter $\\xi$. All the algorithms are asymptotically optimal and Info-p compares quite favorably with the others, especially at early and intermediate times when its regret remains below other curves. \n\n\\subsection{Information about the identity of the best arm}\n\nInfomax approaches can pursue information about diverse quantities. For multi-armed bandits, choosing which arm to play requires {\\it a priori} only the identity of the best arm and not its probability of success. It is then natural to investigate the alternative Infomax approach that maximizes the information gain about the \nidentity $b_{\\text{max}}$ of the best arm. This possibility, which was previously mentioned in Ref.~\\cite{Wyatt},  \nis analyzed in detail in the next Section. Here, we determine the maximum\npossible rate of information gain on $b_{\\text{max}}$. \n\nThe estimated probability for the  $i$-arm to be the best is denoted $q_i$. For two-armed bandits, $q_2+q_1=1$ and\n\n", "index": 13, "text": "\\begin{equation}\nq_2 = \\int_0^1 P_1(p) dp \\int_p^{1} P_2(q) dq\\,.\n\\label{eq:q1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"q_{2}=\\int_{0}^{1}P_{1}(p)dp\\int_{p}^{1}P_{2}(q)dq\\,.\" display=\"block\"><mrow><mrow><msub><mi>q</mi><mn>2</mn></msub><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup><mrow><msub><mi>P</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>p</mi><mn>1</mn></msubsup><mrow><msub><mi>P</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mpadded width=\"+1.7pt\"><mi>q</mi></mpadded></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nThe integrals that define $q_2$ in \\eqref{eq:q1} have three contributions\\,:\n\n\\noindent (I) The region $p\\le\\hat{\\pi}_2$. There, we have $\\int_p^{1} P_2(q) dq\\sim 1$ and $P_1(p)\\sim \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)\\right]$ by large deviations theory \\cite{Cover}. Integrating over $p$ and using that the dominant contribution comes from \n$p\\simeq \\hat{\\pi}_2$, we obtain $\\exp\\left[-n_1D\\left(\\hat{\\pi}_1,\\hat{\\pi}_2\\right)\\right]$.\n\n\\noindent (II) The region of $p$'s between $\\hat{\\pi}_2$ and $\\hat{\\pi}_1$. Its contribution is $\\int \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)-n_2D\\left(\\hat{\\pi}_2,p\\right)\\right]\\,dp$ by large deviations theory. The integral can be calculated by Laplace method (see below) and we denote by $\\pi_s$ the point where the maximum of the exponent is achieved. \n\n\\noindent (III) Finally, the contribution from the rightmost region of $p$'s is dominated by $p\\simeq \\hat{\\pi}_1$ and reads $\\exp\\left[-n_2D\\left(\\hat{\\pi}_2,\\hat{\\pi}_1\\right)\\right]$.\n\n\\smallskip\nIn summary, we obtain\n\n", "itemtype": "equation", "pos": 26424, "prevtext": "\nThe posterior distributions $P_i$ are given by \\eqref{eq:posterior_beta}. \n\nThe entropy of the unknown identity $b_{\\max}$ of the best arm is\n$H(b_{\\text{max}}) = -q_1\\ln q_1 - q_2\\ln q_2$. \nWe are interested in the asymptotic limit of $n_1$ and $n_2$ large. Sample means $\\hat{\\pi}_i = \\frac{w_i}{n_i} $ are then close to their true values $p_i$ and typically satisfy $\\hat{\\pi}_1 >  \\hat{\\pi}_2$. It follows that $q_1$ is close to unity and \n\n", "index": 15, "text": "\\begin{equation}\nH(b_{\\text{max}}) \\sim -q_2\\ln q_2\\,.\n\\label{eq:Hbmax}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"H(b_{\\text{max}})\\sim-q_{2}\\ln q_{2}\\,.\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mtext>max</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mo>-</mo><mrow><msub><mi>q</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mpadded width=\"+1.7pt\"><msub><mi>q</mi><mn>2</mn></msub></mpadded></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $\\pi_s=\\left(n_1\\hat{\\pi}_1+n_2\\hat{\\pi}_2\\right)/n$ (see Appendix \\ref{a2}). \n\nTo minimize $\\ln q_2$ -- thereby achieving the maximum acquisition of information, see \\eqref{eq:Hbmax} -- we must  extremize with respect to $n_1$ and $n_2$. We show in Appendix \\ref{a2} that the dominant contribution comes \nfrom the second exponential term in \\eqref{eq:q2asymp}. The resulting extremum (with $n_1+n_2=n$) gives $D\\left(\\hat{\\pi}_1,\\pi_s\\right)=D\\left(\\hat{\\pi}_2,\\pi_s\\right)$. An important consequence of this equality is that $\\pi_s$ is at a finite distance from $\\hat{\\pi}_1$ and $\\hat{\\pi}_2$. It follows then from the expression \\eqref{eq:q2asymp} of $\\pi_s$ that $n_2\\propto n$, which is also confirmed by explicit expressions derived in the Appendix. \nAs for the fastest rate of decay of the average logarithm of the entropy, we obtain\\,:\n\n", "itemtype": "equation", "pos": 27536, "prevtext": "\n\nThe integrals that define $q_2$ in \\eqref{eq:q1} have three contributions\\,:\n\n\\noindent (I) The region $p\\le\\hat{\\pi}_2$. There, we have $\\int_p^{1} P_2(q) dq\\sim 1$ and $P_1(p)\\sim \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)\\right]$ by large deviations theory \\cite{Cover}. Integrating over $p$ and using that the dominant contribution comes from \n$p\\simeq \\hat{\\pi}_2$, we obtain $\\exp\\left[-n_1D\\left(\\hat{\\pi}_1,\\hat{\\pi}_2\\right)\\right]$.\n\n\\noindent (II) The region of $p$'s between $\\hat{\\pi}_2$ and $\\hat{\\pi}_1$. Its contribution is $\\int \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)-n_2D\\left(\\hat{\\pi}_2,p\\right)\\right]\\,dp$ by large deviations theory. The integral can be calculated by Laplace method (see below) and we denote by $\\pi_s$ the point where the maximum of the exponent is achieved. \n\n\\noindent (III) Finally, the contribution from the rightmost region of $p$'s is dominated by $p\\simeq \\hat{\\pi}_1$ and reads $\\exp\\left[-n_2D\\left(\\hat{\\pi}_2,\\hat{\\pi}_1\\right)\\right]$.\n\n\\smallskip\nIn summary, we obtain\n\n", "index": 17, "text": "\\begin{equation}\nq_2\\sim e^{-n_1D\\left(\\hat{\\pi}_1,\\hat{\\pi}_2\\right)}+e^{-n_1D\\left(\\hat{\\pi}_1,\\pi_s\\right)-n_2D\\left(\\hat{\\pi}_2,\\pi_s\\right)}+\ne^{-n_2D\\left(\\hat{\\pi}_2,\\hat{\\pi}_1\\right)}\\,,\n\\label{eq:q2asymp}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"q_{2}\\sim e^{-n_{1}D\\left(\\hat{\\pi}_{1},\\hat{\\pi}_{2}\\right)}+e^{-n_{1}D\\left(%&#10;\\hat{\\pi}_{1},\\pi_{s}\\right)-n_{2}D\\left(\\hat{\\pi}_{2},\\pi_{s}\\right)}+e^{-n_{%&#10;2}D\\left(\\hat{\\pi}_{2},\\hat{\\pi}_{1}\\right)}\\,,\" display=\"block\"><mrow><mrow><msub><mi>q</mi><mn>2</mn></msub><mo>\u223c</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>)</mo></mrow></mrow></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo>)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo>)</mo></mrow></mrow></mrow></msup><mo>+</mo><mpadded width=\"+1.7pt\"><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>)</mo></mrow></mrow></mrow></msup></mpadded></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $p_s$ is defined by the equality $D(p_1,p_s)=D(p_2,p_s)$.\n\n\n\\subsection{Info-id}The Info-Id algorithm is defined as the decision strategy that chooses the arm which maximizes the expected reduction of $\\ln H(b_{\\text{max}})$. Specifically, the expected reduction $\\langle \\Delta \\ln H\\rangle_i$ upon \nplaying the $i$-th arm with posterior $P_i$ is analogous to \\eqref{eq:DeltaH} with $\\Delta H(\\pi_{\\max})$ replaced by $\\Delta \\ln H(b_{\\text{max}})$.\nIncrements are calculated by updating the posterior as for \\eqref{eq:DeltaH}, by using \\eqref{eq:q1} and finally obtaining the increment using the definition of $H(b_{\\text{max}})$ above. This greedy, one-step-in-time procedure indeed achieves the fastest decrease \\eqref{eq:slopelogH}\nof $\\overline{\\ln H(b_{\\text{max}})}$ (see Fig.~\\ref{fig:cost}). Note the logarithm in the definition of Info-id\\,: \nmaximizing the expected reduction of $H(b_{\\text{max}})$ would not achieve \\eqref{eq:slopelogH} but a slower decay (see Appendix \\ref{a2}).\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{cost_jsp.eps}\n\\caption{The cost of information. A comparison of the entropy $H(b_{\\max})$ on the identity of the best arm in the bandit (left panel) and the corresponding regret (right panel) for Info-id (red curves and axes) and Info-p (blue curves and axes). The left panel is in log-log scale for the blue curve and lin-log for the red curve. The right panel is in log-lin scale for the blue curve and lin-lin for the red curve. Info-id achieves the fastest possible reduction of the entropy, as shown by the agreement with the optimal slope \\eqref{eq:slopelogH} (black solid line in the left panel). For Info-p, the decrease is much slower\\,: $\\propto 1/n$ (dashed  line). Conversely,  Info-id has a linear regret that largely exceeds the optimal Lai-Robbins bound \\eqref{eq:lairob} achieved by Info-p.}\n\\label{fig:cost}\n\\end{center}\n\\end{figure*}\n\n\\subsection{The cost and value of information} \nInformation and payoffs embody the two sides of the exploration/exploitation dilemma for multi-armed bandits.\nThe expressions \\eqref{eq:Hbmax}, \\eqref{eq:q2asymp} and \\eqref{eq:slopelogH} allow to quantify the trade-offs in the optimal behaviors achieved by Info-p and Info-id, respectively. \n\nThe three relations above show that $-\\overline{\\ln H(b_{\\text{max}})}\\propto n\\propto n_2$ for Info-id. Since \nregret is proportional to $n_2$, we conclude that the rate of decay $-\\overline{\\ln H(b_{\\text{max}})}$ is proportional \nto the average regret, i.e. the exponential rate  \\eqref{eq:slopelogH} implies a regret linear in $n$, as confirmed by numerical simulations (see Fig.~\\ref{fig:cost}). \n\n\nA very different trade-off underlies the Info-p optimal regret. Indeed, for $n_2\\propto \\ln n$, the dominant contribution in \\eqref{eq:q2asymp} comes from the last term and implies a power-law decay of the entropy. In particular, if the Lai-Robbins bound \\eqref{eq:lairob} is saturated then $\\overline{\\ln H(b_{\\text{max}})}\\sim -\\ln n$. \nThe information on the identity of the best arm is therefore reducing much more slowly as compared to Info-id.  \n\n\\smallskip\nThe behaviors above clearly illustrate the costs in regret of  reducing $H(b_{\\text{max}})$. \nHowever, information about $b_{\\text{max}}$ has a definite value that can be exploited to \nincrease payoffs. In particular, if we start playing with some {\\it a priori} information on the identity of the best arm, i.e. $H(b_{\\text{max}})=H_0<\\ln 2$, general distortion-type arguments \\cite{Cover,Bialek} suggest that payoffs should increase as $H_0$ reduces. \n\nWe quantify the value of information by measuring the variation in payoff as a function of $H_0$. To generate the initial {\\it a priori} information, we first play the bandit using Info-id until $H(b_{\\text{max}})=H_0$ is achieved. Then, we switch to Info-p to compute the regret obtained with those pre-trained priors. Fig.~\\ref{fig:value} confirms that information on $b_{\\text{max}}$ has indeed a positive value,\nand shows the rate-distortion curve for the variation $\\Delta R$ of regret {\\it vs} the initial entropy $H_0$. \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{value_jsp.eps}\n\\caption{The value of information. The left panel shows the curves for the regret obtained by Info-p after a period of ``pre-training'' by Info-id. The pre-training lasts for a number of iterations such that the initial entropy on the identity of the best arm $H(b_{\\max}) = H_0$. Details are described in the text. From top to bottom (blue to yellow), the initial entropies are $H_0 = \\ln 2/2^m$ for $m = 0,1,\\dots,5$. The regret curves are parallel in the asymptotic regime, i.e. a multiplicative change in $H_0$ leads to a constant change in the regret. In the right \npanel we show that the corresponding rate-distortion curve for the reduction in regret $\\Delta R$ measured at $n = 10^8$ for \ndifferent values of the index $m$ in $H_0=\\ln 2/2^m$. The approximate linearity of the rate-distortion curve \n$-\\Delta R\\propto m$ is captured (dashed line) by theoretical arguments presented in the main text and appendix.}\n\\label{fig:value}\n\\end{center}\n\\end{figure*}\n\nThe rate-distortion curve in Fig.~\\ref{fig:value} is rationalized as follows (see Appendix \\ref{a3} for details). The Info-id ``pre-training'' required to reach $H(b_{\\text{max}})=H_0=\\ln 2/2^m$ lasts for $n^{(pt)}\\propto m$ steps (see \\eqref{eq:slopelogH}). Since $n_1$ and $n_2$ are both $\\propto n$ for Info-id, the typical prior resulting from the pre-training is equivalent to the \nunlikely (for Info-p) situation of a comparable number of plays $n_1^{(pt)}$ and $n_2^{(pt)}$ for the two arms. Info-p will then play a very long stretch on the first arm until its typical decision boundary \\eqref{eq:decisionbound} is reached. The corresponding reduction in average regret $-\\Delta R$ is proportional to the logarithm of the length of the stretch, i.e. $n_2^{(pt)}\\propto m$. We conclude that $-\\Delta R\\propto m$, as observed in Fig.~\\ref{fig:value}.\n\n\\section{Conclusion}\nWe investigated the multi-armed bandit problem \\cite{gittins} with the purpose of gaining insights \non Infomax approaches, which postulate a functional role for the acquisition and transmission of information. We introduced two \nInfomax strategies of decision, and evaluated their performance using \nknown results on optimal decisions for multi-armed bandits. \n\nThe first strategy, Info-id, optimally acquires information on the identity of the best arm of the bandit but has a large, asymptotically linear regret. \nNote that the identity of the best arm is the quantity actually \nneeded for the choice of the arm to play. Therefore, the first natural candidate for an Infomax approach is Info-id, which however \nperforms poorly due to excessive exploration. \n\nThe second strategy, Info-p, shifts the balance towards exploitation by gathering information on the highest expected reward among the arms. That pushes to play more frequently the estimated best arm in the bandit. We showed that this strategy yields asymptotically optimal regrets and compares favorably with state-of-the-art \nmethods. The Info-p balance between exploration and exploitation produces a relatively slow $\\propto 1/n$ acquisition of information on the identity of the best arm, which should be contrasted with the exponential decay achieved by Info-id. \nThe striking differences between Info-p and Info-id clearly demonstrate that the nature of information \nacquired by Infomax critically matters. \n\nInfo-p, like other Infomax approaches, uses information as a proxy, namely of cumulative payoffs for multi-armed bandits. As already mentioned in the Introduction, the advantage is that the proxy has general applicability and that the process of \nacquisition of information is one-step in time (and greedy in the choice among the options). \nThe first point is important because situations where optimal policies are known are \nvery rare. The second point is relevant because optimal policies often involve extended forecasts in the future, as shown by the example of the Gittins index \\eqref{eq:Gittins}. Such calculations can {\\it a priori} be formulated as dynamic programming yet in practice \nthe size of the space to sample makes them unfeasible for computers and {\\it a fortiori} for neural systems or single cells. \nIt is therefore quite non-trivial that optimality on cumulative payoffs can be achieved by an Infomax approach for an appropriate quantity. That constitutes the general lesson \ndrawn here\\,: information in natural or artificial\nsystems could be acquired on quantities that are not \nimmediately recognizable as functionally relevant but could actually allow for effective functional trade-offs. \n\n\\begin{acknowledgements}\nWe are grateful to Boris Shraiman and Eric Siggia for illuminating discussions. \nMV acknowledges ICTP for hospitality and support. This work was supported by a grant from the Simons Foundation (\\#340106, Massimo Vergassola). \n\\end{acknowledgements} \n\n\n\n\\appendix\n\n\\section{Fast Info-p numerical simulations}\\label{a1}\nInfo-p is slowed down by posterior distributions \\eqref{eq:posterior_beta} sharply peaking around their mean, and accuracy demands progressively finer discretization. To speed up the algorithm, we remark that the Lai-Robbins bound \\eqref{eq:lairob} implies\nthat a typical play consists of long stretches of plays of the best arm interspaced with \noccasional plays of suboptimal ones. Suppose then the Info-p policy selects the best\n (with largest sample mean) arm for play. We can exactly bound the minimal length\nof consecutive plays of the best arm as follows\\,: Set the stretch size to some initial\nguess\\,; Consider the worst-case scenario of losses throughout the entire\nstretch\\,; If the Info-p policy chooses a sub-optimal arm at the end of the stretch, halve\nthe stretch size until the best arm is chosen\\,; If the Info-p policy chooses the\nbest arm at the end of stretch, double the stretch size until a sub-optimal arm is chosen\\,; Dissect dichotomically as in binary search algorithms \\cite{Recipes} the \nintervals identified as above. Note that the worst-case\nscenario of consecutive losses ensures that a lower bound on the stretch length is\nobtained and the numerical technique is thereby exact.\n\nOnce the length of consecutive plays is identified, we generate a random variable for the number of wins during the stretch and update the posterior only once, by using the fact that $\\beta$ distributions \\eqref{eq:posterior_beta} are conjugate priors for \nBernoulli likelihood functions \\cite{MacKay}. Discretization of the state space is adaptive and refined as the number of plays increases so as to ensure proper accuracy. \nWe employed a similar procedure for simulations of proportional betting (see \\ref{a4}). \n\n\\section{Proportional betting}\\label{a4}\nKelly's proportional betting \\cite{Kelly} (also called Thompson sampling in the machine learning community) is a randomized  policy that was recently shown to be asymptotically optimal \\cite{Kaufmann}. At each step, the algorithm plays an arm with a probability proportional to its probability to be the best among the arms in the bandit. \n\nOur arguments for showing the optimality of Info-p (see main text) are easily adapted to confirm that proportional betting is indeed optimal. The probabilities for each arm to be \nthe best are denoted $q_1, q_2,\\ldots$. For two arms, in the asymptotic limit $n_1 \\gg n_2$ and $n$ large, we typically have $\\hat{\\pi}_1 > \\hat{\\pi}_2$ and $\\frac{n_1}{n_2} \\approx \\frac{q_1}{q_2}$ with $q_2 \\simeq e^{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}$ and $q_1 \\simeq 1$, which again (as for Info-p) leads to $\\ln n \\simeq n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)$.  \n \nSince proportional betting is a randomized algorithm, the technique used for the Fast Info-p algorithm (see Appendix \\ref{a1}) does not carry over.  In the asymptotic limit, the probability that one of the arms is the best is very close to unity. This probability, say $q_1$, depends primarily on the number of plays of the inferior arms and changes negligibly as the first arm is played. Using this observation, the following approximate algorithm gives very reliable results: the best arm is played for a stretch whose size is randomly chosen from an exponential distribution of mean $\\frac{1}{1-q_1}$. Immediately after the stretch, one of the inferior arms is chosen with probabilities $\\frac{q_2}{1-q_1},\\frac{q_3}{1-q_1},\\dots$. This scheme is exact under the assumption that $q_1$ does not change during the stretch and we found it to be very reliable for the reasons mentioned above. The numerical method is analogous to the Gillespie algorithm  used to simulate chemical kinetics \\cite{gillespie}. \n\n\\section{Theoretical analysis of information on the identity of the best arm}\\label{a2}\n The goal of this Section is to provide further details about optimal information on the identity of the best arm and the related Info-id policy. The policy greedily maximizes the reduction in log-entropy, $\\ln H(b_{\\max})$, where $H(b_{\\max})$ is the entropy of the unknown identity $b_{\\max}$ of the best arm in the bandit. As in the main text, we shall consider the case of a two-armed bandit with probabilities of success $p_1$ and $p_2$ ($p_1 > p_2$). Generalizations to bandits with more than two arms are straightforward.\n \nThe estimated values of the probabilities of success in a given sample of plays are denoted by $\\pi_1$ and $\\pi_2$, respectively. \nTheir posterior distributions are given by \\eqref{eq:posterior_beta}.\n\n\n\n\n\nThe sample mean of $\\pi_i$ is indicated by $\\hat{\\pi}_i$. \n\nWe denote by $q_1 = \\text{Pr}(\\pi_1 > \\pi_2)$ the estimated probability for the first arm to be the best. For a two-armed bandit, $q_2 = 1-q_1$ and is given by \\eqref{eq:q1}.\n\n\n\n\nThe entropy of the unknown identity $b_{\\max}$ of the best arm is\\,: \n$H(b_{\\text{max}}) = -q_1\\ln q_1 -q_2\\ln q_2$. In the asymptotic limit $n_1, n_2 \\gg 1$, when the arms have each been played many times, the sample means $\\hat{\\pi}_i$ are typically close to their respective true values \n$p_i$. Large deviation theory \\cite{Cover} states that the $i$th posterior and its cumulative distribution are both dominated by the exponential factor $e^{-{n_i}D(\\hat{\\pi}_i, p)}$. The probability $q_1$ is then close to unity and the entropy is well approximated by\n\\eqref{eq:Hbmax}. \n\n\n\n\n\nWhen $n_1,n_2 \\gg 1$, the integrals in \\eqref{eq:q1} and \\eqref{eq:Hbmax} can be calculated by Laplace method and have three contributions\\,:\n\n\\noindent (I) The region $p\\le\\hat{\\pi}_2$. There, we have $\\int_p^{1} P_2(q) dq\\sim 1$ and $P_1(p)\\sim \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)\\right]$ by large deviations theory \\cite{Cover}. Integrating over $p$ and using that the dominant contribution comes from \n$p\\simeq \\hat{\\pi}_2$, we obtain\\,: $\\exp\\left[-n_1D\\left(\\hat{\\pi}_1,\\hat{\\pi}_2\\right)\\right]$.\n\n\\noindent (II) The region of $p$'s between $\\hat{\\pi}_2$ and $\\hat{\\pi}_1$. Its contribution is $\\int \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)-n_2D\\left(\\hat{\\pi}_2,p\\right)\\right]\\,dp$ by large deviations theory. Equating to zero the derivative of $n_1D(\\hat{\\pi}_1, p)+n_2D(\\hat{\\pi}_2, p)$ with respect to $p$ and using the definition of the Kullback-Leibler divergence $D(q,p) = q\\ln \\frac{q}{p} + (1-q)\\ln \\frac{1-q}{1-p}$, we obtain that the extremum is located at \n\n", "itemtype": "equation", "pos": 28617, "prevtext": "\nwhere $\\pi_s=\\left(n_1\\hat{\\pi}_1+n_2\\hat{\\pi}_2\\right)/n$ (see Appendix \\ref{a2}). \n\nTo minimize $\\ln q_2$ -- thereby achieving the maximum acquisition of information, see \\eqref{eq:Hbmax} -- we must  extremize with respect to $n_1$ and $n_2$. We show in Appendix \\ref{a2} that the dominant contribution comes \nfrom the second exponential term in \\eqref{eq:q2asymp}. The resulting extremum (with $n_1+n_2=n$) gives $D\\left(\\hat{\\pi}_1,\\pi_s\\right)=D\\left(\\hat{\\pi}_2,\\pi_s\\right)$. An important consequence of this equality is that $\\pi_s$ is at a finite distance from $\\hat{\\pi}_1$ and $\\hat{\\pi}_2$. It follows then from the expression \\eqref{eq:q2asymp} of $\\pi_s$ that $n_2\\propto n$, which is also confirmed by explicit expressions derived in the Appendix. \nAs for the fastest rate of decay of the average logarithm of the entropy, we obtain\\,:\n\n", "index": 19, "text": "\\begin{equation}\n\\overline{\\ln H(b_{\\text{max}})}=-n D(p_1,p_s)=-nD(p_2,p_s)\\,,\n\\label{eq:slopelogH}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\overline{\\ln H(b_{\\text{max}})}=-nD(p_{1},p_{s})=-nD(p_{2},p_{s})\\,,\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>H</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mtext>max</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00af</mo></mover><mo>=</mo><mrow><mo>-</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mi>p</mi><mi>s</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $n = n_1 + n_2$.\n\n\\noindent (III) Finally, the contribution from the rightmost region of $p$'s is dominated by $p\\simeq \\hat{\\pi}_1$ and reads\\,: $\\exp\\left[-n_2D\\left(\\hat{\\pi}_2,\\hat{\\pi}_1\\right)\\right]$.\n\n\\smallskip\nIn summary, the asymptotic expression of the entropy is\n\n", "itemtype": "equation", "pos": 44107, "prevtext": "\nwhere $p_s$ is defined by the equality $D(p_1,p_s)=D(p_2,p_s)$.\n\n\n\\subsection{Info-id}The Info-Id algorithm is defined as the decision strategy that chooses the arm which maximizes the expected reduction of $\\ln H(b_{\\text{max}})$. Specifically, the expected reduction $\\langle \\Delta \\ln H\\rangle_i$ upon \nplaying the $i$-th arm with posterior $P_i$ is analogous to \\eqref{eq:DeltaH} with $\\Delta H(\\pi_{\\max})$ replaced by $\\Delta \\ln H(b_{\\text{max}})$.\nIncrements are calculated by updating the posterior as for \\eqref{eq:DeltaH}, by using \\eqref{eq:q1} and finally obtaining the increment using the definition of $H(b_{\\text{max}})$ above. This greedy, one-step-in-time procedure indeed achieves the fastest decrease \\eqref{eq:slopelogH}\nof $\\overline{\\ln H(b_{\\text{max}})}$ (see Fig.~\\ref{fig:cost}). Note the logarithm in the definition of Info-id\\,: \nmaximizing the expected reduction of $H(b_{\\text{max}})$ would not achieve \\eqref{eq:slopelogH} but a slower decay (see Appendix \\ref{a2}).\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{cost_jsp.eps}\n\\caption{The cost of information. A comparison of the entropy $H(b_{\\max})$ on the identity of the best arm in the bandit (left panel) and the corresponding regret (right panel) for Info-id (red curves and axes) and Info-p (blue curves and axes). The left panel is in log-log scale for the blue curve and lin-log for the red curve. The right panel is in log-lin scale for the blue curve and lin-lin for the red curve. Info-id achieves the fastest possible reduction of the entropy, as shown by the agreement with the optimal slope \\eqref{eq:slopelogH} (black solid line in the left panel). For Info-p, the decrease is much slower\\,: $\\propto 1/n$ (dashed  line). Conversely,  Info-id has a linear regret that largely exceeds the optimal Lai-Robbins bound \\eqref{eq:lairob} achieved by Info-p.}\n\\label{fig:cost}\n\\end{center}\n\\end{figure*}\n\n\\subsection{The cost and value of information} \nInformation and payoffs embody the two sides of the exploration/exploitation dilemma for multi-armed bandits.\nThe expressions \\eqref{eq:Hbmax}, \\eqref{eq:q2asymp} and \\eqref{eq:slopelogH} allow to quantify the trade-offs in the optimal behaviors achieved by Info-p and Info-id, respectively. \n\nThe three relations above show that $-\\overline{\\ln H(b_{\\text{max}})}\\propto n\\propto n_2$ for Info-id. Since \nregret is proportional to $n_2$, we conclude that the rate of decay $-\\overline{\\ln H(b_{\\text{max}})}$ is proportional \nto the average regret, i.e. the exponential rate  \\eqref{eq:slopelogH} implies a regret linear in $n$, as confirmed by numerical simulations (see Fig.~\\ref{fig:cost}). \n\n\nA very different trade-off underlies the Info-p optimal regret. Indeed, for $n_2\\propto \\ln n$, the dominant contribution in \\eqref{eq:q2asymp} comes from the last term and implies a power-law decay of the entropy. In particular, if the Lai-Robbins bound \\eqref{eq:lairob} is saturated then $\\overline{\\ln H(b_{\\text{max}})}\\sim -\\ln n$. \nThe information on the identity of the best arm is therefore reducing much more slowly as compared to Info-id.  \n\n\\smallskip\nThe behaviors above clearly illustrate the costs in regret of  reducing $H(b_{\\text{max}})$. \nHowever, information about $b_{\\text{max}}$ has a definite value that can be exploited to \nincrease payoffs. In particular, if we start playing with some {\\it a priori} information on the identity of the best arm, i.e. $H(b_{\\text{max}})=H_0<\\ln 2$, general distortion-type arguments \\cite{Cover,Bialek} suggest that payoffs should increase as $H_0$ reduces. \n\nWe quantify the value of information by measuring the variation in payoff as a function of $H_0$. To generate the initial {\\it a priori} information, we first play the bandit using Info-id until $H(b_{\\text{max}})=H_0$ is achieved. Then, we switch to Info-p to compute the regret obtained with those pre-trained priors. Fig.~\\ref{fig:value} confirms that information on $b_{\\text{max}}$ has indeed a positive value,\nand shows the rate-distortion curve for the variation $\\Delta R$ of regret {\\it vs} the initial entropy $H_0$. \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{value_jsp.eps}\n\\caption{The value of information. The left panel shows the curves for the regret obtained by Info-p after a period of ``pre-training'' by Info-id. The pre-training lasts for a number of iterations such that the initial entropy on the identity of the best arm $H(b_{\\max}) = H_0$. Details are described in the text. From top to bottom (blue to yellow), the initial entropies are $H_0 = \\ln 2/2^m$ for $m = 0,1,\\dots,5$. The regret curves are parallel in the asymptotic regime, i.e. a multiplicative change in $H_0$ leads to a constant change in the regret. In the right \npanel we show that the corresponding rate-distortion curve for the reduction in regret $\\Delta R$ measured at $n = 10^8$ for \ndifferent values of the index $m$ in $H_0=\\ln 2/2^m$. The approximate linearity of the rate-distortion curve \n$-\\Delta R\\propto m$ is captured (dashed line) by theoretical arguments presented in the main text and appendix.}\n\\label{fig:value}\n\\end{center}\n\\end{figure*}\n\nThe rate-distortion curve in Fig.~\\ref{fig:value} is rationalized as follows (see Appendix \\ref{a3} for details). The Info-id ``pre-training'' required to reach $H(b_{\\text{max}})=H_0=\\ln 2/2^m$ lasts for $n^{(pt)}\\propto m$ steps (see \\eqref{eq:slopelogH}). Since $n_1$ and $n_2$ are both $\\propto n$ for Info-id, the typical prior resulting from the pre-training is equivalent to the \nunlikely (for Info-p) situation of a comparable number of plays $n_1^{(pt)}$ and $n_2^{(pt)}$ for the two arms. Info-p will then play a very long stretch on the first arm until its typical decision boundary \\eqref{eq:decisionbound} is reached. The corresponding reduction in average regret $-\\Delta R$ is proportional to the logarithm of the length of the stretch, i.e. $n_2^{(pt)}\\propto m$. We conclude that $-\\Delta R\\propto m$, as observed in Fig.~\\ref{fig:value}.\n\n\\section{Conclusion}\nWe investigated the multi-armed bandit problem \\cite{gittins} with the purpose of gaining insights \non Infomax approaches, which postulate a functional role for the acquisition and transmission of information. We introduced two \nInfomax strategies of decision, and evaluated their performance using \nknown results on optimal decisions for multi-armed bandits. \n\nThe first strategy, Info-id, optimally acquires information on the identity of the best arm of the bandit but has a large, asymptotically linear regret. \nNote that the identity of the best arm is the quantity actually \nneeded for the choice of the arm to play. Therefore, the first natural candidate for an Infomax approach is Info-id, which however \nperforms poorly due to excessive exploration. \n\nThe second strategy, Info-p, shifts the balance towards exploitation by gathering information on the highest expected reward among the arms. That pushes to play more frequently the estimated best arm in the bandit. We showed that this strategy yields asymptotically optimal regrets and compares favorably with state-of-the-art \nmethods. The Info-p balance between exploration and exploitation produces a relatively slow $\\propto 1/n$ acquisition of information on the identity of the best arm, which should be contrasted with the exponential decay achieved by Info-id. \nThe striking differences between Info-p and Info-id clearly demonstrate that the nature of information \nacquired by Infomax critically matters. \n\nInfo-p, like other Infomax approaches, uses information as a proxy, namely of cumulative payoffs for multi-armed bandits. As already mentioned in the Introduction, the advantage is that the proxy has general applicability and that the process of \nacquisition of information is one-step in time (and greedy in the choice among the options). \nThe first point is important because situations where optimal policies are known are \nvery rare. The second point is relevant because optimal policies often involve extended forecasts in the future, as shown by the example of the Gittins index \\eqref{eq:Gittins}. Such calculations can {\\it a priori} be formulated as dynamic programming yet in practice \nthe size of the space to sample makes them unfeasible for computers and {\\it a fortiori} for neural systems or single cells. \nIt is therefore quite non-trivial that optimality on cumulative payoffs can be achieved by an Infomax approach for an appropriate quantity. That constitutes the general lesson \ndrawn here\\,: information in natural or artificial\nsystems could be acquired on quantities that are not \nimmediately recognizable as functionally relevant but could actually allow for effective functional trade-offs. \n\n\\begin{acknowledgements}\nWe are grateful to Boris Shraiman and Eric Siggia for illuminating discussions. \nMV acknowledges ICTP for hospitality and support. This work was supported by a grant from the Simons Foundation (\\#340106, Massimo Vergassola). \n\\end{acknowledgements} \n\n\n\n\\appendix\n\n\\section{Fast Info-p numerical simulations}\\label{a1}\nInfo-p is slowed down by posterior distributions \\eqref{eq:posterior_beta} sharply peaking around their mean, and accuracy demands progressively finer discretization. To speed up the algorithm, we remark that the Lai-Robbins bound \\eqref{eq:lairob} implies\nthat a typical play consists of long stretches of plays of the best arm interspaced with \noccasional plays of suboptimal ones. Suppose then the Info-p policy selects the best\n (with largest sample mean) arm for play. We can exactly bound the minimal length\nof consecutive plays of the best arm as follows\\,: Set the stretch size to some initial\nguess\\,; Consider the worst-case scenario of losses throughout the entire\nstretch\\,; If the Info-p policy chooses a sub-optimal arm at the end of the stretch, halve\nthe stretch size until the best arm is chosen\\,; If the Info-p policy chooses the\nbest arm at the end of stretch, double the stretch size until a sub-optimal arm is chosen\\,; Dissect dichotomically as in binary search algorithms \\cite{Recipes} the \nintervals identified as above. Note that the worst-case\nscenario of consecutive losses ensures that a lower bound on the stretch length is\nobtained and the numerical technique is thereby exact.\n\nOnce the length of consecutive plays is identified, we generate a random variable for the number of wins during the stretch and update the posterior only once, by using the fact that $\\beta$ distributions \\eqref{eq:posterior_beta} are conjugate priors for \nBernoulli likelihood functions \\cite{MacKay}. Discretization of the state space is adaptive and refined as the number of plays increases so as to ensure proper accuracy. \nWe employed a similar procedure for simulations of proportional betting (see \\ref{a4}). \n\n\\section{Proportional betting}\\label{a4}\nKelly's proportional betting \\cite{Kelly} (also called Thompson sampling in the machine learning community) is a randomized  policy that was recently shown to be asymptotically optimal \\cite{Kaufmann}. At each step, the algorithm plays an arm with a probability proportional to its probability to be the best among the arms in the bandit. \n\nOur arguments for showing the optimality of Info-p (see main text) are easily adapted to confirm that proportional betting is indeed optimal. The probabilities for each arm to be \nthe best are denoted $q_1, q_2,\\ldots$. For two arms, in the asymptotic limit $n_1 \\gg n_2$ and $n$ large, we typically have $\\hat{\\pi}_1 > \\hat{\\pi}_2$ and $\\frac{n_1}{n_2} \\approx \\frac{q_1}{q_2}$ with $q_2 \\simeq e^{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}$ and $q_1 \\simeq 1$, which again (as for Info-p) leads to $\\ln n \\simeq n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)$.  \n \nSince proportional betting is a randomized algorithm, the technique used for the Fast Info-p algorithm (see Appendix \\ref{a1}) does not carry over.  In the asymptotic limit, the probability that one of the arms is the best is very close to unity. This probability, say $q_1$, depends primarily on the number of plays of the inferior arms and changes negligibly as the first arm is played. Using this observation, the following approximate algorithm gives very reliable results: the best arm is played for a stretch whose size is randomly chosen from an exponential distribution of mean $\\frac{1}{1-q_1}$. Immediately after the stretch, one of the inferior arms is chosen with probabilities $\\frac{q_2}{1-q_1},\\frac{q_3}{1-q_1},\\dots$. This scheme is exact under the assumption that $q_1$ does not change during the stretch and we found it to be very reliable for the reasons mentioned above. The numerical method is analogous to the Gillespie algorithm  used to simulate chemical kinetics \\cite{gillespie}. \n\n\\section{Theoretical analysis of information on the identity of the best arm}\\label{a2}\n The goal of this Section is to provide further details about optimal information on the identity of the best arm and the related Info-id policy. The policy greedily maximizes the reduction in log-entropy, $\\ln H(b_{\\max})$, where $H(b_{\\max})$ is the entropy of the unknown identity $b_{\\max}$ of the best arm in the bandit. As in the main text, we shall consider the case of a two-armed bandit with probabilities of success $p_1$ and $p_2$ ($p_1 > p_2$). Generalizations to bandits with more than two arms are straightforward.\n \nThe estimated values of the probabilities of success in a given sample of plays are denoted by $\\pi_1$ and $\\pi_2$, respectively. \nTheir posterior distributions are given by \\eqref{eq:posterior_beta}.\n\n\n\n\n\nThe sample mean of $\\pi_i$ is indicated by $\\hat{\\pi}_i$. \n\nWe denote by $q_1 = \\text{Pr}(\\pi_1 > \\pi_2)$ the estimated probability for the first arm to be the best. For a two-armed bandit, $q_2 = 1-q_1$ and is given by \\eqref{eq:q1}.\n\n\n\n\nThe entropy of the unknown identity $b_{\\max}$ of the best arm is\\,: \n$H(b_{\\text{max}}) = -q_1\\ln q_1 -q_2\\ln q_2$. In the asymptotic limit $n_1, n_2 \\gg 1$, when the arms have each been played many times, the sample means $\\hat{\\pi}_i$ are typically close to their respective true values \n$p_i$. Large deviation theory \\cite{Cover} states that the $i$th posterior and its cumulative distribution are both dominated by the exponential factor $e^{-{n_i}D(\\hat{\\pi}_i, p)}$. The probability $q_1$ is then close to unity and the entropy is well approximated by\n\\eqref{eq:Hbmax}. \n\n\n\n\n\nWhen $n_1,n_2 \\gg 1$, the integrals in \\eqref{eq:q1} and \\eqref{eq:Hbmax} can be calculated by Laplace method and have three contributions\\,:\n\n\\noindent (I) The region $p\\le\\hat{\\pi}_2$. There, we have $\\int_p^{1} P_2(q) dq\\sim 1$ and $P_1(p)\\sim \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)\\right]$ by large deviations theory \\cite{Cover}. Integrating over $p$ and using that the dominant contribution comes from \n$p\\simeq \\hat{\\pi}_2$, we obtain\\,: $\\exp\\left[-n_1D\\left(\\hat{\\pi}_1,\\hat{\\pi}_2\\right)\\right]$.\n\n\\noindent (II) The region of $p$'s between $\\hat{\\pi}_2$ and $\\hat{\\pi}_1$. Its contribution is $\\int \\exp\\left[-n_1D\\left(\\hat{\\pi}_1,p\\right)-n_2D\\left(\\hat{\\pi}_2,p\\right)\\right]\\,dp$ by large deviations theory. Equating to zero the derivative of $n_1D(\\hat{\\pi}_1, p)+n_2D(\\hat{\\pi}_2, p)$ with respect to $p$ and using the definition of the Kullback-Leibler divergence $D(q,p) = q\\ln \\frac{q}{p} + (1-q)\\ln \\frac{1-q}{1-p}$, we obtain that the extremum is located at \n\n", "index": 21, "text": "\\begin{align}\\label{eq:pisdef}\n\\pi_s = \\frac{n_1\\hat{\\pi}_1 + n_2 \\hat{\\pi}_2}{n}\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\pi_{s}=\\frac{n_{1}\\hat{\\pi}_{1}+n_{2}\\hat{\\pi}_{2}}{n}\\,,\" display=\"inline\"><mrow><mrow><msub><mi>\u03c0</mi><mi>s</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></mrow><mi>n</mi></mfrac></mstyle></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $A,B,C$ are subdominant prefactors. \n\n\\smallskip\nThe expression \\eqref{eq:entapprox} still depends on $n_1$ and $n_2$, which are \ncontrolled by the policy of play. The fastest possible rate of acquisition of information is obtained by taking the extremum \nover $n_1$ and $n_2$ with the constraint $n_1+n_2=n$. Suppose for now (as we shall demonstrate  later) that the dominant contribution in \\eqref{eq:entapprox} \nis the last one\\,:\n\n", "itemtype": "equation", "pos": 44486, "prevtext": "\nwhere $n = n_1 + n_2$.\n\n\\noindent (III) Finally, the contribution from the rightmost region of $p$'s is dominated by $p\\simeq \\hat{\\pi}_1$ and reads\\,: $\\exp\\left[-n_2D\\left(\\hat{\\pi}_2,\\hat{\\pi}_1\\right)\\right]$.\n\n\\smallskip\nIn summary, the asymptotic expression of the entropy is\n\n", "index": 23, "text": "\\begin{align}\\label{eq:entapprox}\nH(b_{\\max}) \\sim A\\exp\\big[{-n_1D(\\hat{\\pi}_1, \\hat{\\pi}_2)}\\big] + B\\exp\\big[{-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)}\\big] + C\\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big]\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H(b_{\\max})\\sim A\\exp\\big{[}{-n_{1}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2}%&#10;)}\\big{]}+B\\exp\\big{[}{-n_{2}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})}\\big{]}+C\\exp\\big{%&#10;[}{-n_{1}D(\\hat{\\pi}_{1},\\pi_{s})-n_{2}D(\\hat{\\pi}_{2},\\pi_{s})}\\big{]}\\,,\" display=\"inline\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>B</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe maximum possible rate of reduction of log-entropy is then calculated as follows. If we denote $n_1/n = x$, $n_2/n = 1-x$ and differentiate the exponent in \\eqref{eq:entdom} with respect to $x$, we obtain the relation\n\n", "itemtype": "equation", "pos": 45164, "prevtext": "\nwhere $A,B,C$ are subdominant prefactors. \n\n\\smallskip\nThe expression \\eqref{eq:entapprox} still depends on $n_1$ and $n_2$, which are \ncontrolled by the policy of play. The fastest possible rate of acquisition of information is obtained by taking the extremum \nover $n_1$ and $n_2$ with the constraint $n_1+n_2=n$. Suppose for now (as we shall demonstrate  later) that the dominant contribution in \\eqref{eq:entapprox} \nis the last one\\,:\n\n", "index": 25, "text": "\\begin{align}\\label{eq:entdom}\nH(b_{\\max}) \\sim \\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big]\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle H(b_{\\max})\\sim\\exp\\big{[}{-n_{1}D(\\hat{\\pi}_{1},\\pi_{s})-n_{2}D%&#10;(\\hat{\\pi}_{2},\\pi_{s})}\\big{]}\\,.\" display=\"inline\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhich defines the optimal value $\\pi_s=\\pi_{s,o}$. Using the explicit expression of the Kullback-Leibler divergence $D$\\,:\n\n", "itemtype": "equation", "pos": 45517, "prevtext": "\nThe maximum possible rate of reduction of log-entropy is then calculated as follows. If we denote $n_1/n = x$, $n_2/n = 1-x$ and differentiate the exponent in \\eqref{eq:entdom} with respect to $x$, we obtain the relation\n\n", "index": 27, "text": "\\begin{align}\\label{eq:optx}\nD(\\hat{\\pi}_1, \\pi_s) = D(\\hat{\\pi}_2, \\pi_s)\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D(\\hat{\\pi}_{1},\\pi_{s})=D(\\hat{\\pi}_{2},\\pi_{s})\\,,\" display=\"inline\"><mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nThe optimal proportion of plays on the arms follows from \\eqref{eq:pisdef}\\,:\n\n", "itemtype": "equation", "pos": 45730, "prevtext": "\nwhich defines the optimal value $\\pi_s=\\pi_{s,o}$. Using the explicit expression of the Kullback-Leibler divergence $D$\\,:\n\n", "index": 29, "text": "\\begin{equation}\n\\pi_{s,o} = \\frac{1}{1 + e^{f(\\hat{\\pi}_1,\\hat{\\pi}_2)}}, \\qquad f(\\hat{\\pi}_1,\\hat{\\pi}_2) = \\frac{H(\\hat{\\pi}_1) - H(\\hat{\\pi}_2)}\n{\\hat{\\pi}_1 - \\hat{\\pi}_2}\\,.\n \\label{eq:optpsi}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\pi_{s,o}=\\frac{1}{1+e^{f(\\hat{\\pi}_{1},\\hat{\\pi}_{2})}},\\qquad f(\\hat{\\pi}_{1%&#10;},\\hat{\\pi}_{2})=\\frac{H(\\hat{\\pi}_{1})-H(\\hat{\\pi}_{2})}{\\hat{\\pi}_{1}-\\hat{%&#10;\\pi}_{2}}\\,.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></mfrac></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></mfrac></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nThe decay of the log-entropy averaged over the statistical realizations follows from \\eqref{eq:entdom} and \\eqref{eq:optx}\\,:\n\n", "itemtype": "equation", "pos": 46024, "prevtext": "\n\nThe optimal proportion of plays on the arms follows from \\eqref{eq:pisdef}\\,:\n\n", "index": 31, "text": "\\begin{align}\nx_o=\\left(\\frac{n_1}{n}\\right)_o = \\frac{\\pi_{s,o} - \\hat{\\pi}_2}{\\hat{\\pi}_1 - \\hat{\\pi}_2}\\,. \n\\label{eq:optn1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{o}=\\left(\\frac{n_{1}}{n}\\right)_{o}=\\frac{\\pi_{s,o}-\\hat{\\pi}%&#10;_{2}}{\\hat{\\pi}_{1}-\\hat{\\pi}_{2}}\\,.\" display=\"inline\"><mrow><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>=</mo><msub><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mn>1</mn></msub><mi>n</mi></mfrac></mstyle><mo>)</mo></mrow><mi>o</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></mfrac></mstyle></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwhere $p_{s,o} = (1 + e^{f(p_1,p_2)})^{-1}$ and $f$ is defined in \\eqref{eq:optpsi}. Note that the average of the log-entropy gives the typical behavior over the realizations, while the entropy itself or its higher powers \nare determined by large-deviation fluctuations. That leads to anomalous exponents as a function of the power considered. The  appropriate statistic for the information gathered in a typical realization is $e^{\\langle \\ln H \\rangle}$.\n\n\\medskip\nThe final piece of our analysis is to check that the claimed maximum exponent $-nD(\\hat{\\pi}_1, \\pi_{s,o}) $ in \\eqref{eq:entdom} \nis indeed larger than the other two potential candidates $-n x_oD(\\hat{\\pi}_1, \\hat{\\pi}_2)$ and $-n\\left(1-x_o\\right)D(\\hat{\\pi}_2, \\hat{\\pi}_1)$ in \\eqref{eq:entapprox}\\,:\n\n", "itemtype": "equation", "pos": 46291, "prevtext": "\n\nThe decay of the log-entropy averaged over the statistical realizations follows from \\eqref{eq:entdom} and \\eqref{eq:optx}\\,:\n\n", "index": 33, "text": "\\begin{equation}\n\\label{eq:optrate}\n\\overline{\\ln H(b_{\\rm max})} =-nD(p_1, p_{s,o})\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\overline{\\ln H(b_{\\rm max})}=-nD(p_{1},p_{s,o})\\,.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>H</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>max</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00af</mo></mover><mo>=</mo><mrow><mo>-</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nWe concentrate on the first relation in \\eqref{eq:ineq}\\,; the second one follows by symmetry. The convexity of $D$ in the second argument implies\\,:\n\n", "itemtype": "equation", "pos": 47166, "prevtext": "\nwhere $p_{s,o} = (1 + e^{f(p_1,p_2)})^{-1}$ and $f$ is defined in \\eqref{eq:optpsi}. Note that the average of the log-entropy gives the typical behavior over the realizations, while the entropy itself or its higher powers \nare determined by large-deviation fluctuations. That leads to anomalous exponents as a function of the power considered. The  appropriate statistic for the information gathered in a typical realization is $e^{\\langle \\ln H \\rangle}$.\n\n\\medskip\nThe final piece of our analysis is to check that the claimed maximum exponent $-nD(\\hat{\\pi}_1, \\pi_{s,o}) $ in \\eqref{eq:entdom} \nis indeed larger than the other two potential candidates $-n x_oD(\\hat{\\pi}_1, \\hat{\\pi}_2)$ and $-n\\left(1-x_o\\right)D(\\hat{\\pi}_2, \\hat{\\pi}_1)$ in \\eqref{eq:entapprox}\\,:\n\n", "index": 35, "text": "\\begin{align}\nx_oD(\\hat{\\pi}_1, \\hat{\\pi}_2) \\ge D(\\hat{\\pi}_1,\\pi_{s,o})\\,;\\qquad (1-x_o)D(\\hat{\\pi}_2, \\hat{\\pi}_1) \\ge D(\\hat{\\pi}_2,\\pi_{s,o})\\,.\n\\label{eq:ineq}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{o}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\geq D(\\hat{\\pi}_{1},\\pi_{s,o%&#10;})\\,;\\qquad(1-x_{o})D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})\\geq D(\\hat{\\pi}_{2},\\pi_{s,%&#10;o})\\,.\" display=\"inline\"><mrow><mrow><mrow><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"22.5pt\">;</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": " \nwhere we used the explicit expression of the Kullback-Leibler divergence $D$ to calculate the partial \nderivative at $\\pi_{s,o}$ with respect to the second argument. \nMultiplying by $x_o$ both sides of \\eqref{eq:interm} and using \\eqref{eq:optn1}, it follows that\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nWe concentrate on the first relation in \\eqref{eq:ineq}\\,; the second one follows by symmetry. The convexity of $D$ in the second argument implies\\,:\n\n", "index": 37, "text": "\\begin{align}\nD(\\hat{\\pi}_1, \\hat{\\pi}_2) \\ge D(\\hat{\\pi}_1, \\pi_{s,o}) + (\\hat{\\pi}_2 - \\pi_{s,o}) \\times \\frac{\\pi_{s,o} - \\hat{\\pi}_1}{\\pi_{s,o}(1-\\pi_{s,o})}\\,,\n\\label{eq:interm}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\geq D(\\hat{\\pi}_{1},\\pi_{s,o})+(%&#10;\\hat{\\pi}_{2}-\\pi_{s,o})\\times\\frac{\\pi_{s,o}-\\hat{\\pi}_{1}}{\\pi_{s,o}(1-\\pi_{%&#10;s,o})}\\,,\" display=\"inline\"><mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe ratio $\\frac{D(\\hat{\\pi}_1,\\pi_{s,o})}{D(\\hat{\\pi}_2,\\pi_{s,o})} = 1$, due to \\eqref{eq:optx}, and $\\frac{\\hat{\\pi}_1 -\\pi_{s,o}}{\\hat{\\pi}_1 - \\hat{\\pi}_2}=1-x_o$, due to \\eqref{eq:optn1}. We conclude that\\,:\n\n", "itemtype": "equation", "pos": 47954, "prevtext": " \nwhere we used the explicit expression of the Kullback-Leibler divergence $D$ to calculate the partial \nderivative at $\\pi_{s,o}$ with respect to the second argument. \nMultiplying by $x_o$ both sides of \\eqref{eq:interm} and using \\eqref{eq:optn1}, it follows that\n\n", "index": 39, "text": "\\begin{align}\nx_oD(\\hat{\\pi}_1, \\hat{\\pi}_2) \\ge x_oD(\\hat{\\pi}_1, \\pi_{s,o}) + \\frac{\\hat{\\pi}_1 - \\pi_{s,o}}{\\hat{\\pi}_1 - \\hat{\\pi}_2} \\frac{(\\hat{\\pi}_2 - \\pi_{s,o})^2}{\\pi_{s,o}(1-\\pi_{s,o})} \\times \\frac{D(\\hat{\\pi}_1,\\pi_{s,o})}{D(\\hat{\\pi}_2,\\pi_{s,o})}\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{o}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\geq x_{o}D(\\hat{\\pi}_{1},\\pi%&#10;_{s,o})+\\frac{\\hat{\\pi}_{1}-\\pi_{s,o}}{\\hat{\\pi}_{1}-\\hat{\\pi}_{2}}\\frac{(\\hat%&#10;{\\pi}_{2}-\\pi_{s,o})^{2}}{\\pi_{s,o}(1-\\pi_{s,o})}\\times\\frac{D(\\hat{\\pi}_{1},%&#10;\\pi_{s,o})}{D(\\hat{\\pi}_{2},\\pi_{s,o})}\\,.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>\u00d7</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nTo prove \\eqref{eq:ineq}, it only remains to show that \n\n", "itemtype": "equation", "pos": 48445, "prevtext": "\nThe ratio $\\frac{D(\\hat{\\pi}_1,\\pi_{s,o})}{D(\\hat{\\pi}_2,\\pi_{s,o})} = 1$, due to \\eqref{eq:optx}, and $\\frac{\\hat{\\pi}_1 -\\pi_{s,o}}{\\hat{\\pi}_1 - \\hat{\\pi}_2}=1-x_o$, due to \\eqref{eq:optn1}. We conclude that\\,:\n\n", "index": 41, "text": "\\begin{align}\nx_oD(\\hat{\\pi}_1, \\hat{\\pi}_2) \\ge D(\\hat{\\pi}_1, \\pi_{s,o}) + (1-x_o)D(\\hat{\\pi}_1,\\pi_{s,o})\\bigg[ \\frac{(\\hat{\\pi}_2 - \\pi_{s,o})^2}{\\pi_{s,o}(1-\\pi_{s,o})D(\\hat{\\pi}_2,\\pi_{s,o})} - 1\\bigg]\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{o}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\geq D(\\hat{\\pi}_{1},\\pi_{s,o%&#10;})+(1-x_{o})D(\\hat{\\pi}_{1},\\pi_{s,o})\\bigg{[}\\frac{(\\hat{\\pi}_{2}-\\pi_{s,o})^%&#10;{2}}{\\pi_{s,o}(1-\\pi_{s,o})D(\\hat{\\pi}_{2},\\pi_{s,o})}-1\\bigg{]}\\,.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>x</mi><mi>o</mi></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>x</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>-</mo><mn>1</mn></mrow><mo maxsize=\"210%\" minsize=\"210%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": " \nwhich follows from the inequality between the Kullback-Leibler  divergence and the $\\chi^2$ distance of two distributions (see eqs.~6,7 in \\cite{inequalities}). This completes the proof.\n\n\\subsection{A strategy that maximizes reduction in entropy}\\,\\,\nDoes the Info-id policy (which is greedy in its choice of the arm and one-step in time) attain the maximum rate \\eqref{eq:optrate}\\,? The aim of this subsection is to give a positive answer to this question.\n\n\\smallskip\nThe Info-d policy selects the arm of the bandit which offers the largest expected reduction in log-entropy\n\\begin{eqnarray}\n&\\langle \\Delta \\ln H \\rangle_i = (1-\\hat{\\pi}_i) \\times \\Delta \\ln H(b_{\\max}|\\text{0 observed})+\\nonumber \\\\ & \\hat{\\pi}_i \\times \\Delta \\ln H(b_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltalogH}\n\\end{eqnarray}\nwhere $0/1$ correspond to loss/win and $\\langle\\bullet\\rangle$ denotes the average with respect to the posterior probability distribution. To calculate $\\langle \\Delta \\ln H \\rangle_i$, we use the transformations: \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nTo prove \\eqref{eq:ineq}, it only remains to show that \n\n", "index": 43, "text": "\\begin{align}\n\\frac{(\\hat{\\pi}_2 - \\pi_{s,o})^2}{\\pi_{s,o}(1-\\pi_{s,o})}\\ge  D(\\hat{\\pi}_2,\\pi_{s,o})\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{(\\hat{\\pi}_{2}-\\pi_{s,o})^{2}}{\\pi_{s,o}(1-\\pi_{s,o})}\\geq D%&#10;(\\hat{\\pi}_{2},\\pi_{s,o})\\,,\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2265</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nLet us calculate the expected variation \\eqref{eq:DeltalogH} upon playing the first arm,  $i=1$\\,: \n\\begin{eqnarray}\n\\label{eq:firstline}\n& \\ln H(b_{\\max}|\\text{0 observed}) \\simeq -(n_1 + 1)D\\bigg(\\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1}, \\pi_s - \\frac{\\pi_s}{n}\\bigg) - n_2D\\bigg(\\hat{\\pi}_2,\\pi_s - \\frac{\\pi_s}{n}\\bigg) \\\\ \n\\label{eq:secondline}\n&\\simeq -(n_1+1)\\bigg[D(\\hat{\\pi}_1 , \\pi_s) - \\frac{\\hat{\\pi}_1}{n_1}\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) - \\frac{\\pi_s}{n} \\frac{\\pi_s - \\hat{\\pi}_1}{\\pi_s(1-\\pi_s)}\\bigg]\n\n- n_2\\bigg[D(\\hat{\\pi}_2,\\pi_s) - \\frac{\\pi_s}{n}\\frac{\\pi_s - \\hat{\\pi}_2}{\\pi_s(1-\\pi_s)}\\bigg] \\\\\n& \\simeq -(n_1 + 1)D(\\hat{\\pi}_1 , \\pi_s) - n_2D(\\hat{\\pi}_2,\\pi_s) + \\frac{n_1}{n}\\frac{\\pi_s - \\hat{\\pi}_1}{(1-\\pi_s)} + \\frac{n_2}{n}\\frac{\\pi_s - \\hat{\\pi}_2}{(1-\\pi_s)} + \\hat{\\pi}_1\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) \\label{eq:logH0_1}\\,.\n\\label{eq:thirdline}\n\\end{eqnarray}\nThe first asymptotic equality \\eqref{eq:firstline} follows from \\eqref{eq:entdom} and \\eqref{eq:transform}.\nThe second line \\eqref{eq:secondline} is obtained by expanding $D(p,q)$ to first order in its Taylor series for both arguments, which  \nis legitimate as $n_1,n_2\\gg 1$. Finally, for the third line \\eqref{eq:thirdline} we ignore subdominant terms $o(1)$. Notice that the sum of the third and the fourth terms in \\eqref{eq:logH0_1} vanishes due to \\eqref{eq:pisdef}. \n\nWe conclude that \n\n", "itemtype": "equation", "pos": 49865, "prevtext": " \nwhich follows from the inequality between the Kullback-Leibler  divergence and the $\\chi^2$ distance of two distributions (see eqs.~6,7 in \\cite{inequalities}). This completes the proof.\n\n\\subsection{A strategy that maximizes reduction in entropy}\\,\\,\nDoes the Info-id policy (which is greedy in its choice of the arm and one-step in time) attain the maximum rate \\eqref{eq:optrate}\\,? The aim of this subsection is to give a positive answer to this question.\n\n\\smallskip\nThe Info-d policy selects the arm of the bandit which offers the largest expected reduction in log-entropy\n\\begin{eqnarray}\n&\\langle \\Delta \\ln H \\rangle_i = (1-\\hat{\\pi}_i) \\times \\Delta \\ln H(b_{\\max}|\\text{0 observed})+\\nonumber \\\\ & \\hat{\\pi}_i \\times \\Delta \\ln H(b_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltalogH}\n\\end{eqnarray}\nwhere $0/1$ correspond to loss/win and $\\langle\\bullet\\rangle$ denotes the average with respect to the posterior probability distribution. To calculate $\\langle \\Delta \\ln H \\rangle_i$, we use the transformations: \n\n", "index": 45, "text": "\\begin{equation}\\label{eq:transform}\n \\text{0 is observed\\,:}\\left\\{\\begin{aligned}\n        n_i&\\rightarrow n_i + 1,\\\\\n        \\hat{\\pi}_i&\\rightarrow \\hat{\\pi}_i - \\frac{\\hat{\\pi}_i}{n_i},\\\\\n        \\pi_s &\\rightarrow \\pi_s - \\frac{\\pi_s}{n}\\,;\n       \\end{aligned}\n       \\right.\n \\qquad \n \\text{1 is observed\\,:}\n  \\left\\{\\begin{aligned}\n        n_i&\\rightarrow n_i + 1,\\\\\n        \\hat{\\pi}_i&\\rightarrow \\hat{\\pi}_i + \\frac{1- \\hat{\\pi}_i}{n_i},\\\\\n        \\pi_s &\\rightarrow \\pi_s + \\frac{1-\\pi_s}{n}\\,.\n       \\end{aligned}\n \\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\text{0 is observed\\,:}\\left\\{\\begin{aligned} \\displaystyle n_{i}&amp;%&#10;\\displaystyle\\rightarrow n_{i}+1,\\\\&#10;\\displaystyle\\hat{\\pi}_{i}&amp;\\displaystyle\\rightarrow\\hat{\\pi}_{i}-\\frac{\\hat{%&#10;\\pi}_{i}}{n_{i}},\\\\&#10;\\displaystyle\\pi_{s}&amp;\\displaystyle\\rightarrow\\pi_{s}-\\frac{\\pi_{s}}{n}\\,;\\end{%&#10;aligned}\\right.\\qquad\\text{1 is observed\\,:}\\left\\{\\begin{aligned} %&#10;\\displaystyle n_{i}&amp;\\displaystyle\\rightarrow n_{i}+1,\\\\&#10;\\displaystyle\\hat{\\pi}_{i}&amp;\\displaystyle\\rightarrow\\hat{\\pi}_{i}+\\frac{1-\\hat{%&#10;\\pi}_{i}}{n_{i}},\\\\&#10;\\displaystyle\\pi_{s}&amp;\\displaystyle\\rightarrow\\pi_{s}+\\frac{1-\\pi_{s}}{n}\\,.%&#10;\\end{aligned}\\right.\" display=\"block\"><mrow><mtext>0 is observed\u2009:</mtext><mrow><mo>{</mo><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>n</mi><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><mo>-</mo><mfrac><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><msub><mi>n</mi><mi>i</mi></msub></mfrac></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>\u03c0</mi><mi>s</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mi>\u03c0</mi><mi>s</mi></msub><mo>-</mo><mpadded width=\"+1.7pt\"><mfrac><msub><mi>\u03c0</mi><mi>s</mi></msub><mi>n</mi></mfrac></mpadded></mrow></mrow><mo>;</mo></mrow></mtd></mtr></mtable><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mtext>1 is observed\u2009:</mtext><mrow><mo>{</mo><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>n</mi><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mrow><msub><mi>n</mi><mi>i</mi></msub></mfrac></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>\u03c0</mi><mi>s</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><msub><mi>\u03c0</mi><mi>s</mi></msub><mo>+</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mi>s</mi></msub></mrow><mi>n</mi></mfrac></mpadded></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nSimilarly to \\eqref{eq:dlogH0}, when the outcome of the play on the first arm is a win\\,: \n\\begin{eqnarray}\n& \\ln H(b_{\\max}|\\text{1 observed}) \\sim -(n_1 + 1)D\\bigg(\\hat{\\pi}_1 + \\frac{1-\\hat{\\pi}_1}{n_1}, \\pi_s + \\frac{1-\\pi_s}{n}\\bigg) - n_2D\\bigg(\\hat{\\pi}_2,\\pi_s + \\frac{1-\\pi_s}{n}\\bigg) \\\\ \n&\\simeq -(n_1+1)D(\\hat{\\pi}_1 , \\pi_s) - n_2D(\\hat{\\pi}_2,\\pi_s) - \\left(1-\\hat{\\pi}_1\\right)\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\,,\n\\label{eq:ssecondline}\n\\end{eqnarray}\nwhere a cancellation similar to the one in \\eqref{eq:thirdline} simplified the final expression \\eqref{eq:ssecondline}. \nWe are thereby left with\n\n", "itemtype": "equation", "pos": 51891, "prevtext": "\n\nLet us calculate the expected variation \\eqref{eq:DeltalogH} upon playing the first arm,  $i=1$\\,: \n\\begin{eqnarray}\n\\label{eq:firstline}\n& \\ln H(b_{\\max}|\\text{0 observed}) \\simeq -(n_1 + 1)D\\bigg(\\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1}, \\pi_s - \\frac{\\pi_s}{n}\\bigg) - n_2D\\bigg(\\hat{\\pi}_2,\\pi_s - \\frac{\\pi_s}{n}\\bigg) \\\\ \n\\label{eq:secondline}\n&\\simeq -(n_1+1)\\bigg[D(\\hat{\\pi}_1 , \\pi_s) - \\frac{\\hat{\\pi}_1}{n_1}\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) - \\frac{\\pi_s}{n} \\frac{\\pi_s - \\hat{\\pi}_1}{\\pi_s(1-\\pi_s)}\\bigg]\n\n- n_2\\bigg[D(\\hat{\\pi}_2,\\pi_s) - \\frac{\\pi_s}{n}\\frac{\\pi_s - \\hat{\\pi}_2}{\\pi_s(1-\\pi_s)}\\bigg] \\\\\n& \\simeq -(n_1 + 1)D(\\hat{\\pi}_1 , \\pi_s) - n_2D(\\hat{\\pi}_2,\\pi_s) + \\frac{n_1}{n}\\frac{\\pi_s - \\hat{\\pi}_1}{(1-\\pi_s)} + \\frac{n_2}{n}\\frac{\\pi_s - \\hat{\\pi}_2}{(1-\\pi_s)} + \\hat{\\pi}_1\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) \\label{eq:logH0_1}\\,.\n\\label{eq:thirdline}\n\\end{eqnarray}\nThe first asymptotic equality \\eqref{eq:firstline} follows from \\eqref{eq:entdom} and \\eqref{eq:transform}.\nThe second line \\eqref{eq:secondline} is obtained by expanding $D(p,q)$ to first order in its Taylor series for both arguments, which  \nis legitimate as $n_1,n_2\\gg 1$. Finally, for the third line \\eqref{eq:thirdline} we ignore subdominant terms $o(1)$. Notice that the sum of the third and the fourth terms in \\eqref{eq:logH0_1} vanishes due to \\eqref{eq:pisdef}. \n\nWe conclude that \n\n", "index": 47, "text": "\\begin{equation}\n\\Delta \\ln H(b_{\\max}|\\text{0 observed})\\sim -D(\\hat{\\pi}_1 , \\pi_s)  + \\hat{\\pi}_1\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\,.\n\\label{eq:dlogH0}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\Delta\\ln H(b_{\\max}|\\text{0 observed})\\sim-D(\\hat{\\pi}_{1},\\pi_{s})+\\hat{\\pi}%&#10;_{1}\\ln\\bigg{(}\\frac{\\hat{\\pi}_{1}}{1-\\hat{\\pi}_{1}}\\frac{1-\\pi_{s}}{\\pi_{s}}%&#10;\\bigg{)}\\,.\" display=\"block\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mi>ln</mi><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>max</mi></msub><mo stretchy=\"false\">|</mo><mtext>0 observed</mtext><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mo>-</mo><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mi>ln</mi><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mfrac><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow></mfrac><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mi>s</mi></msub></mrow><msub><mi>\u03c0</mi><mi>s</mi></msub></mfrac><mo maxsize=\"210%\" minsize=\"210%\" rspace=\"4.2pt\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nFinally, combining \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1}, we obtain that\n\n", "itemtype": "equation", "pos": 52754, "prevtext": "\nSimilarly to \\eqref{eq:dlogH0}, when the outcome of the play on the first arm is a win\\,: \n\\begin{eqnarray}\n& \\ln H(b_{\\max}|\\text{1 observed}) \\sim -(n_1 + 1)D\\bigg(\\hat{\\pi}_1 + \\frac{1-\\hat{\\pi}_1}{n_1}, \\pi_s + \\frac{1-\\pi_s}{n}\\bigg) - n_2D\\bigg(\\hat{\\pi}_2,\\pi_s + \\frac{1-\\pi_s}{n}\\bigg) \\\\ \n&\\simeq -(n_1+1)D(\\hat{\\pi}_1 , \\pi_s) - n_2D(\\hat{\\pi}_2,\\pi_s) - \\left(1-\\hat{\\pi}_1\\right)\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\,,\n\\label{eq:ssecondline}\n\\end{eqnarray}\nwhere a cancellation similar to the one in \\eqref{eq:thirdline} simplified the final expression \\eqref{eq:ssecondline}. \nWe are thereby left with\n\n", "index": 49, "text": "\\begin{align}\\label{eq:dlogH1}\n\\Delta \\ln H(b_{\\max}|\\text{1 observed})\\approx -D(\\hat{\\pi}_1 , \\pi_s)  - (1-\\hat{\\pi}_1)\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta\\ln H(b_{\\max}|\\text{1 observed})\\approx-D(\\hat{\\pi}_{1},%&#10;\\pi_{s})-(1-\\hat{\\pi}_{1})\\ln\\bigg{(}\\frac{\\hat{\\pi}_{1}}{1-\\hat{\\pi}_{1}}%&#10;\\frac{1-\\pi_{s}}{\\pi_{s}}\\bigg{)}\\,.\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mi>ln</mi><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mi>max</mi></msub><mo stretchy=\"false\">|</mo><mtext>1 observed</mtext><mo stretchy=\"false\">)</mo></mrow><mo>\u2248</mo><mo>-</mo><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>ln</mi><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mstyle displaystyle=\"true\"><mfrac><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>\u03c0</mi><mi>s</mi></msub></mrow><msub><mi>\u03c0</mi><mi>s</mi></msub></mfrac></mstyle><mo maxsize=\"210%\" minsize=\"210%\" rspace=\"4.2pt\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nBy symmetry, $\\langle \\Delta \\ln H \\rangle_2 = -D(\\hat{\\pi}_2 , \\pi_s)$. We conclude that the decision boundary of Info-id matches the condition \\eqref{eq:optx} and the policy indeed gathers information on the identity of the best arm at the maximum possible rate. \n \n\\subsection{Why the variation of log-entropy rather than entropy?}\n\nWe stressed in the main text that Info-id is based on the expected variation of the log-entropy, as in \\eqref{eq:DeltalogH}, and not the expected variation of the entropy. The reason is that the expected variation of \nthe dominant term in \\eqref{eq:entapprox} happens to vanish for the entropy. The choice of the arm to play is then based on subdominant terms, which yields a suboptimal rate as compared to \\eqref{eq:optrate}.\nThe purpose of this subsection is to clarify this point.\n\nLet us consider the expected variation of the entropy upon playing the $i$th arm\\,: \n\\begin{eqnarray}\n&\\langle \\Delta H \\rangle_i = (1-\\hat{\\pi}_i) \\times \\Delta H(b_{\\max}|\\text{0 observed})+ \\hat{\\pi}_i \\times \\Delta H(b_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltaHSI}\n\\end{eqnarray}\nand consider first the third term \\eqref{eq:entdom} (which is the one that gives the fastest possible \ndecay \\eqref{eq:optrate}). Using again the transformations \\eqref{eq:transform}, its expected variation upon playing the first arm is\n\\begin{eqnarray}\n\\label{eq:boh}\n&\\langle \\Delta \\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big] \\rangle_1 = (1-\\hat{\\pi}_1)\\exp\\big[-(n_1+1)D\\big(\\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1}, \\pi_s - \\frac{\\pi_s}{n}\\big) -  n_2D\\big(\\hat{\\pi}_2, \\pi_s - \\frac{\\pi_s}{n}\\big)\\big] \\nonumber  \\\\ & \\!\\!\\!\\!\\!\\!+ \\hat{\\pi}_1\\!\\exp\\big[\\!-(n_1+1)D\\big(\\hat{\\pi}_1\\! +\\! \\frac{1-\\hat{\\pi}_1}{n_1}, \\pi_s + \\frac{1-\\pi_s}{n} \\big)\\! -  n_2D\\big(\\hat{\\pi}_2, \\pi_s \\!+\\! \\frac{1-\\pi_s}{n}\\big)\\big]\\! -\\! \\exp\\left[-n_1D\\left(\\hat{\\pi}_1, \\pi_s\\right) \\!-\\!  n_2D\\left(\\hat{\\pi}_2, \\pi_s\\right)\\right]\\,.\n\\end{eqnarray}\nNote that the exponents in the first two terms on the right-hand side of \\eqref{eq:boh} are related to the objects that we calculated in the previous subsection. Using \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1}, it follows then from \\eqref{eq:boh} that \n\\begin{eqnarray}\n&& \\langle \\Delta \\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big] \\rangle_1 \\propto \\left\\{(1-\\hat{\\pi}_1)\\exp\\bigg[-D(\\hat{\\pi}_1 , \\pi_s)  + \\hat{\\pi}_1\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\bigg]\\right. \\nonumber \\\\ \n &\n& \\left. +\\hat{\\pi}_1\\exp\\bigg[ -D(\\hat{\\pi}_1 , \\pi_s)  - (1-\\hat{\\pi}_1)\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) \\bigg] - 1\\right\\} \\,. \\label{eq:dH_dom}\n\\end{eqnarray}\nIf the two terms \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1} at the exponent in \\eqref{eq:dH_dom} were small, then one would Taylor expand the exponentials and conclude that the variation of the entropy and the log-entropy are proportional. However, that is not the case because \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1} are $O(1)$. By \ninserting the explicit form of the Kulback-Leibler divergence $D(p,q) = p\\ln \\frac{p}{q} + (1-p) \\ln \\frac{1-p}{1-q}$, the first and second terms on the right-hand side of \\eqref{eq:dH_dom} actually reduce to $1-\\pi_s$ and $\\pi_s$, respectively. Therefore, the expected variation in the dominant term of the entropy turns out to vanish. \n\n\\medskip\nTo determine the policy determined by the maximization of the expected decrease of entropy, we need then to consider  \nsubdominant terms in  \\eqref{eq:entapprox}. Let us start with the first one\\,:\n\n", "itemtype": "equation", "pos": 53039, "prevtext": "\nFinally, combining \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1}, we obtain that\n\n", "index": 51, "text": "\\begin{align}\n\\langle \\Delta \\ln H \\rangle_1 = -D(\\hat{\\pi}_1 , \\pi_s)\\,. \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\Delta\\ln H\\rangle_{1}=-D(\\hat{\\pi}_{1},\\pi_{s})\\,.\" display=\"inline\"><mrow><mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>1</mn></msub><mo>=</mo><mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mi>\u03c0</mi><mi>s</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nBy Taylor expanding the Kullback-Leibler divergence as we have done previously, one can check that the right-hand side \nin \\eqref{eq:bboh} is proportional to the right-hand side in \\eqref{eq:dH_dom} and the expected variation for this term vanishes as well. \n\nThe only non-vanishing contribution upon playing the first arm stems from the second term in \\eqref{eq:entapprox}\\,:\n\n", "itemtype": "equation", "pos": 56726, "prevtext": "\nBy symmetry, $\\langle \\Delta \\ln H \\rangle_2 = -D(\\hat{\\pi}_2 , \\pi_s)$. We conclude that the decision boundary of Info-id matches the condition \\eqref{eq:optx} and the policy indeed gathers information on the identity of the best arm at the maximum possible rate. \n \n\\subsection{Why the variation of log-entropy rather than entropy?}\n\nWe stressed in the main text that Info-id is based on the expected variation of the log-entropy, as in \\eqref{eq:DeltalogH}, and not the expected variation of the entropy. The reason is that the expected variation of \nthe dominant term in \\eqref{eq:entapprox} happens to vanish for the entropy. The choice of the arm to play is then based on subdominant terms, which yields a suboptimal rate as compared to \\eqref{eq:optrate}.\nThe purpose of this subsection is to clarify this point.\n\nLet us consider the expected variation of the entropy upon playing the $i$th arm\\,: \n\\begin{eqnarray}\n&\\langle \\Delta H \\rangle_i = (1-\\hat{\\pi}_i) \\times \\Delta H(b_{\\max}|\\text{0 observed})+ \\hat{\\pi}_i \\times \\Delta H(b_{\\max}|\\text{1 observed})\\,,\n\\label{eq:DeltaHSI}\n\\end{eqnarray}\nand consider first the third term \\eqref{eq:entdom} (which is the one that gives the fastest possible \ndecay \\eqref{eq:optrate}). Using again the transformations \\eqref{eq:transform}, its expected variation upon playing the first arm is\n\\begin{eqnarray}\n\\label{eq:boh}\n&\\langle \\Delta \\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big] \\rangle_1 = (1-\\hat{\\pi}_1)\\exp\\big[-(n_1+1)D\\big(\\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1}, \\pi_s - \\frac{\\pi_s}{n}\\big) -  n_2D\\big(\\hat{\\pi}_2, \\pi_s - \\frac{\\pi_s}{n}\\big)\\big] \\nonumber  \\\\ & \\!\\!\\!\\!\\!\\!+ \\hat{\\pi}_1\\!\\exp\\big[\\!-(n_1+1)D\\big(\\hat{\\pi}_1\\! +\\! \\frac{1-\\hat{\\pi}_1}{n_1}, \\pi_s + \\frac{1-\\pi_s}{n} \\big)\\! -  n_2D\\big(\\hat{\\pi}_2, \\pi_s \\!+\\! \\frac{1-\\pi_s}{n}\\big)\\big]\\! -\\! \\exp\\left[-n_1D\\left(\\hat{\\pi}_1, \\pi_s\\right) \\!-\\!  n_2D\\left(\\hat{\\pi}_2, \\pi_s\\right)\\right]\\,.\n\\end{eqnarray}\nNote that the exponents in the first two terms on the right-hand side of \\eqref{eq:boh} are related to the objects that we calculated in the previous subsection. Using \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1}, it follows then from \\eqref{eq:boh} that \n\\begin{eqnarray}\n&& \\langle \\Delta \\exp\\big[{-n_1D(\\hat{\\pi}_1, \\pi_s)  -n_2D(\\hat{\\pi}_2, \\pi_s)}\\big] \\rangle_1 \\propto \\left\\{(1-\\hat{\\pi}_1)\\exp\\bigg[-D(\\hat{\\pi}_1 , \\pi_s)  + \\hat{\\pi}_1\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg)\\bigg]\\right. \\nonumber \\\\ \n &\n& \\left. +\\hat{\\pi}_1\\exp\\bigg[ -D(\\hat{\\pi}_1 , \\pi_s)  - (1-\\hat{\\pi}_1)\\ln \\bigg( \\frac{\\hat{\\pi}_1}{1- \\hat{\\pi}_1} \\frac{1-\\pi_s}{\\pi_s}\\bigg) \\bigg] - 1\\right\\} \\,. \\label{eq:dH_dom}\n\\end{eqnarray}\nIf the two terms \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1} at the exponent in \\eqref{eq:dH_dom} were small, then one would Taylor expand the exponentials and conclude that the variation of the entropy and the log-entropy are proportional. However, that is not the case because \\eqref{eq:dlogH0} and \\eqref{eq:dlogH1} are $O(1)$. By \ninserting the explicit form of the Kulback-Leibler divergence $D(p,q) = p\\ln \\frac{p}{q} + (1-p) \\ln \\frac{1-p}{1-q}$, the first and second terms on the right-hand side of \\eqref{eq:dH_dom} actually reduce to $1-\\pi_s$ and $\\pi_s$, respectively. Therefore, the expected variation in the dominant term of the entropy turns out to vanish. \n\n\\medskip\nTo determine the policy determined by the maximization of the expected decrease of entropy, we need then to consider  \nsubdominant terms in  \\eqref{eq:entapprox}. Let us start with the first one\\,:\n\n", "index": 53, "text": "\\begin{align}\n\\langle \\Delta \\exp\\big[-n_1D(\\hat{\\pi}_1, \\hat{\\pi}_2)\\big] \\rangle_1 &= (1-\\hat{\\pi}_1)\\exp\\big[-(n_1+1)D\\big(\\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1}, \\hat{\\pi}_2 \\big)\\big]  \\nonumber  \\\\ & + \\hat{\\pi}_1\\exp\\big[-(n_1+1)D\\big(\\hat{\\pi}_1 + \\frac{1-\\hat{\\pi}_1}{n_1}, \\hat{\\pi}_2 \\big) \\big] -  \\exp\\big[-n_1D(\\hat{\\pi}_1, \\hat{\\pi}_2)\\big]\\,.  \n\\label{eq:bboh}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\Delta\\exp\\big{[}-n_{1}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\big{%&#10;]}\\rangle_{1}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(1-\\hat{\\pi}_{1})\\exp\\big{[}-(n_{1}+1)D\\big{(}\\hat{\\pi}_{1}-%&#10;\\frac{\\hat{\\pi}_{1}}{n_{1}},\\hat{\\pi}_{2}\\big{)}\\big{]}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle></mrow><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\hat{\\pi}_{1}\\exp\\big{[}-(n_{1}+1)D\\big{(}\\hat{\\pi}_{1}+\\frac{1-%&#10;\\hat{\\pi}_{1}}{n_{1}},\\hat{\\pi}_{2}\\big{)}\\big{]}-\\exp\\big{[}-n_{1}D(\\hat{\\pi}%&#10;_{1},\\hat{\\pi}_{2})\\big{]}\\,.\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle></mrow><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nExpanding again to first order in Taylor series, we get\n\n", "itemtype": "equation", "pos": 57491, "prevtext": "\nBy Taylor expanding the Kullback-Leibler divergence as we have done previously, one can check that the right-hand side \nin \\eqref{eq:bboh} is proportional to the right-hand side in \\eqref{eq:dH_dom} and the expected variation for this term vanishes as well. \n\nThe only non-vanishing contribution upon playing the first arm stems from the second term in \\eqref{eq:entapprox}\\,:\n\n", "index": 55, "text": "\\begin{align}\n\\langle \\Delta \\exp\\big[-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\big] \\rangle_1 &= (1-\\hat{\\pi}_1)\\exp\\big[-n_2D\\big(\\hat{\\pi}_2, \\hat{\\pi}_1 - \\frac{\\hat{\\pi}_1}{n_1} \\big)\\big]  \\nonumber  \\\\ & + \\hat{\\pi}_1\\exp\\big[-n_2D\\big(\\hat{\\pi}_2, \\hat{\\pi}_1 + \\frac{1-\\hat{\\pi}_1}{n_1}\\big) \\big] -  \\exp\\big[-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\big]\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\Delta\\exp\\big{[}-n_{2}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})\\big{%&#10;]}\\rangle_{1}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(1-\\hat{\\pi}_{1})\\exp\\big{[}-n_{2}D\\big{(}\\hat{\\pi}_{2},\\hat{\\pi%&#10;}_{1}-\\frac{\\hat{\\pi}_{1}}{n_{1}}\\big{)}\\big{]}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\hat{\\pi}_{1}\\exp\\big{[}-n_{2}D\\big{(}\\hat{\\pi}_{2},\\hat{\\pi}_{1%&#10;}+\\frac{1-\\hat{\\pi}_{1}}{n_{1}}\\big{)}\\big{]}-\\exp\\big{[}-n_{2}D(\\hat{\\pi}_{2}%&#10;,\\hat{\\pi}_{1})\\big{]}\\,.\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe terms in the curly braces have $n_2$ and $n_1$ only as ratios and tend to {\\em non-vanishing} \nconstants in the asymptotic limit. \nThe asymptotic behavior is therefore dominated by the exponential decay in $n_2$. The expected variation upon playing \nthe second arm of the bandit is obtained by interchanging indices. We conclude that\n\n", "itemtype": "equation", "pos": 57908, "prevtext": "\nExpanding again to first order in Taylor series, we get\n\n", "index": 57, "text": "\\begin{align}\n\\langle \\Delta \\exp\\big[-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\big] \\rangle_1 &= \\exp\\big[-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\big] \\bigg\\{(1-\\hat{\\pi}_1)\\exp\\bigg[ \\frac{\\hat{\\pi}_1n_2}{n_1} \\frac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\hat{\\pi}_1(1-\\hat{\\pi}_1)} \\bigg] \\\\ & + \\hat{\\pi}_1 \\exp\\bigg[ -\\frac{(1-\\hat{\\pi}_1)n_2}{n_1} \\frac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\hat{\\pi}_1(1-\\hat{\\pi}_1)} \\bigg] \\bigg\\}\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\Delta\\exp\\big{[}-n_{2}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})\\big{%&#10;]}\\rangle_{1}\" display=\"inline\"><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\exp\\big{[}-n_{2}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})\\big{]}\\bigg{\\{}(%&#10;1-\\hat{\\pi}_{1})\\exp\\bigg{[}\\frac{\\hat{\\pi}_{1}n_{2}}{n_{1}}\\frac{\\hat{\\pi}_{1%&#10;}-\\hat{\\pi}_{2}}{\\hat{\\pi}_{1}(1-\\hat{\\pi}_{1})}\\bigg{]}\" display=\"inline\"><mrow><mo>=</mo><mi>exp</mi><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mo>-</mo><msub><mi>n</mi><mn>2</mn></msub><mi>D</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">{</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mi>exp</mi><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\hat{\\pi}_{1}\\exp\\bigg{[}-\\frac{(1-\\hat{\\pi}_{1})n_{2}}{n_{1}}%&#10;\\frac{\\hat{\\pi}_{1}-\\hat{\\pi}_{2}}{\\hat{\\pi}_{1}(1-\\hat{\\pi}_{1})}\\bigg{]}%&#10;\\bigg{\\}}\\,.\" display=\"inline\"><mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mi>exp</mi><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub></mrow><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\" rspace=\"4.2pt\">}</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\n\nIt follows from \\eqref{eq:bbboh} that the behavior of the policy based on the maximization of the expected reduction of entropy \ndepends on the balance between subdominant terms and that the decision boundary satisfies the relation \n\n", "itemtype": "equation", "pos": 58661, "prevtext": "\nThe terms in the curly braces have $n_2$ and $n_1$ only as ratios and tend to {\\em non-vanishing} \nconstants in the asymptotic limit. \nThe asymptotic behavior is therefore dominated by the exponential decay in $n_2$. The expected variation upon playing \nthe second arm of the bandit is obtained by interchanging indices. We conclude that\n\n", "index": 59, "text": "\\begin{align}\n\\langle \\Delta H \\rangle_1 \\sim \\exp\\big[-n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\big] \\,;\\qquad\n\\langle \\Delta H \\rangle_2 \\sim \\exp\\big[-n_1D(\\hat{\\pi}_1, \\hat{\\pi}_2)\\big]\\,.\n\\label{eq:bbboh}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\langle\\Delta H\\rangle_{1}\\sim\\exp\\big{[}-n_{2}D(\\hat{\\pi}_{2},%&#10;\\hat{\\pi}_{1})\\big{]}\\,;\\qquad\\langle\\Delta H\\rangle_{2}\\sim\\exp\\big{[}-n_{1}D%&#10;(\\hat{\\pi}_{1},\\hat{\\pi}_{2})\\big{]}\\,.\" display=\"inline\"><mrow><mrow><mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>H</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>1</mn></msub><mo>\u223c</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow><mo rspace=\"22.5pt\">;</mo><mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>H</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow><mn>2</mn></msub><mo>\u223c</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mo>-</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"4.2pt\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe relations \\eqref{eq:dH_decisionbound} should be contrasted with \\eqref{eq:optx} and \\eqref{eq:optn1}.\n\nIt remains to show that the decay of the average log-entropy generated by the policy \\eqref{eq:dH_decisionbound} is still given by the third term in \\eqref{eq:entapprox} with the exponent evaluated at $x=\\tilde{x}$ (and not $x_o$ as for the optimal policy \\eqref{eq:optx}). The inequality to be proved is\\,:\n\n", "itemtype": "equation", "pos": 59107, "prevtext": "\n\nIt follows from \\eqref{eq:bbboh} that the behavior of the policy based on the maximization of the expected reduction of entropy \ndepends on the balance between subdominant terms and that the decision boundary satisfies the relation \n\n", "index": 61, "text": "\\begin{align}\\label{eq:dH_decisionbound}\nn_1D(\\hat{\\pi}_1, \\hat{\\pi}_2) = n_2D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\quad \\Rightarrow \\quad \\tilde{x}=\\frac{D(\\hat{\\pi}_1, \\hat{\\pi}_2)}{D(\\hat{\\pi}_1, \\hat{\\pi}_2)+D(\\hat{\\pi}_2, \\hat{\\pi}_1) }\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle n_{1}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})=n_{2}D(\\hat{\\pi}_{2},\\hat{%&#10;\\pi}_{1})\\quad\\Rightarrow\\quad\\tilde{x}=\\frac{D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})}{%&#10;D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})+D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})}\\,.\" display=\"inline\"><mrow><mrow><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mo>\u21d2</mo></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwith $\\tilde{\\pi}_s=\\tilde{x}\\hat{\\pi}_1+\\left(1-\\tilde{x}\\right)\\hat{\\pi}_2$. The convexity in the second argument of the Kullback-Leibler \ndivergence gives\n\n", "itemtype": "equation", "pos": 59768, "prevtext": "\nThe relations \\eqref{eq:dH_decisionbound} should be contrasted with \\eqref{eq:optx} and \\eqref{eq:optn1}.\n\nIt remains to show that the decay of the average log-entropy generated by the policy \\eqref{eq:dH_decisionbound} is still given by the third term in \\eqref{eq:entapprox} with the exponent evaluated at $x=\\tilde{x}$ (and not $x_o$ as for the optimal policy \\eqref{eq:optx}). The inequality to be proved is\\,:\n\n", "index": 63, "text": "\\begin{align}\n\\tilde{x}D(\\hat{\\pi}_1, \\tilde{\\pi}_s) + (1-\\tilde{x})D(\\hat{\\pi}_2, \\tilde{\\pi}_s) \\le \\tilde{x}D(\\hat{\\pi}_1, \\hat{\\pi}_2)= (1-\\tilde{x})D\\left(\\hat{\\pi}_2, \\hat{\\pi}_1\\right)\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{x}D(\\hat{\\pi}_{1},\\tilde{\\pi}_{s})+(1-\\tilde{x})D(\\hat{\\pi%&#10;}_{2},\\tilde{\\pi}_{s})\\leq\\tilde{x}D(\\hat{\\pi}_{1},\\hat{\\pi}_{2})=(1-\\tilde{x}%&#10;)D\\left(\\hat{\\pi}_{2},\\hat{\\pi}_{1}\\right)\\,,\" display=\"inline\"><mrow><mrow><mrow><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo rspace=\"4.2pt\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nSumming up the two inequalities above and using \\eqref{eq:dH_decisionbound}, the required relation is obtained. \n\nIn summary, the policy that maximizes the reduction of entropy (rather than the reduction of log-entropy) yields\n\n", "itemtype": "equation", "pos": 60133, "prevtext": "\nwith $\\tilde{\\pi}_s=\\tilde{x}\\hat{\\pi}_1+\\left(1-\\tilde{x}\\right)\\hat{\\pi}_2$. The convexity in the second argument of the Kullback-Leibler \ndivergence gives\n\n", "index": 65, "text": "\\begin{align}\n\\tilde{x}D(\\hat{\\pi}_1, \\tilde{\\pi}_s) &\\le \\tilde{x}(1-\\tilde{x})D(\\hat{\\pi}_1, \\hat{\\pi}_2), \\\\ (1-\\tilde{x})D(\\hat{\\pi}_2,\\tilde{\\pi}_s) &\\le (1-\\tilde{x})\\tilde{x}D(\\hat{\\pi}_2, \\hat{\\pi}_1)\\,.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{x}D(\\hat{\\pi}_{1},\\tilde{\\pi}_{s})\" display=\"inline\"><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\tilde{x}(1-\\tilde{x})D(\\hat{\\pi}_{1},\\hat{\\pi}_{2}),\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1-\\tilde{x})D(\\hat{\\pi}_{2},\\tilde{\\pi}_{s})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq(1-\\tilde{x})\\tilde{x}D(\\hat{\\pi}_{2},\\hat{\\pi}_{1})\\,.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nThe decay is slower than for \nthe optimal value  \\eqref{eq:optx}, which was derived by extremizing over $x$ to obtain the optimal value $x_o$.\nIn Figure \\ref{fig:comparison}, we confirm the theoretical predictions and compare the regret and the entropy for \nthe two algorithms.  \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{infoid_jsp_app.eps}\n\\caption{The average log-entropy $\\overline{\\ln H(b_{\\max})}$ and the average regret $R = \\overline{n}_2(p_1-p_2)$ \nfor the two policies that greedily maximize the expected reduction of the entropy ($-\\langle\\Delta H(b_{\\max}) \\rangle$) and \nthe expected reduction of the log-entropy ($-\\langle\\Delta \\ln H(b_{\\max}) \\rangle$). Numerical results from \nsimulations are shown by green circles and red squares, respectively. Black lines with circular and square symbols are the corresponding theoretical predictions \\eqref{eq:nonso} and \\eqref{eq:optrate}, respectively. The values of the two probabilities of success are $p_1 = 0.9$ and $p_2 = 0.6$, differing from the ones used in the other figures in order to enhance the difference in entropies of the two strategies. The entropy for the Info-id strategy decays faster, although the difference is small. The regret of Info-id is bigger, as expected from the proportionality between average regret and rate of decay of $\\ln H(b_{\\max})$ discussed in the main text.}\\label{fig:comparison}\n\\end{center}\n\\end{figure*}\n\n\\section{Quantifying the value of information}\\label{a3}\nThe value of information is the reduction in the average regret obtained when some {\\it a priori} information is available. In this section, we provide details on the theoretical argument sketched in the main text. The initial entropy of the identity of the best arm is supposed to be $H(b_{\\max}) = H_0=\\frac{\\ln 2}{2^m}$. \n\nAs mentioned in the main text, the ``pre-training'' with Info-id lasts for $n^{(pt)}$ steps. Since \\eqref{eq:entdom} implies that \n$\\ln H(b_{\\max})=-nD(\\hat{\\pi}_1, \\pi_{s,o})$ , the number of steps $n^{(pt)}$ satisfies $n^{(pt)}\\simeq m\\ln 2/D(\\hat{\\pi}_1, \\pi_{s,o})$ with \n$\\pi_{s,o}$ given by \\eqref{eq:optpsi}. \n\nDuring the pre-training, the two arms are played $n_1^{(pt)}$  and $n_2^{(pt)}$ times. Their respective proportions are \ncontrolled by the expression \\eqref{eq:optn1}. \nIn particular, $n_2^{(pt)}=n^{(pt)}(\\hat{\\pi}_1-\\pi_{s,o})/(\\hat{\\pi}_1-\\hat{\\pi}_2)$. Note that $n_2^{(pt)}$ scales linearly \nwith $n^{(pt)}$ and is therefore much bigger than for typical Info-p statistics, where it would scale logarithmically with $n^{(pt)}$.\n\nSince the suboptimal arm has been vastly overplayed in comparison with the typical Info-p statistics, once the algorithm switches to  Info-p after the pre-training, a long stretch of plays of the best arm will ensue. The length $\\ell$ of the stretch is estimated by calculating the time taken to reach the Info-p decision boundary, i.e. $\\ln \\ell\\sim n_2^{(pt)}D(\\hat{\\pi}_2,\\hat{\\pi}_1)$. \nIn the absence of any pre-training, \na stretch of length $\\ell$ would lead to an average regret $R=(p_1-p_2)\\ln \\ell/D(p_2,p_1)$ (see the Lai-Robbins bound \\eqref{eq:lairob} in the main text). We conclude that \nthe expected difference in regret $\\Delta R$ between the case with prior information and the case without, is given by \n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 60584, "prevtext": "\nSumming up the two inequalities above and using \\eqref{eq:dH_decisionbound}, the required relation is obtained. \n\nIn summary, the policy that maximizes the reduction of entropy (rather than the reduction of log-entropy) yields\n\n", "index": 67, "text": "\\begin{equation}\n\\label{eq:nonso}\n\\overline{\\ln H\\left(b_{max}\\right)} = -\\frac{D\\left(p_1,p_2\\right)D(p_1,\\tilde{p}_s)+D(p_2,p_1)D(p_2,\\tilde{p}_s)}{D(p_1, p_2)+D(p_2,p_1) }\\,;\\qquad \\tilde{p}_s=\\frac{p_1D\\left(p_1,p_2\\right)+p_2D(p_2,p_1)}{D(p_1, p_2)+D(p_2,p_1)}\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m1\" class=\"ltx_Math\" alttext=\"\\overline{\\ln H\\left(b_{max}\\right)}=-\\frac{D\\left(p_{1},p_{2}\\right)D(p_{1},%&#10;\\tilde{p}_{s})+D(p_{2},p_{1})D(p_{2},\\tilde{p}_{s})}{D(p_{1},p_{2})+D(p_{2},p_%&#10;{1})}\\,;\\qquad\\tilde{p}_{s}=\\frac{p_{1}D\\left(p_{1},p_{2}\\right)+p_{2}D(p_{2},%&#10;p_{1})}{D(p_{1},p_{2})+D(p_{2},p_{1})}\\,.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>H</mi></mrow><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>b</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>\u00af</mo></mover><mo>=</mo><mrow><mo>-</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mpadded></mrow></mrow><mo rspace=\"22.5pt\">;</mo><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">~</mo></mover><mi>s</mi></msub><mo>=</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>p</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03073.tex", "nexttext": "\nwith $p_s=\\left(1+e^{f(p_1,p_2)}\\right)^{-1}$ and the function $f$ defined by \\eqref{eq:optn1}. The agreement with numerical simulations is shown in Fig.~4. Small deviations are ascribed to finite-size effects, e.g. the Info-p decision boundary that we used to determine the length $\\ell$ of the initial stretch is only asymptotically valid, as evidenced in Fig.~2 (upper left panel). \n\n\n\n\n\n\\bibliography{prx_bandits}   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 64165, "prevtext": "\nThe decay is slower than for \nthe optimal value  \\eqref{eq:optx}, which was derived by extremizing over $x$ to obtain the optimal value $x_o$.\nIn Figure \\ref{fig:comparison}, we confirm the theoretical predictions and compare the regret and the entropy for \nthe two algorithms.  \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=.8\\textwidth]{infoid_jsp_app.eps}\n\\caption{The average log-entropy $\\overline{\\ln H(b_{\\max})}$ and the average regret $R = \\overline{n}_2(p_1-p_2)$ \nfor the two policies that greedily maximize the expected reduction of the entropy ($-\\langle\\Delta H(b_{\\max}) \\rangle$) and \nthe expected reduction of the log-entropy ($-\\langle\\Delta \\ln H(b_{\\max}) \\rangle$). Numerical results from \nsimulations are shown by green circles and red squares, respectively. Black lines with circular and square symbols are the corresponding theoretical predictions \\eqref{eq:nonso} and \\eqref{eq:optrate}, respectively. The values of the two probabilities of success are $p_1 = 0.9$ and $p_2 = 0.6$, differing from the ones used in the other figures in order to enhance the difference in entropies of the two strategies. The entropy for the Info-id strategy decays faster, although the difference is small. The regret of Info-id is bigger, as expected from the proportionality between average regret and rate of decay of $\\ln H(b_{\\max})$ discussed in the main text.}\\label{fig:comparison}\n\\end{center}\n\\end{figure*}\n\n\\section{Quantifying the value of information}\\label{a3}\nThe value of information is the reduction in the average regret obtained when some {\\it a priori} information is available. In this section, we provide details on the theoretical argument sketched in the main text. The initial entropy of the identity of the best arm is supposed to be $H(b_{\\max}) = H_0=\\frac{\\ln 2}{2^m}$. \n\nAs mentioned in the main text, the ``pre-training'' with Info-id lasts for $n^{(pt)}$ steps. Since \\eqref{eq:entdom} implies that \n$\\ln H(b_{\\max})=-nD(\\hat{\\pi}_1, \\pi_{s,o})$ , the number of steps $n^{(pt)}$ satisfies $n^{(pt)}\\simeq m\\ln 2/D(\\hat{\\pi}_1, \\pi_{s,o})$ with \n$\\pi_{s,o}$ given by \\eqref{eq:optpsi}. \n\nDuring the pre-training, the two arms are played $n_1^{(pt)}$  and $n_2^{(pt)}$ times. Their respective proportions are \ncontrolled by the expression \\eqref{eq:optn1}. \nIn particular, $n_2^{(pt)}=n^{(pt)}(\\hat{\\pi}_1-\\pi_{s,o})/(\\hat{\\pi}_1-\\hat{\\pi}_2)$. Note that $n_2^{(pt)}$ scales linearly \nwith $n^{(pt)}$ and is therefore much bigger than for typical Info-p statistics, where it would scale logarithmically with $n^{(pt)}$.\n\nSince the suboptimal arm has been vastly overplayed in comparison with the typical Info-p statistics, once the algorithm switches to  Info-p after the pre-training, a long stretch of plays of the best arm will ensue. The length $\\ell$ of the stretch is estimated by calculating the time taken to reach the Info-p decision boundary, i.e. $\\ln \\ell\\sim n_2^{(pt)}D(\\hat{\\pi}_2,\\hat{\\pi}_1)$. \nIn the absence of any pre-training, \na stretch of length $\\ell$ would lead to an average regret $R=(p_1-p_2)\\ln \\ell/D(p_2,p_1)$ (see the Lai-Robbins bound \\eqref{eq:lairob} in the main text). We conclude that \nthe expected difference in regret $\\Delta R$ between the case with prior information and the case without, is given by \n\n\n\n\n\n\n\n\n", "index": 69, "text": "\\begin{align}\n\\Delta R \\simeq-\\left(p_1-p_2\\right)\\overline{n_2^{(pt)}}\\simeq -\\ln 2\\frac{p_1 -p_s}{D(p_1,p_s)}m\\,,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta R\\simeq-\\left(p_{1}-p_{2}\\right)\\overline{n_{2}^{(pt)}}%&#10;\\simeq-\\ln 2\\frac{p_{1}-p_{s}}{D(p_{1},p_{s})}m\\,,\" display=\"inline\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>\u2243</mo><mrow><mo>-</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>-</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><msubsup><mi>n</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u00af</mo></mover></mrow></mrow><mo>\u2243</mo><mrow><mo>-</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mn>2</mn><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>-</mo><msub><mi>p</mi><mi>s</mi></msub></mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mi>m</mi></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]