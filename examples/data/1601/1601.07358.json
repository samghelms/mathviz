[{"file": "1601.07358.tex", "nexttext": "\nAny algorithm that results in a $\\hat{U}$ such that $p$ approaches 1\naccomplishes this task.\n\nAssume now that (ii) we are required to transform a given orthonormal basis\n(ONB) $\\{|s_i\\rangle\\}$ into another given ONB $\\{|a_i\\rangle\\}$ of a vector\nspace of same dimension, but we are not told which state is to be transformed\ninto which other state. We could build a quantum device that implements some\nunitary $\\hat{U}_{\\mathrm{T}}$ such that\n$|a_i\\rangle$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}|s_i\\rangle$. Preparing the system in\nstate $|s_i\\rangle$ and measuring in the second basis gives outcome\n$|a_i\\rangle$. One may consider the problem as a (trivial) learning task, namely\nthat of an identical mapping of the state-indices $i$. However, if we do not\nknow from the beginning what kind of mapping the solution is, we have to learn\nit. In our quantum device, we would tune $\\hat{U}$ until it gives the desired\nmeasurement statistics. Inspired by \\cite{Bri12}, we call this task\n``invasion game''. To solve it, we initialize the device in states $|s_i\\rangle$\nchosen randomly from the given ONB, while the measurement is done in the second\nONB formed by the $|a_i\\rangle$. The algorithm will drive $\\hat{U}$ to some\nunitary $\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$, where\n$\\hat{U}_1$ ($\\hat{U}_2^\\prime$) are undetermined unitaries which are diagonal\nin the basis $\\{|s_i\\rangle\\}$ ($\\{|a_i\\rangle\\}$).\n\nIf (iii) the percept states are random, this phase freedom is removed up to a\nglobal phase. In the simplest case, we draw the initial states of the device\nfrom an ``overcomplete'' basis, where the set of all possible states is linearly\ndependent. For a $n$-level system, this can be accomplished by (randomly)\nchoosing $n$ SU($n$)-unitaries. During each state initialisation, we then take\none $\\hat{U}_{\\mathrm{R}}$ from this set, a random $|s_i\\rangle$ from our first\nONB, and then prepare the device in a state $\\hat{U}_{\\mathrm{R}}|s_i\\rangle$.\nConsequently, the measurement is done in a transformed basis formed by the\n$\\hat{U}_{\\mathrm{T}}\\hat{U}_{\\mathrm{R}}\\hat{U}_{\\mathrm{T}}^{-1}|a_i\\rangle$\nrather than the $|a_i\\rangle$ themselves.\n\nIn this sense, the navigation of (i) a given input state, (ii) a given ONB, and\n(iii) random states can be described as a navigation of unitaries $\\hat{U}$ with\na varying amount of freedom.\nWhile formally, all three cases (i)-(iii) can be considered as special cases of\na navigation of $\\hat{U}(\\bm{h})$ to a point (\\ref{task}), where a percept\nstatistics-based fidelity (\\ref{F2}) becomes maximum, practically they can be\naccomplished in RL by means of the mentioned reward signal, independently of the\navailability of analytic solutions. In what follows, we consider $\\hat{U}$ as a\nmemory of an RL-agent, that solves tasks arising from its interaction with an\nenvironment.\n\n\\section{\n\\label{sec2}\nA cybernetic perspective}\n\nThe scheme is depicted in Fig.~\\ref{fig1}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig1.eps}\n\\caption{\\label{fig1}\n\nAgent-environment interaction as a feedback scheme. The $s$ are percepts, which\ninitialise the agent's memory in a quantum state $\\hat{\\varrho}({s})$. Choice of\nan action $a$ is made by a measurement process as described by a given POVM\n$\\hat{\\Pi}$. Depending on the rewards $r$ given by the environment, the memory\nis updated at the end of each cycle $t$. The memory can also be modified by\ninternal loops based on a numerical objective NO (dotted line) or measurements\n(dash-dotted line).\n}\n\\end{figure}\n\nThe agent is equipped with some quantum channel that acts as its memory whose\nproperties can be modified by control parameters denoted by a vector $\\bm{h}$.\nExamples of memory structures are listed in Fig.~\\ref{fig2}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig2.eps}\n\\caption{\\label{fig2}\n\nExamples for the agent's memory as shown in Fig.~\\ref{fig1}. (a) unitary\nevolution of the percept states, (b) open system evolution due to interaction\nwith a bath B, (c) composite memory with coupled subsystems for percept (S) and\naction (A) variables, (d) extends (c) to an open system evolution analogous to\n(b) extending (a). Further ancilla systems may be added (not shown), to account\nfor, e.g., emotion degrees of freedom introduced in \\cite{Bri12}.\n}\n\\end{figure}\n\nIn Fig.~\\ref{fig2} and in what follows, we refer to the memory operation by\nmeans of some unitary $\\hat{U}$ for notational simplicity. Since any quantum\nprocess can be treated as unitary on an enlarged space, this is not a conceptual\nrestriction. The agent interacts with an external environment in discrete\ncycles $t$. At the beginning of a cycle, the agent receives (via sensors) some\npercept $s$, which it encodes as a quantum state $\\hat{\\varrho}({s})$, in which\nits memory is prepared. After transformation of $\\hat{\\varrho}({s})$ by the\nmemory channel, a quantum measurement is performed, where we assume for\nsimplicity that the positive operator valued measure (POVM) $\\{\\hat{\\Pi}\\}$\ndescribing this measurement is fixed. Depending on the outcome of this\nmeasurement, an action $a$ is selected and performed on the environment (via\nactuators), which completes the cycle. The environment reacts with a new percept\nand a reward $r$, which are perceived by the agent during the following cycle.\nDepending on the reward, some adjustments are made on the control parameters,\nwhich modify the properties of the memory channel (i.e., its ``hardware''). This\nfeedback loop is adapted from the classical schemes in \\cite{bookSuttonBarto}\nand \\cite{bookRussellNorvig}, where the percepts $s$  in Fig.~\\ref{fig1}\ncorrespond to the states in \\cite{bookSuttonBarto}. The agent's interaction with\nthe environment is here considered classical in the sense that percepts, actions\nand rewards are classical signals. The environment itself is not specified, it\ncould represent, e.g., an experiment performed on a quantum system. Note that\nthe environment in Fig.~\\ref{fig1} is not to be confused with the bath in\nFig.~\\ref{fig2}, which affects the memory channel but is not considered part of\nthe agents ``habitat''.\n\nIn addition to the external loop, we may also equip the agent with two types of\ninternal feedback loops, which allow the agent to undertake what corresponds to\n``planning steps'' in RL and ``reflection'' in PS. One type is similar to the\nexternal loop in that it involves state initialisations and measurements on the\nmemory channel, but exploits that percepts, actions and rewards can be recorded\nand reproduced as a consequence of their classicality. The second type of\ninternal loop does not involve state evolutions but requires some mathematical\nmodel of the memory channel itself, which is used to directly calculate a\nnumerical objective (NO), whose value is used to alter the control parameters.\nFig.~\\ref{fig1} does not imply that all of these loops need to be\nsimultaneously present, they are rather thought of either subprocesses within an\noverall agent scheme or possible modes of its operation. The numerical examples\nin this work will exclusively apply the external loop.\n\n\nAll three loops involve a parameter update $\\delta\\bm{h}$. In a ``first-order''\nupdate, $\\delta\\bm{h}$ is proportional to some quantity that depends on the\ngradient $\\bm{\\nabla}\\hat{U}$ of $\\hat{U}$ with respect to $\\bm{h}$. This\ngradient can either be computed directly from a memory model $\\hat{U}(\\bm{h})$\n(i.e., from some symbolic expression of $\\bm{\\nabla}\\hat{U}$ if available) or\nestimated from measurements. These ``measurements'' can be physical (POVM in\nFig.~\\ref{fig1}) or numerical (NO in Fig.~\\ref{fig1}). For the estimation, one\nvaries the components of $\\bm{h}$ by a small amount and records the changes in\nthe measured POVM or computed NO. Here are some elementary examples:\n\\emph{(1a)} A simulation of an external loop with a given model-based\n(i.e. analytic) $\\bm{\\nabla}\\hat{U}$ is performed in Sec.~\\ref{sec:ig22}\n(Fig.~\\ref{fig5}) for the case Fig.~\\ref{fig2}(c), in Sec.~\\ref{sec:ig44}\n(Figs.~\\ref{fig7}-\\ref{fig8}) for the case Fig.~\\ref{fig2}(a), and in\nSec.~\\ref{sec:gw} (Figs.~\\ref{fig11} and \\ref{fig12}) for the case\nFig.~\\ref{fig2}(c). \n\\emph{(1b)}  A simulation of an external loop with a POVM measurement-based\n$\\bm{\\nabla}\\hat{U}$ is carried out in \\cite{Cla15} (Fig. 6) for the case\nFig.~\\ref{fig2}(b).\n\\emph{(2)} A NO-based internal loop with a model-based $\\bm{\\nabla}\\hat{U}$ is\nconsidered in \\cite{clausen18} for the case Fig.~\\ref{fig2}(b) and in\n\\cite{Cla15} (Figs.2-4) for the case Fig.~\\ref{fig2}(a).\n\\emph{(3)} The POVM-based internal loop in Fig.~\\ref{fig1} can be used to\nestimate $\\bm{\\nabla}\\hat{U}$ in the absence of a model $\\hat{U}(\\bm{h})$ of the\nagent memory. To this end, one of the agent's possibilities consists in\ninserting a number of internal cycles between each external cycle, where it\nrepeatedly prepares its memory in the latest percept state and observes how a\nvariation $\\delta\\bm{h}$ affects the measurement statistics. A discussion of\nthis will be given in Sec.~\\ref{sec6}. Beyond these examples, all three loops\ncan be interlaced with each other in various ways, analogous to the wealth of\napproaches reviewed in \\cite{bookSuttonBarto}.\n\n\\section{Update rule in parameter space}\n\nFor the cycle-wise update of the control parameters $\\bm{h}$ of the memory\nchannel $\\hat{U}$, we apply a rule\n\n", "itemtype": "equation", "pos": 11687, "prevtext": "\n\n\n\\title{Quantum machine learning with glow for episodic tasks and decision games}\n\\date{\\today}\n\\author{Jens Clausen}\n\\affiliation{Institut f\\\"ur Theoretische Physik,\n             Universit\\\"at Innsbruck,\n             Technikerstra\\ss{}e 21a,\n             A-6020 Innsbruck,\n\t     Austria}\n\\author{Hans J. Briegel}\n\\affiliation{Institut f\\\"ur Theoretische Physik,\n             Universit\\\"at Innsbruck,\n             Technikerstra\\ss{}e 21a,\n             A-6020 Innsbruck,\n\t     Austria}\n\\begin{abstract}\nWe consider a general class of models, where a reinforcement learning (RL) agent\nlearns from cyclic interactions with an external environment via classical\nsignals. Perceptual inputs are encoded as quantum states, which are subsequently\ntransformed by a quantum channel representing the agent's memory, while the\noutcomes of measurements performed at the channel's output determine the agent's\nactions. The learning takes place via stepwise modifications of the channel\nproperties. They are described by an update rule that is inspired by the\nprojective simulation (PS) model and equipped with a glow mechanism that allows\nfor a backpropagation of policy changes, analogous to the eligibility traces in\nRL and edge glow in PS. In this way, the model combines features of PS with the\nability for generalization, offered by its physical embodiment as a quantum\nsystem. We apply the agent to various setups of an invasion game and a grid\nworld, which serve as elementary model tasks allowing a direct comparison with a\nbasic classical PS agent.\n\\end{abstract}\n\\pacs{\n      07.05.Mh, \n      87.19.lv, \n      02.50.Le, \n      03.67.-a, \n      03.67.Ac  \n \n \n \n \n}\n\\keywords{\nlearning agents, decision making, artificial intelligence,\nreinforcement learning, projective simulation,\nquantum machine learning, \nquantum information and computation,\nquantum open systems\n}\n\n\\maketitle\n\n\n\\section{\n\\label{sec1}\nIntroduction}\n\n\n\\subsection{\nMotivation}\n\nIf we consider the development of new technologies as a collective learning\nprocess, we can distinguish between different interlaced processes. While basic\nresearch focuses on exploration characterised by a search for potential\nalternatives to established methods, the more promising an approach appears, the\nmore likely it becomes subject to subsequent exploitation, where it is optimised\nand matured with the ultimate hope to supersede what is available. An example of\nexplorative activity are early efforts to solve problems by artificial\nintelligence (AI), such as inventing unconventional heuristic techniques. AI has\nrecently regained interest \\cite{NatureSpecialIssue,ScienceSpecialIssue}, which\nmay be a consequence of new approaches to computation\n\\cite{bookNielsen,bookWittek} as well as improved capacities of classical\ncomputing and networking. The present work aims at drawing a connection between\nthe recently suggested scheme of PS \\cite{Bri12,Mau15} and quantum control\ntheory \\cite{bookAlessandro,bookWiseman}, restricting attention to example\nproblems analogous to those considered in the basic classical PS schemes\n\\cite{Bri12,Mau15,Mel14,Mel15}, rather than a treatment of practically relevant\nbut large-scale applications of, e.g., machine learning. A discussion of\nscalability, quantum speed up, or practical implementability\n\\cite{Pap14,Dun15,Fri15} is beyond the scope of this work.\n\nWe consider a class of schemes, where a quantum agent learns from cyclic\ninteractions with an external environment via classical signals. The learning\ncan be considered as an \\emph{internal} quantum navigation process of\nthe agent's ``hardware'' or ``substrate'' that forms its memory of past\nexperience. For notational convenience, we describe the memory operation as a\nunitary $\\hat{U}$ involving (information carrying and other) controllable and\nuncontrollable degrees of freedom (such as a ``bath''), where the latter are not\nnecessarily identical with the environment, on which the agent operates. While\nconceptually, the memory may hence be seen as an open quantum system\n\\cite{bookBreuer}, the numerical examples considered in the present work\nrestrict to closed system dynamics. This navigation of agent memory $\\hat{U}$\nmust be distinguished from the evolution of quantum states in which, following\nexternal or internal stimulus, the memory is excited \\cite{Bri12,Pap14}.\nLearning as an internal navigation process corresponds to the colloquial notion\nof a learner desiring to quickly ``make progress'' rather than ``marking time''.\nFor the agent's internal dynamics, we talk of a navigation process rather than\na navigation problem that is to be solved, since ultimately, the agent responds\nto its environment that is generally unknown and subject to unpredictable\nchanges.\n\nWhile the proposed PS-model is characterised by an episodic \\& compositional\nmemory (ECM), we here ignore the clip network aspect and restrict attention to a\nparameter updating that is motivated from the basic scheme \\cite{Bri12,Mau15},\nwhich we apply to simple learning tasks involving an agent equipped with a\nquantum memory. We specifically reconsider some of the examples discussed in\n\\cite{Bri12,Mel14,Mel15} in order to investigate to what extent the results can\nbe reproduced. In contrast to the classical scheme, where the parameters are\nweights in a clip network, we here refrain from ascribing a particular role,\nthey could play (e.g., in a quantum walk picture mentioned in \\cite{Bri12}).\nHere, the parameters are simply controls, although in our examples, they are\ndefined as interaction strengths in a stack of layers constituting the agent\nmemory $\\hat{U}$. This choice of construction is however not essential for the\nmain principle. From the viewpoint of the network-based classical PS, drawing a\nconnection to quantum control theory opens the possibility to apply results\nobtained in the latter field over the last years\n\\cite{Dong06a,*Dong06b,*Dong08a,*Dong08b,*Dong09,*Dong10,*Dong14,*Dong15}.\nOn the other\nhand, classical PS is similar to RL \\cite{bookSuttonBarto,bookRussellNorvig},\nwhich considers a type of problems, where an ``agent'' (embodied decision maker\nor ``controller'') learns from interaction with an environment (controlled\nsystem or ``plant'') to achieve a goal. The learning consists in developing a\n(generally stochastic) rule, the agent's ``policy'', of how to act depending on\nthe situation it faces, with the goal to accumulate ``reward'' granted by the\nenvironment. In RL, the environment is anything outside of control of this\ndecision making. The reward could describe for example pleasure or pain felt by\nan individual. It is generated within the individual's body but is beyond it's\ncontrol, and therefore considered originating in the \\emph{agent's} environment.\n Historically, RL, which must be distinguished from supervised\nlearning, originates from merging a trait in animal psychology with a trait in\ncontrol theory. Although dynamic programming as the basis of the latter is well\nunderstood, limited knowledge of the environment along with a vast number of\nconceivable situations, an RL-agent may face, render a direct solution\nimpossible in practice. Analogous to RL growing out of dynamic programming by\nrefining the updates of values, in a quantum context, one could think of\nrefining quantum control schemes with algorithmic elements that enhance their\nresource efficiency.\n\nAnother aspect is embodiment. A historical example is application-specific\nclassical optical computing with a 4F-optical correlator. A more recent effort\nis neuromorphic computing, which aims at a very-large-scale integration\n(VLSI)-based physical implementation of neural networks, whose simulation with\na conventional computer architecture is inefficient. This becomes even more\ncrucial for quantum systems, which may be implemented as superconducting solid\nstate devices, trapped ions or atoms, or wave guide-confined optical fields.\nGiven the availability of a controllable quantum system, it is hence tempting to\ntransform quantum state-encoded sensory input and select actions based on\nmeasurement outcomes. While the parameter update is typically done by some\nstandard linear temporal difference (TD)-rule, the selection of actions is in\nclassical algorithms governed by a separate stochastic rule that tries to\nbalance exploration and exploitation. This stochastic rule is described in terms\nof a policy function, that determines, how the probabilities for choosing the\nrespective actions depend on the value functions in RL, edge strengths in PS, or\ncontrols in direct policy approaches. Examples are the $\\varepsilon$-greedy and\nthe softmax-rule. The quantum measurement here serves as a direct\nphysical realisation of an action-selection, whose uncertainty allows to\nincorporate both exploration and exploitation \\cite{Dong08b}. In our context,\nthe resulting measurement-based (and hence effectively quadratic) policy forms\nan intermediate between the linear stochastic function used in \\cite{Bri12} and\nthe exponential softmax-function applied in \\cite{Mel14}. A measurement-based\npolicy can moreover be tailored on demand by the way in which classical input is\nencoded as a quantum state. One could, e.g., apply mixtures of a pure state and\na maximally mixed state to mimic an $\\varepsilon$-greedy policy function, or one\ncould use thermal input states to mimic an exponential function. In contrast to\nthe value function-based RL, our approach amounts to a direct policy search,\nwhere the agent-environment interaction employs a general state preparation\n$\\to$ transformation $\\to$ measurement scheme, that reflects the kinematic\nstructure of quantum mechanics.\n\n\\subsection{\n\\label{sec3}\nRL as a reward-driven navigation of the agent memory}\n\nConsider the specific task of mapping input states $|s_i\\rangle$ by means of a\ncontrollable unitary $\\hat{U}$ to outputs $|a_i\\rangle$. Under the (restrictive)\nassumption, that for each input there is exactly one correct output, the task is\nto learn this output from interaction with an environment. In our context, the\n$|s_i\\rangle$ ($|a_i\\rangle$) are regarded as encoded percepts (actions), while\n$\\hat{U}$ acts as memory of the learned information and can finally accomplish\nthe mapping as an embodied ``ad hoc'' computer or an ``oracle'', which is\nsimilar to learning an unknown unitary \\cite{Ban14}.\n\nConsider (i) the case where there is only one possible input state $|s\\rangle$.\nIf the task is the navigation of the output state\n$\\hat{\\varrho}$ $\\!=$ $\\!\\hat{U}|s\\rangle\\langle{s}|\\hat{U}^\\dagger$ by means of\n$\\hat{U}$ to a desired destination state $|a\\rangle$, a learning agent has to\nrealize the maximisation of the conditional probability\n$p(a|s)$ $\\!=$ $\\!\\langle{a}|\\hat{\\varrho}|a\\rangle$ by tuning $\\hat{U}$. The\nintuition behind this is that $p$ is bounded and if $\\hat{U}(\\bm{h})$ depends\nanalytically on some control vector $\\bm{h}$, the gradient with respect to\n$\\bm{h}$ must vanish at the maximum of $p$. To give a simple example, we assume\nthat $\\hat{U}(t)$ depends (rather than on $\\bm{h}$) on a single real parameter\n$t$ in a continuous and differentiable way such that it obeys the\nSchr{\\\"o}dinger equation {\\small $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}$\n$\\!=$ $\\!-\\mathrm{i}\\hat{H}\\hat{U}$} with the state boundary conditions \n$\\hat{\\varrho}(0)$ $\\!=$ $\\!|s\\rangle\\langle{s}|$ and\n$\\hat{\\varrho}(t_{\\mathrm{F}})$ $\\!=$ $\\!|a\\rangle\\langle{a}|$. This gives \n\\begin{eqnarray}\n  \\frac{\\mathrm{d}p(t)}{\\mathrm{d}t}&=&2\\mathrm{Re}\\langle{a}|\n  \\Bigl(\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}\\Bigr)|s\\rangle\\langle{s}|\n  \\hat{U}^\\dagger|a\\rangle\n  \\\\\n  &=&2\\mathrm{Im}\\langle{a}|\\hat{H}\\hat{\\varrho}(t)|a\\rangle,\n\\end{eqnarray}\nso that indeed\n\n", "index": 1, "text": "\\begin{equation}\n  \\frac{\\mathrm{d}p(t)}{\\mathrm{d}t}|_{t=t_{\\mathrm{F}}}\n  =2\\mathrm{Im}\\langle{a}|\\hat{H}|a\\rangle=0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\mathrm{d}p(t)}{\\mathrm{d}t}|_{t=t_{\\mathrm{F}}}=2\\mathrm{Im}\\langle{a}|%&#10;\\hat{H}|a\\rangle=0.\" display=\"block\"><mrow><mrow><msub><mrow><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><msub><mi>t</mi><mi mathvariant=\"normal\">F</mi></msub></mrow></msub><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">I</mi><mo>\u2062</mo><mi mathvariant=\"normal\">m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>a</mi><mo fence=\"true\" stretchy=\"false\">|</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mo fence=\"true\" stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\ninspired by the basic model of PS \\cite{Bri12,Mau15}. The number of components\n$h_k$ can range from one ($\\bm{h}$ scalar) to infinity ($\\bm{h}$ may represent a\nfunction or a vector of functions), and the $h_k$ can be assumed to be\nreal-valued without loss of generality. In \\cite{Bri12,Mau15}, the components of\n$\\bm{h}$ are the edge strengths of a directed graph representing a network of\nclips (the graph's vertices). While these clips are considered sequences of\nremembered percepts and actions, the network itself abstracts from the clip's\ninternal contents. Our view of $\\bm{h}$ as a control vector is one further\nsimplification and generalization that may allow for but does not require the\nview of the memory as a network.\n\nIn (\\ref{ur3}), $\\bm{h}$ and $\\bm{h}^\\prime$ are the control vectors before and\nafter the update at cycle $t$, respectively. $\\alpha$ $\\!\\ge$ $\\!0$ is a\n(typically small) learning rate, and $r$ is the reward given at cycle $t$.\n$\\kappa$ $\\!\\in$ $\\![0,1]$ is a relaxation rate towards some equilibrium value\n$\\bm{h}_\\infty$ in the absence of rewards. This allows for what corresponds to\nthe ``forgetting'' process suggested in \\cite{Bri12,Mau15} to account for\ndissipation in an embodied implementation and deal with time-dependent\nenvironments. A natural possibility is to identify the value $\\bm{h}_0$, with\nwhich the memory is initialised before the first cycle, with $\\bm{h}_\\infty$.\nThis could be the zero vector $\\bm{h}_0$ $\\!=$ $\\!\\bm{0}$ yielding, e.g., the\nidentity, $\\hat{U}_0$ $\\!=$ $\\!\\hat{U}(\\bm{h}_0)$ $\\!=$ $\\!\\hat{I}$. The\nlearning process will then be a reward-driven and generally stochastic\nnavigation in parameter space $\\{\\bm{h}\\}$ away from the zero vector $\\bm{0}$.\nLifted to $\\hat{U}(\\bm{h})$, this navigation starts at the identity\n$\\hat{U}_0$ $\\!=$ $\\!\\hat{I}$, that relaxes back to it in the prolonged absence\nof rewards. In this work, we consider static environments as in \\cite{Mel14},\nand hence always set $\\kappa$ $\\!=$ $\\!0$. $\\bm{D}_{t}$ is a difference vector.\nWhile some options for finite difference choices of $\\bm{D}$ are outlined in\nSec.~\\ref{sec6}, in all numerical examples within this work we restrict to the\ncase, where $\\bm{D}_{t}$ $\\!=$ $\\!\\bm{\\nabla}_{t}$ is a short-hand notation for\nthe gradient\n\\begin{eqnarray}\n\\label{grad}\n  \\bm{\\nabla}_{t}&=&\\bm{\\nabla}p({a}|{s})_t=2\\mathrm{Re}\n  \\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\bm{\\nabla}\\hat{U}\\bigr\\rangle_{t},\n\\\\\n\\label{grada}\n  p({a}|{s})&=&\\mathrm{Tr}\\bigl[\\hat{U}\\hat{\\varrho}({s})\\hat{U}^\\dagger\n  \\hat{\\Pi}({a})\\bigr]\n  =\\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\hat{U}\\bigr\\rangle,\n\\end{eqnarray}\nwith components $\\frac{\\partial{p}({a}|{s})}{\\partial{h}_k}$ at cycle $t$.\n$p({a}|{s})$ is the probability of the obtained measurement outcome ${a}$ under\nthe condition of the respective cycle's percept state $\\hat{\\varrho}({s})$,\nwhere\n$\\langle\\cdots\\rangle$ $\\!\\equiv$ $\\!\\mathrm{Tr}[\\hat{\\varrho}({s})\\cdots]$\ndenotes the expectation value with respect to this state, and $\\hat{\\Pi}({a})$\nis the member of the POVM that corresponds to measurement outcome $a$. The\nlatter determines the action performed by the agent, and we use the same symbol\nfor both. $(1$ $\\!-$ $\\!\\eta)$ describes a backward-discount rate, which we have\ndefined via a parameter $\\eta$ $\\!\\in$ $\\![0,1]$ to allow comparison with the\nglow mechanism introduced in \\cite{Mau15}.\nAs mentioned above, the unitary transformation of the respective percept states\n$\\hat{\\varrho}({s}_t)$ by the memory $\\hat{U}$ $\\!=$ $\\!\\hat{U}(\\bm{h}_t)$ at\ncycle $t$ in (\\ref{grada}) refers in general to a larger (dilated) space.\nThe dynamical semigroup of CPT maps proposed in \\cite{Bri12} is included and\ncan be recovered by referring to Fig.~\\ref{fig2}(d) [or alternatively\nFig.~\\ref{fig2}(b)] and the assumption that\n\\begin{eqnarray}\n  \\hat{\\varrho}({s}_t)\\quad(\\equiv\\hat{\\varrho}_{\\mathrm{SAB}})\n  &=&\\hat{\\varrho}_{\\mathrm{SA}}({s}_t)\n  \\otimes\\hat{\\varrho}_{\\mathrm{B}},\n\\\\\n  \\mathrm{Tr}_{\\mathrm{B}}\\left[\\hat{U}\\hat{\\varrho}({s}_t)\n  \\hat{U}^\\dagger\\right]&=&\\mathrm{e}^{\\mathcal{L}_{\\mathrm{SA}}\n  \\Delta{T}^{(\\mathrm{mem})}_t}\\hat{\\varrho}_{\\mathrm{SA}}({s}_t),\n\\end{eqnarray}\nwhere the physical memory evolution time $\\Delta{T}^{(\\mathrm{mem})}_t$ may\ndepend on the cycle $t$ for a chosen parametrisation $\\hat{U}(\\bm{h})$ and must\nbe distinguished from the agent response time that can additionally be affected\nby the potential involvement of internal loops in Fig.~\\ref{fig1}. The\nsuperoperator $\\mathcal{L}$ $\\!=$ $\\!\\mathcal{L}_{\\mathrm{SA}}$, whose effect on\n$\\hat{\\varrho}$ $\\!=$ $\\!\\hat{\\varrho}_{\\mathrm{SA}}$ is defined as a sum\n\n", "itemtype": "equation", "pos": 21153, "prevtext": "\nAny algorithm that results in a $\\hat{U}$ such that $p$ approaches 1\naccomplishes this task.\n\nAssume now that (ii) we are required to transform a given orthonormal basis\n(ONB) $\\{|s_i\\rangle\\}$ into another given ONB $\\{|a_i\\rangle\\}$ of a vector\nspace of same dimension, but we are not told which state is to be transformed\ninto which other state. We could build a quantum device that implements some\nunitary $\\hat{U}_{\\mathrm{T}}$ such that\n$|a_i\\rangle$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}|s_i\\rangle$. Preparing the system in\nstate $|s_i\\rangle$ and measuring in the second basis gives outcome\n$|a_i\\rangle$. One may consider the problem as a (trivial) learning task, namely\nthat of an identical mapping of the state-indices $i$. However, if we do not\nknow from the beginning what kind of mapping the solution is, we have to learn\nit. In our quantum device, we would tune $\\hat{U}$ until it gives the desired\nmeasurement statistics. Inspired by \\cite{Bri12}, we call this task\n``invasion game''. To solve it, we initialize the device in states $|s_i\\rangle$\nchosen randomly from the given ONB, while the measurement is done in the second\nONB formed by the $|a_i\\rangle$. The algorithm will drive $\\hat{U}$ to some\nunitary $\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$, where\n$\\hat{U}_1$ ($\\hat{U}_2^\\prime$) are undetermined unitaries which are diagonal\nin the basis $\\{|s_i\\rangle\\}$ ($\\{|a_i\\rangle\\}$).\n\nIf (iii) the percept states are random, this phase freedom is removed up to a\nglobal phase. In the simplest case, we draw the initial states of the device\nfrom an ``overcomplete'' basis, where the set of all possible states is linearly\ndependent. For a $n$-level system, this can be accomplished by (randomly)\nchoosing $n$ SU($n$)-unitaries. During each state initialisation, we then take\none $\\hat{U}_{\\mathrm{R}}$ from this set, a random $|s_i\\rangle$ from our first\nONB, and then prepare the device in a state $\\hat{U}_{\\mathrm{R}}|s_i\\rangle$.\nConsequently, the measurement is done in a transformed basis formed by the\n$\\hat{U}_{\\mathrm{T}}\\hat{U}_{\\mathrm{R}}\\hat{U}_{\\mathrm{T}}^{-1}|a_i\\rangle$\nrather than the $|a_i\\rangle$ themselves.\n\nIn this sense, the navigation of (i) a given input state, (ii) a given ONB, and\n(iii) random states can be described as a navigation of unitaries $\\hat{U}$ with\na varying amount of freedom.\nWhile formally, all three cases (i)-(iii) can be considered as special cases of\na navigation of $\\hat{U}(\\bm{h})$ to a point (\\ref{task}), where a percept\nstatistics-based fidelity (\\ref{F2}) becomes maximum, practically they can be\naccomplished in RL by means of the mentioned reward signal, independently of the\navailability of analytic solutions. In what follows, we consider $\\hat{U}$ as a\nmemory of an RL-agent, that solves tasks arising from its interaction with an\nenvironment.\n\n\\section{\n\\label{sec2}\nA cybernetic perspective}\n\nThe scheme is depicted in Fig.~\\ref{fig1}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig1.eps}\n\\caption{\\label{fig1}\n\nAgent-environment interaction as a feedback scheme. The $s$ are percepts, which\ninitialise the agent's memory in a quantum state $\\hat{\\varrho}({s})$. Choice of\nan action $a$ is made by a measurement process as described by a given POVM\n$\\hat{\\Pi}$. Depending on the rewards $r$ given by the environment, the memory\nis updated at the end of each cycle $t$. The memory can also be modified by\ninternal loops based on a numerical objective NO (dotted line) or measurements\n(dash-dotted line).\n}\n\\end{figure}\n\nThe agent is equipped with some quantum channel that acts as its memory whose\nproperties can be modified by control parameters denoted by a vector $\\bm{h}$.\nExamples of memory structures are listed in Fig.~\\ref{fig2}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig2.eps}\n\\caption{\\label{fig2}\n\nExamples for the agent's memory as shown in Fig.~\\ref{fig1}. (a) unitary\nevolution of the percept states, (b) open system evolution due to interaction\nwith a bath B, (c) composite memory with coupled subsystems for percept (S) and\naction (A) variables, (d) extends (c) to an open system evolution analogous to\n(b) extending (a). Further ancilla systems may be added (not shown), to account\nfor, e.g., emotion degrees of freedom introduced in \\cite{Bri12}.\n}\n\\end{figure}\n\nIn Fig.~\\ref{fig2} and in what follows, we refer to the memory operation by\nmeans of some unitary $\\hat{U}$ for notational simplicity. Since any quantum\nprocess can be treated as unitary on an enlarged space, this is not a conceptual\nrestriction. The agent interacts with an external environment in discrete\ncycles $t$. At the beginning of a cycle, the agent receives (via sensors) some\npercept $s$, which it encodes as a quantum state $\\hat{\\varrho}({s})$, in which\nits memory is prepared. After transformation of $\\hat{\\varrho}({s})$ by the\nmemory channel, a quantum measurement is performed, where we assume for\nsimplicity that the positive operator valued measure (POVM) $\\{\\hat{\\Pi}\\}$\ndescribing this measurement is fixed. Depending on the outcome of this\nmeasurement, an action $a$ is selected and performed on the environment (via\nactuators), which completes the cycle. The environment reacts with a new percept\nand a reward $r$, which are perceived by the agent during the following cycle.\nDepending on the reward, some adjustments are made on the control parameters,\nwhich modify the properties of the memory channel (i.e., its ``hardware''). This\nfeedback loop is adapted from the classical schemes in \\cite{bookSuttonBarto}\nand \\cite{bookRussellNorvig}, where the percepts $s$  in Fig.~\\ref{fig1}\ncorrespond to the states in \\cite{bookSuttonBarto}. The agent's interaction with\nthe environment is here considered classical in the sense that percepts, actions\nand rewards are classical signals. The environment itself is not specified, it\ncould represent, e.g., an experiment performed on a quantum system. Note that\nthe environment in Fig.~\\ref{fig1} is not to be confused with the bath in\nFig.~\\ref{fig2}, which affects the memory channel but is not considered part of\nthe agents ``habitat''.\n\nIn addition to the external loop, we may also equip the agent with two types of\ninternal feedback loops, which allow the agent to undertake what corresponds to\n``planning steps'' in RL and ``reflection'' in PS. One type is similar to the\nexternal loop in that it involves state initialisations and measurements on the\nmemory channel, but exploits that percepts, actions and rewards can be recorded\nand reproduced as a consequence of their classicality. The second type of\ninternal loop does not involve state evolutions but requires some mathematical\nmodel of the memory channel itself, which is used to directly calculate a\nnumerical objective (NO), whose value is used to alter the control parameters.\nFig.~\\ref{fig1} does not imply that all of these loops need to be\nsimultaneously present, they are rather thought of either subprocesses within an\noverall agent scheme or possible modes of its operation. The numerical examples\nin this work will exclusively apply the external loop.\n\n\nAll three loops involve a parameter update $\\delta\\bm{h}$. In a ``first-order''\nupdate, $\\delta\\bm{h}$ is proportional to some quantity that depends on the\ngradient $\\bm{\\nabla}\\hat{U}$ of $\\hat{U}$ with respect to $\\bm{h}$. This\ngradient can either be computed directly from a memory model $\\hat{U}(\\bm{h})$\n(i.e., from some symbolic expression of $\\bm{\\nabla}\\hat{U}$ if available) or\nestimated from measurements. These ``measurements'' can be physical (POVM in\nFig.~\\ref{fig1}) or numerical (NO in Fig.~\\ref{fig1}). For the estimation, one\nvaries the components of $\\bm{h}$ by a small amount and records the changes in\nthe measured POVM or computed NO. Here are some elementary examples:\n\\emph{(1a)} A simulation of an external loop with a given model-based\n(i.e. analytic) $\\bm{\\nabla}\\hat{U}$ is performed in Sec.~\\ref{sec:ig22}\n(Fig.~\\ref{fig5}) for the case Fig.~\\ref{fig2}(c), in Sec.~\\ref{sec:ig44}\n(Figs.~\\ref{fig7}-\\ref{fig8}) for the case Fig.~\\ref{fig2}(a), and in\nSec.~\\ref{sec:gw} (Figs.~\\ref{fig11} and \\ref{fig12}) for the case\nFig.~\\ref{fig2}(c). \n\\emph{(1b)}  A simulation of an external loop with a POVM measurement-based\n$\\bm{\\nabla}\\hat{U}$ is carried out in \\cite{Cla15} (Fig. 6) for the case\nFig.~\\ref{fig2}(b).\n\\emph{(2)} A NO-based internal loop with a model-based $\\bm{\\nabla}\\hat{U}$ is\nconsidered in \\cite{clausen18} for the case Fig.~\\ref{fig2}(b) and in\n\\cite{Cla15} (Figs.2-4) for the case Fig.~\\ref{fig2}(a).\n\\emph{(3)} The POVM-based internal loop in Fig.~\\ref{fig1} can be used to\nestimate $\\bm{\\nabla}\\hat{U}$ in the absence of a model $\\hat{U}(\\bm{h})$ of the\nagent memory. To this end, one of the agent's possibilities consists in\ninserting a number of internal cycles between each external cycle, where it\nrepeatedly prepares its memory in the latest percept state and observes how a\nvariation $\\delta\\bm{h}$ affects the measurement statistics. A discussion of\nthis will be given in Sec.~\\ref{sec6}. Beyond these examples, all three loops\ncan be interlaced with each other in various ways, analogous to the wealth of\napproaches reviewed in \\cite{bookSuttonBarto}.\n\n\\section{Update rule in parameter space}\n\nFor the cycle-wise update of the control parameters $\\bm{h}$ of the memory\nchannel $\\hat{U}$, we apply a rule\n\n", "index": 3, "text": "\\begin{equation}\n\\label{ur3}\n  \\bm{h}^\\prime\n  =\\bm{h}+\\kappa(\\bm{h}_\\infty-\\bm{h})\n  +{\\alpha}{r}\\sum_{k=0}^{t-1}(1-\\eta)^k\\bm{D}_{t-k},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\bm{h}^{\\prime}=\\bm{h}+\\kappa(\\bm{h}_{\\infty}-\\bm{h})+{\\alpha}{r}\\sum_{k=0}^{t%&#10;-1}(1-\\eta)^{k}\\bm{D}_{t-k},\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udc89</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mi>\ud835\udc89</mi><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc89</mi><mi mathvariant=\"normal\">\u221e</mi></msub><mo>-</mo><mi>\ud835\udc89</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b7</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>k</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc6b</mi><mrow><mi>t</mi><mo>-</mo><mi>k</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\ngenerates in \\cite{Bri12} a quantum walk and is given by a Hamiltonian\n$\\hat{H}$ $\\!=$ $\\!\\sum_{\\{j,k\\}\\in{E}}\\lambda_{jk}(\\hat{c}_{kj}$ $\\!+$\n$\\!\\hat{c}_{kj}^\\dagger)$ $\\!+$ $\\!\\sum_{j\\in{V}}\\epsilon_{j}\\hat{c}_{jj}$\nand a Lindbladian $L\\hat{\\varrho}$ $\\!=$ \n$\\!\\sum_{\\{j,k\\}\\in{E}}\\kappa_{jk}(\\hat{c}_{kj}\\hat{\\varrho}\n\\hat{c}_{kj}^\\dagger$ $\\!-$ $\\!\\frac{1}{2}\n\\{\\hat{c}_{kj}^\\dagger\\hat{c}_{kj},\\hat{\\varrho}\\})$, with\n$\\hat{c}_{kj}$ $\\!=$ $\\!|{c}_{k}\\rangle\\langle{c}_{j}|$ performing transitions\nbetween clip states $|{c}_{l}\\rangle$ $\\!\\in$ $\\!\\mathcal{H}_{\\mathrm{SA}}$\nalong a graph $G$ $\\!=$ $\\!(V,E)$ consisting of a set $V$ of vertices and a set\n$E$ of edges. Since on the one hand, we here do not intend to necessarily\nrepresent $\\hat{U}$ by a clip network, and on the other hand do not want to\nexclude from the outset situations involving time-dependent or non-Markovian\nbath effects \\cite{DSH14}, we use the dilated $\\hat{U}$ for simplicity instead.\nThe set of all probabilities $p({a}|{s})$ in (\\ref{grada}), i.e, the whole\nconditional distribution then defines the agent's policy.\n\nA reward given by the environment at time $t$ raises the\nquestion of the extent to which decisions made by the agent in the past have\ncontributed to this respective reward. A heuristic method is to attribute all\npast decisions, but to a lesser degree the further the decision lies in the\npast. (Considering the agent's life as a trajectory of subsequent percepts and\nactions, we could imagine the latest event trailing a decaying tail behind.)\nA detailed description of this idea is presented in \\cite{bookSuttonBarto} in\nform of the eligibility traces, which can be implemented as accumulating or\nreplacing traces. In the context of PS, a similar idea has been introduced as\nglow mechanism that can be implemented as edge or clip glow \\cite{Mau15,Mel14}.\nIn our context (\\ref{ur3}), we realise it by updating the control vector by a\nbackward-discounted sum of gradients $\\bm{\\nabla}_{t-k}$ referring to percepts\nand actions involved in cycles that happened $k$ steps in the past. A sole\nupdate by the present gradient is included as limit $\\eta$ $\\!=$ $\\!1$, for\nwhich (\\ref{ur3}) reduces to\n$\\bm{h}^\\prime$ $\\!=$ $\\!\\bm{h}+\\alpha{r}\\bm{\\nabla}p({a}|{s})$.\nThis special case is sufficient for the invasion game, which we will consider\nin Sec.~\\ref{sec4}, because at each cycle, the environment provides a feedback\non the correctness of the agent's decision by means of a non-zero reward. After\nthat, we apply the general update (\\ref{ur3}) to a grid world task, where the\nagent's goal cannot be achieved by a single action, and where the long term\nconsequences of its individual decisions cannot be foreseen by the agent. \n\n\\section{Relation to existing methods}\n\nIn this section, we ignore the embodied implementation of our method as a\nquantum agent and briefly summarise and compare the update rules of the methods\nconsidered from a computational point of view. It should be stressed that RL is\nan umbrella term for problems that can be described as agent-environment\ninteractions characterised by percepts/states, actions, and rewards. Hence\n\\emph{all} methods considered here are approaches to RL-problems. For notational\nconvenience however, we here denote the standard value function-based methods\nas ``RL'' in a closer sense, keeping in mind that alternatives such as direct\npolicy search deal with the same type of problem. The standard RL-methods\nsuccessively approximate for each state or state action pair the expected return\n$R_t$ $\\!=$ $\\!\\sum_{k=0}^\\infty\\gamma^kr_{t+k+1}$, i.e., a sum of future\nrewards $r$, forward-discounted by a discount rate $\\gamma\\in[0,1]$, that the\nagent is trying to maximise by policy learning. Corrections to the current\nestimates can be done by shifting them a bit towards actual rewards observed\nduring an arbitrarily given number $n$ of future cycles, giving rise to\n``corrected $n$-step truncated returns''\n$R_t^{(n)}$ $\\!=$ $\\!{r}_{t+1}$ $\\!+$ $\\!\\gamma{r}_{t+2}$ $\\!+$ $\\!\\ldots$ $\\!+$\n$\\!\\gamma^{n-1}{r}_{t+n}$ $\\!+$ $\\!\\gamma^{n}V_t(s_{t+n})$, where $V$ is the\nvalue function of the respective future state $s_{t+n}$ (analogous\nconsiderations hold for state action pairs). A weighted average of these gives\nthe $\\lambda$-return\n$R_t^{\\lambda}$ $\\!=$ $\\!(1-\\lambda)\\sum_{n=1}^\\infty\\lambda^{n-1}R_t^{(n)}$,\nwhere $\\lambda\\in[0,1]$ is a parameter. In an equivalent ``mechanistic''\nbackward view, this gives rise to so-called eligibility traces. Since the glow\nmechanism of PS is closely related to this, we base our comparison on the\n$\\lambda$-extension of one-step RL. $\\lambda$ $\\!=$ $\\!0$ describes the limit of\nshallow sample backups of single-step learning, whereas the other limit\n$\\lambda$ $\\!=$ $\\!1$ refers to the deep backups of Monte Carlo sampling, cf.\nFig. 10.1 in \\cite{bookSuttonBarto}.\n\nIt would be a futile task to try a mapping of the numerous variations,\nextensions, or combinations with other approaches that have been discussed or\nare currently developed for the methods mentioned, such as actor-critic methods\nor planning in RL, or emotion, reflection, composition, generalization, or\nmeta-learning in PS. In particular, our notion of basic PS implies a restriction\nto clips of length $L$ $\\!=$ $\\!1$, which reduces the edge strengths in the ECM\nclip network Fig. 2 in \\cite{Bri12} to values $h(s,a)$ of state-action pairs.\nFurthermore, in this section, we restrict attention to the basic versions that\nare sufficient to treat the numerical example problems discussed in this work.\nWe may think of tasks such as grid world, where actions lead to state\ntransitions, until a terminal state has been reached, which ends the respective\nepisode, cf. Sec.~\\ref{sec4}.\n\n\\subsection{Tabular RL}\n\nIn tabular RL, the updates are performed according to\n\\begin{eqnarray}\n\\label{TRL1}\n  {e}&\\leftarrow&({e}+)1\n  \\quad[\\mathrm{for}\\;s\\;\\mathrm{or}\\;(s,a)\\;\\mathrm{visited}],\n  \\\\\n  U&\\leftarrow&U+\\alpha\\left[r+\\gamma{U}^\\prime-{U}\\right]{e}\n  \\nonumber\\\\\n\\label{TRL2}\n  &&=(1-\\alpha{e})U+\\alpha{e}r+\\alpha\\gamma{e}{U}^\\prime,\n  \\\\\n\\label{TRL3}\n  {e}&\\leftarrow&\\gamma\\lambda{e},\n\\end{eqnarray}\nwhere ${U}$ $\\!=$ $\\!V(s)$ is a state value function in TD($\\lambda$), cf.\nFig. 7.7 in \\cite{bookSuttonBarto}, whereas ${U}$ $\\!=$ $\\!Q(s,a)$ is an action\nvalue function in SARSA($\\lambda$), cf. Fig. 7.11 in \\cite{bookSuttonBarto}. \n${U}^\\prime$ $\\!=$ $\\!V(s^\\prime)$\n[or ${U}^\\prime$ $\\!=$ $\\!Q(s^\\prime,a^\\prime)$] refers to the value of the\nsubsequent state or state action pair.\n${e}$ $\\!=$ $\\!e(s)$ [${e}$ $\\!=$ $\\!e(s,a)$] denote the eligibility trace in\nTD($\\lambda$) [SARSA($\\lambda$)]. They can be updated by accumulating\n(${e}$ $\\!\\leftarrow$ $\\!{e}$ $\\!+$ $\\!1$) or replacing\n(${e}$ $\\!\\leftarrow$ $\\!1$) them in (\\ref{TRL1}) (ignoring other options such\nas clearing traces \\cite{bookSuttonBarto}). $\\alpha$ is a learning rate, $r$ is\nthe reward, and $\\gamma$ the discount rate. Note that there are alternatives to\n(\\ref{TRL1})-(\\ref{TRL3}). One of them is Q-learning, which can be derived from\nSARSA=SARSA($\\lambda=0$) by updating $Q(s,a)$ off-policy, which simplifies a\nmathematical analysis. Since a Q($\\lambda$)-extension of Q-learning is less\nstraightforward, and there are convergence issues with respect to the\ngradient-ascent form discussed below (cf. Sec. 8.5 in \\cite{bookSuttonBarto}),\nwhile the methods discussed here update on-policy, we restrict attention to\n(\\ref{TRL1})-(\\ref{TRL3}).\n\n\\subsection{Gradient-ascent RL}\n\nTabular RL is a special case of gradient-ascent RL, where $U$ is in the latter\ndefined as in (\\ref{TRL1})-(\\ref{TRL3}), except that it is given by a number of\nparameters $\\theta_k$, which are combined to a vector $\\bm{\\theta}$. This\nparametrisation can be done arbitrarily. In the linear case, the parameters\ncould be coefficients of, e.g., some (finite) function expansion, where the\nfunctions represent ``features''. Hence ${U}$ $\\!=$ $\\!U(\\bm{\\theta})$, and the\ncomponents of the gradient $\\bm{\\nabla}{U}$ are\n$\\frac{\\partial{U}}{\\partial{\\theta}_k}$, giving rise to a vector $\\bm{e}$ of\neligibility traces. The updates (\\ref{TRL1})-(\\ref{TRL3}) now generalize to\n\\begin{eqnarray}\n\\label{GRL1}\n  \\bm{e}&\\leftarrow&\\gamma\\lambda\\bm{e}+\\bm{\\nabla}{U},\n  \\\\\n\\label{GRL2}\n  \\bm{\\theta}&\\leftarrow&\\bm{\\theta}\n  +\\alpha\\left[r+\\gamma{U}^\\prime-{U}\\right]\\bm{e},\n\\end{eqnarray}\ncf. Sec. 8 in \\cite{bookSuttonBarto}. While the eligibility traces are\ninitialised with zero, the value functions (by means of their parameters) can be\ninitialised arbitrarily in tabular and gradient-ascent RL.\n\n\\subsection{PS}\n\nClassical PS is a tabular model. By tabular we mean that the percepts and\nactions (and ultimately also clips of length $L$ $\\!>$ $\\!1$ in the ECM) form\ndiscrete (i.e., countable) sets, with the consequence, that the edge strengths\n$h$ can be combined to a table (matrix). Let us hence write the updates as\nsummarised in App.~\\ref{app:A1} in a form allowing comparison with\n(\\ref{TRL1})-(\\ref{TRL3}):\n\\begin{eqnarray}\n\\label{PS1}\n  {g}&\\leftarrow&1\n  \\quad[\\mathrm{for}\\;(s,a)\\;\\mathrm{visited}],\n  \\\\\n  {h}&\\leftarrow&{h}+\\lambda{g}+\\gamma(h^{\\mathrm{eq}}-h)\n  \\nonumber\\\\\n\\label{PS2}\n  &&=(1-\\gamma){h}+\\lambda{g}+\\gamma{h}^{\\mathrm{eq}},\n  \\\\\n\\label{PS3}\n  {g}&\\leftarrow&(1-\\eta){g}.\n\\end{eqnarray}\nIn (\\ref{PS1})-(\\ref{PS3}), we intentionally adopted the notation of PS. The\nglow parameter ${g}$ in (\\ref{PS1})-(\\ref{PS3}) corresponds to a replacing trace\n${e}$ in (\\ref{TRL1})-(\\ref{TRL3}), with $(1$ $\\!-$ $\\!\\eta)$ in (\\ref{PS3})\ncorresponding to $\\gamma\\lambda$ in (\\ref{TRL3}), and $\\lambda$ in (\\ref{PS2})\ncorresponds to the reward $r$ in (\\ref{TRL2}). The discount rate $\\gamma$ in\n(\\ref{TRL1})-(\\ref{TRL3}) must not be confused with the dissipation or damping\nrate $\\gamma\\in[0,1]$ in (\\ref{PS2}). To avoid confusion, we denote the former\nby $\\gamma_{\\mathrm{disc}}$ and the latter by $\\gamma_{\\mathrm{damp}}$ for the\nremainder of this paragraph. If we disregard the absence of a learning rate in\n(\\ref{PS1})-(\\ref{PS3}) [we may set $\\alpha$ $\\!=$ $\\!1$ in\n(\\ref{TRL1})-(\\ref{TRL3})], we can obtain PS from tabular SARSA($\\lambda$) by\nreplacing the action value function $Q(s,a)$ with the connection weight\n$h(s,a)$, and the update of $h$ corresponding to the r.h.s. of (\\ref{TRL2}),\n\n", "itemtype": "equation", "pos": 25941, "prevtext": "\ninspired by the basic model of PS \\cite{Bri12,Mau15}. The number of components\n$h_k$ can range from one ($\\bm{h}$ scalar) to infinity ($\\bm{h}$ may represent a\nfunction or a vector of functions), and the $h_k$ can be assumed to be\nreal-valued without loss of generality. In \\cite{Bri12,Mau15}, the components of\n$\\bm{h}$ are the edge strengths of a directed graph representing a network of\nclips (the graph's vertices). While these clips are considered sequences of\nremembered percepts and actions, the network itself abstracts from the clip's\ninternal contents. Our view of $\\bm{h}$ as a control vector is one further\nsimplification and generalization that may allow for but does not require the\nview of the memory as a network.\n\nIn (\\ref{ur3}), $\\bm{h}$ and $\\bm{h}^\\prime$ are the control vectors before and\nafter the update at cycle $t$, respectively. $\\alpha$ $\\!\\ge$ $\\!0$ is a\n(typically small) learning rate, and $r$ is the reward given at cycle $t$.\n$\\kappa$ $\\!\\in$ $\\![0,1]$ is a relaxation rate towards some equilibrium value\n$\\bm{h}_\\infty$ in the absence of rewards. This allows for what corresponds to\nthe ``forgetting'' process suggested in \\cite{Bri12,Mau15} to account for\ndissipation in an embodied implementation and deal with time-dependent\nenvironments. A natural possibility is to identify the value $\\bm{h}_0$, with\nwhich the memory is initialised before the first cycle, with $\\bm{h}_\\infty$.\nThis could be the zero vector $\\bm{h}_0$ $\\!=$ $\\!\\bm{0}$ yielding, e.g., the\nidentity, $\\hat{U}_0$ $\\!=$ $\\!\\hat{U}(\\bm{h}_0)$ $\\!=$ $\\!\\hat{I}$. The\nlearning process will then be a reward-driven and generally stochastic\nnavigation in parameter space $\\{\\bm{h}\\}$ away from the zero vector $\\bm{0}$.\nLifted to $\\hat{U}(\\bm{h})$, this navigation starts at the identity\n$\\hat{U}_0$ $\\!=$ $\\!\\hat{I}$, that relaxes back to it in the prolonged absence\nof rewards. In this work, we consider static environments as in \\cite{Mel14},\nand hence always set $\\kappa$ $\\!=$ $\\!0$. $\\bm{D}_{t}$ is a difference vector.\nWhile some options for finite difference choices of $\\bm{D}$ are outlined in\nSec.~\\ref{sec6}, in all numerical examples within this work we restrict to the\ncase, where $\\bm{D}_{t}$ $\\!=$ $\\!\\bm{\\nabla}_{t}$ is a short-hand notation for\nthe gradient\n\\begin{eqnarray}\n\\label{grad}\n  \\bm{\\nabla}_{t}&=&\\bm{\\nabla}p({a}|{s})_t=2\\mathrm{Re}\n  \\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\bm{\\nabla}\\hat{U}\\bigr\\rangle_{t},\n\\\\\n\\label{grada}\n  p({a}|{s})&=&\\mathrm{Tr}\\bigl[\\hat{U}\\hat{\\varrho}({s})\\hat{U}^\\dagger\n  \\hat{\\Pi}({a})\\bigr]\n  =\\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\hat{U}\\bigr\\rangle,\n\\end{eqnarray}\nwith components $\\frac{\\partial{p}({a}|{s})}{\\partial{h}_k}$ at cycle $t$.\n$p({a}|{s})$ is the probability of the obtained measurement outcome ${a}$ under\nthe condition of the respective cycle's percept state $\\hat{\\varrho}({s})$,\nwhere\n$\\langle\\cdots\\rangle$ $\\!\\equiv$ $\\!\\mathrm{Tr}[\\hat{\\varrho}({s})\\cdots]$\ndenotes the expectation value with respect to this state, and $\\hat{\\Pi}({a})$\nis the member of the POVM that corresponds to measurement outcome $a$. The\nlatter determines the action performed by the agent, and we use the same symbol\nfor both. $(1$ $\\!-$ $\\!\\eta)$ describes a backward-discount rate, which we have\ndefined via a parameter $\\eta$ $\\!\\in$ $\\![0,1]$ to allow comparison with the\nglow mechanism introduced in \\cite{Mau15}.\nAs mentioned above, the unitary transformation of the respective percept states\n$\\hat{\\varrho}({s}_t)$ by the memory $\\hat{U}$ $\\!=$ $\\!\\hat{U}(\\bm{h}_t)$ at\ncycle $t$ in (\\ref{grada}) refers in general to a larger (dilated) space.\nThe dynamical semigroup of CPT maps proposed in \\cite{Bri12} is included and\ncan be recovered by referring to Fig.~\\ref{fig2}(d) [or alternatively\nFig.~\\ref{fig2}(b)] and the assumption that\n\\begin{eqnarray}\n  \\hat{\\varrho}({s}_t)\\quad(\\equiv\\hat{\\varrho}_{\\mathrm{SAB}})\n  &=&\\hat{\\varrho}_{\\mathrm{SA}}({s}_t)\n  \\otimes\\hat{\\varrho}_{\\mathrm{B}},\n\\\\\n  \\mathrm{Tr}_{\\mathrm{B}}\\left[\\hat{U}\\hat{\\varrho}({s}_t)\n  \\hat{U}^\\dagger\\right]&=&\\mathrm{e}^{\\mathcal{L}_{\\mathrm{SA}}\n  \\Delta{T}^{(\\mathrm{mem})}_t}\\hat{\\varrho}_{\\mathrm{SA}}({s}_t),\n\\end{eqnarray}\nwhere the physical memory evolution time $\\Delta{T}^{(\\mathrm{mem})}_t$ may\ndepend on the cycle $t$ for a chosen parametrisation $\\hat{U}(\\bm{h})$ and must\nbe distinguished from the agent response time that can additionally be affected\nby the potential involvement of internal loops in Fig.~\\ref{fig1}. The\nsuperoperator $\\mathcal{L}$ $\\!=$ $\\!\\mathcal{L}_{\\mathrm{SA}}$, whose effect on\n$\\hat{\\varrho}$ $\\!=$ $\\!\\hat{\\varrho}_{\\mathrm{SA}}$ is defined as a sum\n\n", "index": 5, "text": "\\begin{equation}\n\\label{Lindblad}\n  \\mathcal{L}\\hat{\\varrho}=-\\mathrm{i}[\\hat{H},\\hat{\\varrho}]+L\\hat{\\varrho},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}\\hat{\\varrho}=-\\mathrm{i}[\\hat{H},\\hat{\\varrho}]+L\\hat{\\varrho},\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>L</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwith the update of $h$ given by the r.h.s. of (\\ref{PS2}),\n\n", "itemtype": "equation", "pos": 36366, "prevtext": "\ngenerates in \\cite{Bri12} a quantum walk and is given by a Hamiltonian\n$\\hat{H}$ $\\!=$ $\\!\\sum_{\\{j,k\\}\\in{E}}\\lambda_{jk}(\\hat{c}_{kj}$ $\\!+$\n$\\!\\hat{c}_{kj}^\\dagger)$ $\\!+$ $\\!\\sum_{j\\in{V}}\\epsilon_{j}\\hat{c}_{jj}$\nand a Lindbladian $L\\hat{\\varrho}$ $\\!=$ \n$\\!\\sum_{\\{j,k\\}\\in{E}}\\kappa_{jk}(\\hat{c}_{kj}\\hat{\\varrho}\n\\hat{c}_{kj}^\\dagger$ $\\!-$ $\\!\\frac{1}{2}\n\\{\\hat{c}_{kj}^\\dagger\\hat{c}_{kj},\\hat{\\varrho}\\})$, with\n$\\hat{c}_{kj}$ $\\!=$ $\\!|{c}_{k}\\rangle\\langle{c}_{j}|$ performing transitions\nbetween clip states $|{c}_{l}\\rangle$ $\\!\\in$ $\\!\\mathcal{H}_{\\mathrm{SA}}$\nalong a graph $G$ $\\!=$ $\\!(V,E)$ consisting of a set $V$ of vertices and a set\n$E$ of edges. Since on the one hand, we here do not intend to necessarily\nrepresent $\\hat{U}$ by a clip network, and on the other hand do not want to\nexclude from the outset situations involving time-dependent or non-Markovian\nbath effects \\cite{DSH14}, we use the dilated $\\hat{U}$ for simplicity instead.\nThe set of all probabilities $p({a}|{s})$ in (\\ref{grada}), i.e, the whole\nconditional distribution then defines the agent's policy.\n\nA reward given by the environment at time $t$ raises the\nquestion of the extent to which decisions made by the agent in the past have\ncontributed to this respective reward. A heuristic method is to attribute all\npast decisions, but to a lesser degree the further the decision lies in the\npast. (Considering the agent's life as a trajectory of subsequent percepts and\nactions, we could imagine the latest event trailing a decaying tail behind.)\nA detailed description of this idea is presented in \\cite{bookSuttonBarto} in\nform of the eligibility traces, which can be implemented as accumulating or\nreplacing traces. In the context of PS, a similar idea has been introduced as\nglow mechanism that can be implemented as edge or clip glow \\cite{Mau15,Mel14}.\nIn our context (\\ref{ur3}), we realise it by updating the control vector by a\nbackward-discounted sum of gradients $\\bm{\\nabla}_{t-k}$ referring to percepts\nand actions involved in cycles that happened $k$ steps in the past. A sole\nupdate by the present gradient is included as limit $\\eta$ $\\!=$ $\\!1$, for\nwhich (\\ref{ur3}) reduces to\n$\\bm{h}^\\prime$ $\\!=$ $\\!\\bm{h}+\\alpha{r}\\bm{\\nabla}p({a}|{s})$.\nThis special case is sufficient for the invasion game, which we will consider\nin Sec.~\\ref{sec4}, because at each cycle, the environment provides a feedback\non the correctness of the agent's decision by means of a non-zero reward. After\nthat, we apply the general update (\\ref{ur3}) to a grid world task, where the\nagent's goal cannot be achieved by a single action, and where the long term\nconsequences of its individual decisions cannot be foreseen by the agent. \n\n\\section{Relation to existing methods}\n\nIn this section, we ignore the embodied implementation of our method as a\nquantum agent and briefly summarise and compare the update rules of the methods\nconsidered from a computational point of view. It should be stressed that RL is\nan umbrella term for problems that can be described as agent-environment\ninteractions characterised by percepts/states, actions, and rewards. Hence\n\\emph{all} methods considered here are approaches to RL-problems. For notational\nconvenience however, we here denote the standard value function-based methods\nas ``RL'' in a closer sense, keeping in mind that alternatives such as direct\npolicy search deal with the same type of problem. The standard RL-methods\nsuccessively approximate for each state or state action pair the expected return\n$R_t$ $\\!=$ $\\!\\sum_{k=0}^\\infty\\gamma^kr_{t+k+1}$, i.e., a sum of future\nrewards $r$, forward-discounted by a discount rate $\\gamma\\in[0,1]$, that the\nagent is trying to maximise by policy learning. Corrections to the current\nestimates can be done by shifting them a bit towards actual rewards observed\nduring an arbitrarily given number $n$ of future cycles, giving rise to\n``corrected $n$-step truncated returns''\n$R_t^{(n)}$ $\\!=$ $\\!{r}_{t+1}$ $\\!+$ $\\!\\gamma{r}_{t+2}$ $\\!+$ $\\!\\ldots$ $\\!+$\n$\\!\\gamma^{n-1}{r}_{t+n}$ $\\!+$ $\\!\\gamma^{n}V_t(s_{t+n})$, where $V$ is the\nvalue function of the respective future state $s_{t+n}$ (analogous\nconsiderations hold for state action pairs). A weighted average of these gives\nthe $\\lambda$-return\n$R_t^{\\lambda}$ $\\!=$ $\\!(1-\\lambda)\\sum_{n=1}^\\infty\\lambda^{n-1}R_t^{(n)}$,\nwhere $\\lambda\\in[0,1]$ is a parameter. In an equivalent ``mechanistic''\nbackward view, this gives rise to so-called eligibility traces. Since the glow\nmechanism of PS is closely related to this, we base our comparison on the\n$\\lambda$-extension of one-step RL. $\\lambda$ $\\!=$ $\\!0$ describes the limit of\nshallow sample backups of single-step learning, whereas the other limit\n$\\lambda$ $\\!=$ $\\!1$ refers to the deep backups of Monte Carlo sampling, cf.\nFig. 10.1 in \\cite{bookSuttonBarto}.\n\nIt would be a futile task to try a mapping of the numerous variations,\nextensions, or combinations with other approaches that have been discussed or\nare currently developed for the methods mentioned, such as actor-critic methods\nor planning in RL, or emotion, reflection, composition, generalization, or\nmeta-learning in PS. In particular, our notion of basic PS implies a restriction\nto clips of length $L$ $\\!=$ $\\!1$, which reduces the edge strengths in the ECM\nclip network Fig. 2 in \\cite{Bri12} to values $h(s,a)$ of state-action pairs.\nFurthermore, in this section, we restrict attention to the basic versions that\nare sufficient to treat the numerical example problems discussed in this work.\nWe may think of tasks such as grid world, where actions lead to state\ntransitions, until a terminal state has been reached, which ends the respective\nepisode, cf. Sec.~\\ref{sec4}.\n\n\\subsection{Tabular RL}\n\nIn tabular RL, the updates are performed according to\n\\begin{eqnarray}\n\\label{TRL1}\n  {e}&\\leftarrow&({e}+)1\n  \\quad[\\mathrm{for}\\;s\\;\\mathrm{or}\\;(s,a)\\;\\mathrm{visited}],\n  \\\\\n  U&\\leftarrow&U+\\alpha\\left[r+\\gamma{U}^\\prime-{U}\\right]{e}\n  \\nonumber\\\\\n\\label{TRL2}\n  &&=(1-\\alpha{e})U+\\alpha{e}r+\\alpha\\gamma{e}{U}^\\prime,\n  \\\\\n\\label{TRL3}\n  {e}&\\leftarrow&\\gamma\\lambda{e},\n\\end{eqnarray}\nwhere ${U}$ $\\!=$ $\\!V(s)$ is a state value function in TD($\\lambda$), cf.\nFig. 7.7 in \\cite{bookSuttonBarto}, whereas ${U}$ $\\!=$ $\\!Q(s,a)$ is an action\nvalue function in SARSA($\\lambda$), cf. Fig. 7.11 in \\cite{bookSuttonBarto}. \n${U}^\\prime$ $\\!=$ $\\!V(s^\\prime)$\n[or ${U}^\\prime$ $\\!=$ $\\!Q(s^\\prime,a^\\prime)$] refers to the value of the\nsubsequent state or state action pair.\n${e}$ $\\!=$ $\\!e(s)$ [${e}$ $\\!=$ $\\!e(s,a)$] denote the eligibility trace in\nTD($\\lambda$) [SARSA($\\lambda$)]. They can be updated by accumulating\n(${e}$ $\\!\\leftarrow$ $\\!{e}$ $\\!+$ $\\!1$) or replacing\n(${e}$ $\\!\\leftarrow$ $\\!1$) them in (\\ref{TRL1}) (ignoring other options such\nas clearing traces \\cite{bookSuttonBarto}). $\\alpha$ is a learning rate, $r$ is\nthe reward, and $\\gamma$ the discount rate. Note that there are alternatives to\n(\\ref{TRL1})-(\\ref{TRL3}). One of them is Q-learning, which can be derived from\nSARSA=SARSA($\\lambda=0$) by updating $Q(s,a)$ off-policy, which simplifies a\nmathematical analysis. Since a Q($\\lambda$)-extension of Q-learning is less\nstraightforward, and there are convergence issues with respect to the\ngradient-ascent form discussed below (cf. Sec. 8.5 in \\cite{bookSuttonBarto}),\nwhile the methods discussed here update on-policy, we restrict attention to\n(\\ref{TRL1})-(\\ref{TRL3}).\n\n\\subsection{Gradient-ascent RL}\n\nTabular RL is a special case of gradient-ascent RL, where $U$ is in the latter\ndefined as in (\\ref{TRL1})-(\\ref{TRL3}), except that it is given by a number of\nparameters $\\theta_k$, which are combined to a vector $\\bm{\\theta}$. This\nparametrisation can be done arbitrarily. In the linear case, the parameters\ncould be coefficients of, e.g., some (finite) function expansion, where the\nfunctions represent ``features''. Hence ${U}$ $\\!=$ $\\!U(\\bm{\\theta})$, and the\ncomponents of the gradient $\\bm{\\nabla}{U}$ are\n$\\frac{\\partial{U}}{\\partial{\\theta}_k}$, giving rise to a vector $\\bm{e}$ of\neligibility traces. The updates (\\ref{TRL1})-(\\ref{TRL3}) now generalize to\n\\begin{eqnarray}\n\\label{GRL1}\n  \\bm{e}&\\leftarrow&\\gamma\\lambda\\bm{e}+\\bm{\\nabla}{U},\n  \\\\\n\\label{GRL2}\n  \\bm{\\theta}&\\leftarrow&\\bm{\\theta}\n  +\\alpha\\left[r+\\gamma{U}^\\prime-{U}\\right]\\bm{e},\n\\end{eqnarray}\ncf. Sec. 8 in \\cite{bookSuttonBarto}. While the eligibility traces are\ninitialised with zero, the value functions (by means of their parameters) can be\ninitialised arbitrarily in tabular and gradient-ascent RL.\n\n\\subsection{PS}\n\nClassical PS is a tabular model. By tabular we mean that the percepts and\nactions (and ultimately also clips of length $L$ $\\!>$ $\\!1$ in the ECM) form\ndiscrete (i.e., countable) sets, with the consequence, that the edge strengths\n$h$ can be combined to a table (matrix). Let us hence write the updates as\nsummarised in App.~\\ref{app:A1} in a form allowing comparison with\n(\\ref{TRL1})-(\\ref{TRL3}):\n\\begin{eqnarray}\n\\label{PS1}\n  {g}&\\leftarrow&1\n  \\quad[\\mathrm{for}\\;(s,a)\\;\\mathrm{visited}],\n  \\\\\n  {h}&\\leftarrow&{h}+\\lambda{g}+\\gamma(h^{\\mathrm{eq}}-h)\n  \\nonumber\\\\\n\\label{PS2}\n  &&=(1-\\gamma){h}+\\lambda{g}+\\gamma{h}^{\\mathrm{eq}},\n  \\\\\n\\label{PS3}\n  {g}&\\leftarrow&(1-\\eta){g}.\n\\end{eqnarray}\nIn (\\ref{PS1})-(\\ref{PS3}), we intentionally adopted the notation of PS. The\nglow parameter ${g}$ in (\\ref{PS1})-(\\ref{PS3}) corresponds to a replacing trace\n${e}$ in (\\ref{TRL1})-(\\ref{TRL3}), with $(1$ $\\!-$ $\\!\\eta)$ in (\\ref{PS3})\ncorresponding to $\\gamma\\lambda$ in (\\ref{TRL3}), and $\\lambda$ in (\\ref{PS2})\ncorresponds to the reward $r$ in (\\ref{TRL2}). The discount rate $\\gamma$ in\n(\\ref{TRL1})-(\\ref{TRL3}) must not be confused with the dissipation or damping\nrate $\\gamma\\in[0,1]$ in (\\ref{PS2}). To avoid confusion, we denote the former\nby $\\gamma_{\\mathrm{disc}}$ and the latter by $\\gamma_{\\mathrm{damp}}$ for the\nremainder of this paragraph. If we disregard the absence of a learning rate in\n(\\ref{PS1})-(\\ref{PS3}) [we may set $\\alpha$ $\\!=$ $\\!1$ in\n(\\ref{TRL1})-(\\ref{TRL3})], we can obtain PS from tabular SARSA($\\lambda$) by\nreplacing the action value function $Q(s,a)$ with the connection weight\n$h(s,a)$, and the update of $h$ corresponding to the r.h.s. of (\\ref{TRL2}),\n\n", "index": 7, "text": "\\begin{equation}\n\\label{update1}\n  (1-{g}){h}+\\lambda{g}+\\gamma_{\\mathrm{disc}}{g}{h}^\\prime\n  ={h}+\\lambda{g}+(\\gamma_{\\mathrm{disc}}{h}^\\prime-{h}){g},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"(1-{g}){h}+\\lambda{g}+\\gamma_{\\mathrm{disc}}{g}{h}^{\\prime}={h}+\\lambda{g}+(%&#10;\\gamma_{\\mathrm{disc}}{h}^{\\prime}-{h}){g},\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>g</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>h</mi></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>g</mi></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mi>disc</mi></msub><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><msup><mi>h</mi><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><mi>h</mi><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>g</mi></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>\u03b3</mi><mi>disc</mi></msub><mo>\u2062</mo><msup><mi>h</mi><mo>\u2032</mo></msup></mrow><mo>-</mo><mi>h</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nIn (\\ref{TRL1})-(\\ref{TRL3}), RL is equipped with forward- and\nbackward-discounting mechanisms, as becomes apparent in the product\n$\\gamma_{\\mathrm{disc}}\\lambda$ in (\\ref{TRL3}). Disabling the accumulation of\nforward-discounted future rewards (that give rise in RL to the return mentioned\nabove) by setting $\\gamma_{\\mathrm{disc}}$ $\\!=$ $\\!0$ reduces (\\ref{update1})\nto $(1$ $\\!-$ $\\!g){h}$ $\\!+$ $\\!\\lambda{g}$, while setting\n${h}^{\\mathrm{eq}}$ $\\!=$ $\\!0$ reduces (\\ref{update2}) to\n$(1$ $\\!-$ $\\!\\gamma_{\\mathrm{damp}}){h}$ $\\!+$ $\\!\\lambda{g}$. These\nexpressions are very similar, except that in PS, the constant\n$\\gamma_{\\mathrm{damp}}$ has taken the place of $g$ in RL, so that\n$(1$ $\\!-$ $\\!\\gamma_{\\mathrm{damp}})$ determines the range of\nbackward-discounting in (\\ref{happ}). Since in (\\ref{happ}), it is the\nrespective past excitations (glowing rewards) $r_t$ $\\!=$ $\\!g_t\\lambda_t$,\nrather than the rewards $\\lambda_t$ themselves, which is summed up, damping and\nglow play a similar role. On the other hand, the factor $(1$ $\\!-$ $\\!\\eta)$\ntakes in PS the place of $\\gamma_{\\mathrm{disc}}\\lambda$ in (\\ref{TRL3}), which\nbecomes zero together with $\\gamma_{\\mathrm{disc}}$, as mentioned.\n\n\\subsection{Method presented here}\n\nThe update rule (\\ref{ur3}) is implemented as\n\\begin{eqnarray}\n\\label{GM1}\n  \\bm{e}&\\leftarrow&(1-\\eta)\\bm{e}+\\bm{\\nabla}\\,p(a|s),\n  \\\\\n\\label{GM2}\n  \\bm{h}&\\leftarrow&\n  \\bm{h}+{\\alpha}{r}\\bm{e}+\\kappa(\\bm{h}_\\infty-\\bm{h}),\n\\end{eqnarray}\nwhich can be obtained from gradient-ascent SARSA($\\lambda$) by replacing in\n(\\ref{GRL1})-(\\ref{GRL2}) the action value function $Q(s,a)$ with the\nconditional probability $p(a|s)$, renaming $\\bm{\\theta}$ as $\\bm{h}$, replacing\n$\\gamma\\lambda$ in (\\ref{GRL1}) with $(1$ $\\!-$ $\\!\\eta)$, and replacing in\n(\\ref{GRL2}) the term\n$\\alpha\\bigl[\\gamma{p}(a^\\prime|s^\\prime)$ $\\!-$ $\\!{p}(a|s)\\bigr]\\bm{e}$ with\n$\\kappa(\\bm{h}_\\infty-$ $\\!$ $\\!\\bm{h})$. The latter replacement is similar to\nthe change from (\\ref{update1}) to (\\ref{update2}), where $\\kappa$ in\n(\\ref{GM2}) corresponds to $\\gamma$ in (\\ref{PS2}).\nAnalogous to the comments following (\\ref{update1}) and (\\ref{update2}), in the\ncase $\\gamma$ $\\!=$ $\\!0$, the update corresponding to the r.h.s. of\n(\\ref{GRL2}) becomes\n$(\\bm{h}$ $\\!-$ $\\!\\alpha{p}\\bm{e})$ $\\!+$ $\\!\\alpha{r}\\bm{e}$, whereas for\n$\\bm{h}_\\infty$ $\\!=$ $\\!0$, the r.h.s. of (\\ref{GM2}) reduces to\n$(1$ $\\!-$ $\\!\\kappa)\\bm{h}$ $\\!+$ $\\!\\alpha{r}\\bm{e}$. Similar to the tabular\ncase, the constant damping rate $\\kappa$ has in our method taken the place of\n$\\alpha{p}\\bm{e}$ in gradient-ascent RL.\n\n\\subsection{\n\\label{sec:VE}\nCan PS be recovered from our approach?}\n\nOur method (\\ref{GM1})-(\\ref{GM2}) replaces a value function with a conditional\nprobability (\\ref{grada}), whereas the edge strengths in PS remain value\nfunction-like quantities. While tabular RL can be recovered from gradient-ascent\nRL, one hence cannot expect to recover the basic PS update rule\n(\\ref{PS1})-(\\ref{PS3}) as a special case of our scheme, despite replacements\nanalogous to (\\ref{update1})-(\\ref{update2}). To understand the difference, we\nrestrict attention to an invasion game - like case as shown in\nFig.~\\ref{fig3} as the simplest example, cf. also Sec.~\\ref{sec4} for\ndetails.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig3.eps}\n\\caption{\\label{fig3}\nTransition strengths ${h}_{kl}$ for an invasion game-like task with states\n(which are here synonymous to percept-clips) $s_k$ and action clips $a_l$. The\nstrengthening of a rewarded transition (here ${h}_{22}$) is in an update\n(\\ref{GMcomp}) based on the gradient of $p(a_{j=2}|s_{i=2})$ accompanied by a\nweakening of the respective transitions to the remaining actions\n(here ${h}_{21}$ and ${h}_{23}$), which is absent in PS, cf. (\\ref{PScomp}).\n}\n\\end{figure}\n\nSince here, each episode lasts one cycle, we disable both the eligibility\ntrace/glow mechanism by setting $\\eta$ $\\!=$ $\\!1$ in (\\ref{GM1})-(\\ref{GM2})\nand (\\ref{PS1})-(\\ref{PS3}). As shown in Fig.~\\ref{fig3}, we are given a set\nof states $\\{s_1,s_2,\\ldots,s_{|S|}\\}$, each of which allows one out of a set of\nactions $\\{a_1,a_2,\\ldots,a_{|A|}\\}$. Consider a cycle, where from state $s_i$,\nan action $a_j$ is selected. If the transition probabilities\n\n", "itemtype": "equation", "pos": 36594, "prevtext": "\nwith the update of $h$ given by the r.h.s. of (\\ref{PS2}),\n\n", "index": 9, "text": "\\begin{equation}\n\\label{update2}\n  (1\\!-\\!\\gamma_{\\mathrm{damp}}){h}\\!+\\!\\lambda{g}\\!+\\!\n  \\gamma_{\\mathrm{damp}}{h}^{\\mathrm{eq}}\n  \\!=\\!{h}\\!+\\!\\lambda{g}\\!+\\!\n  \\gamma_{\\mathrm{damp}}({h}^{\\mathrm{eq}}\\!-\\!{h}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"(1\\!-\\!\\gamma_{\\mathrm{damp}}){h}\\!+\\!\\lambda{g}\\!+\\!\\gamma_{\\mathrm{damp}}{h}%&#10;^{\\mathrm{eq}}\\!=\\!{h}\\!+\\!\\lambda{g}\\!+\\!\\gamma_{\\mathrm{damp}}({h}^{\\mathrm{%&#10;eq}}\\!-\\!{h}).\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">-</mo><msub><mi>\u03b3</mi><mi>damp</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mi>h</mi></mpadded></mrow><mo rspace=\"0.8pt\">+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mi>g</mi></mpadded></mrow><mo rspace=\"0.8pt\">+</mo><mrow><msub><mi>\u03b3</mi><mi>damp</mi></msub><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msup><mi>h</mi><mi>eq</mi></msup></mpadded></mrow></mrow><mo rspace=\"0.8pt\">=</mo><mrow><mpadded width=\"-1.7pt\"><mi>h</mi></mpadded><mo rspace=\"0.8pt\">+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mi>g</mi></mpadded></mrow><mo rspace=\"0.8pt\">+</mo><mrow><msub><mi>\u03b3</mi><mi>damp</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><msup><mi>h</mi><mi>eq</mi></msup></mpadded><mo rspace=\"0.8pt\">-</mo><mi>h</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nare given by some policy function $\\Pi$, then the components of the r.h.s. of\n(\\ref{GM1}) read\n\n", "itemtype": "equation", "pos": 41078, "prevtext": "\nIn (\\ref{TRL1})-(\\ref{TRL3}), RL is equipped with forward- and\nbackward-discounting mechanisms, as becomes apparent in the product\n$\\gamma_{\\mathrm{disc}}\\lambda$ in (\\ref{TRL3}). Disabling the accumulation of\nforward-discounted future rewards (that give rise in RL to the return mentioned\nabove) by setting $\\gamma_{\\mathrm{disc}}$ $\\!=$ $\\!0$ reduces (\\ref{update1})\nto $(1$ $\\!-$ $\\!g){h}$ $\\!+$ $\\!\\lambda{g}$, while setting\n${h}^{\\mathrm{eq}}$ $\\!=$ $\\!0$ reduces (\\ref{update2}) to\n$(1$ $\\!-$ $\\!\\gamma_{\\mathrm{damp}}){h}$ $\\!+$ $\\!\\lambda{g}$. These\nexpressions are very similar, except that in PS, the constant\n$\\gamma_{\\mathrm{damp}}$ has taken the place of $g$ in RL, so that\n$(1$ $\\!-$ $\\!\\gamma_{\\mathrm{damp}})$ determines the range of\nbackward-discounting in (\\ref{happ}). Since in (\\ref{happ}), it is the\nrespective past excitations (glowing rewards) $r_t$ $\\!=$ $\\!g_t\\lambda_t$,\nrather than the rewards $\\lambda_t$ themselves, which is summed up, damping and\nglow play a similar role. On the other hand, the factor $(1$ $\\!-$ $\\!\\eta)$\ntakes in PS the place of $\\gamma_{\\mathrm{disc}}\\lambda$ in (\\ref{TRL3}), which\nbecomes zero together with $\\gamma_{\\mathrm{disc}}$, as mentioned.\n\n\\subsection{Method presented here}\n\nThe update rule (\\ref{ur3}) is implemented as\n\\begin{eqnarray}\n\\label{GM1}\n  \\bm{e}&\\leftarrow&(1-\\eta)\\bm{e}+\\bm{\\nabla}\\,p(a|s),\n  \\\\\n\\label{GM2}\n  \\bm{h}&\\leftarrow&\n  \\bm{h}+{\\alpha}{r}\\bm{e}+\\kappa(\\bm{h}_\\infty-\\bm{h}),\n\\end{eqnarray}\nwhich can be obtained from gradient-ascent SARSA($\\lambda$) by replacing in\n(\\ref{GRL1})-(\\ref{GRL2}) the action value function $Q(s,a)$ with the\nconditional probability $p(a|s)$, renaming $\\bm{\\theta}$ as $\\bm{h}$, replacing\n$\\gamma\\lambda$ in (\\ref{GRL1}) with $(1$ $\\!-$ $\\!\\eta)$, and replacing in\n(\\ref{GRL2}) the term\n$\\alpha\\bigl[\\gamma{p}(a^\\prime|s^\\prime)$ $\\!-$ $\\!{p}(a|s)\\bigr]\\bm{e}$ with\n$\\kappa(\\bm{h}_\\infty-$ $\\!$ $\\!\\bm{h})$. The latter replacement is similar to\nthe change from (\\ref{update1}) to (\\ref{update2}), where $\\kappa$ in\n(\\ref{GM2}) corresponds to $\\gamma$ in (\\ref{PS2}).\nAnalogous to the comments following (\\ref{update1}) and (\\ref{update2}), in the\ncase $\\gamma$ $\\!=$ $\\!0$, the update corresponding to the r.h.s. of\n(\\ref{GRL2}) becomes\n$(\\bm{h}$ $\\!-$ $\\!\\alpha{p}\\bm{e})$ $\\!+$ $\\!\\alpha{r}\\bm{e}$, whereas for\n$\\bm{h}_\\infty$ $\\!=$ $\\!0$, the r.h.s. of (\\ref{GM2}) reduces to\n$(1$ $\\!-$ $\\!\\kappa)\\bm{h}$ $\\!+$ $\\!\\alpha{r}\\bm{e}$. Similar to the tabular\ncase, the constant damping rate $\\kappa$ has in our method taken the place of\n$\\alpha{p}\\bm{e}$ in gradient-ascent RL.\n\n\\subsection{\n\\label{sec:VE}\nCan PS be recovered from our approach?}\n\nOur method (\\ref{GM1})-(\\ref{GM2}) replaces a value function with a conditional\nprobability (\\ref{grada}), whereas the edge strengths in PS remain value\nfunction-like quantities. While tabular RL can be recovered from gradient-ascent\nRL, one hence cannot expect to recover the basic PS update rule\n(\\ref{PS1})-(\\ref{PS3}) as a special case of our scheme, despite replacements\nanalogous to (\\ref{update1})-(\\ref{update2}). To understand the difference, we\nrestrict attention to an invasion game - like case as shown in\nFig.~\\ref{fig3} as the simplest example, cf. also Sec.~\\ref{sec4} for\ndetails.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig3.eps}\n\\caption{\\label{fig3}\nTransition strengths ${h}_{kl}$ for an invasion game-like task with states\n(which are here synonymous to percept-clips) $s_k$ and action clips $a_l$. The\nstrengthening of a rewarded transition (here ${h}_{22}$) is in an update\n(\\ref{GMcomp}) based on the gradient of $p(a_{j=2}|s_{i=2})$ accompanied by a\nweakening of the respective transitions to the remaining actions\n(here ${h}_{21}$ and ${h}_{23}$), which is absent in PS, cf. (\\ref{PScomp}).\n}\n\\end{figure}\n\nSince here, each episode lasts one cycle, we disable both the eligibility\ntrace/glow mechanism by setting $\\eta$ $\\!=$ $\\!1$ in (\\ref{GM1})-(\\ref{GM2})\nand (\\ref{PS1})-(\\ref{PS3}). As shown in Fig.~\\ref{fig3}, we are given a set\nof states $\\{s_1,s_2,\\ldots,s_{|S|}\\}$, each of which allows one out of a set of\nactions $\\{a_1,a_2,\\ldots,a_{|A|}\\}$. Consider a cycle, where from state $s_i$,\nan action $a_j$ is selected. If the transition probabilities\n\n", "index": 11, "text": "\\begin{equation}\n  p_{ij}=p(a_j|s_i)=\\frac{\\Pi({h}_{ij})}{c_i},\\quad{c}_i=\\sum_j\\Pi({h}_{ij}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"p_{ij}=p(a_{j}|s_{i})=\\frac{\\Pi({h}_{ij})}{c_{i}},\\quad{c}_{i}=\\sum_{j}\\Pi({h}%&#10;_{ij}),\" display=\"block\"><mrow><msub><mi>p</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mi>c</mi><mi>i</mi></msub></mfrac><mo rspace=\"12.5pt\">,</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><mi mathvariant=\"normal\">\u03a0</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwith which the components of the update (\\ref{GM2}) become\n\n", "itemtype": "equation", "pos": 41283, "prevtext": "\nare given by some policy function $\\Pi$, then the components of the r.h.s. of\n(\\ref{GM1}) read\n\n", "index": 13, "text": "\\begin{equation}\n\\label{eq23}\n  e_{kl|ij}=\\frac{\\partial{p}_{ij}}{\\partial{h}_{kl}}\n  =\\frac{\\delta_{ik}\\Pi^\\prime({h}_{kl})}{c_i^2}\n  \\bigl[\\delta_{jl}c_i-\\Pi({h}_{ij})\\bigr],\n\n\n\n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"e_{kl|ij}=\\frac{\\partial{p}_{ij}}{\\partial{h}_{kl}}=\\frac{\\delta_{ik}\\Pi^{%&#10;\\prime}({h}_{kl})}{c_{i}^{2}}\\bigl{[}\\delta_{jl}c_{i}-\\Pi({h}_{ij})\\bigr{]},%&#10;\\par&#10;\\par&#10;\\par&#10;\\par&#10;\" display=\"block\"><mrow><mrow><msub><mi>e</mi><mrow><mi>k</mi><mi>l</mi><mo stretchy=\"false\">|</mo><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>p</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mfrac><mo>=</mo><mrow><mfrac><mrow><msub><mi>\u03b4</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mrow><msub><mi>\u03b4</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><msub><mi>c</mi><mi>i</mi></msub></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere we have renamed $\\bm{h}_\\infty$ as $\\bm{h}^{\\mathrm{eq}}$. An observer\nignorant of the transitions $i$ $\\!\\to$ $\\!j$ and the corresponding\nprobabilities $p_{ij}$ notices no change,\n\n", "itemtype": "equation", "pos": 41538, "prevtext": "\nwith which the components of the update (\\ref{GM2}) become\n\n", "index": 15, "text": "\\begin{equation}\n  {h}_{kl}\\leftarrow\n  (1-\\kappa){h}_{kl}+\\alpha{r}e_{kl|ij}+\\kappa{h}_{kl}^{\\mathrm{eq}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"{h}_{kl}\\leftarrow(1-\\kappa){h}_{kl}+\\alpha{r}e_{kl|ij}+\\kappa{h}_{kl}^{%&#10;\\mathrm{eq}},\" display=\"block\"><mrow><mrow><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2190</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03ba</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msub><mi>e</mi><mrow><mi>k</mi><mi>l</mi><mo stretchy=\"false\">|</mo><mi>i</mi><mi>j</mi></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><msubsup><mi>h</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow><mi>eq</mi></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nIn the special case $\\Pi({h}_{kl})$ $\\!=$ $\\!{h}_{kl}$, we can simplify\n(\\ref{eq23}) to\n\n", "itemtype": "equation", "pos": 41848, "prevtext": "\nwhere we have renamed $\\bm{h}_\\infty$ as $\\bm{h}^{\\mathrm{eq}}$. An observer\nignorant of the transitions $i$ $\\!\\to$ $\\!j$ and the corresponding\nprobabilities $p_{ij}$ notices no change,\n\n", "index": 17, "text": "\\begin{equation}\n  \\sum_{ij}e_{kl|ij}=0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\sum_{ij}e_{kl|ij}=0.\" display=\"block\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></munder><msub><mi>e</mi><mrow><mi>k</mi><mi>l</mi><mo stretchy=\"false\">|</mo><mi>i</mi><mi>j</mi></mrow></msub></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nFrom (\\ref{GMcomp}) we see that in the gradient method, the strengthening of the\n${h}_{ij}$-edge is accompanied with a weakening of those edges ${h}_{kl}$\nconnecting the respective state $k$ $\\!=$ $\\!i$ with different actions \n$l$ $\\!\\neq$ $\\!j$. As a consequence, the ${h}_{kl}$ may become negative, even\nif $\\Pi({h}_{kl})$ $\\!=$ $\\!{h}_{kl}$ and the rewards are non-negative. This\nweakening is absent in basic PS (\\ref{PS2}), where the corresponding update is\nindependent of the policy function $\\Pi$ and given by\n\\begin{eqnarray}\n  {h}_{kl}&\\leftarrow&\n  {h}_{kl}-\\gamma({h}_{kl}-{h}_{kl}^{\\mathrm{eq}})\n  +\\lambda\\delta_{ik}\\delta_{jl}\n  \\nonumber\\\\\n  &=&(1-\\gamma){h}_{kl}+\\lambda\\delta_{ik}\\delta_{jl}\n  +\\gamma{h}_{kl}^{\\mathrm{eq}}.\n\\label{PScomp}\n\\end{eqnarray}\nHence, ${h}_{kl}$ $\\!\\ge$ $\\!0$ as long as the rewards are non-negative. In any\ncase, choice of a non-negative policy function $\\Pi({h}_{kl})$ renders the\nmethods independent of a need of positive edge strengths. [Note that a similar\nproblem occurs if the parameters in a memory consisting of alternating layers\nsuch as $\\hat{U}$ $\\!=$ $\\!\\cdots\\mathrm{e}^{-\\mathrm{i}{t}_3\\hat{H}^{(1)}}\n\\mathrm{e}^{-\\mathrm{i}{t}_2\\hat{H}^{(2)}}\\mathrm{e}^{-\\mathrm{i}{t}_1\n\\hat{H}^{(1)}}$, cf. App.~\\ref{app:fl}, refer to non-negative physical\nquantities $t_k$. In \\cite{Cla15}, this has been solved by using an exponential\nfunction such as ${t}_k$ $\\!=$ $\\!\\mathrm{e}^{{h}_k}$ for parametrisation in\nterms of the controls $h_k$. In this work, we identify the $h_k$ directly with\nthe $t_k$ for simplicity, which, if the $h_k$ are to be interpreted as\nnon-negative quantities, doubles the set of physically applied Hamiltonians from\n$\\{\\hat{H}^{(1,2)}\\}$ to $\\{\\pm\\hat{H}^{(1,2)}\\}$.]\n\n\\subsection{Discussion}\n\nThe relations between the different methods are summarised in\nFig.~\\ref{fig4}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig4.eps}\n\\caption{\\label{fig4}\nTabular RL (\\ref{TRL1})-(\\ref{TRL3}) is a special case of gradient-ascent RL\n(\\ref{GRL1})-(\\ref{GRL2}). Replacing updates based on values of subsequent\nstates with updates based on a physical damping term yields basic\nPS (\\ref{PS1})-(\\ref{PS3}) and the method presented here\n(\\ref{GM1})-(\\ref{GM2}), which however uses a conditional probability\n(\\ref{grada}) instead of a value function, hence the basic PS update rule cannot\nbe recovered from our approach, as explained in Sec.~\\ref{sec:VE}.\n}\n\\end{figure}\n\nIf one considers the ECM as the core element of PS rather than a specific update\nrule, one could alternatively adopt, e.g., the tabular SARSA($\\lambda$)-update\nrule. The picture of a random walk in clip space does not contradict the general\nframework of RL-problems. One may understand the clips as the agent's states\n(which must be distinguished from the percepts). The same holds for the\ngradient-ascent generalization, which, in physical terms, could be considered as\n``continuous variable RL''. On the one hand, we could equally well apply, e.g.,\nthe gradient-ascent SARSA($\\lambda$)-update instead of our rule. On the other\nhand, before trying to create algorithmic extensions such as those mentioned at\nthe beginning of this section for tabular RL and PS, one should first\ninvestigate whether and how such extensions are accomplished in any existing\ngradient-ascent RL variants.\n\n\\section{\n\\label{sec4}\nExamples\n}\n\n\n\\subsection{\n\\label{sec:ig}\nInvasion game}\n\nIn what follows, we consider a simple invasion game as treated in \\cite{Bri12}.\nAn attacker randomly chooses one out of two possible symbols\n$\\{\\Leftarrow,\\Rightarrow\\}$ which signals the direction in which it intends to\nmove. The chosen symbol may represent, e.g., a head turn and is visible to the\ndefender, whose task is to learn to move in the same direction, which is\nrequired to block the attacker. We approach this learning task as an external\nloop in Fig.~\\ref{fig1} with a closed system (i.e., bath-less) memory [cases\n(a) and (c) in Fig.~\\ref{fig2}], described within a 4-dimensional Hilbert\nspace. The control parameters are updated according to (\\ref{ur3}) in the\nabsence of relaxation ($\\kappa$ $\\!=$ $\\!0$) and gradient glow\n($\\eta$ $\\!=$ $\\!1$). The update is done with an analytic $\\bm{\\nabla}\\hat{U}$\nas described in App.~\\ref{app:fl}, where the memory consists of alternating\nlayers, $\\hat{U}$ $\\!=$ $\\!\\cdots\\mathrm{e}^{-\\mathrm{i}{h}_3\\hat{H}^{(1)}}\n\\mathrm{e}^{-\\mathrm{i}{h}_2\\hat{H}^{(2)}}\\mathrm{e}^{-\\mathrm{i}{h}_1\n\\hat{H}^{(1)}}$, with a given number of controls $h_1,\\ldots,h_n$. At the\nbeginning of the first cycle, the memory is initialised as identity. For the two\nHamiltonians $\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$, we distinguish\n(I) a general case, where $\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$ are two given\n(randomly generated) 4-rowed Hamiltonians acting on the total Hilbert space and\n(II) a more specialised case, in which they have the form\n\\begin{eqnarray}\n\\label{HA}\n  \\hat{H}^{(1)}&=&\\hat{H}^{(1)}_{\\mathrm{S}}\\otimes\\hat{I}_{\\mathrm{A}}\n  +\\hat{I}_{\\mathrm{S}}\\otimes\\hat{H}^{(1)}_{\\mathrm{A}},\n  \\\\\n\\label{HB}\n  \\hat{H}^{(2)}&=&\\hat{H}^{(2)}_{\\mathrm{S}}\\otimes\\hat{H}^{(2)}_{\\mathrm{A}},\n\\end{eqnarray}\nwhere $\\hat{H}^{(1)}_{\\mathrm{S}}$, $\\hat{H}^{(1)}_{\\mathrm{A}}$,\n$\\hat{H}^{(2)}_{\\mathrm{S}}$, $\\hat{H}^{(2)}_{\\mathrm{A}}$ are four given\n(randomly generated) 2-rowed Hamiltonians acting on the percept (S) and action\n(A) subsystems, respectively, with $\\hat{I}$ denoting the identity. The latter\ncase (II) refers to a physical implementation of Fig.~\\ref{fig2}(c) as a\nbath-mediated interaction of the S and A subsystems that is obtained from the\nsetup Fig.~\\ref{fig2}(d) by eliminating the bath \\cite{Cla15}. It has been\nincluded here to demonstrate that this special structure as considered in\n\\cite{Cla15} may be applied in the present context, but this is not mandatory.\nWhile the Hamiltonians have been chosen in both cases (I) and (II) at random to\navoid shifting focus towards a specific physical realization, in an experimental\nsetup, the respective laboratory Hamiltonians will take their place (assuming\nthat they generate universal gates in the sense of \\cite{lloyd2}, which is\nalmost surely the case for a random choice).\n\n\\subsubsection{\n\\label{sec:ig22}\n2 percepts $\\to$ 2 actions}\n\nWe start with a basic version of the game with 2 possible percepts (the two\nsymbols shown by the attacker) and 2 possible actions (the two moves of the\ndefender). For each percept, there is hence exactly one correct action, which is\nto be identified. The memory applied is shown in Fig.~\\ref{fig2}(c), and the\ndifferent input states are\n\\begin{eqnarray}\n\\label{is}\n  \\hat{\\varrho}&=&\\hat{\\varrho}_{\\mathrm{S}}\\otimes\\hat{\\varrho}_{\\mathrm{A}},\n  \\quad\n  \\hat{\\varrho}_{\\mathrm{S}}=|{s}\\rangle\\langle{s}|,\n  \\\\\n\\label{pcoh}\n  \\hat{\\varrho}_{\\mathrm{A}}&=&p_{\\mathrm{coh}}|\\varphi\\rangle\\langle\\varphi|\n  +(1-p_{\\mathrm{coh}})\\frac{1}{d_{\\mathrm{A}}}\\hat{I}_{\\mathrm{A}},\\quad\n  \\\\\n\\label{ia}\n  |\\varphi\\rangle&=&\\frac{1}{\\sqrt{d_{\\mathrm{A}}}}\\sum_a|{a}\\rangle,\n\\end{eqnarray}\nwhere\n$d_{\\mathrm{A}}$ $\\!=$ $\\!\\mathrm{dim}\\mathcal{H}_{\\mathrm{A}}$ $\\!=$ $\\!2$ is\ngiven by the number of actions. $|s\\rangle$ and $|a\\rangle$ can both be one of\nthe two orthonormal states $|0\\rangle$ or $|1\\rangle$ of the S and A subsystem,\nrespectively. The POVM consists of the elements\n\n", "itemtype": "equation", "pos": 41992, "prevtext": "\nIn the special case $\\Pi({h}_{kl})$ $\\!=$ $\\!{h}_{kl}$, we can simplify\n(\\ref{eq23}) to\n\n", "index": 19, "text": "\\begin{equation}\n\\label{GMcomp}\n  e_{kl|ij}=\\frac{1}{c_i}\\left(\\delta_{ik}\\delta_{jl}-\n  \\frac{{h}_{ij}}{c_i}\\delta_{ik}\\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"e_{kl|ij}=\\frac{1}{c_{i}}\\left(\\delta_{ik}\\delta_{jl}-\\frac{{h}_{ij}}{c_{i}}%&#10;\\delta_{ik}\\right).\" display=\"block\"><mrow><mrow><msub><mi>e</mi><mrow><mi>k</mi><mi>l</mi><mo stretchy=\"false\">|</mo><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>c</mi><mi>i</mi></msub></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>\u03b4</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>\u03b4</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow><mo>-</mo><mrow><mfrac><msub><mi>h</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><msub><mi>c</mi><mi>i</mi></msub></mfrac><mo>\u2062</mo><msub><mi>\u03b4</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nChoosing the correct (wrong) action [i.e. $a$ $\\!=$ $\\!s$ ($a$ $\\!\\neq$ $\\!s$)\nin (\\ref{pom}) and (\\ref{is})] returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-1$).\n\nFig.~\\ref{fig5} shows the average reward $\\overline{r}$ received at each\ncycle, where the averaging is performed over an ensemble of $10^3$ independent\nagents.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig5a.eps}\n\\includegraphics[width=4.2cm]{fig5b.eps}\n\\includegraphics[width=4.2cm]{fig5c.eps}\n\\caption{\\label{fig5}\n\nAverage reward $\\overline{r}$ as a function of the number of cycles for an\ninvasion game (2 symbols, 2 moves), learning rate $\\alpha$ $\\!=$ $\\!10^{-3}$,\nwith a reward of +1 (-1) for a correct (false) move, averaged over\n$10^3$ agents. The input states and POVM are given by Eqs.~(\\ref{is}) and\n(\\ref{pom}), respectively.\n(a) The 6 graphs from bottom to top correspond to 1, 2, 3, 4, 8, and 16\ncontrols, respectively, with $p_{\\mathrm{coh}}$ $\\!=$ $\\!1$ in (\\ref{pcoh}).\n(b) 16 controls, where after $4\\cdot10^3$ cycles the meaning\nof the symbols is reversed. The 5 graphs from bottom to top correspond in\nEq.~(\\ref{pcoh}) to $p_{\\mathrm{coh}}$= 0, 0.25, 0.5, 0.75, and 1, respectively.\n(c) applies 32 controls and $p_{\\mathrm{coh}}$ $\\!=$ $\\!1$, but refers to case\n(II) described by (\\ref{HA}) and (\\ref{HB}), whereas Figs.~\\ref{fig5}(a,b)\nrefer to case (I).\n}\n\\end{figure}\n\n$(\\overline{r}+1)/2$ is hence an estimate of the defender's probability to\nblock an attack. Referring to pure states in (\\ref{pcoh}), Fig.~\\ref{fig5}(a)\nshows the increase of learning speed with the number of controls. Significant\nlearning progress begins only after some initial period of stagnation. From the\nviewpoint of control theory, the identity, in which the memory is initialised,\nmay lie on a ``near-flat ground'' (valley), which must first be left before\nprogress can be made \\cite{Goe15}. Asymptotically, perfect blocking can be\nachieved once the memory becomes controllable, i.e., if the number of controls\nequals (or exceeds) the number of group generators.\nFig.~\\ref{fig5}(b) demonstrates the need of a pure input state\n$\\hat{\\varrho}_{\\mathrm{A}}$ in (\\ref{pcoh}) of the action subsystem A rather\nthan an incoherent mixture. After the agent in Fig.~\\ref{fig5}(b) had some time\nto adapt to the attacker, the meaning of the symbols is suddenly interchanged,\nand the agent must now learn to move in the opposite direction. This relearning\ndiffers from the preceding learning period in the absence of the mentioned\ninitial stagnation phase, which supports the above hypothesis of the proposed\nvalley, the agent has left during the initial learning. This plot is motivated\nby Fig. 5 in \\cite{Bri12} describing classical PS. Although the different\nbehaviour in the classical case suggests that this is an effect specific to\nquantum control, the phenomenon, that a dynamically changing environment can\nfacilitate learning in later stages appears to be more general \\cite{z3}. While\nFigs.~\\ref{fig5}(a,b) refer to case (I) described before (\\ref{HA}),\nFig.~\\ref{fig5}(c) refers to the restricted case (II), which appears to impede\nlearning. In the simulations of the following Sec.~\\ref{sec:ig44}, which all\nrefer to case (II), this is resolved by applying a 10 times larger negative\nreward for each wrong action. This demonstrates the flexibility in approaching\nRL problems offered by the freedom to allocate rewards in a suitable way.\n\n\\subsubsection{\n\\label{sec:ig44}\n4 percepts $\\to$  4 or 2 actions}\n\nWe now consider a version with 4 percepts, referring to an attacker presenting\neach of its two symbols in two colors at random. Since we want to keep the\nHilbert space dimension unchanged (rather than doubling it by adding the color\ncategory) for better comparison of the effect of the number of controls on the\nlearning curve, we must apply a memory as shown in Fig.~\\ref{fig2}(a).\nThe 4 percepts are encoded as tensor products of orthonormal projectors\n\n", "itemtype": "equation", "pos": 49447, "prevtext": "\nFrom (\\ref{GMcomp}) we see that in the gradient method, the strengthening of the\n${h}_{ij}$-edge is accompanied with a weakening of those edges ${h}_{kl}$\nconnecting the respective state $k$ $\\!=$ $\\!i$ with different actions \n$l$ $\\!\\neq$ $\\!j$. As a consequence, the ${h}_{kl}$ may become negative, even\nif $\\Pi({h}_{kl})$ $\\!=$ $\\!{h}_{kl}$ and the rewards are non-negative. This\nweakening is absent in basic PS (\\ref{PS2}), where the corresponding update is\nindependent of the policy function $\\Pi$ and given by\n\\begin{eqnarray}\n  {h}_{kl}&\\leftarrow&\n  {h}_{kl}-\\gamma({h}_{kl}-{h}_{kl}^{\\mathrm{eq}})\n  +\\lambda\\delta_{ik}\\delta_{jl}\n  \\nonumber\\\\\n  &=&(1-\\gamma){h}_{kl}+\\lambda\\delta_{ik}\\delta_{jl}\n  +\\gamma{h}_{kl}^{\\mathrm{eq}}.\n\\label{PScomp}\n\\end{eqnarray}\nHence, ${h}_{kl}$ $\\!\\ge$ $\\!0$ as long as the rewards are non-negative. In any\ncase, choice of a non-negative policy function $\\Pi({h}_{kl})$ renders the\nmethods independent of a need of positive edge strengths. [Note that a similar\nproblem occurs if the parameters in a memory consisting of alternating layers\nsuch as $\\hat{U}$ $\\!=$ $\\!\\cdots\\mathrm{e}^{-\\mathrm{i}{t}_3\\hat{H}^{(1)}}\n\\mathrm{e}^{-\\mathrm{i}{t}_2\\hat{H}^{(2)}}\\mathrm{e}^{-\\mathrm{i}{t}_1\n\\hat{H}^{(1)}}$, cf. App.~\\ref{app:fl}, refer to non-negative physical\nquantities $t_k$. In \\cite{Cla15}, this has been solved by using an exponential\nfunction such as ${t}_k$ $\\!=$ $\\!\\mathrm{e}^{{h}_k}$ for parametrisation in\nterms of the controls $h_k$. In this work, we identify the $h_k$ directly with\nthe $t_k$ for simplicity, which, if the $h_k$ are to be interpreted as\nnon-negative quantities, doubles the set of physically applied Hamiltonians from\n$\\{\\hat{H}^{(1,2)}\\}$ to $\\{\\pm\\hat{H}^{(1,2)}\\}$.]\n\n\\subsection{Discussion}\n\nThe relations between the different methods are summarised in\nFig.~\\ref{fig4}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig4.eps}\n\\caption{\\label{fig4}\nTabular RL (\\ref{TRL1})-(\\ref{TRL3}) is a special case of gradient-ascent RL\n(\\ref{GRL1})-(\\ref{GRL2}). Replacing updates based on values of subsequent\nstates with updates based on a physical damping term yields basic\nPS (\\ref{PS1})-(\\ref{PS3}) and the method presented here\n(\\ref{GM1})-(\\ref{GM2}), which however uses a conditional probability\n(\\ref{grada}) instead of a value function, hence the basic PS update rule cannot\nbe recovered from our approach, as explained in Sec.~\\ref{sec:VE}.\n}\n\\end{figure}\n\nIf one considers the ECM as the core element of PS rather than a specific update\nrule, one could alternatively adopt, e.g., the tabular SARSA($\\lambda$)-update\nrule. The picture of a random walk in clip space does not contradict the general\nframework of RL-problems. One may understand the clips as the agent's states\n(which must be distinguished from the percepts). The same holds for the\ngradient-ascent generalization, which, in physical terms, could be considered as\n``continuous variable RL''. On the one hand, we could equally well apply, e.g.,\nthe gradient-ascent SARSA($\\lambda$)-update instead of our rule. On the other\nhand, before trying to create algorithmic extensions such as those mentioned at\nthe beginning of this section for tabular RL and PS, one should first\ninvestigate whether and how such extensions are accomplished in any existing\ngradient-ascent RL variants.\n\n\\section{\n\\label{sec4}\nExamples\n}\n\n\n\\subsection{\n\\label{sec:ig}\nInvasion game}\n\nIn what follows, we consider a simple invasion game as treated in \\cite{Bri12}.\nAn attacker randomly chooses one out of two possible symbols\n$\\{\\Leftarrow,\\Rightarrow\\}$ which signals the direction in which it intends to\nmove. The chosen symbol may represent, e.g., a head turn and is visible to the\ndefender, whose task is to learn to move in the same direction, which is\nrequired to block the attacker. We approach this learning task as an external\nloop in Fig.~\\ref{fig1} with a closed system (i.e., bath-less) memory [cases\n(a) and (c) in Fig.~\\ref{fig2}], described within a 4-dimensional Hilbert\nspace. The control parameters are updated according to (\\ref{ur3}) in the\nabsence of relaxation ($\\kappa$ $\\!=$ $\\!0$) and gradient glow\n($\\eta$ $\\!=$ $\\!1$). The update is done with an analytic $\\bm{\\nabla}\\hat{U}$\nas described in App.~\\ref{app:fl}, where the memory consists of alternating\nlayers, $\\hat{U}$ $\\!=$ $\\!\\cdots\\mathrm{e}^{-\\mathrm{i}{h}_3\\hat{H}^{(1)}}\n\\mathrm{e}^{-\\mathrm{i}{h}_2\\hat{H}^{(2)}}\\mathrm{e}^{-\\mathrm{i}{h}_1\n\\hat{H}^{(1)}}$, with a given number of controls $h_1,\\ldots,h_n$. At the\nbeginning of the first cycle, the memory is initialised as identity. For the two\nHamiltonians $\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$, we distinguish\n(I) a general case, where $\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$ are two given\n(randomly generated) 4-rowed Hamiltonians acting on the total Hilbert space and\n(II) a more specialised case, in which they have the form\n\\begin{eqnarray}\n\\label{HA}\n  \\hat{H}^{(1)}&=&\\hat{H}^{(1)}_{\\mathrm{S}}\\otimes\\hat{I}_{\\mathrm{A}}\n  +\\hat{I}_{\\mathrm{S}}\\otimes\\hat{H}^{(1)}_{\\mathrm{A}},\n  \\\\\n\\label{HB}\n  \\hat{H}^{(2)}&=&\\hat{H}^{(2)}_{\\mathrm{S}}\\otimes\\hat{H}^{(2)}_{\\mathrm{A}},\n\\end{eqnarray}\nwhere $\\hat{H}^{(1)}_{\\mathrm{S}}$, $\\hat{H}^{(1)}_{\\mathrm{A}}$,\n$\\hat{H}^{(2)}_{\\mathrm{S}}$, $\\hat{H}^{(2)}_{\\mathrm{A}}$ are four given\n(randomly generated) 2-rowed Hamiltonians acting on the percept (S) and action\n(A) subsystems, respectively, with $\\hat{I}$ denoting the identity. The latter\ncase (II) refers to a physical implementation of Fig.~\\ref{fig2}(c) as a\nbath-mediated interaction of the S and A subsystems that is obtained from the\nsetup Fig.~\\ref{fig2}(d) by eliminating the bath \\cite{Cla15}. It has been\nincluded here to demonstrate that this special structure as considered in\n\\cite{Cla15} may be applied in the present context, but this is not mandatory.\nWhile the Hamiltonians have been chosen in both cases (I) and (II) at random to\navoid shifting focus towards a specific physical realization, in an experimental\nsetup, the respective laboratory Hamiltonians will take their place (assuming\nthat they generate universal gates in the sense of \\cite{lloyd2}, which is\nalmost surely the case for a random choice).\n\n\\subsubsection{\n\\label{sec:ig22}\n2 percepts $\\to$ 2 actions}\n\nWe start with a basic version of the game with 2 possible percepts (the two\nsymbols shown by the attacker) and 2 possible actions (the two moves of the\ndefender). For each percept, there is hence exactly one correct action, which is\nto be identified. The memory applied is shown in Fig.~\\ref{fig2}(c), and the\ndifferent input states are\n\\begin{eqnarray}\n\\label{is}\n  \\hat{\\varrho}&=&\\hat{\\varrho}_{\\mathrm{S}}\\otimes\\hat{\\varrho}_{\\mathrm{A}},\n  \\quad\n  \\hat{\\varrho}_{\\mathrm{S}}=|{s}\\rangle\\langle{s}|,\n  \\\\\n\\label{pcoh}\n  \\hat{\\varrho}_{\\mathrm{A}}&=&p_{\\mathrm{coh}}|\\varphi\\rangle\\langle\\varphi|\n  +(1-p_{\\mathrm{coh}})\\frac{1}{d_{\\mathrm{A}}}\\hat{I}_{\\mathrm{A}},\\quad\n  \\\\\n\\label{ia}\n  |\\varphi\\rangle&=&\\frac{1}{\\sqrt{d_{\\mathrm{A}}}}\\sum_a|{a}\\rangle,\n\\end{eqnarray}\nwhere\n$d_{\\mathrm{A}}$ $\\!=$ $\\!\\mathrm{dim}\\mathcal{H}_{\\mathrm{A}}$ $\\!=$ $\\!2$ is\ngiven by the number of actions. $|s\\rangle$ and $|a\\rangle$ can both be one of\nthe two orthonormal states $|0\\rangle$ or $|1\\rangle$ of the S and A subsystem,\nrespectively. The POVM consists of the elements\n\n", "index": 21, "text": "\\begin{equation}\n\\label{pom}\n  \\hat{\\Pi}(a)=\\hat{I}_{\\mathrm{S}}\\otimes|{a}\\rangle_{\\mathrm{A}}\\langle{a}|.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\Pi}(a)=\\hat{I}_{\\mathrm{S}}\\otimes|{a}\\rangle_{\\mathrm{A}}\\langle{a}|.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a0</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mover accent=\"true\"><mi>I</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">S</mi></msub><mo>\u2297</mo><msub><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mi mathvariant=\"normal\">A</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>a</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere $j$ $\\!=$ $\\!0,1$ ($k$ $\\!=$ $\\!0,1$) refers to the symbol (color).\nThe POVM operators are the 4 projectors\n\n", "itemtype": "equation", "pos": 53508, "prevtext": "\nChoosing the correct (wrong) action [i.e. $a$ $\\!=$ $\\!s$ ($a$ $\\!\\neq$ $\\!s$)\nin (\\ref{pom}) and (\\ref{is})] returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-1$).\n\nFig.~\\ref{fig5} shows the average reward $\\overline{r}$ received at each\ncycle, where the averaging is performed over an ensemble of $10^3$ independent\nagents.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig5a.eps}\n\\includegraphics[width=4.2cm]{fig5b.eps}\n\\includegraphics[width=4.2cm]{fig5c.eps}\n\\caption{\\label{fig5}\n\nAverage reward $\\overline{r}$ as a function of the number of cycles for an\ninvasion game (2 symbols, 2 moves), learning rate $\\alpha$ $\\!=$ $\\!10^{-3}$,\nwith a reward of +1 (-1) for a correct (false) move, averaged over\n$10^3$ agents. The input states and POVM are given by Eqs.~(\\ref{is}) and\n(\\ref{pom}), respectively.\n(a) The 6 graphs from bottom to top correspond to 1, 2, 3, 4, 8, and 16\ncontrols, respectively, with $p_{\\mathrm{coh}}$ $\\!=$ $\\!1$ in (\\ref{pcoh}).\n(b) 16 controls, where after $4\\cdot10^3$ cycles the meaning\nof the symbols is reversed. The 5 graphs from bottom to top correspond in\nEq.~(\\ref{pcoh}) to $p_{\\mathrm{coh}}$= 0, 0.25, 0.5, 0.75, and 1, respectively.\n(c) applies 32 controls and $p_{\\mathrm{coh}}$ $\\!=$ $\\!1$, but refers to case\n(II) described by (\\ref{HA}) and (\\ref{HB}), whereas Figs.~\\ref{fig5}(a,b)\nrefer to case (I).\n}\n\\end{figure}\n\n$(\\overline{r}+1)/2$ is hence an estimate of the defender's probability to\nblock an attack. Referring to pure states in (\\ref{pcoh}), Fig.~\\ref{fig5}(a)\nshows the increase of learning speed with the number of controls. Significant\nlearning progress begins only after some initial period of stagnation. From the\nviewpoint of control theory, the identity, in which the memory is initialised,\nmay lie on a ``near-flat ground'' (valley), which must first be left before\nprogress can be made \\cite{Goe15}. Asymptotically, perfect blocking can be\nachieved once the memory becomes controllable, i.e., if the number of controls\nequals (or exceeds) the number of group generators.\nFig.~\\ref{fig5}(b) demonstrates the need of a pure input state\n$\\hat{\\varrho}_{\\mathrm{A}}$ in (\\ref{pcoh}) of the action subsystem A rather\nthan an incoherent mixture. After the agent in Fig.~\\ref{fig5}(b) had some time\nto adapt to the attacker, the meaning of the symbols is suddenly interchanged,\nand the agent must now learn to move in the opposite direction. This relearning\ndiffers from the preceding learning period in the absence of the mentioned\ninitial stagnation phase, which supports the above hypothesis of the proposed\nvalley, the agent has left during the initial learning. This plot is motivated\nby Fig. 5 in \\cite{Bri12} describing classical PS. Although the different\nbehaviour in the classical case suggests that this is an effect specific to\nquantum control, the phenomenon, that a dynamically changing environment can\nfacilitate learning in later stages appears to be more general \\cite{z3}. While\nFigs.~\\ref{fig5}(a,b) refer to case (I) described before (\\ref{HA}),\nFig.~\\ref{fig5}(c) refers to the restricted case (II), which appears to impede\nlearning. In the simulations of the following Sec.~\\ref{sec:ig44}, which all\nrefer to case (II), this is resolved by applying a 10 times larger negative\nreward for each wrong action. This demonstrates the flexibility in approaching\nRL problems offered by the freedom to allocate rewards in a suitable way.\n\n\\subsubsection{\n\\label{sec:ig44}\n4 percepts $\\to$  4 or 2 actions}\n\nWe now consider a version with 4 percepts, referring to an attacker presenting\neach of its two symbols in two colors at random. Since we want to keep the\nHilbert space dimension unchanged (rather than doubling it by adding the color\ncategory) for better comparison of the effect of the number of controls on the\nlearning curve, we must apply a memory as shown in Fig.~\\ref{fig2}(a).\nThe 4 percepts are encoded as tensor products of orthonormal projectors\n\n", "index": 23, "text": "\\begin{equation}\n\\label{is4}\n  \\hat{\\varrho}_{jk}=|{j}\\rangle\\langle{j}|\\otimes|{k}\\rangle\\langle{k}|,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\varrho}_{jk}=|{j}\\rangle\\langle{j}|\\otimes|{k}\\rangle\\langle{k}|,\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><mi>j</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>j</mi><mo fence=\"true\" stretchy=\"false\">|</mo><mo>\u2297</mo><mo fence=\"true\" stretchy=\"false\">|</mo><mi>k</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>k</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere $\\hat{U}_{\\mathrm{T}}$ is a given (randomly generated) 4-rowed target\nunitary acting on the total system. The memory in Fig.~\\ref{fig2}(a) is hence\nstill composed of two subsystems referring to the two percept categories\n`symbol' and `color', but both subsystem's initial state depends on the\nrespective percept, and both are measured afterwards. The differences between\nthe setup discussed in the previous Sec.~\\ref{sec:ig22} and the two setups\ndiscussed in the present Sec.~\\ref{sec:ig44} are summarised in Fig.~\\ref{fig6}.\n\n\\begin{figure}[ht]\n\\hspace{0.7cm}\\includegraphics[width=6.7cm]{fig6a.eps}\\\\\n\\vspace{0.5cm}\n\\includegraphics[width=6cm]{fig6b.eps}\\\\\n\\vspace{0.5cm}\n\\hspace{0.7cm}\\includegraphics[width=6.7cm]{fig6c.eps}\n\\caption{\\label{fig6}\nSetups for the invasion game as investigated numerically in Sec.~\\ref{sec:ig}.\nSetup (a) involves 2 percepts (symbols) and 2 actions (moves) as discussed in\nSec.~\\ref{sec:ig22} (Fig.~\\ref{fig5}). Setup (b) involves 4 percepts consisting\nof 2 two-colored symbols and 2 measurements yielding 4 different outcomes\ndescribed by $(j,k=0,1)$ as discussed in Sec.~\\ref{sec:ig44} and determining\neither 4 [Fig.~\\ref{fig7}(a) and Fig.~\\ref{fig8}] or -- if outcome $k$ is\nignored -- 2 [Fig.~\\ref{fig7}(b)] possible actions (moves). While setup (a)\nrefers to Fig.~\\ref{fig2}(c), setup (b) must refer to Fig.~\\ref{fig2}(a), if\nwe want to keep the same Hilbert space dimension of 4 for both setups, which\nallows better comparison of the effect of the number of controls on the learning\ncurve. Setup (c) involves a continuum of percepts consisting of 2\narbitrary-colored symbols and 2 actions (moves) as discussed in\nSec.~\\ref{sec:nec} (Fig.~\\ref{fig9}). In setup (c), separate subsystems are\nused for all 3 categories, hence it refers to Fig.~\\ref{fig2}(c), and the\nHilbert space dimension becomes 8.\n}\n\\end{figure}\n\n\nFig.~\\ref{fig7} shows the average reward $\\overline{r}$ received at each\ncycle, where the averaging is performed over an ensemble of $10^3$ independent\nagents, analogous to Fig.~\\ref{fig5}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig7a.eps}\n\\includegraphics[width=4.2cm]{fig7b.eps}\n\\caption{\\label{fig7}\n\nAverage reward $\\overline{r}$ as a function of the number of cycles for an\ninvasion game (2 symbols in 2 colors), learning rate $\\alpha$ $\\!=$ $\\!10^{-2}$,\nwith a reward of +1 (-10) for a correct (false) move, averaged over\n$10^3$ agents. The input states and POVM are given by Eqs.~(\\ref{is4}) and\n(\\ref{pom4}), respectively. The graphs correspond to 1, 2, 3, 4, 8, 16, and 32\ncontrols, as marked on the right.\n(a) Out of 4 possible moves, the defender must learn the correct one for each\nsymbol and color. After $5\\cdot10^3$ cycles, the meanings of the symbols as well\nas the colors are reversed.\n \n(b) Out of 2 possible moves, the defender must learn the correct one for each\nsymbol, whereas the color is irrelevant. For the first $5\\cdot10^3$ cycles, only\nsymbols in a single color are presented, whereas for the remaining cycles, they\nare shown randomly in both colors.\n \n}\n\\end{figure}\n\nNote that in this Sec.~\\ref{sec:ig44}, all figures refer to case (II) described\nby (\\ref{HA}) and (\\ref{HB}), where S and A now denote symbol and color,\nrespectively. To account for this [cf. the comments on Fig.~\\ref{fig5}(c)\nabove], a reward of $r$ $\\!=$ $\\!-10$ (instead of -1) is now given for a wrong\naction. The estimate of the defender's probability to block an attack is hence\nnow $(\\overline{r}+10)/11$.\n\nIn Fig.~\\ref{fig7}(a), the defender can choose between 4 moves, where for each\npercept, there is exactly one correct action [i.e., detecting $\\hat{\\Pi}_{jk}$\n($\\hat{\\Pi}_{j^\\prime{k}^\\prime}$ $\\!\\neq$ $\\!\\hat{\\Pi}_{jk}$) for\n$\\hat{\\varrho}_{jk}$ in (\\ref{pom4}) and (\\ref{is4}) returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-10$)]. After $5\\cdot10^3$\ncycles, symbol $j$ and color $k$ are read as symbol $1-j$ and color $1-k$,\nrespectively, similar to the manipulations in Fig. 5 in \\cite{Bri12}.\nIn Fig.~\\ref{fig7}(b), the defender can choose between 2 moves, where for each\nsymbol (relevant category), there is exactly one correct action, irrespective\nof its color (irrelevant category) [i.e., detecting\n$\\hat{\\Pi}_{j}$ $\\!=$ $\\!\\sum_{k=0}^{1}\\hat{\\Pi}_{jk}$\n($\\hat{\\Pi}_{j^\\prime}$ $\\!\\neq$ $\\!\\hat{\\Pi}_{j}$) for $\\hat{\\varrho}_{jk}$ in\n(\\ref{pom4}) and (\\ref{is4}) returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-10$)]. The second color is added only after \n$5\\cdot10^3$ cycles, analogous to Fig. 6 in \\cite{Bri12}.\n[Note that the mentioned initial stagnation phase in Fig.~\\ref{fig5} is not\nvisible in Fig.~\\ref{fig7}, which is attributed to the choice of parameters\n(rewards), accelerating the initial learning.]\n\nFigs.~\\ref{fig5}(b) and \\ref{fig7} are all motivated by Figs. 5 and 6 in\n\\cite{Bri12} and confirm that the agent's adaptation to changing environments\nis recovered in our quantum control context. In addition, Figs.~\\ref{fig5}(a)\nand \\ref{fig7} show the behaviour of an underactuated memory, where the number\nof controls is insufficient for its full controllability. Since a $U(n)$-matrix\nis determined by $n^2$ real parameters, and a global phase can be disregarded\n(so that we can restrict to SU($n$)-matrices), $n^2$ $\\!-$ $\\!1$ controls are\nsufficient, i.e., 15 for our invasion game, as mentioned above.\n\nIn (\\ref{pom4}), the measurements are made in a basis rotated by a randomly\ngiven unitary $\\hat{U}_{\\mathrm{T}}$, which serves two purposes. On the one\nhand, it is required to ensure that the agent starts at the beginning of the\nfirst cycle with a policy that does not give exclusive preference to certain\nactions that follow from symmetries of the (identity-) initialised memory. This\nis a flaw of Fig.~\\ref{fig2}(a) and can be overcome by using\nFig.~\\ref{fig2}(c) instead (cf. a more detailed discussion in the grid world\nexample below). On the other hand, $\\hat{U}_{\\mathrm{T}}$ serves as a given\ntarget in our discussion Sec.~\\ref{sec3}, where we consider the agent learning\nas a navigation of its memory $\\hat{U}$, cf. also Fig.~\\ref{fig6}(b).\nFig.~\\ref{fig8} compares the case, where the agent is always fed with percept\nstates drawn from one single ONB defined via (\\ref{is4}) with the case, where\nthe percept states are drawn randomly, i.e., taking\n$\\hat{U}_{\\mathrm{R}}\\hat{\\varrho}_{jk}\\hat{U}_{\\mathrm{R}}^\\dagger$ with a\nrandom unitary $\\hat{U}_{\\mathrm{R}}$ as explained in Sec.~\\ref{sec3} instead of\n(\\ref{is4}). Note that in Fig.~\\ref{fig8}, we generate a new random\n$\\hat{U}_{\\mathrm{R}}$ at each cycle, although a fixed set of dim$\\mathcal{H}$\n(4 in our case) such $\\hat{U}_{\\mathrm{R}}$ is sufficient as mentioned in\nSec.~\\ref{sec3}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig8a.eps}\n\\includegraphics[width=4.2cm]{fig8b.eps}\n\\includegraphics[width=4.2cm]{fig8c.eps}\n\\caption{\\label{fig8}\n\n(a) Reward $r$, (b) fidelity $F$ defined in (\\ref{F}), and (c) squared distance\n$D$ defined in (\\ref{D}), where the overline denotes the ensemble average over\n$10^3$ agents for the setup as in Fig.~\\ref{fig7}(a) (i.e., 4 percepts and 4\nactions) with 16 controls but without the reversal of meaning. The initial\nmemory states are drawn from either a single or multiple orthonormal bases.\n}\n\\end{figure}\n\nFidelity $F$ and squared distance $D$ are defined in (\\ref{F}) and (\\ref{D}),\nwhere $\\hat{U}$ represents the agent memory and $\\hat{U}_{\\mathrm{T}}$ the\ntarget unitary. Each cycle's update constitutes a single navigation step in the\nunitary group [U(4) in our example]. If, for a single ONB, after a number of\ncycles, the average reward has approached unity, $\\hat{U}$ has reached a close\nneighbourhood of any unitary of the form\n$\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$,\nwhere $\\hat{U}_2^\\prime$ $\\!=$\n$\\!\\hat{U}_{\\mathrm{T}}\\hat{U}_2\\hat{U}_{\\mathrm{T}}^\\dagger$\nwith $\\hat{U}_1$ and $\\hat{U}_2$ being undetermined 4-rowed unitary matrices\ndiagonal in the common eigenbasis of the $\\hat{\\varrho}_{jk}$\n(i.e., the ``computational basis''). Fig.~\\ref{fig8}(a) shows that for a\nsolution of the invasion game, a fixed ONB is sufficient. Drawing the percept\nstates randomly, so that the set of all percept states is linearly dependent,\ndoes not affect the agent's ability to achieve perfect blocking efficiency, but\nslows down the learning process. The single ONB case allows for a larger set of\n$\\hat{U}$ $\\!=$ $\\!\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$ with respect\nto $\\hat{U}_{\\mathrm{T}}$, as becomes evident in Fig.~\\ref{fig8}(b), so that\nnavigation of $\\hat{U}$ from the identity to a member of this set takes less\ntime (as measured in cycles). The only freedom left in the case of multiple ONBs\nis a global phase of $\\hat{U}$, which remains undefined: navigation of $\\hat{U}$\ntowards $\\hat{U}_{\\mathrm{T}}$ with respect to the squared Euclidean distance\n$D$ is not required for the learning tasks discussed, as evidenced by\nFig.~\\ref{fig8}(c).\n\n\\subsubsection{\n\\label{sec:nec}\nNeverending-color scenario}\n\nIn Sec.~\\ref{sec:ig44} we considered the case, where the symbols are presented\nin two different colors, as depicted in Fig.~\\ref{fig6}(b). The original\nmotivation for introducing colors as an additional percept category was to\ndemonstrate the agent's ability to learn that they are irrelevant \\cite{Bri12}.\nIn contrast, \\cite{Mel15} present a ``neverending-color scenario'', where at\neach cycle, the respective symbol is presented in a new color. It is shown that\nwhile the basic PS-agent is in this case unable to learn at all, it becomes able\nto \\emph{generalize} (abstract) from the colors, if it is enhanced by a\nwildcard mechanism. The latter consists in adding an additional\n(wildcard ``$\\#$'') value to each percept category, and inserting between the\ninput layer of percept clips and the output layer of action clips hidden layers\nof wildcard percept clips, in which some of the percept categories attain the\nwildcard value. The creation of these wildcard clips follows predefined\ndeterministic rules, and the transitions from percept to action clips take then\nplace via the hidden layers. (The notion of layers in the general ECM clip\nnetwork Fig. 2 in \\cite{Bri12} follows from restricting to clips of length\n$L$ $\\!=$ $\\!1$).\n\nSince the use of wildcard clips is an integrated mechanism within PS (inspired\nby learning classifier systems), the question is raised  how similar ideas could\nbe implemented in our context. For a memory Fig.~\\ref{fig2}(c), we could, e.g.,\nattribute one of the levels (such as the respective ground state) of the quantum\nsystem of each percept category $\\mathrm{S}_i$ to the wildcard-level\n$|\\#\\rangle_i$, so that the percept space\n$\\mathcal{H}_{\\mathrm{S}}$ $\\!=$ $\\!\\otimes\\mathcal{H}_{\\mathrm{S}_i}$ is\nenlarged to\n$\\mathcal{H}_{\\mathrm{S}}$ $\\!=$ $\\!\\otimes(\\mathcal{H}_{\\mathrm{S}_i}$\n$\\!\\oplus$ $\\!\\mathcal{H}_{\\#_i})$, where the $\\mathcal{H}_{\\#_i}$ are\none-dimensional.\n\nInstead of this, let us simply make use of the \\emph{built-in} generalization\ncapacity of a quantum agent resulting from its coding of percepts as quantum\nstates, which is much in the sense of Sec. 8 in \\cite{bookSuttonBarto}, where\nthe percepts can be arbitrarily real-valued rather than being drawn from a\ncountable or finite value set. Consider the setup shown in Fig.~\\ref{fig6}(c),\nwhose percept system includes a symbol and a color category and refers to a\nmemory structure Fig.~\\ref{fig2}(c). To allow for infinite colors, we could\napply a color quantum system with infinite levels $\\mathcal{H}_\\infty$  (such as\nan oscillator-type system), which is initialized at each cycle in a new state\ndrawn from a fixed ONB (such as a higher number state for an oscillator-type\nsystem). While such a scheme becomes more challenging to control, because the\ncontrol vector $\\bm{h}$ has an infinite number of components [we may replace it\nwith a continuous control function $h(t)$], it still ignores the fact that\ncolors (as most percept categories in general) are not countable. With this in\nmind, we can take the notion of colors literally and, to put it simply, code\nthem in some appropriate color space such as RGB, where three parameters\ncorrespond to the red-, green-, and blue-signals of the agent's eye sensors.\nThis suggests to encode a color as a mixed state of a two-level system, which is\nalso given by three real-valued parameters (determining its location in the\nBloch ball). The generalization from two colors to all RGB-colors then\ncorresponds to the generalization from a classical to a quantum bit. In our\nsetup, it is hence sufficient to apply a two-level system for the color category\nand initialize it at each cycle in a randomly chosen \\emph{mixed} state\n$\\hat{\\varrho}_{\\mathrm{C}}$ (for neverending colors) rather than a (pure) state\nrandomly drawn from a single ONB (for two colors), whereas no changes are\nrequired on the agent's memory configuration itself. Fig.~\\ref{fig9}\ndemonstrates the learning process.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig9a.eps}\n\\includegraphics[width=4.2cm]{fig9b.eps}\n\\caption{\\label{fig9}\n(a) Reward $r$ and (b) length $|\\bm{h}|$ of the control vector as a function of\nthe number of cycles for a single agent Fig.~\\ref{fig6}(c) playing an invasion\ngame with 2 symbols, presented in a continuum of neverending colors, and 2\nmoves. A reward of +1 (-10) is given for each correct (false) move. The agent\napplies 64 controls with a learning rate of $\\alpha$ $\\!=$ $\\!10^{-2}$ in an\nalternating layer scheme App.~\\ref{app:fl} defined by two\n(Schmidt-orthonormalized) 8-rowed random Hamiltonians.\n}\n\\end{figure}\n\nSimilar to Fig.~\\ref{fig8}, random initialization slows down the learning\nprocess, so that we restrict to a single agent in Fig.~\\ref{fig9}, rather than\nan ensemble average. As illustrated in Fig.~\\ref{fig9}(a), the agent's response\nbecomes near-deterministic after about $10^6$ cycles, irrespective of the color.\nFig.~\\ref{fig9}(b) illustrates in the example of the Euclidean length of the\ncontrol vector $|\\bm{h}|$ $\\!=$ $\\!\\sqrt{\\bm{h}^\\mathrm{T}\\cdot\\bm{h}}$, that\nthe navigation, which starts at $\\bm{h}_0$ $\\!=$ $\\!\\bm{0}$, eventually comes to\nrest. While the random $\\hat{\\varrho}_{\\mathrm{C}}$ are drawn such that a\npositive probability is attributed to every volume element in the Bloch ball, we\ndid not care about drawing them with a uniform probability density, since\nmapping of an RGB-space of color (as a perceptual property) to the Bloch ball\nis not uniquely defined.\n\nThe ability to learn to distinguish between relevant\n$\\mathrm{S}_{\\mathrm{rel}}$ and an arbitrary number of irrelevant percept\ncategories $\\mathrm{S}_{\\mathrm{irr}}$ as discussed in \\cite{Mel15} is of\nparticular relevance for a quantum agent, where the irrelevant percept\ncategories can be understood as adopting the role of a bath as shown in\nFigs.~\\ref{fig2}(b) and (d). Here, a formal solution consists in a decoupled\n$\\hat{U}_{\\mathrm{SA}}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{S_{rel}A}}$ $\\!\\otimes$\n$\\!\\hat{U}_{\\mathrm{S_{irr}}}$.\n\n\\subsection{\n\\label{sec:gw}\nGrid world}\n\nIn what follows, we consider an arrangement of 8 grid cells as shown in\nFig.~\\ref{fig10}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig10.eps}\n\\caption{\\label{fig10}\n\n$(3\\times3)$-grid world with an obstacle (black cell). The arrows show the\noptimal policy for the shortest path to G, and the numbers present the policy\nnumerically obtained with the agent Fig.~\\ref{fig11}(d) after $10^{4}$\nepisodes. The red numbers in parentheses are the relevant different values\nobtained after $10^{5}$ episodes, if the agent starts each episode from a random\ncell, rather than always at S.\n}\n\\end{figure}\n\nThe agent's task is to find the shortest route to a goal cell G, where at each\nstep, only moves to an adjacent cell in four directions (left, right, up, down)\nare allowed. If the agent hits a boundary of the grid or the black cell, which\nis considered an obstacle, its location remains unchanged.\n \nThis external classical navigation task constitutes a learning problem, because\nsituations/percepts (present location) must be mapped to decisions/actions\n(direction to go). The agent only perceives whether or not it has arrived at the\ngoal cell. It has no access to a ``bird's perspective'' which would allow\nimmediate exact location of the goal. It also has no access to a measure of goal\ndistance or fidelity (as in the case of the internal NO-based loop regarding its\nown quantum memory in Fig.~\\ref{fig1}), which prevents the use of external\ngradient information that could be obtained by testing the nearest neighbourhood\nof the present location. One can thus distinguish two objectives: (a) locating\nthe goal and (b) finding a shortest route to it. This task constitutes a RL type\nproblem, whose composite ``two-objective'' structure is approached by nesting\niterations. The individual action selections, i.e., choices of moves, correspond\nto the cycles in Fig.~\\ref{fig1}. Sequences of cycles form episodes, which are\nterminated only once objective (a) has been solved. Objective (b) is solved by\nsequences of episodes, which allow the agent to gradually solve objective (a)\nmore efficiently and find an optimal policy. In Fig.~\\ref{fig10}, the policy\nconsists of a set of four probabilities for each cell, with which a\ncorresponding move should be made from there. The optimal policy corresponding\nto the shortest route to G is indicated by the arrows in Fig.~\\ref{fig10}.\n\nThis grid world extends the above decision game in two aspects:\n(a) The optimal policy is in contrast to the decision game not deterministic,\nas indicated by the double arrows in the upper left and middle cell in\nFig.~\\ref{fig10}. (b) Depending on where the agent starts, more than a single\nmove is required to reach G in general, preventing the agent from obtaining an\nimmediate environmental feedback on the correctness of each individual move it\nmakes. This second aspect leads to the mentioned notion of episodes. In what\nfollows, we always place the agent at a fixed start cell S at the beginning of\neach episode, which is sufficient for learning the shortest path from S to G.\nWhile in the invasion game, episodes and cycles are synonyms, here, an episode\nis longer than a single cycle, since at least four moves are required to reach G\nfrom S.\n\nWhen designing the agent's memory structure in the sense of Fig.~\\ref{fig2},\nwe must take into account that the unitarity of the state transformation\n$\\hat{U}_{\\mathrm{S}}$ in Fig.~\\ref{fig2}(a) places restrictions on the\npercept-encodings and the action-measurements, since $\\hat{U}_{\\mathrm{S}}$\nmaps an ONB into another one.\nIf we encode in Fig.~\\ref{fig10} each cell location as a member of a given\nONB in an 8-dimensional system Hilbert space $\\mathcal{H}_8$ and perform a naive\nsymmetric $\\mathcal{H}_8$ $\\!=$ $\\!\\mathcal{H}_2$ $\\!\\oplus$\n$\\!\\mathcal{H}_2$ $\\!\\oplus$ $\\!\\mathcal{H}_2$ $\\!\\oplus$ $\\!\\mathcal{H}_2$\n-measurement for action selection, where the four 2-dimensional subspaces\ncorrespond to right, down, left and up moves, we cannot properly ascribe the\nupper left and upper middle cells, because the right and downward pointing\nactions are already ascribed to the remaining 4 white cells. One may either try\nto construct a learning algorithm that exploits the fact that the two mentioned\ncells are off the optimal path from S to G so that the agent quickly ceases to\nvisit them or construct a new POVM such as a\n$\\mathcal{H}_8$ $\\!=$ $\\!\\mathcal{H}_1$ $\\!\\oplus$ $\\!\\mathcal{H}_3$ $\\!\\oplus$\n$\\!\\mathcal{H}_3$ $\\!\\oplus$ $\\!\\mathcal{H}_1$-measurement, where two\n3-dimensional subspaces correspond to right and down, and two 1-dimensional\nsubspaces correspond to left and up moves. These possibilities require insight\ninto the specifics of this problem and are not generalizable. In addition to\nthat, Fig.~\\ref{fig2}(a) requires initialisation of the agent memory in a\nrandom unitary to ensure it starts with a policy that does not give exclusive\npreference to certain actions that follow from symmetries of the initial\n$\\hat{U}_{\\mathrm{S}}$ (such as the identity\n$\\hat{U}_{\\mathrm{S}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{S}}$). If we want to avoid\ninvoking a bath as in Fig.~\\ref{fig2}(b), we hence must resort to\nFig.~\\ref{fig2}(c), which here implies a factorisation\n$\\mathcal{H}$ $\\!=$ $\\!\\mathcal{H}_{\\mathrm{S}}$ $\\!\\otimes$\n$\\!\\mathcal{H}_{\\mathrm{A}}$ of $\\mathcal{H}$ into an 8-dimensional\n$\\mathcal{H}_{\\mathrm{S}}$ for encoding the grid cells and a 4-dimensional\n$\\mathcal{H}_{\\mathrm{A}}$ for encoding the four actions. If we encode the cells\nand actions as members of some ONB in S and A, then initialising the agent's\nmemory as identity, $\\hat{U}_{\\mathrm{SA}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{SA}}$,\nand the initial action states as in (\\ref{ia}) ensures that the agent starts at\nthe beginning of the first episode with a policy that assigns the same\nprobability to all possible actions.\n\nIn Figs.~\\ref{fig11} and \\ref{fig12} we investigate the episode length which\nis defined as the number of cycles per episode. Rather than performing an\nensemble average, we consider individual agents. These agents are described by\n(\\ref{ur3}) with a learning rate of $\\alpha$ $\\!=$ $\\!10^{-1}$, absence of\nrelaxation ($\\kappa$ $\\!=$ $\\!0$), and varying amounts of gradient glow\n($\\eta$ $\\!\\le$ $\\!1$). The number of episodes equals the number of times the\nagent is allowed to restart from S, whereas the time passed equals the sum of\nepisode lengths. The episode length can be infinite but not smaller than four,\nthe length of the shortest path from S to G.\n\nFig.~\\ref{fig11} shows evolutions of episode lengths with the number of\nepisodes, where we have set a maximum of $10^4$ episodes. As explained, each\nepisode starts at S and ends only when G has been reached.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig11a.eps}\n\\includegraphics[width=4.2cm]{fig11b.eps}\n\\includegraphics[width=4.2cm]{fig11c.eps}\n\\includegraphics[width=4.2cm]{fig11d.eps}\n\\includegraphics[width=4.2cm]{fig11e.eps}\n\\includegraphics[width=4.2cm]{fig11f.eps}\n\\caption{\\label{fig11}\n\nEpisode lengths as a function of the number of episodes for the\n$(3\\times3)$-grid world as shown in Fig.~\\ref{fig10}. The plots illustrate the\neffect of gradient glow for single histories of individual agents.\n(a) The agent receives a reward $r$ $\\!=$ $\\!1$ when it has found the goal,\nwithout gradient glow ($\\eta$ $\\!=$ $\\!1$).\n(b) As in (a), but with gradient glow enabled ($\\eta$ $\\!=$ $\\!0.01$).\n(c) In addition to receiving a reward $r$ $\\!=$ $\\!1$ when it has found the\ngoal, the agent is punished with a reward $r$ $\\!=$ $\\!-10$, when it has hit a\nboundary, without gradient glow ($\\eta$ $\\!=$ $\\!1$).\n(d) As in (c), but with gradient glow enabled ($\\eta$ $\\!=$ $\\!0.7$).\n(e) As in (d), but with  gradient glow prolonged further\n($\\eta$ $\\!=$ $\\!0.5$).\n(f) Learning is disabled by always setting the reward to 0. The agent performs a\nrandom walk through the grid with average length 54.1 which is included as\ndashed line in Figs.~\\ref{fig11} and \\ref{fig12}.\n}\n\\end{figure}\n\nFig.~\\ref{fig11}(f) shows for comparison the lengths of $10^4$ random walks\nthrough the grid of an agent whose learning has been disabled by always setting\nthe reward to 0. The average number of 54.1 steps to reach G from S is \nshown in Figs.~\\ref{fig11} and \\ref{fig12} as a dashed line for comparison.\nIn Figs.~\\ref{fig11}(a-e), a positive reward of $r$ $\\!=$ $\\!1$ is given for\nhitting G. While in Fig.~\\ref{fig11}(a), the reward is always zero before G\nhas been hit, in Fig.~\\ref{fig11}(c) hitting a boundary is punished with a\nnegative reward of $r$ $\\!=$ $\\!-10$, which slightly improves the agent's\nperformance. [Note that all plots are specific to the respective learning rate\n(here $\\alpha$ $\\!=$ $\\!10^{-1}$), which has been chosen by hand to observe an\nimprovement within our $10^4$ episode-window and at the same time\nminimising the risk of oversized learning steps. While in general, the learning\nrate is gradually decreased (cf. the conditions Eq. (2.8) in\n\\cite{bookSuttonBarto} to ensure convergence), this is not strictly necessary.\nIn our numerical examples we have kept $\\alpha$ constant for simplicity.\nImplementation of a dynamic adaptation of the learning rate as was done in\n\\cite{clausen18} and \\cite{Cla15} in the present context is left for future\nwork.] The transitions Fig.~\\ref{fig11}(a$\\to$b) and\nFig.~\\ref{fig11}(c$\\to$d) show the effect of enabling gradient glow,\ni.e. $(\\eta=1)$ $\\!\\to$ $\\!(\\eta<1)$ in (\\ref{ur3}). Gradient glow provides a\nmechanism of gradual backpropagation of the policy change from the nearest\nneighbourhood of G to cells more distant from G as the number of episodes\nincreases. In Fig.~\\ref{fig11}, the agent settles in the optimal policy in\ncases (b), (d) and (e).\n\nThe policy resulting after $\\!10^{4}$ episodes in case Fig.~\\ref{fig11}(d) is\ngiven in Fig.~\\ref{fig10}, where the numbers in each cell present the\nprobability to move in the respective direction. While the agent finds the\noptimal policy for all cells forming the shortest path, it remains ignorant for\nthe remaining cells. As the agent finds and consolidates the shortest path, then\nepisode over episode, it soon visits the off-path cells less frequently, so that\nthe transition probabilities from these cells do not accumulate enough\niterations and are ``frozen'' in suboptimal values. This is characteristic of RL\nand can also be observed in learning to play games such as Backgammon\n\\cite{bookSuttonBarto}, where it is sufficient to play well only in typical\nrather than all possible constellations of the game. Since for large games, the\nformer often form a small subset of the latter, this can be seen as a strategy\nto combat with large state spaces (such as number of possible game\nconstellations). To find an optimal policy for \\emph{all} cells in\nFig.~\\ref{fig10}, we may start each episode from a random cell, analogous to\ninitialising the agent in an overcomplete basis as explained in\nFig.~\\ref{fig8}. The red numbers in parentheses shown in Fig.~\\ref{fig10}\npresent a new policy obtained after $\\!10^{5}$ episodes in this way. In contrast\nto the old policy, it is optimal or nearly optimal for all cells, with the\ndifference between 1 and the sum of these numbers quantifying the deviation\nfrom optimality for each cell $({\\scriptstyle\\stackrel{<}{=}}0.02)$. Since on\naverage, the agent starts from a given cell only in 1/7-th of all episodes, the\nlearning is slowed down, analogous to Fig.~\\ref{fig8}(a).\n\nFig.~\\ref{fig12} summarises the effect of gradient glow illustrated in\nFig.~\\ref{fig11} for the two rewarding strategies.\n\n\\begin{figure}[ht]\n\\includegraphics[width=8.6cm]{fig12.eps}\n\\caption{\\label{fig12}\n\nEpisode lengths $c$ averaged over the last 500 episodes in Fig.~\\ref{fig11},\ni.e., episodes $9.5*10^3$ $\\!-$ $\\!1*10^4$ of single histories of individual\nagents in the $(3\\times3)$-grid world as shown in Fig.~\\ref{fig10}. The data\npoints distinguish various rewarding strategies and values of gradient glow\n$\\eta$ as explained in Fig.~\\ref{fig11}. The upper and lower dashed lines\nreflect a random walk and the shortest path, respectively, and the letters\n(a)-(f) correspond to the respective plots in Fig.~\\ref{fig11}. The optimum\nvalue of $\\eta$ depends on the details of the rewarding strategy.\n}\n\\end{figure}\n\nTo limit the numerical effort, we have averaged the episode lengths over the\nlast 500 episodes in Fig.~\\ref{fig11} for individual agents as a ``rule\nof thumb''-measure of the agent's performance for the strategy chosen. For a\ndeterministic calculation we must instead average the length of each episode\n(and for each $\\eta$) over a sufficiently large ensemble of independent agents\nfor as many episodes as needed to reach convergence. Despite these shortcomings,\nthe results indicate a qualitatively similar behaviour as Fig. 4(a) in\n\\cite{Mel14}. Figs.~\\ref{fig11} and \\ref{fig12} demonstrate that gradient\nglow improves the agent performance, irrespective of whether or not it\nreceives information on false intermediate moves by means of negative rewards,\nalthough the latter reduce the required length of glow.\nIt is expected that for an ensemble average, an optimal value of $\\eta$ can be\nfound, with which the fastest convergence to the shortest path can be achieved.\nFig.~\\ref{fig11} distinguishes two qualitatively different modes of\nconvergence. If $\\eta$ is larger than optimal, a gradual improvement is\nobserved, as seen by the damping of spikes in Fig.~\\ref{fig11}(d). If $\\eta$\nis smaller than optimal, then an abrupt collapse to the optimal policy without\nvisible evidence in the preceding statistics that would provide an indication is\nobserved, cf. Fig.~\\ref{fig11}(e). If $\\eta$ is decreased further, this\ntransition is likely to happen later, to the point it will not be observed\nwithin a fixed number of episodes. This results in the steep increase in episode\nlength shown in Fig.~\\ref{fig12}, which would be absent if the ensemble\naverage was used instead. This sudden transition as shown in\nFig.~\\ref{fig11}(e) can also be observed for individual agents in \\cite{Mel14}\n(not shown there), which applies a softmax-policy function along with edge glow.\nIt is surprising that the quadratic measurement-based policy simulated here\nexhibits the same phenomenon. Note however, that convergence does not imply\noptimality. In tabular RL and PS, such an abrupt transition can be observed if\nthe $\\lambda$-parameter and hence the ``correlation length'' is too large\n(in RL) or if the $\\eta$-parameter is too small, so that the glow lasts too long\n(in PS). The policies obtained in this way are typically sub-optimal, especially\nin larger scale tasks such as bigger grid worlds, for which the agent learns\n``fast but bad'' in this case. It is hence expected that a similar behaviour can\nbe observed for our method if we increased the size of the grid.\n\n\\section{\n\\label{sec6}\nFinite difference updates}\n\nThis work's numerical experiments rely on a symbolic expression\n(\\ref{gradcomps}) for the gradient $\\bm{\\nabla}_{t}$ in (\\ref{grad}) for\nsimplicity, which is usually not available in practice, also keeping in mind the\nvariety of compositions Fig.~\\ref{fig2}, so that the agent's memory\n$\\hat{U}(\\bm{h})$ is generally unknown. As explained in the discussion of\nFig.~\\ref{fig1}, the agent may then apply a measurement-based internal loop by\nrepeatedly preparing its memory in a state that corresponds to the last percept\n$s_t$, and register whether or how often the last measurement outcome $a_t$ can\nbe recovered. This approach can be done with either infinitesimal or finite\nchanges in the control vector $\\bm{h}$, where we can distinguish between\nexpectation value- and sample-based updates, depending on how many internal\ncycles are performed between consecutive external cycles.\nIt should be stressed that the external cycles in Fig.~\\ref{fig1}\nrepresent the agent-environment interaction, resulting in sequences of\nstate-action pairs and corresponding rewards. While in an elementary optimal\ncontrol problem, a given objective is to be optimized, here the environment\nposes at each external cycle a separate and generally unpredictable control\nproblem, all of which must be addressed by the agent simultaneously.\n\nDue to the small learning rate $\\alpha$, the update rule (\\ref{ur3}) is in all\ncases local in parameter space, which reflects the assumption, that a physical\nagent cannot completely reconfigure its ``hardware'' in a single instant. While\nit is then consistent to apply a gradient $\\bm{D}_t$ $\\!=$ $\\!\\bm{\\nabla}_t$ as\na local quantity in (\\ref{ur3}), from a computational perspective, it has a few\ndrawbacks, however. One is that the direction of steepest accent at the current\ncontrol vector $\\bm{h}_t$ does not need to coincide with the direction\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$ towards the optimum\n$\\bm{h}_t^*$, as illustrated in Fig.~\\ref{fig13}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig13.eps}\n\\caption{\\label{fig13}\n\nFollowing the direction $\\bm{\\nabla}p(a_t|s_t)$ of steepest ascent (dashed line)\ndoes not necessarily lead to the shortest route\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$\nfrom a given control vector $\\bm{h}_t$ at cycle $t$ to a location $\\bm{h}_t^*$,\nfor which $p(a_t|s_t)$ becomes maximum.\n}\n\\end{figure}\n\n\nAnother aspect is the vanishing of the gradient. Consider for example the\ninitialisation of the action system in a mixed state (\\ref{pcoh}) as done in\nFig.~\\ref{fig5}(b). In particular, the graph with\n$p_{\\mathrm{coh}}$ $\\!=$ $\\!0$ does not display any learning ability.\nSubstituting the corresponding\n$\\hat{\\varrho}_{\\mathrm{A}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{A}}/2$ in (\\ref{pcoh})\nand $\\hat{U}$ $\\!=$ $\\!\\hat{I}$ into (\\ref{gradcomps}), we see that the reason\nis the vanishing gradient, $\\nabla_k$ $\\!=$\n$\\!\\mathrm{Im}\\mathrm{Tr}[\\hat{\\varrho}_{\\mathrm{S}}\\hat{\\Pi}({a})\\hat{H}_k]$\n$\\!=$ $\\!0$. On the other hand, the corresponding setup Fig.~\\ref{fig6}(a)\nreveals that in this case, substituting a SWAP-gate between S and A for\n$\\hat{U}$ provides an optimal solution (along with an X-gate if the meaning of\nthe symbols is reversed) for any $\\hat{\\varrho}_{\\mathrm{A}}$, that is obviously\nnot found in Fig.~\\ref{fig5}(b). This failure occurs despite the fact that the\nagents explore, as indicated by the fluctuations in Fig.~\\ref{fig5}(b). To\nunderstand the difference, note that we may generate an $\\varepsilon$-greedy\npolicy function by replacing in (\\ref{grada}) an (arbitrarily given) state\n$\\hat{\\varrho}({s})$ with\n$\\frac{\\hat{\\varrho}({s})+\\varepsilon\\hat{I}}{1+\\varepsilon{d}}$,\nwhere $0$ $\\!<$ $\\!\\varepsilon$ $\\!\\ll$ $\\!1$ and \n$d$ $\\!=$ $\\!\\mathrm{Tr}\\hat{I}$. The term with $\\hat{I}$ then gives to\n(\\ref{grada}) a contribution $\\sim\\mathrm{Tr}_{\\mathrm{A}}\\hat{\\Pi}({a})$, that\nis independent of $s$. At the same time, it does not contribute in\n(\\ref{gradcomps}) to the gradient, $\\nabla_k$ $\\!=$ $\\!0$.\nIf $\\bm{D}_t$ $\\!=$ $\\!\\bm{\\nabla}_t$ $\\!=$ $\\!0$ for all $t$ in (\\ref{ur3}),\nthe agent's learning comes to rest, however. Finite difference and\nsample-based updates here offer a possibility to explore\n\\emph{in parameter space} the neighbourhood of the present location $\\bm{h}_t$\n(or, colloquially, the ``state'') of the agent's memory, as a consequence of\nasymmetries in the control landscape or statistical fluctuations in the samples.\n\nOf particular relevance is a final fixpoint (\\ref{task}). Intuitively, one\nwould assume that (despite the compactness of the (S)U($n$)-groups, that is in\ncontrast to the potentially unbounded values of $U$ in RL or $h$ in PS) once an\nagent has settled in a point (\\ref{task}), due to the vanishing gradient, it\nwon't be able to react quickly, if the environment suddenly changes its\nallocation of rewards (without confronting the agent with percepts it has not\nperceived before). However, the learning curves for controllable memories (16\nand 32 controls) in Fig.~\\ref{fig7}(a) demonstrate that relearning after\n$5\\cdot10^3$ cycles is not affected. A study of individual agents with 32\ncontrols in Fig.~\\ref{fig7}(a) reveals that the Euclidean length of the\nnumerical gradient rises from $10^{-14}$ at cycle 5000 to a value $>1$ in only\n15 cycles. Better understanding of this is left for future study.\nIn what follows, we outline the mentioned alternatives in some more detail.\n\n\\subsection{\nExpectation value-based updates}\n\nIf the time consumed by the internal cycles is uncritical with respect to the\nexternal cycles, the agent can obtain estimates of $p(a_t|s_t)$ from a\nsufficiently large number of internal binary measurements. With these, it can\neither approximate the components $\\nabla_kp(a_t|s_t;{h}_j)$ $\\!\\approx$\n$\\![p(a_t|s_t;{h}_j+\\delta_{jk}\\delta{h}_k)$ $\\!-$\n$\\!p(a_t|s_t;{h}_j)]/\\delta{h}_k$ of the local gradient\n$\\bm{\\nabla}_t$ $\\!=$ $\\!\\bm{\\nabla}p(a_t|s_t;\\bm{h}_t)$, which is then\nsubstituted as $\\bm{D}_t$ $\\!=\\bm{\\nabla}_t$ into (\\ref{ur3}). Alternatively, it\ncan perform a global search for the location $\\bm{h}_t^*$ of the maximum of\n$p(a_t|s_t)$. A possible algorithm for the latter is differential evolution,\nwhich relies on deterministic values $p(a_t|s_t;\\bm{h})$ rather than noisy\nsamples. Once an estimate for $\\bm{h}_t^*$ has been found, the difference\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$ is used in (\\ref{ur3}).\n\n\\subsection{\nSample-based updates}\n\nReliance on expectation values may give away potential speed gains offered by a\nquantum memory, which poses the question, whether a finite number of sample\nmeasurements is sufficient. Since individual updates in (\\ref{ur3}) are made\nwith a small fraction $\\alpha$ of the whole $\\bm{D}_t$, the assumption is that\nthe individual statistical errors in the sampled $\\bm{D}_t$ cancel out in the\nlong run.\n\nAs for the expectation value-based updates discussed above, samples can be used\nto either create \\emph{discrete} estimates\n$\\nabla_kp(a_t|s_t;{h}_j)$ $\\!\\approx$ $\\![s({h}_j+\\delta_{jk}\\delta{h}_k)$\n$\\!-$ $\\!s({h}_j)]/2$ for the components $k$ of the local gradient\n$\\bm{\\nabla}_t$ $\\!=$ $\\!\\bm{\\nabla}p(a_t|s_t;\\bm{h}_t)$, where\n$s$ $\\!=$ $\\!s(\\bm{h}_t)$ $\\!=$ $\\!\\pm1$ depending on whether the outcome of the\nbinary measurement $(a_t|s_t;\\bm{h}_t)$ is positive or not. Alternatively, for\nfinite difference updates, one may consider a neural gas\n\\cite{MaSc91,*Fritzke95agrowing} inspired approach depicted in Fig.~\\ref{fig14}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=5cm]{fig14.eps}\n\\caption{\\label{fig14}\n\nInternally generated random cloud of sample controls $\\bm{h}_k$ around\na given control vector $\\bm{h}_t$ at cycle $t$ for which binary measurements\n``given $s_t$, detect $a_t$ or not'' are carried out between external cycles,\nyielding positive ($\\bm{h}_k^+$) or negative ($\\bm{h}_k^-$) outcomes.\n}\n\\end{figure}\n\nIn this approach, the differences\n\n", "itemtype": "equation", "pos": 53740, "prevtext": "\nwhere $j$ $\\!=$ $\\!0,1$ ($k$ $\\!=$ $\\!0,1$) refers to the symbol (color).\nThe POVM operators are the 4 projectors\n\n", "index": 25, "text": "\\begin{equation}\n\\label{pom4}\n  \\hat{\\Pi}_{jk}=\\hat{U}_{\\mathrm{T}}\\hat{\\varrho}_{jk}\n  \\hat{U}_{\\mathrm{T}}^\\dagger,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\Pi}_{jk}=\\hat{U}_{\\mathrm{T}}\\hat{\\varrho}_{jk}\\hat{U}_{\\mathrm{T}}^{%&#10;\\dagger},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a0</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nbetween the sampled centers of positive\n[$s_k$ $\\!=$ $\\!s(\\bm{h}_k)$ $\\!=$ $\\!+1$, i.e.,\n$\\bm{h}_k$ $\\!=$ $\\!\\bm{h}_k^+$] and negative outcomes\n($s_k$ $\\!=$ $\\!s(\\bm{h}_k)$  $\\!=$ $\\!-1$, i.e.,\n$\\bm{h}_k$ $\\!=$ $\\!\\bm{h}_k^-$) of the binary measurements $(a_t|s_t;\\bm{h}_k)$\nare then applied in (\\ref{ur3}). Although one could store a current estimate\n$\\bm{D}_t^{(n)}$ for each observed state-action pair $(a_t|s_t)$ and merely\nupdate it according to (\\ref{ng}) with each new measurement point\n$s_{n+1}\\bm{h}_{n+1}$, this would give away the generalization capability\ndescribed in Sec.~\\ref{sec:nec}. One would hence need to perform $n$ internal\ncycles with the POVM-based internal loop between each external cycle.\nThe $\\bm{h}_k$ could be drawn, e.g., from a Gaussian centered around the\nrespective $\\bm{h}_t$. The variance of this Gaussian could be gradually\ndecreased with the number of external cycles to increase the locality (local\nresolution) of the cloud.\n\nFig.~\\ref{fig13} gives the misleading impression that finite difference updates\nare superior to gradient-based methods. To give an illustrative counterexample,\none could think of a two-dimensional $\\bm{h}$ $\\!=$ $\\!(h_x,h_y)$ and a control\nlandscape $p(\\bm{h})$ modelled by the monotonically increasing height $p(l)$\nalong the length $l$ of a tape of paper bent into a spiral and placed onto the\ndashed line in Fig.~\\ref{fig13}, such that one end with $p(0)$ $\\!=$ $\\!0$ is\nlocated at $\\bm{h}_t$ and the other one at $\\bm{h}_t^*$. Here, a gradient-based\nmethod would safely follow the long path on the tape's upper edge, whereas a\nfinite difference method would trade a potential speedup with the risk of\nmissing the paper at all trials. Since a comparison of state of the art optimal\ncontrol methods based on noisy samples for the agent's learning would go beyond\nthe scope of this work, we here restrict ourselves to these sketchy lines of\nthought, whose numerical study is pending, and leave open the question of what\nthe best method is for a given task.\n\nA characteristic shared by the loops of Fig.~\\ref{fig1} and\noptimal control setups is the need of an experimental ``mastermind'' who\ncontrols the controls. An agent which is supposed to act autonomously would be\nrequired to accomplish this by itself, ideally in a ``natural'' or ``organic\ncomputing'' sense. An elementary example from everyday life are ``desire paths''\nwhich form or dissipate, depending on their usage and without a designated\nplanner.\n\n\\section{\n\\label{sec5}\nSummary and outlook}\n\nIn summary, we have adopted an update rule from the basic PS scheme, equipped it\nwith gradient glow, and applied it to small-scale invasion game and grid\nworld tasks. The numerical results show that similar results can be obtained for\na quantum agent, as long as the memory is not underactuated. This is not\nobvious, because of the fundamental difference in the number of free parameters.\nIf $S$ and $A$ denote the number of possible percepts and actions, respectively,\nthen in classical tabular action value RL-methods, the estimated values of all\npercept-action pairs are combined to a $({S}\\times{A})$-matrix, i.e., we have\n$(SA)$ real parameters. If we encoded in our scheme each percept and action\ncategory by a separate subsystem, whose dimensionalities correspond to the\nnumber of values, the respective category can adopt, then $\\hat{U}$ is an at\nleast $U(N=SA)$-matrix for which we are faced with $(SA)^2$ real parameters.\nNote that this work is unrelated to the reflecting PS agents, which are\ndiscussed in \\cite{Pap14}. While the scheme \\cite{Pap14} allows a proof of\nquantum speedup, our approach complements the latter in that it is simple,\nflexible in its construction, and does not involve specific analytic quantum\ngates. The learning of a good policy only for percepts which are ``typical'' and\nhave thus been encountered sufficiently often in the past shares features with\n``soft computing'', where it is sufficient to find a good rather than an exact\nsolution, which would here consist in a policy that is optimal for all possible\npercepts. One may think of, e.g., simplifying a symbolic mathematical\nexpression: while all transformation steps themselves must be exact, there are\nno strict rules, as far as the best way of its formulation is concerned. In\nfuture work, it may be worth to incorporate recent extensions of the classical\nPS scheme such as generalization \\cite{Mel15}.\n\n\\appendix\n\n\\section{\n\\label{app:A1}\nClassical PS update rule}\n\nClassical PS in its general form is based on a discrete network of clips that\nform its ECM. In the examples considered in \\cite{Bri12,Mau15,Mel14}, after each\ndiscrete time step (external cycle), a local edge is updated according to\n\n", "itemtype": "equation", "pos": 91648, "prevtext": "\nwhere $\\hat{U}_{\\mathrm{T}}$ is a given (randomly generated) 4-rowed target\nunitary acting on the total system. The memory in Fig.~\\ref{fig2}(a) is hence\nstill composed of two subsystems referring to the two percept categories\n`symbol' and `color', but both subsystem's initial state depends on the\nrespective percept, and both are measured afterwards. The differences between\nthe setup discussed in the previous Sec.~\\ref{sec:ig22} and the two setups\ndiscussed in the present Sec.~\\ref{sec:ig44} are summarised in Fig.~\\ref{fig6}.\n\n\\begin{figure}[ht]\n\\hspace{0.7cm}\\includegraphics[width=6.7cm]{fig6a.eps}\\\\\n\\vspace{0.5cm}\n\\includegraphics[width=6cm]{fig6b.eps}\\\\\n\\vspace{0.5cm}\n\\hspace{0.7cm}\\includegraphics[width=6.7cm]{fig6c.eps}\n\\caption{\\label{fig6}\nSetups for the invasion game as investigated numerically in Sec.~\\ref{sec:ig}.\nSetup (a) involves 2 percepts (symbols) and 2 actions (moves) as discussed in\nSec.~\\ref{sec:ig22} (Fig.~\\ref{fig5}). Setup (b) involves 4 percepts consisting\nof 2 two-colored symbols and 2 measurements yielding 4 different outcomes\ndescribed by $(j,k=0,1)$ as discussed in Sec.~\\ref{sec:ig44} and determining\neither 4 [Fig.~\\ref{fig7}(a) and Fig.~\\ref{fig8}] or -- if outcome $k$ is\nignored -- 2 [Fig.~\\ref{fig7}(b)] possible actions (moves). While setup (a)\nrefers to Fig.~\\ref{fig2}(c), setup (b) must refer to Fig.~\\ref{fig2}(a), if\nwe want to keep the same Hilbert space dimension of 4 for both setups, which\nallows better comparison of the effect of the number of controls on the learning\ncurve. Setup (c) involves a continuum of percepts consisting of 2\narbitrary-colored symbols and 2 actions (moves) as discussed in\nSec.~\\ref{sec:nec} (Fig.~\\ref{fig9}). In setup (c), separate subsystems are\nused for all 3 categories, hence it refers to Fig.~\\ref{fig2}(c), and the\nHilbert space dimension becomes 8.\n}\n\\end{figure}\n\n\nFig.~\\ref{fig7} shows the average reward $\\overline{r}$ received at each\ncycle, where the averaging is performed over an ensemble of $10^3$ independent\nagents, analogous to Fig.~\\ref{fig5}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig7a.eps}\n\\includegraphics[width=4.2cm]{fig7b.eps}\n\\caption{\\label{fig7}\n\nAverage reward $\\overline{r}$ as a function of the number of cycles for an\ninvasion game (2 symbols in 2 colors), learning rate $\\alpha$ $\\!=$ $\\!10^{-2}$,\nwith a reward of +1 (-10) for a correct (false) move, averaged over\n$10^3$ agents. The input states and POVM are given by Eqs.~(\\ref{is4}) and\n(\\ref{pom4}), respectively. The graphs correspond to 1, 2, 3, 4, 8, 16, and 32\ncontrols, as marked on the right.\n(a) Out of 4 possible moves, the defender must learn the correct one for each\nsymbol and color. After $5\\cdot10^3$ cycles, the meanings of the symbols as well\nas the colors are reversed.\n \n(b) Out of 2 possible moves, the defender must learn the correct one for each\nsymbol, whereas the color is irrelevant. For the first $5\\cdot10^3$ cycles, only\nsymbols in a single color are presented, whereas for the remaining cycles, they\nare shown randomly in both colors.\n \n}\n\\end{figure}\n\nNote that in this Sec.~\\ref{sec:ig44}, all figures refer to case (II) described\nby (\\ref{HA}) and (\\ref{HB}), where S and A now denote symbol and color,\nrespectively. To account for this [cf. the comments on Fig.~\\ref{fig5}(c)\nabove], a reward of $r$ $\\!=$ $\\!-10$ (instead of -1) is now given for a wrong\naction. The estimate of the defender's probability to block an attack is hence\nnow $(\\overline{r}+10)/11$.\n\nIn Fig.~\\ref{fig7}(a), the defender can choose between 4 moves, where for each\npercept, there is exactly one correct action [i.e., detecting $\\hat{\\Pi}_{jk}$\n($\\hat{\\Pi}_{j^\\prime{k}^\\prime}$ $\\!\\neq$ $\\!\\hat{\\Pi}_{jk}$) for\n$\\hat{\\varrho}_{jk}$ in (\\ref{pom4}) and (\\ref{is4}) returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-10$)]. After $5\\cdot10^3$\ncycles, symbol $j$ and color $k$ are read as symbol $1-j$ and color $1-k$,\nrespectively, similar to the manipulations in Fig. 5 in \\cite{Bri12}.\nIn Fig.~\\ref{fig7}(b), the defender can choose between 2 moves, where for each\nsymbol (relevant category), there is exactly one correct action, irrespective\nof its color (irrelevant category) [i.e., detecting\n$\\hat{\\Pi}_{j}$ $\\!=$ $\\!\\sum_{k=0}^{1}\\hat{\\Pi}_{jk}$\n($\\hat{\\Pi}_{j^\\prime}$ $\\!\\neq$ $\\!\\hat{\\Pi}_{j}$) for $\\hat{\\varrho}_{jk}$ in\n(\\ref{pom4}) and (\\ref{is4}) returns a reward of\n$r$ $\\!=$ $\\!+1$ ($r$ $\\!=$ $\\!-10$)]. The second color is added only after \n$5\\cdot10^3$ cycles, analogous to Fig. 6 in \\cite{Bri12}.\n[Note that the mentioned initial stagnation phase in Fig.~\\ref{fig5} is not\nvisible in Fig.~\\ref{fig7}, which is attributed to the choice of parameters\n(rewards), accelerating the initial learning.]\n\nFigs.~\\ref{fig5}(b) and \\ref{fig7} are all motivated by Figs. 5 and 6 in\n\\cite{Bri12} and confirm that the agent's adaptation to changing environments\nis recovered in our quantum control context. In addition, Figs.~\\ref{fig5}(a)\nand \\ref{fig7} show the behaviour of an underactuated memory, where the number\nof controls is insufficient for its full controllability. Since a $U(n)$-matrix\nis determined by $n^2$ real parameters, and a global phase can be disregarded\n(so that we can restrict to SU($n$)-matrices), $n^2$ $\\!-$ $\\!1$ controls are\nsufficient, i.e., 15 for our invasion game, as mentioned above.\n\nIn (\\ref{pom4}), the measurements are made in a basis rotated by a randomly\ngiven unitary $\\hat{U}_{\\mathrm{T}}$, which serves two purposes. On the one\nhand, it is required to ensure that the agent starts at the beginning of the\nfirst cycle with a policy that does not give exclusive preference to certain\nactions that follow from symmetries of the (identity-) initialised memory. This\nis a flaw of Fig.~\\ref{fig2}(a) and can be overcome by using\nFig.~\\ref{fig2}(c) instead (cf. a more detailed discussion in the grid world\nexample below). On the other hand, $\\hat{U}_{\\mathrm{T}}$ serves as a given\ntarget in our discussion Sec.~\\ref{sec3}, where we consider the agent learning\nas a navigation of its memory $\\hat{U}$, cf. also Fig.~\\ref{fig6}(b).\nFig.~\\ref{fig8} compares the case, where the agent is always fed with percept\nstates drawn from one single ONB defined via (\\ref{is4}) with the case, where\nthe percept states are drawn randomly, i.e., taking\n$\\hat{U}_{\\mathrm{R}}\\hat{\\varrho}_{jk}\\hat{U}_{\\mathrm{R}}^\\dagger$ with a\nrandom unitary $\\hat{U}_{\\mathrm{R}}$ as explained in Sec.~\\ref{sec3} instead of\n(\\ref{is4}). Note that in Fig.~\\ref{fig8}, we generate a new random\n$\\hat{U}_{\\mathrm{R}}$ at each cycle, although a fixed set of dim$\\mathcal{H}$\n(4 in our case) such $\\hat{U}_{\\mathrm{R}}$ is sufficient as mentioned in\nSec.~\\ref{sec3}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig8a.eps}\n\\includegraphics[width=4.2cm]{fig8b.eps}\n\\includegraphics[width=4.2cm]{fig8c.eps}\n\\caption{\\label{fig8}\n\n(a) Reward $r$, (b) fidelity $F$ defined in (\\ref{F}), and (c) squared distance\n$D$ defined in (\\ref{D}), where the overline denotes the ensemble average over\n$10^3$ agents for the setup as in Fig.~\\ref{fig7}(a) (i.e., 4 percepts and 4\nactions) with 16 controls but without the reversal of meaning. The initial\nmemory states are drawn from either a single or multiple orthonormal bases.\n}\n\\end{figure}\n\nFidelity $F$ and squared distance $D$ are defined in (\\ref{F}) and (\\ref{D}),\nwhere $\\hat{U}$ represents the agent memory and $\\hat{U}_{\\mathrm{T}}$ the\ntarget unitary. Each cycle's update constitutes a single navigation step in the\nunitary group [U(4) in our example]. If, for a single ONB, after a number of\ncycles, the average reward has approached unity, $\\hat{U}$ has reached a close\nneighbourhood of any unitary of the form\n$\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$,\nwhere $\\hat{U}_2^\\prime$ $\\!=$\n$\\!\\hat{U}_{\\mathrm{T}}\\hat{U}_2\\hat{U}_{\\mathrm{T}}^\\dagger$\nwith $\\hat{U}_1$ and $\\hat{U}_2$ being undetermined 4-rowed unitary matrices\ndiagonal in the common eigenbasis of the $\\hat{\\varrho}_{jk}$\n(i.e., the ``computational basis''). Fig.~\\ref{fig8}(a) shows that for a\nsolution of the invasion game, a fixed ONB is sufficient. Drawing the percept\nstates randomly, so that the set of all percept states is linearly dependent,\ndoes not affect the agent's ability to achieve perfect blocking efficiency, but\nslows down the learning process. The single ONB case allows for a larger set of\n$\\hat{U}$ $\\!=$ $\\!\\hat{U}_2^\\prime\\hat{U}_{\\mathrm{T}}\\hat{U}_1$ with respect\nto $\\hat{U}_{\\mathrm{T}}$, as becomes evident in Fig.~\\ref{fig8}(b), so that\nnavigation of $\\hat{U}$ from the identity to a member of this set takes less\ntime (as measured in cycles). The only freedom left in the case of multiple ONBs\nis a global phase of $\\hat{U}$, which remains undefined: navigation of $\\hat{U}$\ntowards $\\hat{U}_{\\mathrm{T}}$ with respect to the squared Euclidean distance\n$D$ is not required for the learning tasks discussed, as evidenced by\nFig.~\\ref{fig8}(c).\n\n\\subsubsection{\n\\label{sec:nec}\nNeverending-color scenario}\n\nIn Sec.~\\ref{sec:ig44} we considered the case, where the symbols are presented\nin two different colors, as depicted in Fig.~\\ref{fig6}(b). The original\nmotivation for introducing colors as an additional percept category was to\ndemonstrate the agent's ability to learn that they are irrelevant \\cite{Bri12}.\nIn contrast, \\cite{Mel15} present a ``neverending-color scenario'', where at\neach cycle, the respective symbol is presented in a new color. It is shown that\nwhile the basic PS-agent is in this case unable to learn at all, it becomes able\nto \\emph{generalize} (abstract) from the colors, if it is enhanced by a\nwildcard mechanism. The latter consists in adding an additional\n(wildcard ``$\\#$'') value to each percept category, and inserting between the\ninput layer of percept clips and the output layer of action clips hidden layers\nof wildcard percept clips, in which some of the percept categories attain the\nwildcard value. The creation of these wildcard clips follows predefined\ndeterministic rules, and the transitions from percept to action clips take then\nplace via the hidden layers. (The notion of layers in the general ECM clip\nnetwork Fig. 2 in \\cite{Bri12} follows from restricting to clips of length\n$L$ $\\!=$ $\\!1$).\n\nSince the use of wildcard clips is an integrated mechanism within PS (inspired\nby learning classifier systems), the question is raised  how similar ideas could\nbe implemented in our context. For a memory Fig.~\\ref{fig2}(c), we could, e.g.,\nattribute one of the levels (such as the respective ground state) of the quantum\nsystem of each percept category $\\mathrm{S}_i$ to the wildcard-level\n$|\\#\\rangle_i$, so that the percept space\n$\\mathcal{H}_{\\mathrm{S}}$ $\\!=$ $\\!\\otimes\\mathcal{H}_{\\mathrm{S}_i}$ is\nenlarged to\n$\\mathcal{H}_{\\mathrm{S}}$ $\\!=$ $\\!\\otimes(\\mathcal{H}_{\\mathrm{S}_i}$\n$\\!\\oplus$ $\\!\\mathcal{H}_{\\#_i})$, where the $\\mathcal{H}_{\\#_i}$ are\none-dimensional.\n\nInstead of this, let us simply make use of the \\emph{built-in} generalization\ncapacity of a quantum agent resulting from its coding of percepts as quantum\nstates, which is much in the sense of Sec. 8 in \\cite{bookSuttonBarto}, where\nthe percepts can be arbitrarily real-valued rather than being drawn from a\ncountable or finite value set. Consider the setup shown in Fig.~\\ref{fig6}(c),\nwhose percept system includes a symbol and a color category and refers to a\nmemory structure Fig.~\\ref{fig2}(c). To allow for infinite colors, we could\napply a color quantum system with infinite levels $\\mathcal{H}_\\infty$  (such as\nan oscillator-type system), which is initialized at each cycle in a new state\ndrawn from a fixed ONB (such as a higher number state for an oscillator-type\nsystem). While such a scheme becomes more challenging to control, because the\ncontrol vector $\\bm{h}$ has an infinite number of components [we may replace it\nwith a continuous control function $h(t)$], it still ignores the fact that\ncolors (as most percept categories in general) are not countable. With this in\nmind, we can take the notion of colors literally and, to put it simply, code\nthem in some appropriate color space such as RGB, where three parameters\ncorrespond to the red-, green-, and blue-signals of the agent's eye sensors.\nThis suggests to encode a color as a mixed state of a two-level system, which is\nalso given by three real-valued parameters (determining its location in the\nBloch ball). The generalization from two colors to all RGB-colors then\ncorresponds to the generalization from a classical to a quantum bit. In our\nsetup, it is hence sufficient to apply a two-level system for the color category\nand initialize it at each cycle in a randomly chosen \\emph{mixed} state\n$\\hat{\\varrho}_{\\mathrm{C}}$ (for neverending colors) rather than a (pure) state\nrandomly drawn from a single ONB (for two colors), whereas no changes are\nrequired on the agent's memory configuration itself. Fig.~\\ref{fig9}\ndemonstrates the learning process.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig9a.eps}\n\\includegraphics[width=4.2cm]{fig9b.eps}\n\\caption{\\label{fig9}\n(a) Reward $r$ and (b) length $|\\bm{h}|$ of the control vector as a function of\nthe number of cycles for a single agent Fig.~\\ref{fig6}(c) playing an invasion\ngame with 2 symbols, presented in a continuum of neverending colors, and 2\nmoves. A reward of +1 (-10) is given for each correct (false) move. The agent\napplies 64 controls with a learning rate of $\\alpha$ $\\!=$ $\\!10^{-2}$ in an\nalternating layer scheme App.~\\ref{app:fl} defined by two\n(Schmidt-orthonormalized) 8-rowed random Hamiltonians.\n}\n\\end{figure}\n\nSimilar to Fig.~\\ref{fig8}, random initialization slows down the learning\nprocess, so that we restrict to a single agent in Fig.~\\ref{fig9}, rather than\nan ensemble average. As illustrated in Fig.~\\ref{fig9}(a), the agent's response\nbecomes near-deterministic after about $10^6$ cycles, irrespective of the color.\nFig.~\\ref{fig9}(b) illustrates in the example of the Euclidean length of the\ncontrol vector $|\\bm{h}|$ $\\!=$ $\\!\\sqrt{\\bm{h}^\\mathrm{T}\\cdot\\bm{h}}$, that\nthe navigation, which starts at $\\bm{h}_0$ $\\!=$ $\\!\\bm{0}$, eventually comes to\nrest. While the random $\\hat{\\varrho}_{\\mathrm{C}}$ are drawn such that a\npositive probability is attributed to every volume element in the Bloch ball, we\ndid not care about drawing them with a uniform probability density, since\nmapping of an RGB-space of color (as a perceptual property) to the Bloch ball\nis not uniquely defined.\n\nThe ability to learn to distinguish between relevant\n$\\mathrm{S}_{\\mathrm{rel}}$ and an arbitrary number of irrelevant percept\ncategories $\\mathrm{S}_{\\mathrm{irr}}$ as discussed in \\cite{Mel15} is of\nparticular relevance for a quantum agent, where the irrelevant percept\ncategories can be understood as adopting the role of a bath as shown in\nFigs.~\\ref{fig2}(b) and (d). Here, a formal solution consists in a decoupled\n$\\hat{U}_{\\mathrm{SA}}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{S_{rel}A}}$ $\\!\\otimes$\n$\\!\\hat{U}_{\\mathrm{S_{irr}}}$.\n\n\\subsection{\n\\label{sec:gw}\nGrid world}\n\nIn what follows, we consider an arrangement of 8 grid cells as shown in\nFig.~\\ref{fig10}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=6cm]{fig10.eps}\n\\caption{\\label{fig10}\n\n$(3\\times3)$-grid world with an obstacle (black cell). The arrows show the\noptimal policy for the shortest path to G, and the numbers present the policy\nnumerically obtained with the agent Fig.~\\ref{fig11}(d) after $10^{4}$\nepisodes. The red numbers in parentheses are the relevant different values\nobtained after $10^{5}$ episodes, if the agent starts each episode from a random\ncell, rather than always at S.\n}\n\\end{figure}\n\nThe agent's task is to find the shortest route to a goal cell G, where at each\nstep, only moves to an adjacent cell in four directions (left, right, up, down)\nare allowed. If the agent hits a boundary of the grid or the black cell, which\nis considered an obstacle, its location remains unchanged.\n \nThis external classical navigation task constitutes a learning problem, because\nsituations/percepts (present location) must be mapped to decisions/actions\n(direction to go). The agent only perceives whether or not it has arrived at the\ngoal cell. It has no access to a ``bird's perspective'' which would allow\nimmediate exact location of the goal. It also has no access to a measure of goal\ndistance or fidelity (as in the case of the internal NO-based loop regarding its\nown quantum memory in Fig.~\\ref{fig1}), which prevents the use of external\ngradient information that could be obtained by testing the nearest neighbourhood\nof the present location. One can thus distinguish two objectives: (a) locating\nthe goal and (b) finding a shortest route to it. This task constitutes a RL type\nproblem, whose composite ``two-objective'' structure is approached by nesting\niterations. The individual action selections, i.e., choices of moves, correspond\nto the cycles in Fig.~\\ref{fig1}. Sequences of cycles form episodes, which are\nterminated only once objective (a) has been solved. Objective (b) is solved by\nsequences of episodes, which allow the agent to gradually solve objective (a)\nmore efficiently and find an optimal policy. In Fig.~\\ref{fig10}, the policy\nconsists of a set of four probabilities for each cell, with which a\ncorresponding move should be made from there. The optimal policy corresponding\nto the shortest route to G is indicated by the arrows in Fig.~\\ref{fig10}.\n\nThis grid world extends the above decision game in two aspects:\n(a) The optimal policy is in contrast to the decision game not deterministic,\nas indicated by the double arrows in the upper left and middle cell in\nFig.~\\ref{fig10}. (b) Depending on where the agent starts, more than a single\nmove is required to reach G in general, preventing the agent from obtaining an\nimmediate environmental feedback on the correctness of each individual move it\nmakes. This second aspect leads to the mentioned notion of episodes. In what\nfollows, we always place the agent at a fixed start cell S at the beginning of\neach episode, which is sufficient for learning the shortest path from S to G.\nWhile in the invasion game, episodes and cycles are synonyms, here, an episode\nis longer than a single cycle, since at least four moves are required to reach G\nfrom S.\n\nWhen designing the agent's memory structure in the sense of Fig.~\\ref{fig2},\nwe must take into account that the unitarity of the state transformation\n$\\hat{U}_{\\mathrm{S}}$ in Fig.~\\ref{fig2}(a) places restrictions on the\npercept-encodings and the action-measurements, since $\\hat{U}_{\\mathrm{S}}$\nmaps an ONB into another one.\nIf we encode in Fig.~\\ref{fig10} each cell location as a member of a given\nONB in an 8-dimensional system Hilbert space $\\mathcal{H}_8$ and perform a naive\nsymmetric $\\mathcal{H}_8$ $\\!=$ $\\!\\mathcal{H}_2$ $\\!\\oplus$\n$\\!\\mathcal{H}_2$ $\\!\\oplus$ $\\!\\mathcal{H}_2$ $\\!\\oplus$ $\\!\\mathcal{H}_2$\n-measurement for action selection, where the four 2-dimensional subspaces\ncorrespond to right, down, left and up moves, we cannot properly ascribe the\nupper left and upper middle cells, because the right and downward pointing\nactions are already ascribed to the remaining 4 white cells. One may either try\nto construct a learning algorithm that exploits the fact that the two mentioned\ncells are off the optimal path from S to G so that the agent quickly ceases to\nvisit them or construct a new POVM such as a\n$\\mathcal{H}_8$ $\\!=$ $\\!\\mathcal{H}_1$ $\\!\\oplus$ $\\!\\mathcal{H}_3$ $\\!\\oplus$\n$\\!\\mathcal{H}_3$ $\\!\\oplus$ $\\!\\mathcal{H}_1$-measurement, where two\n3-dimensional subspaces correspond to right and down, and two 1-dimensional\nsubspaces correspond to left and up moves. These possibilities require insight\ninto the specifics of this problem and are not generalizable. In addition to\nthat, Fig.~\\ref{fig2}(a) requires initialisation of the agent memory in a\nrandom unitary to ensure it starts with a policy that does not give exclusive\npreference to certain actions that follow from symmetries of the initial\n$\\hat{U}_{\\mathrm{S}}$ (such as the identity\n$\\hat{U}_{\\mathrm{S}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{S}}$). If we want to avoid\ninvoking a bath as in Fig.~\\ref{fig2}(b), we hence must resort to\nFig.~\\ref{fig2}(c), which here implies a factorisation\n$\\mathcal{H}$ $\\!=$ $\\!\\mathcal{H}_{\\mathrm{S}}$ $\\!\\otimes$\n$\\!\\mathcal{H}_{\\mathrm{A}}$ of $\\mathcal{H}$ into an 8-dimensional\n$\\mathcal{H}_{\\mathrm{S}}$ for encoding the grid cells and a 4-dimensional\n$\\mathcal{H}_{\\mathrm{A}}$ for encoding the four actions. If we encode the cells\nand actions as members of some ONB in S and A, then initialising the agent's\nmemory as identity, $\\hat{U}_{\\mathrm{SA}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{SA}}$,\nand the initial action states as in (\\ref{ia}) ensures that the agent starts at\nthe beginning of the first episode with a policy that assigns the same\nprobability to all possible actions.\n\nIn Figs.~\\ref{fig11} and \\ref{fig12} we investigate the episode length which\nis defined as the number of cycles per episode. Rather than performing an\nensemble average, we consider individual agents. These agents are described by\n(\\ref{ur3}) with a learning rate of $\\alpha$ $\\!=$ $\\!10^{-1}$, absence of\nrelaxation ($\\kappa$ $\\!=$ $\\!0$), and varying amounts of gradient glow\n($\\eta$ $\\!\\le$ $\\!1$). The number of episodes equals the number of times the\nagent is allowed to restart from S, whereas the time passed equals the sum of\nepisode lengths. The episode length can be infinite but not smaller than four,\nthe length of the shortest path from S to G.\n\nFig.~\\ref{fig11} shows evolutions of episode lengths with the number of\nepisodes, where we have set a maximum of $10^4$ episodes. As explained, each\nepisode starts at S and ends only when G has been reached.\n\n\\begin{figure}[ht]\n\\includegraphics[width=4.2cm]{fig11a.eps}\n\\includegraphics[width=4.2cm]{fig11b.eps}\n\\includegraphics[width=4.2cm]{fig11c.eps}\n\\includegraphics[width=4.2cm]{fig11d.eps}\n\\includegraphics[width=4.2cm]{fig11e.eps}\n\\includegraphics[width=4.2cm]{fig11f.eps}\n\\caption{\\label{fig11}\n\nEpisode lengths as a function of the number of episodes for the\n$(3\\times3)$-grid world as shown in Fig.~\\ref{fig10}. The plots illustrate the\neffect of gradient glow for single histories of individual agents.\n(a) The agent receives a reward $r$ $\\!=$ $\\!1$ when it has found the goal,\nwithout gradient glow ($\\eta$ $\\!=$ $\\!1$).\n(b) As in (a), but with gradient glow enabled ($\\eta$ $\\!=$ $\\!0.01$).\n(c) In addition to receiving a reward $r$ $\\!=$ $\\!1$ when it has found the\ngoal, the agent is punished with a reward $r$ $\\!=$ $\\!-10$, when it has hit a\nboundary, without gradient glow ($\\eta$ $\\!=$ $\\!1$).\n(d) As in (c), but with gradient glow enabled ($\\eta$ $\\!=$ $\\!0.7$).\n(e) As in (d), but with  gradient glow prolonged further\n($\\eta$ $\\!=$ $\\!0.5$).\n(f) Learning is disabled by always setting the reward to 0. The agent performs a\nrandom walk through the grid with average length 54.1 which is included as\ndashed line in Figs.~\\ref{fig11} and \\ref{fig12}.\n}\n\\end{figure}\n\nFig.~\\ref{fig11}(f) shows for comparison the lengths of $10^4$ random walks\nthrough the grid of an agent whose learning has been disabled by always setting\nthe reward to 0. The average number of 54.1 steps to reach G from S is \nshown in Figs.~\\ref{fig11} and \\ref{fig12} as a dashed line for comparison.\nIn Figs.~\\ref{fig11}(a-e), a positive reward of $r$ $\\!=$ $\\!1$ is given for\nhitting G. While in Fig.~\\ref{fig11}(a), the reward is always zero before G\nhas been hit, in Fig.~\\ref{fig11}(c) hitting a boundary is punished with a\nnegative reward of $r$ $\\!=$ $\\!-10$, which slightly improves the agent's\nperformance. [Note that all plots are specific to the respective learning rate\n(here $\\alpha$ $\\!=$ $\\!10^{-1}$), which has been chosen by hand to observe an\nimprovement within our $10^4$ episode-window and at the same time\nminimising the risk of oversized learning steps. While in general, the learning\nrate is gradually decreased (cf. the conditions Eq. (2.8) in\n\\cite{bookSuttonBarto} to ensure convergence), this is not strictly necessary.\nIn our numerical examples we have kept $\\alpha$ constant for simplicity.\nImplementation of a dynamic adaptation of the learning rate as was done in\n\\cite{clausen18} and \\cite{Cla15} in the present context is left for future\nwork.] The transitions Fig.~\\ref{fig11}(a$\\to$b) and\nFig.~\\ref{fig11}(c$\\to$d) show the effect of enabling gradient glow,\ni.e. $(\\eta=1)$ $\\!\\to$ $\\!(\\eta<1)$ in (\\ref{ur3}). Gradient glow provides a\nmechanism of gradual backpropagation of the policy change from the nearest\nneighbourhood of G to cells more distant from G as the number of episodes\nincreases. In Fig.~\\ref{fig11}, the agent settles in the optimal policy in\ncases (b), (d) and (e).\n\nThe policy resulting after $\\!10^{4}$ episodes in case Fig.~\\ref{fig11}(d) is\ngiven in Fig.~\\ref{fig10}, where the numbers in each cell present the\nprobability to move in the respective direction. While the agent finds the\noptimal policy for all cells forming the shortest path, it remains ignorant for\nthe remaining cells. As the agent finds and consolidates the shortest path, then\nepisode over episode, it soon visits the off-path cells less frequently, so that\nthe transition probabilities from these cells do not accumulate enough\niterations and are ``frozen'' in suboptimal values. This is characteristic of RL\nand can also be observed in learning to play games such as Backgammon\n\\cite{bookSuttonBarto}, where it is sufficient to play well only in typical\nrather than all possible constellations of the game. Since for large games, the\nformer often form a small subset of the latter, this can be seen as a strategy\nto combat with large state spaces (such as number of possible game\nconstellations). To find an optimal policy for \\emph{all} cells in\nFig.~\\ref{fig10}, we may start each episode from a random cell, analogous to\ninitialising the agent in an overcomplete basis as explained in\nFig.~\\ref{fig8}. The red numbers in parentheses shown in Fig.~\\ref{fig10}\npresent a new policy obtained after $\\!10^{5}$ episodes in this way. In contrast\nto the old policy, it is optimal or nearly optimal for all cells, with the\ndifference between 1 and the sum of these numbers quantifying the deviation\nfrom optimality for each cell $({\\scriptstyle\\stackrel{<}{=}}0.02)$. Since on\naverage, the agent starts from a given cell only in 1/7-th of all episodes, the\nlearning is slowed down, analogous to Fig.~\\ref{fig8}(a).\n\nFig.~\\ref{fig12} summarises the effect of gradient glow illustrated in\nFig.~\\ref{fig11} for the two rewarding strategies.\n\n\\begin{figure}[ht]\n\\includegraphics[width=8.6cm]{fig12.eps}\n\\caption{\\label{fig12}\n\nEpisode lengths $c$ averaged over the last 500 episodes in Fig.~\\ref{fig11},\ni.e., episodes $9.5*10^3$ $\\!-$ $\\!1*10^4$ of single histories of individual\nagents in the $(3\\times3)$-grid world as shown in Fig.~\\ref{fig10}. The data\npoints distinguish various rewarding strategies and values of gradient glow\n$\\eta$ as explained in Fig.~\\ref{fig11}. The upper and lower dashed lines\nreflect a random walk and the shortest path, respectively, and the letters\n(a)-(f) correspond to the respective plots in Fig.~\\ref{fig11}. The optimum\nvalue of $\\eta$ depends on the details of the rewarding strategy.\n}\n\\end{figure}\n\nTo limit the numerical effort, we have averaged the episode lengths over the\nlast 500 episodes in Fig.~\\ref{fig11} for individual agents as a ``rule\nof thumb''-measure of the agent's performance for the strategy chosen. For a\ndeterministic calculation we must instead average the length of each episode\n(and for each $\\eta$) over a sufficiently large ensemble of independent agents\nfor as many episodes as needed to reach convergence. Despite these shortcomings,\nthe results indicate a qualitatively similar behaviour as Fig. 4(a) in\n\\cite{Mel14}. Figs.~\\ref{fig11} and \\ref{fig12} demonstrate that gradient\nglow improves the agent performance, irrespective of whether or not it\nreceives information on false intermediate moves by means of negative rewards,\nalthough the latter reduce the required length of glow.\nIt is expected that for an ensemble average, an optimal value of $\\eta$ can be\nfound, with which the fastest convergence to the shortest path can be achieved.\nFig.~\\ref{fig11} distinguishes two qualitatively different modes of\nconvergence. If $\\eta$ is larger than optimal, a gradual improvement is\nobserved, as seen by the damping of spikes in Fig.~\\ref{fig11}(d). If $\\eta$\nis smaller than optimal, then an abrupt collapse to the optimal policy without\nvisible evidence in the preceding statistics that would provide an indication is\nobserved, cf. Fig.~\\ref{fig11}(e). If $\\eta$ is decreased further, this\ntransition is likely to happen later, to the point it will not be observed\nwithin a fixed number of episodes. This results in the steep increase in episode\nlength shown in Fig.~\\ref{fig12}, which would be absent if the ensemble\naverage was used instead. This sudden transition as shown in\nFig.~\\ref{fig11}(e) can also be observed for individual agents in \\cite{Mel14}\n(not shown there), which applies a softmax-policy function along with edge glow.\nIt is surprising that the quadratic measurement-based policy simulated here\nexhibits the same phenomenon. Note however, that convergence does not imply\noptimality. In tabular RL and PS, such an abrupt transition can be observed if\nthe $\\lambda$-parameter and hence the ``correlation length'' is too large\n(in RL) or if the $\\eta$-parameter is too small, so that the glow lasts too long\n(in PS). The policies obtained in this way are typically sub-optimal, especially\nin larger scale tasks such as bigger grid worlds, for which the agent learns\n``fast but bad'' in this case. It is hence expected that a similar behaviour can\nbe observed for our method if we increased the size of the grid.\n\n\\section{\n\\label{sec6}\nFinite difference updates}\n\nThis work's numerical experiments rely on a symbolic expression\n(\\ref{gradcomps}) for the gradient $\\bm{\\nabla}_{t}$ in (\\ref{grad}) for\nsimplicity, which is usually not available in practice, also keeping in mind the\nvariety of compositions Fig.~\\ref{fig2}, so that the agent's memory\n$\\hat{U}(\\bm{h})$ is generally unknown. As explained in the discussion of\nFig.~\\ref{fig1}, the agent may then apply a measurement-based internal loop by\nrepeatedly preparing its memory in a state that corresponds to the last percept\n$s_t$, and register whether or how often the last measurement outcome $a_t$ can\nbe recovered. This approach can be done with either infinitesimal or finite\nchanges in the control vector $\\bm{h}$, where we can distinguish between\nexpectation value- and sample-based updates, depending on how many internal\ncycles are performed between consecutive external cycles.\nIt should be stressed that the external cycles in Fig.~\\ref{fig1}\nrepresent the agent-environment interaction, resulting in sequences of\nstate-action pairs and corresponding rewards. While in an elementary optimal\ncontrol problem, a given objective is to be optimized, here the environment\nposes at each external cycle a separate and generally unpredictable control\nproblem, all of which must be addressed by the agent simultaneously.\n\nDue to the small learning rate $\\alpha$, the update rule (\\ref{ur3}) is in all\ncases local in parameter space, which reflects the assumption, that a physical\nagent cannot completely reconfigure its ``hardware'' in a single instant. While\nit is then consistent to apply a gradient $\\bm{D}_t$ $\\!=$ $\\!\\bm{\\nabla}_t$ as\na local quantity in (\\ref{ur3}), from a computational perspective, it has a few\ndrawbacks, however. One is that the direction of steepest accent at the current\ncontrol vector $\\bm{h}_t$ does not need to coincide with the direction\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$ towards the optimum\n$\\bm{h}_t^*$, as illustrated in Fig.~\\ref{fig13}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=7cm]{fig13.eps}\n\\caption{\\label{fig13}\n\nFollowing the direction $\\bm{\\nabla}p(a_t|s_t)$ of steepest ascent (dashed line)\ndoes not necessarily lead to the shortest route\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$\nfrom a given control vector $\\bm{h}_t$ at cycle $t$ to a location $\\bm{h}_t^*$,\nfor which $p(a_t|s_t)$ becomes maximum.\n}\n\\end{figure}\n\n\nAnother aspect is the vanishing of the gradient. Consider for example the\ninitialisation of the action system in a mixed state (\\ref{pcoh}) as done in\nFig.~\\ref{fig5}(b). In particular, the graph with\n$p_{\\mathrm{coh}}$ $\\!=$ $\\!0$ does not display any learning ability.\nSubstituting the corresponding\n$\\hat{\\varrho}_{\\mathrm{A}}$ $\\!=$ $\\!\\hat{I}_{\\mathrm{A}}/2$ in (\\ref{pcoh})\nand $\\hat{U}$ $\\!=$ $\\!\\hat{I}$ into (\\ref{gradcomps}), we see that the reason\nis the vanishing gradient, $\\nabla_k$ $\\!=$\n$\\!\\mathrm{Im}\\mathrm{Tr}[\\hat{\\varrho}_{\\mathrm{S}}\\hat{\\Pi}({a})\\hat{H}_k]$\n$\\!=$ $\\!0$. On the other hand, the corresponding setup Fig.~\\ref{fig6}(a)\nreveals that in this case, substituting a SWAP-gate between S and A for\n$\\hat{U}$ provides an optimal solution (along with an X-gate if the meaning of\nthe symbols is reversed) for any $\\hat{\\varrho}_{\\mathrm{A}}$, that is obviously\nnot found in Fig.~\\ref{fig5}(b). This failure occurs despite the fact that the\nagents explore, as indicated by the fluctuations in Fig.~\\ref{fig5}(b). To\nunderstand the difference, note that we may generate an $\\varepsilon$-greedy\npolicy function by replacing in (\\ref{grada}) an (arbitrarily given) state\n$\\hat{\\varrho}({s})$ with\n$\\frac{\\hat{\\varrho}({s})+\\varepsilon\\hat{I}}{1+\\varepsilon{d}}$,\nwhere $0$ $\\!<$ $\\!\\varepsilon$ $\\!\\ll$ $\\!1$ and \n$d$ $\\!=$ $\\!\\mathrm{Tr}\\hat{I}$. The term with $\\hat{I}$ then gives to\n(\\ref{grada}) a contribution $\\sim\\mathrm{Tr}_{\\mathrm{A}}\\hat{\\Pi}({a})$, that\nis independent of $s$. At the same time, it does not contribute in\n(\\ref{gradcomps}) to the gradient, $\\nabla_k$ $\\!=$ $\\!0$.\nIf $\\bm{D}_t$ $\\!=$ $\\!\\bm{\\nabla}_t$ $\\!=$ $\\!0$ for all $t$ in (\\ref{ur3}),\nthe agent's learning comes to rest, however. Finite difference and\nsample-based updates here offer a possibility to explore\n\\emph{in parameter space} the neighbourhood of the present location $\\bm{h}_t$\n(or, colloquially, the ``state'') of the agent's memory, as a consequence of\nasymmetries in the control landscape or statistical fluctuations in the samples.\n\nOf particular relevance is a final fixpoint (\\ref{task}). Intuitively, one\nwould assume that (despite the compactness of the (S)U($n$)-groups, that is in\ncontrast to the potentially unbounded values of $U$ in RL or $h$ in PS) once an\nagent has settled in a point (\\ref{task}), due to the vanishing gradient, it\nwon't be able to react quickly, if the environment suddenly changes its\nallocation of rewards (without confronting the agent with percepts it has not\nperceived before). However, the learning curves for controllable memories (16\nand 32 controls) in Fig.~\\ref{fig7}(a) demonstrate that relearning after\n$5\\cdot10^3$ cycles is not affected. A study of individual agents with 32\ncontrols in Fig.~\\ref{fig7}(a) reveals that the Euclidean length of the\nnumerical gradient rises from $10^{-14}$ at cycle 5000 to a value $>1$ in only\n15 cycles. Better understanding of this is left for future study.\nIn what follows, we outline the mentioned alternatives in some more detail.\n\n\\subsection{\nExpectation value-based updates}\n\nIf the time consumed by the internal cycles is uncritical with respect to the\nexternal cycles, the agent can obtain estimates of $p(a_t|s_t)$ from a\nsufficiently large number of internal binary measurements. With these, it can\neither approximate the components $\\nabla_kp(a_t|s_t;{h}_j)$ $\\!\\approx$\n$\\![p(a_t|s_t;{h}_j+\\delta_{jk}\\delta{h}_k)$ $\\!-$\n$\\!p(a_t|s_t;{h}_j)]/\\delta{h}_k$ of the local gradient\n$\\bm{\\nabla}_t$ $\\!=$ $\\!\\bm{\\nabla}p(a_t|s_t;\\bm{h}_t)$, which is then\nsubstituted as $\\bm{D}_t$ $\\!=\\bm{\\nabla}_t$ into (\\ref{ur3}). Alternatively, it\ncan perform a global search for the location $\\bm{h}_t^*$ of the maximum of\n$p(a_t|s_t)$. A possible algorithm for the latter is differential evolution,\nwhich relies on deterministic values $p(a_t|s_t;\\bm{h})$ rather than noisy\nsamples. Once an estimate for $\\bm{h}_t^*$ has been found, the difference\n$\\bm{D}_t$ $\\!=$ $\\!\\bm{h}_t^*$ $\\!-$ $\\!\\bm{h}_t$ is used in (\\ref{ur3}).\n\n\\subsection{\nSample-based updates}\n\nReliance on expectation values may give away potential speed gains offered by a\nquantum memory, which poses the question, whether a finite number of sample\nmeasurements is sufficient. Since individual updates in (\\ref{ur3}) are made\nwith a small fraction $\\alpha$ of the whole $\\bm{D}_t$, the assumption is that\nthe individual statistical errors in the sampled $\\bm{D}_t$ cancel out in the\nlong run.\n\nAs for the expectation value-based updates discussed above, samples can be used\nto either create \\emph{discrete} estimates\n$\\nabla_kp(a_t|s_t;{h}_j)$ $\\!\\approx$ $\\![s({h}_j+\\delta_{jk}\\delta{h}_k)$\n$\\!-$ $\\!s({h}_j)]/2$ for the components $k$ of the local gradient\n$\\bm{\\nabla}_t$ $\\!=$ $\\!\\bm{\\nabla}p(a_t|s_t;\\bm{h}_t)$, where\n$s$ $\\!=$ $\\!s(\\bm{h}_t)$ $\\!=$ $\\!\\pm1$ depending on whether the outcome of the\nbinary measurement $(a_t|s_t;\\bm{h}_t)$ is positive or not. Alternatively, for\nfinite difference updates, one may consider a neural gas\n\\cite{MaSc91,*Fritzke95agrowing} inspired approach depicted in Fig.~\\ref{fig14}.\n\n\\begin{figure}[ht]\n\\includegraphics[width=5cm]{fig14.eps}\n\\caption{\\label{fig14}\n\nInternally generated random cloud of sample controls $\\bm{h}_k$ around\na given control vector $\\bm{h}_t$ at cycle $t$ for which binary measurements\n``given $s_t$, detect $a_t$ or not'' are carried out between external cycles,\nyielding positive ($\\bm{h}_k^+$) or negative ($\\bm{h}_k^-$) outcomes.\n}\n\\end{figure}\n\nIn this approach, the differences\n\n", "index": 27, "text": "\\begin{equation}\n\\label{ng}\n  \\bm{D}_t^{(n)}=\\frac{1}{n}\\sum_{k=1}^ns_k\\bm{h}_k\n  =\\frac{n-1}{n}\\bm{D}_t^{(n-1)}+\\frac{1}{n}s_n\\bm{h}_n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\bm{D}_{t}^{(n)}=\\frac{1}{n}\\sum_{k=1}^{n}s_{k}\\bm{h}_{k}=\\frac{n-1}{n}\\bm{D}_%&#10;{t}^{(n-1)}+\\frac{1}{n}s_{n}\\bm{h}_{n}\" display=\"block\"><mrow><msubsup><mi>\ud835\udc6b</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>s</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mi>k</mi></msub></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mi>n</mi></mfrac><mo>\u2062</mo><msubsup><mi>\ud835\udc6b</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msub><mi>s</mi><mi>n</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mi>n</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere the instantaneous excitation $r_t$ $\\!=$ $\\!g_t\\lambda_t$ is the product\nof the edge's current glow value $g_t$ and the respective non-negative reward\n$\\lambda_t$ given at this time step. The glow values dampen according to\n$g_{t+1}$ $\\!=$ $\\!(1-\\eta)g_t$ with $g_0$ $\\!=$ $\\!0$, where $\\eta\\in[0,1]$ is\na glow parameter, and are reset to 1 if the edge was visited during a cycle.\n$\\gamma\\in[0,1]$ is a damping parameter towards an equilibrium value\n$h^{\\mathrm{eq}}$ in the absence of rewards (e.g., 1). Starting from some given\n$h_0$ (e.g., $h^{\\mathrm{eq}}$), this gives\n\\begin{eqnarray}\n  h_t&=&(1-\\gamma)^th_0+[1-(1-\\gamma)^t]h^{\\mathrm{eq}}\n  +\\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t-k-1}\n  \\nonumber\\\\\n  &=&h_0+\\sum_{k=0}^{t-1}r_{t-k-1}\\quad(\\gamma=0),\n\\label{hdiv}\n  \\\\\n  h_t&\\approx&h^{\\mathrm{eq}}+\\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t-k-1}\n  \\quad(\\gamma>0),\n\\label{happ}\n\\end{eqnarray}\nwhere the approximation as a backward-discounted sum (\\ref{happ}) holds for\ntimes sufficiently large so that $(1-\\gamma)^t$ $\\!\\ll$ $\\!1$ or exactly for\n$h_0$ $\\!=$ $\\!h^{\\mathrm{eq}}$. Note that due to the absence of damping,\n(\\ref{hdiv}) diverges in general if $t$ grows without limit.\n\n\\section{\n\\label{app:A2}\nUpdating the memory $\\hat{U}$}\n\nIn App.~\\ref{app:A2} we focus on a model-based (i.e., symbolic) determination\nof the gradient $\\bm{\\nabla}\\hat{U}(\\bm{h})$. The exact form of the gradient\ndepends on the parametrization. For example, if\n$\\hat{U}(\\bm{h})$ $\\!=$ $\\!\\mathrm{e}^{-\\mathrm{i}\\hat{H}(\\bm{h})}$ is given by\nsome Hermitian $\\hat{H}$, then\n\n", "itemtype": "equation", "pos": 96521, "prevtext": "\nbetween the sampled centers of positive\n[$s_k$ $\\!=$ $\\!s(\\bm{h}_k)$ $\\!=$ $\\!+1$, i.e.,\n$\\bm{h}_k$ $\\!=$ $\\!\\bm{h}_k^+$] and negative outcomes\n($s_k$ $\\!=$ $\\!s(\\bm{h}_k)$  $\\!=$ $\\!-1$, i.e.,\n$\\bm{h}_k$ $\\!=$ $\\!\\bm{h}_k^-$) of the binary measurements $(a_t|s_t;\\bm{h}_k)$\nare then applied in (\\ref{ur3}). Although one could store a current estimate\n$\\bm{D}_t^{(n)}$ for each observed state-action pair $(a_t|s_t)$ and merely\nupdate it according to (\\ref{ng}) with each new measurement point\n$s_{n+1}\\bm{h}_{n+1}$, this would give away the generalization capability\ndescribed in Sec.~\\ref{sec:nec}. One would hence need to perform $n$ internal\ncycles with the POVM-based internal loop between each external cycle.\nThe $\\bm{h}_k$ could be drawn, e.g., from a Gaussian centered around the\nrespective $\\bm{h}_t$. The variance of this Gaussian could be gradually\ndecreased with the number of external cycles to increase the locality (local\nresolution) of the cloud.\n\nFig.~\\ref{fig13} gives the misleading impression that finite difference updates\nare superior to gradient-based methods. To give an illustrative counterexample,\none could think of a two-dimensional $\\bm{h}$ $\\!=$ $\\!(h_x,h_y)$ and a control\nlandscape $p(\\bm{h})$ modelled by the monotonically increasing height $p(l)$\nalong the length $l$ of a tape of paper bent into a spiral and placed onto the\ndashed line in Fig.~\\ref{fig13}, such that one end with $p(0)$ $\\!=$ $\\!0$ is\nlocated at $\\bm{h}_t$ and the other one at $\\bm{h}_t^*$. Here, a gradient-based\nmethod would safely follow the long path on the tape's upper edge, whereas a\nfinite difference method would trade a potential speedup with the risk of\nmissing the paper at all trials. Since a comparison of state of the art optimal\ncontrol methods based on noisy samples for the agent's learning would go beyond\nthe scope of this work, we here restrict ourselves to these sketchy lines of\nthought, whose numerical study is pending, and leave open the question of what\nthe best method is for a given task.\n\nA characteristic shared by the loops of Fig.~\\ref{fig1} and\noptimal control setups is the need of an experimental ``mastermind'' who\ncontrols the controls. An agent which is supposed to act autonomously would be\nrequired to accomplish this by itself, ideally in a ``natural'' or ``organic\ncomputing'' sense. An elementary example from everyday life are ``desire paths''\nwhich form or dissipate, depending on their usage and without a designated\nplanner.\n\n\\section{\n\\label{sec5}\nSummary and outlook}\n\nIn summary, we have adopted an update rule from the basic PS scheme, equipped it\nwith gradient glow, and applied it to small-scale invasion game and grid\nworld tasks. The numerical results show that similar results can be obtained for\na quantum agent, as long as the memory is not underactuated. This is not\nobvious, because of the fundamental difference in the number of free parameters.\nIf $S$ and $A$ denote the number of possible percepts and actions, respectively,\nthen in classical tabular action value RL-methods, the estimated values of all\npercept-action pairs are combined to a $({S}\\times{A})$-matrix, i.e., we have\n$(SA)$ real parameters. If we encoded in our scheme each percept and action\ncategory by a separate subsystem, whose dimensionalities correspond to the\nnumber of values, the respective category can adopt, then $\\hat{U}$ is an at\nleast $U(N=SA)$-matrix for which we are faced with $(SA)^2$ real parameters.\nNote that this work is unrelated to the reflecting PS agents, which are\ndiscussed in \\cite{Pap14}. While the scheme \\cite{Pap14} allows a proof of\nquantum speedup, our approach complements the latter in that it is simple,\nflexible in its construction, and does not involve specific analytic quantum\ngates. The learning of a good policy only for percepts which are ``typical'' and\nhave thus been encountered sufficiently often in the past shares features with\n``soft computing'', where it is sufficient to find a good rather than an exact\nsolution, which would here consist in a policy that is optimal for all possible\npercepts. One may think of, e.g., simplifying a symbolic mathematical\nexpression: while all transformation steps themselves must be exact, there are\nno strict rules, as far as the best way of its formulation is concerned. In\nfuture work, it may be worth to incorporate recent extensions of the classical\nPS scheme such as generalization \\cite{Mel15}.\n\n\\appendix\n\n\\section{\n\\label{app:A1}\nClassical PS update rule}\n\nClassical PS in its general form is based on a discrete network of clips that\nform its ECM. In the examples considered in \\cite{Bri12,Mau15,Mel14}, after each\ndiscrete time step (external cycle), a local edge is updated according to\n\n", "index": 29, "text": "\\begin{equation}\n  h_{t+1}=h_t-\\gamma(h_t-h^{\\mathrm{eq}})+r_t,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"h_{t+1}=h_{t}-\\gamma(h_{t}-h^{\\mathrm{eq}})+r_{t},\" display=\"block\"><mrow><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>-</mo><msup><mi>h</mi><mi>eq</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mi>r</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nFor small $\\hat{H}$, we can expand the exponentials in $\\hat{H}$ to\nlowest order, and the approximation\n$\\bm{\\nabla}\\hat{U}$ $\\!\\approx$ $\\!-\\mathrm{i}\\bm{\\nabla}\\hat{H}$ holds.\n\nIn case of a continuous time dependence, the vector $\\bm{h}$ can be replaced by\na function $h(t)$, with which a unitary propagator from time $t_1$ to time $t_3$\nis given as a positively time ordered integral\n\n", "itemtype": "equation", "pos": 98152, "prevtext": "\nwhere the instantaneous excitation $r_t$ $\\!=$ $\\!g_t\\lambda_t$ is the product\nof the edge's current glow value $g_t$ and the respective non-negative reward\n$\\lambda_t$ given at this time step. The glow values dampen according to\n$g_{t+1}$ $\\!=$ $\\!(1-\\eta)g_t$ with $g_0$ $\\!=$ $\\!0$, where $\\eta\\in[0,1]$ is\na glow parameter, and are reset to 1 if the edge was visited during a cycle.\n$\\gamma\\in[0,1]$ is a damping parameter towards an equilibrium value\n$h^{\\mathrm{eq}}$ in the absence of rewards (e.g., 1). Starting from some given\n$h_0$ (e.g., $h^{\\mathrm{eq}}$), this gives\n\\begin{eqnarray}\n  h_t&=&(1-\\gamma)^th_0+[1-(1-\\gamma)^t]h^{\\mathrm{eq}}\n  +\\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t-k-1}\n  \\nonumber\\\\\n  &=&h_0+\\sum_{k=0}^{t-1}r_{t-k-1}\\quad(\\gamma=0),\n\\label{hdiv}\n  \\\\\n  h_t&\\approx&h^{\\mathrm{eq}}+\\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t-k-1}\n  \\quad(\\gamma>0),\n\\label{happ}\n\\end{eqnarray}\nwhere the approximation as a backward-discounted sum (\\ref{happ}) holds for\ntimes sufficiently large so that $(1-\\gamma)^t$ $\\!\\ll$ $\\!1$ or exactly for\n$h_0$ $\\!=$ $\\!h^{\\mathrm{eq}}$. Note that due to the absence of damping,\n(\\ref{hdiv}) diverges in general if $t$ grows without limit.\n\n\\section{\n\\label{app:A2}\nUpdating the memory $\\hat{U}$}\n\nIn App.~\\ref{app:A2} we focus on a model-based (i.e., symbolic) determination\nof the gradient $\\bm{\\nabla}\\hat{U}(\\bm{h})$. The exact form of the gradient\ndepends on the parametrization. For example, if\n$\\hat{U}(\\bm{h})$ $\\!=$ $\\!\\mathrm{e}^{-\\mathrm{i}\\hat{H}(\\bm{h})}$ is given by\nsome Hermitian $\\hat{H}$, then\n\n", "index": 31, "text": "\\begin{equation}\n\\label{x1}\n  \\bm{\\nabla}\\hat{U}=\\int_0^1\n  \\mathrm{e}^{-\\mathrm{i}x\\hat{H}}(-\\mathrm{i}\\bm{\\nabla}\\hat{H})\n  \\mathrm{e}^{-\\mathrm{i}(1-x)\\hat{H}}\\mathrm{d}x.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\bm{\\nabla}\\hat{U}=\\int_{0}^{1}\\mathrm{e}^{-\\mathrm{i}x\\hat{H}}(-\\mathrm{i}\\bm%&#10;{\\nabla}\\hat{H})\\mathrm{e}^{-\\mathrm{i}(1-x)\\hat{H}}\\mathrm{d}x.\" display=\"block\"><mrow><mrow><mrow><mo mathvariant=\"bold\">\u2207</mo><mo>\u2061</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup><mrow><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mrow><mo mathvariant=\"bold\">\u2207</mo><mo>\u2061</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>x</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 98729, "prevtext": "\nFor small $\\hat{H}$, we can expand the exponentials in $\\hat{H}$ to\nlowest order, and the approximation\n$\\bm{\\nabla}\\hat{U}$ $\\!\\approx$ $\\!-\\mathrm{i}\\bm{\\nabla}\\hat{H}$ holds.\n\nIn case of a continuous time dependence, the vector $\\bm{h}$ can be replaced by\na function $h(t)$, with which a unitary propagator from time $t_1$ to time $t_3$\nis given as a positively time ordered integral\n\n", "index": 33, "text": "\\begin{equation}\n\\label{x2}\n  \\hat{U}(t_3,t_1)=\\mathrm{T}\\mathrm{e}^{-\\mathrm{i}\n  \\int_{t_1}^{t_3}\\mathrm{d}t_2h(t_2)\\hat{H}(t_2)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\hat{U}(t_{3},t_{1})=\\mathrm{T}\\mathrm{e}^{-\\mathrm{i}\\int_{t_{1}}^{t_{3}}%&#10;\\mathrm{d}t_{2}h(t_{2})\\hat{H}(t_{2})},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>3</mn></msub><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>Te</mi><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"false\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mn>1</mn></msub><msub><mi>t</mi><mn>3</mn></msub></msubsup></mstyle><mrow><mrow><mo>d</mo><msub><mi>t</mi><mn>2</mn></msub></mrow><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nIf $\\hat{H}(t)$ $\\!=$ $\\!\\sum_k\\tilde{h}_k(t)\\hat{H}_k$ is expanded in terms of\nsome fixed Hamiltonians $\\hat{H}_k$, then with $h_k$ $\\!=$ $\\!h\\tilde{h}_k$,\n(\\ref{x2}) becomes\n$\\hat{U}(t_3,t_1)$ $\\!=$ $\\!\\mathrm{T}\\mathrm{e}^{-\\mathrm{i}\n\\int_{t_1}^{t_3}\\mathrm{d}t_2\\sum_kh_k(t_2)\\hat{H}_k}$, and (\\ref{x3}) is\nreplaced with $\\frac{\\delta\\hat{U}(t,0)}{\\delta{h_k}}(t_1)$ $\\!=$\n$\\!-\\mathrm{i}\\hat{U}(t,t_1)\\hat{H}_k\\hat{U}(t_1,0)$.\nNavigation on unitary groups becomes discretized if only a restricted (finite)\nset of Hamiltonians $\\hat{H}_k$ can be implemented at a time rather than an\nanalytically time-dependent Hamiltonian $\\hat{H}(t)$, so that only one of the\n$\\tilde{h}_k$ is non-zero for a given $t$.\nA known example is the alternating application of two fixed Hamiltonians\n\\cite{lloyd2}, $\\hat{H}_{2k}$ $\\!=$ $\\!\\hat{H}^{(2)}$ and\n$\\hat{H}_{2k+1}$ $\\!=$\n$\\!\\hat{H}^{(1)}$ $(k=0,1,2,\\ldots,\\lfloor\\frac{n}{2}\\rfloor)$, for a set of\ntimes to be determined from the target unitary \\cite{akulin2}.\nIn this discrete case as defined by a piecewise constant normalized $\\hat{H}$\nin (\\ref{x2}), the function $h(t)$ can be replaced with a vector $\\bm{h}$, and\nthe functional derivatives with respect to $h(t)$ reduce to gradients with\nrespect to $\\bm{h}$.\n\n\\subsection{\n\\label{app:lxl}\nAdding layer after layer}\n\nWe can update the present unitary $\\hat{U}$ by multiplying it from the left with\na new layer $\\hat{U}(\\delta\\bm{h})$ after each cycle,\n\n", "itemtype": "equation", "pos": 98881, "prevtext": "\nand\n\n", "index": 35, "text": "\\begin{equation}\n\\label{x3}\n  \\frac{\\delta\\hat{U}(t,0)}{\\delta{h}}(t_1)\n  =-\\mathrm{i}\\hat{U}(t,t_1)\\hat{H}(t_1)\\hat{U}(t_1,0).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\delta\\hat{U}(t,0)}{\\delta{h}}(t_{1})=-\\mathrm{i}\\hat{U}(t,t_{1})\\hat{H}%&#10;(t_{1})\\hat{U}(t_{1},0).\" display=\"block\"><mrow><mrow><mrow><mfrac><mrow><mi>\u03b4</mi><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>h</mi></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nIf $\\hat{U}(\\delta\\bm{h})$ $\\!=$\n$\\!\\mathrm{e}^{-\\mathrm{i}\\sum_k\\delta{h}_k\\hat{H}_k}$ is a small modification\nclose to the identity, then the mentioned approximation of (\\ref{x1}) gives\n$(\\bm{\\nabla}\\hat{U})_k$ $\\!\\approx$ $\\!-\\mathrm{i}\\hat{H}_k\\hat{U}$. This\nis of advantage if the agent or its simulation is able to store only the present\n$\\hat{U}$ and not the history of updates (layers). The components of\n(\\ref{grad}) then become\n\n", "itemtype": "equation", "pos": 100471, "prevtext": "\nIf $\\hat{H}(t)$ $\\!=$ $\\!\\sum_k\\tilde{h}_k(t)\\hat{H}_k$ is expanded in terms of\nsome fixed Hamiltonians $\\hat{H}_k$, then with $h_k$ $\\!=$ $\\!h\\tilde{h}_k$,\n(\\ref{x2}) becomes\n$\\hat{U}(t_3,t_1)$ $\\!=$ $\\!\\mathrm{T}\\mathrm{e}^{-\\mathrm{i}\n\\int_{t_1}^{t_3}\\mathrm{d}t_2\\sum_kh_k(t_2)\\hat{H}_k}$, and (\\ref{x3}) is\nreplaced with $\\frac{\\delta\\hat{U}(t,0)}{\\delta{h_k}}(t_1)$ $\\!=$\n$\\!-\\mathrm{i}\\hat{U}(t,t_1)\\hat{H}_k\\hat{U}(t_1,0)$.\nNavigation on unitary groups becomes discretized if only a restricted (finite)\nset of Hamiltonians $\\hat{H}_k$ can be implemented at a time rather than an\nanalytically time-dependent Hamiltonian $\\hat{H}(t)$, so that only one of the\n$\\tilde{h}_k$ is non-zero for a given $t$.\nA known example is the alternating application of two fixed Hamiltonians\n\\cite{lloyd2}, $\\hat{H}_{2k}$ $\\!=$ $\\!\\hat{H}^{(2)}$ and\n$\\hat{H}_{2k+1}$ $\\!=$\n$\\!\\hat{H}^{(1)}$ $(k=0,1,2,\\ldots,\\lfloor\\frac{n}{2}\\rfloor)$, for a set of\ntimes to be determined from the target unitary \\cite{akulin2}.\nIn this discrete case as defined by a piecewise constant normalized $\\hat{H}$\nin (\\ref{x2}), the function $h(t)$ can be replaced with a vector $\\bm{h}$, and\nthe functional derivatives with respect to $h(t)$ reduce to gradients with\nrespect to $\\bm{h}$.\n\n\\subsection{\n\\label{app:lxl}\nAdding layer after layer}\n\nWe can update the present unitary $\\hat{U}$ by multiplying it from the left with\na new layer $\\hat{U}(\\delta\\bm{h})$ after each cycle,\n\n", "index": 37, "text": "\\begin{equation}\n\\hat{U}\\leftarrow\\hat{U}(\\delta\\bm{h})\\hat{U}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\hat{U}\\leftarrow\\hat{U}(\\delta\\bm{h})\\hat{U}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2190</mo><mrow><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>\ud835\udc89</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\n\n\\subsection{\n\\label{app:fl}\nFixed number of layers}\n\nIn our numerical examples we consider a discretized memory model, for which\n(\\ref{x2}) reduces to a product of $n$ unitaries\n\n", "itemtype": "equation", "pos": 100988, "prevtext": "\nIf $\\hat{U}(\\delta\\bm{h})$ $\\!=$\n$\\!\\mathrm{e}^{-\\mathrm{i}\\sum_k\\delta{h}_k\\hat{H}_k}$ is a small modification\nclose to the identity, then the mentioned approximation of (\\ref{x1}) gives\n$(\\bm{\\nabla}\\hat{U})_k$ $\\!\\approx$ $\\!-\\mathrm{i}\\hat{H}_k\\hat{U}$. This\nis of advantage if the agent or its simulation is able to store only the present\n$\\hat{U}$ and not the history of updates (layers). The components of\n(\\ref{grad}) then become\n\n", "index": 39, "text": "\\begin{equation}\n  \\frac{\\partial{p}({a}|{s})}{\\partial{h}_k}\n  =2\\mathrm{Im}\n  \\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\hat{H}_k\\hat{U}\\bigr\\rangle.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{p}({a}|{s})}{\\partial{h}_{k}}=2\\mathrm{Im}\\bigl{\\langle}\\hat{U}%&#10;^{\\dagger}\\hat{\\Pi}({a})\\hat{H}_{k}\\hat{U}\\bigr{\\rangle}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">|</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>h</mi><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">I</mi><mo>\u2062</mo><mi mathvariant=\"normal\">m</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e8</mo><mrow><msup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2020</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a0</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e9</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhich simplifies the determination of the gradient, since\n$(\\bm{\\nabla}\\hat{U})_k$ $\\!=$ $\\!-\\mathrm{i}\\hat{U}\\hat{H}_k(t_k)$, where\n$\\hat{H}_k(t_k)$ $\\!=$\n$\\!(\\hat{U}_k\\cdots\\hat{U}_1)^\\dagger\\hat{H}_k(\\hat{U}_k\\cdots\\hat{U}_1)$, so\nthat the components of (\\ref{grad}) now become\n\n", "itemtype": "equation", "pos": 101333, "prevtext": "\n\n\\subsection{\n\\label{app:fl}\nFixed number of layers}\n\nIn our numerical examples we consider a discretized memory model, for which\n(\\ref{x2}) reduces to a product of $n$ unitaries\n\n", "index": 41, "text": "\\begin{equation}\n  \\hat{U}=\\hat{U}_n\\cdots\\hat{U}_2\\hat{U}_1,\\quad\n  \\hat{U}_k=\\mathrm{e}^{-\\mathrm{i}{h}_k\\hat{H}_k},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\hat{U}=\\hat{U}_{n}\\cdots\\hat{U}_{2}\\hat{U}_{1},\\quad\\hat{U}_{k}=\\mathrm{e}^{-%&#10;\\mathrm{i}{h}_{k}\\hat{H}_{k}},\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi>n</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mn>2</mn></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mn>1</mn></msub></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo>=</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub></mrow></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nIn this work, we use alternating layers defined by two fixed Hamiltonians\n$\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$ as mentioned at the beginning of this\nsection.\n\n\\section{Numerical and measurement-based objectives}\n\n\n\\subsection{Distance and uniformly averaged fidelity}\n\nConsider an $n$-level system and two unitary operators $\\hat{U}$ and\n$\\hat{U}_{\\mathrm{T}}$, where $\\hat{U}$ $\\!=$ $\\!\\hat{U}({h})$ depends on an\n(unrestricted) external control field $h(t)$, and $\\!\\hat{U}_{\\mathrm{T}}$ is a\ndesired target. Their (squared) Euclidean distance as induced by the\nHilbert-Schmidt dot product is given by\n\\begin{eqnarray}\n  D&\\equiv&\\|\\hat{U}-\\hat{U}_{\\mathrm{T}}\\|^2\n  =\\mathrm{Tr}\\bigl[(\\hat{U}-\\hat{U}_{\\mathrm{T}})^\\dagger\n  (\\hat{U}-\\hat{U}_{\\mathrm{T}})\\bigr]\n  \\nonumber\\\\\n  &=&2n-2\\mathrm{Re}\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U})\n  \\in[0,4n].\n\\label{D}\n\\end{eqnarray}\nIf $\\hat{U}({h})$ is controllable in the sense that at least one $h(t)$ exists\nsuch that $\\hat{U}({h})$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}$, then (\\ref{D}) has the\nset $\\{0,4,\\ldots,4n\\}$ as possible extremal values\n(i.e, $\\frac{\\delta{D}}{\\delta{h}}$ $\\!=$ $\\!0$), where the values $0$ and $4n$\nare attained for $\\hat{U}$ $\\!=$ $\\!\\pm\\hat{U}_{\\mathrm{T}}$, while the\nremaining extrema are saddle points \\cite{Rab05}. A measure insensitive to\nglobal phases\n$\\hat{U}$ $\\!=$ $\\!\\mathrm{e}^{\\mathrm{i}\\varphi}\\hat{U}_{\\mathrm{T}}$ is the\naverage fidelity defined by\n\n", "itemtype": "equation", "pos": 101748, "prevtext": "\nwhich simplifies the determination of the gradient, since\n$(\\bm{\\nabla}\\hat{U})_k$ $\\!=$ $\\!-\\mathrm{i}\\hat{U}\\hat{H}_k(t_k)$, where\n$\\hat{H}_k(t_k)$ $\\!=$\n$\\!(\\hat{U}_k\\cdots\\hat{U}_1)^\\dagger\\hat{H}_k(\\hat{U}_k\\cdots\\hat{U}_1)$, so\nthat the components of (\\ref{grad}) now become\n\n", "index": 43, "text": "\\begin{equation}\n\\label{gradcomps}\n  \\frac{\\partial{p}({a}|{s})}{\\partial{h}_k}\n  =2\\mathrm{Im}\n  \\bigl\\langle\\hat{U}^\\dagger\\hat{\\Pi}({a})\\hat{U}\\hat{H}_k(t_k)\n  \\bigr\\rangle.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{p}({a}|{s})}{\\partial{h}_{k}}=2\\mathrm{Im}\\bigl{\\langle}\\hat{U}%&#10;^{\\dagger}\\hat{\\Pi}({a})\\hat{U}\\hat{H}_{k}(t_{k})\\bigr{\\rangle}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">|</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>h</mi><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">I</mi><mo>\u2062</mo><mi mathvariant=\"normal\">m</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e8</mo><mrow><msup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2020</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a0</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><msub><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e9</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere the overline denotes uniform average over all $|\\Psi\\rangle$\n\\cite{Ped07,dan05}. Note that $F$ $\\!\\in$ $\\![(n+1)^{-1},1]$ for\n$n$ $\\!>$ $\\!1$, and $F$ $\\!=$ $\\!1$ for $n$ $\\!=$ $\\!1$.\nBoth (\\ref{D}) and (\\ref{F}) are determined by the complex\n\n", "itemtype": "equation", "pos": 103383, "prevtext": "\nIn this work, we use alternating layers defined by two fixed Hamiltonians\n$\\hat{H}^{(1)}$ and $\\hat{H}^{(2)}$ as mentioned at the beginning of this\nsection.\n\n\\section{Numerical and measurement-based objectives}\n\n\n\\subsection{Distance and uniformly averaged fidelity}\n\nConsider an $n$-level system and two unitary operators $\\hat{U}$ and\n$\\hat{U}_{\\mathrm{T}}$, where $\\hat{U}$ $\\!=$ $\\!\\hat{U}({h})$ depends on an\n(unrestricted) external control field $h(t)$, and $\\!\\hat{U}_{\\mathrm{T}}$ is a\ndesired target. Their (squared) Euclidean distance as induced by the\nHilbert-Schmidt dot product is given by\n\\begin{eqnarray}\n  D&\\equiv&\\|\\hat{U}-\\hat{U}_{\\mathrm{T}}\\|^2\n  =\\mathrm{Tr}\\bigl[(\\hat{U}-\\hat{U}_{\\mathrm{T}})^\\dagger\n  (\\hat{U}-\\hat{U}_{\\mathrm{T}})\\bigr]\n  \\nonumber\\\\\n  &=&2n-2\\mathrm{Re}\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U})\n  \\in[0,4n].\n\\label{D}\n\\end{eqnarray}\nIf $\\hat{U}({h})$ is controllable in the sense that at least one $h(t)$ exists\nsuch that $\\hat{U}({h})$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}$, then (\\ref{D}) has the\nset $\\{0,4,\\ldots,4n\\}$ as possible extremal values\n(i.e, $\\frac{\\delta{D}}{\\delta{h}}$ $\\!=$ $\\!0$), where the values $0$ and $4n$\nare attained for $\\hat{U}$ $\\!=$ $\\!\\pm\\hat{U}_{\\mathrm{T}}$, while the\nremaining extrema are saddle points \\cite{Rab05}. A measure insensitive to\nglobal phases\n$\\hat{U}$ $\\!=$ $\\!\\mathrm{e}^{\\mathrm{i}\\varphi}\\hat{U}_{\\mathrm{T}}$ is the\naverage fidelity defined by\n\n", "index": 45, "text": "\\begin{equation}\n\\label{F}\n  F\\equiv\n  \\overline{|\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}|\\Psi\\rangle|^2}\n  =\\frac{n+|\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U})|^2}{n(n+1)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"F\\equiv\\overline{|\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{U}|\\Psi%&#10;\\rangle|^{2}}=\\frac{n+|\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{U})|^{2}%&#10;}{n(n+1)},\" display=\"block\"><mrow><mrow><mi>F</mi><mo>\u2261</mo><mover accent=\"true\"><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi mathvariant=\"normal\">\u03a8</mi><mo fence=\"true\" stretchy=\"false\">|</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo fence=\"true\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><mo>\u00af</mo></mover><mo>=</mo><mfrac><mrow><mi>n</mi><mo>+</mo><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhich is confined to the complex unit circle and whose expectation\n$\\bigl\\langle|\\cos\\sphericalangle(\\hat{U},\\hat{U}_{\\mathrm{T}})|^2\\bigr\\rangle$\n$\\!=$ $\\!n^{-1}$ for uniform random $\\hat{U}$ drops to zero with growing $n$.\n\nIf in (\\ref{F}), the averaging is restricted to a $d$-dimensional subspace P,\nwe must replace (\\ref{F}) with\n\n", "itemtype": "equation", "pos": 103835, "prevtext": "\nwhere the overline denotes uniform average over all $|\\Psi\\rangle$\n\\cite{Ped07,dan05}. Note that $F$ $\\!\\in$ $\\![(n+1)^{-1},1]$ for\n$n$ $\\!>$ $\\!1$, and $F$ $\\!=$ $\\!1$ for $n$ $\\!=$ $\\!1$.\nBoth (\\ref{D}) and (\\ref{F}) are determined by the complex\n\n", "index": 47, "text": "\\begin{equation}\n  \\cos\\sphericalangle\\bigl(\\hat{U},\\hat{U}_{\\mathrm{T}}\\bigr)\n  \\equiv\\frac{\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U})}\n  {\\sqrt{\\mathrm{Tr}(\\hat{U}^\\dagger\\hat{U})}\n  \\sqrt{\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}_{\\mathrm{T}})}}\n  =\\frac{\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U})}{n},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\cos\\sphericalangle\\bigl{(}\\hat{U},\\hat{U}_{\\mathrm{T}}\\bigr{)}\\equiv\\frac{%&#10;\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{U})}{\\sqrt{\\mathrm{Tr}(\\hat{U}^%&#10;{\\dagger}\\hat{U})}\\sqrt{\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{U}_{%&#10;\\mathrm{T}})}}=\\frac{\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{U})}{n},\" display=\"block\"><mrow><mrow><mrow><mrow><mi>cos</mi><mo>\u2061</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\sphericalangle</mtext></merror></mrow><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>\u2261</mo><mfrac><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msqrt><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2020</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt><mo>\u2062</mo><msqrt><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mi>n</mi></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere $\\hat{M}$ $\\!=$ $\\!\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}\\hat{P}$,\nwith $\\hat{P}$ being the projector onto P. Note that\n$F_{\\mathrm{P}}$ $\\!\\in$ $\\![\\frac{\\max(0,2d-n)}{d(d+1)},1]$ for\n$n$ $\\!>$ $\\!1$ [since $\\mathrm{Tr}(\\hat{M}^\\dagger\\hat{M})$ $\\!=$\n$\\!\\mathrm{Tr}(\\hat{P}_{\\mathrm{T}}\\hat{P}_{\\mathrm{U}})$ $\\!\\in$\n$\\!\\max(0,2d-n)$ with $\\hat{P}_{\\mathrm{T}}$ $\\!=$\n$\\!\\hat{U}_{\\mathrm{T}}\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger$,\n$\\hat{P}_{\\mathrm{U}}$ $\\!=$ $\\!\\hat{U}\\hat{P}\\hat{U}^\\dagger$], \nand $F_{\\mathrm{P}}$ $\\!=$ $\\!1$ for $n$ $\\!=$ $\\!1$.\nWhile for a one-dimensional $\\hat{P}$ $\\!=$ $\\!|\\Psi\\rangle\\langle\\Psi|$,\n(\\ref{FP}) reduces to $F_\\Psi$ $\\!=$\n$\\!|\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}|\\Psi\\rangle|^2$, the other\nlimit $d$ $\\!=$ $\\!n$ recovers (\\ref{F}).\n\nIf in (\\ref{FP}), $\\hat{U}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{SB}}$ couples the quantum\nsystem S to a bath B, then we define a projector\n$\\hat{\\Pi}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}\\hat{P}|\\Psi\\rangle\\langle\\Psi|\n\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger\\otimes\\hat{I}_{\\mathrm{B}}$ and generalize\n(\\ref{FP}) to\n\\begin{eqnarray}\n\\label{FPB}\n  F_{\\mathrm{P}}\n  &\\equiv&\\overline{\\mathrm{Tr}_{\\mathrm{SB}}\\bigl[\\hat{U}\\hat{P}|\\Psi\\rangle\n  \\hat{\\varrho}_{\\mathrm{B}}\n  \\langle\\Psi|\\hat{P}\\hat{U}^\\dagger\\hat{\\Pi}\\bigr]}^{(\\mathrm{P})}\n  \\\\\n  &=&\\left\\langle\n  \\frac{\\mathrm{Tr}_{\\mathrm{S}}(\\hat{M}^\\dagger\\hat{M})\n  +(\\mathrm{Tr}_{\\mathrm{S}}\\hat{M})^\\dagger(\\mathrm{Tr}_{\\mathrm{S}}\\hat{M})\n  }{d(d+1)}\\right\\rangle_{\\mathrm{B}},\n\\end{eqnarray}\nwhere $\\langle\\cdots\\rangle_{\\mathrm{B}}$ $\\!\\equiv$\n$\\!\\mathrm{Tr}_{\\mathrm{B}}[\\hat{\\varrho}_{\\mathrm{B}}(\\cdots)]$ with a fixed\nbath state $\\hat{\\varrho}_{\\mathrm{B}}$.\n\nReplacing $\\hat{U}|\\Psi\\rangle\\langle\\Psi|\\hat{U}^\\dagger$ in (\\ref{F}) with the\noutput $\\mathcal{M}(|\\Psi\\rangle\\langle\\Psi|)$ of a quantum channel generalizes\n(\\ref{F}) to \\cite{Ped07}\n\n", "itemtype": "equation", "pos": 104509, "prevtext": "\nwhich is confined to the complex unit circle and whose expectation\n$\\bigl\\langle|\\cos\\sphericalangle(\\hat{U},\\hat{U}_{\\mathrm{T}})|^2\\bigr\\rangle$\n$\\!=$ $\\!n^{-1}$ for uniform random $\\hat{U}$ drops to zero with growing $n$.\n\nIf in (\\ref{F}), the averaging is restricted to a $d$-dimensional subspace P,\nwe must replace (\\ref{F}) with\n\n", "index": 49, "text": "\\begin{equation}\n\\label{FP}\n  F_{\\mathrm{P}}\n  \\equiv\\overline{|\\langle\\Psi|\\hat{M}|\\Psi\\rangle|^2}^{(\\mathrm{P})}\n  =\\frac{\\mathrm{Tr}(\\hat{M}^\\dagger\\hat{M})+|\\mathrm{Tr}(\\hat{M})|^2}{d(d+1)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"F_{\\mathrm{P}}\\equiv\\overline{|\\langle\\Psi|\\hat{M}|\\Psi\\rangle|^{2}}^{(\\mathrm%&#10;{P})}=\\frac{\\mathrm{Tr}(\\hat{M}^{\\dagger}\\hat{M})+|\\mathrm{Tr}(\\hat{M})|^{2}}{%&#10;d(d+1)},\" display=\"block\"><mrow><mrow><msub><mi>F</mi><mi mathvariant=\"normal\">P</mi></msub><mo>\u2261</mo><msup><mover accent=\"true\"><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi mathvariant=\"normal\">\u03a8</mi><mo fence=\"true\" stretchy=\"false\">|</mo><mover accent=\"true\"><mi>M</mi><mo stretchy=\"false\">^</mo></mover><mo fence=\"true\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><mo>\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">P</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mfrac><mrow><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mover accent=\"true\"><mi>M</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2020</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>M</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>M</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere $\\hat{G}_k$ are the Kraus operators of the decomposition of the channel\nmap $\\mathcal{M}(\\hat{\\varrho})$ $\\!=$\n$\\!\\sum_k\\hat{G}_k\\hat{\\varrho}\\hat{G}_k^\\dagger$. Note that a change\n$\\hat{G}_k^\\prime$ $\\!=$ $\\!\\sum_jV_{kj}\\hat{G}_j$ of the Kraus operators as\ndescribed by a unitary matrix $V$ leaves (\\ref{F1}) invariant.\n\n\\subsection{Percept statistics-based fidelity}\n\nThe uniform average in (\\ref{F1}) can be generalized to an arbitrary\ndistribution ${p}(|\\Psi_k\\rangle)$ of possible input states $|\\Psi_k\\rangle$,\n\n", "itemtype": "equation", "pos": 106600, "prevtext": "\nwhere $\\hat{M}$ $\\!=$ $\\!\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}\\hat{P}$,\nwith $\\hat{P}$ being the projector onto P. Note that\n$F_{\\mathrm{P}}$ $\\!\\in$ $\\![\\frac{\\max(0,2d-n)}{d(d+1)},1]$ for\n$n$ $\\!>$ $\\!1$ [since $\\mathrm{Tr}(\\hat{M}^\\dagger\\hat{M})$ $\\!=$\n$\\!\\mathrm{Tr}(\\hat{P}_{\\mathrm{T}}\\hat{P}_{\\mathrm{U}})$ $\\!\\in$\n$\\!\\max(0,2d-n)$ with $\\hat{P}_{\\mathrm{T}}$ $\\!=$\n$\\!\\hat{U}_{\\mathrm{T}}\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger$,\n$\\hat{P}_{\\mathrm{U}}$ $\\!=$ $\\!\\hat{U}\\hat{P}\\hat{U}^\\dagger$], \nand $F_{\\mathrm{P}}$ $\\!=$ $\\!1$ for $n$ $\\!=$ $\\!1$.\nWhile for a one-dimensional $\\hat{P}$ $\\!=$ $\\!|\\Psi\\rangle\\langle\\Psi|$,\n(\\ref{FP}) reduces to $F_\\Psi$ $\\!=$\n$\\!|\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{U}|\\Psi\\rangle|^2$, the other\nlimit $d$ $\\!=$ $\\!n$ recovers (\\ref{F}).\n\nIf in (\\ref{FP}), $\\hat{U}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{SB}}$ couples the quantum\nsystem S to a bath B, then we define a projector\n$\\hat{\\Pi}$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}\\hat{P}|\\Psi\\rangle\\langle\\Psi|\n\\hat{P}\\hat{U}_{\\mathrm{T}}^\\dagger\\otimes\\hat{I}_{\\mathrm{B}}$ and generalize\n(\\ref{FP}) to\n\\begin{eqnarray}\n\\label{FPB}\n  F_{\\mathrm{P}}\n  &\\equiv&\\overline{\\mathrm{Tr}_{\\mathrm{SB}}\\bigl[\\hat{U}\\hat{P}|\\Psi\\rangle\n  \\hat{\\varrho}_{\\mathrm{B}}\n  \\langle\\Psi|\\hat{P}\\hat{U}^\\dagger\\hat{\\Pi}\\bigr]}^{(\\mathrm{P})}\n  \\\\\n  &=&\\left\\langle\n  \\frac{\\mathrm{Tr}_{\\mathrm{S}}(\\hat{M}^\\dagger\\hat{M})\n  +(\\mathrm{Tr}_{\\mathrm{S}}\\hat{M})^\\dagger(\\mathrm{Tr}_{\\mathrm{S}}\\hat{M})\n  }{d(d+1)}\\right\\rangle_{\\mathrm{B}},\n\\end{eqnarray}\nwhere $\\langle\\cdots\\rangle_{\\mathrm{B}}$ $\\!\\equiv$\n$\\!\\mathrm{Tr}_{\\mathrm{B}}[\\hat{\\varrho}_{\\mathrm{B}}(\\cdots)]$ with a fixed\nbath state $\\hat{\\varrho}_{\\mathrm{B}}$.\n\nReplacing $\\hat{U}|\\Psi\\rangle\\langle\\Psi|\\hat{U}^\\dagger$ in (\\ref{F}) with the\noutput $\\mathcal{M}(|\\Psi\\rangle\\langle\\Psi|)$ of a quantum channel generalizes\n(\\ref{F}) to \\cite{Ped07}\n\n", "index": 51, "text": "\\begin{equation}\n\\label{F1}\n  F=\\overline{\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^\\dagger\n  \\mathcal{M}(|\\Psi\\rangle\\langle\\Psi|)\n  \\hat{U}_{\\mathrm{T}}|\\Psi\\rangle}\n  =\\frac{n+\\sum_k|\\mathrm{Tr}(\\hat{U}_{\\mathrm{T}}^\\dagger\\hat{G}_k)|^2}\n  {n(n+1)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"F=\\overline{\\langle\\Psi|\\hat{U}_{\\mathrm{T}}^{\\dagger}\\mathcal{M}(|\\Psi\\rangle%&#10;\\langle\\Psi|)\\hat{U}_{\\mathrm{T}}|\\Psi\\rangle}=\\frac{n+\\sum_{k}|\\mathrm{Tr}(%&#10;\\hat{U}_{\\mathrm{T}}^{\\dagger}\\hat{G}_{k})|^{2}}{n(n+1)},\" display=\"block\"><mrow><mrow><mi>F</mi><mo>=</mo><mover accent=\"true\"><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mi mathvariant=\"normal\">\u03a8</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi mathvariant=\"normal\">\u03a8</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi></msub><mo>\u2062</mo><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>\u00af</mo></mover><mo>=</mo><mfrac><mrow><mi>n</mi><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>k</mi></msub><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mi>Tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>G</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup></mrow></mrow><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nthat reflects the statistics of their occurrence in different instances of\napplications of the device (external cycles in a control loop). This generalizes\n(\\ref{F1}) to\n\\begin{eqnarray}\n\\label{F2}\n  &&F_{\\hat{\\varrho}_{\\mathrm{in}}}\\equiv{p}(\\hat{U}_{\\mathrm{T}})\n  =\\sum_k{p}(\\hat{U}_{\\mathrm{T}}||\\Psi_k\\rangle){p}(|\\Psi_k\\rangle),\n  \\\\\n  &&{p}(\\hat{U}_{\\mathrm{T}}||\\Psi_k\\rangle)\n  =\\langle\\Psi_k|\\hat{U}_{\\mathrm{T}}^\\dagger\n  \\mathcal{M}(|\\Psi_k\\rangle\\langle\\Psi_k|)\n  \\hat{U}_{\\mathrm{T}}|\\Psi_k\\rangle,\\quad\n\\end{eqnarray}\nwhich is just the total probability ${p}(\\hat{U}_{\\mathrm{T}})$ of correctly\ndetecting a $\\hat{U}_{\\mathrm{T}}$-transformed pure (but unknown) input state\ndrawn from a distribution (\\ref{rhoin}).\nOnce $p(\\hat{U}_{\\mathrm{T}})$ $\\!=$ $\\!1$, the channel's effect is\nindistinguishable from that of $\\hat{U}_{\\mathrm{T}}$ for the set of possible\ninputs $|\\Psi_k\\rangle$, (i.e. those for which\n${p}(|\\Psi_k\\rangle)$ $\\!>$ $\\!0$). The case\n$p(\\hat{U}_{\\mathrm{T}})$ $\\!\\scriptstyle{\\stackrel{<}{\\approx}}$ $\\!1$ is\nrelevant from a numerical and experimental point of view. Rare inputs\n$|\\Psi_k\\rangle$, for which $0$ $\\!<$ $\\!{p}(|\\Psi_k\\rangle)$ $\\!\\ll$ $\\!1$,\nwill hardly affect $p(\\hat{U}_{\\mathrm{T}})$ in a control loop, which relaxes\nthe demands on the channel compared to the uniform average (\\ref{F1}). The\nchannel optimization itself is thus economized in the sense that it is required\nto perform well only on \\emph{typical} rather than \\emph{all} inputs.\n\nHere, we consider a navigation of $\\hat{U}$ in the above-mentioned discretized\ncase, $\\hat{U}$ $\\!=$ $\\!\\hat{U}(\\bm{h})$, that starts at the identity\n$\\hat{U}(\\bm{h}$ $\\!=$ $\\!\\bm{0})$ $\\!=$ $\\!\\hat{I}$ and from there undertakes a\ngradient-based maximization of ${p}(\\hat{U}_{\\mathrm{T}})$ as defined in\n(\\ref{F2}) to a point where\n\n", "itemtype": "equation", "pos": 107382, "prevtext": "\nwhere $\\hat{G}_k$ are the Kraus operators of the decomposition of the channel\nmap $\\mathcal{M}(\\hat{\\varrho})$ $\\!=$\n$\\!\\sum_k\\hat{G}_k\\hat{\\varrho}\\hat{G}_k^\\dagger$. Note that a change\n$\\hat{G}_k^\\prime$ $\\!=$ $\\!\\sum_jV_{kj}\\hat{G}_j$ of the Kraus operators as\ndescribed by a unitary matrix $V$ leaves (\\ref{F1}) invariant.\n\n\\subsection{Percept statistics-based fidelity}\n\nThe uniform average in (\\ref{F1}) can be generalized to an arbitrary\ndistribution ${p}(|\\Psi_k\\rangle)$ of possible input states $|\\Psi_k\\rangle$,\n\n", "index": 53, "text": "\\begin{equation}\n\\label{rhoin}\n  \\hat{\\varrho}_{\\mathrm{in}}\n  =\\sum_k{p}(|\\Psi_k\\rangle)|\\Psi_k\\rangle\\langle\\Psi_k|,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\varrho}_{\\mathrm{in}}=\\sum_{k}{p}(|\\Psi_{k}\\rangle)|\\Psi_{k}\\rangle%&#10;\\langle\\Psi_{k}|,\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\u03f1</mi><mo stretchy=\"false\">^</mo></mover><mi>in</mi></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>k</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo fence=\"true\" stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>k</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>k</mi></msub><mo fence=\"true\" stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nThe control vector $\\bm{h}$ typically represents system variables and not those\nof the bath. Rather than solving for the parameters \\cite{akulin2} for which\nthe scheme \\cite{lloyd2} yields a desired given unitary, we search parameters,\nfor which the unitary, whose specific form we are not interested in, solves a\ngiven task.\n\n\\subsection{Optimal memory navigation and constrained optimization}\n\nWhile here we have discussed and compared concrete types of algorithms, a more\nfundamental question concerns the optimality and derivation of general\n(e.g., speed) limits of the learning process. Although the physical time is\ngiven as the sum of the agent and environment response times over each cycle,\none may restrict to counting the number of (a) memory cycles in total, (b)\nexternal cycles only (c) episodes or (d) parameter updates $\\delta\\bm{h}$ of the\nmemory $\\hat{U}(\\bm{h})$, depending on what the most critical criterion is. Not\nonly should the navigation of $\\hat{U}$ from the identity to a point\n(\\ref{task}) follow an optimal trajectory, but also the navigation of the\npercept states by a given $\\hat{U}$ should be such that the individual physical\nmemory evolution times become minimum. Such demands may conflict with\nrestrictions on the practical implementability and complexity of the memory.\nSince these questions are beyond the scope of this work, in what follows we\nrestrict ourselves to outline a connection to constrained optimisation as a\npossible formal approach.\n\nAssuming the Schr{\\\"o}dinger equation\n{\\small $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}$ $\\!=$\n$\\!-\\mathrm{i}\\hat{H}\\hat{U}$} and a fixed energy-type constraint\n$\\|\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}\\|^2$ $\\!=$\n$\\!\\mathrm{Tr}(\\hat{H}^2)$ $\\!\\scriptstyle{\\stackrel{!}{=}}$ $\\!E^2$, the\nlength of a curve in the space of unitaries becomes\n\n", "itemtype": "equation", "pos": 109341, "prevtext": "\nthat reflects the statistics of their occurrence in different instances of\napplications of the device (external cycles in a control loop). This generalizes\n(\\ref{F1}) to\n\\begin{eqnarray}\n\\label{F2}\n  &&F_{\\hat{\\varrho}_{\\mathrm{in}}}\\equiv{p}(\\hat{U}_{\\mathrm{T}})\n  =\\sum_k{p}(\\hat{U}_{\\mathrm{T}}||\\Psi_k\\rangle){p}(|\\Psi_k\\rangle),\n  \\\\\n  &&{p}(\\hat{U}_{\\mathrm{T}}||\\Psi_k\\rangle)\n  =\\langle\\Psi_k|\\hat{U}_{\\mathrm{T}}^\\dagger\n  \\mathcal{M}(|\\Psi_k\\rangle\\langle\\Psi_k|)\n  \\hat{U}_{\\mathrm{T}}|\\Psi_k\\rangle,\\quad\n\\end{eqnarray}\nwhich is just the total probability ${p}(\\hat{U}_{\\mathrm{T}})$ of correctly\ndetecting a $\\hat{U}_{\\mathrm{T}}$-transformed pure (but unknown) input state\ndrawn from a distribution (\\ref{rhoin}).\nOnce $p(\\hat{U}_{\\mathrm{T}})$ $\\!=$ $\\!1$, the channel's effect is\nindistinguishable from that of $\\hat{U}_{\\mathrm{T}}$ for the set of possible\ninputs $|\\Psi_k\\rangle$, (i.e. those for which\n${p}(|\\Psi_k\\rangle)$ $\\!>$ $\\!0$). The case\n$p(\\hat{U}_{\\mathrm{T}})$ $\\!\\scriptstyle{\\stackrel{<}{\\approx}}$ $\\!1$ is\nrelevant from a numerical and experimental point of view. Rare inputs\n$|\\Psi_k\\rangle$, for which $0$ $\\!<$ $\\!{p}(|\\Psi_k\\rangle)$ $\\!\\ll$ $\\!1$,\nwill hardly affect $p(\\hat{U}_{\\mathrm{T}})$ in a control loop, which relaxes\nthe demands on the channel compared to the uniform average (\\ref{F1}). The\nchannel optimization itself is thus economized in the sense that it is required\nto perform well only on \\emph{typical} rather than \\emph{all} inputs.\n\nHere, we consider a navigation of $\\hat{U}$ in the above-mentioned discretized\ncase, $\\hat{U}$ $\\!=$ $\\!\\hat{U}(\\bm{h})$, that starts at the identity\n$\\hat{U}(\\bm{h}$ $\\!=$ $\\!\\bm{0})$ $\\!=$ $\\!\\hat{I}$ and from there undertakes a\ngradient-based maximization of ${p}(\\hat{U}_{\\mathrm{T}})$ as defined in\n(\\ref{F2}) to a point where\n\n", "index": 55, "text": "\\begin{equation}\n\\label{task}\n  \\bm{\\nabla}{p}(\\hat{U}_{\\mathrm{T}})=0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\bm{\\nabla}{p}(\\hat{U}_{\\mathrm{T}})=0.\" display=\"block\"><mrow><mrow><mrow><mrow><mo mathvariant=\"bold\">\u2207</mo><mo>\u2061</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover><mi mathvariant=\"normal\">T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere T is the arrival (or protocol) time,\n$\\hat{U}(t=T)$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}$ \\cite{Rus14a,Wan14}.\n(A ``protocol'' refers to a prescription for the time dependence of ${h}$,\n$\\hat{H}$, or $\\hat{U}$.) In addition to (or instead of) the protocol time $T$,\nwe may also consider\n\n", "itemtype": "equation", "pos": 111249, "prevtext": "\nThe control vector $\\bm{h}$ typically represents system variables and not those\nof the bath. Rather than solving for the parameters \\cite{akulin2} for which\nthe scheme \\cite{lloyd2} yields a desired given unitary, we search parameters,\nfor which the unitary, whose specific form we are not interested in, solves a\ngiven task.\n\n\\subsection{Optimal memory navigation and constrained optimization}\n\nWhile here we have discussed and compared concrete types of algorithms, a more\nfundamental question concerns the optimality and derivation of general\n(e.g., speed) limits of the learning process. Although the physical time is\ngiven as the sum of the agent and environment response times over each cycle,\none may restrict to counting the number of (a) memory cycles in total, (b)\nexternal cycles only (c) episodes or (d) parameter updates $\\delta\\bm{h}$ of the\nmemory $\\hat{U}(\\bm{h})$, depending on what the most critical criterion is. Not\nonly should the navigation of $\\hat{U}$ from the identity to a point\n(\\ref{task}) follow an optimal trajectory, but also the navigation of the\npercept states by a given $\\hat{U}$ should be such that the individual physical\nmemory evolution times become minimum. Such demands may conflict with\nrestrictions on the practical implementability and complexity of the memory.\nSince these questions are beyond the scope of this work, in what follows we\nrestrict ourselves to outline a connection to constrained optimisation as a\npossible formal approach.\n\nAssuming the Schr{\\\"o}dinger equation\n{\\small $\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}$ $\\!=$\n$\\!-\\mathrm{i}\\hat{H}\\hat{U}$} and a fixed energy-type constraint\n$\\|\\frac{\\mathrm{d}}{\\mathrm{d}t}\\hat{U}\\|^2$ $\\!=$\n$\\!\\mathrm{Tr}(\\hat{H}^2)$ $\\!\\scriptstyle{\\stackrel{!}{=}}$ $\\!E^2$, the\nlength of a curve in the space of unitaries becomes\n\n", "index": 57, "text": "\\begin{equation}\n\\label{L}\n  L=\\int_0^T\\Bigl\\|\\frac{\\mathrm{d}\\hat{U}}{\\mathrm{d}t}\\Bigr\\|\\mathrm{d}t\n  \\stackrel{!}{=}ET,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"L=\\int_{0}^{T}\\Bigl{\\|}\\frac{\\mathrm{d}\\hat{U}}{\\mathrm{d}t}\\Bigr{\\|}\\mathrm{d%&#10;}t\\stackrel{!}{=}ET,\" display=\"block\"><mrow><mrow><mi>L</mi><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mrow><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">\u2225</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mover accent=\"true\"><mi>U</mi><mo stretchy=\"false\">^</mo></mover></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">\u2225</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>t</mi></mrow></mrow></mrow><mover><mo movablelimits=\"false\">=</mo><mo lspace=\"0pt\" mathsize=\"142%\" rspace=\"3.5pt\" stretchy=\"false\">!</mo></mover><mrow><mi>E</mi><mo>\u2062</mo><mi>T</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nas a measure of the complexity of the protocol that integrates the changes that\nhave to be made on $\\hat{H}$ via the control fields.\n\nIf the optimization problem comprises two objectives such as minimising a\ndistance $D$ or maximising a fidelity $F$ with minimum amount of (\\ref{L}) or\n(\\ref{C}), then an \\emph{approximate} approach consists in first finding an\n$h(t)$ that optimizes an objective function $J_1$ under a fixed constraint\n$J_2$. Here, $J_1$ represents $D$ or $F$, while $J_2$ may represent\n$L$ or $C$. This can be formulated as an Euler-Lagrange equation\n\n", "itemtype": "equation", "pos": 111673, "prevtext": "\nwhere T is the arrival (or protocol) time,\n$\\hat{U}(t=T)$ $\\!=$ $\\!\\hat{U}_{\\mathrm{T}}$ \\cite{Rus14a,Wan14}.\n(A ``protocol'' refers to a prescription for the time dependence of ${h}$,\n$\\hat{H}$, or $\\hat{U}$.) In addition to (or instead of) the protocol time $T$,\nwe may also consider\n\n", "index": 59, "text": "\\begin{equation}\n\\label{C}\n  C\\equiv\\int_0^T\\Bigl\\|\\frac{\\mathrm{d}\\hat{H}}{\\mathrm{d}t}\\Bigr\\|\\mathrm{d}t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"C\\equiv\\int_{0}^{T}\\Bigl{\\|}\\frac{\\mathrm{d}\\hat{H}}{\\mathrm{d}t}\\Bigr{\\|}%&#10;\\mathrm{d}t\" display=\"block\"><mrow><mi>C</mi><mo>\u2261</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mrow><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">\u2225</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mover accent=\"true\"><mi>H</mi><mo stretchy=\"false\">^</mo></mover></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">\u2225</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>t</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07358.tex", "nexttext": "\nwhere the Lagrange multiplier $\\lambda$ must finally be substituted with\nthe given constant such as $L$ or $C$. This optimisation is then repeated with\nstepwise decreased $L$ or $C$, until the deterioration of the achievable $J_1$\nexceeds a certain threshold. Equivalently, (\\ref{ELE}) may also be thought of\noptimizing $J_2$ under the constraint of constant $J_1$. Eq.~(\\ref{ELE}), which\ncontains both derivatives in a symmetric way, merely states the linear\ndependence of the functional derivatives at an extremal point ${h}$ in the\nfunction space $\\{h\\}$.\n\nIn the discrete case, the time integrals in Eqs.~(\\ref{L}) and (\\ref{C}) reduce\nto sums over time intervals with constant $\\hat{H}$, and in (\\ref{C}) we assume\nthat each jump of $\\hat{H}$ gives a fixed finite contribution. To gain some\nintuition into the meaning of $C$, we may think of navigating through a\nclassical rectangular grid. There is a set of shortest paths connecting the\ndiagonal corners, but they are not equivalent with respect to the number of\nturns the navigator has to make along its way. In the quantum context, the\nnumber of switches equals the number of intervals with constant $\\hat{H}$, which\nmay be thought of ``gates''. In contrast to an analytical design of quantum\ncircuits, the circuit is here generated numerically, however. Since each switch\nto a different $\\hat{H}$ changes the instantaneous eigenbasis, we may thus think\nrather of ``layers'' drawing an analogy to classical artificial neural networks\n\\cite{bookRojas}.\n\n\\begin{acknowledgments}\nThis work was supported in part by the Austrian Science Fund (FWF) through\nproject F04012, and by the Templeton World Charity Foundation (TWCF).\n\\end{acknowledgments}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{40}\n\\makeatletter\n\\providecommand \\@ifxundefined [1]{\n \\@ifx{#1\\undefined}\n}\n\\providecommand \\@ifnum [1]{\n \\ifnum #1\\expandafter \\@firstoftwo\n \\else \\expandafter \\@secondoftwo\n \\fi\n}\n\\providecommand \\@ifx [1]{\n \\ifx #1\\expandafter \\@firstoftwo\n \\else \\expandafter \\@secondoftwo\n \\fi\n}\n\\providecommand \\natexlab [1]{#1}\n\\providecommand \\enquote  [1]{``#1''}\n\\providecommand \\bibnamefont  [1]{#1}\n\\providecommand \\bibfnamefont [1]{#1}\n\\providecommand \\citenamefont [1]{#1}\n\\providecommand \\href@noop [0]{\\@secondoftwo}\n\\providecommand \\href [0]{\\begingroup \\@sanitize@url \\@href}\n\\providecommand \\@href[1]{\\@@startlink{#1}\\@@href}\n\\providecommand \\@@href[1]{\\endgroup#1\\@@endlink}\n\\providecommand \\@sanitize@url [0]{\\catcode `\\\\12\\catcode `\\$12\\catcode\n  `\\&12\\catcode `\\#12\\catcode `\\^12\\catcode `\\_12\\catcode `\\%12\\relax}\n\\providecommand \\@@startlink[1]{}\n\\providecommand \\@@endlink[0]{}\n\\providecommand \\url  [0]{\\begingroup\\@sanitize@url \\@url }\n\\providecommand \\@url [1]{\\endgroup\\@href {#1}{\\urlprefix }}\n\\providecommand \\urlprefix  [0]{URL }\n\\providecommand \\Eprint [0]{\\href }\n\\providecommand \\doibase [0]{http://dx.doi.org/}\n\\providecommand \\selectlanguage [0]{\\@gobble}\n\\providecommand \\bibinfo  [0]{\\@secondoftwo}\n\\providecommand \\bibfield  [0]{\\@secondoftwo}\n\\providecommand \\translation [1]{[#1]}\n\\providecommand \\BibitemOpen [0]{}\n\\providecommand \\bibitemStop [0]{}\n\\providecommand \\bibitemNoStop [0]{.\\EOS\\space}\n\\providecommand \\EOS [0]{\\spacefactor3000\\relax}\n\\providecommand \\BibitemShut  [1]{\\csname bibitem#1\\endcsname}\n\\let\\auto@bib@innerbib\\@empty\n\n\\bibitem [{\\citenamefont {Nature}(2015)}]{NatureSpecialIssue}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibnamefont {Nature}},\\ }\\href@noop\n  {} {\\emph {\\bibinfo {title} {Special issue on Machine intelligence}}},\\ Vol.\\\n  \\bibinfo {volume} {521}\\ (\\bibinfo  {publisher} {Macmillan},\\ \\bibinfo {year}\n  {2015})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Science}(2015)}]{ScienceSpecialIssue}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibnamefont {Science}},\\ }\\href@noop\n  {} {\\emph {\\bibinfo {title} {Special issue on Artificial Intelligence}}},\\\n  Vol.\\ \\bibinfo {volume} {349}\\ (\\bibinfo  {publisher} {AAAS},\\ \\bibinfo\n  {year} {2015})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Nielsen}\\ and\\ \\citenamefont\n  {Chuang}(2000)}]{bookNielsen}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {M.~A.}\\ \\bibnamefont\n  {Nielsen}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {I.~L.}\\ \\bibnamefont\n  {Chuang}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Quantum Computation and\n  Quantum Information}}}\\ (\\bibinfo  {publisher} {Cambridge University Press},\\\n  \\bibinfo {address} {Cambridge},\\ \\bibinfo {year} {2000})\\BibitemShut\n  {NoStop}\n\\bibitem [{\\citenamefont {Wittek}(2014)}]{bookWittek}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {P.}~\\bibnamefont\n  {Wittek}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Quantum Machine\n  Learning}}}\\ (\\bibinfo  {publisher} {Elsevier},\\ \\bibinfo {address}\n  {Amsterdam},\\ \\bibinfo {year} {2014})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Briegel}\\ and\\ \\citenamefont {las\n  Cuevas}(2012)}]{Bri12}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont\n  {Briegel}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {G.~D.}\\ \\bibnamefont {las\n  Cuevas}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Sci.\n  Rep.}\\ }\\textbf {\\bibinfo {volume} {2}},\\ \\bibinfo {pages} {400} (\\bibinfo\n  {year} {2012})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Mautner}\\ \\emph {et~al.}(2015)\\citenamefont\n  {Mautner}, \\citenamefont {Makmal}, \\citenamefont {Manzano}, \\citenamefont\n  {Tiersch},\\ and\\ \\citenamefont {Briegel}}]{Mau15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont\n  {Mautner}}, \\bibinfo {author} {\\bibfnamefont {A.}~\\bibnamefont {Makmal}},\n  \\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont {Manzano}}, \\bibinfo\n  {author} {\\bibfnamefont {M.}~\\bibnamefont {Tiersch}}, \\ and\\ \\bibinfo\n  {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\ }\\href@noop {}\n  {\\bibfield  {journal} {\\bibinfo  {journal} {New Gener. Comput.}\\ }\\textbf\n  {\\bibinfo {volume} {33}},\\ \\bibinfo {pages} {69} (\\bibinfo {year}\n  {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {D'Alessandro}(2008)}]{bookAlessandro}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {D'Alessandro}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Introduction to\n  Quantum Control and Dynamics}}}\\ (\\bibinfo  {publisher} {Chapman \\&\n  Hall/CRC},\\ \\bibinfo {address} {Boca Raton, FL},\\ \\bibinfo {year}\n  {2008})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Wiseman}\\ and\\ \\citenamefont\n  {Milburn}(2009)}]{bookWiseman}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {H.~M.}\\ \\bibnamefont\n  {Wiseman}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {G.~J.}\\ \\bibnamefont\n  {Milburn}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Quantum Measurement and\n  Control}}}\\ (\\bibinfo  {publisher} {Cambridge University Press},\\ \\bibinfo\n  {address} {Cambridge, UK},\\ \\bibinfo {year} {2009})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Melnikov}\\ \\emph {et~al.}(2014)\\citenamefont\n  {Melnikov}, \\citenamefont {Makmal},\\ and\\ \\citenamefont {Briegel}}]{Mel14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {A.~A.}\\ \\bibnamefont\n  {Melnikov}}, \\bibinfo {author} {\\bibfnamefont {A.}~\\bibnamefont {Makmal}}, \\\n  and\\ \\bibinfo {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Artif. Intell.\n  Res.}\\ }\\textbf {\\bibinfo {volume} {3}},\\ \\bibinfo {pages} {24} (\\bibinfo\n  {year} {2014})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Melnikov}\\ \\emph {et~al.}(2015)\\citenamefont\n  {Melnikov}, \\citenamefont {Makmal}, \\citenamefont {Dunjko},\\ and\\\n  \\citenamefont {Briegel}}]{Mel15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {A.~A.}\\ \\bibnamefont\n  {Melnikov}}, \\bibinfo {author} {\\bibfnamefont {A.}~\\bibnamefont {Makmal}},\n  \\bibinfo {author} {\\bibfnamefont {V.}~\\bibnamefont {Dunjko}}, \\ and\\ \\bibinfo\n  {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\ }\\href@noop {}\n  {\\enquote {\\bibinfo {title} {Projective simulation with generalization},}\\ }\n  (\\bibinfo {year} {2015}),\\ \\bibinfo {note}\n  {http://arxiv.org/abs/1504.02247}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Paparo}\\ \\emph {et~al.}(2014)\\citenamefont {Paparo},\n  \\citenamefont {Dunjko}, \\citenamefont {Makmal}, \\citenamefont\n  {Martin-Delgado},\\ and\\ \\citenamefont {Briegel}}]{Pap14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {G.}~\\bibnamefont\n  {Paparo}}, \\bibinfo {author} {\\bibfnamefont {V.}~\\bibnamefont {Dunjko}},\n  \\bibinfo {author} {\\bibfnamefont {A.}~\\bibnamefont {Makmal}}, \\bibinfo\n  {author} {\\bibfnamefont {M.~A.}\\ \\bibnamefont {Martin-Delgado}}, \\ and\\\n  \\bibinfo {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys. Rev. X}\\\n  }\\textbf {\\bibinfo {volume} {4}},\\ \\bibinfo {pages} {031002} (\\bibinfo {year}\n  {2014})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dunjko}\\ \\emph {et~al.}(2015)\\citenamefont {Dunjko},\n  \\citenamefont {Friis},\\ and\\ \\citenamefont {Briegel}}]{Dun15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {V.}~\\bibnamefont\n  {Dunjko}}, \\bibinfo {author} {\\bibfnamefont {N.}~\\bibnamefont {Friis}}, \\\n  and\\ \\bibinfo {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {New J. Phys.}\\\n  }\\textbf {\\bibinfo {volume} {17}},\\ \\bibinfo {pages} {023006} (\\bibinfo\n  {year} {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Friis}\\ \\emph {et~al.}(2015)\\citenamefont {Friis},\n  \\citenamefont {Melnikov}, \\citenamefont {Kirchmair},\\ and\\ \\citenamefont\n  {Briegel}}]{Fri15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {N.}~\\bibnamefont\n  {Friis}}, \\bibinfo {author} {\\bibfnamefont {A.~A.}\\ \\bibnamefont {Melnikov}},\n  \\bibinfo {author} {\\bibfnamefont {G.}~\\bibnamefont {Kirchmair}}, \\ and\\\n  \\bibinfo {author} {\\bibfnamefont {H.~J.}\\ \\bibnamefont {Briegel}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Sci. Rep.}\\\n  }\\textbf {\\bibinfo {volume} {5}},\\ \\bibinfo {pages} {18036} (\\bibinfo {year}\n  {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Breuer}\\ and\\ \\citenamefont\n  {Petruccione}(2002)}]{bookBreuer}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {H.-P.}\\ \\bibnamefont\n  {Breuer}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {F.}~\\bibnamefont\n  {Petruccione}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {The Theory of Open\n  Quantum Systems}}}\\ (\\bibinfo  {publisher} {Oxford University Press},\\\n  \\bibinfo {address} {Oxford},\\ \\bibinfo {year} {2002})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ \\emph\n  {et~al.}(2006{\\natexlab{a}})\\citenamefont {Dong}, \\citenamefont {Chen},\n  \\citenamefont {Zhang},\\ and\\ \\citenamefont {Chen}}]{Dong06a}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.-Y.}\\ \\bibnamefont\n  {Dong}}, \\bibinfo {author} {\\bibfnamefont {C.-L.}\\ \\bibnamefont {Chen}},\n  \\bibinfo {author} {\\bibfnamefont {C.-B.}\\ \\bibnamefont {Zhang}}, \\ and\\\n  \\bibinfo {author} {\\bibfnamefont {Z.-H.}\\ \\bibnamefont {Chen}},\\ }\\href@noop\n  {} {\\bibfield  {journal} {\\bibinfo  {journal} {Robotica}\\ }\\textbf {\\bibinfo\n  {volume} {24}},\\ \\bibinfo {pages} {513} (\\bibinfo {year}\n  {2006}{\\natexlab{a}})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ \\emph\n  {et~al.}(2006{\\natexlab{b}})\\citenamefont {Dong}, \\citenamefont {Chen},\n  \\citenamefont {Chen},\\ and\\ \\citenamefont {Zhang}}]{Dong06b}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.-Y.}\\ \\bibnamefont\n  {Dong}}, \\bibinfo {author} {\\bibfnamefont {C.-L.}\\ \\bibnamefont {Chen}},\n  \\bibinfo {author} {\\bibfnamefont {Z.-H.}\\ \\bibnamefont {Chen}}, \\ and\\\n  \\bibinfo {author} {\\bibfnamefont {C.-B.}\\ \\bibnamefont {Zhang}},\\ }\\href@noop\n  {} {\\bibfield  {journal} {\\bibinfo  {journal} {Chin. Phys. Lett.}\\ }\\textbf\n  {\\bibinfo {volume} {23}},\\ \\bibinfo {pages} {1691} (\\bibinfo {year}\n  {2006}{\\natexlab{b}})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ \\emph\n  {et~al.}(2008{\\natexlab{a}})\\citenamefont {Dong}, \\citenamefont {Zhang},\n  \\citenamefont {Rabitz}, \\citenamefont {Pechen},\\ and\\ \\citenamefont\n  {Tarn}}]{Dong08a}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {Dong}}, \\bibinfo {author} {\\bibfnamefont {C.}~\\bibnamefont {Zhang}},\n  \\bibinfo {author} {\\bibfnamefont {H.}~\\bibnamefont {Rabitz}}, \\bibinfo\n  {author} {\\bibfnamefont {A.}~\\bibnamefont {Pechen}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {T.-J.}\\ \\bibnamefont {Tarn}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {J. Chem. Phys.}\\ }\\textbf {\\bibinfo {volume}\n  {129}},\\ \\bibinfo {pages} {154103} (\\bibinfo {year}\n  {2008}{\\natexlab{a}})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ \\emph\n  {et~al.}(2008{\\natexlab{b}})\\citenamefont {Dong}, \\citenamefont {Chen},\n  \\citenamefont {Li},\\ and\\ \\citenamefont {Tarn}}]{Dong08b}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {Dong}}, \\bibinfo {author} {\\bibfnamefont {C.}~\\bibnamefont {Chen}}, \\bibinfo\n  {author} {\\bibfnamefont {H.}~\\bibnamefont {Li}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {T.-J.}\\ \\bibnamefont {Tarn}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {IEEE Trans. Syst. Man, Cybern. B, Cybern}\\\n  }\\textbf {\\bibinfo {volume} {38}},\\ \\bibinfo {pages} {1207} (\\bibinfo {year}\n  {2008}{\\natexlab{b}})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ and\\ \\citenamefont {Petersen}(2009)}]{Dong09}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {Dong}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {I.~R.}\\ \\bibnamefont\n  {Petersen}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {New\n  J. Phys.}\\ }\\textbf {\\bibinfo {volume} {11}},\\ \\bibinfo {pages} {105033}\n  (\\bibinfo {year} {2009})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ and\\ \\citenamefont {Petersen}(2010)}]{Dong10}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {Dong}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {I.~R.}\\ \\bibnamefont\n  {Petersen}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {IET\n  Control Theory Appl.}\\ }\\textbf {\\bibinfo {volume} {4}},\\ \\bibinfo {pages}\n  {2651} (\\bibinfo {year} {2010})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Chen}\\ \\emph {et~al.}(2014)\\citenamefont {Chen},\n  \\citenamefont {Dong}, \\citenamefont {Long}, \\citenamefont {Petersen},\\ and\\\n  \\citenamefont {Rabitz}}]{Dong14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {C.}~\\bibnamefont\n  {Chen}}, \\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont {Dong}}, \\bibinfo\n  {author} {\\bibfnamefont {R.}~\\bibnamefont {Long}}, \\bibinfo {author}\n  {\\bibfnamefont {I.~R.}\\ \\bibnamefont {Petersen}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {H.~A.}\\ \\bibnamefont {Rabitz}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {Phys. Rev. A}\\ }\\textbf {\\bibinfo {volume}\n  {89}},\\ \\bibinfo {pages} {023402} (\\bibinfo {year} {2014})}\\BibitemShut\n  {NoStop}\n\\bibitem [{\\citenamefont {Dong}\\ \\emph {et~al.}(2015)\\citenamefont {Dong},\n  \\citenamefont {Mabrok}, \\citenamefont {Petersen}, \\citenamefont {Qi},\n  \\citenamefont {Chen},\\ and\\ \\citenamefont {Rabitz}}]{Dong15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {D.}~\\bibnamefont\n  {Dong}}, \\bibinfo {author} {\\bibfnamefont {M.~A.}\\ \\bibnamefont {Mabrok}},\n  \\bibinfo {author} {\\bibfnamefont {I.~R.}\\ \\bibnamefont {Petersen}}, \\bibinfo\n  {author} {\\bibfnamefont {B.}~\\bibnamefont {Qi}}, \\bibinfo {author}\n  {\\bibfnamefont {C.}~\\bibnamefont {Chen}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {H.}~\\bibnamefont {Rabitz}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {IEEE Trans. Control Syst. Technol.}\\ }\\textbf\n  {\\bibinfo {volume} {23}},\\ \\bibinfo {pages} {2155} (\\bibinfo {year}\n  {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Sutton}\\ and\\ \\citenamefont\n  {Barto}(1998)}]{bookSuttonBarto}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {R.~S.}\\ \\bibnamefont\n  {Sutton}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {A.~G.}\\ \\bibnamefont\n  {Barto}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Reinforcement Learning:\n  An Introduction}}}\\ (\\bibinfo  {publisher} {MIT Press},\\ \\bibinfo {address}\n  {Cambridge, MA},\\ \\bibinfo {year} {1998})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Russell}\\ and\\ \\citenamefont\n  {Norvig}(2009)}]{bookRussellNorvig}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {S.~J.}\\ \\bibnamefont\n  {Russell}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {P.}~\\bibnamefont\n  {Norvig}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Artificial Intelligence:\n  A Modern Approach}}}\\ (\\bibinfo  {publisher} {Prentice Hall},\\ \\bibinfo\n  {address} {Englewood Cliffs, NJ},\\ \\bibinfo {year} {2009})\\BibitemShut\n  {NoStop}\n\\bibitem [{\\citenamefont {Bang}\\ \\emph {et~al.}(2014)\\citenamefont {Bang},\n  \\citenamefont {Ryu}, \\citenamefont {Yoo}, \\citenamefont {Pawlowski},\\ and\\\n  \\citenamefont {Lee}}]{Ban14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont\n  {Bang}}, \\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont {Ryu}}, \\bibinfo\n  {author} {\\bibfnamefont {S.}~\\bibnamefont {Yoo}}, \\bibinfo {author}\n  {\\bibfnamefont {M.}~\\bibnamefont {Pawlowski}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {J.}~\\bibnamefont {Lee}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {New J. Phys.}\\ }\\textbf {\\bibinfo {volume}\n  {16}},\\ \\bibinfo {pages} {073017} (\\bibinfo {year} {2014})}\\BibitemShut\n  {NoStop}\n\\bibitem [{\\citenamefont {Clausen}(2015)}]{Cla15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont\n  {Clausen}},\\ }\\href@noop {} {\\enquote {\\bibinfo {title} {Time-optimal\n  bath-induced unitaries by zermelo navigation: speed limit and non-markovian\n  quantum computation},}\\ } (\\bibinfo {year} {2015}),\\ \\bibinfo {note}\n  {http://arxiv.org/abs/1507.08990}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Clausen}\\ \\emph {et~al.}(2010)\\citenamefont\n  {Clausen}, \\citenamefont {Bensky},\\ and\\ \\citenamefont\n  {Kurizki}}]{clausen18}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont\n  {Clausen}}, \\bibinfo {author} {\\bibfnamefont {G.}~\\bibnamefont {Bensky}}, \\\n  and\\ \\bibinfo {author} {\\bibfnamefont {G.}~\\bibnamefont {Kurizki}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys. Rev. Lett.}\\\n  }\\textbf {\\bibinfo {volume} {104}},\\ \\bibinfo {pages} {040401} (\\bibinfo\n  {year} {2010})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Degenfeld-Schonburg}\\ and\\ \\citenamefont\n  {Hartmann}(2014)}]{DSH14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {P.}~\\bibnamefont\n  {Degenfeld-Schonburg}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {M.~J.}\\\n  \\bibnamefont {Hartmann}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo\n  {journal} {Phys. Rev. B}\\ }\\textbf {\\bibinfo {volume} {89}},\\ \\bibinfo\n  {pages} {245108} (\\bibinfo {year} {2014})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Lloyd}(1995)}]{lloyd2}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {S.}~\\bibnamefont\n  {Lloyd}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys.\n  Rev. Lett.}\\ }\\textbf {\\bibinfo {volume} {75}},\\ \\bibinfo {pages} {346}\n  (\\bibinfo {year} {1995})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Goerz}\\ \\emph {et~al.}(2015)\\citenamefont {Goerz},\n  \\citenamefont {Whaley},\\ and\\ \\citenamefont {Koch}}]{Goe15}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {M.~H.}\\ \\bibnamefont\n  {Goerz}}, \\bibinfo {author} {\\bibfnamefont {K.~B.}\\ \\bibnamefont {Whaley}}, \\\n  and\\ \\bibinfo {author} {\\bibfnamefont {C.~P.}\\ \\bibnamefont {Koch}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {EPJ Quantum\n  Technology}\\ }\\textbf {\\bibinfo {volume} {2}},\\ \\bibinfo {pages} {21}\n  (\\bibinfo {year} {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Clausen}(2016)}]{z3}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {J.}~\\bibnamefont\n  {Clausen}},\\ }\\href@noop {} {} (\\bibinfo {year} {2016}),\\ \\bibinfo {note}\n  {work in progress}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Martinetz}\\ and\\ \\citenamefont\n  {Schulten}(1991)}]{MaSc91}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {T.}~\\bibnamefont\n  {Martinetz}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {K.}~\\bibnamefont\n  {Schulten}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal}\n  {Artificial Neural Networks}\\ }\\textbf {\\bibinfo {volume} {I}},\\ \\bibinfo\n  {pages} {397} (\\bibinfo {year} {1991})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Fritzke}(1995)}]{Fritzke95agrowing}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {B.}~\\bibnamefont\n  {Fritzke}},\\ }in\\ \\href@noop {} {\\emph {\\bibinfo {booktitle} {Advances in\n  Neural Information Processing Systems 7}}}\\ (\\bibinfo  {publisher} {MIT\n  Press},\\ \\bibinfo {year} {1995})\\ pp.\\ \\bibinfo {pages}\n  {625--632}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Harel}\\ and\\ \\citenamefont {Akulin}(1999)}]{akulin2}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {G.}~\\bibnamefont\n  {Harel}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {V.~M.}\\ \\bibnamefont\n  {Akulin}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys.\n  Rev. Lett.}\\ }\\textbf {\\bibinfo {volume} {82}},\\ \\bibinfo {pages} {1}\n  (\\bibinfo {year} {1999})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Rabitz}\\ \\emph {et~al.}(2005)\\citenamefont {Rabitz},\n  \\citenamefont {Hsieh},\\ and\\ \\citenamefont {Rosenthal}}]{Rab05}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {H.}~\\bibnamefont\n  {Rabitz}}, \\bibinfo {author} {\\bibfnamefont {M.}~\\bibnamefont {Hsieh}}, \\\n  and\\ \\bibinfo {author} {\\bibfnamefont {C.}~\\bibnamefont {Rosenthal}},\\\n  }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys. Rev. A}\\\n  }\\textbf {\\bibinfo {volume} {72}},\\ \\bibinfo {pages} {052337} (\\bibinfo\n  {year} {2005})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Pedersen}\\ \\emph {et~al.}(2007)\\citenamefont\n  {Pedersen}, \\citenamefont {M{\\o}ller},\\ and\\ \\citenamefont\n  {M{\\o}lmer}}]{Ped07}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {L.~H.}\\ \\bibnamefont\n  {Pedersen}}, \\bibinfo {author} {\\bibfnamefont {N.~M.}\\ \\bibnamefont\n  {M{\\o}ller}}, \\ and\\ \\bibinfo {author} {\\bibfnamefont {K.}~\\bibnamefont\n  {M{\\o}lmer}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal}\n  {Phys. Lett. A}\\ }\\textbf {\\bibinfo {volume} {367}},\\ \\bibinfo {pages} {47}\n  (\\bibinfo {year} {2007})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Dankert}(2005)}]{dan05}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {C.}~\\bibnamefont\n  {Dankert}},\\ }\\emph {\\bibinfo {title} {{Efficient Simulation of Random\n  Quantum States and Operators}}},\\ \\href@noop {} {Master's thesis},\\ \\bibinfo\n  {school} {University of Waterloo}, \\bibinfo {address} {Ontario, Canada}\n  (\\bibinfo {year} {2005})\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Russell}\\ and\\ \\citenamefont\n  {Stepney}(2014)}]{Rus14a}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {B.}~\\bibnamefont\n  {Russell}}\\ and\\ \\bibinfo {author} {\\bibfnamefont {S.}~\\bibnamefont\n  {Stepney}},\\ }\\href@noop {} {\\bibfield  {journal} {\\bibinfo  {journal} {Phys.\n  Rev. A}\\ }\\textbf {\\bibinfo {volume} {90}},\\ \\bibinfo {pages} {012303}\n  (\\bibinfo {year} {2014})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Wang}\\ \\emph {et~al.}(2015)\\citenamefont {Wang},\n  \\citenamefont {Allegra}, \\citenamefont {Jacobs}, \\citenamefont {Lloyd},\n  \\citenamefont {Lupo},\\ and\\ \\citenamefont {Mohseni}}]{Wan14}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {X.}~\\bibnamefont\n  {Wang}}, \\bibinfo {author} {\\bibfnamefont {M.}~\\bibnamefont {Allegra}},\n  \\bibinfo {author} {\\bibfnamefont {K.}~\\bibnamefont {Jacobs}}, \\bibinfo\n  {author} {\\bibfnamefont {S.}~\\bibnamefont {Lloyd}}, \\bibinfo {author}\n  {\\bibfnamefont {C.}~\\bibnamefont {Lupo}}, \\ and\\ \\bibinfo {author}\n  {\\bibfnamefont {M.}~\\bibnamefont {Mohseni}},\\ }\\href@noop {} {\\bibfield\n  {journal} {\\bibinfo  {journal} {Phys. Rev. Lett.}\\ }\\textbf {\\bibinfo\n  {volume} {114}},\\ \\bibinfo {pages} {170501} (\\bibinfo {year}\n  {2015})}\\BibitemShut {NoStop}\n\\bibitem [{\\citenamefont {Rojas}(1996)}]{bookRojas}\n  \\BibitemOpen\n  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {R.}~\\bibnamefont\n  {Rojas}},\\ }\\href@noop {} {\\emph {\\bibinfo {title} {Neural Networks - A\n  Systematic Introduction}}}\\ (\\bibinfo  {publisher} {Springer-Verlag},\\\n  \\bibinfo {address} {Berlin},\\ \\bibinfo {year} {1996})\\BibitemShut {NoStop}\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 112365, "prevtext": "\nas a measure of the complexity of the protocol that integrates the changes that\nhave to be made on $\\hat{H}$ via the control fields.\n\nIf the optimization problem comprises two objectives such as minimising a\ndistance $D$ or maximising a fidelity $F$ with minimum amount of (\\ref{L}) or\n(\\ref{C}), then an \\emph{approximate} approach consists in first finding an\n$h(t)$ that optimizes an objective function $J_1$ under a fixed constraint\n$J_2$. Here, $J_1$ represents $D$ or $F$, while $J_2$ may represent\n$L$ or $C$. This can be formulated as an Euler-Lagrange equation\n\n", "index": 61, "text": "\\begin{equation}\n\\label{ELE}\n  \\frac{\\delta{J_1}}{\\delta{h}}-\\lambda\\frac{\\delta{J_2}}{\\delta{h}}=0,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\delta{J_{1}}}{\\delta{h}}-\\lambda\\frac{\\delta{J_{2}}}{\\delta{h}}=0,\" display=\"block\"><mrow><mrow><mrow><mfrac><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>J</mi><mn>1</mn></msub></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>h</mi></mrow></mfrac><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mfrac><mrow><mi>\u03b4</mi><mo>\u2062</mo><msub><mi>J</mi><mn>2</mn></msub></mrow><mrow><mi>\u03b4</mi><mo>\u2062</mo><mi>h</mi></mrow></mfrac></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}]