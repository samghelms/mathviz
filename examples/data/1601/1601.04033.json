[{"file": "1601.04033.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 6566, "prevtext": "\n\n\\begin{center}\n\\maketitle\n\\end{center}\n\n\\begin{abstract}\n\nAsynchronous distributed stochastic gradient descent methods have trouble converging because of stale gradients.\nA gradient update sent to a parameter server by a client is stale if the parameters used to calculate that gradient have since been updated on the server.\nApproaches have been proposed to circumvent this problem that quantify staleness in terms of the number of elapsed updates.\nIn this work, we propose a novel method that quantifies staleness in terms of moving averages of gradient statistics.\nWe show that this method outperforms previous methods with respect to convergence speed and scalability to many clients.\nWe also discuss how an extension to this method can be used to dramatically reduce bandwidth costs\nin a distributed training context.\nIn particular, our method allows reduction of total bandwidth usage by a factor of 5 with little impact on\ncost convergence.\nWe also describe (and link to) a software library that we have used to simulate these algorithms\ndeterministically on a single machine.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nNeural Networks can be trained on multiple machines simultaneously using distributed stochastic gradient descent (SGD).\nThis can be done on commodity CPU clusters as in \\cite{Dean} and \\cite{ADAM},\nor on a heterogenous mixture of machines as in \\cite{ONEBIT} and \\cite{Wu}.\nIn either case, a replica of the network is created on each separate machine. These replica machines are called clients.\nOne machine holds the canonical snapshot of the model parameters, and this machine is called the server.\nClients take a random mini-batch of training data and perform backpropagation on their replica of the model to maintain a gradient estimate.\nThis estimate is then sent back to the server to be applied to the global parameters.\nIf the server waits to collect all of the client updates before applying them to its parameters, we say that the SGD is synchronous.\nIf the server applies updates as they come in (as first introduced in \\cite{FirstASGD}), we say that the SGD is asynchronous.\nEach method has pros and cons.\nSynchronous SGD is slower because clients spend some time waiting, but it's free from convergence issues.\nAsync SGD removes the waiting, but may have trouble converging due to stale gradients.\nThus, dealing effectively with stale gradients is essential to achieving good performance\nfor asynchronous distributed training algorithms when the number of clients is high.\n\nIn \\cite{Rel}, they suggest an exponential penalty for stale gradients.\nThis could work for a small number of clients, but it will reduce the learning rate too far\nwhen staleness values are large.\n\nIn \\cite{Suyog} they propose a type of staleness aware async SGD.\nIn the context of asynchronous gradient updates in a parameter server context, they say a gradient\nsent to the parameter server from a worker is stale by k steps if the weights used to compute that\ngradient have since been updated k times.\n\nThey then describe a policy of modulating the learning rate as a function of that staleness measure.\nThis is accomplished by simply dividing the gradient update by the staleness measure before applying it.\nWe refer to that strategy as staleness-aware async SGD (SASGD).\n\nSASGD outperforms ASGD in terms of convergence, but it leaves performance on the table by treating all gradients as equal when computing staleness.\nIn this paper, we exploit that slack by modulating the learning rate as a function of moving averages of gradient statistics.\nThis idea yields better convergence performance, as we will show. \nWe call this new algorithm Faster ASGD (FASGD for short).\nWe apply a similar idea to the problem of reducing bandwidth usage in distributed training contexts and achieve good results there as well.\nWe call this second algorithm Bandwidth-Aware Faster ASGD (B-FASGD for short).\n\nThe rest of the paper is organized as follows: first we discuss background and terminology.\nWe move on to cover the FASGD and B-FASGD algorithms.\nThen we introduce our experimental architecture before finally finishing with the experimental results.\n\n\\section{The FASGD Algorithm}\n\nWe first introduce existing staleness aware algorithms before moving on to FASGD and variants.\n\n\\subsection{Background}\nWe adopt the notation from \\cite{Suyog} for describing an asynchronous training session,\nwith some slight modifications:\n\n\\begin{itemize}\n  \\item $\\lambda$: The number of clients.\n  \\item $\\mu$: The minibatch size used by all learners to produce stochastic gradient estimates.\n  \\item $\\alpha$: The master learning rate. Individual learning rates are made by modifications to this.\n  \\item $T$: The scalar timestamp that tracks the number of updates made to the master parameters.\n    The timestamp starts at 0 and is incremented by one for each weight update (regardless\n    of the number of clients or the sizes of the minibatches involved in computing the update). By the timestamp\n    of a gradient, we mean the timestamp of the parameters used to compute that gradient.\n  \\item $\\tau_{i,l}$: The step-staleness of the stochastic gradient from client $l$ with respect to the parameter timestamp\n    $i$. A client $l$ pushes gradient with timestamp $j$ to the parameter server with timestamp $i$, where $i \\geq j$.\n    We calculate the step-staleness $\\tau_{i,l}$ of this gradient as $i - j$.\n    The step-staleness is always non-negative as defined.\n\\end{itemize}\n\nSince there are many possible implementations of Asynchronous SGD, we formally specify the algorithm used for\nthe purposes of this paper:\n\n\\begin{itemize}\n\\item Async SGD Protocol: In this protocol, all $\\lambda$ clients compute gradients asynchronously in parallel.\n  When a client $l$ is done computing gradients $\\Delta \\theta^l$,\n  it waits to take the lock on the parameter server (only one client can communicate with the\n  server at a time). When the client takes the lock, the following things happen atomically before the lock is released:\n  \\begin{enumerate}\n    \\item Client $l$ passes the computed stochastic gradient $\\Delta \\theta^l$ to the server.\n    \\item The server updates the global parameters $\\theta_i$ according to the following equation:\n      $ \\theta_{i + 1} = \\theta_{i} - \\alpha \\Delta \\theta^l$\n    \\item The server passes the updated parameters $\\theta_{i + 1}$ back to client $l$. \n  \\end{enumerate}\n\n\\end{itemize}\n\nIn \\cite{Suyog}, they propose modulating the learning rate used on a per gradient basis.\nThus, their gradient update policy is as follows:\n\n\n", "index": 1, "text": "\\begin{equation}\n  g_i = \\alpha(\\tau_{i,k})\\Delta \\theta^k\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"g_{i}=\\alpha(\\tau_{i,k})\\Delta\\theta^{k}\" display=\"block\"><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c4</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\u03b8</mi><mi>k</mi></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\nwhere $\\alpha(\\tau_{i,k})$ is the step-staleness dependent learning rate\nand $k$ is the client identifier.\n\nThey show that this modification allows convergence for larger values of $\\lambda$.\nThis makes sense, because large values of $\\lambda$ imply higher average step-staleness.\n\n\\subsection{Improvements}\n\nOne of the claims of this paper is that the step-staleness as defined in the last section\ncan be substantially improved upon as a measure of staleness.\nIn what follows, we introduce what we claim is a better measure. \n\nConsider a server with gradients $\\theta_{i}$ and a client $l$ training on parameters $\\theta_{j}$,\nwhere $j \\leq i$. Suppose that the client in question then sends a gradient estimate $\\Delta \\theta^l$\nto the server with no intervening parameter updates.\n\nWe define the \\textbf{B-Staleness} $\\Gamma(\\theta_{i}, \\Delta \\theta^l)$ as follows:\n\n\n", "itemtype": "equation", "pos": 6641, "prevtext": "\n\n\n", "index": 3, "text": "\\begin{equation}\n   \\theta_{i + 1} = \\theta_{i} - g_i\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\theta_{i+1}=\\theta_{i}-g_{i}\" display=\"block\"><mrow><msub><mi>\u03b8</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>\u03b8</mi><mi>i</mi></msub><mo>-</mo><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\nWhere $ \\Delta \\theta^l $ is the stochastic gradient computed by client $l$ and\n$ \\Delta \\theta_i$ is the stochastic gradient that would have been computed with client\n$l$ on \\textit{the same minibatch} if client $l$ had used $\\theta_i$ instead of $\\theta_j$.\n\nIt might not be immediately apparent why this is a better measure, but we'll justify this below.\n\nThe basic idea is as follows: consider a client $l$ training on parameters $\\theta_{i-2}$\nwhile the server has parameters $\\theta_{i}$.\nIf client $l$ is the next client to send its stochastic gradient to the server, then that\ngradient $\\Delta \\theta^l$ will have step staleness $\\tau_{i,l} = 2$.\nNow in this case the copy of the parameters on the master $\\theta_i$ can be expressed as $\\theta_i = \\theta_{i-2} - g_1 - g_2$,\nwhere $g_1$ and $g_2$ are the updates computed by multiplying some learning rate by the mean (across the minibatch) of\nthe the stochastic gradients returned by some clients. In the asynchronous context, $g_1$ and $g_2$ may have been\ngenerated by clients with stale parameters as well.\n\nHowever, not all pairs $g_1, g_2$ will cause the same amount of B-Staleness.\nIf $g_1$ and $g_2$ largely cancel each other out, then $\\Gamma$ will be less than if $g_1$ and $g_2$ mostly have the same sign.\nIf the Hessian has large values at $\\theta_i$, then $\\Gamma$ will be higher than if the Hessian has small values.\nIt seems wise to account for these things in our synchronization protocol.\n\nWe care about staleness because the stochastic gradients computed using stale parameters will be a biased estimate\nof the true gradient at the current point in parameter space.\nIn particular, for any stale gradient, we can decompose its difference from the true gradient\ninto the difference due to using SGD rather than batch GD and the difference $(\\Delta \\theta^l) - (\\Delta \\theta_i)$\n(kind of a bias-variance decomposition for Async SGD.).\nSince we can't control the first component (except by changing $\\mu$),\nit seems reasonable to say that, for our purposes, we just care about the B-Staleness.\n\nB-Staleness as defined in equation \\ref{bdef} can be approximated (using the Taylor series expansion)\nby how far we have moved in the parameter space times the rate of change\nin the gradients with respect to a change in parameter space.\nThis approximation is intractable to compute exactly because it involves higher order derivatives,\nbut to the extent we can approximate it,\nwe will be using a strictly more informative measure than in SASGD, and so should be able to improve convergence.\n\nTaking inspiration from the version of RMSProp (introduced in \\cite{RMSPROP}) in \\cite{Graves}, we can do the following:\nMaintain a global moving average of the standard deviation of each parameter.\nAt update time, modulate the learning rate per parameter based on this moving average and the step-staleness.\nMore formally, we propose the following protocol, which we call FASGD:\n\n\\begin{itemize}\n\\item FASGD: In this protocol, each learner $l$ pulls the weights from the parameter server,\n  calculates the gradients and pushes the gradients to the parameter server,\n  just as in the ASGD protocol defined earlier.\n  As in ASGD, the parameter server updates\n  the weights after receiving a gradient\n  from any of the $\\lambda$ learners.\n  We maintain a moving average of gradient standard deviation as follows:\n\n\n", "itemtype": "equation", "pos": 7582, "prevtext": "\n\nwhere $\\alpha(\\tau_{i,k})$ is the step-staleness dependent learning rate\nand $k$ is the client identifier.\n\nThey show that this modification allows convergence for larger values of $\\lambda$.\nThis makes sense, because large values of $\\lambda$ imply higher average step-staleness.\n\n\\subsection{Improvements}\n\nOne of the claims of this paper is that the step-staleness as defined in the last section\ncan be substantially improved upon as a measure of staleness.\nIn what follows, we introduce what we claim is a better measure. \n\nConsider a server with gradients $\\theta_{i}$ and a client $l$ training on parameters $\\theta_{j}$,\nwhere $j \\leq i$. Suppose that the client in question then sends a gradient estimate $\\Delta \\theta^l$\nto the server with no intervening parameter updates.\n\nWe define the \\textbf{B-Staleness} $\\Gamma(\\theta_{i}, \\Delta \\theta^l)$ as follows:\n\n\n", "index": 5, "text": "\\begin{equation}  \\label{bdef}\n  \\Gamma(\\theta_{i}, \\Delta \\theta^l) = || (\\Delta \\theta^l) - (\\Delta \\theta_i) ||\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\Gamma(\\theta_{i},\\Delta\\theta^{l})=||(\\Delta\\theta^{l})-(\\Delta\\theta_{i})||\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>i</mi></msub><mo>,</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\u03b8</mi><mi>l</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo fence=\"true\">||</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\u03b8</mi><mi>l</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>\u03b8</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 11084, "prevtext": "\n\nWhere $ \\Delta \\theta^l $ is the stochastic gradient computed by client $l$ and\n$ \\Delta \\theta_i$ is the stochastic gradient that would have been computed with client\n$l$ on \\textit{the same minibatch} if client $l$ had used $\\theta_i$ instead of $\\theta_j$.\n\nIt might not be immediately apparent why this is a better measure, but we'll justify this below.\n\nThe basic idea is as follows: consider a client $l$ training on parameters $\\theta_{i-2}$\nwhile the server has parameters $\\theta_{i}$.\nIf client $l$ is the next client to send its stochastic gradient to the server, then that\ngradient $\\Delta \\theta^l$ will have step staleness $\\tau_{i,l} = 2$.\nNow in this case the copy of the parameters on the master $\\theta_i$ can be expressed as $\\theta_i = \\theta_{i-2} - g_1 - g_2$,\nwhere $g_1$ and $g_2$ are the updates computed by multiplying some learning rate by the mean (across the minibatch) of\nthe the stochastic gradients returned by some clients. In the asynchronous context, $g_1$ and $g_2$ may have been\ngenerated by clients with stale parameters as well.\n\nHowever, not all pairs $g_1, g_2$ will cause the same amount of B-Staleness.\nIf $g_1$ and $g_2$ largely cancel each other out, then $\\Gamma$ will be less than if $g_1$ and $g_2$ mostly have the same sign.\nIf the Hessian has large values at $\\theta_i$, then $\\Gamma$ will be higher than if the Hessian has small values.\nIt seems wise to account for these things in our synchronization protocol.\n\nWe care about staleness because the stochastic gradients computed using stale parameters will be a biased estimate\nof the true gradient at the current point in parameter space.\nIn particular, for any stale gradient, we can decompose its difference from the true gradient\ninto the difference due to using SGD rather than batch GD and the difference $(\\Delta \\theta^l) - (\\Delta \\theta_i)$\n(kind of a bias-variance decomposition for Async SGD.).\nSince we can't control the first component (except by changing $\\mu$),\nit seems reasonable to say that, for our purposes, we just care about the B-Staleness.\n\nB-Staleness as defined in equation \\ref{bdef} can be approximated (using the Taylor series expansion)\nby how far we have moved in the parameter space times the rate of change\nin the gradients with respect to a change in parameter space.\nThis approximation is intractable to compute exactly because it involves higher order derivatives,\nbut to the extent we can approximate it,\nwe will be using a strictly more informative measure than in SASGD, and so should be able to improve convergence.\n\nTaking inspiration from the version of RMSProp (introduced in \\cite{RMSPROP}) in \\cite{Graves}, we can do the following:\nMaintain a global moving average of the standard deviation of each parameter.\nAt update time, modulate the learning rate per parameter based on this moving average and the step-staleness.\nMore formally, we propose the following protocol, which we call FASGD:\n\n\\begin{itemize}\n\\item FASGD: In this protocol, each learner $l$ pulls the weights from the parameter server,\n  calculates the gradients and pushes the gradients to the parameter server,\n  just as in the ASGD protocol defined earlier.\n  As in ASGD, the parameter server updates\n  the weights after receiving a gradient\n  from any of the $\\lambda$ learners.\n  We maintain a moving average of gradient standard deviation as follows:\n\n\n", "index": 7, "text": "\\begin{equation}\n  n_i = \\gamma n_{i-1} + (1 - \\gamma) \\Delta (\\theta^l)^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"n_{i}=\\gamma n_{i-1}+(1-\\gamma)\\Delta(\\theta^{l})^{2}\" display=\"block\"><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>n</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03b8</mi><mi>l</mi></msup><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 11175, "prevtext": "\n\n\n", "index": 9, "text": "\\begin{equation}\n   b_i = \\gamma b_{i-1} + (1 - \\gamma) \\Delta \\theta^l \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"b_{i}=\\gamma b_{i-1}+(1-\\gamma)\\Delta\\theta^{l}\" display=\"block\"><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\u03b8</mi><mi>l</mi></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n  \n  where $\\gamma$ and $\\beta$ are hyperparameters and $\\epsilon$ is for numerical\n  stability. $n_i$ is the concatenation of all the parameters into a vector.\n  $b_i$ and $v_i$ are the same shape, and all arithmetic (including the squaring)\n  is elementwise.\n\n  The FASGD weight update rule is then given by:\n\n\n", "itemtype": "equation", "pos": 11264, "prevtext": "\n\n\n", "index": 11, "text": "\\begin{equation}\n   v_i = \\beta  v_{i-1} + (1 - \\beta)  \\frac{1}{\\sqrt{n_i - b_i^2 + \\epsilon}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"v_{i}=\\beta v_{i-1}+(1-\\beta)\\frac{1}{\\sqrt{n_{i}-b_{i}^{2}+\\epsilon}}\" display=\"block\"><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mn>1</mn><msqrt><mrow><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>-</mo><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mi>\u03f5</mi></mrow></msqrt></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 11687, "prevtext": "\n  \n  where $\\gamma$ and $\\beta$ are hyperparameters and $\\epsilon$ is for numerical\n  stability. $n_i$ is the concatenation of all the parameters into a vector.\n  $b_i$ and $v_i$ are the same shape, and all arithmetic (including the squaring)\n  is elementwise.\n\n  The FASGD weight update rule is then given by:\n\n\n", "index": 13, "text": "\\begin{equation}\n    g_i =  \\frac{\\alpha}{v_i \\tau_{i,k}}\\Delta \\theta^k\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"g_{i}=\\frac{\\alpha}{v_{i}\\tau_{i,k}}\\Delta\\theta^{k}\" display=\"block\"><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mrow><mfrac><mi>\u03b1</mi><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03c4</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></mfrac><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\u03b8</mi><mi>k</mi></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\n  where $k$ is the client identifier.  \n  \n\\end{itemize}\nIt seems there ought to be a more principled relationship between the moving average\nwindow and $\\lambda$, but as shown in the Experimental Results section, we\nare able to acheive good performance results with this heuristic method.\n\nFrom an intutive perspective, it would be nice to understand why\ndividing the learning rate by the standard deviation helps us.\nOne way to think of this is as follows:\nIf the Hessian has large values, we expect big changes from gradient to gradient,\nso we should see high gradient standard deviation.\nWe also expect higher B-Staleness in this instance,\nso dividing by a moving average of gradient standard deviation seems likely to help convergence.\n\nWe can also consider the potential for gradients to cancel each other out.\nIf gradients cancel each other out, they must have changed sign.\nWhere there are sign changes, there is likely to be higher gradient\nstandard deviation (or the gradients were small to begin with).\nThus, dividing by the standard deviation ought to account for cancellation effects.\n\nOverall, FASGD should lead to better convergence than SASGD for the following two reasons:\nFirst, it will allow us to keep the learning rate high when B-Staleness is less than step-staleness estimates.\nSecond, it will prevent us from overshooting when B-Staleness is higher than step-stalensss estimates.\n\n\\subsection{Bandwidth Aware FASGD}\n\nIn \\cite{Dean}, they suggest reducing bandwidth consumption of distributed training algorithms\nby limiting each model replica to request updated parameters only every $k_{fetch}$ steps and send updated gradient\nvalues only every $k_{push}$ steps (where $k_{fetch}$ might not be equal to $k_{push}$).\nThis approach works to some extent, but it has two significant downsides.\nFirst, the distribution of bandwidth consumption will be somewhat peaky.\nThis is suboptimal from a throughput perspective.\nSecond, the amount of achievable bandwidth reduction will depend on B-Staleness,\nbut this method fixes an amount at the beginning of training and sticks with it.\n\nWe attempt to fix both of those problems as follows.\nFirst, we make the choice about whether to push or fetch at any given time a probabilistic choice.\nThat is, when a client has to decide whether to push or fetch, it generates a psuedo-random number and\ncompares that number to some other quantity.\nIf the pseudo-random number is greater than that quantity, the data is dropped - otherwise, it is transmitted.\nSecond, we modulate the likelihood of deciding to push or fetch at a given opportunity by the\nmoving average of gradient standard deviation already being computed for the FASGD algorithm.\n\nMore formally, given an opportunity to transmit a gradient or receive a parameter update,\na client will do so if and only if\n\n\n", "itemtype": "equation", "pos": 11776, "prevtext": "\n\n\n", "index": 15, "text": "\\begin{equation}\n   \\theta_{i + 1} = \\theta_{i} - g_i \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\theta_{i+1}=\\theta_{i}-g_{i}\" display=\"block\"><mrow><msub><mi>\u03b8</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>\u03b8</mi><mi>i</mi></msub><mo>-</mo><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04033.tex", "nexttext": "\n\nwhere $r \\in [0,1] $ is a random number, $\\epsilon$ is a small constant added for numerical stability,\n$c$ is a hyper-parameter,\nand v is the mean across all the parameters of the moving averages of their standard deviations as calculated by the FASGD server.\nIn practice, we will have separate hyper-parameters $c_{push}$ and $c_{fetch}$ for pushing and fetching.\nWe refer to the above algorithm (coupled with the FASGD policy) as the B-FASGD policy.\n\nNote that if $v$ is very large, the right hand side of equation \\ref{req} will be close to 1, so transmission is nearly assured.\nIf $v$ is close to 0 (it is always non-negative), the right hand side will be very small.\nThus the right hand side lies in $(0,1)$ and is inversely proportional to the gradient standard deviation.\nThis means that when we expect B-Staleness to be high, we will transmit more frequently,\nincurring less step staleness.\nWhen we expect B-Staleness to be low, we will skip more updates,\nsince each step missed corresponds to less accumulated B-Staleness.\n\nIt's worth mentioning that dropping parameters is different than dropping gradient updates.\nWhen a client drops a parameter update, it simply continues computing gradients with the parameters it already has.\nWhen a client decides to drop a gradient update, it's less clear what should be done.\nWe have elected to re-apply the most recent gradient from that client,\nbut this neccessitates maintaining a gradient cache on the server,\nwhich could be prohibitive for a large values of $\\lambda$ or large models.\nIt's possible that somehow averaging unsent gradients on the clients until transmission time would work better.\nAt any rate, it turns out (see the Experimental Results section), that dropping gradients is much less effective than dropping\nparameters, so this decision may not matter in practice.\n\n\n\\section{Experimental Setup}\n\\label{sec:impl}\n\\vspace{-0.1mm}\nIn this section, we describe FRED - a Python library for simulating distributed training algorithms\ndeterministically on a single node.\nThe source code for FRED is at \\href{https://github.com/DoctorTeeth/fasgd}{https://github.com/DoctorTeeth/fasgd}.\nLater, we'll describe how we used FRED to generate experimental results for FASGD.\n\nFRED works by taking an idiomatic description of a distributed training algorithm and running it\ndeterministically on a single node. This is useful for the following reasons:\n\nFirst, determinism is helpful for debugging and experimentation.\nIt allows us to check that runs which should be bitwise equivalent are bitwise equivalent.\nFor instance, we can check that synchronous SGD with $\\lambda$ clients and batch size $\\mu$ is the same\nalgorithm as vanilla SGD with batch size $\\frac{\\mu}{\\lambda}$.\nWe would have dramatically less confidence in the correctness of our implementations without this feature.\n\nSecond, setting up a big training cluster is an ordeal.\nIt's hard and it's expensive, and differences in configurations make\nit hard to reproduce results.\nMoreover, the quirks involved in a particular system may obscure algorithmic considerations.\nUsing a simulator allows you to test training algorithms for all topologies, since you\nhave full control over the order (which can be made probabilistic) in which updates are\nsent from various clients.\n\nTo generate a FRED run, a user implements a Server class with a specific interface.\nThe implemented class governs how and when gradients will be applied to the Server Parameters.\nMore specifically, the Server class must implement an initialization function and an apply-update function.\n\nThe initialization function creates various data structures for keeping track of what's going on.\nFor instance, an FASGD server needs data structures to track gradient statistics.\n\nThe apply-update function takes 3 arguments: a gradient update, the client ID for the client that generated\nthe update, and the time stamp of the parameters used by the client to generate the update.\nThen the update is applied to the parameters in the appropriate way.\nFor instance, in a synchronous SGD server, the apply update function is implemented with the\nfollowing Python code:\n\n\\begin{verbatim}\n\ndef apply_update(self, grads, timestamp, client):\n\n        unblock = False\n        self.pending_grads[client] = grads\n\n        if len(self.pending_grads) == self.clients:\n\n            # all grads is a list of lists of grads\n            # each of which has the same length\n            all_grads = self.pending_grads.values()\n\n            # apply the param update\n            for this_grad in all_grads:\n                for g, p in zip(this_grad, self.params):\n                    old_p = p.get_value()\n                    mod = g / self.clients \n                    p.set_value(old_p - self.learning_rate * mod)\n\n            self.timestamp += 1 # weights have changed\n            unblock = True\n            self.pending_grads = {}\n\n        return self.params, self.timestamp, unblock\n\\end{verbatim}\n\nSpecifying the behavior of those two functions uniquely defines the behavior of a\nFRED Server.\n\nThe Server is then hooked up to a Dispatcher and a group of Clients.\nThe Dispatcher will manage the simulation of which gradient update come from what client when.\nThis simulation takes as an argument a rule determining each client's probability of being selected\nand how that probability will change upon that client having been selected.\nThe dispatcher will also take as arguments functions that determine whether and how it will fetch and/or push\nparameters and gradients, respectively.\n\nWe've made at least one opinionated decision about the library that merits further discussion.\nAfter a client pushes gradients to a server, it will wait for the resulting parameter update (or non-update) before\nresuming its computation. This is good in some ways and bad in others. If the overhead of applying the updates is\nsmall relative to that of computing the gradients, and/or if we care a lot about staleness, we may as well wait.\nIf we cared a lot about throughput instead, we might just work with whatever the most recently computed set of parameters is.\nWe may decide to make this configurable, or to come up with some more general framework in which the configurability\nof this particular decision comes for free.\n\n\\vspace{-0.1mm}\n\n\\section{Experimental Results}\n\\label{expts}\n\nIn what follows, we discuss experimental results generated using FRED.\nWe compare FASGD to SASGD on the MNIST task from \\cite{MNIST} in a variety of contexts.\nWe also compare FASGD to B-FASGD.\n\n\\subsection{FASGD}\n\nWe test the FASGD protocol using a 2 layer MLP trained on the MNIST task.\nThe MLP has 200 hidden units.\nThese use a relu activation, and the cost is negative log likelihood,\nas is standard.\n\nResults are shown in figure \\ref{costfig}, and are generated in the following way:\nWe considered 4 combinations of $\\mu$ and $\\lambda$: namely $\\mu=1, \\lambda=128$, \n$\\mu=4, \\lambda=32$, $\\mu=8, \\lambda=16$, and $\\mu=32, \\lambda=4$.\nThe product $\\mu \\lambda$ was kept constant in order to keep convergence performance\nrelatively similar across all 4 runs as advised in \\cite{Tradeoffs}.\nWe separately choose the best learning rate (across the set of 4 combinations)\nfor each of FASGD and SASGD from a pool of 16 candidate learning rates.\nThat rate was 0.005 for FASGD and 0.04 for SASGD, as can be seen from the labels in the figure.\nEach configuration ran for 100,000 iterations, \nwhere each iteration corresponds to a client calclulating\na gradient estimate for a single minibatch.\nFASGD performs meaningfully better (converging faster and to a better cost)\nregardless of $\\mu$ and $\\lambda$.\nThis difference is also robust with respect to different random\ninitializations.\nFor the sake of fairness, we did not tune the FASGD hyper-parameters.\nHad we done so, we expect that FASGD would have outperformed SASGD by\na larger margin.\n\nSome of these $\\mu$ values might seem small, but they were chosen for comparability\nwith experiments from \\cite{Suyog}. Moreover, work by \\cite{winograd} suggests that, at least\nfor convolutional networks, good throughput should be achievable for minibatch sizes between\n1 and 64. It's also worth mentioning that the next experiment shows FASGD outperforming SASGD for $\\mu = 128$,\nwhich is a completely standard minibatch size.\n\n\\begin{figure}[ht!]\n  \\centering\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/batch1_clients128_cost.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/batch4_clients32_cost.png}\n  \\end{minipage} \n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/batch8_clients16_cost.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/batch32_clients4_cost.png}\n  \\end{minipage}\n  \\caption{MNIST Validation cost for FASGD (Blue) and SASGD (Green).\n  FASGD outperforms SASGD for all tested combinations of $\\mu$ and $\\lambda$. \n  Left to right and top to bottom, the batch sizes are 1, 4, 8, and 32.\n  The product of $\\mu$ and $\\lambda$ is held constant at 128.\n  }\n  \\label{costfig} \n\\end{figure}\n\nWe also tested how the difference between FASGD and SASGD changed as a function of $\\lambda$.\nThe results are displayed in figure \\ref{clientfig}.\nWe tried $\\lambda$ values of 250, 500, 1000, and 10,000.\nThis was with $\\mu = 128$ and the same learning rates from the first experiment.\nWe used the ASGD policy described earlier.\nFASGD beats SASGD in all instances, but the relative outperformance increases as $\\lambda$ goes up.\nSince overall staleness increases as $\\lambda$ goes up, this provides evidence in favor of our hypothesis\nthat FASGD helps more when staleness is higher.\nThese results also suggest that FASGD might allow improved speed-up in distributed training contexts\nby helping us to increase $\\lambda$.\n\n\\begin{figure}[ht!]\n  \\centering\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/250clients_128bsz_cost.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/500clients_128bsz_cost.png}\n  \\end{minipage} \n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/1000clients_128bsz_cost.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/10000clients_128bsz_cost.png}\n  \\end{minipage}\n  \\caption{MNIST Validation cost for FASGD (Blue) and SASGD (Green),\n    tested with different $\\lambda$ values.\n    Left to right and top to bottom, the $\\lambda$ values used are 250, 500,\n    1000, 10000.\n  }\n  \\label{clientfig} \n\\end{figure}\n\n\\subsection{Bandwidth Aware FASGD}\n\nWe use the same model from before, also on MNIST.\nIn this section, however, we are interested in how much we can reduce bandwidth\nusage while still achieving good convergence performance.\n\nWe use the B-FASGD protocol defined earlier with the standard FASGD model as a baseline - we\nshow results in figure \\ref{bandwidthfig}.\n\nAs above, we divide bandwidth usage into copies of the parameters from the server to a client and\ncopies of the gradients from a client to the server (fetches and pushes, respectively).\nWe find that fetch usage can be reduced significantly without seriously impacting performance,\nwhile reducing push usage quickly causes training to diverge.\n\nIn particular, the experiments show that it's possible to achieve reduction of a factor of\n10 in fetch communication (corresponding to a reduction of a factor of 5 in total bandwidth use),\nbut that attempts to reduce push communication even a small amount were less successful.\nIt's possible that this is an artifact of our simplistic method for dropping gradient updates,\nand that some more sophisticated method would do better.\n\nOne thing that merits special attention is that the graphs of copies versus potential\ncopies have negative `second derivatives'. This suggests that\nit makes sense to modulate bandwidth usage by gradient statistics.\nIf the system is bottlenecked by bandwidth early in training, it doesn't need to be\nbottlenecked later in training.\nIt also suggests an interesting practical application - more clients could\nbe dynamically provisioned as training progresses, keeping bandwidth usage steady.\n\n\\begin{figure}[ht!]\n  \\centering\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/one_million_cost_graph_copies_kfetch.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/one_million_parameter_copies_kfetch.png}\n  \\end{minipage} \n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/one_million_cost_graph_copies_kpush.png}\n  \\end{minipage}\n  \\begin{minipage}[b]{0.5\\linewidth}\n    \n    \\includegraphics[scale=0.35]{figures/one_million_gradient_copies_kpush.png}\n  \\end{minipage}\n  \\caption{\n    Convergence and bandwidth usage (both push and fetch)\n    of B-FASGD for a variety of c-values.\n    The top row shows the results of modulating just kfetch.\n    The bottom row shows the results of modulating just kpush.\n  }\n  \\label{bandwidthfig} \n\\end{figure}\n\n\\vspace{-2.5mm}\n\\section{Future Work}\n\\vspace{-1mm}\n\nWe are excited to explore further extensions to this work.\n\nWe would like to expand B-FASGD by synchronizing parameters on a per-tensor basis.\nSince we are dynamically choosing when to synchronize using per-parameter moving averages,\nwe could choose to update parameters more frequently when their moving averages\nindicate a higher likelihood of staleness, and vice versa.\n\nIt could potentially be interesting to fix a bandwidth budget and use the gradient\nstatistics to dynamically allocate portions of that budget to different tensors\naccording to likelihood of staleness.\n\nWe would also like to explore why convergence performance is so much more sensitive\nto changes in the push rate than it is to changes in the fetch rate.\n\nFinally, it would be nice to give convergence gurarantees of the form in \\cite{Lian} for FASGD.\nWe would further like to more formally characterize the trade-off between bandwidth usage and convergence,\nand the trade-off between accuracy of modulation and computation cost.\nThese last two strike us as fundamental questions worth more consideration.\nThere may be some interplay between this and formal work in distributed systems.\n\n\\vspace{-2.5mm}\n\\section{Conclusion}\n\\vspace{-1mm}\n\nIn summary, the key technical contributions of this work are as follows:\n\n\\begin{itemize}\n\n\\item We introduce a novel algorithm (FASGD) for asynchronous gradient descent that converges faster and to lower cost than\nthe current state of the art. \n\n\\item We demonstrate that an extension of these ideas can be used to significantly reduce bandwidth costs in a distributed context.\n\n\\item We provide an open source framework for evaluating distributed training algorithms in a deterministic way on a single machine.\n\n\\end{itemize}\n\nWe hope that this work will seriously accelerate distributed training of large, deep neural networks in\nall contexts,\nbut we expect it to be especially important in contexts where the staleness distribution is poorly behaved.\nFor instance, when the training cluster is large and heterogenous, we expect FASGD to outperform SASGD even more.\nFASGD could also be useful for distributed training of conditional networks as in \\cite{Conditional}.\nIf only certain tensors are updated for a given input,\nit may be beneficial to selectively send per-tensor updates over the network. \n\n{\\bf Acknowledgments}: We thank the developers of Theano (\\cite{Theano}).\nWe also thank Yoshua Bengio for helpful discussions and feedback.\n\n\\newpage\n\n\\nocite{*}\n\\bibliography{fasgd}\n\\bibliographystyle{fasgd}\n\n\n", "itemtype": "equation", "pos": 14674, "prevtext": "\n\n  where $k$ is the client identifier.  \n  \n\\end{itemize}\nIt seems there ought to be a more principled relationship between the moving average\nwindow and $\\lambda$, but as shown in the Experimental Results section, we\nare able to acheive good performance results with this heuristic method.\n\nFrom an intutive perspective, it would be nice to understand why\ndividing the learning rate by the standard deviation helps us.\nOne way to think of this is as follows:\nIf the Hessian has large values, we expect big changes from gradient to gradient,\nso we should see high gradient standard deviation.\nWe also expect higher B-Staleness in this instance,\nso dividing by a moving average of gradient standard deviation seems likely to help convergence.\n\nWe can also consider the potential for gradients to cancel each other out.\nIf gradients cancel each other out, they must have changed sign.\nWhere there are sign changes, there is likely to be higher gradient\nstandard deviation (or the gradients were small to begin with).\nThus, dividing by the standard deviation ought to account for cancellation effects.\n\nOverall, FASGD should lead to better convergence than SASGD for the following two reasons:\nFirst, it will allow us to keep the learning rate high when B-Staleness is less than step-staleness estimates.\nSecond, it will prevent us from overshooting when B-Staleness is higher than step-stalensss estimates.\n\n\\subsection{Bandwidth Aware FASGD}\n\nIn \\cite{Dean}, they suggest reducing bandwidth consumption of distributed training algorithms\nby limiting each model replica to request updated parameters only every $k_{fetch}$ steps and send updated gradient\nvalues only every $k_{push}$ steps (where $k_{fetch}$ might not be equal to $k_{push}$).\nThis approach works to some extent, but it has two significant downsides.\nFirst, the distribution of bandwidth consumption will be somewhat peaky.\nThis is suboptimal from a throughput perspective.\nSecond, the amount of achievable bandwidth reduction will depend on B-Staleness,\nbut this method fixes an amount at the beginning of training and sticks with it.\n\nWe attempt to fix both of those problems as follows.\nFirst, we make the choice about whether to push or fetch at any given time a probabilistic choice.\nThat is, when a client has to decide whether to push or fetch, it generates a psuedo-random number and\ncompares that number to some other quantity.\nIf the pseudo-random number is greater than that quantity, the data is dropped - otherwise, it is transmitted.\nSecond, we modulate the likelihood of deciding to push or fetch at a given opportunity by the\nmoving average of gradient standard deviation already being computed for the FASGD algorithm.\n\nMore formally, given an opportunity to transmit a gradient or receive a parameter update,\na client will do so if and only if\n\n\n", "index": 17, "text": "\\begin{equation} \\label{req}\n  r < \\frac{1}{1 + \\frac{c}{v + \\epsilon}} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"r&lt;\\frac{1}{1+\\frac{c}{v+\\epsilon}}\" display=\"block\"><mrow><mi>r</mi><mo>&lt;</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mfrac><mi>c</mi><mrow><mi>v</mi><mo>+</mo><mi>\u03f5</mi></mrow></mfrac></mrow></mfrac></mrow></math>", "type": "latex"}]