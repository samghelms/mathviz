[{"file": "1601.05030.tex", "nexttext": "\nIntuitively the hinge embedding loss penalizes positive pairs\nthat have large distance and negative\npairs that have small distance (less than $\\mu$).\n\nHowever as observed in \\cite{simo2015deepdesc}, the majority of the\nnegative patch pairs ($\\mathcal{L}=-1$) do not contribute to\nthe update of the gradients in the optimization process as their distance is already larger than $\\mu$ parameter in\nEq. \\eqref{eq:hinge-embedding-loss}. To address this issue hard negative mining was proposed to include more negative pairs in the training. The hardest negative training pairs were identified by their distance and a subset of these examples were re-fed to the network for gradient update in each iteration. \n\n\\subsubsection{SoftPN loss}\nWe extend the idea of hard negative mining by incorporating both, positive and negative examples simultaneously in a new loss function.\n It uses a triplet of patches where two pairs represent a form of\n soft negative mining without the need for extra back propagation of specific hard negatives trough the network \\cite{simo2015deepdesc}. \n\n Any training triplet $\\mathcal{T}=\\{p_1,p_2,n\\}$  includes two negative $\\Delta(p_1,n) \\;,\\; \\Delta(p_2,n)$ and one positive \n$\\Delta(p_1,p_2)$ distance. Our proposed formulation of the loss function arises from the triplet CNN architecture \nillustrated  in Figure \\ref{fig:architectures} (bottom). \n\nIt is based on the intuitive\nidea that the smallest {\\em  negative} distance within the triplet \nshould be larger than the {\\em positive} distance.  Ideally,\nwe require the positive distance to reach $0$ and the\ntwo negative distances to increase towards $+\\infty$. Therefore the {\\em\n  smaller} negative distance is the soft negative. Identification of such pair requires much less computation than mining of hard negatives and back propagating them through the network after every iteration. \nFormally our SoftPN objective is \n\n", "itemtype": "equation", "pos": 17391, "prevtext": "\n\n\n\\title{PN-Net:  Conjoined Triple Deep Network for \\\\Learning Local Image Descriptors}\n\n\\author{Vassileios Balntas\\\\\nUniversity of Surrey\\\\\n{\\tt\\small v.balntas@surrey.ac.uk}\n\n\n\n\n\n\\and\nEdward Johns\\\\\nImperial College London\\\\\n{\\tt\\small  e.johns@imperial.ac.uk}\n\\and\nLilian Tang\\\\\nUniversity of Surrey\\\\\n{\\tt\\small h.tang@surrey.ac.uk}\n\\and\nKrystian Mikolajczyk \\\\\nImperial College London\\\\\n{\\tt\\small k.mikolajczyk@imperial.ac.uk}\n}\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nIn this paper we propose a new approach for learning local descriptors for matching  image patches. \nIt has recently been demonstrated that descriptors based on\nconvolutional neural networks (CNN) can significantly improve the matching performance. Unfortunately  their computational complexity is prohibitive for any practical application. We address this problem and\npropose a CNN based descriptor with improved matching performance, significantly reduced training and execution time, as well as low dimensionality. We propose to train the network with triplets of patches that include a positive and negative pairs. To that end we introduce a new loss function that exploits  the relations within the triplets.\nWe compare our approach to recently introduced MatchNet and\nDeepCompare and demonstrate the advantages of our descriptor in terms\nof performance, memory footprint and speed i.e. when run in GPU, the\n extraction time of our 128 dimensional feature is comparable to the fastest available\nbinary descriptors such as BRIEF and ORB. \n\\end{abstract}\n\n\n\n\\section{Introduction}\n\nFinding correspondences between images via local descriptors is one of the most extensively studied problems in computer vision due to the wide range of  applications. The field has witnessed several breakthroughs in this area such as SIFT~\\cite{Lowe:2004:DIF:993451.996342}, invariant region detectors~\\cite{mikolajczykIJCV2004}, fast binary descriptors~\\cite{Calonder:2010:BBR:1888089.1888148}, optimised descriptor parameters \\cite{WHB09,simonyan2014learning}  which have made a significant and wide impact in various computer vision tasks.  \nRecently end-to-end learnt descriptors~\\cite{FDB14,simo2015deepdesc,ZagoruykoCVPR2015,Han_2015_CVPR} based on CNN architectures were demonstrated to significantly outperform state of the art features. This was a natural adoption of CNN  to local descriptors as deep learning had already been shown to significantly improve in many computer vision areas \\cite{lecun2015deep}. \nHowever, the performance improvements with CNN based descriptors come at the cost  in terms of of extensive training time, computation,  size of the annotated data as well as significantly larger dimensionality of the feature vector. For example, days of training with GPU on 100s of thousands of training patches  are reported in \\cite{FDB14,simo2015deepdesc,ZagoruykoCVPR2015,Han_2015_CVPR}, and descriptor dimensionality reaches up to 4096. Moreover, slow execution time i.e. descriptor extraction, even using GPUs, negatively outweighs the benefits of improved matching.  Even though the efficiency and dimensionality is improving with new methods, it is still far off descriptors such as BRIEF i.e. 8 bytes, 3$\\mu s$ per descriptor. \n\nAnother issue in the area of matching patches is the limited benchmark data. Typically used Oxford data~\\cite{schmid2003performance} was designed a decade ago and can be considered very small for today's standards. Also, its original evaluation protocol included detection of interest points which currently are less often used in the evaluations. Hence, different protocols and evaluation measures are adopted in various papers which make a comparative study inaccurate. Patch data from~\\cite{BHW10} with well defined groundtruth is more convenient to use but the reported error has decreased  significantly over past years such that the margin for improvement is very small. Furthermore the training and testing is done on the data from similar distribution and over-fitting may occur.\n\nIn this paper we propose a CNN based descriptor that improves the performance of recent methods, reduces  matching error from $26\\%$ (SIFT) to $\\approx 7\\%$, it is of the same dimensionality as SIFT, its extraction time is 40 times faster than SIFT and only 3 times slower than BRIEF, and single epoch training time is 2$min$. We propose to train the network with {\\em Positive} and {\\em Negative} pairs formed by triplets of patches, hence our network is termed PN-Net. We introduce a new loss function, which we call SoftPN, to simultaneously exploit the constraints given by the positive and negative pairs. We compare our network and the loss function to other CNN based approaches and demonstrate the improvements. We extend the Oxford data with new image sequences and modify the evaluation protocol such that the data can be used in a similar way to the one from~\\cite{WHB09}, yet it preserves the advantages of having the various type of noise separated for more detailed analysis of descriptors. Together with the patch data it allows to test the generalisation properties of the evaluated methods. We perform extensive evaluation and comparison to the state of the art descriptors and demonstrate the improvements in matching performance, extraction efficiency, dimensionality, and training time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Related work}\n\n\n\nThe design and implementation of local descriptors has undergone a remarkable evolution over the past two decades ranging from differential or moment invariants, correlations, histograms of gradients or other measurements, PCA projected patches etc. An overview of  pre-2005 descriptors with SIFT \\cite{Lowe:2004:DIF:993451.996342} identified as the top performer can be found in \\cite{schmid2003performance}. Its benchmark data accelerated the progress in this field and there have been a number of notable contributions, including recent DSP-SIFT \n\\cite{DBLP:journals/corr/DongS14}, falling into the same category of descriptors as SIFT but the improvements were not sufficient to replace SIFT in various applications. The research focus shifted to improve the speed and memory footprint e.g. as in BRIEF \\cite{Calonder:2010:BBR:1888089.1888148} and the follow up efforts.  Introduction of datasets with correspondence ground truth \\cite{WHB09} stimulated development of learning based descriptors which try to optimise descriptor parameters and learn projections or distance metrics \\cite{BHW10,simonyan2014learning} for better matching. \n\nEnd-to-end learning of patch descriptors using CNN has been attempted in several works \\cite{FDB14,ZagoruykoCVPR2015,simo2015deepdesc,Han_2015_CVPR} and consistent improvements were reported over the state of the art descriptors.  It was shown in \\cite{FDB14} that the\nfeatures from the last layer of a convolutional deep network trained\non ImageNet \\cite{ILSVRC15} can outperform  SIFT. Furthermore,  training a siamese deep network with hinge loss in \\cite{ZagoruykoCVPR2015,simo2015deepdesc,Han_2015_CVPR} (two CNN's sharing the same\nweights) based on positive and negative\npatch pairs, resulted in significant improvements in matching performance. Explicit metric learning is often performed in such descriptors to classify similar and dissimilar pairs. This may lead to sub-optimal performance if such learnt representation is used as a descriptor for a different task.  \n \nIt is well known that careful selection of training data may lead to significant performance increase.\nInspired by relevant methods in SVM based classifiers \\cite{lsvm-pami}, the approach from \\cite{simo2015deepdesc} proposes to improve learning by  mining  and re-training the network with hard training examples.\nMany similar ideas can be found in the area of distance metric learning \\cite{Bellet2014} which also exploits various methods of sampling data points to train better projections. In particular Linear Discriminant Embedding, Marginal Fisher Analysis, Neighbourhood Component Analysis or Large Margin Nearest Neighbour focus on exploiting nearest neighbours in training examples and their relations rather than treating all data points equally. A notable example in the context of local descriptors is the  nearest neighbour ratio \\cite{Lowe:2004:DIF:993451.996342}  used for matching instead of the absolute Euclidean distance. Similarly, bootstrapping techniques often rely on identifying false positives and false negatives and improve classifiers by re-training on those.\nThe idea of guiding the learning process simultaneously by positive\n and negative constraints was successful exploited in PN-Learning \\cite{kalalCVPR2010} in patch based online learnt object detector. \n \nWe build on the top of these results and propose PN-Net that exploits\nthe positive and negative relations within triplets of training\nexamples in contrast to pairs in siamese networks. A similar idea was\nrecently investigated in the context of image categorisation into 10\nobject classes, but the reported improvements were marginal~\\cite{DBLP:journals/corr/HofferA14}. Our triplet network structure and\nthe loss function is different and a patch can be considered as an\nindependent object class, thus the typical matching problem  includes 1000s of such classes. We also\ndesign a new loss function termed {\\em SoftPN}, which is inspired by SoftMax ratio and hard negative mining from~\\cite{simo2015deepdesc}. As we demonstrate in the experiments our method leads to significant improvements in terms of matching performance, dimensionality,  and both training and test speed. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{PN-Net}\nIn this section, we present our feature descriptor learning method,\nand we give a brief analysis of its strengths against the previously\nused CNN architectures and loss functions.\n\n\\subsection{Overview of the network architecture}\nOur goal is to compute a representation vector $D(p) \\in \\mathbb{R}^{\\mathcal{D}}$ of image  \n patch $p \\in \\mathbb{R}^{N \\times N}$. Descriptor vector $D(p)$ results from the final layer\nof a convolutional neural network where the layer size matches the feature dimensionality. \n\nIn contrast to \\cite{Han_2015_CVPR} or \\cite{ZagoruykoCVPR2015}, which include metric learning, our goal is to generate descriptors that can be used in traditional matching setup i.e. with the $L_2$ distance. This has the advantage of opening the application range to\nvarious well-studied techniques such as KD-Trees or approximate nearest neighbour search. \n\nPrevious work on deep learning of feature descriptors has\nbeen based on the {\\em siamese networks} as illustrated in Figure \\ref{fig:architectures}(top). Such networks\nconsist of two CNNs which accept two parallel inputs and share parameters across networks. The loss function is optimised based on\nthe output of the two networks according to their distinct inputs. A single distance between a pair of patches is considered for every training example. This architecture is used to extract descriptors showing state-of-the art matching performance in \\cite{simo2015deepdesc,ZagoruykoCVPR2015}. \n\n Architectures exploiting three parallel inputs have recently been investigated in \\cite{DBLP:journals/corr/WangSLRWPCW14} and\n\\cite{DBLP:journals/corr/HofferA14} in the context of ranking and classification. Their loss function makes use of a triplet of images\n where  two images are from the same class, and one is from a different class.  Inspired by these techniques we propose an approach to learning local feature descriptors for matching patches. We make use of a triplet of patches, where two of them are positive patches from two views of the same point in the 3D space, and the third one is a negative patch extracted from a\ndifferent point in space. The loss function is then based on three distances considered simultaneously for every training example formed from the triplet. The proposed network is shown in Figure \\ref{fig:architectures}(bottom).\n Examples of the positive and negative training pairs used in previous works as well triplets exploited in our network  are shown in Figure \\ref{fig:pairs_and_triplets}.\n\n\n\n\n\n\n\n\\tikzstyle{cnn} = [draw, fill=blue!20, rectangle, \n    minimum height=3em, minimum width=3em]\n\\tikzstyle{loss} = [draw, fill=green!20, rectangle, \n    minimum height=5em, minimum width=3em]\n\n\\begin{figure}[t]\n\\centering\n\n\n\\begin{tikzpicture}[auto, node distance=2cm,>=latex']\n  \\node (patch1) at (0,0) {\\includegraphics[width=.05\\textwidth]{patch.png}};\n  \\node (patch2) [below = 0.45cm of patch1] {\\includegraphics[width=.05\\textwidth]{patch+.png}};\n\n  \\node (cnn1) [cnn, right = 0.5cm of patch1] {CNN};\n  \\node (cnn2) [cnn, below =0.5cm of cnn1] {CNN};\n\n  \\node (loss) [loss, below right=-0.5cm and 1.2cm of cnn1] {\\begin{tabular}{c}\n                                                               $Loss:\n                                                               \\;\n                                                               function\n                                                               \\;of$\\\\ \n                                                               $\\Downarrow$\\\\ \n                                                               $||D(a)-D(b)||_2$\\\\ \n                                                               $pair\n                                                               \\; label$ \\\\\n                                                             \\end{tabular}};\n\\draw [draw,<->] (cnn1) -- node {$w$} (cnn2);\n\\draw [draw,->] (patch1) -- node {$a$} (cnn1);\n\\draw [draw,->] (patch2) -- node {$b$} (cnn2);\n\\draw [draw,->] (cnn1) -- node [midway, above]  {$D(a)$} (loss);\n\\draw [draw,->] (cnn2) -- node [midway, below] {$D(b)$} (loss);\n\\end{tikzpicture}\n\n\\vspace{0.4cm}\n\n\n\\begin{tikzpicture}[auto, node distance=2cm,>=latex']\n  \\node (patch1) at (0,0) {\\includegraphics[width=.05\\textwidth]{patch.png}};\n  \\node (patch2) [below = 0.45cm of patch1] {\\includegraphics[width=.05\\textwidth]{patch+.png}};\n  \\node (patch3) [below = 0.45cm of patch2]\n  {\\includegraphics[width=.05\\textwidth]{patch-.png}};\n\n  \\node (cnn1) [cnn, right = 0.5cm of patch1] {CNN};\n  \\node (cnn2) [cnn, below =0.5cm of cnn1] {CNN};\n  \\node (cnn3) [cnn, below =0.5cm of cnn2] {CNN};\n\n  \\node (loss) [loss, below right=0.25cm and 1.5cm of cnn1] {\\begin{tabular}{c}\n                                                               $Loss:\n                                                               \\;\n                                                               function\n                                                               \\; of $\\\\\n                                                               $\\Downarrow$\\\\ \n                                                               $||D(p_1)-D(p_2)||_2$\\\\ \n                                                               $||D(p_1)-D(n)||_2$\\\\ \n                                                               $||D(p_2)-D(n)||_2$\\\\ \n\n                                                              \\end{tabular}};\n\\draw [draw,<->] (cnn1) -- node {$w$} (cnn2);\n\\draw [draw,<->] (cnn2) -- node {$w$} (cnn3);\n\\draw [draw,->] (patch1) -- node {$p_1$} (cnn1);\n\\draw [draw,->] (patch2) -- node {$p_2$} (cnn2);\n\\draw [draw,->] (patch3) -- node {$n$} (cnn3);\n\\draw [draw,->] (cnn1) -- node [midway, above]  {$D(p_1)$} (loss);\n\\draw [draw,->] (cnn2) -- node [midway, below] {$D(p_2)$} (loss);\n\\draw [draw,->] (cnn3) -- node [midway, below] {$D(n)$} (loss);\n\\end{tikzpicture}\n\n\\caption{(top) Training of the siamese architecture. The loss is computed based on the\n  distance in a positive or a negative patch pairs. (bottom) Training of the\n  proposed {\\bf PN-Net} architecture. The loss is based on all three\n   distances within the triplet of patches. For details refer to\n  Sec. \\ref{sec:conj-deep-netw}.\n} \\label{fig:architectures}\n\\end{figure}\n\n\n\\begin{figure}[t]\n    \\centering\n    \\begin{subfigure}[b]{1.0\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{pairs_data.png}\\\\\n         \\vspace{3pt}\n        \\includegraphics[width=\\columnwidth]{triplets_data.png}\n    \\end{subfigure}\n    \\caption{ Siamese networks are trained with positive and\n      negative pairs of patches (top). \n       Our network is trained with\n      triplets of patches (bottom), two extracted from the same point representing a positive match example, and\n      one from a different point in space giving  two negative\n      match examples per triplet.}\n\\label{fig:pairs_and_triplets}\n\\end{figure}\n\n\n\n\n\\subsection{Conjoined deep network loss functions}\n\\label{sec:conj-deep-netw}\n\nIn this section we first discuss the {\\em Hinge Embedding} loss\n\\cite{1640964} commonly used in conjoined deep network architectures\nand then we introduce our new loss function that is used in the optimization\nof the proposed {\\bf PN-Net} triplet based architecture.\n\n\\subsubsection{Hinge Embedding Loss}\nThe recent works related to deep learning of feature descriptors\nutilize patch pairs and the {\\em Hinge Embedding}.\n\\cite{1640964,simo2015deepdesc,ZagoruykoCVPR2015,DBLP:journals/corr/WangSLRWPCW14}. \n{\\em Hinge Embedding} criterion was also used with triplets of data points to learn pose descriptors for 3D objects \\cite{wohlhart15}.\n\nLet $\\mathcal{P}=\\{p_L,p_R\\}$ denote a patch pair and $\\mathcal{L}\\in\\{-1,1\\}$\na label indicating negative and positive pairs respectively. \n{\\em Hinge Embedding} loss is then computed\n\n", "index": 1, "text": "\\begin{equation}\n  \\label{eq:hinge-embedding-loss}\n  l(\\mathcal{P}) =\n\\left\\{\n\t\\begin{array}{ll}\n\t\t\\Delta=||D(p_{L})-D(p_{R})||_2  & \\mbox{if } \\mathcal{L}=1 \\\\\n\t\tmax(0,\\mu-||D(p_{L})-D(p_{R})||_2)  & \\mbox{if } \\mathcal{L}=-1\n\t\\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"l(\\mathcal{P})=\\left\\{\\begin{array}[]{ll}\\Delta=||D(p_{L})-D(p_{R})||_{2}&amp;%&#10;\\mbox{if }\\mathcal{L}=1\\\\&#10;max(0,\\mu-||D(p_{L})-D(p_{R})||_{2})&amp;\\mbox{if }\\mathcal{L}=-1\\end{array}\\right.\" display=\"block\"><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>L</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>R</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mrow><mi>\u03bc</mi><mo>-</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>L</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>R</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05030.tex", "nexttext": "\n\nGiven $\\Delta^{*}=min(\\Delta(p_1,n),\\Delta(p_2,n))$ as the soft negative distance, the goal of the loss function is to force $\\Delta^{*}$\nto be  larger than $\\Delta(p_1,p_2)$. Note that unlike in the {\\em\n  Hinge Embedding} loss, negative distances always contribute to the\n optimization and in contrast to the previous works on triplet based learning\n\\cite{DBLP:journals/corr/HofferA14,DBLP:journals/corr/WangSLRWPCW14},\nour negative loss includes both negative distances\n$\\Delta(p_1,n),\\Delta(p_2,n)$. \n\n\nThe {\\em SoftMax Ratio} objective introduced in \\cite{DBLP:journals/corr/HofferA14} is \nbased on triplets and ratios, but does not include the evaluation of\nthe $\\Delta^{*}$ distance inside the triplet. \nAn illustration of the the hinge loss objective\n\\cite{ZagoruykoCVPR2015,simo2015deepdesc,DBLP:journals/corr/WangSLRWPCW14}\n, the {\\em SoftMax Ratio} \\cite{DBLP:journals/corr/HofferA14}\nand the proposed SoftPN loss function is presented in Figure \\ref{fig:objectives}.\n\n\n\\begin{figure}\n    \\begin{subfigure}[b]{0.32\\columnwidth}\n        \\centering\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tikzpicture}\n              \\draw (2,2) circle (1.2cm);\n              \\draw[-, very thin] (2,2) -- (1.55,3.3) node[midway,right] {\\Large$\\Delta$};\n              \\draw [draw=black, fill=olive] (2,2) circle (0.15cm) ;\n              \\node[draw=none] at (1.6,2) {\\Large $p_L$};\n              \\draw [draw=black, fill=gray] (1.4,3.3) rectangle (1.7,3.6) ;\n              \\node[draw=none] at (2,3.7) {\\Large $p_R$};\n              \\node[draw=none] at (1.9,0.5) {\\large $ L^{-} = max(0,\\mu - \\Delta)$};\n              \\draw[-, very thin,orange] (2.15,2) -- (3.2,2)node[midway, below,black]{\\Large $ \\mu$};\n              \\draw[->, very thick] (-.1,0) -- (4,0) node[below] {$\\text{\u00ce\u0091}$};\n              \\draw[->, very thick] (0,-.1) -- (0,4) node[left]{$\\text{\u00ce\u0092}$};\n            \\end{tikzpicture}\n        }\n        \\caption*{Hinge Margin \\cite{simo2015deepdesc}}\n            \\end{subfigure}\n    \\begin{subfigure}[b]{0.32\\columnwidth}\n    \\centering\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tikzpicture}\n\n              \n\n              \\draw[-, very thin] (1,1.5) -- (3,3);\n              \\draw[dashdotted, very thin] (1,1.5) -- (1.9,3);\n              \\draw [draw=black, fill=olive] (1,1.5) circle (0.15cm) ;\n              \\node[draw=none] at (0.5,1.4) {\\Large $p_1$};\n              \\draw [draw=black, fill=olive] (3,3) circle (0.15cm) ;\n              \\node[draw=none] at (3.7,3.3) {\\Large $p_2$};\n              \\draw [draw=black, fill=gray] (1.9,3) rectangle (2.2,3.3) ;\n              \\node[draw=none] at (1.5,3.2) {\\Large $n$};\n              \\node[draw=none] at (2.3,2) {\\large $\\Delta_{+}$};\n              \\node[draw=none] at (1,2.4) {\\large $\\Delta_{-}$};\n              \\node[draw=none] at (2,0.5) {\\large $\\text{optim ratio}(\\Delta_{-},\\Delta_{+})$};\n              \\draw[->, very thick] (-.1,0) -- (4,0) node[below] {$\\text{\u00ce\u0091}$};\n              \\draw[->, very thick] (0,-.1) -- (0,4) node[left]{$\\text{\u00ce\u0092}$};\n            \\end{tikzpicture}\n        }\n        \\caption*{{\\small SoftMax Ratio \\cite{DBLP:journals/corr/HofferA14}}}   \n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.32\\columnwidth}\n        \\centering\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tikzpicture}\n  \\draw[-, very thin] (1,1.5) -- (3,3);\n              \\draw[dashdotted, very thin] (1,1.5) -- (1.9,3);\n              \\draw[dashed, very thin] (1.9,3.2) -- (3,3);\n              \\draw [draw=black, fill=olive] (1,1.5) circle (0.15cm) ;\n              \\node[draw=none] at (0.5,1.4) {\\Large $p_1$};\n              \\draw [draw=black, fill=olive] (3,3) circle (0.15cm) ;\n              \\node[draw=none] at (3.7,3.3) {\\Large $p_2$};\n              \\draw [draw=black, fill=gray] (1.9,3) rectangle (2.2,3.3) ;\n              \\node[draw=none] at (1.5,3.2) {\\Large $n$};\n              \\node[draw=none] at (2.3,2) {\\large $\\Delta_{+}$};\n              \\node[draw=none] at (1,2.4) {\\large $\\Delta_{-}$};\n              \\node[draw=none,blue] at (2.7,3.5) {\\Large $\\Delta^{*}$};\n              \\node[draw=none] at (2,0.5) {\\large $\\text{optim\n                  ratio}({\\color{blue} \\Delta^{*}},\\Delta_{+})$};\n              \\draw[->, very thick] (-.1,0) -- (4,0) node[below] {$\\text{\u00ce\u0091}$};\n              \\draw[->, very thick] (0,-.1) -- (0,4) node[left]{$\\text{\u00ce\u0092}$};\n            \\end{tikzpicture}\n        }\n        \\caption*{Our SoftPN}\n    \\end{subfigure}\n\\caption{Illustration of loss functions. Note that the proposed SoftPN\n  loss (right) evaluates the smallest negative distance $\\Delta^{*}$ within a triplet.} \n\\label{fig:objectives}\n\\end{figure}\n\nFigure \\ref{fig:deep_vs_sergey} visualises two layers of the CNN for the proposed PN-Net as well as for DeepCompare~\\cite{ZagoruykoCVPR2015}. Convolutional filters of PN-Net seem to be more smooth e.g. more regularised compared to DeepCompare. We believe it is the effect of simultaneous use of positive and negative pairs in the loss function during training.\nIn Figure \\ref{fig:deep_vs_siamese_vs_tripletnet} we compare the effect different loss functions discussed in this section have on the matching performance of learnt descriptors. \nWe plot the 95\\% error in matching patches from patch dataset \n\\cite{BHW10},   and show how it decreases with each training epoch. A first observation\nis that the triplet based learning\nis significantly better for learning feature descriptors than the state of the art\nsiamese pair based learning. \nThis is demonstrated by the large difference between the siamese \\& hinge\nmargin architecture compared to the proposed PN-Net architecture. The final error rate is 15\\% lower for SofPN. Next, by comparing results for SoftMax and SoftPN to hinge loss we conclude that using triplets of data points as training examples leads to much better results than using pairs. \n\nWe\nalso note that the proposed SoftPN loss function outperforms SoftMax Ratio function, due to the soft negative mining by\n$\\Delta^{*}$ distance.  \nMoreover,  already the first epoch (\\ie after $2 mins$) of training a local feature descriptor with the proposed\nmethod  leads to  the matching error rate of $9\\%$ when training the network with\nthe Liberty dataset and testing in the Notredame dataset. This is much lower error than many recent descriptors achieve after extensive training as we show in section \\ref{s:results}. \n\n\n\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{pnnet_fl1.png}\n    \\end{subfigure}\n    \\hspace{0.2cm}\n    \\begin{subfigure}[b]{0.64\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{pnnet_fl2.png}\n    \\end{subfigure}\n    \\vspace{0.05cm}\n\n    \\hspace{-0.2cm}\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\raisebox{0.6ex}{\\includegraphics[width=\\columnwidth]{dc_fl1.png}}\n    \\end{subfigure}\n    \\hspace{0.2cm}\n    \\begin{subfigure}[b]{0.635\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{dc_fl2.png}\n    \\end{subfigure}\n    \\caption{ The weights learned in the first layer (left)  and in\n      the second layer (right)  of the\n      CNN. (Top) is our PN-Net and (bottom) is DeepCompare.}\n\\label{fig:deep_vs_sergey}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}[b]{0.5\\columnwidth}\n        \\centering\n        \\includegraphics[trim=15 1 15 1,width=\\columnwidth]{deep3_vs_siamese.pdf}\n        \n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.5\\columnwidth}\n        \\centering\n        \\includegraphics[trim=15 1 15 1,width=\\columnwidth]{deep3_vs_tripletnet.pdf}\n    \\end{subfigure}\n    \\caption{(left) The proposed SoftPN loss compared with the Hinge\n      Margin Loss used commonly in learning deep descriptors, when\n      using the same underlying CNN (Table\n      \\ref{tab:cnn-architecture}), and trained on exactly the same\n      data. Note the significant improvements of performance.(right)\n      The effect of the local negative mining inside each triplet. We\n      see that the use of the soft negative $\\Delta{*}$ distance\n    inside each triplet leads to lower error rates.}\n\\label{fig:deep_vs_siamese_vs_tripletnet}\n\\end{figure}\n\nAnother important note is that for training and comparing different loss functions we use the same set of patches.\nThere are three times more pairs in this set than triplets therefore the siamese network with hinge loss uses three times more training examples than TripletNet with SoftMax and PN-Net with SoftPN. This is a significant advantage for the siamese network, yet the results indicate that the training is much more effective with triplets. It is also more efficient due to use of less training examples. \n\n\n\n\\subsection{Implementation details}\n\nPN-Net and the siamese networks used the same\nunderlying CNN  in the experiments\nabove. Also, unlike in\n\\cite{ZagoruykoCVPR2015} and \\cite{Han_2015_CVPR} there is no extra\nlayer that learns a distance metric between the two patches. Our simplified CNN architecture allows more efficient descriptor extraction and the use of $L_2$ norm for matching. \nThus, a  descriptor is obtained by processing a patch by a single branch of the network.\nThe parameters of the network used in all our experiments are presented in Table\n\\ref{tab:cnn-architecture}. \n\n\n\\begin{table}[h]\n  \\begin{center}\n    \\caption{Architecture of the CNN used in the experiments. Patches\n      of size $32\\times 32$ are used as input. The number in each\n      convolutional layer denotes the number of the output planes that\n    the convolution produces.}\n    \\label{tab:cnn-architecture}\n    \\begin{tabular}{cc}\n      \\toprule\n      Layer \\# & Description\\\\\n      \\midrule\n      1 & Spatial Convolution(7,7) $\\rightarrow$ 32 \\\\\n      2 & Tanh \\\\\n      2 & MaxPooling(2,2) \\\\\n      3 & Spatial Convolution(6,6) $\\rightarrow$ 64 \\\\\n      4 & Tanh \\\\\n      5 & Linear $\\rightarrow$ $\\{128,256\\}$ \\\\\n      6 & Tanh \\\\\n      \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table}\n\nOur implementation is in \\texttt{Torch} \\cite{collobert:2011c}. The\ntraining is done using $\\approx 1.2M$ triplets generated on-the-fly\nusing the patches from \\cite{BHW10}. In contrast to how CNNs are typically trained including DeepCompare and MatchNet we do not use data augmentation. This is to make the training more efficient and to demonstrate that the improvements result from the approach and not from larger number of patches used for training.\n\n\n  \nIn forming the triplets we choose\nrandomly a pair of patches from the same 3D point, and subsequently we\ncomplete the triplet with a randomly chosen patch from another 3D\npoint. This is in contrast to other works where carefully designed\nschemes of choosing the training data are used in order to enhance the\nperformance~\\cite{DBLP:journals/corr/WangSLRWPCW14,Han_2015_CVPR}.\n\nFor the optimization the\nStochastic Gradient Descend \\cite{bottou-tricks-2012} is used, and the\ntraining is done in batches of $128$ items, with a learning\nrate of $0.1$, momentum of $0.9$ and weight decay of $10^{-6}$.\n\nThe convolution\nmethods are  from the NVIDIA cuDNN library\n\\cite{DBLP:journals/corr/ChetlurWVCTCS14}. \nThe training of a single epoch with $\\approx 1.2M$ training triplets\ntakes approximately $2$ minutes in an NVIDIA Titan X GPU.\n\nIt is worth noting that the CNN used in our experiments consists of\nonly two layers, while all of the other state-of-the art deep feature\ndescriptors consist of 3 layers and above\n\\cite{ZagoruykoCVPR2015,simo2015deepdesc,Han_2015_CVPR}. Several other\nimplementation variants could be added such as using different\nnon-linearity layers (e.g. ReLU as in\n\\cite{Han_2015_CVPR,ZagoruykoCVPR2015}), extra normalization layers,\nbut the main focus of our work is to show the effect of learning local\nfeatures with triplets coupled with the SoftPN loss function. Sample results\nfor other network configurations are presented in the supplementary material.\n\n\n\n\n\n\n\n\\section{Experimental evaluations}\n\\label{s:results}\nIn this section we evaluate the proposed local feature descriptor within the two most popular benchmarks in the field of local descriptor matching. We compare our method to SIFT \\cite{Lowe:2004:DIF:993451.996342}, Convex optimization \\cite{simonyan2014learning} the recently introduced MatchNet \\cite{Han_2015_CVPR}\nand DeepCompare \\cite{ZagoruykoCVPR2015} descriptors which are\ncurrently the state of the art in terms of matching accuracy.  \nThe original code available from the authors was used in all the experiments. Code for other  recent methods e.g. DSP-SIFT was not available at the time of this experiment.\n \nNote that for a fair comparison, we use the siamese architectures similar to the one in\nDeepCompare, but we do  not use the multi-scale {\\em 2ch architectures}. Multi-scale approaches uses multiple patches from each example, with one\nbeing a cropped sub-patch around the center. This introduces\ninformation from different samples in the size-space and it has been shown to lead to significant improvements in terms of\nmatching accuracy \\cite{DBLP:journals/corr/DongS14}. Such approach can be uses for various descriptors (e.g. MatchNet-2ch, PN-Net-2ch etc.).\n\nIt would be interesting to evaluate the effect of mining in terms of a\nsiamese network learning as it was proposed in\n\\cite{simo2015deepdesc}, however the implementation is not available\nyet.\n\n\\begin{table*}[ht]\n\\caption {Results form the Photo-Tour dataset \\cite{BHW10}. Numbers\n  are reported in terms of {\\em error at 95\\% correct}. {\\bf Bold}\n  numbers indicate the best performing descriptor. Note the\n  significant reduction in dimensionality by {\\bf PN-Net}, together\n  with the improvements over the state-of-the art results.} \n\\label{tab:benchmark_brown} \n\\begin{tabular}{ccccccccc}\n\\toprule \n    Training& & Notredame & Liberty & Notredame  & Yosemite & Yosemite\n  & Liberty  & \\\\\n    \n    Testing&  & \\multicolumn{2}{c}{Yosemite} & \\multicolumn{2}{c}{Liberty} & \\multicolumn{2}{c}{Notredame} \\\\\n    \\midrule\n    Descriptor & \\# features & & & & & & &mean  \\\\\n    \\midrule\n    SIFT \\cite{Lowe:2004:DIF:993451.996342} & 128 & \\multicolumn{2}{c}{27.29} & \\multicolumn{2}{c}{29.84}\n                                    & \\multicolumn{2}{c}{22.53} &\n                                                                  26.55 \\\\\n    ConvexOpt \\cite{simonyan2014learning} & $\\approx80$ & 10.08 & 11.63\n                                    & 11.42 & 14.58 & 7.22 & 6.17 &\n                                                                    10.28 \\\\\n    DeepCompare {\\em siam} \\cite{ZagoruykoCVPR2015}& 256 &\n                                                                      13.21\n                          & 14.89 & 8.77 & 13.48 & 8.38 & 6.01 & 10.07\n  \\\\\n     \\hspace{1cm} {\\em pseudo-siam} \\cite{ZagoruykoCVPR2015}& 256 &\n                                                                      12.64\n                          & 12.5 & 12.87 & 10.35 & 5.44 & 3.93 & 9.62\\\\\n    MatchNet \\cite{Han_2015_CVPR} & 512 & 11 & 13.58\n                                    & 8.84 & 13.02 & 7.7 & 4.75 & 9.82\\\\\n    \\hspace{1cm}  {\\em no bottleneck} \\cite{Han_2015_CVPR} & 4096 & 8.39 & 10.88\n                                    & {\\bf 6.90} & 10.77 & 5.76 & 3.87 & 7.75\\\\\n  {\\bf PN-Net} & {\\bf 128} & {\\bf 7.74}& {\\bf 9.55}\n                                    & 8.27  & {\\bf 9.76} & {\\bf 4.45}\n  & {\\bf 3.81} & {\\bf 7.26}\\\\\n {\\bf PN-Net} & {\\bf 256} & {\\bf 7.21 }& {\\bf 8.99}\n                                    & 8.13  & {\\bf 9.65 } & {\\bf 4.23 }\n  & {\\bf 3.71} & {\\bf 6.98}\\\\\n    \\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\n\\subsection{Photo Tour dataset}\nWe first evaluate the performance in terms of matching accuracy in\ndistinguishing positive from negative patch pairs on the Photo Tour\ndataset \\cite{BHW10}. This dataset consists of three subsets {\\em\n  Liberty},{\\em Yosemite} \\& {\\em Notredame} containing more than 500k patch pairs extracted around specific feature points.\nWe follow the protocol proposed in  \\cite{BHW10}\nwhere the ROC curve is generated by thesholding the distance scores\nbetween patch pairs. The number reported here is the false positive\nrate at 95\\% true positive rate. For the evaluation we use the $100K$\npatch pairs proposed by the authors.\n\n\n\n\n\n\n\nThe results for each of the combinations of training and testing using\nthe three subsets of the Photo Tour dataset are shown in\nTable~\\ref{tab:benchmark_brown}. The average\nacross all possible combinations is also shown.\nOur PN-Net outperforms the state of the art for a single scale siamese CNN\narchitecture (MatchNet). Moreover, our network does not learn an explicit distance metric and the final layer gives 128 dimensional descriptor in contrast to 4096 of MatchNet. Note that the \nperformance gain is even greater when comparing to DeepCompare descriptors from\n\\cite{ZagoruykoCVPR2015}, event though their dimensionality is still twice larger than from PN-Net. Moreover, our descriptor\noutperforms all the others, except in the combination of training in\nNotredame and testing in Liberty. \n\n\n\nWe can also observe that there is not much difference in \nperformance when comparing the $128$ with the $256$ variants of\nPN-Net. It is important to mention that the proposed descriptor achieves state of\nthe art performance without any data augmentation during training,\n in contrast to the competing CNN based methods \\cite{ZagoruykoCVPR2015,Han_2015_CVPR}. \nSmaller network and no data augmentation or multi-scale examples leads to much faster learning, yet  our PN-Net and SoftPN loss achieves   similar or better performance after 1 epoch ($2min$) than DeepCompare \\cite{ZagoruykoCVPR2015}   after 2 days of training. \n\nFull plots of the ROC curves can be found in the supplementary\nmaterial.\n\n\n\n\\subsection{Oxford image sequences}\n\nWe also test the performance of the proposed descriptor in terms of\nmatching local features between two images based on the benchmark from\n\\cite{schmid2003performance}.\n\n\\noindent{\\bf Evaluation protocol.} As discussed in the introduction the original protocol has been loosely followed in various papers leading to not comparable results. \n\nTo address this issue, we draw ideas from the successful Photo Tour benchmark. \nTo increase the size of the dataset we complement the eight sequences with additional seven that include illumination, rotation and scale changes. The images are acquired in a similar way to the original eight with pair-wise image homographies for establishing correspondence ground truth.\nNote that unlike \\cite{FDB14}, the sequences are not generated artificially by\nwarping single images to various transformations, but\nthey come from naturally captured images together with the noise introduced by varying imaging conditions. \nNext, we apply an interest point detector to identify regions with varying image content.  \nIn contrast to the Photo Tour~\\cite{BHW10} that extracted keypoints with scale-invariant DoG, we use affine invariant Harris-Affine. This makes the patches complementary to the Photo Tour and introduces  a different type of noise resulting from the affine detector inaccuracy.\nWe establish correspondence ground truth using the homographies and the  overlap error from~\\cite{schmid2003performance}.\nWe consider two points in correspondence if the overlap error between the detected regions is less than 50\\%. Note that a region from one image can be in correspondence with several regions from the other image. \nBy reducing each image pair to a collection of patches, the benchmark will eliminate\nsome of the varying factors such as the choice of the interest point detector, the number  of extracted patches, their sizes, etc.\nThe data includes 15 sequences, each consisting of 6 images with increasing degree of change in viewing conditions.\nEach image has an associated set of ~1k patches. A pair of images has an associated text file indicating the overlap error between the patches that overlap by at least 50\\%. \nThis data will be available online and together with Photo Tour will allow to test additional properties of new descriptors such as robustness to different type of noise and generalisation from one dataset to the other.\nNote that the results for Oxford data presented here  are not directly comparable to those reported in other papers. However this has been the case for many papers and we believe that using patches will allow converging to repeatable experiments on this data.\nThe descriptors performance is evaluated in terms of nearest neighbour matching and the results are presented with \n precision-recall curves as it was originally proposed in\n\\cite{schmid2003performance}. \n More specifically, for each patch from the left image we find its nearest neighbour in\nthe right image. Based on the ground truth overlap we can distinguish between false positives and true\npositives, and generate  precision-recall curves. Detailed\ndescription of the dataset and the benchmarking protocol can be found in the\nsupplementary material. \n\nIn  this experiment all the CNN networks are trained using Libery data from Photo Tour. In Figure \\ref{fig:grafpr} we present the precision-recall curves across\nall the pairs of  $graffiti$  and $trees$ images, which are considered  the most \nchallenging sequence in the dataset. MatchNet and the\nproposed PN-Net are  close in terms of the area under the curve,\nalthough PN-Net outperforms MatchNet and DeepCompare in both sequences in particular in $trees$. Clearly DeepCompare struggles with image blurr which confirms the observations in \\cite{ZagoruykoCVPR2015}. \n We include precision recall curves for the other sequences in the suplementary material.\n\n\n \n\n\\begin{figure}[t]\n    \\centering\n        \\hspace{-0.1cm}\\includegraphics[trim=3cm 0 3cm 0,width=0.5\\columnwidth]{graf_pr.pdf}\n        \\includegraphics[trim=3cm 0 3cm 0,width=0.5\\columnwidth]{trees_pr.pdf}\n    \\caption{ Precision-Recall curves for matching performance tested on Oxford data.  Results for the two challenging sequences: $graffiti$ (left) with affine deformations  and $trees$ (right) with blurr and out of focus images.    Our PN-Net gives top scores for both sequences.}\n\\label{fig:grafpr}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome examples of true and false positive matches between several image pairs are shown in Figure \\ref{fig:true_and_false_positives}. True positives are\nshown in the top row, and false positives in the bottom. The patches show extreme affine deformation, scale changes, rotation error due to inaccuracy of the Harris-Affine detector as well as the interpolation noise resulting from normalisation from ellipses to circles.   Some of the correct matches by PN-Net are impressive given that the network was not trained on such errors as they don't occur in Photo Tour data. Also some of the false positives exhibit some visual similarity to be considered as reasonably explicable. \n\n\n\n\n\n\nTo compress the results we use the mean average precision (mAP) which is defined as\nthe area under the precision recall curve. The mAP for different methods and image sequences is presented in Fig. \\ref{fig:oxford}.\nFirst observation is that the scores for the new images fall  between the $graf$ and the $ubc$ sequence, which are typically the most and the least challenging ones in the original data.\nMore importantly, the  PN-Net\\_256 outperforms the matching\nresults of MatchNet by $2\\%$ in terms of mAP averaged across all the\nsequences, even though MatchNet descriptor consists of 4095 dimensions.\nIn particular, in sequences with affine deformations and blurr ($graf$ and $trees$). \nInterestingly, the results for scale changes ($asterix$, $bark$, $croll$ and $boat$) are inverse.\nThis clearly shows that multi-scale training has an impact on the results and further improvements can be made if PN-Net is trained with data augmentation. Note that for this NN matching experiment with MatchNet we used the  L2 distance and the feature vector resulting from an intermediate layer before the final metric layer. This is because the metric layer of\nMatchNet did not produce meaningful results due to the fact that it is\nmainly trained to act as a classifier rather than an accurate distance measure.\nIt is worth noting the significant improvement\nthat PN-Net gives  compared to the descriptor of similar\ndimensionality i.e. DeepCompare \\cite{ZagoruykoCVPR2015}, which clearly falls behind the other two in this experiment.\nAnother positive point is that the generalisation from Liberty to Oxford is good for PN-Net and MatchNet, which may indicate that there is no over-fitting to a specific type of images or interest point detectors.\n\n\n\n\n\n\n\n\\begin{figure}[t]\n    \\centering\n    \\begin{subfigure}[b]{1.0\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{true_positives.png}\\\\\n        \\vspace{3pt}\n\n\n\n\n        \\includegraphics[width=\\columnwidth]{false_positives.png}\n        \n    \\end{subfigure}\n    \\caption{ Examples of true (top) and false positive (bottom)\n      nearest neighbour matching in a large patch dataset. Note the\n      extreme variations that the proposed descriptor can cope with.}\n\\label{fig:true_and_false_positives}\n\\end{figure}\n\n\n\\begin{figure*}\n\\centering\n\\vspace{-3.5cm}\n\\includegraphics[width=\\textwidth]{oxford.pdf}\n\\vspace{-4cm}\n\\caption{Mean average precision (mAP) in terms of nearest neighbor descriptor\n  matching based on the protocol from \\cite{schmid2003performance}.  }\n\\label{fig:oxford}\n\\end{figure*}\n\n\n\n\n\\subsection{Computational efficiency}\n\nOne of the main motivations behind this work, was the need for a fast\nand practical feature descriptor based on CNN. The proposed PN-Net\ndescriptor is very efficient  in  terms of both training and extraction time. \n\nIn Figure \\ref{fig:efficiency} (left) we present results on the extraction\ntimes w.r.t. dimensionality.  For the GPU implementations of the deep networks all\nexperiments were done with an NVIDIA GTX TITAN X GPU. \nWe can see that when compared with the recently proposed deep feature\ndescriptors, the proposed PN-Net is both faster and smaller in\ndimensionality, while at the same time performs better in the\nbenchmarks.\n\nFrom the results in Figure \\ref{fig:efficiency} (right) we can\nconclude that the GPU version of the proposed PN-Net descriptor is\nclose to the speed of the  CPU implementation of BRIEF. This\ngives a significant advantage over the previously proposed descriptors\n and makes CNN based descriptors applicable to practical problems with large datasets. \n\nNote that several works have attempted to port SIFT to  GPU \\cite{sihna2006gpu}, with\nspeedups ranging from 5 to 20 compared to the CPU version. Even when considering such speedups, the proposed descriptor is still faster to compute mainly due to the convolutional operations libraries\n\\cite{DBLP:journals/corr/ChetlurWVCTCS14}.\n\n\n\\begin{figure}[!ht]\n  \\begin{subfigure}[c]{.6\\columnwidth}\n    \\centering\n        \\raisebox{-1.1\\height}{\\includegraphics[trim=15 1 15 1,width=\\columnwidth]{efficiency.pdf}}\n  \\end{subfigure}\\hfill\n  \\begin{subtable}{.38\\columnwidth}\n    \\centering\n    \\begin{tabular}[C]{|l|c|} \n \\multicolumn{2}{ c }{$\\approx \\text{time } \\mu S$}\\\\\n      \n      \\hline\n      \\small SIFT (CPU) & 400 \\\\  \n      \\small BRIEF (CPU) & 3 \\\\  \n      \\small { PN-Net} & { 10} \\\\\n     \\small MatchNet  & 575 \\\\  \n      \\small DeepCompare  & 46 \\\\  \n      \\hline\n    \\end{tabular}\n  \\end{subtable}\n  \\caption{(left) Dimensionality and efficiency  of the proposed PN-Net feature compared to other methods. We report  the time required to extract a descriptor from a single input patch in $\\mu S$. Note\n    that both axes are in logarithmic scale. (right) We note that the\n    GPU version of PN-Net approaches the efficiency of BRIEF which is\n    the fastest CPU descriptor available.}\n  \\label{fig:efficiency}\n\\end{figure}\n\nNote that the proposed PN-Net descriptor also has an advantage in\nthe computational efficiency during training time. While other works\nmention that their optimizations take from several hours\n\\cite{simonyan2014learning} to 2 days\n\\cite{ZagoruykoCVPR2015,Han_2015_CVPR}, our work reaches state of the\nart performance in 100-200 training epochs, which translates to 2-5\nhours training on a single GPU. Surprisingly even after a single\nepoch, \\ie after two minutes of training, we get a descriptor very\nclose to the state of the art. \n\n\n\n\n\n\n\n\\section{Conclusion}\nThis work introduced a new approach to training CNN architecture for extracting local image descriptors in the context of nearest neighbour matching.  It is based on the recent advancements in the area of convolutional neural networks and deep\nlearning.  It makes use of the ideas introduced in the field of distance metric learning and online boosting by training with \npositive and negative constraints simultaneously. \n\nWe have introduced a novel loss function SoftPN that is based on triplets of\npatches extracted from feature points. It incorporates the idea of hard negative mining within the loss function thus avoid   mining  and retraining of the network after each iteration.  The experimental results show that SoftPN leads to faster convergence and lower error than hinge loss or SoftMax ratio.\n\nMoreover, the results show that using triplets for training results in a better descriptor and faster learning.\nThe networks can be made simpler, trained with less examples and extract descriptors with a speed comparable to BRIEF. Also the dimensionality can be significantly reduced compared to other CNN based descriptors.\nWe believe that due to these properties the proposed network is less prone to over-fitting. \nThis is supported by the results showing good generalisation properties. \n\n\n\n\nIn addition, we proposed a new  protocol for evaluating feature matching that aims to correct the significant discrepancies between the protocols used in various experiments and ambiguities in interpreting the results of descriptor performance evaluations in future papers.   We added more image sequences to the frequently used Oxford dataset \\cite{schmid2003performance}.\nThe dataset, the ground truth and the source code of the proposed PN-Net\ndescriptor are available from \\url{https://github.com/vbalnt/pnnet}.\n\nWe believe that the proposed PN-Net descriptor with its\nvery efficient computation, low memory requirements  and state of the art performance will\nenable new real-time applications that are based on a new family of\nhighly accurate but extremely fast deep feature descriptors. We also believe that this work sets a positive example that there is no need to compromise the efficiency of the descriptor to achieve state of the art performance with CNN architectures.\n\n{\\small\n\\bibliographystyle{ieee}\n}\n\n\n", "itemtype": "equation", "pos": 19560, "prevtext": "\nIntuitively the hinge embedding loss penalizes positive pairs\nthat have large distance and negative\npairs that have small distance (less than $\\mu$).\n\nHowever as observed in \\cite{simo2015deepdesc}, the majority of the\nnegative patch pairs ($\\mathcal{L}=-1$) do not contribute to\nthe update of the gradients in the optimization process as their distance is already larger than $\\mu$ parameter in\nEq. \\eqref{eq:hinge-embedding-loss}. To address this issue hard negative mining was proposed to include more negative pairs in the training. The hardest negative training pairs were identified by their distance and a subset of these examples were re-fed to the network for gradient update in each iteration. \n\n\\subsubsection{SoftPN loss}\nWe extend the idea of hard negative mining by incorporating both, positive and negative examples simultaneously in a new loss function.\n It uses a triplet of patches where two pairs represent a form of\n soft negative mining without the need for extra back propagation of specific hard negatives trough the network \\cite{simo2015deepdesc}. \n\n Any training triplet $\\mathcal{T}=\\{p_1,p_2,n\\}$  includes two negative $\\Delta(p_1,n) \\;,\\; \\Delta(p_2,n)$ and one positive \n$\\Delta(p_1,p_2)$ distance. Our proposed formulation of the loss function arises from the triplet CNN architecture \nillustrated  in Figure \\ref{fig:architectures} (bottom). \n\nIt is based on the intuitive\nidea that the smallest {\\em  negative} distance within the triplet \nshould be larger than the {\\em positive} distance.  Ideally,\nwe require the positive distance to reach $0$ and the\ntwo negative distances to increase towards $+\\infty$. Therefore the {\\em\n  smaller} negative distance is the soft negative. Identification of such pair requires much less computation than mining of hard negatives and back propagating them through the network after every iteration. \nFormally our SoftPN objective is \n\n", "index": 3, "text": "\\begin{multline}\n  \\label{eq:deep3-objective}\n   l(\\mathcal{T}) = \\large[ \\big( \\frac{e^{\\Delta(p_1,p_2)}}{e^{min(\\Delta(p_1,n),\\Delta(p_2,n))}+e^{\\Delta(p_1,p_2)}}\\big)^{2}\n   + \\\\\n    \\big(\\frac{e^{min(\\Delta(p_1,n),\\Delta(p_2,n))}}{e^{min(\\Delta(p_1,n),\\Delta(p_2,n))}+e^{\\Delta(p_1,p_2)}}  -1\\big)^{2} \\large]\n\\end{multline}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle l(\\mathcal{T})=\\large[\\big{(}\\frac{e^{\\Delta(p_{1},p_{2})}}{e^{%&#10;min(\\Delta(p_{1},n),\\Delta(p_{2},n))}+e^{\\Delta(p_{1},p_{2})}}\\big{)}^{2}+\\\\&#10;\\displaystyle\\big{(}\\frac{e^{min(\\Delta(p_{1},n),\\Delta(p_{2},n))}}{e^{min(%&#10;\\Delta(p_{1},n),\\Delta(p_{2},n))}+e^{\\Delta(p_{1},p_{2})}}-1\\big{)}^{2}\\large]\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>l</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mfrac><msup><mi mathsize=\"120%\">e</mi><mrow><mi mathsize=\"120%\" mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">1</mn></msub><mo mathsize=\"120%\" stretchy=\"false\">,</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">2</mn></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></msup><mrow><msup><mi mathsize=\"120%\">e</mi><mrow><mi mathsize=\"120%\">m</mi><mo>\u2062</mo><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">n</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi mathsize=\"120%\" mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">1</mn></msub><mo mathsize=\"120%\" stretchy=\"false\">,</mo><mi mathsize=\"120%\">n</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo mathsize=\"120%\" stretchy=\"false\">,</mo><mrow><mi mathsize=\"120%\" mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">2</mn></msub><mo mathsize=\"120%\" stretchy=\"false\">,</mo><mi mathsize=\"120%\">n</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></msup><mo mathsize=\"120%\" stretchy=\"false\">+</mo><msup><mi mathsize=\"120%\">e</mi><mrow><mi mathsize=\"120%\" mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">1</mn></msub><mo mathsize=\"120%\" stretchy=\"false\">,</mo><msub><mi mathsize=\"120%\">p</mi><mn mathsize=\"120%\">2</mn></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></msup></mrow></mfrac><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn mathsize=\"120%\">2</mn></msup><mo mathsize=\"120%\" stretchy=\"false\">+</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mfrac><msup><mi>e</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mrow><msup><mi>e</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></mfrac><mo>-</mo><mn>1</mn><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}]