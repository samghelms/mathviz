[{"file": "1601.08003.tex", "nexttext": "\nIf we let $\\rho$ be a quadratic function, the minimizing $\\mathbf{x}$ is the standard\nmean value. To achieve the desired robustness against outliers, $\\rho$ should be a\nfunction that saturates for large argument values. Such functions are called \\emph{robust\nerror norms}. Some popular choices are the \\emph{truncated quadratic} and \\emph{Tukey's\nbiweight} shown in figure \\ref{fig:pf1}. A simple 1D data set together with its error\nfunction is shown in figure \\ref{fig:pf2}. The $\\mathbf{x}$ which minimizes\n\\eqref{eq:pf1} belongs to a general class of estimators called \\emph{M-estimators}\n\\cite{Winkler02}, and will in this text be referred to as the \\emph{robust mean value}.\n\n\\begin{figure}\n  \\center\n  \\includegraphics[width=0.2\\textwidth]{norm_trunc.eps}\n  \\includegraphics[width=0.2\\textwidth]{norm_tukey.eps}\n  \\caption{Error norms: Truncated quadratic (left), Tukey's biweight (right)}\\label{fig:pf1}\n\\end{figure}\n\n\\begin{figure}\n  \\center\n  \\includegraphics[width=0.3\\textwidth]{errfunc1.eps}\n  \\caption{A simple 1D data set together with the error function generated using the truncated\n  quadratic error norm with cutoff distance 1}\\label{fig:pf2}\n\\end{figure}\n\n\n\\section{Previous Work}\n\\label{sec:prev}\n\nFinding the robust mean is a non-convex optimization problem, and a unique global minimum\nis not guaranteed. The problem is related to clustering, and the well-known \\emph{mean\nshift} iteration has been shown to converge to a local minimum of a robust error function\n\\cite{cheng95}.\n\n\n\n\n\n\n\n\n\nAnother approach is to use the channel representation (soft histograms) \\cite{f04a,\nscia2003, fg04, Scharr03}. Each sample $\\mathbf{x}$ can be encoded into a channel vector\n$\\mathbf{c}$ by the nonlinear transformation\n\n", "itemtype": "equation", "pos": 1547, "prevtext": "\n\n\\title{Efficient Robust Mean Value Calculation \\\\ of 1D Features}\n\\author{Erik Jonsson and Michael Felsberg \\\\\n        Computer Vision Laboratory \\\\\n        Department of Electrical Enginering, Link\\\"{o}ping University, Sweden \\\\\n        erijo@isy.liu.se, mfe@isy.liu.se}\n\\maketitle\n\n\n\\begin{abstract}\nA robust mean value is often a good alternative to the standard mean value when dealing\nwith data containing many outliers. An efficient method for samples of one-dimensional\nfeatures and the truncated quadratic error norm is presented and compared to the method\nof channel averaging (soft histograms).\n\\end{abstract}\n\n\n\\section{Introduction}\n\nIn a lot of applications in image processing we are faced with data containing lots of\noutliers. One example is denoising and edge-preserving smoothing of low-level image\nfeatures, but the outlier problem also occurs in high-level operations like object\nrecognition and stereo vision. A wide range of robust techniques for different\napplications have been presented, where RANSAC \\cite{hartley} and the Hough transform\n\\cite{sonka} are two classical examples.\n\nIn this paper, we focus on the particular problem of calculating a mean value which is\nrobust against outliers. An efficient method for the special case of one-dimensional\nfeatures is presented and compared to the \\emph{channel averaging} \\cite{scia2003}\napproach.\n\n\\section{Problem Formulation}\n\\label{sec:prob} Given a sample set $\\mathbf{X} = [\\mathbf{x}^{(1)} \\ldots\n\\mathbf{x}^{(n)}]$, we seek to minimize an error function given by\n\n", "index": 1, "text": "\\begin{equation}\n  \\label{eq:pf1}\n   \\mathcal{E}(\\mathbf{x}) = \\sum_{k=1}^n \\rho(\\| \\mathbf{x}^{(k)} - \\mathbf{x} \\|)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{E}(\\mathbf{x})=\\sum_{k=1}^{n}\\rho(\\|\\mathbf{x}^{(k)}-\\mathbf{x}\\|)\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2225</mo><mrow><msup><mi>\ud835\udc31</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>-</mo><mi>\ud835\udc31</mi></mrow><mo>\u2225</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08003.tex", "nexttext": "\nwhere $K$ is a localized kernel function and $\\xi_k$ the \\emph{channel centers},\ntypically located uniformly and such that the kernels overlap (fig \\ref{fig:channel1}).\nBy averaging the channel representations of the samples, we get something which resembles\na histogram, but with overlapping and ``smooth'' bins. Depending on the choice of kernel,\nthe representation can be decoded to obtain an approximate robust mean. The distance\nbetween neighboring channels corresponds to the scale of the robust error norm.\n\n\\begin{figure}[t]\n  \\center\n  \\includegraphics[width=0.4\\textwidth, height=0.2\\textwidth]{channels1.eps}\\\\\n  \\caption{Example of channel kernel functions located at the integers}\\label{fig:channel1}\n\\end{figure}\n\n\n\n\\section{Efficient 1D Method\n  \\protect\\footnote{This section has been slightly revised since the\n  original SSBA paper, as it contained some minor errors.}} \\label{sec:fast}\nThis section will cover the case where the $\\mathbf{x}$'s are one-dimensional,\n\\emph{e.g.} intensities in an image, and the truncated quadratic error norm is used. In\nthis case, there is a very efficient method, which we have not discovered in the\nliterature. For clarity, we describe the case where all samples have equal weight, but\nthe extension to weighted samples is straightforward.\n\nFirst, some notation. We assume that our data is sorted in ascending order and numbered\nfrom $1 \\ldots n$. Since the $\\mathbf{x}$'s are one dimensional, we drop the vector\nnotation and write simply $x_k$. The error norm is truncated at $c$, and can be written\nas\n\n", "itemtype": "equation", "pos": 3408, "prevtext": "\nIf we let $\\rho$ be a quadratic function, the minimizing $\\mathbf{x}$ is the standard\nmean value. To achieve the desired robustness against outliers, $\\rho$ should be a\nfunction that saturates for large argument values. Such functions are called \\emph{robust\nerror norms}. Some popular choices are the \\emph{truncated quadratic} and \\emph{Tukey's\nbiweight} shown in figure \\ref{fig:pf1}. A simple 1D data set together with its error\nfunction is shown in figure \\ref{fig:pf2}. The $\\mathbf{x}$ which minimizes\n\\eqref{eq:pf1} belongs to a general class of estimators called \\emph{M-estimators}\n\\cite{Winkler02}, and will in this text be referred to as the \\emph{robust mean value}.\n\n\\begin{figure}\n  \\center\n  \\includegraphics[width=0.2\\textwidth]{norm_trunc.eps}\n  \\includegraphics[width=0.2\\textwidth]{norm_tukey.eps}\n  \\caption{Error norms: Truncated quadratic (left), Tukey's biweight (right)}\\label{fig:pf1}\n\\end{figure}\n\n\\begin{figure}\n  \\center\n  \\includegraphics[width=0.3\\textwidth]{errfunc1.eps}\n  \\caption{A simple 1D data set together with the error function generated using the truncated\n  quadratic error norm with cutoff distance 1}\\label{fig:pf2}\n\\end{figure}\n\n\n\\section{Previous Work}\n\\label{sec:prev}\n\nFinding the robust mean is a non-convex optimization problem, and a unique global minimum\nis not guaranteed. The problem is related to clustering, and the well-known \\emph{mean\nshift} iteration has been shown to converge to a local minimum of a robust error function\n\\cite{cheng95}.\n\n\n\n\n\n\n\n\n\nAnother approach is to use the channel representation (soft histograms) \\cite{f04a,\nscia2003, fg04, Scharr03}. Each sample $\\mathbf{x}$ can be encoded into a channel vector\n$\\mathbf{c}$ by the nonlinear transformation\n\n", "index": 3, "text": "\\begin{equation}\n  \\label{eq:channel1}\n  \\mathbf{c} = [K(\\|\\mathbf{x}-\\xi_1\\|), \\ldots, K(\\|\\mathbf{x}-\\xi_m\\|)]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{c}=[K(\\|\\mathbf{x}-\\xi_{1}\\|),\\ldots,K(\\|\\mathbf{x}-\\xi_{m}\\|)]\" display=\"block\"><mrow><mi>\ud835\udc1c</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><msub><mi>\u03be</mi><mn>1</mn></msub></mrow><mo>\u2225</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc31</mi><mo>-</mo><msub><mi>\u03be</mi><mi>m</mi></msub></mrow><mo>\u2225</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08003.tex", "nexttext": "\nThe method works as follows: We keep track of indices $a,b$ and and a window $w = [a,b]$\nof samples $[x_a, \\ldots, x_b]$. The window $[a,b]$ is said to be\n\n\\begin{itemize}\n  \\item[-] \\emph{feasible} if $|x_b - x_a| < 2c$\n  \\item[-] \\emph{maximal} if the samples are contained in a continuous window of length $2 c$,\n    i.e. if $[a, b]$ is feasible and $[a-1, b+1]$ is infeasible.\n\\end{itemize}\nNow define for a window $w = [a,b]$\n\\begin{eqnarray}\n  \\mu_w & = & \\frac{1}{b-a+1} \\sum_{k=a}^b x_k \\\\\n  n_o & = & (a-1) + (n-b) \\quad \\\\\n  q_w & = & \\sum_{k=a}^b (\\mu_w - x_k)^2 \\\\\n  \\hat{\\mathcal{E}}_w & = & q_w + n_o c^2\n\\end{eqnarray}\nNote that $n_o$ is the number of samples outside the window. Consider the global minimum\n$x_0$ of the error function and the window $w$ of samples $x_k$ that fall within the\nquadratic part of the error function centered around $x_0$, i.e. the samples $x_k$ such\nthat $|x_k - x_0| \\le c$. Either this window is located close to the boundary ($a=1$ or\n$b=n$) or constitutes a maximal window. In both cases, $x_0 = \\mu_w$, and\n$\\hat{\\mathcal{E}}_w = \\mathcal{E}(\\mu_w)$. This is not necessarily true for an arbitrary\nwindow, e.g. if $\\mu_w$ is located close to the window boundary. However, for an\narbitrary window $w$, we have\n\\begin{eqnarray}\n  \\hat{\\mathcal{E}}_w & = & \\sum_{k=a}^b (\\mu_w - x_k)^2 + n_o c^2 \\ge \\\\\n    & \\ge & \\sum_{k=1}^n \\min\\{(\\mu_w - x_k)^2, c^2\\} \\\\\n   & = & \\sum_{k=1}^n \\rho(\\mu_w - x_k) = \\mathcal{E}(\\mu_w)\n\\end{eqnarray}\n\nThe strategy is now to enumerate all maximal and boundary windows, evaluate\n$\\hat{\\mathcal{E}}_w$ for each and take the minimum, which is guaranteed to be the global\nminimum of $\\mathcal{E}$. Note that it does not matter if some non-maximal windows are\nincluded, since we always have $\\hat{\\mathcal{E}}_w \\ge \\mathcal{E}(\\mu_w)$.\n\nThe following iteration does the job: Assume that we have a feasible window $[a, b]$, not\nnecessarily maximal. If $[a, b+1]$ is feasible, take this as the new window. Otherwise,\n$[a, b]$ was the largest maximal window starting at $a$, and we should go on looking for\nmaximal windows starting at $a+1$. Take $[a+1, b]$ as the first candidate, then keep\nincreasing $b$ until the window becomes infeasible, etc. If proper initialization and\ntermination of the loop is provided, this iteration will generate all maximal and\nboundary windows.\n\nThe last point to make is that we do not need to recompute $q_w$ from scratch as the\nwindow size is changed. Similar to the treatment of mean values and variances in\nstatistics, we get by expanding the quadratic expression\n\\begin{eqnarray}\nq_w & = & \\sum_{k=a}^b (\\mu_w - x_k)^2 =  \\nonumber \\\\\n    & = & \\sum_{k=a}^b x_k^2 - (b-a+1) \\mu_w^2 = \\nonumber \\\\\n    & = & S_2 - (b-a+1)^{-1} S_1^2\n\\end{eqnarray}\nwhere we have defined\n\\begin{eqnarray}\n  S_1 & = & \\sum_{k=a}^b x_k = (b-a + 1) \\mu_w \\\\\n  S_2 & = & \\sum_{k=a}^b x_k^2 \\\\\n\\end{eqnarray}\n$S_1$ and $S_2$ can easily be updated in constant time as the window size is increased or\ndecreased, giving the whole algorithm complexity $O(n)$. The algorithm is summarized as\nfollows: \\footnote{The check $a \\le b$ is required to avoid zero division if $a$ was\nincreased beyond $b$ in the previous iteration.}\n\n\\begin{algorithm}\n\\caption{Fast 1D robust mean calculation} \\label{alg:fast1}\n\\begin{algorithmic}\n  \\STATE Initialize $a \\gets 1$, $b \\gets 1$, $S_1 \\gets x_1$, $S_2 \\gets x_1^2$\n  \\WHILE{$a \\le n$}\n    \\IF{$a \\le b$}\n      \\STATE Calculate candidate $\\hat{\\mathcal{E}}_w$ and $\\mu_w$:\n      \\STATE $\\mu_w \\gets (b-a+1)^{-1} S_1$\n      \\STATE $\\hat{\\mathcal{E}}_w \\gets S_2 - \\mu_w S_1 + n_o c^2$\n      \\STATE If $\\hat{\\mathcal{E}}_w$ is the smallest so far, store $\\hat{\\mathcal{E}}_w$, $\\mu_w$.\n    \\ENDIF\n    \\IF{$b < n$ and $|x_{b+1} - x_a| < 2c$}\n      \\STATE $b \\leftarrow b + 1$\n      \\STATE $S_1 \\leftarrow S_1 + x_{b}$\n      \\STATE $S_2 \\leftarrow S_2 + x_{b}^2$\n    \\ELSE\n      \\STATE $S_1 \\leftarrow S_1 - x_{a}$\n      \\STATE $S_2 \\leftarrow S_2 - x_{a}^2$\n      \\STATE $a \\leftarrow a + 1$\n    \\ENDIF\n  \\ENDWHILE\n  \\STATE The $\\mu_w$ corresponding to the smallest $\\hat{\\mathcal{E}}_w$ is now the robust mean.\n\\end{algorithmic}\n\\end{algorithm}\n\nNote that it is straightforward to introduce a weight $w$ for each sample, such that a\nweighted mean value is produced. We should then let $n_0$ be the total weight of the\nsamples outside the window, $\\mu_w$ the weighted mean value of the window $w$, $S_1$ and\n$S_2$ weighted sums etc.\n\n\n\\section{Properties of the Robust \\\\ Mean Value}\n\nIn this section, some properties of the robust mean values generated by the truncated\nquadratic method and the channel averaging will be examined. In figure \\ref{fig:comp1},\nwe show the robust mean of a sample set consisting of some values (inliers) with mean\nvalue $3.0$ and an outlier at varying positions. As the outlier moves sufficiently far\naway from the inliers, it is completely rejected, and when it is close to $3.0$, it is\ntreated as an inlier. As expected, the truncated quadratic method makes a hard decision\nabout whether the outlier should be included or not, whereas the channel averaging\nimplicitly assumes a smoother error norm.\n\nAnother effect is that the channel averaging overcompensates for the outlier at some\npositions (around $x=6.0$ in the plot). Also, the exact behavior of the method can vary\nat different absolute positions due to the \\emph{grid effect} illustrated in figure\n\\ref{fig:comp2}. We calculated the robust mean of two samples $x_1, x_2$, symmetrically\nplaced around some point $x_0$ with $|x_1-x_0| = |x_2-x_1| = d$. The channels were placed\nwith unit distance, and the displacement of the estimated mean $m$ compared to the\ndesired value $x_0$ is shown for varying $x_0$'s in the range between two neighboring\nchannel centers. The figure shows that the method makes some (small) systematic errors\ndepending on the position relative to the channel grid. No such grid effect occurs using\nthe method from section \\ref{sec:fast}.\n\nWhen the robust mean algorithm is applied on sliding spatial windows of an image, we get\nan edge-preserving image smoothing method. In figure \\ref{fig:lenna}, we show the 256x256\nLenna image smoothed with the truncated quadratic method using a spatial window of 5 x 5\nand $c = 0.1$ in the intensity domain, where intensities are in the range $[0,1]$. The\npixels are weighted with a Gaussian function.\n\n\n\\begin{figure}[t]\n  \\center\n  \\includegraphics[width=0.4\\textwidth]{influence.eps}\n  \\caption{The influence of an outlier on the mean value}\\label{fig:comp1}\n\\end{figure}\n\n\\begin{figure}[t]\n  \\center\n  \\includegraphics[width=0.4\\textwidth]{grid.eps}\n  \\caption{The grid effect}\\label{fig:comp2}\n\\end{figure}\n\n\n\\section{Discussion}\n\nWe have shown an efficient way to calculate the robust mean value for the special case of\none-dimensional features and the truncated quadratic error. The advantage of this method\nis that it is simple, exact and global. The disadvantage is of course its limitation to\none-dimensional feature spaces.\n\nOne example of data for which the method could be applied is image features like\nintensity or orientation. If the number of samples is high, e.g. in robust smoothing of a\nhigh resolution image volume, the method might be suitable. If a convolution-like\noperation is to be performed, the overhead of sorting the samples could be reduced\nsignificantly, since the data is already partially sorted when moving to a new spatial\nwindow, leading to an efficient edge-preserving smoothing algorithm.\n\n\\begin{figure}[t]\n  \\center\n  \\includegraphics[width=0.4\\textwidth]{lenna_smooth.eps}\n  \\caption{Lenna, robustly smoothed with the truncated quadratic method}\\label{fig:lenna}\n\\end{figure}\n\n\\section*{Acknowledgment}\nThis work has been supported by EC Grant IST-2003-004176 COSPAL.\n\n\\bibliography{references}\n\n\n\n", "itemtype": "equation", "pos": 5094, "prevtext": "\nwhere $K$ is a localized kernel function and $\\xi_k$ the \\emph{channel centers},\ntypically located uniformly and such that the kernels overlap (fig \\ref{fig:channel1}).\nBy averaging the channel representations of the samples, we get something which resembles\na histogram, but with overlapping and ``smooth'' bins. Depending on the choice of kernel,\nthe representation can be decoded to obtain an approximate robust mean. The distance\nbetween neighboring channels corresponds to the scale of the robust error norm.\n\n\\begin{figure}[t]\n  \\center\n  \\includegraphics[width=0.4\\textwidth, height=0.2\\textwidth]{channels1.eps}\\\\\n  \\caption{Example of channel kernel functions located at the integers}\\label{fig:channel1}\n\\end{figure}\n\n\n\n\\section{Efficient 1D Method\n  \\protect\\footnote{This section has been slightly revised since the\n  original SSBA paper, as it contained some minor errors.}} \\label{sec:fast}\nThis section will cover the case where the $\\mathbf{x}$'s are one-dimensional,\n\\emph{e.g.} intensities in an image, and the truncated quadratic error norm is used. In\nthis case, there is a very efficient method, which we have not discovered in the\nliterature. For clarity, we describe the case where all samples have equal weight, but\nthe extension to weighted samples is straightforward.\n\nFirst, some notation. We assume that our data is sorted in ascending order and numbered\nfrom $1 \\ldots n$. Since the $\\mathbf{x}$'s are one dimensional, we drop the vector\nnotation and write simply $x_k$. The error norm is truncated at $c$, and can be written\nas\n\n", "index": 5, "text": "\\begin{equation}\n  \\rho(x) = \\min\\{x^2, c^2\\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\rho(x)=\\min\\{x^{2},c^{2}\\}\" display=\"block\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>x</mi><mn>2</mn></msup><mo>,</mo><msup><mi>c</mi><mn>2</mn></msup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>", "type": "latex"}]