[{"file": "1601.06068.tex", "nexttext": "\nwhere $\\Phi(p, u, g, q, \\mathcal{K}) \\in \\mathbb{R}^n$ denotes the\nfeatures for the tuple of paraphrase, ungrounded and grounded graphs.\nThe feature function has access to the paraphrase, ungrounded and\ngrounded graphs, the original question, as well as to the content of\nthe knowledge base and the denotation $|g|_\\mathcal{K}$ (the\ndenotation of a grounded graph is defined as the set of entities or\nattributes reachable at its \\textsc{target} node).  See\n\\Cref{sec:details} for the features employed.  The model parameters\nare estimated with the averaged structured perceptron\n\\cite{collins_discriminative_2002}.  Given a training question-answer\npair $(q,\\mathcal{A})$, the update is:\n", "itemtype": "equation", "pos": 37648, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\n  One of the limitations of semantic parsing approaches to open-domain\n  question answering is the lexicosyntactic gap between natural\n  language questions and knowledge base entries -- there are many ways\n  to ask a question, all with the same answer.  In this paper we\n  propose to bridge this gap by generating paraphrases of the input\n  question with the goal that at least one of them will be correctly\n  mapped to a knowledge-base query. We introduce a novel\n  grammar model for paraphrase generation that does not require any\n  sentence-aligned paraphrase corpus. Our key idea is to leverage the\n  flexibility and scalability of latent-variable probabilistic\n  context-free grammars to sample paraphrases. We do an extrinsic\n  evaluation of our paraphrases by plugging them into a semantic\n  parser for Freebase. Our evaluation experiments on the WebQuestions\n  benchmark dataset show that the performance of the semantic parser\n  significantly improves over strong baselines.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nSemantic parsers map sentences onto logical forms that can be used to\nquery databases \\cite{zettlemoyer_learning_2005,wong_learning_2006},\ninstruct robots \\cite{chen_learning_2011}, extract information\n\\cite{krishnamurthy_weakly_2012}, or describe visual scenes\n\\cite{matuszek_joint_2012}. In this paper we consider the problem of\nsemantically parsing questions into Freebase logical forms for the\ngoal of question answering.  Current systems accomplish this by\nlearning task-specific grammars \\cite{berant_semantic_2013},\nstrongly-typed CCG grammars\n\\cite{kwiatkowski_scaling_2013,reddy_largescale_2014}, or neural\nnetworks without requiring any grammar \\cite{yih_semantic_2015}. These\nmethods are sensitive to the words used in a question and their word\norder, making them vulnerable to unseen words and\nphrases. Furthermore, mismatch between natural language and Freebase\nmakes the problem even harder. For example, Freebase expresses the\nfact that \\textit{``Czech is the official language of Czech Republic''}\n(encoded as a graph), whereas to answer a question like \\textit{``What\n  do people in Czech Republic speak?''} one should infer\n\\textit{people in Czech Republic} refers to \\textit{Czech Republic}\nand \\textit{What} refers to the \\textit{language} and \\textit{speak}\nrefers to the predicate \\textit{official language}.\n\nWe address the above problems by using paraphrases of the original\nquestion. Paraphrasing has shown to be promising for semantic parsing\n\\cite{fader_paraphrasedriven_2013,berant_semantic_2014}.\n\n\nWe propose a novel framework for paraphrasing using latent-variable\nPCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based\nmachine translation for text-based QA \\cite{Duboue2006,Riezler2007},\nor hand annotated grammars for KB-based QA\n\\cite{berant_semantic_2014}. We find that phrase-based statistical\nmachine translation (SMT) approaches mainly produce lexical\nparaphrases without much syntactic diversity, whereas our\ngrammar-based approach is capable of producing both lexically and\nsyntactically diverse paraphrases. Unlike SMT based approaches, our\nsystem does not require aligned parallel paraphrase corpora. In\naddition we do not require hand annotated grammars for paraphrase\ngeneration but instead learn the grammar directly from a large scale\nquestion corpus.\n\nThe main contributions of this paper are two fold. First, we present\nan algorithm (\\S\\ref{sec:paragen}) to generate paraphrases using\nlatent-variable PCFGs. We use the spectral method of\n\\newcite{narayan-15} to estimate L-PCFGs on a large scale question\ntreebank. Our grammar model leads to a robust and an efficient system\nfor paraphrase generation in open-domain question answering. While\nCFGs have been explored for paraphrasing using bilingual parallel\ncorpus \\cite{ppdb}, ours is the first implementation of CFG that uses\nonly monolingual data. Second, we show that generated paraphrases can\nbe used to improve semantic parsing of questions into Freebase logical\nforms (\\S\\ref{sec:qaframework}). We build on a strong baseline of\n\\newcite{reddy_largescale_2014} and show that our grammar model\nsurpasses SMT baseline even without using any parallel paraphrase\nresources.\n\n\n\n\n\n\n\n\n\n\\section{Paraphrase Generation Using Grammars}\n\\label{sec:paragen}\n\\vspace{-0.2cm}\n\nOur paraphrase generation algorithm is based on a model in the form of\nan L-PCFG.  L-PCFGs are PCFGs where the nonterminals are refined with\nlatent states that provide some contextual information about each node\nin a given derivation.  L-PCFGs have been used in various ways, most\ncommonly for syntactic parsing\n\\cite{prescher-05,matsuzaki-2005,petrov-2006,cohen-13,narayan-15}.\n\n\n\n\n\n\n\\ignore{\nWe start this section in \\S\\ref{subsec:background} with a background\non L-PCFGs and the Paralex corpus \\cite{fader_paraphrasedriven_2013},\na large scale question corpus, on which we train our model for\nparaphrase generation. \\S\\ref{subsec:samplepara} briefly describes our\nparaphrase generation algorithm. Rest of the section describes various\nsteps of our algorithm in detail.\n}\n\n\n\n\n\n\n\n\n\n\n\n\\ignore{\nAn L-PCFG is a 8-tuple $({\\cal N}, {\\cal I}, {\\cal P}, m, n, \\pi, t, q)$ where\n${\\cal N}$ is the set of nonterminal symbols in the grammar.  ${\\cal I}\n\\subset {\\cal N}$ is a finite set of {\\em interminals}.  ${\\cal P} \\subset {\\cal N}$ is\na finite set of {\\em preterminals}.  We assume that ${\\cal N} = {\\cal I}\n\\cup {\\cal P}$, and ${\\cal I} \\cap {\\cal P}= \\emptyset$.  Hence we have\npartitioned the set of nonterminals into two subsets. $[m]$ is the set\nof possible hidden states.\\footnote{For any integer $n$, we denote by\n  $[n]$ the set of integers $\\{ 1, \\ldots, n \\}$.} $[n]$ is the set of\npossible words. For all $a \\in {\\cal I}$, $b \\in {\\cal N}$, $c \\in {\\cal N}$,\n$h_1, h_2, h_3 \\in [m]$, we have binary context-free rules of the form\n$ a(h_1) \\rightarrow b(h_2) \\;\\; c(h_3) $ with an associated parameter\n$t( a \\rightarrow b \\; c, h_2, h_3 \\; | \\; a, h_1)$. For all $a \\in\n{\\cal P}$, $h \\in [m]$, $x \\in [n]$, we have lexical context-free rules of\nthe form $ a(h) \\rightarrow x $ with an associated parameter $q( a\n\\rightarrow x \\; | \\; a, h)$. For all $a \\in {\\cal I}$, $h \\in [m]$,\n$\\pi(a,h)$ is a parameter specifying the probability of $a(h)$ being\nat the root of a tree.\n}\n\n\n\\ignore{\n\\begin{figure}\n  \\begin{center}\n    \\begin{footnotesize}\n      \\begin{tabular}{lp{0.3in}l}\n      \\tikzset{level distance=20pt, sibling distance=0pt}\n        \\Tree [.VP [.V saw ] [.NP [.D the ] [.N woman ] ] ]\n        &\n        &\n        \\tikzset{level distance=20pt, sibling distance=0pt}\n        \\Tree [.S [.NP [.D the ] [.N dog ] ] VP ]\n      \\end{tabular}\n    \\end{footnotesize}\n  \\end{center}\n  \\caption{The inside tree (left) and outside tree (right) for the\n    nonterminal {\\tt VP} in the parse tree {\\tt (S (NP (D the) (N dog))\n      (VP (V saw) (NP (D the) (N woman)))).}}\n  \\label{fig:iotrees}\n\\end{figure}\n}\n\n\n\n\n\n\n\n\nIn our estimation of L-PCFGs, we use the spectral method of\n\\newcite{narayan-15}, instead of using EM, as has been used in the\npast by \\newcite{matsuzaki-2005} and \\newcite{petrov-2006}. The\nspectral method we use enables the choice of a set of feature\nfunctions that indicate the latent states, which proves to be useful\nin our case. It also leads to sparse grammar estimates and compact\nmodels.\n\nThe spectral method works by identifying feature functions for\n``inside'' and ``outside'' trees, and then clusters them into latent\nstates.  Then it follows with a maximum likelihood estimation step,\nthat assumes the latent states are represented by clusters obtained\nthrough the feature function clustering.  For more details about these\nconstructions, we refer the reader to \\newcite{cohen-13} and\n\\newcite{narayan-15}.\n\nThe rest of this section describes our paraphrase generation\nalgorithm.\n\n\\ignore{\nThe spectral method we use identifies the latent states for a\nnonterminal by finding patterns that co-occur together in {\\em inside}\nand {\\em outside} trees. Given a tree, the ``inside tree'' for a\nnonterminal contains the entire subtree below that node; the ``outside\ntree'' contains everything in the tree excluding the inside tree\n(Figure \\ref{fig:iotrees}). We denote the space of inside trees by $T$\nand the space of outside trees by $O$ over the training treebank. All\ntrees in the training treebank are split into inside and outside trees\nat each node for each tree leading to a set of example $({a^{(i)}},\n{t^{(i)}}, {o^{(i)}}, b^{(i)})$ for $i \\in \\{1 \\ldots M\\}$, where ${a^{(i)}} \\in\n{\\cal N}$; ${t^{(i)}}$ is an inside tree; ${o^{(i)}}$ is an outside tree; and\n$b^{(i)} = 1$ if ${a^{(i)}}$ is the root of tree, $0$ otherwise. Spectral\nmethods define two feature functions, $\\phi \\colon T \\rightarrow\n{\\mbox{\\msym R}}^d$ and $\\psi \\colon O \\rightarrow {\\mbox{\\msym R}}^{d'}$, mapping inside\nand outside trees, respectively, to a real vector, e.g., for syntactic\nconstituency parsing, these feature functions will try to capture the\ncontextual information about a nonterminal.  \\newcite{narayan-15} use\nthese feature representations to clusters the nonterminals in the\ntraining data; this way they assign a cluster identifier for each node\nin each tree in the training data. These clusters are now treated as\nlatent states that are ``observed.''  Finally, Narayan and Cohen\nfollow up with a simple frequency count maximum likelihood estimate to\nestimate the parameters in the L-PCFG. This way of observing latent\nstates leads to a sparse grammar estimate.\n}\n\n\\subsection{Paraphrases Generation Algorithm}\n\\label{subsec:samplepara}\n\n\\ignore{\n\\begin{figure}\n  \\begin{footnotesize}\n    \\begin{algorithmic}[1]\n\n      \\State \\textbf{Input}: An input question $q$, an L-PCFG\n      ${G_{\\mathrm{syn}}}$ with parameters $({\\cal N}, {\\cal I}, {\\cal P}, m_{con}, n,\n      \\pi_{con}, t_{con}, q_{con})$, a paraphrase classifier\n      $C:(s_1,s_2)\\rightarrow[0,1]$ and an integer $M$. ${G_{\\mathrm{syn}}}$ is\n      the grammar of \\newcite{narayan-15} for constituency\n      parsing. $C$ checks if two sentences $s_1$ and $s_2$ are\n      paraphrases. $M$ is the number of sampling to be\n      done. Optionally, the algorithm may take the Paraphrase Database\n      $D_{ppdb}$ or an hybrid L-PCFG ${G_{\\mathrm{layered}}}$.\n\n      \\State \\textbf{Output}: A set of paraphrases $P_q$ to the input\n      question $q$.\n\n      \\State \\textbf{Algorithm}:\n\n      \\State initialize $P_s \\leftarrow \\emptyset$\n\n      \\State build a word lattice $W_q$ for $q$, optionally using\n      $D_{ppdb}$ or ${G_{\\mathrm{layered}}}$\n\n      \\State extract a smaller L-PCFG ${G_{\\mathrm{syn}}^{\\prime}}$ from ${G_{\\mathrm{syn}}^{\\prime}}$ for\n      $W_q$\n\n      \\For{i = 1}{ $M$}\n      \\State sample a sentence $q'$ from ${G_{\\mathrm{syn}}^{\\prime}}$\n      \\If{$C(q,q')==1$}\n      \\State $P_q = P_q \\cup {q'}$ \n      \\EndIf\n      \\EndFor\n\n    \\end{algorithmic}\n  \\end{footnotesize}\n  \\caption{The paraphrase generation algorithm.}\n  \\label{fig:paragenalgo}\n\\end{figure}\n}\n\n\\begin{figure*}[htbp]\n  \\centering\n  \\includegraphics[width=\\textwidth]{wordlattice.pdf}\n  \\caption{An example word lattice for the question {\\it What language\n      do people in Czech Republic speak?} using the lexical and\n    phrasal rules from the PPDB. \n  }\n  \\label{fig:wordlattice}\n  \\vspace{-0.4cm}\n\\end{figure*}\n\n\n\n\n\nWe define our paraphrase generation task as a sampling problem from an\nL-PCFG ${G_{\\mathrm{syn}}}$, which is estimated from a large corpus of parsed\nquestions. Once this grammar is estimated, our algorithm follows a\npipeline with two major steps.\n\nWe first build a word lattice $W_q$ for the input question\n$q$.\\footnote{Word lattices, formally weighted finite state automata,\n  have been used in previous works for paraphrase generation\n  \\cite{langkilde1998,barzilay2003,pang2003,quirk-2004}. We use an\n  unweighted variant of word lattices in our algorithm.}\n\n\n\n\n\n\n\n\n\nWe use the lattice to constrain our paraphrases to a specific choice\nof words and phrases that can be used.\n\nOnce this lattice is created, a grammar ${G_{\\mathrm{syn}}^{\\prime}}$ is then extracted\nfrom ${G_{\\mathrm{syn}}}$.  This grammar is constrained to the lattice.\n\n\\ignore{\nIn this setting, we use the PPDB \\cite{ppdb} to build word\nlattices. For an input question $q$, we extract high-precision rules\nfrom the PPDB\\footnote{For our experiments, we extract rules from the\n  PPDB-Small to maintain the high precision \\cite{ppdb}.} which match\nthe pattern in $q$. We use these rules to build the word lattice. We\nuse both lexical and phrasal paraphrase rules. We refer this system by\n\\textbf{PPDB} in future reference. Our approach is similar to\n\\newcite{langkilde1998} where they use WordNet-based hand-crafted\nrules to build word lattices. Figure~\\ref{fig:wordlattice} shows an\nexample word lattice for the question {\\it What language do people in\n  Czech Republic speak?} using the rules from the PPDB.\n}\n\nWe experiment with three ways of constructing word lattices: na\\\"{i}ve\nword lattices representing the words from the input question only,\nword lattices constructed with the Paraphrase Database \\cite{ppdb} and\nword lattices constructed with a bi-layered L-PCFG, described in\n\\S\\ref{subsec:hybrid}. For example, Figure~\\ref{fig:wordlattice} shows\nan example word lattice for the question {\\it What language do people\n  in Czech Republic speak?} using the lexical and phrasal rules from\nthe PPDB.\\footnote{For our experiments, we extract rules from the\n  PPDB-Small to maintain the high precision \\cite{ppdb}.}\n\n\n\n\n\n\n\n\n\n\\ignore{\nWe then extract a smaller grammar ${G_{\\mathrm{syn}}^{\\prime}}$ from ${G_{\\mathrm{syn}}}$ to\nsample new questions. ${G_{\\mathrm{syn}}^{\\prime}}$ is the compact\nrepresentation of ${G_{\\mathrm{syn}}}$ for $W_q$. We describe this pruning step\nin \\S\\ref{subsec:pruning}. \n}\n\nOnce ${G_{\\mathrm{syn}}^{\\prime}}$ is generated, we sample paraphrases of the input\nquestion $q$. These paraphrases are further filtered with a classifier\nto improve the precision of the generated paraphrases.\n\n\\ignore{The sampling algorithm, described in \\S\\ref{subsec:sampling},\n  is not guaranteed to generate paraphrases. We use a classifier,\n  described in \\S\\ref{subsec:classifier}, to select paraphrases among\n  the sampled questions (step 9).  }\n\n\n\n\n\\vspace{-0.1cm}\n\\paragraph{L-PCFG Estimation}\nWe train the L-PCFG ${G_{\\mathrm{syn}}}$ on the Paralex corpus\n\\cite{fader_paraphrasedriven_2013}.\n\n\nParalex is a large monolingual parallel corpus, containing 18 million\npairs of question paraphrases with 2.4M distinct questions in the\ncorpus.  It is suitable for our task of generating paraphrases since\nits large scale makes our model robust for open-domain questions. We\nconstruct a treebank by parsing 2.4M distinct questions from Paralex\nusing the BLLIP parser \\cite{charniak-johnson:2005:ACL}.\\footnote{We\n  ignore the Paralex alignments for training ${G_{\\mathrm{syn}}}$.}\n\nGiven the treebank, we use the spectral algorithm of\n\\newcite{narayan-15} to learn an L-PCFG for constituency parsing to\nlearn ${G_{\\mathrm{syn}}}$. We follow \\newcite{narayan-15} and use same feature\nfunctions for the inside and outside trees, capturing contextual\nsyntactic information about nonterminals. We refer the reader to\n\\newcite{narayan-15} for more detailed description of the features we\nused.\n\n\n\n\n\n\nIn our experiments, we set the number of latent states to 24.\n\nOnce we estimate ${G_{\\mathrm{syn}}}$ from the Paralex corpus, we restrict it for\neach question to a grammar ${G_{\\mathrm{syn}}^{\\prime}}$ by keeping only the rules that\ncould lead to a derivation over the lattice. This step is similar to\nlexical pruning in standard grammar-based generation process to avoid\nan intermediate derivation which can never lead to a successful\nderivation \\cite{koller-2002,nargar-2012}.\n\n\\ignore{\nIn addition, the spectral algorithm of \\newcite{narayan-15} leads to\nsparse grammar estimates and compact models which allow us to train\nour grammar on the large-scale Paralex corpus (``scalability'') and\nfacilitates paraphrase generation for a question in an open-domain\nquestion answering (``flexibility''). These two criteria\nare important for the problem of paraphrase generation in\nopen-domain question answering as we want our model to capture\nparaphrase information for any given question and to generate\nparaphrases efficiently without falling into the NP-completeness of\nnatural language generation\n\\cite{brew-92,kay-1996,koller-2002,white-2004}.\n}\n\n\\ignore{\n\\subsection{L-PCFG Rule Selection}\n\\label{subsec:pruning}\n\nGiven a question $q$ and its word lattice $W_q$, we extract a smaller\ngrammar ${G_{\\mathrm{syn}}}'$ with parameters $({\\cal N}', {\\cal I}', {\\cal P}', m_{con}, n',\n\\pi_{con}, t_{con}, q_{con})$ from ${G_{\\mathrm{syn}}}$ and we sample from\n${G_{\\mathrm{syn}}}'$. $[n']$ is the set of words in the word lattice $W_q$. ${\\cal P}'$\nconsists of preterminals $a \\in {\\cal P}$ such that we have lexical\ncontext-free rules of the form $ a(h) \\rightarrow x $ with $x \\in\n[n']$ and $h \\in [m]$. ${\\cal I}'$ consists of interminals $a \\in\n{\\cal I}$ such that we have a binary context-free rules of the form $\na(h_1) \\rightarrow b(h_2) \\;\\; c(h_3) $ with $b \\in {\\cal I}' \\cup\n{\\cal P}'$, $c \\in {\\cal I}' \\cup {\\cal P}'$ and $h_1, h_2, h_3 \\in [m]$. ${\\cal N}' =\n{\\cal I}' \\cup {\\cal P}'$ is the set of nonterminal symbols in the new\ngrammar, and ${\\cal I}' \\cap {\\cal P}'= \\emptyset$.\n\nWe note here that the new, smaller grammar only allows those\nnonterminals ${\\cal N}'$ which can possibly lead to a word in $W_q$.\n\n}\n\n\n\n\n\\ignore{\nSampling of a sentence from the grammar ${G_{\\mathrm{syn}}}':({\\cal N}', {\\cal I}',\n{\\cal P}', m_{con}, n', \\pi_{con}, t_{con}, q_{con})$ proceeds in a top-down\nbreadth-first fashion. We first sample the root of the tree $a(h)$\nwith a score of $\\pi_{con}(a,h)$. From the root $a(h)$, we sample a\nbinary rule of the form $a(h) \\rightarrow b(h_2) \\;\\; c(h_3) $ with a\nscore of $t_{con}(a \\rightarrow b \\;\\; c, h_2, h_3 | a, h)$. If a\nbinary rule produces preterminals (e.g., lets consider $b(h_2)$ is a\npreterminal), we sample a lexical rule of the form $ b(h_2)\n\\rightarrow x $ with a score $q_{con}(b \\rightarrow x | b, h_2)$. We\nrecursively explore the tree in top-down breadth-first fashion by\nsampling binary and lexical rules until the tree is complete or no\nmore production is possible.\n}\n\n\\vspace{-0.1cm}\n\\paragraph{Paraphrase Sampling} \n\nSampling a question from the grammar ${G_{\\mathrm{syn}}^{\\prime}}$ is done by recursively\nsampling nodes in the derivation tree, together with their latent\nstates, in a top-down breadth-first fashion. Sampling from the pruned\ngrammar ${G_{\\mathrm{syn}}^{\\prime}}$ raises an issue of oversampling words that are more\nfrequent in the training data. To lessen this problem, \n\n\nwe follow a {\\it controlled sampling} approach where sampling is\nguided by the word lattice $W_q$. Once a word $w$ from a path $e$ in\n$W_q$ is sampled, all other parallel or conflicting paths to $e$ are\nremoved from $W_q$. For example, generating for the word lattice in\nFigure \\ref{fig:wordlattice}, when we sample the word {\\it citizens},\nwe drop out paths {\\it ``human beings''}, {\\it ``people's''}, {\\it\n  ``the population''}, {\\it ``people''} and {\\it ``members of the\n  public''} from $W_q$ and accordingly update the grammar. The\ncontrolled sampling ensures that each sampled question uses words from\na single start-to-end path in $W_q$. For example, we could sample a\nquestion {\\it what is Czech Republic 's language?} by sampling words\nfrom the path {\\it (what, language, do, people 's, in, Czech,\n  Republic, is speaking, ?)} in Figure\n\\ref{fig:wordlattice}. \\ignore{We note that the controlled sampling\n  may lead to a failed derivation. The derivation of a tree could\n  introduce an interminal which will fail to complete because some\n  words from in $W_q$ were pruned out since the interminal was first\n  introduced. Breadth-first search makes it faster to identify an\n  unsuccessful derivation.} We repeat this sampling process to\ngenerate multiple potential paraphrases.\n\n\\ignore{\nOur generation algorithm differs from existing literature\n\\cite{langkilde1998,barzilay2003,pang2003,quirk-2004} on the usage of\nword lattices to generate paraphrases. Existing systems often use\nalternative paths between two nodes in word lattices to generate\nparaphrases. In contrast, we sample words from a path in the word\nlattice using a grammar regardless of their order in the path. For\nexample, we could sample a question {\\it what is Czech Republic 's\n  language?} by sampling words from the path {\\tt (what, language, do,\n  people 's, in, Czech, Republic, is speaking, ?)} in Figure\n\\ref{fig:wordlattice}.\n}\n\nThe resulting generation algorithm has multiple advantages over\nexisting grammar generation methods. First, the sampling from an\nL-PCFG grammar lessens the lexical ambiguity problem evident in\nlexicalized grammars such as TAG \\cite{nargar-2012} and CCG\n\\cite{white-2004}. Our grammar is not lexicalized, only unary\ncontext-free rules are lexicalized. Second, the top-down sampling\nrestricts the combinatorics inherent to bottom-up search\n\\cite{Shieber1990}. Third, we do not restrict the generation by the\norder information in the input. The lack of order information in the\ninput often raises the high combinatorics in lexicalist approaches\n\\cite{kay-1996}. In our case, however, we use sampling to reduce this\nproblem, and it allows us to produce syntactically diverse\nquestions. And fourth, we impose no constraints on the grammar thereby\nmaking it easier to maintain bi-directional (recursive) grammars that\ncan be used both for parsing and for generation \\cite{shieber1988}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[htbp]\n  \\begin{center}\n    \\begin{tiny}\n    \\hspace{-0.7cm}  \\begin{tabular}{lll}\n      \\tikzset{level distance=20pt, sibling distance=0pt}\n        \\Tree [.SBARQ-33-403 [.WHNP-7-291 [.WP-7-254 what ] [.NN-45-142 day ] ] [.SQ-8-925 [.AUX-22-300 is ] [.NN-41-854 nochebuena ] ] ]\n        &\n        \\tikzset{level distance=20pt, sibling distance=0pt}\n        \\Tree [.SBARQ-30-403 [.WRB-42-707 when ] [.SQ-8-709 [.AUX-12-300 is ] [.NN-41-854 nochebuena ] ] ]\n        &\n        \\tikzset{level distance=20pt, sibling distance=0pt}\n        \\Tree [.SBARQ-24-403 [.WRB-42-707 when ] [.SQ-17-709 [.SQ-15-931 [.AUX-29-300 is ] [.NN-30-854 nochebuena ] ] [.JJ-18-579 celebrated ] ] ]\n      \\end{tabular}\n    \\end{tiny}\n    \\end{center}\n    \\caption{Trees used for bi-layered L-PCFG training. The questions {\\it what day is nochebuena}, {\\it\n      when is nochebuena} and {\\it when is nochebuena celebrated} are\n    paraphrases from the Paralex corpus. Each nonterminal is\n    decorated with a syntactic label and two identifiers, e.g., for\n    WP-7-254, WP is the syntactic label assigned by the BLLIP parser,\n    7 is the syntactic latent state, and 254 is the semantic latent\n    state.}\n  \\label{fig:hybridtrees}\n  \\vspace{-0.4cm}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.2cm}\n\\subsection{Bi-Layered L-PCFGs}\n\\label{subsec:hybrid}\n\\vspace{-0.1cm}\n\nAs mentioned earlier, one of our lattice types is based on bi-layered\nPCFGs introduced here.\n\nIn their traditional use, the latent states in L-PCFGs aim to capture\nsyntactic information. We introduce here the use of an L-PCFG with two\nlayers of latent states: one layer is intended to capture the usual\nsyntactic information, and the other aims to capture semantic and\ntopical information by using a large set of states with specific\nfeature functions.\\footnote{For other cases of separating syntax from\n  semantics in a similar way, see \\newcite{mitchell-15}.}\n\n\\ignore{\nWe define an L-PCFG ${G_{\\mathrm{layered}}}$ with parameters $({\\cal N}, {\\cal I}, {\\cal P},\nm_{hyb}, n, \\pi_{hyb}, t_{hyb}, q_{hyb})$ as a hybrid between an\nL-PCFG ${G_{\\mathrm{syn}}}$ for constituency parsing with parameters $({\\cal N},\n{\\cal I}, {\\cal P}, m_{con}, n, \\pi_{con}, t_{con}, q_{con})$ and an L-PCFG\n${G_{\\mathrm{par}}}$ for paraphrasing with parameters $({\\cal N}, {\\cal I}, {\\cal P},\nm_{par}, n, \\pi_{par}, t_{par}, q_{par})$. We have shown how we train\n${G_{\\mathrm{syn}}}$ in \\S\\ref{subsec:samplegrammar}. Here, we describe how we\ntrain ${G_{\\mathrm{par}}}$ on the Paralex corpus, and subsequently ${G_{\\mathrm{layered}}}$.\n\n${G_{\\mathrm{par}}}$ aims to capture the paraphrase information about the\nnonterminals in the grammar. The intuition behind this grammar is that\nthe nonterminals, situated in the context of paraphrases and yielding\nparaphrases, should observe same latent state. For example, in Figure\n\\ref{fig:hybridtrees}, the nonterminal {\\tt SBARQ} in all three trees\nyields paraphrases {\\it what day is nochebuena}, {\\it when is\n  nochebuena} and {\\it when is nochebuena celebrated}, hence {\\tt\n  SBARQ} in all three trees should observe same latent state. Given a\nparallel corpus of aligned paraphrase pairs where these three\nsentences are aligned to each other, {\\tt SBARQ}s could learn this\nparaphrase information from their word alignments. With this\nintuition, we define the inside feature functions $\\phi$ as a mapping\nfrom an inside tree to a bag of words consisting of the direct and\naligned yields of the inside tree. Similarly, we define the outside\nfeature function $\\psi$ for outside trees.\n\n}\n\nTo create the bi-layered L-PCFG, we again use the spectral algorithm\nof \\newcite{narayan-15} to estimate a grammar ${G_{\\mathrm{par}}}$ from the Paralex\ncorpus. We use the word alignment of paraphrase question pairs in\nParalex to map inside and outside trees of each nonterminals in the\ntreebank to bag of word features. The number of latent states we use\nis 1,000.\n\nOnce the two feature functions (syntactic in ${G_{\\mathrm{syn}}}$ and semantic in\n${G_{\\mathrm{par}}}$) are created, each nonterminal in the training treebank is\nassigned two latent states (cluster identifiers). Figure\n\\ref{fig:hybridtrees} shows an example annotation of trees for three\nparaphrase questions from the Paralex corpus. We compute the\nparameters of the bi-layered L-PCFG ${G_{\\mathrm{layered}}}$ with a simple frequency\ncount maximum likelihood estimate over this annotated treebank. As\nsuch, ${G_{\\mathrm{layered}}}$ is a combination of ${G_{\\mathrm{syn}}}$ and ${G_{\\mathrm{par}}}$, resulting in\n24,000~latent states (24~syntactic x 1000~semantic).\n\n\n\n\n\n\nConsider an example where we want to generate paraphrases for the\nquestion {\\it what day is nochebuena}. Parsing it with ${G_{\\mathrm{layered}}}$ will\nlead to the leftmost hybrid structure as shown in Figure\n\\ref{fig:hybridtrees}. The assignment of the first latent states for\neach nonterminals ensures that we retrieve the correct syntactic\nrepresentation of the sentence. Here, however, we are more interested\nin the second latent states assigned to each nonterminals which\ncapture the paraphrase information of the sentence at various levels.\nFor example, we have a unary lexical rule {\\tt (NN-*-142 day)}\nindicating that we observe {\\it day} with {\\tt NN} of the paraphrase\ntype {\\tt 142}. We could use this information to extract unary rules\nof the form {\\tt (NN-*-142 $w$)} in the treebank that will generate\nwords $w$ which are paraphrases to {\\it day}. Similarly, any node {\\tt\n  WHNP-*-291} in the treebank will generate paraphrases for {\\it what\n  day}, {\\tt SBARQ-*-403}, for {\\it what day is nochebuena}. This way\nwe will be able to generate paraphrases {\\it when is nochebuena} and\n{\\it when is nochebuena celebrated} as they both have {\\tt\n  SBARQ-*-403} as their roots.\\footnote{We found out that our ${G_{\\mathrm{par}}}$\n  grammar is not fine-grained enough and often merges different\n  paraphrase information into the same latent state. This problem is\n  often severe for nonterminals at the top level of the bilayered\n  tree. \\ignore{For example, consider a tree {\\tt (SBARQ-1-403\n      (WRB-23-103 where) (SQ-22-809 (SQ-21-910 (AUX-10-866 was)\n      (NP-24-60 (NNP-21-567 gabuella) (NNP-21-290 montez)))\n      (VBN-29-682 born)))} for the sentence {\\it where was gabuella\n      montez born} in the treebank. The root of the tree is assigned\n    {\\tt SBARQ-*-403} but it is not a paraphrase to the sentence {\\it\n      what day is nochebuena}. } Hence, we rely only on unary lexical\n  rules (the rules that produce terminal nodes) to extract paraphrase\n  patterns in our experiments.}\n\n\n\n\n\n\nTo generate a word lattice $W_q$ for a given question $q$, we parse\n$q$ with the bi-layered grammar ${G_{\\mathrm{layered}}}$. For each rule of the form\n$X$-$m_1$-$m_2 \\rightarrow w$ in the bi-layered tree with $X \\in {\\cal P}$,\n$m_1 \\in \\{ 1, \\ldots, 24 \\}$, $m_2 \\in \\{ 1, \\ldots, 1000 \\}$ and $w$\na word in $q$, we extract rules of the form $X$-$*$-$m_2 \\rightarrow\nw'$ from ${G_{\\mathrm{layered}}}$ such that $w' \\neq w$. For each such $(w, w')$, we\nadd a path $w'$ parallel to $w$ in the word lattice.\n\n\n\n\n\n\n\n\n\\ignore{\nExisting systems often use the multi-sequence alignment technique\n\\cite{Bangalore2001,barzilay2002} or a syntax-based alignment approach\n\\cite{pang2003} over multiple English translations of the same source\ntext to produce word lattices. However, these lattices only capture\nparaphrase patterns locally from the translation set and generate new\nsentences for this set of sentences. In contrast, we believe that our\nbi-layered model has stronger generative capacity to induce\nparaphrases, in that our latent states indicating paraphrases are\nlearned from a large-scale parallel corpus. With the help of our\nmodel, we can generate paraphrases for unseen sentences. \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Paraphrase Classification}\n\\label{subsec:classifier}\n\nOur sampling algorithm overgenerates paraphrases which are\nincorrect. To improve its precision, we build a binary classifier to\nfilter the generated paraphrases. We randomly select 100 distinct\nquestions from the Paralex corpus and generate paraphrases using our\ngeneration algorithm with various lattice settings. We randomly select\n1,000 pairs of input-sampled sentences and manually annotate them as\n``correct'' or ``incorrect'' paraphrases.  We train our classifier on\nthis manually created training data.\\footnote{We do not use the\n  paraphrase pairs from the Paralex corpus to train our classifier, as\n  they do not represent the distribution of our sampled paraphrases\n  and the classifier trained on them performs poorly.}\n\n\nWe follow \\newcite{madnani2012}, who used MT metrics for paraphrase\nidentification, and experiment with 8 MT metrics as features for our\nbinary classifier.\n\n\n\n\n\nIn addition, we experiment with a binary feature which checks if the\nsampled paraphrase preserves named entities from the input sentence.\n\n\n\nWe use WEKA \\cite{hall2009weka} to replicate the classifier of\n\\newcite{madnani2012} with our new feature.\n\n\n\n\n\n\n\nWe tune the feature set for our classifier on the development data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      \n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.1cm}\n\\section{Semantic Parsing using Paraphrasing}\n\\vspace{-0.1cm}\n\\label{sec:qaframework}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}\n  \\centering\n  \\begin{minipage}{0.6\\textwidth}\n    \\begin{subfigure}{\\textwidth}\n      \\includegraphics[width=\\textwidth]{czech_question_ungrounded_graph}\n      \\caption{Input sentence: What language do people in Czech Republic speak?}\n      \\label{fig:originalGraph}\n    \\end{subfigure} \\\\\n    \\begin{subfigure}{0.48\\textwidth}\n      \\addtocounter{subfigure}{+1}\n      \\vspace{0.4cm}\n      \\includegraphics[width=\\textwidth]{czech_paraphrase_possesive_ungrounded_graph}\n      \\caption{Paraphrase: What is Czech Republic's language?}\n      \\label{fig:paraphraseGraph2}\n    \\end{subfigure}\n    \\begin{subfigure}{0.48\\textwidth}\n      \\includegraphics[width=1.2\\textwidth]{czech_question_grounded_graph}\n      \\caption{Freebase grounded graph}\n      \\label{fig:groundedGraph}\n    \\end{subfigure}\n  \\end{minipage}\n  \\hspace{1cm}\n  \\begin{minipage}{0.32\\textwidth}\n    \\vspace{0.3cm}\n    \\begin{subfigure}{\\textwidth}\n      \\addtocounter{subfigure}{-3}\n      \\includegraphics[width=\\textwidth]{czech_paraphrase_ungrounded_graph}\n      \\caption{Paraphrase: What language do people speak in Czech Republic?}\n      \\label{fig:paraphraseGraph1}\n    \\end{subfigure}\n  \\end{minipage}\n  \\caption{Ungrounded graphs for an input question and its paraphrases\n    along with its correct grounded graph. The green squares indicate\n    NL or Freebase entities, the yellow rectangles indicate\n    unary NL predicates or Freebase types, the circles indicate NL or\n    Freebase events, the edge labels indicate binary NL predicates or\n    Freebase relations, and the red diamonds attach to the entity of\n    interest (the answer to the question).}\n  \\label{fig:paraphraseQA}\n  \\vspace{-0.4cm}\n\\end{figure*}\n\nIn this section we describe how the paraphrase algorithm is used for\nconverting natural language to Freebase queries. Following\n\\newcite{reddy_largescale_2014}, we formalize the semantic parsing\nproblem as a graph matching problem, i.e., finding the Freebase\nsubgraph (grounded graph) that is isomorphic to the input question\nsemantic structure (ungrounded graph).\n\nThis formulation has a major limitation that can be alleviated by\nusing our paraphrase generation algorithm. Consider the question\n\\emph{What language do people in Czech Republic speak?}. The\nungrounded graph corresponding to this question is shown in\nFigure~\\ref{fig:originalGraph}. The Freebase grounded graph which\nresults in correct answer is shown in Figure~\\ref{fig:groundedGraph}. Note\nthat these two graphs are non-isomorphic making it impossible to\nderive the correct grounding from the ungrounded graph. In fact, at\nleast 15\\% of the examples in our development set fail to satisfy\nisomorphic assumption. In order to address this problem, we use\nparaphrases of the input question to generate additional ungrounded\ngraphs, with the aim that one of those paraphrases will have a\nstructure isomorphic to the correct\ngrounding. Figure~\\ref{fig:paraphraseGraph1} and Figure~\\ref{fig:paraphraseGraph2}\nare two such paraphrases which can be converted to\nFigure~\\ref{fig:groundedGraph} as described in \\Cref{sec:groundedGraphs}.\n\nFor a given input question, first we build ungrounded graphs from its\nparaphrases.  We convert these graphs to Freebase graphs. To learn\nthis mapping, we rely on manually assembled question-answer pairs. For\neach training question, we first find the set of \\emph{oracle}\ngrounded graphs---Freebase subgraphs which when executed yield the\ncorrect answer---derivable from the question's ungrounded\ngraphs. These oracle graphs are then used to train a structured\nperceptron model. These steps are discussed in detail below.\n\n\\subsection{Ungrounded Graphs from Paraphrases}\n\\label{sec:ungroundedGraphs}\nWe use {\\textsc{GraphParser}\\xspace} \\cite{reddy_largescale_2014} to convert paraphrases to\nungrounded graphs. This conversion involves three steps: 1) parsing\nthe paraphrase using a CCG parser to extract syntactic derivations\n\\cite{lewis_ccg_2014}, 2) extracting logical forms from the CCG\nderivations \\cite{bos_widecoverage_2004}, and 3) converting the\nlogical forms to an ungrounded graph.\\footnote{Please see\n  \\newcite{reddy_largescale_2014} for more details.} The ungrounded\ngraph for the example question and its paraphrases are shown in\nFigure~\\ref{fig:originalGraph}, Figure~\\ref{fig:paraphraseGraph1} and\nFigure~\\ref{fig:paraphraseGraph2}, respectively.\n\n\\subsection{Grounded Graphs from Ungrounded Graphs}\n\\label{sec:groundedGraphs}\nThe ungrounded graphs are grounded to Freebase subgraphs by mapping\nentity nodes, entity-entity edges and entity type nodes in the\nungrounded graph to Freebase entities, relations and types,\nrespectively. For example, the graph in Figure~\\ref{fig:paraphraseGraph1}\ncan be converted to a Freebase graph in Figure~\\ref{fig:groundedGraph} by\nreplacing the entity node \\textit{Czech Republic} with the Freebase\nentity \\textsc{CzechRepublic}, the edge\n\\textit{(speak.arg$_2$,~speak.in)} between $x$ and \\textit{Czech\n  Republic} with the Freebase relation\n\\textit{(location.country.official\\_language.2,~location.country.official\\_language.1)},\nthe type node \\textit{language} with the Freebase type\n\\textit{language.human\\_language}, and the \\textsc{target} node\nremains intact. The rest of the nodes, edges and types are grounded to\n\\textit{null}. In a similar fashion, Figure~\\ref{fig:paraphraseGraph2} can\nbe grounded to Figure~\\ref{fig:groundedGraph}, but not\nFigure~\\ref{fig:originalGraph} to Figure~\\ref{fig:groundedGraph}. If at least one\nof the paraphrases is not isomorphic to the target grounded grounded\ngraph, our grounding fails.\n\n\\subsection{Learning}\n\\label{sec:learning}\n\n\n\n\nWe use a linear model to map ungrounded graphs to grounded ones.  The\nparameters of the model are learned from question-answer pairs.  For\nexample, the question \\emph{What language do people in Czech Republic\n  speak?} paired with its answer $\\{\\textsc{CzechLanguage}\\}$. In line\nwith most work on question answering against Freebase, we do not rely\non annotated logical forms associated with the question for training\nand treat the mapping of a question to its grounded graph as latent.\n\nLet $q$ be a question, let $p$ be a paraphrase, let $u$ be an\nungrounded graph for $p$, and let $g$ be a grounded graph formed by\ngrounding the nodes and edges of $u$ to the knowledge base\n$\\mathcal{K}$ (throughout we use Freebase as the knowledge base).\nFollowing \\newcite{reddy_largescale_2014}, we use beam search to find\nthe highest scoring tuple of paraphrase, ungrounded and grounded\ngraphs $(\\hat p, \\hat u, \\hat g)$ under the model $\\theta \\in \\mathbb{R}^n$:\n", "index": 1, "text": "\n\\[\n({\\hat{p},\\hat{u},\\hat{g}}) = {\\operatornamewithlimits{arg\\,max}}_{(p,u,g)} \\theta \\cdot\n\\Phi(p,u,g,q,\\mathcal{K})\\,,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"({\\hat{p},\\hat{u},\\hat{g}})={\\operatornamewithlimits{arg\\,max}}_{(p,u,g)}%&#10;\\theta\\cdot\\Phi(p,u,g,q,\\mathcal{K})\\,,\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><munder><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo movablelimits=\"false\">\u2062</mo><mi>max</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></munder><mo>\u2061</mo><mrow><mi>\u03b8</mi><mo>\u22c5</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mo>,</mo><mi>g</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06068.tex", "nexttext": "\nwhere $({p^+,u^+,g^+})$ denotes the tuple of gold paraphrase, gold\nungrounded and grounded graphs for $q$.  Since we do not have direct\naccess to the gold paraphrase and graphs, we instead rely on the set\nof \\emph{oracle tuples}, $\\mathcal{O}_{\\mathcal{K}, \\mathcal{A}}(q)$,\nas a proxy:\n", "itemtype": "equation", "pos": 38460, "prevtext": "\nwhere $\\Phi(p, u, g, q, \\mathcal{K}) \\in \\mathbb{R}^n$ denotes the\nfeatures for the tuple of paraphrase, ungrounded and grounded graphs.\nThe feature function has access to the paraphrase, ungrounded and\ngrounded graphs, the original question, as well as to the content of\nthe knowledge base and the denotation $|g|_\\mathcal{K}$ (the\ndenotation of a grounded graph is defined as the set of entities or\nattributes reachable at its \\textsc{target} node).  See\n\\Cref{sec:details} for the features employed.  The model parameters\nare estimated with the averaged structured perceptron\n\\cite{collins_discriminative_2002}.  Given a training question-answer\npair $(q,\\mathcal{A})$, the update is:\n", "index": 3, "text": "\n\\[\n\\theta^{t+1} \\leftarrow \\theta^{t} + \\Phi(p^+, u^+, g^+, q,\n\\mathcal{K}) - \\Phi(\\hat{p}, \\hat{u}, \\hat{g}, q, \\mathcal{K})\\,,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\theta^{t+1}\\leftarrow\\theta^{t}+\\Phi(p^{+},u^{+},g^{+},q,\\mathcal{K})-\\Phi(%&#10;\\hat{p},\\hat{u},\\hat{g},q,\\mathcal{K})\\,,\" display=\"block\"><mrow><mrow><msup><mi>\u03b8</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>\u2190</mo><mrow><mrow><msup><mi>\u03b8</mi><mi>t</mi></msup><mo>+</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>p</mi><mo>+</mo></msup><mo>,</mo><msup><mi>u</mi><mo>+</mo></msup><mo>,</mo><msup><mi>g</mi><mo>+</mo></msup><mo>,</mo><mi>q</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>u</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mover accent=\"true\"><mi>g</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mi>q</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06068.tex", "nexttext": "\nwhere $\\mathcal{O}_{\\mathcal{K}, \\mathcal{A}}(q)$ is defined as the\nset of tuples ($p$, $u$, $g$) derivable from the question $q$, whose\ndenotation $|g|_\\mathcal{K}$ has minimal $F_1$-loss against the gold\nanswer $\\mathcal{A}$.  We find the oracle graphs for each question a\npriori by performing beam-search with a very large beam.\n\n\\section{Experimental Setup}\n\\label{sec:exp}\n\\vspace{-0.1cm}\n\nBelow, we give details on the evaluation dataset and baselines used\nfor comparison. We also describe the model features and provide\nimplementation details.\n\n\\subsection{Evaluation Data and Metric}\n\\label{sec:evaldata}\n\nWe evaluate our approach on the {WebQuestions\\xspace} \\cite{berant_semantic_2013}\ndataset. {WebQuestions\\xspace} consists of 5,810 question-answer pairs where questions\nrepresents real Google search queries. We use the standard train/test\nsplits, with 3,778~train and 2,032~test questions. For our development\nexperiments we tune the models on held-out data consisting of\n30\\%~training questions, while for final testing we use the complete\ntraining data. We use average precision (avg P.), average recall (avg\nR.) and average F$_1$ (avg F$_1$) proposed by\n\\newcite{berant_semantic_2013} as evaluation\nmetric.\\footnote{\\url{https://github.com/percyliang/sempre/blob/master/scripts/evaluation.py}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Baselines}\n\n\\paragraph{\\textsc{original}} \nWe use {\\textsc{GraphParser}\\xspace} without paraphrases as our baseline. This gives an idea about the impact\nof using paraphrases.\n\n\n\n\n\\paragraph{\\textsc{mt}} \nWe compare our paraphrasing models with monolingual machine\ntranslation based model for paraphrase generation\n\\cite{quirk-2004,wubben:2010}. In particular, we use Moses\n\\cite{Koehn-2007} to train a monolingual phrase-based MT system on the\nParalex corpus. Finally, we use Moses decoder to generate\n$10$-best distinct paraphrases for the test questions. We tune the\nnumber of distinct paraphrases to sample on the development data.\n\n\\subsection{Implementation Details}\n\\label{sec:details}\n\n\n\n\n\n \n\\paragraph{Entity Resolution}\n\n\nFor {WebQuestions\\xspace}, we use 8~handcrafted part-of-speech patterns (e.g., the\npattern {\\tt (DT)?(JJ.?$\\mid$NN.?)\\{0,2\\}NN.?} matches the noun phrase\n\\textit{the big lebowski}) to identify candidate named entity mention\nspans.  We use the Stanford CoreNLP caseless tagger for part-of-speech\ntagging \\cite{manning-2014-corenlp}.  For each candidate mention span,\nwe retrieve the top 10 entities according to the Freebase\nAPI.\\footnote{\\scriptsize\\url{http://developers.google.com/freebase/}}\nWe then create a lattice in which the nodes correspond to\nmention-entity pairs, scored by their Freebase API scores, and the\nedges encode the fact that no joint assignment of entities to mentions\ncan contain overlapping spans. We take the top 10 paths through the\nlattice as possible entity disambiguations. For each possibility, we\ngenerate $n$-best paraphrases that contains the entity mention\nspans. In the end, this process creates a total of\n$10n$~paraphrases. We generate ungrounded graphs for these paraphrases\nand treat the final entity disambiguation and paraphrase selection as\npart of the semantic parsing problem.\\footnote{To generate ungrounded\n  graphs for a paraphrase, we treat each entity mention as a single\n  word.}\n\n\\paragraph{{\\textsc{GraphParser}\\xspace} Features.}  \nWe use the features from \\newcite{reddy_largescale_2014}.  These\ninclude edge alignments and stem overlaps between ungrounded and\ngrounded graphs, and contextual features such as word and grounded\nrelation pairs.  In addition to these features, we add two new real-valued\nfeatures -- the paraphrase classifier's score and the entity\ndisambiguation lattice score.\n\n\\paragraph{Beam Search} \nWe use beam search to infer the highest scoring graph pair for a\nquestion.  The search operates over entity-entity edges and entity\ntype nodes of each ungrounded graph.  For an entity-entity edge, there\nare two operations: ground the edge to a Freebase relation, or skip\nthe edge. Similarly, for an entity type node, there are two\noperations: ground the node to a Freebase type, or skip the node. We\nuse a beam size of 100 in all our experiments.\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Results and Discussion}\n\\vspace{-0.1cm}\n\\label{sec:results}\n\nIn this section, we present results from five different systems for\nour question-answering experiments: \\textsc{original}, \\textsc{mt},\n\\textsc{naive}, \\textsc{ppdb} and \\textsc{bilayered}. First two are\nbaseline systems. Other three systems are {\\textsc{GraphParser}\\xspace} with paraphrases\ngenerated from an L-PCFG grammar. \\textsc{naive} uses a word\nlattice with a single start-to-end path representing the input\nquestion itself, \\textsc{ppdb} uses a word lattice constructed\nusing the PPDB rules, and \\textsc{bilayered} uses bi-layered\nL-PCFG to build word lattices. Note that \\textsc{naive} does not\nrequire any parallel resource to train, \\textsc{ppdb} requires an\nexternal paraphrase database, and \\textsc{bilayered}, like\n\\textsc{mt}, needs a parallel corpus with paraphrase pairs. We tune\nour classifier features and {\\textsc{GraphParser}\\xspace} features on the development data. We\nuse the best setting from tuning for evaluation on the test data.\n\n\\paragraph{Results on the Development Set}\n\nTable~\\ref{tab:devResults} shows the results with our best settings on\nthe development data. We found that oracle scores improve\nsignificantly with paraphrases. \\textsc{original} achieves an oracle\nscore of $65.1$ whereas with paraphrases we achieve an F$_1$ greater\nthan $70$ across all the models. This shows that with paraphrases we\neliminate substantial mismatch between Freebase and ungrounded\ngraphs. This trend continues for the final prediction with the\nparaphrasing models performing better than the \\textsc{original}.\n\nAll our proposed paraphrasing models beat the \\textsc{mt}\nbaseline. Even the \\textsc{naive} model which does not use any\nparallel or external resource surpass the \\textsc{mt} baseline in the\nfinal prediction. Upon error analysis, we found that the \\textsc{mt}\nmodel produce too similar paraphrases, mostly with only inflectional\nvariations. For the question \\textit{What language do people in Czech\n  Republic speak}, the top ten paraphrases produced by \\textsc{mt} are\nmostly formed by replacing words \\textit{language} with\n\\textit{languages}, do with \\textit{does}, \\textit{people} with\n\\textit{person} and \\textit{speak} with \\textit{speaks}. These\nparaphrases do not address the structural mismatch problem. In\ncontrast, our grammar based models generate syntactically diverse\nparaphrases.\n\n\nOur \\textsc{ppdb} model performs best across the paraphrase models\n(avg F$_1$ = $47.9$). We attribute its success to the high quality\nparaphrase rules from the external paraphrase database. For the\n\\textsc{bilayerd} model we found 1,000~latent semantic states is not\nsufficient for modeling topical differences. Though \\textsc{mt}\ncompetes with \\textsc{naive} and \\textsc{bilayered}, the performance\nof \\textsc{naive} is highly encouraging since it does not require any\nparallel corpus. Furthermore, we observe that the \\textsc{mt} model\nhas larger search space. The number of oracle graphs -- the number of\nways in which one can produce the correct Freebase grounding from the\nungrounded graphs of the given question and its paraphrases -- is\nhigher for \\textsc{mt}~(77.2) than the grammar-based models~(50--60).\n\n\\paragraph{Results on the Test Set}\n\nTable~\\ref{tab:mainResults} shows our final results on the test\ndata. We get similar results on the test data as we reported on the\ndevelopment data. Again, the \\textsc{ppdb} model performs best with an\nF$_1$ score of $47.7$. The baselines, \\textsc{original} and\n\\textsc{mt}, lag with scores of $45.0$ and $47.1$, respectively. We\nalso present the results of existing literature on this dataset. Among\nthese, \\newcite{berant_semantic_2014} also uses paraphrasing but\nunlike ours it is based on a template grammar (containing 8~grammar\nrules) and requires logical forms beforehand to generate\nparaphrases. Our \\textsc{ppdb} outperforms Berant and Liang's model\nby~7.8~F$_1$ points. \\newcite{yih_semantic_2015} remains the current\nstate-of-the-art on this dataset. Yih et al. use continuous word\nrepresentations for input words and a neural network model for\nsemantic parse generation. They also uses sophisticated entity\nresolution \\cite{yang_smart_2015} and a very large unsupervised corpus\nas additional training data. Note that we use {\\textsc{GraphParser}\\xspace} as our semantic\nparsing framework for evaluating our paraphrases extrinsically. We\nleave plugging our paraphrases to other existing methods and other\ntasks for future work. One interesting observation is that recall of\n\\textsc{ppdb} only lags behind \\newcite{bast_more_2015} and\n\\newcite{yih_semantic_2015}.\n\n\\paragraph{Error Analysis} The upper bound of our paraphrasing methods\nis in the range of 71.2--71.8. We examine the reason where we lose the\nrest. For the \\textsc{ppdb} model, the majority~(78.4\\%) of the errors\nare partially correct answers occurring due to incomplete gold answer\nannotations or partially correct groundings. 13.5\\%~are due to bad\nparaphrases, and the rest~(8.1\\%) are due to wrong entity annotations.\n\n\\begin{table}\n  \\small\n  \n  \\begin{tabular}{lx{0.7in}x{0.5in}x{0.5in}}\n    \\toprule\n    Method & avg oracle F$_1$ & \\# oracle graphs & avg F$_1$ \\\\\n    \\midrule\n    \\textsc{original} & 65.1 & 11.0 & 44.7 \\\\\n    \\textsc{mt}  & 71.5 & 77.2 & 47.0 \\\\\n    \\textsc{naive} & 71.2 & 53.6 & 47.5 \\\\\n    \\textsc{ppdb} & 71.8 & 59.8 & 47.9 \\\\\n    \\textsc{bilayered} & 71.6 & 55.0 & 47.1 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Oracle statistics and results on the {WebQuestions\\xspace} development\n    set.}\n\\label{tab:devResults}\n\\vspace{-0.5em}\n\\end{table}\n\n\\begin{table}\n  \\centering\n  \\small\n  \n  \\begin{tabular}{l|ccc}\n    \\hline \n    Method & avg P. & avg R. & avg F$_1$ \\\\ \n    \\hline\n    \n    Berant and Liang '14\\nocite{berant_semantic_2014} & 40.5 & 46.6 & 39.9 \\\\\n    Bordes et al. '14\\nocite{bordes_question_2014} & - & - & 39.2 \\\\\n    Dong et al. '15\\nocite{dong_question_2015}   & - & - & 40.8 \\\\\n    Yao '15\\nocite{yao_lean_2015}        & 52.6 & 54.5 & 44.3 \\\\\n    Bao et al. '15\\nocite{bao_knowledgebased_2014} & 44.7 & 52.5 & 45.3 \\\\ \n    Bast and Haussmann '15\\nocite{bast_more_2015}       & 49.8 & 60.4 & 49.4 \\\\\n    Berant and Liang '15\\nocite{berant_imitation_2015}& 50.4 & 55.7 & 49.7 \\\\\n    Yih et al. '15\\nocite{yih_semantic_2015} & 52.8 & 60.7 & 52.5 \\\\ \n    \\hline\n    \\multicolumn{4}{c}{This paper} \\\\\n    \\hline\n    \\textsc{original} & 53.2 & 54.2 & 45.0 \\\\\n    \\textsc{mt} & 48.0 & 56.9 & 47.1 \\\\\n    \\textsc{naive} & 48.1 & 57.7 & 47.2 \\\\\n    \\textsc{ppdb} & 48.4 & 58.1 & 47.7 \\\\\n    \\textsc{bilayered} & 47.0 & 57.6 & 47.2 \\\\ \n    \\hline\n  \\end{tabular}\n  \\caption{Results on {WebQuestions\\xspace} test dataset.}\n  \\vspace{-0.5em}\n  \\label{tab:mainResults}\n\\end{table}\n\n\n\n\\section{Conclusion}\n\\vspace{-0.2cm}\n\\label{sec:conclusion}\n\nWe described a grammar method to generate paraphrases for questions,\nand applied it to a question answering system based on semantic\nparsing. We showed that using paraphrases for a question answering\nsystem is a useful way to improve its performance. Our method is\nrather generic and can be applied to any question answering system.\n\n\\section*{Acknowledgements}\n\nThe authors would like to thank Nitin Madnani for his help with the\nimplementation of the paraphrase classifier. This research was\nsupported by an EPSRC grant (EP/L02411X/1), and a Google PhD\nFellowship for the second author.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{naaclhlt2016}\n\n\\bibliography{gen-qa}\n\n\n", "itemtype": "equation", "pos": 38879, "prevtext": "\nwhere $({p^+,u^+,g^+})$ denotes the tuple of gold paraphrase, gold\nungrounded and grounded graphs for $q$.  Since we do not have direct\naccess to the gold paraphrase and graphs, we instead rely on the set\nof \\emph{oracle tuples}, $\\mathcal{O}_{\\mathcal{K}, \\mathcal{A}}(q)$,\nas a proxy:\n", "index": 5, "text": "\n\\[\n(p^{+},u^{+},{g^{+}}) = {\\operatornamewithlimits{arg\\,max}}_{(p,u,g) \\in\n  \\mathcal{O}_{\\mathcal{K},\\mathcal{A}}(q)} \\theta \\cdot\n\\Phi({p,u,g,q,\\mathcal{K}})\\,,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"(p^{+},u^{+},{g^{+}})={\\operatornamewithlimits{arg\\,max}}_{(p,u,g)\\in\\mathcal{%&#10;O}_{\\mathcal{K},\\mathcal{A}}(q)}\\theta\\cdot\\Phi({p,u,g,q,\\mathcal{K}})\\,,\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mi>p</mi><mo>+</mo></msup><mo>,</mo><msup><mi>u</mi><mo>+</mo></msup><mo>,</mo><msup><mi>g</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mrow><munder><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo movablelimits=\"false\">\u2062</mo><mi>max</mi></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mrow><mi>\u03b8</mi><mo>\u22c5</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mo>,</mo><mi>g</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]