[{"file": "1601.00248.tex", "nexttext": "\n\n\nThe second metric, perplexity(per word), is an information theoretic\nmeasure that evaluates the similarity of proposed model $m$ to the\noriginal distribution $p$. It can be computed as a inverse of (geometric)average\nprobability of test set $T$ \n\\begin{eqnarray}\nPPL(D) & = & \\sqrt[N]{\\frac{1}{m(T)}}\\nonumber \\\\\n & = & 2^{-\\frac{1}{N}lg(m(T))}\\label{eq:ppl_def}\n\\end{eqnarray}\nwhere $N$ is the number of words in test set $T$.\n\nEquation \\ref{eq:ppl_def} can be seen as exponentiated cross entropy,\nwhere cross entropy $H(p,m)$ is approximated as \n", "itemtype": "equation", "pos": 1581, "prevtext": "\n\\maketitle \n\\begin{abstract}\nPerplexity(per word) is the most widely used metric for evaluating\nlanguage models. This is mostly due to a its ease of computation,\nlack of dependence on external tools like speech recognition pipeline\nand a good theoretical justification for why it should work. Despite\nthis, there has been no dearth of criticism for this metric. Most\nof this criticism center around lack of correlation with extrinsic\nmetrics like word error rate(WER), dependence upon shared vocabulary\nfor model comparison and unsuitability for un-normalized language\nmodel evaluation. In this paper we address the last problem of inability\nto evaluate un-normalized models by introducing a new discriminative\nevaluation metric that predicts model's performance based on its ability\nto discriminate between test sentences and their deformed version.\nDue to its discriminative formulation, this approach can work with\nun-normalized probabilities while retaining perplexity's ease of computation.\nWe show a strong correlation between our new metric and perplexity\nacross a range of models on WSJ datasets. We also hypothesize a stronger\ncorrelation between WER and our new metric vis-a-vis perplexity due\nto similar discriminative objective.\n\\end{abstract}\n\n\\section{Introduction}\n\nThere are two standard evaluation metrics for language models: perplexity\nor word error rate(WER). The simpler of these measures, WER, is simply\nthe percentage of erroneously recognized words $E$ (deletions, insertions,\nsubstitutions) to total number of words $N$, in a speech recognition\ntask i.e.\n\n", "index": 1, "text": "\n\\[\nWER=\\frac{E}{N}\\times100\\%\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"WER=\\frac{E}{N}\\times 100\\%\" display=\"block\"><mrow><mrow><mi>W</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mrow><mfrac><mi>E</mi><mi>N</mi></mfrac><mo>\u00d7</mo><mrow><mn>100</mn><mo lspace=\"0pt\" rspace=\"3.5pt\">%</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00248.tex", "nexttext": "\n\n\nIn many ways, WER is an better metric as any improvement on language\nmodeling benchmarks is meaningful only if it translates in to improvements\nin Automatic Speech Recognition(ASR) or Machine Translation. The problem\nwith WER is that it needs a complete ASR pipeline to evaluate. Also,\nalmost all benchmarking datasets are behind pay-wall, hence not readily\navailable for evaluation.\n\nPerplexity, on the other hand, is a theoretically elegant and easy\nto compute metric which correlates well with WER for simpler n-gram\nmodels. This makes PPL a good substitute for WER when evaluating n-grams\nmodel. For more complex language models, the correlation is not so\nstrong{[}{]}. In addition to this, perplexity is an unsuitable metric\nto evaluate un-normalized models like sentence level models for which\npartition function computation is intractable. Also, two compare two\nmodels using perplexity, they must share the same vocabulary.\n\nMost of the previous work done to improve upon perplexity has been\nfocused on achieving better correlation with WER. Iyer et al. \\cite{iyer1997analyzing}\nproposed a decision tree based metric that uses additional features\nlike word length, POS tags and phonetic length of words to improve\nthe WER correlation. Chen et al. \\cite{chen1998evaluation} propose\na new metric \\emph{M-ref }in which they attempt to learn likelihood\ncurve between WER and perplexity. Clarkson et al.\\cite{clarkson1999towards}\nattempt to use entropy in conjugation with perplexity, empirically\nlearning mixing coefficient. \n\nIn this paper we focus on a different problem of extending perplexity\nto enable it to be used for un-normalized language models. We do so\nby introducing a discriminative approach to language model evaluation.\nOur approach is very much inspired by Contrastive Estimation by \\cite{smith2005contrastive}\nand works on the philosophy that a superior language model would be\nable better to distinguish between the sentence from the test set\nand its slightly deformed version. \n\nIn next Section, we derive our new metric, Contrastive perplexity,\nfrom scratch and give an intuitive understanding of why it should\nwork. In Section 4, we analyze this new metric across various models\non most widely used datasets, namely, Pen-TreeBank section of WSJ\ndataset and Brown Corpus. We report a very strong correlation between\nour new metric and the perplexity. We conclude this paper by hypothesizing\na better correlation between WER and contrastive perplexity due to\nsimilar objective of minimizing the errors in prediction.\n\n\n\\section{Contrastive Entropy\\label{sec:Contrastive-Entropy}}\n\nLet $T$ be the test set. We pass this test set through a noise channel\nand let the distorted version of test set be $\\hat{T}$. Now, we define\nContrastive Entropy can as:\n\\begin{alignat*}{1}\nH_{C}(T) & =H(\\hat{T})-H(T)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{p(\\hat{T})}{p(T)}\\right)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{\\tilde{p}(\\hat{T})/Z}{\\tilde{p}(T)/Z}\\right)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{\\tilde{p}(\\hat{T})}{\\tilde{p}(T)}\\right)\n\\end{alignat*}\n$N$ here is size of test set.\n\nNow, using the definition of Contrastive Entropy Rate, we calculate\nContrastive Perplexity as:\n", "itemtype": "equation", "pos": 2164, "prevtext": "\n\n\nThe second metric, perplexity(per word), is an information theoretic\nmeasure that evaluates the similarity of proposed model $m$ to the\noriginal distribution $p$. It can be computed as a inverse of (geometric)average\nprobability of test set $T$ \n\\begin{eqnarray}\nPPL(D) & = & \\sqrt[N]{\\frac{1}{m(T)}}\\nonumber \\\\\n & = & 2^{-\\frac{1}{N}lg(m(T))}\\label{eq:ppl_def}\n\\end{eqnarray}\nwhere $N$ is the number of words in test set $T$.\n\nEquation \\ref{eq:ppl_def} can be seen as exponentiated cross entropy,\nwhere cross entropy $H(p,m)$ is approximated as \n", "index": 3, "text": "\n\\[\nH(p,m)=-\\frac{1}{N}lg(m(T))\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"H(p,m)=-\\frac{1}{N}lg(m(T))\" display=\"block\"><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00248.tex", "nexttext": "\n\n\nFor a sentence level language model, the probability distribution\n$m(D)$ can be modeled as product of sentence probabilities, i.e.\n", "itemtype": "equation", "pos": 5370, "prevtext": "\n\n\nIn many ways, WER is an better metric as any improvement on language\nmodeling benchmarks is meaningful only if it translates in to improvements\nin Automatic Speech Recognition(ASR) or Machine Translation. The problem\nwith WER is that it needs a complete ASR pipeline to evaluate. Also,\nalmost all benchmarking datasets are behind pay-wall, hence not readily\navailable for evaluation.\n\nPerplexity, on the other hand, is a theoretically elegant and easy\nto compute metric which correlates well with WER for simpler n-gram\nmodels. This makes PPL a good substitute for WER when evaluating n-grams\nmodel. For more complex language models, the correlation is not so\nstrong{[}{]}. In addition to this, perplexity is an unsuitable metric\nto evaluate un-normalized models like sentence level models for which\npartition function computation is intractable. Also, two compare two\nmodels using perplexity, they must share the same vocabulary.\n\nMost of the previous work done to improve upon perplexity has been\nfocused on achieving better correlation with WER. Iyer et al. \\cite{iyer1997analyzing}\nproposed a decision tree based metric that uses additional features\nlike word length, POS tags and phonetic length of words to improve\nthe WER correlation. Chen et al. \\cite{chen1998evaluation} propose\na new metric \\emph{M-ref }in which they attempt to learn likelihood\ncurve between WER and perplexity. Clarkson et al.\\cite{clarkson1999towards}\nattempt to use entropy in conjugation with perplexity, empirically\nlearning mixing coefficient. \n\nIn this paper we focus on a different problem of extending perplexity\nto enable it to be used for un-normalized language models. We do so\nby introducing a discriminative approach to language model evaluation.\nOur approach is very much inspired by Contrastive Estimation by \\cite{smith2005contrastive}\nand works on the philosophy that a superior language model would be\nable better to distinguish between the sentence from the test set\nand its slightly deformed version. \n\nIn next Section, we derive our new metric, Contrastive perplexity,\nfrom scratch and give an intuitive understanding of why it should\nwork. In Section 4, we analyze this new metric across various models\non most widely used datasets, namely, Pen-TreeBank section of WSJ\ndataset and Brown Corpus. We report a very strong correlation between\nour new metric and the perplexity. We conclude this paper by hypothesizing\na better correlation between WER and contrastive perplexity due to\nsimilar objective of minimizing the errors in prediction.\n\n\n\\section{Contrastive Entropy\\label{sec:Contrastive-Entropy}}\n\nLet $T$ be the test set. We pass this test set through a noise channel\nand let the distorted version of test set be $\\hat{T}$. Now, we define\nContrastive Entropy can as:\n\\begin{alignat*}{1}\nH_{C}(T) & =H(\\hat{T})-H(T)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{p(\\hat{T})}{p(T)}\\right)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{\\tilde{p}(\\hat{T})/Z}{\\tilde{p}(T)/Z}\\right)\\\\\n & =-\\frac{1}{N}lg\\left(\\frac{\\tilde{p}(\\hat{T})}{\\tilde{p}(T)}\\right)\n\\end{alignat*}\n$N$ here is size of test set.\n\nNow, using the definition of Contrastive Entropy Rate, we calculate\nContrastive Perplexity as:\n", "index": 5, "text": "\n\\[\nPPL_{C}(T)=2^{-H_{c}(T)}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"PPL_{C}(T)=2^{-H_{c}(T)}\" display=\"block\"><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><msub><mi>L</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mn>2</mn><mrow><mo>-</mo><mrow><msub><mi>H</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.00248.tex", "nexttext": "\n\n\nNow, contrastive entropy can be modeled as \n\\begin{eqnarray*}\nH_{c}(D) & = & -\\frac{1}{N}\\sum_{w_{d}\\in D}lg\\left(\\frac{m(w_{d})}{m(w_{d})}\\right)\\\\\n & = & -\\frac{1}{N}\\sum_{w_{d}\\in D}lg\\left(\\frac{\\tilde{m}(w_{d})}{\\tilde{m}(w_{d})}\\right)\n\\end{eqnarray*}\n\n\nwhere $\\tilde{m}$ is the un-normalized probability of sentence $w_{d}$.\n\nThe intuition behind our evaluation technique is that the distorted\nsentence $\\hat{W}$, should be seen as a out of domain text and that\na better probability model should be able to distinguish between an\nin-domain sentence from the language versus a malformed sentence that\nis less likely to be generated by the language. \n\nNow we look at distortion generation mechanism. We allow only two\ntype of distortions: substitution and transpositions. For substitutions,\nwe randomly select a word from the vocabulary to substitute. For transposition,\nwe randomly select a word from the same sentence to swap. For each\nword in a sentence there are three possible outcomes: no distortion\nwith probability $x_{\\mathcal{N}}$, substitution with probability\n$x_{S}$ and transposition with probability $x_{T}$ such that $x_{\\mathcal{N}}+x_{S}+x_{T}=1$.\n\n\n\\section{Results}\n\nPen TreeBank corpus is one of the most widely used dataset in statistical\nmodeling community to report perplexity results. We use Pen TreeBank\ndataset with following split and preprocessing: Sections 0-20 were\nused as training data, sections 21-22 for validation and 23-24 for\ntesting. The training, validation and testing token sizes are 930k,\n74k and 82k respectively. Vocabulary is limited to 10k words with\nall words outside this set mapped to a special token\\textbf{ $<unk>$.}\n\nWe start by looking at the distortion process. Table \\ref{tab:Example-sentence}\nshows some example sentences for this dataset and corresponding output\nproduced by the noisy channel. As we can see at 20\\% distortion the\nsentence is still coherent and meaning is still being conveyed. At\n40\\% distortion, it is difficult even for human beings to discern\nwhat original sentence meant to say. This observation clearly indicates\nthat a better language model should have considerably high contrastive\nperplexity for 40 \\% distortion as compared to 20\\%.\n\n\\begin{table}[!h]\n\\centering{}\\caption{Example sentence with 20\\% and 40\\% distortion\\label{tab:Example-sentence}}\n\\begin{tabular*}{1\\columnwidth}{@{\\extracolsep{\\fill}}m{0.3\\columnwidth}m{0.3\\columnwidth}m{0.3\\columnwidth}}\n\\hline \nOriginal Sentence & Sentence with 20\\% distortion & Sentence with 40\\% distortion\\tabularnewline\n\\hline \n\\noalign{\\vskip\\doublerulesep}\nno it was n't black monday & no it deeply black n't monday/ & no it generating proceeds black monday\\tabularnewline[\\doublerulesep]\n\\noalign{\\vskip\\doublerulesep}\n\\noalign{\\vskip\\doublerulesep}\nat the end of the day N million shares were traded & nights the meantime of the day N million shares fourth traded & centrust fundamentals away too encourage to secretary government for\nstop them now\\tabularnewline[\\doublerulesep]\n\\noalign{\\vskip\\doublerulesep}\n\\noalign{\\vskip\\doublerulesep}\nthings have gone too far for the government to stop them now & things have gone too far charles goldberg government to stop them\nopenly & concentrate did a n't get even chance to do we slight the wanted to\nbear\\tabularnewline[\\doublerulesep]\n\\noalign{\\vskip\\doublerulesep}\n\\noalign{\\vskip\\doublerulesep}\nbut stocks kept falling & but ride kept falling & falling stocks kept but\\tabularnewline[\\doublerulesep]\n\\noalign{\\vskip\\doublerulesep}\n\\noalign{\\vskip\\doublerulesep}\nthey never considered themselves to be anything else & they never considered themselves to anything be else & promises never be themselves considered to anything else\\tabularnewline[\\doublerulesep]\n\\noalign{\\vskip\\doublerulesep}\n\\noalign{\\vskip\\doublerulesep}\nbusinesses were borrowing at interest rates higher than their own\nearnings & businesses were borrowing their interest rates higher advise at own\ninvestigated & rates were borrowing intense interest businesses own than their equivalents\nibm\\tabularnewline[\\doublerulesep]\n\\hline \n\\noalign{\\vskip\\doublerulesep}\n\\end{tabular*}\n\\end{table}\n\n\nNow, we look at the contrastive perplexities of various well known\nmodels at different distortion rates. The objective here is to verify\nthe following hypothesis about contrastive perplexity. For a good\nlanguage model contrastive perplexity should rise faster with the\ndistortion level. This is akin to saying that a good language model\nshould do a lot better job at differentiating the language generated\nby the model and distorted language as the distortion level increases.\nAt the same time contrastive perplexity should be somehow correlated\nto perplexity across the models. This means that for same distortion\nlevel, range of models should be ranked similarly on two metrics,\nperplexity and contrastive perplexity. Table \\ref{tab:ppl_ngram}\nshows the results for our experiments. The results were generated\nusing open source language modeling SRILM toolkit\\cite{stolcke2002srilm}\nfor n-gram models and RNNLM toolkit\\cite{mikolov2011rnnlm} for RNN\nbased models. The results shown in Table \\ref{tab:ppl_ngram} were\naveraged for 10 runs.\n\n\\begin{table}[!h]\n\\begin{centering}\n\\caption{Comparing n-gram models and RNNLM model perplexity for different level\nof distortion levels.\\label{tab:ppl_ngram}}\n\n\\par\\end{centering}\n\n\\begin{tabular*}{0.9\\columnwidth}{@{\\extracolsep{\\fill}}p{0.3\\columnwidth}>{\\raggedright}p{0.17\\columnwidth}>{\\raggedright}p{0.1\\columnwidth}>{\\raggedright}p{0.1\\columnwidth}>{\\raggedright}p{0.1\\columnwidth}}\n\\hline \nModel & Original & 10\\% dist & 30\\% dist & 50\\% dist\\tabularnewline\n\\hline \n3-gram  & 1009.90 & 1.18 & 1.68 & 2.34\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\n3-gram GT & 166.57 & 2.185 & 7.29 & 16.91\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\n3-gram KN & 148.28 & 2.183 & 6.91 & 14.93\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\n5-gram  & 1177.85 & 1.112 & 1.49 & 2.02\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\n5-gram GT & 169.33 & 2.17 & 7.20 & 16.70\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\n5-gram KN & 141.46 & 2.22 & 7.08 & 15.26\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\nRNN-100 & 148.78 & 2.57 & 10.87 & 28.84\\tabularnewline\n\\noalign{\\vskip\\doublerulesep}\nRNN-200 & 141.31 & 2.649 & 11.52 & 31.21\\tabularnewline\n\\hline \n\\end{tabular*}\n\\end{table}\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[width=1\\columnwidth]{images/convppl_v_ppl}\n\\par\\end{centering}\n\n\\caption{Contrastive Perplexity and Perplexity for all models, all distortion\nlevel\\label{fig:cppl_ppl_vs_model}}\n\\end{figure}\n\n\nFigure \\ref{fig:cppl_ppl_vs_model} shows the results discussed in\nTable \\ref{tab:ppl_ngram}. First thing to observe is the perplexity\nis inversely correlated to contrastive perplexity. This is in line\nwith what we expect. A better language model would lead to a lower\nperplexity score as it would to a better job of lowering the cross\nentropy of model and original distribution but contrastive perplexity\nshould increase as it should be able to do a better job at differentiating\nbetween original and distorted language. Another interesting observation\nhere is about increase in contrastive perplexity with distortion across\nmodels. Contrastive perplexity increase rapidly for models with lower\nperplexity like RNN-100, RNN-200 and 5-gram KN. This is in line with\nthe hypothesis that state of the art models should do a lot better\njob at discriminating between good and bad language as compared to\nthe bad models for example 5-grams without any smoothing. We can see\nhere that increase in contrastive perplexity is minimal for it.\n\n\\begin{figure}\n\\centering{}\\includegraphics[width=1\\columnwidth]{images/conppl_ppl_coerr_models}\\caption{Correlation between Contrastive Perplexity vs Perplexity over models\\label{fig:cppl_ppl_coer_models}}\n\\end{figure}\n\n\nNow, let's consider correlations between perplexity and contrastive\nperplexity across models and distortions. Figure \\ref{fig:cppl_ppl_coer_models}\nand Figure \\ref{fig:cppl_vs_ppl_distortion} plots the correlation\nbetween perplexity and contrastive perplexity at various models and\ndistortion levels respectively. Figure \\ref{fig:cppl_ppl_coer_models}\nshows a very strong correlation between contrastive perplexity and\nperplexity across various models. From Figure \\ref{fig:cppl_vs_ppl_distortion},\nwe have two observations. Firstly, as expected the correlation is\nnegative which indicates the inverse relation between the two quantities\nacross distortion level. Second, and more interesting observation\nis the the increasing slope of correlation across the distortion levels.\nThis can be explained by the same rationale that at higher distortion\nlevels contrastive perplexity rises considerably faster as compared\nto decrease of perplexity, hence the slope. This indicates in many\nway contrastive perplexity might be a better metric to evaluation\nlanguage models than perplexity.\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[width=1\\columnwidth]{images/conppl_vs_ppl_distortion}\n\\par\\end{centering}\n\n\\caption{Correlation between Contrastive Perplexity and Perplexity over percentage\ndistortion.\\label{fig:cppl_vs_ppl_distortion}}\n\\end{figure}\n\n\n\n\\section{Conclusion}\n\nIn this paper we proposed a new evaluation criteria which can be used\nto evaluate un-normalized language models. We showed that this new\ncriteria has a very strong correlation with perplexity. The correlation\nacross distortions indicate it might be a better metric than perplexity.\nContrastive perplexity ranks models with better differentiating ability\nhigher and WER ranks the model with lesser number of errors higher,\nwe hypothesize there might be a higher correlation between these two\nas compared to WER's correlation with perplexity.\n\n\\bibliographystyle{acl}\n\\bibliography{NLU}\n\n\n", "itemtype": "equation", "pos": 5534, "prevtext": "\n\n\nFor a sentence level language model, the probability distribution\n$m(D)$ can be modeled as product of sentence probabilities, i.e.\n", "index": 7, "text": "\n\\[\nm(D)=\\prod_{w_{d}\\in D}m(w_{d})\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"m(D)=\\prod_{w_{d}\\in D}m(w_{d})\" display=\"block\"><mrow><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><msub><mi>w</mi><mi>d</mi></msub><mo>\u2208</mo><mi>D</mi></mrow></munder><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>d</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]