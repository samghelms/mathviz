[{"file": "1601.07279.tex", "nexttext": "\nAfter the state transition, an observation $z'\\in{\\mathcal{Z}}$ is perceived and the information it provides is incorporated via the update step:\n\n", "itemtype": "equation", "pos": 11257, "prevtext": "\n\n\\title{\\LARGE \\bf Myopic Policy Bounds for Information Acquisition POMDPs}\n\n\\author{Mikko Lauri, Nikolay Atanasov, George J. Pappas, and Risto Ritala\n\\thanks{M. Lauri and R. Ritala are with Department of Automation Science and Engineering, Tampere University of Technology, P.O. Box 692, FI-33101, Tampere, Finland, {\\tt\\small\\{mikko.lauri, risto.ritala\\}@tut.fi}.}\n\\thanks{N. Atanasov and G. Pappas are with GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA, {\\tt\\small\\{atanasov, pappasg\\}@seas.upenn.edu}. This work was supported by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.}\n}\n\\maketitle\n\n\n\n\\begin{abstract}\nThis paper addresses the problem of optimal control of robotic sensing systems aimed at autonomous information gathering in scenarios such as environmental monitoring, search and rescue, and surveillance and reconnaissance. The information gathering problem is formulated as a partially observable Markov decision process (POMDP) with a reward function that captures uncertainty reduction. Unlike the classical POMDP formulation, the resulting reward structure is nonlinear in the belief state and the traditional approaches do not apply directly. Instead of developing a new approximation algorithm, we show that if attention is restricted to a class of problems with certain structural properties, one can derive (often tight) upper and lower bounds on the optimal policy via an efficient myopic computation. These policy bounds can be applied in conjunction with an online branch-and-bound algorithm to accelerate the computation of the optimal policy. We obtain informative lower and upper policy bounds with low computational effort in a target tracking domain. The performance of branch-and-bounding is demonstrated and compared with exact value iteration.\n\\end{abstract}\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec:intro}\nThe current proliferation of sensors and robots has potential to transform fields as diverse as environmental monitoring, security and surveillance, localization and mapping, and structure inspection. \nOne of the technical challenges in these scenarios is to control the sensors and robots to extract accurate information about various physical phenomena autonomously. \n\nRobotic information acquisition may be thought of as an optimization problem: given constraints on robot motion and the available sensing resources, find an optimal control policy for the robot resulting in the greatest amout of collected information. \nAssuming Markovian system dynamics and observations conditionally independent given the system state, the optimization problem is formalized as a partially observable Markov decision process (POMDP) \\cite{Kaelbling1998}.  \nA POMDP consists of the state, action and observation spaces, stochastic system dynamics and observation models and a real-valued reward function. \nAs the true state of the robot and environment are unknown, a probability density function (pdf) known as a belief state is maintained to represent information regarding the state. \nThe optimal control policy is a mapping from belief states to actions such that the expected total reward accumulated over a given optimization horizon is maximized.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\columnwidth]{policy_comparison-eps-converted-to}\n\\caption{The optimal policy $\\mu^*$ in a POMDP (shown on the right) can be lower and upper bounded by myopic lower and upper bound policies $\\underline{\\mu}^*_1$ and $\\overline{\\mu}^*_1$ (shown on the left and middle, respectively). The ternary plots cover the belief simplex ${\\mathcal{B}}$ in the three-state case with state space ${\\mathcal{S}} = \\{s_1, s_2, s_3 \\}$, while the dots indicate the action selected by the policy for a particular belief point. The inequalities $\\underline{\\mu}^*_1(b) \\leq \\mu^*(b) \\leq \\overline{\\mu}^*_1$ hold for all $b\\in{\\mathcal{B}}$. The optimal policy is fully determined if $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b)$. Even if the bounds do not agree they may provide information regarding the optimal action.}\n\\label{fig:policy_bound_example}\n\\end{figure}\n\n\nWhile POMDPs offer a principled framework handling probabilistic uncertainty in both control actions and sensing outcomes, obtaining optimal policies for them is computationally hard~\\cite{Madani2003}.\nA wide range of algorithms for computing approximately optimal policies for POMDPs have been proposed in the literature, see e.g.\\ \\cite{Lovejoy1991,Hauskrecht2000,Shani2013} for reviews. \nMost research, however, focuses on a standard formulation in which the reward is a function of the hidden system state, and hence necessarily linear in the belief state. \nWhen it comes to information gathering applications in robotics (see e.g.\\ \\cite{Stachniss2005,Charrow2014,Atanasov2014,Lauri2015}), reward functions that capture uncertainty reduction (e.g.\\, negative Shannon entropy~\\cite{InfoTheoryBook}) or information gain (e.g., mutual information~\\cite{InfoTheoryBook}) are often more interesting.\nThese reward function are nonlinear in the belief state, and hence, many of the existing approximation techniques for the standard POMDP formulation do not apply.\nApproximation methods applicable for such information acquisition POMDPs include some online algorithms \\cite{Ross2008} and specialized value iteration techniques, see e.g.\\ \\cite{Araya2010}.\nSampling-based online planning algorithms, such as those based on variants of Monte Carlo tree search (see e.g.\\ \\cite{Silver2010,Somani2013}), are also applicable.\n\n\n\n\n\n\n\nBesides approximation algorithms, an alternative route to handling the inherent complexity of POMDPs with nonlinear belief-dependent rewards is to make structural statements about the optimal value function. \nIn the case of state-dependent rewards, earlier work \\cite{Lovejoy1987,Rieder1991,Krishnamurthy2015} shows that, if we restrict attention to a class of POMDPs that satisfy certain assumptions on the state and observation processes, we can bound the optimal policy from both below and above by an easily computable myopic (greedy) policy (see Fig.~\\ref{fig:policy_bound_example} for details). \nThe specific case of POMDP multi-armed bandits was studied in \\cite{Krishnamurthy2009}.\n\n\n\n\n\n\n\nThe goal of this work is to extend these structural results to POMDPs with belief-dependent rewards. \nKrishnamurthy and Djonin~\\cite{Krishnamurthy2007} determined conditions under which the optimal policy has a threshold structure with respect to the monotone likelihood ratio (MLR) order, a partial order on the belief states. \nSuch partial orders on the belief states have been used in the related active hypothesis testing field~\\cite{Naghshvar2013} to determine when a given sensing action dominates all other actions. \nFor example, Naghshvar and Javidi~\\cite{Naghshvar2010} used Blackwell ordering of the observation models to reduce an active hypothesis test with $K$ actions to a passive test with a single action. \nCompared to active hypothesis testing, information acquisition in POMDPs is even more challenging because the true underlying state evolves during the sensing process. \n\nOur first contribution is to extend the structural results of \\cite{Krishnamurthy2015} for POMDPs with state-dependent rewards to POMDPs with nonlinear belief-dependent rewards. \nThis allows us to generate upper and lower bounds on the optimal policy (which agree in large portions of the belief state space and hence determine the optimal action) via a very efficient myopic policy computation. \nOur second contribution is to apply the myopic policy bounds in conjunction with an online branch-and-bound pruning algorithm to accelerate computation of the optimal policy.\nOur approach allows anytime computation with a bounded error from the optimal solution, unlike for instance sampling-based planning which is only asymptotically optimal.\n\nThe remainder of the paper is organized as follows.\nSection~\\ref{sec:problem} formulates the information acquisition problem as a POMDP, and discusses suitable choices of reward functions.\nSection~\\ref{sec:myopic} briefly reviews existing structural results. \nSection~\\ref{sec:mlr_rewards} introduces our extension to belief-dependent rewards and defines the myopic lower and upper bound policies.\nIn Section~\\ref{sec:bb}, we show how to apply the myopic policy bounds in a branch-and-bound pruning algorithm.\nSection~\\ref{sec:empirical} presents empirical evaluation in target tracking domains, and Section~\\ref{sec:conclusion} concludes the paper.\n\n\n\n\\section{Information acquisition in POMDPs}\n\\label{sec:problem}\nTo model information acquisition problems in robotic applications we define a POMDP with a belief-dependent reward function. \nIn detail, a POMDP is a tuple $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$, where ${\\mathcal{T}}$ is a set of decision epochs and ${\\mathcal{S}} = \\{1, 2, \\ldots, S\\}$, ${\\mathcal{A}} = \\{1, 2, \\ldots, A\\}$, and  ${\\mathcal{Z}} = \\{1,2, \\ldots, Z\\}$ are the finite state, action, and observation spaces, respectively.\nThe function ${\\mathbb{T}}:{\\mathcal{S}} \\times {\\mathcal{S}} \\times {\\mathcal{A}} \\to [0,1]$ is a stochastic state transition model such that ${\\mathbb{T}}(s',s,a)$ is the probability of reaching state $s'\\in{\\mathcal{S}}$ from state $s \\in {\\mathcal{S}}$ after executing action $a\\in{\\mathcal{A}}$ and ${\\mathbb{O}}: {\\mathcal{Z}} \\times {\\mathcal{S}} \\times {\\mathcal{A}} \\to [0,1]$ is a stochastic observation model such that ${\\mathbb{O}}(z',s',a)$ is the conditional probability of observing $z'\\in Z$ when state $s'\\in{\\mathcal{S}}$ was reached after executing action $a\\in{\\mathcal{A}}$. \nTo simplify notation, let ${\\mathbb{T}}^a := [{\\mathbb{T}}(j,i,a)]_{i,j=1}^S$ be the matrix of state transition probabilities for a given action $a \\in {\\mathcal{A}}$, i.e.\\ previous states are column-wise and next states row-wise. \nSimilarly, let ${\\mathbb{O}}^a := [{\\mathbb{O}}(z,j,a)]$ for $z \\in {\\mathcal{Z}}$, $j \\in {\\mathcal{S}}$, and $a \\in {\\mathcal{A}}$.\n\nAs information about the system state is incomplete, it is modeled by a probability density function (pdf) $b(s), s\\in {\\mathcal{S}}$ over the system state. \nThe set ${\\mathcal{B}}$, called the belief space, is the set of all possible pdfs over the system state. \nFinally, $\\rho:{\\mathcal{B}}\\times{\\mathcal{A}}\\to {\\mathbb{R}}$ is the expected reward, which is a (nonlinear) function of the belief state $b$.\n\n\n\nOur definition subsumes the traditional POMDP definition with a state-and-action-dependent reward function $r:{\\mathcal{S}}\\times{\\mathcal{A}}\\to{\\mathbb{R}}$.\nThis is seen defining $\\rho(b,a) := \\sum_{s\\in{\\mathcal{S}}} r(s,a) b(s)$.\n\n\n\nThe evolution of the belief state $b$ in a POMDP is conditional on the actions and observations. When action $a \\in {\\mathcal{A}}$ is executed, the belief state evolves according to the state transition model ${\\mathbb{T}}$. The evolution can be tracked by Bayesian filtering, which consists of iterating prediction and update steps. The prediction step revises the current belief state $b\\in{\\mathcal{B}}$ to the predicted belief $b^{a}\\in{\\mathcal{B}}$ according to\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:predicted_belief}\nb^a(s') = \\sum\\limits_{s\\in{\\mathcal{S}}} {\\mathbb{T}}(s',s,a) b(s).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"b^{a}(s^{\\prime})=\\sum\\limits_{s\\in{\\mathcal{S}}}{\\mathbb{T}}(s^{\\prime},s,a)b%&#10;(s).\" display=\"block\"><mrow><mrow><mrow><msup><mi>b</mi><mi>a</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>\ud835\udd4b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>\u2032</mo></msup><mo>,</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $\\eta(z'\\mid b,a) := \\sum_{s \\in {\\mathcal{S}}} {\\mathbb{O}}(z',s,a) b^a(s)$ is a normalization factor equal to the conditional probability of observing $z'$.\n\n\\subsection{Reward functions for information acquisition}\nThe goal in information acquisition is to reduce the uncertainty in the unobservable state of interest $s \\in {\\mathcal{S}}$. \nIt is therefore desirable to reach ``peaked'' belief states that have the majority of their probability mass on a single underlying state.\nIn other words, actions that lead the belief state towards the vertices of the probability simplex ${\\mathcal{B}}$ should be rewarded. \nSuitable measures of uncertainty are concave functions of the belief state, called uncertainty functions \\cite[Sec.~14.16]{DeGroot2004}.\n\\begin{definition}[Uncertainty function and information gain \\cite{DeGroot2004}]\nAn uncertainty function is a non-negative, concave function $f:{\\mathcal{B}}\\to{\\mathbb{R}}^+$.\nThe information gain $\\mathbf{I}_f(b, a)$ of an action $a\\in{\\mathcal{A}}$ in belief state $b\\in{\\mathcal{B}}$ is the expected reduction in the uncertainty function $f$:\n\n", "itemtype": "equation", "pos": 11532, "prevtext": "\nAfter the state transition, an observation $z'\\in{\\mathcal{Z}}$ is perceived and the information it provides is incorporated via the update step:\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:updated_belief}\nb'(s') = \\tau(b,a,z') := \\frac{1}{\\eta(z'\\mid b,a)} \\cdot {\\mathbb{O}}(z',s',a) b^a(s'),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"b^{\\prime}(s^{\\prime})=\\tau(b,a,z^{\\prime}):=\\frac{1}{\\eta(z^{\\prime}\\mid b,a)%&#10;}\\cdot{\\mathbb{O}}(z^{\\prime},s^{\\prime},a)b^{a}(s^{\\prime}),\" display=\"block\"><mrow><mrow><mrow><msup><mi>b</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mfrac><mn>1</mn><mrow><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u22c5</mo><mi>\ud835\udd46</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>s</mi><mo>\u2032</mo></msup><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>b</mi><mi>a</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\n\\end{definition}\nExamples of uncertainty functions include Shannon entropy, the more-general R{\\'e}nyi quadratic entropy, and variants of the value of information, such as the probability of error in hypothesis testing~\\cite{InfoTheoryBook}. For example, the information gain associated with Shannon entropy is known as mutual information.\n\nFor information acquisition tasks, either information gain or a negative uncertainty function may be used as the reward function in the POMDP formulation. However, as the following example shows, using an uncertainty function might be more appropriate when the magnitude of the predicted uncertainty $f(b^a)$ is significantly affected by the action choice.\n\n\\begin{example}\n\\label{xm:uncertainty_vs_infogain}\nConsider an active localization problem~{\\color{red}{\\cite{Thrun2006}}} in which a mobile robot needs to choose an appropriate action $a$ to reduce the uncertainty in the distribution $b$ of its current position $s$. Suppose that the entropy $f(b)$ of its current position distribution is $n$ bits. The robot may choose between a risky high-velocity motion $a_r$ leading to a predicted entropy of $f(b^{a_r}) = n+m$ bits or a safe low-velocity motion $a_s$ leading to $f(b^{a_s}) = n+h$ bits, where $h < m$. Suppose that in both cases, after moving, the robot receives the same amount of information, $k$ bits, from its sensor measurement $z'$. In this example, the negative uncertainty function correctly predicts that action $a_r$ is risky:\n\n", "itemtype": "equation", "pos": 12789, "prevtext": "\nwhere $\\eta(z'\\mid b,a) := \\sum_{s \\in {\\mathcal{S}}} {\\mathbb{O}}(z',s,a) b^a(s)$ is a normalization factor equal to the conditional probability of observing $z'$.\n\n\\subsection{Reward functions for information acquisition}\nThe goal in information acquisition is to reduce the uncertainty in the unobservable state of interest $s \\in {\\mathcal{S}}$. \nIt is therefore desirable to reach ``peaked'' belief states that have the majority of their probability mass on a single underlying state.\nIn other words, actions that lead the belief state towards the vertices of the probability simplex ${\\mathcal{B}}$ should be rewarded. \nSuitable measures of uncertainty are concave functions of the belief state, called uncertainty functions \\cite[Sec.~14.16]{DeGroot2004}.\n\\begin{definition}[Uncertainty function and information gain \\cite{DeGroot2004}]\nAn uncertainty function is a non-negative, concave function $f:{\\mathcal{B}}\\to{\\mathbb{R}}^+$.\nThe information gain $\\mathbf{I}_f(b, a)$ of an action $a\\in{\\mathcal{A}}$ in belief state $b\\in{\\mathcal{B}}$ is the expected reduction in the uncertainty function $f$:\n\n", "index": 5, "text": "\\begin{equation}\n\\mathbf{I}_f(b, a) := f(b^a) - {\\mathbb{E}}_Z[ f(\\tau(b,a,Z)) ].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{I}_{f}(b,a):=f(b^{a})-{\\mathbb{E}}_{Z}[f(\\tau(b,a,Z))].\" display=\"block\"><mrow><mrow><mrow><msub><mi>\ud835\udc08</mi><mi>f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\ud835\udd3c</mi><mi>Z</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nbut, perhaps surprisingly, the information gain prefers the risky action:\n", "itemtype": "equation", "pos": 14379, "prevtext": "\n\\end{definition}\nExamples of uncertainty functions include Shannon entropy, the more-general R{\\'e}nyi quadratic entropy, and variants of the value of information, such as the probability of error in hypothesis testing~\\cite{InfoTheoryBook}. For example, the information gain associated with Shannon entropy is known as mutual information.\n\nFor information acquisition tasks, either information gain or a negative uncertainty function may be used as the reward function in the POMDP formulation. However, as the following example shows, using an uncertainty function might be more appropriate when the magnitude of the predicted uncertainty $f(b^a)$ is significantly affected by the action choice.\n\n\\begin{example}\n\\label{xm:uncertainty_vs_infogain}\nConsider an active localization problem~{\\color{red}{\\cite{Thrun2006}}} in which a mobile robot needs to choose an appropriate action $a$ to reduce the uncertainty in the distribution $b$ of its current position $s$. Suppose that the entropy $f(b)$ of its current position distribution is $n$ bits. The robot may choose between a risky high-velocity motion $a_r$ leading to a predicted entropy of $f(b^{a_r}) = n+m$ bits or a safe low-velocity motion $a_s$ leading to $f(b^{a_s}) = n+h$ bits, where $h < m$. Suppose that in both cases, after moving, the robot receives the same amount of information, $k$ bits, from its sensor measurement $z'$. In this example, the negative uncertainty function correctly predicts that action $a_r$ is risky:\n\n", "index": 7, "text": "\\begin{align*}\n-f(\\tau(b,a_r,z') ) &= -(n + m - k) \\\\\n&< -(n + h - k) = -f(\\tau(b,a_s,z) )\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-f(\\tau(b,a_{r},z^{\\prime}))\" display=\"inline\"><mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><msub><mi>a</mi><mi>r</mi></msub><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-(n+m-k)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow><mo>-</mo><mi>k</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;-(n+h-k)=-f(\\tau(b,a_{s},z))\" display=\"inline\"><mrow><mi/><mo>&lt;</mo><mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>+</mo><mi>h</mi></mrow><mo>-</mo><mi>k</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><msub><mi>a</mi><mi>s</mi></msub><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\n\\end{example}\n\nIn the rest of the paper, we restrict attention to belief-dependent reward functions, specific to the task of information gathering, which have the following form:\n\n", "itemtype": "equation", "pos": 14556, "prevtext": "\nbut, perhaps surprisingly, the information gain prefers the risky action:\n", "index": 9, "text": "\n\\[\n\\mathbf{I}_f(b,a_r) = m-k > h-k = \\mathbf{I}_f(b,a_s).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{I}_{f}(b,a_{r})=m-k&gt;h-k=\\mathbf{I}_{f}(b,a_{s}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>\ud835\udc08</mi><mi>f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><msub><mi>a</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>m</mi><mo>-</mo><mi>k</mi></mrow><mo>&gt;</mo><mrow><mi>h</mi><mo>-</mo><mi>k</mi></mrow><mo>=</mo><mrow><msub><mi>\ud835\udc08</mi><mi>f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><msub><mi>a</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $f$ is an uncertainty function (preferred to information gain due to the observations in Example \\ref{xm:uncertainty_vs_infogain}), $r(s,a)$ is any standard state-dependent reward function, and $w_a$ are user-specified weights that trade-off measurement rewards and state uncertainty.\n\n\\subsection{Value functions and optimal policies}\nLet $H \\in \\mathbb{N}\\cup\\{\\infty\\}$ denote a planning horizon so that ${\\mathcal{T}} := \\{1,2,\\ldots,H\\}$, and let $0\\leq \\gamma \\leq 1$\\footnote{Note that $\\gamma=1$ is only valid for finite $H$.} denote a discount factor determining the relative value of immediate and future rewards.\nThe goal in information acquisition is to choose a policy $\\mu^*_k:{\\mathcal{B}} \\rightarrow {\\mathcal{A}}$ for each $k \\in {\\mathcal{T}}$ such that the expected sum of rewards over the decisions epochs is maximized.\nThe sequence of optimal policies $\\mu^*_k$ for $k\\in{\\mathcal{T}}$ remaining decisions can be computed via value iteration~\\cite{Bertsekas1996} according to:\n\n", "itemtype": "equation", "pos": 14797, "prevtext": "\n\\end{example}\n\nIn the rest of the paper, we restrict attention to belief-dependent reward functions, specific to the task of information gathering, which have the following form:\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:info_reward}\n\\rho(b,a) := \\sum_{s \\in {\\mathcal{S}}} r(s,a) b(s) - w_a f(b),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\rho(b,a):=\\sum_{s\\in{\\mathcal{S}}}r(s,a)b(s)-w_{a}f(b),\" display=\"block\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>w</mi><mi>a</mi></msub><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $V_k^*:{\\mathcal{B}} \\to {\\mathbb{R}}$ is the optimal value function. For infinite-horizon problems with $H\\to\\infty$ and bounded rewards, value iteration converges to a unique fixed point $V^*$ that satisfies the Bellman equation:\n\n", "itemtype": "equation", "pos": 15921, "prevtext": "\nwhere $f$ is an uncertainty function (preferred to information gain due to the observations in Example \\ref{xm:uncertainty_vs_infogain}), $r(s,a)$ is any standard state-dependent reward function, and $w_a$ are user-specified weights that trade-off measurement rewards and state uncertainty.\n\n\\subsection{Value functions and optimal policies}\nLet $H \\in \\mathbb{N}\\cup\\{\\infty\\}$ denote a planning horizon so that ${\\mathcal{T}} := \\{1,2,\\ldots,H\\}$, and let $0\\leq \\gamma \\leq 1$\\footnote{Note that $\\gamma=1$ is only valid for finite $H$.} denote a discount factor determining the relative value of immediate and future rewards.\nThe goal in information acquisition is to choose a policy $\\mu^*_k:{\\mathcal{B}} \\rightarrow {\\mathcal{A}}$ for each $k \\in {\\mathcal{T}}$ such that the expected sum of rewards over the decisions epochs is maximized.\nThe sequence of optimal policies $\\mu^*_k$ for $k\\in{\\mathcal{T}}$ remaining decisions can be computed via value iteration~\\cite{Bertsekas1996} according to:\n\n", "index": 13, "text": "\\begin{align}\nQ_1^*(b,a) &= \\rho(b,a)\\\\\n\\label{eq:value_iter}\nV_k^*(b) &= \\max\\limits_{a\\in{\\mathcal{A}}} Q_k^*(b,a), \\\\\n\\label{eq:q_value}\nQ_k^*(b,a) &= \\rho(b,a) + \\gamma \\sum\\limits_{z'\\in{\\mathcal{Z}}} \\eta(z'\\mid b,a) V_{k-1}^*(\\tau(b,a,z')),\\\\\n\\label{eq:opt_policy}\n\\mu_k^*(b) &= {\\operatorname{argmax}}\\limits_{a\\in{\\mathcal{A}}} Q_k^*(b,a),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Q_{1}^{*}(b,a)\" display=\"inline\"><mrow><msubsup><mi>Q</mi><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\rho(b,a)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V_{k}^{*}(b)\" display=\"inline\"><mrow><msubsup><mi>V</mi><mi>k</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max\\limits_{a\\in{\\mathcal{A}}}Q_{k}^{*}(b,a),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><munder><mi>max</mi><mrow><mi>a</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></munder><mo>\u2061</mo><msubsup><mi>Q</mi><mi>k</mi><mo>*</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Q_{k}^{*}(b,a)\" display=\"inline\"><mrow><msubsup><mi>Q</mi><mi>k</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\rho(b,a)+\\gamma\\sum\\limits_{z^{\\prime}\\in{\\mathcal{Z}}}\\eta(z^{%&#10;\\prime}\\mid b,a)V_{k-1}^{*}(\\tau(b,a,z^{\\prime})),\" display=\"inline\"><mrow><mo>=</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03b3</mi><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></munder></mstyle><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>V</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu_{k}^{*}(b)\" display=\"inline\"><mrow><msubsup><mi>\u03bc</mi><mi>k</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{argmax}}\\limits_{a\\in{\\mathcal{A}}}Q_{k}^{*}(b,a),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><munder><mo movablelimits=\"false\">argmax</mo><mrow><mi>a</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></munder><mo>\u2061</mo><msubsup><mi>Q</mi><mi>k</mi><mo>*</mo></msubsup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nand the corresponding optimal infinite-horizon policy $\\mu^*$ is stationary \\cite{Bertsekas1996}.\nAs the set ${\\mathcal{B}}$ of belief states over which the value iteration must be solved is uncountable, the basic scheme above rarely results in practical methods for computing optimal policies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Myopic policy bounds for POMDPs}\n\\label{sec:myopic}\nIn this section, we review existing results on myopic policy bounds for POMDPs with state-depend rewards. The idea of a myopic policy bound is that the optimal policy $\\mu_k^*$ for any $k$ can be either \\emph{lower or upper bounded} by the myopic policy $\\mu_1^*$ (Eq.~\\ref{eq:opt_policy}).\nThe results exploit concepts of stochastic partial orders, summarized in the following definition.\n\n\\begin{definition}[Stochastic partial orders \\cite{Shaked2007}]\n\\label{def:stoch_orders}\nLet $b_1, b_2 \\in {\\mathcal{B}}$.\n\\begin{enumerate}\n\\item $b_1$ first-order stochastically domainates $b_2$, denoted $b_1 \\geq_s b_2$, if $\\sum\\limits_{i=j}^{S}b_1(i) \\geq \\sum\\limits_{i=j}^{S} b_2(i)$ $\\forall j \\in {\\mathcal{S}}$. \n\\item Equivalently, $b_1 \\geq_s b_2$ iff $\\sum\\limits_{i=1}^{S}b_1(i)g(i) \\geq \\sum\\limits_{i=1}^{S}b_2(i)g(i)$ for all increasing functions $g:{\\mathcal{S}}\\to{\\mathbb{R}}$.\n\\item $b_1$ is greater than $b_2$ in the monotone likelihood ratio (MLR) order, denoted $b_1 \\geq_r b_2$, if $b_1(i)b_2(j) \\leq b_2(i)b_1(j)$, for all $i<j$, $j\\in{\\mathcal{S}}$.\n\\item A function $g:{\\mathcal{B}}\\to{\\mathbb{R}}$ is MLR increasing (decreasing) if $b_1 \\geq_r b_2 \\Rightarrow g(b_1) \\geq g(b_2)$ $(g(b_1) \\leq g(b_2))$.\n\\item MLR dominance implies first-order stochastic dominance, $b_1 \\geq_r b_2 \\Rightarrow b_1 \\geq_s b_2$.\n\\end{enumerate}\n\\end{definition}\n\nGiven conditions on the state transition and observation models, it was shown in \\cite{Lovejoy1987,Rieder1991} that the optimal value function is MLR increasing and that the optimal policy may be lower bounded by $\\mu_1^*$.\nThe required conditions, however, were thought to be too restrictive to be of practical significance until a recent extension~\\cite{Krishnamurthy2015} proposed the following revised assumptions.\n\\begin{assumption}[Sufficient conditions for existence of myopic policy bounds~\\cite{Krishnamurthy2015}]\n\\label{ass:model_assumptions}\n\n\n$ $\n\\begin{enumerate}[label=(A\\arabic*)]\n\\item \\label{asm:tp2} ${\\mathbb{T}}^a$ and ${\\mathbb{O}}^a$ are totally positive or order 2 (TP2) $\\forall a\\in{\\mathcal{A}}$,\n\\item \\label{asm:copositivity} For all $j \\in \\{1, 2, \\ldots, S-1\\}$, $a\\in \\{1, 2, \\ldots, A-1\\}$, and $z \\in {\\mathcal{Z}}$, the matrices $D^{j,a,z} := [d_{m,n}^{j,a,z} + d_{n,m}^{j,a,z}] \\in \\mathbb{R}^{S \\times S}$ are copositive, where $d_{m,n}^{j,a,z} := {\\mathbb{O}}^a_{z,j}{\\mathbb{O}}^{a+1}_{z,j+1}{\\mathbb{T}}^a_{m,j}{\\mathbb{T}}^{a+1}_{n,j+1} - {\\mathbb{O}}^a_{z,j+1}{\\mathbb{O}}^{a+1}_{z,j}{\\mathbb{T}}^a_{m,j+1}{\\mathbb{T}}^{a+1}_{n,j}$, and\n\n\n\\item \\label{asm:sum} $\\sum\\limits_{z\\leq \\bar{z}} \\sum\\limits_{j\\in{\\mathcal{S}}} \\left[ {\\mathbb{T}}^a_{i,j}{\\mathbb{O}}^a_{z,j} - {\\mathbb{T}}^{a+1}_{i,j}{\\mathbb{O}}^{a+1}_{z,j}\\right]$\n$\\geq 0, \\forall i\\in {\\mathcal{S}}, \\forall \\bar{z} \\in {\\mathcal{Z}}$, $\\forall a\\in\\{1,2,\\ldots,A-1\\}$.\n\n\n\\end{enumerate}\n\\end{assumption}\n\nThe TP2 property for a matrix requires that all its second-order minors are nonnegative~\\cite{Fallat2011}. \nExamples of TP2 observation models include exponential family distributions such as Gaussian, exponential, gamma, and binomial pdfs~\\cite{Fallat2011}; more examples may be found e.g.\\ in \\cite{Karlin1968}. \nWhile~\\ref{asm:copositivity} may be checked directly~\\cite{copositive_check}, it is much more efficient (but also more restrictive) to check the following:\n\\begin{enumerate}[leftmargin=2.5eM]\n  \\item[(A2')] \\label{asm:copositivity_relaxed} $d_{m,n}^{j,a,z} + d_{n,m}^{j,a,z} \\geq 0$ for all $m, n \\in {\\mathcal{S}}$, $j \\in \\{1, \\ldots, S-1\\}$, $a\\in \\{1, 2, \\ldots, A-1\\}$, and $z \\in {\\mathcal{Z}}$.\n\\end{enumerate}\nAssumption~\\ref{asm:sum} is a revised version of the original assumption in \\cite{Lovejoy1987,Rieder1991}.\nIt states that for every state $i\\in{\\mathcal{S}}$, the prior pdf of perceiving $z\\in{\\mathcal{Z}}$ after ending in state $j\\in{\\mathcal{S}}$ after executing action $a+1$ first order stochastically dominates that of action $a$.\nThe assumptions lead to the following lemma.\n\n\n\\begin{lemma}[\\cite{Krishnamurthy2015}]\n\\label{lem:update_monotone}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$. Then for all $b\\in{\\mathcal{B}}$, and $a', a\\in{\\mathcal{A}}$, $a'\\geq a$,\n\\begin{enumerate}[label=(L\\arabic*)]\n\\item \\label{lem:sdom} if \\ref{asm:sum} holds, $\\eta(z'\\mid b, a') \\geq_s \\eta(z'\\mid b, a)$, and\n\\item \\label{lem:rdom} if \\ref{asm:copositivity} holds, $\\tau(b,a',z') \\geq_r \\tau(b,a,z')$ for all $z'\\in{\\mathcal{Z}}$.\n\\end{enumerate}\n\\end{lemma}\n\nThe following theorem for POMDPs with rewards linear in the belief state is due to \\cite[Prop.~2]{Lovejoy1987} and \\cite[Theorem~1]{Krishnamurthy2015}:\n\\begin{theorem}\n\\label{thm:mlr_policy_bounds}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$ satisfying Assumptions~\\ref{asm:tp2}-\\ref{asm:sum}, with $\\rho(b,a) := \\sum_{s\\in{\\mathcal{S}}} r(s,a) b(s)$.\nIf $r(s,a)$ is increasing (decreasing) in ${\\mathcal{S}}$ for all $a\\in{\\mathcal{A}}$, then the optimal value function $V_k^*$ is MLR increasing (decreasing) and the optimal policy $\\mu_k^*(b)$ for any $k\\in{\\mathcal{T}}$ satisfies $\\mu_1^*(b) \\leq \\mu_k^*(b)$ ($\\mu_1^*(b) \\geq \\mu_k^*(b)$) $\\forall b\\in{\\mathcal{B}}$.\n\\end{theorem}\n\n\n\n\\section{Transformed MLR monotone rewards for information acquisition POMDPs}\n\\label{sec:mlr_rewards}\nIn this section, we extend Theorem~\\ref{thm:mlr_policy_bounds} to apply to reward functions of the form in~\\eqref{eq:info_reward}, which are of interest for information acquisition tasks. To proceed, we need the following minor extension of Theorem~\\ref{thm:mlr_policy_bounds}.\n\\begin{corollary}\n\\label{cor:mlr_policy_bounds}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$ satisfying Assumptions~\\ref{asm:tp2}-\\ref{asm:sum} and suppose that $\\rho$ is MLR increasing (decreasing) in ${\\mathcal{B}}$ for all $a\\in{\\mathcal{A}}$. Then, the optimal policy $\\mu_k^*(b)$ for any $k\\in{\\mathcal{T}}$ satisfies $\\mu_1^*(b) \\leq \\mu_k^*(b)$ ($\\mu_1^*(b) \\geq \\mu_k^*(b)$) $\\forall b\\in{\\mathcal{B}}$.\n\\end{corollary}\n\\begin{proof}\nBy \\cite[Prop.~2]{Lovejoy1987}, it suffices to show $V_k^*$ is MLR increasing.\nWe proceed by induction.\nFor the base case $k=1$, $V_1^*(b) = \\max\\limits_{a\\in{\\mathcal{A}}} \\rho(b,a)$ is clearly MLR increasing.\nFor the induction step, suppose the claim holds for $V_{k-1}^*$. \nConsider the sum part of the value iteration in Eq.~\\eqref{eq:q_value}.\nLet $b_1, b_2\\in{\\mathcal{B}}$ s.t.\\ $b_1 \\geq_r b_2$.\nNow\n\n", "itemtype": "equation", "pos": 16520, "prevtext": "\nwhere $V_k^*:{\\mathcal{B}} \\to {\\mathbb{R}}$ is the optimal value function. For infinite-horizon problems with $H\\to\\infty$ and bounded rewards, value iteration converges to a unique fixed point $V^*$ that satisfies the Bellman equation:\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eq:bellman_fixedpoint}\nV^*(b) = \\rho(b,a) + \\gamma \\sum\\limits_{z'\\in{\\mathcal{Z}}} \\eta(z'\\mid b,a) V^*(\\tau(b,a,z')),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"V^{*}(b)=\\rho(b,a)+\\gamma\\sum\\limits_{z^{\\prime}\\in{\\mathcal{Z}}}\\eta(z^{%&#10;\\prime}\\mid b,a)V^{*}(\\tau(b,a,z^{\\prime})),\" display=\"block\"><mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03b3</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb5</mi></mrow></munder><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nThe first inequality follows by Lemma~\\ref{lem:sdom}, \\cite[Lemma~1.2(2),1.3(1)]{Lovejoy1987} which state $\\tau(b,a,z')$ is MLR increasing in $z'$, and the the induction hypothesis.\nThe second inequality follows by Lemma~\\ref{lem:rdom} and the induction hypothesis.\nThe proof for the MLR decreasing part is similar and omitted.\n\\end{proof}\n\nFollowing the observation in Corollary~\\ref{cor:mlr_policy_bounds}, the main idea is to transform the reward function $\\rho(b,a)$~\\eqref{eq:info_reward} to one that is MLR monotone but leaves the optimal policy unchanged. Accordingly, we define two transformed reward functions:\n\n", "itemtype": "equation", "pos": 23699, "prevtext": "\nand the corresponding optimal infinite-horizon policy $\\mu^*$ is stationary \\cite{Bertsekas1996}.\nAs the set ${\\mathcal{B}}$ of belief states over which the value iteration must be solved is uncountable, the basic scheme above rarely results in practical methods for computing optimal policies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Myopic policy bounds for POMDPs}\n\\label{sec:myopic}\nIn this section, we review existing results on myopic policy bounds for POMDPs with state-depend rewards. The idea of a myopic policy bound is that the optimal policy $\\mu_k^*$ for any $k$ can be either \\emph{lower or upper bounded} by the myopic policy $\\mu_1^*$ (Eq.~\\ref{eq:opt_policy}).\nThe results exploit concepts of stochastic partial orders, summarized in the following definition.\n\n\\begin{definition}[Stochastic partial orders \\cite{Shaked2007}]\n\\label{def:stoch_orders}\nLet $b_1, b_2 \\in {\\mathcal{B}}$.\n\\begin{enumerate}\n\\item $b_1$ first-order stochastically domainates $b_2$, denoted $b_1 \\geq_s b_2$, if $\\sum\\limits_{i=j}^{S}b_1(i) \\geq \\sum\\limits_{i=j}^{S} b_2(i)$ $\\forall j \\in {\\mathcal{S}}$. \n\\item Equivalently, $b_1 \\geq_s b_2$ iff $\\sum\\limits_{i=1}^{S}b_1(i)g(i) \\geq \\sum\\limits_{i=1}^{S}b_2(i)g(i)$ for all increasing functions $g:{\\mathcal{S}}\\to{\\mathbb{R}}$.\n\\item $b_1$ is greater than $b_2$ in the monotone likelihood ratio (MLR) order, denoted $b_1 \\geq_r b_2$, if $b_1(i)b_2(j) \\leq b_2(i)b_1(j)$, for all $i<j$, $j\\in{\\mathcal{S}}$.\n\\item A function $g:{\\mathcal{B}}\\to{\\mathbb{R}}$ is MLR increasing (decreasing) if $b_1 \\geq_r b_2 \\Rightarrow g(b_1) \\geq g(b_2)$ $(g(b_1) \\leq g(b_2))$.\n\\item MLR dominance implies first-order stochastic dominance, $b_1 \\geq_r b_2 \\Rightarrow b_1 \\geq_s b_2$.\n\\end{enumerate}\n\\end{definition}\n\nGiven conditions on the state transition and observation models, it was shown in \\cite{Lovejoy1987,Rieder1991} that the optimal value function is MLR increasing and that the optimal policy may be lower bounded by $\\mu_1^*$.\nThe required conditions, however, were thought to be too restrictive to be of practical significance until a recent extension~\\cite{Krishnamurthy2015} proposed the following revised assumptions.\n\\begin{assumption}[Sufficient conditions for existence of myopic policy bounds~\\cite{Krishnamurthy2015}]\n\\label{ass:model_assumptions}\n\n\n$ $\n\\begin{enumerate}[label=(A\\arabic*)]\n\\item \\label{asm:tp2} ${\\mathbb{T}}^a$ and ${\\mathbb{O}}^a$ are totally positive or order 2 (TP2) $\\forall a\\in{\\mathcal{A}}$,\n\\item \\label{asm:copositivity} For all $j \\in \\{1, 2, \\ldots, S-1\\}$, $a\\in \\{1, 2, \\ldots, A-1\\}$, and $z \\in {\\mathcal{Z}}$, the matrices $D^{j,a,z} := [d_{m,n}^{j,a,z} + d_{n,m}^{j,a,z}] \\in \\mathbb{R}^{S \\times S}$ are copositive, where $d_{m,n}^{j,a,z} := {\\mathbb{O}}^a_{z,j}{\\mathbb{O}}^{a+1}_{z,j+1}{\\mathbb{T}}^a_{m,j}{\\mathbb{T}}^{a+1}_{n,j+1} - {\\mathbb{O}}^a_{z,j+1}{\\mathbb{O}}^{a+1}_{z,j}{\\mathbb{T}}^a_{m,j+1}{\\mathbb{T}}^{a+1}_{n,j}$, and\n\n\n\\item \\label{asm:sum} $\\sum\\limits_{z\\leq \\bar{z}} \\sum\\limits_{j\\in{\\mathcal{S}}} \\left[ {\\mathbb{T}}^a_{i,j}{\\mathbb{O}}^a_{z,j} - {\\mathbb{T}}^{a+1}_{i,j}{\\mathbb{O}}^{a+1}_{z,j}\\right]$\n$\\geq 0, \\forall i\\in {\\mathcal{S}}, \\forall \\bar{z} \\in {\\mathcal{Z}}$, $\\forall a\\in\\{1,2,\\ldots,A-1\\}$.\n\n\n\\end{enumerate}\n\\end{assumption}\n\nThe TP2 property for a matrix requires that all its second-order minors are nonnegative~\\cite{Fallat2011}. \nExamples of TP2 observation models include exponential family distributions such as Gaussian, exponential, gamma, and binomial pdfs~\\cite{Fallat2011}; more examples may be found e.g.\\ in \\cite{Karlin1968}. \nWhile~\\ref{asm:copositivity} may be checked directly~\\cite{copositive_check}, it is much more efficient (but also more restrictive) to check the following:\n\\begin{enumerate}[leftmargin=2.5eM]\n  \\item[(A2')] \\label{asm:copositivity_relaxed} $d_{m,n}^{j,a,z} + d_{n,m}^{j,a,z} \\geq 0$ for all $m, n \\in {\\mathcal{S}}$, $j \\in \\{1, \\ldots, S-1\\}$, $a\\in \\{1, 2, \\ldots, A-1\\}$, and $z \\in {\\mathcal{Z}}$.\n\\end{enumerate}\nAssumption~\\ref{asm:sum} is a revised version of the original assumption in \\cite{Lovejoy1987,Rieder1991}.\nIt states that for every state $i\\in{\\mathcal{S}}$, the prior pdf of perceiving $z\\in{\\mathcal{Z}}$ after ending in state $j\\in{\\mathcal{S}}$ after executing action $a+1$ first order stochastically dominates that of action $a$.\nThe assumptions lead to the following lemma.\n\n\n\\begin{lemma}[\\cite{Krishnamurthy2015}]\n\\label{lem:update_monotone}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$. Then for all $b\\in{\\mathcal{B}}$, and $a', a\\in{\\mathcal{A}}$, $a'\\geq a$,\n\\begin{enumerate}[label=(L\\arabic*)]\n\\item \\label{lem:sdom} if \\ref{asm:sum} holds, $\\eta(z'\\mid b, a') \\geq_s \\eta(z'\\mid b, a)$, and\n\\item \\label{lem:rdom} if \\ref{asm:copositivity} holds, $\\tau(b,a',z') \\geq_r \\tau(b,a,z')$ for all $z'\\in{\\mathcal{Z}}$.\n\\end{enumerate}\n\\end{lemma}\n\nThe following theorem for POMDPs with rewards linear in the belief state is due to \\cite[Prop.~2]{Lovejoy1987} and \\cite[Theorem~1]{Krishnamurthy2015}:\n\\begin{theorem}\n\\label{thm:mlr_policy_bounds}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$ satisfying Assumptions~\\ref{asm:tp2}-\\ref{asm:sum}, with $\\rho(b,a) := \\sum_{s\\in{\\mathcal{S}}} r(s,a) b(s)$.\nIf $r(s,a)$ is increasing (decreasing) in ${\\mathcal{S}}$ for all $a\\in{\\mathcal{A}}$, then the optimal value function $V_k^*$ is MLR increasing (decreasing) and the optimal policy $\\mu_k^*(b)$ for any $k\\in{\\mathcal{T}}$ satisfies $\\mu_1^*(b) \\leq \\mu_k^*(b)$ ($\\mu_1^*(b) \\geq \\mu_k^*(b)$) $\\forall b\\in{\\mathcal{B}}$.\n\\end{theorem}\n\n\n\n\\section{Transformed MLR monotone rewards for information acquisition POMDPs}\n\\label{sec:mlr_rewards}\nIn this section, we extend Theorem~\\ref{thm:mlr_policy_bounds} to apply to reward functions of the form in~\\eqref{eq:info_reward}, which are of interest for information acquisition tasks. To proceed, we need the following minor extension of Theorem~\\ref{thm:mlr_policy_bounds}.\n\\begin{corollary}\n\\label{cor:mlr_policy_bounds}\nConsider a POMDP $\\langle {\\mathcal{T}}, {\\mathcal{S}}, {\\mathcal{A}}, {\\mathcal{Z}}, {\\mathbb{T}}, {\\mathbb{O}}, \\rho \\rangle$ satisfying Assumptions~\\ref{asm:tp2}-\\ref{asm:sum} and suppose that $\\rho$ is MLR increasing (decreasing) in ${\\mathcal{B}}$ for all $a\\in{\\mathcal{A}}$. Then, the optimal policy $\\mu_k^*(b)$ for any $k\\in{\\mathcal{T}}$ satisfies $\\mu_1^*(b) \\leq \\mu_k^*(b)$ ($\\mu_1^*(b) \\geq \\mu_k^*(b)$) $\\forall b\\in{\\mathcal{B}}$.\n\\end{corollary}\n\\begin{proof}\nBy \\cite[Prop.~2]{Lovejoy1987}, it suffices to show $V_k^*$ is MLR increasing.\nWe proceed by induction.\nFor the base case $k=1$, $V_1^*(b) = \\max\\limits_{a\\in{\\mathcal{A}}} \\rho(b,a)$ is clearly MLR increasing.\nFor the induction step, suppose the claim holds for $V_{k-1}^*$. \nConsider the sum part of the value iteration in Eq.~\\eqref{eq:q_value}.\nLet $b_1, b_2\\in{\\mathcal{B}}$ s.t.\\ $b_1 \\geq_r b_2$.\nNow\n\n", "index": 17, "text": "\\begin{equation}\n\\begin{split}\n&\\sum\\limits_{z'=1}^Z \\eta(z'\\mid b_1,a) V_{k-1}^*(\\tau(b_1,a,z')) \\\\\n\\geq &\\sum\\limits_{z'=1}^Z \\eta(z'\\mid b_2,a) V_{k-1}^*(\\tau(b_1,a,z'))\\\\\n\\geq &\\sum\\limits_{z'=1}^Z \\eta(z'\\mid b_2,a) V_{k-1}^*(\\tau(b_2,a,z')).\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\sum\\limits_{z^{\\prime}=1}^{Z}\\eta(z^{\\prime}\\mid b%&#10;_{1},a)V_{k-1}^{*}(\\tau(b_{1},a,z^{\\prime}))\\\\&#10;\\displaystyle\\geq&amp;\\displaystyle\\sum\\limits_{z^{\\prime}=1}^{Z}\\eta(z^{\\prime}%&#10;\\mid b_{2},a)V_{k-1}^{*}(\\tau(b_{1},a,z^{\\prime}))\\\\&#10;\\displaystyle\\geq&amp;\\displaystyle\\sum\\limits_{z^{\\prime}=1}^{Z}\\eta(z^{\\prime}%&#10;\\mid b_{2},a)V_{k-1}^{*}(\\tau(b_{2},a,z^{\\prime})).\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Z</mi></munderover><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>V</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mo>\u2265</mo></mtd><mtd columnalign=\"left\"><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Z</mi></munderover><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>V</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mo>\u2265</mo></mtd><mtd columnalign=\"left\"><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>z</mi><mo>\u2032</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Z</mi></munderover><mi>\u03b7</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo>\u2223</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><msubsup><mi>V</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mn>2</mn></msub><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>z</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $g,h \\in \\mathbb{R}^S$ are parameter vectors chosen so that $\\underline{\\rho}(b,a)$ is MLR increasing and $\\overline{\\rho}(b,a)$ is MLR decreasing in ${\\mathcal{B}}$.\n\nNote that, by construction the transformed rewards in \\eqref{eq:transformed_reward} leave the optimal policy unaffected. This can be verified by plugging either one into the Bellman equation \\eqref{eq:bellman_fixedpoint}. The corresponding infinite horizon value functions are $\\underline{V}^*(b) = V^*(b) + g^T b$, $\\overline{V}^*(b) = V^*(b) + h^T b$, and $\\underline{\\mu}^* \\equiv \\mu^* \\equiv \\overline{\\mu}^*$~\\cite{Krishnamurthy2015}. The following theorem gives necessary conditions for the existence of vectors $g$ and $h$ at a single belief state.\n\n\\begin{theorem}\n\\label{thm:mlr_LP}\nLet $b_0\\in{\\mathcal{B}}$. If there exists a $g\\in{\\mathbb{R}}^S$ such that the set of linear constraints given by\n\n", "itemtype": "equation", "pos": 24594, "prevtext": "\nThe first inequality follows by Lemma~\\ref{lem:sdom}, \\cite[Lemma~1.2(2),1.3(1)]{Lovejoy1987} which state $\\tau(b,a,z')$ is MLR increasing in $z'$, and the the induction hypothesis.\nThe second inequality follows by Lemma~\\ref{lem:rdom} and the induction hypothesis.\nThe proof for the MLR decreasing part is similar and omitted.\n\\end{proof}\n\nFollowing the observation in Corollary~\\ref{cor:mlr_policy_bounds}, the main idea is to transform the reward function $\\rho(b,a)$~\\eqref{eq:info_reward} to one that is MLR monotone but leaves the optimal policy unchanged. Accordingly, we define two transformed reward functions:\n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq:transformed_reward}\n\\begin{split}\n\\underline{\\rho}(b,a) &= \\rho(b,a) + [(I-\\gamma({\\mathbb{T}}^a)^T)g]^T b\\\\\n\\overline{\\rho}(b,a) &= \\rho(b,a) + [(I-\\gamma({\\mathbb{T}}^a)^T)h]^T b,\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\underline{\\rho}(b,a)&amp;\\displaystyle=\\rho(b,a)+[(I-%&#10;\\gamma({\\mathbb{T}}^{a})^{T})g]^{T}b\\\\&#10;\\displaystyle\\overline{\\rho}(b,a)&amp;\\displaystyle=\\rho(b,a)+[(I-\\gamma({\\mathbb{%&#10;T}}^{a})^{T})h]^{T}b,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><munder accentunder=\"true\"><mi>\u03c1</mi><mo>\u00af</mo></munder><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udd4b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi></mrow><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>b</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mover accent=\"true\"><mi>\u03c1</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udd4b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>h</mi></mrow><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>b</mi></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\n\n\n\n\nwhere $K$ is an $S$-by-$S$ matrix with entries\n\n", "itemtype": "equation", "pos": 25712, "prevtext": "\nwhere $g,h \\in \\mathbb{R}^S$ are parameter vectors chosen so that $\\underline{\\rho}(b,a)$ is MLR increasing and $\\overline{\\rho}(b,a)$ is MLR decreasing in ${\\mathcal{B}}$.\n\nNote that, by construction the transformed rewards in \\eqref{eq:transformed_reward} leave the optimal policy unaffected. This can be verified by plugging either one into the Bellman equation \\eqref{eq:bellman_fixedpoint}. The corresponding infinite horizon value functions are $\\underline{V}^*(b) = V^*(b) + g^T b$, $\\overline{V}^*(b) = V^*(b) + h^T b$, and $\\underline{\\mu}^* \\equiv \\mu^* \\equiv \\overline{\\mu}^*$~\\cite{Krishnamurthy2015}. The following theorem gives necessary conditions for the existence of vectors $g$ and $h$ at a single belief state.\n\n\\begin{theorem}\n\\label{thm:mlr_LP}\nLet $b_0\\in{\\mathcal{B}}$. If there exists a $g\\in{\\mathbb{R}}^S$ such that the set of linear constraints given by\n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:mlr_LP}\n\\left.\\frac{\\partial \\rho(b,a)}{\\partial b}\\right|_{b=b_0}K + [(I-\\gamma({\\mathbb{T}}^a)^T)g]^T K \\geq 0 \\quad \\forall a \\in {\\mathcal{A}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\left.\\frac{\\partial\\rho(b,a)}{\\partial b}\\right|_{b=b_{0}}K+[(I-\\gamma({%&#10;\\mathbb{T}}^{a})^{T})g]^{T}K\\geq 0\\quad\\forall a\\in{\\mathcal{A}}\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03c1</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>b</mi></mrow></mfrac><mo fence=\"true\">|</mo></mrow><mrow><mi>b</mi><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub></mrow></msub><mo>\u2062</mo><mi>K</mi></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udd4b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi></mrow><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>K</mi></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mo>\u2200</mo><mi>a</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nare satisfied, then $\\underline{\\rho}(b,a)$ as defined in Eq.~\\eqref{eq:transformed_reward} is MLR increasing at $b_0$. \n\\end{theorem}\n\\begin{proof}\nAs in the supplementary material of \\cite{Krishnamurthy2015}, let $\\Delta = \\{\\delta \\in \\mathbb{R}^S: 1=\\delta(1)\\geq \\delta(2) \\geq \\ldots \\geq \\delta(S) \\}$.\nAny belief state $b\\in{\\mathcal{B}}$ may be represented as $\\delta = Kb \\in \\Delta$.\nLet $b_1 = K\\delta_1, b_2 = K\\delta_2$ such that $b_1 \\geq_r b_2$.\nBy Definition~\\ref{def:stoch_orders}, $b_1 \\geq_s b_2$, and the equivalent partial order on $\\Delta \\subset \\mathbb{R}^S$ is the component-wise partial order between $\\delta_1$ and $\\delta_2$.\nThen, the function $\\underline{\\rho}(K\\delta,a)$ is increasing on $\\Delta$ if its partial derivatives are non-negative:\n\n", "itemtype": "equation", "pos": 25952, "prevtext": "\n\n\n\n\nwhere $K$ is an $S$-by-$S$ matrix with entries\n\n", "index": 23, "text": "\\begin{equation}\nk_{ij} = \\begin{cases}\n1 & \\text{if } i=j\\\\\n-1 & \\text{if } j = i+1\n\\end{cases},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"k_{ij}=\\begin{cases}1&amp;\\text{if }i=j\\\\&#10;-1&amp;\\text{if }j=i+1\\end{cases},\" display=\"block\"><mrow><mrow><msub><mi>k</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>=</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nand the claim follows by evaluating the derivative at $b_0$.\n\\end{proof}\nThe proof for $\\overline{\\rho}(b,a)$ being MLR decreasing is obtained similarly, by swapping the required sign for the partial derivatives. \nPractically, the condition of the theorem may be checked by solving a feasibility linear program (LP) with $(S-1)\\cdot A$ constraints.\nIf a single $g$ and $h$ may be chosen such that the transformed rewards are MLR monotone for \\emph{all} $b\\in{\\mathcal{B}}$, then the myopic policies\n\n", "itemtype": "equation", "pos": 26840, "prevtext": "\nare satisfied, then $\\underline{\\rho}(b,a)$ as defined in Eq.~\\eqref{eq:transformed_reward} is MLR increasing at $b_0$. \n\\end{theorem}\n\\begin{proof}\nAs in the supplementary material of \\cite{Krishnamurthy2015}, let $\\Delta = \\{\\delta \\in \\mathbb{R}^S: 1=\\delta(1)\\geq \\delta(2) \\geq \\ldots \\geq \\delta(S) \\}$.\nAny belief state $b\\in{\\mathcal{B}}$ may be represented as $\\delta = Kb \\in \\Delta$.\nLet $b_1 = K\\delta_1, b_2 = K\\delta_2$ such that $b_1 \\geq_r b_2$.\nBy Definition~\\ref{def:stoch_orders}, $b_1 \\geq_s b_2$, and the equivalent partial order on $\\Delta \\subset \\mathbb{R}^S$ is the component-wise partial order between $\\delta_1$ and $\\delta_2$.\nThen, the function $\\underline{\\rho}(K\\delta,a)$ is increasing on $\\Delta$ if its partial derivatives are non-negative:\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{split}\n\\frac{\\partial \\underline{\\rho}(K\\delta,a)}{\\partial \\delta} &= \\frac{\\partial \\rho(K\\delta,a)}{\\partial \\delta} + \\frac{\\partial [(I-\\gamma({\\mathbb{T}}^a)^T)g]^T K\\delta}{\\partial \\delta}\\\\\n&=\\frac{\\partial \\rho(b,a)}{\\partial b}K + [(I-\\gamma({\\mathbb{T}}^a)^T)g]^T K \\geq 0\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\frac{\\partial\\underline{\\rho}(K\\delta,a)}{\\partial%&#10;\\delta}&amp;\\displaystyle=\\frac{\\partial\\rho(K\\delta,a)}{\\partial\\delta}+\\frac{%&#10;\\partial[(I-\\gamma({\\mathbb{T}}^{a})^{T})g]^{T}K\\delta}{\\partial\\delta}\\\\&#10;&amp;\\displaystyle=\\frac{\\partial\\rho(b,a)}{\\partial b}K+[(I-\\gamma({\\mathbb{T}}^{%&#10;a})^{T})g]^{T}K\\geq 0\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><munder accentunder=\"true\"><mi>\u03c1</mi><mo>\u00af</mo></munder></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mi>\u03b4</mi></mrow><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b4</mi></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03c1</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>K</mi><mo>\u2062</mo><mi>\u03b4</mi></mrow><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b4</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mo>\u2202</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mo>-</mo><mi>\u03b3</mi><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udd4b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>g</mi><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup><mi>K</mi><mi>\u03b4</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b4</mi></mrow></mfrac></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03c1</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>b</mi></mrow></mfrac><mo>\u2062</mo><mi>K</mi></mrow><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udd4b</mi><mi>a</mi></msup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>g</mi></mrow><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>K</mi></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nsatisfy $\\underline{\\mu}_1^*(b) \\leq \\mu^*(b) \\leq \\overline{\\mu}_1^*(b)$ for all $b\\in{\\mathcal{B}}$.\nThis follows from Corollary~\\ref{cor:mlr_policy_bounds}, and the fact that the optimal stationary policy is unaffected by applying the transformed rewards.\nIt is imporant to note that the preceding inequalities do \\emph{not} hold anymore for finite horizon optimal policies.\n\nConsider the following two examples of nonlinear belief-dependent reward functions obtained by applying different uncertainty functions in Eq.~\\eqref{eq:info_reward}. In both examples, we denote by $r_a$ the column vector formed from the elements of $r(s,a)$ for all $s\\in{\\mathcal{S}}$.\n\n\\begin{example}[Shannon Entropy]\nLet the uncertainty function in~\\eqref{eq:info_reward} be the Shannon entropy:\n", "itemtype": "equation", "pos": 27675, "prevtext": "\nand the claim follows by evaluating the derivative at $b_0$.\n\\end{proof}\nThe proof for $\\overline{\\rho}(b,a)$ being MLR decreasing is obtained similarly, by swapping the required sign for the partial derivatives. \nPractically, the condition of the theorem may be checked by solving a feasibility linear program (LP) with $(S-1)\\cdot A$ constraints.\nIf a single $g$ and $h$ may be chosen such that the transformed rewards are MLR monotone for \\emph{all} $b\\in{\\mathcal{B}}$, then the myopic policies\n\n", "index": 27, "text": "\\begin{equation}\n\\begin{split}\n\\underline{\\mu}_1^*(b) &= {\\operatorname{argmax}}_{a\\in{\\mathcal{A}}} \\underline{\\rho}(b,a)\\\\\n\\overline{\\mu}_1^*(b) &= {\\operatorname{argmax}}_{a\\in{\\mathcal{A}}} \\overline{\\rho}(b,a)\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\underline{\\mu}_{1}^{*}(b)&amp;\\displaystyle={%&#10;\\operatorname{argmax}}_{a\\in{\\mathcal{A}}}\\underline{\\rho}(b,a)\\\\&#10;\\displaystyle\\overline{\\mu}_{1}^{*}(b)&amp;\\displaystyle={\\operatorname{argmax}}_{%&#10;a\\in{\\mathcal{A}}}\\overline{\\rho}(b,a)\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><msubsup><munder accentunder=\"true\"><mi>\u03bc</mi><mo>\u00af</mo></munder><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mo>argmax</mo><mrow><mi>a</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></msub><mo>\u2061</mo><munder accentunder=\"true\"><mi>\u03c1</mi><mo>\u00af</mo></munder></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><msubsup><mover accent=\"true\"><mi>\u03bc</mi><mo>\u00af</mo></mover><mn>1</mn><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mo>argmax</mo><mrow><mi>a</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></msub><mo>\u2061</mo><mover accent=\"true\"><mi>\u03c1</mi><mo>\u00af</mo></mover></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nThe derivative of $\\rho(b,a)$ for the constraints in Thm.~\\ref{thm:mlr_LP} is:\n", "itemtype": "equation", "pos": 28696, "prevtext": "\nsatisfy $\\underline{\\mu}_1^*(b) \\leq \\mu^*(b) \\leq \\overline{\\mu}_1^*(b)$ for all $b\\in{\\mathcal{B}}$.\nThis follows from Corollary~\\ref{cor:mlr_policy_bounds}, and the fact that the optimal stationary policy is unaffected by applying the transformed rewards.\nIt is imporant to note that the preceding inequalities do \\emph{not} hold anymore for finite horizon optimal policies.\n\nConsider the following two examples of nonlinear belief-dependent reward functions obtained by applying different uncertainty functions in Eq.~\\eqref{eq:info_reward}. In both examples, we denote by $r_a$ the column vector formed from the elements of $r(s,a)$ for all $s\\in{\\mathcal{S}}$.\n\n\\begin{example}[Shannon Entropy]\nLet the uncertainty function in~\\eqref{eq:info_reward} be the Shannon entropy:\n", "index": 29, "text": "\n\\[\nf(b) := -\\sum\\limits_{i=1}^S b(i)\\log b(i).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"f(b):=-\\sum\\limits_{i=1}^{S}b(i)\\log b(i).\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nAs $\\log(0)$ is not well defined, we consider the inner belief simplex ${\\mathcal{B}}_\\epsilon := \\{b\\in{\\mathcal{B}} \\mid  \\forall i :  b(i) \\geq \\epsilon\\}$ for $\\epsilon \\in (0,1)$.\nNote that \n", "itemtype": "equation", "pos": 28825, "prevtext": "\nThe derivative of $\\rho(b,a)$ for the constraints in Thm.~\\ref{thm:mlr_LP} is:\n", "index": 31, "text": "\n\\[\n\\frac{\\partial \\rho(b,a)}{\\partial b} = r_a^T + w_a\\left[\\begin{matrix} 1+\\log(b(1)), & \\ldots & 1+\\log(b(S))\\end{matrix}\\right].\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\rho(b,a)}{\\partial b}=r_{a}^{T}+w_{a}\\left[\\begin{matrix}1+\\log%&#10;(b(1)),&amp;\\ldots&amp;1+\\log(b(S))\\end{matrix}\\right].\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03c1</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mrow><msubsup><mi>r</mi><mi>a</mi><mi>T</mi></msubsup><mo>+</mo><mrow><msub><mi>w</mi><mi>a</mi></msub><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mrow><mn>1</mn><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nFor $i = 1,\\ldots,S-1$ and $\\forall a\\in{\\mathcal{A}}$, the $i$th constraint in Eq.~\\eqref{eq:mlr_LP} can then be bounded by a belief-independent quantity:\n", "itemtype": "equation", "pos": 29157, "prevtext": "\nAs $\\log(0)$ is not well defined, we consider the inner belief simplex ${\\mathcal{B}}_\\epsilon := \\{b\\in{\\mathcal{B}} \\mid  \\forall i :  b(i) \\geq \\epsilon\\}$ for $\\epsilon \\in (0,1)$.\nNote that \n", "index": 33, "text": "\n\\[\n\\log(\\epsilon) \\leq \\log(b(i+1)) - \\log(b(i)) \\leq -\\log(\\epsilon).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\log(\\epsilon)\\leq\\log(b(i+1))-\\log(b(i))\\leq-\\log(\\epsilon).\" display=\"block\"><mrow><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $\\phi^a := r_a +(I-\\gamma({\\mathbb{T}}^a)^T)g$.\n\n\\end{example}\n\\begin{example}[R{\\'e}nyi Quadratic Entropy]\n\\label{xm:quadratic_entropy}\nLet the uncertainty function in~\\eqref{eq:info_reward} be the R{\\'e}nyi quadratic entropy:\n", "itemtype": "equation", "pos": 29387, "prevtext": "\nFor $i = 1,\\ldots,S-1$ and $\\forall a\\in{\\mathcal{A}}$, the $i$th constraint in Eq.~\\eqref{eq:mlr_LP} can then be bounded by a belief-independent quantity:\n", "index": 35, "text": "\n\\[\n\\phi^a_{i} - \\phi^a_{i+1} \\geq w_a \\left( \\log(b(i+1) - \\log(b(i)) \\right) \\geq -w_a\\log(\\epsilon),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\phi^{a}_{i}-\\phi^{a}_{i+1}\\geq w_{a}\\left(\\log(b(i+1)-\\log(b(i))\\right)\\geq-w%&#10;_{a}\\log(\\epsilon),\" display=\"block\"><mrow><msubsup><mi>\u03d5</mi><mi>i</mi><mi>a</mi></msubsup><mo>-</mo><msubsup><mi>\u03d5</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>a</mi></msubsup><mo>\u2265</mo><msub><mi>w</mi><mi>a</mi></msub><mrow><mo>(</mo><mi>log</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>log</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mo>\u2265</mo><mo>-</mo><msub><mi>w</mi><mi>a</mi></msub><mi>log</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\n\n\n\n\n\nThe derivative of $\\rho(b,a)$ for the constraints in Thm. \\ref{thm:mlr_LP} is:\n", "itemtype": "equation", "pos": 29727, "prevtext": "\nwhere $\\phi^a := r_a +(I-\\gamma({\\mathbb{T}}^a)^T)g$.\n\n\\end{example}\n\\begin{example}[R{\\'e}nyi Quadratic Entropy]\n\\label{xm:quadratic_entropy}\nLet the uncertainty function in~\\eqref{eq:info_reward} be the R{\\'e}nyi quadratic entropy:\n", "index": 37, "text": "\n\\[\nf(b) := -\\log \\sum_{i=1}^S b^2(i).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"f(b):=-\\log\\sum_{i=1}^{S}b^{2}(i).\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nNote that:\n", "itemtype": "equation", "pos": 29852, "prevtext": "\n\n\n\n\n\nThe derivative of $\\rho(b,a)$ for the constraints in Thm. \\ref{thm:mlr_LP} is:\n", "index": 39, "text": "\n\\[\n\\frac{\\partial \\rho(b,a)}{\\partial b} = r_a^T + \\frac{2 w_a}{\\sum_{i=1}^S b^2(i)} b^T.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\rho(b,a)}{\\partial b}=r_{a}^{T}+\\frac{2w_{a}}{\\sum_{i=1}^{S}b^{%&#10;2}(i)}b^{T}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03c1</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mrow><msubsup><mi>r</mi><mi>a</mi><mi>T</mi></msubsup><mo>+</mo><mrow><mfrac><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>w</mi><mi>a</mi></msub></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2062</mo><msup><mi>b</mi><mi>T</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhich can be used to obtain belief-independent bounds in \\eqref{eq:mlr_LP}. In particular, the $i$th constraint in \\eqref{eq:mlr_LP} may be bounded by a belief-independent quantity:\n", "itemtype": "equation", "pos": 29956, "prevtext": "\nNote that:\n", "index": 41, "text": "\n\\[\n-1 \\leq b(i+1) - b(i) \\leq 1 \\qquad \\text{and} \\qquad \\frac{1}{S} \\leq \\sum_{i=1}^S b^2(i) \\leq 1,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"-1\\leq b(i+1)-b(i)\\leq 1\\qquad\\text{and}\\qquad\\frac{1}{S}\\leq\\sum_{i=1}^{S}b^{%&#10;2}(i)\\leq 1,\" display=\"block\"><mrow><mrow><mrow><mrow><mo>-</mo><mn>1</mn></mrow><mo>\u2264</mo><mrow><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mrow><mtext>and</mtext><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mfrac><mn>1</mn><mi>S</mi></mfrac></mrow><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere $\\phi^a := r_a +(I-\\gamma({\\mathbb{T}}^a)^T)g$ and $i = 1,\\ldots,S-1$.\n\n\\end{example}\nIn both examples, belief independent bounds for finding $h$ are obtained with straightforward changes.\nFor Shannon entropy, one ideally wants to choose $\\epsilon$ as small as possible to cover the greatest possible subset of the belief space.\nOn the other hand, the smaller $\\epsilon$ is, the more restrictive the constraints become, making it more unlikely to find feasible solutions.\nThe issue is resolved by considering a different quantification of uncertainty, such as the R{\\'e}nyi quadratic entropy.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\columnwidth]{reward_example-eps-converted-to}\n\\caption{Expected original (left axes) and transformed (right axes) rewards as function of the belief state according to Example~\\ref{xm:quadratic_entropy}.\nRewards for action $a_1$ are shown by a solid line and for action $a_2$ by a dashed line.\nThe increasing blue curves in the right axes depict the MLR increasing rewards $\\underline{\\rho}(b,a)$, and the decreasing red curves depict the MLR decreasing rewards $\\overline{\\rho}(b,a)$.\nThe two regions on the right axes indicated by lines starting and ending with circle markers are the parts of the belief space where the optimal policy is fully determined by the myopic policy bounds.\nThe dashed region corresponds to $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b) = a_2$ and the solid region to $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b) = a_1$.}\n\\label{fig:reward_example}\n\\end{figure}\n\nFigure~\\ref{fig:reward_example} shows an example of the transformation of R{\\'e}nyi quadratic entropy rewards and the associated monotonicity properties as discussed in Example~\\ref{xm:quadratic_entropy}.\nThe left axes show the original expected reward as function of the belief state, and the right axes show the transformed MLR increasing and decreasing rewards, respectively.\nFor $S=2$, the belief space ${\\mathcal{B}}$ can be represented by $[0,1]\\subset \\mathbb{R}$ and MLR monotonicity is equivalent to the familiar notion of monotonicity on $\\mathbb{R}$.\nOn the bottom of the right axes, two regions in ${\\mathcal{B}}$ are indicated by a dashed and a solid line.\nThese are regions where the myopic policy bounds agree and the optimal policy is thus fully determined.\n\n\\section{Myopic bounds for branch-and-bound pruning}\n\\label{sec:bb}\nThe significance of Theorems~\\ref{thm:mlr_policy_bounds} and~\\ref{thm:mlr_LP} is that they give a prescription for constructing policy bounds $\\underline{\\mu}_1^*(b) \\leq \\mu^*(b) \\leq \\overline{\\mu}_1^*(b)$ for all $b\\in{\\mathcal{B}}$. In this section, we show that the bounds can be used in an online branch-and-bound scheme to accelerate the computation of the optimal policy for POMDPs that satisfy Assumption~\\ref{ass:model_assumptions}.\n\nBranch-and-bound pruning for POMDPs is based on a tree search over the set of belief states reachable by finite-length action and observation sequences, see e.g.\\ \\cite{Ross2008}. The search is started from the current belief state $b_0$ at the root of the search tree.\nThe tree branches due to action choices and observation possibilities, so that each node corresponds to a belief state $b$ reachable from $b_0$ via the prediction and update iteration in~\\eqref{eq:predicted_belief},~\\eqref{eq:updated_belief}. The standard branch-and-bound algorithm works with lower and upper bounds on the optimal action \\emph{values}, i.e., $\\underline{Q}_k(b,a) \\leq Q^*_k(b,a) \\leq \\overline{Q}_k(b,a)$. Starting from node $b$, the subtree corresponding to action $a$ may be pruned if it is known to be suboptimal (i.e, the upper bound $\\overline{Q}_k(b,a)$ is lower than the lower bound $\\underline{Q}_k(b,\\hat{a})$ for some other action $\\hat{a}$), thus saving computational resources.\n\nFollowing the intuition of the branch-and-bound scheme above, we design a new algorithm (Alg.~\\ref{alg:bb}), which employs our policy bounds, \\emph{instead of} bounds on the optimal action values. Given a belief state $b$ and a search depth $d>0$, the algorithm constructs a search tree over reachable belief states, storing the estimate of the value $\\hat{Q}(b,a)$ for each visited belief state.\nOnly actions within the myopic lower and upper bound policies are considered.\n\n\\begin{algorithm}[ht]\n\\caption{Branch-and-bound with myopic policy bounds.}          \n\\label{alg:bb}                           \n\\begin{algorithmic}[1]\n\\Require Belief state $b$, search depth $d$\n\\Ensure Estimate of the value of the best action at $b$\n\\Function{Search}{$b$, $d$}\n\\If {$d = 1$} \\Return $\\max\\limits_{a}\\rho(b,a)$ \\EndIf\n\\ForAll{$a \\in {\\mathcal{A}} \\cap [\\underline{\\mu}^*_1(b), \\overline{\\mu}^*_1(b)]$} \n\n\\State $\\hat{Q}(b,a) \\gets \\rho(b,a) +\\gamma \\sum\\limits_{z' \\in {\\mathcal{Z}}}\\biggl(\\eta(z'\\mid b,a)$\n\\State \\hfill$\\times \\text{\\Call{Search}{$\\tau(b,a,z')$, $d-1$}}\\biggr)$\n\\EndFor\n\\State \\Return $\\max\\limits_{a} \\hat{Q}(b,a)$\n\\EndFunction\n\\end{algorithmic}\n\\end{algorithm}\nThe next proposition follows from \\cite[Theorem~3.1]{Ross2008}.\n\\begin{proposition}\nAfter Alg.~\\ref{alg:bb} terminates, its action recommendation $\\hat{a}$ is guaranteed to coincide with the optimal action $a^*:=\\mu^*(b)$ if the myopic lower and upper bounds agree at $b$, i.e., if $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b)$. Otherwise, the suboptimality of $\\hat{a}$ is of order $\\gamma^d$, i.e., $|Q^*(b,\\hat{a}) - Q^*(b,a^*)| \\sim o(\\gamma^d)$. \n\\end{proposition}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application to Target Tracking}\n\\label{sec:empirical}\nWe evaluate the performance of Algorithm~\\ref{alg:bb} in a target tracking application. Let $s \\in {\\mathcal{S}}$ model the state of a physical process of interest such as the position of an adversarial aircraft or certain environment conditions (e.g., temperature, chemical concentration, air pollution, etc.). Let $s=1$ correspond to an innocuous target state (e.g., the aircraft poses no danger or the environment levels are normal) and, at the other extreme, let $s=S$ correspond to a red-alert state in which there is imminent danger. We aim to design a control policy for an autonomous robotic platform to track the target state $s$. Let the available robot actions ${\\mathcal{A}}=\\{1, 2, \\ldots, A\\}$ model the priority invested into tracking the target, with $1$ corresponding to greatest priority (e.g, all available resources being dedicated to tracking the target) and $A$ corresponding to lowest effort, i.e.\\ the target is not being tracked.\n\nThe state transition model is such that the lower the priority invested into tracking, the more likely it is to transition to a state that is considered dangerous.\nConversely, the higher the priority given to tracking, the less likely it is to accidentally enter a dangerous state.\nIn other words, supposing the current state is $i$, the likelihood of transitioning to state $j>i$ should increase as function of $a$.\nState transition matrices ${\\mathbb{T}}^a$ modeling this can be defined e.g. as follows.\nLet $\\{x_i\\}_{i=1}^S$ and $\\{y_j\\}_{j=1}^{SA}$ be two increasing sequences of real numbers, and define a matrix $A_{ij}=\\exp(x_iy_j)$, normalizing it such that each column sums to one, and define ${\\mathbb{T}}^a$ as the submatrix of $A$ containing all rows and columns $j$ for which $1+(a-1)\\cdot S \\leq j \\leq a\\cdot S$.\nEach ${\\mathbb{T}}^a$ also satisfies the required TP2 condition \\cite{Fallat2011}.\n\nThe robot has access to $Z=S$ observations providing information about the target state.\nThe observation matrix ${\\mathbb{O}}^a$ for each action $a\\in{\\mathcal{A}}$ has elements\n\n", "itemtype": "equation", "pos": 30243, "prevtext": "\nwhich can be used to obtain belief-independent bounds in \\eqref{eq:mlr_LP}. In particular, the $i$th constraint in \\eqref{eq:mlr_LP} may be bounded by a belief-independent quantity:\n", "index": 43, "text": "\n\\[\n\\phi^a_i - \\phi^a_{i+1} \\geq \\frac{2 w_a}{\\sum_{j=1}^S b^2(j)} (b(i+1) - b(i))\\geq -2 w_a,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\phi^{a}_{i}-\\phi^{a}_{i+1}\\geq\\frac{2w_{a}}{\\sum_{j=1}^{S}b^{2}(j)}(b(i+1)-b(%&#10;i))\\geq-2w_{a},\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>\u03d5</mi><mi>i</mi><mi>a</mi></msubsup><mo>-</mo><msubsup><mi>\u03d5</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>a</mi></msubsup></mrow><mo>\u2265</mo><mrow><mfrac><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>w</mi><mi>a</mi></msub></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><mrow><msup><mi>b</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>w</mi><mi>a</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\ni.e.\\ with probability $q<1$ the observation identifies the true state, with symmetric error probability in either direction.\nThis model corresponds to a situation where e.g.\\ entering a dangerous state is perceived e.g.\\ by the robot being damaged by hazardous environmental conditions.\nThis class of target tracking domains can be checked to satisfy Assumptions~\\ref{ass:model_assumptions}.\n\n\n\\subsection{Minimizing the uncertainty of the target state}\nWe first consider target tracking with a penalty for uncertain tracker state information.\nWe set $S=A=Z=3$ and defined the sequences $\\{x_i\\}=\\{1,2,3\\}$, $\\{y_j\\}=\\{-9,-8,\\ldots,-1\\}$.\nThe observation model was as defined in Eq.~\\eqref{eq:obsmodel_example}.\nThe reward was as in Eq.~\\eqref{eq:info_reward}, with\n\n", "itemtype": "equation", "pos": 37952, "prevtext": "\nwhere $\\phi^a := r_a +(I-\\gamma({\\mathbb{T}}^a)^T)g$ and $i = 1,\\ldots,S-1$.\n\n\\end{example}\nIn both examples, belief independent bounds for finding $h$ are obtained with straightforward changes.\nFor Shannon entropy, one ideally wants to choose $\\epsilon$ as small as possible to cover the greatest possible subset of the belief space.\nOn the other hand, the smaller $\\epsilon$ is, the more restrictive the constraints become, making it more unlikely to find feasible solutions.\nThe issue is resolved by considering a different quantification of uncertainty, such as the R{\\'e}nyi quadratic entropy.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\columnwidth]{reward_example-eps-converted-to}\n\\caption{Expected original (left axes) and transformed (right axes) rewards as function of the belief state according to Example~\\ref{xm:quadratic_entropy}.\nRewards for action $a_1$ are shown by a solid line and for action $a_2$ by a dashed line.\nThe increasing blue curves in the right axes depict the MLR increasing rewards $\\underline{\\rho}(b,a)$, and the decreasing red curves depict the MLR decreasing rewards $\\overline{\\rho}(b,a)$.\nThe two regions on the right axes indicated by lines starting and ending with circle markers are the parts of the belief space where the optimal policy is fully determined by the myopic policy bounds.\nThe dashed region corresponds to $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b) = a_2$ and the solid region to $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b) = a_1$.}\n\\label{fig:reward_example}\n\\end{figure}\n\nFigure~\\ref{fig:reward_example} shows an example of the transformation of R{\\'e}nyi quadratic entropy rewards and the associated monotonicity properties as discussed in Example~\\ref{xm:quadratic_entropy}.\nThe left axes show the original expected reward as function of the belief state, and the right axes show the transformed MLR increasing and decreasing rewards, respectively.\nFor $S=2$, the belief space ${\\mathcal{B}}$ can be represented by $[0,1]\\subset \\mathbb{R}$ and MLR monotonicity is equivalent to the familiar notion of monotonicity on $\\mathbb{R}$.\nOn the bottom of the right axes, two regions in ${\\mathcal{B}}$ are indicated by a dashed and a solid line.\nThese are regions where the myopic policy bounds agree and the optimal policy is thus fully determined.\n\n\\section{Myopic bounds for branch-and-bound pruning}\n\\label{sec:bb}\nThe significance of Theorems~\\ref{thm:mlr_policy_bounds} and~\\ref{thm:mlr_LP} is that they give a prescription for constructing policy bounds $\\underline{\\mu}_1^*(b) \\leq \\mu^*(b) \\leq \\overline{\\mu}_1^*(b)$ for all $b\\in{\\mathcal{B}}$. In this section, we show that the bounds can be used in an online branch-and-bound scheme to accelerate the computation of the optimal policy for POMDPs that satisfy Assumption~\\ref{ass:model_assumptions}.\n\nBranch-and-bound pruning for POMDPs is based on a tree search over the set of belief states reachable by finite-length action and observation sequences, see e.g.\\ \\cite{Ross2008}. The search is started from the current belief state $b_0$ at the root of the search tree.\nThe tree branches due to action choices and observation possibilities, so that each node corresponds to a belief state $b$ reachable from $b_0$ via the prediction and update iteration in~\\eqref{eq:predicted_belief},~\\eqref{eq:updated_belief}. The standard branch-and-bound algorithm works with lower and upper bounds on the optimal action \\emph{values}, i.e., $\\underline{Q}_k(b,a) \\leq Q^*_k(b,a) \\leq \\overline{Q}_k(b,a)$. Starting from node $b$, the subtree corresponding to action $a$ may be pruned if it is known to be suboptimal (i.e, the upper bound $\\overline{Q}_k(b,a)$ is lower than the lower bound $\\underline{Q}_k(b,\\hat{a})$ for some other action $\\hat{a}$), thus saving computational resources.\n\nFollowing the intuition of the branch-and-bound scheme above, we design a new algorithm (Alg.~\\ref{alg:bb}), which employs our policy bounds, \\emph{instead of} bounds on the optimal action values. Given a belief state $b$ and a search depth $d>0$, the algorithm constructs a search tree over reachable belief states, storing the estimate of the value $\\hat{Q}(b,a)$ for each visited belief state.\nOnly actions within the myopic lower and upper bound policies are considered.\n\n\\begin{algorithm}[ht]\n\\caption{Branch-and-bound with myopic policy bounds.}          \n\\label{alg:bb}                           \n\\begin{algorithmic}[1]\n\\Require Belief state $b$, search depth $d$\n\\Ensure Estimate of the value of the best action at $b$\n\\Function{Search}{$b$, $d$}\n\\If {$d = 1$} \\Return $\\max\\limits_{a}\\rho(b,a)$ \\EndIf\n\\ForAll{$a \\in {\\mathcal{A}} \\cap [\\underline{\\mu}^*_1(b), \\overline{\\mu}^*_1(b)]$} \n\n\\State $\\hat{Q}(b,a) \\gets \\rho(b,a) +\\gamma \\sum\\limits_{z' \\in {\\mathcal{Z}}}\\biggl(\\eta(z'\\mid b,a)$\n\\State \\hfill$\\times \\text{\\Call{Search}{$\\tau(b,a,z')$, $d-1$}}\\biggr)$\n\\EndFor\n\\State \\Return $\\max\\limits_{a} \\hat{Q}(b,a)$\n\\EndFunction\n\\end{algorithmic}\n\\end{algorithm}\nThe next proposition follows from \\cite[Theorem~3.1]{Ross2008}.\n\\begin{proposition}\nAfter Alg.~\\ref{alg:bb} terminates, its action recommendation $\\hat{a}$ is guaranteed to coincide with the optimal action $a^*:=\\mu^*(b)$ if the myopic lower and upper bounds agree at $b$, i.e., if $\\underline{\\mu}^*_1(b) = \\overline{\\mu}^*_1(b)$. Otherwise, the suboptimality of $\\hat{a}$ is of order $\\gamma^d$, i.e., $|Q^*(b,\\hat{a}) - Q^*(b,a^*)| \\sim o(\\gamma^d)$. \n\\end{proposition}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Application to Target Tracking}\n\\label{sec:empirical}\nWe evaluate the performance of Algorithm~\\ref{alg:bb} in a target tracking application. Let $s \\in {\\mathcal{S}}$ model the state of a physical process of interest such as the position of an adversarial aircraft or certain environment conditions (e.g., temperature, chemical concentration, air pollution, etc.). Let $s=1$ correspond to an innocuous target state (e.g., the aircraft poses no danger or the environment levels are normal) and, at the other extreme, let $s=S$ correspond to a red-alert state in which there is imminent danger. We aim to design a control policy for an autonomous robotic platform to track the target state $s$. Let the available robot actions ${\\mathcal{A}}=\\{1, 2, \\ldots, A\\}$ model the priority invested into tracking the target, with $1$ corresponding to greatest priority (e.g, all available resources being dedicated to tracking the target) and $A$ corresponding to lowest effort, i.e.\\ the target is not being tracked.\n\nThe state transition model is such that the lower the priority invested into tracking, the more likely it is to transition to a state that is considered dangerous.\nConversely, the higher the priority given to tracking, the less likely it is to accidentally enter a dangerous state.\nIn other words, supposing the current state is $i$, the likelihood of transitioning to state $j>i$ should increase as function of $a$.\nState transition matrices ${\\mathbb{T}}^a$ modeling this can be defined e.g. as follows.\nLet $\\{x_i\\}_{i=1}^S$ and $\\{y_j\\}_{j=1}^{SA}$ be two increasing sequences of real numbers, and define a matrix $A_{ij}=\\exp(x_iy_j)$, normalizing it such that each column sums to one, and define ${\\mathbb{T}}^a$ as the submatrix of $A$ containing all rows and columns $j$ for which $1+(a-1)\\cdot S \\leq j \\leq a\\cdot S$.\nEach ${\\mathbb{T}}^a$ also satisfies the required TP2 condition \\cite{Fallat2011}.\n\nThe robot has access to $Z=S$ observations providing information about the target state.\nThe observation matrix ${\\mathbb{O}}^a$ for each action $a\\in{\\mathcal{A}}$ has elements\n\n", "index": 45, "text": "\\begin{equation}\n\\label{eq:obsmodel_example}\n{\\mathbb{O}}^a_{z,j} = \\begin{cases}\nq & \\textnormal{if } z=j\\\\\n(1-q)/2 &\\textnormal{if } j\\neq 1, i\\neq S \\textnormal{ and } |z-j|=1\\\\\n(1-q) &\\textnormal{if } j=1, i=S \\textnormal{ and } |z-j|=1\\\\\n0 & \\textnormal{otherwise}\n\\end{cases},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{O}}^{a}_{z,j}=\\begin{cases}q&amp;\\textnormal{if }z=j\\\\&#10;(1-q)/2&amp;\\textnormal{if }j\\neq 1,i\\neq S\\textnormal{ and }|z-j|=1\\\\&#10;(1-q)&amp;\\textnormal{if }j=1,i=S\\textnormal{ and }|z-j|=1\\\\&#10;0&amp;\\textnormal{otherwise}\\end{cases},\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udd46</mi><mrow><mi>z</mi><mo>,</mo><mi>j</mi></mrow><mi>a</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi>q</mi></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>z</mi></mrow><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2260</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>\u2260</mo><mrow><mi>S</mi><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>z</mi><mo>-</mo><mi>j</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi>S</mi><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>z</mi><mo>-</mo><mi>j</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\n$w_a = \\left[\\begin{matrix}1.1 & 1.6 &1\\end{matrix}\\right]$, and $f$ equal to R{\\'e}nyi quadratic entropy.\nThe discount factor was $\\gamma = 0.99$, and the observation accuracy parameter was $q=0.7$.\n\nWe computed the lower and upper myopic policy bounds for the problem.\nThe bounds are visualized on the belief simplex ${\\mathcal{B}}$ in Figure~\\ref{fig:policy_bound_example}.\nThe figure shows on the left and middle the lower and upper myopic policies, and on the right the optimal policy determined when the two bounds agree.\n\nWe defined a larger problem with $S=Z=8$ and $A=3$, and in it compared Algorithm~\\ref{alg:bb} against an exhaustive tree search.\nThere are $N_d = (AZ)^d$ reachable belief states after $d$ decisions, and the number of belief states in a complete search tree is $\\sum_{i=0}^d N_i$.\n\n\\begin{table}[ht]\n\\centering\n\\caption{Average percentage of belief states pruned as function of the search depth $d$ applying the myopic policy bounds.}\n\\label{tab:expansions}\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\n        \t\t\t        & $d=2$ & $d=3$ & $d=4$ & $d=5$ & $d=6$ \\\\ \\midrule\nBelief states pruned & 26.3\\%  & 36.3\\%   & 44.9\\%  & 52.3\\%  & 58.8\\%  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nTable~\\ref{tab:expansions} shows the average percentage of belief state pruned from the search tree for 100 randomly sampled initial belief states.\nComputing the myopic policy bound requires solving a feasibility LP with $(S-1)\\cdot A$ constraints offline, and then during the search computing for each belief encountered the action maximizing the immediate expected transformed reward.\nEmpirically, we found that for $d\\geq 3$ branch-and-bounding was faster than the exhaustive search measured by computation time.\n\n\n\n\\subsection{Comparison with existing approaches}\nNext, we compare the performance of our approach to an optimal incremental pruning algorithm\\footnote{We applied the implementation from the pomdp-solve package of A. Cassandra, see \\url{http://pomdp.org}} in a target tracking scenario with a state space size $S$ between $4$ and $256$, and action space size $A$ between $4$ and $64$. \nSince the incremental pruning algorithm (as well as other existing approaches) can handle only state-dependent rewards, we do not add an uncertainty function to the reward, i.e., $f \\equiv 0$ in~\\eqref{eq:info_reward}.\nThe state-dependent reward $r(s,a)$ is designed to 1) incur a lower reward for investing higher priority in target tracking and to 2) penalize for both tracking the target poorly or too dangerously.\nThe penalty on dangerous tracking models a situation where attempting to track the state too aggressively may make the robot vulnerable to environmental hazards.\nThe reward is set as $r(s,a) = -c_a\\cdot (A-a+1) + t(s,a)$, where $c_a$ is the cost of expending one unit of effort in tracking, and $t(s,a)$ is a tracking performance reward, defined\n\n", "itemtype": "equation", "pos": 39017, "prevtext": "\ni.e.\\ with probability $q<1$ the observation identifies the true state, with symmetric error probability in either direction.\nThis model corresponds to a situation where e.g.\\ entering a dangerous state is perceived e.g.\\ by the robot being damaged by hazardous environmental conditions.\nThis class of target tracking domains can be checked to satisfy Assumptions~\\ref{ass:model_assumptions}.\n\n\n\\subsection{Minimizing the uncertainty of the target state}\nWe first consider target tracking with a penalty for uncertain tracker state information.\nWe set $S=A=Z=3$ and defined the sequences $\\{x_i\\}=\\{1,2,3\\}$, $\\{y_j\\}=\\{-9,-8,\\ldots,-1\\}$.\nThe observation model was as defined in Eq.~\\eqref{eq:obsmodel_example}.\nThe reward was as in Eq.~\\eqref{eq:info_reward}, with\n\n", "index": 47, "text": "\\begin{equation}\nr(s,a) = \\left[\\begin{matrix}\n2 & 2.5 & 1\\\\\n1.1 & 1.2 & 0.5\\\\\n0.3 & 2 & 0.2\n\\end{matrix}\\right],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"r(s,a)=\\left[\\begin{matrix}2&amp;2.5&amp;1\\\\&#10;1.1&amp;1.2&amp;0.5\\\\&#10;0.3&amp;2&amp;0.2\\end{matrix}\\right],\" display=\"block\"><mrow><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mn>2</mn></mtd><mtd columnalign=\"center\"><mn>2.5</mn></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>1.1</mn></mtd><mtd columnalign=\"center\"><mn>1.2</mn></mtd><mtd columnalign=\"center\"><mn>0.5</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0.3</mn></mtd><mtd columnalign=\"center\"><mn>2</mn></mtd><mtd columnalign=\"center\"><mn>0.2</mn></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07279.tex", "nexttext": "\nwhere ${\\mathcal{S}}_p$ and ${\\mathcal{S}}_d$ are the regions of the state space corresponding to poor and dangerous tracking, respectively, and $c_p$ and $c_d$ are the respective penalty costs.\nIf the state is not in ${\\mathcal{S}}_l$ or ${\\mathcal{S}}_h$, a tracking reward proportional to $k_r$ is received.\n\nIn our experiments, we set $c_a=\\frac{1}{2A}$, $c_p=1$, $c_d=0.1$, and $k_r=\\frac{2}{S}$.\nFurthermore, we set ${\\mathcal{S}}_l = \\{s\\in{\\mathcal{S}}\\mid s \\leq S/5\\}$ and ${\\mathcal{S}}_h = \\{s\\in{\\mathcal{S}}\\mid s\\geq9S/10\\}$, and $\\gamma=0.99$.\nThe observation accuracy parameter was $q=0.8$.\n\nTable~\\ref{tab:tracking_pruning} shows the minimum, average and maximum percentage of actions that could be pruned by applying the myopic policy bounds as function of the domain size.\nA value of 0\\% would indicate that the bounds were completely non-informative, and a value of 100\\% that the optimal policy is fully determined by the bounds.\nThe results were obtained by evaluating the bounds with 500 randomly sampled reachable belief states in each problem.\nWe note that pruning efficiency tends to decrease both as function of the state space and action space size, but in all cases at least one third pruning rate was achieved even in the worst case.\nFor $S=256$ and $A=64$, a vector satisfying the constraints of Thm.~\\ref{thm:mlr_LP} could not be found.\n\n\\begin{table*}[ht]\n\\centering\n\\caption{The (minimum, average, maximum) percentage of actions pruned in target tracking domains as function of state space size $S$ and action space size $A$.}\n\\label{tab:tracking_pruning}\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\n$S$   & $A=4$ & $A=8$ & $A=16$ & $A=32$ & $A=64$ \\\\ \\midrule\n4   & (75.0, 94.9, 100)    & (75.0, 83.3, 87.5)    & (75.0, 78.7, 87.5)     & (71.9, 74.7, 75.0)     & (64.1, 72.0, 75.0)     \\\\\n8   & (50.0, 75.0, 75)    & (37.5, 59.5, 62.5)    & (50.0, 58.0, 75.0)     & (59.4, 64.9, 71.9)     & (48.4, 57.6, 67.2)    \\\\ \\midrule\n16  & (50.0, 72.7, 75)    & (50, 65.2, 75.0)    & (43.8, 61.3, 68.8)     & (46.9, 55.0, 62.5)     & (46.9, 55.5, 62.5)    \\\\\n32  & (50.0, 73.9, 75)    & (37.5, 58.5, 62.5)    & (37.8, 54.9, 62.5)     & (37.5, 49.9, 59.4)     & (43.8, 52.6, 59.4)     \\\\ \\midrule\n64  & (50.0, 71.9, 75)    & (37.5, 60.4, 62.5)    & (43.8, 54.0, 62.5)     & (37.5, 49.6, 59.4)     & (40.6, 47.9, 53.1)     \\\\\n128 & (50.0, 73.2, 75)    & (50, 61.1, 62.5)    & (43.8, 54.7, 56.3)     & (40.6, 49.3, 53.1)     & (42.2, 48.3, 53.1)     \\\\ \\midrule\n256 & (50.0, 72.8, 75)    & (50, 60.4, 62.5)    & (43.8, 50.9, 56.3)     & (43.8, 49.1, 53.1)     & (n/a)     \\\\\n    \\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\\begin{figure}[thpb]\n\\centering\n\\includegraphics[width=\\columnwidth]{looseness-eps-converted-to}\n\\caption{The average distance between the upper bound and optimal policy as function of state space size $S$ for various action space sizes $A$.}\n\\label{fig:looseness}\n\\end{figure}\n\n\n\nAs we computed the optimal solution, we could examine how close the bounds are to the optimal policy.\nFigure~\\ref{fig:looseness} shows an example of the looseness of the upper bound measured by the average of $\\overline{\\mu}^*_1(b)-\\mu^*(b)$ over the 500 belief states.\nThe greater the difference is, the further away the myopic upper bound policy is from the optimal policy, and the less informative it is.\nWe note that even as the number of states and actions increases, the upper bound remains at a distance of less than 10 actions from the optimal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe examined information acquisition in POMDPs with a reward function nonlinear in the belief state.\nWe showed that if the POMDP fulfills certain structural properties, the optimal infinite horizon stationary policy may be lower and upper bounded by myopic (greedy) lower and upper bound policies.\nBased on the bounds, we designed a branch-and-bound pruning algorithm for online planning in POMDPs, and demonstrated its effectiveness in a target tracking application.\n\nThe main advantage of our approach is that, if the structural properties are satisified in the POMDP model, the resulting branch-and-bound algorithm saves orders of magnitude in computation compared to the existing approaches. \nThe main drawback is that the requred structural properties can be quite restrictive in some applications. \nFor example, a natural order in the state and action spaces is required.\n\nFuture work will focus on relaxing the structural requirements while keeping track of the effect on the policy bounds. \nThis has potential to widen the scope of the possible applications of our apporach significantly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{ref}\n\n\n", "itemtype": "equation", "pos": 42024, "prevtext": "\n$w_a = \\left[\\begin{matrix}1.1 & 1.6 &1\\end{matrix}\\right]$, and $f$ equal to R{\\'e}nyi quadratic entropy.\nThe discount factor was $\\gamma = 0.99$, and the observation accuracy parameter was $q=0.7$.\n\nWe computed the lower and upper myopic policy bounds for the problem.\nThe bounds are visualized on the belief simplex ${\\mathcal{B}}$ in Figure~\\ref{fig:policy_bound_example}.\nThe figure shows on the left and middle the lower and upper myopic policies, and on the right the optimal policy determined when the two bounds agree.\n\nWe defined a larger problem with $S=Z=8$ and $A=3$, and in it compared Algorithm~\\ref{alg:bb} against an exhaustive tree search.\nThere are $N_d = (AZ)^d$ reachable belief states after $d$ decisions, and the number of belief states in a complete search tree is $\\sum_{i=0}^d N_i$.\n\n\\begin{table}[ht]\n\\centering\n\\caption{Average percentage of belief states pruned as function of the search depth $d$ applying the myopic policy bounds.}\n\\label{tab:expansions}\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\n        \t\t\t        & $d=2$ & $d=3$ & $d=4$ & $d=5$ & $d=6$ \\\\ \\midrule\nBelief states pruned & 26.3\\%  & 36.3\\%   & 44.9\\%  & 52.3\\%  & 58.8\\%  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nTable~\\ref{tab:expansions} shows the average percentage of belief state pruned from the search tree for 100 randomly sampled initial belief states.\nComputing the myopic policy bound requires solving a feasibility LP with $(S-1)\\cdot A$ constraints offline, and then during the search computing for each belief encountered the action maximizing the immediate expected transformed reward.\nEmpirically, we found that for $d\\geq 3$ branch-and-bounding was faster than the exhaustive search measured by computation time.\n\n\n\n\\subsection{Comparison with existing approaches}\nNext, we compare the performance of our approach to an optimal incremental pruning algorithm\\footnote{We applied the implementation from the pomdp-solve package of A. Cassandra, see \\url{http://pomdp.org}} in a target tracking scenario with a state space size $S$ between $4$ and $256$, and action space size $A$ between $4$ and $64$. \nSince the incremental pruning algorithm (as well as other existing approaches) can handle only state-dependent rewards, we do not add an uncertainty function to the reward, i.e., $f \\equiv 0$ in~\\eqref{eq:info_reward}.\nThe state-dependent reward $r(s,a)$ is designed to 1) incur a lower reward for investing higher priority in target tracking and to 2) penalize for both tracking the target poorly or too dangerously.\nThe penalty on dangerous tracking models a situation where attempting to track the state too aggressively may make the robot vulnerable to environmental hazards.\nThe reward is set as $r(s,a) = -c_a\\cdot (A-a+1) + t(s,a)$, where $c_a$ is the cost of expending one unit of effort in tracking, and $t(s,a)$ is a tracking performance reward, defined\n\n", "index": 49, "text": "\\begin{equation}\nt(s,a) = \\begin{cases}\n-c_p\\cdot \\frac{1}{s} & \\textnormal{if } s \\in {\\mathcal{S}}_p\\\\\n-c_d\\cdot \\frac{1}{S-s+1} & \\textnormal{if } s \\in {\\mathcal{S}}_d\\\\\nk_r\\cdot s & \\textnormal{otherwise}\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"t(s,a)=\\begin{cases}-c_{p}\\cdot\\frac{1}{s}&amp;\\textnormal{if }s\\in{\\mathcal{S}}_{%&#10;p}\\\\&#10;-c_{d}\\cdot\\frac{1}{S-s+1}&amp;\\textnormal{if }s\\in{\\mathcal{S}}_{d}\\\\&#10;k_{r}\\cdot s&amp;\\textnormal{otherwise}\\end{cases}\" display=\"block\"><mrow><mrow><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>\u22c5</mo><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><mi>s</mi></mfrac></mstyle></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>s</mi></mrow><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>p</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><msub><mi>c</mi><mi>d</mi></msub><mo>\u22c5</mo><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><mrow><mrow><mi>S</mi><mo>-</mo><mi>s</mi></mrow><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>s</mi></mrow><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>d</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>k</mi><mi>r</mi></msub><mo>\u22c5</mo><mi>s</mi></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}]