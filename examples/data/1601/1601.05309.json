[{"file": "1601.05309.tex", "nexttext": "\nwhere the data channel $E^\\prime$ denotes the observed (see below) photon energy\nand $s$ has units of counts\\,s$^{-1}$\\,keV$^{-1}$. The response function\n$R(E^\\prime,E)$ has the dimensions of an (effective) area, and can be given in,\ne.g. m$^2$.\n\nThe variable $E^\\prime$ is denoted as the observed photon energy, but in\npractice it is often some electronic signal in the detector, the strength of\nwhich usually cannot take arbitrary values, but can have only a limited set of\ndiscrete values, for instance as a result of analogue to digital conversion\n(ADC) in the processing electronics. A good example of this is the pulse-height\nchannel for a CCD detector. Alternatively, it may be the pixel number on an\nimaging detector if gratings are used.\n\nIn almost all circumstances it is not possible to carry out the integration in\nEqn.~(\\ref{eqn:matrix_cont}) analytically because of the complexity of both the\nmodel spectrum and instrument response. For that reason, the model spectrum is\nevaluated at a limited set of energies, corresponding to the same predefined set\nof energies that is used for the instrument response $R(E^\\prime,E)$. Then the\nintegration in eqn.~(\\ref{eqn:matrix_cont}) is replaced by a summation. We call\nthis limited set of energies the model energy grid or for short the model grid.\nFor each bin $j$ of this model grid, we define a lower and upper bin boundary\n$E_{1j}$ and $E_{2j}$, a bin centre $E_j=0.5(E_{1j}+E_{2j}),$ and a bin width\n$\\Delta E_j = E_{2j} - E_{1j}$.\n\nProvided that the bin width of the model energy bins is sufficiently small\ncompared to the spectral resolution of the instrument, the summation\napproximation to (\\ref{eqn:matrix_cont}) is in general accurate. The response\nfunction $R(E^\\prime,E)$ has therefore been replaced by a response matrix\n$R_{ij}$, where the first index $i$ denotes the data channel, and the second\nindex $j$ the model energy bin number.\n\nWe have explicitly \n\n\n", "itemtype": "equation", "pos": 16023, "prevtext": "\n\n\\title{Optimal binning of X-ray spectra and response matrix design}\n\n\\author{J.S. Kaastra\\inst{1,2,3}\n\\and\nJ.A.M. Bleeker\\inst{1,3}\n}\n\n\\offprints{J.S. Kaastra}\n\\date{\\today}\n\n\\institute{SRON Netherlands Institute for Space Research, Sorbonnelaan 2,\n           3584 CA Utrecht, the Netherlands \n           \\and\n           Leiden Observatory, Leiden University, PO Box 9513, \n           2300 RA Leiden, the Netherlands\n           \\and\n           Department of Physics and Astronomy, Universiteit Utrecht, \n           P.O. Box 80000, 3508 TA Utrecht, the Netherlands\n         }\n         \n\\abstract\n\n\n{  }\n\n{ A theoretical framework is developed to estimate the optimal binning of X-ray\nspectra.} \n\n{ We derived expressions for the optimal bin size for model spectra as well as\nfor observed data using different levels of sophistication. }\n\n{ It is shown that by taking into account both the number of photons in a given\nspectral model bin and their average energy over the bin size, the number of\nmodel energy bins and the size of the response matrix can be reduced by a factor\nof $10-100$. The response matrix should then contain the response at the bin\ncentre as well as its derivative with respect to the incoming photon energy. We\nprovide practical guidelines for how to construct optimal energy grids as well\nas how to structure the response matrix. A few examples are presented to\nillustrate the present methods.} \n\n{ }\n\n\\keywords{Instrumentation: spectrographs -- Methods: data analysis \n-- X-rays: general }\n\\titlerunning{Optimal binning}\n\n\n\\maketitle\n\n\\section{Introduction}\n\nUntil two decades ago X-ray spectra of cosmic X-ray sources were obtained using\ninstruments such as proportional counters and gas scintillation proportional\ncounters with moderate spectral resolution (typically 5--20~\\%). With the\nintroduction of charge-coupled devices (CCDs) (ASCA, launch 1993) a major leap\nin energy resolution has been achieved (up to 2~\\%) and very high resolution has\nbecome available through grating spectrometers, first on the Extreme Ultraviolet\nExplorer (EUVE), and later on the Chandra and XMM-Newton observatories.\n\nThese X-ray spectra are usually analysed with a forwards folding technique.\nFirst a spectral model appropriate for the observed source is chosen. This model\nis convolved with the instrument response, which is represented usually by a\nresponse matrix. The convolved spectrum is compared to the observed spectrum and\nthe parameters of the model are varied in a fitting procedure in order to obtain\nthe best solution.\n\nThis classical way of analysing X-ray spectra has been widely adopted and is\nimplemented, e.g. in spectral fitting packages such as XSPEC \\citep{arnaud1996},\nSHERPA \\citep{freeman2001}, and SPEX \\citep{kaastra1996}. However, the\napplication of standard concepts, such as a response matrix, is not at all\ntrivial for high-resolution instruments. For example, with the RGS of\nXMM-Newton, the properly binned response matrix is 120 Megabytes in size,\ncounting only non-zero elements. Taking into account that usually data from both\nRGS detectors and of two spectral orders are fit simultaneously, makes it at\nbest slow to handle even by most present day computer systems. Also the higher\nspectral resolution considerably enhances the computation time needed to\nevaluate the spectral models. Since the models applied to Chandra and XMM-Newton\ndata are much more complex than those applied to data from previous missions,\ncomputational efficiency is important to take into consideration.\n\nFor these reasons we discuss here the optimal binning of both model spectra and\ndata. In this paper, we critically re-evaluate the concept of response matrices\nand the way spectra are analysed. In fact, we conclude that it is necessary to\ndrop the classical concept of a matrix, and to use a modified approach instead.\n\nThe outline of this paper is as follows. We start with a more in-depth\ndiscussion regarding the motivation for this work (Sect.~\\ref{sect:motivation}).\nIn the following sections, we discuss the classical approach to spectral\nmodelling and its limitations (Sect.~\\ref{sect:classical}), the optimal bin size\nfor model spectra (Sect.~\\ref{sect:modelbinning}) and data\n(Sect.~\\ref{sect:data}) followed by a practical example\n(Sect.~\\ref{sect:example}). We then turn to the response matrix\n(Sect.~\\ref{sect:matrix}), its practical construction\n(Sect.~\\ref{sect:construct}), and briefly to the proposed file format\n(Sect.~\\ref{sect:formats}) before reaching our conclusions.\n\n\\section{Motivation for this work\\label{sect:motivation}}\n\nIn this section we present in more depth the arguments leading to the proposed\nbinning of model spectra and observational data and our choice for the response\nmatrix design. We do this by addressing the following questions.\n\n\\subsection{Why not use straightforward deconvolution?}\n\nIn high-resolution optical spectra the instrumental broadening is often small\ncompared to the intrinsic line widths. In those cases it is common practice to\nobtain the source spectrum by dividing the observed spectrum at each energy by\nthe nominal effective area (straightforward deconvolution).\n\nAlthough straightforward deconvolution, due to its simplicity would seem to be\nattractive for high-resolution X-ray spectroscopy, it fails in several\nsituations. For example, spectral orders may overlap as with the EUVE\nspectrometers \\citep{welsh1990} or the Chandra \\citep{weisskopf1996} Low-Energy\nTransmission Grating Spectrometer \\citep[LETGS;][]{brinkman2000} when the HRC-S\ndetector is used. In these cases only careful instrument calibration in\ncombination with proper modelling of the short wavelength spectrum can help. In\ncase of the Reflection Grating Spectrometer \\citep[RGS;][]{denherder2001} on\nboard XMM-Newton \\citep{jansen2001} 30\\% of all line flux is contained in broad\nwings as a result of scattering on the mirror and gratings, and this flux is\npractically impossible to recover by straightforward deconvolution. Finally,\nhigh-resolution X-ray spectra often suffer from both severe line blending and\nrelatively large statistical errors because of low numbers of counts in some\nparts of the spectrum. This renders the method unsuitable.\n\n\\subsection{Why are high-resolution spectra much more complex to handle than\nlow-resolution spectra?}\n\nThe enhanced spectral resolution and sensitivity of the current high-resolution\nX-ray spectrometers require much more complex source models with a multitude of\nfree parameters as compared to the crude models that suffice for low-resolution\nspectra to cover all relevant spectral details that can be exposed by the far\nsuperior resolving power of state-of-the-art cosmic X-ray spectrometers. This\ndoes not merely involve adding more lines to the old models with the same number\nof parameters. Whereas investigators first fit single-temperature, solar\nabundance spectra, now multi-temperature, free abundance models are to be\nemployed for stars and clusters of galaxies, among others. Models of AGN have\nevolved from simple power laws with a Gaussian line profile occasionally\nsuperimposed to complex continua spectra, including features due to reflection,\nrelativistic blurring, soft excesses, multiple absorbing photo-ionised outflow\ncomponents, low-ionisation emission contributions originating at large\ndistances, which yet again leads to much larger numbers of free parameters to be\naccounted for.\n\n\\subsection{Why is it not possible to model complex spectra by the sum of a simple continuum plus\ndelta lines?}\n\nPractically all X-ray spectra in the Universe are too complex to simulate with\nsimple continuum shapes with superimposed $\\delta$-functions mimicking line\nfeatures when observed at high resolution. Apart from spectral lines, other\nnarrow features often found are: for example, radiative recombination continua,\nabsorption edges, Compton shoulders, or dust features. Moreover spectral lines\nare too complex to be dealt with by delta functions, i.e. the astrophysics\nderiving from Doppler broadening or natural broadening, or a combination thereof\n(Voigt profiles) remains totally unaccounted for.\n\nEven if one would model a line emission spectrum by the sum of $\\delta$-lines,\nhowever, a physical model connecting the line intensities is needed. The line\nfluxes are mutually not independent and, as a consequence, many weak lines may\nnot be detected individually but added together they may give a detectable\nsignal.\n\nFurthermore, except for the nearest stars, almost all X-ray sources are subject\nto foreground interstellar absorption, also yielding narrow and sharp spectral\nfeatures such as absorption edges or absorption lines (for instance the\nwell-known \\ion{O}{i} 1s--2p transition at 23.5~\\AA).\n\nAs an example, Seyfert 1 galaxies show a range of emission lines with different\nwidths, often superimposed on each other and corresponding to the same\ntransition, from narrow-line region lines (width few 100 km\\,s$^{-1}$),\nintermediate width lines (1000 km\\,s$^{-1}$), broad-line region lines (several\n1000 up to tens of thousands km\\,s$^{-1}$) up to relativistically broadened\nlines. In such cases splitting in narrow- and broadband features is impossible,\nas there is spectral structure at many different (Doppler) scales. As already\nstated above, there is a need for physically relevant models, regrettably,\n$\\delta$-functions do not qualify for this.\n\nIn principle, the user could design an input model energy grid where there is\nonly substantially higher spectral resolution at the energies where the model\nwould predict narrow spectral features, for instance near foreground absorption\nedges or strong sharp emission lines. However, the real source spectrum may\ncontain additional narrow features that are not anticipated by the user, and we\nwant to avoid recreating grids and matrices multiple times. For this reason, the\nbinning scheme proposed in this paper only depends on the properties of the\ninstrument and the observed spectrum in terms of counts per resolution element.\n\n\\subsection{Why not use precalculated model grids?}\n\nSome spectral models are computationally intensive. We are aware of the method\nof precalculating grids of models, such that the spectral fitting proceeds\nfaster. This is common practice with for instance the APEC model for collisional\nplasmas as implemented in XSPEC or XSTAR models for photo-ionised plasmas. For a\nlimited set of parameters this is an acceptable solution. However, as has been\noutlined above in many situations, astrophysical models require substantially\nmore free parameters than can be provided with grids for 2--4 free parameters.\n\nAs an example, for stellar spectra not only temperature but also density and the\nUV radiation field (for He-like triplets) constitute important parameters.\nHowever, the standard APEC implementation only has temperature and abundances as\nfree parameters.\n\nPhoto-ionised, warm absorber models for AGN depend non-linearly on the shape of\nthe ionising spectral energy distribution and on source parameters such as\nabundances and turbulence. In realistic descriptions, this requires at least\nfive or more free parameters and is much too expensive to construct grids. Using\ngrids in such cases always implies less realistic models.\n\nIn addition, creating these grids offers efficiency gain in the fitting\nprocedure but shifts the computational burden to the grid creation. The\ndevelopers of APEC communicated to us that a full grid covering all relevant\ntemperatures requires of the order of a week computation time. A single XSTAR\nmodel (1 spectrum) takes on average about 20 minutes to complete, hence even\nmodest grids may take a week or so to complete. So, also it is extremely useful \nto be able to limit the number of energy bins in those cases.\n\n\\subsection{Why not use brute force computing power?}\n\nWe note that for individual spectra extensive energy grids and large sizes of\nresponse matrices can be handled with present-day computers, but the computation\ntime in folding the response matrix into the spectrum is substantial. This\nholds, in particular, if this process has to be repeated thousands of times in\nspectral fitting and error searches for models with many free parameters.\n\nMore importantly, in several cases users want to fit spectra of time-variable\nsources taken at different epochs together, using spectral models where some\nparameters are fixed between the observations while others are not. This\nrequires loading as many response matrices as number of observations, and we\nhave seen cases where such analyses simply cannot be carried out in this way.\n\nThere are also other cases where investigators have to rely on indirect methods\nsuch as making maps of line centroids or equivalent widths, rather than full\nspectral fitting. A striking example constitutes the Chandra CCD spectra of Cas\nA. With 1 arcsec spatial resolution the remnant covers of order $10^5$\nindependent regions, which should ideally be fit together with a common\nspectral model with position-dependent parameters.\n\n\\subsection{Some explicit examples of spectral analyses that require considerable\ncomputation time}\n\nA few typical examples may help to stipulate the relevance of our case regarding\nrequired computing time.\n\n\\citet{mernier2015} analysed a cluster of galaxies with the XMM-Newton EPIC\ndetectors. These detectors still have a modest spectral resolution. The spectra\nof three detectors in two observations with different background levels were fit\njointly. The model was a multi-temperature plasma model with about 50 free\nparameters, including abundances and different instrumental and astrophysical\nbackground components. Fits including error searches took about 5--10 hours on a\nmodern workstation. Additional time is needed to verify that fits do not get\nstuck in local subminima. These authors are extending the work now to radial\nprofiles in clusters, incorporating eight annuli per cluster, on a sample of 40\nclusters. Several months of cpu time using multiple workstations are needed to\ncomplete this analysis.\n\n\\citet{kaastra2014b} studied the AGN Mrk~509. Chandra HETGS spectra were fit\njointly with archival XMM-Newton spectra. As a result of the amount of\ninformation available by virtue of the high spectral resolution, the spectrum\nwas modelled using an absorption model that comprised the product of 28\ncomponents (four velocity components times seven ionisation components). With\nthe inclusion of free parameters for the modelling of the continuum and the\nnarrow and broad emission features, the model ended up with about 60 free\nparameters. It took two weeks of computing time to obtain the final model\nincluding the assessment of uncertainties on all parameters.\n\n\\citet{kaastra2014a} observed the Seyfert galaxy NGC 5548 in an obscured state.\nStacked RGS, pn, NuSTAR, and INTEGRAL data were analysed jointly. The model\n(table S3 of that paper) had 35 free parameters and consisted of six`warm\nabsorber' components and two `obscured' components, giving a total of eight\nmultiplied transmission factors, which had to be determined iteratively in about\neight steps to reach full convergence. This is because the inner, obscurer\ncomponents affect the ionising spectral energy distribution for the outer, warm\nabsorber components. At each intermediate step a full grid of Cloudy models had\nto be calculated to update the transmittance of the obscured nuclear spectrum.\nThe full computation also took several days in addition to months of trial to\nset up the proper model and calculation scheme.\n\n\\section{Classical approach to spectral modelling and its\nlimitations\\label{sect:classical}}\n\n\\subsection{Response matrices}\n\nThe spectrum of an X-ray source is given by its photon spectrum $f(E)$, a\nfunction of the continuous variable $E$, the photon energy, and has units of,\ne.g. photons\\,m$^{-2}$\\,s$^{-1}$\\,keV$^{-1}$. To produce the predicted count\nspectrum $s(E^\\prime)$ measured by an instrument, $f(E)$ is convolved with the\ninstrument response $R(E^\\prime,E)$ as follows:\n\n", "index": 1, "text": "\\begin{equation}\ns(E^\\prime) = \\int\\limits_0^{\\infty} R(E^\\prime,E)f(E){\\rm d}E,\n\\label{eqn:matrix_cont}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"s(E^{\\prime})=\\int\\limits_{0}^{\\infty}R(E^{\\prime},E)f(E){\\rm d}E,\" display=\"block\"><mrow><mrow><mrow><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>E</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>E</mi><mo>\u2032</mo></msup><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nwhere now $S_i$ is the observed count rate in counts\\,s$^{-1}$ for data channel\n$i$ and $F_j$ is the model spectrum (photons\\,m$^{-2}$\\,s$^{-1}$) for model\nenergy bin $j$.\n\n\\subsection{Evaluation of the model spectrum}\n\nThe model spectrum $F_j$ can be evaluated in two ways. First, the model can be\nevaluated at the bin centre $E_j$, essentially taking\n\n", "itemtype": "equation", "pos": 18072, "prevtext": "\nwhere the data channel $E^\\prime$ denotes the observed (see below) photon energy\nand $s$ has units of counts\\,s$^{-1}$\\,keV$^{-1}$. The response function\n$R(E^\\prime,E)$ has the dimensions of an (effective) area, and can be given in,\ne.g. m$^2$.\n\nThe variable $E^\\prime$ is denoted as the observed photon energy, but in\npractice it is often some electronic signal in the detector, the strength of\nwhich usually cannot take arbitrary values, but can have only a limited set of\ndiscrete values, for instance as a result of analogue to digital conversion\n(ADC) in the processing electronics. A good example of this is the pulse-height\nchannel for a CCD detector. Alternatively, it may be the pixel number on an\nimaging detector if gratings are used.\n\nIn almost all circumstances it is not possible to carry out the integration in\nEqn.~(\\ref{eqn:matrix_cont}) analytically because of the complexity of both the\nmodel spectrum and instrument response. For that reason, the model spectrum is\nevaluated at a limited set of energies, corresponding to the same predefined set\nof energies that is used for the instrument response $R(E^\\prime,E)$. Then the\nintegration in eqn.~(\\ref{eqn:matrix_cont}) is replaced by a summation. We call\nthis limited set of energies the model energy grid or for short the model grid.\nFor each bin $j$ of this model grid, we define a lower and upper bin boundary\n$E_{1j}$ and $E_{2j}$, a bin centre $E_j=0.5(E_{1j}+E_{2j}),$ and a bin width\n$\\Delta E_j = E_{2j} - E_{1j}$.\n\nProvided that the bin width of the model energy bins is sufficiently small\ncompared to the spectral resolution of the instrument, the summation\napproximation to (\\ref{eqn:matrix_cont}) is in general accurate. The response\nfunction $R(E^\\prime,E)$ has therefore been replaced by a response matrix\n$R_{ij}$, where the first index $i$ denotes the data channel, and the second\nindex $j$ the model energy bin number.\n\nWe have explicitly \n\n\n", "index": 3, "text": "\\begin{equation}\nS_i = \\sum\\limits_{j}^{} R_{ij}F_j,\n\\label{eqn:rmatrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"S_{i}=\\sum\\limits_{j}R_{ij}F_{j},\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>F</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nThis is appropriate for smooth continuum models such as blackbody radiation and\npower laws. For line-like emission, it is more appropriate to integrate the line\nflux within the bin analytically, taking\n\n", "itemtype": "equation", "pos": 18514, "prevtext": "\n\nwhere now $S_i$ is the observed count rate in counts\\,s$^{-1}$ for data channel\n$i$ and $F_j$ is the model spectrum (photons\\,m$^{-2}$\\,s$^{-1}$) for model\nenergy bin $j$.\n\n\\subsection{Evaluation of the model spectrum}\n\nThe model spectrum $F_j$ can be evaluated in two ways. First, the model can be\nevaluated at the bin centre $E_j$, essentially taking\n\n", "index": 5, "text": "\\begin{equation}\nF_j = f(E_j)\\Delta E_j.\n\\label{eqn:Fcentral}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"F_{j}=f(E_{j})\\Delta E_{j}.\" display=\"block\"><mrow><mrow><msub><mi>F</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>E</mi><mi>j</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nNow a serious flaw occurs in most spectral analysis codes. The parameter $S_i$\nis evaluated in a straightforward way using (\\ref{eqn:rmatrix}). Hereby it is\ntacitly assumed that all photons in the model bin $j$ have exactly the energy\n$E_j$. This is necessary since all information on the energy distribution within\nthe bin is lost and $F_j$ is essentially only the total number of photons in the\nbin. If the model bin width $\\Delta E_j$ is sufficiently small this is no\nproblem, however, this is (most often) not the case.\n\nAn example of this is the standard response matrix for the ASCA SIS detector\n\\citep{tanaka1994}, in which the investigators used a uniform model grid with a\nbin size of 10\\,eV. At a photon energy of 1\\,keV, the spectral resolution (full\nwidth at half maximum; FWHM) of the instrument was about 50\\,eV, hence the line\ncentroid of an isolated narrow line feature containing $N$ counts can be\ndetermined with a statistical uncertainty of 50/(2.35$\\sqrt{N})$\\,eV. We assume\nhere for simplicity a Gaussian instrument response (FWHM is approximately\n2.35$\\sigma$, see Eq.~\\ref{eqn:fwhm_gauss})). Thus, for a line with 400 counts\nthe line centroid can be determined with an accuracy of 1\\,eV, ten times better\nthan the bin size of the model grid. If the true line centroid is close to the\nboundary of the energy bin, there is a mismatch (shift) of 5\\,eV between the\nobserved count spectrum and the predicted count spectrum at about the $5\\sigma$\nsignificance level. If there are more of these lines in the spectrum, it is\npossible that a satisfactory fit (e.g. acceptable $\\chi^2$ value) is never\nobtained, even in cases where the true source spectrum is known and the\ninstrument is perfectly calibrated. The problem becomes even more worrisome if,\nfor example detailed line centroiding is performed to derive velocity fields.\n\nA simple way to resolve these problems is just to increase the number of model\nbins. This robust method always works, but at the expense of a lot of computing\ntime. For CCD-resolution spectra this is perhaps not a problem, but with the\nincreased spectral resolution and sensitivity of the grating spectrometers of\nChandra and XMM-Newton this becomes cumbersome.\n\nFor example, the LETGS spectrometer of Chandra \\citep{brinkman2000} has a\nspectral resolution (FWHM) between 0.040--0.076~\\AA\\ over the 1--175~\\AA\\ band.\nA 85~ks observation of Capella \\citep[obsid.~1248,][]{mewe2001,ness2001}\nproduced $N=$14\\,000 counts in the \\ion{Fe}{xvii} line at 15~\\AA. Because line\ncentroids can be determined with a statistical accuracy of $\\sigma/\\sqrt{N}$,\nthis makes it necessary to have a model energy grid bin size $\\sigma/\\sqrt{N}$\nof about 0.00014~\\AA, corresponding to 278 bins per FWHM resolution element and\nrequiring 1.2 million model bins up to 175~\\AA\\ for a uniform wavelength grid.\nAlthough this small bin size is less than the (thermal) width of the line,\nlarger model bin sizes would lead to a significant shift of the observed line\nwith respect to the predicted line profile and a corresponding significant\nworsening of the goodness of fit. With future, more sensitive instruments like\nAthena \\citep{nandra2013} such concerns will become even more frequent.\n\nMost of the computing time in thermal plasma models stems from the evaluation of\nthe continuum. The radiative recombination continuum has to be evaluated for all\nenergies, for all relevant ions and for all relevant electronic subshells of\neach ion. On the other hand, the line power needs to be evaluated only once for\neach line, regardless of the number of energy bins. Therefore the computing time\nis approximately proportional to the number of bins. Therefore, a factor of\n1\\,000 increase in computing time is implied from the used number of ASCA-SIS\nbins (1180) to the required number of bins for the LETGS Capella spectrum (1.2\nmillion bins). \n\nFurthermore, because of the higher spectral resolution of grating spectrometers\ncompared with CCD detectors, more complex spectral models are needed to explain\nthe observed spectra, with more free parameters, which also leads to additional\ncomputing time. Finally, the response matrices of instruments like the\nXMM-Newton RGS become extremely large owing to extended scattering wings caused\nby the gratings.\n\nIt is therefore important to keep the number of energy bins as small as possible\nwhile maintaining the required accuracy for proper line centroiding. Fortunately\nthere is a more sophisticated way to evaluate the spectrum and convolve it with\nthe instrument response. Basically, if more information on the distribution of\nphotons within a bin is taken into account (like their average energy), energy\ngrids with broader bins (and hence with substantially fewer bins) can give\nresults as accurate as fine grids where all photons are assumed to be at the bin\ncenter. \n\n\\section{What is the optimal bin size for model\nspectra?\\label{sect:modelbinning}}\n\n\\subsection{Definition of the problem}\n\nIn the example of the LETGS spectrum of Capella we showed that to maintain full\naccuracy for the strong and narrow emission lines, very small bin sizes are\nrequired, which leads to more than a million grid points for the model\nspectrum. \n\nFortunately, there are several ways to improve the situation. Firstly, the\nspectral resolution of most instruments is not constant, and one might adapt the\nbinning to the local resolution. Some X-ray instruments use this procedure. It\nhelps, but the improvement is not very good for instruments like LETGS with a\ndifference of only a factor of two in resolution from short to long wavelengths.\n\nSecondly, the finest binning is needed near features with large numbers of\ncounts, for instance the strong spectral lines of Capella. One might therefore\nadjust the binning according to the properties of the spectrum: narrow bins near\nhigh-count regions and broader bins near low-count regions. As far as we know,\nno X-ray mission utilises this procedure. Perhaps the main reason is that one\nfirst needs to know the observed count spectrum before being able to create the\nmodel energy grid for the response matrix. In most cases, the redistribution\nmatrix is either precalculated for all spectra, or is calculated on the fly for\nindividual spectra only to account for time-dependent instrumental features.\n\nA third way to reduce the number of bins is to revisit the way model spectra are\ncalculated. As indicated before, in most standard procedures, the integrated\nnumber of photons in a bin is calculated. In the convolution with the response\nmatrix it is then tacitly assumed that all photons of the bin have the same\nenergy, i.e. the energy $E_j$ of the bin centre. However, it could well be that\nthe bin contains only one spectral line; it makes a difference if the line is at\nthe lower or upper limit of the energy bin or at the bin centre. The proper\ncentroid of the line in the observed count spectrum, after convolution with the\nresponse matrix, is only reproduced if in the model spectrum not only the number\nof photons but also their average energy is accounted for. As we show (see\nFig.~\\ref{fig:plotres}), accounting for the average energy of the photons within\na bin allows us to have an order of magnitude larger bin sizes.\n\nIn the procedure proposed here we combine all three options to reduce the number\nof model energy bins: making use of the local spectral resolution, the strength\nof the spectral features in number of photons $N,$ and accounting for the\naverage energies of the photons within bins.\n\n\\subsection{Binning the model spectrum}\n\n\\subsubsection{Zeroth order approximation}\n\nWe have seen in the previous subsection that the classical way to evaluate model\nspectra is to calculate the total number of photons in each model energy bin,\nand to act as if all flux is at the centre of the bin. In practice, this implies\nthat the true spectrum $f_0(E)$ within the bin is replaced by its zeroth order\napproximation, $f_{1,0}(E)$, and is written as\n\n\n", "itemtype": "equation", "pos": 18793, "prevtext": "\nThis is appropriate for smooth continuum models such as blackbody radiation and\npower laws. For line-like emission, it is more appropriate to integrate the line\nflux within the bin analytically, taking\n\n", "index": 7, "text": "\\begin{equation}\nF_j = \\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}\nf(E){\\rm d}E.\n\\label{eqn:Fintegral}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"F_{j}=\\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}f(E){\\rm d}E.\" display=\"block\"><mrow><mrow><msub><mi>F</mi><mi>j</mi></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">1</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">2</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nwhere $N$ is the total number of photons in the bin, $E_j$ the bin centre as\ndefined in the previous section, and $\\delta$ is the Dirac delta-function.\n\nWe assume, for simplicity, that the instrument response is Gaussian, centred at\nthe true photon energy and with a standard deviation $\\sigma$. Many instruments\nhave line spread functions (lsf) with Gaussian cores. For instrument responses\nwith extended wings (e.g. a Lorentz profile) the model binning is a less\nimportant problem, since in the wings all spectral details are washed out, and\nonly the lsf core is important. For a Gaussian profile, the FWHM of the lsf is\nwritten as\n\n\n", "itemtype": "equation", "pos": 26893, "prevtext": "\nNow a serious flaw occurs in most spectral analysis codes. The parameter $S_i$\nis evaluated in a straightforward way using (\\ref{eqn:rmatrix}). Hereby it is\ntacitly assumed that all photons in the model bin $j$ have exactly the energy\n$E_j$. This is necessary since all information on the energy distribution within\nthe bin is lost and $F_j$ is essentially only the total number of photons in the\nbin. If the model bin width $\\Delta E_j$ is sufficiently small this is no\nproblem, however, this is (most often) not the case.\n\nAn example of this is the standard response matrix for the ASCA SIS detector\n\\citep{tanaka1994}, in which the investigators used a uniform model grid with a\nbin size of 10\\,eV. At a photon energy of 1\\,keV, the spectral resolution (full\nwidth at half maximum; FWHM) of the instrument was about 50\\,eV, hence the line\ncentroid of an isolated narrow line feature containing $N$ counts can be\ndetermined with a statistical uncertainty of 50/(2.35$\\sqrt{N})$\\,eV. We assume\nhere for simplicity a Gaussian instrument response (FWHM is approximately\n2.35$\\sigma$, see Eq.~\\ref{eqn:fwhm_gauss})). Thus, for a line with 400 counts\nthe line centroid can be determined with an accuracy of 1\\,eV, ten times better\nthan the bin size of the model grid. If the true line centroid is close to the\nboundary of the energy bin, there is a mismatch (shift) of 5\\,eV between the\nobserved count spectrum and the predicted count spectrum at about the $5\\sigma$\nsignificance level. If there are more of these lines in the spectrum, it is\npossible that a satisfactory fit (e.g. acceptable $\\chi^2$ value) is never\nobtained, even in cases where the true source spectrum is known and the\ninstrument is perfectly calibrated. The problem becomes even more worrisome if,\nfor example detailed line centroiding is performed to derive velocity fields.\n\nA simple way to resolve these problems is just to increase the number of model\nbins. This robust method always works, but at the expense of a lot of computing\ntime. For CCD-resolution spectra this is perhaps not a problem, but with the\nincreased spectral resolution and sensitivity of the grating spectrometers of\nChandra and XMM-Newton this becomes cumbersome.\n\nFor example, the LETGS spectrometer of Chandra \\citep{brinkman2000} has a\nspectral resolution (FWHM) between 0.040--0.076~\\AA\\ over the 1--175~\\AA\\ band.\nA 85~ks observation of Capella \\citep[obsid.~1248,][]{mewe2001,ness2001}\nproduced $N=$14\\,000 counts in the \\ion{Fe}{xvii} line at 15~\\AA. Because line\ncentroids can be determined with a statistical accuracy of $\\sigma/\\sqrt{N}$,\nthis makes it necessary to have a model energy grid bin size $\\sigma/\\sqrt{N}$\nof about 0.00014~\\AA, corresponding to 278 bins per FWHM resolution element and\nrequiring 1.2 million model bins up to 175~\\AA\\ for a uniform wavelength grid.\nAlthough this small bin size is less than the (thermal) width of the line,\nlarger model bin sizes would lead to a significant shift of the observed line\nwith respect to the predicted line profile and a corresponding significant\nworsening of the goodness of fit. With future, more sensitive instruments like\nAthena \\citep{nandra2013} such concerns will become even more frequent.\n\nMost of the computing time in thermal plasma models stems from the evaluation of\nthe continuum. The radiative recombination continuum has to be evaluated for all\nenergies, for all relevant ions and for all relevant electronic subshells of\neach ion. On the other hand, the line power needs to be evaluated only once for\neach line, regardless of the number of energy bins. Therefore the computing time\nis approximately proportional to the number of bins. Therefore, a factor of\n1\\,000 increase in computing time is implied from the used number of ASCA-SIS\nbins (1180) to the required number of bins for the LETGS Capella spectrum (1.2\nmillion bins). \n\nFurthermore, because of the higher spectral resolution of grating spectrometers\ncompared with CCD detectors, more complex spectral models are needed to explain\nthe observed spectra, with more free parameters, which also leads to additional\ncomputing time. Finally, the response matrices of instruments like the\nXMM-Newton RGS become extremely large owing to extended scattering wings caused\nby the gratings.\n\nIt is therefore important to keep the number of energy bins as small as possible\nwhile maintaining the required accuracy for proper line centroiding. Fortunately\nthere is a more sophisticated way to evaluate the spectrum and convolve it with\nthe instrument response. Basically, if more information on the distribution of\nphotons within a bin is taken into account (like their average energy), energy\ngrids with broader bins (and hence with substantially fewer bins) can give\nresults as accurate as fine grids where all photons are assumed to be at the bin\ncenter. \n\n\\section{What is the optimal bin size for model\nspectra?\\label{sect:modelbinning}}\n\n\\subsection{Definition of the problem}\n\nIn the example of the LETGS spectrum of Capella we showed that to maintain full\naccuracy for the strong and narrow emission lines, very small bin sizes are\nrequired, which leads to more than a million grid points for the model\nspectrum. \n\nFortunately, there are several ways to improve the situation. Firstly, the\nspectral resolution of most instruments is not constant, and one might adapt the\nbinning to the local resolution. Some X-ray instruments use this procedure. It\nhelps, but the improvement is not very good for instruments like LETGS with a\ndifference of only a factor of two in resolution from short to long wavelengths.\n\nSecondly, the finest binning is needed near features with large numbers of\ncounts, for instance the strong spectral lines of Capella. One might therefore\nadjust the binning according to the properties of the spectrum: narrow bins near\nhigh-count regions and broader bins near low-count regions. As far as we know,\nno X-ray mission utilises this procedure. Perhaps the main reason is that one\nfirst needs to know the observed count spectrum before being able to create the\nmodel energy grid for the response matrix. In most cases, the redistribution\nmatrix is either precalculated for all spectra, or is calculated on the fly for\nindividual spectra only to account for time-dependent instrumental features.\n\nA third way to reduce the number of bins is to revisit the way model spectra are\ncalculated. As indicated before, in most standard procedures, the integrated\nnumber of photons in a bin is calculated. In the convolution with the response\nmatrix it is then tacitly assumed that all photons of the bin have the same\nenergy, i.e. the energy $E_j$ of the bin centre. However, it could well be that\nthe bin contains only one spectral line; it makes a difference if the line is at\nthe lower or upper limit of the energy bin or at the bin centre. The proper\ncentroid of the line in the observed count spectrum, after convolution with the\nresponse matrix, is only reproduced if in the model spectrum not only the number\nof photons but also their average energy is accounted for. As we show (see\nFig.~\\ref{fig:plotres}), accounting for the average energy of the photons within\na bin allows us to have an order of magnitude larger bin sizes.\n\nIn the procedure proposed here we combine all three options to reduce the number\nof model energy bins: making use of the local spectral resolution, the strength\nof the spectral features in number of photons $N,$ and accounting for the\naverage energies of the photons within bins.\n\n\\subsection{Binning the model spectrum}\n\n\\subsubsection{Zeroth order approximation}\n\nWe have seen in the previous subsection that the classical way to evaluate model\nspectra is to calculate the total number of photons in each model energy bin,\nand to act as if all flux is at the centre of the bin. In practice, this implies\nthat the true spectrum $f_0(E)$ within the bin is replaced by its zeroth order\napproximation, $f_{1,0}(E)$, and is written as\n\n\n", "index": 9, "text": "\\begin{equation}\nf_{1,0}(E) = N\\delta(E-E_j),\n\\label{eqn:f0}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"f_{1,0}(E)=N\\delta(E-E_{j}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>-</mo><msub><mi>E</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nHow can we measure the error introduced with approximation (\\ref{eqn:f0})? We\ncompare the cumulative probability distribution functions (cdf) of the true\nspectrum and the approximation (\\ref{eqn:f0}), which are both convolved with the\ninstrumental lsf. The approach is described in detail in\nSect.~\\ref{sect:testing}, using a Kolmogorov-Smirnov test we derive the model\nbin size $\\Delta E$ for which in 97.5\\% of all cases the approximation\n(\\ref{eqn:f0}) leads to the same conclusion as a test using the exact\ndistribution $f_0$ in tests of $f_0$ versus any alternative model $f_2$.\n\nThe approximation eqn.~(\\ref{eqn:f0}) fails most seriously in the case that the\ntrue spectrum within the bin is also a $\\delta$-function, but located at the bin\nboundary, at a distance $\\Delta/2$ from the assumed line position at the bin\ncentre.\n\nThe maximum deviation $\\delta_k$ of the absolute difference of both cumulative\ndistribution functions, $\\delta_k = \\vert F_0(x)-F_{1,0}(x) \\vert$ (see also\neqn.~\\ref{eqn:lambda_k}) occurs where $f_0(x)=f_{1,0}(x)$, as outlined at the\nend of Sect.~\\ref{sect:anap}. Because $f_{1,0}(x) = f_0(x-\\Delta/2)$, we find\nthat the maximum occurs at $x=\\Delta /4$. After some algebra we find that in\nthis case \n\n", "itemtype": "equation", "pos": 27605, "prevtext": "\n\nwhere $N$ is the total number of photons in the bin, $E_j$ the bin centre as\ndefined in the previous section, and $\\delta$ is the Dirac delta-function.\n\nWe assume, for simplicity, that the instrument response is Gaussian, centred at\nthe true photon energy and with a standard deviation $\\sigma$. Many instruments\nhave line spread functions (lsf) with Gaussian cores. For instrument responses\nwith extended wings (e.g. a Lorentz profile) the model binning is a less\nimportant problem, since in the wings all spectral details are washed out, and\nonly the lsf core is important. For a Gaussian profile, the FWHM of the lsf is\nwritten as\n\n\n", "index": 11, "text": "\\begin{equation}\n{\\rm FWHM}\\ = \\sqrt{\\ln(256)}\\sigma \\simeq 2.35\\sigma.\n\\label{eqn:fwhm_gauss}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\rm FWHM}\\ =\\sqrt{\\ln(256)}\\sigma\\simeq 2.35\\sigma.\" display=\"block\"><mrow><mrow><mpadded width=\"+5pt\"><mi>FWHM</mi></mpadded><mo>=</mo><mrow><msqrt><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>256</mn><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt><mo>\u2062</mo><mi>\u03c3</mi></mrow><mo>\u2243</mo><mrow><mn>2.35</mn><mo>\u2062</mo><mi>\u03c3</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": " \nwhere $P$ is the cumulative normal distribution. This approximation holds in the\nlimit of $\\Delta \\ll \\sigma$. Inserting (\\ref{eqn:fwhm_gauss}) we find that the\nbin size should be smaller than \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nHow can we measure the error introduced with approximation (\\ref{eqn:f0})? We\ncompare the cumulative probability distribution functions (cdf) of the true\nspectrum and the approximation (\\ref{eqn:f0}), which are both convolved with the\ninstrumental lsf. The approach is described in detail in\nSect.~\\ref{sect:testing}, using a Kolmogorov-Smirnov test we derive the model\nbin size $\\Delta E$ for which in 97.5\\% of all cases the approximation\n(\\ref{eqn:f0}) leads to the same conclusion as a test using the exact\ndistribution $f_0$ in tests of $f_0$ versus any alternative model $f_2$.\n\nThe approximation eqn.~(\\ref{eqn:f0}) fails most seriously in the case that the\ntrue spectrum within the bin is also a $\\delta$-function, but located at the bin\nboundary, at a distance $\\Delta/2$ from the assumed line position at the bin\ncentre.\n\nThe maximum deviation $\\delta_k$ of the absolute difference of both cumulative\ndistribution functions, $\\delta_k = \\vert F_0(x)-F_{1,0}(x) \\vert$ (see also\neqn.~\\ref{eqn:lambda_k}) occurs where $f_0(x)=f_{1,0}(x)$, as outlined at the\nend of Sect.~\\ref{sect:anap}. Because $f_{1,0}(x) = f_0(x-\\Delta/2)$, we find\nthat the maximum occurs at $x=\\Delta /4$. After some algebra we find that in\nthis case \n\n", "index": 13, "text": "\\begin{equation} \n\\delta_k  = \\delta_{k,0} = P(\\Delta/4)-P(-\\Delta/4) =\n2P(\\Delta/4) - 1  = \\frac{\\Delta}{2\\sqrt{2\\pi}\\sigma}, \\label{eqn:d0}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\delta_{k}=\\delta_{k,0}=P(\\Delta/4)-P(-\\Delta/4)=2P(\\Delta/4)-1=\\frac{\\Delta}{%&#10;2\\sqrt{2\\pi}\\sigma},\" display=\"block\"><mrow><mrow><msub><mi>\u03b4</mi><mi>k</mi></msub><mo>=</mo><msub><mi>\u03b4</mi><mrow><mi>k</mi><mo>,</mo><mn>0</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>/</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>/</mo><mn>4</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>/</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow><mo>=</mo><mfrac><mi mathvariant=\"normal\">\u0394</mi><mrow><mn>2</mn><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt><mo>\u2062</mo><mi>\u03c3</mi></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere the number $\\lambda_k$ is defined by (\\ref{eqn:epsilon_lambda}). Monte\nCarlo results are presented in Sect.~\\ref{sect:montecarlo}.\n\n\\subsubsection{First order approximation}\n\nA further refinement can be reached as follows. Instead of putting all photons\nat the bin centre, we can put them at their average energy. This first-order\napproximation can be written as\n\n", "itemtype": "equation", "pos": 29300, "prevtext": " \nwhere $P$ is the cumulative normal distribution. This approximation holds in the\nlimit of $\\Delta \\ll \\sigma$. Inserting (\\ref{eqn:fwhm_gauss}) we find that the\nbin size should be smaller than \n\n", "index": 15, "text": "\\begin{equation} \\frac{\\Delta}{{\\rm FWHM}} <2.1289\\ \\lambda_k N^{-0.5}, \n\\label{eqn:de0} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{{\\rm FWHM}}&lt;2.1289\\ \\lambda_{k}N^{-0.5},\" display=\"block\"><mrow><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>&lt;</mo><mrow><mpadded width=\"+5pt\"><mn>2.1289</mn></mpadded><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>k</mi></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mn>0.5</mn></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $E_a$ is given by\n\n", "itemtype": "equation", "pos": 29774, "prevtext": "\nwhere the number $\\lambda_k$ is defined by (\\ref{eqn:epsilon_lambda}). Monte\nCarlo results are presented in Sect.~\\ref{sect:montecarlo}.\n\n\\subsubsection{First order approximation}\n\nA further refinement can be reached as follows. Instead of putting all photons\nat the bin centre, we can put them at their average energy. This first-order\napproximation can be written as\n\n", "index": 17, "text": "\\begin{equation}\nf_{1,1}(E) = N\\delta(E-E_a),\n\\label{eqn:f1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"f_{1,1}(E)=N\\delta(E-E_{a}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>-</mo><msub><mi>E</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nFor the worst case zeroth order approximation $f_{1,0}$ for $f_0$, namely the\ncase that $f_0$ is a narrow line at the bin boundary, the approximation\n$f_{1,1}$ yields exact results. Thus, it constitutes a major improvement. In\nfact, it is easy to see that the worst case for $f_{1,1}$ is a situation where\n$f_0$ consists of two $\\delta$-lines of equal strength: one at each bin\nboundary. In that case, the width of the resulting count spectrum is broader\nthan $\\sigma$. Again in the limit of small $\\Delta$, it is easy to show that the\nmaximum error $\\delta_{k,1}$ for $f_{1,1}$ to be used in the Kolmogorov-Smirnov\ntest is written as\n\n", "itemtype": "equation", "pos": 29874, "prevtext": "\nwhere $E_a$ is given by\n\n", "index": 19, "text": "\\begin{equation}\nE_a = \\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}\nf(E)E{\\rm d}E.\n\\label{eqn:eq}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"E_{a}=\\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}f(E)E{\\rm d}E.\" display=\"block\"><mrow><mrow><msub><mi>E</mi><mi>a</mi></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">1</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">2</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $e$ is the base of the natural logarithms. Accordingly, the limiting bin\nsize for $f_{1,1}$ is expressed as\n\n", "itemtype": "equation", "pos": 30638, "prevtext": "\n\nFor the worst case zeroth order approximation $f_{1,0}$ for $f_0$, namely the\ncase that $f_0$ is a narrow line at the bin boundary, the approximation\n$f_{1,1}$ yields exact results. Thus, it constitutes a major improvement. In\nfact, it is easy to see that the worst case for $f_{1,1}$ is a situation where\n$f_0$ consists of two $\\delta$-lines of equal strength: one at each bin\nboundary. In that case, the width of the resulting count spectrum is broader\nthan $\\sigma$. Again in the limit of small $\\Delta$, it is easy to show that the\nmaximum error $\\delta_{k,1}$ for $f_{1,1}$ to be used in the Kolmogorov-Smirnov\ntest is written as\n\n", "index": 21, "text": "\\begin{equation}\n\\delta_{k,1} = \\frac{1}{8\\sqrt{2\\pi e}} (\\frac{\\Delta}{\\sigma})^2,\n\\label{eqn:d1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\delta_{k,1}=\\frac{1}{8\\sqrt{2\\pi e}}(\\frac{\\Delta}{\\sigma})^{2},\" display=\"block\"><mrow><mrow><msub><mi>\u03b4</mi><mrow><mi>k</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>8</mn><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi><mo>\u2062</mo><mi>e</mi></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>\u03c3</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nIt is seen immediately that for large $N$ the approximation $f_{1,1}$ requires a\nsignificantly smaller number of bins than $f_{1,0}$.\n\n\\subsubsection{Second order approximation}\n\nWe can decrease the number of bins further by not only calculating the average\nenergy of the photons in the bin (the first moment of the photon distribution),\nbut also its variance (the second moment). In this case we approximate\n\n", "itemtype": "equation", "pos": 30866, "prevtext": "\nwhere $e$ is the base of the natural logarithms. Accordingly, the limiting bin\nsize for $f_{1,1}$ is expressed as\n\n", "index": 23, "text": "\\begin{equation}\n\\frac{\\Delta_1}{{\\rm FWHM}} < 2.4418\\ \\lambda(R)^{0.5} N^{-0.25}.\n\\label{eqn:de1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta_{1}}{{\\rm FWHM}}&lt;2.4418\\ \\lambda(R)^{0.5}N^{-0.25}.\" display=\"block\"><mrow><mrow><mfrac><msub><mi mathvariant=\"normal\">\u0394</mi><mn>1</mn></msub><mi>FWHM</mi></mfrac><mo>&lt;</mo><mrow><mpadded width=\"+5pt\"><mn>2.4418</mn></mpadded><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow><mn>0.5</mn></msup><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mn>0.25</mn></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $\\tau$ is given by\n\n", "itemtype": "equation", "pos": 31390, "prevtext": "\n\nIt is seen immediately that for large $N$ the approximation $f_{1,1}$ requires a\nsignificantly smaller number of bins than $f_{1,0}$.\n\n\\subsubsection{Second order approximation}\n\nWe can decrease the number of bins further by not only calculating the average\nenergy of the photons in the bin (the first moment of the photon distribution),\nbut also its variance (the second moment). In this case we approximate\n\n", "index": 25, "text": "\\begin{equation}\nf_{1,2}(E) = N\\exp[(E-E_a)/2\\tau^2)],\n\\label{eqn:f2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"f_{1,2}(E)=N\\exp[(E-E_{a})/2\\tau^{2})],\" display=\"block\"><mrow><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>N</mi><mi>exp</mi><mrow><mo stretchy=\"false\">[</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo>-</mo><msub><mi>E</mi><mi>a</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn><msup><mi>\u03c4</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nThe resulting count spectrum is then simply Gaussian with the average value\ncentred at $E_a$ and the width slightly larger than the instrumental width\n$\\sigma$, namely $\\sqrt{\\sigma^2 + \\tau^2}$.\n\nThe worst case again occurs for two $\\delta$-lines at the opposite bin\nboundaries, but now with unequal strength. It can be shown in the small bin\nwidth limit that\n\n", "itemtype": "equation", "pos": 31500, "prevtext": "\nwhere $\\tau$ is given by\n\n", "index": 27, "text": "\\begin{equation}\n\\tau^2 = \\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}\nf(E)(E-E_a)^2{\\rm d}E\n\\label{eqn:tau}\n.\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\tau^{2}=\\int\\limits_{\\displaystyle{E_{1j}}}^{\\displaystyle{E_{2j}}}f(E)(E-E_{%&#10;a})^{2}{\\rm d}E.\" display=\"block\"><mrow><mrow><msup><mi>\u03c4</mi><mn>2</mn></msup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">1</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub><msub><mi mathsize=\"142%\">E</mi><mrow><mn mathsize=\"140%\">2</mn><mo>\u2062</mo><mi mathsize=\"140%\">j</mi></mrow></msub></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>-</mo><msub><mi>E</mi><mi>a</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nand that this maximum occurs for a line ratio of $3+\\sqrt{3}:3-\\sqrt{3}$. The\nlimiting bin size for $f_{1,2}$ is written as\n\n", "itemtype": "equation", "pos": 32002, "prevtext": "\n\nThe resulting count spectrum is then simply Gaussian with the average value\ncentred at $E_a$ and the width slightly larger than the instrumental width\n$\\sigma$, namely $\\sqrt{\\sigma^2 + \\tau^2}$.\n\nThe worst case again occurs for two $\\delta$-lines at the opposite bin\nboundaries, but now with unequal strength. It can be shown in the small bin\nwidth limit that\n\n", "index": 29, "text": "\\begin{equation}\n\\delta_{1,2} = \\frac{1}{36\\sqrt{6\\pi}} (\\frac{\\Delta}{\\sigma})^3\n\\label{eqn:d2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\delta_{1,2}=\\frac{1}{36\\sqrt{6\\pi}}(\\frac{\\Delta}{\\sigma})^{3}\" display=\"block\"><mrow><msub><mi>\u03b4</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>36</mn><mo>\u2062</mo><msqrt><mrow><mn>6</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>\u03c3</mi></mfrac><mo stretchy=\"false\">)</mo></mrow><mn>3</mn></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\n\\subsection{Monte Carlo results\\label{sect:montecarlo}}\n\nIn addition to the analytical approximations described above, we have performed\nMonte Carlo calculations as outlined in Sect.~\\ref{sect:mccalculations}. For our\napproximations of order 0, 1, and 2, corresponding to accounting for the number\nof photons only, both the number of photons and centroid and the number of\nphotons, centroid, and dispersion, respectively, we found the following\napproximations:\n\n", "itemtype": "equation", "pos": 32238, "prevtext": "\nand that this maximum occurs for a line ratio of $3+\\sqrt{3}:3-\\sqrt{3}$. The\nlimiting bin size for $f_{1,2}$ is written as\n\n", "index": 31, "text": "\\begin{equation}\n\\frac{\\Delta_2}{{\\rm FWHM}} < 2.2875\\ \\lambda(R)^{1/3} N^{-1/6}.\n\\label{eqn:de2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta_{2}}{{\\rm FWHM}}&lt;2.2875\\ \\lambda(R)^{1/3}N^{-1/6}.\" display=\"block\"><mrow><mrow><mfrac><msub><mi mathvariant=\"normal\">\u0394</mi><mn>2</mn></msub><mi>FWHM</mi></mfrac><mo>&lt;</mo><mrow><mpadded width=\"+5pt\"><mn>2.2875</mn></mpadded><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>6</mn></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwith\n\\begin{eqnarray}\n\\mbox{order 0}\\ &:&\\ y=\\frac{0.5707}{x^{1/2}}\\Bigl( 1 + \\frac{1.0}{x} \\Bigr),\n\\label{eqn:app0}\\\\\n\\mbox{order 1}\\ &:&\\ y=\\frac{1.404}{x^{1/4}}\\Bigl( 1 + \\frac{18}{x} \\Bigr), \n\\label{eqn:app1}\\\\\n\\mbox{order 2}\\ &:&\\ y=\\frac{1.569}{x^{1/6}}\\Bigl( 1 + \\frac{1.14}{x^{1/3}}\n\\Bigr),\\label{eqn:app2}\n\\end{eqnarray}\nwhere\\begin{eqnarray}\n\\mbox{order 0}\\ &:&\\ x = N_r(1+0.3\\ln R), \\label{eqn:appr0}\\\\\n\\mbox{order 1}\\ &:&\\ x = N_r(1+0.1\\ln R), \\label{eqn:appr1}\\\\\n\\mbox{order 2}\\ &:&\\ x = N_r(1+0.6\\ln R). \\label{eqn:appr2}\n\\end{eqnarray}\nHere $N_r$ is the number of photons within a resolution element and $R$ the\nnumber of resolution elements. The precise upper cut-off value of 1.0 is a\nlittle arbitrary, but we adopted it here for simplicity as the same as for our\napproximation of the data grid binning (see Sect.~\\ref{sect:data}).\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{plotres.ps}}\n\\caption{Optimal bin size $\\Delta$ for model spectrum binning with a Gaussian\nlsf, as a function of the number of counts per resolution element $N_r$.\nDash-dotted line: zeroth order approximation (\\ref{eqn:app0}); thick solid line:\nfirst order approximation (\\ref{eqn:app1}); dashed line: second order\napproximation (\\ref{eqn:app2}). For comparison we also show as the dotted curve\nthe approximation (\\ref{eqn:databin}) for the data grid binning. All results\nshown are for $R=1$.}\n\\label{fig:plotres}\n\\end{figure}\n\nWe show these approximations in Fig.~\\ref{fig:plotres}. The asymptotic behaviour\nis as described by (\\ref{eqn:de0}), (\\ref{eqn:de1}), and (\\ref{eqn:de2}), i.e.\nproportional to $N_r^{-1/2}$, $N_r^{-1/4}$, and $N_r^{-1/6}$, respectively,\nalthough the normalisations for the Monte Carlo results as compared to the\nanalytical approximations are higher by factors of 2.27, 1.63 and 1.38,\nrespectively. The difference is caused by the fact that the analytical\napproximation assumed that the maximum of the Kolmogorov-Smirnov statistic\n$D^\\prime$ is reached at the $x$-value $x_m$ where the cumulative distributions\n$F_0$ and $F_1$ reach their maximum distance. In practice, because of\nstatistical fluctuations the maximum for $D^\\prime$ can also be reached for\nother values of $x$ close to $x_m$, and this causes a somewhat more relaxed\nconstraint on the bin size for the Monte Carlo results.\n\n\\subsection{Which approximation do we choose?}\n\nWe now compare the different approximations $f_0$, $f_1$, and $f_2$ as derived\nin the previous subsection. Fig.~\\ref{fig:plotres} shows that the approximation\n$f_1$ yields an order of magnitude or more improvement over the classical\napproximation $f_0$. However, the approximation $f_2$ is only slightly better\nthan $f_1$. Moreover, the computational burden of approximation $f_2$ is large.\nThe evaluation of (\\ref{eqn:tau}) is rather straightforward, although care\nshould be taken with single machine precision; first the average energy $E_a$\nshould be determined and then this value should be used in the estimation of\n$\\tau$. A more serious problem is that the width of the lsf should be adjusted\nfrom $\\sigma$ to $\\sqrt{\\sigma^2+\\tau^2}$. If the lsf is a pure Gaussian, this\ncan be carried out analytically; however, for a slightly non-Gaussian lsf the\ntrue lsf should be convolved in general numerically with a Gaussian of width\n$\\tau$ to obtain the effective lsf for the particular bin, and the computational\nburden is very heavy. On the other hand, for $f_1$ only a shift in the lsf is\nsufficient.\n\nTherefore we recommend using the linear approximation $f_1$. The optimal bin\nsize is thus given by (\\ref{eqn:dey}), (\\ref{eqn:app1}), and (\\ref{eqn:appr1}).\n\n\\subsection{The effective area}\n\nAbove we showed how the optimal model energy grid can be determined, taking into\naccount the possible presence of narrow spectral features, the number of\nresolution elements, and the flux of the source. We also need to account for \nthe energy dependence of the effective area, however. In the previous section,\nwe considered merely the effect of the spectral redistribution (rmf); here we\nconsider the effective area (arf).\n\nIf the effective area $A_j(E)$ would be a constant $A_j$ within a model bin $j$,\nthen for a photon flux $F_j$ in the bin the total count rate produced by this\nbin would be simply $A_j F_j$. This approach is actually used in the classical\nway of analysing spectra. In general $A_j(E)$ is not constant, however, and the\nabove approach is justified only when all photons of the model bin have the\nenergy of the bin centre. It is better to take into account not only the flux\n$F_j$ but also the average energy $E_a$ of the photons within the model bin. \nThis average energy $E_a$ is in general not equal to the bin centre $E_j$, and\nhence we need to evaluate the effective area not at $E_j$ but at $E_a$.\n\nWe consider here the most natural first-order extension, namely the assumption\nthat the effective area within a model bin is a linear function of the energy.\nFor each model bin $j$, we develop the effective area $A(E)$ as a Taylor series\nin $E-E_j$, which is written as\n\n", "itemtype": "equation", "pos": 32813, "prevtext": "\n\n\\subsection{Monte Carlo results\\label{sect:montecarlo}}\n\nIn addition to the analytical approximations described above, we have performed\nMonte Carlo calculations as outlined in Sect.~\\ref{sect:mccalculations}. For our\napproximations of order 0, 1, and 2, corresponding to accounting for the number\nof photons only, both the number of photons and centroid and the number of\nphotons, centroid, and dispersion, respectively, we found the following\napproximations:\n\n", "index": 33, "text": "\\begin{equation}\n\\frac{\\Delta}{{\\rm FWHM}} = \\min (1,y),\\label{eqn:dey}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{{\\rm FWHM}}=\\min(1,y),\" display=\"block\"><mrow><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>=</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nThe maximum relative deviation $\\epsilon_{\\max}$ from this approximation occurs\nwhen $E$ is at one of the bin boundaries. It is given by\n\n", "itemtype": "equation", "pos": 37980, "prevtext": "\nwith\n\\begin{eqnarray}\n\\mbox{order 0}\\ &:&\\ y=\\frac{0.5707}{x^{1/2}}\\Bigl( 1 + \\frac{1.0}{x} \\Bigr),\n\\label{eqn:app0}\\\\\n\\mbox{order 1}\\ &:&\\ y=\\frac{1.404}{x^{1/4}}\\Bigl( 1 + \\frac{18}{x} \\Bigr), \n\\label{eqn:app1}\\\\\n\\mbox{order 2}\\ &:&\\ y=\\frac{1.569}{x^{1/6}}\\Bigl( 1 + \\frac{1.14}{x^{1/3}}\n\\Bigr),\\label{eqn:app2}\n\\end{eqnarray}\nwhere\\begin{eqnarray}\n\\mbox{order 0}\\ &:&\\ x = N_r(1+0.3\\ln R), \\label{eqn:appr0}\\\\\n\\mbox{order 1}\\ &:&\\ x = N_r(1+0.1\\ln R), \\label{eqn:appr1}\\\\\n\\mbox{order 2}\\ &:&\\ x = N_r(1+0.6\\ln R). \\label{eqn:appr2}\n\\end{eqnarray}\nHere $N_r$ is the number of photons within a resolution element and $R$ the\nnumber of resolution elements. The precise upper cut-off value of 1.0 is a\nlittle arbitrary, but we adopted it here for simplicity as the same as for our\napproximation of the data grid binning (see Sect.~\\ref{sect:data}).\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{plotres.ps}}\n\\caption{Optimal bin size $\\Delta$ for model spectrum binning with a Gaussian\nlsf, as a function of the number of counts per resolution element $N_r$.\nDash-dotted line: zeroth order approximation (\\ref{eqn:app0}); thick solid line:\nfirst order approximation (\\ref{eqn:app1}); dashed line: second order\napproximation (\\ref{eqn:app2}). For comparison we also show as the dotted curve\nthe approximation (\\ref{eqn:databin}) for the data grid binning. All results\nshown are for $R=1$.}\n\\label{fig:plotres}\n\\end{figure}\n\nWe show these approximations in Fig.~\\ref{fig:plotres}. The asymptotic behaviour\nis as described by (\\ref{eqn:de0}), (\\ref{eqn:de1}), and (\\ref{eqn:de2}), i.e.\nproportional to $N_r^{-1/2}$, $N_r^{-1/4}$, and $N_r^{-1/6}$, respectively,\nalthough the normalisations for the Monte Carlo results as compared to the\nanalytical approximations are higher by factors of 2.27, 1.63 and 1.38,\nrespectively. The difference is caused by the fact that the analytical\napproximation assumed that the maximum of the Kolmogorov-Smirnov statistic\n$D^\\prime$ is reached at the $x$-value $x_m$ where the cumulative distributions\n$F_0$ and $F_1$ reach their maximum distance. In practice, because of\nstatistical fluctuations the maximum for $D^\\prime$ can also be reached for\nother values of $x$ close to $x_m$, and this causes a somewhat more relaxed\nconstraint on the bin size for the Monte Carlo results.\n\n\\subsection{Which approximation do we choose?}\n\nWe now compare the different approximations $f_0$, $f_1$, and $f_2$ as derived\nin the previous subsection. Fig.~\\ref{fig:plotres} shows that the approximation\n$f_1$ yields an order of magnitude or more improvement over the classical\napproximation $f_0$. However, the approximation $f_2$ is only slightly better\nthan $f_1$. Moreover, the computational burden of approximation $f_2$ is large.\nThe evaluation of (\\ref{eqn:tau}) is rather straightforward, although care\nshould be taken with single machine precision; first the average energy $E_a$\nshould be determined and then this value should be used in the estimation of\n$\\tau$. A more serious problem is that the width of the lsf should be adjusted\nfrom $\\sigma$ to $\\sqrt{\\sigma^2+\\tau^2}$. If the lsf is a pure Gaussian, this\ncan be carried out analytically; however, for a slightly non-Gaussian lsf the\ntrue lsf should be convolved in general numerically with a Gaussian of width\n$\\tau$ to obtain the effective lsf for the particular bin, and the computational\nburden is very heavy. On the other hand, for $f_1$ only a shift in the lsf is\nsufficient.\n\nTherefore we recommend using the linear approximation $f_1$. The optimal bin\nsize is thus given by (\\ref{eqn:dey}), (\\ref{eqn:app1}), and (\\ref{eqn:appr1}).\n\n\\subsection{The effective area}\n\nAbove we showed how the optimal model energy grid can be determined, taking into\naccount the possible presence of narrow spectral features, the number of\nresolution elements, and the flux of the source. We also need to account for \nthe energy dependence of the effective area, however. In the previous section,\nwe considered merely the effect of the spectral redistribution (rmf); here we\nconsider the effective area (arf).\n\nIf the effective area $A_j(E)$ would be a constant $A_j$ within a model bin $j$,\nthen for a photon flux $F_j$ in the bin the total count rate produced by this\nbin would be simply $A_j F_j$. This approach is actually used in the classical\nway of analysing spectra. In general $A_j(E)$ is not constant, however, and the\nabove approach is justified only when all photons of the model bin have the\nenergy of the bin centre. It is better to take into account not only the flux\n$F_j$ but also the average energy $E_a$ of the photons within the model bin. \nThis average energy $E_a$ is in general not equal to the bin centre $E_j$, and\nhence we need to evaluate the effective area not at $E_j$ but at $E_a$.\n\nWe consider here the most natural first-order extension, namely the assumption\nthat the effective area within a model bin is a linear function of the energy.\nFor each model bin $j$, we develop the effective area $A(E)$ as a Taylor series\nin $E-E_j$, which is written as\n\n", "index": 35, "text": "\\begin{equation} \nA_j(E) = A(E_j) + A^\\prime(E_j) (E-E_j) + \\ldots.\n\\label{eqn:area_taylor}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"A_{j}(E)=A(E_{j})+A^{\\prime}(E_{j})(E-E_{j})+\\ldots.\" display=\"block\"><mrow><mrow><mrow><msub><mi>A</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>-</mo><msub><mi>E</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi mathvariant=\"normal\">\u2026</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $\\Delta E$ is the model bin width. Therefore by using the approximation\n(\\ref{eqn:area_taylor}) we make at most a relative error in the effective area\ngiven by (\\ref{eqn:epsilon_max}). This can be translated directly into an error\nin the predicted count rate by multiplying $\\epsilon_{\\max}$ by the photon flux\n$F_j$. The relative error in the count rate is thus also given by\n$\\epsilon_{\\max}$, which should be sufficiently small compared to the Poissonian\nfluctuations $1/\\sqrt{N_r}$ in the relevant range.\n\nWe can do this in a more formal way by finding for which value of $\\epsilon$ a\ntest of the hypothesis that $N_r$ is drawn from a Poissonian distribution with\naverage value $\\mu$ is effectively indistinguishable from tests that $N_r$ is\ndrawn from a distribution with mean $\\mu(1+\\epsilon)$. We have outlined such a\nprocedure in Sect.~\\ref{sect:testing}, and in the limit of large $\\mu$ we write\n\n", "itemtype": "equation", "pos": 38224, "prevtext": "\nThe maximum relative deviation $\\epsilon_{\\max}$ from this approximation occurs\nwhen $E$ is at one of the bin boundaries. It is given by\n\n", "index": 37, "text": "\\begin{equation}\n\\epsilon_{\\max} = \\frac{1}{8} (\\Delta E)^2 A^{\\prime\\prime}(E_j)/A(E_j),\n\\label{eqn:epsilon_max}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\epsilon_{\\max}=\\frac{1}{8}(\\Delta E)^{2}A^{\\prime\\prime}(E_{j})/A(E_{j}),\" display=\"block\"><mrow><mrow><msub><mi>\u03f5</mi><mi>max</mi></msub><mo>=</mo><mrow><mrow><mrow><mfrac><mn>1</mn><mn>8</mn></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>E</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>A</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $\\alpha$ is the size of the test and $q_\\alpha$ is given by\n$G(q_\\alpha)=1-\\alpha$ with $G$ the cumulative normal probability distribution.\nFurther $k\\alpha$ is the size of the test when we use the approximation\n(\\ref{eqn:area_taylor}). In the limit of small $\\epsilon$, the solution of\n(\\ref{eqn:epsilon_area}) is given by\n\n", "itemtype": "equation", "pos": 39264, "prevtext": "\nwhere $\\Delta E$ is the model bin width. Therefore by using the approximation\n(\\ref{eqn:area_taylor}) we make at most a relative error in the effective area\ngiven by (\\ref{eqn:epsilon_max}). This can be translated directly into an error\nin the predicted count rate by multiplying $\\epsilon_{\\max}$ by the photon flux\n$F_j$. The relative error in the count rate is thus also given by\n$\\epsilon_{\\max}$, which should be sufficiently small compared to the Poissonian\nfluctuations $1/\\sqrt{N_r}$ in the relevant range.\n\nWe can do this in a more formal way by finding for which value of $\\epsilon$ a\ntest of the hypothesis that $N_r$ is drawn from a Poissonian distribution with\naverage value $\\mu$ is effectively indistinguishable from tests that $N_r$ is\ndrawn from a distribution with mean $\\mu(1+\\epsilon)$. We have outlined such a\nprocedure in Sect.~\\ref{sect:testing}, and in the limit of large $\\mu$ we write\n\n", "index": 39, "text": "\\begin{equation}\n\\mu + q_\\alpha\\sqrt{\\mu} = \n \\mu (1+\\epsilon ) + q_{k\\alpha}\\sqrt{\\mu (1+\\epsilon)},\n \\label{eqn:epsilon_area}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\mu+q_{\\alpha}\\sqrt{\\mu}=\\mu(1+\\epsilon)+q_{k\\alpha}\\sqrt{\\mu(1+\\epsilon)},\" display=\"block\"><mrow><mrow><mrow><mi>\u03bc</mi><mo>+</mo><mrow><msub><mi>q</mi><mi>\u03b1</mi></msub><mo>\u2062</mo><msqrt><mi>\u03bc</mi></msqrt></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>q</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow></msub><mo>\u2062</mo><msqrt><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere we have approximated $\\mu$ by the observed value $N_r$, and where\n$p(\\alpha,k)=q_{\\alpha}-q_{k\\alpha}$. For $\\alpha=0.025$ and $k=2,$ we obtain\n$p(\\alpha,k)=0.31511$.\nCombining these results with (\\ref{eqn:epsilon_max}), we obtain an expression\nfor the model bin size\n\n", "itemtype": "equation", "pos": 39737, "prevtext": "\nwhere $\\alpha$ is the size of the test and $q_\\alpha$ is given by\n$G(q_\\alpha)=1-\\alpha$ with $G$ the cumulative normal probability distribution.\nFurther $k\\alpha$ is the size of the test when we use the approximation\n(\\ref{eqn:area_taylor}). In the limit of small $\\epsilon$, the solution of\n(\\ref{eqn:epsilon_area}) is given by\n\n", "index": 41, "text": "\\begin{equation}\n\\epsilon \\simeq p(\\alpha,k) / \\sqrt(N_r),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\epsilon\\simeq p(\\alpha,k)/\\sqrt{(}N_{r}),\" display=\"block\"><mrow><mi>\u03f5</mi><mo>\u2243</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msqrt><mo stretchy=\"false\">(</mo></msqrt><msub><mi>N</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nThe bin width constraint derived here depends upon the dimensionless curvature\nof the effective area ${A / E_j^2A^{\\prime\\prime}}$. In most parts of the energy\nrange this is a number of order unity or less. Since the second prefactor ${E_j\n/ {\\rm FWHM}}$ is by definition the resolution of the instrument, we see by\ncomparing (\\ref{eqn:de1area}) with (\\ref{eqn:de1}) that, in general,\n(\\ref{eqn:de1}) gives the most severe constraint upon the bin width. This is the\ncase unless either the resolution becomes of order unity, which happens, for\nexample for the Rosat \\citep{truemper1982} PSPC \\citep{pfefferman1987} detector\nat low energies, or the effective area curvature becomes large, which may\nhappen, for example near the exponential cut-offs caused by filters.\n\nLarge effective area curvature due to the presence of exponential cut-offs is\nusually not a problem, since these cut-offs also cause the count rate to be low\nand hence weaken the binning requirements. Of course, discrete edges in the\neffective area should always be avoided in the sense that edges should always\ncoincide with bin boundaries.\n\nIn practice, it is a little complicated to estimate from, for example a look-up table of\nthe effective area its curvature, although this is not impossible. As a\nsimplification for order of magnitude estimates, we use the case where\n$A(E)=A_0{\\rm e}^{bE}$ locally, which after differentiation yields\n\n", "itemtype": "equation", "pos": 40085, "prevtext": "\nwhere we have approximated $\\mu$ by the observed value $N_r$, and where\n$p(\\alpha,k)=q_{\\alpha}-q_{k\\alpha}$. For $\\alpha=0.025$ and $k=2,$ we obtain\n$p(\\alpha,k)=0.31511$.\nCombining these results with (\\ref{eqn:epsilon_max}), we obtain an expression\nfor the model bin size\n\n", "index": 43, "text": "\\begin{equation}\n\\frac{\\Delta}{{\\rm FWHM}} = \\Bigl( \\frac{8Ap(\\alpha,k)}{E_j^2 A^{\\prime\\prime}} \n\\Bigr)^{0.5} \n\\Bigl( \\frac{E_j}{{\\rm FWHM}} \\Bigr)\nN_r^{-0.25}.\n\\label{eqn:de1area}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{{\\rm FWHM}}=\\Bigl{(}\\frac{8Ap(\\alpha,k)}{E_{j}^{2}A^{\\prime%&#10;\\prime}}\\Bigr{)}^{0.5}\\Bigl{(}\\frac{E_{j}}{{\\rm FWHM}}\\Bigr{)}N_{r}^{-0.25}.\" display=\"block\"><mrow><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>=</mo><mrow><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><mrow><mn>8</mn><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>E</mi><mi>j</mi><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup></mrow></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mn>0.5</mn></msup><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><msub><mi>E</mi><mi>j</mi></msub><mi>FWHM</mi></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>N</mi><mi>r</mi><mrow><mo>-</mo><mn>0.25</mn></mrow></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nInserting this into (\\ref{eqn:de1area}), we obtain our recommended bin size,\nas far as effective area curvature is concerned\n\n", "itemtype": "equation", "pos": 41692, "prevtext": "\n\nThe bin width constraint derived here depends upon the dimensionless curvature\nof the effective area ${A / E_j^2A^{\\prime\\prime}}$. In most parts of the energy\nrange this is a number of order unity or less. Since the second prefactor ${E_j\n/ {\\rm FWHM}}$ is by definition the resolution of the instrument, we see by\ncomparing (\\ref{eqn:de1area}) with (\\ref{eqn:de1}) that, in general,\n(\\ref{eqn:de1}) gives the most severe constraint upon the bin width. This is the\ncase unless either the resolution becomes of order unity, which happens, for\nexample for the Rosat \\citep{truemper1982} PSPC \\citep{pfefferman1987} detector\nat low energies, or the effective area curvature becomes large, which may\nhappen, for example near the exponential cut-offs caused by filters.\n\nLarge effective area curvature due to the presence of exponential cut-offs is\nusually not a problem, since these cut-offs also cause the count rate to be low\nand hence weaken the binning requirements. Of course, discrete edges in the\neffective area should always be avoided in the sense that edges should always\ncoincide with bin boundaries.\n\nIn practice, it is a little complicated to estimate from, for example a look-up table of\nthe effective area its curvature, although this is not impossible. As a\nsimplification for order of magnitude estimates, we use the case where\n$A(E)=A_0{\\rm e}^{bE}$ locally, which after differentiation yields\n\n", "index": 45, "text": "\\begin{equation}\n\\sqrt{ \\frac{8A}{ E_j^2A^{\\prime\\prime}} } = \n\\sqrt{8}\\, \\frac{{\\rm d}\\ln E}{{\\rm d}\\ln A}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\sqrt{\\frac{8A}{E_{j}^{2}A^{\\prime\\prime}}}=\\sqrt{8}\\,\\frac{{\\rm d}\\ln E}{{\\rm&#10;d%&#10;}\\ln A}.\" display=\"block\"><mrow><mrow><msqrt><mfrac><mrow><mn>8</mn><mo>\u2062</mo><mi>A</mi></mrow><mrow><msubsup><mi>E</mi><mi>j</mi><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup></mrow></mfrac></msqrt><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msqrt><mn>8</mn></msqrt></mpadded><mo>\u2062</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>E</mi></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>A</mi></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\n\\subsection{Final remarks}\n\nIn the previous two subsections we have given the constraints for determining\nthe optimal model energy grid. Combining both requirements (\\ref{eqn:app1}) and\n(\\ref{eqn:de1ar}) we obtain the following optimal bin size:\n\n", "itemtype": "equation", "pos": 41941, "prevtext": "\nInserting this into (\\ref{eqn:de1area}), we obtain our recommended bin size,\nas far as effective area curvature is concerned\n\n", "index": 47, "text": "\\begin{equation}\n\\frac{\\Delta}{{\\rm FWHM}} = 1.5877\\, \n \\Bigl( \\frac{{\\rm d}\\ln E }{ {\\rm d}\\ln A} \\Bigr)\n \\Bigl( \\frac{E_j}{{\\rm FWHM}} \\Bigr)\n  N_r^{-0.25}.\n\\label{eqn:de1ar}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{{\\rm FWHM}}=1.5877\\,\\Bigl{(}\\frac{{\\rm d}\\ln E}{{\\rm d}\\ln A}%&#10;\\Bigr{)}\\Bigl{(}\\frac{E_{j}}{{\\rm FWHM}}\\Bigr{)}N_{r}^{-0.25}.\" display=\"block\"><mrow><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mn>1.5877</mn></mpadded><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>E</mi></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>A</mi></mrow></mrow></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><msub><mi>E</mi><mi>j</mi></msub><mi>FWHM</mi></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>N</mi><mi>r</mi><mrow><mo>-</mo><mn>0.25</mn></mrow></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $w_1$ and $w_a$ are the values of $\\Delta$/FWHM as calculated using\n(\\ref{eqn:app1}) and (\\ref{eqn:de1ar}), respectively.\n\nThis choice of model binning ensures that no significant errors are made either\ndue to inaccuracies in the model or the effective area for flux distributions\nwithin the model bins that have $E_a\\ne E_j$.\n\n\\section{\\label{sect:data}Data binning}\n\n\\subsection{Introduction}\n\nMost X-ray detectors count the individual photons and do not register the exact\nenergy value but a digitised version of it. Then a histogram is produced\ncontaining the number of events as a function of the energy. The bin size of\nthese data channels ideally should not exceed the resolution of the instrument,\notherwise important information may be lost. On the other hand, if the bin size\nis too small, one may have to deal with low statistics per data channel,\ninsufficient sensitivity of statistical tests or a large computational overhead\ncaused by the large number of data channels. Low numbers of counts per data\nchannel can be alleviated by using C-statistics \\citep[often slightly modified\nfrom the original definition by][]{cash1979}; computational overhead can be a\nburden for complex models, but insufficient sensitivity of statistical tests can\nlead to inefficient use of the information that is contained in a spectrum. We\nillustrate this inefficient use of information in Appendix~\\ref{sect:appc}. In\nthis section we derive the optimal bin size for the data channels. \n\n\\subsection{The Shannon theorem}\n\nThe \\citet{shannon1949} sampling theorem states the following: Let $f(x)$ be a\ncontinuous signal. Let $g(\\omega)$ be its Fourier transform, given by\n\n", "itemtype": "equation", "pos": 42380, "prevtext": "\n\n\\subsection{Final remarks}\n\nIn the previous two subsections we have given the constraints for determining\nthe optimal model energy grid. Combining both requirements (\\ref{eqn:app1}) and\n(\\ref{eqn:de1ar}) we obtain the following optimal bin size:\n\n", "index": 49, "text": "\\begin{equation}\n\\frac{\\Delta}{{\\rm FWHM}} = \\frac{1}{1/w_1 + 1/w_a},\n\\label{eqn:de1tot}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{{\\rm FWHM}}=\\frac{1}{1/w_{1}+1/w_{a}},\" display=\"block\"><mrow><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>w</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>w</mi><mi>a</mi></msub></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nIf $g(\\omega)=0$ for all $\\vert \\omega \\vert > W$ for a given frequency $W$,\nthen $f(x)$ is band limited, and in that case Shannon has shown that\n\n", "itemtype": "equation", "pos": 44153, "prevtext": "\nwhere $w_1$ and $w_a$ are the values of $\\Delta$/FWHM as calculated using\n(\\ref{eqn:app1}) and (\\ref{eqn:de1ar}), respectively.\n\nThis choice of model binning ensures that no significant errors are made either\ndue to inaccuracies in the model or the effective area for flux distributions\nwithin the model bins that have $E_a\\ne E_j$.\n\n\\section{\\label{sect:data}Data binning}\n\n\\subsection{Introduction}\n\nMost X-ray detectors count the individual photons and do not register the exact\nenergy value but a digitised version of it. Then a histogram is produced\ncontaining the number of events as a function of the energy. The bin size of\nthese data channels ideally should not exceed the resolution of the instrument,\notherwise important information may be lost. On the other hand, if the bin size\nis too small, one may have to deal with low statistics per data channel,\ninsufficient sensitivity of statistical tests or a large computational overhead\ncaused by the large number of data channels. Low numbers of counts per data\nchannel can be alleviated by using C-statistics \\citep[often slightly modified\nfrom the original definition by][]{cash1979}; computational overhead can be a\nburden for complex models, but insufficient sensitivity of statistical tests can\nlead to inefficient use of the information that is contained in a spectrum. We\nillustrate this inefficient use of information in Appendix~\\ref{sect:appc}. In\nthis section we derive the optimal bin size for the data channels. \n\n\\subsection{The Shannon theorem}\n\nThe \\citet{shannon1949} sampling theorem states the following: Let $f(x)$ be a\ncontinuous signal. Let $g(\\omega)$ be its Fourier transform, given by\n\n", "index": 51, "text": "\\begin{equation}\ng(\\omega) = \\int\\limits_{-\\infty}^{\\infty}\nf(x) {\\rm e}^{\\displaystyle{i\\omega x}} {\\rm d}x.\n\\label{eqn:fourier}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"g(\\omega)=\\int\\limits_{-\\infty}^{\\infty}f(x){\\rm e}^{\\displaystyle{i\\omega x}}%&#10;{\\rm d}x.\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c9</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi mathvariant=\"normal\">e</mi><mrow><mi mathsize=\"142%\">i</mi><mo>\u2062</mo><mi mathsize=\"142%\">\u03c9</mi><mo>\u2062</mo><mi mathsize=\"142%\">x</mi></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>x</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nIn (\\ref{eqn:shannon}), the bin size $\\Delta=1/2W$. Thus, a band-limited signal\nis completely determined by its values at an equally spaced grid with spacing\n$\\Delta$.\n\nThe above is used for continuous signals sampled at a discrete set of intervals\n$x_i$. However, X-ray spectra are essentially a histogram of the number of\nevents as a function of channel number. We do not measure the signal at the data\nchannel boundaries, but we measure the sum (integral) of the signal between the\ndata channel boundaries. Hence for X-ray spectra it is more appropriate to study\nthe integral of $f(x)$ instead of $f(x)$ itself. \n\nWe scale $f(x)$ to represent a true probability distribution. The cumulative\nprobability density distribution function $F(x)$ is written as\n\n", "itemtype": "equation", "pos": 44444, "prevtext": "\nIf $g(\\omega)=0$ for all $\\vert \\omega \\vert > W$ for a given frequency $W$,\nthen $f(x)$ is band limited, and in that case Shannon has shown that\n\n", "index": 53, "text": "\\begin{equation}\nf(x) = f_s(x) \\equiv \\sum\\limits_{n=-\\infty}^{\\infty}\nf(n\\Delta) \\frac{\\sin \\pi(x/\\Delta-n) }{ \\pi(x/\\Delta-n)}.\n\\label{eqn:shannon}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"f(x)=f_{s}(x)\\equiv\\sum\\limits_{n=-\\infty}^{\\infty}f(n\\Delta)\\frac{\\sin\\pi(x/%&#10;\\Delta-n)}{\\pi(x/\\Delta-n)}.\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>f</mi><mi>s</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mrow><mrow><mi>sin</mi><mo>\u2061</mo><mi>\u03c0</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>x</mi><mo>/</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>-</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>x</mi><mo>/</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>-</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nIf we insert the Shannon reconstruction (\\ref{eqn:shannon}) in\n(\\ref{eqn:cumdefinition}), after interchanging the integration and summation and\nkeeping into mind that we cannot evaluate $F(x)$ at all arbitrary points but\nonly at those grid points $m\\Delta$ for integer $m$ where also $f_s$ is sampled,\nwe obtain\n\n", "itemtype": "equation", "pos": 45366, "prevtext": "\nIn (\\ref{eqn:shannon}), the bin size $\\Delta=1/2W$. Thus, a band-limited signal\nis completely determined by its values at an equally spaced grid with spacing\n$\\Delta$.\n\nThe above is used for continuous signals sampled at a discrete set of intervals\n$x_i$. However, X-ray spectra are essentially a histogram of the number of\nevents as a function of channel number. We do not measure the signal at the data\nchannel boundaries, but we measure the sum (integral) of the signal between the\ndata channel boundaries. Hence for X-ray spectra it is more appropriate to study\nthe integral of $f(x)$ instead of $f(x)$ itself. \n\nWe scale $f(x)$ to represent a true probability distribution. The cumulative\nprobability density distribution function $F(x)$ is written as\n\n", "index": 55, "text": "\\begin{equation}\nF(x) = \\int\\limits_{-\\infty}^{x} f(y) {\\rm d}y.\n\\label{eqn:cumdefinition}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"F(x)=\\int\\limits_{-\\infty}^{x}f(y){\\rm d}y.\" display=\"block\"><mrow><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow><mi>x</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>y</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nThe function ${\\rm Si}(x)$ is the sine-integral as defined, for example in\n\\citet{abramowitz1965}. The expression (\\ref{eqn:shannon_int}) for $F_s$ equals\n$F$ if $f(x)$ is band limited. In that case at the grid points $F$ is completely\ndetermined by the value of $f$ at the grid points. By inverting this relation,\none could express $f$ at the grid points as a unique linear combination of the\n$F$-values at the grid. Since Shannon's theorem states that $f(x)$ for arbitrary\n$x$ is determined completely by the $f$-values at the grid, we infer that $f(x)$\ncan be completely reconstructed from the discrete set of $F$-values. And then,\nby integrating this reconstructed $f(x)$, $F(x)$ is also determined.\n\nWe conclude that $F(x)$ is also completely determined by the set of discrete\nvalues $F(m\\Delta)$ at $x=m\\Delta$ for integer values of $m$, provided that\n$f(x)$ is band limited.\n\nFor non-band-limited responses, we use (\\ref{eqn:shannon_int}) to approximate\nthe true cumulative distribution function at the energy grid. In doing this, a\nsmall error is made. The errors can be calculated easily by comparing\n$F_s(m\\Delta)$ with the true $F(m\\Delta)$ values. The binning $\\Delta$ is\nsufficient if the sampling errors are sufficiently small compared with the\nPoissonian noise. We elaborate on what is sufficient below.\n\n\\subsection{Optimal binning of data}\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{sagaus.ps}}\n\\caption{Maximum difference for the cumulative distribution function of a\nGaussian and its Shannon approximation, as a function of the bin size $\\Delta$.\nThe maximum is also taken over all phases of the energy grid with respect to the\ncentre of the Gaussian.}\n\\label{fig:sagaus}\n\\end{figure}\n\nWe apply the theory outlined above to the case of a Gaussian redistribution\nfunction. We first determine the maximum difference $\\delta_k$ between the\ncumulative Gaussian distribution function $F_0(x)$ and its Shannon approximation\n$F_1(x)=F_s(x)$, as defined by (\\ref{eqn:lambda_k}). We show this quantity in\nFig.~\\ref{fig:sagaus} as a function of the bin size $\\Delta$. It is seen that\nthe Shannon approximation converges quickly to the true distribution for\ndecreasing values $\\Delta$. Using $\\delta_k=\\lambda_k/\\sqrt{N}$ (see\n(\\ref{eqn:epsilon_lambda})), for any given value of $N$ and $\\lambda_k=0.122$ we\ncan invert the relation and find the optimal bin size $\\Delta$. This is plotted\nin Fig.~\\ref{fig:databin}. We have also verified this with the Monte Carlo\nmethod described before. \n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{newdatabin.ps}}\n\\caption{Optimal bin size $\\Delta$ for data binning with a Gaussian lsf as a\nfunction of the number of counts per resolution element $N_r$. Dotted curve:\nanalytical approximation using Sect.~\\ref{sect:anap}; stars: results of Monte\nCarlo calculation; solid line: our finally adopted bin size\nEq.~(\\ref{eqn:databin}) based on a fit to our Monte Carlo results for large\nvalues of $N_r$; dashed line: commonly adopted bin size 1/3~FWHM.}\n\\label{fig:databin}\n\\end{figure}\n\nIt is seen that the Monte Carlo results indeed converge to the analytical\nsolution for large $N_r$. For $N_r=10$, the difference is 20\\%. We have made a\nsimple approximation to our Monte Carlo results (shown as the solid line in\nFig.~\\ref{fig:databin}), where we cut it off to a constant value for $N_r$ of\nless than about 2. Both the Monte Carlo results and the analytical approximation\nbreak down at this small number of counts. In fact, for a small number of counts\nbinning to about 1 FWHM is sufficient.\n\nWe also determined the dependence on the number of resolution elements $R$ for\nvalues of $R$ of 1, 10, 100, 1000, and 10000. In general, the results even for\n$R=10^4$ are not too different from the $R=1$ case. We found a very simple\napproximation for the dependence on $R$ that is well described by the following\nfunction:\n\n", "itemtype": "equation", "pos": 45784, "prevtext": "\nIf we insert the Shannon reconstruction (\\ref{eqn:shannon}) in\n(\\ref{eqn:cumdefinition}), after interchanging the integration and summation and\nkeeping into mind that we cannot evaluate $F(x)$ at all arbitrary points but\nonly at those grid points $m\\Delta$ for integer $m$ where also $f_s$ is sampled,\nwe obtain\n\n", "index": 57, "text": "\\begin{equation}\nF_s(m\\Delta) = \\frac{\\Delta }{ \\pi}\n\\sum\\limits_{n=-\\infty}^{\\infty} f(n\\Delta)\n\\Bigl\\{\n\\frac{\\pi}{ 2} + {\\rm Si}[\\pi(m-n)]\n\\Bigr\\}.\n\\label{eqn:shannon_int}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"F_{s}(m\\Delta)=\\frac{\\Delta}{\\pi}\\sum\\limits_{n=-\\infty}^{\\infty}f(n\\Delta)%&#10;\\Bigl{\\{}\\frac{\\pi}{2}+{\\rm Si}[\\pi(m-n)]\\Bigr{\\}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>F</mi><mi>s</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mrow><mo>-</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">{</mo><mrow><mfrac><mi>\u03c0</mi><mn>2</mn></mfrac><mo>+</mo><mrow><mi>Si</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>-</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">}</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwith\n\n", "itemtype": "equation", "pos": 49890, "prevtext": "\nThe function ${\\rm Si}(x)$ is the sine-integral as defined, for example in\n\\citet{abramowitz1965}. The expression (\\ref{eqn:shannon_int}) for $F_s$ equals\n$F$ if $f(x)$ is band limited. In that case at the grid points $F$ is completely\ndetermined by the value of $f$ at the grid points. By inverting this relation,\none could express $f$ at the grid points as a unique linear combination of the\n$F$-values at the grid. Since Shannon's theorem states that $f(x)$ for arbitrary\n$x$ is determined completely by the $f$-values at the grid, we infer that $f(x)$\ncan be completely reconstructed from the discrete set of $F$-values. And then,\nby integrating this reconstructed $f(x)$, $F(x)$ is also determined.\n\nWe conclude that $F(x)$ is also completely determined by the set of discrete\nvalues $F(m\\Delta)$ at $x=m\\Delta$ for integer values of $m$, provided that\n$f(x)$ is band limited.\n\nFor non-band-limited responses, we use (\\ref{eqn:shannon_int}) to approximate\nthe true cumulative distribution function at the energy grid. In doing this, a\nsmall error is made. The errors can be calculated easily by comparing\n$F_s(m\\Delta)$ with the true $F(m\\Delta)$ values. The binning $\\Delta$ is\nsufficient if the sampling errors are sufficiently small compared with the\nPoissonian noise. We elaborate on what is sufficient below.\n\n\\subsection{Optimal binning of data}\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{sagaus.ps}}\n\\caption{Maximum difference for the cumulative distribution function of a\nGaussian and its Shannon approximation, as a function of the bin size $\\Delta$.\nThe maximum is also taken over all phases of the energy grid with respect to the\ncentre of the Gaussian.}\n\\label{fig:sagaus}\n\\end{figure}\n\nWe apply the theory outlined above to the case of a Gaussian redistribution\nfunction. We first determine the maximum difference $\\delta_k$ between the\ncumulative Gaussian distribution function $F_0(x)$ and its Shannon approximation\n$F_1(x)=F_s(x)$, as defined by (\\ref{eqn:lambda_k}). We show this quantity in\nFig.~\\ref{fig:sagaus} as a function of the bin size $\\Delta$. It is seen that\nthe Shannon approximation converges quickly to the true distribution for\ndecreasing values $\\Delta$. Using $\\delta_k=\\lambda_k/\\sqrt{N}$ (see\n(\\ref{eqn:epsilon_lambda})), for any given value of $N$ and $\\lambda_k=0.122$ we\ncan invert the relation and find the optimal bin size $\\Delta$. This is plotted\nin Fig.~\\ref{fig:databin}. We have also verified this with the Monte Carlo\nmethod described before. \n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{newdatabin.ps}}\n\\caption{Optimal bin size $\\Delta$ for data binning with a Gaussian lsf as a\nfunction of the number of counts per resolution element $N_r$. Dotted curve:\nanalytical approximation using Sect.~\\ref{sect:anap}; stars: results of Monte\nCarlo calculation; solid line: our finally adopted bin size\nEq.~(\\ref{eqn:databin}) based on a fit to our Monte Carlo results for large\nvalues of $N_r$; dashed line: commonly adopted bin size 1/3~FWHM.}\n\\label{fig:databin}\n\\end{figure}\n\nIt is seen that the Monte Carlo results indeed converge to the analytical\nsolution for large $N_r$. For $N_r=10$, the difference is 20\\%. We have made a\nsimple approximation to our Monte Carlo results (shown as the solid line in\nFig.~\\ref{fig:databin}), where we cut it off to a constant value for $N_r$ of\nless than about 2. Both the Monte Carlo results and the analytical approximation\nbreak down at this small number of counts. In fact, for a small number of counts\nbinning to about 1 FWHM is sufficient.\n\nWe also determined the dependence on the number of resolution elements $R$ for\nvalues of $R$ of 1, 10, 100, 1000, and 10000. In general, the results even for\n$R=10^4$ are not too different from the $R=1$ case. We found a very simple\napproximation for the dependence on $R$ that is well described by the following\nfunction:\n\n", "index": 59, "text": "\\begin{equation}\n\\frac{\\Delta}{\\mathrm{FWHM}} = \\left\\{\n\\begin{array}{ll}\n1 & \\mbox{\\ \\ if $x\\leq 2.119$};\\\\\n\\strut & \\strut \\\\\n\\displaystyle{\\frac{0.08 + 7.0/ x + 1.8 /x^2}{1 + 5.9 /x}} \n   & \\mbox{\\ \\ if $x>2.119$},\n\\end{array} \\right.\n\\label{eqn:databin}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Delta}{\\mathrm{FWHM}}=\\left\\{\\begin{array}[]{ll}1&amp;\\mbox{\\ \\ if $x\\leq 2%&#10;.119$};\\\\&#10;&amp;\\\\&#10;\\displaystyle{\\frac{0.08+7.0/x+1.8/x^{2}}{1+5.9/x}}&amp;\\mbox{\\ \\ if $x&gt;2.119$},%&#10;\\end{array}\\right.\" display=\"block\"><mrow><mfrac><mi mathvariant=\"normal\">\u0394</mi><mi>FWHM</mi></mfrac><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mrow><mi>x</mi><mo>\u2264</mo><mn>2.119</mn></mrow></mrow><mo>;</mo></mrow></mtd></mtr><mtr><mtd/><mtd/></mtr><mtr><mtd columnalign=\"left\"><mfrac><mrow><mn>0.08</mn><mo>+</mo><mrow><mn>7.0</mn><mo>/</mo><mi>x</mi></mrow><mo>+</mo><mrow><mn>1.8</mn><mo>/</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mn>5.9</mn><mo>/</mo><mi>x</mi></mrow></mrow></mfrac></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mrow><mi>x</mi><mo>&gt;</mo><mn>2.119</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nWe note that for digitisation of analogue electronic signals that represent\nspectral information, a binning ($=$channel) criterion of 1/3~FWHM or better is\noften adopted \\citep[e.g.][]{davelaar1979}, since one does not know a priori the\nlevel of significance of the measured signal (see the dashed line in\nFig.~\\ref{fig:databin}).\n\n\\subsection{Final remarks\\label{sect:datafinal}}\n\nWe have estimated conservative upper bounds for the required data bin size. In\nthe case of multiple resolution elements, we have determined the bounds for the\nworst case phase of the grid with respect to the data. In practice, it is not\nlikely that all resolution elements would have the worst possible alignment.\nHowever, we recommend using the conservative upper limits as given by\n(\\ref{eqn:gausdelta}).\n\nAnother issue is the determination of $N_r$, the number of events within the\nresolution element. We have argued that an upper limit to $N_r$ can be obtained\nby counting the number of events within one FWHM and multiplying it by the ratio\nof the total area under the lsf (should be equal to 1) to the area under the lsf\nwithin one FWHM (should be smaller than 1). For the Gaussian lsf, this ratio\nequals 1.314; for other lsfs,it is better to determine the ratio numerically and\nnot to use the Gaussian value 1.314.\n\nFor the Gaussian lsf, the resolution depends only weakly upon the number of\ncounts $N_r$ (see Fig.~\\ref{fig:databin}). For low count rate parts of the\nspectrum, the binning rule of 1/3 FWHM usually is too conservative.\n\nFinally, in some cases the lsf may contain more than one component. For example,\nthe grating spectra have higher order contributions. Other instruments have\nescape peaks or fluorescent contributions. In general it is advisable to\ndetermine the bin size for each of these components individually and simply take\nthe smallest bin size as the final one.\n\n\\section{A practical example\\label{sect:example}}\n\nIn order to demonstrate the benefits of the proposed binning schemes, We have\napplied them to the 85~ks Chandra LETGS observation of Capella mentioned\nearlier. The spectrum spans the 0.86--175.55~\\AA\\ range with a spectral\nresolution (FWHM) ranging between 0.040--0.076~\\AA. In Table~\\ref{tab:capella}\nwe show the number of data bins, model bins, and response matrix elements for\ndifferent rebinning schemes of this spectrum. For simplicity we consider only\nthe positive and negative first-order spectrum, and that the non-zero elements\nof the response matrix span a full range of four times the instrumental FWHM. We\napply the algorithms as described in this paper to generate the data and model\nenergy grids.\n\nThe highest resolution of 0.040~\\AA\\ occurs at short wavelengths, and the\nstrongest line is the \\ion{Fe}{xvii} blend at 17.06 and 17.10~\\AA, with a\nmaximum number of counts $N_r$ of 15\\,000 counts. \n\nThe first case we consider (case A afterwards) is that we adopt a data and model\ngrid with constant step size for the full range. Accounting for the maximum\n$N_r$ value, we have a data bin size of 0.02~\\AA\\ and a model bin size of\n0.14~m\\AA.\n\nCase B is similar to case A except that we account for the variable resolution\nof the instrument. This only decreases the number of bins by a factor of 1.375.\n\nIn case C we account for the number of counts in each resolution element, but we\nstill keep the ''classical'' approach of putting all photons at the centre of\nthe model bins. The number of resolution elements $R$ in the spectrum is 3110.\n\nIn case D we account for the average energy of the photons within a bin. The\nnumber of model bins and response elements needed drops by more than an order of\nmagnitude for this case, as compared to case C.\n\n\\begin{table}[!htbp]\n\\caption{Number of data bins, model bins, and response matrix elements for\ndifferent rebinning schemes of the Chandra LETGS spectrum of Capella.}\n\\label{tab:capella}\n\\centerline{\n\\begin{tabular}{lrrr}\n\\hline\\hline\nBinning & Data bins & Model bins & Response \\\\\n        &           &            & elements \\\\\n\\hline\nA & $8.52\\times 10^3$ & $1.26\\times 10^6$ & $1.01\\times 10^7$ \\\\\nB & $6.20\\times 10^3$ & $9.14\\times 10^5$ & $7.31\\times 10^6$ \\\\\nC & $5.12\\times 10^3$ & $1.23\\times 10^5$ & $9.84\\times 10^5$ \\\\\nD & $5.12\\times 10^3$ & $8.21\\times 10^3$ & $6.57\\times 10^4$ \\\\\n\\hline\\noalign{\\smallskip}\n\\end{tabular}\n}\n\\end{table}\n\n\\section{The response matrix\\label{sect:matrix}}\n\nWe have shown in the previous sections how the optimal model and data energy\ngrids can be constructed. We have proposed that for the evaluation of the model\nspectrum both the number of photons in each bin as well as their average energy\nshould be determined. We now determine how this impacts the concept of the\nresponse matrix.\n\nIn order to acquire high accuracy, we need to convolve the model spectrum for\nthe bin, approximated as a $\\delta$-function centred around $E_a$, with the\ninstrument response. In most cases we cannot do this convolution analytically,\nso we have to make approximations. From our expressions for the observed count\nspectrum $s(E^\\prime)$, eqns.~(\\ref{eqn:matrix_cont}) and (\\ref{eqn:rmatrix}),\nit can be easily derived that the number of counts or count rate for data\nchannel $i$ is given by\n\n", "itemtype": "equation", "pos": 50168, "prevtext": "\nwith\n\n", "index": 61, "text": "\\begin{equation}\nx \\equiv \\ln [ N_r (1+0.20 \\ln R) ].\n\\label{eqn:gausdelta}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"x\\equiv\\ln[N_{r}(1+0.20\\ln R)].\" display=\"block\"><mrow><mrow><mi>x</mi><mo>\u2261</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>N</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mn>0.20</mn><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>R</mi></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere, as before, $E^\\prime_{i1}$ and $E^\\prime_{i2}$ are the formal channel\nlimits for data channel $i$ and $S_i$ is the observed count rate in counts/s for\ndata channel $i$. Interchanging the order of the integrations and defining the\nmono-energetic response for data channel $i$ by $\\tilde R_i(E)$ as follows:\n\n", "itemtype": "equation", "pos": 55470, "prevtext": "\n\nWe note that for digitisation of analogue electronic signals that represent\nspectral information, a binning ($=$channel) criterion of 1/3~FWHM or better is\noften adopted \\citep[e.g.][]{davelaar1979}, since one does not know a priori the\nlevel of significance of the measured signal (see the dashed line in\nFig.~\\ref{fig:databin}).\n\n\\subsection{Final remarks\\label{sect:datafinal}}\n\nWe have estimated conservative upper bounds for the required data bin size. In\nthe case of multiple resolution elements, we have determined the bounds for the\nworst case phase of the grid with respect to the data. In practice, it is not\nlikely that all resolution elements would have the worst possible alignment.\nHowever, we recommend using the conservative upper limits as given by\n(\\ref{eqn:gausdelta}).\n\nAnother issue is the determination of $N_r$, the number of events within the\nresolution element. We have argued that an upper limit to $N_r$ can be obtained\nby counting the number of events within one FWHM and multiplying it by the ratio\nof the total area under the lsf (should be equal to 1) to the area under the lsf\nwithin one FWHM (should be smaller than 1). For the Gaussian lsf, this ratio\nequals 1.314; for other lsfs,it is better to determine the ratio numerically and\nnot to use the Gaussian value 1.314.\n\nFor the Gaussian lsf, the resolution depends only weakly upon the number of\ncounts $N_r$ (see Fig.~\\ref{fig:databin}). For low count rate parts of the\nspectrum, the binning rule of 1/3 FWHM usually is too conservative.\n\nFinally, in some cases the lsf may contain more than one component. For example,\nthe grating spectra have higher order contributions. Other instruments have\nescape peaks or fluorescent contributions. In general it is advisable to\ndetermine the bin size for each of these components individually and simply take\nthe smallest bin size as the final one.\n\n\\section{A practical example\\label{sect:example}}\n\nIn order to demonstrate the benefits of the proposed binning schemes, We have\napplied them to the 85~ks Chandra LETGS observation of Capella mentioned\nearlier. The spectrum spans the 0.86--175.55~\\AA\\ range with a spectral\nresolution (FWHM) ranging between 0.040--0.076~\\AA. In Table~\\ref{tab:capella}\nwe show the number of data bins, model bins, and response matrix elements for\ndifferent rebinning schemes of this spectrum. For simplicity we consider only\nthe positive and negative first-order spectrum, and that the non-zero elements\nof the response matrix span a full range of four times the instrumental FWHM. We\napply the algorithms as described in this paper to generate the data and model\nenergy grids.\n\nThe highest resolution of 0.040~\\AA\\ occurs at short wavelengths, and the\nstrongest line is the \\ion{Fe}{xvii} blend at 17.06 and 17.10~\\AA, with a\nmaximum number of counts $N_r$ of 15\\,000 counts. \n\nThe first case we consider (case A afterwards) is that we adopt a data and model\ngrid with constant step size for the full range. Accounting for the maximum\n$N_r$ value, we have a data bin size of 0.02~\\AA\\ and a model bin size of\n0.14~m\\AA.\n\nCase B is similar to case A except that we account for the variable resolution\nof the instrument. This only decreases the number of bins by a factor of 1.375.\n\nIn case C we account for the number of counts in each resolution element, but we\nstill keep the ''classical'' approach of putting all photons at the centre of\nthe model bins. The number of resolution elements $R$ in the spectrum is 3110.\n\nIn case D we account for the average energy of the photons within a bin. The\nnumber of model bins and response elements needed drops by more than an order of\nmagnitude for this case, as compared to case C.\n\n\\begin{table}[!htbp]\n\\caption{Number of data bins, model bins, and response matrix elements for\ndifferent rebinning schemes of the Chandra LETGS spectrum of Capella.}\n\\label{tab:capella}\n\\centerline{\n\\begin{tabular}{lrrr}\n\\hline\\hline\nBinning & Data bins & Model bins & Response \\\\\n        &           &            & elements \\\\\n\\hline\nA & $8.52\\times 10^3$ & $1.26\\times 10^6$ & $1.01\\times 10^7$ \\\\\nB & $6.20\\times 10^3$ & $9.14\\times 10^5$ & $7.31\\times 10^6$ \\\\\nC & $5.12\\times 10^3$ & $1.23\\times 10^5$ & $9.84\\times 10^5$ \\\\\nD & $5.12\\times 10^3$ & $8.21\\times 10^3$ & $6.57\\times 10^4$ \\\\\n\\hline\\noalign{\\smallskip}\n\\end{tabular}\n}\n\\end{table}\n\n\\section{The response matrix\\label{sect:matrix}}\n\nWe have shown in the previous sections how the optimal model and data energy\ngrids can be constructed. We have proposed that for the evaluation of the model\nspectrum both the number of photons in each bin as well as their average energy\nshould be determined. We now determine how this impacts the concept of the\nresponse matrix.\n\nIn order to acquire high accuracy, we need to convolve the model spectrum for\nthe bin, approximated as a $\\delta$-function centred around $E_a$, with the\ninstrument response. In most cases we cannot do this convolution analytically,\nso we have to make approximations. From our expressions for the observed count\nspectrum $s(E^\\prime)$, eqns.~(\\ref{eqn:matrix_cont}) and (\\ref{eqn:rmatrix}),\nit can be easily derived that the number of counts or count rate for data\nchannel $i$ is given by\n\n", "index": 63, "text": "\\begin{equation}\nS_i = \\int\\limits_{\\displaystyle{E^\\prime_{i1}}}^{\\displaystyle{E^\\prime_{i2}}}\n    {\\rm d}E^\\prime \\int\\limits_{0}^{\\infty}  R(E^\\prime,E)f(E) {\\rm d}E,\n\\label{eqn:countint}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"S_{i}=\\int\\limits_{\\displaystyle{E^{\\prime}_{i1}}}^{\\displaystyle{E^{\\prime}_{%&#10;i2}}}{\\rm d}E^{\\prime}\\int\\limits_{0}^{\\infty}R(E^{\\prime},E)f(E){\\rm d}E,\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><msubsup><mi mathsize=\"142%\">E</mi><mrow><mi mathsize=\"140%\">i</mi><mo>\u2062</mo><mn mathsize=\"140%\">1</mn></mrow><mo mathsize=\"140%\" stretchy=\"false\">\u2032</mo></msubsup><msubsup><mi mathsize=\"142%\">E</mi><mrow><mi mathsize=\"140%\">i</mi><mo>\u2062</mo><mn mathsize=\"140%\">2</mn></mrow><mo mathsize=\"140%\" stretchy=\"false\">\u2032</mo></msubsup></munderover><mrow><mrow><mo>d</mo><msup><mi>E</mi><mo>\u2032</mo></msup></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>E</mi><mo>\u2032</mo></msup><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwe obtain\n\n", "itemtype": "equation", "pos": 55990, "prevtext": "\nwhere, as before, $E^\\prime_{i1}$ and $E^\\prime_{i2}$ are the formal channel\nlimits for data channel $i$ and $S_i$ is the observed count rate in counts/s for\ndata channel $i$. Interchanging the order of the integrations and defining the\nmono-energetic response for data channel $i$ by $\\tilde R_i(E)$ as follows:\n\n", "index": 65, "text": "\\begin{equation}\n\\tilde R_i(E)\\equiv \n\\int\\limits_{\\displaystyle{E^\\prime_{i1}}}^{\\displaystyle{E^\\prime_{i2}}}\n R(E^\\prime,E){\\rm d}E^\\prime,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\tilde{R}_{i}(E)\\equiv\\int\\limits_{\\displaystyle{E^{\\prime}_{i1}}}^{%&#10;\\displaystyle{E^{\\prime}_{i2}}}R(E^{\\prime},E){\\rm d}E^{\\prime},\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><msubsup><mi mathsize=\"142%\">E</mi><mrow><mi mathsize=\"140%\">i</mi><mo>\u2062</mo><mn mathsize=\"140%\">1</mn></mrow><mo mathsize=\"140%\" stretchy=\"false\">\u2032</mo></msubsup><msubsup><mi mathsize=\"142%\">E</mi><mrow><mi mathsize=\"140%\">i</mi><mo>\u2062</mo><mn mathsize=\"140%\">2</mn></mrow><mo mathsize=\"140%\" stretchy=\"false\">\u2032</mo></msubsup></munderover><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>E</mi><mo>\u2032</mo></msup><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>E</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nFrom the above equation we see that as long as we are interested in the observed\ncount rate $S_i$ of a given data channel $i$, we get that number by integrating\nthe model spectrum multiplied by the effective area $\\tilde R_i(E)$ for that\nparticular data channel. We have approximated $f(E)$ for each model bin $j$ by\n(\\ref{eqn:f1}), so that (\\ref{eqn:si_int}) becomes\n\n", "itemtype": "equation", "pos": 56158, "prevtext": "\nwe obtain\n\n", "index": 67, "text": "\\begin{equation}\nS_i = \\int\\limits_0^{\\infty}  f(E) \\tilde R_i(E) {\\rm d}E.\n\\label{eqn:si_int}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"S_{i}=\\int\\limits_{0}^{\\infty}f(E)\\tilde{R}_{i}(E){\\rm d}E.\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><mi>E</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere as before $E_{a,j}$ is the average energy of the photons in bin $j$ given\nby (\\ref{eqn:eq}), and $F_j$ is the total photon flux for bin $j$, in e.g.\nphotons\\,m$^{-2}$\\,s$^{-1}$. It is seen from (\\ref{eqn:sisum}) that we need to\nevaluate $\\tilde R_i$ not at the bin centre $E_j$ but at $E_{a,j}$, as expected.\n\nFormally we may split up $\\tilde R_i(E)$ in an effective area part $A(E)$ and a\nredistribution part $\\tilde r_i(E)$ in such a way that\n\n", "itemtype": "equation", "pos": 56636, "prevtext": "\nFrom the above equation we see that as long as we are interested in the observed\ncount rate $S_i$ of a given data channel $i$, we get that number by integrating\nthe model spectrum multiplied by the effective area $\\tilde R_i(E)$ for that\nparticular data channel. We have approximated $f(E)$ for each model bin $j$ by\n(\\ref{eqn:f1}), so that (\\ref{eqn:si_int}) becomes\n\n", "index": 69, "text": "\\begin{equation}\nS_i = \\sum\\limits_j^{} F_j \\tilde R_i(E_{a,j}),\n\\label{eqn:sisum}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"S_{i}=\\sum\\limits_{j}F_{j}\\tilde{R}_{i}(E_{a,j}),\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><mrow><msub><mi>F</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mrow><mi>a</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nWe have chosen our binning already in such a way that we have sufficient\naccuracy when the total effective area $A(E)$ within each model energy grid bin\n$j$ is approximated by a linear function of the photon energy $E$. Hence the\narf-part of $\\tilde R_i$ is of no concern.  We only need to check how the\nredistribution (rmf) part $\\tilde r_i$ can be calculated with sufficiently\naccuracy.  \n\nFor $\\tilde r_i$ the arguments are exactly the same as for $A(E)$ in the sense\nthat if we approximate it locally for each bin $j$ by a linear function of\nenergy, the maximum error that we make is proportional to the second derivative\nwith respect to $E$ of $\\tilde r_i$, cf. (\\ref{eqn:epsilon_max}).\n\nIn fact, for a Gaussian redistribution function the following is straightforward\nto prove:\n\n\\begin{theorem}\nAssume that for a given model energy bin $j$ all photons are located at the\nupper bin boundary $E_j+\\Delta /2$. Suppose that for all data channels we\napproximate $\\tilde r_i$ by a linear function of $E$, and the coefficients are\nthe first two terms in the Taylor expansion around the bin centre $E_j$. Then\nthe maximum error $\\delta$ made in the cumulative count distribution (as a\nfunction of the data channel) is given by (\\ref{eqn:de1}) in the limit of small\n$\\Delta$.\n\\label{the:rmfacc}\n\\end{theorem}\n\nThe importance of the above theorem is that it shows that the binning for the\nmodel energy grid that we have chosen in Sect.~\\ref{sect:modelbinning} is also\nsufficiently accurate so that $\\tilde r_i(E)$ can be approximated by a linear\nfunction of energy within a model energy bin $j$, for each data channel $i$. \nSince we already showed that our binning is also sufficient for a similar linear\napproximation to $A(E)$, it also follows that the total response $\\tilde R_i(E)$\ncan be approximated by a linear function. Hence, within the bin $j$ we use\n\n", "itemtype": "equation", "pos": 57185, "prevtext": "\nwhere as before $E_{a,j}$ is the average energy of the photons in bin $j$ given\nby (\\ref{eqn:eq}), and $F_j$ is the total photon flux for bin $j$, in e.g.\nphotons\\,m$^{-2}$\\,s$^{-1}$. It is seen from (\\ref{eqn:sisum}) that we need to\nevaluate $\\tilde R_i$ not at the bin centre $E_j$ but at $E_{a,j}$, as expected.\n\nFormally we may split up $\\tilde R_i(E)$ in an effective area part $A(E)$ and a\nredistribution part $\\tilde r_i(E)$ in such a way that\n\n", "index": 71, "text": "\\begin{equation}\n\\tilde R_i(E) = A(E) \\tilde r_i(E).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m1\" class=\"ltx_Math\" alttext=\"\\tilde{R}_{i}(E)=A(E)\\tilde{r}_{i}(E).\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>r</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\nInserting the above in (\\ref{eqn:sisum}) and comparing with (\\ref{eqn:rmatrix})\nfor the classical response matrix, we finally obtain \n\n", "itemtype": "equation", "pos": 59110, "prevtext": "\nWe have chosen our binning already in such a way that we have sufficient\naccuracy when the total effective area $A(E)$ within each model energy grid bin\n$j$ is approximated by a linear function of the photon energy $E$. Hence the\narf-part of $\\tilde R_i$ is of no concern.  We only need to check how the\nredistribution (rmf) part $\\tilde r_i$ can be calculated with sufficiently\naccuracy.  \n\nFor $\\tilde r_i$ the arguments are exactly the same as for $A(E)$ in the sense\nthat if we approximate it locally for each bin $j$ by a linear function of\nenergy, the maximum error that we make is proportional to the second derivative\nwith respect to $E$ of $\\tilde r_i$, cf. (\\ref{eqn:epsilon_max}).\n\nIn fact, for a Gaussian redistribution function the following is straightforward\nto prove:\n\n\\begin{theorem}\nAssume that for a given model energy bin $j$ all photons are located at the\nupper bin boundary $E_j+\\Delta /2$. Suppose that for all data channels we\napproximate $\\tilde r_i$ by a linear function of $E$, and the coefficients are\nthe first two terms in the Taylor expansion around the bin centre $E_j$. Then\nthe maximum error $\\delta$ made in the cumulative count distribution (as a\nfunction of the data channel) is given by (\\ref{eqn:de1}) in the limit of small\n$\\Delta$.\n\\label{the:rmfacc}\n\\end{theorem}\n\nThe importance of the above theorem is that it shows that the binning for the\nmodel energy grid that we have chosen in Sect.~\\ref{sect:modelbinning} is also\nsufficiently accurate so that $\\tilde r_i(E)$ can be approximated by a linear\nfunction of energy within a model energy bin $j$, for each data channel $i$. \nSince we already showed that our binning is also sufficient for a similar linear\napproximation to $A(E)$, it also follows that the total response $\\tilde R_i(E)$\ncan be approximated by a linear function. Hence, within the bin $j$ we use\n\n", "index": 73, "text": "\\begin{equation}\n\\tilde R_i(E_{a,j}) = \\tilde R_i(E_j) + \n\\frac{{\\rm d}\\tilde R_i }{ {\\rm d}E_j}(E_j)\\,\\, (E_{a,j} - E_j).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m1\" class=\"ltx_Math\" alttext=\"\\tilde{R}_{i}(E_{a,j})=\\tilde{R}_{i}(E_{j})+\\frac{{\\rm d}\\tilde{R}_{i}}{{\\rm d%&#10;}E_{j}}(E_{j})\\,\\,(E_{a,j}-E_{j}).\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mrow><mi>a</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><msub><mi>E</mi><mi>j</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mi>j</mi></msub><mo rspace=\"5.9pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>E</mi><mrow><mi>a</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>E</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $R_{ij}$ is the classical response matrix, evaluated for photons at the\nbin centre, and $R^\\prime_{ij}$ is its derivative with respect to the photon\nenergy $E_j$. In addition to the classical convolution, we thus get a second\nterm containing the relative offset of the photons with respect to the bin\ncentre. This is exactly what we intended to have when we argued that the number\nof bins could be reduced considerably by just taking that offset into account.\nIt is just at the expense of an additional derivative matrix, which means only a\nfactor of two more storage space and computation time. But for this extra\nexpenditure we gained much more storage space and computational efficiency\nbecause the number of model bins is reduced by a factor between 10--100.\n\nFinally we make a practical note. The derivative $R^\\prime_{ij}$ can be\ncalculated in practice either analytically or by numerical differentiation. In\nthe last case, it is more accurate to evaluate the derivative by taking the\ndifference at $E_j+\\Delta /2$ and $E_j-\\Delta /2$, and, wherever possible, not\nto evaluate it at one of these boundaries and the bin centre. This last\nsituation is perhaps only unavoidable at the first and last energy value.\n\nAlso, negative response values should be avoided. Thus it should be ensured that\n$R_{ij} + R^\\prime_{ij} h$ is  non-negative everywhere for $-\\Delta /2\\le h \\le\n\\Delta /2$. This can be translated into the constraint that $R^\\prime_{ij}$\nshould be limited always to the following interval:\n\n", "itemtype": "equation", "pos": 59383, "prevtext": "\n\nInserting the above in (\\ref{eqn:sisum}) and comparing with (\\ref{eqn:rmatrix})\nfor the classical response matrix, we finally obtain \n\n", "index": 75, "text": "\\begin{equation}\nS_i = \\sum\\limits_{j}^{} R_{ij} F_j + R^\\prime_{ij} (E_{a,j} - E_j) F_j,\n\\label{eqn:rmatrixder}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E38.m1\" class=\"ltx_Math\" alttext=\"S_{i}=\\sum\\limits_{j}R_{ij}F_{j}+R^{\\prime}_{ij}(E_{a,j}-E_{j})F_{j},\" display=\"block\"><mrow><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>F</mi><mi>j</mi></msub></mrow></mrow><mo>+</mo><mrow><msubsup><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>E</mi><mrow><mi>a</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>E</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>F</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nWhenever the calculated value of $R^\\prime_{ij}$ should exceed the above limits,\nthe limiting value should be inserted instead. This situation may happen, for\nexample for a Gaussian redistribution for responses a few $\\sigma$ away from the\ncentre, where the response falls off exponentially. However, the response\n$R_{ij}$ is small for those energies anyway, so this limitation is not serious;\nthis is only because we want to avoid negative predicted count rates.\n\n\\section{Constructing the response matrix\\label{sect:construct}}\n\nIn the previous section we outlined the basic structure of the response matrix.\nFor an accurate description of the spectrum, we need both the response matrix\nand its derivative with respect to the photon energy. We build on this to\nconstruct general response matrices for more complex situations.\n\n\\subsection{Dividing the response into components}\n\nUsually only the non-zero matrix elements of a response matrix are stored and\nused. This is done to save both storage space and computational time. The\nprocedure as used in XSPEC \\citep{arnaud1996} and the older versions of SPEX\n\\citep{kaastra1996} is  that for each model energy bin $j$ the relevant column\nof the response matrix is subdivided into groups. A group is a contiguous piece\nof the column with non-zero response. These groups are stored in a specific\norder; starting from the lowest energy, all groups belonging to a single photon\nenergy are stored before turning to the next higher photon energy.\n\nThis is not optimal neither in terms of storage space nor computational\nefficiency, as illustrated by the following example. For the XMM-Newton/RGS, the\nresponse consists of a narrow Gaussian-like core with a broad scattering\ncomponent due to the gratings in addition. The FWHM of the scattering component\nis $\\sim$10 times broader than the core of the response. As a result, if the\nresponse is saved as a classical matrix, we end up with one response group per\nenergy, namely the combined core and wings response because they overlap in the\nregion of the Gaussian-like core. As a result, the response file becomes large.\nThis is not necessary because the scattering contribution with its ten times\nlarger width needs to be specified only on a model energy grid with ten times\nfewer bins, compared to the Gaussian-like core. Thus, by separating out the core\nand the scattering contribution, the total size of the response matrix can be\nreduced by about a factor of 10. Of course, as a consequence each contribution\nneeds to carry its own model energy grid with it.\n\nTherefore we propose  subdividing the response matrix into its constituent\ncomponents. Then for each response component, the optimal model energy grid can\nbe determined according to the methods described in\nSect.~\\ref{sect:modelbinning}, and this model energy grid for the component can\nbe stored together with the response matrix part of that component. \nFurthermore, at any energy each component may have at most one response group.\nIf there were more response groups, the component should be subdivided further.\n\nIn X-ray detectors other than the RGS detector the subdivision could be\ndifferent. For example, with CCD detectors one could split up the response into\nfor components: the main diagonal, the Si fluorescence line, the escape peak,\nand a broad component due to split events.\n\n\\subsection{Complex configurations}\n\nIn most cases an observer analyses the data of a single source with a single\nspectrum and response for a single instrument. However, more complicated\nsituations may arise. Examples are:\n\n\\begin{enumerate}\n\n\\item A spatially extended source, such as a cluster of galaxies, with observed\nspectra extracted from different regions of the detector, but with the need to\nbe analysed simultaneously due to the overlap in point-spread function from one\nregion to the other. \n\n\\item For the RGS of XMM-Newton, the actual data space in the dispersion\ndirection is actually two-dimensional: the position $z$ of a photon on the\ndetector and its energy or pulse height $E^\\prime$ measured with the CCD\ndetector. Spatially extended X-ray sources are characterised by spectra that are\na function of both the energy $E$ and off-axis angle $\\phi$. The sky photon\ndistribution as a function of $(\\phi,E)$ is then mapped onto the\n$(z,E^\\prime)$-plane. One may analyse such sources by defining appropriate\nregions in both planes and evaluating the correct (overlapping) responses.\n\n\\item One may also fit simultaneously several time-dependent spectra using the\nsame response, for example data obtained during a stellar flare.\n\n\\end{enumerate}\n\nIt is relatively easy to model all these situations (provided that the\ninstrument is understood sufficiently, of course), as we show below.\n\n\\subsection{Sky sectors}\n\nFirst, the relevant part of the sky is subdivided into sectors, each sector\ncorresponding to a particular region of the source, for example a circular\nannulus centred around the core of a cluster or an arbitrarily shaped piece of a\nsupernova remnant, etc.\n\nA sector may also be a point-like region on the sky. For example if there is a\nbright point source superimposed upon the diffuse emission of the cluster, we\ncan define two sectors:  an extended sector for the cluster emission and a\npoint-like sector for the point source.  Both sectors might even overlap, as\nthis example shows.  \n\nAnother example is the two nearby components of the close binary $\\alpha$~Cen\nobserved with the XMM-Newton instruments with overlapping psfs of both\ncomponents.  In that case we would have two point-like sky sectors, each sector\ncorresponding to one of the double star components.\n\nThe model spectrum for each sky sector may and is different in general. For\nexample, in the case of an AGN superimposed upon a cluster of galaxies, one\nmight model the spectrum of the point-like AGN sector with a power law and the\nspectrum from the surrounding cluster emission with a thermal plasma model.\n\n\\subsection{Detector regions}\n\nThe observed count rate spectra are extracted in practice in different  regions\nof the detector. It is necessary to distinguish  the (sky) sectors and\n(detector) regions clearly. A detector region for the XMM-Newton EPIC camera\nwould be for example a rectangular box, spanning a certain number of pixels in\nthe x- and y-directions. It may also be a circular or annular extraction region\ncentred around a particular pixel of the detector, or whatever spatial filter is\ndesired. \n\nThe detector regions need not coincide with the sky sectors and  their number\nshould not be equal. A good example of this is again an AGN superimposed upon a\ncluster of galaxies.  The sky sector corresponding to the AGN is simply a point,\nwhile, for a finite instrumental psf, its extraction region at the detector is\nfor example a circular region centred around the pixel corresponding to the sky\nposition of the AGN.\n\nAlso, one could observe the same source with a number of different instruments\nand analyse the data simultaneously. In this case one would have only one sky\nsector but more detector regions, namely one for each participating instrument.\n\n\\subsection{Consequences for the response}\n\nIn all cases mentioned above, with more than one sky sector or more than one\ndetector region involved, the response contribution for each combination of sky\nsector - detector region must be generated. In the spectral analysis,  the model\nphoton spectrum is calculated for each sky sector, and all these model spectra\nare convolved with the relevant response contributions  to predict the count\nspectra for all detector regions. Each response contribution for a sky sector -\ndetector region combination itself may consist again of different response\ncomponents, as outlined in the previous subsection.\n\nCombining all this, the total response matrix then consists of a list of\ncomponents, each component corresponding to a particular sky sector and detector\nregion.  For example, we assume that the RGS has two response contributions: one\ncorresponding to the core of the lsf and the other to the scattering wings. We\nassume that this instrument observes a triple star where the instrument cannot\nresolve two of the three components.  The total response for this configuration\nthen consists of 12 components. These components include three sky sectors,\nassuming each star has its own characteristic spectrum, times two detector\nregions, including a spectrum extracted around the two unresolved stars and one\naround the other star, times two instrumental contributions (the lsf core and\nscattering wings).\n\n\\section{Proposed file formats}\\label{sect:formats}\n\nIn the previous sections it was shown how the optimal data and model binning can\nbe determined, and the corresponding optimal way to create the instrumental\nresponse. Now we focus on the possible data formats for these files.\n\nA widely used response matrix format is NASA's OGIP\\footnote{see the OGIP manual\nby K.A. Arnaud and I.M. George at\n\\url{https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/spectra/ogip_92_007/ogip_92_007.html}}\n\\citep{corcoran1995} FITS format \\citep{wells1981}. This is used, for example,\nas the data format for XSPEC. There are a few reasons that we propose not to\nadhere to the OGIP format in its present form here, as listed below:\n\n\\begin{enumerate}\n\n\\item The OGIP response file format, as it is currently defined, does not\naccount for the possibility of response derivatives. As was shown in previous\nsections, these derivatives are needed for the optimal binning. \n\n\\item As was shown in this work, it is more efficient to do the grouping within\nthe response matrix differently, splitting the matrix into components where each\ncomponent may have its own energy grid. This is not possible within the present\nOGIP format.\n\n\\end{enumerate}\n\n The FITS format used by the SPEX package version 2 obeys all the constraints\nthat we outlined in this paper \\citep{kaastra1996}. This format is described in\ndetail in the manual of that package.\\footnote{See \\url{www.sron.nl/spex}}\n\n\\section{Conclusion}\n\nIn this paper, we derived  the optimal bin size for both binning X-ray data as\nwell as binning the model energy grids used to calculate predicted X-ray\nspectra. \n\nThe optimal bin size for model energy grids, in the way it is usually used (i.e.\nputting all photons at the bin centres) requires a large number of model bins.\nWe have shown here that the number of model bins can be reduced by an order of\nmagnitude or more using a variable binning scheme where not only the number of\nphotons but also their average energy within a bin are calculated. The basic\nequations are (\\ref{eqn:app1}), (\\ref{eqn:de1ar}), and (\\ref{eqn:de1tot}).\n\nThe data binning depends mostly on the number of counts per resolution element,\nand to a lesser extent on the number of resolution elements. The commonly used\nprescription of binning to 1/3 of the instrumental FWHM is a safe number to use\nfor most practical cases, but in most cases a somewhat courser sampling can be\nused without losing any sensitivity to test spectral models. The recommended\nbinning scheme is given by (\\ref{eqn:databin}).\n\nCorrespondingly, the response matrix needs to be extended to two components: the\n`classical' part and its derivative with respect to photon energy (See\neq.~(\\ref{eqn:rmatrixder})). With these combined improvements a strong reduction\nof data storage needs and computational time is reached, which is very useful\nfor the analysis of high-resolution spectra with large numbers of bins and\ncomputational complex spectral models.\n\nFinally, we have outlined a few simple methods to reduce the size of the\nresponse matrix by splitting it into constituent components with different\nspectral resolution.\n\nAll these improvements are fully available in the spectral fitting package SPEX.\n\n\\begin{acknowledgements}\n\nSRON is supported financially by NWO, the Netherlands Organization for\nScientific Research. \n\n\\end{acknowledgements}\n\n\\bibliographystyle{aa}\n\\bibliography{newbin}\n\n\\begin{appendix}\n\n\\section{Testing models\\label{sect:testing}}\n\n\\subsection{How is it possible to test an approximation to the spectrum?\\label{sect:optimal}}\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{pow.cps}}\n\\caption{Probability distributions $g_i(T)$ for the example of $T=\\chi^2$,\ncorresponding to different spectral distribution functions $f_0$ (the ``true''\nmodel underlying the data), $f_1$ (an approximation to $f_0$ based on finite\nsampling distance $\\Delta$), and $f_2$ (an alternative physical model of the\nspectrum). The calculation is made for $100$ degrees of freedom and\n$\\alpha=\\beta=0.025$. Thus, 2.5\\% of the area under $g_0$ is to the right of the\ndashed vertical line, and 2.5\\% of the area under $g_2$ is to the left of this\nsame line. Further, 5\\% of the area under $g_1$ is to the right of the vertical\nline.}\n\\label{fig:pow}\n\\end{figure}\n\nWe make use of probability distributions to assess the accuracy of an\napproximation to the model spectrum. \n\nSuppose that $f_0(x)$ is the unknown, true probability density function (pdf)\ndescribing the observed spectrum. Further let $f_1(x)$ be an approximation to\n$f_0$ (discretely sampled with a bin size $\\Delta$).  Further let $f_2(x)$ be\nthe pdf of an alternative spectral model. Ideally, one now designs a statistical\ntest, based on some test statistics $T$. Choices for $T$ would be $\\chi^2$ in a\nchi-squared test, or $D$ (see later) in a Kolmogorov-Smirnov test. The\nhypothesis $H_0$ that $f_0(x)$ describes the spectrum is accepted if $T$ is\nsmaller than some threshold $c_\\alpha$, and rejected if $T>c_\\alpha$.\n\nThe probability $\\alpha$ that $H_0$ will be rejected if it is true is called the\nsize of the test. The probability that $H_0$ will be rejected if $H_2$ is true\nis called the power of the test and is denoted by $1-\\beta$ as follows:\n\\begin{eqnarray}\n{\\rm Prob}(T>c_\\alpha \\vert H_0) & = & \\alpha,\\nonumber \\label{eqn:crith0h2}\\\\\n{\\rm Prob}(T>c_\\alpha \\vert H_2) & = & 1-\\beta. \n\\end{eqnarray}\nAn ideal statistical test of $H_0$ against $H_2$ has $\\beta$ small (large\npower). However, in our case we are not interested in getting the most\ndiscriminative power between the alternatives $H_0$ and $H_2$, but we want to\nknow how well the approximation $f_1$ works in reaching the right conclusions\nabout the model. When we use $f_1$ instead of $f_0$, we want to reach the same\nconclusion in the majority of all cases. We define the size of the test of $H_1$\nversus $H_2$ to be $k\\alpha$ with the same $\\alpha$ as above. For $k=1$, both\ndistributions would of course be equal, but this happens only for bin size\n$\\Delta=0$.\n\nWe adopt as a working value $k=2$, $\\alpha=0.025$. That is, using a classical\n$\\chi^2$ test $H_1$ will be rejected against $H_2$ in 5\\% of the cases, the\nusual criterion in $\\chi^2$-analysis. This means that in practice $H_1$ reaches\nthe same correct conclusion as $H_0$ in $0.95/0.975\\simeq 0.975$ of all cases.\nFor an example, see Fig.~\\ref{fig:pow}.\n\n\\subsection{Statistical tests}\n\nUp to now we have not specified the test statistic $T$. It is common to do a\n$\\chi^2$-test. However, the $\\chi^2$-test has some drawbacks, in particular for\nsmall sample sizes. It can be shown, for instance, for a Gaussian distribution\nthat the test is rather insensitive in discriminating relatively large relative\ndifferences in the tails of the distribution. \n\nFor our purpose, a Kolmogorov-Smirnov test is more appropriate. This powerful,\nnon-parametric test is based upon the test statistic $D$ given by\n\n", "itemtype": "equation", "pos": 61023, "prevtext": "\nwhere $R_{ij}$ is the classical response matrix, evaluated for photons at the\nbin centre, and $R^\\prime_{ij}$ is its derivative with respect to the photon\nenergy $E_j$. In addition to the classical convolution, we thus get a second\nterm containing the relative offset of the photons with respect to the bin\ncentre. This is exactly what we intended to have when we argued that the number\nof bins could be reduced considerably by just taking that offset into account.\nIt is just at the expense of an additional derivative matrix, which means only a\nfactor of two more storage space and computation time. But for this extra\nexpenditure we gained much more storage space and computational efficiency\nbecause the number of model bins is reduced by a factor between 10--100.\n\nFinally we make a practical note. The derivative $R^\\prime_{ij}$ can be\ncalculated in practice either analytically or by numerical differentiation. In\nthe last case, it is more accurate to evaluate the derivative by taking the\ndifference at $E_j+\\Delta /2$ and $E_j-\\Delta /2$, and, wherever possible, not\nto evaluate it at one of these boundaries and the bin centre. This last\nsituation is perhaps only unavoidable at the first and last energy value.\n\nAlso, negative response values should be avoided. Thus it should be ensured that\n$R_{ij} + R^\\prime_{ij} h$ is  non-negative everywhere for $-\\Delta /2\\le h \\le\n\\Delta /2$. This can be translated into the constraint that $R^\\prime_{ij}$\nshould be limited always to the following interval:\n\n", "index": 77, "text": "\\begin{equation}\n-2R_{ij}/\\Delta E \\le  R^\\prime_{ij} \\le 2R_{ij}/\\Delta E.\n\\label{eqn:rdercrit}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m1\" class=\"ltx_Math\" alttext=\"-2R_{ij}/\\Delta E\\leq R^{\\prime}_{ij}\\leq 2R_{ij}/\\Delta E.\" display=\"block\"><mrow><mrow><mrow><mo>-</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>/</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>\u2062</mo><mi>E</mi></mrow></mrow><mo>\u2264</mo><msubsup><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>\u2264</mo><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>/</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>\u2062</mo><mi>E</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere $S_N(x)$ is the observed cumulative distribution for the sample of size\n$N$, and $F(x)$ is the cumulative distribution for the model that is tested. \nClearly, if $D$ is large, $F(x)$ is a bad representation of the observed data\nset. The statistic $D^\\prime\\equiv\\sqrt{N}D$ for large $N$ (typically 10 or\nlarger) has the limiting Kolmogorov-Smirnov distribution, with expectation value\n$\\sqrt{\\pi/2}\\ln 2$=0.86873 and standard deviation $\\pi\\sqrt{1/12-(\\ln\n2)^2/2\\pi}$=0.26033. The hypothesis that the true cumulative distribution is $F$\nwill be rejected if $D^\\prime >c_\\alpha$, where the critical value $c_\\alpha$\ncorresponds to a certain size $\\alpha$ of the test.  \n\nA good criterion for determining the optimal bin size $\\Delta$ is that the\nmaximum difference\n\n", "itemtype": "equation", "pos": 76666, "prevtext": "\nWhenever the calculated value of $R^\\prime_{ij}$ should exceed the above limits,\nthe limiting value should be inserted instead. This situation may happen, for\nexample for a Gaussian redistribution for responses a few $\\sigma$ away from the\ncentre, where the response falls off exponentially. However, the response\n$R_{ij}$ is small for those energies anyway, so this limitation is not serious;\nthis is only because we want to avoid negative predicted count rates.\n\n\\section{Constructing the response matrix\\label{sect:construct}}\n\nIn the previous section we outlined the basic structure of the response matrix.\nFor an accurate description of the spectrum, we need both the response matrix\nand its derivative with respect to the photon energy. We build on this to\nconstruct general response matrices for more complex situations.\n\n\\subsection{Dividing the response into components}\n\nUsually only the non-zero matrix elements of a response matrix are stored and\nused. This is done to save both storage space and computational time. The\nprocedure as used in XSPEC \\citep{arnaud1996} and the older versions of SPEX\n\\citep{kaastra1996} is  that for each model energy bin $j$ the relevant column\nof the response matrix is subdivided into groups. A group is a contiguous piece\nof the column with non-zero response. These groups are stored in a specific\norder; starting from the lowest energy, all groups belonging to a single photon\nenergy are stored before turning to the next higher photon energy.\n\nThis is not optimal neither in terms of storage space nor computational\nefficiency, as illustrated by the following example. For the XMM-Newton/RGS, the\nresponse consists of a narrow Gaussian-like core with a broad scattering\ncomponent due to the gratings in addition. The FWHM of the scattering component\nis $\\sim$10 times broader than the core of the response. As a result, if the\nresponse is saved as a classical matrix, we end up with one response group per\nenergy, namely the combined core and wings response because they overlap in the\nregion of the Gaussian-like core. As a result, the response file becomes large.\nThis is not necessary because the scattering contribution with its ten times\nlarger width needs to be specified only on a model energy grid with ten times\nfewer bins, compared to the Gaussian-like core. Thus, by separating out the core\nand the scattering contribution, the total size of the response matrix can be\nreduced by about a factor of 10. Of course, as a consequence each contribution\nneeds to carry its own model energy grid with it.\n\nTherefore we propose  subdividing the response matrix into its constituent\ncomponents. Then for each response component, the optimal model energy grid can\nbe determined according to the methods described in\nSect.~\\ref{sect:modelbinning}, and this model energy grid for the component can\nbe stored together with the response matrix part of that component. \nFurthermore, at any energy each component may have at most one response group.\nIf there were more response groups, the component should be subdivided further.\n\nIn X-ray detectors other than the RGS detector the subdivision could be\ndifferent. For example, with CCD detectors one could split up the response into\nfor components: the main diagonal, the Si fluorescence line, the escape peak,\nand a broad component due to split events.\n\n\\subsection{Complex configurations}\n\nIn most cases an observer analyses the data of a single source with a single\nspectrum and response for a single instrument. However, more complicated\nsituations may arise. Examples are:\n\n\\begin{enumerate}\n\n\\item A spatially extended source, such as a cluster of galaxies, with observed\nspectra extracted from different regions of the detector, but with the need to\nbe analysed simultaneously due to the overlap in point-spread function from one\nregion to the other. \n\n\\item For the RGS of XMM-Newton, the actual data space in the dispersion\ndirection is actually two-dimensional: the position $z$ of a photon on the\ndetector and its energy or pulse height $E^\\prime$ measured with the CCD\ndetector. Spatially extended X-ray sources are characterised by spectra that are\na function of both the energy $E$ and off-axis angle $\\phi$. The sky photon\ndistribution as a function of $(\\phi,E)$ is then mapped onto the\n$(z,E^\\prime)$-plane. One may analyse such sources by defining appropriate\nregions in both planes and evaluating the correct (overlapping) responses.\n\n\\item One may also fit simultaneously several time-dependent spectra using the\nsame response, for example data obtained during a stellar flare.\n\n\\end{enumerate}\n\nIt is relatively easy to model all these situations (provided that the\ninstrument is understood sufficiently, of course), as we show below.\n\n\\subsection{Sky sectors}\n\nFirst, the relevant part of the sky is subdivided into sectors, each sector\ncorresponding to a particular region of the source, for example a circular\nannulus centred around the core of a cluster or an arbitrarily shaped piece of a\nsupernova remnant, etc.\n\nA sector may also be a point-like region on the sky. For example if there is a\nbright point source superimposed upon the diffuse emission of the cluster, we\ncan define two sectors:  an extended sector for the cluster emission and a\npoint-like sector for the point source.  Both sectors might even overlap, as\nthis example shows.  \n\nAnother example is the two nearby components of the close binary $\\alpha$~Cen\nobserved with the XMM-Newton instruments with overlapping psfs of both\ncomponents.  In that case we would have two point-like sky sectors, each sector\ncorresponding to one of the double star components.\n\nThe model spectrum for each sky sector may and is different in general. For\nexample, in the case of an AGN superimposed upon a cluster of galaxies, one\nmight model the spectrum of the point-like AGN sector with a power law and the\nspectrum from the surrounding cluster emission with a thermal plasma model.\n\n\\subsection{Detector regions}\n\nThe observed count rate spectra are extracted in practice in different  regions\nof the detector. It is necessary to distinguish  the (sky) sectors and\n(detector) regions clearly. A detector region for the XMM-Newton EPIC camera\nwould be for example a rectangular box, spanning a certain number of pixels in\nthe x- and y-directions. It may also be a circular or annular extraction region\ncentred around a particular pixel of the detector, or whatever spatial filter is\ndesired. \n\nThe detector regions need not coincide with the sky sectors and  their number\nshould not be equal. A good example of this is again an AGN superimposed upon a\ncluster of galaxies.  The sky sector corresponding to the AGN is simply a point,\nwhile, for a finite instrumental psf, its extraction region at the detector is\nfor example a circular region centred around the pixel corresponding to the sky\nposition of the AGN.\n\nAlso, one could observe the same source with a number of different instruments\nand analyse the data simultaneously. In this case one would have only one sky\nsector but more detector regions, namely one for each participating instrument.\n\n\\subsection{Consequences for the response}\n\nIn all cases mentioned above, with more than one sky sector or more than one\ndetector region involved, the response contribution for each combination of sky\nsector - detector region must be generated. In the spectral analysis,  the model\nphoton spectrum is calculated for each sky sector, and all these model spectra\nare convolved with the relevant response contributions  to predict the count\nspectra for all detector regions. Each response contribution for a sky sector -\ndetector region combination itself may consist again of different response\ncomponents, as outlined in the previous subsection.\n\nCombining all this, the total response matrix then consists of a list of\ncomponents, each component corresponding to a particular sky sector and detector\nregion.  For example, we assume that the RGS has two response contributions: one\ncorresponding to the core of the lsf and the other to the scattering wings. We\nassume that this instrument observes a triple star where the instrument cannot\nresolve two of the three components.  The total response for this configuration\nthen consists of 12 components. These components include three sky sectors,\nassuming each star has its own characteristic spectrum, times two detector\nregions, including a spectrum extracted around the two unresolved stars and one\naround the other star, times two instrumental contributions (the lsf core and\nscattering wings).\n\n\\section{Proposed file formats}\\label{sect:formats}\n\nIn the previous sections it was shown how the optimal data and model binning can\nbe determined, and the corresponding optimal way to create the instrumental\nresponse. Now we focus on the possible data formats for these files.\n\nA widely used response matrix format is NASA's OGIP\\footnote{see the OGIP manual\nby K.A. Arnaud and I.M. George at\n\\url{https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/docs/spectra/ogip_92_007/ogip_92_007.html}}\n\\citep{corcoran1995} FITS format \\citep{wells1981}. This is used, for example,\nas the data format for XSPEC. There are a few reasons that we propose not to\nadhere to the OGIP format in its present form here, as listed below:\n\n\\begin{enumerate}\n\n\\item The OGIP response file format, as it is currently defined, does not\naccount for the possibility of response derivatives. As was shown in previous\nsections, these derivatives are needed for the optimal binning. \n\n\\item As was shown in this work, it is more efficient to do the grouping within\nthe response matrix differently, splitting the matrix into components where each\ncomponent may have its own energy grid. This is not possible within the present\nOGIP format.\n\n\\end{enumerate}\n\n The FITS format used by the SPEX package version 2 obeys all the constraints\nthat we outlined in this paper \\citep{kaastra1996}. This format is described in\ndetail in the manual of that package.\\footnote{See \\url{www.sron.nl/spex}}\n\n\\section{Conclusion}\n\nIn this paper, we derived  the optimal bin size for both binning X-ray data as\nwell as binning the model energy grids used to calculate predicted X-ray\nspectra. \n\nThe optimal bin size for model energy grids, in the way it is usually used (i.e.\nputting all photons at the bin centres) requires a large number of model bins.\nWe have shown here that the number of model bins can be reduced by an order of\nmagnitude or more using a variable binning scheme where not only the number of\nphotons but also their average energy within a bin are calculated. The basic\nequations are (\\ref{eqn:app1}), (\\ref{eqn:de1ar}), and (\\ref{eqn:de1tot}).\n\nThe data binning depends mostly on the number of counts per resolution element,\nand to a lesser extent on the number of resolution elements. The commonly used\nprescription of binning to 1/3 of the instrumental FWHM is a safe number to use\nfor most practical cases, but in most cases a somewhat courser sampling can be\nused without losing any sensitivity to test spectral models. The recommended\nbinning scheme is given by (\\ref{eqn:databin}).\n\nCorrespondingly, the response matrix needs to be extended to two components: the\n`classical' part and its derivative with respect to photon energy (See\neq.~(\\ref{eqn:rmatrixder})). With these combined improvements a strong reduction\nof data storage needs and computational time is reached, which is very useful\nfor the analysis of high-resolution spectra with large numbers of bins and\ncomputational complex spectral models.\n\nFinally, we have outlined a few simple methods to reduce the size of the\nresponse matrix by splitting it into constituent components with different\nspectral resolution.\n\nAll these improvements are fully available in the spectral fitting package SPEX.\n\n\\begin{acknowledgements}\n\nSRON is supported financially by NWO, the Netherlands Organization for\nScientific Research. \n\n\\end{acknowledgements}\n\n\\bibliographystyle{aa}\n\\bibliography{newbin}\n\n\\begin{appendix}\n\n\\section{Testing models\\label{sect:testing}}\n\n\\subsection{How is it possible to test an approximation to the spectrum?\\label{sect:optimal}}\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{pow.cps}}\n\\caption{Probability distributions $g_i(T)$ for the example of $T=\\chi^2$,\ncorresponding to different spectral distribution functions $f_0$ (the ``true''\nmodel underlying the data), $f_1$ (an approximation to $f_0$ based on finite\nsampling distance $\\Delta$), and $f_2$ (an alternative physical model of the\nspectrum). The calculation is made for $100$ degrees of freedom and\n$\\alpha=\\beta=0.025$. Thus, 2.5\\% of the area under $g_0$ is to the right of the\ndashed vertical line, and 2.5\\% of the area under $g_2$ is to the left of this\nsame line. Further, 5\\% of the area under $g_1$ is to the right of the vertical\nline.}\n\\label{fig:pow}\n\\end{figure}\n\nWe make use of probability distributions to assess the accuracy of an\napproximation to the model spectrum. \n\nSuppose that $f_0(x)$ is the unknown, true probability density function (pdf)\ndescribing the observed spectrum. Further let $f_1(x)$ be an approximation to\n$f_0$ (discretely sampled with a bin size $\\Delta$).  Further let $f_2(x)$ be\nthe pdf of an alternative spectral model. Ideally, one now designs a statistical\ntest, based on some test statistics $T$. Choices for $T$ would be $\\chi^2$ in a\nchi-squared test, or $D$ (see later) in a Kolmogorov-Smirnov test. The\nhypothesis $H_0$ that $f_0(x)$ describes the spectrum is accepted if $T$ is\nsmaller than some threshold $c_\\alpha$, and rejected if $T>c_\\alpha$.\n\nThe probability $\\alpha$ that $H_0$ will be rejected if it is true is called the\nsize of the test. The probability that $H_0$ will be rejected if $H_2$ is true\nis called the power of the test and is denoted by $1-\\beta$ as follows:\n\\begin{eqnarray}\n{\\rm Prob}(T>c_\\alpha \\vert H_0) & = & \\alpha,\\nonumber \\label{eqn:crith0h2}\\\\\n{\\rm Prob}(T>c_\\alpha \\vert H_2) & = & 1-\\beta. \n\\end{eqnarray}\nAn ideal statistical test of $H_0$ against $H_2$ has $\\beta$ small (large\npower). However, in our case we are not interested in getting the most\ndiscriminative power between the alternatives $H_0$ and $H_2$, but we want to\nknow how well the approximation $f_1$ works in reaching the right conclusions\nabout the model. When we use $f_1$ instead of $f_0$, we want to reach the same\nconclusion in the majority of all cases. We define the size of the test of $H_1$\nversus $H_2$ to be $k\\alpha$ with the same $\\alpha$ as above. For $k=1$, both\ndistributions would of course be equal, but this happens only for bin size\n$\\Delta=0$.\n\nWe adopt as a working value $k=2$, $\\alpha=0.025$. That is, using a classical\n$\\chi^2$ test $H_1$ will be rejected against $H_2$ in 5\\% of the cases, the\nusual criterion in $\\chi^2$-analysis. This means that in practice $H_1$ reaches\nthe same correct conclusion as $H_0$ in $0.95/0.975\\simeq 0.975$ of all cases.\nFor an example, see Fig.~\\ref{fig:pow}.\n\n\\subsection{Statistical tests}\n\nUp to now we have not specified the test statistic $T$. It is common to do a\n$\\chi^2$-test. However, the $\\chi^2$-test has some drawbacks, in particular for\nsmall sample sizes. It can be shown, for instance, for a Gaussian distribution\nthat the test is rather insensitive in discriminating relatively large relative\ndifferences in the tails of the distribution. \n\nFor our purpose, a Kolmogorov-Smirnov test is more appropriate. This powerful,\nnon-parametric test is based upon the test statistic $D$ given by\n\n", "index": 79, "text": "\\begin{equation}\n\\label{eqn:kolstat}\nD = \\max \\vert S_N(x) - F(x) \\vert,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E40.m1\" class=\"ltx_Math\" alttext=\"D=\\max|S_{N}(x)-F(x)|,\" display=\"block\"><mrow><mrow><mi>D</mi><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>S</mi><mi>N</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nshould be sufficiently small compared to the statistical fluctuations described\nby $D^\\prime$. Here $F_0(x)$ is the true underlying cumulative distribution and\n$F_1(x)$ is again its approximation, which we assume here are sampled on an\nenergy grid with spacing $\\Delta$.\n\nWe can apply exactly the same arguments as discussed before and use\nFig.~\\ref{fig:pow} with $T=D^\\prime$.\n\nThe Kolmogorov-Smirnov test was designed for comparing distributions based on\ndrawing $N$ independent values from the distribution $f(x)$; it is assumed that\n$x$ can have any real value within the interval where $f(x)>0$. In our case,\nbecause of the binning with finite bin size $\\Delta$, we first round the\nobserved values of $x$ to a set of discrete values and then perform a test. This\nmakes a significant difference; for a binned Gaussian redistribution function\n$f(x)$ with $N$ large, we found from numerical simulations that the average\nvalue of $D^\\prime$ equals 0.64, 0.69, 0.77, and 0.84 for $\\Delta$ equal to 0.5,\n0.3, 0.1, and 0.01, respectively. Thus, convergence to the asymptotic value for\n$D^\\prime$ of 0.86873 for $\\Delta\\rightarrow 0$ is rather slow.\n\n\\subsection{Analytical approximation\\label{sect:anap}}\n\nWe now make a simple analytical approximation for the optimal bin size. We first\ndetermine the maximum deviation of the approximation $F_1$ from $F_0$: \n$\\delta_k$ given by (\\ref{eqn:lambda_k}). This maximum occurs at a given value\nof $x_m$. We are interested in tests of $H_0$ versus alternatives $H_2$, and as\nwe have seen (Fig.~\\ref{fig:pow}), this requires knowledge of the distribution\n$g_0(D^\\prime)$ at high values of $D^\\prime$ (typically the highest 5\\% of the\ndistribution). Because the observed sample is drawn from the distribution\n$f_0(x)$, $D^\\prime$ can be reached at any value of $x$, wherever the random\nfluctuations reach the highest values. However, in the test using $f_1$ as an\napproximation to $f_0$, the $x$-values near $x_m$ have a higher probability of\nyielding the maximum because in addition to the random noise there is an offset\n$\\lambda_k$ due to the maximum difference between $F_0$ and $F_1$. In fact, we\nmay approximate the tail of the distributions $g_0$ and $g_1$ by\n\n", "itemtype": "equation", "pos": 77524, "prevtext": "\nwhere $S_N(x)$ is the observed cumulative distribution for the sample of size\n$N$, and $F(x)$ is the cumulative distribution for the model that is tested. \nClearly, if $D$ is large, $F(x)$ is a bad representation of the observed data\nset. The statistic $D^\\prime\\equiv\\sqrt{N}D$ for large $N$ (typically 10 or\nlarger) has the limiting Kolmogorov-Smirnov distribution, with expectation value\n$\\sqrt{\\pi/2}\\ln 2$=0.86873 and standard deviation $\\pi\\sqrt{1/12-(\\ln\n2)^2/2\\pi}$=0.26033. The hypothesis that the true cumulative distribution is $F$\nwill be rejected if $D^\\prime >c_\\alpha$, where the critical value $c_\\alpha$\ncorresponds to a certain size $\\alpha$ of the test.  \n\nA good criterion for determining the optimal bin size $\\Delta$ is that the\nmaximum difference\n\n", "index": 81, "text": "\\begin{equation} \n\\delta_k \\equiv  \\ \\max_m^{} \\vert F_1(m\\Delta) - F_0(m\\Delta) \\vert\n\\label{eqn:lambda_k}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E41.m1\" class=\"ltx_Math\" alttext=\"\\delta_{k}\\equiv\\ \\max_{m}|F_{1}(m\\Delta)-F_{0}(m\\Delta)|\" display=\"block\"><mrow><msub><mi>\u03b4</mi><mi>k</mi></msub><mo rspace=\"7.5pt\">\u2261</mo><mrow><munder><mi>max</mi><mi>m</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>F</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nWe now approximate the distribution function $g_0$ of the $D^\\prime$ statistic\nby the limiting Kolmogorov-Smirnov distribution $g_{\\rm{KS}}$. We thus have to\nsolve for\n\\begin{eqnarray}\n G_0(c_\\alpha)  &=& 1-\\alpha, \\\\\n G_1(c_\\alpha)  &=& 1-k\\alpha,\n\\end{eqnarray}\nwhich translates into\n\\begin{eqnarray}\n\\label{eqn:ksh0r}\n G_{\\rm{KS}}(c_\\alpha)  &=& 1-\\alpha, \\\\\n\\label{eqn:ksh1r}\n G_{\\rm{KS}}(c_\\alpha-\\lambda_k)  &=& 1-k\\alpha.\n\\end{eqnarray}\nFor our choice of $\\alpha=0.025$ and $k=2,$ we obtain $\\lambda_k = 0.122$.  We note\nthat the dependence on $\\alpha$ is weak: for $\\alpha=0.01$ or $\\alpha=0.05,$ we\nget $\\lambda_k$ values of 0.110 and 0.134, respectively. To get the optimal bin\nsize for any value of $N$, we calculate \n\n", "itemtype": "equation", "pos": 79853, "prevtext": "\nshould be sufficiently small compared to the statistical fluctuations described\nby $D^\\prime$. Here $F_0(x)$ is the true underlying cumulative distribution and\n$F_1(x)$ is again its approximation, which we assume here are sampled on an\nenergy grid with spacing $\\Delta$.\n\nWe can apply exactly the same arguments as discussed before and use\nFig.~\\ref{fig:pow} with $T=D^\\prime$.\n\nThe Kolmogorov-Smirnov test was designed for comparing distributions based on\ndrawing $N$ independent values from the distribution $f(x)$; it is assumed that\n$x$ can have any real value within the interval where $f(x)>0$. In our case,\nbecause of the binning with finite bin size $\\Delta$, we first round the\nobserved values of $x$ to a set of discrete values and then perform a test. This\nmakes a significant difference; for a binned Gaussian redistribution function\n$f(x)$ with $N$ large, we found from numerical simulations that the average\nvalue of $D^\\prime$ equals 0.64, 0.69, 0.77, and 0.84 for $\\Delta$ equal to 0.5,\n0.3, 0.1, and 0.01, respectively. Thus, convergence to the asymptotic value for\n$D^\\prime$ of 0.86873 for $\\Delta\\rightarrow 0$ is rather slow.\n\n\\subsection{Analytical approximation\\label{sect:anap}}\n\nWe now make a simple analytical approximation for the optimal bin size. We first\ndetermine the maximum deviation of the approximation $F_1$ from $F_0$: \n$\\delta_k$ given by (\\ref{eqn:lambda_k}). This maximum occurs at a given value\nof $x_m$. We are interested in tests of $H_0$ versus alternatives $H_2$, and as\nwe have seen (Fig.~\\ref{fig:pow}), this requires knowledge of the distribution\n$g_0(D^\\prime)$ at high values of $D^\\prime$ (typically the highest 5\\% of the\ndistribution). Because the observed sample is drawn from the distribution\n$f_0(x)$, $D^\\prime$ can be reached at any value of $x$, wherever the random\nfluctuations reach the highest values. However, in the test using $f_1$ as an\napproximation to $f_0$, the $x$-values near $x_m$ have a higher probability of\nyielding the maximum because in addition to the random noise there is an offset\n$\\lambda_k$ due to the maximum difference between $F_0$ and $F_1$. In fact, we\nmay approximate the tail of the distributions $g_0$ and $g_1$ by\n\n", "index": 83, "text": "\\begin{equation}\ng_1(x+\\lambda_k)\\simeq g_0(x).\n\\label{eqn:ks-shift}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E42.m1\" class=\"ltx_Math\" alttext=\"g_{1}(x+\\lambda_{k})\\simeq g_{0}(x).\" display=\"block\"><mrow><mrow><mrow><msub><mi>g</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><msub><mi>\u03bb</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2243</mo><mrow><msub><mi>g</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nWe then approximate $\\delta_k\\simeq \\epsilon$ and using (\\ref{eqn:lambda_k}) we\ncan determine for which bin size $\\Delta$ this occurs.\n\nIn evaluating (\\ref{eqn:lambda_k}) for a given cumulative distribution $F_0(x)$\nand $F_1(x)$, it is useful to remember that the extrema of $F_1(x)-F_0(x)$ occur\nwhere its derivative is zero, i.e. where $f_1(x) = f_0(x),$ where $f_0$ and\n$f_1$ are the corresponding probability density distributions.\n\n\\subsection{Monte Carlo calculations\\label{sect:mccalculations}}\n\nThe analytical approximations described above yield interesting asymptotic\nlimits. However, we need to test their accuracy and applicability. To do this,\nwe have used Monte Carlo simulations. For a set of sample sizes $N$ and bin\nsizes $\\Delta$, we  calculated the true distribution function $F_0$ and its\napproximation $F_1$. Using $F_0$, we have drawn a large number (typically\n$10^7$) of realisations. For each random realisation, we calculated $D^\\prime$\nagainst both models $F_0$ and $F_1$. Combining all runs, we determined the\ncumulative distributions $G_0(D^\\prime)$ and $G_1(D^\\prime)$. From these\ndistributions, we determined the critical value $c_\\alpha$ where $G_0(c_\\alpha)\n= 1-\\alpha$ (using $\\alpha=0.025$). For a given sample size $N$, we then varied\n$\\Delta$ in such a way that $G_1(c_\\alpha) = 1-k\\alpha$ with as before $k=2$.\nThis procedure then yields the optimal bin size $\\Delta$ as a function of $N$.\n\nThe Monte Carlo method works well for large values for $N$. Because we work with\n$\\alpha=0.025$, for $N\\leq 1/\\alpha$ the distribution $G_0(x)$ begins suffering\nfrom discontinuous steps due to the limited number $N$ of values that the\nobserved distribution $S_N(x)$ can have. In practice, this is not a major\nproblem as our results to be presented later show that in those cases the\nderived bin sizes become of the order of the FWHM of the instrumental response\nfunction anyway.\n\n\\subsection{Extension to complex spectra and instruments}\n\nThe estimates that we derived for determining bin sizes hold for any spectrum.\nThe optimal bin size $\\Delta$ as a function of $N$ can be determined using the\nanalytical approximations or numerical calculations outlined above.\n\nThis method is not always practical, however. First, the spectral resolution can\nbe a function of the energy, hence binning with a uniform bin width over the\nentire spectrum is not always desired.\n\nFurther, regions with poor statistics contain less information than regions with\ngood statistics, hence can be sampled on a much coarser grid.\n\nFinally, a spectrum often extends over a much wider energy band than the\nspectral resolution of the detector. In order to estimate $\\epsilon$, this would\nrequire that first the model spectrum, convolved with the instrument response,\nshould be used to determine $f_0(x)$. However, the true model spectrum is known\nin general only after spectral fitting. But the spectral fitting can be done\nonly after a bin size $\\Delta$ has been set.\n\nTo overcome these problems, we consider the instrumental line-spread function\n(lsf). This is the response (probability distribution) of mono-energetic photons\nthat the instrument measures. Most X-ray spectrometers have a lsf with a\ncharacteristic width $\\Delta E$ smaller than the incoming photon energy $E$, and\nwe only consider  instruments obeying that condition; there is little use in\nbinning data from instruments lacking spectral resolution. To be more precise,\nwe define a resolution element to be a region in data space corresponding to\nthe  FWHM of the detector; this FWHM is called $\\Delta E$.\n\nWe now consider a region $r$ of the spectrum with a width of the order of\n$\\Delta E$. In this region the observed spectrum is mainly determined by the\nconvolution of the instrumental lsf with the true model spectrum in an energy\nband with a width of the order of $\\Delta E$. We ignore here any substantial\ntails in the lsf, since they are of little importance in the determination of\nthe optimal bin size/resolution.\n\nIf all $N_r$ photons within the resolution element $r$ would be emitted at the\nsame energy $E_r$, and no photons would be emitted at any other energy, the\nobserved spectrum would be simply the lsf for energy $E_r$ multiplied by the\nnumber of photons $N_r$. For this situation, we can easily evaluate $\\lambda_k$\nfor a given binning. \n\nIf the spectrum is not mono-energetic, then the observed spectrum within the\nresolution element is the convolution of the full photon distribution with the\nlsf. In this case, the observed spectrum $S_r$ in resolution element $r$ is a\nweighted sum of the photon spectra $F_i$ in a number of usually adjacent\nresolution elements $i$, say $S_r = \\sum w_i F_i$. Because for a set of weights\n$w_i$ in general $\\sqrt{\\sum w_i^2} \\le \\sum w_i$, the squared modelling\nuncertainties $\\epsilon_r^2$ are smaller for the case of a broadband spectrum\nthan for a mono-energetic spectrum. It follows that if we determine\n$\\epsilon_r^2$ for the mono-energetic case from the sampling errors in the lsf,\nthis is an upper limit to the true value for $\\epsilon^2$ within the resolution\nelement $r$.\n\nWe now need to combine the resolution elements. Suppose that in each of the $R$\nresolution elements we perform a Kolmogorov-Smirnov test. For the maximum of\n$D^\\prime$ over all resolution elements $r$, we test \n\n", "itemtype": "equation", "pos": 80666, "prevtext": "\nWe now approximate the distribution function $g_0$ of the $D^\\prime$ statistic\nby the limiting Kolmogorov-Smirnov distribution $g_{\\rm{KS}}$. We thus have to\nsolve for\n\\begin{eqnarray}\n G_0(c_\\alpha)  &=& 1-\\alpha, \\\\\n G_1(c_\\alpha)  &=& 1-k\\alpha,\n\\end{eqnarray}\nwhich translates into\n\\begin{eqnarray}\n\\label{eqn:ksh0r}\n G_{\\rm{KS}}(c_\\alpha)  &=& 1-\\alpha, \\\\\n\\label{eqn:ksh1r}\n G_{\\rm{KS}}(c_\\alpha-\\lambda_k)  &=& 1-k\\alpha.\n\\end{eqnarray}\nFor our choice of $\\alpha=0.025$ and $k=2,$ we obtain $\\lambda_k = 0.122$.  We note\nthat the dependence on $\\alpha$ is weak: for $\\alpha=0.01$ or $\\alpha=0.05,$ we\nget $\\lambda_k$ values of 0.110 and 0.134, respectively. To get the optimal bin\nsize for any value of $N$, we calculate \n\n", "index": 85, "text": "\\begin{equation}\n\\epsilon \\equiv \\lambda_k / \\sqrt{N}.\n\\label{eqn:epsilon_lambda}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E43.m1\" class=\"ltx_Math\" alttext=\"\\epsilon\\equiv\\lambda_{k}/\\sqrt{N}.\" display=\"block\"><mrow><mrow><mi>\u03f5</mi><mo>\u2261</mo><mrow><msub><mi>\u03bb</mi><mi>k</mi></msub><mo>/</mo><msqrt><mi>N</mi></msqrt></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\nwhere as before $N_r$ is the number of photons in the resolution element $r$,\nand $D_r$ is given by (\\ref{eqn:kolstat}) over the interval $r$. All the\n$\\sqrt{N_r}D_r$ values are independently distributed, hence the cumulative\ndistribution function for their maximum is simply the $R$th power of a single\nstochastic variable with the relevant distribution. Therefore we obtain\n\\begin{eqnarray}\n\\label{eqn:ksh0rb}\n[ G_0(c_\\alpha) ]^R &=& 1-\\alpha, \\\\\n\\label{eqn:ksh1rb}\n[ G_1(c_\\alpha) ]^R &=& 1-k\\alpha.\n\\end{eqnarray}\n\n\\section{Determining the grids in practice}\n\nBased upon the theory developed in the previous sections, we present here a\npractical set of algorithms for the determination of both the optimal data\nbinning and the model energy grid determination. This may be helpful for the\npractical purpose of developing software for a particular instrument to\nconstruct the relevant response matrix.\n\n\\subsection{Creating the data bins}\n\nGiven an observed spectrum obtained by some instrument, the following steps\nshould be performed to generate an optimally binned spectrum.\n\n\\begin{enumerate}\n\n\\item Determine for each original data channel $i$ the nominal energy $E_{j0}$,\ndefined as the energy for which the response at channel $i$ reaches its maximum\nvalue. In most cases, this is the nominal channel energy.\n\n\\item Determine for each data channel $i$ the limiting points $(i1,i2)$ for the\nFWHM in such a way that $R_{k,j0}\\ge 0.5\\,R_{i,j0}$ for all $i1\\le k\\le i2,$\nwhile the range of $(i1,i2)$ is as broad as possible.\n\n\\item By (linear) interpolation, determine for each data channel the points\n(fractional channel numbers) $c1$ and $c2$ near $i1$ and $i2,$ where the\nresponse is actually half its maximum value. By virtue of the previous step, the\nabsolute difference $\\vert c1-i1\\vert$ and $\\vert c2-i2\\vert$ never can exceed\n1.\n\n\\item Determine for each data channel $i$ the FWHM $c_i$ in units of channels,\nby calculating $c2-c1$. Assure that $c_i$ is at least 1.\n\n\\item Determine for each original data channel $i$ the FWHM in energy units\n(e.g. in keV). Call this $W_i$. This and the previous steps may of course also\nbe performed directly using instrument calibration data.\n\n\\item Determine the number of resolution elements $R$ by the following\napproximation:\n\n\n", "itemtype": "equation", "pos": 86085, "prevtext": "\nWe then approximate $\\delta_k\\simeq \\epsilon$ and using (\\ref{eqn:lambda_k}) we\ncan determine for which bin size $\\Delta$ this occurs.\n\nIn evaluating (\\ref{eqn:lambda_k}) for a given cumulative distribution $F_0(x)$\nand $F_1(x)$, it is useful to remember that the extrema of $F_1(x)-F_0(x)$ occur\nwhere its derivative is zero, i.e. where $f_1(x) = f_0(x),$ where $f_0$ and\n$f_1$ are the corresponding probability density distributions.\n\n\\subsection{Monte Carlo calculations\\label{sect:mccalculations}}\n\nThe analytical approximations described above yield interesting asymptotic\nlimits. However, we need to test their accuracy and applicability. To do this,\nwe have used Monte Carlo simulations. For a set of sample sizes $N$ and bin\nsizes $\\Delta$, we  calculated the true distribution function $F_0$ and its\napproximation $F_1$. Using $F_0$, we have drawn a large number (typically\n$10^7$) of realisations. For each random realisation, we calculated $D^\\prime$\nagainst both models $F_0$ and $F_1$. Combining all runs, we determined the\ncumulative distributions $G_0(D^\\prime)$ and $G_1(D^\\prime)$. From these\ndistributions, we determined the critical value $c_\\alpha$ where $G_0(c_\\alpha)\n= 1-\\alpha$ (using $\\alpha=0.025$). For a given sample size $N$, we then varied\n$\\Delta$ in such a way that $G_1(c_\\alpha) = 1-k\\alpha$ with as before $k=2$.\nThis procedure then yields the optimal bin size $\\Delta$ as a function of $N$.\n\nThe Monte Carlo method works well for large values for $N$. Because we work with\n$\\alpha=0.025$, for $N\\leq 1/\\alpha$ the distribution $G_0(x)$ begins suffering\nfrom discontinuous steps due to the limited number $N$ of values that the\nobserved distribution $S_N(x)$ can have. In practice, this is not a major\nproblem as our results to be presented later show that in those cases the\nderived bin sizes become of the order of the FWHM of the instrumental response\nfunction anyway.\n\n\\subsection{Extension to complex spectra and instruments}\n\nThe estimates that we derived for determining bin sizes hold for any spectrum.\nThe optimal bin size $\\Delta$ as a function of $N$ can be determined using the\nanalytical approximations or numerical calculations outlined above.\n\nThis method is not always practical, however. First, the spectral resolution can\nbe a function of the energy, hence binning with a uniform bin width over the\nentire spectrum is not always desired.\n\nFurther, regions with poor statistics contain less information than regions with\ngood statistics, hence can be sampled on a much coarser grid.\n\nFinally, a spectrum often extends over a much wider energy band than the\nspectral resolution of the detector. In order to estimate $\\epsilon$, this would\nrequire that first the model spectrum, convolved with the instrument response,\nshould be used to determine $f_0(x)$. However, the true model spectrum is known\nin general only after spectral fitting. But the spectral fitting can be done\nonly after a bin size $\\Delta$ has been set.\n\nTo overcome these problems, we consider the instrumental line-spread function\n(lsf). This is the response (probability distribution) of mono-energetic photons\nthat the instrument measures. Most X-ray spectrometers have a lsf with a\ncharacteristic width $\\Delta E$ smaller than the incoming photon energy $E$, and\nwe only consider  instruments obeying that condition; there is little use in\nbinning data from instruments lacking spectral resolution. To be more precise,\nwe define a resolution element to be a region in data space corresponding to\nthe  FWHM of the detector; this FWHM is called $\\Delta E$.\n\nWe now consider a region $r$ of the spectrum with a width of the order of\n$\\Delta E$. In this region the observed spectrum is mainly determined by the\nconvolution of the instrumental lsf with the true model spectrum in an energy\nband with a width of the order of $\\Delta E$. We ignore here any substantial\ntails in the lsf, since they are of little importance in the determination of\nthe optimal bin size/resolution.\n\nIf all $N_r$ photons within the resolution element $r$ would be emitted at the\nsame energy $E_r$, and no photons would be emitted at any other energy, the\nobserved spectrum would be simply the lsf for energy $E_r$ multiplied by the\nnumber of photons $N_r$. For this situation, we can easily evaluate $\\lambda_k$\nfor a given binning. \n\nIf the spectrum is not mono-energetic, then the observed spectrum within the\nresolution element is the convolution of the full photon distribution with the\nlsf. In this case, the observed spectrum $S_r$ in resolution element $r$ is a\nweighted sum of the photon spectra $F_i$ in a number of usually adjacent\nresolution elements $i$, say $S_r = \\sum w_i F_i$. Because for a set of weights\n$w_i$ in general $\\sqrt{\\sum w_i^2} \\le \\sum w_i$, the squared modelling\nuncertainties $\\epsilon_r^2$ are smaller for the case of a broadband spectrum\nthan for a mono-energetic spectrum. It follows that if we determine\n$\\epsilon_r^2$ for the mono-energetic case from the sampling errors in the lsf,\nthis is an upper limit to the true value for $\\epsilon^2$ within the resolution\nelement $r$.\n\nWe now need to combine the resolution elements. Suppose that in each of the $R$\nresolution elements we perform a Kolmogorov-Smirnov test. For the maximum of\n$D^\\prime$ over all resolution elements $r$, we test \n\n", "index": 87, "text": "\\begin{equation}\n\\label{eqn:maxd}\n\\delta \\equiv \\max_r \\sqrt{N_r}D_r,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E44.m1\" class=\"ltx_Math\" alttext=\"\\delta\\equiv\\max_{r}\\sqrt{N_{r}}D_{r},\" display=\"block\"><mrow><mrow><mi>\u03b4</mi><mo>\u2261</mo><mrow><munder><mi>max</mi><mi>r</mi></munder><mo>\u2061</mo><mrow><msqrt><msub><mi>N</mi><mi>r</mi></msub></msqrt><mo>\u2062</mo><msub><mi>D</mi><mi>r</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05309.tex", "nexttext": "\n\n\\item Determine for each bin the effective number of events $N_r$ from the\nfollowing expressions:\n\n\\begin{eqnarray}\nC_r &=& \\sum\\limits_{k=i1-1}^{i2+1} C_k, \\\\\nh_r &=& \\sum\\limits_{k=1}^{N_c} R_{k,j0} / \n        \\sum\\limits_{k=i1-1}^{i2+1} R_{k,j0}, \\\\\nN_r &=& C_r h_r.\n\\label{eqn:nrcalc}\n\\end{eqnarray}\n\nIn the above, $C_k$ is the number of observed counts in channel $k$, and $N_c$\nis the total number of channels.  Take care that in the summations $i1-1$ and\n$i2+1$ are not out of their valid range $(1,N_c)$. If for some reason there is\nnot a first-order approximation available for the response matrix $R_{k,j}$ then\none might simply approximate $h_r$ from e.g. the Gaussian approximation, namely\n$h_r = 1.314$; cf. Sect.~\\ref{sect:datafinal}. This is justified since the\noptimal bin size is not a strong function of $N_r$; cf. Fig.~\\ref{fig:databin}.\nEven a factor of two error in $N_r$ in most cases does not affect the optimal\nbinning too much.\n\n\\item Using (\\ref{eqn:databin}), determine for each data channel the optimal\ndata bin size in terms of the FWHM. The true bin size $b_i$ in terms of number\nof data channels is obtained by multiplying this by $c_i$ calculated above\nduring step~4.  Make $b_i$ an integer number by ignoring all decimals (rounding\nit to below), but take care that $b_i$ is at least 1.\n\n\\item It is now time to merge the data channels into bins. In a loop over all\ndata channels, start with the first data channel. Name the current channel $i$.\nTake in principle all channels $k$ from channel $i$ to $i+b_i-1$ together.\nHowever, check that the bin size does not decrease significantly over the\nrebinning range. In order to do that check, determine for all $k$ between $i$\nand $i+b_i-1$ the minimum $a_i$ of $k+b_k$. Extend the summation only from\nchannel $i$ to $a_i-1$. In the next step of the merging, $a_i$ becomes the new\nstarting value $i$. The process is finished when $a_i$ exceeds $N_c$.\n\n\\end{enumerate}\n\n\\subsection{Creating the model bins}\n\nAfter having created the data bins, it is possible to generate the model energy\nbins. Some of the information obtained from the previous steps that created the\ndata bins is needed.\n\nThe following steps need to be taken:\n\n\\begin{enumerate}\n\n\\item Sort the FWHM of the data bins in energy units ($W_i$) as a function of\nthe corresponding energies $E_{j0}$. Use this array to interpolate any true FWHM\nlater. Also use the corresponding values of $N_r$ derived during that same\nstage. Alternatively, one may use directly the FWHM as obtained from calibration\nfiles.\n\n\\item Choose an appropriate start and end energy, e.g. the nominal lower and\nupper energy of the first and last data bin with an offset of a few FWHMs (for a\nGaussian, about 3 FWHM is sufficient). In the case of a lsf with broad wings\n(like the scattering due to the RGS gratings), it may be necessary to take an\neven broader energy range.\n\n\\item In a loop over all energies, as determined in the previous steps,\ncalculate the bin size in units of the FWHM using (\\ref{eqn:app1}).\n\n\\item Also, determine the effective area factor $\\frac{{\\rm d}\\ln E }{{\\rm d}\\ln\nA}$ for each energy; one may do that using a linear approximation.\n\n\\item For the same energies, determine the necessary bin width in units of the\nFWHM using eqn.~(\\ref{eqn:de1tot}). Combining this with the FWHMs determined\nabove gives for these energies the optimal model bin size $\\Delta E$ in keV.\n\n\\item Now the final energy grid can be created. Start at the lowest energy\n$E_{1,1}$, and interpolate in the $\\Delta E$ table the appropriate $\\Delta E\n(E_{1,1})$ value for the current energy.  The upper bin boundary $E_{2,1}$ of\nthe first bin is then simply $E_{1,1}+\\Delta E(E_{1,1})$.\n\n\\item Using the recursive scheme $E_{1,j} = E_{2,j-1}$, $E_{2,j} = E_{1,j} +\n\\Delta E(E_{1,j})$ determine all bin boundaries until the maximum energy has\nbeen reached.  The bin centres are simply defined as $E_j =\n0.5(E_{1,j}+E_{2,j})$.\n\n\\item Finally, if there are any sharp edges in the effective area of the\ninstrument, it is necessary to add these edges to the list of bin boundaries.\nAll edges should coincide with bin boundaries.\n\n\\end{enumerate}\n\n\\section{Oversampling of data\\label{sect:appc}}\n\nIf a spectrum is binned with  data bins that are too narrow, important\ninformation about the spectrum is lost. This was shown in a more formal way, for\ninstance by \\citet[sect.~2.6]{kaastra1999} using $\\chi^2$-statistics. The\nargument also holds for lower count statistics. Here we give a practical example\nusing C-statistics, but our conclusions are independent of the precise\nstatistics used.\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{chi-aa.ps}}\n\\caption{Simulated X-ray spectrum (data points) sampled with a bin size of\n0.01~\\AA\\ and fitted with a blackbody with a temperature of 50~eV (solid line).\nThe goodness of the fit is indicated in the figure.}\n\\label{fig:chibb1}\n\\end{figure}\n\nWe have simulated a spectrum consisting of a 50~eV blackbody plus a weak\nGaussian emission line at 30~\\AA\\ with a FWHM of 1~\\AA. The instrument\nover-samples the spectrum with a bin size of 0.01~\\AA. The spectrum was fit with\na simple blackbody (without a Gaussian line) using C-statistics and the fit was\n'perfect', with a C-stat value of 2775, to be compared to the expected value of\n$2825\\pm 76$ for the best-fit model. See Fig.~\\ref{fig:chibb1} for the fit. For\nthe fitting we have used the SPEX software, which calculates for each fit the\nexpected value of C-stat (in this case 2825) and its variance. This expected\nvalue is simple to calculated knowing the expected number of counts from the\nbest-fit model and the observed number of counts from the data in each data\nchannel.\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{chi-bb.ps}}\n\\caption{Same data as Fig.~\\ref{fig:chibb1} but rebinned by a factor of 100\n(diamonds). The same best-fit model of Fig.~\\ref{fig:chibb1} using a simple\nblackbody with a temperature of 50~eV is indicated with the red line. The blue\nline indicates the model for the source that entered the simulation: the sum of\na blackbody spectrum and a weak Gaussian line at 30~\\AA.}\n\\label{fig:chibb2}\n\\end{figure}\n\nThus, with this bin size, the observer would conclude that a simple blackbody\nspectrum yields an accurate representation of the data, and there is no need to\nadd more components because the present fit is already statistically acceptable.\nHowever, in Fig.~\\ref{fig:chibb2} we show the same data and model rebinned by a\nfactor of 100 (i.e. 1~\\AA\\ wide bins). It is seen only at this bin size that the\nsource has an emission line in addition to the simple blackbody model.\n\n\\begin{figure}[!htbp]\n\\resizebox{\\hsize}{!}{\\includegraphics[angle=-90]{chi.ps}}\n\\caption{Best-fit C-statistic of the spectrum of Fig.~\\ref{fig:chibb1} when\nfitted with a simple blackbody. We subtract here the expected value of C-stat\nand scale by its expected root-mean-square value, so that it is expressed in\nequivalent number of $\\sigma$. The dotted lines indicate the $\\pm 3\\sigma$\nrange; when the best fit is outside this $\\pm 3\\sigma$ range, most investigators would\nreject the model (in this case a simple blackbody). The FWHM used in this figure\nis the FWHM of the additional Gaussian component in the spectrum (1~\\AA).}\n\\label{fig:chibb3}\n\\end{figure}\n\nWe elaborate on this by rebinning the original spectrum of Fig.~\\ref{fig:chibb1}\nby different factors, fitting the spectrum with a single blackbody component and\ncomparing the best-fit C-stat with the relevant expected C-stat and its\nvariance. The results are shown in Fig.~\\ref{fig:chibb3}. It is clear that for\nall bin sizes less than about 0.04~\\AA\\ the observer cannot reject the simple\nblackbody hypothesis, and has no reason to add the Gaussian component to the\nmodel. Only for bin sizes of the order of 1~\\AA, the simple blackbody hypothesis\ncan be rejected at the $>15\\sigma$ significance level. For higher binning\nfactors, the bin size becomes larger than the width of the Gaussian line, and\nthe significance drops again.\n\nWhile the proper flux of the Gaussian line, including its uncertainties, is\nrecovered at the proper value, regardless of the binning, when the additional\nGaussian is introduced in the model, the detection and proof of existence of\nthis component is only achieved when the spectrum is optimally binned, in this\ncase (relatively weak line) with a bin size close to the FWHM of the Gaussian.\n\n\\end{appendix}\n\n\n", "itemtype": "equation", "pos": 88451, "prevtext": "\nwhere as before $N_r$ is the number of photons in the resolution element $r$,\nand $D_r$ is given by (\\ref{eqn:kolstat}) over the interval $r$. All the\n$\\sqrt{N_r}D_r$ values are independently distributed, hence the cumulative\ndistribution function for their maximum is simply the $R$th power of a single\nstochastic variable with the relevant distribution. Therefore we obtain\n\\begin{eqnarray}\n\\label{eqn:ksh0rb}\n[ G_0(c_\\alpha) ]^R &=& 1-\\alpha, \\\\\n\\label{eqn:ksh1rb}\n[ G_1(c_\\alpha) ]^R &=& 1-k\\alpha.\n\\end{eqnarray}\n\n\\section{Determining the grids in practice}\n\nBased upon the theory developed in the previous sections, we present here a\npractical set of algorithms for the determination of both the optimal data\nbinning and the model energy grid determination. This may be helpful for the\npractical purpose of developing software for a particular instrument to\nconstruct the relevant response matrix.\n\n\\subsection{Creating the data bins}\n\nGiven an observed spectrum obtained by some instrument, the following steps\nshould be performed to generate an optimally binned spectrum.\n\n\\begin{enumerate}\n\n\\item Determine for each original data channel $i$ the nominal energy $E_{j0}$,\ndefined as the energy for which the response at channel $i$ reaches its maximum\nvalue. In most cases, this is the nominal channel energy.\n\n\\item Determine for each data channel $i$ the limiting points $(i1,i2)$ for the\nFWHM in such a way that $R_{k,j0}\\ge 0.5\\,R_{i,j0}$ for all $i1\\le k\\le i2,$\nwhile the range of $(i1,i2)$ is as broad as possible.\n\n\\item By (linear) interpolation, determine for each data channel the points\n(fractional channel numbers) $c1$ and $c2$ near $i1$ and $i2,$ where the\nresponse is actually half its maximum value. By virtue of the previous step, the\nabsolute difference $\\vert c1-i1\\vert$ and $\\vert c2-i2\\vert$ never can exceed\n1.\n\n\\item Determine for each data channel $i$ the FWHM $c_i$ in units of channels,\nby calculating $c2-c1$. Assure that $c_i$ is at least 1.\n\n\\item Determine for each original data channel $i$ the FWHM in energy units\n(e.g. in keV). Call this $W_i$. This and the previous steps may of course also\nbe performed directly using instrument calibration data.\n\n\\item Determine the number of resolution elements $R$ by the following\napproximation:\n\n\n", "index": 89, "text": "\\begin{equation}\nR = \\sum\\limits_i^{} \\frac{1}{ c_i}. \n\\label{eqn:rdet}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E45.m1\" class=\"ltx_Math\" alttext=\"R=\\sum\\limits_{i}\\frac{1}{c_{i}}.\" display=\"block\"><mrow><mrow><mi>R</mi><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mfrac><mn>1</mn><msub><mi>c</mi><mi>i</mi></msub></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]