[{"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 4545, "prevtext": "\n\n\n\n\\maketitle\n\n\\begin{abstract}\nStata users have access to two easy-to-use implementations of Bayesian inference:  Stata's native {\\tt bayesmh} function and StataStan, which calls the general Bayesian engine Stan.  We compare these on two models that are important for education research:  the Rasch model and the hierarchical Rasch model.  Stan (as called from Stata) fits a more general range of models than can be fit by {\\tt bayesmh} and is also more scalable, in that it could easily fit models with at least ten times more parameters than could be fit using Stata's native Bayesian implementation.  In addition, Stan runs between two and ten times faster than {\\tt bayesmh} as measured in effective sample size per second:  that is, compared to Stan, it takes Stata's built-in Bayesian engine twice to ten times as long to get inferences with equivalent precision.  We attribute Stan's advantage in flexibility to its general modeling language, and its advantages in scalability and speed to an efficient sampling algorithm:  Hamiltonian Monte Carlo using the no-U-turn sampler.  In order to further investigate scalability, we also compared to the package Jags, which performed better than Stata's native Bayesian engine but not as well as StataStan.\n\nGiven its advantages in speed, generality, and scalability, and that Stan is open-source and can be run directly from Stata using StataStan, we recommend that Stata users adopt Stan as their Bayesian inference engine of choice.\n\\end{abstract}\n\n\\section{Introduction}\n\nStata is a statistical software package that is popular in social science, economics, and biostatistics.  In 2015, it became possible to routinely fit Bayesian models in Stata in two different ways:  (a) the introduction of Bayesian modeling commands in Stata software version 14 \\citep{stata14}, which work using the Metropolis-Hastings algorithm and Gibbs sampler; and (b) the release of StataStan, an interface to the open-source Bayesian software Stan \\citep{stanmanual}.\nPreviously, Bayesian methods were only available in Stata by user-written commands to interface with external software such as BUGS, JAGS, or MLwiN.\n\nAt the time of writing, the native Bayes implementation in Stata, {\\tt bayesmh}, allows a choice among 10 likelihood functions and 18 prior distributions. The built-in likelihoods are focused around regression models, and extensions to hierarchical (multilevel) models are possible with the inclusion of hyperpriors.  In addition, the user may write customized likelihood functions or customized posterior distributions.\n\nStan is a language for Bayesian inference which allows general continuous-parameter models, including all the models that can be fit in Stata's {\\tt bayesmh} and many others.  Stan can run from various data analysis environments such as Stata, R, Python, and Julia, and also has a command-line interface. Stan uses Hamiltonian Monte Carlo (HMC) and the no-U-turn sampler (NUTS) with also the options of variational inference \\citep{kucukelbir} and the L-BFGS optimization algorithm \\citep{nocedal}. The advantages of HMC and NUTS in speed, stability, and scalability over Metropolis-Hastings and Gibbs have been described elsewhere \\citep{neal, hoffman}. As a result of the Hamiltonian dynamics, HMC is rotation-invariant, making it well-suited to highly correlated parameters. It is also not slowed down by non-conjugate models.\n\nThe languages used by these packages are notably different. In Stata, each line of code starts with a command (\\texttt{bayesmh}) and is followed by arguments, typically one or more variable names with options following after a comma. This allows easy specification of certain standard models. However, flexible programming of complex models is difficult with Stata, whereas the more general specification possible in Stan facilitates more flexible model development. Stan works by translating a model into C++ and compiling that code.\n\nIn the present paper, we compare Stata's Bayesian implementation to Stan (as run from Stata) on some item response models.  These logistic regression, or Rasch, models, are popular in education research and in political science (where they are called ideal-point models).\n\n\\section{Models}\n\nWe fit the \\cite{rasch1960} model  using data simulated from the model.  We check that the Stata and Stan implementations give the same answer (modulo the inevitable Monte Carlo error of these stochastic algorithms) and we then compare the programs on speed and scalability.\n\nThe Rasch model can be written as,\n\n", "index": 1, "text": "\\begin{equation}\n\t\\rm{Pr} (y_{ip} = 1 | \\theta_p, \\delta_i) =\n\t\\rm{logit}^{-1} (\\theta_p + \\delta_i)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\rm{Pr}(y_{ip}=1|\\theta_{p},\\delta_{i})=\\rm{logit}^{-1}(\\theta_{p}+\\delta_{i})\" display=\"block\"><mrow><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">y</mi><mi>ip</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">p</mi></msub><mo>,</mo><msub><mi>\u03b4</mi><mi mathvariant=\"normal\">i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mi>logit</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">p</mi></msub><mo>+</mo><msub><mi>\u03b4</mi><mi mathvariant=\"normal\">i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\nwhere $y_{ip}=1$ if person $p$ responded to item $i$ correctly and is 0 otherwise.  The parameter $\\theta_p$ represents the latent ``ability'' of person $p$, and $\\delta_i$ is a parameter for item $i$. We shall consider a simple version of the model in which the abilities are modeled as exchangeable draws from a normal distribution with scale $\\sigma$. Ordinarily the model is specified such that $\\delta_i$ is subtracted from $\\theta_p$, giving  $\\delta_i$ the interpretation of item difficulty. We use the above parameterization because the alternative is more difficult to express in Stata's \\texttt{bayesmh}. We assign the following prior distributions to $\\delta_i$ and $\\sigma$:\n\n", "itemtype": "equation", "pos": 4661, "prevtext": "\n\n", "index": 3, "text": "\\begin{equation}\n\t\\theta_p \\sim \\mathrm{N} (0, \\sigma^2),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\theta_{p}\\sim\\mathrm{N}(0,\\sigma^{2}),\" display=\"block\"><mrow><mrow><msub><mi>\u03b8</mi><mi>p</mi></msub><mo>\u223c</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 5421, "prevtext": "\nwhere $y_{ip}=1$ if person $p$ responded to item $i$ correctly and is 0 otherwise.  The parameter $\\theta_p$ represents the latent ``ability'' of person $p$, and $\\delta_i$ is a parameter for item $i$. We shall consider a simple version of the model in which the abilities are modeled as exchangeable draws from a normal distribution with scale $\\sigma$. Ordinarily the model is specified such that $\\delta_i$ is subtracted from $\\theta_p$, giving  $\\delta_i$ the interpretation of item difficulty. We use the above parameterization because the alternative is more difficult to express in Stata's \\texttt{bayesmh}. We assign the following prior distributions to $\\delta_i$ and $\\sigma$:\n\n", "index": 5, "text": "\\begin{equation}\n  \\delta_i \\sim \\mathrm{N}(0, 10)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\delta_{i}\\sim\\mathrm{N}(0,10)\" display=\"block\"><mrow><msub><mi>\u03b4</mi><mi>i</mi></msub><mo>\u223c</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>10</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\nThese priors closely match those given in the Rasch model example in the Stata 14 manual \\cite{stata14}.  In general we do not recommend this sort of inverse-gamma prior, as it can be more informative than users realize \\citep{priors}, but we use it here to be consistent with Stata's documentation.  It is easy enough in Stan (or StataStan) to switch in other priors.\n\nA natural hierarchical extension of the Rasch model adds a hyperprior for $\\delta_i$ so that,\n\n", "itemtype": "equation", "pos": 5487, "prevtext": "\n\n", "index": 7, "text": "\\begin{equation}\n  \\sigma^2 \\sim \\operatorname{Inv-Gamma}(1, 1)\n.\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\sigma^{2}\\sim\\operatorname{Inv-Gamma}(1,1).\" display=\"block\"><mrow><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u223c</mo><mrow><mrow><mi>Inv</mi><mo>-</mo><mi>Gamma</mi></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 6031, "prevtext": "\nThese priors closely match those given in the Rasch model example in the Stata 14 manual \\cite{stata14}.  In general we do not recommend this sort of inverse-gamma prior, as it can be more informative than users realize \\citep{priors}, but we use it here to be consistent with Stata's documentation.  It is easy enough in Stan (or StataStan) to switch in other priors.\n\nA natural hierarchical extension of the Rasch model adds a hyperprior for $\\delta_i$ so that,\n\n", "index": 9, "text": "\\begin{equation}\n\t\\mathrm{Pr} (y_{ip} = 1 | \\theta_p, \\delta_i) =\n\t\\mathrm{logit}^{-1} (\\theta_p + \\delta_i)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{Pr}(y_{ip}=1|\\theta_{p},\\delta_{i})=\\mathrm{logit}^{-1}(\\theta_{p}+%&#10;\\delta_{i})\" display=\"block\"><mrow><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>p</mi></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><msub><mi>\u03b8</mi><mi>p</mi></msub><mo>,</mo><msub><mi>\u03b4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mi>logit</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi>p</mi></msub><mo>+</mo><msub><mi>\u03b4</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 6155, "prevtext": "\n\n", "index": 11, "text": "\\begin{equation}\n\t\\theta_p \\sim \\mathrm{N} (0, \\sigma^2)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\theta_{p}\\sim\\mathrm{N}(0,\\sigma^{2})\" display=\"block\"><mrow><msub><mi>\u03b8</mi><mi>p</mi></msub><mo>\u223c</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\nwhere $\\mu$ is the model intercept. Persons and items are regarded as two sets of exchangeable draws. The prior distributions are\n\n", "itemtype": "equation", "pos": 6227, "prevtext": "\n\n", "index": 13, "text": "\\begin{equation}\n\t\\delta_i \\sim \\mathrm{N} (\\mu, \\tau^2),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\delta_{i}\\sim\\mathrm{N}(\\mu,\\tau^{2}),\" display=\"block\"><mrow><mrow><msub><mi>\u03b4</mi><mi>i</mi></msub><mo>\u223c</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bc</mi><mo>,</mo><msup><mi>\u03c4</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 6430, "prevtext": "\nwhere $\\mu$ is the model intercept. Persons and items are regarded as two sets of exchangeable draws. The prior distributions are\n\n", "index": 15, "text": "\\begin{equation}\n\t\\mu \\sim \\mathrm{N}(0, 10)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\mu\\sim\\mathrm{N}(0,10)\" display=\"block\"><mrow><mi>\u03bc</mi><mo>\u223c</mo><mrow><mi mathvariant=\"normal\">N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>10</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 6490, "prevtext": "\n\n", "index": 17, "text": "\\begin{equation}\n\t\\tau^2 \\sim \\operatorname{Inv-Gamma}(1, 1)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\tau^{2}\\sim\\operatorname{Inv-Gamma}(1,1)\" display=\"block\"><mrow><msup><mi>\u03c4</mi><mn>2</mn></msup><mo>\u223c</mo><mrow><mrow><mi>Inv</mi><mo>-</mo><mi>Gamma</mi></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03443.tex", "nexttext": "\n\n\n\\section{Methods}\n\nWe simulated data from the above model with $500$ persons answering  $20$ items. For true values of $\\delta_i$, we assigned equally-spaced values from $-1.5$ to $1.5$, and we set the true $\\sigma$ to 1.\n\nWe set up the Rasch and hierarchical Rasch models in a similar manner, running four chains in series in Stan version 2.8 and Stata version 14.1. We drew initial values for the chains from independent uniform distributions $-1$ to 1 on the location parameters\n\t$\\mu^{(0)}$,\n\t$\\delta^{(0)}$, and\n\t$\\theta^{(0)}$; and uniform distributions from 0 to 2 on the scale parameters\n\t$\\sigma^{(0)}$ and\n\t$\\tau^{(0)}$.\nWe assigned all $\\delta_i$'s identical starting values for each chain, and the same for the $\\theta_p$'s. The reason for this (admittedly unusual) choice is that this approach is much easier to employ with \\texttt{bayesmh}. We used the same starting values for both Stan and Stata (and in the scalability comparison described below, for JAGS).  These item-response models were not sensitive to starting values.\n\nWe ran ten chains for 2500 discarded warm-up iterations and 2500 posterior draws each.\nFor timing purposes, we ran all chains in serial, thus eliminating one of Stan's advantages which is that it can automatically run multiple chains in parallel on a multi-core machine, regardless of the version of Stata.\nWe provide the Stan programs and Stata function calls in the appendix. The options specified for the Stata function call for the Rasch model are nearly identical to those in the example provided in the Stata manual \\cite{stata14}.\n\nWe monitored convergence for each parameter using the $\\widehat R$ statistic, which is a rough estimate of the square root of the ratio of overall (across chains) posterior variance to within-chain posterior variance \\citep{bda3}. Values of $\\widehat R$ near $1$ imply convergence, while greater values indicate non-convergence. Values less than $1.1$ are generally considered acceptable. The efficiency of the estimations is evaluated by the estimated effective sample size per second, $\\hat n_{\\mathrm{eff}} / s$  \\citep{bda3}. The timings throughout excluded compiling time in Stan but included the warm-up iterations; in Stata's \\texttt{bayesmh} they cover the time that Stata takes to run the command for both warm-up and sampling.\n\nTo further investigate the scalability of the software, we carried out the same analyses on simulated data with 20 items and 100, 500, 1000, 5000 and 10,000 people. We compared Stan 2.8 (using StataStan) and Stata 14.1 \\texttt{bayesmh} as above and also the open-source software JAGS 4.0.0 \\citep{plummer} via the rjags package in R 3.2.2, and ran four chains in each instance.\n\n\n\\section{Results}\n\n\n\\begin{figure}\n\t\\centering\t\\includegraphics[width=\\textwidth]{neff_sec.pdf}\n\t\\caption{\\em Histograms of effective sample size per second for parameters in the Rasch and hierarchical Rasch models (20 item-level parameters $\\delta$ and 500 person-level parameters $\\theta$).  Stan (StataStan) is about four times more efficient than Stata's \\texttt{bayesmh} command.}\n\t\\label{fig:rasch-neff-sec}\n\\end{figure}\n\n\n\\begin{table}\n\t\\centering\n\t\n\n\\begin{tabular}{lccc}\n&  & Stata 14.1 bayesmh & StataStan \\\\ \nModel & Parameter & $n_\\mathrm{eff} / \\mathrm{sec}$ & $n_\\mathrm{eff} / \\mathrm{sec}$ \\\\ \n  \\hline\nRasch & $\\sigma^2$ & 0.8 & 5.4 \\\\ \n  Hierarchical Rasch & $\\mu$ & 2.3 & 7.8 \\\\ \n  Hierarchical Rasch & $\\sigma^2$ & 0.9 & 5.6 \\\\ \n  Hierarchical Rasch & $\\tau^2$ & 2.4 & 7.8 \n\\end{tabular}\n\n\t\\caption{\\em Efficiency statistics for the hyperparameters in the two models.  Stan (StataStan) is between three and seven times more efficient than Stata's native Bayesian engine (\\texttt{bayesmh}).}\n\t\\label{tab:table}\n\\end{table}\n\n\nFor the Rasch model, we ran Stan (StataStan) for ten chains of 5,000 iterations (first half as warm-up) in {$54$ } minutes; at that point, $\\widehat R$ was less than {$1.01$ } for all parameters. We ran Stata (\\texttt{bayesmh}) for ten chains of the same length in {$26$ } minutes; $\\widehat R$ was less than {$1.02$ } for all parameters. Convergence appears satisfactory for both. Figure~\\ref{fig:rasch-neff-sec} compares values of $\\hat n_{\\rm eff}/ s$ between Stan and Stata for $\\delta$ and $\\theta$. Table~\\ref{tab:table} provides the same information for the remaining parameter, $\\sigma$. Stan was also more efficient, with values of $\\hat n_{\\rm eff} / s$ between three and seven times those for Stata across all parameters.  Our timings include warmup but not compilation time.\n\nResults for the hierarchical Rasch model parallel those for the Rasch model. Estimation with Stan required {$54$ } minutes for the same number of chains and iterations, and $\\widehat R$ was less than {$1.01$ } for all parameters. Stata ran for {$26$ } minutes and yielded values of $\\widehat R$ less than {$1.01$ } for all parameters. Both estimations appear to have converged. Stan again was more than three times as efficient as Stata in terms of $\\hat n_{\\rm eff} / s$.\n\n\n\\begin{figure}\n\t\\centering\t\\includegraphics[width=.51\\textwidth]{rasch_theta1.png}\\includegraphics[width=.51\\textwidth]{hrasch_theta1.png}\n\t\\caption{\\em Time per effective sample size for the first $\\theta$ parameter in the Rasch and hierarchical Rasch models, plotted against the number of $\\theta$ parameters in the model.  Stan is faster than the alternatives and is more scalable in the sense of being able to fit larger models to more data.}\n\t\\label{fig:scale-rasch-theta}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\t\\includegraphics[width=.51\\textwidth]{rasch_delta1.png}\\includegraphics[width=.51\\textwidth]{hrasch_delta1.png}\n\t\\caption{\\em Time per effective sample size for the first $\\delta$ parameter in the Rasch and hierarchical Rasch models, plotted against the number of $\\theta$ parameters in the model. Stan (here, StataStan) again is consistently faster and scales better to larger problems.}\n\t\\label{fig:scale-rasch-delta}\n\\end{figure}\n\nIn scalability testing, all three packages showed an approximately linear relationship between time per effective sample and number of parameters on the logarithmic scale, but Stan was consistently more than twice faster than JAGS, and more than five times faster than Stata.  As the size of the data and models became large ($p > 1000$), Stata's \\texttt{bayesmh} and JAGS both failed, returning error messages related to memory availability, while StataStan completed up to the desired maximum of $p=10,\\!000$, and presumably could grow beyond this. Observed times per effective sample size are shown in Figures \\ref{fig:scale-rasch-theta} and \\ref{fig:scale-rasch-delta}.\n\n\\section{Discussion}\n\nThe implementation of adaptive Hamiltonian Monte Carlo in Stan is substantially more efficient than the adaptive Markov chain Monte Carlo algorithm featured in Stata for the Rasch and hierarchical Rasch models. We would expect the results to generalize to logistic generalized linear mixed models, given that these include the Rasch models as a special case \\citep{rijmen2003nonlinear, zheng2007estimating}.\n\nThe Stata interface to Stan, StataStan, operates by writing specified variables (as vectors), matrices and scalars from Stata to a text file and calls the command-line implementation of Stan. Progress is displayed inside Stata (even under Windows) and there is the option to write the Stan model inside a comment block in the Stata code (which Stata users call a do-file). Results can then be read back into Stata for diagnostics and saving in Stata's data format.\n\nIn conclusion, we find Stan to be faster than Stata's current implementation of Bayesian inference, which is no surprise given Stan's advanced algorithms and efficient autodifferentiation code. Given that Stan is open-source, and that Stan can be run directly from Stata using StataStan, we recommend that Stata users adopt Stan as their Bayesian inference engine of choice.\n\n\n\\bibliographystyle{plain}\n\\bibliography{references}\n\n\\section*{Appendix}\n\nHere is the code for the Rasch model in Stan.  As noted in the text, we would not usually use the inverse-gamma prior distribution on $\\sigma^2$; we do so to keep our model comparable to what is documented for Stata.\n\n\\begin{small}\n\\begin{verbatim}\ndata {\n  int<lower=1> N;\n  int<lower=1> I;\n  int<lower=1> P;\n  int<lower=1, upper=I> ii[N];\n  int<lower=1, upper=P> pp[N];\n  int<lower=0, upper=1> y[N];\n}\nparameters {\n  real<lower=0> sigmasq;\n  vector[I] delta;\n  vector[P] theta;\n}\nmodel {\n  vector[N] eta;\n  theta ~ normal(0, sqrt(sigmasq));\n  delta ~ normal(0, sqrt(10));\n  sigmasq ~ inv_gamma(1, 1);\n  for (n in 1:N)\n    eta[n] <- theta[pp[n]] + delta[ii[n]];\n  y ~ bernoulli_logit(eta);\n}\n\\end{verbatim}\n\\end{small}\nHere is the Stata call for the Rasch model:\n\\begin{small}\n\\begin{verbatim}\nbayesmh y i.item, noconstant reffects(person) likelihood(logit) ///\n  mcmcsize(2500) burnin(2500) ///\n  prior({y:i.person}, normal(0, {sigmasq})) ///\n  prior({y:i.item}, normal(0, 10)) ///\n  prior({sigmasq}, igamma(1,1)) ///\n  block({sigmasq}) block({y:i.item}, reffects)\n\\end{verbatim}\n\\end{small}\nAnd here is the JAGS code for the Rasch model:\n\\begin{small}\n\\begin{verbatim}\nmodel {\n  for (i in 1:I) {\n    delta[i] ~ dunif(-1e6, 1e6)\n  }\n  sigma ~ dunif(0, 1e6)\n  inv_sigma_sq <- pow(sigma, -2)\n  for (p in 1:P) {\n    theta[p] ~ dnorm(0, inv_sigma_sq)\n  }\n  for (n in 1:N) {\n  # eta on different scale here\n    logit(inv_logit_eta[n]) <- theta[pp[n]] + delta[ii[n]]\n    y[n] ~ dbern(inv_logit_eta[n])\n  }\n}\n\\end{verbatim}\n\\end{small}\nHere is the hierarchical Rasch model in Stan:\n\\begin{small}\n\\begin{verbatim}\ndata {\n  int<lower=1> N;\n  int<lower=1> I;\n  int<lower=1> P;\n  int<lower=1, upper=I> ii[N];\n  int<lower=1, upper=P> pp[N];\n  int<lower=0, upper=1> y[N];\n}\n  parameters {\n  real<lower=0> sigmasq;\n  real<lower=0> tausq;\n  real mu;\n  vector[I] delta;\n  vector[P] theta;\n}\nmodel {\n  vector[N] eta;\n  theta ~ normal(0, sqrt(sigmasq));\n  delta ~ normal(mu, sqrt(tausq));\n  mu ~ normal(0, sqrt(10));\n  sigmasq ~ inv_gamma(1, 1);\n  tausq ~ inv_gamma(1, 1);\n  for (n in 1:N)\n    eta[n] <- theta[pp[n]] + delta[ii[n]];\n  y ~ bernoulli_logit(eta);\n}\n\\end{verbatim}\n\\end{small}\nHere is the Stata call for the hierarchical Rasch model:\n\\begin{small}\n\\begin{verbatim}\nbayesmh y i.item, noconstant reffects(person) likelihood(logit) ///\n  mcmcsize(2500) burnin(2500) ///\n  prior({y:i.person}, normal(0, {sigmasq})) ///\n  prior({y:i.item}, normal({mu}, {tausq})) ///\n  prior({mu}, normal(0, 10)) ///\n  prior({sigmasq} {tausq}, igamma(1,1)) ///\n  block({sigmasq} {tausq} {mu}, split) block({y:i.item}, reffects)\n\\end{verbatim}\n\\end{small}\nAnd here is the JAGS code for the hierarchical Rasch model:\n\\begin{small}\n\\begin{verbatim}\nmodel {\n  for (i in 1:I) {\n    delta[i] ~ dunif(-1e6, 1e6)\n  }\n  sigma ~ dunif(0, 1e6)\n  inv_sigma_sq <- pow(sigma, -2)\n  for (p in 1:P) {\n    theta[p] ~ dnorm(0, inv_sigma_sq)\n  }\n  for (n in 1:N) {\n  # eta on different scale here\n    logit(inv_logit_eta[n]) <- theta[pp[n]] + delta[ii[n]]\n    y[n] ~ dbern(inv_logit_eta[n])\n  }\n}\n\\end{verbatim}\n\\end{small}\n\n\n\n", "itemtype": "equation", "pos": 6566, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation}\n\t\\sigma^2 \\sim \\operatorname{Inv-Gamma}(1, 1)\n.\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\sigma^{2}\\sim\\operatorname{Inv-Gamma}(1,1).\" display=\"block\"><mrow><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>\u223c</mo><mrow><mrow><mi>Inv</mi><mo>-</mo><mi>Gamma</mi></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]