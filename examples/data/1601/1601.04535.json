[{"file": "1601.04535.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 25528, "prevtext": "\n\n\\title{A nonlinear impact: evidences of causal effects of social media on market prices}\n\n\\titlerunning{A nonlinear impact: evidences of causal effects of social media on market prices}  \n\n\n\n\\author{Th\\'{a}rsis T. P. Souza\\inst{1,}\\footnote{Correspondence author: T.Souza@cs.ucl.ac.uk.} \\and Tomaso Aste\\inst{1,2}}\n\n\n\\authorrunning{Souza et al.} \n\n\n\\tocauthor{Souza et al.}\n\n\\institute{Department of Computer Science, UCL, Gower Street, London, WC1E 6BT, UK\\and\nSystemic Risk Centre, London School of Economics and Political Sciences, London,  WC2A 2AE, UK.}\n\n\\maketitle              \n\n\\begin{abstract}\nWe provide empirical evidence that suggests social media and stock markets have a nonlinear causal relationship.\nWe take advantage of an extensive data set composed of social media messages related to DJIA index components.\nBy using information-theoretic measures to cope for possible nonlinear causal coupling between social media and stock markets systems, \nwe point out stunning differences in the results with respect to linear coupling. \nTwo main conclusions are drawn: First,\nsocial media significant causality on stocks' returns are purely nonlinear in most cases; \nSecond, social media dominates the directional coupling with stock market, an effect not observable within linear modeling.\nResults also serve as empirical guidance on model adequacy \nin the investigation of sociotechnical and financial systems.\n\\keywords{financial markets, complex systems, social media, nonlinear causality, information theory}\n\\end{abstract}\n\n\\section{Introduction}\n\\label{Intro}\n\nInvestors' decisions are modulated not only by companies' fundamentals but also by personal beliefs, \npeers influence and information generated from news and the Internet. \nRational and irrational investor's behavior and their relation with the market efficiency hypothesis \\cite{JOFI:JOFI518}\nhave been largely debated in the economics and financial literature \\cite{shleifer2000inefficient}. \nHowever, it was only recently that the availability of vast amounts of data from online systems paved the way \nfor the large-scale investigation of investor's collective behavior in financial markets. \n\n\nTesting for nonlinear dependence is of great importance in financial econometrics due to its implications in model adequacy, market efficiency, and predictability \\cite{doi:10.1080/096031096334105}.\nTaking social media as a proxy for investor's collective attention over the stock market, \nwe provide empirical evidence that characterize social media impact on market prices as nonlinear.\n\nPrevious studies have investigated the predictive power of online expressed opinions\nand measures of collective attention on market movements .\nNews are perhaps the most explored source of information, especially after the availability of electronically\ntransmitted services and machine readable news \\cite{tetlock2007giving, tetlock2008more, Tobias:2013, Lillo2012, citeulike:11703961, \n2011arXiv1112.1051M, HestonRanjan:2014, RePEc:2014, mann2011321, Li2014826, Chan2003223, Zhang_tradingstrategies}.\nThe use of search engines \\cite{PreisCurme:2014,Preis5707, citeulike:12299800, mao2014quantifying, 10.1371/journal.pone.0040014,2011arXiv1112.1051M, JOFI:JOFI1679} \nand Wikipedia \\cite{wrap54525} are examples of the extension of this investigation to broader types of online systems. \nIn addition to that, social media and micro-blogging platforms \nplay an increasingly significant role as proxies of collective intelligence  and sentiment of the real world. \nNot only do they mimic real-world peer-to-peer relationships but they also provide a fine-grained real-time information channel \nthat include stories, facts and shifts in collective opinion. \nNonetheless, to what extent this information flood reflects financial dynamics is a relatively novel topic under great debate \n\\cite{Bollen20111, mao2014quantifying, Zhang_tradingstrategies, citeulike:13108056, Timm:2014, \n10.1371/journal.pone.0138441, YangTwitter:2014, Sehgal:2007:SSP:1335998.1336036, Antweiler+Frank:04a, \nRePEc:bla:jbfnac:v:41:y:2014:i:7-8:p:791-830, 2011arXiv1112.1051M, DBLP:journals/eswa/NasseriTC15,Dzeroski:2014, Liu:2015:SAP:2774253.2774354, 1507.00784}.\n\nRecent developments have shown the importance of Twitter as an information channel about financial markets.\nAn example is the U.S. Securities and Exchange Commission allowance of official company's disclosure via Twitter in compliance with Regulation Fair Disclosure \\cite{SEC:2013}.\nSeveral research evidences also indicate that Twitter may describe and predict financial dynamics. \nAmong the first and most influential works is Bollen et al. (2011) \\cite{Bollen20111}, where\nthe authors used emotion analytics to forecast movements in the DJIA index. \nLater on, in a report for the European Central Bank, the same authors \\cite{mao2014quantifying} \nshowed that Twitter collective opinion not only has predictive power over stocks' returns but it actually precedes changes in search volume (Google Trends), a known predictor of economic indicators.\nFurther, Zheludev et al. (2014) \\cite{citeulike:13108056} showed that Twitter can contain statistically-significant lead-time information about securities' returns, \nmost remarkably over\nfuture prices of the S\\&P500 index.\nSprenger et al. (2014) \\cite{Timm:2014} proposed a methodology that quantified the impact of Twitter messages \nin the market as well as identified different types of company specific events. \nSubsequent research by Ranco et al. (2015) \\cite{10.1371/journal.pone.0138441} \nreinforced these results while analyzing links among Twitter peaks, excess of stocks' returns and the identification of earnings announcements. \n\nThese recent works provide evidence that exogenous information gathered from sociotechnical systems may be useful to describe financial dynamics. \nHowever, the current body of the literature presents mixed results on the stocks' returns predictability. \nOn the one hand, some researches indicate predictability of price movements using News and social media \\cite{tetlock2007giving, tetlock2008more, Bollen20111, mao2014quantifying}.\nOn the other hand, other studies report weak results \\cite{10.1371/journal.pone.0138441, citeulike:13108056} suggesting that \nsocial media analytics have low power when used alone.  \nMoreover, the use of ad hoc functional forms and assumptions in different studies makes it difficult to draw \ngeneral conclusions about the nature of the relationship between sociotechnical systems and stock markets.\n\n\nWe take advantage of an information-theoretic framework to study the causality between social media and stock returns in a nonparametric way.\nWe detect directional and dynamical coupling while not assuming any particular type of interaction between the systems. \nTo our knowledge, our results provide the first empirical evidence that suggests social media and stock markets not only have significant lead-time coupling but also are dominated by nonlinear interactions.\n\n\\section{Data Analyzed}\nOur analysis is conducted on the 30 components of the Dow Jones Industrial Average (DJIA) index, which we monitored during the two-year period from March 31, 2012 to March 31, 2014. \nThe choice of these stocks was due to their representativeness of the stock market (see Supporting Information \\ref{sec:companies} for the complete list of companies). \nWe consider two streams of time series data: (i) market data, which are given at the daily stock price, and (ii) social media data analytics based on 1,767,997 Twitter messages.\nLet $P(t)$ be the closing price of an asset at day $t$, as financial variable we consider the stocks' daily log-returns: $R(t) = \\log{P(t)} - \\log{P(t-1)}$.\n\nWe consider Twitter sentiment analytics as a proxy for the collective opinion over a stock.\nAs Twitter sentiment analysis \\cite{1507.00955} per se is out of scope of this study, we build our analysis on top of Twitter data analytics supplied by PsychSignal.com \\cite{PsychSignal}.  \nFor this data set, artificial neural networks were used to classify the likelihood of a Twitter message being bullish/bearish towards a security, i.e., the probability of a message being indicative of a up/down movement over the mentioned stock.\nWe take the daily total number of bullish tweets related to a company as the social media time series $SM(t)$. \nFig. \\ref{fig:companies} shows the volume of bearish and bullish messages for the selected companies.   \nA company is defined to be related to a given message if its ticker-id is mentioned as a \\textit{cashtag}, i.e., with its name preceded by a dollar symbol, \ne.g., \\$CSCO for the company CISCO SYSTEMS INC. In Twitter, a \\textit{cashtag} is a standard way to refer to a listed security.\nSee Supporting Information \\ref{sec:Twitter} for further details on the Twitter data analytics.\n\n\n\\begin{figure}[!t]\n\\centering\n\\scalebox{0.5}{\\includegraphics{fig/volume.png}}\n\\caption{\\textbf{Volume of bearish and bullish Twitter messages mentioning a ticker of a stock component of the DJIA index.}\n}\n\\label{fig:companies}\n \\end{figure}\n\n\n\n\n\n\\section{Social Media and Stocks' Returns: Linear and Nonlinear Causality}\nWe investigate the characterization of causal inference between social media and stocks' returns under the notion of Granger (G-causality) \\cite{Wiener56, granger:econ}.\nWe test the null hypothesis of social media not causing stocks' returns.\nFirstly, we verify this hypothesis with a standard G-causality test under a linear vector-autoregressive framework.\nThis linear model is tested against misspecification via a BDS test \\cite{citeulike:9300127} which is a nonparametric method that is powerful to detect nonlinearity \\cite{Barnett97asingle-blind}.\nSecondly, we detect significant causalities in possible nonlinear dynamical interactions. \nThis is done without assuming any a priori type of interaction.\nWe consider Transfer Entropy (TE) as the measure for nonparametric causality.\nSince its introduction by Schreiber (2000) \\cite{PhysRevLett.85.461}, \nTE has been recognized as an important tool in the analysis of causal relationships in nonlinear systems \\cite{citeulike:1447442}.\nIt naturally detects directional and dynamical information \\cite{10.1371/journal.pone.0109462}. \nThis measure can be interpreted as the information flow between social media and future outcomes of stocks' returns at lag $\\Delta t$, controlled by current information on stocks' returns.\n\nConsistent with a nonparametric analysis, we estimate the TE significance via randomized permutation tests. \nIf the null hypothesis is rejected, there is evidence of nonlinear causality, otherwise we consider that there is no significant causality.\nThe hypothesis tests are performed for lags $\\Delta t$ ranging from 1 to 10 trading days. \nWe apply the Bonferroni correction to reduce the probability of a Type I (false positive) error due to multiple hypotheses testing.\n\n\n \\begin{figure}[!t]\n\\centering\n\\textbf{\\Large Social Media $ \\rightarrow $ Stocks' Returns}\\par\\medskip\n\\scalebox{0.585}{\\includegraphics{fig/SM-R-2.png}}\n\\caption{\n\\textbf{Demonstration that the causality between social media and stocks' returns are mostly nonlinear.}\nLinear causality test indicated that social media caused stock's returns only for 3 stocks.\nNonparametric analysis showed that almost 1/3 of the stocks rejected in the linear case have significant nonlinear causality.\nIn the nonlinear case, Transfer Entropy was used to quantify causal inference between the systems with randomized permutations test for significance estimation. \nIn the linear case, a standard linear G-causality test was performed with a F-test under a linear vector-autoregressive framework. \nA significant linear G-causality was accepted if its linear specification was not rejected by the BDS test. \np-values are adjusted with the Bonferroni correction. Significance is given at p-value $ < 0.05$. \n}\n\\label{fig:sigpoints-0}\n \\end{figure}\n\n\n \\begin{figure}[!t]\n\\centering\n\\scalebox{0.35}{\\includegraphics{fig/sigpoints-NEW.png}}\n\\caption{\\textbf{Social media causality on stock's return is mostly nonlinear in the next-day period.}\nFigure shows the number of companies with significant causality aggregated by lag. \nCausality between social media and next-day stocks' returns presents a stunning difference between linear and nonlinear cases.\nNonlinear analysis identify much higher causality in the first lag. \nHence, linear-constraints may be neglecting social media causality over stocks' returns, especially in the next-day period.\nFurther lags present a lower number of significant causalities in both methods.\np-values are adjusted with a Bonferroni correction to reduce the probability of a Type I (false positive) error due to multiple hypotheses testing.\nSignificance is given at p-value $ < 0.05$.\n}\n\\label{fig:sigpoints2}\n \\end{figure}\n\n \n\nFig. \\ref{fig:sigpoints-0} shows the significant causality links between social media and stocks' returns considering both cases: nonlinear (TE) and linear G-causality \\footnote{See \nSupporting Information \\ref{sec:table-causality} for the complete set of p-values obtained.}. \nLinear analysis discovers only three stocks with significant causality: INTEL CORP., NIKE INC. and WALT DISNEY CO. \nNonlinear analysis discovers that several other\nstocks have significant causality. In addition to the 3 stocks identified with significant linear causality, other 8 stocks present purely nonlinear causality.\n\nIn Fig. \\ref{fig:sigpoints2}, we show the number of stocks with significant causality aggregated by lag of interaction. \nThe causality between social media and next-day stocks' returns presents a stunning difference between linear and nonlinear cases. \nFrom linear G-causality one would say that there is significant causality between social media and\nnext-day stocks' movements for one stock only. Conversely, nonlinear measures indicated that 10 companies have significant causality in this direction. \nHigher delays show a drop on this number.\nThese results suggest that linear-constraints are neglecting social media causality over stocks' returns especially in the next-day period or in the short-term.\n\nThe low level of causality obtained under linear constraints is inline with results from similar\nstudies in the literature, where it was found that stocks' returns show weak causality links \\cite{Tobias:2013, Antweiler+Frank:04a} and\nsocial media sentiment analytics, at least when taken alone, have very small or no predictive power\n\\cite{10.1371/journal.pone.0138441} and do not have significant lead-time information about \nstock's movements for the majority of the stocks \\cite{citeulike:13108056}.\nContrariwise, results from the nonlinear analyses unveiled a much higher level of causality indicating that linear constraints may be neglecting \nthe relationship between social media and stock markets.\n\n\n \\begin{table}[!t]\n\\begin{center}\n\\centering\n\\caption{\\textbf{Nonlinearities found are non-trivial.} Test for linear adequacy for commonly used function forms in the relationship between social media and stocks' returns. Test for misspecification is performed with the BDS test.\n$x$ represents the standard linear regression of returns on the social media time series. $\\nabla x$ and $\\nabla^2 x$ are, respectively, the first and second differencing taken in both time series.\n$f(x,vol)$ represents a regression of returns on social media controlled by the stocks' returns daily volatility.\nIn the log-transformation we apply the function $\\log(1+x)$ in both time series. \nThe module $|x|$ is applied in the returns time-series which is then regressed over the original social media data.\nGARCH(1,1) and ARIMA(1,1,1) transformations were applied on returns, then we regressed the resulting residuals on the original social media time series. See Section \\ref{sec:funcs} for the description of the functional forms used.}\n\\label{tb:BDS}\n\\begin{tabular}{lcccccccc}\n\\hline\\hline\nTicker&$x$&$\\nabla x$ &$\\nabla^2 x$ & $f(x,vol)$ & $\\log(1+x)$ & $|x|$  & GARCH(1,1) & ARIMA(1,1,1) \\tabularnewline\n\\hline \nCSCO&\\scalebox{1.5}{$\\circ$}&$ $&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\bullet$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}\\tabularnewline\nMSFT&\\scalebox{1.5}{$\\circ$}&$ $&$ $&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\bullet$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}\\tabularnewline\nAXP&\\scalebox{1.5}{$\\circ$}&$ $&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}\\tabularnewline\nJPM&\\scalebox{1.5}{$\\circ$}&$ $&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\bullet$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}\\tabularnewline\nIBM&\\scalebox{1.5}{$\\circ$}&$ $&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&\\scalebox{1.5}{$\\circ$}&$ $&\\scalebox{1.5}{$\\circ$}&$ $\\tabularnewline\nV&$ $&$ $&\\scalebox{1.5}{$\\circ$}&$ $&$ $&$ $&$ $&$ $\\tabularnewline\nJNJ&$ $&$ $&$ $&$ $&$ $&$ $&$ $&$ $\\tabularnewline\nAAPL&$ $&$ $&$ $&$ $&$ $&$ $&$ $&$ $\\tabularnewline\n\\hline\n\\multicolumn{9}{c}{\\scalebox{1.5}{$\\circ$}: Not misspecified; \\scalebox{1.5}{$\\bullet$}: Not misspecified and with significant G-causality.}\n\\label{tb:func}\n\\end{tabular}\\end{center}\n\n\\end{table}\n\n\nFor the companies identified with nonlinear causality, we tested whether common functional forms and transformations used in the literature can explain the nonlinearities.\nWe checked model adequacy and causality significance for various functional forms listed in Table \\ref{tb:func}, where the results are also reported.\nThe original linear functional form is adequate for 5 companies but can not explain the nonlinear causality. \nSecond-order differencing makes a linear functional adequate for the company VISA, but turns Microsoft as misspecified. \nGARCH and ARIMA filtering were applied in a tentative to separate signal from noise and to linearize the original time series.\nNonetheless, significant causality was not observed.\nOther functional forms performed no better than the original linear specification a part from the absolute value transformation.\nIt is indeed known that social media and news analytics predict absolute changes in market prices \\cite{citeulike:13108056, Tobias:2013} better than stock's returns.\nThis functional form is a proxy for stock returns volatility and therefore it has higher predictability than stock returns. \nYet, half of the companies still had an unexplained nonlinear causality.\n\nIt is clear from the results obtained that the nonlinearities found can not be fully explained by \nreturns' volatility neither by naive transformations often employed in related studies. \nThis indicates that the nonlinear causality is nontrivial and that there is forecastable structure that can not be explained by commonly-used functional forms.\nTherefore, the impact of social media on market prices may be higher than currently reported in related studies, \nbecause commonly-used functional forms are hiding significant causality, that are here reveled instead with a nonparametric analysis.\n\n\n\\section{Quantifying the Direction of Information Flow}\n\nTransfer-entropy is an asymmetric measure, i.e., $T_{X \\rightarrow Y} \\neq T_{Y \\rightarrow X}$, and thus allows the quantification of directional coupling between systems. \nThe Net Information Flow is defined as $\\widehat{TE}_{X \\rightarrow Y} = TE_{X \\rightarrow Y} - TE_{Y \\rightarrow X}$. \nOne can interpret this quantity as a measure of dominant direction of information flow, i.e., \na positive result indicates a dominant information flow from $X$ to $Y$ compared to the other direction\nor, similarly, it indicates which system provides more predictive information about the other system \\cite{Michalowicz:2013:HDE:2601840}.\n\nTE has an intuitive interpretation under the notion of G-causality in the sense that social media may cause (future) stocks' returns only when \nit provides more information to stocks' returns than past stocks' returns themselves. \nIn fact, Barnett et al. (2009) \\cite{PhysRevLett.103.238701} showed that linear G-causality and Transfer Entropy are equivalent for Gaussian variables.\nThis result provides a direct mapping between the information-theoretic framework and the linear VAR approach of G-causality.\nHence, it is possible to estimate TE both in its general form and with its equivalent form for linear G-causality. \nThe former case is a nonparametric approach that is able to capture possible nonlinear coupling that are likely to be neglected in the latter case. \n\n\nWe can therefore quantify the Net Information Flow from social media to stocks' returns using both nonlinear and linear frameworks. \nWe investigated which direction of coupling is the strongest and to what extent the consideration of nonlinear dynamics affects the results compared to a linear-constrained analysis.\nFig. \\ref{fig:TE-NIF} A) shows the results for the linear case. \nWe observe an asymmetry of information, i.e., the systems are not coupled with the same amount of information flow in both directions.\nThe stocks are clearly divided in two groups of approximately same sizes. One group shows stocks with positive net information flow, \nindicating that social media provides more predictive information about the stock market than the opposite.\nA second group of stocks indicates the opposite, i.e., information flows more from stocks' returns to social media than in the other direction. In both cases, the absolute value of net information flow decreases with lag.\n\nSurprisingly, the consideration of nonlinear dynamics unveils a much different scenario. \nFig \\ref{fig:TE-NIF} B) shows the results of the same analysis without linear constraints. \nThe net information flow becomes positive for all stocks analyzed. \nThis result suggests that social media is the dominant information source indicating\nthat the information provided by social media contributes more to the description of stock markets dynamics than the opposite.\n\n\n\n\\begin{figure}[!t]\n\\centering\n\\scalebox{0.38}{\\includegraphics{fig/GC-TE-NIF.png}}\n\\caption{\\textbf{Evidence that linear constraints change to a great extent the direction of Information Flow between social media and stock market.}\nFigure shows the Net Information Flow from social media to stocks' returns: $\\widehat{TE}_{SM \\rightarrow R} = TE_{SM \\rightarrow R} - TE_{R \\rightarrow SM}$.\nIn A), Net Information Flow is estimated with linear constraints. Positive values indicate that $TE_{SM \\rightarrow R} > TE_{R \\rightarrow SM}$, \nthis is an evidence that information flows from social media to stock returns. \nContrariwise, negative values indicate that stock market provide more information about social media movements than the opposite.\nIn B), estimation of Net Information Flow considers nonlinear dynamics. All companies indicate a positive information flow from social media to stocks' returns. \nThis indicate that, when nonlinear dynamics are considered, the information flows predominantly from social media to stock market. \nWe observe a change of direction of information flow in about half of the companies, compared to the same analysis with linear constraints.\nFigure shows the stocks ranked by total Net Information Flow considering all lags, i.e., $\\sum_{\\Delta t = 1}^{10}{\\widehat{TE}_{SM \\rightarrow R}}$.\n}\n\\label{fig:TE-NIF}\n \\end{figure}\n\n\\section{Summary}\n\\label{sec:Conclusion}\n\nThe present study has revealed that social media has a significant nonlinear impact on stocks' returns.\n\nWe analyzed an extensive data set of social media analytics related to stocks components of the DJIA index.\nNonparametric tests for nonlinear specification and causality indicated\nthree major empirical findings: \n\\begin{enumerate}\n \\item The consideration of nonlinear dynamics increased the number \n of stocks with relevant social media signal from 1/10, in the linear case, to more than 1/3 indicating \n that social media significant causality on stocks' returns are purely nonlinear in most cases;\\\\\n \\item The nonlinearities found were nontrivial and could not be explained by common functional forms used in the literature. \n This indicates that the impact of social media on stocks' returns may be higher than currently reported in related studies;\\\\\n \\item Nonparametric analysis indicated that social media dominates the directional coupling with stock market; an effect not observable within linear constraints.\n\\end{enumerate}\n\n\nWe suggest that social media explanatory power on stock markets may be intensified if nonlinear dynamics are considered.\nIn this respect, we provided strong evidence that supports the use of social media as a valuable source of information about the stock market.\n\nFrom a methodological point of view, results indicate that a nonparametric approach is highly preferable for the \ninvestigation of causal relationships between sociotechnical and financial systems. \n\n\n\\section{Methods}\n\\label{sec:Method}\n\n\n\\subsection{BDS Test for Linear Misspecification}\n\nWhen applied to the residuals of a linear model, the BDS test \\cite{citeulike:9300127} is a powerful test to detect nonlinearity \\cite{Barnett97asingle-blind}.\nLet $\\epsilon_t = (\\epsilon_{t=1}, \\ldots, \\epsilon_{t=n})$ be\nthe residuals of the linear fitted model and define its $m$-embedding as\n$\\epsilon_t^m = (\\epsilon_{t}, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-m+1})$. The  $m$-embedding correlation integral is given by\n\n", "index": 1, "text": "\\begin{align}\nC_{m,n}(\\Delta \\epsilon) = \\frac{2}{k(k-1)}\\sum_{s = 1}^{t}{\\sum_{t=s}^{n}{ \\chi(\\| \\epsilon_s^m - \\epsilon_t^m \\|, \\Delta \\epsilon)    }},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{m,n}(\\Delta\\epsilon)=\\frac{2}{k(k-1)}\\sum_{s=1}^{t}{\\sum_{t=s%&#10;}^{n}{\\chi(\\|\\epsilon_{s}^{m}-\\epsilon_{t}^{m}\\|,\\Delta\\epsilon)}},\" display=\"inline\"><mrow><mrow><mrow><msub><mi>C</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>2</mn><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mi>s</mi></mrow><mi>n</mi></munderover></mstyle><mrow><mi>\u03c7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2225</mo><mrow><msubsup><mi>\u03f5</mi><mi>s</mi><mi>m</mi></msubsup><mo>-</mo><msubsup><mi>\u03f5</mi><mi>t</mi><mi>m</mi></msubsup></mrow><mo>\u2225</mo></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere $\\chi$ is an indicator function with $\\chi(\\| \\epsilon_s^m - \\epsilon_t^m \\|, \\Delta \\epsilon) = 1$ if \n$\\| \\epsilon_s^m - \\epsilon_t^m \\| < \\Delta \\epsilon$ and zero, otherwise.\nThe null hypothesis of the BDS test assumes that $\\epsilon_t$ is iid. In this case,\n\n", "itemtype": "equation", "pos": 25698, "prevtext": "\nand\n\n", "index": 3, "text": "\\begin{align}\nC_{m}(\\Delta \\epsilon) = \\lim_{n\\to\\infty} C_{m,n}(\\Delta \\epsilon),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{m}(\\Delta\\epsilon)=\\lim_{n\\to\\infty}C_{m,n}(\\Delta\\epsilon),\" display=\"inline\"><mrow><mrow><mrow><msub><mi>C</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>n</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><msub><mi>C</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nThe BDS statistic is a measure of the extent that this relation holds in the data. It is given by:\n\n", "itemtype": "equation", "pos": 26062, "prevtext": "\nwhere $\\chi$ is an indicator function with $\\chi(\\| \\epsilon_s^m - \\epsilon_t^m \\|, \\Delta \\epsilon) = 1$ if \n$\\| \\epsilon_s^m - \\epsilon_t^m \\| < \\Delta \\epsilon$ and zero, otherwise.\nThe null hypothesis of the BDS test assumes that $\\epsilon_t$ is iid. In this case,\n\n", "index": 5, "text": "\\begin{align}\nC_{m}(\\Delta \\epsilon) = C_{1}(\\Delta \\epsilon)^m.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{m}(\\Delta\\epsilon)=C_{1}(\\Delta\\epsilon)^{m}.\" display=\"inline\"><mrow><mrow><mrow><msub><mi>C</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>m</mi></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere $\\sigma_m(\\Delta \\epsilon)$ can be estimated as described in \\cite{citeulike:9300127}. \nThe null hypothesis of the BDS test indicates that the model tested is not misspecified and it is rejected at 5\\% significance level if $\\|V_m(\\Delta \\epsilon)\\| > 1.96$.\n\n$\\Delta \\epsilon$ is commonly set as a factor of the variance ($\\sigma_\\epsilon$) of $\\epsilon$. We report results for $\\Delta \\epsilon = \\sigma_\\epsilon/2$ and the embedding dimension $m = 2$. \nWe also performed tests for $\\Delta \\epsilon = \\sigma_\\epsilon$ and $m = 3$ with no significant differences in the results.\n\n\n\\subsection{Linear G-causality}\nConsider the linear vector-autoregressive (VAR) equations:\n\n", "itemtype": "equation", "pos": 26238, "prevtext": "\nThe BDS statistic is a measure of the extent that this relation holds in the data. It is given by:\n\n", "index": 7, "text": "\\begin{align}\nV_{m}(\\Delta \\epsilon) = \\sqrt{n}\\frac{C_{m}(\\Delta \\epsilon) - C_{1}(\\Delta \\epsilon)^m}{\\sigma_m(\\Delta \\epsilon)},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V_{m}(\\Delta\\epsilon)=\\sqrt{n}\\frac{C_{m}(\\Delta\\epsilon)-C_{1}(%&#10;\\Delta\\epsilon)^{m}}{\\sigma_{m}(\\Delta\\epsilon)},\" display=\"inline\"><mrow><mrow><mrow><msub><mi>V</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msqrt><mi>n</mi></msqrt><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>C</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>m</mi></msup></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwe test whether $SM$ G-causes $R$ by comparing the errors in the prediction of $R$ in the restricted and unrestricted regression models in Eq. \\eqref{eq:AR1} and Eq. \\eqref{eq:AR2}, respectively.\nSignificance estimation is performed via analysis of variance. \nWe indicated a significant causality if there is significant causality in at least one of the lags tested.\nWe adjusted the p-values with a Bonferroni correction to control for multiple hypotheses testing.\n\n\n\\subsection{Functional Forms Tested}\n\\label{sec:funcs}\n\nFunctional forms referenced in Table \\ref{tb:func} were used as following.\n\n\\subsubsection{Differencing: $\\nabla x$.} The first differencing is taken in both social media and returns time series.\n\n", "itemtype": "equation", "pos": 27060, "prevtext": "\nwhere $\\sigma_m(\\Delta \\epsilon)$ can be estimated as described in \\cite{citeulike:9300127}. \nThe null hypothesis of the BDS test indicates that the model tested is not misspecified and it is rejected at 5\\% significance level if $\\|V_m(\\Delta \\epsilon)\\| > 1.96$.\n\n$\\Delta \\epsilon$ is commonly set as a factor of the variance ($\\sigma_\\epsilon$) of $\\epsilon$. We report results for $\\Delta \\epsilon = \\sigma_\\epsilon/2$ and the embedding dimension $m = 2$. \nWe also performed tests for $\\Delta \\epsilon = \\sigma_\\epsilon$ and $m = 3$ with no significant differences in the results.\n\n\n\\subsection{Linear G-causality}\nConsider the linear vector-autoregressive (VAR) equations:\n\n", "index": 9, "text": "\\begin{align}\nR(t) &= {\\alpha} + \\sum^k_{\\Delta t=1}{{\\beta}_{\\Delta t} R(t-\\Delta t)} + \\epsilon_t, \\label{eq:AR1}\\\\\nR(t) &= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} R(t-\\Delta t)} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-\\Delta t)}+ \\widehat{\\epsilon}_t, \\label{eq:AR2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(t)\" display=\"inline\"><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\beta}_{\\Delta t}R(t-\\Delta t)}+%&#10;\\epsilon_{t},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\u03b1</mi><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mi>\u03b2</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mi>\u03f5</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(t)\" display=\"inline\"><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\widehat{\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta&#10;t%&#10;}R(t-\\Delta t)}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-\\Delta&#10;t%&#10;)}+\\widehat{\\epsilon}_{t},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo>^</mo></mover><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nThe second differencing $\\nabla^2 x$ was tested in analogous way.\n\n\\subsubsection{$f(x, vol)$.} Represents a regression of returns on social media controlled by the stocks' returns daily volatility.\n\n\n", "itemtype": "equation", "pos": 28106, "prevtext": "\nwe test whether $SM$ G-causes $R$ by comparing the errors in the prediction of $R$ in the restricted and unrestricted regression models in Eq. \\eqref{eq:AR1} and Eq. \\eqref{eq:AR2}, respectively.\nSignificance estimation is performed via analysis of variance. \nWe indicated a significant causality if there is significant causality in at least one of the lags tested.\nWe adjusted the p-values with a Bonferroni correction to control for multiple hypotheses testing.\n\n\n\\subsection{Functional Forms Tested}\n\\label{sec:funcs}\n\nFunctional forms referenced in Table \\ref{tb:func} were used as following.\n\n\\subsubsection{Differencing: $\\nabla x$.} The first differencing is taken in both social media and returns time series.\n\n", "index": 11, "text": "\\begin{align}\n\\nabla(R(t), 1) &= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} \\nabla(R(t-\\Delta t),1)} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}\\nabla(SM(t-\\Delta t),1)}+ \\widehat{\\epsilon}_t.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\nabla(R(t),1)\" display=\"inline\"><mrow><mo>\u2207</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\widehat{\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta&#10;t%&#10;}\\nabla(R(t-\\Delta t),1)}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}%&#10;\\nabla(SM(t-\\Delta t),1)}+\\widehat{\\epsilon}_{t}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo>^</mo></mover><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere we consider \n\n", "itemtype": "equation", "pos": 28545, "prevtext": "\nThe second differencing $\\nabla^2 x$ was tested in analogous way.\n\n\\subsubsection{$f(x, vol)$.} Represents a regression of returns on social media controlled by the stocks' returns daily volatility.\n\n\n", "index": 13, "text": "\\begin{align}\nR(t) &= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} R(t-\\Delta t)} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-\\Delta t)} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\theta}}_{\\Delta t}vol(t-\\Delta t)} + \\widehat{\\epsilon}_t,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(t)\" display=\"inline\"><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\widehat{\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta&#10;t%&#10;}R(t-\\Delta t)}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-\\Delta&#10;t%&#10;)}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\theta}}_{\\Delta t}vol(t-\\Delta t)}+%&#10;\\widehat{\\epsilon}_{t},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo>^</mo></mover><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b8</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nas an approximation of the daily returns volatility. $P_{high}$ and $P_{low}$ are the highest and lowest intraday price value, respectively.\n\n\n\\subsubsection{Log-transformation: $log(x+1)$.}\n\n", "itemtype": "equation", "pos": 28842, "prevtext": "\nwhere we consider \n\n", "index": 15, "text": "\\begin{align}\nvol(t) = 2\\frac{P_{high}(t) - P_{low}(t)}{P_{high}(t) + P_{low}(t)}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle vol(t)=2\\frac{P_{high}(t)-P_{low}(t)}{P_{high}(t)+P_{low}(t)}\" display=\"inline\"><mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>P</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>P</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><msub><mi>P</mi><mrow><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>P</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\n\\subsubsection{Absolute value: $|x|$.}\n\n", "itemtype": "equation", "pos": 29127, "prevtext": "\nas an approximation of the daily returns volatility. $P_{high}$ and $P_{low}$ are the highest and lowest intraday price value, respectively.\n\n\n\\subsubsection{Log-transformation: $log(x+1)$.}\n\n", "index": 17, "text": "\\begin{align}\n\\log(R(t)+ 1) &= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} \\log(R(t-\\Delta t)+1)} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}\\log(SM(t-\\Delta t) + 1)}+ \\widehat{\\epsilon}_t.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\log(R(t)+1)\" display=\"inline\"><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\widehat{\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta&#10;t%&#10;}\\log(R(t-\\Delta t)+1)}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}%&#10;\\log(SM(t-\\Delta t)+1)}+\\widehat{\\epsilon}_{t}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo>^</mo></mover><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\n\n\\subsubsection{GARCH(1,1).}\n\nA GARCH filtering was applied in the original returns time series as follows:\n\n", "itemtype": "equation", "pos": 29402, "prevtext": "\n\n\\subsubsection{Absolute value: $|x|$.}\n\n", "index": 19, "text": "\\begin{align}\n|R(t)| &= \\widehat{\\alpha} + \\sum^k_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta t} |R(t-\\Delta t)|} +  \\sum^k_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-\\Delta t)}+ \\widehat{\\epsilon}_t.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle|R(t)|\" display=\"inline\"><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\widehat{\\alpha}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\beta}}_{\\Delta&#10;t%&#10;}|R(t-\\Delta t)|}+\\sum^{k}_{\\Delta t=1}{{\\widehat{\\gamma}}_{\\Delta t}SM(t-%&#10;\\Delta t)}+\\widehat{\\epsilon}_{t}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo>^</mo></mover><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b2</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msub><mover accent=\"true\"><mi>\u03b3</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwith $p = 1$, $q = 1$ and\n\n", "itemtype": "equation", "pos": 29723, "prevtext": "\n\n\n\\subsubsection{GARCH(1,1).}\n\nA GARCH filtering was applied in the original returns time series as follows:\n\n", "index": 21, "text": "\\begin{align}\nR(t) &= \\alpha + \\sum^p_{\\Delta t=1}{{\\beta}_{\\Delta t} R(t-\\Delta t)} +  \\sum^q_{\\Delta t=1}{{\\gamma}_{\\Delta t}\\epsilon_{t-\\Delta t}}, \\label{eq:AR2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(t)\" display=\"inline\"><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\alpha+\\sum^{p}_{\\Delta t=1}{{\\beta}_{\\Delta t}R(t-\\Delta t)}+%&#10;\\sum^{q}_{\\Delta t=1}{{\\gamma}_{\\Delta t}\\epsilon_{t-\\Delta t}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\u03b1</mi><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover></mstyle><mrow><msub><mi>\u03b2</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover></mstyle><mrow><msub><mi>\u03b3</mi><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>\u2062</mo><msub><mi>\u03f5</mi><mrow><mi>t</mi><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nThe resulting residuals $\\epsilon_t$ were then used instead of the original returns time series $R(t)$.\n\n\n\\subsubsection{ARIMA(1,1,1).}\nARIMA filtering was applied in the original returns time series as follows:\n\n", "itemtype": "equation", "pos": 29927, "prevtext": "\nwith $p = 1$, $q = 1$ and\n\n", "index": 23, "text": "\\begin{align}\n\\epsilon_t \\sim N(0, R(t)).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\epsilon_{t}\\sim N(0,R(t)).\" display=\"inline\"><mrow><mrow><msub><mi>\u03f5</mi><mi>t</mi></msub><mo>\u223c</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nThe resulting residuals $\\epsilon_t$ were then used instead of the original returns time series $R(t)$.\n\n\\subsection{Nonparametric G-Causality: Transfer Entropy}\n\\label{Method:TE}\nTransfer Entropy (TE) was estimated as a sum of Shannon entropies:\n\n", "itemtype": "equation", "pos": 30193, "prevtext": "\nThe resulting residuals $\\epsilon_t$ were then used instead of the original returns time series $R(t)$.\n\n\n\\subsubsection{ARIMA(1,1,1).}\nARIMA filtering was applied in the original returns time series as follows:\n\n", "index": 25, "text": "\\begin{align}\nR(t) &= R(t-1) + \\alpha(SM(t-1)-SM(t-2))  +  \\beta\\epsilon_{t-1}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(t)\" display=\"inline\"><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=R(t-1)+\\alpha(SM(t-1)-SM(t-2))+\\beta\\epsilon_{t-1}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msub><mi>\u03f5</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere $Y^F$ is a forward time-shifted version of $Y$ at lag $\\Delta t$ relatively to the contemporaneous time-series $X^P$ and $Y^P$.\nWe reject the null hypothesis of causality if the Transfer Entropy from social media to stocks' returns is significant.\nTo remain in a nonparametric framework, the statistical significance of TE was performed using surrogate data.\nIn that way, 400 replicates of $TE(X_{Shuffled} \\rightarrow Y)$ were estimated, where $X_{Shuffled}$ is a random permutation of $X$ relatively to $Y$.\nWe computed the randomized Transfer Entropy at each permutation for each time-shift ($\\Delta t$) from 1 to 10 days. \nWe then calculated the frequency at which the observed Transfer Entropy was equal or more extreme \nthan the randomized Transfer Entropy of the surrogate data. Statistical significance was given at p-value $< 0.05$.\np-values were also Bonferroni corrected.\n\nThe estimation of the empirical probability density distribution, required for the entropy estimation, \nwas performed using a Kernel Density Estimation (KDE) method, which has several advantages over the commonly used Histogram based methods (see \\ref{sec:KDE}).\n\n\\subsection{Net Information Flow}\nThe Net Information Flow from social media to the stock market is defined as:  $\\widehat{TE}_{SM \\rightarrow R} = TE_{SM \\rightarrow R} - TE_{R \\rightarrow SM}$.\nFor the nonlinear case, transfer entropy was computed as defined in the previous Section \\ref{Method:TE}.\nInstead, to estimate a linear version of Net Information Flow, we compute Transfer Entropy for the linear case based on the work of \\cite{PhysRevLett.103.238701}. \nThis work provides a direct mapping between Transfer Entropy and the linear G-causality implemented in the standard VAR framework.\nThe authors showed that Transfer Entropy and linear G-causality are equivalent for Gaussian variables.\n\nParticularly, assuming the standard measure of linear G-causality for the bivariate case as\n\n", "itemtype": "equation", "pos": 30532, "prevtext": "\nThe resulting residuals $\\epsilon_t$ were then used instead of the original returns time series $R(t)$.\n\n\\subsection{Nonparametric G-Causality: Transfer Entropy}\n\\label{Method:TE}\nTransfer Entropy (TE) was estimated as a sum of Shannon entropies:\n\n", "index": 27, "text": "\\begin{align}\nTE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle TE\\left(X\\rightarrow Y\\right)=H\\left(Y^{P},X^{P}\\right)-H\\left(Y%&#10;^{F},Y^{P},X^{P}\\right)+H\\left(Y^{F},Y^{P}\\right)-H\\left(Y^{P}\\right),\" display=\"inline\"><mrow><mi>T</mi><mi>E</mi><mrow><mo>(</mo><mi>X</mi><mo>\u2192</mo><mi>Y</mi><mo>)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>,</mo><msup><mi>X</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>,</mo><msup><mi>X</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>+</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\\cite{PhysRevLett.103.238701} shows that:\n\n", "itemtype": "equation", "pos": 32637, "prevtext": "\nwhere $Y^F$ is a forward time-shifted version of $Y$ at lag $\\Delta t$ relatively to the contemporaneous time-series $X^P$ and $Y^P$.\nWe reject the null hypothesis of causality if the Transfer Entropy from social media to stocks' returns is significant.\nTo remain in a nonparametric framework, the statistical significance of TE was performed using surrogate data.\nIn that way, 400 replicates of $TE(X_{Shuffled} \\rightarrow Y)$ were estimated, where $X_{Shuffled}$ is a random permutation of $X$ relatively to $Y$.\nWe computed the randomized Transfer Entropy at each permutation for each time-shift ($\\Delta t$) from 1 to 10 days. \nWe then calculated the frequency at which the observed Transfer Entropy was equal or more extreme \nthan the randomized Transfer Entropy of the surrogate data. Statistical significance was given at p-value $< 0.05$.\np-values were also Bonferroni corrected.\n\nThe estimation of the empirical probability density distribution, required for the entropy estimation, \nwas performed using a Kernel Density Estimation (KDE) method, which has several advantages over the commonly used Histogram based methods (see \\ref{sec:KDE}).\n\n\\subsection{Net Information Flow}\nThe Net Information Flow from social media to the stock market is defined as:  $\\widehat{TE}_{SM \\rightarrow R} = TE_{SM \\rightarrow R} - TE_{R \\rightarrow SM}$.\nFor the nonlinear case, transfer entropy was computed as defined in the previous Section \\ref{Method:TE}.\nInstead, to estimate a linear version of Net Information Flow, we compute Transfer Entropy for the linear case based on the work of \\cite{PhysRevLett.103.238701}. \nThis work provides a direct mapping between Transfer Entropy and the linear G-causality implemented in the standard VAR framework.\nThe authors showed that Transfer Entropy and linear G-causality are equivalent for Gaussian variables.\n\nParticularly, assuming the standard measure of linear G-causality for the bivariate case as\n\n", "index": 29, "text": "\\begin{align}\nGC_{X \\rightarrow Y} = \\log\\left( \\frac{var(\\epsilon_t)}{var( \\widehat{\\epsilon}_t)} \\right).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle GC_{X\\rightarrow Y}=\\log\\left(\\frac{var(\\epsilon_{t})}{var(%&#10;\\widehat{\\epsilon}_{t})}\\right).\" display=\"inline\"><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><msub><mi>C</mi><mrow><mi>X</mi><mo>\u2192</mo><mi>Y</mi></mrow></msub></mrow><mo>=</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03f5</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\u03f5</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nif all processes ($X$ and $Y$) are jointly Gaussian.\n\nSee Supporting Information for the Material (\\ref{sec:material}) used and further details in the Transfer Entropy estimation \\ref{sec:TE}.\n\n\\section*{Acknowledgments}\n\nThis work was supported by PsychSignal.com, which provided the social media analytics. T.A. acknowledges support of the UK Economic and Social Research Council (ESRC) in funding the Systemic Risk Centre (ES/K002309/1). T.T.P.S. acknowledges financial support from CNPq - The Brazilian National Council for Scientific and Technological Development.\n\n\n\\bibliographystyle{splncs}\n\\bibliography{nonlinear}\n\n\\newpage\n\n\n\\section*{Supporting Information}\n\\label{sec:support}\n\n\\setcounter{section}{0}\n\\setcounter{table}{0}\n\n\n\\section{Material}\n\\label{sec:material}\n\n\\subsection{List of Companies Analyzed}\n\\label{sec:companies}\nThe name of the investigated stocks with respective Reuters Instrument Codes (RIC) follow: \nINTEL CORP. (INTC.O), VISA INC. (V.N), NIKE INC. (NKE.N), E.I. DUPONT DE NEMOURS \\& CO. (DD.N), JPMORGAN CHASE \\& CO. (JPM.N), \nBOEING CO. (BA.N), MERCK \\& CO. INC. (MRK.N), PFIZER INC. (PFE.N), MICROSOFT CORP. (MSFT.O), COCA-COLA CO. (KO.N), \nGOLDMAN SACHS GROUP INC. (GS.N), MCDONALD'S CORP. (MCD.N), GENERAL ELECTRIC CO. (GE.N), 3M CO. (MMM.N), UNITED TECHNOLOGIES CORP. (UTX.N), \nVERIZON COMMUNICATIONS INC. (VZ.N), CISCO SYSTEMS INC. (CSCO.O), HOME DEPOT INC. (HD.N), INTERNATIONAL BUSINESS MACHINES CORP. (IBM.N), \nAMERICAN EXPRESS CO. (AXP.N), PROCTER \\& GAMBLE CO. (PG.N), APPLE INC. (AAPL.O), UNITEDHEALTH GROUP INC. (UNH.N), CATERPILLAR INC. (CAT.N), \nEXXON MOBIL CORP. (XOM.N), JOHNSON \\& JOHNSON (JNJ.N), WAL-MART STORES INC. (WMT.N), WALT DISNEY CO. (DIS.N),  CHEVRON CORP. (CVX.N) and THE TRAVELERS COMPANIES INC. (TRV.N).\n\n\n\\subsection{Twitter Data Analytics}\n\\label{sec:Twitter}\n\nTwitter data analytics were provided by PsychSignal.com \\cite{PsychSignal}. The data are comprised of volume measures and sentiment analytics. \nTwitter messages are classified in two dimensions according to their likelihood of bullishness and bearishness towards a company. \nArtificial neural networks are used as main machine learning classification technique. A company is defined to be related to a given message if its ticker is mentioned. \nThe data set are based on English language content and it is agnostic to the country source of the Twitter message. \nThe  information is aggregated in a daily fashion and it is composed of the following variables:\n\\begin{itemize}\n  \\item symbol: the stock symbol (ticker) for which the sentiment data refer to;\\\\\n  \\item timestamp\\_utc: date and time of the analyzed data in UTC format;\\\\\n  \\item bull\\_scored\\_messages: this indicator is the total count of bullish sentiment messages;\\\\\n  \\item bear\\_scored\\_messages: this indicator is the total count of bearish sentiment messages.\n\\end{itemize}\nSome messages may be classified as ``neutral'' or at least not having relevant bullish or bearish tones. \nThose types of messages do not affect the bull\\_scored\\_messages and bear\\_scored\\_messages scores. \nIt is also possible that no messages cite a company in a given day. In that case, the scores are zero.\n\nTable \\ref{tb:TA} shows an example of the Twitter sentiment analytics for the company APPLE INC. \nTable \\ref{tb:Comp} shows a summary description of the selected companies with the number of bearish/bullish Twitter messages identified in the period.\n\n\n  \\begin{table}[!ht]\n\\centering\n\\caption{\\textbf{Sample of Twitter Sentiment Analytics for the company APPLE INC}. Twitter messages mentioning a company's ticker (symbol) are classified according to their bullish/bearish tones. \nThe table shows a sample of the daily total of bearish and bullish messages classified for the company APPLE INC.}\n\\label{tb:TA}\n\\begin{center}\n \\begin{tabular}{c c c c} \n\\hline\n timestamp\\_utc & symbol & bull\\_scored\\_messages & bear\\_scored\\_messages \\\\\n \\hline\n2015-06-19 & AAPL.O  & 216  & 55 \\\\\n2015-06-20 & AAPL.O  & 66  & 25 \\\\\n2015-06-21  & AAPL.O  & 90  & 24 \\\\\n2015-06-22  & AAPL.O  & 241  & 75 \\\\ \n2015-06-23  & AAPL.O  & 208  & 75\\\\ \n2015-06-24  & AAPL.O  & 561  & 211\\\\ \n2015-06-25  & AAPL.O  & 286  & 107\\\\ \n2015-06-26  & AAPL.O  & 192  & 82\\\\ \n2015-06-27  & AAPL.O  & 145 &12\\\\\n2015-06-28  & AAPL.O  & 216  & 69 \\\\\n \\hline\n\\end{tabular}\n\n\\end{center}\n \\end{table}  \n\n  \\begin{table}[!t]\n\\centering\n\\caption{\\textbf{Summary table of selected companies.} DJIA stocks along with their total and daily mean number of Bearish and Bullish tweets during the period from March 31, 2012 to March 31, 2014. \nThe number of total messages processed include not only messages labeled as bullish and bearish but also neutral messages.}\n\\label{tb:Comp}\n\\begin{center}\n \\begin{tabular}{l c c c c c} \n\\hline\n& \\multicolumn{2}{c}{Bullish messages} & \\multicolumn{2}{c}{Bearish messages} \\\\\n\\cline{2-3}  \\cline{4-5}\nTicker & Total  & Daily mean & Total & Daily mean & Total Messages\\\\ \\hline\nAAPL&151143&279.89&95819&177.443&800638\\tabularnewline\nMSFT& 16730& 30.98& 7062& 13.078&139343\\tabularnewline\nJPM& 11259& 20.85& 6090& 11.278& 82265\\tabularnewline\nGS& 13971& 25.87& 8023& 14.857& 75578\\tabularnewline\nIBM&  7387& 13.68& 4284&  7.933& 53547\\tabularnewline\nINTC&  6808& 12.61& 3199&  5.924& 47653\\tabularnewline\nGE&  4888&  9.05& 1522&  2.819& 41271\\tabularnewline\nCSCO&  5919& 10.96& 2535&  4.694& 39665\\tabularnewline\nWMT&  4702&  8.71& 2438&  4.515& 39607\\tabularnewline\nXOM&  4495&  8.32& 1780&  3.296& 33194\\tabularnewline\nCAT&  5854& 10.84& 4035&  7.472& 31911\\tabularnewline\nVZ&  4101&  7.59& 1651&  3.057& 30936\\tabularnewline\nBA&  4432&  8.21& 1693&  3.135& 30421\\tabularnewline\nJNJ&  3575&  6.62& 1345&  2.491& 28392\\tabularnewline\nMCD&  3750&  6.94& 2157&  3.994& 28059\\tabularnewline\nKO&  3786&  7.01& 1385&  2.565& 26331\\tabularnewline\nDIS&  4170&  7.72& 1282&  2.374& 25863\\tabularnewline\nPFE&  3131&  5.80& 1091&  2.020& 24817\\tabularnewline\nV&  4436&  8.21& 1726&  3.196& 24118\\tabularnewline\nCVX&  2696&  4.99&  986&  1.826& 21322\\tabularnewline\nNKE&  3549&  6.57& 1461&  2.706& 20941\\tabularnewline\nMRK&  2623&  4.86&  929&  1.720& 20708\\tabularnewline\nPG&  2382&  4.41&  968&  1.793& 20226\\tabularnewline\nHD&  3262&  6.04& 1221&  2.261& 17550\\tabularnewline\nMMM&  1399&  2.59&  465&  0.861& 12382\\tabularnewline\nAXP&  1740&  3.22&  674&  1.248& 12072\\tabularnewline\nUTX&  1363&  2.55&  369&  0.690& 11255\\tabularnewline\nDD&  1498&  2.78&  559&  1.037& 10746\\tabularnewline\nUNH&  1348&  2.50&  532&  0.987&  9196\\tabularnewline\nTRV&   798&  1.53&  316&  0.604&  7990\\tabularnewline\n\\hline\nTOTAL & 287195 & - & 157597 & - & 1767997\n\\\\\n\\hline\n\\end{tabular}\n\n\\end{center}\n \\end{table} \n\n \n\n\\section{Transfer-Entropy Estimation}\n\\label{sec:TE}\n\n\\subsection{Definitions of information theoretic measures}\n\nLet $X$ be a random variable and $P_X(x)$ its probability density function (pdf). The entropy $H(X)$ is a measure of uncertainty of $X$, and is defined in the discrete case, as:\n\n", "itemtype": "equation", "pos": 32799, "prevtext": "\n\\cite{PhysRevLett.103.238701} shows that:\n\n", "index": 31, "text": "\\begin{align}\nGC_{X \\rightarrow Y} = 2TE_{X \\rightarrow Y},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle GC_{X\\rightarrow Y}=2TE_{X\\rightarrow Y},\" display=\"inline\"><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><msub><mi>C</mi><mrow><mi>X</mi><mo>\u2192</mo><mi>Y</mi></mrow></msub></mrow><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><msub><mi>E</mi><mrow><mi>X</mi><mo>\u2192</mo><mi>Y</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\nIf the $\\log$ is taken to base two, then the unit of $H$ is the \\textit{bit} (binary digit). \nHere, we employ the natural logarithm which implies the unit in \\textit{nat} (natural unit of information).  \n\nGiven a coupled system $(X,Y)$, where $P_Y(y)$ is the pdf of the random variable $Y$ and $P_{X,Y}$ the joint pdf between $X$ and $Y$, the joint entropy is given by:\n\n", "itemtype": "equation", "pos": 39836, "prevtext": "\nif all processes ($X$ and $Y$) are jointly Gaussian.\n\nSee Supporting Information for the Material (\\ref{sec:material}) used and further details in the Transfer Entropy estimation \\ref{sec:TE}.\n\n\\section*{Acknowledgments}\n\nThis work was supported by PsychSignal.com, which provided the social media analytics. T.A. acknowledges support of the UK Economic and Social Research Council (ESRC) in funding the Systemic Risk Centre (ES/K002309/1). T.T.P.S. acknowledges financial support from CNPq - The Brazilian National Council for Scientific and Technological Development.\n\n\n\\bibliographystyle{splncs}\n\\bibliography{nonlinear}\n\n\\newpage\n\n\n\\section*{Supporting Information}\n\\label{sec:support}\n\n\\setcounter{section}{0}\n\\setcounter{table}{0}\n\n\n\\section{Material}\n\\label{sec:material}\n\n\\subsection{List of Companies Analyzed}\n\\label{sec:companies}\nThe name of the investigated stocks with respective Reuters Instrument Codes (RIC) follow: \nINTEL CORP. (INTC.O), VISA INC. (V.N), NIKE INC. (NKE.N), E.I. DUPONT DE NEMOURS \\& CO. (DD.N), JPMORGAN CHASE \\& CO. (JPM.N), \nBOEING CO. (BA.N), MERCK \\& CO. INC. (MRK.N), PFIZER INC. (PFE.N), MICROSOFT CORP. (MSFT.O), COCA-COLA CO. (KO.N), \nGOLDMAN SACHS GROUP INC. (GS.N), MCDONALD'S CORP. (MCD.N), GENERAL ELECTRIC CO. (GE.N), 3M CO. (MMM.N), UNITED TECHNOLOGIES CORP. (UTX.N), \nVERIZON COMMUNICATIONS INC. (VZ.N), CISCO SYSTEMS INC. (CSCO.O), HOME DEPOT INC. (HD.N), INTERNATIONAL BUSINESS MACHINES CORP. (IBM.N), \nAMERICAN EXPRESS CO. (AXP.N), PROCTER \\& GAMBLE CO. (PG.N), APPLE INC. (AAPL.O), UNITEDHEALTH GROUP INC. (UNH.N), CATERPILLAR INC. (CAT.N), \nEXXON MOBIL CORP. (XOM.N), JOHNSON \\& JOHNSON (JNJ.N), WAL-MART STORES INC. (WMT.N), WALT DISNEY CO. (DIS.N),  CHEVRON CORP. (CVX.N) and THE TRAVELERS COMPANIES INC. (TRV.N).\n\n\n\\subsection{Twitter Data Analytics}\n\\label{sec:Twitter}\n\nTwitter data analytics were provided by PsychSignal.com \\cite{PsychSignal}. The data are comprised of volume measures and sentiment analytics. \nTwitter messages are classified in two dimensions according to their likelihood of bullishness and bearishness towards a company. \nArtificial neural networks are used as main machine learning classification technique. A company is defined to be related to a given message if its ticker is mentioned. \nThe data set are based on English language content and it is agnostic to the country source of the Twitter message. \nThe  information is aggregated in a daily fashion and it is composed of the following variables:\n\\begin{itemize}\n  \\item symbol: the stock symbol (ticker) for which the sentiment data refer to;\\\\\n  \\item timestamp\\_utc: date and time of the analyzed data in UTC format;\\\\\n  \\item bull\\_scored\\_messages: this indicator is the total count of bullish sentiment messages;\\\\\n  \\item bear\\_scored\\_messages: this indicator is the total count of bearish sentiment messages.\n\\end{itemize}\nSome messages may be classified as ``neutral'' or at least not having relevant bullish or bearish tones. \nThose types of messages do not affect the bull\\_scored\\_messages and bear\\_scored\\_messages scores. \nIt is also possible that no messages cite a company in a given day. In that case, the scores are zero.\n\nTable \\ref{tb:TA} shows an example of the Twitter sentiment analytics for the company APPLE INC. \nTable \\ref{tb:Comp} shows a summary description of the selected companies with the number of bearish/bullish Twitter messages identified in the period.\n\n\n  \\begin{table}[!ht]\n\\centering\n\\caption{\\textbf{Sample of Twitter Sentiment Analytics for the company APPLE INC}. Twitter messages mentioning a company's ticker (symbol) are classified according to their bullish/bearish tones. \nThe table shows a sample of the daily total of bearish and bullish messages classified for the company APPLE INC.}\n\\label{tb:TA}\n\\begin{center}\n \\begin{tabular}{c c c c} \n\\hline\n timestamp\\_utc & symbol & bull\\_scored\\_messages & bear\\_scored\\_messages \\\\\n \\hline\n2015-06-19 & AAPL.O  & 216  & 55 \\\\\n2015-06-20 & AAPL.O  & 66  & 25 \\\\\n2015-06-21  & AAPL.O  & 90  & 24 \\\\\n2015-06-22  & AAPL.O  & 241  & 75 \\\\ \n2015-06-23  & AAPL.O  & 208  & 75\\\\ \n2015-06-24  & AAPL.O  & 561  & 211\\\\ \n2015-06-25  & AAPL.O  & 286  & 107\\\\ \n2015-06-26  & AAPL.O  & 192  & 82\\\\ \n2015-06-27  & AAPL.O  & 145 &12\\\\\n2015-06-28  & AAPL.O  & 216  & 69 \\\\\n \\hline\n\\end{tabular}\n\n\\end{center}\n \\end{table}  \n\n  \\begin{table}[!t]\n\\centering\n\\caption{\\textbf{Summary table of selected companies.} DJIA stocks along with their total and daily mean number of Bearish and Bullish tweets during the period from March 31, 2012 to March 31, 2014. \nThe number of total messages processed include not only messages labeled as bullish and bearish but also neutral messages.}\n\\label{tb:Comp}\n\\begin{center}\n \\begin{tabular}{l c c c c c} \n\\hline\n& \\multicolumn{2}{c}{Bullish messages} & \\multicolumn{2}{c}{Bearish messages} \\\\\n\\cline{2-3}  \\cline{4-5}\nTicker & Total  & Daily mean & Total & Daily mean & Total Messages\\\\ \\hline\nAAPL&151143&279.89&95819&177.443&800638\\tabularnewline\nMSFT& 16730& 30.98& 7062& 13.078&139343\\tabularnewline\nJPM& 11259& 20.85& 6090& 11.278& 82265\\tabularnewline\nGS& 13971& 25.87& 8023& 14.857& 75578\\tabularnewline\nIBM&  7387& 13.68& 4284&  7.933& 53547\\tabularnewline\nINTC&  6808& 12.61& 3199&  5.924& 47653\\tabularnewline\nGE&  4888&  9.05& 1522&  2.819& 41271\\tabularnewline\nCSCO&  5919& 10.96& 2535&  4.694& 39665\\tabularnewline\nWMT&  4702&  8.71& 2438&  4.515& 39607\\tabularnewline\nXOM&  4495&  8.32& 1780&  3.296& 33194\\tabularnewline\nCAT&  5854& 10.84& 4035&  7.472& 31911\\tabularnewline\nVZ&  4101&  7.59& 1651&  3.057& 30936\\tabularnewline\nBA&  4432&  8.21& 1693&  3.135& 30421\\tabularnewline\nJNJ&  3575&  6.62& 1345&  2.491& 28392\\tabularnewline\nMCD&  3750&  6.94& 2157&  3.994& 28059\\tabularnewline\nKO&  3786&  7.01& 1385&  2.565& 26331\\tabularnewline\nDIS&  4170&  7.72& 1282&  2.374& 25863\\tabularnewline\nPFE&  3131&  5.80& 1091&  2.020& 24817\\tabularnewline\nV&  4436&  8.21& 1726&  3.196& 24118\\tabularnewline\nCVX&  2696&  4.99&  986&  1.826& 21322\\tabularnewline\nNKE&  3549&  6.57& 1461&  2.706& 20941\\tabularnewline\nMRK&  2623&  4.86&  929&  1.720& 20708\\tabularnewline\nPG&  2382&  4.41&  968&  1.793& 20226\\tabularnewline\nHD&  3262&  6.04& 1221&  2.261& 17550\\tabularnewline\nMMM&  1399&  2.59&  465&  0.861& 12382\\tabularnewline\nAXP&  1740&  3.22&  674&  1.248& 12072\\tabularnewline\nUTX&  1363&  2.55&  369&  0.690& 11255\\tabularnewline\nDD&  1498&  2.78&  559&  1.037& 10746\\tabularnewline\nUNH&  1348&  2.50&  532&  0.987&  9196\\tabularnewline\nTRV&   798&  1.53&  316&  0.604&  7990\\tabularnewline\n\\hline\nTOTAL & 287195 & - & 157597 & - & 1767997\n\\\\\n\\hline\n\\end{tabular}\n\n\\end{center}\n \\end{table} \n\n \n\n\\section{Transfer-Entropy Estimation}\n\\label{sec:TE}\n\n\\subsection{Definitions of information theoretic measures}\n\nLet $X$ be a random variable and $P_X(x)$ its probability density function (pdf). The entropy $H(X)$ is a measure of uncertainty of $X$, and is defined in the discrete case, as:\n\n", "index": 33, "text": "\\begin{equation}\nH(X) = -\\sum_{x \\in X}{P_X(x)\\log{P_X(x)}}.\n\\label{eq:H}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"H(X)=-\\sum_{x\\in X}{P_{X}(x)\\log{P_{X}(x)}}.\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>P</mi><mi>X</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n \nThe conditional entropy is defined by:\n\n", "itemtype": "equation", "pos": 40296, "prevtext": "\n\nIf the $\\log$ is taken to base two, then the unit of $H$ is the \\textit{bit} (binary digit). \nHere, we employ the natural logarithm which implies the unit in \\textit{nat} (natural unit of information).  \n\nGiven a coupled system $(X,Y)$, where $P_Y(y)$ is the pdf of the random variable $Y$ and $P_{X,Y}$ the joint pdf between $X$ and $Y$, the joint entropy is given by:\n\n", "index": 35, "text": "\\begin{equation}\nH(X,Y) = -\\sum_{x \\in X}{\\sum_{y \\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}.\n\\label{eq:HXY}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"H(X,Y)=-\\sum_{x\\in X}{\\sum_{y\\in Y}{P_{X,Y}(x,y)\\log{P_{X,Y}(x,y)}}}.\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>y</mi><mo>\u2208</mo><mi>Y</mi></mrow></munder><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nWe can interpret $H\\left(Y\\middle\\vert X\\right)$ as the uncertainty of $Y$ given a realization of $X$. \n\nTransfer Entropy can be defined as a difference between conditional entropies: \n\n", "itemtype": "equation", "pos": 40458, "prevtext": "\n \nThe conditional entropy is defined by:\n\n", "index": 37, "text": "\\begin{equation}\nH\\left(Y\\middle\\vert X\\right) = H(X,Y) - H(X).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"H\\left(Y\\middle|X\\right)=H(X,Y)-H(X).\" display=\"block\"><mrow><mi>H</mi><mrow><mo>(</mo><mi>Y</mi><mo>|</mo><mi>X</mi><mo>)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhich can be rewritten as a sum of Shannon entropies:\n\n", "itemtype": "equation", "pos": 40722, "prevtext": "\nWe can interpret $H\\left(Y\\middle\\vert X\\right)$ as the uncertainty of $Y$ given a realization of $X$. \n\nTransfer Entropy can be defined as a difference between conditional entropies: \n\n", "index": 39, "text": "\\begin{align}\nTE\\left(X \\rightarrow Y\\right) = TE\\left(X^P,Y^F\\middle\\vert Y^P\\right) =  H\\left(Y^F\\middle\\vert Y^P\\right) - H\\left(Y^F\\middle\\vert X^P, Y^P\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle TE\\left(X\\rightarrow Y\\right)=TE\\left(X^{P},Y^{F}\\middle|Y^{P}%&#10;\\right)=H\\left(Y^{F}\\middle|Y^{P}\\right)-H\\left(Y^{F}\\middle|X^{P},Y^{P}\\right),\" display=\"inline\"><mrow><mi>T</mi><mi>E</mi><mrow><mo>(</mo><mi>X</mi><mo>\u2192</mo><mi>Y</mi><mo>)</mo></mrow><mo>=</mo><mi>T</mi><mi>E</mi><mrow><mo>(</mo><msup><mi>X</mi><mi>P</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>|</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>|</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>|</mo><msup><mi>X</mi><mi>P</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere $Y^F$ is a forward time-shifted version of $Y$ at lag $\\Delta t$ relatively to the contemporaneous time-series $X^P$ and $Y^P$.\nWe reject the null hypothesis of causality if the Transfer Entropy from social media to stocks' returns is significant.\nTo remain in a nonparametric framework, the statistical significance of TE was performed using surrogate data.\nIn that way, 400 replicates of $TE(X_{Shuffled} \\rightarrow Y)$ were estimated, where $X_{Shuffled}$ is a random permutation of $X$ relatively to $Y$.\nWe computed the randomized Transfer Entropy at each permutation for each time-shift ($\\Delta t$) from 1 to 10 days. \nWe then calculated the frequency at which the observed Transfer Entropy was equal or more extreme \nthan the randomized Transfer Entropy of the surrogate data. Statistical significance was given at p-value $< 0.05$.\np-values were also Bonferroni corrected.\n\nThe estimation of the empirical probability density distribution, required for the entropy estimation, \nwas performed using a Kernel Density Estimation (KDE) method, which has several advantages over the commonly used Histogram based methods (see \\ref{sec:KDE}).\n\n\\subsection{Net Information Flow}\nThe Net Information Flow from social media to the stock market is defined as:  $\\widehat{TE}_{SM \\rightarrow R} = TE_{SM \\rightarrow R} - TE_{R \\rightarrow SM}$.\nFor the nonlinear case, transfer entropy was computed as defined in the previous Section \\ref{Method:TE}.\nInstead, to estimate a linear version of Net Information Flow, we compute Transfer Entropy for the linear case based on the work of \\cite{PhysRevLett.103.238701}. \nThis work provides a direct mapping between Transfer Entropy and the linear G-causality implemented in the standard VAR framework.\nThe authors showed that Transfer Entropy and linear G-causality are equivalent for Gaussian variables.\n\nParticularly, assuming the standard measure of linear G-causality for the bivariate case as\n\n", "itemtype": "equation", "pos": 30532, "prevtext": "\nThe resulting residuals $\\epsilon_t$ were then used instead of the original returns time series $R(t)$.\n\n\\subsection{Nonparametric G-Causality: Transfer Entropy}\n\\label{Method:TE}\nTransfer Entropy (TE) was estimated as a sum of Shannon entropies:\n\n", "index": 27, "text": "\\begin{align}\nTE\\left(X \\rightarrow Y\\right) = H\\left(Y^P, X^P\\right) - H\\left(Y^F, Y^P, X^P\\right) + H\\left(Y^F, Y^P\\right) - H\\left(Y^P\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle TE\\left(X\\rightarrow Y\\right)=H\\left(Y^{P},X^{P}\\right)-H\\left(Y%&#10;^{F},Y^{P},X^{P}\\right)+H\\left(Y^{F},Y^{P}\\right)-H\\left(Y^{P}\\right),\" display=\"inline\"><mrow><mi>T</mi><mi>E</mi><mrow><mo>(</mo><mi>X</mi><mo>\u2192</mo><mi>Y</mi><mo>)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>,</mo><msup><mi>X</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>,</mo><msup><mi>X</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>+</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>F</mi></msup><mo>,</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo>(</mo><msup><mi>Y</mi><mi>P</mi></msup><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\nA usual choice for the kernel $K$, which we use here, is the (Gaussian) radial basis function:\n\n", "itemtype": "equation", "pos": 42167, "prevtext": "\nwhere $Y^F$ is a forward time-shifted version of $Y$ at lag $\\Delta t$ relatively to the contemporaneous time-series $X^P$ and $Y^P$.\n\n\\subsection{Kernel Density Estimation}\n\\label{sec:KDE}\nIn the entropy computation, the empirical probability distribution must be estimated.\nHistogram based methods and kernel density estimations are  the  two  main methods for that.\nHistogram-based is the simplest and most used nonparametric density estimator. \nNonetheless, it yields density estimates that have discontinuities and vary significantly depending on the bin's size choice.\n\n\nAlso known as Parzen-Rosenblatt window method, the kernel density estimation (KDE) approach approximates the density function at a point $x$ using neighboring observations. \nHowever, instead of building up the estimate according to bin edges as in histograms, the KDE method uses each point of estimation $x$ as the center of a bin of width $2h$ \nand weight it according to a kernel function. Thereby, the kernel estimate of the probability density function $f(x)$ is defined as\n\n", "index": 43, "text": "\\begin{equation}\n\\hat{f} = \\frac{1}{nh}\\sum_{x' \\in X}{K\\left(\\frac{x - x'}{h}\\right)}. \n\\label{pdf}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}=\\frac{1}{nh}\\sum_{x^{\\prime}\\in X}{K\\left(\\frac{x-x^{\\prime}}{h}\\right%&#10;)}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u2062</mo><mi>h</mi></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>x</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msup><mi>x</mi><mo>\u2032</mo></msup></mrow><mi>h</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\n\nThe problem of selecting the bandwidth $h$ in equation \\eqref{pdf} is crucial in the density estimation. \nA large $h$ will over-smooth the estimated density and mask the structure of the data. \nOn the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. \nIf we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of $h$ that minimizes the mean integrated squared error (MISE) is\n\n", "itemtype": "equation", "pos": 42379, "prevtext": "\n\nA usual choice for the kernel $K$, which we use here, is the (Gaussian) radial basis function:\n\n", "index": 45, "text": "\\begin{equation}\nK(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^2}. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"K(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}x^{2}}.\" display=\"block\"><mrow><mrow><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mfrac><mo>\u2062</mo><msup><mi>exp</mi><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04535.tex", "nexttext": "\nwhere $N$ is the total number of points and $\\sigma$ can be estimated as the sample standard deviation. \nThis bandwidth estimation is often called Gaussian approximation or Silverman's rule of thumb for kernel density estimation \\cite{silverman}. This is the most common used method and it is here employed. Other common methods are given by Sheather and Jones \\cite{sheather1991reliable} and Scott \\cite{scott1992multivariate}.\n\n\n\\section{Results of the BDS and causality tests}\n\\label{sec:table-causality}\n\n\n \\begin{table}[!t]\n\\centering\n\\caption{\\textbf{Results of the BDS test. Significance of the null hypothesis of linear specification between Social Media and stocks' returns.} \nSocial media bullishness is taken as independent variable with stocks' returns as outcome variable.\np-values higher than 0.05 are evidence of misspecification of the linear functional form indicating a nonlinearly neglected relationship. Lags of up to 10 days were tested.  \np-value $< 0.05$: *; p-value $< 0.01$: **. }\n\\label{tb:GC-results}\n\\begin{center}\n\n \\begin{tabular}{|l | c c c c c c c c c c |} \n \\cline{2-11}\n \\multicolumn{1}{l|}{} & \\multicolumn{10}{c|}{ Lags $\\Delta t$ for: $R_t = \\sum_{i=1}^{\\Delta t}\\alpha_i R_{t-\\Delta t} + \\sum_{i=1}^{\\Delta t}\\beta_i SM_{t-\\Delta t}$} \\\\\n\\hline\nTicker & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\\\\n\\hline\nMMM.N&0.928&0.780&0.768&0.999&0.668&0.747&0.739&0.362&0.389&0.676\\tabularnewline\nAXP.N&0.906&0.987&0.503&0.795&0.734&0.432&0.737&0.336&0.197&0.132\\tabularnewline\nAAPL.O&0.015*&0.023*&0.028*&0.018*&0.018*&0.008**&0.007**&0.002**&0.003**&0.009**\\tabularnewline\nBA.N&0.834&0.700&0.591&0.239&0.187&0.363&0.287&0.345&0.187&0.213\\tabularnewline\nCAT.N&0.001**&0.002**&0.001**&0.000**&0.000**&0.000**&0.000**&0.001**&0.004**&0.002**\\tabularnewline\nCVX.N&0.113&0.073&0.202&0.330&0.150&0.151&0.389&0.268&0.275&0.439\\tabularnewline\nCSCO.O&0.404&0.443&0.500&0.463&0.436&0.814&0.883&0.639&0.597&0.590\\tabularnewline\nKO.N&0.544&0.905&0.712&0.864&0.451&0.734&0.653&0.702&0.471&0.793\\tabularnewline\nDD.N&0.523&0.373&0.170&0.295&0.759&0.254&0.311&0.199&0.204&0.169\\tabularnewline\nXOM.N&0.986&0.871&0.504&0.382&0.502&0.704&0.635&0.648&0.178&0.202\\tabularnewline\nGE.N&0.055&0.016*&0.017*&0.112&0.117&0.129&0.103&0.098&0.101&0.101\\tabularnewline\nGS.N&0.077&0.010**&0.041*&0.021*&0.028*&0.068&0.102&0.065&0.079&0.014*\\tabularnewline\nHD.N&0.171&0.092&0.348&0.211&0.079&0.231&0.188&0.034*&0.011*&0.037*\\tabularnewline\nINTC.O&0.888&0.931&0.808&0.880&0.828&0.858&0.735&0.968&0.756&0.825\\tabularnewline\nIBM.N&0.184&0.201&0.121&0.126&0.066&0.216&0.393&0.344&0.263&0.209\\tabularnewline\nJNJ.N&0.000**&0.000**&0.000**&0.000**&0.000**&0.000**&0.000**&0.000**&0.000**&0.000**\\tabularnewline\nJPM.N&0.171&0.133&0.146&0.305&0.401&0.322&0.270&0.304&0.246&0.798\\tabularnewline\nMCD.N&0.837&0.970&0.953&0.944&0.497&0.708&0.768&0.775&0.995&0.988\\tabularnewline\nMRK.N&0.031*&0.026*&0.015*&0.054&0.074&0.134&0.157&0.089&0.126&0.210\\tabularnewline\nMSFT.O&0.138&0.079&0.129&0.031*&0.009**&0.074&0.066&0.080&0.201&0.136\\tabularnewline\nNKE.N&0.220&0.126&0.124&0.158&0.055&0.146&0.033*&0.061&0.035*&0.023*\\tabularnewline\nPFE.N&0.606&0.601&0.599&0.573&0.346&0.405&0.526&0.777&0.651&0.838\\tabularnewline\nPG.N&0.005**&0.010*&0.012*&0.016*&0.054&0.052&0.015*&0.017*&0.022*&0.017*\\tabularnewline\nTRV.N&0.000**&0.000**&0.000**&0.000**&0.000**&0.002**&0.003**&0.000**&0.000**&0.000**\\tabularnewline\nUNH.N&0.007**&0.014*&0.036*&0.016*&0.019*&0.013*&0.006**&0.039*&0.016*&0.011*\\tabularnewline\nUTX.N&0.176&0.204&0.256&0.577&0.084&0.079&0.185&0.254&0.725&0.585\\tabularnewline\nVZ.N&0.001**&0.000**&0.000**&0.001**&0.002**&0.000**&0.001**&0.027*&0.000**&0.000**\\tabularnewline\nV.N&0.001**&0.000**&0.000**&0.000**&0.003**&0.022*&0.049*&0.017*&0.023*&0.023*\\tabularnewline\nWMT.N&0.200&0.364&0.525&0.937&0.839&0.646&0.719&0.976&0.946&0.753\\tabularnewline\nDIS.N&0.294&0.310&0.217&0.173&0.421&0.285&0.174&0.179&0.270&0.432\\tabularnewline\n\\hline\n\\end{tabular}\n\n\\end{center}\n \\end{table} \n\n\\begin{table}[!t]\n \\centering\n\\rotatebox{90}{\n\\begin{minipage}{\\textheight}\n\\centering\n\\caption{\\textbf{Significance for nonlinear causality tests}. Lags of up to 10 days were tested \nin both directions of causality: social media causing returns $SM \\rightarrow R$ and the opposite, returns causing social media $R \\rightarrow SM$.  p-value $< 0.05$: *; p-value $< 0.01$: **. }\n\\label{tb:TE-results}\n\\begin{center}\n\n \\begin{tabular}{|l | c c c c c c c c c c | c c c c c c c c c c |} \n \\cline{2-21}\n \\multicolumn{1}{l|}{} & \\multicolumn{10}{c|}{Lagged $R \\rightarrow SM$} & \\multicolumn{10}{c|}{Lagged $SM \\rightarrow R$} \\\\\n\\hline\nTicker & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\\\n\\hline\nMMM.N&0.257&0.248&0.970&0.530&0.198&0.838&0.655&0.767&0.505&0.615&0.410&0.765&0.755&0.570&0.568&0.973&0.980&0.797&0.825&0.945\\tabularnewline\nAXP.N&0.145&0.275&0.480&0.002**&0.157&0.310&0.380&0.475&0.328&0.142&0.000**&0.297&0.282&0.160&0.407&0.098&0.440&0.152&0.405&0.118\\tabularnewline\nAAPL.O&0.440&0.995&0.710&0.512&0.912&0.988&0.980&0.897&0.615&0.990&0.000**&0.745&0.590&0.140&0.682&0.585&0.782&0.715&0.095&0.975\\tabularnewline\nBA.N&0.090&0.475&0.110&0.110&0.718&0.160&0.062&0.345&0.050&0.150&0.287&0.297&0.262&0.070&0.103&0.123&0.095&0.545&0.510&0.130\\tabularnewline\nCAT.N&0.265&0.562&0.162&0.607&0.170&0.787&0.623&0.235&0.480&0.435&0.055&0.540&0.245&0.228&0.297&0.025*&0.157&0.035*&0.485&0.348\\tabularnewline\nCVX.N&0.422&0.500&0.710&0.667&0.130&0.040*&0.073&0.955&0.745&0.585&0.270&0.973&0.218&0.382&0.478&0.932&0.528&0.343&0.500&0.330\\tabularnewline\nCSCO.O&0.083&0.220&0.047*&0.340&0.797&0.605&0.703&0.355&0.325&0.818&0.000**&0.655&0.488&0.528&0.292&0.218&0.500&0.785&0.537&0.645\\tabularnewline\nKO.N&0.160&0.377&0.095&0.617&0.010*&0.103&0.270&0.405&0.020*&0.073&0.193&0.297&0.365&0.295&0.568&0.162&0.005**&0.015*&0.235&0.575\\tabularnewline\nDD.N&0.330&0.098&0.055&0.047*&0.758&0.620&0.752&0.722&0.330&0.542&0.535&0.792&0.275&0.295&0.323&0.147&0.478&0.705&0.760&0.377\\tabularnewline\nXOM.N&0.955&0.677&0.580&0.738&0.838&0.735&0.375&0.593&0.002**&0.375&0.100&0.015*&0.532&0.105&0.662&0.742&0.333&0.722&0.720&0.575\\tabularnewline\nGE.N&0.698&0.190&0.540&0.262&0.415&0.292&0.660&0.610&0.415&0.377&0.182&0.443&0.633&0.032*&0.260&0.613&0.522&0.277&0.080&0.007**\\tabularnewline\nGS.N&0.655&0.350&0.083&0.595&0.175&0.252&0.657&0.245&0.580&0.145&0.353&0.735&0.752&0.520&0.200&0.490&0.885&0.722&0.838&0.867\\tabularnewline\nHD.N&0.345&0.350&0.000**&0.007**&0.080&0.532&0.767&0.490&0.320&0.532&0.495&0.225&0.062&0.200&0.167&0.115&0.392&0.555&0.080&0.177\\tabularnewline\nINTC.O&0.150&0.902&0.290&0.660&0.088&0.463&0.718&0.910&0.550&0.745&0.000**&0.848&0.248&0.818&0.420&0.570&0.443&0.245&0.330&0.310\\tabularnewline\nIBM.N&0.037*&0.458&0.248&0.390&0.672&0.205&0.318&0.505&0.463&0.800&0.000**&0.017*&0.333&0.722&0.198&0.637&0.088&0.607&0.052&0.498\\tabularnewline\nJNJ.N&0.490&0.292&0.275&0.677&0.703&0.282&0.402&0.930&0.508&0.287&0.108&0.015*&0.000**&0.380&0.020*&0.125&0.562&0.290&0.137&0.015*\\tabularnewline\nJPM.N&0.040*&0.057&0.060&0.353&0.035*&0.017*&0.057&0.262&0.330&0.125&0.000**&0.027*&0.030*&0.010*&0.010*&0.145&0.050&0.012*&0.123&0.000**\\tabularnewline\nMCD.N&0.088&0.943&0.287&0.885&0.037*&0.680&0.287&0.597&0.848&0.780&0.180&0.502&0.740&0.552&0.565&0.090&0.657&0.392&0.840&0.838\\tabularnewline\nMRK.N&0.220&0.675&0.345&0.475&0.595&0.767&0.445&0.132&0.213&0.277&0.307&0.593&0.835&0.518&0.568&0.502&0.532&0.190&0.542&0.653\\tabularnewline\nMSFT.O&0.040*&0.732&0.690&0.200&0.093&0.118&0.162&0.270&0.573&0.390&0.000**&0.468&0.568&0.580&0.302&0.115&0.498&0.407&0.323&0.815\\tabularnewline\nNKE.N&0.032*&0.287&0.435&0.145&0.152&0.020*&0.243&0.483&0.235&0.147&0.000**&0.167&0.390&0.167&0.627&0.110&0.225&0.440&0.593&0.358\\tabularnewline\nPFE.N&0.113&0.057&0.762&0.438&0.758&0.167&0.020*&0.762&0.667&0.680&0.017*&0.267&0.290&0.307&0.198&0.110&0.570&0.993&0.873&0.643\\tabularnewline\nPG.N&0.797&0.387&0.838&0.175&0.645&0.267&0.340&0.118&0.453&0.103&0.215&0.277&0.167&0.297&0.550&0.427&0.200&0.375&0.830&0.988\\tabularnewline\nTRV.N&0.415&0.040*&0.695&0.330&0.177&0.045*&0.427&0.083&0.395&0.395&0.380&0.238&0.270&0.377&0.115&0.115&0.085&0.132&0.255&0.545\\tabularnewline\nUNH.N&0.022*&0.035*&0.002**&0.093&0.037*&0.090&0.015*&0.170&0.123&0.245&0.022*&0.030*&0.050&0.050&0.243&0.213&0.100&0.165&0.402&0.152\\tabularnewline\nUTX.N&0.432&0.528&0.560&0.218&0.280&0.867&0.855&0.228&0.480&0.703&0.713&0.302&0.275&0.848&0.037*&0.682&0.407&0.050&0.325&0.805\\tabularnewline\nVZ.N&0.098&0.650&0.052&0.090&0.108&0.135&0.463&0.748&0.127&0.600&0.090&0.758&0.535&0.782&0.300&0.630&0.198&0.027*&0.007**&0.373\\tabularnewline\nV.N&0.400&0.645&0.782&0.353&0.742&0.603&0.218&0.613&0.818&0.915&0.000**&0.970&0.780&0.400&0.130&0.368&0.802&0.483&0.715&0.905\\tabularnewline\nWMT.N&0.458&0.762&0.787&0.493&0.693&0.262&0.848&0.365&0.365&0.272&0.115&0.382&0.150&0.302&0.267&0.195&0.203&0.445&0.147&0.438\\tabularnewline\nDIS.N&0.510&0.083&0.448&0.630&0.090&0.640&0.985&0.147&0.262&0.795&0.002**&0.742&0.902&0.550&0.762&0.532&0.130&0.935&0.627&0.665\\tabularnewline\n\\hline\n\\end{tabular}\n\n\\end{center}\n\\end{minipage}\n}\n \\end{table} \n\n \n\\begin{table}[!t]\n \\centering\n\\rotatebox{90}{\n\\begin{minipage}{\\textheight}\n\\centering\n\\caption{\\textbf{Significance for linear causality tests.} Lags of up to 10 days were tested \nin both directions of causality: social media causing returns $SM \\rightarrow R$ and the opposite, returns causing social media $R \\rightarrow SM$.  p-value $< 0.05$: *; p-value $< 0.01$: **. }\n\\label{tb:GC-results}\n\\begin{center}\n\n \\begin{tabular}{|l | c c c c c c c c c c | c c c c c c c c c c |} \n \\cline{2-21}\n \\multicolumn{1}{l|}{} & \\multicolumn{10}{c|}{Lagged $R \\rightarrow SM$} & \\multicolumn{10}{c|}{Lagged $SM \\rightarrow R$} \\\\\n\\hline\nTicker & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\\\n\\hline\nMMM.N&0.024*&0.080&0.131&0.213&0.260&0.292&0.308&0.180&0.232&0.240&0.063&0.122&0.129&0.175&0.029*&0.058&0.077&0.131&0.165&0.238\\tabularnewline\nAXP.N&0.584&0.687&0.785&0.812&0.905&0.942&0.971&0.987&0.972&0.937&0.671&0.878&0.241&0.341&0.441&0.398&0.425&0.260&0.386&0.158\\tabularnewline\nAAPL.O&0.068&0.029*&0.066&0.084&0.189&0.073&0.173&0.214&0.284&0.355&0.242&0.434&0.607&0.451&0.527&0.028*&0.039*&0.049*&0.035*&0.026*\\tabularnewline\nBA.N&0.554&0.813&0.943&0.982&0.852&0.878&0.809&0.645&0.667&0.613&0.370&0.574&0.615&0.382&0.469&0.518&0.505&0.518&0.646&0.430\\tabularnewline\nCAT.N&0.141&0.240&0.160&0.264&0.269&0.201&0.302&0.231&0.177&0.232&0.967&0.989&0.991&0.995&0.993&0.994&0.967&0.542&0.609&0.687\\tabularnewline\nCVX.N&0.517&0.658&0.778&0.888&0.294&0.364&0.258&0.253&0.314&0.336&0.678&0.919&0.906&0.965&0.941&0.973&0.986&0.991&0.995&0.997\\tabularnewline\nCSCO.O&0.870&0.954&0.985&0.984&0.959&0.942&0.968&0.453&0.548&0.606&0.834&0.886&0.945&0.980&0.982&0.988&0.979&0.991&0.996&0.995\\tabularnewline\nKO.N&0.753&0.562&0.694&0.776&0.477&0.565&0.687&0.658&0.119&0.211&0.149&0.264&0.024*&0.046*&0.084&0.125&0.198&0.009**&0.012*&0.025*\\tabularnewline\nDD.N&0.283&0.390&0.533&0.088&0.165&0.232&0.332&0.323&0.358&0.419&0.457&0.575&0.759&0.660&0.673&0.557&0.647&0.720&0.809&0.831\\tabularnewline\nXOM.N&0.640&0.282&0.393&0.549&0.650&0.702&0.866&0.927&0.153&0.217&0.192&0.266&0.375&0.112&0.089&0.084&0.111&0.069&0.062&0.096\\tabularnewline\nGE.N&0.465&0.327&0.615&0.528&0.574&0.468&0.602&0.685&0.425&0.531&0.719&0.883&0.954&0.745&0.806&0.887&0.735&0.775&0.755&0.625\\tabularnewline\nGS.N&0.516&0.566&0.822&0.829&0.529&0.517&0.651&0.602&0.628&0.517&0.245&0.287&0.493&0.632&0.708&0.725&0.823&0.831&0.836&0.815\\tabularnewline\nHD.N&0.293&0.461&0.012*&0.031*&0.047*&0.087&0.134&0.153&0.204&0.226&0.773&0.971&0.473&0.269&0.315&0.431&0.641&0.414&0.427&0.463\\tabularnewline\nINTC.O&0.040*&0.089&0.130&0.196&0.280&0.352&0.479&0.507&0.329&0.354&0.000**&0.000**&0.002**&0.004**&0.009**&0.019*&0.012*&0.023*&0.037*&0.079\\tabularnewline\nIBM.N&0.612&0.338&0.258&0.364&0.529&0.345&0.285&0.314&0.300&0.208&0.008**&0.032*&0.073&0.163&0.217&0.128&0.164&0.230&0.264&0.323\\tabularnewline\nJNJ.N&0.278&0.306&0.538&0.484&0.388&0.501&0.532&0.657&0.739&0.754&0.085&0.224&0.244&0.425&0.563&0.189&0.203&0.126&0.097&0.147\\tabularnewline\nJPM.N&0.134&0.010**&0.020*&0.042*&0.078&0.092&0.135&0.176&0.245&0.317&0.010*&0.038*&0.053&0.013*&0.041*&0.077&0.070&0.106&0.151&0.104\\tabularnewline\nMCD.N&0.691&0.542&0.806&0.912&0.936&0.727&0.627&0.694&0.683&0.714&0.176&0.350&0.320&0.529&0.304&0.318&0.250&0.443&0.392&0.482\\tabularnewline\nMRK.N&0.893&0.232&0.314&0.417&0.144&0.167&0.198&0.178&0.103&0.062&0.864&0.965&0.990&0.995&0.990&0.772&0.679&0.863&0.918&0.932\\tabularnewline\nMSFT.O&0.245&0.466&0.666&0.616&0.771&0.211&0.225&0.313&0.319&0.411&0.328&0.347&0.451&0.311&0.406&0.124&0.218&0.112&0.173&0.206\\tabularnewline\nNKE.N&0.054&0.043*&0.028*&0.063&0.104&0.147&0.174&0.180&0.232&0.308&0.008**&0.001**&0.002**&0.003**&0.006**&0.013*&0.017*&0.034*&0.038*&0.050\\tabularnewline\nPFE.N&0.827&0.974&0.968&0.974&0.965&0.957&0.968&0.949&0.695&0.700&0.821&0.972&0.789&0.880&0.887&0.896&0.904&0.927&0.932&0.963\\tabularnewline\nPG.N&0.115&0.236&0.235&0.343&0.409&0.475&0.382&0.463&0.599&0.616&0.668&0.476&0.615&0.552&0.527&0.652&0.749&0.766&0.745&0.825\\tabularnewline\nTRV.N&0.097&0.217&0.262&0.192&0.060&0.036*&0.067&0.014*&0.018*&0.029*&0.532&0.787&0.155&0.229&0.341&0.203&0.284&0.213&0.058&0.081\\tabularnewline\nUNH.N&0.038*&0.093&0.139&0.226&0.102&0.151&0.173&0.250&0.277&0.322&0.116&0.279&0.423&0.583&0.668&0.731&0.779&0.790&0.846&0.912\\tabularnewline\nUTX.N&0.700&0.780&0.973&0.988&0.525&0.486&0.432&0.581&0.635&0.566&0.618&0.761&0.249&0.343&0.402&0.507&0.505&0.706&0.347&0.441\\tabularnewline\nVZ.N&0.348&0.454&0.635&0.808&0.740&0.494&0.456&0.564&0.664&0.748&0.231&0.450&0.586&0.702&0.553&0.434&0.424&0.474&0.026*&0.040*\\tabularnewline\nV.N&0.033*&0.059&0.097&0.209&0.245&0.269&0.222&0.062&0.095&0.130&0.242&0.369&0.204&0.174&0.069&0.113&0.178&0.158&0.141&0.160\\tabularnewline\nWMT.N&0.845&0.993&0.957&0.792&0.921&0.894&0.932&0.856&0.899&0.946&0.147&0.126&0.199&0.178&0.508&0.378&0.534&0.489&0.576&0.674\\tabularnewline\nDIS.N&0.838&0.314&0.513&0.364&0.397&0.519&0.512&0.598&0.656&0.609&0.018*&0.063&0.116&0.212&0.183&0.221&0.005**&0.007**&0.017*&0.024*\\tabularnewline\n\\hline\n\\end{tabular}\n\n\\end{center}\n\\end{minipage}\n}\n \\end{table} \n \n \n\n\n", "itemtype": "equation", "pos": 42962, "prevtext": "\n\nThe problem of selecting the bandwidth $h$ in equation \\eqref{pdf} is crucial in the density estimation. \nA large $h$ will over-smooth the estimated density and mask the structure of the data. \nOn the other hand, a small bandwidth will reduce the bias of the density estimate at the expense of a larger variance in the estimates. \nIf we assume that the true distribution is Gaussian and we use a Gaussian kernel, the optimal value of $h$ that minimizes the mean integrated squared error (MISE) is\n\n", "index": 47, "text": "\\begin{equation*}\nh^* = 1.06\\sigma N^{-1/5},\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"h^{*}=1.06\\sigma N^{-1/5},\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mo>*</mo></msup><mo>=</mo><mrow><mn>1.06</mn><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>5</mn></mrow></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]