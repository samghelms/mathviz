[{"file": "1601.07714.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 8740, "prevtext": "\n \\title{ Log-Normal Matrix Completion for Large Scale Link Prediction}\n \\author{Brian Mohtashemi\n \\and\n Thomas Ketseoglou}\n \\maketitle\n\n\\begin{abstract}\nThe ubiquitous proliferation of online social networks has led to the widescale emergence of relational graphs expressing unique patterns in link formation and descriptive user node features. Matrix Factorization and Completion have become popular methods for Link Prediction due to the low rank nature of mutual node friendship information, and the availability of parallel computer architectures for rapid matrix processing. Current Link Prediction literature has demonstrated vast performance improvement through the utilization of sparsity in addition to the low rank matrix assumption. However, the majority of research has introduced sparsity through the limited $L_1$ or Frobenius norms, instead of considering the more detailed distributions which led to the graph formation and relationship evolution. In particular, social networks have been found to express either Pareto, or more recently discovered, Log Normal distributions. Employing the convexity-inducing Lovasz Extension, we demonstrate how incorporating specific degree distribution information can lead to large scale improvements in Matrix Completion based Link prediction. We introduce Log-Normal Matrix Completion (LNMC), and solve the complex optimization problem by employing Alternating Direction Method of Multipliers. Using data from three popular social networks, our experiments yield up to $5\\%$ AUC increase over top-performing non-structured sparsity based methods.\n\\end{abstract}\n\n\n\n\\section{Introduction}\nAs a result of widespread research on large scale relational data, the matrix completion problem has emerged as a topic of interest in collaborative filtering, link prediction \\cite{15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30}, and machine learning communities. Relationships between products, people, and organizations, have been found to generate low rank sparse matrices, with a broad distribution of rank and sparsity patterns. More specifically, the node degrees in these networks exhibit well known Probability Mass Functions (PMFs), whose parameters can be determined via Maximum Likelihood Estimation. In collaborative filtering or link prediction applications, row and column degrees may be characterized by differing PMFs, which may be harnessed to provide improved estimation accuracy. Directed networks have unique in-degree and out-degree distributions, whereas undirected networks are symmetric and thus exhibit the same row-wise and column wise degree distributions. Though originally thought to follow strict Power Law Distributions, modern social networks have been found to exhibit Log Normal degree patterns in link formation \\cite{2}.\n\\\\\n\\hspace*{.5em} In this work, we propose Log Normal Matrix Completion (LNMC) as an alternative to typical $L_1$ or Frobenius norm constrained matrix completion for Link Prediction. The incorporation of the degree distribution prior generally leads to a non-convex optimization problem. However, by employing the Lovasz extension on the resulting objective, we reduce the problem to a convex minimization over the Lagrangian, which is subsequently solved with Proximal Descent and Alternating Direction Method of Multipliers (ADMM). Through experimentation on Google Plus, Flickr, and Blog Catalog social networks, we demonstrate the advantage of incorporating structured sparsity information in the resulting optimization problem.\n\n\\section{Related Work}\nLink prediction has been thoroughly researched in the field of social network analysis as an essential element in forecasting future relationships, estimating unknown acquaintances, and deriving shared attributes. In particular, \\cite{3} introduces the concept of the Social Attribute Network, and uses it to predict the formation and dissolution of links. Their method combines features from matrix factorization, Adamic Adar, and Random walk with Restart using logistic regression to give link probabilities. However, the calculations of such inputs may be time-intensive, and shared attributes may be unlikely, leading to non-descriptive feature vectors.\n\\\\\n\\hspace*{.5em} Matrix Completion for Link Prediction has previously been investigated within the Positive Unlabeled (PU) Learning Framework, where the nuclear norm regularizes a weighted value-specific objective function \\cite{1}. Although the weighted objective improves the prediction results, the subsequent optimization is non-convex and thus subject to instability. Binary Matrix completion employing proximal gradient descent is studied in \\cite{7}, however, sparsity is not considered, and Link Prediction is not included in the experiment section. The structural constraints that must be satisfied for provably exact completion are described in \\cite{8}. In this technical report, the required cardinality of uniformly selected elements is bounded based on the rank of the matrix. Unique rank bounds for matrix completion are considered in \\cite{9}, where the Schatten p-Norm is utilized on the singluar values of the matrix. Matrix Completion for Power Law distributed samples is studied in \\cite{11}, where various models are compared, including the Random Graph, Chung Lu-Vu, Preferential Attachment, and Forest Fire models. However, link prediction is not considered and the resulting optimization problem is non-convex.\n\\\\\n\\hspace*{.5em} The concept of simultaneously sparse and low rank matrices was introduced in \\cite{4}, where Incremental Proximal Descent is employed to sequentially minimize the objective, and threshold the singular values and matrix entries. Due to the sequentiality of the optimization, the memory footprint is reduced, however, the objective is non-convex and may result in a local minimum solution.  Also, the tested methods employed in simulation are elementary, and more advanced techniques are well known in the link prediction community. Simultaneous row and column-wise sparsity is discussed in \\cite{11}, where a Laplacian based norm is employed on rows and a Dirichlet semi-norm is utilized on columns. A comparison between nuclear and graph based norms is additionally provided. In \\cite{12}, Kim et. al present a matrix factorization method which utilizes group wise sparsity, to enable specifically targeted regularization. However, the datasets which we utilize do not identify group membership, and thus we will not consider affiliation in our prediction models.\n\\\\\n\\hspace*{.5em} Structured sparsity was thoroughly investigated in \\cite{5}, and applied to Graphical Model Learning. However, the paper focuses solely on the Pareto Distribution which characterizes scale-free networks, and does not cover the Log Normal Methods which are presented in this paper. Also, Link Prediction is not considered in the experimental section. Node specific degree priors are introduced in \\cite{6}, and the Lovasz Extension is additionally employed to learn scale free networks commonly formed by Gaussian Models. However, the stability of the edge rank updating is not proven, and Log Normally distributed networks are not considered.\n\\\\\n\\hspace*{.5em} The Lovasz Extension and background theory are presented in \\cite{13}, where Bach provides an overview on submodular functions and minimization.\n\n\\section{Proposed Approach}\n\\subsection{Link Prediction}\nIn this paper, we consider social network graphs, since they have been proven to follow Pareto, and more recently discovered, Log Normal, degree distributions. The Social Network Link Prediction problem involves estimating the link status, $X_{i,j}$, between node $i$ and node $j$, where $X_{i,j}$ is limited to binary outcomes. Together, the set of all nodes, $V$, and links, $E$, form the graph $G=(V,E)$, where $E$ is only partially known. Unknown link statuses may exist when either the relationship between $i$ and $j$ is non-public, or the observation is considered unreliable over several crawls of the social network. Combined, the observations can be expressed in the form of a partial adjacency matrix, $A_{\\Omega}$, which contains all known values in the set of observed pairs, $\\Omega$. Unmeasured states between two nodes are set to 0 in $A_{\\Omega}$. This matrix can be stored in sparse format for memory conservation, and operation complexity reduction.\n\\subsection{Structured Sparsity based Matrix Completion for Link Prediction}\nAs demonstrated in \\cite{1,4,7}, Matrix Completion involves solving for unknown entries in matrices by employing the low-rank assumption in addition to other side information regarding matrix formation and evolution. Traditionally, matrix completion problems are expressed as\n\n", "index": 1, "text": "\\begin{equation}\n\n\\hat{X} = {\\operatorname{\\arg\\!\\min}}_X  \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\lambda \\|X\\|_{*},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\hat{X}={\\operatorname{\\arg\\!\\min}}_{X}\\|A_{\\Omega}-X_{\\Omega}\\|_{F}^{2}+%&#10;\\lambda\\|X\\|_{*},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>X</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>A</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><mo>-</mo><msub><mi>X</mi><mi mathvariant=\"normal\">\u03a9</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\n$\\|\\cdot\\|_{F}$ is the Frobenius norm, and $\\|\\cdot\\|_{*}$ is the nuclear norm (Schatten p-norm with $p=1$). The nuclear norm can be defined as\n\n", "itemtype": "equation", "pos": 8873, "prevtext": "\nwhere\n\n", "index": 3, "text": "\\begin{align*}\nX_{\\Omega \\hspace*{.1em} {i,j}}=\n\\begin{cases}\nX_{i,j}, \\text{$if$ $\\{i,j\\}$ $\\epsilon$ $\\Omega$} \\\\\n0, \\hspace*{1.2em} otherwise, \\\\\n\\end{cases}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle X_{\\Omega\\hskip 1.0pt{i,j}}=\\begin{cases}X_{i,j},\\text{$if$ $\\{i%&#10;,j\\}$ $\\epsilon$ $\\Omega$}\\\\&#10;0,\\hskip 12.0ptotherwise,\\\\&#10;\\end{cases}\" display=\"inline\"><mrow><msub><mi>X</mi><mrow><mrow><mpadded width=\"+1pt\"><mi mathvariant=\"normal\">\u03a9</mi></mpadded><mo>\u2062</mo><mi>i</mi></mrow><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow><mtext>\u00a0</mtext><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">}</mo></mrow><mtext>\u00a0</mtext><mi>\u03f5</mi><mtext>\u00a0</mtext><mi mathvariant=\"normal\">\u03a9</mi></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>0</mn><mo rspace=\"14.5pt\">,</mo><mrow><mi>o</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>w</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>e</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd/></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $\\sigma_i$ is the $i_{th}$ eigenvalue, when arranged in decreasing order, and $m$ and $n$ are the row count and column count, respectively. In this paper, $m$ is assumed equal to $n$. $\\hat{X}$ is the estimated complete matrix after convergence is attained. Generally, these problems are solved using proximal gradient descent, which employs singular value thresholding on each iteration \\cite{31}. However, this problem generally lacks incorporation of prior sparsity information encoded into the matrix. Thus we augment the problem as\n\n", "itemtype": "equation", "pos": 9191, "prevtext": "\n$\\|\\cdot\\|_{F}$ is the Frobenius norm, and $\\|\\cdot\\|_{*}$ is the nuclear norm (Schatten p-norm with $p=1$). The nuclear norm can be defined as\n\n", "index": 5, "text": "\\begin{equation}\n\\|X\\|_{*} = \\sum_{i=1}^{min\\{m,n\\}} \\sigma_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\|X\\|_{*}=\\sum_{i=1}^{min\\{m,n\\}}\\sigma_{i},\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mo>*</mo></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy=\"false\">}</mo></mrow></mrow></munderover><msub><mi>\u03c3</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $G$ is defined as follows:\n\n", "itemtype": "equation", "pos": 9812, "prevtext": "\nwhere $\\sigma_i$ is the $i_{th}$ eigenvalue, when arranged in decreasing order, and $m$ and $n$ are the row count and column count, respectively. In this paper, $m$ is assumed equal to $n$. $\\hat{X}$ is the estimated complete matrix after convergence is attained. Generally, these problems are solved using proximal gradient descent, which employs singular value thresholding on each iteration \\cite{31}. However, this problem generally lacks incorporation of prior sparsity information encoded into the matrix. Thus we augment the problem as\n\n", "index": 7, "text": "\\begin{equation}\n\\hat{X} = {\\operatorname{\\arg\\!\\min}}_X \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\lambda_1 \\|X\\|_{*} + G(X),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\hat{X}={\\operatorname{\\arg\\!\\min}}_{X}\\|A_{\\Omega}-X_{\\Omega}\\|_{F}^{2}+%&#10;\\lambda_{1}\\|X\\|_{*}+G(X),\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>X</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>A</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><mo>-</mo><msub><mi>X</mi><mi mathvariant=\"normal\">\u03a9</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nHere, $\\Gamma_{i,\\alpha}(X)$ is a sparsity inducing term, where $i$ implies that the sparsity is applied on matrix rows, j implies sparsity is applied on matrix columns, $\\alpha$ is the prior in-degree distribution, and $\\beta$ is the out-degree distribution.\n\n\n\n\n\n\n\nFor the rest of this paper, we will consider the case of symmetric adjacency matrices, and thus set $\\lambda_3$ to $0$.\n\\subsection{Log-Normal Degree Prior}\nAs demonstrated in \\cite{2}, many social networks, including Google+, tend to exhibit the Log-Normal Degree Distribution\n\n", "itemtype": "equation", "pos": 9979, "prevtext": "\nwhere $G$ is defined as follows:\n\n", "index": 9, "text": "\\begin{equation}\nG(X) = \\lambda_2 \\Gamma_{i,\\alpha}(X) + \\lambda_3 \\Gamma_{j,\\beta}(X).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"G(X)=\\lambda_{2}\\Gamma_{i,\\alpha}(X)+\\lambda_{3}\\Gamma_{j,\\beta}(X).\" display=\"block\"><mrow><mrow><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mrow><mi>i</mi><mo>,</mo><mi>\u03b1</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>3</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mrow><mi>j</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nThus we derive $\\Gamma(X)$ as the Maximum Likelihood Estimate\n\n", "itemtype": "equation", "pos": 10627, "prevtext": "\nHere, $\\Gamma_{i,\\alpha}(X)$ is a sparsity inducing term, where $i$ implies that the sparsity is applied on matrix rows, j implies sparsity is applied on matrix columns, $\\alpha$ is the prior in-degree distribution, and $\\beta$ is the out-degree distribution.\n\n\n\n\n\n\n\nFor the rest of this paper, we will consider the case of symmetric adjacency matrices, and thus set $\\lambda_3$ to $0$.\n\\subsection{Log-Normal Degree Prior}\nAs demonstrated in \\cite{2}, many social networks, including Google+, tend to exhibit the Log-Normal Degree Distribution\n\n", "index": 11, "text": "\\begin{equation}\np(d) = \\frac{1}{d \\sigma \\sqrt{2\\pi}} e^{-\\frac{(\\ln d - \\mu)^2}{2\\sigma^2}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"p(d)=\\frac{1}{d\\sigma\\sqrt{2\\pi}}e^{-\\frac{(\\ln d-\\mu)^{2}}{2\\sigma^{2}}}.\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>d</mi><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mrow></mfrac><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>d</mi></mrow><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $d_{X_i}$ is the degree of the $i_{th}$ row of $X$, which simplifies to the following:\n\n", "itemtype": "equation", "pos": 10799, "prevtext": "\nThus we derive $\\Gamma(X)$ as the Maximum Likelihood Estimate\n\n", "index": 13, "text": "\\begin{equation}\n\\Gamma(X) = -\\ln \\prod _{i}p(d_{X_i}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\Gamma(X)=-\\ln\\prod_{i}p(d_{X_{i}}),\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>ln</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>i</mi></munder><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>d</mi><msub><mi>X</mi><mi>i</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nThis is equivalent to a summation of scaled Pareto Distributions with shape parameter 1 added to additional square terms. Thus the final optimization problem becomes\n\n", "itemtype": "equation", "pos": 10963, "prevtext": "\nwhere $d_{X_i}$ is the degree of the $i_{th}$ row of $X$, which simplifies to the following:\n\n", "index": 15, "text": "\\begin{equation} \\label{eq:-1}\n\\Gamma(X) = \\sum_{i} \\ln(d_{X_{i}}\\sigma \\sqrt{2 \\pi}) + \\frac{(\\ln d_{X_{i}}-\\mu)^2}{2\\sigma^2}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\Gamma(X)=\\sum_{i}\\ln(d_{X_{i}}\\sigma\\sqrt{2\\pi})+\\frac{(\\ln d_{X_{i}}-\\mu)^{2%&#10;}}{2\\sigma^{2}}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>d</mi><msub><mi>X</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>d</mi><msub><mi>X</mi><mi>i</mi></msub></msub></mrow><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nDue to the presence of the log term in the optimization, convex methods cannot be directly applied to the minimization, since the problem is not guaranteed to have an absolute minimum. Optimization of this problem is a multi-part minimization, which can be solved using the Alternating Direction Method of Multipliers (ADMM).\n\n\\subsection{Optimization}\nADMM allows the optimization problem to be split into less complex sub-problems, which can be solved using convex minimization techniques. In order to decouple (\\ref{eq:zz}) into smaller subproblems, the additional variable, $Y$, is introduced as\n\n", "itemtype": "equation", "pos": 11273, "prevtext": "\nThis is equivalent to a summation of scaled Pareto Distributions with shape parameter 1 added to additional square terms. Thus the final optimization problem becomes\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:zz}\n\\begin{split}\n\\hat{X} = {\\operatorname{\\arg\\!\\min}}_X \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\lambda_1 \\|X\\|_{*} +\\\\\n \\lambda_2 \\sum_i \\ln(d_{X_i}\\sigma \\sqrt{2 \\pi}) + \\frac{(\\ln d_{X_i}-\\mu)^2}{2\\sigma^2}.\n \\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\hat{X}={\\operatorname{\\arg\\!\\min}}_{X}\\|A_{\\Omega}-%&#10;X_{\\Omega}\\|_{F}^{2}+\\lambda_{1}\\|X\\|_{*}+\\\\&#10;\\displaystyle\\lambda_{2}\\sum_{i}\\ln(d_{X_{i}}\\sigma\\sqrt{2\\pi})+\\frac{(\\ln d_{%&#10;X_{i}}-\\mu)^{2}}{2\\sigma^{2}}.\\end{split}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>X</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>A</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><mo>-</mo><msub><mi>X</mi><mi mathvariant=\"normal\">\u03a9</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>d</mi><msub><mi>X</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow></msqrt></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>d</mi><msub><mi>X</mi><mi>i</mi></msub></msub></mrow><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nExpressing the problem in ADMM update form, the sequential optimization becomes\n\n", "itemtype": "equation", "pos": 12134, "prevtext": "\nDue to the presence of the log term in the optimization, convex methods cannot be directly applied to the minimization, since the problem is not guaranteed to have an absolute minimum. Optimization of this problem is a multi-part minimization, which can be solved using the Alternating Direction Method of Multipliers (ADMM).\n\n\\subsection{Optimization}\nADMM allows the optimization problem to be split into less complex sub-problems, which can be solved using convex minimization techniques. In order to decouple (\\ref{eq:zz}) into smaller subproblems, the additional variable, $Y$, is introduced as\n\n", "index": 19, "text": "\\begin{gather*}\n{\\operatorname{\\arg\\!\\min}}_X \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\lambda_1 \\|X\\|_{*} + \\Gamma(Y)\\\\\n\\text{s.t. } X=Y.\n\\end{gather*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\operatorname{\\arg\\!\\min}}_{X}\\|A_{\\Omega}-X_{\\Omega}\\|_{F}^{2}+%&#10;\\lambda_{1}\\|X\\|_{*}+\\Gamma(Y)\" display=\"block\"><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>X</mi></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>A</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><mo>-</mo><msub><mi>X</mi><mi mathvariant=\"normal\">\u03a9</mi></msub></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>X</mi><mo>\u2225</mo></mrow><mo>*</mo></msub></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{s.t. }X=Y.\" display=\"block\"><mrow><mrow><mrow><mtext>s.t.\u00a0</mtext><mo>\u2062</mo><mi>X</mi></mrow><mo>=</mo><mi>Y</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nIn practice, step size values, $\\mu$, in the range $[.01,.1]$ have been found to work well. Convergence is assumed, and the sequence is terminated once $\\|X^{k+1} - X^{k}\\|^2_F < \\delta $. The initial values, $X^0$, $Y^0$ and $V^0$ are set to zeros matrices. Although ADMM has slow convergence properties, a relatively accurate solution can be attained in a few iterations. Due to the convexity of the initial equation, proximal gradient descent is employed for minimization.\nThe proximal gradient method minimizes problems of the form\n\n", "itemtype": "equation", "pos": 12360, "prevtext": "\nExpressing the problem in ADMM update form, the sequential optimization becomes\n\n", "index": 21, "text": "\\begin{align}\n&X^{k+1} = {\\operatorname{\\arg\\!\\min}}_X \\hspace{.2em} \\{ \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\lambda_1 \\|X\\|_{*}\\\\ \\nonumber& \\hspace*{3.5em} +\\frac{\\mu}{2} \\|X - Y^{k} + V^{k} \\|_{F}^2 \\} \\\\\n&\\label{eq:0}Y^{k+1} = {\\operatorname{\\arg\\!\\min}}_Y \\lambda_2 \\Gamma(Y) + \\frac{\\mu}{2} \\|X^{k+1} - Y + V^{k} \\|_F^2 \\\\\n&V^{k+1} = V^{k} + X^{k+1} - Y^{k+1}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle X^{k+1}={\\operatorname{\\arg\\!\\min}}_{X}\\hskip 2.0pt\\{\\|A_{\\Omega%&#10;}-X_{\\Omega}\\|_{F}^{2}+\\lambda_{1}\\|X\\|_{*}\" display=\"inline\"><mrow><msup><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mpadded width=\"+2pt\"><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>X</mi></msub></mpadded><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mi>A</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><mo>-</mo><msub><mi>X</mi><mi mathvariant=\"normal\">\u03a9</mi></msub><msubsup><mo>\u2225</mo><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2225</mo><mi>X</mi><msub><mo>\u2225</mo><mo>*</mo></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0+\\frac{\\mu}{2}\\|X-Y^{k}+V^{k}\\|_{F}^{2}\\}\" display=\"inline\"><mrow><mi>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</mi><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2225</mo><mi>X</mi><mo>-</mo><msup><mi>Y</mi><mi>k</mi></msup><mo>+</mo><msup><mi>V</mi><mi>k</mi></msup><msubsup><mo>\u2225</mo><mi>F</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle Y^{k+1}={\\operatorname{\\arg\\!\\min}}_{Y}\\lambda_{2}\\Gamma(Y)+%&#10;\\frac{\\mu}{2}\\|X^{k+1}-Y+V^{k}\\|_{F}^{2}\" display=\"inline\"><mrow><msup><mi>Y</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>Y</mi></msub><mo>\u2061</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>-</mo><mi>Y</mi></mrow><mo>+</mo><msup><mi>V</mi><mi>k</mi></msup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle V^{k+1}=V^{k}+X^{k+1}-Y^{k+1}.\" display=\"inline\"><mrow><mrow><msup><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mrow><msup><mi>V</mi><mi>k</mi></msup><mo>+</mo><msup><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo>-</mo><msup><mi>Y</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nusing the gradient and proximal operator as\n\n", "itemtype": "equation", "pos": 13272, "prevtext": "\nIn practice, step size values, $\\mu$, in the range $[.01,.1]$ have been found to work well. Convergence is assumed, and the sequence is terminated once $\\|X^{k+1} - X^{k}\\|^2_F < \\delta $. The initial values, $X^0$, $Y^0$ and $V^0$ are set to zeros matrices. Although ADMM has slow convergence properties, a relatively accurate solution can be attained in a few iterations. Due to the convexity of the initial equation, proximal gradient descent is employed for minimization.\nThe proximal gradient method minimizes problems of the form\n\n", "index": 23, "text": "\\begin{equation}\n\\text{minimize } g(X) + h(X),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\text{minimize }g(X)+h(X),\" display=\"block\"><mrow><mrow><mrow><mtext>minimize\u00a0</mtext><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $\\psi^{l+1} = \\phi \\psi^{l}$, and $\\phi$ is a multiplier utilized on each gradient descent round. Typically a value of $.5$ is sufficient for $\\phi$, leading to rapid convergence in $10$ rounds, however, a value $<.5$ would result in slower, but more accurate minimization. The optimal value for $\\psi^0$ is determined through experimentation. For Log-Normal Matrix Completion, $g(X) = \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\frac{\\mu}{2} \\|X-Y^k + V^k \\|^{2}_F$, and $h(X) = \\lambda \\|X\\|_{*}$. The proximal operator of $h(X)$ becomes a sequential thresholding on the eigenvalues, $\\sigma$, of the argument in (\\ref{eq:a})\n\n", "itemtype": "equation", "pos": 13378, "prevtext": "\nusing the gradient and proximal operator as\n\n", "index": 25, "text": "\\begin{equation}\\label{eq:a}\nX^{k,l+1} = \\text{prox}_{\\psi^l h}(X^{k,l} - \\psi^{l}\\nabla g(X^{k,l})),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"X^{k,l+1}=\\text{prox}_{\\psi^{l}h}(X^{k,l}-\\psi^{l}\\nabla g(X^{k,l})),\" display=\"block\"><mrow><mrow><msup><mi>X</mi><mrow><mi>k</mi><mo>,</mo><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></mrow></msup><mo>=</mo><mrow><msub><mtext>prox</mtext><mrow><msup><mi>\u03c8</mi><mi>l</mi></msup><mo>\u2062</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msup><mo>-</mo><mrow><msup><mi>\u03c8</mi><mi>l</mi></msup><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $Q$ is the matrix of eigenvectors. The subproblem reaches convergence when $\\|X^{k,l+1} - X^{k,l}\\|_F^2 < \\kappa $.\nThe noise of the matrix is reduced through sequential thresholding, leaving only the strongest components of the low rank matrix. This algorithm is advantageous due to rapid convergence properties and automatic rank selection. Known as the Iterative Soft Thresholding Algorithm (ISTA), this method can be parallelized for gradient calculation and recombined for the Eigenvalue decomposition. Although the interim result of each round of minimization is generally not sparse, matrix entries with values below a given threshhold can be forced to 0 to allow sparse matrix Eigenvalue Decomposition (such as eigs in Matlab) to be performed with minimal error.\n\\subsection{Lovasz Extension}\n  (\\ref{eq:0}) is a non-convex optimization problem due to the log of the set cardinality function. However, the problem can be altered into a convex form using the Lovasz Extension on the submodular set function. As described in \\cite{13}, the Lovasz Extension takes on the following form:\n\n", "itemtype": "equation", "pos": 14120, "prevtext": "\nwhere $\\psi^{l+1} = \\phi \\psi^{l}$, and $\\phi$ is a multiplier utilized on each gradient descent round. Typically a value of $.5$ is sufficient for $\\phi$, leading to rapid convergence in $10$ rounds, however, a value $<.5$ would result in slower, but more accurate minimization. The optimal value for $\\psi^0$ is determined through experimentation. For Log-Normal Matrix Completion, $g(X) = \\|A_{\\Omega} - X_{\\Omega}\\|_F^2 + \\frac{\\mu}{2} \\|X-Y^k + V^k \\|^{2}_F$, and $h(X) = \\lambda \\|X\\|_{*}$. The proximal operator of $h(X)$ becomes a sequential thresholding on the eigenvalues, $\\sigma$, of the argument in (\\ref{eq:a})\n\n", "index": 27, "text": "\\begin{equation}\n\\text{prox}_{\\psi h} = Q \\text{ diag }((\\sigma_i - \\psi)_{+})_i Q^T,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\text{prox}_{\\psi h}=Q\\text{ diag }((\\sigma_{i}-\\psi)_{+})_{i}Q^{T},\" display=\"block\"><mrow><mrow><msub><mtext>prox</mtext><mrow><mi>\u03c8</mi><mo>\u2062</mo><mi>h</mi></mrow></msub><mo>=</mo><mrow><mi>Q</mi><mo>\u2062</mo><mtext>\u00a0diag\u00a0</mtext><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>-</mo><mi>\u03c8</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><mo>\u2062</mo><msup><mi>Q</mi><mi>T</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\n Here, $z$ is a permutation of $j$ which ensures components of $w$ are ordered in decreasing fashion, $w_{z_1}\\geq w_{z_2}\\geq w_{z_n}$, and $F$ is a submodular set function. The Lovasz Extension is always convex when $F$ is submodular, thus allowing convex optimization techniques to be used on the resulting transformed problem.\n \\\\\nIn order to transform each individual row of sampled relationship information into a set, $S$, the support function, $S_i = \\text{Supp }(X_i) $ is utilized. As a result $S_i \\epsilon \\{0,1\\}^n$, where $n$ is the number of columns present in the matrix X. A submodular set function must obey the relationship\n\n", "itemtype": "equation", "pos": 15319, "prevtext": "\nwhere $Q$ is the matrix of eigenvectors. The subproblem reaches convergence when $\\|X^{k,l+1} - X^{k,l}\\|_F^2 < \\kappa $.\nThe noise of the matrix is reduced through sequential thresholding, leaving only the strongest components of the low rank matrix. This algorithm is advantageous due to rapid convergence properties and automatic rank selection. Known as the Iterative Soft Thresholding Algorithm (ISTA), this method can be parallelized for gradient calculation and recombined for the Eigenvalue decomposition. Although the interim result of each round of minimization is generally not sparse, matrix entries with values below a given threshhold can be forced to 0 to allow sparse matrix Eigenvalue Decomposition (such as eigs in Matlab) to be performed with minimal error.\n\\subsection{Lovasz Extension}\n  (\\ref{eq:0}) is a non-convex optimization problem due to the log of the set cardinality function. However, the problem can be altered into a convex form using the Lovasz Extension on the submodular set function. As described in \\cite{13}, the Lovasz Extension takes on the following form:\n\n", "index": 29, "text": "\\begin{equation}\nf(w) = \\sum_{j=1}^n w_{z_j} [ F(\\{z_1,... , z_j\\}) - F(\\{z_1,... , z_{j-1}\\})].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"f(w)=\\sum_{j=1}^{n}w_{z_{j}}[F(\\{z_{1},...,z_{j}\\})-F(\\{z_{1},...,z_{j-1}\\})].\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>w</mi><msub><mi>z</mi><mi>j</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nwhere $A \\subseteq B$, and ${p}$ is an additional set element. In this paper, $F$ is a log-normal transformation on the degree $d$. The degree, $d_i = \\sum_i^n S_{i,j}$, is modular, and thus follows (\\ref{eq:1}) with strict equality. Thus for $F$ to be sub modular, the subsequent transformation of the degree must be submodular as well.\n\\\\\nAfter applying the Lovasz Extension to (\\ref{eq:-1}), the result is\n\n", "itemtype": "equation", "pos": 16074, "prevtext": "\n Here, $z$ is a permutation of $j$ which ensures components of $w$ are ordered in decreasing fashion, $w_{z_1}\\geq w_{z_2}\\geq w_{z_n}$, and $F$ is a submodular set function. The Lovasz Extension is always convex when $F$ is submodular, thus allowing convex optimization techniques to be used on the resulting transformed problem.\n \\\\\nIn order to transform each individual row of sampled relationship information into a set, $S$, the support function, $S_i = \\text{Supp }(X_i) $ is utilized. As a result $S_i \\epsilon \\{0,1\\}^n$, where $n$ is the number of columns present in the matrix X. A submodular set function must obey the relationship\n\n", "index": 31, "text": "\\begin{equation} \\label{eq:1}\nF(A \\cup \\{p\\}) - F(A) \\geq F(B \\cup \\{p\\}) - F(B),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"F(A\\cup\\{p\\})-F(A)\\geq F(B\\cup\\{p\\})-F(B),\" display=\"block\"><mrow><mrow><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>\u222a</mo><mrow><mo stretchy=\"false\">{</mo><mi>p</mi><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>B</mi><mo>\u222a</mo><mrow><mo stretchy=\"false\">{</mo><mi>p</mi><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nHere, $|X|$ is used in order to maintain the positivity required for the Lovasz Extension to remain convex. Further details regarding the optimization of this problem can be obtained in Appendix A.\n\\subsection{Considerations}\nIn order for (\\ref{eq:3}) to be utilized, (\\ref{eq:-1}) must remain a submodular function of the degree. Thus, both the first derivative and the second derivative of the function must remain positive, creating the following constraint:\n\n", "itemtype": "equation", "pos": 16580, "prevtext": "\nwhere $A \\subseteq B$, and ${p}$ is an additional set element. In this paper, $F$ is a log-normal transformation on the degree $d$. The degree, $d_i = \\sum_i^n S_{i,j}$, is modular, and thus follows (\\ref{eq:1}) with strict equality. Thus for $F$ to be sub modular, the subsequent transformation of the degree must be submodular as well.\n\\\\\nAfter applying the Lovasz Extension to (\\ref{eq:-1}), the result is\n\n", "index": 33, "text": "\\begin{align}\\label{eq:3}\n&\\Gamma(X) = \\sum_{i=1}^m \\sum_{j=1}^n [\\text{ln}^2 (j+1) - \\text{ln}^2(j)\\\\ \\nonumber &+ \\frac{(\\sigma^2-\\mu)(\\text{ln}(j+1) - \\text{ln}(j))}{\\sigma^2}]|X_{i,j}|.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Gamma(X)=\\sum_{i=1}^{m}\\sum_{j=1}^{n}[\\text{ln}^{2}(j+1)-\\text{%&#10;ln}^{2}(j)\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0393</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo stretchy=\"false\">[</mo><msup><mtext>ln</mtext><mn>2</mn></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msup><mtext>ln</mtext><mn>2</mn></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{(\\sigma^{2}-\\mu)(\\text{ln}(j+1)-\\text{ln}(j))}{\\sigma^{2}}%&#10;]|X_{i,j}|.\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mtext>ln</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mtext>ln</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><msup><mi>\u03c3</mi><mn>2</mn></msup></mfrac></mstyle><mo stretchy=\"false\">]</mo><mo stretchy=\"false\">|</mo><mi>X</mi><msub><mi/><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\n $\\tau$ is introduced to prevent the left side of the inequality from approaching $-\\infty$. In practice, a small constant is also subtracted or added from the obtained set function in order to assure that $F(\\emptyset) = 0$. These small coefficients are determined during the Cross Validation phase, after obtaining the optimal $\\sigma$ and $\\mu$ values which satisfy the given constraints.\n\\section{Experiment}\nIn order to compare the performance of the LNMC method with other popular Link Prediction methods, an experiment was performed using several data sets from existing literature:\n\\begin{enumerate}\n\\item Google + - The Google + dataset \\cite{3} contains $5,200$ nodes and  $24,690$ links, captured in AUG 2011. The data contains both Graph topology and node attribute information; however, the side-features are removed since our method requires edge status only.\n\\item Flickr -  Flickr is a social network based on image hosting, where users form communities and friendships based on common interests. The Flickr dataset \\cite{14} contains $80,513$ nodes, $5,899,882$ links, and $195$ groups. Group affiliation was discarded due to irrelevance to the LNMC method.\n\\item Blog Catalog - Blog Catalog \\cite{14} is a blogging site where users can form friendships, and acquire group membership. The utilized dataset contains $10,312$ nodes, $333,983$ links, and $39$ groups. Again, for the context of this paper, the group information was removed.\n\\end{enumerate}\n\nAs seen in Fig. \\ref{fig:22}, all datasets follow a roughly Log-Normal distribution, with varying amounts of degree sparsity, and variance. Due to the high number of low degree nodes in the Google+ dataset, all points appear constrained to the left of the plot axis; however, as we will illustrate, the Log-Normal Distribution is still superior to the Pareto Distribution for link prediction.\nDuring the training phase, $10\\%$ of the data was removed in order to use for future predictions. For the purposes of demonstration, only $1,000$ of the highest degree nodes are maintained for adjacency matrix formation.\n\n\n\\begin{figure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{gplus_data_dist}\n                \\centering\n                \\caption{Google+  }\n                \\label{fig:x}\n        \\end{subfigure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{flickr_data_dist}\n                \\centering\n                \\caption{Flickr}\n                \\label{fig:y}\n        \\end{subfigure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{bc_data_dist}\n                \\centering\n                \\caption{Blog Cat.}\n                \\label{fig:z}\n        \\end{subfigure}\n        \\caption{Empirical Node Degree Data and Fitted Log-Normal Probability Distribution Functions}\\label{fig:animals}\n\\label{fig:22}\n\\end{figure}\n\n\n\n\\section{Results}\n\\subsection{Baseline Methods and Performance Metrics}\nIn order to understand the advantage of LNMC, the results are compared against the following methods:\n\n\\begin{enumerate}\n\\item Matrix Completion with Pareto Sparsity (MCPS) - MCPS \\cite{5} utilizes the same algorithm which we have outlined in the paper with the exception of the prior. MCPS employs the Pareto Distribution $f(d) = (\\frac{\\delta}{d})^\\chi $.\n\\item Matrix Completion with $L_1$ Sparsity (MCLS) - MCLS is used by Richard et al. \\cite{4}, and represents one of the first attempts at incorporating $L_1$ sparsity with the Low Rank assumption.\n\\item Logistic Regression (MF + RwR + AA) - In their paper on Social Attribute Networks, Gong et al. \\cite{2} provide a method which combines features from Matrix Factorization, Random Walks with Restart, and Adamic Adar, which effectively solves the link prediction problem with high accuracy. In this paper, the attributes are removed from the network for equal comparison with our method.\n\\end{enumerate}\n\nIn order to provide a fair basis on which to judge the performance, Area Under the Curve (AUC) is employed for comparison. By utilizing the AUC as the performance metric, we avoid the need for data balancing, a process which frequently results in undersampling negative samples. Thus, all methods can benefit from the additional training data.\n\\\\\nThe results are obtained via $10-\\text{fold}$ Cross Validation, using a random sampling method for hyper-parameter selection. The rounds are averaged to produce the results shown in Table \\ref{table:1}.\n\\subsection{Performance Comparison}\nAs demonstrated in Fig. \\ref{fig:1}, LNMC outperforms MCPS, MCLS, and LR, on the Google Plus dataset. Due to the highly Log-Normal characteristic \\cite{2} of the data set, LNMC's fine-tuned degree specific prior captures the degree distribution behavior in combination with the low rank features of the data, leading to high AUC values. The high number of true positives compared to the false positive rate leads to jagged graph distribution. In Fig \\ref{fig:2}, it is clear that matrix completion with Pareto Sparsity produces low AUC values due to the inaccurate distribution representation. Similarly the LR method fails to capture accurate low rank information because the low rank matrix factorization is done prior to the the gradient descent training for Logistic Regression.\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{gplus_data_plot4}\n\\caption{Receiver Operating Characteristic for Google Plus Data}\n\\label{fig:1}\n\\end{figure}\nDue to the Pareto nature of the Flickr dataset, both the LNMC and MCPS methods perform the same. As can be seen in (\\ref{eq:3}), LNMC can adapt to Scale Free Networks when the first term is small compared to the second term. Logistic Regression performs poorly since the features are set, whereas Matrix Completion methods automatically select the number of latent parameters to utilize.\n\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{flickr_data_plot4}\n\\caption{Receiver Operating Characteristic for Flickr Data}\n\\label{fig:2}\n\\end{figure}\nAs seen in Fig. \\ref{fig:3}, LNMC outperforms the Pareto Sparisty based matrix completion, due to the inclusion of the squared log terms. The $L_1$ sparsity used in the MCLS method is insufficiently descriptive for accurate matrix estimation. Thus Logistic Regression, which incorporates more descriptive features outperforms the MCLS method.\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{bc_data_plot4}\n\\caption{Receiver Operating Characteristic for Blog Catalog Data}\n\\label{fig:3}\n\\end{figure}\nFor purposes of comparison, AUC values for each method and dataset, are contained in Table \\ref{table:1}. As highlighted by the AUC Table, LNMC provides optimal results over all datasets.\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|}\n      \\hline\n      Data Set & LNMC & MCPS & MCLS & LR(MF+RwR+AA)\\\\\n      \\hline\n      Google+ & .8541 & .8439 & .8113 & .8434\\\\\n      \\hline\n      Flickr & .9052 & .9052 & .8504 & .8972\\\\\n      \\hline\n      Blog Catalog & .7918 & .7846 & .7150 & .7727\\\\\n      \\hline\n\\end{tabular}\n\\vspace{.5cm}\\\\\n\\caption{AUC Performance Comparison}\n\\label{table:1}\n\\end{center}\n\\end{table}\n\\section{Conclusion}\nAs demonstrated both theoretically, and experimentally, LNMC is able to sufficiently encapsulate the advantages of Pareto Sparsity in addition to Log Normal Sparsity. Previously described by Gong et al. in \\cite{2},\nmany modern social networks with undirected graph topologies exhibit Log Normal degree distributions. Thus by incorporating the degree-specific prior the optimization encourages convergence to a Log-Normal degree distribution.\nDue to the non-convexity of solving the joint low-rank and structured sparsity inducing prior, the Lovasz Extension is introduced to solve the complex problem efficiently. Through analysis on three datasets, and using 3 top performing methods, we provide results which exceed the current optimum.\nThese results reveal the fundamental value of prior degree information in Link Prediction, and can provide insight into understanding the complex dynamics which cause links to form in a similar way across different networks.\n\\\\\nIn future research we plan to investigate the incorporation of side information into the objective. Node attributes introduce additional challenges, including missing features, and additional training complexities.\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{references}\n\n\\appendix\nAs seen in \\cite{5}, the optimization of (\\ref{eq:0}) is performed by first imposing the symmetry constraint on $Y$ as\n\n", "itemtype": "equation", "pos": 17244, "prevtext": "\nHere, $|X|$ is used in order to maintain the positivity required for the Lovasz Extension to remain convex. Further details regarding the optimization of this problem can be obtained in Appendix A.\n\\subsection{Considerations}\nIn order for (\\ref{eq:3}) to be utilized, (\\ref{eq:-1}) must remain a submodular function of the degree. Thus, both the first derivative and the second derivative of the function must remain positive, creating the following constraint:\n\n", "index": 35, "text": "\\begin{equation}\n\\text{ln}(d+\\tau) \\geq (1 + \\mu - \\sigma^2).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\text{ln}(d+\\tau)\\geq(1+\\mu-\\sigma^{2}).\" display=\"block\"><mrow><mrow><mrow><mtext>ln</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>+</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>1</mn><mo>+</mo><mi>\u03bc</mi></mrow><mo>-</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07714.tex", "nexttext": "\nThis minimization leads to the following algorithm:\n\n\\begin{algorithm}[!htb]\n \\KwData{$X^{k+1}, V^{k},\\mu, Yinit=(X^{k+1}+V^{k})$}\n \\KwData{$\\gamma, U=0_N,\\omega$}\n \\KwResult{$Y$}\n initialization\\;\n \\While{$\\|Y-Y^T\\|_2 < \\omega$}{\n \\For{ $r=0 \\to N-1$}{\n $Y_{r,*} = \\text{LovaszOptimize}(Yinit_{r,*},U_{r,*})$\n }\n $U=U+\\gamma(Y-Y^T)$\n }\n $Y=\\frac{1}{2}(Y+Y^T)$\\\\\n \\Return{$Y$}\n \\vspace{.5cm}\\\\\n \\caption{Optimization with Symmetry Constraint}\n\\end{algorithm}\n\n\\begin{algorithm}[!htb]\n \\KwData{$yinit, u, M$}\n \\KwData{$d=yinit-u, p = 0_M$}\n \\KwData{Set membership function $\\zeta$}\n \\KwData{$\\theta$ transformation which translates sorted position index to original index}\n \\KwResult{$y$}\n initialization\\;\n\\For{$l=0 \\to M-1$}\n{\n$q = \\theta(l)$\n$p_{q} = |d_q|- \\frac{\\lambda_2}{\\mu}(\\text{ln}^2 (l+1) - \\text{ln}^2(l)+ \\frac{(\\sigma^2-\\mu)(\\text{ln}(l+1) - \\text{ln}(l))}{\\sigma^2})$\n$\\zeta(q).\\text{value}=p_{q}$\n$r=l$\\\\\n\\While{$r>1 \\text{ and } \\zeta(\\theta(r)).\\text{value} \\geq \\zeta(\\theta(r-1)).\\text{value}$}\n{\n$\\text{Join the sets containing } \\theta(r) \\text{ and } \\theta(r-1)$\n$\\zeta(\\theta(r)).\\text{value} = \\frac{1}{|\\zeta(\\theta(r))}\\sum_{i \\epsilon \\zeta(\\theta(r))}p_i$\n$\\text{set: r to the first element of }\\zeta(\\theta(r))\\text{ by sort ordering}$\n}\n}\n\\For{$j=1$ to \\text{N}}\n{\n$y_j = \\zeta(i).\\text{value}$\n\\If {$y_j < 0$}\n{\n$y_q = 0$\n}\n\\If {$d_i < 0$}\n{\n $y_q = -y_q$\n}\n}\n\\Return{ $y$}\n \\vspace{.5cm}\n \\caption{LovaszOptimize Problem}\n\\end{algorithm}\n\n\n", "itemtype": "equation", "pos": 25980, "prevtext": "\n $\\tau$ is introduced to prevent the left side of the inequality from approaching $-\\infty$. In practice, a small constant is also subtracted or added from the obtained set function in order to assure that $F(\\emptyset) = 0$. These small coefficients are determined during the Cross Validation phase, after obtaining the optimal $\\sigma$ and $\\mu$ values which satisfy the given constraints.\n\\section{Experiment}\nIn order to compare the performance of the LNMC method with other popular Link Prediction methods, an experiment was performed using several data sets from existing literature:\n\\begin{enumerate}\n\\item Google + - The Google + dataset \\cite{3} contains $5,200$ nodes and  $24,690$ links, captured in AUG 2011. The data contains both Graph topology and node attribute information; however, the side-features are removed since our method requires edge status only.\n\\item Flickr -  Flickr is a social network based on image hosting, where users form communities and friendships based on common interests. The Flickr dataset \\cite{14} contains $80,513$ nodes, $5,899,882$ links, and $195$ groups. Group affiliation was discarded due to irrelevance to the LNMC method.\n\\item Blog Catalog - Blog Catalog \\cite{14} is a blogging site where users can form friendships, and acquire group membership. The utilized dataset contains $10,312$ nodes, $333,983$ links, and $39$ groups. Again, for the context of this paper, the group information was removed.\n\\end{enumerate}\n\nAs seen in Fig. \\ref{fig:22}, all datasets follow a roughly Log-Normal distribution, with varying amounts of degree sparsity, and variance. Due to the high number of low degree nodes in the Google+ dataset, all points appear constrained to the left of the plot axis; however, as we will illustrate, the Log-Normal Distribution is still superior to the Pareto Distribution for link prediction.\nDuring the training phase, $10\\%$ of the data was removed in order to use for future predictions. For the purposes of demonstration, only $1,000$ of the highest degree nodes are maintained for adjacency matrix formation.\n\n\n\\begin{figure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{gplus_data_dist}\n                \\centering\n                \\caption{Google+  }\n                \\label{fig:x}\n        \\end{subfigure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{flickr_data_dist}\n                \\centering\n                \\caption{Flickr}\n                \\label{fig:y}\n        \\end{subfigure}\n        \\begin{subfigure}[b]{0.175\\textwidth}\n                \\includegraphics[trim = 14mm 0mm 0mm 0mm, width=30mm]{bc_data_dist}\n                \\centering\n                \\caption{Blog Cat.}\n                \\label{fig:z}\n        \\end{subfigure}\n        \\caption{Empirical Node Degree Data and Fitted Log-Normal Probability Distribution Functions}\\label{fig:animals}\n\\label{fig:22}\n\\end{figure}\n\n\n\n\\section{Results}\n\\subsection{Baseline Methods and Performance Metrics}\nIn order to understand the advantage of LNMC, the results are compared against the following methods:\n\n\\begin{enumerate}\n\\item Matrix Completion with Pareto Sparsity (MCPS) - MCPS \\cite{5} utilizes the same algorithm which we have outlined in the paper with the exception of the prior. MCPS employs the Pareto Distribution $f(d) = (\\frac{\\delta}{d})^\\chi $.\n\\item Matrix Completion with $L_1$ Sparsity (MCLS) - MCLS is used by Richard et al. \\cite{4}, and represents one of the first attempts at incorporating $L_1$ sparsity with the Low Rank assumption.\n\\item Logistic Regression (MF + RwR + AA) - In their paper on Social Attribute Networks, Gong et al. \\cite{2} provide a method which combines features from Matrix Factorization, Random Walks with Restart, and Adamic Adar, which effectively solves the link prediction problem with high accuracy. In this paper, the attributes are removed from the network for equal comparison with our method.\n\\end{enumerate}\n\nIn order to provide a fair basis on which to judge the performance, Area Under the Curve (AUC) is employed for comparison. By utilizing the AUC as the performance metric, we avoid the need for data balancing, a process which frequently results in undersampling negative samples. Thus, all methods can benefit from the additional training data.\n\\\\\nThe results are obtained via $10-\\text{fold}$ Cross Validation, using a random sampling method for hyper-parameter selection. The rounds are averaged to produce the results shown in Table \\ref{table:1}.\n\\subsection{Performance Comparison}\nAs demonstrated in Fig. \\ref{fig:1}, LNMC outperforms MCPS, MCLS, and LR, on the Google Plus dataset. Due to the highly Log-Normal characteristic \\cite{2} of the data set, LNMC's fine-tuned degree specific prior captures the degree distribution behavior in combination with the low rank features of the data, leading to high AUC values. The high number of true positives compared to the false positive rate leads to jagged graph distribution. In Fig \\ref{fig:2}, it is clear that matrix completion with Pareto Sparsity produces low AUC values due to the inaccurate distribution representation. Similarly the LR method fails to capture accurate low rank information because the low rank matrix factorization is done prior to the the gradient descent training for Logistic Regression.\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{gplus_data_plot4}\n\\caption{Receiver Operating Characteristic for Google Plus Data}\n\\label{fig:1}\n\\end{figure}\nDue to the Pareto nature of the Flickr dataset, both the LNMC and MCPS methods perform the same. As can be seen in (\\ref{eq:3}), LNMC can adapt to Scale Free Networks when the first term is small compared to the second term. Logistic Regression performs poorly since the features are set, whereas Matrix Completion methods automatically select the number of latent parameters to utilize.\n\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{flickr_data_plot4}\n\\caption{Receiver Operating Characteristic for Flickr Data}\n\\label{fig:2}\n\\end{figure}\nAs seen in Fig. \\ref{fig:3}, LNMC outperforms the Pareto Sparisty based matrix completion, due to the inclusion of the squared log terms. The $L_1$ sparsity used in the MCLS method is insufficiently descriptive for accurate matrix estimation. Thus Logistic Regression, which incorporates more descriptive features outperforms the MCLS method.\n \\begin{figure}[htb]\n\\centering\n\\includegraphics[width=90mm]{bc_data_plot4}\n\\caption{Receiver Operating Characteristic for Blog Catalog Data}\n\\label{fig:3}\n\\end{figure}\nFor purposes of comparison, AUC values for each method and dataset, are contained in Table \\ref{table:1}. As highlighted by the AUC Table, LNMC provides optimal results over all datasets.\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|}\n      \\hline\n      Data Set & LNMC & MCPS & MCLS & LR(MF+RwR+AA)\\\\\n      \\hline\n      Google+ & .8541 & .8439 & .8113 & .8434\\\\\n      \\hline\n      Flickr & .9052 & .9052 & .8504 & .8972\\\\\n      \\hline\n      Blog Catalog & .7918 & .7846 & .7150 & .7727\\\\\n      \\hline\n\\end{tabular}\n\\vspace{.5cm}\\\\\n\\caption{AUC Performance Comparison}\n\\label{table:1}\n\\end{center}\n\\end{table}\n\\section{Conclusion}\nAs demonstrated both theoretically, and experimentally, LNMC is able to sufficiently encapsulate the advantages of Pareto Sparsity in addition to Log Normal Sparsity. Previously described by Gong et al. in \\cite{2},\nmany modern social networks with undirected graph topologies exhibit Log Normal degree distributions. Thus by incorporating the degree-specific prior the optimization encourages convergence to a Log-Normal degree distribution.\nDue to the non-convexity of solving the joint low-rank and structured sparsity inducing prior, the Lovasz Extension is introduced to solve the complex problem efficiently. Through analysis on three datasets, and using 3 top performing methods, we provide results which exceed the current optimum.\nThese results reveal the fundamental value of prior degree information in Link Prediction, and can provide insight into understanding the complex dynamics which cause links to form in a similar way across different networks.\n\\\\\nIn future research we plan to investigate the incorporation of side information into the objective. Node attributes introduce additional challenges, including missing features, and additional training complexities.\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{references}\n\n\\appendix\nAs seen in \\cite{5}, the optimization of (\\ref{eq:0}) is performed by first imposing the symmetry constraint on $Y$ as\n\n", "index": 37, "text": "\\begin{gather*}\n{\\operatorname{\\arg\\!\\min}}_Y \\lambda_2 \\Gamma(Y) + \\frac{\\mu}{2} \\|X^{k+1} - Y + V^{k} \\|_{2}^2 \\\\\n\\text{s.t. } Y=Y^{T}.\n\\end{gather*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\operatorname{\\arg\\!\\min}}_{Y}\\lambda_{2}\\Gamma(Y)+\\frac{\\mu}{2}%&#10;\\|X^{k+1}-Y+V^{k}\\|_{2}^{2}\" display=\"block\"><mrow><mrow><mrow><msub><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mi>min</mi></mrow><mi>Y</mi></msub><mo>\u2061</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>X</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>-</mo><mi>Y</mi></mrow><mo>+</mo><msup><mi>V</mi><mi>k</mi></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{s.t. }Y=Y^{T}.\" display=\"block\"><mrow><mrow><mrow><mtext>s.t.\u00a0</mtext><mo>\u2062</mo><mi>Y</mi></mrow><mo>=</mo><msup><mi>Y</mi><mi>T</mi></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}]