[{"file": "1601.04083.tex", "nexttext": "\nWe then employ these assumptions to reconstruct an \nobservational data set that is suited for causal inference. \n\nTo develop these assumptions we require a new\nnotion that we refer to as the ``Last Plausible Randomized Experiment Time'' (termed\nLaPRET and represented by ${T_i^{\\rm LaPRET}}$).\nIn the context of an idealized randomized experiment, outlined\nin the left hand panel of Figure \\ref{fig:ass2}, LaPRET defines the first time point at which\nthe two potential outcomes are differentiable. In other words,\nin a randomized experiment where the treatment has an effect\nafter being administered at treatment time ${T_i^{\\rm treat}}$,\nif the outcome were to be observed at time ${T_i^{\\rm obs}}\\leq {T_i^{\\rm LaPRET}}$ then\nno difference between $Y_{i,{T_i^{\\rm treat}},{T_i^{\\rm obs}}}(Z_i=0)$ and $Y_{i,{T_i^{\\rm treat}},{T_i^{\\rm obs}}}(Z_i=1)$ would be discernible (that is, a treatment effect would be undetectable).\nOn the other hand\nobserving at time ${T_i^{\\rm obs}}>{T_i^{\\rm LaPRET}}$ would yield a non-zero treatment\neffect. It is clear that when there is no treatment effect, ${T_i^{\\rm LaPRET}}$ is not well defined -- the method proposed in this paper for identifying ${T_i^{\\rm LaPRET}}$ in observational studies accounts for this issue.\n\n\\subsection{Formal assumptions}\n\nWe require two new assumptions to discuss Equation \\eqref{bigeq}. The first\nfacilitates the comparison between the realized treatment and the associated\nevent while the second provides a rationale for identifying the\ntimes at which treatment could have happened.\n\n\\paragraph*{Assumption 1. (Unit Treatment Status Identification Strength)}\n\n\nFor treatment indicator $Z_i$ and associated event indicator\n$D_i$ assume that\n\n\n", "itemtype": "equation", "pos": 8342, "prevtext": "\n\n\\pagestyle{empty}\n\n\n\\title{Observational studies with unknown time of treatment}\n\\author{Guillaume W. Basse$^\\dagger$ \\and Alexander Volfovsky$^\\dagger$ \n \n\n\\and Edoardo M. Airoldi\\thanks{Guillaume W.\\ Basse is a graduate student in the Department of Statistics at Harvard University (gbasse@fas.harvard.edu). Alexander Volfovsky is an National Science Foundation Postdoctoral Fellow in the Department of Statistics at Harvard University (volfovsky@fas.harvard.edu). \n\n\nEdoardo M.\\ Airoldi is an Associate Professor of Statistics at Harvard University (airoldi@fas.harvard.edu). \n\nThis work was partially supported \n by the National Science Foundation under grants \n  CAREER IIS-1149662, IIS-1409177, and DMS-1402235,\n and by the Office of Naval Research under grant \n  YIP N00014-14-1-0485. \n Edoardo M.~Airoldi is an Alfred P. Sloan Research Fellow, and a Shutzer Fellow at the Radcliffe Institute for Advanced Studies.\n\nThe authors wish to thank Michael Els, Robert Haslinger, Mark Lowe, Sean Murphy, and Donald B.\\ Rubin for providing data, comments, and valuable insights. \n\n~ $^\\dagger\\,$These authors contributed equally to this work.}}\n\\date{}\n\n\\maketitle\n\\thispagestyle{empty}\n\n\\newpage\n\n\n\n\n\\begin{abstract}\nTime plays a fundamental role in causal analyses, where the goal is to quantify the effect of a specific treatment on future outcomes.\n\n\nIn a randomized experiment, times of treatment, and when outcomes are observed, are typically well defined.\nIn an observational study, treatment time marks the point from which pre-treatment variables must be regarded as outcomes, and it is often straightforward to establish.\n\nMotivated by a natural experiment in online marketing, we consider a situation where useful conceptualizations of the experiment behind an observational study of interest lead to uncertainty in the determination of times at which individual treatments take place. \nOf interest is the causal effect of heavy snowfall in several parts of the country on daily measures of online searches for batteries, and then purchases. The data available give information on actual snowfall, whereas the natural treatment is the anticipation of heavy snowfall, which is not observed. \n\nIn this article, we introduce formal assumptions and inference methodology centered around a novel notion of plausible time of treatment. These methods allow us to explicitly bound the last plausible time of treatment in observational studies with unknown times of treatment, and ultimately  yield valid causal estimates in such situations.\\newline\n\\vfill\n\n\\noindent\\textbf{Keywords:} Causal inference; Rubin causal model; Plausible time of treatment; Natural experiment; Online marketing.\n\\end{abstract}\n\n\\newpage\n\\onehalfspacing\n\\tableofcontents\n\n\n\n\n\n\n\\doublespacing\n\n \n \n \n\n\\newpage\n\n\\pagestyle{fancy}\n\\setcounter{page}{1}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\nObservational studies are common across many disciplines, \nincluding the health and social sciences where practitioners\nare interested in assessing the causal effect of a non-randomized\ntreatment \\citep{Rosenbaum:2002aa,Rosenbaum:2010aa}. For example, a drug company might solicit \ninformation on\nthe blood pressure of individuals following the \nvoluntary ingestion\nof a particular drug, or a fast food restaurant might record\nobservations on sales over time. The former might be interested\nin the effect of the drug on blood pressure, while the latter\nmight be interested in the effect of the forecast of an adverse weather event on sales. \nIn both cases, the data are gathered from historical records, thus without the ability\nto randomize treatment. Because of the lack of randomization, \nnaive estimators of the average treatment effect that consider the difference between\nthe average effect of treated and untreated individuals\nare likely biased for the effect of interest \\citep[][]{rubin1991aa, imbens2014causal}.\n\nTo make the study of causal effects concrete \nan analyst is required to \ndefine the treatment of interest, the outcome of interest that\nshe believes the treatment might affect, times at which\nto conceptualize treatment happened, and times at which to measure the outcomes\n\\citep{splawa1990application,rubin1974estimating}. \nWithin the context of a randomized experiment the\nmeaning and definition of these three components\nare very straightforward, but in the context of an\nobservational study much greater care is required in \ndefining them to allow for proper causal inference. In particular,\none might only observe a proxy for treatment and so the exact treatment\nand, more importantly, the exact time of treatment\nare unknown. When this is the case it becomes unclear when the\noutcome of interest should have been measured in order to make \na proper causal statement. This is the setting we consider in this paper. \n\nAs a concrete example, we consider studying the relationship \nbetween the adverse\nweather in February and March of 2015 across the \nUnited States and online battery searches at a major US retailer. The relationship between weather and sales has\nbeen studied in the literature \\citep[e.g., see][]\n{murray2010effect,starr2000effects,zwebner2013temperature}, with\nthe sometimes implicit assumption of a clearly defined treatment (e.g., temperature, exposure to sunlight) and so causal statements are reserved for post-weather measurements.\nIn \nour case we are interested in studying whether the perception\nof future extreme snow increases conversions but we do not observe this perception. Having observed the snow the methods proposed in Section \\ref{sec:method} determines the time points prior to the snow event for which causal statements can be made.\nIn a novel data set,\nprovided by a large online advertising\ntechnology firm,\nwe have measurements of these outcomes across February and March of 2015 for 79 designated marketing areas (DMA) \nacross the United\nStates. For each of these super-metropolitan areas we have covariate information pertaining\nto the demographics of the people living in the area, as well\nas measurement of snow accumulation for each day. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rest of the paper is organized as follows: \nIn Section~\\ref{sec:notation} we briefly outline the formal notation\nof the Rubin Causal Model and we introduce two new assumptions, which \n complement the assumptions employed\nin causal inference, that enable causal analyses of \ndata that are missing an exact time of treatment. \nSection~\\ref{sec:method} describes the proposed three stage \nmethodology. Section~\\ref{sec:sims} provides simulation results.\nDetails of the marketing \ndata set and the data analysis are reported in Section~\\ref{sec:data}.\nRemarks follow, in Section~\\ref{sec:disc}.\n\n \n \n \n\n\\section{A notion of plausible time of treatment}\\label{sec:notation}\n\nThroughout this paper we restrict ourselves to two levels\nof treatment and for convenience we refer to ``control''\nfor the lower level of treatment, and  to ``treatment'' for\nthe higher. As in the classical causal inference literature $Z_{i}$ denotes\ntreatment indicator for an individual $i$ and is equal to 1 if\nthe unit is assigned to treatment at time ${T_i^{\\rm treat}}$\nand is 0 otherwise \\citep{imbens2014causal}.\nPotential outcomes for each unit are denoted\nby $Y_{i,{T_i^{\\rm treat}},t}(Z_i=0)$ or $Y_{i,{T_i^{\\rm treat}},t}(Z_i=1)$ for control and treated levels\nof treatment when treatment occurs at time ${T_i^{\\rm treat}}$\nand the outcomes are observed at time $t > {T_i^{\\rm treat}}$.\n\n\n\n\n\nIn contrast with the classical setting, the treatment indicator $Z_i$ and the \ntime of treatment ${T_i^{\\rm treat}}$\nremain hidden and instead, $D_i$ and ${T_i^{\\rm event}}$ are observed. \n$D_i$ is the indicator that an event associated with the treatment\noccured at time ${T_i^{\\rm event}}$. In particular it is not necessary that\n$Z_i=D_i$ and it is likely that ${T_i^{\\rm treat}}<{T_i^{\\rm event}}$. That is, the event\nthat is observed is not necessarily a perfect proxy for the \nactual treatment and the actual treatment time occurs\nbefore the event. \nIn what follows\nwe formulate a set of assumptions on the \nrelationship between $Z_i$, $D_i$, ${T_i^{\\rm treat}}$ and ${T_i^{\\rm event}}$ that allow us to determine\nthe times $t$ for which an observed quantity $Y_{i,t}^{\\rm obs}$ corresponds to the potential outcome under treatment or under control:\n\n\n\n\n\n", "index": 1, "text": "\\begin{equation}\\label{bigeq}\nY_{i,t}^{\\rm obs}=D_i \\cdot Y_{i,{T_i^{\\rm treat}},t}(Z_i=1) + (1-D_i) \\cdot Y_{i,{T_i^{\\rm treat}},t}(Z_i=0)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Y_{i,t}^{\\rm obs}=D_{i}\\cdot Y_{i,{T_{i}^{\\rm treat}},t}(Z_{i}=1)+(1-D_{i})%&#10;\\cdot Y_{i,{T_{i}^{\\rm treat}},t}(Z_{i}=0)\" display=\"block\"><mrow><msubsup><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>obs</mi></msubsup><mo>=</mo><msub><mi>D</mi><mi>i</mi></msub><mo>\u22c5</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><msubsup><mi>T</mi><mi>i</mi><mi>treat</mi></msubsup><mo>,</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>D</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><msubsup><mi>T</mi><mi>i</mi><mi>treat</mi></msubsup><mo>,</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\n\\paragraph*{Assumption 2. (Constant Unit Time of Treatment)}\n\nFor LaPRET, ${T_i^{\\rm LaPRET}}$, treatment time ${T_i^{\\rm treat}}$ and associated event time ${T_i^{\\rm event}}$ assume\n\\begin{itemize}\n\t\\item[(i)] $d_i =  {T_i^{\\rm event}}  - {T_i^{\\rm LaPRET}} =d >0$ \\mbox{ for all $i$}\n\t\\item[(ii)] ${T_i^{\\rm treat}} \\leq {T_i^{\\rm LaPRET}}$  \\mbox{ for all $i$}\n\\end{itemize}\n\nThe statement of Assumption 1 reflects potential uncertainty\nabout realized treatment status based on event status. In the extreme\nsetting where $\\eta=0$ the assumption states that\n$Z_i=D_i$ for all units $i$, fully revealing the treatment\nindicator. This is the setting when a longitudinal \nstudy reveals information about a treatment that\noccurred during a previous wave of the study. In the less\nextreme setting of $\\eta$ small, the assumption provides\na generative method for multiple imputed data sets all of which\nare suitable for causal inference (see e.g. \\cite{rubin1996multiple}). \nIn particular, if we reconceptualize Equation~\\ref{bigeq} as:\n\n\n", "itemtype": "equation", "pos": 10209, "prevtext": "\nWe then employ these assumptions to reconstruct an \nobservational data set that is suited for causal inference. \n\nTo develop these assumptions we require a new\nnotion that we refer to as the ``Last Plausible Randomized Experiment Time'' (termed\nLaPRET and represented by ${T_i^{\\rm LaPRET}}$).\nIn the context of an idealized randomized experiment, outlined\nin the left hand panel of Figure \\ref{fig:ass2}, LaPRET defines the first time point at which\nthe two potential outcomes are differentiable. In other words,\nin a randomized experiment where the treatment has an effect\nafter being administered at treatment time ${T_i^{\\rm treat}}$,\nif the outcome were to be observed at time ${T_i^{\\rm obs}}\\leq {T_i^{\\rm LaPRET}}$ then\nno difference between $Y_{i,{T_i^{\\rm treat}},{T_i^{\\rm obs}}}(Z_i=0)$ and $Y_{i,{T_i^{\\rm treat}},{T_i^{\\rm obs}}}(Z_i=1)$ would be discernible (that is, a treatment effect would be undetectable).\nOn the other hand\nobserving at time ${T_i^{\\rm obs}}>{T_i^{\\rm LaPRET}}$ would yield a non-zero treatment\neffect. It is clear that when there is no treatment effect, ${T_i^{\\rm LaPRET}}$ is not well defined -- the method proposed in this paper for identifying ${T_i^{\\rm LaPRET}}$ in observational studies accounts for this issue.\n\n\\subsection{Formal assumptions}\n\nWe require two new assumptions to discuss Equation \\eqref{bigeq}. The first\nfacilitates the comparison between the realized treatment and the associated\nevent while the second provides a rationale for identifying the\ntimes at which treatment could have happened.\n\n\\paragraph*{Assumption 1. (Unit Treatment Status Identification Strength)}\n\n\nFor treatment indicator $Z_i$ and associated event indicator\n$D_i$ assume that\n\n\n", "index": 3, "text": "\\begin{equation*}\n{\\rm cor}(Z_i, D_i) \\geq 1 - \\eta\\ \\forall i\\text{ for }\\eta\\text{ small.}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\rm cor}(Z_{i},D_{i})\\geq 1-\\eta\\ \\forall i\\text{ for }\\eta\\text{ small.}\" display=\"block\"><mrow><mrow><mi>cor</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>D</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mrow><mpadded width=\"+5pt\"><mi>\u03b7</mi></mpadded><mo>\u2062</mo><mrow><mo>\u2200</mo><mrow><mi>i</mi><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>\u03b7</mi><mo>\u2062</mo><mtext>\u00a0small.</mtext></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\n we have\n$\\delta=\\delta(\\eta)$ which\ninforms the interpretation of a sensitivity analysis that\nvaries $\\eta$. When $\\eta\\neq 0$ there are multiple versions of the ``complete data'' as there is now a nonzero probability that $D_i=1$ and $Z_i=0$. This condition can easily be accommodated by constructing multiple data sets in which the correlation between $Z_i$ and $D_i$ is $1-\\eta$ and reporting causal estimates across these data sets \\citep{rubin1996multiple}.\n\n\\begin{figure}[b!]\n\t\\centering\n\t\n\t\n\t\\includegraphics[width=\\textwidth]{figures/jointplot.pdf}\n\t\\caption{\\onehalfspacing This is an illustration of Assumption 2. The left panel hand describes the scenario of an idealized experiment -- ${T_i^{\\rm treat}}$ is the known treatment time, ${T_i^{\\rm LaPRET}}$ is the LaPRET and ${T_i^{\\rm obs}}$ is the observation time. The right hand panel describes the observational data set where an event associated with treatment is observed at time ${T_i^{\\rm event}}$ and $d$ (of Assumption 2), the time between the event time and the LaPRET is the same for all $i$ (and hence does not require a subscript).}\n\t\\label{fig:ass2}\n\\end{figure}\n \nAssumption 2 is an identification assumption for LaPRET that explicitly addresses the lack of information about the time of treatment. The first part of the assumption states\nthat the time between an individual LaPRET and the time\nof the associated event is equal for all individuals in the \npopulation. \n\n\nAs such, by observing two similar individuals, one in the control\nand one in the treatment groups (using the information\nfrom Assumption 1), one can compute the LaPRET as illustrated in the right hand panel of Figure \\ref{fig:ass2}.\nThe second part of the assumption states that the time\nof treatment is always strictly prior to LaPRET. \n meaning that \nobservations at the LaPRET are potential outcomes and so\ncan be used to compute causal estimates.\nOur methodology relies on the first part of this assumption - \nwe estimate $\\hat{d}$ from a pilot study and then leverage\nthis information in the larger population. If this assumption\nis violated then the estimate of $\\hat{d}$ is unreliable and \ncare must be taken to insure estimates are causal. In such\nsituations, illustrated in the simulation study, a smaller\nvalue of $d$ can be chosen to insure that the estimate\nis causal. This suggests a relaxation of the condition in  part (i) of Assumption\n2, to the get the following.\n\n\\paragraph*{Assumption 2'. (Stable Unit Time of Treatment)}\n\nFor LaPRET, ${T_i^{\\rm LaPRET}}$, treatment time ${T_i^{\\rm treat}}$ and associated event time ${T_i^{\\rm event}}$ assume\n\\begin{itemize}\n\t\\item[(i)] $d_i =  {T_i^{\\rm event}}  - {T_i^{\\rm LaPRET}} >0$ \\mbox{ for all $i$}\n\t\\item[(ii)] ${T_i^{\\rm treat}} \\leq {T_i^{\\rm LaPRET}}$  \\mbox{ for all $i$}\n\\end{itemize}\n\n\\noindent In this more general case, there exists $d>0$ such that $d_i =  {T_i^{\\rm obs}}  - {T_i^{\\rm LaPRET}}>d$ for all $i$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\\section{Designing an observational study with unknown time of treatment}\n\\label{sec:method}\n\n\nIn this section, we describe how one may leverage\nthe assumptions of Section \\ref{sec:notation} in order to take a large data set that does\nnot include treatment indicators or time of treatment indicators\nand to produce a potentially reduced data set that can be\nused for conceptualizing and performing a complete observational study \\citep{Rosenbaum:2002aa,Rosenbaum:2010aa}.\n\n\\subsection{Conceptualization of treatment}\n\nUnlike in classical causal inference where treatment and treatment\ntime are known to the scientist, in this setting a proper notion of treatment\nmust be conceptualized. The issue here is similar to the one faced in the \nperceived treatment literature where one cannot use race or sex \nas the treatment in an experiment but instead ``perceived race'' or ``perceived\nsex'' can be used \\citep{greiner2011causal}. We also rely on the notion of \n``perceived treatment''. For example, in the \napplication to the advertising data in Section \\ref{sec:data}\nthe treatment is the expectation of an extreme snow\nevent happening in the future perceived by individuals. The notion of perception\nis needed here as the true treatment is never observed in our\nsetting and so we reconstruct it based on an event that would\nhave been foreshadowed by such a perception. This is exactly the\ntype of correlation between the treatment indicator $Z_i$\nand the event indicator $D_i$ that Assumption 1 describes quantitatively.\nIt must be noted that $D_i$ identifies both the treatment\nand the control groups and so it is likely that some units\nwill be discarded in order to better satisfy Assumption 1.\nThis data reduction step is explored in detail in Section \\ref{sec:data} where\nthe only units allowed to be considered as control units\nare those that always experience less than a thresholded amount of snow. \n\nOnce we have identified the event that serves as proxy for the true \ntreatment we must identify the correlation in Assumption 1. In a longitudinal study setting,\nwhere a later wave question (serving as the indicator $D_i$) might\nask if an individual received an intervention at a \nprevious wave (the treatment indicator $Z_i$) the correlation \ncan be set to one. In more ambiguous situations, such\nas the one discussed in the Section \\ref{sec:sims}, a sensitivity\nanalysis based on this correlation should be performed. \n\n\n\\subsection{Pilot study to identify plausible times of treatment}\n\nOnce the treatment is defined and a variable $D_i$ is identified\nwe need to find the individual LaPRET values ${T_i^{\\rm LaPRET}}$. Since these\nare only identifiable from the joint distribution of outcomes we\nmust infer those from the data. However, if we use a data driven\napproach that considers the complete data set we would be violating\na fundamental principle of causal inference that does not allow parameters\nin the analysis to depend on the post-treatment data. \n\nTo overcome this difficulty we propose to perform a pilot study to\nidentify ${T_i^{\\rm LaPRET}}$. An example of this approach was recently undertaken by \\citet{wager2015estimation} to apply decision tree methods to causal inference. Under the pilot study we consider a small sample\nof  individuals that are identified as treated and as control units. \nOnce this sample is chosen we match treated and control pairs. The choice of a particular matching mechanism depends on the applied problem and the available observed covariates associated with each unit -- in our simulations and applied example we employ propensity score matching \\citep{rosenbaum1983assessing}. In a slight abuse of notation, the matched pairs are now identified by the subscript $i$.\nLeveraging Assumption 2(i) (or 2(i)$^\\prime$) we can identify \nthe difference between the time of the event $D_i$ and\nthe LaPRET, $d_i={T_i^{\\rm event}}-{T_i^{\\rm LaPRET}}$, as it is assumed to be constant (or greater\nthan a non-zero constant) for all units. \n\n\nAs such finding\n$d_i$ becomes a problem of identifying time points $t$\nwhere $Y_{i,t}^{\\rm obs}|D_i=1$ and $Y_{i,t}^{\\rm obs}|D_i=0$ are close. Letting $\\Delta_{i,t}$ equal that difference for matched pair $i$ at time $t$\n\n\nwe say that the LaPRET for matched pair $i$ is \n\n", "itemtype": "equation", "pos": 11354, "prevtext": "\n\n\\paragraph*{Assumption 2. (Constant Unit Time of Treatment)}\n\nFor LaPRET, ${T_i^{\\rm LaPRET}}$, treatment time ${T_i^{\\rm treat}}$ and associated event time ${T_i^{\\rm event}}$ assume\n\\begin{itemize}\n\t\\item[(i)] $d_i =  {T_i^{\\rm event}}  - {T_i^{\\rm LaPRET}} =d >0$ \\mbox{ for all $i$}\n\t\\item[(ii)] ${T_i^{\\rm treat}} \\leq {T_i^{\\rm LaPRET}}$  \\mbox{ for all $i$}\n\\end{itemize}\n\nThe statement of Assumption 1 reflects potential uncertainty\nabout realized treatment status based on event status. In the extreme\nsetting where $\\eta=0$ the assumption states that\n$Z_i=D_i$ for all units $i$, fully revealing the treatment\nindicator. This is the setting when a longitudinal \nstudy reveals information about a treatment that\noccurred during a previous wave of the study. In the less\nextreme setting of $\\eta$ small, the assumption provides\na generative method for multiple imputed data sets all of which\nare suitable for causal inference (see e.g. \\cite{rubin1996multiple}). \nIn particular, if we reconceptualize Equation~\\ref{bigeq} as:\n\n\n", "index": 5, "text": "\\begin{equation}\n\\Pr \\bigm( Y_{i,t}^{\\rm obs}=D_i, Y_{i,{T_i^{\\rm treat}},t}(Z_i=1) + (1-D_i) \\, Y_{i,{T_i^{\\rm treat}},t}(Z_i=0) \\bigm) ~\\geq 1-\\delta\\label{eqn:reconc}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\Pr\\bigm{(}Y_{i,t}^{\\rm obs}=D_{i},Y_{i,{T_{i}^{\\rm treat}},t}(Z_{i}=1)+(1-D_{%&#10;i})\\,Y_{i,{T_{i}^{\\rm treat}},t}(Z_{i}=0)\\bigm{)}~{}\\geq 1-\\delta\" display=\"block\"><mrow><mi>Pr</mi><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msubsup><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mi>obs</mi></msubsup><mo>=</mo><msub><mi>D</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><msubsup><mi>T</mi><mi>i</mi><mi>treat</mi></msubsup><mo>,</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>D</mi><mi>i</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><msub><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><msubsup><mi>T</mi><mi>i</mi><mi>treat</mi></msubsup><mo>,</mo><mi>t</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\" rspace=\"5.8pt\">)</mo><mo>\u2265</mo><mn>1</mn><mo>-</mo><mi>\u03b4</mi></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\nThat is, the estimated LaPRET for pair $i$ is at the \nmaximal time point such that the difference $\\Delta_{i,t}$\nis smaller than a $(1/\\alpha)$ fraction of the maximal difference\nand the rate of change of $\\Delta_{i,t}$ with respect to time is small but there \nexists a later time point where the rate of change is large.\nThe parameter $\\alpha$ captures the expected variability in the\nvalues of $Y_{i,t}^{\\rm obs}$. Small values of $\\alpha$ allow for larger differences between treated and control observations to be evaluated as no treatment effect. As such, larger values of $\\alpha$ lead to more conservative ${\\hat{T}_i^{\\rm LaPRET}}$ -- those that are closer to ${T_i^{\\rm event}}$.\nThe parameter $\\epsilon$\ncontrols the rate of change of $\\Delta_{i,t}$. It insures\nthat the procedure is able to differentiate between \nno effect (where $Y_{i,t}(1)=Y_{i,t}(0)\\ \\forall t$) and \na situation where the effect of treatment either induces\nvolatility or decreases to zero before the time\nof observation. Small values of $\\epsilon$ require the volatility between the treated and control observations to be small almost all the time while larger values allow for lots of volatility but require that a bigger volatility event occurs. As such both too small and too large values of $\\epsilon$ lead to extremely conservative behavior. \nWe study the behavior of $\\alpha$ and $\\epsilon$ in a \nsimulation study.\n\nAfter computing ${\\hat{T}_i^{\\rm LaPRET}}$ we can construct $\\hat{d}$ for\nAssumption 2 using some function of the set $\\{{T_i^{\\rm event}}-{\\hat{T}_i^{\\rm LaPRET}} \\}_i$.\nIn the simulation study we explore using the average of those differences \nto choose $\\hat{d}$. Choosing the minimum forms\na conservative estimate of $d$ that \naccommodates the relaxed Assumption 2$^\\prime$.\n\nIn practice, the pilot study can be performed on a subset of \nunits treated at a period of time before or after the main study happened.\n\n\\subsection{Main study}\n\nThe final step of the pipeline is the construction\nof a possibly reduced data set from the units that were not\nused in the pilot study which allows for valid causal inference \\citep{Rosenbaum:2002aa,Rosenbaum:2010aa,imbens2014causal}.\nIn particular, having identified the relationship\nbetween the treatment indicator $Z_i$ and the\nevent indicator $D_i$ as well as the latency between\nLaPRET and the event in the previous two steps one constructs\nthe data set as follows: (1) among the remaining units, construct\na matched sample then (2) for each matched pair, discard information \nthat is recorded prior to ${\\hat{T}_i^{\\rm LaPRET}}={T_i^{\\rm event}}-\\hat{d}$. The remaining\ndata then represents the potential outcomes under treatment\nand control.\n\n\n\n \n \n \n\n\\section{Simulations}\\label{sec:sims}\n\n\n\nThe purpose of our method is to give the analyst an interval of\ndays $\\hat{d}$ prior to the event, for which he can make causal\nstatements about the conceptualized treatment. This section has two objectives: first, we assess how\nthe choice of parameters $\\alpha$ and $\\epsilon$ affects the behavior\nof our method in different scenarios. Second, we explore how this \nbehavior is affected by different types and levels of noise in the observations.\n\n\\subsection{Design choices} \\label{sect:sims-description}\n\nWe conduct three experiments of increasing complexity, trying capturing different \nreal life response profiles. For each of the three scenarios, we consider a pair of ``idealized\nresponse surfaces'' under control and treatment -- that is, \nresponse surfaces with no noise that capture one of the three \nscenarios of interest. We denote pair of responses corresponding\nto the $k^{th}$ scenario by $(\\mu^{(k)}_0(t), \\mu^{(k)}_1(t))$. For\nall three scenarios, the idealized control response surface is the \nflat line at zero, $\\mu^{(k)}_0(t) = 0$. The idealized response \nsurfaces are represented in Figure~\\ref{fig:resp}.\n\nWe also consider two potential sources of noise in the observations. \nFirst we simulate the fact the potential outcomes are only noisy versions of the idealized\nresponse surface. So for each scenario $k$, we generate $N$ treatment and \ncontrol potential outcomes surfaces:\n\n\n", "itemtype": "equation", "pos": 18763, "prevtext": "\n\n we have\n$\\delta=\\delta(\\eta)$ which\ninforms the interpretation of a sensitivity analysis that\nvaries $\\eta$. When $\\eta\\neq 0$ there are multiple versions of the ``complete data'' as there is now a nonzero probability that $D_i=1$ and $Z_i=0$. This condition can easily be accommodated by constructing multiple data sets in which the correlation between $Z_i$ and $D_i$ is $1-\\eta$ and reporting causal estimates across these data sets \\citep{rubin1996multiple}.\n\n\\begin{figure}[b!]\n\t\\centering\n\t\n\t\n\t\\includegraphics[width=\\textwidth]{figures/jointplot.pdf}\n\t\\caption{\\onehalfspacing This is an illustration of Assumption 2. The left panel hand describes the scenario of an idealized experiment -- ${T_i^{\\rm treat}}$ is the known treatment time, ${T_i^{\\rm LaPRET}}$ is the LaPRET and ${T_i^{\\rm obs}}$ is the observation time. The right hand panel describes the observational data set where an event associated with treatment is observed at time ${T_i^{\\rm event}}$ and $d$ (of Assumption 2), the time between the event time and the LaPRET is the same for all $i$ (and hence does not require a subscript).}\n\t\\label{fig:ass2}\n\\end{figure}\n \nAssumption 2 is an identification assumption for LaPRET that explicitly addresses the lack of information about the time of treatment. The first part of the assumption states\nthat the time between an individual LaPRET and the time\nof the associated event is equal for all individuals in the \npopulation. \n\n\nAs such, by observing two similar individuals, one in the control\nand one in the treatment groups (using the information\nfrom Assumption 1), one can compute the LaPRET as illustrated in the right hand panel of Figure \\ref{fig:ass2}.\nThe second part of the assumption states that the time\nof treatment is always strictly prior to LaPRET. \n meaning that \nobservations at the LaPRET are potential outcomes and so\ncan be used to compute causal estimates.\nOur methodology relies on the first part of this assumption - \nwe estimate $\\hat{d}$ from a pilot study and then leverage\nthis information in the larger population. If this assumption\nis violated then the estimate of $\\hat{d}$ is unreliable and \ncare must be taken to insure estimates are causal. In such\nsituations, illustrated in the simulation study, a smaller\nvalue of $d$ can be chosen to insure that the estimate\nis causal. This suggests a relaxation of the condition in  part (i) of Assumption\n2, to the get the following.\n\n\\paragraph*{Assumption 2'. (Stable Unit Time of Treatment)}\n\nFor LaPRET, ${T_i^{\\rm LaPRET}}$, treatment time ${T_i^{\\rm treat}}$ and associated event time ${T_i^{\\rm event}}$ assume\n\\begin{itemize}\n\t\\item[(i)] $d_i =  {T_i^{\\rm event}}  - {T_i^{\\rm LaPRET}} >0$ \\mbox{ for all $i$}\n\t\\item[(ii)] ${T_i^{\\rm treat}} \\leq {T_i^{\\rm LaPRET}}$  \\mbox{ for all $i$}\n\\end{itemize}\n\n\\noindent In this more general case, there exists $d>0$ such that $d_i =  {T_i^{\\rm obs}}  - {T_i^{\\rm LaPRET}}>d$ for all $i$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n\n\\section{Designing an observational study with unknown time of treatment}\n\\label{sec:method}\n\n\nIn this section, we describe how one may leverage\nthe assumptions of Section \\ref{sec:notation} in order to take a large data set that does\nnot include treatment indicators or time of treatment indicators\nand to produce a potentially reduced data set that can be\nused for conceptualizing and performing a complete observational study \\citep{Rosenbaum:2002aa,Rosenbaum:2010aa}.\n\n\\subsection{Conceptualization of treatment}\n\nUnlike in classical causal inference where treatment and treatment\ntime are known to the scientist, in this setting a proper notion of treatment\nmust be conceptualized. The issue here is similar to the one faced in the \nperceived treatment literature where one cannot use race or sex \nas the treatment in an experiment but instead ``perceived race'' or ``perceived\nsex'' can be used \\citep{greiner2011causal}. We also rely on the notion of \n``perceived treatment''. For example, in the \napplication to the advertising data in Section \\ref{sec:data}\nthe treatment is the expectation of an extreme snow\nevent happening in the future perceived by individuals. The notion of perception\nis needed here as the true treatment is never observed in our\nsetting and so we reconstruct it based on an event that would\nhave been foreshadowed by such a perception. This is exactly the\ntype of correlation between the treatment indicator $Z_i$\nand the event indicator $D_i$ that Assumption 1 describes quantitatively.\nIt must be noted that $D_i$ identifies both the treatment\nand the control groups and so it is likely that some units\nwill be discarded in order to better satisfy Assumption 1.\nThis data reduction step is explored in detail in Section \\ref{sec:data} where\nthe only units allowed to be considered as control units\nare those that always experience less than a thresholded amount of snow. \n\nOnce we have identified the event that serves as proxy for the true \ntreatment we must identify the correlation in Assumption 1. In a longitudinal study setting,\nwhere a later wave question (serving as the indicator $D_i$) might\nask if an individual received an intervention at a \nprevious wave (the treatment indicator $Z_i$) the correlation \ncan be set to one. In more ambiguous situations, such\nas the one discussed in the Section \\ref{sec:sims}, a sensitivity\nanalysis based on this correlation should be performed. \n\n\n\\subsection{Pilot study to identify plausible times of treatment}\n\nOnce the treatment is defined and a variable $D_i$ is identified\nwe need to find the individual LaPRET values ${T_i^{\\rm LaPRET}}$. Since these\nare only identifiable from the joint distribution of outcomes we\nmust infer those from the data. However, if we use a data driven\napproach that considers the complete data set we would be violating\na fundamental principle of causal inference that does not allow parameters\nin the analysis to depend on the post-treatment data. \n\nTo overcome this difficulty we propose to perform a pilot study to\nidentify ${T_i^{\\rm LaPRET}}$. An example of this approach was recently undertaken by \\citet{wager2015estimation} to apply decision tree methods to causal inference. Under the pilot study we consider a small sample\nof  individuals that are identified as treated and as control units. \nOnce this sample is chosen we match treated and control pairs. The choice of a particular matching mechanism depends on the applied problem and the available observed covariates associated with each unit -- in our simulations and applied example we employ propensity score matching \\citep{rosenbaum1983assessing}. In a slight abuse of notation, the matched pairs are now identified by the subscript $i$.\nLeveraging Assumption 2(i) (or 2(i)$^\\prime$) we can identify \nthe difference between the time of the event $D_i$ and\nthe LaPRET, $d_i={T_i^{\\rm event}}-{T_i^{\\rm LaPRET}}$, as it is assumed to be constant (or greater\nthan a non-zero constant) for all units. \n\n\nAs such finding\n$d_i$ becomes a problem of identifying time points $t$\nwhere $Y_{i,t}^{\\rm obs}|D_i=1$ and $Y_{i,t}^{\\rm obs}|D_i=0$ are close. Letting $\\Delta_{i,t}$ equal that difference for matched pair $i$ at time $t$\n\n\nwe say that the LaPRET for matched pair $i$ is \n\n", "index": 7, "text": "\\begin{align}\n{\\hat{T}_i^{\\rm LaPRET}} \\in {\\operatorname{arg\\,max}}_{t<{T_i^{\\rm event}}}\\Bigm\\{ t \n & \\text{ s.t. } |\\Delta_{i,t}|<\\max_t \\frac{|\\Delta_{i,t}|}{\\alpha}\\text{ and } \\nonumber \\\\\n & \\text{ s.t. } |\\partial\\Delta_{i,t}|<\\epsilon, ~|\\partial\\Delta_{i,s_1}|>\\epsilon, ~|\\partial\\Delta_{i,s_2}|<\\epsilon \\text{ for some } s_2>s_1>t \\Bigm\\}. \\nonumber \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\hat{T}_{i}^{\\rm LaPRET}}\\in{\\operatorname{arg\\,max}}_{t&lt;{T_{i}^%&#10;{\\rm event}}}\\Bigm{\\{}t\" display=\"inline\"><mrow><msubsup><mover accent=\"true\"><mi>T</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mi>LaPRET</mi></msubsup><mo>\u2208</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mi>t</mi><mo>&lt;</mo><msubsup><mi>T</mi><mi>i</mi><mi>event</mi></msubsup></mrow></msub><mo maxsize=\"160%\" minsize=\"160%\">{</mo><mi>t</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{ s.t. }|\\Delta_{i,t}|&lt;\\max_{t}\\frac{|\\Delta_{i,t}|}{\\alpha}%&#10;\\text{ and }\" display=\"inline\"><mrow><mrow><mtext>s.t.\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&lt;</mo><mrow><munder><mi>max</mi><mi>t</mi></munder><mo>\u2061</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mi>\u03b1</mi></mfrac></mstyle><mo>\u2062</mo><mtext>\u00a0and</mtext></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{ s.t. }|\\partial\\Delta_{i,t}|&lt;\\epsilon,~{}|\\partial\\Delta_{%&#10;i,s_{1}}|&gt;\\epsilon,~{}|\\partial\\Delta_{i,s_{2}}|&lt;\\epsilon\\text{ for some }s_{2%&#10;}&gt;s_{1}&gt;t\\Bigm{\\}}.\" display=\"inline\"><mrow><mtext>s.t.\u00a0</mtext><mo stretchy=\"false\">|</mo><mo>\u2202</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo><mo>&lt;</mo><mi>\u03f5</mi><mo rspace=\"5.8pt\">,</mo><mo stretchy=\"false\">|</mo><mo>\u2202</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>s</mi><mn>1</mn></msub></mrow></msub><mo stretchy=\"false\">|</mo><mo>&gt;</mo><mi>\u03f5</mi><mo rspace=\"5.8pt\">,</mo><mo stretchy=\"false\">|</mo><mo>\u2202</mo><msub><mi mathvariant=\"normal\">\u0394</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></msub><mo stretchy=\"false\">|</mo><mo>&lt;</mo><mi>\u03f5</mi><mtext>\u00a0for some\u00a0</mtext><msub><mi>s</mi><mn>2</mn></msub><mo>&gt;</mo><msub><mi>s</mi><mn>1</mn></msub><mo>&gt;</mo><mi>t</mi><mo maxsize=\"160%\" minsize=\"160%\">}</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\nfor $a=0,1$, and $i=1\\ldots N$. The noise parameter $\\sigma$ was given the following\nvalues $\\sigma = 0.005, 0.01, 0.015, 0.02$, but such that the potential outcome surfaces generated\nin a given simulation all shared the same parameter $\\sigma$. The second source of \nnoise we consider is with the time of observation ${T_i^{\\rm event}}$ of the event, which we simulate\nwith a contamination model: ${T_i^{\\rm event}} = {T_i^{\\rm LaPRET}} + d + c$ where ${T_i^{\\rm LaPRET}}$ and $d$ are fixed and $c$\nis a contamination model that is governed by one of the following distributions:\n\n \n", "itemtype": "equation", "pos": 23289, "prevtext": "\nThat is, the estimated LaPRET for pair $i$ is at the \nmaximal time point such that the difference $\\Delta_{i,t}$\nis smaller than a $(1/\\alpha)$ fraction of the maximal difference\nand the rate of change of $\\Delta_{i,t}$ with respect to time is small but there \nexists a later time point where the rate of change is large.\nThe parameter $\\alpha$ captures the expected variability in the\nvalues of $Y_{i,t}^{\\rm obs}$. Small values of $\\alpha$ allow for larger differences between treated and control observations to be evaluated as no treatment effect. As such, larger values of $\\alpha$ lead to more conservative ${\\hat{T}_i^{\\rm LaPRET}}$ -- those that are closer to ${T_i^{\\rm event}}$.\nThe parameter $\\epsilon$\ncontrols the rate of change of $\\Delta_{i,t}$. It insures\nthat the procedure is able to differentiate between \nno effect (where $Y_{i,t}(1)=Y_{i,t}(0)\\ \\forall t$) and \na situation where the effect of treatment either induces\nvolatility or decreases to zero before the time\nof observation. Small values of $\\epsilon$ require the volatility between the treated and control observations to be small almost all the time while larger values allow for lots of volatility but require that a bigger volatility event occurs. As such both too small and too large values of $\\epsilon$ lead to extremely conservative behavior. \nWe study the behavior of $\\alpha$ and $\\epsilon$ in a \nsimulation study.\n\nAfter computing ${\\hat{T}_i^{\\rm LaPRET}}$ we can construct $\\hat{d}$ for\nAssumption 2 using some function of the set $\\{{T_i^{\\rm event}}-{\\hat{T}_i^{\\rm LaPRET}} \\}_i$.\nIn the simulation study we explore using the average of those differences \nto choose $\\hat{d}$. Choosing the minimum forms\na conservative estimate of $d$ that \naccommodates the relaxed Assumption 2$^\\prime$.\n\nIn practice, the pilot study can be performed on a subset of \nunits treated at a period of time before or after the main study happened.\n\n\\subsection{Main study}\n\nThe final step of the pipeline is the construction\nof a possibly reduced data set from the units that were not\nused in the pilot study which allows for valid causal inference \\citep{Rosenbaum:2002aa,Rosenbaum:2010aa,imbens2014causal}.\nIn particular, having identified the relationship\nbetween the treatment indicator $Z_i$ and the\nevent indicator $D_i$ as well as the latency between\nLaPRET and the event in the previous two steps one constructs\nthe data set as follows: (1) among the remaining units, construct\na matched sample then (2) for each matched pair, discard information \nthat is recorded prior to ${\\hat{T}_i^{\\rm LaPRET}}={T_i^{\\rm event}}-\\hat{d}$. The remaining\ndata then represents the potential outcomes under treatment\nand control.\n\n\n\n \n \n \n\n\\section{Simulations}\\label{sec:sims}\n\n\n\nThe purpose of our method is to give the analyst an interval of\ndays $\\hat{d}$ prior to the event, for which he can make causal\nstatements about the conceptualized treatment. This section has two objectives: first, we assess how\nthe choice of parameters $\\alpha$ and $\\epsilon$ affects the behavior\nof our method in different scenarios. Second, we explore how this \nbehavior is affected by different types and levels of noise in the observations.\n\n\\subsection{Design choices} \\label{sect:sims-description}\n\nWe conduct three experiments of increasing complexity, trying capturing different \nreal life response profiles. For each of the three scenarios, we consider a pair of ``idealized\nresponse surfaces'' under control and treatment -- that is, \nresponse surfaces with no noise that capture one of the three \nscenarios of interest. We denote pair of responses corresponding\nto the $k^{th}$ scenario by $(\\mu^{(k)}_0(t), \\mu^{(k)}_1(t))$. For\nall three scenarios, the idealized control response surface is the \nflat line at zero, $\\mu^{(k)}_0(t) = 0$. The idealized response \nsurfaces are represented in Figure~\\ref{fig:resp}.\n\nWe also consider two potential sources of noise in the observations. \nFirst we simulate the fact the potential outcomes are only noisy versions of the idealized\nresponse surface. So for each scenario $k$, we generate $N$ treatment and \ncontrol potential outcomes surfaces:\n\n\n", "index": 9, "text": "\\begin{equation}\n\tY^{(k)}_{i,t}(a) \\sim \\mbox{Normal }(\\mu^{(k)}_a(t), \\sigma^2)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"Y^{(k)}_{i,t}(a)\\sim\\mbox{Normal }(\\mu^{(k)}_{a}(t),\\sigma^{2})\" display=\"block\"><mrow><mrow><msubsup><mi>Y</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u223c</mo><mrow><mtext>Normal\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03bc</mi><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msup><mi>\u03c3</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\nFor each of the $4\\times 4=16$ possible noise structures $(\\sigma^2,f_i)$, \nwe study the behavior of our method for $\\alpha = 1, 6, \\ldots 96$ and \n$\\epsilon = 0.0001, 0.02, 0.2, 0.3, 0.4, 0.5$. So for each combination of parameters\n$(\\sigma^2, f_i, \\alpha, \\epsilon)$, we simulated a data set with $N=600$\nobservations, and compute $\\hat{d}$ as in \\textit{step 2} of Section~\\ref{sec:method}. We\nforgo matching in the simulations since this is beyond the scope of our contributions. The \nthree simulations are described in more details below, and\nthe results are analyzed in Section~\\ref{section:analysis-simul-results}.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/simul-surface-123.pdf}\n\t\\caption{\\onehalfspacing Response surface for the three simulations. The solid lines are the \n\tobservation times, ${T_i^{\\rm event}}$, and the dotted lines are the LaPRET, ${T_i^{\\rm LaPRET}}$}\n\t\\label{fig:resp}\n\\end{figure}\n\n\\subsubsection{Non-negative effect curve, zero at observation time}\n\\label{sec:sim1}\n\nThe first simulation, we consider the scenario in the left panel of \nFigure~\\ref{fig:resp}, where the LaPRET is ${T_i^{\\rm LaPRET}} = 3$, the event is \nobserved at time ${T_i^{\\rm event}}=14$ (and so $d_i = {T_i^{\\rm event}}-{T_i^{\\rm LaPRET}} = 11$) and the treatment response surface \nis represented, for mathematical convenience, by the function \n$\\mu^{(1)}_1(t)= \\max \\bigm\\{ 0, ~\\sin(\\frac{2\\pi}{15}(t-3.5)) \\bigm\\}$. In this \nsetup, the true effect is always non-negative, but is zero at \nthe observation time, and so a naive analysis might find no \neffect even when one is present. This version of a response\nis likely when the natural event is catastrophic---if individuals\nexpect a snow storm, for instance, they might order batteries and snow tires in advance so \nas to have them ready for after the storm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Non-negative effect curve, positive at observation time} \n\\label{sec:sim2}\n\nThe second simulation considers the same response surface\nas Simulation~1 but a shifted observation time ${T_i^{\\rm event}} = 9$ such\nthat $d=6$ as displayed in the second panel of Figure~\\ref{fig:resp}.\nIn this scenario, the true effect is still always non-negative, but \nthe observation time corresponds to a point where there is still\na difference between the treated and control levels of the \npotential outcomes. This is a possible response surface\nfor insurance quote requests due to an upcoming\nstorm---there is an increase due to the forecast\nand it does not necessarily go back down to the\nprevious levels until after the storm. \nSimilar response surfaces were observed\nin for online marketing \ncampaigns \\citep{lewis2011here}.\n\n\n\n\\subsubsection{Positive-negative effect curve, zero at observation time}\n\\label{sec:sim3}\n\nThe third simulation introduces volatility into the \nresponse surface. Here $\\mu^{(k)}_1(t)$ is as in \\eqref{sim3}, ${T_i^{\\rm LaPRET}}=2$ and\n${T_i^{\\rm event}}=14$. In this scenario, the true effect is positive immediately \nafter ${T_i^{\\rm LaPRET}}$, but then changes sign before reaching zero shortly before ${T_i^{\\rm event}}$. \nIn particular, there is a point of zero effect between ${T_i^{\\rm LaPRET}}$ and ${T_i^{\\rm event}}$ which does\nnot correspond to the LaPRET. In the context of medical trials, this volatility \ncould correspond to the side-effect of a drug on a person's blood pressure. It could\nalso correspond to the purchasing of commodities before a big storm -- the dip \nillustrated in the right panel of Figure~\\ref{fig:resp} corresponds to the fact that once\nan individual stocks up on a commodity, he is likely to buy less of it for a while. Another\ninterpretation is that in anticipation of the event, individuals move their usual purchasing\nday to before the event, provoking a dip in the days immediately preceding the event.\n\n\n", "itemtype": "equation", "pos": 23969, "prevtext": "\n\nfor $a=0,1$, and $i=1\\ldots N$. The noise parameter $\\sigma$ was given the following\nvalues $\\sigma = 0.005, 0.01, 0.015, 0.02$, but such that the potential outcome surfaces generated\nin a given simulation all shared the same parameter $\\sigma$. The second source of \nnoise we consider is with the time of observation ${T_i^{\\rm event}}$ of the event, which we simulate\nwith a contamination model: ${T_i^{\\rm event}} = {T_i^{\\rm LaPRET}} + d + c$ where ${T_i^{\\rm LaPRET}}$ and $d$ are fixed and $c$\nis a contamination model that is governed by one of the following distributions:\n\n \n", "index": 11, "text": "\\begin{align*}\n & f_1(c) = 0 \\text{ w.p. 1} \n & \\qquad\n f_2(c) = \\begin{cases}\n \t-1 & \\text{ w.p. 0.25}\\\\\n\t0  & \\text{ w.p. 0.5}\\\\\n\t1  & \\text{ w.p. 0.25}\n\t\\end{cases}\n \\\\\n & f_3(c) = \\begin{cases}\n \t-1 & \\text{ w.p. 0.1}\\\\\n\t0  & \\text{ w.p. 0.5}\\\\\n\t1  & \\text{ w.p. 0.4}\n\t\\end{cases}\n & \\qquad\n f_4(c) = \\begin{cases}\n \t-1 & \\text{ w.p. 0.4}\\\\\n\t0  & \\text{ w.p. 0.5}\\\\\n\t1  & \\text{ w.p. 0.1}\n\t\\end{cases}\t\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{1}(c)=0\\text{ w.p. 1}\" display=\"inline\"><mrow><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0w.p. 1</mtext></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad f_{2}(c)=\\begin{cases}-1&amp;\\text{ w.p. 0.25}\\\\&#10;0&amp;\\text{ w.p. 0.5}\\\\&#10;1&amp;\\text{ w.p. 0.25}\\end{cases}\" display=\"inline\"><mrow><mrow><mi>\u2003\u2003</mi><mo>\u2062</mo><msub><mi>f</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.25</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.5</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.25</mtext></mtd></mtr></mtable></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{3}(c)=\\begin{cases}-1&amp;\\text{ w.p. 0.1}\\\\&#10;0&amp;\\text{ w.p. 0.5}\\\\&#10;1&amp;\\text{ w.p. 0.4}\\end{cases}\" display=\"inline\"><mrow><mrow><msub><mi>f</mi><mn>3</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.1</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.5</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.4</mtext></mtd></mtr></mtable></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad f_{4}(c)=\\begin{cases}-1&amp;\\text{ w.p. 0.4}\\\\&#10;0&amp;\\text{ w.p. 0.5}\\\\&#10;1&amp;\\text{ w.p. 0.1}\\end{cases}\" display=\"inline\"><mrow><mrow><mi>\u2003\u2003</mi><mo>\u2062</mo><msub><mi>f</mi><mn>4</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.4</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.5</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0w.p. 0.1</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\n\n\\subsection{Analysis of the results}\n\\label{section:analysis-simul-results}\n\nThe effect of the different parameters is similar in all three scenarios, so we\nonly describe the results of the second Simulation, described in Section \\ref{sec:sim2}. The results are summarized in Figure~\\ref{fig:simtest2}. \nFigures \\ref{fig:simest1} and \\ref{fig:simest3} summarize the results of the first and third simulations, respectively. We start by noticing\nthat although the contamination models have a small impact on the aggregated value of $\\hat{d}$,\nthis can have a large impact on the integer part of $\\hat{d}$. This quantity, which we denote by $\\left \\lfloor {\\hat{d}} \\right \\rfloor$, is the quantity of interest\nsince it represents the number of days before the event date for which we can make causal\nstatements. For instance in Figure~\\ref{fig:simtest2}, for low levels of $\\alpha$, and $\\sigma=0.005$,\nwe see that $\\left \\lfloor {\\hat{d}} \\right \\rfloor = 6$ for contamination models $f_1, f_2$ and $f_3$, but $\\left \\lfloor {\\hat{d}} \\right \\rfloor = 5$ \nfor $f_4$. This uncertainty is difficult to account for, and our method is very sensitive to it. For\nfixed values of $\\sigma$ and $\\epsilon$, we see that $\\hat{d}$ is close to the true value $d$ for\nlow values of $\\alpha$, but then decreases as $\\alpha$ increases. For very low values of $\\alpha$,\nhowever, the $\\hat{d}$ decreases. For fixed values of $\\alpha$ and\n$\\sigma$, $\\hat{d}$ is close to zero for low values of $\\epsilon$, increases until a certain point with $\\epsilon$,\nthen decreases again to reach zero for high values of $\\epsilon$. Finally, we see that as the noise\nlevel $\\sigma$ increases, $\\hat{d}$ decreases, especially for high values of $\\alpha$.\\\\\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_2-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 2, described in Section \\ref{sec:sim2}.}\n\t\\label{fig:simtest2}\n\\end{figure}\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_1-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 1, described in Section \\ref{sec:sim1}.}\n\t\\label{fig:simest1}\n\\end{figure}\n\nA decrease in the value of $\\hat{d}$ is a conservative behavior from a causal perspective, as \nit reduces the range of effects that we can call causal. This means, based on our observations in \nthe previous paragraph, that as the noise $\\sigma$ increases, the estimator becomes increasingly\nconservative in these scenarios, which is a desirable behavior. This property of our estimator is \nfurther explored in Section~\\ref{sec:data}. Our observations have also made clear the fact that \nincreasing the value of the parameters $\\alpha$ tends to make the estimator more conservative.\n\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_3-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 3, described in Section \\ref{sec:sim3}.}\n\t\\label{fig:simest3}\n\\end{figure}\n\n \n \n \n\n\\section{Analyzing the effect of snowfall on online behavior}\n\\label{sec:data}\n\nIn this section we estimate the causal treatment effect of\nperceived large quantities of future snow on sales of products\non the internet. Our estimates are based on data  \nprovided by MaxPoint Interactive Inc., an advertising\ntechnology company based in Raleigh, North Carolina. The data was \nprovided according to designated marketing area (DMA) which \ncorresponds to 79 super-metropolitan areas in the United States. For \neach DMA, the data contains three types of information over a period of\ntwo months: \n (1) searches for batteries on a major retailer's website, which we will consider as the outcome of our analysis, \n (2) Demographic information from the census bureau, summarized in Figure~\\ref{fig:datapop} and Figure~\\ref{fig:datasum}, and \n (3) the cumulative daily snowfalls. This data set is interesting as it can be seen as recording a natural experiment at the nationwide scale, in which the treatment time is unknown \\citep[e.g., see][]{angrist2000aa,dunning_natural_2012,phan2015aa}.\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\includegraphics[scale = 0.3]{figures/data-prep-1}\n\t\\caption{Distribution of the population of the 79 DMAs considered.}\n\t\\label{fig:datapop}\n\\end{figure}\n\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\includegraphics[scale = 0.3]{figures/pilot-1-new}\n\t\n\t\\caption{Distribution of the demographic variables across\n\t\tthe 79 DMAs considered}\n\t\\label{fig:datasum}\n\\end{figure}\n\n\\subsection{The data}\n\\label{section:synthetic-data}\n\nDue to the large populations and geographical extents that\ndifferent DMAs cover, it is unreasonable to believe that an \naggregate of weather at the DMA level will constitute a treatment\nfor all units in said DMA. Another way to say this is that DMAs\nare not homogenous when it comes to weather, and there\ncan be a significant weather event happening in a DMA that will\naffect only a small portion of the units inside that DMA. We thus\nbuild a synthetic scenario that better illustrates the methodology\nwe propose. We introduce the concept of tradezones which can be\nthought of as smaller subergions of DMAs, within which weather will\nbe more homogeneous. For each DMA, a number of tradezones proportional\nto its population, totaling 3676 synthetic tradezones over all DMAs. \n\nFor tradezone $j$ in DMA $i$, we create for each day $t$ a synthetic \nobservation from a $\\text{Normal }(\\mu_i(t), \\sigma^2)$ for $y_{ij}(t)$ truncated below at zero, \nwhere $\\mu_i(t)$ is the real observed outcome for DMA $i$ on day $t$, such that\n$y_{ij}(t) \\geq 0$ for all $t$. Varying the noise level $\\sigma$ allows us to\nevaluate the robustness of our procedure. We ran simulations for $\\sigma = 2^k$, $k= 1 \\ldots 7$.\nTo give some perspective about the relative size of the noise, we report that the median \noutcome in the data across all DMAs and all days is 13, and the $95^{th}$ percentile is $60$.\n\nAnother element required to construct a realistic data set is to handle the \nweather, keeping in mind that the original motivation for introducing the\ntradezones was to deal with weather heterogeneity within DMAs. To address\nthis, we set a threshold $h=1 \\,{\\rm kg}/{\\rm m}^2$ of snow, and for each DMA, we considered\nthe days for which the snow precipitations exceeded that threshold. Suppose\nthere where $K$ such days for DMA $i$, which we will label $\\{t_1, \\ldots, t_K\\}$, and\nlet $S_1, \\ldots, S_K$ be the corresponding precipitations in ${\\rm kg}/{\\rm m}^2$. For each \ntradezone $j$, we selected a day of observation $T^{({\\rm event}, j)}$ at random among\n$\\{t_1, \\ldots, t_K\\}$, such that $p(T^{({\\rm event},j)} = t_l) = \\frac{S_l}{\\sum_k S_k}$, that is,\nproportional to snow precipitations in the eligible days. Tradezones in DMAs for \nwhich no snow precipitation exceeded $l = 0.3 \\,{\\rm kg}/{\\rm m}^2$ were all assigned to the \ncontrol group, while all remaining tradezones were ignored.\n\nWith this definition of treatment, it is reasonable to say that in most cases,\nno unit with $D_i = 0$ would have had the perception that it is going to be hit\nby an event such as $1\\, {\\rm kg}/{\\rm m}^2$ of snow. This definition justifies setting the correlation in Assumption 1 to be one (that is $\\eta=0$) and hence the observed outcomes are realizations of the potential outcomes (that is $\\delta=0$ in Eq~\\eqref{eqn:reconc}). \nLarger $h$ strengthen this assumption but drastically reduce the number of DMAs in treatment.  \n\n\n\n\n\n\n\nCensus data at the DMA level are used to complete the synthetic data set. For each DMA $i$, we have a vector of covariates $x_i$.\nSince the propensity score matching is performed at the tradezone level, we let\ntradezone $j$ in DMA $i$ inherit its covariate vector $x_i^{(j)}$ from the \nparent DMA. That is, we assume that $x_i^{(j)} = x_i\\ \\forall j$.\n\nIn summary, for each DMA (for which we have real data), we have generated\nsynthetic tradezones, for which we simulated synthetic observations, and\nselected a day of observation for the snow event based on the distribution of snowfall in the DMA. Our purpose is\nto illustrate how an analyst would apply our method to that kind of data, \nand what kind of robustness he should be expecting. The results we report below represent causal estimates of nationwide battery searches online \n(for a major american retailer) based on the synthetic data.\n\n\n\\subsection{Sensitivity analysis}\n\nFor each of the seven levels of noise $\\sigma = 2^k$, $k=1\\ldots7$, we generated \none data set as described in Section~\\ref{section:synthetic-data}, and ran both a \npilot study and an analysis as described in Section~\\ref{sec:method}, with parameters \n$\\alpha = 2.5$ and $\\epsilon = 4.$. Injecting different levels of noise assesses the \nsensitivity of the analysis to the synthetic data generating process. We discuss heuristics\nfor the choice of $\\alpha$ and $\\epsilon$ in Section~\\ref{section:heuristics}.\n\nTo stabilize the data and account for\ndifference in baseline outcomes among DMAs, we consider lagged differences between outcomes. That is, if $y_{ij}(t)$\nis the outcome of tradezone $j$ in DMA $i$ at time $t$, then we considered the transformed\noutcomes $y^*_{ij}(t) = y_{ij}(t) - y_{ij}(t-1)$ (we ignore the first day in the data set), and \napplied the method in Section~\\ref{section:synthetic-data} to the transformed data.This analysis provides insight into the causal changes in behavior from day to day due to perceived future snow.\nFigure~\\ref{fig:dhat_sensitivity} shows the values of $\\hat{d}$ obtained in the seven pilot studies,\nwhere each pilot study contains 878 of the 3676 tradezones. The solid line shows the integer part of $\\hat{d}$ which is the real\nquantity of interest, since $d$ is the number of days before the event for which we can report\nthe effect as being causal. We see that the $\\hat{d}$ (and hence the solid line) decreases\nas the standard deviation of the noise increases in our simulations. This confirms the \nconservative behavior identified in Section~\\ref{sec:sims}: the noisier the data, the more\nconservative we become about making causal statements.\n\n\\begin{figure}[b!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/dhat_sensitivity}\n\t\\caption{Sensitivity of $\\hat{d}$ to noise in the tradezones}\n\t\\label{fig:dhat_sensitivity}\n\\end{figure}\n\nAfter obtaining values for $\\hat{d}$ from the pilot studies, we completed the analysis\nfor each data set based on the remaining 2798 tradezones. Figure~\\ref{fig:effects_sensitivity}\nsummarizes the results, and illustrate the conservative nature of our procedure: for \nhigh levels of noise, the only effect that can be reported as causal is that on the day\nof the observed event. For low values of the noise however, causal statements can \nbe made up to two days prior to the day of the weather event.\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/effects_sensitivity}\n\t\n\t\\caption{\\onehalfspacing Sensitivity of the estimated effect to noise in the tradezones. Each frame represents the average causal effect and a 95\\% confidence interval.}\n\t\\label{fig:effects_sensitivity}\n\\end{figure}\n\n\nFrom this data we conclude that there is a causal relationship between searches for batteries and perceived future snow events that takes on the form of the ATE in Simulation 3 (see Fig~\\ref{fig:resp}). That is, the causal effect appears to be consistently positive and growing several days prior to the weather event (at all noise levels) and then decreases drastically on the day of the event. For example, at the lowest noise level introduced into the synthetic data the day-to-day change is close to zero three days prior to the event, but increases to 0.95 two days before the event and again increases to 2.55 one day before the event. From the day before the event to the day of the event there is a drop of 7.8 in the rate of searches on average.\n\n\\subsection{A heuristic for the choice of $\\alpha$ and $\\epsilon$}\n\\label{section:heuristics}\n\nWe have seen in Section~\\ref{sec:sims} that the choice of $\\alpha$ and $\\epsilon$ governs how conservative our method is. Although it is ultimately up to the analyst to chose and justify the parameters he uses in the analysis, we provide some heuristics to guide this choice. In this section, we will let $Y^{obs}_{i,t}$ be either the observed outcomes, or the first order differences which we denoted by $Y^*$ in the previous section. Consider $\\Delta_{i,t}$ and $\\partial \\Delta_i,t$ as in Section~\\ref{sec:method}, then let $\\bar{\\Delta}$ and $\\overline{\\partial \\Delta}$ be the respective averages of their absolute values, $\\tilde{\\Delta}$ and $\\widetilde{\\partial \\Delta}$ the respective maxima of their absolute values, and  $\\mbox{se}(\\Delta)$ and $\\mbox{se}(\\partial \\Delta)$ the respective standard errors of their absolute values. We suggest choosing values of $\\alpha$ and $\\epsilon$ satisfying:\n\n\n", "itemtype": "equation", "pos": 28239, "prevtext": "\n\nFor each of the $4\\times 4=16$ possible noise structures $(\\sigma^2,f_i)$, \nwe study the behavior of our method for $\\alpha = 1, 6, \\ldots 96$ and \n$\\epsilon = 0.0001, 0.02, 0.2, 0.3, 0.4, 0.5$. So for each combination of parameters\n$(\\sigma^2, f_i, \\alpha, \\epsilon)$, we simulated a data set with $N=600$\nobservations, and compute $\\hat{d}$ as in \\textit{step 2} of Section~\\ref{sec:method}. We\nforgo matching in the simulations since this is beyond the scope of our contributions. The \nthree simulations are described in more details below, and\nthe results are analyzed in Section~\\ref{section:analysis-simul-results}.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/simul-surface-123.pdf}\n\t\\caption{\\onehalfspacing Response surface for the three simulations. The solid lines are the \n\tobservation times, ${T_i^{\\rm event}}$, and the dotted lines are the LaPRET, ${T_i^{\\rm LaPRET}}$}\n\t\\label{fig:resp}\n\\end{figure}\n\n\\subsubsection{Non-negative effect curve, zero at observation time}\n\\label{sec:sim1}\n\nThe first simulation, we consider the scenario in the left panel of \nFigure~\\ref{fig:resp}, where the LaPRET is ${T_i^{\\rm LaPRET}} = 3$, the event is \nobserved at time ${T_i^{\\rm event}}=14$ (and so $d_i = {T_i^{\\rm event}}-{T_i^{\\rm LaPRET}} = 11$) and the treatment response surface \nis represented, for mathematical convenience, by the function \n$\\mu^{(1)}_1(t)= \\max \\bigm\\{ 0, ~\\sin(\\frac{2\\pi}{15}(t-3.5)) \\bigm\\}$. In this \nsetup, the true effect is always non-negative, but is zero at \nthe observation time, and so a naive analysis might find no \neffect even when one is present. This version of a response\nis likely when the natural event is catastrophic---if individuals\nexpect a snow storm, for instance, they might order batteries and snow tires in advance so \nas to have them ready for after the storm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Non-negative effect curve, positive at observation time} \n\\label{sec:sim2}\n\nThe second simulation considers the same response surface\nas Simulation~1 but a shifted observation time ${T_i^{\\rm event}} = 9$ such\nthat $d=6$ as displayed in the second panel of Figure~\\ref{fig:resp}.\nIn this scenario, the true effect is still always non-negative, but \nthe observation time corresponds to a point where there is still\na difference between the treated and control levels of the \npotential outcomes. This is a possible response surface\nfor insurance quote requests due to an upcoming\nstorm---there is an increase due to the forecast\nand it does not necessarily go back down to the\nprevious levels until after the storm. \nSimilar response surfaces were observed\nin for online marketing \ncampaigns \\citep{lewis2011here}.\n\n\n\n\\subsubsection{Positive-negative effect curve, zero at observation time}\n\\label{sec:sim3}\n\nThe third simulation introduces volatility into the \nresponse surface. Here $\\mu^{(k)}_1(t)$ is as in \\eqref{sim3}, ${T_i^{\\rm LaPRET}}=2$ and\n${T_i^{\\rm event}}=14$. In this scenario, the true effect is positive immediately \nafter ${T_i^{\\rm LaPRET}}$, but then changes sign before reaching zero shortly before ${T_i^{\\rm event}}$. \nIn particular, there is a point of zero effect between ${T_i^{\\rm LaPRET}}$ and ${T_i^{\\rm event}}$ which does\nnot correspond to the LaPRET. In the context of medical trials, this volatility \ncould correspond to the side-effect of a drug on a person's blood pressure. It could\nalso correspond to the purchasing of commodities before a big storm -- the dip \nillustrated in the right panel of Figure~\\ref{fig:resp} corresponds to the fact that once\nan individual stocks up on a commodity, he is likely to buy less of it for a while. Another\ninterpretation is that in anticipation of the event, individuals move their usual purchasing\nday to before the event, provoking a dip in the days immediately preceding the event.\n\n\n", "index": 13, "text": "\\begin{equation}\\label{sim3}\n\tu(t) = \n\t\\begin{cases}\n\t\t0 & \\text{if $v(t) \\leq 0$ and $t < 4$}\\text{ or }\n\t\t\\text{if $v(t) \\geq 0$ and $t > 10$}\\\\\n\t\t\\sin(\\frac{3.5\\pi}{15}(t-2.5)) &\\text{otherwise}.\n\t\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"u(t)=\\begin{cases}0&amp;\\text{if $v(t)\\leq 0$ and $t&lt;4$}\\text{ or }\\text{if $v(t)%&#10;\\geq 0$ and $t&gt;10$}\\\\&#10;\\sin(\\frac{3.5\\pi}{15}(t-2.5))&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><mrow><mi>u</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mtext>\u00a0and\u00a0</mtext><mrow><mi>t</mi><mo>&lt;</mo><mn>4</mn></mrow><mtext>\u00a0or if\u00a0</mtext><mrow><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mn>0</mn></mrow><mtext>\u00a0and\u00a0</mtext><mrow><mi>t</mi><mo>&gt;</mo><mn>10</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"false\"><mfrac><mrow><mn>3.5</mn><mo>\u2062</mo><mi>\u03c0</mi></mrow><mn>15</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>2.5</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\nand \n\n\n", "itemtype": "equation", "pos": 41757, "prevtext": "\n\n\n\\subsection{Analysis of the results}\n\\label{section:analysis-simul-results}\n\nThe effect of the different parameters is similar in all three scenarios, so we\nonly describe the results of the second Simulation, described in Section \\ref{sec:sim2}. The results are summarized in Figure~\\ref{fig:simtest2}. \nFigures \\ref{fig:simest1} and \\ref{fig:simest3} summarize the results of the first and third simulations, respectively. We start by noticing\nthat although the contamination models have a small impact on the aggregated value of $\\hat{d}$,\nthis can have a large impact on the integer part of $\\hat{d}$. This quantity, which we denote by $\\left \\lfloor {\\hat{d}} \\right \\rfloor$, is the quantity of interest\nsince it represents the number of days before the event date for which we can make causal\nstatements. For instance in Figure~\\ref{fig:simtest2}, for low levels of $\\alpha$, and $\\sigma=0.005$,\nwe see that $\\left \\lfloor {\\hat{d}} \\right \\rfloor = 6$ for contamination models $f_1, f_2$ and $f_3$, but $\\left \\lfloor {\\hat{d}} \\right \\rfloor = 5$ \nfor $f_4$. This uncertainty is difficult to account for, and our method is very sensitive to it. For\nfixed values of $\\sigma$ and $\\epsilon$, we see that $\\hat{d}$ is close to the true value $d$ for\nlow values of $\\alpha$, but then decreases as $\\alpha$ increases. For very low values of $\\alpha$,\nhowever, the $\\hat{d}$ decreases. For fixed values of $\\alpha$ and\n$\\sigma$, $\\hat{d}$ is close to zero for low values of $\\epsilon$, increases until a certain point with $\\epsilon$,\nthen decreases again to reach zero for high values of $\\epsilon$. Finally, we see that as the noise\nlevel $\\sigma$ increases, $\\hat{d}$ decreases, especially for high values of $\\alpha$.\\\\\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_2-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 2, described in Section \\ref{sec:sim2}.}\n\t\\label{fig:simtest2}\n\\end{figure}\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_1-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 1, described in Section \\ref{sec:sim1}.}\n\t\\label{fig:simest1}\n\\end{figure}\n\nA decrease in the value of $\\hat{d}$ is a conservative behavior from a causal perspective, as \nit reduces the range of effects that we can call causal. This means, based on our observations in \nthe previous paragraph, that as the noise $\\sigma$ increases, the estimator becomes increasingly\nconservative in these scenarios, which is a desirable behavior. This property of our estimator is \nfurther explored in Section~\\ref{sec:data}. Our observations have also made clear the fact that \nincreasing the value of the parameters $\\alpha$ tends to make the estimator more conservative.\n\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/lapret_simul_3-alt.pdf}\n\t\\caption{\\onehalfspacing Estimate $\\hat{d}$ as a function $\\alpha$ under different\n\tcontamination models, different levels of noise, and different values of $\\epsilon$ for simulation 3, described in Section \\ref{sec:sim3}.}\n\t\\label{fig:simest3}\n\\end{figure}\n\n \n \n \n\n\\section{Analyzing the effect of snowfall on online behavior}\n\\label{sec:data}\n\nIn this section we estimate the causal treatment effect of\nperceived large quantities of future snow on sales of products\non the internet. Our estimates are based on data  \nprovided by MaxPoint Interactive Inc., an advertising\ntechnology company based in Raleigh, North Carolina. The data was \nprovided according to designated marketing area (DMA) which \ncorresponds to 79 super-metropolitan areas in the United States. For \neach DMA, the data contains three types of information over a period of\ntwo months: \n (1) searches for batteries on a major retailer's website, which we will consider as the outcome of our analysis, \n (2) Demographic information from the census bureau, summarized in Figure~\\ref{fig:datapop} and Figure~\\ref{fig:datasum}, and \n (3) the cumulative daily snowfalls. This data set is interesting as it can be seen as recording a natural experiment at the nationwide scale, in which the treatment time is unknown \\citep[e.g., see][]{angrist2000aa,dunning_natural_2012,phan2015aa}.\n\n\n\n\n\n\n\n\n\n\\begin{figure}[!htbp]\n\t\\centering\n\t\\includegraphics[scale = 0.3]{figures/data-prep-1}\n\t\\caption{Distribution of the population of the 79 DMAs considered.}\n\t\\label{fig:datapop}\n\\end{figure}\n\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\includegraphics[scale = 0.3]{figures/pilot-1-new}\n\t\n\t\\caption{Distribution of the demographic variables across\n\t\tthe 79 DMAs considered}\n\t\\label{fig:datasum}\n\\end{figure}\n\n\\subsection{The data}\n\\label{section:synthetic-data}\n\nDue to the large populations and geographical extents that\ndifferent DMAs cover, it is unreasonable to believe that an \naggregate of weather at the DMA level will constitute a treatment\nfor all units in said DMA. Another way to say this is that DMAs\nare not homogenous when it comes to weather, and there\ncan be a significant weather event happening in a DMA that will\naffect only a small portion of the units inside that DMA. We thus\nbuild a synthetic scenario that better illustrates the methodology\nwe propose. We introduce the concept of tradezones which can be\nthought of as smaller subergions of DMAs, within which weather will\nbe more homogeneous. For each DMA, a number of tradezones proportional\nto its population, totaling 3676 synthetic tradezones over all DMAs. \n\nFor tradezone $j$ in DMA $i$, we create for each day $t$ a synthetic \nobservation from a $\\text{Normal }(\\mu_i(t), \\sigma^2)$ for $y_{ij}(t)$ truncated below at zero, \nwhere $\\mu_i(t)$ is the real observed outcome for DMA $i$ on day $t$, such that\n$y_{ij}(t) \\geq 0$ for all $t$. Varying the noise level $\\sigma$ allows us to\nevaluate the robustness of our procedure. We ran simulations for $\\sigma = 2^k$, $k= 1 \\ldots 7$.\nTo give some perspective about the relative size of the noise, we report that the median \noutcome in the data across all DMAs and all days is 13, and the $95^{th}$ percentile is $60$.\n\nAnother element required to construct a realistic data set is to handle the \nweather, keeping in mind that the original motivation for introducing the\ntradezones was to deal with weather heterogeneity within DMAs. To address\nthis, we set a threshold $h=1 \\,{\\rm kg}/{\\rm m}^2$ of snow, and for each DMA, we considered\nthe days for which the snow precipitations exceeded that threshold. Suppose\nthere where $K$ such days for DMA $i$, which we will label $\\{t_1, \\ldots, t_K\\}$, and\nlet $S_1, \\ldots, S_K$ be the corresponding precipitations in ${\\rm kg}/{\\rm m}^2$. For each \ntradezone $j$, we selected a day of observation $T^{({\\rm event}, j)}$ at random among\n$\\{t_1, \\ldots, t_K\\}$, such that $p(T^{({\\rm event},j)} = t_l) = \\frac{S_l}{\\sum_k S_k}$, that is,\nproportional to snow precipitations in the eligible days. Tradezones in DMAs for \nwhich no snow precipitation exceeded $l = 0.3 \\,{\\rm kg}/{\\rm m}^2$ were all assigned to the \ncontrol group, while all remaining tradezones were ignored.\n\nWith this definition of treatment, it is reasonable to say that in most cases,\nno unit with $D_i = 0$ would have had the perception that it is going to be hit\nby an event such as $1\\, {\\rm kg}/{\\rm m}^2$ of snow. This definition justifies setting the correlation in Assumption 1 to be one (that is $\\eta=0$) and hence the observed outcomes are realizations of the potential outcomes (that is $\\delta=0$ in Eq~\\eqref{eqn:reconc}). \nLarger $h$ strengthen this assumption but drastically reduce the number of DMAs in treatment.  \n\n\n\n\n\n\n\nCensus data at the DMA level are used to complete the synthetic data set. For each DMA $i$, we have a vector of covariates $x_i$.\nSince the propensity score matching is performed at the tradezone level, we let\ntradezone $j$ in DMA $i$ inherit its covariate vector $x_i^{(j)}$ from the \nparent DMA. That is, we assume that $x_i^{(j)} = x_i\\ \\forall j$.\n\nIn summary, for each DMA (for which we have real data), we have generated\nsynthetic tradezones, for which we simulated synthetic observations, and\nselected a day of observation for the snow event based on the distribution of snowfall in the DMA. Our purpose is\nto illustrate how an analyst would apply our method to that kind of data, \nand what kind of robustness he should be expecting. The results we report below represent causal estimates of nationwide battery searches online \n(for a major american retailer) based on the synthetic data.\n\n\n\\subsection{Sensitivity analysis}\n\nFor each of the seven levels of noise $\\sigma = 2^k$, $k=1\\ldots7$, we generated \none data set as described in Section~\\ref{section:synthetic-data}, and ran both a \npilot study and an analysis as described in Section~\\ref{sec:method}, with parameters \n$\\alpha = 2.5$ and $\\epsilon = 4.$. Injecting different levels of noise assesses the \nsensitivity of the analysis to the synthetic data generating process. We discuss heuristics\nfor the choice of $\\alpha$ and $\\epsilon$ in Section~\\ref{section:heuristics}.\n\nTo stabilize the data and account for\ndifference in baseline outcomes among DMAs, we consider lagged differences between outcomes. That is, if $y_{ij}(t)$\nis the outcome of tradezone $j$ in DMA $i$ at time $t$, then we considered the transformed\noutcomes $y^*_{ij}(t) = y_{ij}(t) - y_{ij}(t-1)$ (we ignore the first day in the data set), and \napplied the method in Section~\\ref{section:synthetic-data} to the transformed data.This analysis provides insight into the causal changes in behavior from day to day due to perceived future snow.\nFigure~\\ref{fig:dhat_sensitivity} shows the values of $\\hat{d}$ obtained in the seven pilot studies,\nwhere each pilot study contains 878 of the 3676 tradezones. The solid line shows the integer part of $\\hat{d}$ which is the real\nquantity of interest, since $d$ is the number of days before the event for which we can report\nthe effect as being causal. We see that the $\\hat{d}$ (and hence the solid line) decreases\nas the standard deviation of the noise increases in our simulations. This confirms the \nconservative behavior identified in Section~\\ref{sec:sims}: the noisier the data, the more\nconservative we become about making causal statements.\n\n\\begin{figure}[b!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/dhat_sensitivity}\n\t\\caption{Sensitivity of $\\hat{d}$ to noise in the tradezones}\n\t\\label{fig:dhat_sensitivity}\n\\end{figure}\n\nAfter obtaining values for $\\hat{d}$ from the pilot studies, we completed the analysis\nfor each data set based on the remaining 2798 tradezones. Figure~\\ref{fig:effects_sensitivity}\nsummarizes the results, and illustrate the conservative nature of our procedure: for \nhigh levels of noise, the only effect that can be reported as causal is that on the day\nof the observed event. For low values of the noise however, causal statements can \nbe made up to two days prior to the day of the weather event.\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.95\\textwidth]{figures/effects_sensitivity}\n\t\n\t\\caption{\\onehalfspacing Sensitivity of the estimated effect to noise in the tradezones. Each frame represents the average causal effect and a 95\\% confidence interval.}\n\t\\label{fig:effects_sensitivity}\n\\end{figure}\n\n\nFrom this data we conclude that there is a causal relationship between searches for batteries and perceived future snow events that takes on the form of the ATE in Simulation 3 (see Fig~\\ref{fig:resp}). That is, the causal effect appears to be consistently positive and growing several days prior to the weather event (at all noise levels) and then decreases drastically on the day of the event. For example, at the lowest noise level introduced into the synthetic data the day-to-day change is close to zero three days prior to the event, but increases to 0.95 two days before the event and again increases to 2.55 one day before the event. From the day before the event to the day of the event there is a drop of 7.8 in the rate of searches on average.\n\n\\subsection{A heuristic for the choice of $\\alpha$ and $\\epsilon$}\n\\label{section:heuristics}\n\nWe have seen in Section~\\ref{sec:sims} that the choice of $\\alpha$ and $\\epsilon$ governs how conservative our method is. Although it is ultimately up to the analyst to chose and justify the parameters he uses in the analysis, we provide some heuristics to guide this choice. In this section, we will let $Y^{obs}_{i,t}$ be either the observed outcomes, or the first order differences which we denoted by $Y^*$ in the previous section. Consider $\\Delta_{i,t}$ and $\\partial \\Delta_i,t$ as in Section~\\ref{sec:method}, then let $\\bar{\\Delta}$ and $\\overline{\\partial \\Delta}$ be the respective averages of their absolute values, $\\tilde{\\Delta}$ and $\\widetilde{\\partial \\Delta}$ the respective maxima of their absolute values, and  $\\mbox{se}(\\Delta)$ and $\\mbox{se}(\\partial \\Delta)$ the respective standard errors of their absolute values. We suggest choosing values of $\\alpha$ and $\\epsilon$ satisfying:\n\n\n", "index": 15, "text": "\\begin{equation}\n\t\\alpha_{min} = \\frac{\\tilde{\\Delta}}{\\bar{\\Delta} + 3 \\mbox{se}(\\Delta)} \\leq \\alpha \\leq \\frac{\\tilde{\\Delta}}{\\bar{\\Delta} + \\mbox{se}(\\Delta)} = \\alpha_{max}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\alpha_{min}=\\frac{\\tilde{\\Delta}}{\\bar{\\Delta}+3\\mbox{se}(\\Delta)}\\leq\\alpha%&#10;\\leq\\frac{\\tilde{\\Delta}}{\\bar{\\Delta}+\\mbox{se}(\\Delta)}=\\alpha_{max}\" display=\"block\"><mrow><msub><mi>\u03b1</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mover accent=\"true\"><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">~</mo></mover><mrow><mover accent=\"true\"><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>+</mo><mrow><mn>3</mn><mo>\u2062</mo><mtext>se</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2264</mo><mi>\u03b1</mi><mo>\u2264</mo><mfrac><mover accent=\"true\"><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">~</mo></mover><mrow><mover accent=\"true\"><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>+</mo><mrow><mtext>se</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><msub><mi>\u03b1</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.04083.tex", "nexttext": "\n\nThese heuristics are based on the interpretation of $\\alpha$ and $\\epsilon$ as measures of variation in the outcomes and the first differences of the outcomes. The choice of $\\alpha_{min}$ is the ratio of the maximum absolute variation in outcomes to three standard deviations more than the mean while $\\alpha_{max}$ is the ratio of the maximum variation in outcomes to one standard deviation more than the mean. The smaller $\\alpha$ values are thus associated with how extreme the maximum is in comparison to the mean. Similarly for $\\epsilon$, smaller values are associated with how extreme the maximum of the absolute value of first differences in outcomes is in comparison to the mean. The larger $\\alpha$ and $\\epsilon$ values have similar interpretation but with respect to the less extreme single standard deviation from the mean. The choice of one and three standard deviations is motivated by normal asymptotics.\n\nWith our synthetic data, the ranges of $[\\alpha_{min}, \\alpha_{max}]$ and $[\\epsilon_{min}, \\epsilon_{max}]$ depend on the noise level $\\sigma$ --- for illustration purposes, we consider the ranges $\\alpha \\in [1.6, 5.5]$ and $\\epsilon \\in [1.6, 5]$ obtained by the unions of the ranges for the different values $\\sigma = 2^i, \\,\\,\\, i=1\\ldots7$. Figure~\\ref{fig:heuristics} displays the values of $\\hat{d}$ that would be obtained for different combinations of $\\alpha$ and $\\epsilon$ within the range of our heuristics. Note that the parameters $\\alpha$ and $\\epsilon$ do not affect the estimate of the effect, only their causal interpretation.\n\n\n\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[scale=0.68]{figures/different-eps-alpha-vals.pdf}\n\t\\caption{\\onehalfspacing Values of $\\hat{d}$ for a range of parameters $\\alpha$ and $\\epsilon$ obtained using the heuristics in Section~\\ref{section:heuristics}, for different levels of noise}\n\t\\label{fig:heuristics}\n\\end{figure}\n\n \n \n \n\n\\section{Concluding remarks}\n\\label{sec:disc}\n\nThe methodology we developed in this article is not\nintended to supersede any of the traditional methodology\nfor dealing with observational studies, but rather to \ncomplement it. At a very high level, one can see our method\nas a pre-processing step, which provide the analyst with\none level of protection against unsubstantiated causal claims.\n\nWe make two main contributions. First we provide a set of\nassumptions and a method to determine a window before the\nday of the observed event for which we can we can make\ncausal statement. Our second contribution is cleanly separate\nthe overall process into a pilot study, which is used to determine the \nwindow in which we can make causal statements, and the\ncausal analysis, which is carried on a disjoint subset of data. This\nprecaution insulates the causal analysis from any dependence \non the observed outcomes used int he analysis. We have shown\nin simulation studies that our method becomes increasingly \nconservative when the observed outcomes become volatile,\nand that passed a certain level of noise, the method precludes\nany causal statement beyond the date when the treatment\nproxy is observed.\n\nOur methodology extends the reach of causal inference to a \nspecific type of observational studies, in which it is suspected\nthat the causal effect happens before the date in which a proxy\nto the treatment is observed. The price paid for this extension is \na reliance on extra assumptions, and a loss of efficiency since the\ncausal analysis is carried only on a subset of data. We also\nemphasize the fact that the causal analysis carried being an \nobservational studies, it suffers from the usual limitations.\n\n\n\n\n\n\n\n\n\n \n \n \n \n\\bibliographystyle{plainnat}\n\\addcontentsline{toc}{section}{References}\n\\bibliography{biblio}\n\n", "itemtype": "equation", "pos": 41958, "prevtext": "\n\nand \n\n\n", "index": 17, "text": "\\begin{equation}\n\t\\epsilon_{min} = \\frac{\\widetilde{\\partial \\Delta}}{\\overline{\\partial \\Delta} + 3 \\mbox{se}(\\partial \\Delta)} \\leq \\epsilon \\leq \\frac{\\widetilde{\\partial \\Delta}}{\\overline{\\partial \\Delta} + \\mbox{se}(\\partial \\Delta)} = \\epsilon_{max}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\epsilon_{min}=\\frac{\\widetilde{\\partial\\Delta}}{\\overline{\\partial\\Delta}+3%&#10;\\mbox{se}(\\partial\\Delta)}\\leq\\epsilon\\leq\\frac{\\widetilde{\\partial\\Delta}}{%&#10;\\overline{\\partial\\Delta}+\\mbox{se}(\\partial\\Delta)}=\\epsilon_{max}\" display=\"block\"><mrow><msub><mi>\u03f5</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mover accent=\"true\"><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>~</mo></mover><mrow><mover accent=\"true\"><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>\u00af</mo></mover><mo>+</mo><mrow><mn>3</mn><mo>\u2062</mo><mtext>se</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2264</mo><mi>\u03f5</mi><mo>\u2264</mo><mfrac><mover accent=\"true\"><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>~</mo></mover><mrow><mover accent=\"true\"><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>\u00af</mo></mover><mo>+</mo><mrow><mtext>se</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><msub><mi>\u03f5</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub></mrow></math>", "type": "latex"}]