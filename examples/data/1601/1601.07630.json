[{"file": "1601.07630.tex", "nexttext": " \n$\\lambda$ is the scaling factor, $\\mathbf{K}$ the camera intrinsic matrix, $\\mathbf{R}$ the camera rotation matrix, and $\\mathbf{C}$ the location of the camera center in world coordinates. Note that $-\\mathbf{R}\\mathbf{C}$ equals to the camera translation. Given a pair of $p$ and $P$, we have \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\n\n\n\n\n\n\n\\title{Automatic Generation of Building Models Using 2D Maps and Street View Images}\n\n\n\n\n\\author{\n\\IEEEauthorblockN{Jiangye Yuan}\n       \\IEEEauthorblockA{Computational Sciences \\& Engineering Division\\\\\n       Oak Ridge National Laboratory\\\\\n       Oak Ridge, Tennessee 37831\\\\\n       Email: yuanj@ornl.gov}\n\\and\n\\IEEEauthorblockN{Anil M. Cheriyadat}\n \\IEEEauthorblockA{Computational Sciences \\& Engineering Division\\\\\n       Oak Ridge National Laboratory\\\\\n       Oak Ridge, Tennessee 37831\\\\\n       Email: cheriyadatam@ornl.gov}\n}\n\n\n\\maketitle\n\n\\begin{abstract}\nWe introduce a new approach for generating simple 3D building models by combining building footprints from 2D maps with street level images. The approach works with crowd sourced maps such as the OpenStreetMap and street level images acquired by a calibrated camera mounted on a moving vehicle. Buildings are modeled as boxes extruded from building footprints with the height information estimated from images. Building footprints are elevated in world coordinates and projected onto images. Building heights are estimated by scoring projected footprints based on their alignment with visible building features in images. However, it is challenging to achieve accurate projections due to camera pose errors inherited from external sensors resulting in incorrect height estimation. We derive a solution to precisely locate cameras on maps using correspondence between image features and building footprints. We tightly couple the camera localization and height estimation steps to produce an effective method for 3D building model generation. Experiments using Google Street View images and publicly available map data show the promise of our method.\n\\end{abstract}\n\n\n\n\\section{Introduction}\nReconstruction of 3D building models has been extensively studied in computer vision and photogrammetry. Reconstructed models range from detailed photo-realistic 3D models to dense point cloud based representations. Correspondingly, reconstruction approaches too range widely in their cost from expensive multimodal sensor data fusion, such as airborne LiDAR with aerial images~\\cite{poullis2009,zhou2011,lin2013se} or ground based laser with street level images~\\cite{cornelis2008,xiao2009}, to structure estimation from ordered collection of stereo pair images~ \\cite{zebedin2006,bryan2013} or internet-scale community collected photos~\\cite{agarwal2010}. Although significant progress has been made, automatic reconstruction at large scale remains challenging \\cite{musialski2013}. In this paper, we take a unique approach by combining building ground plan data obtained from GIS maps with street level images to produce box models for buildings as shown in Fig~\\ref{fig:overview}. Such building models are adopted in many previous studies \\cite{cham2010,cho2013}. Given the wide availability of input data, the proposed approach can scale up to very large areas. \n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.48\\textwidth]{figs/3DBuild2.pdf}\n\\end{center}\n \\caption{Generating building models using building footprints on maps and street level images. Google Street View images are used here.}\n\\label{fig:overview}\n\\end{figure}\n\n\nOur approach involves extending building footprints in 2D maps vertically with absolute heights estimated from street level images. To estimate building heights, 2D footprints in maps with different elevations are projected onto images, which represent potential rooftop lines. However, even when camera projection parameters are available from sensors, cross alignment of building features between maps and images still poses a major challenge. A main reason is that the measurement of camera positions mostly relies on Global Positioning Systems (GPS), which tend to be affected by measurement conditions, especially in high density urban areas. Despite various new techniques combined with additional sensory information \\cite{qi2002,drawil2010,jo2012}, GPS errors remain at a noticeable level. For example, a median position error of 5-8.5 meters is reported for current generation smartphones \\cite{zandbergen2011}. Also, positional accuracy of map data is affected by errors inherent in data collection and map projections. To overcome spatial alignment errors, we derive an effective approach based on correspondence between map and image features to refine camera positions on maps. Upon refining camera position parameters, building heights are estimated based on the overlap between projected building footprints and visible building features in images. The camera localization step is tightly integrated with the height estimation step, which reduces the ambiguities in map and image correspondence. \n\nWe demonstrate our approach using OpenStreetMap (OSM)\\footnote{http://www.openstreetmap.org/} -- the largest crowd sourced maps produced and validated by contributors using aerial imagery and low-tech field maps. The OSM platform has millions of contributors and is able to generate map data with high efficiency. Street view services such as Google Street View and Microsoft StreetSide have collected huge amounts of images with a wide coverage, which provide ideal input images for our method. These two disparate data sources are fed to our method to automatically generate 3D building models. We validate the results by comparing with building height data obtained using LiDAR data sources and inspecting the alignment of 3D building model projections on images. \n\n\n\\section{Related work}\n\nCreating building models based on 2D map has been explored in a number of previous studies, where a critical step is to obtain height information. In \\cite{haala1996}, building heights and roof shapes are set to a few fixed values based on known building usage. In more recent work, stereo pairs of high resolution aerial images and aerial LiDAR data are used to estimate height information \\cite{zebedin2008,tack2012, chen2008}. However, since those types of data are not available for most areas in the world, the methods do not scale well. \n\nWe derive height information in a novel way. Through camera projections, we map edges of building footprints with different heights onto images. The building height is determined by matching the projected edges with building rooflines in street view images. We design an indicator that incorporates color and texture information and reliably find the projected edges aligned with rooflines.\n\nIn order to correctly project map data onto images, images need to be registered with maps. This is a difficult task  because it requires matching between abstract shapes on a ground plane and images from very different views. This has been pursued in a few recent studies. In \\cite{cham2010} omnidirectional images are used to extract building corner edges and plane normals of neighboring walls, which are then compared with structures on maps to identify camera positions. The method in \\cite{chu2014} follows the same framework and aims to refine camera positions initially from GPS. It works on a single view image, but it involves manual segmentation of buildings in images to obtain highly accurate building corner lines. Instead of 2D maps, digital elevation models (DEM) has also been utilized, where building roof corners are extracted and matched with those in images \\cite{bansal2014}. However, DEM is much less accessible than maps. It should be noted that existing techniques for camera pose estimation given 2D-to-3D point correspondences (the P$n$P problem) are not applicable here because point correspondences are not available. We propose a method that registers a single view image with maps through a voting scheme. The method is fully automated and works reliably on real-world data. \n\nThe rest of the paper is organized as follows. Section~\\ref{sec:bldmdl} presents the method to generate building models based on images and maps. The method for estimating accurate camera position on maps is discussed in Section~\\ref{sec:locenh}. In Section~\\ref{sec:Experiments} we conduct experiments on large datasets and provide quantitative evaluation. We conclude in Section~\\ref{sec:Conclusions}. \n\n\\section{Building models from images and maps}\n\\label{sec:bldmdl}\n\nFor a building footprint in a georeferenced map, we have access to 2D geographic coordinates (e.g., latitude and longitude) of the building footprint. If we set the elevation of the building footprint to the ground elevation, we have 3D world coordinates. Then we project the edges of the building footprint onto the image through camera projections. The projected edges should outline the building extent at its bottom. As we increase the elevation of the building footprint, the projected edges move toward the building roof. When the projected edges are aligned with building rooflines, the corresponding elevation is the roof elevation of the building. The building height is simply the increased elevation. This procedure is illustrated in Fig.~\\ref{fig:bldht}, where we project footprint edges within the field of view.   \n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.21\\textwidth]{figs/bldht1.png}\n\\hspace{2mm}\n\\includegraphics[trim = 0mm 60mm 0mm 20mm, clip, width=0.21\\textwidth]{figs/bldht2.png}\n\\makebox[0.21\\textwidth][c]{(a)}\n\\makebox[0.21\\textwidth][c]{(b)}\n\\end{center}\n \\caption{Building height estimation. (a) A map containing building footprints. The green marker represents the camera location. Blue edges indicate the part within the field of view. (b) A street level image. All blue lines are the projected edges with different elevation values. }\n\\label{fig:bldht}\n\\end{figure}\n\nWe need to determine whether projected footprint edges are aligned with building rooflines. A straightforward way is to compute image gradients and select the projected edges with the maximum gradient magnitude as the roofline. However, because lighting conditions change significantly and there exist many straight edges other than rooflines, gradient magnitude often fails to represent the existence of rooflines.   \n\nHere we use an edgeness indicator that incorporates both color and texture information \\cite{liu2002}. It is fast to compute and performs reliably. We first compute spectral histograms at a local window around each pixel location (the window size is set to $17\\times17$ in our experiments). A spectral histogram of an image window is a feature vector consisting of histograms of different filters responses. It has been shown that spectral histograms are powerful to characterize image appearances \\cite{liu2002}. Here we use a spectral histogram concatenating histograms of RGB bands and two Laplacian of Gaussian (LoG) filter responses. Two LoG filters are applied to the grayscale image converted from RGB bands ($\\sigma$ is set to 0.5 and 1, respectively). The histogram of each band has 11 equally spaced bins. Local histograms can be computed efficiently using the integral histogram approach. Since histograms of RGB bands are sensitive to lighting conditions, we weight those histograms by 0.5. The edgeness indicator value at $(x, y)$ is then defined as the sum of two feature distances between pixel locations $<(x + h, y), (x - h, y)>$ and $<(x, y + h), (x, y - h)>$, where $h$ is half of the window side length. $\\chi^2$ difference is used as the distance metric. This edgeness indicator reflects the appearance change between neighboring windows and thus better captures rooflines, which are essentially boundaries separating two distinctive regions. \n\n\n\n\n\n\n\n\n\n\n\n\n   \nThe main steps of the proposed method are described as follows. \n\n\\begin{enumerate}\n\\item \\textit{Identify visible building edges on a map.} Based on camera parameters, we determine the field of view on the map. We select the edges of building footprints that are not occluded by other buildings. Buildings far away from the camera may not show clear rooflines in an image. Therefore, we only consider buildings within a distance of 60 meters.   \n\n\\item \\textit{Project selected edges onto the image.} 2D geographic coordinates of the edges are known. We assign ground elevations to the edges and map them onto the image through a projective camera projection, which results in polylines in the image. Ground elevations can be obtained from public geographic databases or approximated based on camera positions.\n\n\\item \\textit{Determine building heights.} For each visible building, we gradually increase the elevation of selected edges and project them onto the image. The projected edges scan the image through a series of polylines. When the sum of the edgeness scores on a polyline reaches the maximum, the corresponding elevation gives the building height. There are cases where taller buildings behind the target building are also visible, but they usually have rooflines with different length and shape, which do not give maximum edgeness values.     \n\n\\item \\textit{Process a set of images.} The height of a building can be estimated sufficiently well with one image as long as building rooflines are visible in the image. If a collection of images are available, where a building appears in multiple images, we can utilize the redundancy to improve accuracy. For each building, we create a one-dimensional array with each element representing a height value and initialize all elements to zeros. After scanning an image for a building, we add one to the element corresponding to the estimated height value. Once all images containing the building are processed, we choose the element with the maximum value to obtain the building height. In some images where only lower part of a building can be seen, estimated heights are incorrect. We use a simple technique to deal with this issue. Because in such a case the projected edges are within building facades, the edgeness scores are generally small. We ignore the estimated height if the corresponding indicator value is smaller than a threshold, and thus incorrect estimates are not recorded in the array.  \n  \n\\end{enumerate}\n\nOur method treats building roofs as flat planes. For buildings with other roof shapes, the resulting height is generally between the top and the base of roofs. Since most non-flat roofs have a small height, our method can still give a reasonable estimate. The method may not work well when building rooflines in an image are completely occluded by other objects (e.g., trees and other buildings). However, with multiple views available, the method can exploit the rooflines of the same building visible in other images taken from different angles. If a building is obstructed on the map but visible in images because it is taller than obstructing buildings, the method will skip the building.   \n\n\\section{Camera localization on maps}\n\\label{sec:locenh}\n\nAs discussed earlier, we need to register images with maps for accurate projections. In this paper, we focus on camera positions because the position measurement is more susceptible to surrounding conditions than measurements of other camera parameters. To illustrate the effect of camera position errors on projection results, Fig.~\\ref{fig:locoff} shows the building footprint edges projected onto images with shifted camera positions. As can be seen, there are clear misalignments between projected footprint edges and buildings in images even though camera positions are shifted by only 3 meters. Such misalignments may cause projected building footprints to fail to match the corresponding rooflines and hence incorrect height estimation. Note that it is difficult to find point correspondence between images and maps due to the lack of appearance information on maps. As a result, PnP based methods are not applicable in this case. In the following we propose a method to estimate accurate camera positions on maps without appearance matching.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[trim = 0mm 60mm 0mm 0mm, clip, width=0.15\\textwidth]{figs/off0.png}\n\n\\includegraphics[trim = 0mm 60mm 0mm 0mm, clip,width=0.15\\textwidth]{figs/off1.png}\n\n\\includegraphics[trim = 0mm 60mm 0mm 0mm, clip,width=0.15\\textwidth]{figs/off2.png}\n\\makebox[0.15\\textwidth][c]{(a)}\n\n\\makebox[0.15\\textwidth][c]{(b)}\n\n\\makebox[0.15\\textwidth][c]{(c)}\n\\end{center}\n\\vspace{-2mm}\n \\caption{Impact of camera position errors. (a) A building footprint edge projected onto an image with a correct camera position. A series of elevation values are used. (b) and (c) The projections with camera positions moved 3 meters away from the correct one. }\n\\label{fig:locoff}\n\\end{figure}\n\n\\subsection{Camera position from image and map correspondence}\n\nA 3D point $\\mathbf{P}=(X,Y,Z)^T$ in world coordinates can be projected onto a pixel location $p=(u,v,1)^T$ in an image plane through the camera projection equation:\n\n", "index": 1, "text": "\\begin{equation}\n\\lambda\\mathbf{p}=\\left[ \\mathbf{K} | \\mathbf{0}_3 \\right]\n\\begin{bmatrix}\n\\mathbf{R} & - \\mathbf{R}\\mathbf{C}\\\\ \n\\mathbf{0}_3^T & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n \\mathbf{P} \\\\\n1\n\\end{bmatrix}\n=\\mathbf{K}\\mathbf{R}\\mathbf{P}- \\mathbf{K}\\mathbf{R}\\mathbf{C}.\n\\label{cp}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\lambda\\mathbf{p}=\\left[\\mathbf{K}|\\mathbf{0}_{3}\\right]\\begin{bmatrix}\\mathbf%&#10;{R}&amp;-\\mathbf{R}\\mathbf{C}\\\\&#10;\\mathbf{0}_{3}^{T}&amp;1\\end{bmatrix}\\begin{bmatrix}\\mathbf{P}\\\\&#10;1\\end{bmatrix}=\\mathbf{K}\\mathbf{R}\\mathbf{P}-\\mathbf{K}\\mathbf{R}\\mathbf{C}.\" display=\"block\"><mrow><mi>\u03bb</mi><mi>\ud835\udc29</mi><mo>=</mo><mrow><mo>[</mo><mi>\ud835\udc0a</mi><mo stretchy=\"false\">|</mo><msub><mn/><mn>3</mn></msub><mo>]</mo></mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc11</mi></mtd><mtd columnalign=\"center\"><mrow><mo>-</mo><mi>\ud835\udc11\ud835\udc02</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mn/><mn>3</mn><mi>T</mi></msubsup></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc0f</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mi>\ud835\udc0a\ud835\udc11\ud835\udc0f</mi><mo>-</mo><mi>\ud835\udc0a\ud835\udc11\ud835\udc02</mi><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07630.tex", "nexttext": " \nThat is, given correspondence between a pixel location and a 3D point in world coordinates, the possible camera positions $\\mathbf{C'}$ lie on a 3D line defined by (\\ref{cl}).    \n\nIn this work, we assume that camera position errors are mainly in a horizontal plane. Let $\\mathbf{C'}=(X_C,Y_C,Z_C)^T$. We can discard Z dimension and simplify (\\ref{cl}) to \n\n", "itemtype": "equation", "pos": -1, "prevtext": " \n$\\lambda$ is the scaling factor, $\\mathbf{K}$ the camera intrinsic matrix, $\\mathbf{R}$ the camera rotation matrix, and $\\mathbf{C}$ the location of the camera center in world coordinates. Note that $-\\mathbf{R}\\mathbf{C}$ equals to the camera translation. Given a pair of $p$ and $P$, we have \n\n", "index": 3, "text": "\\begin{equation}\n\\mathbf{C'}=\\mathbf{P}+\\lambda\\mathbf{R}^{-1}\\mathbf{K}^{-1}\\mathbf{p}.\n\\label{cl}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{C^{\\prime}}=\\mathbf{P}+\\lambda\\mathbf{R}^{-1}\\mathbf{K}^{-1}\\mathbf{p}.\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udc02</mi><mo>\u2032</mo></msup><mo>=</mo><mrow><mi>\ud835\udc0f</mi><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\ud835\udc11</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc0a</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc29</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07630.tex", "nexttext": " \nwhere $\\Delta X$ and $\\Delta Y$ are truncated from\n$(\\Delta X,\\Delta Y, \\Delta Z)^T=\\mathbf{R}^{-1}\\mathbf{K}^{-1}\\mathbf{p}$. This defines a line on a 2D plane. If correspondence of two pairs of points is given, the camera position can be uniquely determined, which is the intersection of two lines. \n\nBased on the above analysis, we can determine camera positions based on point correspondence between images and maps. We use corners on building footprints, which can be easily identified from map data. However, it is difficult to find the corresponding points in an image. To provide better features for matching, we place a vertical line segment on each building footprint corner (two end points have the same 2D coordinate but different elevation values). We will refer to such a line as a building corner line (BCL). The line length is fixed. Note that at this stage building height is not available. When projecting a BCL onto an image with an accurate camera position, it should be well aligned with building edges in the image. \n\nUnder the assumption that position errors are within a horizontal plane, for a BCL projected onto an image with an inaccurate camera position, the displacement is along image columns. We first project a BCL onto an image using an initial camera position. Next, we horizontally move the projected BCL toward both directions within a certain range. The range is set to be inversely proportional to the distance from the BCL to the camera position. At each moving step, we have a pair of $\\mathbf{P}$, the building footprint corner, and $\\mathbf{p}$, the projected point moved along columns. By using (\\ref{cl}), we can compute a line on the map that represents potential camera positions. We create an accumulator, which is a 2D array centered at the initial camera position. For each line, we increment the value of the bins the line passes through. The increased value is determined based on how well the moved BCL is aligned with building edges. At the end, the bin with the highest value gives the most likely camera position, which results in the best match between BCLs and building edges in images. The procedure is illustrated in Fig.~\\ref{fig:locest}.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.9\\textwidth]{figs/LocEst1.pdf}\n\\end{center}\n\\vspace{-5mm}\n \\caption{Illustration of camera position estimation. By moving projected BCLs (blues lines) in images, we compute potential camera positions (blue lines) on the map, which add values to an accumulator based on alignment measure between projected BCLs and building edges. The green marker indicates the initial camera position, and the black window shows the extend of the accumulator. In the accumulator, red pixels represent large values.  }\n\\label{fig:locest}\n\\end{figure*}\n\nWhen moving a projected BCL, we need to know whether it is aligned with a building edge so that proper values can be assigned to the accumulator. Detecting building edge is another challenging task. In an image of a street scene, contrast of building edges varies significantly due to changes of lighting conditions. Building edges can be fully or partially occluded by other objects. Moreover, there exist a large number of edges from other objects. Although the voting-based method is able to tolerate a reasonable amount of errors in detection results, a large number of false positives and false negatives can still confuse the method. \n\nWe exploit the idea of line support regions \\cite{burns1986}, which is widely used for straight line extraction. A line support region consists of pixels with similar gradient orientations, where a line segment can be extracted. A common practice for building edge extraction in previous work \\cite{cham2010,chu2014} is to assume that building edges have relatively large contrast. However, we observe that in many cases there are only slight color changes around building edges. Line support regions are formed based on consistency of gradient orientations regardless of magnitudes and hence better suited in this task.     \n\nSince street level images are captured by a camera with its Y-axis orthogonal to the ground, building edges in images should be close to vertical lines. We select pixels with gradient orientations within $22.5^{\\circ}$ around the vertical direction and find connected regions with large vertical extents and small horizontal extents (in the experiments two thresholds are set to 50 and 20 pixels, respectively). When a projected BCL hits a region center, the corresponding accumulator bins increment by one. A Gaussian density kernel is placed on each region center, and a projected BCL close to a center point casts a value equal to the maximum density value it reaches.\n\n\\subsection{Integration with height estimation}\nGiven an image and a map, the overall method consists of the following two steps. The first is to apply the voting based method to determine the camera position, and the second is to estimate the building height by examining projected building footprints. In certain scenarios, a set of line support regions that do not correspond to building edges happen to form a strong peak, resulting in an incorrect camera location. To address this issue, we integrate height estimation with camera position estimation, which utilizes the information of building footprint edges in addition to corners to obtain accurate positions and simultaneously produces building heights.  \n\nFrom a resulting accumulator, we select the top $k$ local maxima as candidate camera positions. For each of them, we project building footprint edges to match building rooflines in an image, as described in Section~\\ref{sec:bldmdl}. We calculate the sum of edgeness scores for all projected building footprint edges that matches rooflines. We select the candidate with the largest sum, where rooflines in the image should be best aligned with building footprints. This strategy integrates two separate steps and reduces ambiguities in correspondence between BCLs and building edges. Algorithm \\ref{alg1} summarizes the algorithm for processing a street view image. \n\n\\begin{algorithm}                     \n\\caption{Building height estimation with camera position refinement}  \n\\begin{algorithmic}[1] \n \\STATE Extract line support regions\n \\STATE Compute an accumulator using the voting method. Select the locations of top k peaks ${pos(i),i=1,\\ldots,k}$\n\\FOR{$i$:=1 \\TO $k$ }\n  \\STATE Set camera position to $pos(i)$\n  \\STATE Identify visible buildings on map\n \\FOR{each visible building}  \n   \\STATE Project building footprints onto image and estimate height based on edgeness values\n \\ENDFOR\n \\STATE Compute $S(i)$: the sum of edgeness values of projected edges matching rooflines\n\\ENDFOR\n \\STATE Output building heights with $\\max(S)$\n \\end{algorithmic}\n\\label{alg1}\n \\end{algorithm}\n\n\\section{Experiments}\n\\label{sec:Experiments}\nTo evaluate the proposed method, we compile a dataset consisting of Google Street View images and OSM building footprints. Images are collected by a camera mounted on a moving vehicle. The camera faces the front of the vehicle so that more building rooflines are visible. The image size is $905\\times640$. The intrinsic and extrinsic camera parameters are provided, where extrinsic parameters are derived from GPS, wheel encoder, and inertial navigation sensor data \\cite{anguelov2010}. We use 400 images covering an area in San Francisco, CA. Map data of the same area is downloaded from OSM, which are in the vector form. We convert the map data into an image plane with a spatial resolution of 0.3 meter. Geo-location information of map data and camera positions is converted into the UTM coordinate system for easy distance computation. Although camera positions in the dataset are more accurate than those solely based on GPS, we observe clear misalignments when projecting map data onto images.  \n\nWe apply the proposed method to the dataset with the following parameter setting. For camera position estimation, we use an accumulator corresponding to a local window on the image plane converted from the map. The local window is centered at the initial camera position and of size $40 \\times 40$ pixels, which is to correct position errors up to 6 meters. When there are very few detected line support regions corresponding to building edges, estimated camera positions are not reliable. To address this issue, we set an accumulator threshold (set as 1.5) and use estimated camera positions only when the detected peak is above the threshold. We select the largest 5 peaks in an accumulator as candidates of camera positions. To scan an image with projected building footprints for height estimation, we use discrete elevation values from 3 to 100 meters with a step size of 0.2 meter, which cover the range of building heights in the dataset.  \n\nFig.~\\ref{fig:bldmdllg} shows a subset of resulting building models around a city block. We use building height information derived from LiDAR data as ground truth. Building models using ground truth heights are also displayed in Fig.~\\ref{fig:bldmdllg}. As we can see, they are very close to each other. There is one building where the estimated height is significantly different from ground truth, which is indicated by a red arrow. We examine the images that the building height is derived from and find that the error is caused by low image quality combined with unusual lighting conditions. Two images are shown in Fig.~\\ref{fig:err}, where the proposed method confuses shadow borders as rooflines because the actual rooflines have a extremely low contrast.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.95\\textwidth]{figs/3Dres.pdf}\n\\end{center}\n\\vspace{-2mm}\n\\caption{3D building models. Left: building models generated by our method. Right: building models using LiDAR derived height information. The red arrow indicates one building with incorrect height estimation.}\n\\label{fig:bldmdllg}\n\\end{figure*}\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[trim = 0mm 60mm 0mm 20mm, clip, width=0.2\\textwidth]{figs/err3.png}\n\\includegraphics[trim = 0mm 60mm 0mm 20mm, clip, width=0.2\\textwidth]{figs/err4.png}\n\\end{center}\n \\caption{Images causing incorrect height estimation. Blue lines represent detected rooflines. }\n\\label{fig:err}\n\\end{figure}\n\nTo provide quantitative measurements, we calculate errors by comparing the height values from our method with ground truth. We set different error tolerance values (the maximum allowable deviation from ground truth) and compute the percentage of the buildings that have correct height estimation, which is reported in Table~\\ref{tab:data1perc}. The results agree with the observation in Fig.~\\ref{fig:bldmdllg}. We also evaluate the heights obtained without applying camera position estimation and show accuracy measurements in the table. As can be seen, refined camera positions lead to a clear improvement over initial ones. We have tested a structure-from-motion approach \\cite{snavely2006} on the images, which can be an alternative to generate 3D models. However, it failed to produce reasonable results because light conditions, image quality, distance between images adversely affect feature matching.    \n\n\\begin{table}\n\\small\n\\begin{center}\n\\caption{Accuracy of height estimation over different error tolerance.}\n\\begin{tabular}{c|c c c}\n\\hline\nError tolerance & 2 m & 3 m & 4 m \\\\ \\hline\nAccuracy (refined positions) & 72.2\\%  & 83.6\\%  & 90.1\\%  \\\\ \\hline\nAccuracy (initial positions) & 67.3\\%  & 78.6\\%  & 87.7\\%  \\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\label{tab:data1perc}\n\\end{table}\n\n\nWith images aligned with 3D maps, buildings in images can be easily segmented by projecting visible parts of building models onto images. Fig.~\\ref{fig:bldfcd} presents example results, where the masks obtained by projecting building models are well aligned with building facades in images. Detecting building facades in images has been recognized as an important yet challenging task. Without the aid of laser scanning data, our method achieves highly accurate results. Moreover, based on the established link between buildings in images and buildings on maps, attribute information from map data can be readily transfered to the images. For example, we can identify the buildings in an image corresponding to a specific restaurant if the information is given on maps. \n\nTo assess the quality of segmented buildings, we randomly select 20 images and manually segment buildings. Each building has a unique label associated to the corresponding footprint in the map. We calculate an accuracy rate as correctly labeled pixels divided by overall building pixels. Our results reach an accuracy rate of 85.3\\%. Camera position accuracy is particularly important for the segmentation quality. We find using raw positions the segmentation accuracy drops by 9.2\\%. Fig.~\\ref{fig:bldseg} shows building masks from projections using raw and refined camera positions. It can be seen that using the raw position projected buildings severely deviate from the corresponding buildings in images. The mismatch also causes building footprints to match wrong rooflines and produce incorrect heights. \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.95\\textwidth]{figs/fcdRes1.pdf}\n\\end{center}\n\\caption{Example results of projecting building models onto images. Different buildings are represented by distinct colors.}\n\\label{fig:bldfcd}\n\\end{figure*}\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[trim = 0mm 90mm 70mm 30mm, clip, width=0.2\\textwidth]{figs/align3.png}\n\\hspace{.5mm}\n\\includegraphics[trim = 0mm 90mm 70mm 30mm, clip,width=0.2\\textwidth]{figs/align31.png}\n\\end{center}\n \\caption{Building segmentation by projecting building models with raw camera position (left) and refined position (right). }\n\\label{fig:bldseg}\n\\end{figure}\n\nWe implement the method using MATLAB on a 3.2-GHz Intel processor. The current version of the code takes on average 3 seconds to process one image, including camera position estimation and height estimation. The efficiency can be further enhanced by using parallel computing resources because each image is processed independently.  \n\n\\section{Conclusions}\n\\label{sec:Conclusions}\n\nWe have presented a new method to generate simple 3D building models using street level images and map data. The method makes effective use of data acquired from two distinct platforms. 3D models can be generated even from a single image. Due to the wide availability of input data, the method is highly scalable. Our experiments show that the method performs reliably on real world data.\n\nThe proposed method to estimate camera position can work as an add-on for enhancing GPS accuracy. Although in this work we do not measure exact improvements on location accuracy because of the lack of ground truth, we indeed find that even one meter shift of an estimated camera position causes noticeable misalignments between projected building footprints and buildings in images. Worth mentioning is that this method is particularly suited for dense urban neighborhoods, which is often the most challenging situation for acquiring accurate GPS measurements.  \n\nWe find that more accurate roofline detection and building edge detection can further improve results. In future work, we will investigate supervised learning based approaches. Like in \\cite{martin2004}, a building boundary detector learned from labeled data is expected to provide more meaningful boundary maps.  \n\n\n\\bibliographystyle{ieee}\n\\bibliography{iccv15}\n\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \nThat is, given correspondence between a pixel location and a 3D point in world coordinates, the possible camera positions $\\mathbf{C'}$ lie on a 3D line defined by (\\ref{cl}).    \n\nIn this work, we assume that camera position errors are mainly in a horizontal plane. Let $\\mathbf{C'}=(X_C,Y_C,Z_C)^T$. We can discard Z dimension and simplify (\\ref{cl}) to \n\n", "index": 5, "text": "\\begin{equation}\n\\begin{bmatrix}\nX_C \\\\\nY_C\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n+\\lambda\n\\begin{bmatrix}\n\\Delta X \\\\\n\\Delta Y\n\\end{bmatrix},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}X_{C}\\\\&#10;Y_{C}\\end{bmatrix}=\\begin{bmatrix}X\\\\&#10;Y\\end{bmatrix}+\\lambda\\begin{bmatrix}\\Delta X\\\\&#10;\\Delta Y\\end{bmatrix},\" display=\"block\"><mrow><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>X</mi><mi>C</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>Y</mi><mi>C</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>X</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>Y</mi></mtd></mtr></mtable><mo>]</mo></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>X</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>Y</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]