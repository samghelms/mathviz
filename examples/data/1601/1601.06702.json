[{"file": "1601.06702.tex", "nexttext": " \nwhere the measurability of $Q$, which follows from the assumed smoothness properties of $Q$, implies $Q^{-1}(A)\\in\\mathcal{B}_\\Lambda$. \n\nThe corresponding {\\em deterministic} inverse sensitivity analysis problem is to analyze $Q^{-1}(A)$ for some $A\\in\\mathcal{B}_{\\mathcal{D}}$.  \nWhile $Q^{-1}(A)\\in\\mathcal{B}_\\Lambda$, the practical computation of $Q^{-1}(A)$ is complicated by the fact that $Q^{-1}$ does not map individual points in $\\mathcal{D}$ to individual points in $\\Lambda$.\nAs described above, assuming $m<n$, $Q^{-1}$ maps a point in $\\mathcal{D}$ to an $(n-m)$-dimensional manifold embedded in $\\Lambda$.\nThus, $Q^{-1}(A)$ defines a {\\em generalized contour event}, or just contour event, belonging to an {\\em induced contour $\\sigma$-algebra} $\\mathcal{C}_\\Lambda\\subset\\mathcal{B}_\\Lambda$ (where the inclusion is often proper). \nIn other words, solutions to the corresponding inverse problem can be described in the measurable space $(\\Lambda,\\mathcal{C}_\\Lambda)$ and the volume measure $\\mu_\\Lambda$ can be used to provide quantitative assessments of solutions to this inverse sensitivity analysis problem. \n\nAs shown below in describing solutions to the stochastic inverse problem, it is useful to consider solutions to this deterministic inverse sensitivity problem in a space of equivalence classes.\nSpecifically, we may use the generalized contours to define an equivalence class representation of $\\Lambda$ where two points are considered equivalent if they lie on the same generalized contour.  \nWe let $\\mathcal{L}$ denote the space of such equivalence classes and let $\\pi_{\\mathcal{L}}:\\Lambda\\to\\mathcal{L}$ denote the projection map where $\\pi_{\\mathcal{L}}(\\lambda)=\\ell\\in\\mathcal{L}$ defines the equivalence class corresponding to a particular $\\lambda$ and $\\pi_{\\mathcal{L}}^{-1}(\\ell)=C_\\ell$ is the generalized contour in $\\Lambda$ corresponding to the point $\\ell\\in\\mathcal{L}$.\nIt is possible to explicitly represent $\\mathcal{L}$ in $\\Lambda$ by choosing a specific representative element from each equivalence class.\nAs described in \\cite{BE1, BE3}, such a representation of $\\mathcal{L}$ can be constructed by piecewise $m$-dimensional manifolds that {\\em index} the $(n-m)$-dimensional generalized contours.\nFrom a non-technical perspective, this is like defining a particular hiking path using a contour map where each elevation is transversed once and only once, see the middle plot in Figure~\\ref{fig:contour_illustration}. \nWe refer to any such indexing manifold as a {\\em transverse parameterization}.\nGiven a particular indexing manifold representing $\\mathcal{L}$, $Q$ defines a bijection between $\\mathcal{L}$ and $\\mathcal{D}$.\nThe measure space $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}},\\mu_{\\mathcal{L}})$ can be defined as an induced space using the bijection $Q$ and $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}},\\mu_{\\mathcal{D}})$. \nThen, solutions to the deterministic inverse sensitivity analysis problem can be described and analyzed as measurable sets of points in $\\mathcal{L}$ instead of measurable generalized contour events in $\\Lambda$. \nWhile the measure space $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}},\\mu_{\\mathcal{L}})$ is useful in describing both the theoretical solutions and computational algorithms approximating solutions to the stochastic inverse problem defined below, it is not necessary to explicitly construct a transverse parameterization. \n\\begin{figure}[htbp]\n  \\centering\n  \n \\hbox{ \\includegraphics[width=0.33\\textwidth]{illustration1}\n  \\includegraphics[width=0.33\\textwidth]{invprob2b}\n  \\includegraphics[width=0.33\\textwidth]{illustration3}}\n  \\caption{\\it Left: The inverse of $Q^{-1}(Q(\\lambda))$ is often set-valued even when $\\lambda\\in\\Lambda$ specifies a particular $Q(\\lambda)\\in\\mathcal{D}$. Middle: The representation of $\\mathcal{L}$ as a transverse parameterization. Right: A probability measure described as a density on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$ maps uniquely to a probability density on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$. Figures adopted from \\cite{BE3} and \\cite{BE1}.}\\label{fig:contour_illustration}\n\\end{figure}\n\n\\vskip 5pt\n{\\noindent\\it Level 3}\n\nThe third forward problem builds on the second forward problem.\nWe now assume that there is uncertainty, described in terms of probabilities, as to which set $A\\in\\mathcal{B}_\\Lambda$ the model parameters $\\lambda$ belong, and the goal is to analyze the probabilities of sets in $\\mathcal{B}_{\\mathcal{D}}$. \nIn the language of probability theory, measurable sets are referred to as events.  \nThus, we assume that a probability measure $P_{\\Lambda}$ is given on $(\\Lambda,\\mathcal{B}_\\Lambda)$ describing uncertainty in the events for which parameters may belong, and the goal is to determine $P_{\\mathcal{D}}$ on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$.\nWe refer to this as the {\\em stochastic} forward problem. \nWhen $P_\\Lambda$ (resp., $P_\\mathcal{D}$) is absolutely continuous with respect to the volume measure $\\mu_{\\Lambda}$ (resp., $\\mu_{\\mathcal{D}}$), the corresponding Radon-Nikodym derivative (i.e., the probability density function) $\\rho_{\\Lambda}$ (resp., $\\rho_{\\mathcal{D}}$) is usually given in place of the probability measure.\nWe assume this is the case so that we can refer to probabilities of events in terms of the more common representation using integrals and density functions, e.g., \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\title{\\bf Experimental Design : Optimizing Quantities of Interest to Reliably Reduce the Uncertainty in Model Input Parameters}\n\\author{Scott Walsh\\\\{Advisor: Troy Butler}}\n\\date{}\n\\maketitle\n\\tableofcontents\n\n\n\n\n\n\n\n\n\n\n\n\n\\newenvironment{mat}{\\left[\\begin{array}{ccccccccccccccc}}{\\end{array}\\right]}\n\n\n\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}\n\n\n\\newenvironment{choices}{\\left\\{ \\begin{array}{ll}}{\\end{array}\\right.}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\pagebreak\n\n\n\\vskip 10pt\n\\section{\\bf Introduction}\n\\vskip 10pt\n\n\nWith an abundance of computing resources available today mathematical models are capable of simulating extraordinarily complex physical systems.  These complex models in turn induce a set of model input parameters, many of which are subject to uncertainty.  For example, the diffusion coefficient of a model simulating the heating of a thin plate by some external source is subject to uncertainty, possibly due to the manufacturing imperfections of the thin plate.  The Manning's n parameters of a coastal storm surge model may initially be determined by extensive exploration of the coastal boundary, however each passing storm has the potential to significantly alter regions of this coastal boundary introducing uncertainty into these parameters.  Our goal is to reduce the uncertainty in these model input parameters, therefore improving the predictive capabilities of the model.\n\nOne approach to reducing the uncertainty in these model input parameters is to gather quantities of interest (QoI) of the physical system and use this data to inform us about the parameters that may have produced this data.  This data may be a temperature measurement at a point in space time or a maximum water elevation at a given location in the physical domain.  As our model input parameters are uncertain, so is the data we gather.  The gathered data is uncertain for a multitude of reasons, e.g., measurement instruments have finite precision, instrument locations in space time are subject to uncertainty.  This uncertainty in each QoI gathered may also be a function of the region of the parameter space that produced this physical reality.  Different observed solutions (corresponding to different regions of model input parameters) may produce physical processes that are not being modeled that pollute measurements even more.  For example, some regions of a parameter space of a coastal storm surge model may produce large and chaotic waves that pollute measurements.\n\n\nThis uncertainty in the QoI produces a set valued inverse problem.  In \\cite{BE1, BE3} the foundation for a measure theoretic approach to solving this set valued inverse problem is carefully developed.  With this foundation built on well understood mathematical concepts, the uniqueness of such set valued inverse solutions is proved, under suitable conditions.  However, each solution is dependent on the data used in the inverse problem, i.e., different sets of data produce different inverse solutions whose geometric properties may vary substantially.  In this paper we develop an approach for determining an optimal choice of QoI to produce a {\\em precise} and {\\em accurate} inverse solution, i.e., reduce the uncertainty in our model parameters.\n\n\\vskip 10pt\n\nIn Section \\ref{Sec:Three_Levels} we establish the foundation of the measure theoretic framework for solving the inverse problem.  In Section \\ref{Sec:Numerical_Approximation} we consider the numerical algorithm that simultaneously approximates the support of the probability density solving the stochastic inverse problem.  We use simple linear maps to demonstrate the impact the geometry of the inverse density has on the numerical approximation of this support.  In Section \\ref{Sec:Precision_Accuracy} we quantify the two guiding geometric properties of the support, the {\\em $\\mu_\\Lambda$-measure} and the {\\it skewness}, which represent, in a sense, the precision and accuracy, respectively, of the computed inverse density.  In Section \\ref{Sec:Optimizing} we address the need to optimize the minimization of both the $\\mu_\\Lambda$-measure and skewness.  With this multi-criteria optimization problem defined, we use simple linear and nonlinear maps to develop an intuition about solutions to this problem.  In Section \\ref{Sec:Map_Defined}, we demonstrate the impact of determining optimal sets of QoI on solutions to stochastic inverse problems involving physics based models.  \nIn Section \\ref{Sec:Conclusion} we provide concluding remarks and discuss promising future research directions.\n\n\n\\section{\\bf A Measure-Theoretic Stochastic Inverse Problem}\\label{Sec:Three_Levels}\n\n\n\n\\subsection{A Mathematical Formulation}\n\n\nWe provide a taxonomy of three forward problems with increasing levels of uncertainty and the direct inverses of these problems along with a brief summary of the corresponding solutions.\nFor a more thorough description and analysis of these problems and solutions, see \\cite{BE1, BE3}. \nLet $\\Lambda$ denote the set of all uncertain input parameters for some physics-based model and $\\mathcal{D}$ denote the set of possible output data defined as the range of the QoI map, $Q:\\Lambda\\to\\mathcal{D}$, which we assume is piecewise smooth.\n\n\\vskip 5pt\n{\\it\\noindent Level 1}\n\nThe first forward problem is the simplest in predictive science which is to evaluate the map $Q$ for a fixed $\\lambda\\in\\Lambda$ to determine the corresponding output datum $Q(\\lambda)=q\\in\\mathcal{D}$. \nIn other words, once the inputs of a model are specified, the simplest forward problem is to solve the model in order to predict the output datum. \nGenerally, solutions to this problem require developing a computational model to determine approximate solutions to the physics-based model and to then apply a functional to the numerical solution to obtain the value of $q$. \nWith the exception of possible discretization errors in numerically evaluating $Q(\\lambda)$, there is no uncertainty in the forward problem since $Q(\\lambda)$ is well-defined. \n\nThe corresponding inverse problem is to determine the possible parameters $\\lambda\\in\\Lambda$ that produce a particular value of $q\\in\\mathcal{D}$.  \nOftentimes $Q^{-1}(q)$ defines a set of values in $\\Lambda$ either due to non-linearities in the map $Q$ and/or the dimension of $\\Lambda$ being greater than the dimension of $\\mathcal{D}$. \nThus, the simplest inverse problem often has uncertainty as to the particular value of $\\lambda\\in\\Lambda$ that produced a fixed output datum. \nHowever, there is no uncertainty about which set-valued inverse produced the fixed output datum.\nWhen $\\dim(\\Lambda)=2$ and $\\dim(\\mathcal{D})=1$, a set-valued inverse, $Q^{-1}(q)$, defines a contour in $\\Lambda$ that we are familiar with from contour maps, see the left plot in Figure~\\ref{fig:contour_illustration}. \nWe restrict focus to problems where $\\Lambda\\subset{{\\rm l} \\kern -.15em {\\rm R} }^n$, $\\mathcal{D}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^m$, and  $m<n$ (or occasionally $m\\leq n$), which corresponds to the common case when there are more input parameters than observable outputs.\nWe refer to the set-valued inverses of the map $Q$ as {\\em generalized contours}.\nGeneralized contours may be defined by the union of piecewise-defined manifolds in $\\Lambda$, and the local dimension of the manifolds depends upon $m$, $n$, and the rank of the Jacobian of $Q$.\n\\begin{definition}\nThe component maps of the $m$-dimensional piecewise-smooth vector-valued map $Q(\\lambda)$ are {\\bf geometrically distinct (GD)} if the Jacobian of $Q$ has full rank at every point in $\\Lambda$.\nWhen the component maps are GD, we say that $Q$ is GD. \n\\end{definition}\nWhen $m<n$ and $Q$ is GD, the generalized contours exist as piecewise-defined $(n-m)$-dimensional manifolds in $\\Lambda$.\nIn \\cite{BE1}, a method for explicit pointwise approximation of generalized contours for scalar multivariate $Q$ maps is provided.\nIn \\cite{BE3}, this was extended for the more general case of vector-valued multivariate $Q$ maps. \nWhile explicit approximations are required for numerical solutions to this ``simplest'' inverse problem, it is thankfully not necessary for solutions to the stochastic inverse problem which implicitly exploit the geometric structure of the generalized contour map. \n\n\\vskip 5pt\n{\\noindent\\it Level 2}\n\nFollowing solutions to the first forward problem, it is typical to define a second forward problem from the general class of problems that fall under the category of deterministic sensitivity analysis.  \nSome specific deterministic sensitivity analysis problems arise in the context of dynamical systems analysis, where we study how sets of initial conditions vary over time, or in perturbation analysis, where we study how small changes in model inputs affect model outputs.\nThese types of forward problems can often be written mathematically as analyzing $Q(A)\\subset\\mathcal{D}$ given a set $A\\subset\\Lambda$. \nIn other words, there is generally uncertainty as to the precise value $\\lambda$ takes in a set $A\\subset\\Lambda$ and subsequently there is uncertainty as to the particular datum to predict in the set $Q(A)$. \nAnswering such questions often requires, at a minimum, specification of metrics on both $\\Lambda$ and $\\mathcal{D}$ so that we can describe distances between points.\nThus, in the formulation of a forward {\\em deterministic} sensitivity analysis problem, we often must specify metrics on both $\\Lambda$ and $\\mathcal{D}$. \nBy transfinite induction using sets in the metric topologies, we can construct the Borel $\\sigma$-algebras, $\\mathcal{B}_\\Lambda$ and $\\mathcal{B}_{\\mathcal{D}}$, on $\\Lambda$ and $\\mathcal{D}$, respectively.\nThe metrics can be used to define outer measures on the measurable spaces $(\\Lambda,\\mathcal{B}_\\Lambda)$ and $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$.\nThen, Carath\\'{e}odory's theorem can be used to define the Hausdorff measures, which we refer to as the ``volume'' measures on $(\\Lambda,\\mathcal{B}_\\Lambda)$ and $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$, and denote by $\\mu_\\Lambda$ and $\\mu_{\\mathcal{D}}$, respectively. \nNote that if the metrics are norm-induced on $\\Lambda\\subset\\mathbb{R}^n$ and $\\mathcal{D}\\subset\\mathbb{R}^m$, then up to a scaling constant, $\\mu_\\Lambda$ and $\\mu_{\\mathcal{D}}$ are identical to the standard Lebesgue measures on these spaces. \nWe observe that construction of the measure spaces $(\\Lambda,\\mathcal{B}_\\Lambda,\\mu_\\Lambda)$ and $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}},\\mu_{\\mathcal{D}})$ follows directly from the specification of metrics on the sets $\\Lambda$ and $\\mathcal{D}$. \nTherefore, we refer to the specification of metrics as the {\\em minimal assumptions} required to obtain the measure-theoretic framework in which the remaining problems in the taxonomy are formulated. \n\nIt is worth noting that a metric need not be specified on $\\mathcal{D}$ as long as $\\mathcal{D}$ is a topological space.\nIn this case, we can still obtain a Borel $\\sigma$-algebra on $\\mathcal{D}$ as before, and define the volume measure $\\mu_{\\mathcal{D}}$ on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$ in terms of the induced ``push forward'' measure \n", "index": 1, "text": "\\begin{equation}\n    \\mu_{\\mathcal{D}}(A) = \\mu_{\\Lambda}(Q^{-1}(A)), \\ A\\in\\mathcal{B}_{\\mathcal{D}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mu_{\\mathcal{D}}(A)=\\mu_{\\Lambda}(Q^{-1}(A)),\\ A\\in\\mathcal{B}_{\\mathcal{D}},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>A</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nSolution to this stochastic forward problem are given by the induced push-forward probability measure $P_{\\mathcal{D}}$ defined for any $A\\in\\mathcal{B}_{\\mathcal{D}}$ by  \n\n", "itemtype": "equation", "pos": 16689, "prevtext": " \nwhere the measurability of $Q$, which follows from the assumed smoothness properties of $Q$, implies $Q^{-1}(A)\\in\\mathcal{B}_\\Lambda$. \n\nThe corresponding {\\em deterministic} inverse sensitivity analysis problem is to analyze $Q^{-1}(A)$ for some $A\\in\\mathcal{B}_{\\mathcal{D}}$.  \nWhile $Q^{-1}(A)\\in\\mathcal{B}_\\Lambda$, the practical computation of $Q^{-1}(A)$ is complicated by the fact that $Q^{-1}$ does not map individual points in $\\mathcal{D}$ to individual points in $\\Lambda$.\nAs described above, assuming $m<n$, $Q^{-1}$ maps a point in $\\mathcal{D}$ to an $(n-m)$-dimensional manifold embedded in $\\Lambda$.\nThus, $Q^{-1}(A)$ defines a {\\em generalized contour event}, or just contour event, belonging to an {\\em induced contour $\\sigma$-algebra} $\\mathcal{C}_\\Lambda\\subset\\mathcal{B}_\\Lambda$ (where the inclusion is often proper). \nIn other words, solutions to the corresponding inverse problem can be described in the measurable space $(\\Lambda,\\mathcal{C}_\\Lambda)$ and the volume measure $\\mu_\\Lambda$ can be used to provide quantitative assessments of solutions to this inverse sensitivity analysis problem. \n\nAs shown below in describing solutions to the stochastic inverse problem, it is useful to consider solutions to this deterministic inverse sensitivity problem in a space of equivalence classes.\nSpecifically, we may use the generalized contours to define an equivalence class representation of $\\Lambda$ where two points are considered equivalent if they lie on the same generalized contour.  \nWe let $\\mathcal{L}$ denote the space of such equivalence classes and let $\\pi_{\\mathcal{L}}:\\Lambda\\to\\mathcal{L}$ denote the projection map where $\\pi_{\\mathcal{L}}(\\lambda)=\\ell\\in\\mathcal{L}$ defines the equivalence class corresponding to a particular $\\lambda$ and $\\pi_{\\mathcal{L}}^{-1}(\\ell)=C_\\ell$ is the generalized contour in $\\Lambda$ corresponding to the point $\\ell\\in\\mathcal{L}$.\nIt is possible to explicitly represent $\\mathcal{L}$ in $\\Lambda$ by choosing a specific representative element from each equivalence class.\nAs described in \\cite{BE1, BE3}, such a representation of $\\mathcal{L}$ can be constructed by piecewise $m$-dimensional manifolds that {\\em index} the $(n-m)$-dimensional generalized contours.\nFrom a non-technical perspective, this is like defining a particular hiking path using a contour map where each elevation is transversed once and only once, see the middle plot in Figure~\\ref{fig:contour_illustration}. \nWe refer to any such indexing manifold as a {\\em transverse parameterization}.\nGiven a particular indexing manifold representing $\\mathcal{L}$, $Q$ defines a bijection between $\\mathcal{L}$ and $\\mathcal{D}$.\nThe measure space $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}},\\mu_{\\mathcal{L}})$ can be defined as an induced space using the bijection $Q$ and $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}},\\mu_{\\mathcal{D}})$. \nThen, solutions to the deterministic inverse sensitivity analysis problem can be described and analyzed as measurable sets of points in $\\mathcal{L}$ instead of measurable generalized contour events in $\\Lambda$. \nWhile the measure space $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}},\\mu_{\\mathcal{L}})$ is useful in describing both the theoretical solutions and computational algorithms approximating solutions to the stochastic inverse problem defined below, it is not necessary to explicitly construct a transverse parameterization. \n\\begin{figure}[htbp]\n  \\centering\n  \n \\hbox{ \\includegraphics[width=0.33\\textwidth]{illustration1}\n  \\includegraphics[width=0.33\\textwidth]{invprob2b}\n  \\includegraphics[width=0.33\\textwidth]{illustration3}}\n  \\caption{\\it Left: The inverse of $Q^{-1}(Q(\\lambda))$ is often set-valued even when $\\lambda\\in\\Lambda$ specifies a particular $Q(\\lambda)\\in\\mathcal{D}$. Middle: The representation of $\\mathcal{L}$ as a transverse parameterization. Right: A probability measure described as a density on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$ maps uniquely to a probability density on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$. Figures adopted from \\cite{BE3} and \\cite{BE1}.}\\label{fig:contour_illustration}\n\\end{figure}\n\n\\vskip 5pt\n{\\noindent\\it Level 3}\n\nThe third forward problem builds on the second forward problem.\nWe now assume that there is uncertainty, described in terms of probabilities, as to which set $A\\in\\mathcal{B}_\\Lambda$ the model parameters $\\lambda$ belong, and the goal is to analyze the probabilities of sets in $\\mathcal{B}_{\\mathcal{D}}$. \nIn the language of probability theory, measurable sets are referred to as events.  \nThus, we assume that a probability measure $P_{\\Lambda}$ is given on $(\\Lambda,\\mathcal{B}_\\Lambda)$ describing uncertainty in the events for which parameters may belong, and the goal is to determine $P_{\\mathcal{D}}$ on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$.\nWe refer to this as the {\\em stochastic} forward problem. \nWhen $P_\\Lambda$ (resp., $P_\\mathcal{D}$) is absolutely continuous with respect to the volume measure $\\mu_{\\Lambda}$ (resp., $\\mu_{\\mathcal{D}}$), the corresponding Radon-Nikodym derivative (i.e., the probability density function) $\\rho_{\\Lambda}$ (resp., $\\rho_{\\mathcal{D}}$) is usually given in place of the probability measure.\nWe assume this is the case so that we can refer to probabilities of events in terms of the more common representation using integrals and density functions, e.g., \n\n", "index": 3, "text": "\\begin{equation}\n    P_{\\Lambda}(A) = \\int_A \\rho_{\\Lambda}d\\mu_{\\Lambda}, \\ A\\in\\mathcal{B}_\\Lambda.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"P_{\\Lambda}(A)=\\int_{A}\\rho_{\\Lambda}d\\mu_{\\Lambda},\\ A\\in\\mathcal{B}_{\\Lambda}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>A</mi></msub><mrow><msub><mi>\u03c1</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>A</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nThis is a familiar problem in uncertainty quantification.  \nThe approximate solution can be obtained by a classic Monte Carlo method. \n\nThe corresponding inverse problem assumes we are given a probability measure $P_{\\mathcal{D}}$ on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$, and the goal is to determine a probability measure $P_\\Lambda$ on $(\\Lambda,\\mathcal{B}_\\Lambda)$ such that $P_\\Lambda(Q^{-1}(A)) = P_{\\mathcal{D}}(A)$ for all $A\\in\\mathcal{B}_{\\mathcal{D}}$.  \nIn other words, the push-forward measure of a solution to the stochastic inverse problem should be $P_{\\mathcal{D}}$. \nTo determine such a solution to the stochastic inverse problem, we first consider the stochastic inverse problem posed on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$.\n\nSince $Q$ defines a bijection between $\\mathcal{L}$ and $\\mathcal{D}$, there is a unique $P_{\\mathcal{L}}$ on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$ (see the right-hand plot in Figure~\\ref{fig:contour_illustration} and \\cite{BE3}).\nWe can then use the projection map $\\pi_{\\mathcal{L}}$ to prove the following theorem.\n\\begin{theorem}\nThe stochastic inverse problem has a unique solution on $(\\Lambda,\\mathcal{C}_{\\Lambda})$. \n\\end{theorem}\n\nHowever, the goal is to define a probability measure $P_\\Lambda$ on the measurable space $(\\Lambda,\\mathcal{B}_{\\Lambda})$ not on a space involving contour events that are complicated to describe.\nThis requires an application of the Disintegration Theorem, which allows for the rigorous description of conditional probability measures defined on sets of zero $\\mu_\\Lambda$-measure \\cite{Dellacherie_Meyer, BE3, BET}.\n\\begin{theorem}\\label{thm:ProbabililtyDisintegration}\nLet $({\\Lambda}, \\mathcal{B}_{{\\Lambda}})$ be a measurable space. \nAssume that $P_{{\\Lambda}}$ is a probability measure on $({\\Lambda}, \\mathcal{B}_{{\\Lambda}})$. \nThere exists a family of conditional probability measures $\\left\\{{P_{\\ell}}\\right\\}$ on $\\left\\{{(C_{\\ell},\\mathcal{B}_{C_\\ell})}\\right\\}$ giving the disintegration,\n\n", "itemtype": "equation", "pos": 16979, "prevtext": "\nSolution to this stochastic forward problem are given by the induced push-forward probability measure $P_{\\mathcal{D}}$ defined for any $A\\in\\mathcal{B}_{\\mathcal{D}}$ by  \n\n", "index": 5, "text": "\\begin{equation}\n    P_{\\mathcal{D}}(A) = \\int_A\\rho_{\\mathcal{D}}d\\mu_{\\mathcal{D}} = \\int_{Q^{-1}(A)}\\rho_{\\Lambda}d\\mu_{\\Lambda} = P_{\\Lambda}(Q^{-1}(A)).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"P_{\\mathcal{D}}(A)=\\int_{A}\\rho_{\\mathcal{D}}d\\mu_{\\mathcal{D}}=\\int_{Q^{-1}(A%&#10;)}\\rho_{\\Lambda}d\\mu_{\\Lambda}=P_{\\Lambda}(Q^{-1}(A)).\" display=\"block\"><mrow><mrow><mrow><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>A</mi></msub><mrow><msub><mi>\u03c1</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub></mrow></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mrow><msub><mi>\u03c1</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\end{theorem}\n\nThus, any probability measure on $(\\Lambda,\\mathcal{B}_{\\Lambda})$ can be decomposed into a form involving a probability measure on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$ uniquely defined by $P_{\\mathcal{D}}$ and probability measures on each measurable generalized contour space $(C_{\\ell},\\mathcal{B}_{C_{\\ell}})$ defined by the conditional probabilities $P_{\\ell}$.\nThe conditional probability measures on $\\left\\{{(C_{\\ell},\\mathcal{B}_{C_{\\ell}})}\\right\\}$ can not be determined by observations of $Q(\\lambda)\\in\\mathcal{D}$. \nWe follow \\cite{BE3, BET} and adopt what is referred to as the {\\em standard Ansatz} determined by the disintegration of the volume measure $\\mu_\\Lambda$ to compute probabilities of events inside of a contour event. The standard Ansatz is given by\n\n", "itemtype": "equation", "pos": 19156, "prevtext": "\nThis is a familiar problem in uncertainty quantification.  \nThe approximate solution can be obtained by a classic Monte Carlo method. \n\nThe corresponding inverse problem assumes we are given a probability measure $P_{\\mathcal{D}}$ on $(\\mathcal{D},\\mathcal{B}_{\\mathcal{D}})$, and the goal is to determine a probability measure $P_\\Lambda$ on $(\\Lambda,\\mathcal{B}_\\Lambda)$ such that $P_\\Lambda(Q^{-1}(A)) = P_{\\mathcal{D}}(A)$ for all $A\\in\\mathcal{B}_{\\mathcal{D}}$.  \nIn other words, the push-forward measure of a solution to the stochastic inverse problem should be $P_{\\mathcal{D}}$. \nTo determine such a solution to the stochastic inverse problem, we first consider the stochastic inverse problem posed on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$.\n\nSince $Q$ defines a bijection between $\\mathcal{L}$ and $\\mathcal{D}$, there is a unique $P_{\\mathcal{L}}$ on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$ (see the right-hand plot in Figure~\\ref{fig:contour_illustration} and \\cite{BE3}).\nWe can then use the projection map $\\pi_{\\mathcal{L}}$ to prove the following theorem.\n\\begin{theorem}\nThe stochastic inverse problem has a unique solution on $(\\Lambda,\\mathcal{C}_{\\Lambda})$. \n\\end{theorem}\n\nHowever, the goal is to define a probability measure $P_\\Lambda$ on the measurable space $(\\Lambda,\\mathcal{B}_{\\Lambda})$ not on a space involving contour events that are complicated to describe.\nThis requires an application of the Disintegration Theorem, which allows for the rigorous description of conditional probability measures defined on sets of zero $\\mu_\\Lambda$-measure \\cite{Dellacherie_Meyer, BE3, BET}.\n\\begin{theorem}\\label{thm:ProbabililtyDisintegration}\nLet $({\\Lambda}, \\mathcal{B}_{{\\Lambda}})$ be a measurable space. \nAssume that $P_{{\\Lambda}}$ is a probability measure on $({\\Lambda}, \\mathcal{B}_{{\\Lambda}})$. \nThere exists a family of conditional probability measures $\\left\\{{P_{\\ell}}\\right\\}$ on $\\left\\{{(C_{\\ell},\\mathcal{B}_{C_\\ell})}\\right\\}$ giving the disintegration,\n\n", "index": 7, "text": "\\begin{equation}\\label{eq:finaldisintegration}\n\tP_{{\\Lambda}}(A)  = \\int_{\\pi_{\\mathcal{L}}(A)} \\bigg( \\int_{\\pi^{-1}_{\\mathcal{L}}(\\ell) \\cap A} \\, dP_{\\ell}(\\lambda) \\bigg) dP_{\\mathcal{L}}(\\ell), \\ \\forall A\\in\\mathcal{B}_{\\Lambda}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"P_{{\\Lambda}}(A)=\\int_{\\pi_{\\mathcal{L}}(A)}\\bigg{(}\\int_{\\pi^{-1}_{\\mathcal{L%&#10;}}(\\ell)\\cap A}\\,dP_{\\ell}(\\lambda)\\bigg{)}dP_{\\mathcal{L}}(\\ell),\\ \\forall A%&#10;\\in\\mathcal{B}_{\\Lambda}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>\u03c0</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mrow><mpadded width=\"+1.7pt\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mrow><msubsup><mi>\u03c0</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2229</mo><mi>A</mi></mrow></msub></mpadded><mrow><mrow><mo>\ud835\udc51</mo><msub><mi>P</mi><mi mathvariant=\"normal\">\u2113</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>P</mi><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>A</mi></mrow><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $\\mu_{C_\\ell}$ is the disintegrated volume measure on the generalized contour $C_\\ell$. \n\nNote that the standard Ansatz can be used as long as $\\mu_{\\Lambda}(\\Lambda)<\\infty$.\nThe approximation method and resulting non-intrusive computational algorithm can be easily modified for any Ansatz.\nSee \\cite{BE3} for more details and theory regarding general choices of the Ansatz.  \nCombining an Ansatz with the Disintegration Theorem proves the following theorem.\n\\begin{theorem}\\label{thm:Lambda_inverse}\nUnder the Ansatz, the stochastic inverse problem has a unique solution on $(\\Lambda,\\mathcal{B}_{\\Lambda})$. \n\\end{theorem}\n\nThe standard Ansatz results in a probability measure $P_\\Lambda$ that inherits key geometric features from the generalized contour map.\nThe only assumption necessary for employing the standard Ansatz is that a finite volume measure exists, which actually means we only assume that a metric exists as described above and that the diameter of $\\Lambda$ is bounded.\nAny other choice of Ansatz imposes some other geometric constraints on the solution to the inverse problem that are not coming from the map $Q$.\nIf we choose not to use any Ansatz, then we can always solve the stochastic inverse problem on $(\\Lambda,\\mathcal{C}_\\Lambda)$, i.e., we can always compute the unique probability measure when restricted to contour events.\n\n\n\\subsection{Numerical Approximation of Solutions}\\label{Sec:Numerical_Approximation}\n\n\nFundamental to approximating solutions $P_\\Lambda$ to the stochastic inverse problem is the approximation of events in the various $\\sigma$-algebras, $\\mathcal{B}_{\\mathcal{D}}$, $\\mathcal{C}_\\Lambda$ and $\\mathcal{B}_\\Lambda$. \nSince $\\mathcal{C}_\\Lambda\\subset\\mathcal{B}_\\Lambda$, we can simultaneously approximate events in both of these $\\sigma$-algebras using the same set of events partitioning $\\Lambda$.\nLet $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ denote such a partition of $\\Lambda$ where $\\mathcal{V}_i\\in\\mathcal{B}_\\Lambda$ for each $i$. \nAssume that we are given a collection of sets $\\left\\{{D_k}\\right\\}_{k=1}^M\\subset\\mathcal{B}_{\\mathcal{D}}$ partitioning $\\mathcal{D}$.\nThe basic algorithmic procedure for approximating $P_\\Lambda(\\mathcal{V}_i)$ for each $i$ is to determine which of the $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$s approximate $Q^{-1}(D_k)$, and then to apply the Ansatz on this approximation of the contour event which has known probability $P_{\\mathcal{D}}(D_k)$.\nLetting $p_{\\Lambda,i}$ denote this approximation, we can then define an approximation to $P_{\\Lambda}$ as\n", "itemtype": "equation", "pos": 20204, "prevtext": "\n\\end{theorem}\n\nThus, any probability measure on $(\\Lambda,\\mathcal{B}_{\\Lambda})$ can be decomposed into a form involving a probability measure on $(\\mathcal{L},\\mathcal{B}_{\\mathcal{L}})$ uniquely defined by $P_{\\mathcal{D}}$ and probability measures on each measurable generalized contour space $(C_{\\ell},\\mathcal{B}_{C_{\\ell}})$ defined by the conditional probabilities $P_{\\ell}$.\nThe conditional probability measures on $\\left\\{{(C_{\\ell},\\mathcal{B}_{C_{\\ell}})}\\right\\}$ can not be determined by observations of $Q(\\lambda)\\in\\mathcal{D}$. \nWe follow \\cite{BE3, BET} and adopt what is referred to as the {\\em standard Ansatz} determined by the disintegration of the volume measure $\\mu_\\Lambda$ to compute probabilities of events inside of a contour event. The standard Ansatz is given by\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:StandardAnsatz}\n\tP_{\\ell} = \\mu_{C_{\\ell}}/\\mu_{C_{\\ell}}(C_{\\ell}), \\ \\forall\\ell\\in\\mathcal{L},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"P_{\\ell}=\\mu_{C_{\\ell}}/\\mu_{C_{\\ell}}(C_{\\ell}),\\ \\forall\\ell\\in\\mathcal{L},\" display=\"block\"><mrow><mrow><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo>=</mo><mrow><mrow><msub><mi>\u03bc</mi><msub><mi>C</mi><mi mathvariant=\"normal\">\u2113</mi></msub></msub><mo>/</mo><msub><mi>\u03bc</mi><msub><mi>C</mi><mi mathvariant=\"normal\">\u2113</mi></msub></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nWe summarize this basic procedure for approximating $P_\\Lambda$ in Algorithm~\\ref{Alg:1}.\n\n\\begin{algorithm}\n\t\\begin{enumerate}[1.]\n\t\t\\item Let $\\{\\mathcal{V}_i\\}_{i=1}^N\\subset\\mathcal{B}_\\Lambda$ partition $\\Lambda$.\n\t\t\\item Determine a nominal value $Q_i$ for the map $Q(\\lambda)$ on $\\mathcal{V}_i$ for $i=1,..,N$.\n\t\t\\item Choose a partitioning of $\\mathcal{D}$, $\\{ D_k\\}_{k=1}^M \\subset \\mathcal{D}$.\n\t\t\\item Compute $p_{\\mathcal{D},k} =P_{\\mathcal{D}}(D_k)$ for $k=1,...,M$.\n\t\t\\item Let $\\mathcal{C}_k = \\{i | Q_i \\in D_k \\}$ for  $k=1,...,M$.\n\t\t\\item Let $\\mathcal{O}_i = \\{k | Q_i \\in D_k \\}$ for $i=1,..,N$.\n\t\t\\item Compute $V_i= \\mu_\\Lambda(\\mathcal{V}_i)$ for $i=1,..,N$.\n\t\t\\item Set $p_{\\Lambda, i} = (V_{i}/ \\sum_{j \\in \\mathcal{C}_{\\mathcal{O}_i}} V_j) p_{\\mathcal{D}, \\mathcal{O}_i}$ for $i=1,..,N$.\n\t\\end{enumerate}\n\t\\caption{Numerical Approximation of Inverse Probability Measure}\\label{Alg:1}\n\\end{algorithm}\n\nIn Algorithm~\\ref{Alg:1}, $\\mathcal{C}_k$ is used to determine which sets from $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ approximate the contour event $Q^{-1}(D_k)$ for each $k$.\nSimilarly, $\\mathcal{O}_i$ is used to determine which contour event $Q^{-1}(D_k)$ is associated to $\\mathcal{V}_i$ for each $i$.\nThe Ansatz is applied in the final step where the probability of each $\\mathcal{V}_i$ is determined by the probability $P_{\\mathcal{D}}(D_{\\mathcal{O}_i})$ multiplied by the ratio of the volume of $\\mathcal{V}_i$ to the volume of the approximate contour event of $Q^{-1}(D_{\\mathcal{O}_i})$.\n\nThe ability to accurately approximate contour events by subsets of $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ is critical in computing accurate approximations of $P_{\\Lambda}$.\nIn other words, the ability to numerically approximate solutions to the deterministic inverse sensitivity problem determines the numerical accuracy in the stochastic inverse problem. \nRefining the partition $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ generally means additional solutions of the model are required in order to compute the nominal value $Q_i$ required in Step 2 of Algorithm~\\ref{Alg:1}.\nA typical goal is to determine the minimal number of sets partitioning $\\Lambda$ such that $P_\\Lambda$ can be approximated for a fixed discretization of $P_{\\mathcal{D}}$. \nSuch ideas and goals were first explored in \\cite{Butler2015b} where the concept of {\\em skewness} was defined, which described the computational complexity in solving the inverse problem entirely in terms of the number of sets partitioning $\\Lambda$ required for accurate approximation of $P_\\Lambda$. \nWhile there are other approximation issues that arise from numerical evaluation of the map $Q$ and approximation of measures by densities on partitions of the various spaces, these have been addressed elsewhere, e.g., see \\cite{BE2, BET}, and are only exacerbated by the fundamental problem of approximating contour events. \nTo illustrate the geometric concept of skewness and the affect on the number of samples $N$ needed in Algorithm~\\ref{Alg:1} to accurately approximate the inverse image, we use the simple example below, where for simplicity, we assume $n=m$. \n\nSuppose we are given two different QoI maps $Q^{(a)} = (Q_1^{(a)},Q_2^{(a)}) : \\Lambda=[0, 1]^2\\rightarrow\\mathcal{D}^{(a)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^2$ and $Q^{(b)} = (Q_1^{(b)},Q_2^{(b)}) : \\Lambda=[0, 1]^2\\rightarrow\\mathcal{D}^{(b)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^2$ defined by\n\\begin{eqnarray}\n    Q_1^{(a)}(\\lambda_1, \\lambda_2) &=& \\lambda_1, \\\\\n    Q_2^{(a)}(\\lambda_1, \\lambda_2) &=& \\lambda_2, \\\\\n    Q_1^{(b)}(\\lambda_1, \\lambda_2) &=& \\lambda_1 + \\lambda_2, \\\\\n    Q_2^{(b)}(\\lambda_1, \\lambda_2) &=& 0.74\\lambda_1 + 1.26\\lambda_2.\n\\end{eqnarray}\nLet $\\mathcal{D}^{(a)}$ and $\\mathcal{D}^{(b)}$ be partitioned by rectangles $\\left\\{{D_k^{(a)}}\\right\\}_{k=1}^{M_a}$ and $\\left\\{{D_l^{(b)}}\\right\\}_{l=1}^{M_b}$, respectively.\nIn Figure~\\ref{Fig:voronoi_symmdiff}, we show the affect of skewness by considering $(Q^{(a)})^{-1}(D_k^{(a)})$ and $(Q^{(b)})^{-1}(D_l^{(b)})$ for a fixed $k$ and $l$, the error in the Voronoi approximation of this contour event which is given by the symmetric difference (shown by the shaded regions of green), and how this error decreases with increased $N$.  In Figure~\\ref{Fig:loglog_symmdiff} and Table~\\ref{Table:symmdiff}, we show the convergence of the measure of the symmetric difference to be near the known convergence rate of MC methods, $N^{-1/2}$.  Although numerical approximations of the inverse densities converge at the same rate for both $Q^{(a)}$ and $Q^{(b)}$, for a given $N$ the error for the well conditioned map ($Q^{(a)}$) is about half the error of the poorly conditioned (or highly skewed) map ($Q^{(b)}$).  We describe how to quantify the skewness of a given map $Q$ in Section~\\ref{Sec:Skewness}.\n\n\\begin{figure}\n    \\begin{center}\n        \\textbf{$Q^{(a)}$ \\hskip 155pt $Q^{(b)}$}\\par\\medskip\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_50samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_50samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_200samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_200samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_800samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_800samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_3200samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_3200samples_small}\n        \\caption{\\it We plot the implicit discretization of $\\Lambda$ $\\{\\mathcal{V}_i\\}_{i=1}^N$ (given by the uniform random samples), the exact inverse density (given by the intersection of the red and blue contour events) of $D^{(a)}_k$ and $D^{(b)}_l$ for fixed $k$ and $l$, and the symmetric difference of the exact inverese image with the approximated inverse image (seen in green).  We show this for both $Q^{(a)}$ (left) and $Q^{(b)}$ (right).  This is shown for 50 samples (top), 200 samples (second from the top), 800 samples (second from the bottom), and 3200 samples (bottom).}\\label{Fig:voronoi_symmdiff}\n    \\end{center}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{.4\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n\t\\includegraphics[width=\\textwidth]{loglog_symmdiff}\n\t\\caption{\\it Loglog convergence plot for the mean symmetric difference shown in Table~\\ref{Table:symmdiff}.}\\label{Fig:loglog_symmdiff}\n\t\\end{minipage}\n\t\\hskip 20pt\n\t\\begin{minipage}[t]{.4\\textwidth}\n\t\\centering\n\t\\vspace{14pt}\n\t\\begin{tabular}{ccc}\n  \t\tNum. Samples & $Q^{(a)}$ & $Q^{(b)}$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$50$ &  $4.66E-2$ & $8.01E-2$ \\\\\n\t\t\\vspace{2pt}\n\t\t$200$ & $2.37E-2$ & $4.45E-2$ \\\\\n\t\t\\vspace{2pt}\n\t\t$800$ & $1.22E-2$  & $2.33E-2$ \\\\\n\n\t\t$3200$ & $6.13E-3$  & $1.22E-2$ \\\\\n\t\\end{tabular}\n\t\\vspace{40pt}\n\t\\captionof{table}{\\it Mean of the measure of the symmetric difference for 100 sets of N random samples.}\n\t\\label{Table:symmdiff}\n\\end{minipage}\n\\end{figure}\n\n\n\n\\vskip 10pt\n\\section{\\bf Precision and Accuracy}\\label{Sec:Precision_Accuracy}\n\\vskip 10pt\n\n\nFollowing solution to the stochastic inverse problem, we generally want to make, and quantify uncertainty in, predictions that can be formulated as solutions to a stochastic forward problem, e.g., computing a confidence interval on storm surge levels of a hurricane forecast using an ensemble of predictions determined from sampling model input parameters. \nIn some cases, we may be able to design the observation network defining the QoI map used in the stochastic inverse problem, e.g., by determining where and when to deploy sensors such as buoys in the Gulf of Mexico.\nThe problem of defining the ``best'' observation network is generally referred to as optimal experimental design.   \nBelow, we motivate both the definitions we use to define what is an optimal QoI map as well as the development of criteria for choosing the optimal QoI map. \nOur starting point is the assumption that we solve the stochastic inverse problem in order to {\\em quantifiably improve} the predictive capabilities of the model. \n\nSuppose two different QoI maps, $Q^{(a)}$ and $Q^{(b)}$, define two stochastic inverse problems with solutions represented by the two densities $\\rho_\\Lambda^{(a)}$ and $\\rho_\\Lambda^{(b)}$ such that \n", "itemtype": "equation", "pos": 22904, "prevtext": "\nwhere $\\mu_{C_\\ell}$ is the disintegrated volume measure on the generalized contour $C_\\ell$. \n\nNote that the standard Ansatz can be used as long as $\\mu_{\\Lambda}(\\Lambda)<\\infty$.\nThe approximation method and resulting non-intrusive computational algorithm can be easily modified for any Ansatz.\nSee \\cite{BE3} for more details and theory regarding general choices of the Ansatz.  \nCombining an Ansatz with the Disintegration Theorem proves the following theorem.\n\\begin{theorem}\\label{thm:Lambda_inverse}\nUnder the Ansatz, the stochastic inverse problem has a unique solution on $(\\Lambda,\\mathcal{B}_{\\Lambda})$. \n\\end{theorem}\n\nThe standard Ansatz results in a probability measure $P_\\Lambda$ that inherits key geometric features from the generalized contour map.\nThe only assumption necessary for employing the standard Ansatz is that a finite volume measure exists, which actually means we only assume that a metric exists as described above and that the diameter of $\\Lambda$ is bounded.\nAny other choice of Ansatz imposes some other geometric constraints on the solution to the inverse problem that are not coming from the map $Q$.\nIf we choose not to use any Ansatz, then we can always solve the stochastic inverse problem on $(\\Lambda,\\mathcal{C}_\\Lambda)$, i.e., we can always compute the unique probability measure when restricted to contour events.\n\n\n\\subsection{Numerical Approximation of Solutions}\\label{Sec:Numerical_Approximation}\n\n\nFundamental to approximating solutions $P_\\Lambda$ to the stochastic inverse problem is the approximation of events in the various $\\sigma$-algebras, $\\mathcal{B}_{\\mathcal{D}}$, $\\mathcal{C}_\\Lambda$ and $\\mathcal{B}_\\Lambda$. \nSince $\\mathcal{C}_\\Lambda\\subset\\mathcal{B}_\\Lambda$, we can simultaneously approximate events in both of these $\\sigma$-algebras using the same set of events partitioning $\\Lambda$.\nLet $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ denote such a partition of $\\Lambda$ where $\\mathcal{V}_i\\in\\mathcal{B}_\\Lambda$ for each $i$. \nAssume that we are given a collection of sets $\\left\\{{D_k}\\right\\}_{k=1}^M\\subset\\mathcal{B}_{\\mathcal{D}}$ partitioning $\\mathcal{D}$.\nThe basic algorithmic procedure for approximating $P_\\Lambda(\\mathcal{V}_i)$ for each $i$ is to determine which of the $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$s approximate $Q^{-1}(D_k)$, and then to apply the Ansatz on this approximation of the contour event which has known probability $P_{\\mathcal{D}}(D_k)$.\nLetting $p_{\\Lambda,i}$ denote this approximation, we can then define an approximation to $P_{\\Lambda}$ as\n", "index": 11, "text": "\n\\[\n\tP_\\Lambda(A) \\approx P_{\\Lambda,N}(A) = \\sum_{i=1}^N p_{\\Lambda,i}\\chi_{\\mathcal{V}_i}(A), \\ A\\in\\mathcal{B}_\\Lambda.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"P_{\\Lambda}(A)\\approx P_{\\Lambda,N}(A)=\\sum_{i=1}^{N}p_{\\Lambda,i}\\chi_{%&#10;\\mathcal{V}_{i}}(A),\\ A\\in\\mathcal{B}_{\\Lambda}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><msub><mi>P</mi><mrow><mi mathvariant=\"normal\">\u039b</mi><mo>,</mo><mi>N</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>p</mi><mrow><mi mathvariant=\"normal\">\u039b</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msub><mi>\u03c7</mi><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb1</mi><mi>i</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>A</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": " \nFor simplicity, assume that $\\Lambda\\subset\\mathbb{R}^n$, $\\mu_\\Lambda$ is the Lebesgue measure, and the supports of the densities are convex subsets. \nLet $Q^{(p)}$ denote a prediction QoI that is a linear map on $\\Lambda$, and let $\\mathcal{D}=Q^{(p)}(\\Lambda)$ denote the space of possible outcomes for the predictions.\nThen, $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)}))$ and $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$ define the events of all probable predictions based on solutions to the separate stochastic inverse problems, and $\\mu_{\\mathcal{D}}(Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)})) \\ll \\mu_{\\mathcal{D}}(Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$.\nConsequently, we generally expect that statistical inferences drawn from $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)}))$ (e.g., the mean of the predictions) to have greater precision in terms of reduced variances, smaller confidence intervals, etc., compared to those drawn from $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$.\n\nThis motivates a general measure-theoretic goal for designing experiments where the goal is to determine an observation network defining the map $Q$ such that solution to the stochastic inverse problem results in a significant amount of probability contained in events of small $\\mu_\\Lambda$-measure. \nIt is clear from Eq.~(\\ref{eq:finaldisintegration}), the Ansatz, and Algorithm~\\ref{Alg:1}, that the $\\mu_\\Lambda$-measures of induced contour events play a pivotal role in determining an optimal map $Q$. \nIn Section~\\ref{Sec:Support}, we describe an efficient computational approach for quantifying the $\\mu_\\Lambda$-measures of induced contour events for nonlinear maps $Q$.\nGiven multiple choices for the QoI map, we may use such results to determine the so-called optimal map $Q$.\nHowever, given finite computational resources, it may not be possible to accurately approximate solutions of the stochastic inverse problem using the optimal map $Q$ if only a relatively small number of model solves are allowed as described in Section~\\ref{Sec:Numerical_Approximation}.\nA computational method for quantifying the global affect of skewness from a nonlinear map $Q$ is described in Section~\\ref{Sec:Skewness}. \nIn Section~\\ref{Sec:Optimizing}, we describe how to optimize the choice of $Q$ to take into account the separate goals of obtaining {\\em precise} predictions from stochastic inverse problems that can be solved {\\em accurately} with relatively few model evaluations. \n\n\n\\subsection{$\\mu_\\Lambda$-measure of support}\\label{Sec:Support}\n\n\nSuppose in an experiment we can obtain a total of $m$ QoI.  Let $B\\in\\mathcal{B}_{\\mathcal{D}}$ define a typical output event from a partition of $\\mathcal{D}$ used in Algorithm~\\ref{Alg:1}.  If $P_{\\mathcal{D}}(B)\\approx 1$, then as described above, the goal is to quantify the expected $\\mu_\\Lambda$-measure of the support of $Q^{-1}(B)$.  Since we do not know $P_{\\mathcal{D}}$ a priori, we want to choose a $B$ that is representative of typical tessellations used to discretize any probability measure and also geometrically easy to describe such that $\\mu_\\Lambda(Q^{-1}(B))$ is computationally inexpensive to approximate.\nIn measure theory, it is common for $\\sigma$-algebras on higher dimensional spaces to be generated from generalized rectangles.\nWe therefore consider $B$ to be a generalized rectangle in $\\mathcal{D}\\subset\\mathbb{R}^m$. \nSuppose $Q$ is a linear GD map and $n=m$, i.e., $Q$ is defined by the square invertible matrix $J$, $Q(\\lambda)=J\\lambda$.\nIn this case, if $\\Lambda=\\mathbb{R}^n$, we have that\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nWe summarize this basic procedure for approximating $P_\\Lambda$ in Algorithm~\\ref{Alg:1}.\n\n\\begin{algorithm}\n\t\\begin{enumerate}[1.]\n\t\t\\item Let $\\{\\mathcal{V}_i\\}_{i=1}^N\\subset\\mathcal{B}_\\Lambda$ partition $\\Lambda$.\n\t\t\\item Determine a nominal value $Q_i$ for the map $Q(\\lambda)$ on $\\mathcal{V}_i$ for $i=1,..,N$.\n\t\t\\item Choose a partitioning of $\\mathcal{D}$, $\\{ D_k\\}_{k=1}^M \\subset \\mathcal{D}$.\n\t\t\\item Compute $p_{\\mathcal{D},k} =P_{\\mathcal{D}}(D_k)$ for $k=1,...,M$.\n\t\t\\item Let $\\mathcal{C}_k = \\{i | Q_i \\in D_k \\}$ for  $k=1,...,M$.\n\t\t\\item Let $\\mathcal{O}_i = \\{k | Q_i \\in D_k \\}$ for $i=1,..,N$.\n\t\t\\item Compute $V_i= \\mu_\\Lambda(\\mathcal{V}_i)$ for $i=1,..,N$.\n\t\t\\item Set $p_{\\Lambda, i} = (V_{i}/ \\sum_{j \\in \\mathcal{C}_{\\mathcal{O}_i}} V_j) p_{\\mathcal{D}, \\mathcal{O}_i}$ for $i=1,..,N$.\n\t\\end{enumerate}\n\t\\caption{Numerical Approximation of Inverse Probability Measure}\\label{Alg:1}\n\\end{algorithm}\n\nIn Algorithm~\\ref{Alg:1}, $\\mathcal{C}_k$ is used to determine which sets from $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ approximate the contour event $Q^{-1}(D_k)$ for each $k$.\nSimilarly, $\\mathcal{O}_i$ is used to determine which contour event $Q^{-1}(D_k)$ is associated to $\\mathcal{V}_i$ for each $i$.\nThe Ansatz is applied in the final step where the probability of each $\\mathcal{V}_i$ is determined by the probability $P_{\\mathcal{D}}(D_{\\mathcal{O}_i})$ multiplied by the ratio of the volume of $\\mathcal{V}_i$ to the volume of the approximate contour event of $Q^{-1}(D_{\\mathcal{O}_i})$.\n\nThe ability to accurately approximate contour events by subsets of $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ is critical in computing accurate approximations of $P_{\\Lambda}$.\nIn other words, the ability to numerically approximate solutions to the deterministic inverse sensitivity problem determines the numerical accuracy in the stochastic inverse problem. \nRefining the partition $\\left\\{{\\mathcal{V}_i}\\right\\}_{i=1}^N$ generally means additional solutions of the model are required in order to compute the nominal value $Q_i$ required in Step 2 of Algorithm~\\ref{Alg:1}.\nA typical goal is to determine the minimal number of sets partitioning $\\Lambda$ such that $P_\\Lambda$ can be approximated for a fixed discretization of $P_{\\mathcal{D}}$. \nSuch ideas and goals were first explored in \\cite{Butler2015b} where the concept of {\\em skewness} was defined, which described the computational complexity in solving the inverse problem entirely in terms of the number of sets partitioning $\\Lambda$ required for accurate approximation of $P_\\Lambda$. \nWhile there are other approximation issues that arise from numerical evaluation of the map $Q$ and approximation of measures by densities on partitions of the various spaces, these have been addressed elsewhere, e.g., see \\cite{BE2, BET}, and are only exacerbated by the fundamental problem of approximating contour events. \nTo illustrate the geometric concept of skewness and the affect on the number of samples $N$ needed in Algorithm~\\ref{Alg:1} to accurately approximate the inverse image, we use the simple example below, where for simplicity, we assume $n=m$. \n\nSuppose we are given two different QoI maps $Q^{(a)} = (Q_1^{(a)},Q_2^{(a)}) : \\Lambda=[0, 1]^2\\rightarrow\\mathcal{D}^{(a)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^2$ and $Q^{(b)} = (Q_1^{(b)},Q_2^{(b)}) : \\Lambda=[0, 1]^2\\rightarrow\\mathcal{D}^{(b)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^2$ defined by\n\\begin{eqnarray}\n    Q_1^{(a)}(\\lambda_1, \\lambda_2) &=& \\lambda_1, \\\\\n    Q_2^{(a)}(\\lambda_1, \\lambda_2) &=& \\lambda_2, \\\\\n    Q_1^{(b)}(\\lambda_1, \\lambda_2) &=& \\lambda_1 + \\lambda_2, \\\\\n    Q_2^{(b)}(\\lambda_1, \\lambda_2) &=& 0.74\\lambda_1 + 1.26\\lambda_2.\n\\end{eqnarray}\nLet $\\mathcal{D}^{(a)}$ and $\\mathcal{D}^{(b)}$ be partitioned by rectangles $\\left\\{{D_k^{(a)}}\\right\\}_{k=1}^{M_a}$ and $\\left\\{{D_l^{(b)}}\\right\\}_{l=1}^{M_b}$, respectively.\nIn Figure~\\ref{Fig:voronoi_symmdiff}, we show the affect of skewness by considering $(Q^{(a)})^{-1}(D_k^{(a)})$ and $(Q^{(b)})^{-1}(D_l^{(b)})$ for a fixed $k$ and $l$, the error in the Voronoi approximation of this contour event which is given by the symmetric difference (shown by the shaded regions of green), and how this error decreases with increased $N$.  In Figure~\\ref{Fig:loglog_symmdiff} and Table~\\ref{Table:symmdiff}, we show the convergence of the measure of the symmetric difference to be near the known convergence rate of MC methods, $N^{-1/2}$.  Although numerical approximations of the inverse densities converge at the same rate for both $Q^{(a)}$ and $Q^{(b)}$, for a given $N$ the error for the well conditioned map ($Q^{(a)}$) is about half the error of the poorly conditioned (or highly skewed) map ($Q^{(b)}$).  We describe how to quantify the skewness of a given map $Q$ in Section~\\ref{Sec:Skewness}.\n\n\\begin{figure}\n    \\begin{center}\n        \\textbf{$Q^{(a)}$ \\hskip 155pt $Q^{(b)}$}\\par\\medskip\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_50samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_50samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_200samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_200samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_800samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_800samples_small}\\\\\n        \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_orthog_3200samples_small}\n            \\includegraphics[width=.38\\textwidth]{voronoi_symmdiff_skew_3200samples_small}\n        \\caption{\\it We plot the implicit discretization of $\\Lambda$ $\\{\\mathcal{V}_i\\}_{i=1}^N$ (given by the uniform random samples), the exact inverse density (given by the intersection of the red and blue contour events) of $D^{(a)}_k$ and $D^{(b)}_l$ for fixed $k$ and $l$, and the symmetric difference of the exact inverese image with the approximated inverse image (seen in green).  We show this for both $Q^{(a)}$ (left) and $Q^{(b)}$ (right).  This is shown for 50 samples (top), 200 samples (second from the top), 800 samples (second from the bottom), and 3200 samples (bottom).}\\label{Fig:voronoi_symmdiff}\n    \\end{center}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{.4\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n\t\\includegraphics[width=\\textwidth]{loglog_symmdiff}\n\t\\caption{\\it Loglog convergence plot for the mean symmetric difference shown in Table~\\ref{Table:symmdiff}.}\\label{Fig:loglog_symmdiff}\n\t\\end{minipage}\n\t\\hskip 20pt\n\t\\begin{minipage}[t]{.4\\textwidth}\n\t\\centering\n\t\\vspace{14pt}\n\t\\begin{tabular}{ccc}\n  \t\tNum. Samples & $Q^{(a)}$ & $Q^{(b)}$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$50$ &  $4.66E-2$ & $8.01E-2$ \\\\\n\t\t\\vspace{2pt}\n\t\t$200$ & $2.37E-2$ & $4.45E-2$ \\\\\n\t\t\\vspace{2pt}\n\t\t$800$ & $1.22E-2$  & $2.33E-2$ \\\\\n\n\t\t$3200$ & $6.13E-3$  & $1.22E-2$ \\\\\n\t\\end{tabular}\n\t\\vspace{40pt}\n\t\\captionof{table}{\\it Mean of the measure of the symmetric difference for 100 sets of N random samples.}\n\t\\label{Table:symmdiff}\n\\end{minipage}\n\\end{figure}\n\n\n\n\\vskip 10pt\n\\section{\\bf Precision and Accuracy}\\label{Sec:Precision_Accuracy}\n\\vskip 10pt\n\n\nFollowing solution to the stochastic inverse problem, we generally want to make, and quantify uncertainty in, predictions that can be formulated as solutions to a stochastic forward problem, e.g., computing a confidence interval on storm surge levels of a hurricane forecast using an ensemble of predictions determined from sampling model input parameters. \nIn some cases, we may be able to design the observation network defining the QoI map used in the stochastic inverse problem, e.g., by determining where and when to deploy sensors such as buoys in the Gulf of Mexico.\nThe problem of defining the ``best'' observation network is generally referred to as optimal experimental design.   \nBelow, we motivate both the definitions we use to define what is an optimal QoI map as well as the development of criteria for choosing the optimal QoI map. \nOur starting point is the assumption that we solve the stochastic inverse problem in order to {\\em quantifiably improve} the predictive capabilities of the model. \n\nSuppose two different QoI maps, $Q^{(a)}$ and $Q^{(b)}$, define two stochastic inverse problems with solutions represented by the two densities $\\rho_\\Lambda^{(a)}$ and $\\rho_\\Lambda^{(b)}$ such that \n", "index": 13, "text": "\n\\[\n\t\\mu_\\Lambda({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)})) \\ll \\mu_\\Lambda({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)})).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\mu_{\\Lambda}({\\mathop{\\mathrm{supp}}}(\\rho_{\\Lambda}^{(a)}))\\ll\\mu_{\\Lambda}(%&#10;{\\mathop{\\mathrm{supp}}}(\\rho_{\\Lambda}^{(b)})).\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo movablelimits=\"false\">supp</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c1</mi><mi mathvariant=\"normal\">\u039b</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u226a</mo><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo movablelimits=\"false\">supp</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\u03c1</mi><mi mathvariant=\"normal\">\u039b</mi><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nNote that if $\\Lambda\\subset\\mathbb{R}^n$ is proper, then the above equation is not necessarily true as $Q^{-1}(B)$ may intersect the boundary of $\\Lambda$. \nWe neglect such boundary effects in the computations, and simply note that in certain cases they may play an important role although this is not the typical case in our experience. \n\nIn general $Q$ is not linear, so we consider local linear approximations of $Q$ to quantify the $\\mu_{\\Lambda}$-measure of inverting sets such as $B$ into different regions of $\\Lambda$.\nGiven $\\lambda^{(i)}=(\\lambda_1^{(i)}, \\lambda_2^{(i)} \\ldots, \\lambda_n^{(i)})\\in\\Lambda$, denote the Jacobian of $Q$ evaluated at $\\lambda^{(i)}$, by\n\\begin{eqnarray}\n    J_{\\lambda^{(i)}} = \\begin{mat} \\frac{\\partial Q_1(\\lambda^{(i)})}{\\partial \\lambda_1} & \\hdots & \\frac{\\partial Q_1(\\lambda^{(i)})}{\\partial \\lambda_n} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\frac{\\partial Q_m(\\lambda^{(i)})}{\\partial \\lambda_1} & \\hdots & \\frac{\\partial Q_m(\\lambda^{(i)})}{\\partial \\lambda_n} \\end{mat}.\n\\end{eqnarray}\nFor simplicity in describing the sets $Q^{-1}(B)$ we initially assume $n=m$ and the map $Q$ is GD, which implies that the Jacobian is square and invertible.  Suppose now that the generalized rectangle $B$ is centered at $Q(\\lambda^{(i)})$, then from above we have that\n\n", "itemtype": "equation", "pos": 35227, "prevtext": " \nFor simplicity, assume that $\\Lambda\\subset\\mathbb{R}^n$, $\\mu_\\Lambda$ is the Lebesgue measure, and the supports of the densities are convex subsets. \nLet $Q^{(p)}$ denote a prediction QoI that is a linear map on $\\Lambda$, and let $\\mathcal{D}=Q^{(p)}(\\Lambda)$ denote the space of possible outcomes for the predictions.\nThen, $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)}))$ and $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$ define the events of all probable predictions based on solutions to the separate stochastic inverse problems, and $\\mu_{\\mathcal{D}}(Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)})) \\ll \\mu_{\\mathcal{D}}(Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$.\nConsequently, we generally expect that statistical inferences drawn from $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(a)}))$ (e.g., the mean of the predictions) to have greater precision in terms of reduced variances, smaller confidence intervals, etc., compared to those drawn from $Q^{(p)}({\\mathop{\\mathrm{supp}}}(\\rho_\\Lambda^{(b)}))$.\n\nThis motivates a general measure-theoretic goal for designing experiments where the goal is to determine an observation network defining the map $Q$ such that solution to the stochastic inverse problem results in a significant amount of probability contained in events of small $\\mu_\\Lambda$-measure. \nIt is clear from Eq.~(\\ref{eq:finaldisintegration}), the Ansatz, and Algorithm~\\ref{Alg:1}, that the $\\mu_\\Lambda$-measures of induced contour events play a pivotal role in determining an optimal map $Q$. \nIn Section~\\ref{Sec:Support}, we describe an efficient computational approach for quantifying the $\\mu_\\Lambda$-measures of induced contour events for nonlinear maps $Q$.\nGiven multiple choices for the QoI map, we may use such results to determine the so-called optimal map $Q$.\nHowever, given finite computational resources, it may not be possible to accurately approximate solutions of the stochastic inverse problem using the optimal map $Q$ if only a relatively small number of model solves are allowed as described in Section~\\ref{Sec:Numerical_Approximation}.\nA computational method for quantifying the global affect of skewness from a nonlinear map $Q$ is described in Section~\\ref{Sec:Skewness}. \nIn Section~\\ref{Sec:Optimizing}, we describe how to optimize the choice of $Q$ to take into account the separate goals of obtaining {\\em precise} predictions from stochastic inverse problems that can be solved {\\em accurately} with relatively few model evaluations. \n\n\n\\subsection{$\\mu_\\Lambda$-measure of support}\\label{Sec:Support}\n\n\nSuppose in an experiment we can obtain a total of $m$ QoI.  Let $B\\in\\mathcal{B}_{\\mathcal{D}}$ define a typical output event from a partition of $\\mathcal{D}$ used in Algorithm~\\ref{Alg:1}.  If $P_{\\mathcal{D}}(B)\\approx 1$, then as described above, the goal is to quantify the expected $\\mu_\\Lambda$-measure of the support of $Q^{-1}(B)$.  Since we do not know $P_{\\mathcal{D}}$ a priori, we want to choose a $B$ that is representative of typical tessellations used to discretize any probability measure and also geometrically easy to describe such that $\\mu_\\Lambda(Q^{-1}(B))$ is computationally inexpensive to approximate.\nIn measure theory, it is common for $\\sigma$-algebras on higher dimensional spaces to be generated from generalized rectangles.\nWe therefore consider $B$ to be a generalized rectangle in $\\mathcal{D}\\subset\\mathbb{R}^m$. \nSuppose $Q$ is a linear GD map and $n=m$, i.e., $Q$ is defined by the square invertible matrix $J$, $Q(\\lambda)=J\\lambda$.\nIn this case, if $\\Lambda=\\mathbb{R}^n$, we have that\n\n", "index": 15, "text": "\\begin{equation}\n\t\\mu_\\Lambda(Q^{-1}(B)) = \\mu_\\Lambda(J^{-1}(B)) = \\mu_{\\mathcal{D}}(B)\\det(J^{-1}) = \\frac{\\mu_{\\mathcal{D}}(B)}{\\det J}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mu_{\\Lambda}(Q^{-1}(B))=\\mu_{\\Lambda}(J^{-1}(B))=\\mu_{\\mathcal{D}}(B)\\det(J^{%&#10;-1})=\\frac{\\mu_{\\mathcal{D}}(B)}{\\det J}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>J</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo movablelimits=\"false\">det</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>J</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>det</mo><mo>\u2061</mo><mi>J</mi></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nHere, we introduce the notation $M_Q(\\lambda^{(i)})$ as a shorthand for the the size (i.e., measure) of local induced contour events defined around a sample $\\lambda^{(i)}$, and we make explicit the dependence on the QoI map $Q$. \nSince the determinant of $J$ can be written as the product of the singular values of $J$, we have\n\n", "itemtype": "equation", "pos": 36699, "prevtext": "\nNote that if $\\Lambda\\subset\\mathbb{R}^n$ is proper, then the above equation is not necessarily true as $Q^{-1}(B)$ may intersect the boundary of $\\Lambda$. \nWe neglect such boundary effects in the computations, and simply note that in certain cases they may play an important role although this is not the typical case in our experience. \n\nIn general $Q$ is not linear, so we consider local linear approximations of $Q$ to quantify the $\\mu_{\\Lambda}$-measure of inverting sets such as $B$ into different regions of $\\Lambda$.\nGiven $\\lambda^{(i)}=(\\lambda_1^{(i)}, \\lambda_2^{(i)} \\ldots, \\lambda_n^{(i)})\\in\\Lambda$, denote the Jacobian of $Q$ evaluated at $\\lambda^{(i)}$, by\n\\begin{eqnarray}\n    J_{\\lambda^{(i)}} = \\begin{mat} \\frac{\\partial Q_1(\\lambda^{(i)})}{\\partial \\lambda_1} & \\hdots & \\frac{\\partial Q_1(\\lambda^{(i)})}{\\partial \\lambda_n} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n        \\frac{\\partial Q_m(\\lambda^{(i)})}{\\partial \\lambda_1} & \\hdots & \\frac{\\partial Q_m(\\lambda^{(i)})}{\\partial \\lambda_n} \\end{mat}.\n\\end{eqnarray}\nFor simplicity in describing the sets $Q^{-1}(B)$ we initially assume $n=m$ and the map $Q$ is GD, which implies that the Jacobian is square and invertible.  Suppose now that the generalized rectangle $B$ is centered at $Q(\\lambda^{(i)})$, then from above we have that\n\n", "index": 17, "text": "\\begin{equation}\n    \\mu_\\Lambda(Q^{-1}(B)) \\approx M_Q(\\lambda^{(i)}):=\\mu_\\Lambda(J_{\\lambda^{(i)}}^{-1}(B))  = \\frac{\\mu_{\\mathcal{D}}(B)}{\\det J_{\\lambda^{(i)}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mu_{\\Lambda}(Q^{-1}(B))\\approx M_{Q}(\\lambda^{(i)}):=\\mu_{\\Lambda}(J_{\\lambda%&#10;^{(i)}}^{-1}(B))=\\frac{\\mu_{\\mathcal{D}}(B)}{\\det J_{\\lambda^{(i)}}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><msub><mi>M</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>det</mo><mo>\u2061</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $\\left\\{{\\sigma^*_{ik}}\\right\\}_{k=1}^m$ are the singular values of $J_{\\lambda^{(i)}}^{-1}$ and $\\left\\{{\\sigma_{ik}}\\right\\}_{k=1}^m$ are the singular values of $J_{\\lambda^{(i)}}$.   \nThe {\\it average $M_Q(\\lambda)$} is given by\n\n", "itemtype": "equation", "pos": 37210, "prevtext": "\nHere, we introduce the notation $M_Q(\\lambda^{(i)})$ as a shorthand for the the size (i.e., measure) of local induced contour events defined around a sample $\\lambda^{(i)}$, and we make explicit the dependence on the QoI map $Q$. \nSince the determinant of $J$ can be written as the product of the singular values of $J$, we have\n\n", "index": 19, "text": "\\begin{equation}\\label{Eq:prod_singvals}\n    M_Q(\\lambda^{(i)}) = \\mu_{\\mathcal{D}}(B)\\prod_{k=1}^{m} \\, \\sigma^*_{ik} = \\frac{\\mu_{\\mathcal{D}}(B)}{\\prod_{k=1}^{m} \\, \\sigma_{ik}}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"M_{Q}(\\lambda^{(i)})=\\mu_{\\mathcal{D}}(B)\\prod_{k=1}^{m}\\,\\sigma^{*}_{ik}=%&#10;\\frac{\\mu_{\\mathcal{D}}(B)}{\\prod_{k=1}^{m}\\,\\sigma_{ik}},\" display=\"block\"><mrow><mrow><mrow><msub><mi>M</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mpadded width=\"+1.7pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mpadded><msubsup><mi>\u03c3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow><mo>*</mo></msubsup></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mpadded width=\"+1.7pt\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mpadded><msub><mi>\u03c3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nGiven a set of $N$ samples $\\left\\{{\\lambda^{(i)}}\\right\\}_{i=1}^N$ in $\\Lambda$, we approximate $\\overline{M_Q}$ using the Monte Carlo estimate\n\n", "itemtype": "equation", "pos": 37646, "prevtext": "\nwhere $\\left\\{{\\sigma^*_{ik}}\\right\\}_{k=1}^m$ are the singular values of $J_{\\lambda^{(i)}}^{-1}$ and $\\left\\{{\\sigma_{ik}}\\right\\}_{k=1}^m$ are the singular values of $J_{\\lambda^{(i)}}$.   \nThe {\\it average $M_Q(\\lambda)$} is given by\n\n", "index": 21, "text": "\\begin{equation}\\label{Eq:suppint}\n\t\\overline{M_Q} = \\frac{1}{\\mu_{\\Lambda}(\\Lambda)}\\int_{\\Lambda} M_Q(\\lambda) \\, d\\mu_{\\Lambda}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\overline{M_{Q}}=\\frac{1}{\\mu_{\\Lambda}(\\Lambda)}\\int_{\\Lambda}M_{Q}(\\lambda)%&#10;\\,d\\mu_{\\Lambda}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><msub><mi>M</mi><mi>Q</mi></msub><mo>\u00af</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi mathvariant=\"normal\">\u039b</mi></msub><mrow><msub><mi>M</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\nWe now relax the assumption that $n=m$ and consider the (more common) case of $m<n$.  \nIn this case, we recall that a transverse parameterization, which is an explicit representation in $\\Lambda$ of the equivalence class structure $\\mathcal{L}$ defined by the generalized contours, exists as a (possibly piecewise defined) $m$-dimensional manifold.  \nRestricting $Q:\\Lambda\\to\\mathcal{D}$ to be $Q:\\mathcal{L}\\to\\mathcal{D}$, which we denote by $Q|_{\\mathcal{L}}$, we then have an $m\\times m$ map and can apply the above computations to $Q|_{\\mathcal{L}}$. \nIn other words, we are now interested in the $\\mu_{\\mathcal{L}}$-measure of $Q|_{\\mathcal{L}}^{-1}(B)\\in\\mathcal{B}_{\\mathcal{L}}$.  \nGeometrically, we may think of this problem as determining the measure of the {\\em cross section of the contour event intersected with a transverse parameterization manifold} in $\\Lambda$. \nGiven a local linear approximation of $Q$ at some point $\\lambda^{(i)}\\in\\Lambda$, the following lemma shows that we may approximate $\\mu_{\\mathcal{L}}(M_{Q|_{\\mathcal{L}}}(\\lambda^{(i)}))$ similar to before. \nConsequently, the lemma implies that we may compute $\\overline{M_{Q|_{\\mathcal{L}},N}}$ using $M_{Q|_{\\mathcal{L}}}(\\lambda^{(i)})$ in Eq.~(\\ref{Eq:supp}). \n\n\n\\begin{lemma}\\label{Lemma:prod_singvals}\nLet $J$ be a full rank $m\\times n$ matrix with $m\\leq n$.  Then the Lebesgue measure $\\mu$ in ${{\\rm l} \\kern -.15em {\\rm R} }^m$ of the $m$-dimensional parallelepiped defined by the $m$ rows of $J$, denoted $\\mu(Pa(J))$, is given by the product of the singular values of $J$,\n\n", "itemtype": "equation", "pos": 37938, "prevtext": "\nGiven a set of $N$ samples $\\left\\{{\\lambda^{(i)}}\\right\\}_{i=1}^N$ in $\\Lambda$, we approximate $\\overline{M_Q}$ using the Monte Carlo estimate\n\n", "index": 23, "text": "\\begin{equation}\\label{Eq:supp}\n\t\\overline{M_Q} \\approx \\overline{M_{Q,N}} := \\frac{1}{N}\\sum_{i=1}^N M_Q(\\lambda^{(i)}) = \\frac{1}{N}\\sum_{i=1}^N \\frac{\\mu_{\\mathcal{D}}(B)}{\\prod_{k=1}^m\\sigma_{ik}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\overline{M_{Q}}\\approx\\overline{M_{Q,N}}:=\\frac{1}{N}\\sum_{i=1}^{N}M_{Q}(%&#10;\\lambda^{(i)})=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\mu_{\\mathcal{D}}(B)}{\\prod_{k=1%&#10;}^{m}\\sigma_{ik}}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><msub><mi>M</mi><mi>Q</mi></msub><mo>\u00af</mo></mover><mo>\u2248</mo><mover accent=\"true\"><msub><mi>M</mi><mrow><mi>Q</mi><mo>,</mo><mi>N</mi></mrow></msub><mo>\u00af</mo></mover><mo>:=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>M</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mrow><msub><mi>\u03bc</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi>\u03c3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mfrac></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{proof}\nThe singular values of $J$ are equal to the singular values of $J^{\\top}$.  Consider the reduced QR factorization of $J^{\\top}$,\n\n", "itemtype": "equation", "pos": 39725, "prevtext": "\n\nWe now relax the assumption that $n=m$ and consider the (more common) case of $m<n$.  \nIn this case, we recall that a transverse parameterization, which is an explicit representation in $\\Lambda$ of the equivalence class structure $\\mathcal{L}$ defined by the generalized contours, exists as a (possibly piecewise defined) $m$-dimensional manifold.  \nRestricting $Q:\\Lambda\\to\\mathcal{D}$ to be $Q:\\mathcal{L}\\to\\mathcal{D}$, which we denote by $Q|_{\\mathcal{L}}$, we then have an $m\\times m$ map and can apply the above computations to $Q|_{\\mathcal{L}}$. \nIn other words, we are now interested in the $\\mu_{\\mathcal{L}}$-measure of $Q|_{\\mathcal{L}}^{-1}(B)\\in\\mathcal{B}_{\\mathcal{L}}$.  \nGeometrically, we may think of this problem as determining the measure of the {\\em cross section of the contour event intersected with a transverse parameterization manifold} in $\\Lambda$. \nGiven a local linear approximation of $Q$ at some point $\\lambda^{(i)}\\in\\Lambda$, the following lemma shows that we may approximate $\\mu_{\\mathcal{L}}(M_{Q|_{\\mathcal{L}}}(\\lambda^{(i)}))$ similar to before. \nConsequently, the lemma implies that we may compute $\\overline{M_{Q|_{\\mathcal{L}},N}}$ using $M_{Q|_{\\mathcal{L}}}(\\lambda^{(i)})$ in Eq.~(\\ref{Eq:supp}). \n\n\n\\begin{lemma}\\label{Lemma:prod_singvals}\nLet $J$ be a full rank $m\\times n$ matrix with $m\\leq n$.  Then the Lebesgue measure $\\mu$ in ${{\\rm l} \\kern -.15em {\\rm R} }^m$ of the $m$-dimensional parallelepiped defined by the $m$ rows of $J$, denoted $\\mu(Pa(J))$, is given by the product of the singular values of $J$,\n\n", "index": 25, "text": "\\begin{equation}\n    \\mu(Pa(J)) = \\prod_{i=1}^m\\sigma_i.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mu(Pa(J))=\\prod_{i=1}^{m}\\sigma_{i}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>J</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03c3</mi><mi>i</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $\\tilde Q$ is $n\\times m$ and $R$ is $m\\times m$.  \nBy the properties of the QR factorization, we know the singular values of $R$ are the same as the singular values of $J^{\\top}$.  Let $x\\in{{\\rm l} \\kern -.15em {\\rm R} }^m$, then\n\n", "itemtype": "equation", "pos": 39953, "prevtext": "\n\\end{lemma}\n\n\\begin{proof}\nThe singular values of $J$ are equal to the singular values of $J^{\\top}$.  Consider the reduced QR factorization of $J^{\\top}$,\n\n", "index": 27, "text": "\\begin{equation}\n    J^{\\top} = \\tilde QR,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"J^{\\top}=\\tilde{Q}R,\" display=\"block\"><mrow><mrow><msup><mi>J</mi><mo>\u22a4</mo></msup><mo>=</mo><mrow><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>R</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nso $\\tilde Q$ is an isometry.  This implies the Lebesgue measure of the parallelepiped defined by the rows of $R$ is equal to the Lebesgue measure of the parallelepiped defined by the columns of $J^{\\top}$, or the rows of $J$,\n\n", "itemtype": "equation", "pos": 40249, "prevtext": "\nwhere $\\tilde Q$ is $n\\times m$ and $R$ is $m\\times m$.  \nBy the properties of the QR factorization, we know the singular values of $R$ are the same as the singular values of $J^{\\top}$.  Let $x\\in{{\\rm l} \\kern -.15em {\\rm R} }^m$, then\n\n", "index": 29, "text": "\\begin{equation}\n\t||\\tilde Qx||^2=(\\tilde Qx)^{\\top}(\\tilde Qx)=x^{\\top}\\tilde Q^{\\top}\\tilde Qx=x^{\\top}x=||x||^2,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"||\\tilde{Q}x||^{2}=(\\tilde{Q}x)^{\\top}(\\tilde{Q}x)=x^{\\top}\\tilde{Q}^{\\top}%&#10;\\tilde{Q}x=x^{\\top}x=||x||^{2},\" display=\"block\"><mrow><mrow><msup><mrow><mo fence=\"true\">||</mo><mrow><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>x</mi></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>x</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><msup><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>Q</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mi>x</mi></mrow><mo>=</mo><mrow><msup><mi>x</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mi>x</mi></mrow><mo>=</mo><msup><mrow><mo fence=\"true\">||</mo><mi>x</mi><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $\\gamma_k$ are the singular values of $R$ and $\\sigma_k$ are the singular values of $J$.\n\\end{proof}\n\n\n\\subsection{Skewness}\\label{Sec:Skewness}\n\n\nWhile determining the QoI map $Q$ that optimally reduces $\\mu_\\Lambda(Q^{-1}(B))$ for $B\\in\\mathcal{B}_\\mathcal{D}$ should reduce uncertainty (i.e., increase precision) in predictions, we often need to computationally approximate $Q^{-1}(B)$ as described in Algorithm~\\ref{Alg:1}. Errors in the approximations to $P_\\Lambda$ propagate to errors in predictions, and while the predicted sets of high probability may have small $\\mu_{\\mathcal{D}}$-measure, the errors in the prediction may be so large as to render such predictions useless in practice.  Hence, in this section we quantify the {\\em skewness} of local induced contour maps and then expand this to the {\\em average skewness} as done in Section~\\ref{Sec:Support} for the $\\mu_\\Lambda$-measure, $M_Q$.\n\nAgain, we refer to local linear approximations of the given map $Q$ at some $\\lambda^{(i)}\\in\\Lambda$, $J_{\\lambda^{(i)}}$.  Let $Pa(J_{\\lambda^{(i)}})$ denote the parallelepiped defined by the $n$-dimensional vectors $j_1, \\hdots, j_m$ that are the rows of $J_{\\lambda^{(i)}}$.  We use $\\mu(Pa(J_{\\lambda^{(i)}}))$ to denote the measure of the parallelepiped as an $m$-dimensional object.  This measure is calculated in terms of the singular values of $J_{\\lambda^{(i)}}$ by Lemma \\ref{Lemma:prod_singvals}.  A fundamental decomposition is\n\n\\begin{theorem}\n    Given $n$-dimensional vectors $j_1, \\hdots, j_m$, there exists vectors $j_1^{\\perp}, j_1^0$ such that\n    \n", "itemtype": "equation", "pos": 40607, "prevtext": "\nso $\\tilde Q$ is an isometry.  This implies the Lebesgue measure of the parallelepiped defined by the rows of $R$ is equal to the Lebesgue measure of the parallelepiped defined by the columns of $J^{\\top}$, or the rows of $J$,\n\n", "index": 31, "text": "\\begin{equation}\n    \\mu(Pa(J)) = \\mu(Pa(R)) = \\prod_{k=1}^m\\gamma_k = \\prod_{k=1}^m\\sigma_k,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\mu(Pa(J))=\\mu(Pa(R))=\\prod_{k=1}^{m}\\gamma_{k}=\\prod_{k=1}^{m}\\sigma_{k},\" display=\"block\"><mrow><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>J</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03b3</mi><mi>k</mi></msub></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03c3</mi><mi>k</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n    and\n\t\n", "itemtype": "equation", "pos": 42299, "prevtext": "\nwhere $\\gamma_k$ are the singular values of $R$ and $\\sigma_k$ are the singular values of $J$.\n\\end{proof}\n\n\n\\subsection{Skewness}\\label{Sec:Skewness}\n\n\nWhile determining the QoI map $Q$ that optimally reduces $\\mu_\\Lambda(Q^{-1}(B))$ for $B\\in\\mathcal{B}_\\mathcal{D}$ should reduce uncertainty (i.e., increase precision) in predictions, we often need to computationally approximate $Q^{-1}(B)$ as described in Algorithm~\\ref{Alg:1}. Errors in the approximations to $P_\\Lambda$ propagate to errors in predictions, and while the predicted sets of high probability may have small $\\mu_{\\mathcal{D}}$-measure, the errors in the prediction may be so large as to render such predictions useless in practice.  Hence, in this section we quantify the {\\em skewness} of local induced contour maps and then expand this to the {\\em average skewness} as done in Section~\\ref{Sec:Support} for the $\\mu_\\Lambda$-measure, $M_Q$.\n\nAgain, we refer to local linear approximations of the given map $Q$ at some $\\lambda^{(i)}\\in\\Lambda$, $J_{\\lambda^{(i)}}$.  Let $Pa(J_{\\lambda^{(i)}})$ denote the parallelepiped defined by the $n$-dimensional vectors $j_1, \\hdots, j_m$ that are the rows of $J_{\\lambda^{(i)}}$.  We use $\\mu(Pa(J_{\\lambda^{(i)}}))$ to denote the measure of the parallelepiped as an $m$-dimensional object.  This measure is calculated in terms of the singular values of $J_{\\lambda^{(i)}}$ by Lemma \\ref{Lemma:prod_singvals}.  A fundamental decomposition is\n\n\\begin{theorem}\n    Given $n$-dimensional vectors $j_1, \\hdots, j_m$, there exists vectors $j_1^{\\perp}, j_1^0$ such that\n    \n", "index": 33, "text": "\\begin{equation*}\n        j_1 = j_1^{\\perp} + j_1^0, \\hskip 10pt j_1^{\\perp}\\perp j_1^0, \\hskip 10pt j_1^0\\in span\\{j_2, \\hdots, j_m\\},    \n    \\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"j_{1}=j_{1}^{\\perp}+j_{1}^{0},\\hskip 10.0ptj_{1}^{\\perp}\\perp j_{1}^{0},\\hskip&#10;1%&#10;0.0ptj_{1}^{0}\\in span\\{j_{2},\\ldots,j_{m}\\},\" display=\"block\"><mrow><mrow><mrow><msub><mi>j</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mi>j</mi><mn>1</mn><mo>\u27c2</mo></msubsup><mo>+</mo><msubsup><mi>j</mi><mn>1</mn><mn>0</mn></msubsup></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><msubsup><mi>j</mi><mn>1</mn><mo>\u27c2</mo></msubsup><mo>\u27c2</mo><msubsup><mi>j</mi><mn>1</mn><mn>0</mn></msubsup></mrow><mo rspace=\"12.5pt\">,</mo><mrow><msubsup><mi>j</mi><mn>1</mn><mn>0</mn></msubsup><mo>\u2208</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>j</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\twhere $J_{\\lambda^{(i)}}$ denotes the $m\\times n$ matrix formed by the vectors $j_1, \\hdots, j_m$, $J_{1,\\lambda^{(i)}}$ denotes the matrix $J_{\\lambda^{(i)}}$ with the first row deleted, and $\\mu(Pa(J_{1,\\lambda^{(i)}}))$ denotes the measure of the parallelepiped defined by the $n$-dimensional vectors $j_2, \\hdots, j_m$ as an $(m-1)$-dimensional object.\n\\end{theorem}\nWith this fundamental decomposition we define the local skewness of a given map $Q$ at some point $\\lambda^{(i)}\\in\\Lambda$, see \\cite{Butler2015b}.\n\n\\begin{definition}\n    For a vector $j_k$, we define\n\t\n", "itemtype": "equation", "pos": 42468, "prevtext": "\n    and\n\t\n", "index": 35, "text": "\\begin{equation}\\label{Eq:mu_ji}\n        \\mu(Pa(J_{\\lambda^{(i)}})) = |j_1^{\\perp}| \\times \\mu(Pa(J_{1,\\lambda^{(i)}})),\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\mu(Pa(J_{\\lambda^{(i)}}))=|j_{1}^{\\perp}|\\times\\mu(Pa(J_{1,\\lambda^{(i)}})),\" display=\"block\"><mrow><mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>j</mi><mn>1</mn><mo>\u27c2</mo></msubsup><mo stretchy=\"false\">|</mo></mrow><mo>\u00d7</mo><mi>\u03bc</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><mrow><mn>1</mn><mo>,</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n    Then we define the {\\bf local skewness} at a point $\\lambda^{(i)}$ as\n\t\n", "itemtype": "equation", "pos": 43181, "prevtext": "\n\twhere $J_{\\lambda^{(i)}}$ denotes the $m\\times n$ matrix formed by the vectors $j_1, \\hdots, j_m$, $J_{1,\\lambda^{(i)}}$ denotes the matrix $J_{\\lambda^{(i)}}$ with the first row deleted, and $\\mu(Pa(J_{1,\\lambda^{(i)}}))$ denotes the measure of the parallelepiped defined by the $n$-dimensional vectors $j_2, \\hdots, j_m$ as an $(m-1)$-dimensional object.\n\\end{theorem}\nWith this fundamental decomposition we define the local skewness of a given map $Q$ at some point $\\lambda^{(i)}\\in\\Lambda$, see \\cite{Butler2015b}.\n\n\\begin{definition}\n    For a vector $j_k$, we define\n\t\n", "index": 37, "text": "\\begin{equation}\\label{Eq:skew}\n        S_Q(J_{\\lambda^{(i)}}, j_k) = \\frac{|j_k|}{|j_k^{\\perp}|}.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"S_{Q}(J_{\\lambda^{(i)}},j_{k})=\\frac{|j_{k}|}{|j_{k}^{\\perp}|}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>j</mi><mi>k</mi><mo>\u27c2</mo></msubsup><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\end{definition}\n\n\\begin{corollary}\\label{Cor:singular_values}\n    The local skewness of $Q$, $S_Q(J_{\\lambda^{(i)}})$, can be completely determined by the the norms of $n$-dimensional vectors and products of singular values of the Jacobians of QoI maps of dimenions $m-1$ and $m$,\n    \t\n", "itemtype": "equation", "pos": 43371, "prevtext": "\n    Then we define the {\\bf local skewness} at a point $\\lambda^{(i)}$ as\n\t\n", "index": 39, "text": "\\begin{equation}\\label{Eq:Skew}\n        S_Q(J_{\\lambda^{(i)}}) = \\max_k \\, S_Q(J_{\\lambda^{(i)}}, j_k).\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"S_{Q}(J_{\\lambda^{(i)}})=\\max_{k}\\,S_{Q}(J_{\\lambda^{(i)}},j_{k}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mi>k</mi></munder></mpadded><mo>\u2061</mo><msub><mi>S</mi><mi>Q</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\end{corollary}\n\n\\begin{proof}\n\t\n", "itemtype": "equation", "pos": 43778, "prevtext": "\n\\end{definition}\n\n\\begin{corollary}\\label{Cor:singular_values}\n    The local skewness of $Q$, $S_Q(J_{\\lambda^{(i)}})$, can be completely determined by the the norms of $n$-dimensional vectors and products of singular values of the Jacobians of QoI maps of dimenions $m-1$ and $m$,\n    \t\n", "index": 41, "text": "\\begin{equation}\n    \t\tS_Q(J_{\\lambda^{(i)}}) = \\max_k \\frac{|j_k|\\mu(Pa(J_{k,\\lambda^{(i)}}))}{\\mu(Pa(J_{\\lambda^{(i)}}))}.\n    \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"S_{Q}(J_{\\lambda^{(i)}})=\\max_{k}\\frac{|j_{k}|\\mu(Pa(J_{k,\\lambda^{(i)}}))}{%&#10;\\mu(Pa(J_{\\lambda^{(i)}}))}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi>k</mi></munder><mo>\u2061</mo><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\tthen applying Lemma~\\ref{Lemma:prod_singvals} we have\n\t\n", "itemtype": "equation", "pos": 43954, "prevtext": "\n\\end{corollary}\n\n\\begin{proof}\n\t\n", "index": 43, "text": "\\begin{equation}\n\t\tS_Q(J_{\\lambda^{(i)}}) = \\max_k \\, S_Q(J_{\\lambda^{(i)}}, j_k) = \\max_k \\, \\frac{|j_k|}{|j_k^{\\perp}|} = \\max_k \\, \\frac{|j_k|\\mu(Pa(J_{k,\\lambda^{(i)}}))}{\\mu(Pa(J_{\\lambda^{(i)}}))},\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"S_{Q}(J_{\\lambda^{(i)}})=\\max_{k}\\,S_{Q}(J_{\\lambda^{(i)}},j_{k})=\\max_{k}\\,%&#10;\\frac{|j_{k}|}{|j_{k}^{\\perp}|}=\\max_{k}\\,\\frac{|j_{k}|\\mu(Pa(J_{k,\\lambda^{(i%&#10;)}}))}{\\mu(Pa(J_{\\lambda^{(i)}}))},\" display=\"block\"><mrow><mrow><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mi>k</mi></munder></mpadded><mo>\u2061</mo><msub><mi>S</mi><mi>Q</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mi>k</mi></munder></mpadded><mo>\u2061</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>j</mi><mi>k</mi><mo>\u27c2</mo></msubsup><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mi>k</mi></munder></mpadded><mo>\u2061</mo><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\twhere $\\sigma_r$ are the singular values of $J_{\\lambda^{(i)}}$ and $\\sigma_{kr}$ are the singular values of $J_{k,\\lambda^{(i)}}$.\n\\end{proof}\n\nCorollary \\ref{Cor:singular_values} allows us to take advantage of efficient singular value decompositions to algorithmically approximate $S_Q(J_{\\lambda^{(i)}})$.  The average skewness is given by\n\n", "itemtype": "equation", "pos": 44230, "prevtext": "\n\tthen applying Lemma~\\ref{Lemma:prod_singvals} we have\n\t\n", "index": 45, "text": "\\begin{equation}\n\t\t\\max_k \\frac{|j_k|\\mu(Pa(J_{k,\\lambda^{(i)}}))}{\\mu(Pa(J_{\\lambda^{(i)}}))} = \\max_k \\, \\frac{|j_k|\\prod_{r=1}^{m-1}\\sigma_{kr}}{\\prod_{r=1}^{m}\\sigma_r}.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\max_{k}\\frac{|j_{k}|\\mu(Pa(J_{k,\\lambda^{(i)}}))}{\\mu(Pa(J_{\\lambda^{(i)}}))}%&#10;=\\max_{k}\\,\\frac{|j_{k}|\\prod_{r=1}^{m-1}\\sigma_{kr}}{\\prod_{r=1}^{m}\\sigma_{r%&#10;}}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mi>k</mi></munder><mo>\u2061</mo><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03bc</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>J</mi><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>max</mi><mi>k</mi></munder></mpadded><mo>\u2061</mo><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>j</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u220f</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msubsup><msub><mi>\u03c3</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>r</mi></mrow></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u220f</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi>\u03c3</mi><mi>r</mi></msub></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nand approximated by\n\n", "itemtype": "equation", "pos": 44764, "prevtext": "\n\twhere $\\sigma_r$ are the singular values of $J_{\\lambda^{(i)}}$ and $\\sigma_{kr}$ are the singular values of $J_{k,\\lambda^{(i)}}$.\n\\end{proof}\n\nCorollary \\ref{Cor:singular_values} allows us to take advantage of efficient singular value decompositions to algorithmically approximate $S_Q(J_{\\lambda^{(i)}})$.  The average skewness is given by\n\n", "index": 47, "text": "\\begin{equation}\\label{Eq:skewint}\n\t\\overline{S_Q} = \\frac{1}{\\mu_{\\Lambda}(\\Lambda)}\\int_{\\Lambda} S_Q(\\lambda) \\, d\\mu_{\\Lambda},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\overline{S_{Q}}=\\frac{1}{\\mu_{\\Lambda}(\\Lambda)}\\int_{\\Lambda}S_{Q}(\\lambda)%&#10;\\,d\\mu_{\\Lambda},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><msub><mi>S</mi><mi>Q</mi></msub><mo>\u00af</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi mathvariant=\"normal\">\u039b</mi></msub><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03bc</mi><mi mathvariant=\"normal\">\u039b</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\begin{remark}\n\tNotice the definition of average local skewness is independent of $B\\in\\mathcal{B_D}$.  This is because $B$ is assumed to be a generalized rectangle, i.e., has perfect skewness properties itself.  This is a reasonable assumption as $B$ is typically defined by the cross product of intervals defining the uncertainty in each QoI.\n\\end{remark}\n\n\n\n\\section{\\bf Multicriteria Optimization}\\label{Sec:Optimizing}\n\n\nA common problem in designing observation networks is to determine where and when to deploy a finite number of sensors in order to record {\\em useful} data related to a physics-based model.\nThe various possible configurations of sensors defines a family of possible QoI maps. \nGiven a family of possible QoI maps, our goal is to determine a particular QoI map resulting in {\\em precise} and {\\em accurate} numerical approximation of the inverse solution.\nHere, we frame the problem of determining such a QoI map  as an optimization problem that simultaneously reduces both $\\overline{M_Q}$ and $\\overline{S_Q}$.\n\nLet $\\mathcal{Q}$ denote the space of all possible sets of QoI, let $Q^{(z)}\\in\\mathcal{Q}$ represent a set of $m$ QoI, and let $\\mathcal{D}^{(z)}$ represent the $m$-dimensional data space defined by $Q^{(z)}$, i.e., $Q^{(z)} : \\Lambda\\subset{{\\rm l} \\kern -.15em {\\rm R} }^n\\rightarrow\\mathcal{D}^{(z)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^m$.  \nFor each $Q^{(z)}$ there is a corresponding generalized rectangle $B^{(z)}\\in\\mathcal{B}_{\\mathcal{D}^{(z)}}$ that represents the uncertainty in a possible recorded datum in $\\mathcal{D}^{(z)}$.  \n\nRecall from Section \\ref{Sec:Precision_Accuracy} the goals are to reduce $\\overline{S_{Q^{(z)}}}$ and $\\overline{M_{Q^{(z)}}}$. \nLet $\\mathcal{S}\\subset\\mathbb{R}$ denote the set of all possible values of $\\overline{S_{Q^{(z)}}}$ and $\\mathcal{M}\\subset\\mathbb{R}$ denote the set of all possible values of $\\overline{M_{Q^{(z)}}}$.  \nWe then define the metric spaces $(\\mathcal{S}, d_{\\mathcal{S}})$ and $(\\mathcal{M}, d_{\\mathcal{M}})$ where\n\n", "itemtype": "equation", "pos": 44931, "prevtext": "\nand approximated by\n\n", "index": 49, "text": "\\begin{equation}\n    \\overline{S_Q}\\approx\\overline{S_{Q,N}} := \\frac{1}{N}\\sum_{i=1}^{N}S_Q(\\lambda^{(i)}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\overline{S_{Q}}\\approx\\overline{S_{Q,N}}:=\\frac{1}{N}\\sum_{i=1}^{N}S_{Q}(%&#10;\\lambda^{(i)}).\" display=\"block\"><mrow><mrow><mover accent=\"true\"><msub><mi>S</mi><mi>Q</mi></msub><mo>\u00af</mo></mover><mo>\u2248</mo><mover accent=\"true\"><msub><mi>S</mi><mrow><mi>Q</mi><mo>,</mo><mi>N</mi></mrow></msub><mo>\u00af</mo></mover><mo>:=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 47083, "prevtext": "\n\\begin{remark}\n\tNotice the definition of average local skewness is independent of $B\\in\\mathcal{B_D}$.  This is because $B$ is assumed to be a generalized rectangle, i.e., has perfect skewness properties itself.  This is a reasonable assumption as $B$ is typically defined by the cross product of intervals defining the uncertainty in each QoI.\n\\end{remark}\n\n\n\n\\section{\\bf Multicriteria Optimization}\\label{Sec:Optimizing}\n\n\nA common problem in designing observation networks is to determine where and when to deploy a finite number of sensors in order to record {\\em useful} data related to a physics-based model.\nThe various possible configurations of sensors defines a family of possible QoI maps. \nGiven a family of possible QoI maps, our goal is to determine a particular QoI map resulting in {\\em precise} and {\\em accurate} numerical approximation of the inverse solution.\nHere, we frame the problem of determining such a QoI map  as an optimization problem that simultaneously reduces both $\\overline{M_Q}$ and $\\overline{S_Q}$.\n\nLet $\\mathcal{Q}$ denote the space of all possible sets of QoI, let $Q^{(z)}\\in\\mathcal{Q}$ represent a set of $m$ QoI, and let $\\mathcal{D}^{(z)}$ represent the $m$-dimensional data space defined by $Q^{(z)}$, i.e., $Q^{(z)} : \\Lambda\\subset{{\\rm l} \\kern -.15em {\\rm R} }^n\\rightarrow\\mathcal{D}^{(z)}\\subset{{\\rm l} \\kern -.15em {\\rm R} }^m$.  \nFor each $Q^{(z)}$ there is a corresponding generalized rectangle $B^{(z)}\\in\\mathcal{B}_{\\mathcal{D}^{(z)}}$ that represents the uncertainty in a possible recorded datum in $\\mathcal{D}^{(z)}$.  \n\nRecall from Section \\ref{Sec:Precision_Accuracy} the goals are to reduce $\\overline{S_{Q^{(z)}}}$ and $\\overline{M_{Q^{(z)}}}$. \nLet $\\mathcal{S}\\subset\\mathbb{R}$ denote the set of all possible values of $\\overline{S_{Q^{(z)}}}$ and $\\mathcal{M}\\subset\\mathbb{R}$ denote the set of all possible values of $\\overline{M_{Q^{(z)}}}$.  \nWe then define the metric spaces $(\\mathcal{S}, d_{\\mathcal{S}})$ and $(\\mathcal{M}, d_{\\mathcal{M}})$ where\n\n", "index": 51, "text": "\\begin{equation}\n\td_{\\mathcal{S}}(x, y) = \\frac{|x-y|}{1 + |x-y|} \\, \\text{for all } x,y\\in\\mathcal{S},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"d_{\\mathcal{S}}(x,y)=\\frac{|x-y|}{1+|x-y|}\\,\\text{for all }x,y\\in\\mathcal{S},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>d</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mfrac></mpadded><mo>\u2062</mo><mtext>for all\u00a0</mtext><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nWe define the metrics this way so that the effect of the scaling of each component is limited in solving the minimization problem defined below. \nLet $Y_\\omega$ denote the 2-dimensional product space $\\mathcal{S} \\times \\mathcal{M}$, with metric defined by\n\n", "itemtype": "equation", "pos": 47206, "prevtext": "\nand\n\n", "index": 53, "text": "\\begin{equation}\n\td_{\\mathcal{M}}(x, y) = \\frac{|x-y|}{1 + |x-y|} \\, \\text{for all } x,y\\in\\mathcal{M}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"d_{\\mathcal{M}}(x,y)=\\frac{|x-y|}{1+|x-y|}\\,\\text{for all }x,y\\in\\mathcal{M}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>d</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>x</mi><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mfrac></mpadded><mo>\u2062</mo><mtext>for all\u00a0</mtext><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $\\omega\\in[0,1]$ determines the relative importance we place on either precision or accuracy.  \n\n\\begin{remark}\n\tThe weighting of the $\\mu_\\Lambda$-measure and the skewness is a current topic of discussion.  When more computational resources are available for solving the model, it is likely we weight the skewness less than the $\\mu_\\Lambda$-measure.  When we can only afford few samples we weight the skewness more than $\\mu_\\Lambda$-measure.  This is a topic for future work.\n\\end{remark}\n\nWith this notion of distance on $Y_\\omega$ we define the multicriteria optimization problem as finding the solution to,\n\n", "itemtype": "equation", "pos": 47582, "prevtext": "\nWe define the metrics this way so that the effect of the scaling of each component is limited in solving the minimization problem defined below. \nLet $Y_\\omega$ denote the 2-dimensional product space $\\mathcal{S} \\times \\mathcal{M}$, with metric defined by\n\n", "index": 55, "text": "\\begin{equation}\\label{Eq:dist_suppskew}\n\td_{Y_\\omega}(x, y) = \\omega d_{\\mathcal{S}}(x_1, y_1) + (1-\\omega )d_{\\mathcal{M}}(x_2, y_2) \\, \\text{for all } x,y\\in Y_\\omega,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"d_{Y_{\\omega}}(x,y)=\\omega d_{\\mathcal{S}}(x_{1},y_{1})+(1-\\omega)d_{\\mathcal{%&#10;M}}(x_{2},y_{2})\\,\\text{for all }x,y\\in Y_{\\omega},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>d</mi><msub><mi>Y</mi><mi>\u03c9</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03c9</mi><mo>\u2062</mo><msub><mi>d</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03c9</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>d</mi><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>for all\u00a0</mtext><mo>\u2062</mo><mi>x</mi></mrow></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>\u2208</mo><msub><mi>Y</mi><mi>\u03c9</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\nwhere $p=(1, 0)$ is the {\\it ideal point} and $y_z=(\\overline{S_{Q^{(z)}}}, \\overline{M_{Q^{(z)}}})$.  \n\n\n\n\\subsection{Discrete Optimization}\\label{Sec:Discrete}\n\n\nSuppose $\\mathcal{Q}$ is a finite family of possible QoI maps, e.g., as defined by identifying a finite set of $d$ physically possible configurations of $m$ sensors with $d\\geq m$. \nThe solution to the minimization problem defined by Eq.~\\eqref{eq:optimal_Q} can be found by an exhaustive search through the $d \\choose m$ possible QoI maps.\nThis combinatorial problem can clearly become computationally expensive as the number of possible maps gets large. \nHowever, it is completely straightforward to implement and is embarrassingly parallel. \nIn the concluding remarks of Section~\\ref{Sec:Conclusion}, we describe some possible future directions to mitigate the cost of solving the optimization problem. \nWe provide two numerical examples to clarify this method of determining an optimal QoI map to use in the inverse problem.\n\n\\begin{remark}\nWe note that another equivalent way to frame the discrete optimization problem is to define a ``theoretical'' QoI map $Q:\\Lambda\\to\\mathcal{D}\\subset\\mathbb{R}^d$ where the goal is to determine the ``practical'' QoI map given by a subset of $m$ components of the map $Q$.\nThe advantage of this approach from an implementation point of view is that for each sample in $\\Lambda$, the model is solved once in order to compute all the possible QoI values.\nIt also provides an index to each possible QoI making the description of the optimal QoI more straightforward.\nTherefore, we use this in the numerical examples below. \n\\end{remark}\n\n\n\\subsection{Numerical Examples}\\label{Sec:1}\n\n\n\n\\subsubsection{Linear Example}\\label{Sec:Linear}\n\n\nLet $Q : {{\\rm l} \\kern -.15em {\\rm R} }^2 \\rightarrow {{\\rm l} \\kern -.15em {\\rm R} }^3$,\n\\begin{eqnarray}\n    Q = \\begin{mat} 0.5 & 0.5 \\\\\n        \t\t 2.5 & 0.5 \\\\\n        \t\t -0.2 & 0.3  \\end{mat}.\n\\end{eqnarray}\nNotice the rows of $Q$ are pairwise linearly independent, i.e., pairwise GD.  Let $B^{(z)}\\in \\mathcal{B}_{\\mathcal{D}^{(z)}}$ be the rectangle of uncertainty in the data space defined by the $z^{th}$ possible pair of QoI.  For simplicity, we let the uncertainty in each QoI be the same, i.e., $\\mu_{\\mathcal{D}^{(z)}}(B^{(z)})$ is constant for all $z$.  \nNote that because $Q$ is linear we can easily use the exact Jacobian of $Q$.  \nFurthermore, we use the exact averages $\\overline{M_{Q^{(z)}}}$ and $\\overline{S_{Q^{(z)}}}$ not the approximate averages. \nThe linearity of $Q$ implies $M_{Q^{(z)}}(\\lambda^{(i)})$ and $S_{Q^{(z)}}(\\lambda^{(i)})$ are each constant for all $i$, so we need not numerically approximate the integrals in Eqs.~\\eqref{Eq:suppint} and \\eqref{Eq:skewint}.  \nIn Section~\\ref{Sec:Nonlinear} we consider a nonlinear map and compute approximations of the Jacobian, $\\overline{M_{Q^{(z)}}}$, and $\\overline{S_{Q^{(z)}}}$.\n\nWe see in Figure \\ref{Fig:linear_2to3} and Table~\\ref{Table:linear_2to3} the optimal choice of pair of QoI to use in the inverse problem for three different optimization problems; minimize $\\overline{{M_{Q^{(z)}}}}$, minimize $\\overline{{S_{Q^{(z)}}}}$, and minimize Eq.~\\eqref{Eq:dist_suppskew} with $\\omega = 0.5$.  In the top row of Figure~\\ref{Fig:linear_2to3} we see the inverse image of $B^{(z)}$ as the intersection of the red and blue contour events corresponding to the individual components of the possible QoI maps.  \nIt is visually evident from the top-left plot of Figure~\\ref{Fig:linear_2to3} that the pair of QoI that minimizes $\\overline{M_{Q^{(z)}}}$ also produces the inverse image with the sharpest corners, i.e., it has the highest skewness.  \nIn the top-middle plot of Figure~\\ref{Fig:linear_2to3} we observe the opposite effect, the pair of QoI that minimizes the skewness $\\overline{S_{Q^{(z)}}}$ maximizes $\\overline{M_{Q^{(z)}}}$. \nIn the top-right plot of Figure~\\ref{Fig:linear_2to3}, we observe a pair of QoI that produces reasonably low $\\overline{S_{Q^{(z)}}}$ and $\\overline{M_{Q^{(z)}}}$ simultaneously.\n\n\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n        \\includegraphics[width=.3\\textwidth]{linear_2to3_01}\n\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_02}\n\t\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_12}\\\\\n\t\t\t        \n        \t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter01}\n\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter02}\n        \t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter12}\n\t\\caption{\\it On the top we show the inverse density for the three possible pairs of QoI.  On the bottom we show the point in $Y_\\omega$ defined by each pair of QoI.  (left): The pair of QoI that minimizes $\\overline{M_{Q^{(z)}}}$ ($Q_{0,1}$).  (middle): The pair of QoI that minimizes $\\overline{S_{Q^{(z)}}}$ ($Q_{0,2}$).  (right): The pair of QoI that minimizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{1,2}$). }\\label{Fig:linear_2to3}\n\t\\end{minipage}\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{10pt}\n\t\\begin{tabular}{rrrr}\n  \t\tPair & $\\overline{M_{Q^{(z)}}}$ & $\\overline{S_{Q^{(z)}}}$ & $d_{Y_\\omega}(p, y_z)$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$Q_{0,1}$ &  $4.0E-2$ & $1.80E+0$ & $4.84E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{0,2}$ & $1.6E-2$ & $1.02E+0$ & $1.57E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,2}$ & $4.7E-2$  & $1.08E+0$ & $1.20E-1$\\\\\n\t\\end{tabular}\n\t\\captionof{table}{\\it $\\overline{M_{Q^{(z)}}}$, $\\overline{S_{Q^{(z)}}}$, and $d_{Y_\\omega}(p, y_z)$ for each of the three possible pairs of QoI.}\n\t\\label{Table:linear_2to3}\n\\end{minipage}\n\\end{figure}\n\n\n\\subsubsection{Nonlinear}\\label{Sec:Nonlinear}\n\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n        \\includegraphics[width=.24\\textwidth]{nonlinear_2to10_supp}\n        \t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_skew}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_sumskewsupp}\n        \t\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_WORST}\\\\\n        \t\t\t\t\n        \t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_supp_scatter}\n\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_skew_scatter}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_sumskewsupp_scatter}\n        \t\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_WORST_scatter}\n\t\\caption{\\it On the top we show the inverse image defined by four possible pairs of QoI.  On the bottom we show the point in $Y_\\omega$ defined by each pair of QoI.  (left): The pair of QoI that minimizes $\\overline{M_{Q^{(z)},N}}$ ($Q_{5,9}$).  (middle-left): The pair of QoI that minimizes $\\overline{S_{Q^{(z)},N}}$ ($Q_{1,4}$).  (middle-right): The pair of QoI that minimizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{1,7}$).  (right): The pair of QoI that maximizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{0,8}$).}\\label{Fig:nonlinear_2to10}\n\t\\end{minipage}\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{10pt}\n\t\\begin{tabular}{rrrr}\n  \t\tPair & $\\overline{M_{Q^{(z)},N}}$ & $\\overline{S_{Q^{(z)},N}}$ & $d_{Y_\\omega}(p, y_z)$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$Q_{5,9}$ &  $1.50E-2$ & $1.482E+0$ & $3.40E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,4}$ & $2.60E-2$ & $1.040E+0$ & $6.40E-2$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,7}$ & $2.10E-2$  & $1.044E+0$ & $6.30E-2$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{0,8}$ & $1.33E-1$  & $4.016E+0$ & $8.68E-1$\\\\\n\t\\end{tabular}\n\t\\captionof{table}{\\it $\\overline{M_{Q^{(z)},N}}$, $\\overline{S_{Q^{(z)},N}}$, and $d_{Y_\\omega}(p, y_z)$ for the four pairs of QoI considered in Figure~\\ref{Fig:nonlinear_2to10}.}\n\t\\label{Table:nonlinear_2to10}\n\\end{minipage}\n\\end{figure}\n\nLet $Q : {{\\rm l} \\kern -.15em {\\rm R} }^2 \\rightarrow {{\\rm l} \\kern -.15em {\\rm R} }^{10}$ be {\\em nonlinear} where each component is a polynomial of the form\n\\begin{eqnarray*}\n\tQ_j = r_{j,0}\\lambda_1^5 + r_{j,1}\\lambda_2^3 + r_{j,2}\\lambda_1^3\\lambda_2 + r_{j,3}\\lambda_1 + r_{j,4}\\lambda_2 + r_{j,5} \\, \\, \\text{for}\\, \\, j=0,1,\\hdots,9,\n\\end{eqnarray*}\nwhere the coefficients $r_{j,k}$ are fixed random numbers in $[-1,1]$.  Let $B^{(z)}\\in \\mathcal{B}_{\\mathcal{D}^{(z)}}$ be the rectangle of uncertainty in the data space defined by the $z^{th}$ possible pair of QoI.  \nAgain, we assume the uncertainty in each QoI is the same so that $\\mu(B^{(z)})$ is constant for all $z$.  \nNow that $Q$ is nonlinear, we approximate both the local Jacobians $J_{\\lambda^{(i)}}$ and the integrals in Eqs.~\\eqref{Eq:suppint} and \\eqref{Eq:skewint}.  \nWe take 1000 uniform random samples $\\lambda^{(i)}$ in $\\Lambda$ and then compute the approximate Jacobian at $N=100$ of these samples using a Radial Basis Function (RBF) interpolation method.  \nSpecifically, for a given $\\lambda^{(i)}\\in\\Lambda$, we use the 20 nearest neighbors of $\\lambda^{(i)}$ (from the original 1000 samples) to approximate the gradient for each QoI and subsequently construct the Jacobian.\n\n\\begin{remark}\n\tBecause $Q$ is a vector valued polynomial function, we could analytically determine the exact gradient vectors for each component and simply evalute at each $\\lambda^{(i)}$.  However, the RBF method (or any finite difference method) is a more general approach that can be used to approximate gradient vectors for maps $Q$ defined by functionals of solutions to partial differential equations where we cannot analytically determine the gradients of each component.\n\\end{remark}\n\nWe illustrate the various parts of the optimization problem by separately solving four optimization problems; minimize $\\overline{{M_{Q^{(z)},N}}}$, minimize $\\overline{{S_{Q^{(z)},N}}}$, minimize Equation~\\ref{Eq:dist_suppskew} with $\\omega= 0.5$, and maximize Equation~\\ref{Eq:dist_suppskew} with $\\omega=0.5$.  \nThe results are summarized in Figure~\\ref{Fig:nonlinear_2to10} and Table~\\ref{Table:nonlinear_2to10}.  \nIn the bottom left of Figure~\\ref{Fig:nonlinear_2to10} we see the location in $Y_\\omega$ of $s_{5,9}=(\\overline{S_{Q_{5,9},N}}, \\overline{M_{Q_{5,9},N}})$.  Although this pair of QoI minimizes $\\overline{M_{Q^{(z)},N}}$ it is obvious from the scatter plot there are pairs of QoI that have similar average $\\mu_\\Lambda$-measure that have much smaller average skewness.  \nIn the bottom right we see the location in $Y_\\omega$ of $s_{0,8}$ that corresponds to the pair of QoI that maximizes the sum of the average $\\mu_\\Lambda$-measure and the average skewness.  \nThe corresponding inverse image, seen in the top right of Figure~\\ref{Fig:nonlinear_2to10}, clearly has both the largest measure of support and the largest skewness of the four pairs considered.\n\n\n\\section{\\bf Maps Defined by the Solution of a Physics-Based Model}\\label{Sec:Map_Defined}\n\n\nIn this section we consider the inverse density of sets under functions defined by the solutions to partial differential equations.  \nWe first consider a simple time dependent diffusion model with uncertain diffusion coefficient that allows us to understand the results intuitively.\nThen we consider the ADvaned CIRCulation model for oceanic, coastal, and estuarine waters (ADCIRC) which incorporates a spatially varying bottom friction model.  \nThe ADCIRC model depends on many parameter fields, we focus on the bottom friction parameter due to its inherent uncertainty.\n\n\n\\subsection{Time Dependent Diffusion}\\label{Sec:heatplate}\n\n\n\n\\subsubsection{The Model}\\label{Sec:The_Model}\n\n\nWe consider the time dependent diffusion equation\n\\vskip 10pt\n\n", "itemtype": "equation", "pos": 48387, "prevtext": "\nwhere $\\omega\\in[0,1]$ determines the relative importance we place on either precision or accuracy.  \n\n\\begin{remark}\n\tThe weighting of the $\\mu_\\Lambda$-measure and the skewness is a current topic of discussion.  When more computational resources are available for solving the model, it is likely we weight the skewness less than the $\\mu_\\Lambda$-measure.  When we can only afford few samples we weight the skewness more than $\\mu_\\Lambda$-measure.  This is a topic for future work.\n\\end{remark}\n\nWith this notion of distance on $Y_\\omega$ we define the multicriteria optimization problem as finding the solution to,\n\n", "index": 57, "text": "\\begin{equation}\\label{eq:optimal_Q}\n\t\\min_{Q^{(z)}\\in\\mathcal{Q}} d_{Y_\\omega}(p, y_z),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\min_{Q^{(z)}\\in\\mathcal{Q}}d_{Y_{\\omega}}(p,y_{z}),\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><msup><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcac</mi></mrow></munder><mo>\u2061</mo><msub><mi>d</mi><msub><mi>Y</mi><mi>\u03c9</mi></msub></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mi>y</mi><mi>z</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06702.tex", "nexttext": "\n\\vskip 10pt\n\\begin{eqnarray*}\n    f(x) = \n    \\begin{cases}\n    Ae^{-\\frac{(x_0-p_0)^2+(x_1-p_1)^2}{w}} & \\text{if } t \\leq t_{source} \\\\\n    0       & \\text{if } t > t_{source}\n    \\end{cases}\n\\end{eqnarray*}\n\\begin{eqnarray*}\n    T(x;0) = 0 \\\\\n    \\frac{\\partial T}{\\partial n} = 0 ,  x \\in \\partial \\Omega\n\\end{eqnarray*}\nwhere $\\Omega = [-\\frac{1}{2}, \\frac{1}{2}] \\times [-\\frac{1}{2}, \\frac{1}{2}]$ represents a thin plate made of some alloys, $\\rho$ is the density of the plate, $c$ is the heat capacity, and $\\kappa$ is the thermal conductivity.  The forcing function $f$ represents an external source with the following source parameters: $A$ is the amplitude, $\\bar p$ is the position, and $w$ is the width.  The external source is positioned at the center of the plate.  We remove the external source at time $t_{source}=\\frac{t_f}{2}$.  The homogeneous Neumann boundary conditions model perfect insulation around the boundary.\n\nWe assume that the square plates are manufactured by welding together two rectangular plates of the same alloy.\nHowever, due to imperfections in the original manufacturing process, we assume that the alloy composition varies significantly from plate to plate.\nThus, we let the thermal conductivity $\\kappa$ vary in space as described below. \nWe assume that the left-half of $\\Omega$ contains an alloy with thermal conductivity $\\kappa_0$ and the right-half of $\\Omega$ contains an alloy with thermal conductivity $\\kappa_1$ on the right, both of which are uncertain within $[0.01, 0.2]$..  \nThus, the model parameters for this problem are $\\kappa_0$ and $\\kappa_1$ and we have defined our parameter space, $\\Lambda = [0.01,0.2] \\times [0.01,0.2] \\subset \\mathbb{R}^2$.\nSee Figure~\\ref{Fig:qoi_locations} where the green and purple colors indicate the two rectangular plates that are welded together to form the single plate defining the physical domain $\\Omega$\n\n\n\\subsubsection{The Inverse Problem}\\label{Sec:The_Inverse_Problem_heatplate}\n\n\nOur goal is to determine which pair of QoI produce the best solution to the inverse problem, i.e., determine which two points in space time we should record temperature measurements in order to obtain the ``best'' inferences about the thermal conductivities of each side of the plate.  \nIntuitively, we expect to take one temperature measurement on each side of the plate, but it is not clear at what time steps each temperature measurement should be recorded or exactly where we should place the sensors on each side of the plate.  \nWe limit ourselves to a discrete set of possible temperature measurements in space-time.  \nWe choose 20 points on the plate in space, see Figure \\ref{Fig:qoi_locations}, and gather these temperature measurements at each of 20 time steps.  \nThus, $\\mathcal{Q}$ is defned by 400 possible QoI values in space-time for which it is only possible to record two.\n\nTo make inferences about the optimal experimental design we must solve the model for a set of samples in the parameter space.  \nFor the numerical solution of the model,  we use the state of the art open source finite element software FEniCS \\cite{AlnaesBlechta2015a} with a $40 \\times 40$  triangular mesh, piecewise linear finite elements and a Crank Nicolson time stepping scheme.  We take $5000$ uniform random samples from $\\Lambda$ and run the simulation for each sample.  \nWe mathematically model the sensors recording data at a point in space-time as a functional of the solution by computing the average temperature on a small disc about a point in space but at a fixed time, i.e., each QoI is modeled as\n\\begin{eqnarray*}\n    Q_{(p_i;t_i)} = \\frac{1}{\\mu_{\\Omega}(\\mathbf{B}_r(p_i))}\\int_{\\Omega}T(x;t_i)\\chi_{\\mathbf{B}_r(p_i)} \\, dx. \\\\\n\\end{eqnarray*}\nThis yields a map $Q: \\Lambda \\to \\mathcal{D}$ where $\\Lambda$ is 2-dimensional and $\\mathcal{D}$ is 400-dimensional.  \nThus, we have $400 \\choose 2$ $=79,800$ possible pairs of QoI to consider.\n\n\\begin{figure}\n    \\begin{center}\n        \\textbf{QoI Locations}\\par\\medskip\n        \\includegraphics[width=.53\\textwidth]{QoILocations}\n        \\caption{\\it The QoI locations in the domain $\\Omega$.}\\label{Fig:qoi_locations}\n    \\end{center}\n\\end{figure}\n\nWe approximate $\\overline{M_{Q^{(z)}}}$ and $\\overline{S_{Q^{(z)}}}$, for each $z$, using the $N=5000$ samples for which we solved the model.  We solve three optimization problems; minimize $\\overline{M_{Q^{(z)},N}}$, minimize $\\overline{S_{Q^{(z)},N}}$, and minimize the distance defined in Eq.~(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$.  \nRecall these minimization problems tell us {\\em on average} the best possible set of QoI to use in the inverse problem before any physical experiment occurs.\n\nThe results of the optimization problems are shown in Table~\\ref{Table:heatplate}.  \nAs expected, each pair of QoI found has one point on the left side of the plate and one point on the right.  \nHowever, there are notable differences in space-time locations of each optimal pair of QoI.  \nThe pair of QoI that minimizes $\\overline{M_{Q^{(z)},N}}$ is produced by measurements made near the end of the simulation.  \nThe pair of QoI that minimizes $\\overline{S_{Q^{(z)},N}}$ is produced by measurements made at the first time step. \n Intuitively, this makes sense because at $t_1$ the temperature measurements are primarily determined by local alloy properties, i.e., not enough time has passed for the left side of the plate to influence the temperature measurements on the right side of the plate and vice versa.\n\nWe solve the inverse problem three times, once for each pair of QoI described above.  \nWe simulate real data, i.e., a temperature measurement made on a given plate with uncertain thermal conductivities in a laboratory experiement, by mapping a random point $\\lambda_{ref}\\in\\Lambda$ forward to $Q^{(z)}(\\lambda_{ref})=q^{(z)}_{ref}\\in\\mathcal{D}^{(z)}$.  \nWe assume the uncertainty in this {\\em observed} datum is the same for each QoI and let this uncertainty be $0.5$ degrees, i.e., the 2-dimensional square we invert for each pair of QoI is centered at $q^{(z)}_{ref}$ and has side length of $0.5$.  \nWe solve the inverse problem using the open source BET \\cite{BET} python package for each of the resulting pairs of QoI, see Figure \\ref{Fig:heatplate_inverse}.\n\nIn the top row we see in purple the inverse image for each pair of QoI approximated with $N=5000$ uniform samples.  \nNotice in the top middle, the pair that minimizes the average skewness, the inverse image is the entire parameter space, i.e., we have not reduced the uncertainty in our model input parameters at all.  \nAlthough this inverse image can be well approximated by relatively few samples, it is useless as it gives no new information about the location of the parameters that produced the observed datum.  \nIn the bottom row we show the location of $y_z\\in Y_\\omega$ for each pair of QoI.  \nIn the bottom right, notice the apparent smooth curve being defined by these these points in $Y_\\omega$.  \nIn multicriteria optimization this is referred to as the Pareto frontier, and we see in all three cases our optimal choice of pair of QoI produces a point in $Y_\\omega$ that lies very near the Pareto frontier.\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n        \\includegraphics[width=.3\\textwidth]{heatplate_inverse_supp_bw_small}\n        \t\t\\includegraphics[width=.3\\textwidth]{heatplate_inverse_skew_bw_small}\n        \t\t\t\t\\includegraphics[width=.3\\textwidth]{heatplate_inverse_sum_bw_small}\\\\\n\n        \t\\includegraphics[width=.3\\textwidth]{heatplate_scatter_supp}\n\t\t\t\\includegraphics[width=.3\\textwidth]{heatplate_scatter_skew}\n        \t\t\t\\includegraphics[width=.3\\textwidth]{heatplate_scatter_sum}\n\t\\caption{\\it (top): The inverse image approximated using the BET package.  (bottom): The space $Y_\\omega$ defined in Section \\ref{Sec:Optimizing}.  (left): The pair of QoI that minimizes $\\overline{M_{Q^{(z)},N}}$.  (middle) : The pair of QoI that minimizes $\\overline{S_{Q^{(z)},N}}$.  (right): The pair of QoI that minimizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$.}\\label{Fig:heatplate_inverse}\n\t\\end{minipage}\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{10pt}\n\t\\begin{tabular}{rrrr}\n\t    QoI Locations & $\\overline{M_{Q^{(z)},N}}$ & $\\overline{S_{Q^{(z)},N}}$ & $d_{Y_\\omega}(p, y_z)$ \\\\ \\hline \\hline\n\t\n\t    $Q_{(p_{18}, t_{14})}, Q_{(p_{17}, t_{15})}$  &  $1.72E-05$ & $1.46E+00$ & $3.31E-01$ \\\\\n\t\n\t\t$Q_{(p_{1}, t_{1})}, Q_{(p_{12}, t_{1})}$ & $3.76E+03$ & $1.00E+00$ & $9.99E-01$ \\\\\n\t\n\t\t$Q_{(p_{17}, t_{4})}, Q_{(p_{18}, t_{4})}$ & $6.26E-04$ & $1.0004E+00$ & $9.84E-04$ \\\\\n\t\n\t\\end{tabular}\n\t\\captionof{table}{\\it $\\overline{M_{Q^{(z)},N}}$, $\\overline{S_{Q^{(z)},N}}$, and $d_{Y_\\omega}(p, y_z)$ for the three pairs of QoI considered in Figure~\\ref{Fig:heatplate_inverse}.}\n\t\\label{Table:heatplate}\n\\end{minipage}\n\\end{figure}\n\n\n\\subsubsection{The Prediction Problem}\\label{Sec:The_Prediction_Problem_heatplate}\n\n\nAs mentioned in Section~\\ref{Sec:Precision_Accuracy}, following solution to the stochastic inverse problem, we generally want to make, and quantify uncertainty in, predictions that can be formulated as solutions to a stochastic forward problem.   \nTypically these predictions correspond to unobservable QoI, e.g., the maximum storm surge elevation for a future hurricane being forced by currently unknown winds and tides.  \nFor simplicity, we consider a prediction problem for a similar QoI in the heating of a thin plate as described above.  \nWe denote the prediction QoI as $Q^{(p)}$ which represents the average temperature along the right side of the plate  at the final time step with the external source now located at the bottom left of the plate.\n\\begin{eqnarray*}\n\tQ^{(p)} = \\frac{1}{\\mu_{\\Omega}(E)}\\int_{\\Omega}T(x;t_{20})\\chi_{E} \\, dx,\\\\\n    f(x) = \n    \\begin{cases}\n    Ae^{-\\frac{(x_0 + \\frac{1}{2})^2 + (x_1 + \\frac{1}{2})^2}{w}} & \\text{if } t \\leq t_{source} \\\\\n    0       & \\text{if } t > t_{source}.\n    \\end{cases}\n\\end{eqnarray*}\nHere, $E$ is a small vertical strip in $\\Omega$ along the right boundary.  \nFor each inverse solution found in Section~\\ref{Sec:The_Inverse_Problem_heatplate} we have a corresponding set of samples from the original $N=5000$ that lie inside the inverse density.  \nFor each of these samples, we run the simulation and evaluate $Q^{(p)}$. \nThe range of the results for each of the three inverse images is shown in Table~\\ref{Table:prediction}.  \nIn the first column we give the pair of QoI used to solve the stochastic inverse problem.  \nIn the second column we show the relative measure of the support of the probability measure solving the stochastic inverse problem.  \nIn the third column we show the interval of possible prediction values for $Q^{(p)}$.  \nWe see the pair of QoI that minimizes the sum of the average $\\mu_\\Lambda$-measure and the average skewness (last row) produces a prediction interval about 5 times smaller than propagating the entire parameter space forward (second row).\nAlso, this pair of QoI produces a prediction interval of similar precision (as determined by length of the interval) to the prediction interval corresponding to the first row, which we would expect to be the best but perhaps the most computationally complex to use. \n\n\\begin{table}\n\\centering\n\\begin{tabular}{ccc}\n    QoI Locations & $\\frac{\\mu_\\Lambda({\\mathop{\\mathrm{supp}}}((Q^{(z)})^{-1}(B)))}{\\mu_\\Lambda(\\Lambda)}$ & Prediction Interval  \\\\ \\hline \\hline\n\n    $Q_{(p_{18}, t_{14})}, Q_{(p_{17}, t_{15})}$  & $6.68E-2$  & [18.3, 24.0] \\\\\n\n\t$Q_{(p_{1}, t_{1})}, Q_{(p_{12}, t_{1})}$ & $1.00E+0$ & [0.06, 25.3] \\\\\n\n\t$Q_{(p_{17}, t_{4})}, Q_{(p_{18}, t_{4})}$ & $8.62E-2$ & [16.9, 23.1] \\\\\n\n\\end{tabular}\n\\captionof{table}{\\it Predictions made by propagating forward each of the three inverse images in Figure~\\ref{Fig:heatplate_inverse}.}\n\\label{Table:prediction}\n\\end{table}\n\n\n\\subsection{Coastal Ocean Model}\\label{Sec:Costal_Ocean}\n\n\n\n\\subsubsection{The Model}\\label{Sec:The_Model_adcirc}\n\n\nCoastal ocean models numerically solve the shallow water equations (SWEs), which model the flow of water processes on domains with vertical length scales that are negligible relative to the horizontal length scales.  Integrating out the depth results in a first-order hyperbolic continuity equation coupled to the momentum equations for horizontal depth-averaged velocities given by\n\\begin{eqnarray}\n\t\\frac{\\partial H}{\\partial t} + \\frac{\\partial}{\\partial x}(Q_x) + \\frac{\\partial}{\\partial y}(Q_y) &=& 0, \\label{Eq:continuity} \\\\ \n\t\\frac{\\partial Q_y}{\\partial t} + \\frac{\\partial UQ_x}{\\partial x} + \\frac{\\partial VQ_x}{\\partial y} - fQ_x &=&\\nonumber\\\\\n\t-gH\\frac{\\partial(\\zeta + P_s/g\\rho_0-\\alpha\\eta)}{\\partial x} &+& \\frac{\\tau_{sx}}{\\rho_0} - \\frac{\\tau_{bx}}{\\rho_0} + M_x - D_x - B_x, \\label{Eq:mom1} \\\\ \n\t\\frac{\\partial Q_x}{\\partial t} + \\frac{\\partial UQ_y}{\\partial x} + \\frac{\\partial VQ_y}{\\partial x} - fQ_x &=&\\nonumber\\\\\n\t-gH\\frac{\\partial(\\zeta + P_s/g\\rho_0-\\alpha\\eta)}{\\partial y} &+& \\frac{\\tau_{sy}}{\\rho_0} - \\frac{\\tau_{by}}{\\rho_0} + M_y - D_y - B_y.\\label{Eq:mom2}\n\\end{eqnarray}\nHere, $\\zeta$ is the free surface departure from the geoid, $h$ is the bottom surface departure from the geoid, $H=\\zeta + h$ is the total water column height, $U_i$ is the depth-averaged velocity in the $x_i$ direction, $f$ is the Coriolis parameter, $P_s$ is the atmospheric pressure at the free surface, $\\rho_0$ is the reference density of water, $\\alpha$ is the effective earth elasticity factor, $\\eta$ is the Newtonian equilibrium tide potential, $\\tau_{sx_i}$ are the imposed surface stresses, $\\tau_{bx_i}$ are the bottom stress components, $M_{x_i}$ is the vertically-integrated lateral stress gradient, $D_{x_i}$ is the momentum dispersion, and $B_{x_i}$ is the vertically-integrated baroclinic pressure gradient.\n\nIn the ADvaced CIRCulation (ADCIRC) model the continuity equation, Equation~\\ref{Eq:continuity}, is replaced by the second order hyperbolic generalized wave continuity equation (GWCE), Eq.~\\eqref{Eq:GWCE} \\cite{Butler2012b}.  \nTogether, the GWCE and the momentum equations, Eqs.~\\eqref{Eq:mom1} and \\eqref{Eq:mom2}, define the modified form of the SWEs solved by ADCIRC.  \nThe GWCE and momentum equations are discretized in space by a piecewise linear triangular mesh.  \nThe time stepping is done with centered finite differences for the GWCE and forward finite difference for the momentum equations.\n\n\\begin{eqnarray}\n\t&&\\frac{\\partial^2\\zeta}{\\partial t^2} + \\tau_0\\frac{\\partial \\zeta}{\\partial t} + \\frac{\\partial}{\\partial x}\\bigg(-\\frac{\\partial UQ_x}{\\partial x} - \\frac{\\partial VQ_x}{\\partial x} + fQ_y - gH\\frac{\\partial(\\zeta + P_s/g\\rho_0-\\alpha\\eta)}{\\partial x}\\nonumber\\\\\n\t&&+ \\frac{\\tau_{sx}}{\\rho_0} - \\frac{\\tau_{bx}}{\\rho_0} + M_x - D_x - B_x + \\tau_0Q_x\\bigg) + \\frac{\\partial}{\\partial y}\\bigg(-\\frac{\\partial UQ_y}{\\partial x} - \\frac{\\partial VQ_y}{\\partial x} + fQ_x\\nonumber\\\\\n\t&&- gH\\frac{\\partial(\\zeta + P_s/g\\rho_0-\\alpha\\eta)}{\\partial y}\t+ \\frac{\\tau_{sy}}{\\rho_0} - \\frac{\\tau_{by}}{\\rho_0} + M_y - D_y - B_y + \\tau_0Q_y\\bigg)\\nonumber\\\\\n\t&&- UH\\frac{\\partial\\tau_0}{\\partial x} - VH\\frac{\\partial\\tau_0}{\\partial y} = 0.\\label{Eq:GWCE}\n\\end{eqnarray}\n\nA driving application of the ADCIRC model is predicting maximum storm surge elevations during extreme weather events such as hurricanes.  \nAccurate estimation of storm surge elevations requires accurate estimation of model parameters.  \nThe Manning's n coefficients of roughness are of particular importance.  \nThey enter into the SWEs through the bottom stress components $\\tau_{bx_i}$ in the momentum equations.  \nThe Manning's n coefficient is a highly variable spatial parameter dependent on the surface characteristics of the seabed and is inherently uncertain.\n\nFor the purposes of this work, the derivation of how the Manning's n coefficient enters into the momentum equation is not particularly important.  \nIn the end, as we change the Manning's n coefficient we see changes in the maximum storm surge elevations in the physical domain.\n\n\\begin{table}\n\\centering\n\\begin{tabular}{ccc}\n    Parameter & Range of Manning's n values & Land Classification  \\\\ \\hline \\hline\n\n    $\\lambda_1$  & $[0.0396, 0.21]$  & Low-intensity developed \\\\\n\n\t$\\lambda_2$ & $[0.0594, 0.315]$ & Evergreen forest \\\\\n\n\t$\\lambda_3$ & $[0.0495, 0.2625]$ & Palustrine forested wetland \\\\\n\n\t$\\lambda_4$ & $[0.00825, 0.04375]$ & Open water \\\\\n\\end{tabular}\n\\captionof{table}{\\it Manning\u00e2\u0080\u0099s n coeffcient ranges for the subdomain.}\n\\label{Table:Mannings}\n\\end{table}\n\n\n\\subsubsection{The Inverse Problem}\\label{Sec:adcirc}\n\n\nWe consider a subdomain of the Golf Coast that has four dominant land classifications and therefore four dominant Manning's n coefficients that enter into the bottom stress components in the momentum equations, see Table~\\ref{Table:Mannings}.  \nWe run ADCIRC to simulate the storm surge under the forcing of Hurricane Gustav and record maximum storm surge elevations at 194 spatial locations, see Figure~\\ref{Fig:subdomain}.  \nOur goal is to determine which set of four spatial locations provides us with data that produces the best solution to the inverse problem.\n\n\\begin{figure}\n    \\begin{center}\n        \n        \\includegraphics[width=.45\\textwidth]{subdomain_194stations_small}\\\\\n        \t\t\\includegraphics[width=.45\\textwidth]{subdomain_optimalstations_small}\n        \t\t\t\\includegraphics[width=.45\\textwidth]{subdomain_suboptimalstations_small}\n        \\caption{\\it (top): 194 potential station locations.  (bottom left): Stations [11, 40, 160, 91] define the optimal observation network.  (bottom right): Stations [16, 40, 22, 88] define a suboptimal observation network.}\\label{Fig:subdomain}\n    \\end{center}\n\\end{figure}\n\n\\begin{figure}\n    \\begin{center}\n        \n        \\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_0_1}\n        \t\t\\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_0_2}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_0_3} \\\\\n\t\t\\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_1_2}\n        \t\t\\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_1_3}\n\t\t\t\t\\includegraphics[width=.24\\textwidth]{marg2D_16_uniform_P_1e6_2D_2_3}\n        \\caption{\\it Plots of the marginals of the inverse solution using uniform samples for optimal stations [11, 40, 160, 191]. Here, $\\rho_{\\mathcal{D}}$ is defined as a uniform density on a small rectangular box centered at the reference QoI values associated with $\\lambda_{ref} = (0.1523, 0.1751, 0.1561, 0.0269)$. The reference value is illustrated by a white circle. (left): In order from top to bottom $(\\lambda_1, \\lambda_2),(\\lambda_1, \\lambda_4),(\\lambda_2, \\lambda_4)$. (right): In order from top to bottom $(\\lambda_1, \\lambda_3),(\\lambda_2, \\lambda_3),(\\lambda_3, \\lambda_4)$.}\\label{Fig:adcirc_optimal}\n        \\textbf{}\\par\\medskip\n        \\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_0_1}\n        \t\t\\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_0_2}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_0_3} \\\\\n\t\t\\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_1_2}\n        \t\t\\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_1_3}\n\t\t\t\t\\includegraphics[width=.24\\textwidth]{bad_marg2D_16_uniform_P_1e6_2D_2_3}\n        \\caption{\\it Plots of the marginals of the inverse solution using uniform samples for suboptimal stations [16, 40, 22, 88]. Here, $\\rho_{\\mathcal{D}}$ is defined as a uniform density on a small rectangular box centered at the reference QoI values associated with $\\lambda_{ref} = (0.1523, 0.1751, 0.1561, 0.0269)$. The reference value is illustrated by a white circle. (left): In order from top to bottom $(\\lambda_1, \\lambda_2),(\\lambda_1, \\lambda_4),(\\lambda_2, \\lambda_4)$. (right): In order from top to bottom $(\\lambda_1, \\lambda_3),(\\lambda_2, \\lambda_3),(\\lambda_3, \\lambda_4)$.}\\label{Fig:adcirc_bad}\n    \\end{center}\n\\end{figure}\n\n\\begin{remark}\n\tThe results from this section are from Lindley Graham's PhD Thesis \\cite{Lindleysthesis} and were computed during the summer of 2015.  The algorithm used to determine the optimal set of QoI has since developed into the methods described in Sections~\\ref{Sec:Precision_Accuracy} and \\ref{Sec:Optimizing}.  Although these results were found using a less tuned algorithm, they still display fundamental characteristics of inverse solutions defined by optimal and suboptimal sets of QoI.\n\\end{remark}\n\nIn Figure~\\ref{Fig:subdomain} we see three plots of the subdomain.  The points in each plot represent particular stations of interest, the smooth coloring of the domain represents the difference in the computationally approximated maximum storm surge elevation over 480 uniform samples in $\\Lambda$.  Notice in the top image we do not place potential station locations in regions of the subdomain that do not display significant sensitivity in storm surge elevations as the Manning's n coefficients (parameters) vary.\n\nIn Figure~\\ref{Fig:adcirc_optimal} we see the marginal densities computed from the inverse solution for the optimal set of QoI seen in the bottom left of Figure~\\ref{Fig:subdomain}.  \nIn Figure~\\ref{Fig:adcirc_bad} we see the marginal densities computed from the inverse solution for the suboptimal set of QoIs seen in the bottom right of Figure~\\ref{Fig:subdomain}.  \nTo clearly draw distinctions between these separate solutions, we examine the top left plots in Figures~\\ref{Fig:adcirc_optimal} and \\ref{Fig:adcirc_bad}.  Note that although both marginal densities contain the reference point $\\lambda_{ref}$ within their supports, the solution from the optimal QoI does a much better job of concentrating high probability in a smaller region of $\\Lambda$ containing this reference point.  \nThe relative measure of the support of the density computed for the optimal set of stations is $\\mu_\\Lambda((Q^{(opt)})^{-1}(B)) / \\mu_\\Lambda(\\Lambda)=7.904E-3$, where as for the suboptimal set $\\mu_\\Lambda((Q^{(subopt)})^{-1}(B)) / \\mu_\\Lambda(\\Lambda)=1.917E-2$.  Using the optimal set of stations has reduced the relative measure of the support of the inverse density by approximately a factor of 3.\n\n\n\\section{\\bf Conclusion}\\label{Sec:Conclusion}\n\n\nIn \\cite{Butler2015b} the local skewness of the inverse image of a generalized rectangle in some $\\mathcal{D}$ was defined.  \nWe have extended this initial work to quantify the global effect of skewness which is a way of quantifying the accuracy we can obtain in solution to the stochastic inverse problem from a finite number of samples.  \nWe also developed a way to quantify the precision in solutions to stochastic inverse problems in terms of measures of supports of densities and/or high probability events.  \nA multicriteria optimization problem was defined to determine the best QoI map from the space of theoretically possible QoI that simultaneously reduces both the skewness and measure of the support of the inverse density.  \nSeveral numerical examples demonstrated the effect of the QoI choice on the solution to the stochastic inverse problem. \n\n\n\\vskip 10pt\n{\\it Future Work}\n\\vskip 10pt\n\n\nAs mentioned in a previous remark, the choice of weights used in Eq.~\\eqref{Eq:dist_suppskew} is a current topic of discussion.  \nIt is clear as more computational resources are available we desire to reduce the $\\mu_\\Lambda$-measure of the inverse image more than the skewness, i.e., we set $\\omega\\ll 1$. \nA fundamental problem is to determine if the value of $\\omega$ can be solved for directly as a function of the number of model solves available.\n\nIn this work we considered the discrete optimization problem, i.e., given $m$ sensors and $d$ theoretical QoI to choose from, determine the optimal set to improve solutions to the corresponding stochastic inverse problem.  \nThe continuous analogue is simply choose $m$ optimal QoI from infinitely many possibilities.  \nAs a concrete example, suppose that for the PDE in Section~\\ref{Sec:heatplate} we can place two temperature sensors at {\\em any} point in space-time, and we wish to choose the best two points in space-time to record data.\nThis problem is approached by considering the function $R : \\mathcal{Q}\\rightarrow Y_\\omega$, where $\\mathcal{Q}$ is now the set of infinitely many possible sets of QoI, and finding local minima of this function.  \nIn the case of QoI being represented by some specific type of functional at a point in space time (temperature measurement), the space $\\mathcal{Q}$ has dimension $(\\dim(\\Omega) + 1) \\times m$ where $\\Omega$ is the spatial domain of the model and $m$ is the number of QoI to be chosen.  Although the dimension space $\\mathcal{Q}$ increases quickly as $m$ increase, there appears to be symmetry to exploit in this space.\n\nIn high dimensional parameter spaces any set implicitly defined by the solution to a stochastic inverse problem, skewed or not, is difficult to approximate with a reasonably low number of samples.  \nIn \\cite{Adaptive} adaptive sampling algorithms are used to combat this curse of dimensionality.  With the ability to place samples near the boundary of sets in $\\Lambda$ the error in the approximation of these sets is greatly reduced.  With the addition of gradient information the efficiency of these adaptive sampling algorithms is improved.  Preliminary results indicate the efficiency is improved further as the skewness of the set is reduced.  A thorough numerical exploration of the effects of skewness on the efficiency of adaptive sampling algorithms is an obvious path to explore.\n\nIn many driving applications the collection of field data is difficult, time consuming, and expensive.  Obtaining a good solution to the inverse problem while using as little data as possible is greatly desired.  In the examples in Section~\\ref{Sec:Map_Defined} we look to determine the best set of $m$ QoI to use to solve the inverse problem, however, possibly a set of $m-1$ QoI produces a very similar inverse solution, or a set of $m-2$.  This information can greatly reduce the resources needed to gather the field data required.  With the $M_{Q^{(z)}}$ and $S_{Q^{(z)}}$ defined as in Section~\\ref{Sec:Precision_Accuracy}, can we use $M_{Q^{(z),k}}$ and $S_{Q^{(z),k}}$ (the $\\mu_\\Lambda$-measure and skewness of the map $Q^{(z)}$ without the $k^{th}$ QoI) to draw conclusions about the maximum number of useful QoI to use from a given set?\n\n\n\\pagebreak\n\\bibliographystyle{plain}\n\\bibliography{masters_report.v4.arxiv}\n\n\n", "itemtype": "equation", "pos": 59975, "prevtext": "\nwhere $p=(1, 0)$ is the {\\it ideal point} and $y_z=(\\overline{S_{Q^{(z)}}}, \\overline{M_{Q^{(z)}}})$.  \n\n\n\n\\subsection{Discrete Optimization}\\label{Sec:Discrete}\n\n\nSuppose $\\mathcal{Q}$ is a finite family of possible QoI maps, e.g., as defined by identifying a finite set of $d$ physically possible configurations of $m$ sensors with $d\\geq m$. \nThe solution to the minimization problem defined by Eq.~\\eqref{eq:optimal_Q} can be found by an exhaustive search through the $d \\choose m$ possible QoI maps.\nThis combinatorial problem can clearly become computationally expensive as the number of possible maps gets large. \nHowever, it is completely straightforward to implement and is embarrassingly parallel. \nIn the concluding remarks of Section~\\ref{Sec:Conclusion}, we describe some possible future directions to mitigate the cost of solving the optimization problem. \nWe provide two numerical examples to clarify this method of determining an optimal QoI map to use in the inverse problem.\n\n\\begin{remark}\nWe note that another equivalent way to frame the discrete optimization problem is to define a ``theoretical'' QoI map $Q:\\Lambda\\to\\mathcal{D}\\subset\\mathbb{R}^d$ where the goal is to determine the ``practical'' QoI map given by a subset of $m$ components of the map $Q$.\nThe advantage of this approach from an implementation point of view is that for each sample in $\\Lambda$, the model is solved once in order to compute all the possible QoI values.\nIt also provides an index to each possible QoI making the description of the optimal QoI more straightforward.\nTherefore, we use this in the numerical examples below. \n\\end{remark}\n\n\n\\subsection{Numerical Examples}\\label{Sec:1}\n\n\n\n\\subsubsection{Linear Example}\\label{Sec:Linear}\n\n\nLet $Q : {{\\rm l} \\kern -.15em {\\rm R} }^2 \\rightarrow {{\\rm l} \\kern -.15em {\\rm R} }^3$,\n\\begin{eqnarray}\n    Q = \\begin{mat} 0.5 & 0.5 \\\\\n        \t\t 2.5 & 0.5 \\\\\n        \t\t -0.2 & 0.3  \\end{mat}.\n\\end{eqnarray}\nNotice the rows of $Q$ are pairwise linearly independent, i.e., pairwise GD.  Let $B^{(z)}\\in \\mathcal{B}_{\\mathcal{D}^{(z)}}$ be the rectangle of uncertainty in the data space defined by the $z^{th}$ possible pair of QoI.  For simplicity, we let the uncertainty in each QoI be the same, i.e., $\\mu_{\\mathcal{D}^{(z)}}(B^{(z)})$ is constant for all $z$.  \nNote that because $Q$ is linear we can easily use the exact Jacobian of $Q$.  \nFurthermore, we use the exact averages $\\overline{M_{Q^{(z)}}}$ and $\\overline{S_{Q^{(z)}}}$ not the approximate averages. \nThe linearity of $Q$ implies $M_{Q^{(z)}}(\\lambda^{(i)})$ and $S_{Q^{(z)}}(\\lambda^{(i)})$ are each constant for all $i$, so we need not numerically approximate the integrals in Eqs.~\\eqref{Eq:suppint} and \\eqref{Eq:skewint}.  \nIn Section~\\ref{Sec:Nonlinear} we consider a nonlinear map and compute approximations of the Jacobian, $\\overline{M_{Q^{(z)}}}$, and $\\overline{S_{Q^{(z)}}}$.\n\nWe see in Figure \\ref{Fig:linear_2to3} and Table~\\ref{Table:linear_2to3} the optimal choice of pair of QoI to use in the inverse problem for three different optimization problems; minimize $\\overline{{M_{Q^{(z)}}}}$, minimize $\\overline{{S_{Q^{(z)}}}}$, and minimize Eq.~\\eqref{Eq:dist_suppskew} with $\\omega = 0.5$.  In the top row of Figure~\\ref{Fig:linear_2to3} we see the inverse image of $B^{(z)}$ as the intersection of the red and blue contour events corresponding to the individual components of the possible QoI maps.  \nIt is visually evident from the top-left plot of Figure~\\ref{Fig:linear_2to3} that the pair of QoI that minimizes $\\overline{M_{Q^{(z)}}}$ also produces the inverse image with the sharpest corners, i.e., it has the highest skewness.  \nIn the top-middle plot of Figure~\\ref{Fig:linear_2to3} we observe the opposite effect, the pair of QoI that minimizes the skewness $\\overline{S_{Q^{(z)}}}$ maximizes $\\overline{M_{Q^{(z)}}}$. \nIn the top-right plot of Figure~\\ref{Fig:linear_2to3}, we observe a pair of QoI that produces reasonably low $\\overline{S_{Q^{(z)}}}$ and $\\overline{M_{Q^{(z)}}}$ simultaneously.\n\n\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n        \\includegraphics[width=.3\\textwidth]{linear_2to3_01}\n\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_02}\n\t\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_12}\\\\\n\t\t\t        \n        \t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter01}\n\t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter02}\n        \t\t\t\\includegraphics[width=.3\\textwidth]{linear_2to3_scatter12}\n\t\\caption{\\it On the top we show the inverse density for the three possible pairs of QoI.  On the bottom we show the point in $Y_\\omega$ defined by each pair of QoI.  (left): The pair of QoI that minimizes $\\overline{M_{Q^{(z)}}}$ ($Q_{0,1}$).  (middle): The pair of QoI that minimizes $\\overline{S_{Q^{(z)}}}$ ($Q_{0,2}$).  (right): The pair of QoI that minimizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{1,2}$). }\\label{Fig:linear_2to3}\n\t\\end{minipage}\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{10pt}\n\t\\begin{tabular}{rrrr}\n  \t\tPair & $\\overline{M_{Q^{(z)}}}$ & $\\overline{S_{Q^{(z)}}}$ & $d_{Y_\\omega}(p, y_z)$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$Q_{0,1}$ &  $4.0E-2$ & $1.80E+0$ & $4.84E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{0,2}$ & $1.6E-2$ & $1.02E+0$ & $1.57E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,2}$ & $4.7E-2$  & $1.08E+0$ & $1.20E-1$\\\\\n\t\\end{tabular}\n\t\\captionof{table}{\\it $\\overline{M_{Q^{(z)}}}$, $\\overline{S_{Q^{(z)}}}$, and $d_{Y_\\omega}(p, y_z)$ for each of the three possible pairs of QoI.}\n\t\\label{Table:linear_2to3}\n\\end{minipage}\n\\end{figure}\n\n\n\\subsubsection{Nonlinear}\\label{Sec:Nonlinear}\n\n\n\\begin{figure}\n\t\\centering\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{0pt}\n        \\includegraphics[width=.24\\textwidth]{nonlinear_2to10_supp}\n        \t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_skew}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_sumskewsupp}\n        \t\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_WORST}\\\\\n        \t\t\t\t\n        \t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_supp_scatter}\n\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_skew_scatter}\n        \t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_sumskewsupp_scatter}\n        \t\t\t\t\\includegraphics[width=.24\\textwidth]{nonlinear_2to10_WORST_scatter}\n\t\\caption{\\it On the top we show the inverse image defined by four possible pairs of QoI.  On the bottom we show the point in $Y_\\omega$ defined by each pair of QoI.  (left): The pair of QoI that minimizes $\\overline{M_{Q^{(z)},N}}$ ($Q_{5,9}$).  (middle-left): The pair of QoI that minimizes $\\overline{S_{Q^{(z)},N}}$ ($Q_{1,4}$).  (middle-right): The pair of QoI that minimizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{1,7}$).  (right): The pair of QoI that maximizes the distance defined in Eq.(\\ref{Eq:dist_suppskew}) with $\\omega=0.5$ ($Q_{0,8}$).}\\label{Fig:nonlinear_2to10}\n\t\\end{minipage}\n\t\\begin{minipage}[t]{\\textwidth}\n\t\\centering\n\t\\vspace{10pt}\n\t\\begin{tabular}{rrrr}\n  \t\tPair & $\\overline{M_{Q^{(z)},N}}$ & $\\overline{S_{Q^{(z)},N}}$ & $d_{Y_\\omega}(p, y_z)$ \\\\ \\hline \\hline\n\t\t\\vspace{2pt}\n  \t\t$Q_{5,9}$ &  $1.50E-2$ & $1.482E+0$ & $3.40E-1$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,4}$ & $2.60E-2$ & $1.040E+0$ & $6.40E-2$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{1,7}$ & $2.10E-2$  & $1.044E+0$ & $6.30E-2$\\\\\n\t\t\\vspace{2pt}\n\t\t$Q_{0,8}$ & $1.33E-1$  & $4.016E+0$ & $8.68E-1$\\\\\n\t\\end{tabular}\n\t\\captionof{table}{\\it $\\overline{M_{Q^{(z)},N}}$, $\\overline{S_{Q^{(z)},N}}$, and $d_{Y_\\omega}(p, y_z)$ for the four pairs of QoI considered in Figure~\\ref{Fig:nonlinear_2to10}.}\n\t\\label{Table:nonlinear_2to10}\n\\end{minipage}\n\\end{figure}\n\nLet $Q : {{\\rm l} \\kern -.15em {\\rm R} }^2 \\rightarrow {{\\rm l} \\kern -.15em {\\rm R} }^{10}$ be {\\em nonlinear} where each component is a polynomial of the form\n\\begin{eqnarray*}\n\tQ_j = r_{j,0}\\lambda_1^5 + r_{j,1}\\lambda_2^3 + r_{j,2}\\lambda_1^3\\lambda_2 + r_{j,3}\\lambda_1 + r_{j,4}\\lambda_2 + r_{j,5} \\, \\, \\text{for}\\, \\, j=0,1,\\hdots,9,\n\\end{eqnarray*}\nwhere the coefficients $r_{j,k}$ are fixed random numbers in $[-1,1]$.  Let $B^{(z)}\\in \\mathcal{B}_{\\mathcal{D}^{(z)}}$ be the rectangle of uncertainty in the data space defined by the $z^{th}$ possible pair of QoI.  \nAgain, we assume the uncertainty in each QoI is the same so that $\\mu(B^{(z)})$ is constant for all $z$.  \nNow that $Q$ is nonlinear, we approximate both the local Jacobians $J_{\\lambda^{(i)}}$ and the integrals in Eqs.~\\eqref{Eq:suppint} and \\eqref{Eq:skewint}.  \nWe take 1000 uniform random samples $\\lambda^{(i)}$ in $\\Lambda$ and then compute the approximate Jacobian at $N=100$ of these samples using a Radial Basis Function (RBF) interpolation method.  \nSpecifically, for a given $\\lambda^{(i)}\\in\\Lambda$, we use the 20 nearest neighbors of $\\lambda^{(i)}$ (from the original 1000 samples) to approximate the gradient for each QoI and subsequently construct the Jacobian.\n\n\\begin{remark}\n\tBecause $Q$ is a vector valued polynomial function, we could analytically determine the exact gradient vectors for each component and simply evalute at each $\\lambda^{(i)}$.  However, the RBF method (or any finite difference method) is a more general approach that can be used to approximate gradient vectors for maps $Q$ defined by functionals of solutions to partial differential equations where we cannot analytically determine the gradients of each component.\n\\end{remark}\n\nWe illustrate the various parts of the optimization problem by separately solving four optimization problems; minimize $\\overline{{M_{Q^{(z)},N}}}$, minimize $\\overline{{S_{Q^{(z)},N}}}$, minimize Equation~\\ref{Eq:dist_suppskew} with $\\omega= 0.5$, and maximize Equation~\\ref{Eq:dist_suppskew} with $\\omega=0.5$.  \nThe results are summarized in Figure~\\ref{Fig:nonlinear_2to10} and Table~\\ref{Table:nonlinear_2to10}.  \nIn the bottom left of Figure~\\ref{Fig:nonlinear_2to10} we see the location in $Y_\\omega$ of $s_{5,9}=(\\overline{S_{Q_{5,9},N}}, \\overline{M_{Q_{5,9},N}})$.  Although this pair of QoI minimizes $\\overline{M_{Q^{(z)},N}}$ it is obvious from the scatter plot there are pairs of QoI that have similar average $\\mu_\\Lambda$-measure that have much smaller average skewness.  \nIn the bottom right we see the location in $Y_\\omega$ of $s_{0,8}$ that corresponds to the pair of QoI that maximizes the sum of the average $\\mu_\\Lambda$-measure and the average skewness.  \nThe corresponding inverse image, seen in the top right of Figure~\\ref{Fig:nonlinear_2to10}, clearly has both the largest measure of support and the largest skewness of the four pairs considered.\n\n\n\\section{\\bf Maps Defined by the Solution of a Physics-Based Model}\\label{Sec:Map_Defined}\n\n\nIn this section we consider the inverse density of sets under functions defined by the solutions to partial differential equations.  \nWe first consider a simple time dependent diffusion model with uncertain diffusion coefficient that allows us to understand the results intuitively.\nThen we consider the ADvaned CIRCulation model for oceanic, coastal, and estuarine waters (ADCIRC) which incorporates a spatially varying bottom friction model.  \nThe ADCIRC model depends on many parameter fields, we focus on the bottom friction parameter due to its inherent uncertainty.\n\n\n\\subsection{Time Dependent Diffusion}\\label{Sec:heatplate}\n\n\n\n\\subsubsection{The Model}\\label{Sec:The_Model}\n\n\nWe consider the time dependent diffusion equation\n\\vskip 10pt\n\n", "index": 59, "text": "\\begin{equation*}\n    \\rho c\\frac{\\partial T}{\\partial t} = \\nabla \\cdot (\\kappa \\nabla T) + f, \\hskip 10pt x \\in \\Omega,  t \\in (t_0, t_f)\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\rho c\\frac{\\partial T}{\\partial t}=\\nabla\\cdot(\\kappa\\nabla T)+f,\\hskip 10.0%&#10;ptx\\in\\Omega,t\\in(t_{0},t_{f})\" display=\"block\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>T</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac></mrow><mo>=</mo><mrow><mrow><mo>\u2207</mo><mo>\u22c5</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mrow><mo>\u2207</mo><mo>\u2061</mo><mi>T</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>f</mi></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mi>x</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow><mo>,</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>0</mn></msub><mo>,</mo><msub><mi>t</mi><mi>f</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]