[{"file": "1601.00159.tex", "nexttext": "\n    where $\\kappa$ is a\n    key, $p$ is the pointer to the value associated with $\\kappa$, and\n    $\\Gamma$ is the height of $\\kappa$, representing the number of linked lists\n    where key $\\kappa$ is present. We say a key $\\kappa \\in S$ if there exists\n    a data node $\\alpha$ in the storage layer of the index, $S$,\n    such that $\\alpha.\\kappa = \\kappa$.\n\\label{def::datanode}\n\\end{Def}\n\nThe difference between a traditional skip list and PI lies in the index layer.\nIn a traditional skip list, the node of a composing linked list of the index\nlayer contains only one key.\nIn contrast, a fixed number of keys are included in a list\nnode of the index layer, which we shall call an \\textit{entry} hereafter. The\nreason for this arrangement of keys is that it enables SIMD processing, which\nrequires operands to be contiguously stored.\nAn instance of PI with four keys in an entry is shown in Figure~\\ref{fig::arch},\n\n\twhere three different operations are collectively\n    processed by two threads, which are represented by the two arrows\n    colored purple and green, respectively.\n\n\n\nGiven an initial dataset,\nthe index layer can be constructed in a bottom up manner.\nWe only need to scan the storage layer once to fill the high-level\nentries as well as the associated data structure, i.e., routing table,\nwhich will be discussed in the next section in detail.\nThis construction process is $O(n)$ where $n$ is the number of records \n\nat the storage level, typically taking less than 0.2s for 16M\nrecords when running on a 2.0 GHz CPU core.\n\n\nFurther, the construction process can be parallelized,\nand hence can be sped up using more computing resources.\n\n\n\\subsection{Queries and Algorithms}\\label{subquery}\n\nPI, like most indexing structures, supports three types of queries, namely search,\ninsert and delete. Their detailed descriptions are as follows, and an\nabstraction of them is further given in Definition~\\ref{def::query}. \n\n\\begin{itemize}\n    \\item\\textbf{Search($\\kappa$):}\n        \n        \n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, \n        $\\alpha.p$ will be returned, and null otherwise. \n        \n        \n        \n\t\\item\\textbf{Insert($\\kappa$, $p$):}  \n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, \n        update the this data node by replacing\n        $\\alpha.p$ with $p$; \n        otherwise insert a new data node $(\\kappa, p, \\Gamma)$ \n        into the storage layer,\n        where $\\Gamma$ is drawn from a geometrical\n        distribution with a specified probability parameter.\t\n\t\\item\\textbf{Delete($\\kappa$):}\n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, remove \n        this data node from the storage layer and return 1;\n        otherwise return $null$.\n\\end{itemize}\n\n\\begin{Def}\n    A query, denoted by $q$, is a triplet \n    \n", "itemtype": "equation", "pos": 18301, "prevtext": "\n\t\n\n\n\n\n\n\n\n\n\\title{PI : a Parallel in-memory skip list based Index}\n\n\\author{\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\tZhongle Xie$^{*}$, Qingchao Cai$^{*}$, H.V. Jagadish$^{+}$, Beng Chin Ooi$^{*}$, Weng-Fai Wong$^{*}$ \\\\\n\t\\affaddr{$^{*}$National University of Singapore} \\quad\n\t\\affaddr{$^{+}$University of Michigan}  \\\\\n\t\\email{$^{*}$\\{xiezhongle, caiqc, ooibc, wongwf\\}@comp.nus.edu.sg}\\quad\n\t\\email{$^{+}$jag@umich.edu}\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\\begin{abstract}\n    Due to the coarse granularity of data accesses and the heavy use of latches, indices in \n    the B-tree family are not \n    \n    efficient\n    for in-memory databases, especially in the context of today's \n    multi-core architecture.\n\n    In this paper, we present \\textbf{PI}, a \\textbf{P}arallel in-memory skip list based \\textbf{I}ndex\n    that lends itself naturally to the parallel and concurrent environment, particularly with non-uniform memory access.  In PI,\n    incoming queries are collected, and disjointly distributed among\n    multiple threads for processing to avoid the use of latches.\n    For each query, PI traverses the index in a Breadth-First-Search (BFS)\n    manner to find the list node with the matching key, exploiting SIMD processing\n    to speed up the search process.\n    In order for query processing to be latch-free, PI employs a light-weight\n    communication protocol that enables threads to re-distribute the query workload\n    among themselves such that each list node that will be modified as a result of\n    query processing will be accessed by exactly one thread.\n    We conducted extensive experiments, and the results show that PI can be up to three times\n    as fast as the Masstree, a state-of-the-art B-tree based index.\n\n\\end{abstract}\n\n\n\n\\keywords{Database index, skip list, B-tree, parallelization}\n\n\\section{Introduction}\\label{sec::intro}\n\nDRAM has orders of magnitude higher bandwidth and lower\nlatency than hard disks, or even flash memory for that matter.  \nWith exponentially increasing memory sizes and falling prices, \nit is now frequently possible\nto accommodate the entire database and its associated indices in memory,\nthereby completely eliminating the significant overheads of slow disk\naccesses \\cite{diaconu2013hekaton, kallman2008h, kemper2011hyper,\nsikka2012efficient, tu2013speedy}.\nNon-volatile memory such as phase-change memory looming on the horizon\nis destined to push the envelope further.\nTraditional database indices, e.g., B$^+$-trees \\cite{comer1979ubiquitous}, \nthat were mainly optimized for disk accesses,\nare no longer suitable for in-memory databases since\nthey may suffer from poor cache utilization due to their hierarchical\nstructure, \ncoarse granularity of data access and poor parallelism.\n\nThe integration of multiple cores into a single CPU chip makes many-core\ncomputation a norm today. Ideally, an in-memory database index\nshould scale with the number of on-chip cores to fully\nunleash the computing power.\nThe\nB$^+$-tree index, however, is ill-suited\nfor such a parallel environment.\nSuppose a thread is\nabout to modify an intermediate node in a B-tree index. \nIt should first prevent other\nconcurrent threads from descending into the sub-tree rooted at that node\nin order to guarantee correctness, thereby forcing serialization among these threads.\n\nWorse, if the root node is being updated, all of the\nother threads cannot proceed to process queries.\n\n\nConsequently, \nthe traditional B-tree index does not provide the parallelism \nrequired for effective use of the concurrency provided by a many-core environment.\n\nOn the other hand, \n{\\em single instruction multiple data} (SIMD) is now supported by almost all\nmodern processors. It enables performing the same computation, e.g.,\narithmetic operations and comparisons, on multiple data simultaneously,\nholding the promise of significantly reducing the time complexity of computation.\nHowever, to operate on indices using SIMD requires a non-trivial rethink\nsince SIMD operations require operands to be contiguously stored in memory.\n\nThe aforementioned issues highlight the need for a new parallelizable\nin-memory index, and\nmotivate us to re-examine the {\\em skip list}~\\cite{pugh1990skip}\nas a possible candidate as\nthe base indexing structure in place of the B$^+$-tree (or B-tree).\nSkip list employs a probabilistic\nmodel to build multiple linked lists such that each linked list consists of\nnodes selected according to this model from the list at the next level.\nLike B$^+$-trees,\n\n\n\nthe search for a query key in a skip list follows\nbreadth-First traversal in the sense that it starts from the list\nat the top level and moves forward\nalong this level until a node with a larger\nkey is encountered. Thereupon, the search moves to \nthe next level, and proceeds as it did in the previous level.\nHowever, since only one node is being touched by the search\nprocess at any time, its predecessors at the same level\ncan be accessed by another search thread, which means data access in\nthe skip list has a finer granularity than the B$^+$-tree where an intermediate tree\nnode contains multiple keys, and should be accessed in its entirety.\nMoreover, with a relaxation on the structure hierarchy,\na skip list can\nbe divided vertically into disjoint partitions,\neach of which can\nbe individually processed on multi-core systems.\n\nHence, we can expect\nthe skip list to be an efficient and much more suitable indexing technique for concurrent settings.\n\nIt is natural to use latches to implement concurrent accesses over a given\nskip list.\nLatches, however, are costly.\nIn fact, merely inspecting a latch \nmay require it to be flushed from other caches, and is therefore costly.\nLatch modification is even more expensive, \nas it will invalidate the latch\nreplicas located at the caches of other cores,\nand force the threads\nrunning on these cores to re-read the latch for inspection, incurring\nsignificant bandwidth cost, \nespecially in a Non-Uniform Memory Access (NUMA) architecture where accessing\nmemory of remote NUMA nodes can be an order of magnitude slower than\nthat of a local NUMA node.\n\nIn this paper, we propose a highly\nparallel in-memory database index based on a latch-free skip list.\nWe call it \\textbf{PI}, a \\textbf{P}arallel in-memory skip list based \\textbf{I}ndex.\n\n\nIn PI,\nwe employ a fine-grained processing strategy to avoid using latches.\nQueries are organized into batches and each batch processed simultaneously \n\nusing multiple threads.\nGiven a query, PI traverses the index\nin a breadth-first manner to find the corresponding list node. \nSIMD instructions are used to accelerate the search process.\nTo handle the case in which two threads find the same list node for some keys,\n\nPI adjusts the query workload among \nexecution threads to ensure that each list node that will be modified as a\nresult of query processing is accessed by exactly one thread, thereby\neliminating the need for latches.\n\n\n\n\n\n\nOur main contributions include:\n\\begin{itemize}\n\t\\item We propose a latch-free skip list index that shows high\n        performance and scalability. It serves as an\n        alternative to tree-like indices for in-memory databases and suits\n        the many-core concurrent environment due to its high degree of parallelism.\n\n\t\\item We use SIMD instructions to accelerate query processing of the index.\n        \\item  A set of optimization techniques are employed in the proposed\n            index to enhance the performance of the index in\n            many-core environment.\n            \n            \n            \n\n\t\\item We conduct an extensive performance study on PI as well as a\n        comparison study between PI and Masstree~\\cite{mao2012cache}, a\n        state-of-the-art index used in SILO~\\cite{tu2013speedy}.\n        \n\t    \n\t    \n        \n        \n        The results show that PI is able to perform\n        up to more than $3\\times$ better than Masstree in terms of query throughput.\n\\end{itemize}\n\nThe remainder of this paper is structured as follows.\nSection~\\ref{sec::relate}\npresents related work.\nWe describe the design and implementation of PI in\nSection~\\ref{sec::index} and \\ref{sec::impl}, respectively, and develop a\nmathematical model to analyze PI's performance of query processing in\nSection~\\ref{sec::model}.\nSection~\\ref{sec::eval} presents the performance study of PI.\nSection~\\ref{sec::conclusion} concludes this work.\n\n\\section{Related Work}\\label{sec::relate}\n\\subsection{\\text{B$^+$-tree}}\n\nThe B$^+$-tree \\cite{comer1979ubiquitous} is perhaps the most widely used index in\ndatabase systems. \n\n\nHowever, it has two fundamental problems\nwhich render it inappropriate for in-memory databases \n\n\n\n\nFirst, its hierarchical structure leads to poor cache utilization which\nin turn seriously restricts its query performance. \nSecond, it does not\nsuit concurrent environment well\ndue to its coarse granularity of data access\nand heavy use of latches. \nIn order to solve those drawbacks, exploiting cache and removing latches \nbecome the core direction for implementing in-memory B$^+$-trees.\t\n\n\\subsubsection{Cache Exploitation}\nA better cache utilization can substantially enhance the query performance of\nB$^+$-trees since reading a cache line from the cache is much faster than\nfrom memory. \nSoftware prefetching is used in \\cite{chen2001improving} to\nhide the slow memory access. \nRao et al.\\ \\cite{conf/vldb/RaoR99} presents a cache-sensitive search tree\n(CSS-tree), where nodes are stored in a contiguous\nmemory area such that the address of each node can be arithmetically computed,\neliminating the use of child pointers in each node. \nThis idea is further applied to B$^+$-trees, and the resultant structure, \ncalled the CSB$^+$-tree, \n\n\nis able to\nachieve cache consciousness and meanwhile support efficient update. The\nrelationship between the node size and cache performance of the CSB$^+$-tree is\nanalyzed in~\\cite{hankins2003effect}. \nMasstree \\cite{mao2012cache} is a trie of B$^+$-trees to efficiently handle\nkeys of arbitrary length. \nWith all the optimizations presented in \n\\cite{chen2001improving, conf/vldb/RaoR99, rao2000making} enabled, Masstree can \nachieve a high query throughput. However, its performance is still restricted \nby the locks upon which it relies to update records.\n\n\tIn addition, Masstree \nis NUMA agnostic as NUMA-aware techniques have been shown to be not providing\nmuch performance gain~\\cite{mao2012cache}. As a result, expensive remote memory \naccesses are incurred during query processing.\n\n\n\\subsubsection{Latch and Parallelizability}\nThere have been many works trying to improve the performance of the\nB$^+$-tree by avoiding latches or enhancing parallelizability.\n\nThe B$^{link}$-tree~\\cite{lehman1981efficient}\n\nis an early attempt towards enhancing the\nparallelizability of B$^+$-trees\n by adding to each node, except the rightmost ones,\nan additional pointer\npointing to its right sibling node so that a node being modified does not\nprevent itself from being read by other concurrent threads. \nHowever, Lomet\n\\cite{lomet2004simple} points out that the\ndeletion of nodes in B$^{link}$-trees can incur a decrease in performance and\nconcurrency, \nand addresses this problem through the use of additional state\ninformation and latch coupling.\n\n\tBraginsky and Petrank present a lock-free B$^+$-tree implemented\n    with single-word CAS instructions~\\cite{braginsky2012lock}.\n\tTo achieve a dynamic structure, the nodes being split or merged will be\n    frozen, but search operations are still allowed to perform against\n    such frozen nodes.\n\n\nDue to the overhead incurred by latches, a\nlatch-free B$^+$-tree has also\nattracted some attention. \nSewall et al. propose a latch-free concurrent\nB$^+$-Tree, named PALM~\\cite{sewall2011palm}, \nwhich adopts bulk synchronous\nparallel (BSP) model to process queries in batches. \nQueries in a batch are disjointly\ndistributed among threads to eliminate the synchronization among them and\nthus the use of latches. \nFAST~\\cite{kim2010fast} uses the same model, and\nachieves twice query throughput as PALM at a cost of not being able to make\nupdates to the index tree. \nThe Bw-tree~\\cite{levandoski2013bw}, developed for\nHekaton~\\cite{diaconu2013hekaton, levandoski2014indexing}, \nis another\nlatch-free B-tree which manages its memory layout in a \nlog-structured manner and\nis well-suited for flash solid state disks (SSDs) where random writes \nare costlier than sequential\nones.\n\n\\subsection{CAS Instruction and Skip List}\nCompare-and-swap (CAS) instructions are atomic operations introduced \nin the concurrent environment\nto ease the implementation of synchronization primitives, \nsuch as semaphores and mutexes.\n Herlihy proposes a sophisticated model to show that CAS instructions can be used in implementing wait-free data structures~\\cite{herlihy1991wait}. \n These instructions are not the only means to realize concurrent data structures. \n Brown et al. also present a new set of primitive operations \n for the same purpose~\\cite{brown2013pragmatic}.\n\nSkip lists~\\cite{pugh1990skip} are considered to be an alternative to B$^+$-trees.\nCompared with B$^+$-trees, a skip list has approximately the same average search\nperformance, but requires much less effort to implement. \nIn particular, even a latch-free implementation, which is notoriously difficult\nfor B$^+$ trees, can be easily achieved for skip lists by using\nCAS instructions~\\cite{herlihy2012art}. \nCrain et al. propose new skip list algorithms~\\cite{crain2013no}\nto avoid contention on hot spots.\nAbraham et al. combine skip lists and B-trees for efficient query processing~\\cite{abraham2006skip}.\nIn addition, skip lists can also be integrated into distributed settings.\nAspnes and Shah present Skip Graph~\\cite{aspnes2007skip} for peer-to-peer systems,\n a novel data structure leveraging skip lists to support fault tolerance.\n\nAs argued earlier,\nskip lists are more parallelizable than B$^+$-trees because of the\nfine-grained data access and relaxed structure hierarchy.\nHowever, na\\\"{\\i}ve linked list based implementation of skip lists have\npoor cache utilization due to the nature of linked lists.\n In PI, we address this\nproblem by separating the Index Layer from the Storage Layer such that the\nlayout of the Index Layer is \noptimized for cache utilization and hence enables an efficient search of keys.\n\n\n\n\\subsection{Single Instruction Multiple Data}\nSingle Instruction Multiple Data (SIMD) processing has been extensively used in\ndatabase research to boost the performance of database operations. \nChhugani et\nal.~\\cite{chhugani2008efficient} show how the performance of mergesort, \na classical sort algorithm, can be improved, when equipped with SIMD.\nSimilarly, it has been shown in \\cite{zhou2002implementing, balkesen2013main,\nbalkesen2013multi} that the SIMD-based\nimplementation of many database\noperators, including scan, aggregation, indexing and join,\nperform much better than its non-SIMD counterpart. \n\nRecently, tree-based in-memory indices leveraging SIMD have been proposed to\nspeed up query processing \\cite{kim2010fast, sewall2011palm}.\nFAST~\\cite{kim2010fast}, a read only binary tree, can achieve an extremely high\nthroughput of query processing as a consequence of SIMD processing and enhanced cache\nconsciousness enabled by a carefully designed memory layout of\ntree nodes. \nAnother representative of\nSIMD-enabled B$^+$-trees is PALM~\\cite{sewall2011palm}, which overcomes the\nFAST's limitation of not being able to support updates\nat the expense of decreased query throughput.\n\n\\subsection{NUMA-awareness}\n\nNUMA architecture opens up opportunities for optimization in terms of\ncache coherence and memory access, which can significantly hinder the\nperformance if not taken into \n\n\ndesign\n\n~\\cite{li2013numa}~\\cite{blagodurov2010case}.\nSince accessing the memory affiliated with remote (non-local) NUMA nodes is\nsubstantially costlier than accessing local memory,\nthe major direction for\nNUMA-aware optimization is to reduce the accesses of remote memory,\nand meanwhile keep load balancing among NUMA nodes~\\cite{psaroudakis2015scaling}.\n\n\tMany systems have been proposed with NUMA aware optimizations.\nERIS~\\cite{kissinger2014eris} is an in-memory storage engine \nwhich employs an adaptive partitioning mechanism to realize NUMA topology\nand hence reduce remote memory accesses.\n\nATraPos~\\cite{porobic2014atrapos} further avoids commutative synchronizations \namong NUMA nodes during transaction processing.\n\nThere are also many efforts devoted to optimizing database operations with NUMA-awareness. \nAlbutiu et al. propose a NUMA-aware sort-merge join approach, and \nleverage prefetching to further enhance the performance~\\cite{albutiu2012massively}.\nLang et al. explore how various implementation techniques for  \nNUMA-aware hash join~\\cite{lang2015massively}.\nLi et al. study data shuffling algorithms in the context of NUMA architecture~\\cite{blagodurov2010case}.\n\n\n\\section{INDEX DESCRIPTION}\\label{sec::index}\n\nIn a traditional skip list,\nsince nodes with\ndifferent heights are dynamically allocated, they do not reside within a\ncontiguous memory area.\nNon-contiguous storage of nodes causes\ncache misses during key search and limits\n\nthe exploitation of \n\nSIMD processing, which requires the operands to be stored \nwithin a contiguous memory area.\n\n\nWe shall elaborate on how PI overcomes these two limitations and meanwhile achieves\nlatch-free query processing.\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{architecture.pdf}\n\t\\caption{An instance of PI}\n\t\\label{fig::arch}\n\\end{figure*}\n\n\\subsection{Structure}\n\nLike a typical skip list, \nPI also consists of multiple levels of sorted linked lists.\nThe bottommost level is a list of data nodes, whose definition is given in\nDefinition~\\ref{def::datanode}; an upper-level list is composed of the keys\nrandomly selected with a fixed probability from those contained in the linked\nlist of the next lower level. For the sake of expression,\nthese composing linked lists are logically separated into two layers: the\n\\textit{storage layer} and the \\textit{index layer}. The storage layer is merely the\nlinked list of the bottommost level, and the index layer is made up of the\nremaining linked lists.   \n\n\\begin{Def}\n    A data node $\\alpha$ is a triplet \n    \n", "index": 1, "text": "$$(\\kappa, p, \\Gamma)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"(\\kappa,p,\\Gamma)\" display=\"block\"><mrow><mo stretchy=\"false\">(</mo><mi>\u03ba</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi mathvariant=\"normal\">\u0393</mi><mo stretchy=\"false\">)</mo></mrow></math>", "type": "latex"}, {"file": "1601.00159.tex", "nexttext": "\n    where $t$ and\n    $\\kappa$ are the type and key of $q$, respectively,\n    and if $t$ is insert, $p$ provides the  \n    pointer to the new value associated with key $\\kappa$. \n\\label{def::query}\n\\end{Def}\n\nWe now define the query set $Q$ in\nDefinition~\\ref{def::qset}. There are two points worth mentioning in this\ndefinition. First, the queries in a query set are in non-decreasing order of\nthe query key $k$, and the reason for doing so will be elaborated in\nSection ~\\ref{subsubsec::latchfree}.\nSecond, a query set $Q$ only contains point queries, and we will show\nhow such a query set can be constructed and leveraged to answer range queries in\nSection~\\ref{ssec:range}. \n\n\\begin{Def}\n    A query set $Q$ is given by\n    \n", "itemtype": "equation", "pos": 21246, "prevtext": "\n    where $\\kappa$ is a\n    key, $p$ is the pointer to the value associated with $\\kappa$, and\n    $\\Gamma$ is the height of $\\kappa$, representing the number of linked lists\n    where key $\\kappa$ is present. We say a key $\\kappa \\in S$ if there exists\n    a data node $\\alpha$ in the storage layer of the index, $S$,\n    such that $\\alpha.\\kappa = \\kappa$.\n\\label{def::datanode}\n\\end{Def}\n\nThe difference between a traditional skip list and PI lies in the index layer.\nIn a traditional skip list, the node of a composing linked list of the index\nlayer contains only one key.\nIn contrast, a fixed number of keys are included in a list\nnode of the index layer, which we shall call an \\textit{entry} hereafter. The\nreason for this arrangement of keys is that it enables SIMD processing, which\nrequires operands to be contiguously stored.\nAn instance of PI with four keys in an entry is shown in Figure~\\ref{fig::arch},\n\n\twhere three different operations are collectively\n    processed by two threads, which are represented by the two arrows\n    colored purple and green, respectively.\n\n\n\nGiven an initial dataset,\nthe index layer can be constructed in a bottom up manner.\nWe only need to scan the storage layer once to fill the high-level\nentries as well as the associated data structure, i.e., routing table,\nwhich will be discussed in the next section in detail.\nThis construction process is $O(n)$ where $n$ is the number of records \n\nat the storage level, typically taking less than 0.2s for 16M\nrecords when running on a 2.0 GHz CPU core.\n\n\nFurther, the construction process can be parallelized,\nand hence can be sped up using more computing resources.\n\n\n\\subsection{Queries and Algorithms}\\label{subquery}\n\nPI, like most indexing structures, supports three types of queries, namely search,\ninsert and delete. Their detailed descriptions are as follows, and an\nabstraction of them is further given in Definition~\\ref{def::query}. \n\n\\begin{itemize}\n    \\item\\textbf{Search($\\kappa$):}\n        \n        \n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, \n        $\\alpha.p$ will be returned, and null otherwise. \n        \n        \n        \n\t\\item\\textbf{Insert($\\kappa$, $p$):}  \n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, \n        update the this data node by replacing\n        $\\alpha.p$ with $p$; \n        otherwise insert a new data node $(\\kappa, p, \\Gamma)$ \n        into the storage layer,\n        where $\\Gamma$ is drawn from a geometrical\n        distribution with a specified probability parameter.\t\n\t\\item\\textbf{Delete($\\kappa$):}\n        if there exists a data node $\\alpha$ in the index \n        with $\\alpha.\\kappa = \\kappa$, remove \n        this data node from the storage layer and return 1;\n        otherwise return $null$.\n\\end{itemize}\n\n\\begin{Def}\n    A query, denoted by $q$, is a triplet \n    \n", "index": 3, "text": "$$(t, \\kappa, [p])$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"(t,\\kappa,[p])\" display=\"block\"><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\u03ba</mi><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><mi>p</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">)</mo></mrow></math>", "type": "latex"}, {"file": "1601.00159.tex", "nexttext": "\n    where $N$ is the number of queries in $Q$, $q_i$ is a query defined in\n    Definition~\\ref{def::query}, and  \n    $q_i.\\kappa \\leq q_j.\\kappa$ \\emph{iff} $i < j$.\n    \\label{def::qset}\n\\end{Def}\n    \n\\begin{Def}\n    For a query $q$, we define the corresponding interception, $I_q$, as\n    the data node with the largest key among those in \n    $\\{\\alpha|\\alpha.\\Gamma > 1, \\alpha.\\kappa \\leq q.\\kappa\\}$.\n   \n   \n\\label{def::interception}\n\\end{Def}\n\n\\begin{algorithm}[t]\n\t\\label{alg:query}\n\t\\caption{{Query\\ processing}}\n\t\\small\n\t\n\t\\SetKwInOut{Input}{Input}\n\t\\SetKwInOut{Output}{Output}\n\t\\SetKwFunction{redis}{redistribute}\n\t\\SetKwFunction{partition}{partition}\n\t\\SetKwFunction{trav}{traverse}\n\t\\SetKwFunction{exec}{execute}\n\t\n\t\n\t\\Input{$S$, PI index \\\\\n\t\t$Q$, query set \\\\\n\t\t$t_1, ..., t_{N_T}$, $N_T$ threads\\\\\n\t}\n\t\\Output{$R$, Result Set\n\t}\n\t\n\t$R = \\emptyset$\\;\n\t\\For {$i=1 \\rightarrow N_T$}\n\t{\n\t\t$Q_i = \\partition(Q,N)$\\;\n\t}\n\t/* traverse the index layer to get interceptions */ \\\\\n\t\\ForEach{Thread $t_i$}\n\t{\n\t\t$\\Pi _i = \t\\trav(Q_i,S)$\\;\n\t}\n\twaitTillAllDone()\\;\n\t\n\t/* redistribute query workload */\\\\\n\t\\For {$i=1 \\rightarrow N_T$}\n\t{\n\t\t\n\t\t$\\redis(\\Pi_i, Q_i, i)$\\;\n\t}\n\t/* query execution */ \\\\\n\t\\ForEach{Thread $t_i$}\n\t{\n\t\t$R_i = \t\\exec(\\Pi_i,Q_i)$\\;\n\t}\t\n\twaitTillAllDone()\\;\n\t$R = \\cup R_i$\\;\n\treturn $R$\\;\n\t\n\\end{algorithm}\n\nPI accepts a query set as input, and employs a batch technique \n\n\n\nto process the queries in the input. \nGenerally, batch processing may increase the latency of query processing, \nas queries may need to be buffered before being processed. \nHowever, since batch processing can significantly improve the \nthroughput of query processing, as shown in Section~\\ref{subsec:batch},\nthe average processing time of queries are not much affected.  \n\nThe detailed query processing of PI is given in Algorithm~\\ref{alg:query}.\nFirst, the query set $Q$ is evenly partitioned into disjoint \nsubsets according to the number of threads, and the $i$-th subset is\nallocated to thread $i$ for processing (line 3). The ordered set of queries\nallocated to a thread is also a query set defined in Definition~\\ref{def::qset},\nand we call it a query batch in order to differentiate it from the input\nquery set.\nEach thread traverses the index layer and generates for each query in its \nquery batch an interception which is defined in\nDefinition~\\ref{def::interception} (line 5 and 6).\nAfter this search process, the resultant interceptions\nare leveraged to adjust query batches\namong execution threads such that each thread is able to  \n\\textit{safely} execute all the queries assigned to it after the adjustment (line\n9 and 10).\nFinally, each thread\nindividually executes the queries in its query batch (line 12 and 13).\nThe whole procedure is exemplified in Figure~\\ref{fig::arch}, \nwhere three queries making up a query set are collectively processed by\nthree threads.\n\n    Following the purple arrows, thread 1 traverses downwards to fetch the \n    data node with key 8 and delete the data node with key 26 in storage layer,\n\tand thread 2 moves along with the green arrows to insert the data node with key 102.\n\n\n\n\n\\subsubsection{Traversing the Index Layer}\n\nAlgorithm~\\ref{alg:index} shows how the index layer is traversed to find the\ninterceptions for queries.\nFor each query key, the traversal starts from\nthe top level of the index layer and moves forward along this level until an\nentry containing a larger key is encountered, upon which it moves on to\nthe next level and proceeds as it does in the previous level.\nThe traversal terminates when it is about to leave the bottom level of the index layer,\n\nand records the first data node that will be encountered in the storage layer as the\ninterception for the current query.\n\n\\begin{algorithm}[t]\n\t\\caption{{Traversing\\ the\\ index\\ layer}}\n\t\\label{alg:index}\n\t\\small\n\t\n\t\\SetKwInOut{Input}{Input}\n\t\\SetKwInOut{Output}{Output}\n\t\\SetKwFunction{top}{getTopEntry}\n\t\\SetKwFunction{isbtm}{isStorageLayer}\n\t\\SetKwFunction{load}{\\_simd\\_load}\n\t\\SetKwFunction{compare}{\\_simd\\_compare}\n\t\\SetKwFunction{findnext}{findNextEntry}\n\t\n\t\\Input{$S$, PI index \\\\\n\t\t$Q$, query batch\\\\\n\t}\n\t\\Output{$\\Pi$, interception set \\\\\n\t}\n\t\n\t$\\Pi = \\emptyset$\\;\n\t\\ForEach{$q \\in Q$}\n\t{\n\t\t$e_{next} = \\top(S)$\\;\n\t\t$v_b = \\load(q.\\kappa)$\\;\n        \\While {$\\isbtm(e_{next})==\\emph{false}$}\n\t\t{\n\t\t\t$v_a = \\load(e_{next})$\\;\n\t\t\t$mask = \\compare(v_a,v_b)$\\;\n\t\t\t/* $R_{next}$ is the routing table of $e_{next}$ */\\\\\n\t\t\t$e_{next} = \\findnext(e_{next},mask,R_{next})$\\;\n\t\t}\n        $\\Pi = \\Pi \\cup \\{e_{next}\\}$\\;\n\t}\n\treturn $\\Pi$\\;\n\t\n\\end{algorithm}\n\nWe exploit Single Instruction, Multiple Data (SIMD) processing to accelerate\nthe traversal.\nIn particular, multiple keys within an entry can be\nsimultaneously compared\nwith the query key using an \\_simd\\_compare instruction, which significantly\nreduces the number of comparisons, and this is the main reason we put multiple keys\nin each entry.\nIn our implementation, keys in an entry exactly occupy a\nwhole SIMD vector, and can be loaded into an SIMD register using a\nsingle \\_simd\\_load instruction.\n\nSince each \\_simd\\_compare instruction compares multiple keys in an entry,\nthe comparison result can be diversified,\n\nand hence an efficient\nway to determine the next entry to visit for each comparison result is needed.\nTo this end, we\ngenerate a routing table for each entry during the construction of PI.\n\n\nFor each possible result of SIMD comparison, the routing table contains\nthe address of the next entry to visit.\nFigure~\\ref{subfig::cmplogic} gives an example of SIMD comparison.\nAs shown\nin this figure, each SIMD comparison leads to a 4-bit mask, representing\nfive potential results indicated by the red arrows shown in\nFigure~\\ref{subfig::rt1}.\nThis mask is then indexed into the routing\ntable, which is also shown in Figure~\\ref{subfig::rt2}, to find the\nnext entry for comparison.\n\n\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\subfigure[SIMD comparison]{\n\t\\begin{minipage}[b]{.4\\linewidth}\n\t\t \\centering\n\t\t \\label{subfig::cmplogic}\n\t\t \\includegraphics[width=1.1\\linewidth,height=1.1in]{comparison.pdf} \\\\\n\t\t \n\t\\end{minipage}\n\t}\n\t\\subfigure[Routing table for Entry 1]{\n\t\\begin{minipage}[b]{.5\\linewidth}\n\t\t\\centering\n\t\t\\label{subfig::rt2}\n\t\t\\includegraphics[width=.5\\linewidth]{routetable2.pdf} \\\\\n\t\t\n\t\\end{minipage}\n\t}\n\t\\subfigure[Routing process for Entry 1]{\n\t\\begin{minipage}[b]{1.\\linewidth}\n\t\t\\centering\n\t\t\\label{subfig::rt1}\n\t\t\\includegraphics[width=.9\\linewidth]{routetable1.pdf} \\\\\n\t\t\n\t\\end{minipage}\n\t}\n\t\\caption{Querying with a routing table}\n\t\\label{fig::routetable}\n\\end{figure}\n\n\\subsubsection{Redistribute Query Workload}\\label{sec:interception}\n\nGiven the interception set output by Algorithm~\\ref{alg:index}, a thread can\nfind for each allocated query $q$ the data node with the largest key that is\nless than or equal to $q.\\kappa$ by walking along the\nstorage layer, starting from $I_q$. However, it is possible that two queries\nallocated to two adjacent threads have the same interception, leading to\ncontention between the two threads. To handle this case, we slightly adjust the\nquery workload among the execution threads such that each interception is\nexclusively accessed by a single thread, and so are the data nodes between two\nadjacent interceptions. \nTo this end, each thread iterates backward over\nits interception set until it finds an interception\ndifferent from the first interception of the next thread,\nand hands over the queries corresponding\nto the iterated interceptions to the next thread.\nThe details of this process are summarized in\nAlgorithm~\\ref{alg:adjust} and exemplified in Figure~\\ref{fig::latchfree}.\nAfter the adjustment, a thread can individually execute\nthe allocated query workload without \ncontending for data nodes with other threads. \n\n\n\\begin{algorithm}[!tbpn]\n    \\caption{Redistribute query workload}\n\t\\label{alg:adjust}\n\t\\small\n\t\n\t\\SetKwInOut{Input}{Input}\n\t\\SetKwInOut{Output}{Output}\n\t\\SetKwFunction{recvKey}{recvKey}\n\t\\SetKwFunction{sendKey}{sendKey}\n\t\\SetKwFunction{recvQuery}{recvQuery}\n\t\\SetKwFunction{sendQuery}{sendQuery}\n\t\\SetKwFunction{recvInt}{recvInterception}\n\t\\SetKwFunction{sendInt}{sendInterception}\n\t\n\t\n\t\\Input{ $Q = \\{q_1, q_2, ...\\}$, query batch\\\\\n        $\\Pi = \\{I_{q_1}, I_{q_2}, ...\\}$, interception set \\\\\n        $T_{last}$, last execution thread\\\\\n        $T_{next}$, next execution thread\\\\\n\t}\n\t\n\t/* exchange the key of the first interception*/\\\\\n    $\\sendKey(T_{last}, I_{q_1}.\\kappa)$\\;\n    $\\kappa=\\recvKey(T_{next})$\\;\n\t\n\t/* wait for the adjustment from last thread */\\\\\n\t\n    $Q' = \\recvQuery(T_{last})$, $Q = Q' \\cup Q$\\;\n    \\ForEach{$q \\in Q'$}\n    {\n        /* $I_q$ is same as $I_{q_1}$*/\\\\\n        $\\Pi = I_{q_1} \\cup \\Pi$\n    }\n\t\n\t$Q' = \\emptyset$\\;\n\t\\For {$i=|Q| \\rightarrow 1$}\n\t{\n        \\uIf{$I_{q_i}.\\kappa = \\kappa$}\n\t\t{\n            $Q' = Q' \\cup \\{q_i\\}$, $Q = Q \\setminus \\{q_i\\}, \\Pi = \\Pi \\setminus \\{I_{q_i}\\} $\\;\n\n\t\t}\n\t\t\\Else\n\t\t{\n\t\t\tbreak\\;\n\t\t}\n\t}\n\t\n    $\\sendQuery(T_{next}, Q')$\\;\n\t\n\\end{algorithm}\n\n\n\\subsubsection{Query Execution}\\label{subsubsec:execution}\n\nThe query execution process at each\nthread is demonstrated in Algorithm~\\ref{alg:storage}.\nFor each query $q$,\nan execution thread iterates over the storage layer, starting from the\ncorresponding interception,\nand executes the query against the data node with the largest key that is less\nthan or equal to $q.\\kappa$.\nIf the query type is delete, we do not\nremove the data node immediately from the storage layer, but merely set\na flag $F_{del}$ instead, which is necessary for latch-free query\nprocessing, as we shall explain in Section~\\ref{subsubsec::latchfree}.\nFor the search query, the $F_{del}$ flag of\nthe resultant data node will be checked to decide the validity of its\npointer.\nFor the update query, a new data node will be inserted into the\nstorage layer if the query key does not match that of the resultant data\nnode. Unlike a typical skip list, PI only allocates a random height\nfor the new node, but does not update the index layer immediately.\nWith more and more updates made to the storage layer, the index layer should\nbe updated accordingly to guarantee the performance of query processing.\nCurrently, a background process monitors the updates to the \nstorage layer, and asynchronously rebuild the whole index layer \nwhen the number of updates exceeds a certain threshold. The detail is\ngiven in Section~\\ref{ssub:background_update}.\n\n\n\n\\begin{algorithm}[!tbpn]\n\t\\caption{{Query execution}}\n\t\\label{alg:storage}\n\t\\small\n\t\n\t\\SetKwInOut{Input}{Input}\n\t\\SetKwInOut{Output}{Output}\n\t\\SetKwFunction{type}{queryType}\n\t\\SetKwFunction{get}{getNode}\n\t\\SetKwFunction{update}{updateNode}\n\t\\SetKwFunction{remove}{removeNode}\n\t\\SetKwFunction{getnode}{getNode}\n\t\\SetKwFunction{insert}{insertNode}\n\t\n    \\Input{ $Q = \\{q_1, q_2, ...\\}$, adjusted query batches\\\\\n        $\\Pi = \\{I_{q_1}, I_{q_2}, ...\\}$, adjusted interception set \\\\\n\t}\n\t\\Output{ $R$, result set\\\\\n\t}\n\t\n\t$R=\\emptyset$\\;\n\t\\For {$i=1 \\rightarrow |Q|$}\n\t{\n        /* walk along the storage layer, */ \\\\\n        /* starting from the corresponding interception */\\\\\n        $node = \\getnode(q_i, I_{q_i})$\\;\n\t\t$r_i = null$\\;\n\t\t\\uIf {$q_i.t == ``Search\"$}\n\t\t{\n\t\t\t\\If {$node.\\kappa == q_i.\\kappa\\ \\&\\&\\ !node.F_{del}$}\n\t\t\t{\n\t\t\t\t$r_i = node.p$\\;\n\t\t\t}\n\t\t}\n\t\t\\uElseIf {$q_i.t == ``Insert\"$}\n\t\t{\n\t\t\t\\uIf {$node.\\kappa == q_i.\\kappa$}\n\t\t\t{\n\t\t\t\t$node.p = q_i.p$\\;\n\t\t\t}\n\t\t\t\\Else\n\t\t\t{\n\t\t\t\t$node = \\insert(node, q_i.\\kappa, q_i.p, \\Gamma)$\\;\n\t\t\t}\n\t\t\t$node.F_{del} = false$\\;\n\t\t}\n\t\t\\Else\n\t\t{\n\t\t\t\\If {$node.\\kappa == q_i.\\kappa$}\n\t\t\t{\n\t\t\t\t$node.F_{del} = true$\\;\n\t\t\t}\n\t\t}\n        $R = R \\cup \\{r_i\\}$\n\t}\n\treturn $R$\\;\n\t\n\\end{algorithm}\n\n\n\\subsubsection{Naturally Latch-free Processing}\\label{subsubsec::latchfree}\n\nIt is easy to see that query processing in PI is latch-free.\nIn the\ntraversal of the index layer, since the access to each entry is read-only,\nthe use of latches can be avoided at this stage.\nFor the adjustment of query workload,\neach thread communicates with its adjacent threads via\nmessages and thus does not rely on latches.\nIn addition, each query $q$\nallocated to thread $i$ after the adjustment of query workload satisfies\n$I^i_{q_1}.\\kappa \\leq q.\\kappa < I^{i+1}_{q_1}.\\kappa$, where $I^i_{q_1}$ and\n$I^{i+1}_{q_1}$ are the first element\nin the interception sets of thread $i$ and $i+1$, respectively.\nConsequently, thread $i$ can individually execute without latches all its \nqueries except those which require reading $I^{i+1}_{q_1}$ or inserting a new node\ndirectly before $I^{i+1}_{q_1}$, since the data nodes that will be accessed\nduring the execution of these queries will never be accessed by other threads.\nThe remaining queries can still be executed\nwithout latches as the first interception of each thread will never be deleted,\nas described in Section~\\ref{subsubsec:execution}.\n\nIn our algorithm, a query set $Q$ is ordered mainly due to two reasons.\nFirst, cache utilization can be improved by processing ordered queries\nin Algorithm~\\ref{alg:query}, since the entries and/or data nodes to be \naccessed for a query may have already been loaded into the\ncache during the processing of previous queries. \nSecond, a sorted query set leads to sorted\ninterception sets (ordered by query key), which is necessary for\ninterception adjustment and query execution to work as expected.\n\n\n\\subsubsection{Range Query}\n\\label{ssec:range}\n\n\n\n\nRange query is supported in PI.\nGiven a set of range queries\n(a normal point query defined in Definition~\\ref{def::query} is also \na range query with the upper and lower bound of the key range\nbeing the same),\nwe first sort them according to the lower bound of their key\nrange and then construct a query set defined in Definition~\\ref{def::qset}\nusing these lower bounds. This query set is then distributed among a set of\nthreads to find the corresponding interceptions, as in the case of point\nqueries. The redistribution of query workload, however, is slightly different.\nDenote the first element in the interception set of thread $i$, i.e., the\ninterception corresponding to the first query in the query batch of thread\n$i$, by $I_{q_1}^i$. For each allocated query with a key range of $[\\kappa_s,\n\\kappa_e]$, where $\\kappa_e \\geq I_{q_1}^{i+1}$,\nthread $i$ partitions it into two queries with the key\nranges being $[\\kappa_s, I_{q_1}^{i+1}.\\kappa)$ and\n    $[I_{q_1}^{i+1}.\\kappa, \\kappa_e]$, respectively,\n    and hands over the second query to thread $i+1$. As in\n    Algorithm~\\ref{alg:adjust}, this redistribution process must be performed in\n    the order of thread \\textit{id} in order to handler the case where the key range of a\n    range query is only covered by the union of the interception sets of three or\n    more threads.\nAfter the redistribution of query workload, each thread then executes the\nallocated queries one by one, which is quite\nstraightforward. Starting from the corresponding interception, PI iterates over\nthe storage layer to find the first data node within the key range, and then\nexecutes the query upon it and each following data node until the upper bound of the\nkey range is encountered. The final result of an original range query can be\nacquired by combining the result of corresponding partitioned queries.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=3.3in]{assignwork.pdf}\n    \\caption{Interception adjustment}\n\t\\label{fig::latchfree}\n\\end{figure}\n\n\n\n\\section{Implementation}\n\\label{sec::impl}\n\n\\subsection{Storage Layout}\n\\label{sub:storage_layout}\nAs we have mentioned, PI logically consists of the index layer and the\nstorage layer. The index layer further comprises multiple levels, each\ncontaining the keys appearing at that level. Keys at the same level are\norganized into entries to exploit SIMD processing, and each entry is associated\nwith a routing table to guide the traversal of the index layer. \nFor better cache\nutilization, entries of the same level are stored in a contiguous memory area.\nThe storage layer of PI is implemented as a \n\ntypical\nlinked list of data nodes to support efficient insertions.\n\nSince entries contained in each level of the index layer are stored compactly in a\ncontiguous memory area, PI cannot immediately update the index layer upon the\ninsertion/deletion of a data node with height $h>1$. \nCurrently, we implement a simple\nstrategy to realize these deferred updates. Once the number of\ninsertions and deletions exceeds a certain threshold, the entire index layer is\nrebuilt from the storage layer in a bottom-up manner. Although this strategy seems\nto be time-consuming, it is highly parallelizable and the rebuilding process can\nthus be shortened by using more threads. Specifically, each thread can be assigned\na disjoint portion of the storage layer and made responsible to construct the\ncorresponding part of the index layer. \nThe final index layer can then be obtained by\nsimply concatenating these parts level by level.\n\n\\subsection{Parallelization and Serializability}\n\nWe focus on two kinds of parallelization in the implementation, i.e.,\ndata-level parallelization and scale-up parallelization. It is also\nworth noting that serializability should be guaranteed in the parallel\nprocess.\n\nData-level parallelization is realized through the use of SIMD\ninstructions during the traversal of the index layer.\nAs mentioned before,\nin our implementation, each entry contains multiple keys, and their\ncomparison with the query key can be done using a single SIMD\ncomparison instruction, which substantially accelerates the search\nprocess of interceptions.\nMoreover, SIMD instructions can be introduced\nin sorting the query set $Q$ to improve the performance.\nIn our\nimplementation, we use Intel Intrinsic Library to implement SIMD related\nfunctions.\n\nWe exploit scale-up parallelization provided in multi-core systems by\ndistributing the query workload among different cores such that the execution\nthread running at each core can independently process the\nqueries assigned to it.\nSerializability issues may arise as a result of coexistence of search\nand update queries in one batch,\nand we completely eliminate these issues by\nensuring that only one thread takes\ncharge of each data node at any time.\n\n\\subsection{Optimization}\n\\subsubsection{NUMA-aware Optimization}\n\nThe hierarchical structure of a modern memory system, such as multiple\ncache levels and NUMA (Non-Uniform Memory Access) architecture, should\nbe taken into consideration during query processing.\nIn our implementation, we organize incoming\nqueries into batches,\nand process the queries batch by batch.\nThe queries\nwithin one batch are sorted before processing.\nIn this manner, cache\nlocality can be effectively exploited,\nas the search process for a query\nkey is likely to traverse the entries/data nodes that have just been accessed\nduring the search of the previous query\nand thus have already been loaded into\nthe cache.\n\n\nA NUMA architecture is commonly used to enable the\nperformance of a multi-core system to scale with the number of\nprocessors/cores. In a NUMA architecture, there are multiple\ninterconnected NUMA nodes, each with several processors and its\nown memory.\nFor each processor, accessing local memory residing\nin the same NUMA node is much faster than accessing remote\nmemory of other NUMA nodes. It is thus extremely important for\na system running over a NUMA architecture to reduce or even\neliminate remote memory access for better\nperformance.\n\n\n\n\n\tPI evenly distributes data nodes among available NUMA nodes\n\tsuch that the key ranges corresponding to the data nodes\n\tallocated to each NUMA node are disjoint.\n    One or more threads will then be spawned at each NUMA node\n    to build the index from the data nodes of the same NUMA node,\n    during which only local memory accesses are incurred.\nAs a result,\nfor each NUMA node, there is a separate\nindex, which can be used to independently answer queries falling\nin the range of its key set.\nEach incoming query will be routed to the corresponding NUMA node, \nand get processed by the threads spawned in that node.\nIn this way, there is no remote memory access during\nquery processing, which will translate into significantly enhanced query\nthroughput, as shown in the experiments.\nMoreover, the indices at different NUMA\nnodes also collectively improve the degree of parallelism since they can be used\nto independently answer queries.\n\n\tThis idea of parallelism is aptly illustrated in Figure~\\ref{fig::arch}, where two threads are spawned in the first NUMA node and the other three nodes can have multiple threads running in parallel as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Load Balancing}\n\\label{subsec:load}\n\nIt can be inferred from Algorithm~\\ref{alg:adjust} that PI\nensures the data nodes between two adjacent interceptions are handled by\nthe same thread.\nAs long as the queried keys of a batch are\nnot limited to a very short range, PI is able to distribute the query\nworkload evenly among multiple threads (within a NUMA node) due to the fact that queries of a batch\nare sorted and the way we partition the queries.\nHowever, it is more\nlikely that the load among multiple NUMA nodes is unbalanced, which\noccurs when most incoming queries should be answered by the data nodes\nlocated at a single NUMA node.\nTo address this problem, we use a\nself-adjusted threading mechanism.\nIn particular, when a query batch\nhas been partitioned and each partitioned sub-query has been assigned to\nthe corresponding NUMA node,\nwe spawn threads in each NUMA node to\nprocess its allocated queries such that the number of threads in each\nNUMA node is proportional to the number of queries assigned to it. \nConsequently,\na NUMA node that has been allocated more queries will also \nspawn more threads to process queries.\nAn example and its further explanation on the threading mechanism are given in\nSection~\\ref{sssub:threading}.\n\\begin{figure}[tbp]\n\t\\centering\n\t\\subfigure[Uniform query workload] {\n\t\t\\includegraphics[width=.45\\textwidth]{threaduniform.pdf}\n\t\t\\label{fig:threadnormal}\n\t}\n\t\\subfigure[Skewed query workload] {\n\t\t\\includegraphics[width=.45\\textwidth]{threadskew.pdf}\n\t\t\\label{fig:threadskew}\n\t}\n\t\\caption{Self-adjusted threading}\n\t\\label{fig:threadmanage}\n\\end{figure}\n\n\n\n\\subsubsection{Self-adjusted threading}\n\\label{sssub:threading}\n\nIn this section, we shall elaborate on \nour self-adjusted threading mechanism, which allocates threads among NUMA nodes \nfor query processing \nsuch that query performance can be maximized within a given budget of computing\nresource. As mentioned in Section~\\ref{subsec:load}, if there are several NUMA nodes\navailable, PI will allocate the data nodes among them, build a separate\nindex in each NUMA node from its allocated data nodes,\nand route arriving queries to the NUMA node with matching keys in order not to\nincur expensive remote memory accesses.  \nGiven the query workload at each NUMA node, our\nmechanism dynamically allocates execution threads among NUMA nodes such that \nthe number of threads running on each NUMA node is proportional to its query\nworkload, i.e., the number of queries routed to it. In the cases where \na NUMA node has used up its hardware threads, our mechanism will offload\npart of its query workload to other NUMA nodes with available computing resource. \n\\subsubsection{Group Query Processing and Prefetching}\n\\label{ssub:prefetch_and_group_execution}\nSince entries at the same level of the index are stored in\na contiguous memory area, it is thus more convenient (due to fewer translation lookaside buffer misses)\nto fetch\nentries of the same level than to fetch entries locating at\ndifferent levels. \nIn order to realize this location proximity,\ninstead of processing queries one by one,\nPI organizes the queries of a batch into multiple\nquery groups, and processes all queries in a group simultaneously.\nAt each level of the index layer, PI traverses\nalong this level to find the first entry at the next level to compare with \n\nand then moves downward to the next lower level\nto repeat this process.  \n\n\nMoreover, this way of group query processing\nnaturally calls for the use of prefetching.\nWhen the entry to be compared with at the next level\nis located for a query, PI issues a prefetch instruction\nfor this entry before turning to the next query. Therefore,\nthe requested entries at the next level may have\nalready been loaded into L1 cache before the comparison \nbetween them and the queried keys, thereby overlapping the\nslow memory latency.\n\n\\subsubsection{Background Updating}\n\\label{ssub:background_update}\n\nWe use a daemon thread running in background to update index layer. \nIf the number of update operations meets a pre-defined threshold, \nthe daemon thread will start rebuilding the index layer.\nThe rebuilding of the index layer is fairly straightforward. \nThe daemon thread traverses the storage layer,\nput the key of each valid data node with height $ > 1$ encountered\ninto an array sequentially,\nand updates the associated route table with the address of this data node.\nThe new index layer will be put into use after all running threads \ncomplete the processing of current query batch, \nand meanwhile the old index layer will be discarded.\n\\\\\n\\\\\n\n\\section{Performance Modeling}\n\\label{sec::model}\n\nIn this section,\nwe develop a performance model for query processing\nof PI.\nThe symbols used in our\nanalysis are summarized in \nTable~\\ref{tab:analysis}.\n\n\n\\begin{table}\n\t\\centering\n\t\\caption{Notations for Analysis}\\label{tab:analysis}\n\t\\begin{tabular}{|l|l|}\n\t\t\\hline\n\t\tSymbol & Description \\\\\n\t\t\\hline\n        {$H$} & index height\\\\\n\t\t\\hline\n        {$P$} & probability parameter\\\\\n\t\t\\hline\n        {$M$} & number of keys contained in an entry\\\\\n\t\t\\hline\n        {$L$} & memory access latency\\\\\n\t\t\\hline\n        {$N$} & number of the initial data nodes \\\\\n        \\hline\n        {$R$} & ratio of insert queries \\\\\n        \\hline\n        {$S_{e}$} & entry size \\\\\n        \\hline\n        {$S_{n}$} & data node size \\\\\n        \\hline\n        {$S_{l}$} & size of a cache line \\\\\n        \\hline\n        {$S_{c}$} & size of the last level cache \\\\\n        \\hline\n        {$T_{c}$} & time to read a cache line \\\\ \n                  & from the last level cache \\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\\subsection{Key Search}\n\\label{sub:key_search}\n\nThe key search process is to locate the data node with matching key at Storage\nLayer. The time spent in this process is dominated by the number of\nentries and data nodes that need to be read for key comparison.\n\n\nGiven the number of initial data nodes,\n$N$, the height of PI, $H$, is\nabout $\\ceil{-\\log_PN}$.\nAt each level of PI,\nthe number of keys between two adjacent ones that\nalso appear at a higher level follows a geometric distribution, and has\nan expected value of $1/P$.\n\nTherefore, the average number of entries\nneeded to compare with at each level of the index layer is approximately\n$\\ceil{\\frac{1+P}{2PM}}$, where $\\frac{1+P}{2P}$ is the average number of keys\nthat need to be compared with. The number of cache lines that need to be read at each level\nis thus $\\ceil{\\frac{S_{e}}{S_{l}}}\\ceil{\\frac{1+P}{2PM}}$.\nConsequently, the total number of cache lines\n\nduring the traversal of the index layer is\n$(H-1)\\ceil{\\frac{S_{e}}{S_{l}}}\\ceil{\\frac{1+P}{2PM}}$.\n\nThis is however a slight over-estimate,\nsince the top levels of the index layer may already have been read into L1 cache.\n\nWhen the interception has been located in the storage layer for a given query,\nthe search process proceeds by iterating over the data nodes from\nthe one contained in the interception until the node with matching key\nis encountered.\nThe number of data nodes scanned during this phase is\nabout half of the number of data nodes contained between two adjacent\nones with their key appearing at the index layer, which is given by\n$\\ceil{\\frac{1+P}{2PM}}$, and the number of cache lines read during this stage is\n$\\ceil{\\frac{S_n}{S_l}}\\ceil{\\frac{1+P}{2PM}}$.\n\nThe number of data nodes read during the scanning of the storage layer is not\naffected by the delete queries because of the PI query processing strategy.  \nHowever, insert queries do impact this number.\nAssuming insert queries are uniformly distributed among the storage layer, and the\naggregate number of insert and delete queries does not exceed the threshold\nupon which the rebuilding of the index layer will be triggered. The number of data\nnodes during key search process will become $\\frac{(1+iR/N)(1+P)}{2P}$, where $i$ is the\nnumber of queries that have been processed, and $R$ is the ratio of insert\nqueries among the processed $i$ queries.\n\nIn our implementation, the probability of key elevation, $P$, is 0.25\nas suggested in~\\cite{pugh1990skip},\n\n\n\n\nand each entry contains 4 keys, each being a 32-bit float number.\nTherefore, the size of each entry, $S_e$, is 4*(4+8)=48\nbytes, where the additional 8 bytes for each key is required by\nroute table to determine the next entry to compare. Each data node\noccupies 20 bytes: 4 bytes are used for key, and the other 16 bytes\ncompose two pointers, one pointing to the value, e.g., a tuple in a\ndatabase table, and the other for the next data node. Consider an\ninstance of PI with 512K keys.  Its size can be computed by\n$20 * 512K + 1/3 * 48 * 512K = 18M$,\n and the whole index can be kept in\nthe last level cache of most servers (e.g. the one we use for the experiments).\nTherefore,\nthe processing of each search/delete query fetches about 12 cache lines, \n\n9 for the index layer and 3 for the storage layer.\nThe total time cost is thus $12T_c$, where $T_c$ is the time to read a cache line from\nthe last level cache.\n\n\n\n\n\n\n\n\\subsection{Rebuilding the Index Layer}\n\\label{sub:rebuilding_index_layer}\nFor the sake of simplicity, we only focus on the indices with a lot of data\nnodes, in which case the data nodes are unlikely to be cached, and hence\nrequire to be read from memory during the rebuilding of the index layer.\nIn addition, a data node occupies 48 bytes, as mentioned in last section, and\nhence can be fetched within a single memory access, which normally brings a \n64-byte cache line into the cache. Therefore, for an index with $N$ data nodes,\nscanning the storage layer costs a time of $NL$. In addition, there are\n$NP/(1-P)$ entries and routing tables that need to be written back into\nmemory, which costs another $2NP/(1-P)$ memory accesses. Therefore, the total\ntime of rebuilding the index layer can be approached by $(1+P)NL/(1-P)$.\nHowever, with more threads participating in the rebuilding process, this time \ncan be reduced almost linearly before bounded by memory bandwidth.\n\\begin{figure}[tb]\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{cpu.pdf}\n\t\\caption{CPU architecture for the experiments}\n\t\\label{fig:cpu}\n\\end{figure}\n\n\n\n\n\\section{Performance Evaluation}\n\\label{sec::eval}\n\nWe evaluate the performance of PI on a platform with 512 GB of memory evenly\ndistributed among four NUMA nodes. Each NUMA node is\nequipped with an Intel Xeon 7540 processor, which supports 128-bit wide SIMD\nprocessing, and \nhas a L3 cache of 18MB and six on-chip cores, each running at 2 GHz. \n\nThe CPU architecture is described in Figure~\\ref{fig:cpu},\n    where QPI stands for Intel QuickPath Interconnect.\n\nThe operating system installed in the\nexperimental platform is Ubuntu 12.04 with kernel version 3.8.0-37.\n\n\n\\begin{table}\n\t\\centering\n\t\\caption{Parameter table for experiments}\\label{tab:para}\n\t\\begin{tabular}{|l|l|}\n\t\t\\hline\n\t\tParameter & Value \\\\\n\t\t\\hline\n\t\tDataset size(M) & 2, 4, 8, \\underline{16}, 32, 64, 128, 256\\\\\n\t\t\\hline\n\t\tBatch size & 2048, 4096, \\underline{8192}, 16384, 32768\\\\\n\t\t\\hline\n\t\tNumber of Threads & 1, 2, 4, \\underline{8}, 16, 32\\\\\n\t\t\\hline\n\t\tWrite Ratio(\\%) & 0, 20, 40, 60, 80, 100\\\\\n\t\t\\hline\n\t\tZipfian parameter {$\\theta$} & \\underline{0}, 0.5, 0.9 \\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\nThe performance of PI is extensively evaluated from various perspectives.\nFirst, we show the adaptivity of PI's performance of query processing by varying\nthe size of the dataset and batch, and then adjust the number of execution\nthreads to investigate the scalability of PI. \nAfterwards, we study how PI performs in the presence of mixed and skewed query\nworkloads, and finally examine PI's performance with respect to range query. \nFor comparison, the result of Masstree~\\cite{mao2012cache} under the\nsame experiment setting is also given, whenever possible. \n\n\tWe choose Masstree as our baseline mainly due to its high performance\n    and maturity which is evidenced by its adoption in a widely recognized system, \n    \n    namely\nSILO~\\cite{tu2013speedy}.\n\nThe Masstree code we use is retrieved from github~\\cite{masstreesource}, and its\nreturned results are consistent with (or even better than) those presented in the\noriginal paper~\\cite{mao2012cache}, as shown in the following sections. \n\nIf not otherwise specified,\nwe use the following default settings for the experiments.\nThe key length is four bytes.\nThere are eight execution threads running on the four NUMA nodes, and the number\nof threads running on each node is proportional to the query workload for this\nnode, as mentioned in Section~\\ref{sub:storage_layout}.\nThree datasets, each with a different number of keys, are used.\nThe small and medium datasets have 2M and 16M keys, respectively,\nand the large dataset has 128M keys. \nThe index built from the dataset\nare evenly distributed among the four NUMA nodes.\nEach NUMA node holds a separate index accounting\nfor approximately 1/4 of the\nkeys in the dataset.\n\n\tAll the parameters for the experiments are summarized in\n    Table~\\ref{tab:para}, where the default value is underlined when applicable.\n\n\n\n\tThe query workload is generated from \\textit{Yahoo! Cloud\n    Serving Benchmark} (YCSB)~\\cite{cooper2010benchmarking}, and\nthe keys queried in the workload follow a zipfian distribution\nwith parameter $\\theta = 0$, i.e., a uniform distribution. \n\nA query batch, i.e., the set of queries allocated to a thread after \npartitioning in Algorithm~\\ref{alg:query}, contains 8192 queries, \nand a discussion on how to tune this parameter is given in\nSection~\\ref{subsec:batch}.\nThe whole index layer is asynchronously rebuilt\nafter a fixed number\n(15\\% of the original dataset size) of data nodes have been\ninserted/deleted into/from the index. \n\n\n\\subsection{Dataset Size}\n\\label{subsec:dataset}\n\n\n\\begin{figure}[tb]\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{dataset.pdf}\n\t\\caption{Query throughput vs dataset size}\n\t\\label{fig:dataset}\n\\end{figure}\nFigure~\\ref{fig:dataset} shows the processing throughput of PI and Masstree\nfor search and insert queries.\nFor this experiment, we\nvary the number of keys in the dataset from 2M to 256M,\nand examine the query throughput for each dataset size.\nThe whole index and query workload is evenly distributed \namong the four NUMA nodes, and there are two threads running\nover each NUMA node to process queries.\n\nFrom Figure~\\ref{fig:dataset},\none can see that the throughput for both search and insert\n experiences a moderate decrease as the dataset size increases from\n2M to 64M,\nand then becomes relatively stable for larger dataset sizes.\nThis variation\ntrend in the throughput is natural.\nFor the dataset with 2M keys,\nas we have explained in Section~\\ref{sec::model},\nthe entire index can be\n\naccommodated in the last level caches of the four NUMA nodes,\nand the throughput is hence mainly determined by the latency \nto fetch the entries and data nodes from the cache.\nAs the dataset size increases,\nmore and more entries and data nodes are no longer able to reside in the cache\nand hence can only be accessed from memory,\nresulting in higher latency and lower query throughput.  \n\n\nThe throughput of insert queries of PI is not as high\nas that of search queries. The reason is two-fold.\nFirst, as insert queries are processed, more and more\ndata nodes are inserted into the storage layer of the index, \nresulting in an increase in the time to iterate over the \nstorage layer. Second, the creation of data nodes leads to\nthe eviction of entries and data nodes from the cache, \nand a reduced cache hit rate. This also explains why the performance gap\nbetween insert and search queries gradually shrinks with the size of dataset. \n\n\n\n\n\n\nAs can be observed from Figure~\\ref{fig:dataset},\nthe throughput of both search and\ninsert queries in Masstree is much less than that of PI.\nIn particular, PI is\nable to perform 34M search queries or 29M insert queries in one second\nwhen the index can be\naccommodated in the cache,\nwhich are respectively five and four times more than \nthe corresponding throughput \nMasstree achieves for the same dataset size.\n\n\n\nFor larger datasets, PI can\nstill consistently perform at least 1.5x and 1x\nbetter than Masstree in terms of the\nsearch throughput and insert throughout, respectively.\n\n\n\\subsection{Batch Size}\n\\label{subsec:batch}\n\n\nWe now examine the effect of the size of query batches, \n\n\non the \nthroughput of PI.\nFor this experiment, the three default datasets,\ni.e., the small, medium and large datasets\nwith 2M, 16M and 128M keys, respectively, are used.\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:batch} shows the result of query\nthroughput with respect to query batch size for the three datasets.\nIt can be seen that the size of query\nbatches indeed affects query throughput.\nIn particular, as the size of\nquery batch increases,\nthe throughput first undergoes a moderate increase.\nThis is due to the fact\nthat the queries contained in a\nbatch are sorted based on the key value, and\na larger batch size implies a better utilization\nof CPU caches.\nIn addition, there is an interception adjustment\nstage between key search and query execution\nin the processing of each\nquery batch, whose\ncost only depends on the number of running threads, and thus is similar\nacross different batch sizes.\nConsequently, with more queries in a\nsingle batch, the number of interception adjustments can be reduced,\nwhich in turn translates into an increase in query throughput.\n\nFigure~\\ref{fig:batch} also\ndemonstrates that the effect of batch size exerting on query throughput is\nmore significant for smaller datasets than for larger datasets.\nThe reasons are as follows.\nFor query batches of the same size, the\nprocessing time increases with the size of dataset,\nas we have already\nshown in Figure~\\ref{fig:dataset}.\n\nTherefore, for smaller datasets, the\nadditional time spent in interception adjustment and warming up the cache, \nwhich is similar across the three datasets,\nplays a more important role than\nfor larger datasets.\n\nAs a result, smaller datasets benefit more from the increase in query batch size\nthan larger datasets do.\n\n\n\\begin{figure}[tb]\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{batchsize.pdf}\n\t\\caption{Query throughput vs batch size}\n\t\\label{fig:batch}\n\\end{figure}\nIt can be seen from Figure~\\ref{fig:batch} that PI performs reasonably well\nunder the default setting of 8192 for batch size, but there still remains space\nof performance improvement for small datasets. Hence, there is no\none-size-fits-all optimal setting for batch size, and we leave behind\nthe automatic determination of optimal batch size as future work.\n\n\\subsection{Scalability}\n\\begin{figure*}[tbp]\n\t\\centering\n\t\\subfigure[dataset size = 2M] {\n\t\t\\includegraphics[width=.30\\textwidth]{2M.pdf}\n\t\t\\label{fig:thread:2m}\n\t}\n\t\\subfigure[dataset size = 16M] {\n\t\t\\includegraphics[width=.30\\textwidth]{16M.pdf}\n\t\t\\label{fig:thread:16m}\n\t}\n\t\\subfigure[dataset size = 128M] {\n\t\t\\includegraphics[width=.30\\textwidth]{128M.pdf}\n\t\t\\label{fig:thread:128m}\n\t}\n\t\\caption{Query throughput vs thread number}\n\t\\label{fig:thread}\n\\end{figure*}\n\n\n\nFigure~\\ref{fig:thread} \nshows how the query throughput of PI and of Masstree varies\nwith the number of execution threads.\nThe threads and the index are both evenly distributed among the\nfour NUMA nodes.\nFor the case in which there are $n < 4$ execution threads, threads and\nthe whole index are evenly distributed among the same number of NUMA nodes which\nare randomly selected from the available four.\nWe use the three datasets, i.e., 2M, 16M and 256M respectively, for this experiment.\n\nIt can be seen from Figure~\\ref{fig:thread} that\napparently, both PI and Masstree can get their query throughput to \nincrease significantly with more computing resources, but there exists a substantial gap in \nthe rate of improvement between PI and Masstree, especially in the cases with a\nsmall number of threads.\nIn PI, when the number of threads\nchanges from 1 to 4, the throughput undergoes a super-linear increase. \n\n    This is because when the number of threads is no larger than 4, the whole index is\n    evenly distributed among the same number of NUMA nodes. Consequently,\n    the index size, i.e., the number of entries and data nodes, halves when the number\nof threads doubles.\n\nWith a smaller index size, cache can be more effectively\nutilized, leading to an increased single-thread throughout, and hence a\nsuper-linear increase in aggregate throughout. Since smaller indices are more cache\nsensitive than larger ones, they benefit more from the increase of the number of\nthreads in terms of the throughput of query processing, as can be observed from\nFigure~\\ref{fig:thread}.\n\nWhen the number of threads\ncontinues to increase, the increase rate \nin query throughput of PI gradually slows down,\nbut is still always better than or equal to that of Masstree. \nThis flattening can\nbe attributed to the adjustment of interceptions which take place when there are\nmore than one thread servicing the queries in a NUMA node. And since the cost of\ninterception adjustment only depends on the number of threads, it is same across the\nindices with different sizes, and hence accounts for a larger fraction of query\nprocessing time for smaller indices. This also explains why query\nthroughput of smaller indices drops faster than that of larger ones. \n\n\\subsection{Mixed Query Workload}\n\n\n\\begin{figure}[tbp]\n\t\\centering\n\t\\includegraphics[width=.9\\linewidth]{updateratio_0.pdf}\n\t\\caption{Query throughput of mixed workloads}\n\t\\label{fig:mix}\n\\end{figure}\nIn order to thoroughly profile the performance of PI,\nwe study how it\nbehaves in the presence of query workload consisting of \nvarious types of queries.\nAs before, we conduct this\nexperiment over three default datasets \n\n(2M, 16M and 128M).\n\nThe query workload consists of keys following a uniform distribution,\nand the entire index layer is rebuilt\nonce a specified number (15\\%\nof dataset size) of data nodes have been inserted into the index.\n\nFigure~\\ref{fig:mix} shows the result when the ratio of insert queries\nincreases from 0\\% to 100\\%. For each ratio, the number of queries issued\nto the index is such that the index layer is rebuilt for the\nsame number as for other ratios.\nAs shown in Figure~\\ref{fig:mix}, with the increase of updates in the query\nworkload, the throughput of PI undergoes a slight decrease as a result\nof more data nodes in storage layer being traversed, demonstrating \nPI's capability in processing uniform query workload. \n\n\tMasstree also experiences a similar variance trend \n    in the query throughput.\n\n\n\n\n\\subsection{Resistance to Skewness}\n\\label{ssub:skew}\n\\begin{figure*}[tb]\n\t\\centering\n\t\\subfigure[$\\theta=0.5$] {\n\t\t\\includegraphics[width=.40\\textwidth]{updateratio_5.pdf}\n\t\t\\label{fig:ycsb:updateratio0.5}\n\t}\n\t\\subfigure[$\\theta=0.9$] {\n\t\t\\includegraphics[width=.40\\textwidth]{updateratio_9.pdf}\n\t\t\\label{fig:ycsb:updateratio0.9}\n\t}\n\t\\caption{Query throughput vs query skewness (with self-adjusted threading)}\n\t\\label{fig:ycsbratio}\n\\end{figure*}\n\n\\begin{figure*}[tb]\n\t\\centering\n\t\\subfigure[$\\theta=0.5$] {\n\t\t\\includegraphics[width=.40\\textwidth]{uniformratio5.pdf}\n\t\t\\label{fig:ycsb:uniformratio0.5}\n\t}\n\t\\subfigure[$\\theta=0.9$] {\n\t\t\\includegraphics[width=.40\\textwidth]{uniformratio9.pdf}\n\t\t\\label{fig:ycsb:uniformratio0.9}\n\t}\n\t\\caption{Query throughput vs query skewness (without self-adjusted threading)}\n\t\\label{fig:threading}\n\\end{figure*}\n\n\nIn this section, PI's performance of query processing is explored\nin the presence of query skewness.\nAs before, there are eight threads running over\nthe four NUMA nodes, and three datasets with default sizes\n\n(2M, 16M and 128M)\n\n are used.\nThe skewness in query workload \nis realized via varying the probability parameter, $\\theta$, of zipfian distribution\nfor workload generation.\nAn intuitive impression on the skewness of the query\nworkloads used in this section is given in Appendix~\\ref{sub:ycsbworkload} .\n\n\nFigure~\\ref{fig:ycsbratio} exhibits the variation in the throughput\nquery processing with respect to query skewness and update ratio in\nquery workloads. By comparing this figure with Figure~\\ref{fig:mix},\nwe can observe that query skewness has only a little impact on the \nperformance of PI in terms of query processing. \nWe attribute this resistance to query skewness of PI to the self-adjusted\nthreading mechanism presented in Section~\\ref{subsec:load}, which \ndynamically allocates computing resources among NUMA nodes \nbased on the query load on each node. In fact, for the query workload\nwith zipfian probability parameter $\\theta = 0.5$, the numbers of threads\nspawned at four NUMA nodes are 3, 2, 2 and 1, respectively, and for \nthe other query workload with $\\theta = 0.9$,\nthese numbers become 4, 2, 1 and 1, respectively.\nFor comparison purposes, the corresponding result measured with \nthe threading mechanism disabled is shown in Figure~\\ref{fig:threading}, \nfrom which one can see that the threading mechanism does significantly \nenhance the throughput.\n\nIt should also be noted that the skewness in query workload does not\nalways exert a negative impact on the throughput.\nWhen fed with a workload consisting of pure search queries against keys\nfollowing a zipfian distribution with $\\theta = 0.5$, PI is able to achieve\na throughput that is even higher than what is achieved in the case of no query \nskewness, as shown in Figure~\\ref{fig:mix} and \\ref{fig:ycsb:updateratio0.5}.\nThis is probably because in\nthe NUMA node with the most amount of query load, each of the three spawned\nthread accesses only\nan even restricted portion of the index, and hence can utilize the cache \nmore efficiently.\n \n\\subsection{Details on YCSB Workload}\n\\label{sub:ycsbworkload}\n\nFigure~\\ref{fig:heatmap} shows the variance of skewness in the key distribution\nof the three YCSB workloads for the experiment of\nSection~\\ref{ssub:skew}. \n\nIn this figure, each circle consists of 100 sectors separating the whole\nkey space into 100 disjoint ranges with equal coverage, \nand the color of a sector represents how frequently the keys within the\nrelative range are queried. \n\nSince the keys in each YCSB workload follow a zipfian\ndistribution, the workload becomes more skewed with the increase of probability\nparameter $\\theta$. However, as shown in Figure~\\ref{fig:ycsbratio}, the\nskewness in the query workload rarely affects the performance of PI\n\n(as well as Masstree)\n\n, which\nis also validated in Figure~\\ref{fig:workload}, where the zipfian parameter in\nthree YCSB query workloads, each with a different update ratio (0.5, 0.05 and 0,\nrespectively), are varied from 0 to 0.9 \nto investigate how PI can adapt to query skewness. \n\\begin{figure*}[!htbp]\n\t\\centering\n\t\\subfigure[$\\theta=0$] {\n\t\t\\includegraphics[width=.28\\textwidth]{heatmap0.png}\n\t\t\\label{fig:heatmap0}\n\t}\n\t\\subfigure[$\\theta=0.5$] {\n\t\t\\includegraphics[width=.28\\textwidth]{heatmap5.png}\n\t\t\\label{fig:heatmap5}\n\t}\n\t\\subfigure[$\\theta=0.9$] {\n\t\t\\includegraphics[width=.28\\textwidth]{heatmap9.png}\n\t\t\\label{fig:heatmap9}\n\t}\n\t\\caption{Key distribution in three YCSB query workloads}\n\t\\label{fig:heatmap}\n\\end{figure*}\n\n\\begin{figure*}[!htb]\n\t\\centering\n\t\\subfigure[workload A] {\n\t\t\\includegraphics[width=.3\\textwidth]{workloadA.pdf}\n\t\t\\label{fig:workloadA}\n\t}\n\t\\subfigure[workload B] {\n\t\t\\includegraphics[width=.3\\textwidth]{workloadB.pdf}\n\t\t\\label{fig:workloadB}\n\t}\n\t\\subfigure[workload C] {\n\t\t\\includegraphics[width=.3\\textwidth]{workloadC.pdf}\n\t\t\\label{fig:workloadC}\n\t}\n\t\\caption{Query throughput vs skewness}\n\t\\label{fig:workload}\n\\end{figure*}\n\n\\subsection{Range Queries}\n\\label{sub:range}\n\n\\begin{figure}[tbp]\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{select.pdf}\n\t\\caption{Query throughput of range query}\n\t\\label{fig:range}\n\\end{figure}\nFigure~\\ref{fig:range} describes how the performance of range queries\nvaries with granularity, by which we mean the average number of results (data nodes)\nreturned for a given range query. In this experiment, only the performance of\nsearch queries is explored, and the same number of query batches, each with 8192\nrange queries, are issued for each dataset.\nFor comparison, the result for point query (granularity = 1)\nis also shown in this figure.\n\n\n\nIt can be observed from Figure~\\ref{fig:range} that query throughput decreases\nwith the granularity of range query at a constant rate for each dataset.\nDue to the better cache utilization of the index built from smaller datasets, the\nquery throughput decreases a little more slowly for smaller datasets than\nfor larger datasets. For the granularity of 1000, \n\nthe processing of each query batch\naccesses almost 8M of data nodes, which means there are many data nodes being accessed\nmultiple times for the small and medium datasets with 2M and 16M keys, respectively.\nHence, the number of slow memory accesses incurred by reading entries and data nodes\nis further reduced by a larger amount for these two datasets than for the large dataset,\nand this is the reason to that the search throughput for the large dataset drops much\nfaster than that for the other two datasets when the granularity varies from 100 to 1000.\n\n\n\n\\subsection{Effects of Optimizations}\n\\label{sub:effect_of_different_optimizations}\n\n\nWe now explore how the optimization techniques such as\nSIMD processing, NUMA-aware index partitioning,\nprefetching and group query processing,\naffect the performance of PI.\nThe impact of organizing queries into batches\nhas already been studied in Section~\\ref{subsec:batch},\nand hence is not discussed here. The dataset used for this\nexperiment contains 16M keys, and the other parameters\nare set to the default sizes.\n\nFigure~\\ref{fig:speedup} shows the breakdown of the gap\nbetween the query performance of PI and a typical\nskip list with none of the optimizations enabled.\nBy grouping the keys appearing at higher layers into entries,\nand leveraging SIMD to significantly reduce the number\nof key comparisons and hence memory/cache accesses,\nthe throughput of PI can be improved by 1.3x and 1x\nfor search and insert queries, respectively.\nNUMA-aware operation leads to another huge performance gain,\nimproving the query throughput by 1.2x for both kinds\nof queries. This performance gain is because our NUMA-aware\noptimization largely eliminates\naccessing the memory of remote NUMA nodes,\nwhich is several times slower than accessing local memory.\nGroup query processing brings in a slight improvement of 0.05x\nin query throughput, and prefetching contributes to the final\nperformance gain of 0.2x and 0.14x in the throughput of search and insert \nqueries, respectively.\n\n\n\\begin{figure}[tbp]\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{speedup.pdf}\n\t\\caption{Effects of optimizations}\n\t\\label{fig:speedup}\n\\end{figure}\n\n\\section{conclusion and Future Work}\n\\label{sec::conclusion}\n\nIn this paper, we argue that skip list, due to its high parallezability, is a better candidate for in-memory index\nthan $B^+$-tree in concurrent environment. Based on this argument, we propose\nPI, a cache-friendly, latch-free index that supports both point query and\nrange query. PI consists of an index layer, which is in charge of key search,\nand a storage layer responsible for data retrieval, and the layout of the index\nlayer is carefully designed such that SIMD processing can be applied to \naccelerate key search.  The experimental results show that PI is three times faster than\nMasstree in terms of query throughput. \n\nFor future work, we seek to implement a finer-grained mechanism for the\nrebuilding of the index layer, which is currently conducted against the whole\nindex layer and thus not necessary in the presence of skewed queries which only\nupdate a small portion of the index layer. In addition, we are also exploring\napplying several other optimizations to PI. For instance, cache locality can be\nfurther enhanced by pinning the high levels of the\nindex layer in the cache to prevent them from being evicted in the case of\ninsufficient cache space, and a large memory page size can reduce the number of\nTLB misses incurred by memory accesses.\n\n\n\\bibliographystyle{abbrv}\n\\bibliography{sigproc}  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 21995, "prevtext": "\n    where $t$ and\n    $\\kappa$ are the type and key of $q$, respectively,\n    and if $t$ is insert, $p$ provides the  \n    pointer to the new value associated with key $\\kappa$. \n\\label{def::query}\n\\end{Def}\n\nWe now define the query set $Q$ in\nDefinition~\\ref{def::qset}. There are two points worth mentioning in this\ndefinition. First, the queries in a query set are in non-decreasing order of\nthe query key $k$, and the reason for doing so will be elaborated in\nSection ~\\ref{subsubsec::latchfree}.\nSecond, a query set $Q$ only contains point queries, and we will show\nhow such a query set can be constructed and leveraged to answer range queries in\nSection~\\ref{ssec:range}. \n\n\\begin{Def}\n    A query set $Q$ is given by\n    \n", "index": 5, "text": "$$ Q = \\{q_i| 1\\leq i\\leq N\\}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"Q=\\{q_{i}|1\\leq i\\leq N\\}\" display=\"block\"><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi><mo>\u2264</mo><mi>N</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}]