[{"file": "1601.00078.tex", "nexttext": "\nThat is, \n\n", "itemtype": "equation", "pos": 3695, "prevtext": "\n\\begin{abstract} \n\n\n\n Kagan and Shalaevski \\cite{KaganShalaevski} have shown that if the random variables  ${\\mathbb X}_1,\\dots,{\\mathbb X}_n$ are  independent and identically distributed  and the  distribution of $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2$ $a_i\\in \\mathbb{R}$ depends only on $\\sum_{i=1}^na_i^2$ , then each ${\\mathbb X}_i$ follows the normal distribution $N(0, \\sigma)$. Cook \\cite{Cook} generalized this result  replacing independence of all ${\\mathbb X}_i$ by the independence of  $({\\mathbb X}_1,\\dots, {\\mathbb X}_m) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n )$ and removing the requirement that ${\\mathbb X}_i$ have the same distribution.\n\n\nIn this\npaper, we will give  other characterizations of the normal distribution which are formulated in a similar spirit.\n\\end{abstract}\n\\maketitle\n\n\n\\section{Introduction}\n\\begin{quote}\n\\textit{It will be shown that the formulae are much\nsimplified by the use of cumulative moment\nfunctions, or semi-invariants, in place of the\ncrude moments.\nR.A. Fisher \\cite{Fisher}}.\n\\end{quote}\n\n\nThe original motivation for this paper comes from a desire to understand the results about  characterization of normal distribution which were shown in \\cite{Cook} and \\cite{KaganShalaevski}. They proved, that the characterizations of a normal law  are given by a certain\ninvariance of the  noncentral chi-square distribution. \nIt is a known fact that if ${\\mathbb X}_1,\\dots,{\\mathbb X}_n$ are i.i.d. and following the normal distribution $N(0,\\sigma)$ then the distribution of the statistic $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2$, $a_i\\in \\mathbb{R}$ depends on $\\sum_{i=1}^na_i^2$ only (see  \\cite{Bryc1,Morgan}). Kagan and Shalaevski \\cite{KaganShalaevski} have shown that if the random variables \n${\\mathbb X}_1 ,{\\mathbb X}_2 ,..., {\\mathbb X}_n$ are independent and identically distributed and the distribution\nof $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2$ depends only on $\\sum_{i=1}^na_i^2$, then each ${\\mathbb X}_i$ is normally distributed as $N(0, \\sigma)$. Cook generalized this result  replacing independence of all ${\\mathbb X}_i$ by the independence of  $({\\mathbb X}_1,\\dots, {\\mathbb X}_m) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n )$ and removing the requirement that ${\\mathbb X}_i$ have the same distribution. The theorem proved below gives a new look on this subject, i.e. we will show that in the  statistic $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2=\\sum_{i=1}^n{\\mathbb X}_i^2 +2\\sum_{i=1}^n{\\mathbb X}_ia_i+ \\sum_{i=1}^na_i^2$ only the linear part $\\sum_{i=1}^n {\\mathbb X}_ia_i$ is important. \n In particular, from the above result we get  Cook Theorem from \\cite{Cook},  but under the assumption  \nthat all moments exist. Note that Cook does not assume any moments, but he gets this result under integrability assumptions imposed on the corresponding random variable.  This paper is removing or at least relaxing its integrability assumptions.\n\n\nThe paper is organized as follows. In section 2 we review basic facts about cumulants. Next in the third section we   state and prove the main results (proposition). In this section we also discuss   the problem. \n\n\\section{Cumulants and moments}\n\nCumulants were first defined and studied by the Danish scientist T. N. Thiele.\nHe called them semi-invariants. The importance of cumulants comes from\nthe observation that many properties of random variables can be better\nrepresented by cumulants than by moments. We refer to Brillinger \\cite{Brillinger} and\nGnedenko and Kolmogorov \\cite{G-K} for further detailed probabilistic aspects\nof this topic.\n\nGiven a random variable ${\\mathbb X}$ with the moment generating function $g(t)$, its\n$i$th cumulant $r_i$ is defined as\n\n", "index": 1, "text": "$$r_i({\\mathbb X}):=r_{i}(\\underbrace{ {\\mathbb X},\\dots,{\\mathbb X}}_{i-times})=\\frac{d^i}{dt^i}\\Big|_{t=0}log( g(t)).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"r_{i}({\\mathbb{X}}):=r_{i}(\\underbrace{{\\mathbb{X}},\\dots,{\\mathbb{X}}}_{i-%&#10;times})=\\frac{d^{i}}{dt^{i}}\\Big{|}_{t=0}log(g(t)).\" display=\"block\"><mrow><mrow><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd4f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd4f</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd4f</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mfrac><msup><mi>d</mi><mi>i</mi></msup><mrow><mi>d</mi><mo>\u2062</mo><msup><mi>t</mi><mi>i</mi></msup></mrow></mfrac><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nwhere $m_i$ is the $i$th moment of ${\\mathbb X}$.\n\nGenerally, if $\\sigma$ denotes the standard deviation, then\n\n", "itemtype": "equation", "pos": 3827, "prevtext": "\nThat is, \n\n", "index": 3, "text": "$$\\sum_{i=0}^\\infty\\frac{m_i}{i!}t^i=g(t)=exp\\Big(\\sum_{i=1}^\\infty\\frac{r_i}{i!}t^i\\Big)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=0}^{\\infty}\\frac{m_{i}}{i!}t^{i}=g(t)=exp\\Big{(}\\sum_{i=1}^{\\infty}%&#10;\\frac{r_{i}}{i!}t^{i}\\Big{)}\" display=\"block\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mfrac><msub><mi>m</mi><mi>i</mi></msub><mrow><mi>i</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac><mo>\u2062</mo><msup><mi>t</mi><mi>i</mi></msup></mrow></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mfrac><msub><mi>r</mi><mi>i</mi></msub><mrow><mi>i</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac><mo>\u2062</mo><msup><mi>t</mi><mi>i</mi></msup></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nThe joint cumulant of several random variables ${\\mathbb X}_1, \\dots {\\mathbb X}_n$ of order $(i_1,\\dots,i_n)$, where $i_j$ are nonnegative integers, is defined by a similar  generating function $g(t_1,\\dots,t_n)=E\\big(e^{\\sum_{i=1}^nt_i{\\mathbb X}_i}\\big)$\n\n", "itemtype": "equation", "pos": 4030, "prevtext": "\nwhere $m_i$ is the $i$th moment of ${\\mathbb X}$.\n\nGenerally, if $\\sigma$ denotes the standard deviation, then\n\n", "index": 5, "text": "$$r_1=m_1,\\qquad r_2=m_2-m_1^2=\\sigma, \\qquad r_3=m_3-3m_2m_1+2m_1^3.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"r_{1}=m_{1},\\qquad r_{2}=m_{2}-m_{1}^{2}=\\sigma,\\qquad r_{3}=m_{3}-3m_{2}m_{1}%&#10;+2m_{1}^{3}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>=</mo><msub><mi>m</mi><mn>1</mn></msub></mrow><mo rspace=\"22.5pt\">,</mo><mrow><msub><mi>r</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>m</mi><mn>2</mn></msub><mo>-</mo><msubsup><mi>m</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mo>=</mo><mi>\u03c3</mi></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><msub><mi>r</mi><mn>3</mn></msub><mo>=</mo><mrow><mrow><msub><mi>m</mi><mn>3</mn></msub><mo>-</mo><mrow><mn>3</mn><mo>\u2062</mo><msub><mi>m</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>m</mi><mn>1</mn><mn>3</mn></msubsup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nwhere $t=(t_1,\\dots,t_n).$ \n\n\n\n\n\n\n\n\n\n\n\n\\\\ \\\\\n\\noindent Random variables $ \\mathbb{X}_{1},\\dots ,\\mathbb{X}_{n} $  are  independent if and only if, for every $n \\geq 1$ and every non-constant choice of $\\mathbb{Y}_{i} \\in \\{ \\mathbb{X}_{1},\\dots ,\\mathbb{X}_{n}  \\}$, where $i \\in \\{1,\\dots,k\\}$ (for some positive integer  $k\\geq 2$) we get $r_{k}( \\mathbb{Y}_{1},\\dots ,\\mathbb{Y}_{k} )=0$. \n\\\\ \\\\\nCumulants of some important and familiar random distributions are listed\nas follows:\n\\begin{itemize}\n\\item The Gaussian distribution $N(\\mu,\\sigma)$ possesses the simplest list of cumulants: $r_1=\\mu$, $r_2=\\sigma$ and  $r_n=0$ for $n\\geq 3 $,\n\\item for the Poisson distribution with mean $\\lambda$ we have $r_n=\\lambda$.\n\\end{itemize}\nThese classical examples clearly demonstrate the simplicity and efficiency\nof cumulants for describing random variables. \n\n\nApparently, it is not accidental that cumulants  encode the most important information of the associated\nrandom variables. The underlying reason may well reside in the following\nthree important properties (which are in fact related to each other):\n\\begin{itemize}\n\\item\n(Translation Invariance) For any constant $c$,\n$r_1({\\mathbb X}+c)=c+r_1({\\mathbb X})$ and  $r_n({\\mathbb X}+c)=r_n({\\mathbb X})$,  $n\\geq 2$.\n\\item (Additivity) Let ${\\mathbb X}_1,\\dots, {\\mathbb X}_m$ be any independent random variables.\nThen, $r_n({\\mathbb X}_1+\\dots+{\\mathbb X}_m)=r_n({\\mathbb X}_1)+\\dots+r_n({\\mathbb X}_m)$, $n\\geq 1$.\n\\item  (Commutative property)  $r_n({\\mathbb X}_1,\\dots,{\\mathbb X}_n)=r_n({\\mathbb X}_{\\sigma(1)},\\dots,{\\mathbb X}_{\\sigma(n)})$ for any permutation $\\sigma\\in S_n.$\n\\item (Multilinearity) $r_{k}$ are the $k$-linear maps. \n\\end{itemize}\nFor more details about cumulants and\n probability theory, the reader can consult \\cite{Lehner} or \\cite{Rota} .\n\n\\section{The Characterization theorem}\nThe main result of this paper is the following characterization of normal distribution\nin terms of independent random vectors. \n\n\\begin{prop}\nSuppose vectors $({\\mathbb S}_1,{\\mathbb Y}) \\textrm{ and } ({\\mathbb S}_2,  {\\mathbb Z} )$ with all moments are independent and ${\\mathbb S}_1,{\\mathbb S}_2$ are\nnondegenerate. If for every $a,b \\in \\mathbb{R}$ the linear combination $a{\\mathbb S}_1 + {\\mathbb Y} + b{\\mathbb S}_2 + {\\mathbb Z}$ has the law that\ndepends on $(a, b)$ through $a^2 + b^2$ only, then random variables ${\\mathbb S}_1, {\\mathbb S}_2$ have the same normal distribution \n\nand $cov({\\mathbb S}_1,{\\mathbb Y})=cov({\\mathbb S}_2,{\\mathbb Z})=0$.\n\n\n\n\\label{prop:1}\n\\end{prop}\n\\begin{proof}\nLet $h_k(a^2+b^2)=r_k(a{\\mathbb S}_1+{\\mathbb Y}+b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Y}+{\\mathbb Z})$. \nBecause of the independence of $({\\mathbb S}_1,{\\mathbb Y}) \\textrm{ and } ({\\mathbb S}_2,  {\\mathbb Z} )$  we may write \n\n", "itemtype": "equation", "pos": 4360, "prevtext": "\nThe joint cumulant of several random variables ${\\mathbb X}_1, \\dots {\\mathbb X}_n$ of order $(i_1,\\dots,i_n)$, where $i_j$ are nonnegative integers, is defined by a similar  generating function $g(t_1,\\dots,t_n)=E\\big(e^{\\sum_{i=1}^nt_i{\\mathbb X}_i}\\big)$\n\n", "index": 7, "text": "$$r_{i_1+\\dots +i_n}(\\underbrace{ {\\mathbb X}_1,\\dots,{\\mathbb X}_1}_{i_1-times},\\dots,\\underbrace{ {\\mathbb X}_n,\\dots,{\\mathbb X}_n}_{i_n-times})=\\frac{d^{i_1+\\dots +i_n}}{dt_1^{i_1}\\dots dt_n^{i_n}}\\Big|_{t=0}log( g(t_1,\\dots,t_n)),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"r_{i_{1}+\\dots+i_{n}}(\\underbrace{{\\mathbb{X}}_{1},\\dots,{\\mathbb{X}}_{1}}_{i_%&#10;{1}-times},\\dots,\\underbrace{{\\mathbb{X}}_{n},\\dots,{\\mathbb{X}}_{n}}_{i_{n}-%&#10;times})=\\frac{d^{i_{1}+\\dots+i_{n}}}{dt_{1}^{i_{1}}\\dots dt_{n}^{i_{n}}}\\Big{|%&#10;}_{t=0}log(g(t_{1},\\dots,t_{n})),\" display=\"block\"><mrow><mrow><mrow><msub><mi>r</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant=\"normal\">\u2026</mi><mo>+</mo><msub><mi>i</mi><mi>n</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><msub><mi>\ud835\udd4f</mi><mn>1</mn></msub><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><msub><mi>\ud835\udd4f</mi><mn>1</mn></msub></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><munder><munder accentunder=\"true\"><mrow><msub><mi>\ud835\udd4f</mi><mi>n</mi></msub><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><msub><mi>\ud835\udd4f</mi><mi>n</mi></msub></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><msub><mi>i</mi><mi>n</mi></msub><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mfrac><msup><mi>d</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant=\"normal\">\u2026</mi><mo>+</mo><msub><mi>i</mi><mi>n</mi></msub></mrow></msup><mrow><mi>d</mi><mo>\u2062</mo><msubsup><mi>t</mi><mn>1</mn><msub><mi>i</mi><mn>1</mn></msub></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><msubsup><mi>t</mi><mi>n</mi><msub><mi>i</mi><mi>n</mi></msub></msubsup></mrow></mfrac><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nEvaluating \\eqref{eq:niezalensoc} first when $b=0$ and then when $a=0$, we get\n$h_k(a^2)=r_k(a{\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})$ and $h_k(b^2)=r_k(b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Z})$, respectively. Substituting this into \\eqref{eq:niezalensoc}, we see\n\n\n \n", "itemtype": "equation", "pos": 7397, "prevtext": "\nwhere $t=(t_1,\\dots,t_n).$ \n\n\n\n\n\n\n\n\n\n\n\n\\\\ \\\\\n\\noindent Random variables $ \\mathbb{X}_{1},\\dots ,\\mathbb{X}_{n} $  are  independent if and only if, for every $n \\geq 1$ and every non-constant choice of $\\mathbb{Y}_{i} \\in \\{ \\mathbb{X}_{1},\\dots ,\\mathbb{X}_{n}  \\}$, where $i \\in \\{1,\\dots,k\\}$ (for some positive integer  $k\\geq 2$) we get $r_{k}( \\mathbb{Y}_{1},\\dots ,\\mathbb{Y}_{k} )=0$. \n\\\\ \\\\\nCumulants of some important and familiar random distributions are listed\nas follows:\n\\begin{itemize}\n\\item The Gaussian distribution $N(\\mu,\\sigma)$ possesses the simplest list of cumulants: $r_1=\\mu$, $r_2=\\sigma$ and  $r_n=0$ for $n\\geq 3 $,\n\\item for the Poisson distribution with mean $\\lambda$ we have $r_n=\\lambda$.\n\\end{itemize}\nThese classical examples clearly demonstrate the simplicity and efficiency\nof cumulants for describing random variables. \n\n\nApparently, it is not accidental that cumulants  encode the most important information of the associated\nrandom variables. The underlying reason may well reside in the following\nthree important properties (which are in fact related to each other):\n\\begin{itemize}\n\\item\n(Translation Invariance) For any constant $c$,\n$r_1({\\mathbb X}+c)=c+r_1({\\mathbb X})$ and  $r_n({\\mathbb X}+c)=r_n({\\mathbb X})$,  $n\\geq 2$.\n\\item (Additivity) Let ${\\mathbb X}_1,\\dots, {\\mathbb X}_m$ be any independent random variables.\nThen, $r_n({\\mathbb X}_1+\\dots+{\\mathbb X}_m)=r_n({\\mathbb X}_1)+\\dots+r_n({\\mathbb X}_m)$, $n\\geq 1$.\n\\item  (Commutative property)  $r_n({\\mathbb X}_1,\\dots,{\\mathbb X}_n)=r_n({\\mathbb X}_{\\sigma(1)},\\dots,{\\mathbb X}_{\\sigma(n)})$ for any permutation $\\sigma\\in S_n.$\n\\item (Multilinearity) $r_{k}$ are the $k$-linear maps. \n\\end{itemize}\nFor more details about cumulants and\n probability theory, the reader can consult \\cite{Lehner} or \\cite{Rota} .\n\n\\section{The Characterization theorem}\nThe main result of this paper is the following characterization of normal distribution\nin terms of independent random vectors. \n\n\\begin{prop}\nSuppose vectors $({\\mathbb S}_1,{\\mathbb Y}) \\textrm{ and } ({\\mathbb S}_2,  {\\mathbb Z} )$ with all moments are independent and ${\\mathbb S}_1,{\\mathbb S}_2$ are\nnondegenerate. If for every $a,b \\in \\mathbb{R}$ the linear combination $a{\\mathbb S}_1 + {\\mathbb Y} + b{\\mathbb S}_2 + {\\mathbb Z}$ has the law that\ndepends on $(a, b)$ through $a^2 + b^2$ only, then random variables ${\\mathbb S}_1, {\\mathbb S}_2$ have the same normal distribution \n\nand $cov({\\mathbb S}_1,{\\mathbb Y})=cov({\\mathbb S}_2,{\\mathbb Z})=0$.\n\n\n\n\\label{prop:1}\n\\end{prop}\n\\begin{proof}\nLet $h_k(a^2+b^2)=r_k(a{\\mathbb S}_1+{\\mathbb Y}+b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Y}+{\\mathbb Z})$. \nBecause of the independence of $({\\mathbb S}_1,{\\mathbb Y}) \\textrm{ and } ({\\mathbb S}_2,  {\\mathbb Z} )$  we may write \n\n", "index": 9, "text": "\\begin{align} & h_k(a^2+b^2)=r_k(a{\\mathbb S}_1+{\\mathbb Y}+b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Y}+{\\mathbb Z})=r_k(a{\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})+r_k(b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Z}). \\label{eq:niezalensoc} \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h_{k}(a^{2}+b^{2})=r_{k}(a{\\mathbb{S}}_{1}+{\\mathbb{Y}}+b{%&#10;\\mathbb{S}}_{2}+{\\mathbb{Z}})-r_{k}({\\mathbb{Y}}+{\\mathbb{Z}})=r_{k}(a{\\mathbb%&#10;{S}}_{1}+{\\mathbb{Y}})-r_{k}({\\mathbb{Y}})+r_{k}(b{\\mathbb{S}}_{2}+{\\mathbb{Z}%&#10;})-r_{k}({\\mathbb{Z}}).\" display=\"inline\"><mrow><mrow><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub></mrow><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><mi>b</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>2</mn></msub></mrow><mo>+</mo><mi>\u2124</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udd50</mi><mo>+</mo><mi>\u2124</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub></mrow><mo>+</mo><mi>\ud835\udd50</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>b</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>2</mn></msub></mrow><mo>+</mo><mi>\u2124</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u2124</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " \n\n\nNote that $h_k(u)$ is continuous in $u\\in[0,\\infty)$, which implies $h_k(u)=h_k(1)u$ and so we have \n$h_k(a^2+b^2)=(a^2+b^2)h_k(1)=(a^2+b^2)\\varphi_k(\\alpha,\\beta)$, where \n", "itemtype": "equation", "pos": -1, "prevtext": "\nEvaluating \\eqref{eq:niezalensoc} first when $b=0$ and then when $a=0$, we get\n$h_k(a^2)=r_k(a{\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})$ and $h_k(b^2)=r_k(b{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Z})$, respectively. Substituting this into \\eqref{eq:niezalensoc}, we see\n\n\n \n", "index": 11, "text": "$$h_k(a^2+b^2)=h_k(a^2)+h_k(b^2).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"h_{k}(a^{2}+b^{2})=h_{k}(a^{2})+h_{k}(b^{2}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>a</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>b</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " \nwith $\\alpha^2+\\beta^2=1$.  In the next part of the proof, we will  compare polynomial,  which give us the correct\ncumulants values. \nLet's first consider\nthe following equation \n", "itemtype": "equation", "pos": -1, "prevtext": " \n\n\nNote that $h_k(u)$ is continuous in $u\\in[0,\\infty)$, which implies $h_k(u)=h_k(1)u$ and so we have \n$h_k(a^2+b^2)=(a^2+b^2)h_k(1)=(a^2+b^2)\\varphi_k(\\alpha,\\beta)$, where \n", "index": 13, "text": "$$\\varphi_k(\\alpha,\\beta)=h_k(\\alpha^2+\\beta^2)=r_k(\\alpha{\\mathbb S}_1+{\\mathbb Y}+\\beta{\\mathbb S}_2+{\\mathbb Z})-r_k({\\mathbb Y}+{\\mathbb Z}), $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\varphi_{k}(\\alpha,\\beta)=h_{k}(\\alpha^{2}+\\beta^{2})=r_{k}(\\alpha{\\mathbb{S}}%&#10;_{1}+{\\mathbb{Y}}+\\beta{\\mathbb{S}}_{2}+{\\mathbb{Z}})-r_{k}({\\mathbb{Y}}+{%&#10;\\mathbb{Z}}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03c6</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03b1</mi><mn>2</mn></msup><mo>+</mo><msup><mi>\u03b2</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub></mrow><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>2</mn></msub></mrow><mo>+</mo><mi>\u2124</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udd50</mi><mo>+</mo><mi>\u2124</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " which gives us \n\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \nwith $\\alpha^2+\\beta^2=1$.  In the next part of the proof, we will  compare polynomial,  which give us the correct\ncumulants values. \nLet's first consider\nthe following equation \n", "index": 15, "text": "$$h_k( a^2)= a^2\\varphi_k(1,0),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"h_{k}(a^{2})=a^{2}\\varphi_{k}(1,0),\" display=\"block\"><mrow><mrow><mrow><msub><mi>h</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>a</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\u03c6</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nFor $k=1$ we get  $E({\\mathbb S}_1)=0$, because $r_1(a{\\mathbb S}_1+{\\mathbb Y})=E(a{\\mathbb S}_1+{\\mathbb Y})$.\nBy putting $k=2$ in \\eqref{eq:1} and using $r_2(a{\\mathbb S}_1+{\\mathbb Y})=Var(a{\\mathbb S}_1+{\\mathbb Y})=a^2Var({\\mathbb S}_1)+2acov({\\mathbb S}_1,{\\mathbb Y})+Var({\\mathbb Y})$ we see  \n", "itemtype": "equation", "pos": 8513, "prevtext": " which gives us \n\n\n", "index": 17, "text": "\\begin{align} r_k(a{\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})=a^2(r_k({\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})). \\label{eq:1} \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle r_{k}(a{\\mathbb{S}}_{1}+{\\mathbb{Y}})-r_{k}({\\mathbb{Y}})=a^{2}(%&#10;r_{k}({\\mathbb{S}}_{1}+{\\mathbb{Y}})-r_{k}({\\mathbb{Y}})).\" display=\"inline\"><mrow><mrow><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub></mrow><mo>+</mo><mi>\ud835\udd50</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>+</mo><mi>\ud835\udd50</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " for all $a\\in{\\mathbb R}$, which implies that    $cov( {\\mathbb S}_1, {\\mathbb Y})=0$.\n\n\n\nNow by expanding equation \\eqref{eq:1}  ($r_k$ are $k$-linear maps, we also  use independence),\n\nwe may write \n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nFor $k=1$ we get  $E({\\mathbb S}_1)=0$, because $r_1(a{\\mathbb S}_1+{\\mathbb Y})=E(a{\\mathbb S}_1+{\\mathbb Y})$.\nBy putting $k=2$ in \\eqref{eq:1} and using $r_2(a{\\mathbb S}_1+{\\mathbb Y})=Var(a{\\mathbb S}_1+{\\mathbb Y})=a^2Var({\\mathbb S}_1)+2acov({\\mathbb S}_1,{\\mathbb Y})+Var({\\mathbb Y})$ we see  \n", "index": 19, "text": "$$2a cov( {\\mathbb S}_1, {\\mathbb Y})+a^2 Var( {\\mathbb S}_1)=a^2(2cov( {\\mathbb S}_1, {\\mathbb Y})+Var( {\\mathbb S}_1)), $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"2acov({\\mathbb{S}}_{1},{\\mathbb{Y}})+a^{2}Var({\\mathbb{S}}_{1})=a^{2}(2cov({%&#10;\\mathbb{S}}_{1},{\\mathbb{Y}})+Var({\\mathbb{S}}_{1})),\" display=\"block\"><mrow><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>,</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>,</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>V</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nfor $k\\geq 2$.\n This gives us  $r_k({\\mathbb S}_1)=0$ for $k>2$ and we have actually proved that ${\\mathbb S}_1$ have the normal distribution  with zero mean.  Analogously, we will show that   $cov( {\\mathbb S}_2, {\\mathbb Z})=0$ and  normality of  ${\\mathbb S}_2.$ \n\\\\\n\\\\\n\\noindent \nThe next example presents an analogous construction for $h_k( a^2)= a^2\\varphi_k^{0,1}(1)$, which involves the element $\\varphi_k^{0,1}(1)$ instead of  $\\varphi_k^{1,0}(1)$ which leads to $2a cov( {\\mathbb S}_1, {\\mathbb Y})+a^2 Var( {\\mathbb S}_1)=a^2(2cov( {\\mathbb S}_2, {\\mathbb Z})+Var( {\\mathbb S}_2)) $. But in the previous paragraph we calculated that $cov( {\\mathbb S}_1, {\\mathbb Y})=cov( {\\mathbb S}_2, {\\mathbb Z})=0$ which means that $Var( {\\mathbb S}_1)=Var( {\\mathbb S}_2)$ (we have common variance), i.e. we have the same distribution. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}\n\\noindent\nAs a corollary we get the following theorem.\n\\begin{theo}\nLet $({\\mathbb X}_1,\\dots, {\\mathbb X}_m,{\\mathbb Y}) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n,{\\mathbb Z})$ be independent  random vectors with all moments, where  ${\\mathbb X}_i$ are\nnondegenerate,  and let statistic $\\sum_{i=1}^na_i{\\mathbb X}_i+{\\mathbb Y}+{\\mathbb Z}$\nhave a distribution which depends only on $\\sum_{i=1}^n a_i^2$, where $a_i\\in \\mathbb{R}$ and $1\\leq m < n$.  Then ${\\mathbb X}_i $ are independent and have the same normal distribution with zero means and $cov({\\mathbb X}_i,{\\mathbb Y})=cov({\\mathbb X}_i,{\\mathbb Z})=0$ for $i\\in\\{1,\\dots,n\\}$. \n\\label{tw:Main1}\n\\end{theo}\n\\begin{proof}\nWithout loss of generality, we may assume\nthat $m\\geq 2.$\nIf we put \n", "itemtype": "equation", "pos": 9284, "prevtext": " for all $a\\in{\\mathbb R}$, which implies that    $cov( {\\mathbb S}_1, {\\mathbb Y})=0$.\n\n\n\nNow by expanding equation \\eqref{eq:1}  ($r_k$ are $k$-linear maps, we also  use independence),\n\nwe may write \n\n\n", "index": 21, "text": "\\begin{align} \\sum_{i=1}^ka^{i} {k \\choose i} r_k(\\underbrace{ {\\mathbb S}_1,\\dots,{\\mathbb S}_1}_{i-times},\\underbrace{ {\\mathbb Y},\\dots,{\\mathbb Y}}_{k-i-times})=a^2(r_k({\\mathbb S}_1+{\\mathbb Y})-r_k({\\mathbb Y})), \\label{eq:2} \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}a^{i}{k\\choose i}r_{k}(\\underbrace{{\\mathbb{S}}_{1}%&#10;,\\dots,{\\mathbb{S}}_{1}}_{i-times},\\underbrace{{\\mathbb{Y}},\\dots,{\\mathbb{Y}}%&#10;}_{k-i-times})=a^{2}(r_{k}({\\mathbb{S}}_{1}+{\\mathbb{Y}})-r_{k}({\\mathbb{Y}})),\" display=\"inline\"><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msup><mi>a</mi><mi>i</mi></msup><mo>\u2062</mo><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>k</mi><mi>i</mi></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo>,</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd50</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd50</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>k</mi><mo>-</mo><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>+</mo><mi>\ud835\udd50</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " and $a=\\sqrt{\\sum_{i=1}^m a_i^2}$, $b=\\sqrt{\\sum_{i=m+1}^n a_i^2}$, in Proposition \\ref{prop:1} then we get  that the distribution of\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nfor $k\\geq 2$.\n This gives us  $r_k({\\mathbb S}_1)=0$ for $k>2$ and we have actually proved that ${\\mathbb S}_1$ have the normal distribution  with zero mean.  Analogously, we will show that   $cov( {\\mathbb S}_2, {\\mathbb Z})=0$ and  normality of  ${\\mathbb S}_2.$ \n\\\\\n\\\\\n\\noindent \nThe next example presents an analogous construction for $h_k( a^2)= a^2\\varphi_k^{0,1}(1)$, which involves the element $\\varphi_k^{0,1}(1)$ instead of  $\\varphi_k^{1,0}(1)$ which leads to $2a cov( {\\mathbb S}_1, {\\mathbb Y})+a^2 Var( {\\mathbb S}_1)=a^2(2cov( {\\mathbb S}_2, {\\mathbb Z})+Var( {\\mathbb S}_2)) $. But in the previous paragraph we calculated that $cov( {\\mathbb S}_1, {\\mathbb Y})=cov( {\\mathbb S}_2, {\\mathbb Z})=0$ which means that $Var( {\\mathbb S}_1)=Var( {\\mathbb S}_2)$ (we have common variance), i.e. we have the same distribution. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{proof}\n\\noindent\nAs a corollary we get the following theorem.\n\\begin{theo}\nLet $({\\mathbb X}_1,\\dots, {\\mathbb X}_m,{\\mathbb Y}) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n,{\\mathbb Z})$ be independent  random vectors with all moments, where  ${\\mathbb X}_i$ are\nnondegenerate,  and let statistic $\\sum_{i=1}^na_i{\\mathbb X}_i+{\\mathbb Y}+{\\mathbb Z}$\nhave a distribution which depends only on $\\sum_{i=1}^n a_i^2$, where $a_i\\in \\mathbb{R}$ and $1\\leq m < n$.  Then ${\\mathbb X}_i $ are independent and have the same normal distribution with zero means and $cov({\\mathbb X}_i,{\\mathbb Y})=cov({\\mathbb X}_i,{\\mathbb Z})=0$ for $i\\in\\{1,\\dots,n\\}$. \n\\label{tw:Main1}\n\\end{theo}\n\\begin{proof}\nWithout loss of generality, we may assume\nthat $m\\geq 2.$\nIf we put \n", "index": 23, "text": "\\begin{align} {\\mathbb S}_1=\\frac{\\sum_{i=1}^m a_i{\\mathbb X}_i}{\\sqrt{\\sum_{i=1}^m a_i^2}} \\textrm{ and }{\\mathbb S}_2=\\frac{\\sum_{i=m+1}^n a_i{\\mathbb X}_i}{{\\sqrt{\\sum_{i=m+1}^n a_i^2}}}\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbb{S}}_{1}=\\frac{\\sum_{i=1}^{m}a_{i}{\\mathbb{X}}_{i}}{\\sqrt%&#10;{\\sum_{i=1}^{m}a_{i}^{2}}}\\textrm{ and }{\\mathbb{S}}_{2}=\\frac{\\sum_{i=m+1}^{n%&#10;}a_{i}{\\mathbb{X}}_{i}}{{\\sqrt{\\sum_{i=m+1}^{n}a_{i}^{2}}}}\" display=\"inline\"><mrow><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><msqrt><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mfrac></mstyle><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udd4a</mi><mn>2</mn></msub></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>n</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><msqrt><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>n</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mfrac></mstyle></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\ndepends only on $a^2+b^2=\\sum_{i=1}^n a_i^2$ ,which by Proposition  \\ref{prop:1} implies that $\\sum_{i=1}^m a_i{\\mathbb X}_i$ \n\nhave the normal distribution and $cov(\\sum_{i=1}^m a_i{\\mathbb X}_i,{\\mathbb Y})=0$ for all $a_i\\in{\\mathbb R}$.  Now, we once again use Proposition  \\ref{prop:1} with   ${\\mathbb S}_1={\\mathbb X}_1, \\textrm{  }{\\mathbb S}_2={\\mathbb X}_{m+1},$ \n\nthen we see from assumption that the distribution of\n\n", "itemtype": "equation", "pos": 11489, "prevtext": " and $a=\\sqrt{\\sum_{i=1}^m a_i^2}$, $b=\\sqrt{\\sum_{i=m+1}^n a_i^2}$, in Proposition \\ref{prop:1} then we get  that the distribution of\n\n", "index": 25, "text": "$${\\mathbb S}_1a+{\\mathbb Y}+{\\mathbb S}_2b+{\\mathbb Z}=\\sum_{i=1}^m a_i{\\mathbb X}_i+{\\mathbb Y}+\\sum_{i=m+1}^na_i{\\mathbb X}_i+{\\mathbb Z},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{S}}_{1}a+{\\mathbb{Y}}+{\\mathbb{S}}_{2}b+{\\mathbb{Z}}=\\sum_{i=1}^{m}a_%&#10;{i}{\\mathbb{X}}_{i}+{\\mathbb{Y}}+\\sum_{i=m+1}^{n}a_{i}{\\mathbb{X}}_{i}+{%&#10;\\mathbb{Z}},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>\ud835\udd4a</mi><mn>1</mn></msub><mo>\u2062</mo><mi>a</mi></mrow><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><msub><mi>\ud835\udd4a</mi><mn>2</mn></msub><mo>\u2062</mo><mi>b</mi></mrow><mo>+</mo><mi>\u2124</mi></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>n</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mi>\u2124</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\n(where ${\\mathbb X}_2+{\\mathbb Y}$ play a role of ${\\mathbb Y}$ from Proposition  \\ref{prop:1}), depends only on $ a_1^2+a_{m+1}^2$ ($ a_1^2+a_{m+1}^2+1$). This gives us $cov({\\mathbb X}_1,{\\mathbb X}_2+{\\mathbb Y})=0$, but we know  that $cov( {\\mathbb X}_1,{\\mathbb Y})=0$ which implies $cov({\\mathbb X}_1,{\\mathbb X}_2)=0.$ Similarly, we show that  $cov({\\mathbb X}_i,{\\mathbb X}_j)=0$ for $i\\neq j$. Now we use  well known facts from the general theory of probability that\n \nif a random vector has a multivariate normal distribution (joint normality), then any two or more of its components that are uncorrelated, are independent. This implies that any two or more of its components that are pairwise independent are independent.  \nNormality of linear combinations \n$\\sum_{i=1}^m a_i{\\mathbb X}_i$ for all $a_i\\in{\\mathbb R}$, means\njoint normality of $({\\mathbb X}_1,\\dots, {\\mathbb X}_m)$ (see e.g. the definition of multivariate normal law in Billingsley \\cite{Billingsley}) and taking into account that random variables ${\\mathbb X}_1,\\dots, {\\mathbb X}_m$ are pairwise uncorrelated, we obtain independence of   ${\\mathbb X}_1,\\dots,{\\mathbb X}_m$. \n\\end{proof}\nThe above theorem gives us the main result by Cook \\cite{Cook} but under the additional assumption that all moments exist. Note that Cook does not assume any moments but assumes integrability.\n\n\n\n\\begin{cor}\nLet $({\\mathbb X}_1,\\dots, {\\mathbb X}_m) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n)$ be independent  random vectors  with all moments, where  ${\\mathbb X}_i$ are\nnondegenerate, and let statistic $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2$\nhave a distribution which depends only on $\\sum_{i=1}^n a_i^2$, $a_i\\in \\mathbb{R}$  and $1\\leq m < n$.  Then ${\\mathbb X}_i $ are independent and have the same normal distribution with zero means. \n\\end{cor}\n\\begin{proof}\nIf we put ${\\mathbb Y}=\\sum_{i=1}^m {\\mathbb X}_i^2$ and  ${\\mathbb Z}=\\sum_{i=m+1}^n {\\mathbb X}_i^2$  in Theorem \\ref{tw:Main1} then we get  \n\n", "itemtype": "equation", "pos": 12061, "prevtext": "\ndepends only on $a^2+b^2=\\sum_{i=1}^n a_i^2$ ,which by Proposition  \\ref{prop:1} implies that $\\sum_{i=1}^m a_i{\\mathbb X}_i$ \n\nhave the normal distribution and $cov(\\sum_{i=1}^m a_i{\\mathbb X}_i,{\\mathbb Y})=0$ for all $a_i\\in{\\mathbb R}$.  Now, we once again use Proposition  \\ref{prop:1} with   ${\\mathbb S}_1={\\mathbb X}_1, \\textrm{  }{\\mathbb S}_2={\\mathbb X}_{m+1},$ \n\nthen we see from assumption that the distribution of\n\n", "index": 27, "text": "$$a_1{\\mathbb X}_1+{\\mathbb X}_2+{\\mathbb Y}+a_{m+1}{\\mathbb X}_{m+1}+{\\mathbb Z}$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"a_{1}{\\mathbb{X}}_{1}+{\\mathbb{X}}_{2}+{\\mathbb{Y}}+a_{m+1}{\\mathbb{X}}_{m+1}+%&#10;{\\mathbb{Z}}\" display=\"block\"><mrow><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mn>1</mn></msub></mrow><mo>+</mo><msub><mi>\ud835\udd4f</mi><mn>2</mn></msub><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><msub><mi>a</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mi>\u2124</mi></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "\nThis means that the distribution of $\\sum_{i=1}^m a_i{\\mathbb X}_i+{\\mathbb Y}+\\sum_{i=m+1}^na_i{\\mathbb X}_i+{\\mathbb Z}$ depends only on $\\sum_{i=1}^n a_i^2$, which by Theorem  \\ref{tw:Main1} implies the statement. \n\\end{proof}\n\n A simple modification of the above arguments can be applied to get the following proposition. In this  proposition we assume a bit more than in Proposition \\ref{prop:1} and it offers a bit\nstronger conclusion. \n\n\\begin{prop}\nLet $({\\mathbb X},{\\mathbb Y}) \\textrm{ and } ({\\mathbb Z},{\\mathbb T} )$ be independent  and nondegenerate random vectors  with all moments and let \n$ a{\\mathbb X}+{\\mathbb Y}+ b{\\mathbb Z}+{\\mathbb T}$ and $ {\\mathbb X}+a{\\mathbb Y} +{\\mathbb Z}+b{\\mathbb T}$  \nhave a distribution which depends only on $a^2+b^2$, $a,b\\in \\mathbb{R}$. Then ${\\mathbb X},{\\mathbb Y},{\\mathbb Z},{\\mathbb T} $ are independent and have normal distribution with zero means and $Var({\\mathbb X})=Var({\\mathbb Z})$, $Var({\\mathbb Y})=Var({\\mathbb T})$.  \n\\end{prop}\n\\begin{proof}\nFrom  Proposition \\ref{prop:1} we get that ${\\mathbb X},{\\mathbb Y},{\\mathbb Z},{\\mathbb T} $  have normal distribution with zero means and $Var({\\mathbb X})=Var({\\mathbb Z})$, $Var({\\mathbb Y})=Var({\\mathbb T})$. Now we will show that  ${\\mathbb X}$ and ${\\mathbb Y} $ are independent random variables.\nWe  proceed analogously to the proof of Proposition \\ref{prop:1} with $h_k( a^2+b^2)=r_k( a{\\mathbb X}+{\\mathbb Y}+b{\\mathbb Z} +{\\mathbb T})-r_k({\\mathbb Y}+{\\mathbb T})$, \n which gives us equality \\eqref{eq:2}, i.e. \n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 14133, "prevtext": "\n(where ${\\mathbb X}_2+{\\mathbb Y}$ play a role of ${\\mathbb Y}$ from Proposition  \\ref{prop:1}), depends only on $ a_1^2+a_{m+1}^2$ ($ a_1^2+a_{m+1}^2+1$). This gives us $cov({\\mathbb X}_1,{\\mathbb X}_2+{\\mathbb Y})=0$, but we know  that $cov( {\\mathbb X}_1,{\\mathbb Y})=0$ which implies $cov({\\mathbb X}_1,{\\mathbb X}_2)=0.$ Similarly, we show that  $cov({\\mathbb X}_i,{\\mathbb X}_j)=0$ for $i\\neq j$. Now we use  well known facts from the general theory of probability that\n \nif a random vector has a multivariate normal distribution (joint normality), then any two or more of its components that are uncorrelated, are independent. This implies that any two or more of its components that are pairwise independent are independent.  \nNormality of linear combinations \n$\\sum_{i=1}^m a_i{\\mathbb X}_i$ for all $a_i\\in{\\mathbb R}$, means\njoint normality of $({\\mathbb X}_1,\\dots, {\\mathbb X}_m)$ (see e.g. the definition of multivariate normal law in Billingsley \\cite{Billingsley}) and taking into account that random variables ${\\mathbb X}_1,\\dots, {\\mathbb X}_m$ are pairwise uncorrelated, we obtain independence of   ${\\mathbb X}_1,\\dots,{\\mathbb X}_m$. \n\\end{proof}\nThe above theorem gives us the main result by Cook \\cite{Cook} but under the additional assumption that all moments exist. Note that Cook does not assume any moments but assumes integrability.\n\n\n\n\\begin{cor}\nLet $({\\mathbb X}_1,\\dots, {\\mathbb X}_m) \\textrm{ and } ({\\mathbb X}_{m+1},\\dots,{\\mathbb X}_n)$ be independent  random vectors  with all moments, where  ${\\mathbb X}_i$ are\nnondegenerate, and let statistic $\\sum_{i=1}^n({\\mathbb X}_i+a_i)^2$\nhave a distribution which depends only on $\\sum_{i=1}^n a_i^2$, $a_i\\in \\mathbb{R}$  and $1\\leq m < n$.  Then ${\\mathbb X}_i $ are independent and have the same normal distribution with zero means. \n\\end{cor}\n\\begin{proof}\nIf we put ${\\mathbb Y}=\\sum_{i=1}^m {\\mathbb X}_i^2$ and  ${\\mathbb Z}=\\sum_{i=m+1}^n {\\mathbb X}_i^2$  in Theorem \\ref{tw:Main1} then we get  \n\n", "index": 29, "text": "$$\\sum_{i=1}^m a_i{\\mathbb X}_i+{\\mathbb Y}+\\sum_{i=m+1}^na_i{\\mathbb X}_i+{\\mathbb Z} =\\sum_{i=1}^n({\\mathbb X}_i+a_i/2)^2-\\frac{1}{4} \\times\\sum_{i=1}^n a_i^2.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{m}a_{i}{\\mathbb{X}}_{i}+{\\mathbb{Y}}+\\sum_{i=m+1}^{n}a_{i}{\\mathbb%&#10;{X}}_{i}+{\\mathbb{Z}}=\\sum_{i=1}^{n}({\\mathbb{X}}_{i}+a_{i}/2)^{2}-\\frac{1}{4}%&#10;\\times\\sum_{i=1}^{n}a_{i}^{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mi>\ud835\udd50</mi><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>n</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mi>\u2124</mi></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udd4f</mi><mi>i</mi></msub><mo>+</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>/</mo><mn>2</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac><mo>\u00d7</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " \n for  $k\\geq 2$.  From this we conclude  that \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThis means that the distribution of $\\sum_{i=1}^m a_i{\\mathbb X}_i+{\\mathbb Y}+\\sum_{i=m+1}^na_i{\\mathbb X}_i+{\\mathbb Z}$ depends only on $\\sum_{i=1}^n a_i^2$, which by Theorem  \\ref{tw:Main1} implies the statement. \n\\end{proof}\n\n A simple modification of the above arguments can be applied to get the following proposition. In this  proposition we assume a bit more than in Proposition \\ref{prop:1} and it offers a bit\nstronger conclusion. \n\n\\begin{prop}\nLet $({\\mathbb X},{\\mathbb Y}) \\textrm{ and } ({\\mathbb Z},{\\mathbb T} )$ be independent  and nondegenerate random vectors  with all moments and let \n$ a{\\mathbb X}+{\\mathbb Y}+ b{\\mathbb Z}+{\\mathbb T}$ and $ {\\mathbb X}+a{\\mathbb Y} +{\\mathbb Z}+b{\\mathbb T}$  \nhave a distribution which depends only on $a^2+b^2$, $a,b\\in \\mathbb{R}$. Then ${\\mathbb X},{\\mathbb Y},{\\mathbb Z},{\\mathbb T} $ are independent and have normal distribution with zero means and $Var({\\mathbb X})=Var({\\mathbb Z})$, $Var({\\mathbb Y})=Var({\\mathbb T})$.  \n\\end{prop}\n\\begin{proof}\nFrom  Proposition \\ref{prop:1} we get that ${\\mathbb X},{\\mathbb Y},{\\mathbb Z},{\\mathbb T} $  have normal distribution with zero means and $Var({\\mathbb X})=Var({\\mathbb Z})$, $Var({\\mathbb Y})=Var({\\mathbb T})$. Now we will show that  ${\\mathbb X}$ and ${\\mathbb Y} $ are independent random variables.\nWe  proceed analogously to the proof of Proposition \\ref{prop:1} with $h_k( a^2+b^2)=r_k( a{\\mathbb X}+{\\mathbb Y}+b{\\mathbb Z} +{\\mathbb T})-r_k({\\mathbb Y}+{\\mathbb T})$, \n which gives us equality \\eqref{eq:2}, i.e. \n\n\n\n\n\n\n\n\n\n", "index": 31, "text": "\\begin{align} \\sum_{i=1}^ka^{i} {k \\choose i} r_k(\\underbrace{ {\\mathbb X},\\dots,{\\mathbb X}}_{i-times},\\underbrace{ {\\mathbb Y},\\dots,{\\mathbb Y}}_{k-i-times})=a^2(r_k({\\mathbb X}+{\\mathbb Y})-r_k({\\mathbb Y})).\\label{twr:pom1} \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}a^{i}{k\\choose i}r_{k}(\\underbrace{{\\mathbb{X}},%&#10;\\dots,{\\mathbb{X}}}_{i-times},\\underbrace{{\\mathbb{Y}},\\dots,{\\mathbb{Y}}}_{k-%&#10;i-times})=a^{2}(r_{k}({\\mathbb{X}}+{\\mathbb{Y}})-r_{k}({\\mathbb{Y}})).\" display=\"inline\"><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msup><mi>a</mi><mi>i</mi></msup><mo>\u2062</mo><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>k</mi><mi>i</mi></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd4f</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd4f</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo>,</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd50</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd50</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>k</mi><mo>-</mo><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udd4f</mi><mo>+</mo><mi>\ud835\udd50</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udd50</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": " \n for all $i,l\\in\\mathbb{N}$, $i\\neq 2$. By a similar argument applied to a statistic  $ {\\mathbb X}+a{\\mathbb Y} +{\\mathbb Z}+b{\\mathbb T}$   we get  \n", "itemtype": "equation", "pos": -1, "prevtext": " \n for  $k\\geq 2$.  From this we conclude  that \n\n", "index": 33, "text": "\\begin{align}  r_k(\\underbrace{ {\\mathbb X},\\dots,{\\mathbb X}}_{i-times},\\underbrace{ {\\mathbb Y},\\dots,{\\mathbb Y}}_{l-times})=0\n\\label{twr:pom3} \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle r_{k}(\\underbrace{{\\mathbb{X}},\\dots,{\\mathbb{X}}}_{i-times},%&#10;\\underbrace{{\\mathbb{Y}},\\dots,{\\mathbb{Y}}}_{l-times})=0\" display=\"inline\"><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd4f</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd4f</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo>,</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd50</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd50</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>l</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.00078.tex", "nexttext": "  \nfor all\n$i,l\\in\\mathbb{N}$, $i\\neq 2$   \nwhich together with \\eqref{twr:pom3} gives us independence of ${\\mathbb X}$ and ${\\mathbb Y}$.  Independence of ${\\mathbb Z}$ and ${\\mathbb T}$ follows similarly.\n\n\n\n\n\\end{proof}\n\n\n\\begin{center} Open Problem and Remark\n\\end{center}\n\n\n\\noindent \\textbf{Problem 1.} In Proposition \\ref{prop:1} (Theorem \\ref{tw:Main1}) in this paper we assume that random\nvariables have all moments. I thought it would be interesting to show that we can skip this assumption.  A\nversion of Proposition \\ref{prop:1}, with integrability replaced by the assumption that ${\\mathbb S}_1, {\\mathbb S}_2$ have the\nsame law, can be deduced from known results (note that this version  does not imply Theorem \\ref{tw:Main1}). \n\nHere we sketch the proof\nof a version of Proposition \\ref{prop:1} under reduced moment assumptions but we assume additionally that random variables ${\\mathbb S}_1, {\\mathbb S}_2$ have the same law. Assume that ${\\mathbb S}_1, {\\mathbb S}_2, {\\mathbb Y}, {\\mathbb Z}$ have finite\nmoments of some positive order $p\\geq 3$. Then, for every $\\epsilon>0$ and any two values of $p_i > 0$, where $i\\in\\{1,2\\}$, we see that $E(a{\\mathbb S}_1 + \\epsilon{\\mathbb Y} + b{\\mathbb S}_2 + \\epsilon{\\mathbb Z})^{p_i}$  is a function of $a^2+b^2$. Passing to the limit as $\\epsilon \\to 0$ and using homogeneity, we deduce that\n$E(a{\\mathbb S}_1  + b{\\mathbb S}_2 )^{p_i} = K(a^2 + b^2)^{p_i/2}$ with $K = 2^{-p_i/2}E|{\\mathbb S}_1 + {\\mathbb S}_2|^{p_i} = E|{\\mathbb S}_1|^{p_i} = E|{\\mathbb S}_2|^{p_i}$. Since this\nholds for any two  odd values  $0 < p_1 < p_2 < p$, by  Theorem 2 from \\cite{Braverman} we see that ${\\mathbb S}_1$ is normal (the reason  why we assume $p\\geq 3$ is that Braverman \\cite{Braverman}  assumed that $p_1, p_2$ are odd). \n\n\\noindent \\textbf{Problem 2.} \n At the end it is worthwhile to mention  the  most important characterization which is true in noncommutative\nand classical probability.  In free probability   Bo\\.zejko, Bryc and Ejsmont  proved that the first conditional linear  moment and \n conditional quadratic variances  characterize free Meixner laws (Bo\\.zejko and Bryc \\cite{BoBr}, Ejsmont \\cite{Ejs}). \nLaha-Lukacs type  characterizations  of random variables in free probability are also studied by Szpojankowski,  Weso\\l owski \\cite{SzWes}.\n They give a characterization of noncommutative\nfree-Poisson and free-Binomial variables by properties of the first two conditional moments,\nwhich mimics Lukacs-type assumptions known from classical probability. \nThe article \\cite{Kemp} studies  the asymptotic behavior of the Wigner integrals. Authors prove that a normalized sequence of multiple Wigner integrals (in a fixed order of free Wigner chaos) converges in law to the\nstandard semicircular distribution if and only if the corresponding\nsequence of fourth moments converges to 2, the fourth moment of\nthe semicircular law.  This finding extends the recent results by Nualart and Peccati \\cite{Nualart} to free probability theory. \n\n\n\n At this point it is worth mentioning \\cite{Hiwatashi}, where the Kagan-Shalaevski characterization for free random variable was shown. \n\n\n\n It would be worth asking whether the Theorem \\ref{tw:Main1} is true in free probability theory.\n Unfortunately the above proof of Theorem \\ref{tw:Main1} doesn't work in free probability theory, because free cumulates are noncommutative. But the Proposition \\ref{prop:1} is true in free probability, with nearly the same proof (thus we can only get that ${\\mathbb X}_i$ has  free normal distribution  under the assumption of Theorem \\ref{tw:Main1}). \n\n\n\\begin{center} Acknowledgments\n\\end{center}\nThe author would like to thank  M. Bo\\.zejko  for several discussions and helpful comments during \nthe preparation of this paper. \nThe work was partially supported by  the OPUS grant DEC-2012/05/B/ST1/00626 of National\nCentre of Science and  by the Austrian Science Fund (FWF) Project No P 25510-N26. \n\n\\begin{thebibliography}{99}\n\n\n\\bibitem{Billingsley} Billingsley P.: Probability and measure. John Wiley and Sons, New York, (1995).\n\\bibitem{Brillinger}  Brillinger D. R.: Time Series, Data Analysis and Theory. Holt, Rinehart and Winston, New York, (1975).\n\n\\bibitem{Braverman}  Braverman M.: A characterization of probability distributions by moments of sums of independent random variables. J. Theoret. Probab. 7 (1), 187-198, (1994).\n\n\\bibitem{Bryc1}  Bryc W.: Normal distribution: characterizations with applications. Lecture Notes in\nStatist. 100. Springer, Berlin, (1995). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibitem{BoBr} Bo\\.zejko M., Bryc W.: On a class of free L\\'evy laws related to a regression problem. \nJ. Funct. Anal. 236, 59-77, (2006).\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibitem{Cook} Cook L.: A characterization of the normal distribution by the independence of a pair of random vectors and a property of the noncentral chi-square statistic. J. Multivariate Anal. 1, 457-460, (1971).\n\\bibitem{Ejs} Ejsmont W.: Laha-Lukacs properties of some free processes. Electron. Commun. Probab., 17 (13), 1-8, (2012). \n\n\n\n\\bibitem{Fisher} Fisher R.: Moments and product moments of sampling distributions. Proc. Lond. Math. Soc.\nSeries 2(30), 199-238, (1929).\n\\bibitem{G-K}  Gnedenko B. V.,  Kolmogorov A.N.: Limit Distributions for Sums of Independent\nRandom Variables. Addison and Wesley, Reading, MA, (1954).\n\\bibitem{Hiwatashi} Hiwatashi O.,  Kuroda T.,  Nagisa M.,  Yoshida H.: The free analogue of noncentral chi-square\ndistributions and symmetric quadratic forms   in free random variables. Math. Z. 230, 63-77, (1999).\n\\bibitem{KaganShalaevski} Kagan A., Shalaevski 0.: Characterization of normal law by a property\nof the non-central $\\chi^2$-distribution. Lithuanian Math. J. 7, 57-58, (1967).\n\\bibitem{Kemp}  Kemp T., Nourdin I., Peccati G., Speicher R.: Wigner chaos and the fourth moment. Ann. Probab. 40, 4, 1577-1635, (2012). \n\n\n\\bibitem{Lehner} Lehner F.: Cumulants in noncommutative probability theory I. Noncommutative Exchangeability Systems, Math. Z. 248, 1, 67-100, (2004).\n\n\\bibitem{Morgan} Moran P.: An Introduction to Probability Theory. Oxford University Press, London, (1968).\n\\bibitem{Nualart} Nualart D. and Peccati G.: Central limit theorems for sequences of multiple stochastic integrals. Ann. Probab. 33, 177-193, (2005). \n\n\n\n\n\n\n\\bibitem{Rota} Rota G.C.,  Jianhong S.: On the combinatorics of cumulants. J. Combin. Theory Ser. A, 91(1), 283-304, (2000).\n\\bibitem{SzWes}  Szpojankowski K.,  Weso\\l owski J.: Dual Luakcs regressions for non-commutative random variables.\nJ. Funct. Anal. 266, 36-54, (2014). \n\\end{thebibliography}\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \n for all $i,l\\in\\mathbb{N}$, $i\\neq 2$. By a similar argument applied to a statistic  $ {\\mathbb X}+a{\\mathbb Y} +{\\mathbb Z}+b{\\mathbb T}$   we get  \n", "index": 35, "text": "$$ r_k(\\underbrace{ {\\mathbb X},\\dots,{\\mathbb X}}_{l-times},\\underbrace{ {\\mathbb Y},\\dots,{\\mathbb Y}}_{i-times})=0$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"r_{k}(\\underbrace{{\\mathbb{X}},\\dots,{\\mathbb{X}}}_{l-times},\\underbrace{{%&#10;\\mathbb{Y}},\\dots,{\\mathbb{Y}}}_{i-times})=0\" display=\"block\"><mrow><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd4f</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd4f</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>l</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo>,</mo><munder><munder accentunder=\"true\"><mrow><mi>\ud835\udd50</mi><mo movablelimits=\"false\">,</mo><mi mathvariant=\"normal\">\u2026</mi><mo movablelimits=\"false\">,</mo><mi>\ud835\udd50</mi></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mi>i</mi><mo>-</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></munder><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}]