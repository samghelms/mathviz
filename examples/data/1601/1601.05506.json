[{"file": "1601.05506.tex", "nexttext": "\n Intuitively, the PageRank vector indicates how much of the diffusion flows from the source node (i.e. the nonzero entry in the vector $\\vv$) to each node in the graph. In our context, this means that $\\vx$ will indicate how much of the diffusion flows from the protein of interest to each other protein in the graph. In our model, we combine proteins and functions into a single network so that this PageRank vector can indicate diffusion flow between proteins and functions.\n PageRank is also called Random Walk with Restart (RWR) in other literature \\citep{tong2006fast}.\n\nThe solution to the Personalized PageRank linear system in Equation~\\eqref{eq:ppr} can be expressed as\n\n\n", "itemtype": "equation", "pos": 16846, "prevtext": "\n\\maketitle\n\n\n\n\n\n\n\n\\footnotetext[1]{Department of Biological Sciences, Purdue University, West Lafayette, IN. }\n\\footnotetext[2]{Department of Mathematics, Purdue University, West Lafayette, IN.}\n\\footnotetext[3]{Computer Science Department, Purdue University, West Lafayette, IN.}\n\n\n\n\n\\begin{abstract}\n\n\\noindent \\textbf{Motivation.} Diffusion-based network models are widely used for protein function prediction using protein network data, and have been shown to outperform neighborhood-based and module-based methods. Recent studies have shown that integrating the hierarchical structure of the Gene Ontology (GO) data dramatically improves prediction accuracy. However, previous methods usually used the GO hierarchy to refine the prediction results of multiple classifiers, or flattened the hierarchy into a function-function similarity kernels. Only rarely has the hierarchy been taken into account as a second layer of the network connecting the protein network with functional annotations. \\medskip\n\n\\noindent \\textbf{Results.} We first construct a Bi-relational graph (Birg) model comprised of both protein-protein association and function-function hierarchical networks. We then propose two diffusion-based methods, BirgRank and AptRank, both of which use PageRank to diffuse information flows on this two-layer graph model. BirgRank is a direct application of traditional PageRank with fixed decay parameters. In contrast, AptRank utilizes an adaptive diffusion mechanism to improve the performance of BirgRank. We evaluate the ability of both methods to predict protein function on yeast, fly, and human protein datasets, and compare with four previous methods: GeneMANIA, TMC, ProteinRank and clusDCA. We find that both BirgRank and AptRank outperform the previous methods, especially when only $10\\%$ of the data are given for training. \\medskip\n\n\\noindent \\textbf{Conclusion.} AptRank naturally combines protein-protein associations and function-function relationships into a two-layer network model, and takes full advantage of the hierarchical structure of the Gene Ontology, using directional diffusion without flattening the ontological hierarchy into a similarity kernel. Introducing an adaptive mechanism to the traditional, fixed-parameter model of PageRank greatly improves the accuracy of protein function prediction.\\medskip\n\n\\noindent \\textbf{Code.} \\url{https://github.rcac.purdue.edu/mgribsko/aptrank}. \\\\\n\\textbf{Contact.} \\href{gribskov@purdue.edu}{gribskov@purdue.edu}\n\\end{abstract}\n\n\n\n\\pagestyle{myheadings}\n\\thispagestyle{plain}\n\\markboth{Jiang \\textit{et~al}.}{An Adaptive pageRank Model for Protein Function Prediction}\n\n\n\\section{Introduction}\n\nGiven a set of functionally uncharacterized genes or proteins from a Genome-Wide Association Study, or differential expression analysis,\nexperimental biologists often have little \\emph{a priori} information available to guide the design of hypothesis-based experiments to determine molecular functions. For example, what is the expected phenotype if a particular gene is removed? It would greatly improve hypothesis formation if biologists had prior insight from predicted functions of interesting genes or proteins in databases. Computational annotation of genes or proteins with unknown functions is thus a fundamental research area in computational biology.\n\nIn the past decade, there has been much work to accurately predict functional annotations of genes or proteins using heterogeneous molecular feature data \\citep{critical2008mouse,eval2013dk}.\nThe collected molecular features include gene expression, sequence patterns, evolutionary conservation profiles, protein structures and domains, protein-protein interactions (PPIs), and phenotypes or disease associations.\nIn one comprehensive assessment \\citep{critical2008mouse}, one of the methods, GeneMANIA \\citep{genemania} slightly outperformed the other eight methods by integrating the multiple molecular features into a functional association network (a.k.a., a kernel).\nThe success story of GeneMANIA suggests two important ideas.\nFirst, we can significantly improve prediction methods that rely on a single data type by integrating data of many types. And second, kernel integration is a particularly powerful approach to combining multiple types of data.\n\nGiven an integrated functional association network, methods for protein function prediction can be divided into three different types: neighborhood-based, module-assisted, and diffusion-based \\citep{sharan2007network}.\nNeighborhood-based methods \\citep{schwikowski2000network} predict the function of one protein by using the functions of its neighbors in the network, i.e., the guilt-by-association approach. This approach has two obvious drawbacks. On one hand, it ignores the functional information from all the other proteins outside the neighborhoods of the query proteins, which leads to a low true-positive rate. On the other hand, it may also have high false-positive rates when the query protein has a single function but is surrounded by many multi-functional proteins.\n\nModule-assisted methods operate by first partitioning a network or a kernel into small functional modules \\citep{enright2002efficient,bader2003automated}. Biologically, a functional module in a PPI network is a group of physically interacting proteins engaged in a biological activity, e.g., to form a scaffold or to relay signals. In network science, the quality of the modules, the modularity, is commonly defined as a densely connected subgraph with loose connections to the outside \\citep{newman2004finding}. This definition is naturally coincident with protein complexes, but not signaling cascades. Obtaining a high-quality graph partition is challenging, and this field of study is still highly active.\n\nDiffusion-based methods generally stimulate information flows from functionally known proteins to unknown ones through network connectivity \\citep{nabieva2005whole,freschi2007protein,genemania,yu2013protein}. Nabieva \\emph{et al.} constructed a network flow model with fixed diffusion distances and capacities on network edges \\citep{nabieva2005whole}. This method was claimed to capture both global network topology as well as local network structure to improve the function predictability over the first two domains of methods mentioned above. Freschi \\emph{et al.} devised a tool called ProteinRank by utilizing PageRank \\citep{page1999pagerank}, the method used by Google to rank webpages, to diffuse functional annotation information throughout a network without setting a fixed diffusion distance or edge capacities \\citep{freschi2007protein}. Mostafavi \\emph{et al.} utilized the Label Propagation algorithm \\citep{zhou2004learning} to develop GeneMANIA as a classification model with multiple heterogeneous network datasets using weighted kernels and labeled negative samples \\citep{genemania}. The method achieved approximately $70\\sim 90\\%$ accuracy in three-fold cross validation using a benchmark dataset \\citep{critical2008mouse}. Yu \\emph{et al.} developed the Transductive Multilabel Classifier (TMC), based on a Bi-relational graph \\citep{wang2011image} consisting of a protein interactome and cosine similarities in a protein functional profile as two kernels in each graph layer. Then they used PageRank on this two-layer graph to diffuse functional information to predict protein functions \\citep{yu2013protein}.\n\nFunctional annotation data are usually organized in a \\emph{tree-like} ontological structure with general terms at the root and specific terms on the leaves \\citep{go2004gene}. However, the majority of previous methods disregard this intrinsic hierarchical structure by assuming that the relationships between functions are independent.\nRecently, several methods have been proposed in order to take into account the interdependent relationships between functional terms in the hierarchical structure. Barutcuoglu \\emph{et al.} and Valentini \\emph{et al.} proposed a hierarchical Bayesian framework and a True Path Rule, respectively, to perform ensemble learning of the classification results yielded by multiple Support Vector Machines (SVMs) \\citep{barutcuoglu2006hierarchical,valentini2011true,cesa2012synergy,valentini2014hierarchical}. They demonstrated that the accuracy of protein function prediction can be significantly improved by integrating the functional hierarchy \\citep{valentini2014hierarchical}. Tao \\emph{et al.} and Pandey \\emph{et al.} utilized Lin's similarity \\citep{lin1998information} to flatten the functional hierarchy, and then predicted protein functions using a \\emph{k-}Nearest Neighbor (\\emph{k}-NN) method \\citep{tao2007information,pandey2009incorporating}. Sokolov and Ben-Hur directly modeled the hierarchical structure of functional ontology using structured SVM \\citep{structsvm}, and showed that their method outperformed \\emph{k}-NN and other binary classifiers without taking the hierarchy into account \\citep{gostruct}. Recently, Yu \\emph{et al.} combined Lin's similarity of protein functional profile with ontological hierarchy using downward random walks with restarts so as to improve the TMC model \\citep{yu2013protein}, which can predict a protein's functions that are not available in its neighborhood but are present in the hierarchy \\citep{yu2015down}. Wang \\emph{et al.} proposed clusDCA for protein function prediction by integrating protein networks and functional hierarchy, and using PageRank for network smoothing and low-rank matrix approximation to de-noise the network data \\citep{wang2015exploiting}.\n\nIn this study, we propose two methods that\nincorporate the functional hierarchy in protein function prediction by directly diffusing information on the hierarchy connected with a protein-protein functional association network.\nThe first method, which we call BirgRank, constructs a Bi-relational graph model with a protein-protein functional association network as one layer and an unflattened ontological hierarchy as a second layer, then directly applies  PageRank to diffuse annotation information across the two-layer network. The second method, which we call AptRank, \nemploys an adaptive version of PageRank that replaces the standard PageRank parameters with values dynamically chosen to better fit the training data.\nThe main differences between our methods and other diffusion-based methods are 1) we do not require any negative labeled samples since our method is not a classification model, and 2) we take full advantage of the functional hierarchy as a two-way directed graph, and do not use Lin's similarity \\citep{lin1998information} or any kernel trick to flatten the hierarchy.\n\nTo avoid the inflated accuracies of network-based methods in protein function prediction noted by Gillis and Pavlidis \\citep{gillis2011impact,gillis2012guilt,pavlidis2013progress,gillis2014bias}, we conduct a large and strict evaluation of our methods against the other state-of-the-art methods. We use an up-to-date protein interaction network data and exclude the functional annotations inferred from protein interactions (evidence code: IPI). Rather than two-\\citep{freschi2007protein}, three-\\citep{genemania,wang2015exploiting} or five-fold\\citep{yu2013protein} cross validation, we randomly hold out a specific percent of functional annotations ranging from $10\\%$ to $80\\%$, which is more stringent than any cross-validations previously performed in the other methods. To overcome the drawback of using Area Under the ROC curve (AUROC) as a criterion in evaluating performance on imbalanced data with a small number of positive samples, we also utilize Mean Average Precision (MAP) which focuses on the ranking of positive samples only, and is widely used in the field of information retrieval. \n\n\n\\section{Methods}\n\n\\subsection{Problem Statement}\nThis study is motivated by the fact that there is still a large portion of proteins whose functions are poorly characterized. To examine the extent to which each protein has been experimentally annotated, we downloaded three benchmark datasets of yeast, mouse and human proteins maintained by GeneMANIA-SW since $2010$ \\citep{swdataweb}, and also the human Gene Ontology Annotation (GOA) data \\citep{gene2015gene} in March $2015$. For the human GOA data, we only consider the annotations in Biological Process (BP), regardless of Molecular Function (MF) and Cellular Component (CC) terms. Also, we only take the annotations with experimental evidence codes into account, within which we remove the terms inferred by physical interaction (evidence code: IPI). All of these four data sets will be used for evaluation in this study. We show the distribution of the number of functional annotations of each protein in Figure~\\ref{fig:distfun}. We can see that there is still a large number of proteins with fewer than $3$ functional annotations. This is primarily due to bias in biological research interests and the difficulty of experimentally determining protein functions.\n\n        \\begin{figure}[!]\n        \\centering\n        \\includegraphics[width=0.6\\textwidth]{fig/new_pie}\\\\\n        \\caption{\\label{fig:distfun} Distribution of annotated functions of proteins in (A) yeast, (B) human collected in 2010, (C) fly and (D) human collected in 2015.}\n    \\end{figure}\n\nThe aim of this study is to predict protein functions given a protein-protein association network and a hierarchical structure of functional terms. The hypothesis is that associated proteins in the protein network are likely to share similar functions. Here, we define a protein-protein association network as pairwise quantitative relationships of proteins which can be either sparse and binary, e.g., protein-protein physical interaction network, or weighted and dense, e.g., pairwise similarity of protein sequences.\n\n\\subsection{Preliminaries of Personalized PageRank}\nPageRank is a well-studied tool in network analysis used to study how information diffuses across a network~\\citep{page1999pagerank}. We will use PageRank to diffuse annotation information from well-annotated proteins through a functional association network to less well-annotated proteins.\nIn particular, we use a ``personalized'' variation of PageRank~\\citep{jeh2003-personalized}, that models the flow of information from a small number of specific objects (in our case, a single protein) to the remainder of a network, and which we use to quantify which functions are most relevant to a particular protein.\n\nIntuitively, personalized PageRank operates on a network of interconnected nodes by placing a quantity of ``dye\" at a starting node of interest, then letting the dye diffuse across the edges of the network, decaying as it spreads. Once the diffusion process decays to zero, the network regions where the largest amount of dye has concentrated are then the most important regions to the starting node. See Figure~\\ref{fig:prvis} for a visualization of the dye diffused from a starting node.\n\n        \\begin{figure}[h!]\n        \\centering\n        \\includegraphics[width=0.6\\textwidth]{./fig/graphsub_diffusion_aptrank.png}\\\\\n        \\caption{\\label{fig:prvis}\n        A personalized PageRank diffusion started from the node colored black. Greenish dye diffuses from the black node and appears brightest on nodes that are most strongly connected to the black node.\n        }\n    \\end{figure}\n\n\nMathematically, on a network with $n$ objects, the network is modeled by an adjacency matrix $\\mA \\in {\\mathbb{R}}^{n \\times n}$ such that $\\mA_{ij}$ is 1 if node $j$ has an edge to node $i$, and is 0 otherwise. To model the diffusion process beginning with ``dye\" at a starting node,\nwe use a vector $\\vv \\in {\\mathbb{R}}^{n \\times 1}$ that is all 0s except with a 1 in the entry corresponding to the starting node. This vector $\\vv$ is called the personalization vector. If $\\vy \\in {\\mathbb{R}}^{n \\times 1}$ is a vector representing how much dye is at each node in the network at some point during the diffusion process, then we model the diffusion of that dye across the graph by multiplying $\\vy$ by a column-stochastic version of $\\mA$; this represents the dye on node $j$ distributing in equal parts to each neighbor $i$ of node $j$. We denote the column-stochastic version of any nonnegative matrix $\\mM$ using the operator $\\mathcal{C}\\left({\\mM}\\right) $; this is computed by dividing each column of the matrix $\\mM$ by the sum of the entries in that column.\n\nFinally, the decay of the diffusion process is controlled by the so-called PageRank teleportation paramter, $\\alpha \\in (0,1)$.\n\nDuring each stage of the diffusion, the dye that spreads across the network decays proportionally to $\\alpha$, so that the amount of dye still diffusing after $k$ steps is $\\alpha^k$.\nThen the PageRank vector $\\vx$ is given by the solution of the linear system\n\n", "index": 1, "text": "\\begin{equation}\\label{eq:ppr}\n  (\\mI - \\alpha \\mathcal{C}\\left({\\mA}\\right) ) \\vx = (1 - \\alpha)\\vv.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"(\\mI-\\alpha\\mathcal{C}\\left({\\mA}\\right))\\vx=(1-\\alpha)\\vv.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vv</mtext></merror></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nThis expression will become useful when we introduce the idea of using adaptive coefficients in place of $\\alpha^k$ to optimize prediction quality (see Section~\\ref{sec:aptrank}).\nWe note that, although PageRank has an interpretation as a Markov chain and Markov chains must meet certain conditions to guarantee convergence to a stationary distribution, this matrix power series~\\eqref{eq:prseries} always converges for any $\\alpha \\in (0,1)$ and stochastic matrix $\\mathcal{C}\\left({\\mA}\\right) $. Thus, the existence of the unique solution $\\vx$ is guaranteed regardless of the structure of the matrix $\\mA$.\nWe emphasize this because the form of linear system that we use differs from the traditional PageRank setting, which uses Markov chain analysis in the proof of its convergence; in contrast, our computations do not rely on this analysis. \n\n\n\\subsection{BirGRank: Bi-relational Graph PageRank model}\nWe denote the number of proteins by $m$ and the number of function terms by $n$. Then the three given datasets (protein-protein association network, protein-function annotations, and function-function hierarchy) are denoted by the following matrices:\n\\begin{itemize}\n  \\item $\\mG \\in {\\mathbb{R}}^{m \\times m}$, a symmetric matrix where $G(i,j)$ denotes to which extent protein $i$ is associated with protein $j$;\n  \\item $\\mR \\in {\\mathbb{R}}^{m \\times n}$, a binary matrix where $R(i,j) = 1$ if protein $i$ is annotated by function $j$, 0 otherwise; and\n  \\item $\\mH \\in {\\mathbb{R}}^{n \\times n}$, a binary matrix where $H(i,j) = 1$ if functional term $i$ is the child of term $j$, 0 otherwise.\n\\end{itemize}\nWe illustrate these three components in Figure~\\ref{fig:example}(A), (B) and (C), using a small, simple example with $6$ proteins and $7$ functional terms. For simplification, Figure~\\ref{fig:example}(A) shows a protein-protein binary interaction network, but it can be replaced by any protein-protein association network. Functional terms are hierarchically structured in a Gene Ontology (Figure~\\ref{fig:example}(C)) like an upside down ``tree'', where the terms on the top (root) are more general and the ones in the bottom (leaves) are more specific. The annotation rule is that if one gene or protein is annotated by one term, then this gene or protein is automatically annotated by all the parental terms of that term in the hierarchy.\n\n    \\begin{figure}[h!]\n        \\centering\n        \\includegraphics[width = 0.6\\textwidth]{fig/modelfig.pdf}\\\\\n        \\caption{\\label{fig:example} Given data visualization using simple example. (A) protein-protein interaction network, (B) protein-function reference matrix, (C) function-function hierarchy, (D) adjacency matrix $\\mA$ of a Bi-relational graph.}\n    \\end{figure}\n\nNext, we construct a Bi-relational graph \\citep{wang2011image} to incorporate these three datasets into a single network, as shown in Figure~\\ref{fig:example}(D).\n\nTo evaluate prediction performance, we split all the annotations in $\\mR$ into $\\mR_T$, which we use for training during model construction, and $\\mR_E$, which we use for evaluating predictions. For each protein $i$, we predict its functions using Equation~\\eqref{eq:ppr} by setting it as the diffusion source, i.e., by computing the diffusion using $\\vv = \\ve_i$.\n\nTo predict the functions of all proteins, we extend the linear system in Equation~\\eqref{eq:ppr} to a matrix form:\n\n", "itemtype": "equation", "pos": 17643, "prevtext": "\n Intuitively, the PageRank vector indicates how much of the diffusion flows from the source node (i.e. the nonzero entry in the vector $\\vv$) to each node in the graph. In our context, this means that $\\vx$ will indicate how much of the diffusion flows from the protein of interest to each other protein in the graph. In our model, we combine proteins and functions into a single network so that this PageRank vector can indicate diffusion flow between proteins and functions.\n PageRank is also called Random Walk with Restart (RWR) in other literature \\citep{tong2006fast}.\n\nThe solution to the Personalized PageRank linear system in Equation~\\eqref{eq:ppr} can be expressed as\n\n\n", "index": 3, "text": "\\begin{equation}\\label{eq:prseries}\n\n  \\vx = \\sum_{k=0}^{\\infty} (1-\\alpha)\\alpha^k \\left(\\mathcal{C}\\left({\\mA}\\right)  \\right) ^k \\vv.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\vx=\\sum_{k=0}^{\\infty}(1-\\alpha)\\alpha^{k}\\left(\\mathcal{C}\\left({\\mA}%&#10;\\right)\\right)^{k}\\vv.\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\u03b1</mi><mi>k</mi></msup><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mi>k</mi></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vv</mtext></merror></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere the lower block of the solution, $\\mX_H$, is the matrix containing function predictions, and has the\nsame dimensions as $\\mR^T$. To further control the proportion of diffusion passing between the two layers of the Bi-relational graph, we parameterize the model in Equation~\\eqref{eq:pprfunc} as\n\n", "itemtype": "equation", "pos": 21183, "prevtext": "\nThis expression will become useful when we introduce the idea of using adaptive coefficients in place of $\\alpha^k$ to optimize prediction quality (see Section~\\ref{sec:aptrank}).\nWe note that, although PageRank has an interpretation as a Markov chain and Markov chains must meet certain conditions to guarantee convergence to a stationary distribution, this matrix power series~\\eqref{eq:prseries} always converges for any $\\alpha \\in (0,1)$ and stochastic matrix $\\mathcal{C}\\left({\\mA}\\right) $. Thus, the existence of the unique solution $\\vx$ is guaranteed regardless of the structure of the matrix $\\mA$.\nWe emphasize this because the form of linear system that we use differs from the traditional PageRank setting, which uses Markov chain analysis in the proof of its convergence; in contrast, our computations do not rely on this analysis. \n\n\n\\subsection{BirGRank: Bi-relational Graph PageRank model}\nWe denote the number of proteins by $m$ and the number of function terms by $n$. Then the three given datasets (protein-protein association network, protein-function annotations, and function-function hierarchy) are denoted by the following matrices:\n\\begin{itemize}\n  \\item $\\mG \\in {\\mathbb{R}}^{m \\times m}$, a symmetric matrix where $G(i,j)$ denotes to which extent protein $i$ is associated with protein $j$;\n  \\item $\\mR \\in {\\mathbb{R}}^{m \\times n}$, a binary matrix where $R(i,j) = 1$ if protein $i$ is annotated by function $j$, 0 otherwise; and\n  \\item $\\mH \\in {\\mathbb{R}}^{n \\times n}$, a binary matrix where $H(i,j) = 1$ if functional term $i$ is the child of term $j$, 0 otherwise.\n\\end{itemize}\nWe illustrate these three components in Figure~\\ref{fig:example}(A), (B) and (C), using a small, simple example with $6$ proteins and $7$ functional terms. For simplification, Figure~\\ref{fig:example}(A) shows a protein-protein binary interaction network, but it can be replaced by any protein-protein association network. Functional terms are hierarchically structured in a Gene Ontology (Figure~\\ref{fig:example}(C)) like an upside down ``tree'', where the terms on the top (root) are more general and the ones in the bottom (leaves) are more specific. The annotation rule is that if one gene or protein is annotated by one term, then this gene or protein is automatically annotated by all the parental terms of that term in the hierarchy.\n\n    \\begin{figure}[h!]\n        \\centering\n        \\includegraphics[width = 0.6\\textwidth]{fig/modelfig.pdf}\\\\\n        \\caption{\\label{fig:example} Given data visualization using simple example. (A) protein-protein interaction network, (B) protein-function reference matrix, (C) function-function hierarchy, (D) adjacency matrix $\\mA$ of a Bi-relational graph.}\n    \\end{figure}\n\nNext, we construct a Bi-relational graph \\citep{wang2011image} to incorporate these three datasets into a single network, as shown in Figure~\\ref{fig:example}(D).\n\nTo evaluate prediction performance, we split all the annotations in $\\mR$ into $\\mR_T$, which we use for training during model construction, and $\\mR_E$, which we use for evaluating predictions. For each protein $i$, we predict its functions using Equation~\\eqref{eq:ppr} by setting it as the diffusion source, i.e., by computing the diffusion using $\\vv = \\ve_i$.\n\nTo predict the functions of all proteins, we extend the linear system in Equation~\\eqref{eq:ppr} to a matrix form:\n\n", "index": 5, "text": "\\begin{equation}\\label{eq:pprfunc}\n  \\left(\\begin{bmatrix} {\\mI_m & {\\boldsymbol{\\mathrm{{0}}}} \\\\ {\\boldsymbol{\\mathrm{{0}}}} & \\mI_n} \\end{bmatrix} - \\alpha\n  \\mathcal{C}\\left({\\begin{bmatrix} {\\mG & {\\boldsymbol{\\mathrm{{0}}}} \\\\ \\mR_T^T & \\mH} \\end{bmatrix}}\\right) \n  \\right) \\begin{bmatrix} {\\mX_G \\\\ \\mX_H} \\end{bmatrix} = (1 - \\alpha) \\begin{bmatrix} {\\mI_m \\\\ {\\boldsymbol{\\mathrm{{0}}}}} \\end{bmatrix},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\left(\\begin{bmatrix}{\\mI_{m}\\hfil&amp;{\\boldsymbol{\\mathrm{{0}}}}\\\\&#10;{\\boldsymbol{\\mathrm{{0}}}}&amp;\\mI_{n}\\end{bmatrix}-\\alpha\\mathcal{C}\\left({%&#10;\\begin{bmatrix}{\\mG\\hfil&amp;{\\boldsymbol{\\mathrm{{0}}}}\\\\&#10;\\mR_{T}^{T}&amp;\\mH\\end{bmatrix}}\\right)\\right)\\begin{bmatrix}{\\mX_{G}\\hfil\\\\&#10;\\mX_{H}\\end{bmatrix}=(1-\\alpha)\\begin{bmatrix}{\\mI_{m}\\hfil\\\\&#10;{\\boldsymbol{\\mathrm{{0}}}}\\end{bmatrix},}}}}\" display=\"block\"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>m</mi></msub><mo>\u2062</mo><mn/></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>n</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo>\u2062</mo><mn/></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi><mi>T</mi></msubsup></mtd><mtd columnalign=\"center\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mH</mtext></merror></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>m</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere $\\mH^* = \\lambda \\mH + (1-\\lambda)\\mH^T$, and $\\lambda$ controls the diffusion direction on $\\mH$. Specifically, $\\lambda = 0$ indicates that the diffusion flows down the hierarchy, and $1$ indicates flow up the hierarchy. The parameter $\\mu \\in (0,1)$ controls the proportion of the diffusion flowing within $\\mG$, and $\\theta \\in (0,1)$ controls\nthe weighting of the proteins and functional annotations in the right-hand side of Equation~\\eqref{eq:pprparameters}.\n\n\n\n\\subsection{Extension to AptRank} \\label{sec:aptrank}\n\nIn the traditional model of PageRank, which we use in our method BirGRank, the teleportation parameter $\\alpha \\in (0,1)$ can be thought of as controlling the rate of decay of the diffusion as it spreads from the nodes in the personalization vector $\\vv$ to the rest of the graph. After $k$ steps the diffusion has decayed by a factor of $\\alpha^k$, for $k = 1,\\cdots,\\infty$ (see Equation~\\eqref{eq:prseries}). There are a variety of other empirical weighting schemes \\citep{baeza2006generalizing,constantine2009random,chung2007heat}, each with slightly different theoretical properties.\n\nIn this section, we seek to replace the standard, fixed diffusion coefficients $\\alpha^k$ at each step with an adaptive parameter, denoted by $\\gamma^{(k)}$, to better guide the prediction power of the Markov chain.\nTo do this we repeatedly split the training set of protein function annotations, $\\mR_T$, into different subsets to use in fitting and validating the coefficients. We denote the matrix used for fitting by $\\mR_F$, and the matrix used in validation by $\\mR_V$. These matrices have the same dimensions as $\\mR_T$ and consist of entries of $\\mR_T$.\n\n\n\n\n\nTo determine the adaptive coefficients $\\gamma^{(k)}$ so that they bias predictions toward the training data, we proceed as follows.\nThe AptRank method begins by computing terms using the system in Equation~\\eqref{eq:pprfunc}:\n\n", "itemtype": "equation", "pos": 21912, "prevtext": "\nwhere the lower block of the solution, $\\mX_H$, is the matrix containing function predictions, and has the\nsame dimensions as $\\mR^T$. To further control the proportion of diffusion passing between the two layers of the Bi-relational graph, we parameterize the model in Equation~\\eqref{eq:pprfunc} as\n\n", "index": 7, "text": "\\begin{equation}\\label{eq:pprparameters}\n\\begin{split}\n  &\\left(\\begin{bmatrix} {\\mI_m & {\\boldsymbol{\\mathrm{{0}}}} \\\\ {\\boldsymbol{\\mathrm{{0}}}} & \\mI_n} \\end{bmatrix} - \\alpha\n  \\mathcal{C}\\left({ \\begin{bmatrix} {\\mu\\mG & {\\boldsymbol{\\mathrm{{0}}}} \\\\ (1-\\mu)\\mR_T^T & \\mH^*} \\end{bmatrix}}\\right) \n  \\right) \\begin{bmatrix} {\\mX_G \\\\ \\mX_H} \\end{bmatrix} \\\\\n  &= (1 - \\alpha) \\begin{bmatrix} {\\theta\\mI_m \\\\ (1-\\theta)\\mR_T^T} \\end{bmatrix},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\left(\\begin{bmatrix}{\\mI_{m}\\hfil&amp;{\\boldsymbol{%&#10;\\mathrm{{0}}}}\\\\&#10;{\\boldsymbol{\\mathrm{{0}}}}&amp;\\mI_{n}\\end{bmatrix}-\\alpha\\mathcal{C}\\left({%&#10;\\begin{bmatrix}{\\mu\\mG\\hfil&amp;{\\boldsymbol{\\mathrm{{0}}}}\\\\&#10;(1-\\mu)\\mR_{T}^{T}&amp;\\mH^{*}\\end{bmatrix}}\\right)\\right)\\begin{bmatrix}{\\mX_{G}%&#10;\\hfil\\\\&#10;\\mX_{H}\\end{bmatrix}\\\\&#10;&amp;\\displaystyle=(1-\\alpha)\\begin{bmatrix}{\\theta\\mI_{m}\\hfil\\\\&#10;(1-\\theta)\\mR_{T}^{T}\\end{bmatrix},\\end{split}}}}}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>m</mi></msub><mo>\u2062</mo><mn/></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>n</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><mi>\u03bc</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo>\u2062</mo><mn/></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi><mi>T</mi></msubsup></mrow></mtd><mtd columnalign=\"center\"><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mH</mtext></merror><mo>*</mo></msup></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>\u03b8</mi><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>m</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi><mi>T</mi></msubsup></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 24302, "prevtext": "\nwhere $\\mH^* = \\lambda \\mH + (1-\\lambda)\\mH^T$, and $\\lambda$ controls the diffusion direction on $\\mH$. Specifically, $\\lambda = 0$ indicates that the diffusion flows down the hierarchy, and $1$ indicates flow up the hierarchy. The parameter $\\mu \\in (0,1)$ controls the proportion of the diffusion flowing within $\\mG$, and $\\theta \\in (0,1)$ controls\nthe weighting of the proteins and functional annotations in the right-hand side of Equation~\\eqref{eq:pprparameters}.\n\n\n\n\\subsection{Extension to AptRank} \\label{sec:aptrank}\n\nIn the traditional model of PageRank, which we use in our method BirGRank, the teleportation parameter $\\alpha \\in (0,1)$ can be thought of as controlling the rate of decay of the diffusion as it spreads from the nodes in the personalization vector $\\vv$ to the rest of the graph. After $k$ steps the diffusion has decayed by a factor of $\\alpha^k$, for $k = 1,\\cdots,\\infty$ (see Equation~\\eqref{eq:prseries}). There are a variety of other empirical weighting schemes \\citep{baeza2006generalizing,constantine2009random,chung2007heat}, each with slightly different theoretical properties.\n\nIn this section, we seek to replace the standard, fixed diffusion coefficients $\\alpha^k$ at each step with an adaptive parameter, denoted by $\\gamma^{(k)}$, to better guide the prediction power of the Markov chain.\nTo do this we repeatedly split the training set of protein function annotations, $\\mR_T$, into different subsets to use in fitting and validating the coefficients. We denote the matrix used for fitting by $\\mR_F$, and the matrix used in validation by $\\mR_V$. These matrices have the same dimensions as $\\mR_T$ and consist of entries of $\\mR_T$.\n\n\n\n\n\nTo determine the adaptive coefficients $\\gamma^{(k)}$ so that they bias predictions toward the training data, we proceed as follows.\nThe AptRank method begins by computing terms using the system in Equation~\\eqref{eq:pprfunc}:\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:iter}\n  \\mX^{(k)} = \\begin{bmatrix} {\\mX_G^{(k)} \\\\ \\mX_H^{(k)}} \\end{bmatrix} = \\mathcal{C}\\left({ \\begin{bmatrix} {\\mG & \\mR_F \\\\ \\mR_F^T & \\mH} \\end{bmatrix} }\\right) ^k \\mX^{(0)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\mX^{(k)}=\\begin{bmatrix}{\\mX_{G}^{(k)}\\hfil\\\\&#10;\\mX_{H}^{(k)}\\end{bmatrix}=\\mathcal{C}\\left({\\begin{bmatrix}{\\mG\\hfil&amp;\\mR_{F}%&#10;\\\\&#10;\\mR_{F}^{T}&amp;\\mH\\end{bmatrix}}\\right)^{k}\\mX^{(0)},}}\" display=\"block\"><mrow><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>F</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>F</mi><mi>T</mi></msubsup></mtd><mtd columnalign=\"center\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mH</mtext></merror></mtd></mtr></mtable><mo>]</mo></mrow><mo>)</mo></mrow><mi>k</mi></msup><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\n To compute the optimal set of coefficients $\\gamma^{(i)}$ that best fits the validation set $\\mR_V$, we solve the following constrained least squares model,\n\n", "itemtype": "equation", "pos": 24533, "prevtext": "\nwhere\n\n", "index": 11, "text": "\\begin{equation}\\label{eq:x0}\n  \\mX^{(0)} = \\begin{bmatrix} {\\mX_G^{(0)} \\\\ \\mX_H^{(0)}} \\end{bmatrix} = \\begin{bmatrix} {\\mI_m \\\\ {\\boldsymbol{\\mathrm{{0}}}}} \\end{bmatrix}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mX^{(0)}=\\begin{bmatrix}{\\mX_{G}^{(0)}\\hfil\\\\&#10;\\mX_{H}^{(0)}\\end{bmatrix}=\\begin{bmatrix}{\\mI_{m}\\hfil\\\\&#10;{\\boldsymbol{\\mathrm{{0}}}}\\end{bmatrix}.}}\" display=\"block\"><mrow><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mi>m</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere $\\text{vec}(\\cdot)$ is a matrix-to-vector transformation that stacks the columns of the matrix into a single column vector. We perform this fitting-validating process multiple times, each time splitting $\\mR_T$ into new matrices $\\mR_F$ and $\\mR_V$ by choosing entries from $\\mR_T$ uniformly at random. Each such iteration generates a new set of coefficients $\\gamma^{(i)}$, which we store. We call these iterations ``shuffles\" because in essence they consist of shuffling the entries of $\\mR_T$ into the two matrices $\\mR_F$ and $\\mR_V$. The number of shuffles performed is an input parameter; after the prescribed number of shuffles is completed, we compute the average ${\\gamma^*}^{(i)}$ of the $\\gamma^{(i)}$ computed across all shuffles, and use those averages ${\\gamma^*}^{(i)}$ to compute the final diffusion values.\nWe summarize the AptRank framework in Algorithm~\\ref{alg:aptrank}.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{algorithm}[]\n\\DontPrintSemicolon\n\\SetAlgoLined\n\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output}\n\\SetKwFunction{SplitR}{splitR}\n\\SetKwFunction{Vec}{vec}\n\\SetKwFunction{Qr}{qr}\n\\SetKwFunction{Median}{median}\n\n\\Input{$\\mG$,$\\mR_T$,$\\mH^*$,$K$,$S$,$t$}\n\\Output{$\\mX_{\\text{AptRank}}$}\n\\BlankLine\n\n\\For{$s \\leftarrow 1$ \\KwTo $S$}{\n    [$\\mR_F$, $\\mR_V$] $\\leftarrow$ \\SplitR{$\\mR_T$,$t$}\\;\n    \\tcp*[h]{Choose $t\\%$ of entries in $\\mR_T$ uniformly at random and split to $\\mR_F$, and $1-t\\%$ to $\\mR_V$.}\\;\n    Initialize $\\mX^{(0)}$ using Equation~\\eqref{eq:x0}\\;\n    \\For{$k \\leftarrow 1$ \\KwTo $K$}{\n        Compute $\\mX^{(k)}$ using Equation~\\eqref{eq:iter}\\;\n        $\\mA[:,k] \\leftarrow $ \\Vec{$\\mX_H^{(k)}$}\\;\n    }\n    [$\\mQ_A$,$\\mR_A$] $\\leftarrow$ \\Qr{$\\mA$}\\;\n    $\\vb$ $\\leftarrow $ \\Vec{$\\mR_V$}\\;\n    Solve $\\begin{array}{ll} \\displaystyle {\\operatorname{minimize}}_{{{\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}}} & {{{\\|{\\mQ_A^T \\vb - \\mR_A {\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}}\\|}_2^2}} \\\\ {\\operatorname{subject\\ to}} & {{\\mathlarger{\\sum}_k \\gamma_k^{(s)} = 1, \\gamma_k^{(s)} \\geq 0}} \\end{array}$\\;\n    \\tcp*[h]{Equivalently as Equation~\\eqref{eq:opt}.}\\;\n}\n${\\boldsymbol{\\mathrm{{\\gamma}}}}^*$ $\\leftarrow$ \\Median{${\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}$}\\;\n\\tcp*[h]{Take the median over all $s = 1$ to $S$ for each $k$.}\\;\n$\\begin{bmatrix} {\\mX_G^* \\\\ \\mX_H^*} \\end{bmatrix} \\leftarrow \\mathlarger{\\mathlarger{\\sum}}_{k=1}^K \\gamma^*_k \\,X^{(k)}\n$\\;\n\nOutput $\\mX_{\\text{AptRank}} \\leftarrow \\mX_H^*$ for use in predictions.\n\\caption{\\label{alg:aptrank}AptRank}\n\\end{algorithm}\n\nWe remark that the \\emph{percentage} of entries in $\\mR_T$ that are used in $\\mR_F$ versus in $\\mR_V$ is a tunable parameter, which we call the \\emph{splitting} parameter, denoted $t$ in Algorithm~\\ref{alg:aptrank}. In practice we found that a splitting percentage of $50\\%$ works well. We discuss the splitting values used in our experiments in Section~\\eqref{sec:results}, where we report the quality of the predictions of $\\mX_{\\text{AptRank}}$ by comparing against the evaluation set $\\mR_E$.\n\n\\subsection{Connection with Other Methods}\nTo investigate the similarities and differences of our method and the other four previous methods used for evaluation, we perform analysis and comparison here, and summarize the features of each method in Table~\\ref{tab:summet}.\n\\begin{table*}[t]\n\t\t\\centering\n\t\t\\caption{Summary of the Six Methods \\label{tab:summet}}\n   \\resizebox{1.0\\textwidth}{!}{\n\t\t\\begin{tabular}{lccccccl}\n\t\t\\hline\n\t\t\\multicolumn{1}{c}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Method\\\\ Name\\end{tabular}}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Method\\\\ Type\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Functional\\\\ Hierarchy\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Bi-relational\\\\ Graph\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Negative\\\\ Samples\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Random\\\\ Walk\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Stationary\\\\ PageRank\\end{tabular}} & \\multicolumn{1}{c}{\\textbf{Reference}} \\\\ \\hline\n\t\tGeneMANIA-SW & \\begin{tabular}[c]{@{}c@{}}kernel integration\\\\ \\& classification\\end{tabular} &  &  & \\checkmark & \\checkmark & \\checkmark & \\begin{tabular}[c]{@{}l@{}}{\\citep}{genemania}\\\\ {\\citep}{sw}\\end{tabular} \\\\\n\t\tTMC & diffusion &  & \\checkmark &  & \\checkmark & \\checkmark & {\\citep}{yu2013protein} \\\\\n\t\tProteinRank & regression &  &  &  & \\checkmark & \\checkmark & {\\citep}{freschi2007protein} \\\\\n\t\tDCA-clusDCA & \\begin{tabular}[c]{@{}c@{}}diffusion \\&\\\\ decomposition\\end{tabular} & \\checkmark &  & \\checkmark & \\checkmark & \\checkmark & \\begin{tabular}[c]{@{}l@{}}{\\citep}{dca}\\\\ {\\citep}{wang2015exploiting}\\end{tabular} \\\\\n\t\tBirgRank & diffusion & \\checkmark & \\checkmark &  & \\checkmark & \\checkmark & this study \\\\\n\t\tAptRank & diffusion & \\checkmark & \\checkmark &  & \\checkmark &  & this study \\\\ \\hline\n\t\t\\end{tabular}\n}\n\\end{table*}\n\nThe linear system of BirgRank in Equation~\\eqref{eq:pprfunc} can be expanded into\n\n", "itemtype": "equation", "pos": 24881, "prevtext": "\n To compute the optimal set of coefficients $\\gamma^{(i)}$ that best fits the validation set $\\mR_V$, we solve the following constrained least squares model,\n\n", "index": 13, "text": "\\begin{equation}\\label{eq:opt}\n\\begin{aligned}\n& \\underset{\\gamma}{\\text{minimize}}\n\n& &  \\left\\| \\text{vec}(\\mR_V^T) - \\sum_{i=1}^k \\gamma^{(i)}\\text{vec}(\\mX_H^{(i)}) \\right\\|_2^2\\\\\n& \\text{subject to}\n& & \\sum_{i=1}^k \\gamma^{(i)} = 1, \\\\\n&&& \\gamma^{(i)} \\geq 0,\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{\\gamma}{\\text{minimize}}\\par&#10;\" display=\"inline\"><munder accentunder=\"true\"><mtext>minimize</mtext><mo>\ud835\udefe</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left\\|\\text{vec}(\\mR_{V}^{T})-\\sum_{i=1}^{k}\\gamma^{(i)}\\text{%&#10;vec}(\\mX_{H}^{(i)})\\right\\|_{2}^{2}\" display=\"inline\"><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>V</mi><mi>T</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><msup><mi>\u03b3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mtext>vec</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}\\gamma^{(i)}=1,\" display=\"inline\"><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><msup><mi>\u03b3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\gamma^{(i)}\\geq 0,\" display=\"inline\"><mrow><mrow><msup><mi>\u03b3</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere $\\tilde{\\mG}$ and $\\tilde{\\mR_T}$ denote\nthe submatrices of $\\mathcal{C}\\left({\\begin{bmatrix} {\\mG & {\\boldsymbol{\\mathrm{{0}}}} \\\\ \\mR_T^T & \\mH} \\end{bmatrix}}\\right) $ corresponding to the positions of $\\mG$ and $\\mR_T^T$.\nBy solving the above Equations~\\eqref{eq:twoeq} for $\\mX_H$, we get\n\n", "itemtype": "equation", "pos": 30114, "prevtext": "\nwhere $\\text{vec}(\\cdot)$ is a matrix-to-vector transformation that stacks the columns of the matrix into a single column vector. We perform this fitting-validating process multiple times, each time splitting $\\mR_T$ into new matrices $\\mR_F$ and $\\mR_V$ by choosing entries from $\\mR_T$ uniformly at random. Each such iteration generates a new set of coefficients $\\gamma^{(i)}$, which we store. We call these iterations ``shuffles\" because in essence they consist of shuffling the entries of $\\mR_T$ into the two matrices $\\mR_F$ and $\\mR_V$. The number of shuffles performed is an input parameter; after the prescribed number of shuffles is completed, we compute the average ${\\gamma^*}^{(i)}$ of the $\\gamma^{(i)}$ computed across all shuffles, and use those averages ${\\gamma^*}^{(i)}$ to compute the final diffusion values.\nWe summarize the AptRank framework in Algorithm~\\ref{alg:aptrank}.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{algorithm}[]\n\\DontPrintSemicolon\n\\SetAlgoLined\n\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output}\n\\SetKwFunction{SplitR}{splitR}\n\\SetKwFunction{Vec}{vec}\n\\SetKwFunction{Qr}{qr}\n\\SetKwFunction{Median}{median}\n\n\\Input{$\\mG$,$\\mR_T$,$\\mH^*$,$K$,$S$,$t$}\n\\Output{$\\mX_{\\text{AptRank}}$}\n\\BlankLine\n\n\\For{$s \\leftarrow 1$ \\KwTo $S$}{\n    [$\\mR_F$, $\\mR_V$] $\\leftarrow$ \\SplitR{$\\mR_T$,$t$}\\;\n    \\tcp*[h]{Choose $t\\%$ of entries in $\\mR_T$ uniformly at random and split to $\\mR_F$, and $1-t\\%$ to $\\mR_V$.}\\;\n    Initialize $\\mX^{(0)}$ using Equation~\\eqref{eq:x0}\\;\n    \\For{$k \\leftarrow 1$ \\KwTo $K$}{\n        Compute $\\mX^{(k)}$ using Equation~\\eqref{eq:iter}\\;\n        $\\mA[:,k] \\leftarrow $ \\Vec{$\\mX_H^{(k)}$}\\;\n    }\n    [$\\mQ_A$,$\\mR_A$] $\\leftarrow$ \\Qr{$\\mA$}\\;\n    $\\vb$ $\\leftarrow $ \\Vec{$\\mR_V$}\\;\n    Solve $\\begin{array}{ll} \\displaystyle {\\operatorname{minimize}}_{{{\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}}} & {{{\\|{\\mQ_A^T \\vb - \\mR_A {\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}}\\|}_2^2}} \\\\ {\\operatorname{subject\\ to}} & {{\\mathlarger{\\sum}_k \\gamma_k^{(s)} = 1, \\gamma_k^{(s)} \\geq 0}} \\end{array}$\\;\n    \\tcp*[h]{Equivalently as Equation~\\eqref{eq:opt}.}\\;\n}\n${\\boldsymbol{\\mathrm{{\\gamma}}}}^*$ $\\leftarrow$ \\Median{${\\boldsymbol{\\mathrm{{\\gamma}}}}^{(s)}$}\\;\n\\tcp*[h]{Take the median over all $s = 1$ to $S$ for each $k$.}\\;\n$\\begin{bmatrix} {\\mX_G^* \\\\ \\mX_H^*} \\end{bmatrix} \\leftarrow \\mathlarger{\\mathlarger{\\sum}}_{k=1}^K \\gamma^*_k \\,X^{(k)}\n$\\;\n\nOutput $\\mX_{\\text{AptRank}} \\leftarrow \\mX_H^*$ for use in predictions.\n\\caption{\\label{alg:aptrank}AptRank}\n\\end{algorithm}\n\nWe remark that the \\emph{percentage} of entries in $\\mR_T$ that are used in $\\mR_F$ versus in $\\mR_V$ is a tunable parameter, which we call the \\emph{splitting} parameter, denoted $t$ in Algorithm~\\ref{alg:aptrank}. In practice we found that a splitting percentage of $50\\%$ works well. We discuss the splitting values used in our experiments in Section~\\eqref{sec:results}, where we report the quality of the predictions of $\\mX_{\\text{AptRank}}$ by comparing against the evaluation set $\\mR_E$.\n\n\\subsection{Connection with Other Methods}\nTo investigate the similarities and differences of our method and the other four previous methods used for evaluation, we perform analysis and comparison here, and summarize the features of each method in Table~\\ref{tab:summet}.\n\\begin{table*}[t]\n\t\t\\centering\n\t\t\\caption{Summary of the Six Methods \\label{tab:summet}}\n   \\resizebox{1.0\\textwidth}{!}{\n\t\t\\begin{tabular}{lccccccl}\n\t\t\\hline\n\t\t\\multicolumn{1}{c}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Method\\\\ Name\\end{tabular}}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Method\\\\ Type\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Functional\\\\ Hierarchy\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Bi-relational\\\\ Graph\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Negative\\\\ Samples\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Random\\\\ Walk\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Stationary\\\\ PageRank\\end{tabular}} & \\multicolumn{1}{c}{\\textbf{Reference}} \\\\ \\hline\n\t\tGeneMANIA-SW & \\begin{tabular}[c]{@{}c@{}}kernel integration\\\\ \\& classification\\end{tabular} &  &  & \\checkmark & \\checkmark & \\checkmark & \\begin{tabular}[c]{@{}l@{}}{\\citep}{genemania}\\\\ {\\citep}{sw}\\end{tabular} \\\\\n\t\tTMC & diffusion &  & \\checkmark &  & \\checkmark & \\checkmark & {\\citep}{yu2013protein} \\\\\n\t\tProteinRank & regression &  &  &  & \\checkmark & \\checkmark & {\\citep}{freschi2007protein} \\\\\n\t\tDCA-clusDCA & \\begin{tabular}[c]{@{}c@{}}diffusion \\&\\\\ decomposition\\end{tabular} & \\checkmark &  & \\checkmark & \\checkmark & \\checkmark & \\begin{tabular}[c]{@{}l@{}}{\\citep}{dca}\\\\ {\\citep}{wang2015exploiting}\\end{tabular} \\\\\n\t\tBirgRank & diffusion & \\checkmark & \\checkmark &  & \\checkmark & \\checkmark & this study \\\\\n\t\tAptRank & diffusion & \\checkmark & \\checkmark &  & \\checkmark &  & this study \\\\ \\hline\n\t\t\\end{tabular}\n}\n\\end{table*}\n\nThe linear system of BirgRank in Equation~\\eqref{eq:pprfunc} can be expanded into\n\n", "index": 15, "text": "\\begin{equation}\\label{eq:twoeq}\n  \\begin{aligned}\n  \\begin{cases}\n    (\\mI - \\alpha \\tilde{\\mG})\\mX_G = (1-\\alpha)\\mI\\\\\n    \\alpha \\tilde{\\mR_T}^T \\mX_G = (\\mI - \\alpha \\mathcal{C}\\left({\\mH}\\right)  )\\mX_H,\n  \\end{cases}\n  \\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{cases}(\\mI-\\alpha\\tilde{\\mG})\\mX_{G}=(1-\\alpha)\\mI\\\\&#10;\\alpha\\tilde{\\mR_{T}}^{T}\\mX_{G}=(\\mI-\\alpha\\mathcal{C}\\left({\\mH}\\right))\\mX_%&#10;{H},\\end{cases}\" display=\"inline\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror></mrow></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mover accent=\"true\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi></msub><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>G</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mH</mtext></merror><mo>)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd><mtd/></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\n\nIn contrast, ProteinRank \\citep{freschi2007protein} uses only the protein-protein association network $\\mG$ as a one-layer network model --- and does not directly take into consideration the functional hierarchy $\\mH$ --- then computes PageRank using $\\mR_T$ as the personalization vectors (matrix). ProteinRank constructs a regression model and solves the linear system\n\n", "itemtype": "equation", "pos": 30669, "prevtext": "\nwhere $\\tilde{\\mG}$ and $\\tilde{\\mR_T}$ denote\nthe submatrices of $\\mathcal{C}\\left({\\begin{bmatrix} {\\mG & {\\boldsymbol{\\mathrm{{0}}}} \\\\ \\mR_T^T & \\mH} \\end{bmatrix}}\\right) $ corresponding to the positions of $\\mG$ and $\\mR_T^T$.\nBy solving the above Equations~\\eqref{eq:twoeq} for $\\mX_H$, we get\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:split}\n  \\mX_H = \\alpha(1-\\alpha)(\\mI - \\alpha\\mathcal{C}\\left({\\mH}\\right) )^{-1} \\tilde{\\mR_T}^T (\\mI - \\alpha\\tilde{\\mG})^{-1}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\mX_{H}=\\alpha(1-\\alpha)(\\mI-\\alpha\\mathcal{C}\\left({\\mH}\\right))^{-1}\\tilde{%&#10;\\mR_{T}}^{T}(\\mI-\\alpha\\tilde{\\mG})^{-1}.\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mi>H</mi></msub><mo>=</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mH</mtext></merror><mo>)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mover accent=\"true\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi></msub><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhich can cause poor prediction quality due to the assumption of independence between functions (see Section~\\ref{sec:results}). Our method BirgRank is closely related to ProteinRank: \nif we plug $\\mH = \\mI$ into Equation~\\eqref{eq:split}, then the resulting BirgRank solution differs from the ProteinRank solution (Equation~\\eqref{eq:proteinrank}) only by a scalar coefficient and a slightly different normalization of $\\mG$.\n\nSimilar to ProteinRank, GeneMANIA \\citep{genemania} models protein function prediction as a multiclass multilabel classification problem by integrating multiple heterogeneous network datasets and then using the Label Propagation algorithm \\citep{zhou2004learning} as\n\n", "itemtype": "equation", "pos": 31213, "prevtext": "\n\nIn contrast, ProteinRank \\citep{freschi2007protein} uses only the protein-protein association network $\\mG$ as a one-layer network model --- and does not directly take into consideration the functional hierarchy $\\mH$ --- then computes PageRank using $\\mR_T$ as the personalization vectors (matrix). ProteinRank constructs a regression model and solves the linear system\n\n", "index": 19, "text": "\\begin{equation}\\label{eq:proteinrank}\n  \\mX_{\\text{ProteinRank}} = (1-\\alpha) (\\mI - \\alpha\\mathcal{C}\\left({\\mG}\\right) )^{-1} \\mR_T,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\mX_{\\text{ProteinRank}}=(1-\\alpha)(\\mI-\\alpha\\mathcal{C}\\left({\\mG}\\right))^{%&#10;-1}\\mR_{T},\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mtext>ProteinRank</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo>(</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mG</mtext></merror><mo>)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mi>T</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere $\\mL = \\mD - \\mW$ is the Laplacian matrix, $\\mW$ is a weighted sum of multiple kernel matrices from heterogeneous network data sets, and $\\mD$ is a diagonal matrix with $D_{ii} = \\sum_j W_{ij}$. Additionally, GeneMANIA extends the binary matrix $\\mR_T^T$ to $\\mR^{*T}$ by introducing negative samples in which $R_{i,j}^* = -1$ if protein $i$ is known not to have function $j$. The developers of GeneMANIA further accelerated their algorithm by introducing Simultaneous Weights (hereafter GeneMANIA-SW) \\citep{sw}.\n\nYu \\emph{et al.} proposed Transductive Multilabel Classifier (TMC) \\citep{yu2013protein} by directly applying a Bi-relational graph model used in image annotation \\citep{wang2011image} to protein function prediction without consideration of the functional hierarchy. Instead, they use the cosine similarity of functional annotations to construct a function-function similarity matrix to replace $\\mH$. The key difference between TMC and our BirgRank is that TMC allows information to diffuse from functional terms to proteins, but not proteins to functional terms as in our BirgRank. Mathematically, the transition matrix of PageRank used in TMC is\n\n", "itemtype": "equation", "pos": 32059, "prevtext": "\nwhich can cause poor prediction quality due to the assumption of independence between functions (see Section~\\ref{sec:results}). Our method BirgRank is closely related to ProteinRank: \nif we plug $\\mH = \\mI$ into Equation~\\eqref{eq:split}, then the resulting BirgRank solution differs from the ProteinRank solution (Equation~\\eqref{eq:proteinrank}) only by a scalar coefficient and a slightly different normalization of $\\mG$.\n\nSimilar to ProteinRank, GeneMANIA \\citep{genemania} models protein function prediction as a multiclass multilabel classification problem by integrating multiple heterogeneous network datasets and then using the Label Propagation algorithm \\citep{zhou2004learning} as\n\n", "index": 21, "text": "\\begin{equation}\\label{eq:genemania}\n  \\mX_{\\text{GeneMANIA}} = (\\mI - \\mL)^{-1} \\mR^{*T},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mX_{\\text{GeneMANIA}}=(\\mI-\\mL)^{-1}\\mR^{*T},\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mtext>GeneMANIA</mtext></msub><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mI</mtext></merror><mo>-</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mL</mtext></merror></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mrow><mi/><mo>*</mo><mi>T</mi></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere matrix $\\mW_F$ is the degree-weighted function-function cosine similarity, i.e., $\\cos(\\mR_T^T,\\mR_T)$, $\\mW_G$ is a degree-weighed graph kernel of protein-protein association network, and $\\mW_R$ is a normalized function profile derived from $\\mR_T$. The developers of TMC further flatten the functional hierarchy by using a random walk with restart \\citep{yu2015down}. But this new method, called dRW \\citep{yu2015down} does not use a Bi-relational graph model, and was tested using a very small data set.\n\nWang \\emph{et al.} proposed clusDCA \\citep{wang2015exploiting} by extending their original method, Diffusion Component Analysis (DCA) \\citep{dca}. The clusDCA algorithm first uses PageRank to smooth both of the graphs, $\\mG$ and $\\mH$, and then computes a Singular Value Decomposition (SVD) of the two smoothed matrices for low-rank matrix approximations. Finally, it attempts to find the optimal projection between the two low-rank matrices, $\\bar{\\mX}$ and $\\bar{\\mY}$ by solving the following equation,\n\n", "itemtype": "equation", "pos": 33335, "prevtext": "\nwhere $\\mL = \\mD - \\mW$ is the Laplacian matrix, $\\mW$ is a weighted sum of multiple kernel matrices from heterogeneous network data sets, and $\\mD$ is a diagonal matrix with $D_{ii} = \\sum_j W_{ij}$. Additionally, GeneMANIA extends the binary matrix $\\mR_T^T$ to $\\mR^{*T}$ by introducing negative samples in which $R_{i,j}^* = -1$ if protein $i$ is known not to have function $j$. The developers of GeneMANIA further accelerated their algorithm by introducing Simultaneous Weights (hereafter GeneMANIA-SW) \\citep{sw}.\n\nYu \\emph{et al.} proposed Transductive Multilabel Classifier (TMC) \\citep{yu2013protein} by directly applying a Bi-relational graph model used in image annotation \\citep{wang2011image} to protein function prediction without consideration of the functional hierarchy. Instead, they use the cosine similarity of functional annotations to construct a function-function similarity matrix to replace $\\mH$. The key difference between TMC and our BirgRank is that TMC allows information to diffuse from functional terms to proteins, but not proteins to functional terms as in our BirgRank. Mathematically, the transition matrix of PageRank used in TMC is\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:tmc}\n  \\mP_{\\text{TMC}} = \\begin{bmatrix} {\\mW_G & \\mW_R \\\\ {\\boldsymbol{\\mathrm{{0}}}} & \\mW_F} \\end{bmatrix},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\mP_{\\text{TMC}}=\\begin{bmatrix}{\\mW_{G}\\hfil&amp;\\mW_{R}\\\\&#10;{\\boldsymbol{\\mathrm{{0}}}}&amp;\\mW_{F}\\end{bmatrix},}\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mP</mtext></merror><mtext>TMC</mtext></msub><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>G</mi></msub><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>R</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>F</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05506.tex", "nexttext": "\nwhere $\\mF$ is a protein-function matrix containing positive and negative samples of annotations.\n\n\n\n\n\n\\section{Results}\\label{sec:results}\n\n\\subsection{Experimental Setup}\nWe present a comprehensive evaluation of the six methods using the three benchmark datasets from yeast, human, and fly protein datasets that can be downloaded from the GeneMANIA-SW website \\citep{swdataweb}, and one up-to-date human dataset. For the newest human dataset, the network $\\mG$ was downloaded from BioGRID \\citep{biogrid}, and the annotations $\\mR$ and the hierarchy $\\mH$ from Gene Ontology \\citep{gene2015gene}. The basic statistics of the four datasets used in evaluation are shown in Table~\\ref{tab:dataset}. The number of direct GO indicates the direct of annotations of individual proteins by functional terms in the Gene Ontology Annotation database. This does not reflect the implied inclusion of parental terms. We extended the hierarchy by finding their parental terms throughout the hierarchy up to the root, and obtained the number of all terms in the hierarchy (Table~\\ref{tab:dataset}).\nThe multiple kernels from heterogeneous molecular data are directly downloaded from GeneMANIA-SW website \\citep{swdataweb}, and and combined into a single network (i.e., $\\mG$) with the weights provided in the datasets.\n\n\\begin{table}[ht!]\n\\centering\n\\caption{Statistics of datasets}\n\\label{tab:dataset}\n\\begin{tabular}{lcccc}\n\\hline\n\\multicolumn{1}{c}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Data\\\\ Set\\end{tabular}}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}No. of\\\\ proteins\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}No. of\\\\ direct GO\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}No. of\\\\ all GO\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}No. of\\\\ kernels\\end{tabular}} \\\\ \\hline\n\\textbf{Yeast} & 3904 & 1188 & 1695 & 44 \\\\\n\\textbf{Human-2010} & 13281 & 1952 & 2919 & 8 \\\\\n\\textbf{Fly} & 13562 & 2195 & 2919 & 38 \\\\\n\\textbf{Human-2015} & 14515 & 11519 & 27106 & 1 \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\nTo evaluate the quality of each method in protein function prediction, we split the non-zero entries in the functional annotation matrix $\\mR$ into two sets: $\\mR_T$, which the methods use in training, and $\\mR_E$, which we use to evaluate all of the methods' predictions (Figure~\\ref{fig:split}). In each experiment, $\\mR$ is split by removing a certain percentage of its nonzero entries chosen uniformly at random (the percentage used is indicated along the x-axis of the subplots in Figure~\\ref{fig:yauc}).\nWe use two different evaluation metrics to evaluate the prediction quality of each method. One is the Area Under the Receiver Operating Characteristic curve (AUROC) which is widely used in protein function prediction. The other is Mean Average Precision (MAP), which is widely used in the field of information retrieval (Figure~\\ref{fig:split}). When calculating AUROC or MAP, we do not take the annotations used during training into account, as shown in the red grid elements and red forks in Figure~\\ref{fig:split}.\n    \\begin{figure}[!ht]\n        \\centering\n        \\includegraphics[width=0.8\\textwidth]{fig/split.pdf}\\\\\n        \\caption{Evaluation Strategy. Split the given annotations $\\mR$ by putting $50\\%$ into the training set $\\mR_T$ and $50\\%$ into the evaluation set $\\mR_E$. Then compare the predictions against $\\mR_E$ and evaluate the performance of each method using AUROC and MAP.}\n        \\label{fig:split}\n    \\end{figure}\n\n\\subsection{Comparison of Prediction Performances}\nWe chose parameter settings for our algorithms as follows.\n We set $\\lambda = 0.5$ in determining $\\mH^*$, to allow equal diffusion upward and downward the hierarchy. For the other three parameters $\\alpha$, $\\theta$, and $\\mu$ in BirgRank (See \\eqref{eq:pprparameters}), we observed that different settings of these three parameters did not yield significant differences in performance, and found that a value of $0.5$ empirically achieved good results.\n\n For the paramters in AptRank, we set the total iteration number $K$ to be $8$ for all datasets. Interestingly, the coefficients output by AptRank tended to concentrate on the earliest terms: in many cases, the prediction performance is optimized using just the first two terms for this dataset (See Table~\\ref{tab:gamma}).\n\nFor the yeast, fly, and human-2010 datasets we set the splitting parameter to $t = 50\\%$. For the human-2015 dataset, we set $t = 75\\%$.\n\nThis process (lines 1 to 11 in Algorithm~\\ref{alg:aptrank}) is repeated $5$ times ($S = 5$ in Algorithm~\\ref{alg:aptrank}) to compute the optimal coefficients~$\\gamma^*$ used in producing the final AptRank prediction matrix.\n\nFor each dataset we carried out a protein function prediction using each of the 6 methods discussed, as follows.\nFirst, fix a percentage of the annotation data that will be used in the matrix $\\mR_T$ (we start with $10\\%$ and increase by increments of 10 up to $80\\%$). Then we run an experiment with each algorithm on each dataset, using that matrix $\\mR_T$ for training. We plot the results in Figure~\\ref{fig:yauc}).\nFor all algorithms we carried out 5 trials of this process for the yeast, fly, and human-2010 datasets, and show the standard deviation in Figure~\\ref{fig:yauc}) as an error bar. For the human-2015 dataset we carried out 3 trials of this process.\n\n\n\nThese results show that our methods both outperform the other four methods used in comparison in the evaluation. For example, both BirgRank and AptRank achieved AUROCs of $97\\%-98\\%$ given $80\\%$ of annotations in training. In terms of MAP, both of our methods achieve significantly higher MAP than the other methods, with approximately 2- to 3-fold improvement. Interestingly, BirgRank outperforms AptRank in the human-2015 dataset which is larger and sparser than the other three benchmark datasets. This suggests that $8-$step diffusion in AptRank may be not sufficient for a large but sparse network data.\n    \\begin{figure*}[t]\n        \\centering\n        \\includegraphics[width=0.9\\textwidth]{fig/performance.pdf}\\\\\n        \\caption{Comparison of prediction performance of the 6 methods on Yeast, Human-2010, Fly, and Human-2015 datasets. The left column displays the Area Under the ROC curve (AUROC) for each method, while the right column displays the Mean Average Precision (MAP).\n        For each subplot, the percentage label along the x-axis indicates the percentage of the function annotations in $\\mR$ that are extracted for use in the training portion of each method. The prediction evaluation is carried out on only the entries of $\\mR$ not used in this training.\n        }\n        \\label{fig:yauc}\n    \\end{figure*}\n\nIn terms of the use of functional hierarchy, we observed that the three methods using the functional hierarchy, clusDCA, BirgRank, and AptRank, all performed better than the methods that do not use the functional hierarchy. The models of TMC and our BirgRank are quite similar, differing in how the two methods direct the diffusion between the protein network $\\mG$ and functional hierarchy $\\mH$ (or function-function similarity in TMC). Comparing the performance of TMC and BirgRank, we can find that BirgRank, incorporating the functional hierarchy and diffusing information from proteins to functional terms, dramatically improves the prediction quality over TMC.\n\nIn addition, we find that GeneMANIA-SW and ProteinRank achieve similar performance. The key difference between GeneMANIA-SW and ProteinRank is that GeneMANIA-SW requires negative samples in the classification framework. This demonstrates that negative samples have very limited contribution to the performance of GeneMANIA-SW. Indeed, it is difficult to confirm that one protein does not have a function.\n\nFor clusDCA, its performance is better than the other three existing methods due to the use of functional hierarchy. It also has relatively good performance when only $10\\%$ of data are given (Figure~\\ref{fig:yauc}(C) and (D)). However, its performance levels off when more data are given, which is probably due to the difficulty of finding the best reduced dimensionality $d$. We chose $d = 2500$ as claimed by the authors of clusDCA in their paper \\citep{wang2015exploiting}.\n\n\n\n\n\n\n\\subsection{Analysis of Adaptive Coefficients}\n\nThe adaptive coefficients of AptRank ($\\gamma$) is the unique feature which differs from traditional PageRank. To investigate its behavior in the prediction, we list the medians of $\\gamma$ over different shuffles in the prediction of yeast and human-2015 datasets in Table~\\ref{tab:gamma}. We can see that there are three main features of $\\gamma$'s behaviors,\n\\begin{enumerate}\n  \\item $\\gamma^{(1)}$ is always zero, since the information flows diffuse within $\\mG$ from proteins at the first step and do not reach the hierarchy yet;\n  \\item as shown in yeast dataset, the distribution of $\\gamma$ is not uniform but concentrates on specific terms of Markov chains, which demonstrates that AptRank can adaptively select the most predictive terms rather than weights all terms uniformly like traditional PageRank; and\n  \\item in comparison of $\\gamma$ in yeast and human-2015 datasets, we find that AptRank mostly selects the 2nd term in human-2015 dataset but a few more terms in yeast dataset, which is due to the different densities of the two datasets. Yeast dataset is smaller but denser since it integrates $44$ different kernels into $\\mG$; human-2015 dataset is larger but sparser, and all the entries in the raw human-2015 dataset are binary.\n\\end{enumerate}\n\n\\begin{table}[]\n\\centering\n\\caption{Medians of $\\gamma$ in Prediction of Yeast and Human-2015 Datasets}\n\\label{tab:gamma}\n\\begin{tabular}{cccccccccc}\n\\hline\n\\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Data\\\\ Set\\end{tabular}}} & \\multirow{2}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Training\\\\ (\\%)\\end{tabular}}} & \\multicolumn{8}{c}{\\textbf{Markov chain iteration}} \\\\\n &  & \\textbf{1st} & \\textbf{2nd} & \\textbf{3rd} & \\textbf{4th} & \\textbf{5th} & \\textbf{6th} & \\textbf{7th} & \\textbf{8th} \\\\ \\hline\n\\multirow{8}{*}{\\textbf{Yeast}} & \\textbf{10\\%} & 0 & 0 & 0 & 0 & 0 & 0 & 0.08 & 0.92 \\\\\n & \\textbf{20\\%} & 0 & 0.11 & 0 & 0 & 0 & 0 & 0.23 & 0.66 \\\\\n & \\textbf{30\\%} & 0 & 0.34 & 0 & 0.08 & 0 & 0 & 0.58 & 0 \\\\\n & \\textbf{40\\%} & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n & \\textbf{50\\%} & 0 & 0 & 0 & 0 & 0.84 & 0 & 0.16 & 0 \\\\\n & \\textbf{60\\%} & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n & \\textbf{70\\%} & 0 & 0 & 0.09 & 0 & 0.91 & 0 & 0 & 0 \\\\\n & \\textbf{80\\%} & 0 & 0 & 0.64 & 0 & 0.36 & 0 & 0 & 0 \\\\ \\hline\n\\multirow{8}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Human\\\\ 2015\\end{tabular}}} & \\textbf{10\\%} & 0 & 0.20 & 0 & 0 & 0 & 0 & 0.31 & 0.49 \\\\\n & \\textbf{20\\%} & 0 & 0.65 & 0 & 0 & 0 & 0 & 0.11 & 0.24 \\\\\n & \\textbf{30\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n & \\textbf{40\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n & \\textbf{50\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n & \\textbf{60\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n & \\textbf{70\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n & \\textbf{80\\%} & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\n\\subsection{Comparison of Runtimes}\nThe average computational time of the six methods compared in this study are shown in Figure~\\ref{tab:ctime}. In this comparison, the computational time is recorded in the prediction using human-2015 dataset, the largest one in dimensions. We can clearly see AptRank takes the third longest, likely because it involves massive dense matrix operations. The SVD computations required in clusDCA are likely responsible for clusDCA having the longest running time. GeneMANIA-SW is the second most computationally expansive method since it computes the prediction scores function by function, which is extremely expensive when the number of functions is large, even though we only used direct GO terms (Table~\\ref{tab:dataset}) in GeneMANIA. BirgRank and TMC both use Bi-relational graph, and take only several minutes to solve the PageRank linear system. ProteinRank has the most simple model, and it takes the shortest time since it needs only to solve a PageRank linear system with approximately half the dimension of the systems involved in BirgRank and TMC.\n\n\\begin{table}[h]\n\\centering\n\\caption{Runtimes of the Six Methods in Minutes (Human-2015 Dataset)*}\n\\label{tab:ctime}\n\\begin{tabular}{lrrrrrr}\n\\hline\n\\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Methods}}} & \\multicolumn{6}{c}{\\textbf{Training Data Proportion}} \\\\\n\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\textbf{10\\%}} & \\multicolumn{1}{c}{\\textbf{20\\%}} & \\multicolumn{1}{c}{\\textbf{40\\%}} & \\multicolumn{1}{c}{\\textbf{50\\%}} & \\multicolumn{1}{c}{\\textbf{70\\%}} & \\multicolumn{1}{c}{\\textbf{80\\%}} \\\\ \\hline\n\\textbf{GM-SW} & 252.52 & 214.47 & 232.02 & 231.65 & 225.54 & 234.56 \\\\\n\\textbf{TMC} & 6.71 & 7.10 & 7.52 & 7.58 & 7.37 & 7.12 \\\\\n\\textbf{ProteinRank} & 0.85 & 0.87 & 0.87 & 0.87 & 0.88 & 0.88 \\\\\n\\textbf{clusDCA} & 1054 & 1019 & 1072 & 1061 & 1025 & 1050 \\\\\n\\textbf{BirgRank} & 9.42 & 9.46 & 9.46 & 9.45 & 9.42 & 9.49 \\\\\n\\textbf{AptRank} & 51.79 & 53.48 & 55.82 & 55.28 & 57.85 & 58.69 \\\\ \\hline\n\\end{tabular}\n\\begin{flushleft}\n{\\footnotesize  *The runtimes of $30\\%$ and $60\\%$ is not shown due to space limit. The AptRank uses 12-core parallel computing for matrix multiplication.}\n\\end{flushleft}\n\\end{table}\n\n\n\n\n\n\n\\section{Conclusion}\n\nWe have presented two network-based methods for the prediction of protein function annotations. Our first method, BirGRank, uses PageRank on a Bi-relational Graph model that incorporates protein-protein and function-function networks. Our second method, AptRank, introduces an adaptive mechanism to the PageRank framework that computes an optimal set of weights for the first several steps of diffusion so as to maximize recovery of a subset of known function annotations. We show that both of these methods outperform several state-of-the-art methods in almost all cases, and in particular, outperform those methods that do not incorporate information about the functional hierarchy. Our results also suggest that diffusion-based methods are still among the most competitive in network-based protein function predictions, compared to classification- and decomposition-based methods.\n\n\\section*{Acknowledgements}\n\nThe authors would like to thank Purdue Rosen Center for Advanced Computing (RCAC) for their support.\n\n\\section*{Funding}\n\nThis research has been supported in part by NSF CAREER CCF-1149756 and DARPA SIMPLEX.\n\n\\bibliographystyle{natbib}\n\n\\bibliography{mybib2}      \n\n\n\n\n", "itemtype": "equation", "pos": 34509, "prevtext": "\nwhere matrix $\\mW_F$ is the degree-weighted function-function cosine similarity, i.e., $\\cos(\\mR_T^T,\\mR_T)$, $\\mW_G$ is a degree-weighed graph kernel of protein-protein association network, and $\\mW_R$ is a normalized function profile derived from $\\mR_T$. The developers of TMC further flatten the functional hierarchy by using a random walk with restart \\citep{yu2015down}. But this new method, called dRW \\citep{yu2015down} does not use a Bi-relational graph model, and was tested using a very small data set.\n\nWang \\emph{et al.} proposed clusDCA \\citep{wang2015exploiting} by extending their original method, Diffusion Component Analysis (DCA) \\citep{dca}. The clusDCA algorithm first uses PageRank to smooth both of the graphs, $\\mG$ and $\\mH$, and then computes a Singular Value Decomposition (SVD) of the two smoothed matrices for low-rank matrix approximations. Finally, it attempts to find the optimal projection between the two low-rank matrices, $\\bar{\\mX}$ and $\\bar{\\mY}$ by solving the following equation,\n\n", "index": 25, "text": "\\begin{equation}\\label{eq:clusdca}\n  \\mX_{\\text{clusDCA}} = \\frac{\\bar{\\mX}^T\\mF\\bar{\\mY}}{{\\|{\\bar{\\mX}^T\\mF\\bar{\\mY}}\\|}_2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\mX_{\\text{clusDCA}}=\\frac{\\bar{\\mX}^{T}\\mF\\bar{\\mY}}{{\\|{\\bar{\\mX}^{T}\\mF\\bar%&#10;{\\mY}}\\|}_{2}},\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mtext>clusDCA</mtext></msub><mo>=</mo><mfrac><mrow><msup><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mo stretchy=\"false\">\u00af</mo></mover><mi>T</mi></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mF</mtext></merror><mo>\u2062</mo><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mY</mtext></merror><mo stretchy=\"false\">\u00af</mo></mover></mrow><msub><mrow><mo>\u2225</mo><mrow><msup><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mX</mtext></merror><mo stretchy=\"false\">\u00af</mo></mover><mi>T</mi></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mF</mtext></merror><mo>\u2062</mo><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mY</mtext></merror><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}]