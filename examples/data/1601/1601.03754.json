[{"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.0em}\n\n\\noindent where $c_q$ is the cluster centroid nearest to point $p_q$, then the\nnode ${\\mathscr{N}}_r$ can contain no centroids that own any descendant points of ${\\mathscr{N}}_q$ if\n\n\\vspace*{-1.0em}\n\n", "itemtype": "equation", "pos": 20496, "prevtext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\twocolumn[\n\\vspace*{-0.1in}\n\\icmltitle{Dual-tree $k$-means with bounded iteration runtime}\n\n\\vspace*{-0.5em}\n\\icmlauthor{Ryan R. Curtin}{ryan@ratml.org}\n\\icmladdress{School of Computational Science and Engineering, \\\\\nGeorgia Institute of Technology, Atlanta, GA 30332 USA}\n\n\\icmlkeywords{clustering, k-means, dual-tree algorithms, kd-tree, cover tree}\n\n\\vskip 0.15in\n]\n\n\\begin{abstract}\n$k$-means is a widely used clustering algorithm, but for $k$ clusters and a\ndataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.\nAlthough there are existing techniques to accelerate single Lloyd iterations,\nnone of these are tailored to the case of large $k$, which is increasingly\ncommon as dataset sizes grow.  We propose a dual-tree algorithm that gives the\n{\\it exact} same results as standard $k$-means; when using cover trees, we use\nadaptive analysis techniques to, under some assumptions, bound the\nsingle-iteration runtime of the algorithm as $O(N + k \\log k)$.  To our\nknowledge these are the first sub-$O(kN)$ bounds for exact Lloyd iterations.  We\nthen show that this theoretically favorable algorithm performs competitively in\npractice, especially for large $N$ and $k$ in low dimensions.  Further, the\nalgorithm is tree-independent, so any type of tree may be used.\n\\end{abstract}\n\n\\vspace*{-1.8em}\n\\section{Introduction}\n\\vspace*{-0.4em}\n\nOf all the clustering algorithms in use today, among the simplest and most\nutilized is the venerated $k$-means clustering algorithm, usually implemented\nvia Lloyd's algorithm: given a dataset $S$, repeat the following two steps (a\n`Lloyd iteration') until the centroids of each of the $k$ clusters converge:\n\n\\vspace*{-1.0em}\n\\begin{enumerate} \\itemsep -2pt\n  \\item Assign each point $p_i \\in S$ to the cluster with nearest centroid.\n  \\item Recalculate the centroids for each cluster using the assignments of each\npoint in $S$.\n\\end{enumerate}\n\\vspace*{-1.0em}\n\nClearly, a simple implementation of this algorithm will take $O(kN)$ time where\n$N = |S|$.  However, the number of iterations is not bounded unless the\npractitioner manually sets a maximum, and $k$-means is not guaranteed to\nconverge to the global best clustering.  Despite these shortcomings, in practice\n$k$-means tends to quickly converge to reasonable solutions.  Even so, there is\nno shortage of techniques for improving the clusters $k$-means converges to:\nrefinement of initial centroids \\cite{bradley1998refining} and weighted\nsampling of initial centroids \\cite{arthur2007k} are just two of many popular\nexisting strategies.\n\nThere are also a number of methods for accelerating the runtime of a single\niteration of $k$-means.  In general, these ideas use the triangle inequality to\nprune work during the assignments step.  Algorithms of this sort include the\nwork of Pelleg and Moore \\yrcite{pelleg1999accelerating}, Elkan\n\\yrcite{elkan2003using}, Hamerly \\yrcite{hamerly2010making}, and Ding et~al.\n\\yrcite{ding2015yinyang}.  However, the scaling of these algorithms can make\nthem problematic for the case of large $k$ and large $N$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{table*}[t!]\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n{\\bf Algorithm} & {\\bf Setup} & {\\bf Worst-case} & {\\bf Memory} \\\\\n\\hline\nnaive & n/a & $O(kN)$ & $O(k + N)$ \\\\\nblacklist & $O(N \\log N)$ & $O(kN)$ & $O(k \\log N + N)$ \\\\\nelkan & n/a & $O(k^2 + kN)$ & $O(k^2 + kN)$ \\\\\nhamerly & n/a & $O(k^2 + kN)$ & $O(k + N)$ \\\\\nyinyang & $O(k^2 + kN)$ & $O(kN)$ & $O(kN)$ \\\\\n{\\bf dualtree} & $O(N \\log N)$ & $O(k \\log k + N)^1$ & $O(k + N)$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\vspace*{-1.0em}\n\\caption{Runtime and memory bounds for $k$-means algorithms.}\n\\label{tab:runtimes}\n\\vspace*{-1.0em}\n\\end{table*}\n\nIn this paper, we describe a dual-tree $k$-means algorithm tailored to the large\n$k$ and large $N$ case that outperforms all competing algorithms in that\nsetting; this dual-tree algorithm also has bounded single-iteration runtime in\nsome situations (see Section \\ref{sec:theory}).  This algorithm, which is our\nmain contribution, has several appealing aspects:\n\n\\vspace*{-0.5em}\n\\begin{itemize} \\itemsep -1pt\n  \\item {\\bf Empirical efficiency}.  In the large $k$ and large $N$ setting for\nwhich this algorithm is designed, it outperforms all other alternatives, and\nscales better to larger datasets.  The algorithm is especially efficient in\nlow dimensionality.\n\n  \\item {\\bf Runtime guarantees}.  Using adaptive runtime analysis\ntechniques, we bound the single-iteration runtime of our algorithm with respect\nto the intrinsic dimensionality of the centroids and data, when cover trees are\nused.  This gives theoretical support for the use of our algorithm in large data\nsettings.  In addition, the bound is dependent on the intrinsic dimensionality,\n{\\it not} the extrinsic dimensionality.\n\n  \\item {\\bf Generalizability}.  We develop our algorithm using a\ntree-independent dual-tree algorithm abstraction \\cite{curtin2013tree}; this\nmeans that our algorithm may be used with {\\it any} type of valid tree.  This\nincludes not just $kd$-trees but also metric trees, cone trees,\noctrees, and others.  Different trees may be suited to different types of data,\nand since our algorithm is general, one may use any type of tree as a\nplug-and-play parameter.\n\n  \\item {\\bf Separation of concerns}.  The abstraction we use to develop our\nalgorithm allows us to focus on and formalize each of the pruning rules\nindividually (Section \\ref{sec:strategies}).  This aids understanding of the\nalgorithm and eases insertion of future improvements and better pruning rules.\n\\end{itemize}\n\\vspace*{-0.8em}\n\nSection \\ref{sec:scaling} shows the relevance of the large $k$ case; then, in\nSection \\ref{sec:trees}, we show that we can build a tree on the $k$ clusters,\nand then a dual-tree algorithm \\cite{curtin2013tree} can be used to efficiently\nperform an exact single iteration of $k$-means clustering.  Section\n\\ref{sec:strategies} details the four pruning strategies used in our algorithm,\nand Section \\ref{sec:algorithm} introduces the algorithm itself.  Sections\n\\ref{sec:theory} and \\ref{sec:empirical} show the theoretical and empirical\nresults for the algorithm, and finally Section \\ref{sec:conclusion} concludes\nthe paper and paints directions for future improvements.\n\n\\vspace*{-0.2em}\n\\section{Scaling $k$-means}\n\\label{sec:scaling}\n\\vspace*{-0.1em}\n\nAlthough the original publications on $k$-means only applied the algorithm to a\nmaximum dataset size of 760 points, the half-century of relentless progress\nsince then has seen dataset sizes scale into billions.  Due to its\nsimplicity, though, $k$-means has remained relevant, and is still applied in\nmany large-scale applications.\n\nIn cases where $N$ scales but $k$ remains small, a good choice of algorithm is a\nsampling algorithm, which will return an approximate clustering.  One sampling\ntechnique, coresets, can produce good clusterings for $n$ in the millions using\nseveral hundred or a few thousand points \\cite{coresets}.  However, for large\n$k$, the number of samples required to produce good clusterings can become\nprohibitive.\n\nFor large $k$, then, we turn to an alternative approach: accelerating exact\nLloyd iterations.  Existing techniques include the brute-force\nimplementation, the {\\it blacklist} algorithm\n\\cite{pelleg1999accelerating}, Elkan's algorithm \\yrcite{elkan2003using}, and\nHamerly's algorithm \\yrcite{hamerly2010making}, as well as the recent Yinyang\n$k$-means algorithm \\cite{ding2015yinyang}.  The blacklist algorithm builds a\n$kd$-tree on the dataset and, while the tree is traversed, blacklists individual\nclusters that cannot be the closest cluster (the {\\it owner}) of any descendant\npoints of a node.  Elkan's algorithm maintains an upper bound and a lower bound\non the distance between each point and centroid; Hamerly's algorithm is a\nmemory-efficient\nsimplification of this technique.  The Yinyang algorithm\norganizes the centroids into groups of about 10 (depending on algorithm\nparameters) using 5 iterations of $k$-means on the centroids followed by a\nsingle iteration of standard $k$-means on the points.  Once groups are built,\nthe Yinyang algorithm attempts to prune groups of centroids at a time using\nrules similar to Elkan and Hamerly's algorithms.\n\nOf these algorithms, only Yinyang $k$-means considers centroids in groups at\nall, but it does not consider points in groups.  On the other hand, the\nblacklist algorithm is the only algorithm that builds a tree on the points and\nis able to assign multiple points to a single cluster at once.  So, although\neach algorithm has its own useful region, none of the four we have considered\nhere are particularly suited to the case of large $N$ {\\bf and} large $k$.\n\nTable \\ref{tab:runtimes} shows setup costs,\nworst-case per-iteration runtimes, and memory usage of each of these algorithms\nas well as the proposed dual-tree algorithm\\footnote{The dual-tree algorithm\nworst-case runtime bound also depends on some assumptions on dataset-dependent\nconstants.  This is detailed further in Section \\ref{sec:theory}.}.  The\nexpected runtime of the blacklist algorithm is, under some assumptions,\n$O(k + k \\log N + N)$ per iteration.  The expected runtime of Hamerly's and\nElkan's algorithm is $O(k^2 + \\alpha N)$ time, where $\\alpha$ is the expected\nnumber of clusters visited by each point (in both Elkan and Hamerly's results,\n$\\alpha$ seems to be small).\n\nHowever, none of these algorithms are specifically tailored to the large $k$\ncase, and the large $k$ case is common.  Pelleg and Moore\n\\yrcite{pelleg1999accelerating} report several hundred clusters in a subset of\n800k objects from the SDSS dataset.  Clusterings for $n$-body simulations on\nastronomical data often involve several thousand clusters\n\\cite{kwon2010scalable}.  Csurka\net~al. \\yrcite{csurka} extract vocabularies from image sets using $k$-means with\n$k \\sim 1000$.  Coates et~al. \\yrcite{coates} show that $k$-means can work\nsurprisingly well for unsupervised feature learning for images, using $k$ as\nlarge as 4000 on 50000 images.  Also, in text mining, datasets can have up to\n18000 unique labels \\cite{bengio2010label}.  Can and Ozkarahan\n\\yrcite{can1990concepts} suggest that the number of clusters in text data is\ndirectly related to the size of the vocabulary, suggesting $k \\sim mN/t$ where\n$m$ is the vocabulary size, $n$ is the number of documents, and $t$ is the\nnumber of nonzero entries in the term matrix.\n\n\nThus, it is important to have an algorithm with favorable scaling properties for\nboth large $k$ and $N$.\n\n\\vspace*{-0.4em}\n\\section{Tree-based algorithms}\n\\label{sec:trees}\n\\vspace*{-0.2em}\n\nThe blacklist algorithm is an example of a {\\it single-tree algorithm}: one tree\n(the {\\it reference tree}) is built on the dataset, and then that tree is\ntraversed.  This approach is applicable to a surprising variety of\nother problems, too \\cite{bentley1975multidimensional, moore1998very,\ncurtin2013fast}.  Following the blacklist algorithm, then, it is only\nnatural to build a tree on the data points.  Tree-building is (generally) a\none-time $O(N \\log N)$ cost and for large $N$ or $k$, the cost of tree\nbuilding is often negligible compared to the time it takes to perform the\nclustering.\n\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[b]{0.31\\textwidth}\n  \\begin{tikzpicture}\n    \\filldraw [lightgray!60!blue] (0.0, 0.0) circle (0.6) { };\n    \\draw [thin] (0.0, 0.0) circle (0.6) { };\n    \\node [ ] at (0.0, 0.0) { $\\mathscr{N}_q$ };\n\n    \\filldraw [lightgray!60!red] (-0.6, 0.2) circle (0.4) { };\n    \\draw [thin] (-0.6, 0.2) circle (0.4) { };\n    \\node [ ] at (-0.6, 0.2) { $\\mathscr{N}_{r2}$ };\n\n    \\filldraw [lightgray!60!red] (2.1, 0.1) circle (0.4) { };\n    \\draw [thin] (2.1, 0.1) circle (0.4) { };\n    \\node [ ] at (2.1, 0.1) { $\\mathscr{N}_r$ };\n\n    \\draw [black,dashed,domain=-30:30] plot ({1.63246*cos(\\x)},\n{1.63246*sin(\\x)});\n    \\draw [black,dashed,domain=150:210] plot ({1.63246*cos(\\x)},\n{1.63246*sin(\\x)});\n\n    \\draw (0.6, 0.0) -- (1.63246, 0.0) { };\n    \\draw (0.6, 0.0) -- (0.7, 0.1) { };\n    \\draw (0.6, 0.0) -- (0.7, -0.1) { };\n    \\draw (1.63246, 0.0) -- (1.53246, 0.1) { };\n    \\draw (1.63246, 0.0) -- (1.53246, -0.1) { };\n    \\node [ ] at (1.1, 0.3) { $\\scriptstyle{{\\operatorname{ub}}({\\mathscr{N}}_q)}$ };\n  \\end{tikzpicture}\n  \\caption{$\\mathscr{N}_r$ can be pruned.}\n  \\label{fig:prune-1}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.31\\textwidth}\n  \\begin{tikzpicture}\n    \\draw [gray,dashed] (-0.65, -0.5) -- (-0.65, 1.1) { };\n\n    \\filldraw [lightgray!60!blue] (0.0, 0.0) circle (0.05) { };\n    \\draw [thin] (0.0, 0.0) circle (0.05) { };\n    \\node [ ] at (-0.26, 0.0) { $p_q$ };\n\n    \\filldraw [lightgray!60!red] (0.4, 0.2) circle (0.05) { };\n    \\draw [thin] (0.4, 0.2) circle (0.05) { };\n    \\node [ ] at (0.3, 0.42) { $c_j$ };\n\n    \\draw [gray] (0.44, 0.23) -- (1.2, 0.6) { };\n    \\draw [gray] (0.44, 0.23) -- (0.47, 0.3) { };\n    \\draw [gray] (0.44, 0.23) -- (0.51, 0.2) { };\n    \\draw [gray] (1.2, 0.6) -- (1.17, 0.53) { };\n    \\draw [gray] (1.2, 0.6) -- (1.13, 0.63) { };\n    \\node [ ] at (0.9, 0.1) { $m_j$ };\n\n    \\draw [black,dashed,domain=-30:50] plot ({1.3416*cos(\\x)},\n{1.3416*sin(\\x)});\n\n    \\draw [gray] (0.045, -0.005) -- (1.3212, -0.23297) { };\n    \\draw [gray] (0.045, -0.005) -- (0.1, 0.03) { };\n    \\draw [gray] (0.045, -0.005) -- (0.095, -0.065) { };\n    \\draw [gray] (1.3212, -0.23297) -- (1.25, -0.26) { };\n    \\draw [gray] (1.3212, -0.23297) -- (1.27, -0.17) { };\n    \\node [ ] at (0.5, -0.4) { $\\scriptstyle{{\\operatorname{ub}}(p_q) + m_j}$ };\n\n    \\filldraw [lightgray!60!red] (2.4, 0.3) circle (0.05) { };\n    \\draw [thin] (2.4, 0.3) circle (0.05) { };\n    \\node [ ] at (2.4, 0.5) { $c_k$ };\n\n    \\draw [gray] (2.45, 0.3) -- (3.3, 0.3) { };\n    \\draw [gray] (2.45, 0.3) -- (2.52, 0.27) { };\n    \\draw [gray] (2.45, 0.3) -- (2.52, 0.33) { };\n    \\draw [gray] (3.3, 0.3) -- (3.23, 0.27) { };\n    \\draw [gray] (3.3, 0.3) -- (3.23, 0.33) { };\n    \\node [ ] at (2.8, 0.1) { $\\scriptstyle{\\min_k m_k}$ };\n\n    \\draw [black,dotted] (2.4, 0.3) circle (0.9) { };\n\n  \\end{tikzpicture}\n  \\caption{$p_q$'s owner cannot change.}\n  \\label{fig:prune-2}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.31\\textwidth}\n  \\begin{tikzpicture}\n    \\draw [gray,dashed] (-0.9, -0.5) -- (-0.9, 1.1) { };\n\n    \\filldraw [lightgray!60!blue] (0.0, 0.0) circle (0.05) { };\n    \\draw [thin] (0.0, 0.0) circle (0.05) { };\n    \\node [ ] at (-0.26, 0.0) { $p_q$ };\n\n    \\filldraw [lightgray!60!red] (0.4, 0.2) circle (0.05) { };\n    \\draw [thin] (0.4, 0.2) circle (0.05) { };\n    \\node [ ] at (0.3, 0.42) { $c_j$ };\n\n    \\draw [gray] (0.44, 0.23) -- (1.2, 0.6) { };\n    \\draw [gray] (0.44, 0.23) -- (0.47, 0.3) { };\n    \\draw [gray] (0.44, 0.23) -- (0.51, 0.2) { };\n    \\draw [gray] (1.2, 0.6) -- (1.17, 0.53) { };\n    \\draw [gray] (1.2, 0.6) -- (1.13, 0.63) { };\n    \\node [ ] at (0.9, 0.1) { $m_j$ };\n\n    \\draw [black,dashed,domain=-30:50] plot ({1.3416*cos(\\x)},\n{1.3416*sin(\\x)});\n\n    \\draw [gray] (0.045, -0.005) -- (1.3212, -0.23297) { };\n    \\draw [gray] (0.045, -0.005) -- (0.1, 0.03) { };\n    \\draw [gray] (0.045, -0.005) -- (0.095, -0.065) { };\n    \\draw [gray] (1.3212, -0.23297) -- (1.25, -0.26) { };\n    \\draw [gray] (1.3212, -0.23297) -- (1.27, -0.17) { };\n    \\node [ ] at (0.5, -0.4) { $\\scriptstyle{{\\operatorname{ub}}(p_q) + m_j}$ };\n\n    \\filldraw [lightgray!60!red] (2.0, 0.3) circle (0.05) { };\n    \\draw [thin] (2.0, 0.3) circle (0.05) { };\n    \\node [ ] at (2.0, 0.5) { $c_k$ };\n\n    \\draw [gray] (2.05, 0.3) -- (2.9, 0.3) { };\n    \\draw [gray] (2.05, 0.3) -- (2.12, 0.33) { };\n    \\draw [gray] (2.05, 0.3) -- (2.12, 0.27) { };\n    \\draw [gray] (2.9, 0.3) -- (2.83, 0.33) { };\n    \\draw [gray] (2.9, 0.3) -- (2.83, 0.27) { };\n    \\node [ ] at (2.4, 0.1) { $\\scriptstyle{\\min_k m_k}$ };\n\n    \\draw [black,dotted] (2.0, 0.3) circle (0.9) { };\n\n  \\end{tikzpicture}\n\\caption{$p_q$'s owner can change.}\n\\label{fig:prune-3}\n\\end{subfigure}\n\\vspace*{-0.7em}\n\\caption{Different pruning situations.}\n\\label{fig:prune_one}\n\\vspace*{-1.0em}\n\\end{figure*}\n\nThe speedup of the blacklist algorithm comes from the hierarchical nature of\ntrees: during the algorithm, we may rule out a cluster centroid for {\\it many\npoints at once}.  The same reason is responsible for the impressive speedups\nobtained for other single-tree algorithms, such as nearest neighbor search\n\\cite{bentley1975multidimensional, liu2004investigation}.  But\nfor nearest neighbor search, the nearest neighbor is often required not just for\na query point but instead a {\\it query set}.  This observation\nmotivated the development of {\\it dual-tree algorithms}, which also build a tree\non the query set (the {\\it query tree}) in order to share work across query\npoints.  Both trees are recursed in such a way that combinations of query\nnodes and reference nodes are visited.  Pruning criteria are applied to\nthese node combinations, and if a combination may be pruned, then the\nrecursion does not continue in that direction.\n\nThis approach\nis applicable to $k$-means with large $k$: we may build a tree on the\n$k$ cluster centroids, as well as a tree on the data points, and then we may\nrule out {\\it many} centroids for {\\it many} points at once.\n\nA recent result generalizes the class of dual-tree\nalgorithms, simplifying their expression and development\n\\cite{curtin2013tree}.  Any dual-tree algorithm can be decomposed into three\nparts: a type of space tree, a pruning dual-tree traversal, and a point-to-point\n\\texttt{BaseCase()} function and node-to-node \\texttt{Score()} function that\ndetermines when pruning is possible.  Precise definitions and details of the\nabstraction are given by \\citet{curtin2013tree}, but for our purposes, this means\nthat we can describe a dual-tree $k$-means algorithm entirely with a\nstraightforward \\texttt{BaseCase()} function and \\texttt{Score()} function.  Any\ntree and any traversal can then be used to create a working dual-tree algorithm.\n\n\n\nThe two types of trees we will explicitly consider in this paper are the\n$kd$-tree and the cover tree \\cite{langford2006}, but it should be remembered\nthat the algorithm as provided is sufficiently general to work with any other\ntype of tree.  Therefore, we standardize notation for trees: a tree is denoted\nwith $\\mathscr{T}$, and a node in the tree is denoted by $\\mathscr{N}$.  Each\nnode in a tree may have children; the set of children of $\\mathscr{N}_i$ is\ndenoted $\\mathscr{C}_i$.  In addition, each node may hold some points; this set\nof points is denoted $\\mathscr{P}_i$.  Lastly, the set of {\\it descendant}\npoints of a node $\\mathscr{N}_i$ is denoted $\\mathscr{D}^p_i$.  The descendant\npoints are all points held by descendant nodes, and it is important to note that\nthe set $\\mathscr{P}_i$ is {\\it not} equivalent to $\\mathscr{D}^p_i$.  This\nnotation is taken from \\citet{curtin2013tree} and is detailed more\ncomprehensively there.  Lastly, we say that a centroid $c$ {\\it owns} a point\n$p$ if $c$ is the closest centroid to $p$.\n\n\\vspace*{-0.5em}\n\\section{Pruning strategies}\n\\label{sec:strategies}\n\\vspace*{-0.2em}\n\nAll of the existing accelerated $k$-means algorithms operate by avoiding\nunnecessary work via the use of pruning strategies.  Thus, we will pursue four\npruning strategies, each based on or related to earlier work\n\\cite{pelleg1999accelerating, elkan2003using, hamerly2010making}.\n\nThese pruning strategies are meant to be used during the dual-tree traversal,\nfor which we have built a query tree $\\mathscr{T}_q$ on the points and\na reference tree $\\mathscr{T}_r$ on the centroids.  Therefore, these pruning\nstrategies consider not just combinations of single points and centroid\n$p_q$ and $c_i$, but the combination of sets of points and sets of centroids,\nrepresented by a query tree node ${\\mathscr{N}}_q$ and a centroid tree node ${\\mathscr{N}}_r$.  This\nallows us to prune many centroids for many points simultaneously.\n\n\n\n\n\n\n\n\n\n\n\n\n{\\bf Strategy one.} When visiting a particular combination $({\\mathscr{N}}_q, {\\mathscr{N}}_r)$\n(with ${\\mathscr{N}}_q$ holding points in the dataset and ${\\mathscr{N}}_r$ holding\ncentroids), the combination should be pruned if every descendant centroid in\n${\\mathscr{N}}_r$ can be shown to\nown none of the points in ${\\mathscr{N}}_q$.  If we have cached an upper bound ${\\operatorname{ub}}({\\mathscr{N}}_q)$\non the distance between any descendant point of ${\\mathscr{N}}_q$ and its nearest cluster\ncentroid that satisfies\n\n\\vspace*{-1.0em}\n\n", "index": 1, "text": "\\begin{equation}\n{\\operatorname{ub}}({\\mathscr{N}}_q) \\ge \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q,\nc_q)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{ub}}({\\mathscr{N}}_{q})\\geq\\max_{p_{q}\\in\\mathscr{D}^{p}_{q}}d(%&#10;p_{q},c_{q})\" display=\"block\"><mrow><mrow><mo>ub</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>p</mi><mi>q</mi></msub><mo>\u2208</mo><msubsup><mi class=\"ltx_font_mathscript\">\ud835\udc9f</mi><mi>q</mi><mi>p</mi></msubsup></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.5em}\n\nThis relation bears similarity to the pruning rules for nearest neighbor search\n\\cite{curtin2013tree} and max-kernel search \\cite{curtin2014dual}.  Figure\n\\ref{fig:prune-1} shows a situation where ${\\mathscr{N}}_r$ can be pruned; in this case,\nball-shaped tree nodes are used, and the upper bound ${\\operatorname{ub}}({\\mathscr{N}}_q)$ is set to\n$d_{\\max}({\\mathscr{N}}_q, {\\mathscr{N}}_{r2})$.\n\n{\\bf Strategy two.} The recursion down a particular branch of the query tree\nshould\nterminate early if we can determine that only one cluster can possibly own all\nof the descendant points of that branch.  This is related to the first strategy.\nIf we have been caching the number of pruned centroids (call this\n${\\operatorname{pruned}}({\\mathscr{N}}_q)$), as well as the identity of any\narbitrary non-pruned centroid (call this ${\\operatorname{closest}}({\\mathscr{N}}_q)$), then if\n${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, we may conclude that the\ncentroid ${\\operatorname{closest}}({\\mathscr{N}}_q)$ is the owner of all descendant\npoints of ${\\mathscr{N}}_q$, and there is no need for further recursion in\n${\\mathscr{N}}_q$.\n\n{\\bf Strategy three.} The traversal should not visit nodes whose owner could not\nhave possibly changed between iterations; that is, the tree should be coalesced\nto include only nodes whose owners may have changed.\n\nThere are two easy ways to use the triangle inequality to show that the owner of\na point cannot change between iterations.  Figures \\ref{fig:prune-2} and\n\\ref{fig:prune-3} show the\nfirst: we have a point $p_q$ with owner $c_j$ and second-closest\ncentroid $c_k$.  Between iterations, each centroid will move when it is\nrecalculated; define the distance that centroid $c_i$ has moved as\n$m_i$.  Then we bound the distances for the next\niteration: $d(p_q, c_j) + m_j$ is an upper bound on the distance from $p_q$\nto its owner next iteration, and $d(p_q, c_k) - \\max_i m_i$ is a lower bound on\nthe distance from $p_q$ to its second closest centroid next iteration.  We\nmay use these bounds to conclude that if\n\n\\vspace*{-1.2em}\n\n", "itemtype": "equation", "pos": 20830, "prevtext": "\n\\vspace*{-1.0em}\n\n\\noindent where $c_q$ is the cluster centroid nearest to point $p_q$, then the\nnode ${\\mathscr{N}}_r$ can contain no centroids that own any descendant points of ${\\mathscr{N}}_q$ if\n\n\\vspace*{-1.0em}\n\n", "index": 3, "text": "\\begin{equation}\nd_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r) > {\\operatorname{ub}}({\\mathscr{N}}_q).\n\\label{eqn:prune}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"d_{\\min}({\\mathscr{N}}_{q},{\\mathscr{N}}_{r})&gt;{\\operatorname{ub}}({\\mathscr{N}%&#10;}_{q}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>d</mi><mi>min</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mo>ub</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.6em}\n\n\\noindent then the owner of $p_q$ next iteration must be $c_j$.  Generalizing\nfrom individual points $p_q$ to tree nodes ${\\mathscr{N}}_q$ is easy.\nThis pruning strategy can only be used when all descendant points of\n${\\mathscr{N}}_q$ are owned by a single centroid, and in order to perform the\nprune, we need to establish a lower bound on the distance between any\ndescendant point of the node ${\\mathscr{N}}_q$ and the second closest centroid.\nCall this bound ${\\operatorname{lb}}({\\mathscr{N}}_q)$.  Remember that\n${\\operatorname{ub}}({\\mathscr{N}}_q)$ provides an upper bound on the distance\nbetween any descendant point of ${\\mathscr{N}}_q$ and its nearest centroid.  Then,\nif all descendant points of ${\\mathscr{N}}_q$ are owned by some cluster $c_j$ in\none iteration, and\n\n\\vspace*{-1.4em}\n\n", "itemtype": "equation", "pos": 23053, "prevtext": "\n\\vspace*{-1.5em}\n\nThis relation bears similarity to the pruning rules for nearest neighbor search\n\\cite{curtin2013tree} and max-kernel search \\cite{curtin2014dual}.  Figure\n\\ref{fig:prune-1} shows a situation where ${\\mathscr{N}}_r$ can be pruned; in this case,\nball-shaped tree nodes are used, and the upper bound ${\\operatorname{ub}}({\\mathscr{N}}_q)$ is set to\n$d_{\\max}({\\mathscr{N}}_q, {\\mathscr{N}}_{r2})$.\n\n{\\bf Strategy two.} The recursion down a particular branch of the query tree\nshould\nterminate early if we can determine that only one cluster can possibly own all\nof the descendant points of that branch.  This is related to the first strategy.\nIf we have been caching the number of pruned centroids (call this\n${\\operatorname{pruned}}({\\mathscr{N}}_q)$), as well as the identity of any\narbitrary non-pruned centroid (call this ${\\operatorname{closest}}({\\mathscr{N}}_q)$), then if\n${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, we may conclude that the\ncentroid ${\\operatorname{closest}}({\\mathscr{N}}_q)$ is the owner of all descendant\npoints of ${\\mathscr{N}}_q$, and there is no need for further recursion in\n${\\mathscr{N}}_q$.\n\n{\\bf Strategy three.} The traversal should not visit nodes whose owner could not\nhave possibly changed between iterations; that is, the tree should be coalesced\nto include only nodes whose owners may have changed.\n\nThere are two easy ways to use the triangle inequality to show that the owner of\na point cannot change between iterations.  Figures \\ref{fig:prune-2} and\n\\ref{fig:prune-3} show the\nfirst: we have a point $p_q$ with owner $c_j$ and second-closest\ncentroid $c_k$.  Between iterations, each centroid will move when it is\nrecalculated; define the distance that centroid $c_i$ has moved as\n$m_i$.  Then we bound the distances for the next\niteration: $d(p_q, c_j) + m_j$ is an upper bound on the distance from $p_q$\nto its owner next iteration, and $d(p_q, c_k) - \\max_i m_i$ is a lower bound on\nthe distance from $p_q$ to its second closest centroid next iteration.  We\nmay use these bounds to conclude that if\n\n\\vspace*{-1.2em}\n\n", "index": 5, "text": "\\begin{equation}\nd(p_q, c_j) + m_j < d(p_q, c_k) - \\max_i m_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"d(p_{q},c_{j})+m_{j}&lt;d(p_{q},c_{k})-\\max_{i}m_{i},\" display=\"block\"><mrow><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>&lt;</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><munder><mi>max</mi><mi>i</mi></munder><mo>\u2061</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.4em}\n\n\\noindent then ${\\mathscr{N}}_q$ is owned by cluster $c_j$ in the next iteration.\nImplementationally, it is convenient to have ${\\operatorname{lb}}({\\mathscr{N}}_q)$ store a\nlower bound on the distance between any descendant point of ${\\mathscr{N}}_q$ and\nthe nearest pruned centroid.  Then, if ${\\mathscr{N}}_r$ is entirely owned by one\ncluster, all other centroids are pruned, and ${\\operatorname{lb}}({\\mathscr{N}}_q)$\nholds the necessary lower bound for pruning according to the rule above.\n\nThe second way to use the triangle inequality to show that an owner cannot\nchange depends on the distances between centroids.  Suppose that $p_q$ is\nowned by $c_j$ at the current iteration; then, if\n\n\\vspace*{-1.3em}\n\n", "itemtype": "equation", "pos": 23945, "prevtext": "\n\\vspace*{-1.6em}\n\n\\noindent then the owner of $p_q$ next iteration must be $c_j$.  Generalizing\nfrom individual points $p_q$ to tree nodes ${\\mathscr{N}}_q$ is easy.\nThis pruning strategy can only be used when all descendant points of\n${\\mathscr{N}}_q$ are owned by a single centroid, and in order to perform the\nprune, we need to establish a lower bound on the distance between any\ndescendant point of the node ${\\mathscr{N}}_q$ and the second closest centroid.\nCall this bound ${\\operatorname{lb}}({\\mathscr{N}}_q)$.  Remember that\n${\\operatorname{ub}}({\\mathscr{N}}_q)$ provides an upper bound on the distance\nbetween any descendant point of ${\\mathscr{N}}_q$ and its nearest centroid.  Then,\nif all descendant points of ${\\mathscr{N}}_q$ are owned by some cluster $c_j$ in\none iteration, and\n\n\\vspace*{-1.4em}\n\n", "index": 7, "text": "\\begin{equation}\n{\\operatorname{ub}}({\\mathscr{N}}_q) + m_j < {\\operatorname{lb}}({\\mathscr{N}}_q) -\n\\max_i m_i,\n\\label{eqn:static-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{ub}}({\\mathscr{N}}_{q})+m_{j}&lt;{\\operatorname{lb}}({\\mathscr{N}}%&#10;_{q})-\\max_{i}m_{i},\" display=\"block\"><mrow><mrow><mrow><mrow><mo>ub</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>&lt;</mo><mrow><mrow><mo>lb</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><munder><mi>max</mi><mi>i</mi></munder><mo>\u2061</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.3em}\n\n\\noindent then $c_j$ will own $p_q$ next iteration \\cite{elkan2003using}.  We\nmay adapt this rule to tree nodes ${\\mathscr{N}}_q$ in the same way as the previous rule;\nif ${\\mathscr{N}}_q$ is owned by cluster $c_j$ during this iteration and\n\n\\vspace*{-1.3em}\n\n", "itemtype": "equation", "pos": 24825, "prevtext": "\n\\vspace*{-1.4em}\n\n\\noindent then ${\\mathscr{N}}_q$ is owned by cluster $c_j$ in the next iteration.\nImplementationally, it is convenient to have ${\\operatorname{lb}}({\\mathscr{N}}_q)$ store a\nlower bound on the distance between any descendant point of ${\\mathscr{N}}_q$ and\nthe nearest pruned centroid.  Then, if ${\\mathscr{N}}_r$ is entirely owned by one\ncluster, all other centroids are pruned, and ${\\operatorname{lb}}({\\mathscr{N}}_q)$\nholds the necessary lower bound for pruning according to the rule above.\n\nThe second way to use the triangle inequality to show that an owner cannot\nchange depends on the distances between centroids.  Suppose that $p_q$ is\nowned by $c_j$ at the current iteration; then, if\n\n\\vspace*{-1.3em}\n\n", "index": 9, "text": "\\begin{equation}\nd(p_q, c_j) - m_j < 2 \\left( \\min_{c_i \\in C, c_i \\ne c_j} d(c_i, c_j) \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"d(p_{q},c_{j})-m_{j}&lt;2\\left(\\min_{c_{i}\\in C,c_{i}\\neq c_{j}}d(c_{i},c_{j})\\right)\" display=\"block\"><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>&lt;</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><munder><mi>min</mi><mrow><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2208</mo><mi>C</mi></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2260</mo><msub><mi>c</mi><mi>j</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.3em}\n\n\\noindent then ${\\mathscr{N}}_q$ is owned by cluster $c_j$ in the next iteration.\nNote that the above rules do work with individual points $p_q$ instead of nodes\n${\\mathscr{N}}_q$ if we have a valid upper bound ${\\operatorname{ub}}(p_q)$ and a\nvalid lower bound ${\\operatorname{lb}}(p_q)$.  Any nodes or points that satisfy\nthe above conditions do not need to be visited during the next iteration, and\ncan be removed from the tree for the next iteration.\n\n{\\bf Strategy four.} The traversal should use bounding information from previous\niterations; for instance, ${\\operatorname{ub}}({\\mathscr{N}}_q)$ should not be reset\nto $\\infty$ at the beginning of each iteration.  Between iterations, we may\nupdate ${\\operatorname{ub}}({\\mathscr{N}}_q)$, ${\\operatorname{ub}}(p_q)$,\n${\\operatorname{lb}}({\\mathscr{N}}_q)$, and ${\\operatorname{lb}}(p_q)$ according to\nthe following rules:\n\n\\vspace*{-1.5em}\n\\begin{eqnarray}\n{\\operatorname{ub}}({\\mathscr{N}}_q) &\\gets&\n  \\begin{cases}\n    {\\operatorname{ub}}({\\mathscr{N}}_q) + m_j & \\text{if } {\\mathscr{N}}_q \\text{ is}\\\\\n\\multicolumn{2}{l}{\\text{\\ \\ \\ \\ owned by a single cluster $c_j$}}\n\\\\\n    {\\operatorname{ub}}({\\mathscr{N}}_q) + \\max_i m_i & \\text{if } {\\mathscr{N}}_q \\text{ is}\\\\\n\\multicolumn{2}{l}{\\text{\\ \\ \\ \\ not owned by a single cluster},}\n  \\end{cases} \\label{eqn:special} \\\\\n{\\operatorname{ub}}(p_q) &\\gets& {\\operatorname{ub}}(p_q) + m_j, \\\\\n{\\operatorname{lb}}({\\mathscr{N}}_q) &\\gets& {\\operatorname{lb}}({\\mathscr{N}}_q) -\n\\max_i m_i, \\\\\n{\\operatorname{lb}}(p_q) &\\gets& {\\operatorname{lb}}(p_q) - \\max_i m_i.\n\\end{eqnarray}\n\\vspace*{-1.0em}\n\nSpecial handling is required when descendant points of ${\\mathscr{N}}_q$\nare not owned by a single centroid (Equation \\ref{eqn:special}).  It is also\ntrue that for a child node ${\\mathscr{N}}_c$ of ${\\mathscr{N}}_q$, ${\\operatorname{ub}}({\\mathscr{N}}_q)$ is a valid upper bound\nfor ${\\mathscr{N}}_c$ and ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is a valid lower bound for ${\\mathscr{N}}_c$: that is, the upper\nand lower bounds may be taken from a parent, and they are still valid.\n\n\\vspace*{-0.6em}\n\\section{The dual-tree $k$-means algorithm}\n\\label{sec:algorithm}\n\\vspace*{-0.3em}\n\nThese four pruning strategies lead to a high-level $k$-means algorithm,\ndescribed in Algorithm \\ref{alg:high_level}.  During the course of this\nalgorithm, to implement each of our pruning strategies, we will need to maintain\nthe following quantities:\n\n\\vspace*{-1.0em}\n\\begin{itemize} \\itemsep -1.5pt\n  \\item ${\\operatorname{ub}}({\\mathscr{N}}_q)$: an upper bound on the distance\nbetween any descendant point of a node ${\\mathscr{N}}_q$ and the nearest centroid\nto that point.\n  \\item ${\\operatorname{lb}}({\\mathscr{N}}_q)$: a lower bound on the distance\nbetween any descendant point of a node ${\\mathscr{N}}_q$ and the nearest pruned\ncentroid.\n  \\item ${\\operatorname{pruned}}({\\mathscr{N}}_q)$: the number of centroids pruned\nduring traversal for ${\\mathscr{N}}_q$.\n  \\item ${\\operatorname{closest}}({\\mathscr{N}}_q)$: if ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, this\nholds the owner of all descendant points of ${\\mathscr{N}}_q$.\n  \\item ${\\operatorname{canchange}}({\\mathscr{N}}_q)$: whether or not\n${\\mathscr{N}}_q$ can change owners next iteration.\n  \\item ${\\operatorname{ub}}(p_q)$: an upper bound on the distance between point\n$p_q$ and its nearest centroid.\n  \\item ${\\operatorname{lb}}(p_q)$: a lower bound on the distance between point\n$p_q$ and its second nearest centroid.\n  \\item ${\\operatorname{closest}}(p_q)$: the closest centroid to $p_q$ (this is\nalso the owner of $p_q$).\n  \\item ${\\operatorname{canchange}}(p_q)$: whether or not $p_q$ can change owners\nnext iteration.\n\\end{itemize}\n\\vspace*{-0.8em}\n\nAt the beginning of the algorithm, each upper bound is initialized to $\\infty$,\neach lower bound is initialized to $\\infty$, ${\\operatorname{pruned}}(\\cdot)$\nis initialized to $0$ for each node, and\n${\\operatorname{closest}}(\\cdot)$ is initialized to an invalid centroid for each\nnode and point.  ${\\operatorname{canchange}}(\\cdot)$ is set to {\\tt\ntrue} for each node and point.  Thus line\n6 does nothing on the first iteration.\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} dataset $S \\in \\mathcal{R}^{N \\times d}$, initial\ncentroids $C \\in \\mathcal{R}^{k \\times d}$.\n  \\STATE {\\bf Output:} converged centroids $C$.\n  \\medskip\n  \\STATE $\\mathscr{T} \\gets$ a tree built on $S$\n  \\WHILE{centroids $C$ not converged}\n    \\STATE \\COMMENT{Remove nodes in the tree if possible.}\n    \\STATE $\\mathscr{T} \\gets \\mathtt{CoalesceNodes(}\\mathscr{T}\\mathtt{)}$\n    \\STATE $\\mathscr{T}_c \\gets$ a tree built on $C$\n    \\medskip\n    \\STATE \\COMMENT{Call dual-tree algorithm.}\n    \\STATE Perform a dual-tree recursion with $\\mathscr{T}$, $\\mathscr{T}_c$,\n\\texttt{BaseCase()}, and \\texttt{Score()}.\n    \\medskip\n    \\STATE \\COMMENT{Restore the tree to its non-coalesced form.}\n    \\STATE $\\mathscr{T} \\gets \\mathtt{DecoalesceNodes(\\mathscr{T})}$\n    \\medskip\n    \\STATE \\COMMENT{Update centroids and bounding information.}\n    \\STATE $C \\gets \\mathtt{UpdateCentroids(}\\mathscr{T}\\mathtt{)}$\n    \\STATE $\\mathscr{T} \\gets \\mathtt{UpdateTree(}\\mathscr{T}\\mathtt{)}$\n  \\ENDWHILE\n  \\STATE {\\bf return} $C$\n\\end{algorithmic}\n\\caption{High-level outline of dual-tree $k$-means.}\n\\label{alg:high_level}\n\\end{algorithm}\n\nFirst, consider the dual-tree algorithm called on line\n9.  As detailed earlier, we can describe a dual-tree\nalgorithm as a combination of tree type, traversal, and point-to-point\n\\texttt{BaseCase()} and node-to-node \\texttt{Score()} functions.  Thus, we\nneed only present \\texttt{BaseCase()} (Algorithm \\ref{alg:base_case}) and\n\\texttt{Score()} (Algorithm \\ref{alg:score})\\footnote{In these algorithms, we\nassume that any point present in a node ${\\mathscr{N}}_i$ will also be present in at least\none child ${\\mathscr{N}}_c \\in \\mathscr{C}_i$.  It is possible to fully\ngeneralize to any tree type, but the exposition is significantly more complex,\nand our assumption covers most standard tree types anyway.}.\n\nThe \\texttt{BaseCase()} function is simple: given a point $p_q$ and a\ncentroid $c_r$, the distance $d(p_q, c_r)$ is calculated; ${\\operatorname{ub}}(p_q)$,\n${\\operatorname{lb}}(p_q)$, and ${\\operatorname{closest}}(p_q)$ are updated if needed.\n\n\\texttt{Score()} is more complex.  The first stanza (lines 4--6) takes the\nvalues of ${\\operatorname{pruned}}(\\cdot)$ and ${\\operatorname{lb}}(\\cdot)$ from the parent node of ${\\mathscr{N}}_q$; this\nis necessary to prevent ${\\operatorname{pruned}}(\\cdot)$ from undercounting.  Next, we prune if\nthe owner of ${\\mathscr{N}}_q$ is already\nknown (line 7).  If the minimum distance between any descendant point of ${\\mathscr{N}}_q$\nand any descendant centroid of ${\\mathscr{N}}_r$ is greater than ${\\operatorname{ub}}({\\mathscr{N}}_q)$,\nthen we may prune the combination (line 16).  In that case we may also improve\nthe lower bound (line 14).  Note the special handling in line 15: our definition\nof tree allows points to be held in more than one node; thus, we must avoid\ndouble-counting clusters that we prune.\\footnote{For trees like the $kd$-tree\nand the metric tree, which do not hold points in more than one node, no special\nhandling is required: we will never prune a cluster twice for a given query node\n${\\mathscr{N}}_q$.}.  If the node combination cannot be pruned in this way, an attempt is\nmade to update the upper bound (lines 17--20).  Instead of using $d_{\\max}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$, we may use a tighter upper bound: select any\ndescendant centroid $c$ from ${\\mathscr{N}}_r$ and use $d_{\\max}({\\mathscr{N}}_q, c)$.  This still\nprovides a valid upper bound, and in practice is generally smaller than\n$d_{\\max}({\\mathscr{N}}_q, {\\mathscr{N}}_r)$.  We simply set ${\\operatorname{closest}}({\\mathscr{N}}_q)$ to $c$ (line 20);\n${\\operatorname{closest}}({\\mathscr{N}}_q)$ only holds the owner of ${\\mathscr{N}}_q$ if all centroids\nexcept one are pruned---in which case the owner {\\it must} be $c$.\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} query point $p_q$, reference centroid $c_r$\n  \\STATE {\\bf Output:} distance between $p_q$ and $c_r$\n  \\medskip\n  \\IF{$d(p_q, c_r) < {\\operatorname{ub}}(p_q)$}\n    \\STATE ${\\operatorname{lb}}(p_q) \\gets {\\operatorname{ub}}(p_q)$\n    \\STATE ${\\operatorname{ub}}(p_q) \\gets d(p_q, c_r)$\n    \\STATE ${\\operatorname{closest}}(p_q) \\gets c_r$\n  \\ELSIF{$d(p_q, c_r) < {\\operatorname{lb}}(p_q)$}\n    \\STATE ${\\operatorname{lb}}(p_q) \\gets d(p_q, c_r)$\n  \\ENDIF\n  \\medskip\n  \\STATE {\\bf return} $d(p_q, c_r)$\n\\end{algorithmic}\n\\caption{\\texttt{BaseCase()} for dual-tree $k$-means.}\n\\label{alg:base_case}\n\\end{algorithm}\n\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} query node ${\\mathscr{N}}_q$, reference node ${\\mathscr{N}}_r$\n  \\STATE {\\bf Output:} score for node combination $({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$, or $\\infty$ if the combination can be pruned\n  \\medskip\n  \\STATE \\COMMENT{Update the number of pruned nodes, if needed.}\n  \\IF{${\\mathscr{N}}_q$ not yet visited and is not the root node}\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_q) \\gets\n{\\operatorname{parent}}({\\mathscr{N}}_q)$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_q) \\gets\n{\\operatorname{lb}}({\\operatorname{parent}}({\\mathscr{N}}_q))$\n  \\ENDIF\n  \\STATE{{\\bf if} ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$ {\\bf then return} $\\infty$}\n  \\medskip\n  \\STATE $s \\gets d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r)$\n  \\STATE $c \\gets \\mathrm{any\\ descendant\\ cluster\\ centroid\\ of } {\\mathscr{N}}_r$\n  \\IF{$d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r) >\n{\\operatorname{ub}}({\\mathscr{N}}_q)$}\n    \\STATE \\COMMENT{This cluster node owns no descendant points.}\n    \\IF{$d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r) <\n{\\operatorname{lb}}({\\mathscr{N}}_q)$}\n      \\STATE \\COMMENT{Improve the lower bound for pruned nodes.}\n      \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_q) \\gets d_{\\min}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$\n    \\ENDIF\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_q) \\mathrel{+}= |\\mathscr{D}^p_r \\setminus \\{ \\textrm{clusters\nnot pruned} \\}|$\n    \\STATE $s \\gets \\infty$\n  \\medskip\n  \\ELSIF{$d_{\\max}({\\mathscr{N}}_q, c) <\n{\\operatorname{ub}}({\\mathscr{N}}_q)$}\n    \\STATE \\COMMENT{We may improve the upper bound.}\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_q) \\gets d_{\\max}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$\n    \\STATE ${\\operatorname{closest}}({\\mathscr{N}}_q) \\gets c$\n  \\ENDIF\n  \\medskip\n  \\STATE \\COMMENT{Check if all clusters (except one) are pruned.}\n  \\STATE {\\bf if} ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$ {\\bf then return} $\\infty$\n  \\medskip\n  \\STATE {\\bf return} $s$\n\\end{algorithmic}\n\\caption{\\texttt{Score()} for dual-tree $k$-means.}\n\\label{alg:score}\n\\end{algorithm}\n\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} tree $\\mathscr{T}$ built on dataset $S$\n  \\STATE {\\bf Output:} new centroids $C$\n  \\medskip\n  \\STATE $C := \\{ c_0, \\ldots, c_{k - 1} \\} \\gets \\bm{0}^{k \\times d}$; \\ $n =\n\\bm{0}^k$\n  \\medskip\n  \\STATE \\COMMENT{$s$ is a stack.}\n  \\STATE $s \\gets \\{ \\operatorname{root}(\\mathscr{T}) \\}$\n  \\WHILE{$|s| > 0$}\n    \\STATE ${\\mathscr{N}}_i \\gets s\\mathtt{.pop()}$\n    \\IF{${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$}\n      \\STATE \\COMMENT{The node is entirely owned by a cluster.}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n      \\STATE $c_j \\gets c_j + |\\mathscr{D}^p_i|\n\\operatorname{centroid}({\\mathscr{N}}_i)$\n      \\STATE $n_j \\gets n_j + |\\mathscr{D}^p_i|$\n    \\ELSE\n      \\STATE \\COMMENT{The node is not entirely owned by a cluster.}\n      \\STATE {{\\bf if} $|\\mathscr{C}_i| > 0$ {\\bf then}\n$s\\mathtt{.push(}\\mathscr{C}_i\\mathtt{)}$}\n      \\STATE {\\bf else}\n        \\STATE {\\ \\ \\ \\ {\\bf for} $p_i \\in \\mathscr{P}_i$ not yet considered}\n          \\STATE \\ \\ \\ \\ \\ \\ \\ $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n          \\STATE \\ \\ \\ \\ \\ \\ \\ $c_j \\gets c_j + p_i$; \\ \\ $n_j \\gets n_j + 1$\n    \\ENDIF\n  \\ENDWHILE\n  \\medskip\n  \\STATE{{\\bf for} $c_i \\in C${\\bf, if} $n_i > 0$ {\\bf then} $c_i \\gets c_i /\nn_i$}\n  \\STATE {\\bf return} $C$\n\\end{algorithmic}\n\\caption{\\texttt{UpdateCentroids()}.}\n\\label{alg:update_centroids}\n\\end{algorithm}\n\nThus, at the end of the dual-tree algorithm, we know the owner of every node (if\nit exists) via ${\\operatorname{closest}}(\\cdot)$ and ${\\operatorname{pruned}}(\\cdot)$, and we know the owner of\nevery point via ${\\operatorname{closest}}(\\cdot)$.  A simple\nalgorithm to do this is given here as Algorithm \\ref{alg:update_centroids}\n(\\texttt{UpdateCentroids()}); it\nis a depth-first recursion through the tree that terminates a branch when a node\nis owned by a single cluster.\n\n\n\n\n\n\n\n\n\nNext is updating the bounds in the tree and determining if nodes and\npoints can change owners next iteration; this work is encapsulated in the\n\\texttt{UpdateTree()} algorithm, which is an implementation of strategies 3 and\n4 (see the appendix for details).  Once\n\\texttt{UpdateTree()} sets the correct value of ${\\operatorname{canchange}}(\\cdot)$ for every\npoint and node, we coalesce the tree for the next iteration with the\n\\texttt{CoalesceTree()} function.  Coalescing the tree is straightforward:\nwe simply remove any nodes from the tree where ${\\operatorname{canchange}}(\\cdot)$\nis \\texttt{false}.  This leaves a smaller tree with no nodes where\n${\\operatorname{canchange}}(\\cdot)$ is \\texttt{false}.\n\n\n\n\nDecoalescing the tree (\\texttt{DecoalesceTree()}) is done by restoring\nthe tree to its original state.  See the appendix for more details.\n\n\\vspace*{-0.7em}\n\\section{Theoretical results}\n\\label{sec:theory}\n\\vspace*{-0.4em}\n\nSpace constraints allow us to only provide proof sketches for the first two\ntheorems here.  Detailed proofs are given in the appendix.\n\n\\begin{thm}\nA single iteration of dual-tree $k$-means as given in Algorithm\n\\ref{alg:high_level} will produce exactly the same results as the\nbrute-force $O(kN)$ implementation.\n\\end{thm}\n\\vspace*{-1.7em}\n\\begin{proof}\n(Sketch.)  First, we show that the dual-tree algorithm (line 9) produces correct\nresults for ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$\nfor every point and node.  Next, we show that \\texttt{UpdateTree()} maintains\nthe correctness of those four quantities and only marks ${\\operatorname{canchange}}(\\cdot)$ to\n\\texttt{false} when the node or point truly cannot change owner.  Next, it is\neasily shown that \\texttt{CoalesceTree()} and \\texttt{DecoalesceTree()} do not\naffect the results of the dual-tree algorithm because the only nodes and points\nremoved are those where ${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$.  Lastly, we show\nthat \\texttt{UpdateCentroids()} produces centroids correctly.\n\\end{proof}\n\\vspace*{-1.0em}\n\nNext, we consider the runtime of the algorithm.  Our results are with respect to\nthe {\\it expansion constant} $c_k$ of the centroids \\cite{langford2006}, which\nis a measure of intrinsic dimension.  $c_{qk}$ is a related quantity:\nthe largest expansion constant of $C$ plus any point in the dataset.  Our\nresults also depend on the imbalance of the tree $i_t(\\mathscr{T})$, which in\npractice generally scales linearly in $N$ \\cite{curtin2015plug}.  As with the\nother theoretical results, more detail on each of these quantities is available\nin the appendix.\n\n\\begin{thm}\nWhen cover trees are used, a single iteration of dual-tree $k$-means as in\nAlgorithm \\ref{alg:high_level} can be performed in $O(c_k^4 c_{qk}^5 (N +\ni_t(\\mathscr{T})) + c_k^9 k \\log k)$ time.\n\\end{thm}\n\\vspace*{-1.0em}\n\\begin{proof}\n(Sketch.)  Cover trees have $O(N)$ nodes \\cite{langford2006}; because\n\\texttt{CoalesceTree()}, \\texttt{DecoalesceTree()}, \\texttt{UpdateCentroids()},\nand \\texttt{UpdateTree()} can be performed in one pass of the tree, these steps\nmay each be completed in $O(N)$ time.  Building a tree on the centroids takes\n$O(c_k^6 k \\log k)$ time, where $c_k$ is the expansion constant of the\ncentroids.  Recent results show that dual-tree algorithms that use the cover\ntree may have their runtime easily bounded \\cite{curtin2015plug}.  We may\nobserve that our pruning rules are at least as tight as nearest neighbor search;\nthis means that the dual-tree algorithm (line 11) may be performed in\n$O(c_{kr}^9 (N + i_t(\\mathscr{T})))$ time.  Also, we must perform nearest\nneighbor search on the centroids, which costs $O(c_k^9 (k +\ni_t(\\mathscr{T_c})))$ time.  This gives a total per-iteration runtime of\n$O(c_{kr}^9 (N + i_t(\\mathscr{T})) + c_k^6 k \\log k + c_k^9\ni_t(\\mathscr{T}_k))$.\n\\end{proof}\n\n\\vspace*{-1.0em}\n\n\n\n\n\nThis result holds intuitively.  By building a tree on the centroids, we are able\nto prune many centroids at once, and as a result the amortized cost of finding\nthe nearest centroid to a point is $O(1)$.  This meshes with earlier theoretical\nresults \\cite{langford2006, curtin2015plug, ram2009} and earlier empirical\nresults \\cite{gray2003nonparametric, gray2001nbody} that suggest that an answer\ncan be obtained for a single query point in $O(1)$ time.  Note that this\nworst-case bound depends on the intrinsic dimension (the expansion constant)\nof the centroids, $c_k$, and the related quantity $c_{qk}$.  If the intrinsic\ndimension of the centroids is low---that is, if the centroids are distributed\nfavorably---the dual-tree algorithm will be more efficient.\n\nHowever, this bound is generally quite loose in practice.  First, runtime bounds\nfor cover trees are known to be loose \\cite{curtin2015plug}.  Second, this\nparticular bound does not consider the effect of coalescing the tree.\nIn any given iteration, especially toward the end of the $k$-means\nclustering, most points will have $\\operatorname{canchange}(\\cdot) =\n\\mathtt{false}$ and thus the coalesced tree\nwill be far smaller than the full tree built on all $N$ points.\n\n\\begin{thm}\nAlgorithm \\ref{alg:high_level} uses no more than $O(N + k)$ memory when cover\ntrees are used.\n\\end{thm}\n\\vspace*{-1.0em}\n\\begin{proof}\nThis proof is straightforward.  A cover tree on $N$ points takes $O(N)$\nspace.  So the trees and associated bounds take $O(N)$ and $O(k)$ space.  Also,\nthe dataset and centroids take $O(N)$ and $O(k)$ space.\n\\end{proof}\n\n\\vspace*{-1.3em}\n\\section{Experiments}\n\\label{sec:empirical}\n\\vspace*{-0.3em}\n\n\\setlength{\\textfloatsep}{1.2em}\n\\begin{table}[t!]\n{\\small\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n & & & \\multicolumn{2}{|c|}{\\bf tree build time} \\\\\n{\\bf Dataset} & $N$ & $d$ & $kd$-tree & cover tree \\\\\n\\hline\ncloud & 2048 & 10 & 0.001s & 0.005s \\\\\ncup98b & 95413 & 56 & 1.640s & 32.41s \\\\\nbirch3 & 100000 & 2 & 0.037s & 2.125s \\\\\nphy & 150000 & 78 & 4.138s & 22.99s \\\\\n\npower & 2075259 & 7 & 7.342s & 1388s \\\\\nlcdm & 6000000 & 3 & 4.345s & 6214s \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n}\n\\vspace*{-1.0em}\n\\caption{Dataset information.}\n\\label{tab:datasets}\n\\end{table}\n\n\\begin{table*}\n\\begin{center}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|r|l|l|l|l|l|l|}\n\\hline\n & & & \\multicolumn{6}{|c|}{\\bf avg. per-iteration runtime (distance\ncalculations)} \\\\\n{\\bf dataset} & $k$ & {\\bf iter.} & {\\tt elkan}                 & {\\tt hamerly}           & {\\tt yinyang}               & {\\tt blacklist}             & {\\tt dualtree-kd}           & {\\tt dualtree-ct}\n\\\\\n\\hline\ncloud         & 3   & 8           & 1.50e-4s (867)              & 1.11e-4s (1.01k)        & 1.11e-1s (2.00k)             & {\\bf 4.68e-5s} (302)        & 1.27e-4s ({\\bf 278})        & 2.77e-4s (443)    \\\\\ncloud         & 10  & 14          & 2.09e-4s ({\\bf 1.52k})      & 1.92e-4s (4.32k)        & 7.66e-2s (9.55k)             & {\\bf 1.55e-4s} (2.02k)      & 3.69e-4s (1.72k)            & 5.36e-4s (2.90k)  \\\\\ncloud         & 50  & 19          & 5.87e-4s ({\\bf 2.57k})      & {\\bf 5.30e-4s} (21.8k)  & 9.66e-3s (15.6k)            & 8.20e-4s (12.6k)            & 1.23e-3s (5.02k)            & 1.09e-3s (9.84k)  \\\\\n\\hline\ncup98b        & 50  & 224         & 0.0445s ({\\bf 25.9k})       & 0.0557s (962k)          & 0.0465s (313k)              & {\\bf 0.0409s} (277k)        & 0.0955s (254k)              & 0.1089s (436k) \\\\\ncup98b        & 250 & 168         & 0.1972s ({\\bf 96.8k})       & 0.4448s (8.40M)         & {\\bf 0.1417s} (898k)        & 0.2033s (1.36M)             & 0.4585s (1.38M)             & 0.3237s (2.73M)   \\\\\ncup98b        & 750 & 116         & 1.1719s ({\\bf 373k})        & 1.8778s (36.2M)         & {\\bf 0.2653s} (1.26M)       & 0.6365s (4.11M)             & 1.2847s (4.16M)             & 0.8056s (81.4M)   \\\\\n\\hline\nbirch3        & 50  & 129         & 0.0194s ({\\bf 24.2k})       & 0.0093s (566k)          & 0.0378s (399k)              & {\\bf 0.0030s} (42.7k)        & 0.0082s (37.4k)             & 0.0378s (67.9k) \\\\\nbirch3        & 250 & 812         & 0.0895s ({\\bf 42.8k})       & 0.0314s (2.59M)         & 0.0711s (239k)              & {\\bf 0.0164s} (165k)         & 0.0183s (79.7k)             & 0.0485s (140k)    \\\\\nbirch3        & 750 & 373         & 0.3253s (292k)              & 0.0972s (8.58M)         & 0.1423s (476k)              & 0.0554s (450k)               & {\\bf 0.02989s} ({\\bf 126k}) & 0.0581s (235k)    \\\\\n\\hline\nphy           & 50  & 34          & 0.0668s (82.3k)             & 0.1064s (1.38M)         & 0.1072s (808k)              & {\\bf 0.0081s} ({\\bf 33.0k})  & 0.02689s (67.8k)            & 0.0945s (188k)    \\\\\nphy           & 250 & 38          & 0.1627s (121k)              & 0.4634s (6.83M)         & 0.2469s (2.39M)             & {\\bf 0.0249s} (104k)         & 0.0398s ({\\bf 90.4k})       & 0.1023s (168k)    \\\\\nphy           & 750 & 35          & 0.7760s ({\\bf 410k})        & 2.9192s (43.8M)         & 0.6418s (5.61M)             & {\\bf 0.2478s} (1.19M)        & 0.2939s (1.10M)             & 0.3330s (1.84M)   \\\\\n\\hline\n\n\n\n\n\n\n\n\n\n\npower         & 25  & 4           & 0.3872s (2.98M)             & 0.2880s (12.9M)         & 1.1257s (33.5M)             & {\\bf 0.0301s} (216k)         & 0.0950s ({\\bf 87.4k})       & 0.6658s (179k)    \\\\\npower         & 250 & 101         & 2.6532s (425k)              & 0.1868s (7.83M)         & 1.2684s (10.3M)             & 0.1504s (1.13M)              & {\\bf 0.1354s} ({\\bf 192k})  & 0.6405s (263k)    \\\\\npower         & 1000& 870         & {\\it out of memory}         & 6.2407s (389M)          & 4.4261s (9.41M)             & 0.6657s (2.98M)              & {\\bf 0.4115s} ({\\bf 1.57M}) & 1.1799s (4.81M) \\\\\npower         & 5000& 504         & {\\it out of memory}         & 29.816s (1.87B)         & 22.7550s (58.6M)            & 4.1597s (11.7M)              & {\\bf 1.0580s} ({\\bf 3.85M}) & 1.7070s (12.3M)   \\\\\npower        & 15000& 301         & {\\it out of memory}         & 111.74s (6.99B)         & {\\it out of memory}         & {\\it out of memory}          & {\\bf 2.3708s} ({\\bf 8.65M}) & 2.9472s (30.9M)   \\\\\n\\hline\nlcdm          & 500 & 507         & {\\it out of memory}         & 6.4084s (536M)          & 8.8926s (44.5M)             & 0.9347s (4.20M)              & {\\bf 0.7574s} ({\\bf 3.68M}) & 2.9428s (7.03M) \\\\\nlcdm          & 1000& 537         & {\\it out of memory}         & 16.071s (1.31B)         & 18.004s (74.7M)             & 2.0345s (5.93M)              & {\\bf 0.9827s} ({\\bf 5.11M}) & 3.3482s (10.0M)   \\\\\nlcdm          & 5000& 218         & {\\it out of memory}         & 64.895s (5.38B)         & {\\it out of memory}         & 12.909s (16.2M)              & {\\bf 1.8972s} ({\\bf 8.54M}) & 3.9110s (19.0M)   \\\\\nlcdm          &20000& 108         & {\\it out of memory}         & 298.55s (24.7B)         & {\\it out of memory}         & {\\it out of memory}          & {\\bf 4.1911s} ({\\bf 17.8M}) & 5.5771s (43.2M)   \\\\\n\\hline\n\n\\end{tabular}\n}\n\\end{center}\n\\vspace*{-1.0em}\n\\caption{Empirical results for $k$-means.}\n\\label{tab:runtime}\n\\vspace*{-1.0em}\n\\end{table*}\n\nThe next thing to consider is the empirical performance of the algorithm.  We\nuse the publicly available \\texttt{kmeans} program in {\\bf mlpack}\n\\cite{mlpack2013}; in our experiments, we run it as follows:\n\n\\vspace*{-0.5em}\n\\begin{verbatim}\n$ kmeans -i dataset.csv -I centroids.csv -c\n    $k -v -e -a $algorithm\n\\end{verbatim}\n\\vspace*{-0.5em}\n\n\\noindent where \\texttt{\\$k} is the number of clusters and \\texttt{\\$algorithm}\nis the algorithm to be used.  Each algorithm is implemented in C++.  For the\n{\\tt yinyang} algorithm, we use the authors' implementation.  We use a variety\nof $k$ values on mostly real-world datasets; details are shown in Table\n\\ref{tab:datasets} \\cite{uci, birch3, lcdm}.  The table also contains the time\ntaken to build a $kd$-tree (for \\texttt{blacklist} and \\texttt{dualtree-kd}) and\na cover tree (for \\texttt{dualtree-ct}).  Cover trees are far more complex to\nbuild than $kd$-trees; this explains the long cover tree build time.  Even so,\nthe tree only needs to be built once during the $k$-means run.  If results are\nrequired for multiple values of $k$---such as in the X-means algorithm\n\\cite{pelleg2000x}---then the tree built on the points may be re-used.\n\nClusters were initialized using the Bradley-Fayyad refined start procedure\n\\yrcite{bradley1998refining}; however, this was too slow for the very large\ndatasets, so in those cases points were randomly sampled as the initial\ncentroids.  $k$-means was then run until convergence on each dataset.  These\nsimulations were performed on a modest consumer desktop with an\nIntel i5 with 16GB RAM, using {\\bf mlpack}'s benchmarking system\n\\cite{edel2014automatic}.\n\nAverage runtime per iteration results are shown in Table \\ref{tab:runtime}.\nThe amount of work that is being pruned away is somewhat unclear from the\nruntime results, because the \\texttt{elkan} and \\texttt{hamerly} algorithms\naccess points linearly and thus benefit from cache effects; this is not true of\nthe tree-based algorithms.  Therefore, the average number of distance\ncalculations per iteration are also included in the results.\n\nIt is immediately clear that for large datasets, \\texttt{dualtree-kd} is\nfastest, and \\texttt{dualtree-ct} is almost as fast.\nThe \\texttt{elkan} algorithm, because it\nholds $kN$ bounds, is able to prune away a huge amount of work and is very fast\nfor small datasets; however,\nmaintaining all of these bounds becomes prohibitive with large $k$ and the\nalgorithm exhausts all\navailable memory.  The \\texttt{blacklist} algorithm has the same issue: on the\nlargest datasets, with the largest $k$ values, the space required to maintain\nall the blacklists is too much.  This is also true of the \\texttt{yinyang}\nalgorithm, which must maintain bounds between each point and each group of\ncentroids.  For large $k$, this burden becomes too much and the algorithm fails.\nThe \\texttt{hamerly} and dual-tree algorithms, on the other hand, are the\nbest-behaved with memory usage and do not have any issues with large $N$ or\nlarge $k$; however, the \\texttt{hamerly} algorithm is very slow on large\ndatasets because it is not able to prune many points at once.\n\nSimilar to the observations about the \\texttt{blacklist} algorithm, the\ntree-based approaches are less effective in higher dimensions\n\\cite{pelleg1999accelerating}.  This is an important point: the performance of\ntree-based approaches suffer in high dimensions in part because the bound \n$d_{\\min}(\\cdot, \\cdot)$ generally becomes looser as dimension increases.\nThis is partly because the volume of nodes in high dimensions is much higher;\nconsider that a ball has volume that is exponential in the dimension.\n\nEven so, in our results, we see speedup in reasonable dimensions (for example,\nthe {\\tt phy} dataset has 78 dimensions).  Further, because our algorithm is\ntree-independent, we may use tree structures that are tailored to\nhigh-dimensional data \\cite{arya1998optimal}---including ones that\nhave not yet been developed.  From our results we believe\nas a rule of thumb that the dual-tree $k$-means algorithm can be effective up to\na hundred dimensions or more.\n\nAnother clear observation is that when $k$ is scaled on a single dataset, the\n\\texttt{dualtree-kd} and \\texttt{dualtree-ct} algorithms nearly always scale\nbetter (in terms of runtime) than the other algorithms.  These results show that\nour algorithm satisfies its original goals: to be able to scale effectively to\nlarge $k$ and $N$.\n\n\\vspace*{-0.6em}\n\\section{Conclusion and future directions}\n\\label{sec:conclusion}\n\\vspace*{-0.2em}\n\nUsing four pruning strategies, we have developed a flexible,\ntree-independent dual-tree $k$-means algorithm that is the best-performing\nalgorithm for large datasets and large $k$ in small-to-medium dimensions.  It\nis theoretically favorable, has a small memory footprint, and may be used in\nconjunction with initial point selection and approximation schemes for\n additional speedup.\n\nThere are still interesting future directions to pursue, though.  The first\ndirection is parallelism: because our dual-tree algorithm is agnostic to the\ntype of traversal used, we may use a parallel traversal \\cite{curtin2013tree},\nsuch as an adapted version of a recent parallel dual-tree algorithm\n\\cite{lee2012distributed}.  The second direction is kernel $k$-means and other\nspectral clustering techniques: our algorithm may be merged with the\nideas of \\citet{curtin2014dual} to perform kernel $k$-means.  The third\ndirection is theoretical.  Recently, more general notions of intrinsic\ndimensionality have been proposed \\cite{houle2013dimensionality,\namsaleg2015estimating}; these may enable tighter and more descriptive runtime\nbounds.  Our work thus provides a useful and fast $k$-means algorithm and also\nopens promising avenues to further accelerated clustering algorithms.\n\n\\nocite{ram2009rank}\n\\nocite{march2010euclidean}\n\n\\bibliographystyle{icml2016}\n\\bibliography{kmeans}\n\n\\appendix\n\n\\section{Supplementary material}\n\nUnfortunately, space constraints prevent adequate explanation of each of the\npoints in the main paper.  This supplementary material is meant to clarify all\nof the parts of the dual-tree $k$-means algorithm that space did not permit in\nthe main paper.\n\n\\subsection{Updating the tree}\n\nIn addition to updating the centroids, the bounding information contained within\nthe tree must be updated according to pruning strategies 3 and 4.\nUnfortunately, this yields a particularly complex recursive algorithm, given in\nAlgorithm \\ref{alg:update_tree}.\n\n\\begin{algorithm*}\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} node ${\\mathscr{N}}_i$, ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$,\n${\\operatorname{pruned}}(\\cdot)$, ${\\operatorname{closest}}(\\cdot)$, ${\\operatorname{canchange}}(\\cdot)$, centroid movements $m$\n  \\STATE {\\bf Output:} updated ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$,\n${\\operatorname{canchange}}(\\cdot)$\n  \\medskip\n  \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{true}$\n  \\IF{${\\mathscr{N}}_i$ has a parent and ${\\operatorname{canchange}}({\\operatorname{parent}}({\\mathscr{N}}_i)) = \\mathtt{false}$}\n    \\STATE \\COMMENT{Use the parent's bounds.}\n    \\STATE ${\\operatorname{closest}}({\\mathscr{N}}_i) \\gets {\\operatorname{closest}}({\\operatorname{parent}}({\\mathscr{N}}_i))$\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets {\\operatorname{lb}}({\\mathscr{N}}_i) + \\max_i m_i$\n    \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n  \\ELSIF{${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$}\n    \\STATE \\COMMENT{${\\mathscr{N}}_i$ is owned by a single cluster.  Can that owner change\nnext iteration?}\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets \\max \\left({\\operatorname{lb}}({\\mathscr{N}}_i) - \\max_i m_i, \\min_{k \\ne j}\nd(c_k, c_j) / 2 \\right)$\n    \\IF{${\\operatorname{ub}}({\\mathscr{N}}_i) < {\\operatorname{lb}}({\\mathscr{N}}_i)$}\n      \\STATE \\COMMENT{The owner cannot change next iteration.}\n      \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n    \\ELSE\n      \\STATE \\COMMENT{Tighten the upper bound and try to prune again.}\n      \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets \\min \\left({\\operatorname{ub}}({\\mathscr{N}}_i), d_{\\max}({\\mathscr{N}}_i, c_j)\\right)$\n      \\STATE {\\bf if} ${\\operatorname{ub}}({\\mathscr{N}}_i) < {\\operatorname{lb}}({\\mathscr{N}}_i)$ {\\bf then} ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets\n\\mathtt{false}$\n    \\ENDIF\n  \\ELSE\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets {\\operatorname{lb}}({\\mathscr{N}}_i) - \\max_k m_k$\n  \\ENDIF\n  \\STATE \\COMMENT{Recurse into each child.}\n  \\STATE{{\\bf for each} child ${\\mathscr{N}}_c$ of ${\\mathscr{N}}_i${\\bf , call}\n\\texttt{UpdateTree(${\\mathscr{N}}_c$)}}\n  \\STATE \\COMMENT{Try to determine points whose owner cannot change if ${\\mathscr{N}}_i$\ncan change owners.}\n  \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{true}$}\n    \\FOR{$p_i \\in \\mathscr{P}_i$}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n      \\STATE ${\\operatorname{ub}}(p_i) \\gets {\\operatorname{ub}}(p_i) + m_j$\n      \\STATE ${\\operatorname{lb}}(p_i) \\gets \\min\\left( {\\operatorname{lb}}(p_i) - \\max_k m_k, \\min_{k \\ne j}\nd(c_k, c_j) / 2 \\right)$\n      \\IF{${\\operatorname{ub}}(p_i) < {\\operatorname{lb}}(p_i)$}\n        \\STATE ${\\operatorname{canchange}}(p_i) \\gets \\mathtt{false}$\n      \\ELSE\n        \\STATE \\COMMENT{Tighten the upper bound and try again.}\n        \\STATE ${\\operatorname{ub}}(p_i) \\gets \\min\\left( {\\operatorname{ub}}(p_i), d(p_i, c_j) \\right)$\n        \\IF{${\\operatorname{ub}}(p_i) < {\\operatorname{lb}}(p_i)$} \n          \\STATE ${\\operatorname{canchange}}(p_i) \\gets \\mathtt{false}$\n        \\ELSE\n          \\STATE \\COMMENT{Point cannot be pruned.}\n          \\STATE ${\\operatorname{ub}}(p_i) \\gets \\infty$\n          \\STATE ${\\operatorname{lb}}(p_i) \\gets \\infty$\n        \\ENDIF\n      \\ENDIF\n    \\ENDFOR\n  \\ELSE\n    \\FOR{$p_i \\in \\mathscr{P}_i$ where ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$}\n      \\STATE \\COMMENT{Maintain upper and lower bounds for points whose owner\ncannot change.}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n      \\STATE ${\\operatorname{ub}}(p_i) \\gets {\\operatorname{ub}}(p_i) + m_j$\n      \\STATE ${\\operatorname{lb}}(p_i) \\gets {\\operatorname{lb}}(p_i) - \\max_k m_k$\n    \\ENDFOR\n  \\ENDIF\n  \\IF{${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$ for all children ${\\mathscr{N}}_c$ of\n${\\mathscr{N}}_i$ and all points $p_i \\in \\mathscr{P}_i$}\n    \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n  \\ENDIF\n  \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{true}$}\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_i) \\gets 0$\n  \\ENDIF\n\\end{algorithmic}\n\\caption{\\texttt{UpdateTree()} for dual-tree $k$-means.}\n\\label{alg:update_tree}\n\\end{algorithm*}\n\nThe first if statement (lines 4--10) catches the case where the parent cannot\nchange owner next iteration; in this case, the parent's upper bound and lower\nbound can be taken as valid bounds.  In addition, the upper and lower bounds are\nadjusted to account for cluster movement between iterations, so that the bounds\nare valid for next iteration.\n\nIf the node $\\mathscr{N}_i$ has an owner, the algorithm then attempts to use the\npruning rules established in Equations 4 and 6 in the main paper,\nto determine if the owner of $\\mathscr{N}_i$ can change next iteration.  If not,\n${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is set to \\texttt{false} (line 18).  On the other hand, if\nthe pruning check fails, the upper bound is tightened and the pruning check is\nperformed a second time.  It is worth noting that $d_{\\max}(\\mathscr{N}_i, c_j)$\nmay not actually be less than the current value of ${\\operatorname{ub}}({\\mathscr{N}}_i)$, which is why the\n$\\min$ is necessary.\n\nAfter recursing into the children of ${\\mathscr{N}}_i$, if ${\\mathscr{N}}_i$ could have an owner\nchange, each point is individually checked using the same approach (lines\n31--45).  However, there is a slight difference: if a point's owner can change,\nthe upper and lower bounds must be set to $\\infty$ (lines 44--45).  This is only\nnecessary with points; \\texttt{BaseCase()} does not take bounding information\nfrom previous iterations into account, because no work can be avoided in that\nway.\n\nThen, we may set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} if every point in ${\\mathscr{N}}_i$\nand every child of ${\\mathscr{N}}_i$ cannot change owners (and the points and nodes do not\nnecessarily have to have the same owner).  Otherwise, we must set\n${\\operatorname{pruned}}({\\mathscr{N}}_i)$ to $0$ for the next iteration.\n\n\\subsection{Coalescing the tree}\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} tree $\\mathscr{T}$\n  \\STATE {\\bf Output:} coalesced tree $\\mathscr{T}$\n  \\medskip\n  \\STATE \\COMMENT{A depth-first recursion to hide nodes where\n${\\operatorname{canchange}}(\\cdot)$ is \\texttt{false}.}\n  \\STATE $s \\gets \\{ \\operatorname{root}(\\mathscr{T}) \\}$\n  \\WHILE{$|s| > 0$}\n    \\STATE ${\\mathscr{N}}_i \\gets s\\mathtt{.pop()}$\n    \\medskip\n    \\STATE \\COMMENT{Special handling is required for leaf nodes and the root\nnode.}\n    \\IF{$|\\mathscr{C}_i| = 0$}\n      \\STATE {\\bf continue}\n    \\ELSIF{${\\mathscr{N}}_i$ is the root node}\n      \\FOR{${\\mathscr{N}}_c \\in \\mathscr{C}_i$}\n        \\STATE $s\\mathtt{.push(}{\\mathscr{N}}_c\\mathtt{)}$\n      \\ENDFOR\n    \\ENDIF\n    \\medskip\n    \\STATE \\COMMENT{See if children can be removed.}\n    \\FOR{${\\mathscr{N}}_c \\in \\mathscr{C}_i$}\n      \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_c) = \\mathtt{false}$}\n        \\STATE remove child ${\\mathscr{N}}_c$\n      \\ELSE\n        \\STATE $s\\mathtt{.push(}{\\mathscr{N}}_c\\mathtt{)}$\n      \\ENDIF\n    \\ENDFOR\n    \\medskip\n    \\STATE \\COMMENT{If only one child is left, then this node is unnecessary.}\n    \\IF{$|\\mathscr{C}_i| = 1$}\n      \\STATE add child to ${\\operatorname{parent}}({\\mathscr{N}}_i)$\n      \\STATE remove ${\\mathscr{N}}_i$ from ${\\operatorname{parent}}({\\mathscr{N}}_i)$'s children\n    \\ENDIF\n  \\ENDWHILE\n  \\medskip\n  \\STATE {\\bf return} $\\mathscr{T}$\n\\end{algorithmic}\n\\caption{\\texttt{CoalesceTree()} for dual-tree $k$-means.}\n\\label{alg:coalesce}\n\\end{algorithm}\n\nAfter \\texttt{UpdateTree()} is called, the tree must be coalesced to remove any\nnodes where ${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$.  This can be accomplished via\na single pass over the tree.  A simple implementation is given in Algorithm\n\\ref{alg:coalesce}.  \\texttt{DecoalesceTree()} may be implemented by\nsimply restoring a pristine copy of the tree which was cached right before\n\\texttt{CoalesceTree()} is called.\n\n\\subsection{Correctness proof}\n\nAs mentioned in the main document, a correctness proof is possible but\ndifficult.  We will individually prove the correctness of various pieces of the\ndual-tree $k$-means algorithm, and then we will prove the main correctness\nresult.  For precision, we must introduce the exact definition of a space tree\nand a pruning dual-tree traversal, as given by Curtin et~al.\n\\cite{curtin2013tree}.\n\n\\begin{defn}\nA \\textbf{space tree} on a dataset $S \\in \\Re^{N \\times D}$ is an undirected,\nconnected, acyclic, rooted simple graph with the following properties:\n\n\\vspace*{-0.5em}\n\\begin{itemize}\n  \\item Each \\textit{node} (or vertex), holds a number of points (possibly\nzero) and is connected to one parent node and a number of child nodes (possibly\nzero).\n  \\item There is one node in every space tree with no parent; this\nis the \\textit{root node} of the tree.\n  \\item Each point in $S$ is contained in at least one node.\n  \\item Each node $\\mathscr{N}$ has a convex subset of\n$\\Re^{D}$ containing each point in that node and also the convex\nsubsets represented by each child of the node.\n\\end{itemize}\n\\end{defn}\n\n\\begin{defn}\n\\label{def:dtpt}\nA {\\it pruning dual-tree traversal} is a process that, given two space trees\n$\\mathscr{T}_q$ (the query tree, built on the query set $S_q$) and\n$\\mathscr{T}_r$ (the reference tree, built on the reference set $S_r$), will\nvisit combinations of nodes $(\\mathscr{N}_q, \\mathscr{N}_r)$ such that\n$\\mathscr{N}_q \\in \\mathscr{T}_q$ and $\\mathscr{N}_r \\in \\mathscr{T}_r$ no more\nthan once, and call a function \\texttt{Score($\\mathscr{N}_q$, $\\mathscr{N}_r$)}\nto assign a score to that node.  If the score is $\\infty$ (or above some bound),\nthe combination is pruned and no combinations ($\\mathscr{N}_{qc}$,\n$\\mathscr{N}_{rc}$) such that $\\mathscr{N}_{qc} \\in \\mathscr{D}^n_q$ and\n$\\mathscr{N}_{rc} \\in \\mathscr{D}^n_r$ are visited.  Otherwise, for every\ncombination of points ($p_q$, $p_r$) such that $p_q \\in \\mathscr{P}_q$ and $p_r\n\\in \\mathscr{P}_r$, a function \\texttt{BaseCase($p_q$, $p_r$)} is called.  If no\nnode combinations are pruned during the traversal, \\texttt{BaseCase($p_q$,\n$p_r$)} is called at least once on each combination of $p_q \\in S_q$ and $p_r\n\\in S_r$.\n\\end{defn}\n\nFor more description and clarity on these definitions, refer to\n\\cite{curtin2013tree}.\n\n\\begin{lemma}\nA pruning dual-tree traversal which uses \\texttt{BaseCase()} as given in\nAlgorithm 2 in the main paper and \\texttt{Score()} as given in Algorithm\n3 in the main paper which starts with valid ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$,\n${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$ for each node\n${\\mathscr{N}}_i \\in \\mathscr{T}$, and ${\\operatorname{ub}}(p_q) = {\\operatorname{lb}}(p_q) = \\infty$ for each point $p_q\n\\in S$, will satisfy the following conditions upon completion:\n\n\\vspace*{-0.4em}\n\\begin{itemize}\n  \\item For every $p_q \\in S$ that is a descendant of a node ${\\mathscr{N}}_i$ that has\nbeen pruned (${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$), ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is an upper bound on the\ndistance between $p_q$ and its closest centroid, and ${\\operatorname{closest}}({\\mathscr{N}}_i)$ is the\nowner of $p_q$.\n\n  \\item For every $p_q \\in S$ that is not a descendant of any node that has been\npruned, ${\\operatorname{ub}}(p_q)$ is an upper bound on the distance between $p_q$ and its\nclosest centroid, and ${\\operatorname{closest}}(p_q)$ is the owner of $p_q$.\n\n  \\item For every $p_q \\in S$ that is a descendant of a node ${\\mathscr{N}}_i$ that has\nbeen pruned (${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$), ${\\operatorname{lb}}({\\mathscr{N}}_i)$ is a lower bound on the\ndistance between $p_q$ and its second closest centroid.\n\n  \\item For every $p_q \\in S$ that is not a descendant of any node that has been\npruned, $\\min({\\operatorname{lb}}(p_q), {\\operatorname{lb}}({\\mathscr{N}}_q))$ where ${\\mathscr{N}}_q$ is a node such that $p_q \\in\n\\mathscr{P}_q$ is a lower bound on the distance between $p_q$ and its second\nclosest centroid.\n\\end{itemize}\n\\label{lem:dt_correct}\n\\vspace*{-0.4em}\n\\end{lemma}\n\n\\begin{proof}\nIt is easiest to consider each condition individually.  Thus, we will first\nconsider the upper bound on the distance to the closest cluster centroid.\nConsider some $p_q$ and suppose that the closest cluster centroid to $p_q$ is\n$c^*$.\n\nNow, suppose first that the point $p_q$ is a descendant point of a node ${\\mathscr{N}}_q$\nthat has been pruned.  We must show, then, that $c^*$ is ${\\operatorname{closest}}({\\mathscr{N}}_q)$.  Take\n$R = \\{ {\\mathscr{N}}_{r0}, {\\mathscr{N}}_{r1}, \\ldots, {\\mathscr{N}}_{rj} \\}$ to be the set of reference nodes\nvisited during the traversal with ${\\mathscr{N}}_q$ as a query node; that is, the\ncombinations $({\\mathscr{N}}_q, {\\mathscr{N}}_{ri})$ were visited for all ${\\mathscr{N}}_{ri} \\in R$.  Any\n${\\mathscr{N}}_{ri}$ is pruned only if\n\n\\vspace*{-0.8em}\n\n", "itemtype": "equation", "pos": 25213, "prevtext": "\n\\vspace*{-1.3em}\n\n\\noindent then $c_j$ will own $p_q$ next iteration \\cite{elkan2003using}.  We\nmay adapt this rule to tree nodes ${\\mathscr{N}}_q$ in the same way as the previous rule;\nif ${\\mathscr{N}}_q$ is owned by cluster $c_j$ during this iteration and\n\n\\vspace*{-1.3em}\n\n", "index": 11, "text": "\\begin{equation}\n{\\operatorname{ub}}({\\mathscr{N}}_q) + m_j < 2 \\left( \\min_{c_i \\in C, c_i \\ne c_j}\nd(c_i, c_j) \\right)\n\\label{eqn:static-2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{ub}}({\\mathscr{N}}_{q})+m_{j}&lt;2\\left(\\min_{c_{i}\\in C,c_{i}\\neq&#10;c%&#10;_{j}}d(c_{i},c_{j})\\right)\" display=\"block\"><mrow><mrow><mrow><mo>ub</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msub><mi>m</mi><mi>j</mi></msub></mrow><mo>&lt;</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><munder><mi>min</mi><mrow><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2208</mo><mi>C</mi></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2260</mo><msub><mi>c</mi><mi>j</mi></msub></mrow></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-0.8em}\n\n\\noindent according to line 10 of \\texttt{Score()}.  Thus, as\nlong as ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is a valid upper bound on the closest cluster distance for\nevery descendant point in ${\\mathscr{N}}_q$, then no nodes are incorrectly pruned.  It is\neasy to see that the upper bound is valid: initially, it is valid by assumption;\neach time the bound is updated with some node ${\\mathscr{N}}_{ri}$ (on lines\n19 and 20), it is set to\n$d_{\\max}({\\mathscr{N}}_i, c)$ where $c$ is some descendant centroid of ${\\mathscr{N}}_{ri}$.  This is\nclearly a valid upper bound, since $c$ cannot be any closer to any descendant\npoint of ${\\mathscr{N}}_i$ than $c^*$.  We may thus conclude that no node is incorrectly\npruned from $R$; we may apply this reasoning recursively to the ${\\mathscr{N}}_q$'s\nancestors to see that no reference node is incorrectly pruned.\n\nWhen a node is pruned from $R$, the number of pruned clusters for ${\\mathscr{N}}_q$ is\nupdated: the count of all clusters not previously pruned by ${\\mathscr{N}}_q$ (or its\nancestors) is added.  We cannot double-count the pruning of a cluster; thus the\nonly way that ${\\operatorname{pruned}}({\\mathscr{N}}_q)$ can be equal to $k - 1$ is if every centroid\nexcept one is pruned.  The centroid which is not pruned will be the nearest\ncentroid $c^*$, regardless of if ${\\operatorname{closest}}({\\mathscr{N}}_q)$ was set during this traversal\nor still holds its initial value, and therefore it must be true that ${\\operatorname{ub}}({\\mathscr{N}}_q)$\nis an upper bound on the distance between $p_q$ and $c^*$, and ${\\operatorname{closest}}({\\mathscr{N}}_q) =\nc^*$.\n\nThis allows us to finally conclude that if $p_q$ is a descendant of a node\n${\\mathscr{N}}_q$ that has been pruned, then ${\\operatorname{ub}}({\\mathscr{N}}_q)$ contains a valid upper bound on\nthe distance between $p_q$ and its closest cluster centroid, and\n${\\operatorname{closest}}({\\mathscr{N}}_q)$ is that closest cluster centroid.\n\nNow, consider the other case, where $p_q$ is not a descendant of any node that\nhas been pruned.  Take ${\\mathscr{N}}_i$ to be any node containing $p_q$\\footnote{Note that\nthe meaning here is not that $p_q$ is a descendant of ${\\mathscr{N}}_i$ ($p_i \\in\n\\mathscr{D}^p_i$), but instead that $p_q$ is held directly in ${\\mathscr{N}}_i$: $p_q \\in\n\\mathscr{P}_i$.}.  We have already reasoned that any cluster centroid node that\ncould possibly contain the closest cluster centroid to $p_q$ cannot have been\npruned; therefore, by the definition of pruning dual-tree traversal, we are\nguaranteed that \\texttt{BaseCase()} will be called with $p_q$ as the query point\nand the closest cluster centroid as the reference point.  This will then cause\n${\\operatorname{ub}}(p_q)$ to hold the distance to the closest cluster centroid---assuming\n${\\operatorname{ub}}(p_q)$ is always valid, which it is even at the beginning of the traversal\nbecause it is initialized to $\\infty$---and ${\\operatorname{closest}}(p_q)$ to hold the closest\ncluster centroid.\n\nTherefore, the first two conditions are proven.  The third and fourth\nconditions, for the lower bounds, require a slightly different strategy.\n\nThere are two ways ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is modified: first, at line\n14, when a node combination is pruned, and second, at\nline 6 when the lower bound is taken from the parent.\nAgain, consider the set $R = \\{ {\\mathscr{N}}_{r0}, {\\mathscr{N}}_{r1}, \\ldots, {\\mathscr{N}}_{rj} \\}$ which is\nthe set of reference nodes visited during the traversal with ${\\mathscr{N}}_q$ as a query\nnode.  Call the set of reference nodes that were pruned $R^p$.  At the end of\nthe traversal, then,\n\n\\vspace*{-1.3em}\n\\begin{eqnarray}\n{\\operatorname{lb}}({\\mathscr{N}}_q) &\\le& \\min_{{\\mathscr{N}}_{ri} \\in R^p} d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_{ri}) \\\\\n  &\\le& \\min_{c_k \\in C^p} d_{\\min}({\\mathscr{N}}_q, c_k)\n\\end{eqnarray}\n\\vspace*{-1.3em}\n\n\\noindent where $C^p$ is the set of centroids that are descendants of nodes in\n$R^p$.  Applying this reasoning recursively to the ancestors of ${\\mathscr{N}}_q$ shows\nthat at the end of the dual-tree traversal, ${\\operatorname{lb}}({\\mathscr{N}}_q)$ will contain a lower\nbound on the distance between any descendant point of ${\\mathscr{N}}_q$ and any pruned\ncentroid.  Thus, if ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, then ${\\operatorname{lb}}({\\mathscr{N}}_q)$ will contain a\nlower bound on the distance between any descendant point in ${\\mathscr{N}}_q$ and its\nsecond closest centroid.  So if we consider some point $p_q$ which is a\ndescendant of ${\\mathscr{N}}_q$ and ${\\mathscr{N}}_q$ is pruned (${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$), then\n${\\operatorname{lb}}({\\mathscr{N}}_q)$ is indeed a lower bound on the distance between $p_q$ and its second\nclosest centroid.\n\nNow, consider the case where $p_q$ is not a descendant of any node that has been\npruned, and take ${\\mathscr{N}}_q$ to be some node that owns $p_q$ (that is, $p_q \\in\n\\mathscr{P}_q$).  In this case, \\texttt{BaseCase()} will be called with every\ncentroid that has not been pruned.  So ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is a lower bound on the\ndistance between $p_q$ and every pruned centroid, and ${\\operatorname{lb}}(p_q)$ will be a lower\nbound on the distance between $p_q$ and the second-closest non-pruned centroid,\ndue to the structure of the \\texttt{BaseCase()} function.  Therefore,\n$\\min({\\operatorname{lb}}(p_q), {\\operatorname{lb}}({\\mathscr{N}}_q))$ must be a lower bound on the distance between $p_q$\nand its second closest centroid.\n\nFinally, we may conclude that each item in the theorem holds.\n\\end{proof}\n\nNext, we must prove that \\texttt{UpdateTree()} functions correctly.\n\n\\begin{lemma}\nIn the context of Algorithm 1 in the main paper, given a\ntree $\\mathscr{T}$ with all associated bounds ${\\operatorname{ub}}(\\cdot)$ and ${\\operatorname{lb}}(\\cdot)$ and\ninformation ${\\operatorname{pruned}}(\\cdot)$, ${\\operatorname{closest}}(\\cdot)$, and ${\\operatorname{canchange}}(\\cdot)$, a run\nof \\texttt{UpdateTree()} as given in Algorithm \\ref{alg:update_tree} will have\nthe following effects:\n\n\\begin{itemize}\n  \\item For every node ${\\mathscr{N}}_i$, ${\\operatorname{ub}}({\\mathscr{N}}_i)$ will be a valid upper bound on the\ndistance between any descendant point of ${\\mathscr{N}}_i$ and its nearest centroid next\niteration.\n\n  \\item For every node ${\\mathscr{N}}_i$, ${\\operatorname{lb}}({\\mathscr{N}}_i)$ will be a valid lower bound on the\ndistance between any descendant point of ${\\mathscr{N}}_i$ and any pruned centroid next\niteration.\n\n  \\item A node ${\\mathscr{N}}_i$ will only have ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ if the\nowner of any descendant point of ${\\mathscr{N}}_i$ cannot change next iteration.\n\n  \\item A point $p_i$ will only have ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$ if the\nowner of $p_i$ cannot change next iteration.\n\n  \\item Any point $p_i$ with ${\\operatorname{canchange}}(p_i) = \\mathtt{true}$ that does not\nbelong to any node ${\\mathscr{N}}_i$ with ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ will have\n${\\operatorname{ub}}(p_i) = {\\operatorname{lb}}(p_i) = \\infty$, as required by the dual-tree traversal.\n\n  \\item Any node ${\\mathscr{N}}_i$ with ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ at the end of\n\\texttt{UpdateTree()} will have ${\\operatorname{pruned}}({\\mathscr{N}}_i) = 0$.\n\\end{itemize}\n\\vspace*{-0.6em}\n\\label{lem:update_correct}\n\\end{lemma}\n\n\\begin{proof}\nEach point is best considered individually.  It is important to remember during\nthis proof that the centroids have been updated, but the bounds have not.  So\nany cluster centroid $c_i$ is already set for next iteration.  Take $c^l_i$ to\nmean the cluster centroid $c_i$ {\\it before} adjustment (that is, the old\ncentroid).  Also take ${\\operatorname{ub}}^l(\\cdot)$, ${\\operatorname{lb}}^l(\\cdot)$, ${\\operatorname{pruned}}^l(\\cdot)$, and\n${\\operatorname{canchange}}^l(\\cdot)$ to be the values at the time \\texttt{UpdateTree()} is\ncalled, before any of those values are changed.  Due to the assumptions in the\nstatement of the lemma, each of these quantities is valid.\n\nSuppose that for some node ${\\mathscr{N}}_i$, ${\\operatorname{closest}}({\\mathscr{N}}_i)$ is some cluster $c_j$.  For\n${\\operatorname{ub}}({\\mathscr{N}}_i)$ to be valid for next iteration, we must guarantee that ${\\operatorname{ub}}({\\mathscr{N}}_i)\n\\ge \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c_j)$ at the end of\n\\texttt{UpdateTree()}.  There are four ways ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is updated: it may be\ntaken from the parent and adjusted (line 8), it may be adjusted before a prune\nattempt (line 14), it may be tightened after a failed prune attempt (line 21),\nor it may be adjusted without a prune attempt (line 25).  If we can show that\neach of these four ways always results in ${\\operatorname{ub}}({\\mathscr{N}}_i)$ being valid, then the\nfirst condition of the theorem holds.\n\nIf ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is adjusted in line 14 or 25, the resulting value of ${\\operatorname{ub}}({\\mathscr{N}}_i)$,\nassuming ${\\operatorname{closest}}({\\mathscr{N}}_i) = c_j$, is\n\n\\vspace*{-1em}\n\\begin{eqnarray}\n{\\operatorname{ub}}({\\mathscr{N}}_i) &=& {\\operatorname{ub}}^l({\\mathscr{N}}_i) + m_j \\\\\n  &\\ge& \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c^l_j) + m_j \\\\\n  &\\ge& \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c_j)\n\\end{eqnarray}\n\\vspace*{-1em}\n\n\\noindent where the last step follows by the triangle inequality: $d(c_j, c^l_j)\n= m_j$.  Therefore those two updates to ${\\operatorname{ub}}({\\mathscr{N}}_i)$ result in valid upper bounds\nfor next iteration.  If ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is recalculated, in line\n21, then we are guaranteed that ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is valid because\n\n\\vspace*{-1em}\n\n", "itemtype": "equation", "pos": 69444, "prevtext": "\n\\vspace*{-1.3em}\n\n\\noindent then ${\\mathscr{N}}_q$ is owned by cluster $c_j$ in the next iteration.\nNote that the above rules do work with individual points $p_q$ instead of nodes\n${\\mathscr{N}}_q$ if we have a valid upper bound ${\\operatorname{ub}}(p_q)$ and a\nvalid lower bound ${\\operatorname{lb}}(p_q)$.  Any nodes or points that satisfy\nthe above conditions do not need to be visited during the next iteration, and\ncan be removed from the tree for the next iteration.\n\n{\\bf Strategy four.} The traversal should use bounding information from previous\niterations; for instance, ${\\operatorname{ub}}({\\mathscr{N}}_q)$ should not be reset\nto $\\infty$ at the beginning of each iteration.  Between iterations, we may\nupdate ${\\operatorname{ub}}({\\mathscr{N}}_q)$, ${\\operatorname{ub}}(p_q)$,\n${\\operatorname{lb}}({\\mathscr{N}}_q)$, and ${\\operatorname{lb}}(p_q)$ according to\nthe following rules:\n\n\\vspace*{-1.5em}\n\\begin{eqnarray}\n{\\operatorname{ub}}({\\mathscr{N}}_q) &\\gets&\n  \\begin{cases}\n    {\\operatorname{ub}}({\\mathscr{N}}_q) + m_j & \\text{if } {\\mathscr{N}}_q \\text{ is}\\\\\n\\multicolumn{2}{l}{\\text{\\ \\ \\ \\ owned by a single cluster $c_j$}}\n\\\\\n    {\\operatorname{ub}}({\\mathscr{N}}_q) + \\max_i m_i & \\text{if } {\\mathscr{N}}_q \\text{ is}\\\\\n\\multicolumn{2}{l}{\\text{\\ \\ \\ \\ not owned by a single cluster},}\n  \\end{cases} \\label{eqn:special} \\\\\n{\\operatorname{ub}}(p_q) &\\gets& {\\operatorname{ub}}(p_q) + m_j, \\\\\n{\\operatorname{lb}}({\\mathscr{N}}_q) &\\gets& {\\operatorname{lb}}({\\mathscr{N}}_q) -\n\\max_i m_i, \\\\\n{\\operatorname{lb}}(p_q) &\\gets& {\\operatorname{lb}}(p_q) - \\max_i m_i.\n\\end{eqnarray}\n\\vspace*{-1.0em}\n\nSpecial handling is required when descendant points of ${\\mathscr{N}}_q$\nare not owned by a single centroid (Equation \\ref{eqn:special}).  It is also\ntrue that for a child node ${\\mathscr{N}}_c$ of ${\\mathscr{N}}_q$, ${\\operatorname{ub}}({\\mathscr{N}}_q)$ is a valid upper bound\nfor ${\\mathscr{N}}_c$ and ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is a valid lower bound for ${\\mathscr{N}}_c$: that is, the upper\nand lower bounds may be taken from a parent, and they are still valid.\n\n\\vspace*{-0.6em}\n\\section{The dual-tree $k$-means algorithm}\n\\label{sec:algorithm}\n\\vspace*{-0.3em}\n\nThese four pruning strategies lead to a high-level $k$-means algorithm,\ndescribed in Algorithm \\ref{alg:high_level}.  During the course of this\nalgorithm, to implement each of our pruning strategies, we will need to maintain\nthe following quantities:\n\n\\vspace*{-1.0em}\n\\begin{itemize} \\itemsep -1.5pt\n  \\item ${\\operatorname{ub}}({\\mathscr{N}}_q)$: an upper bound on the distance\nbetween any descendant point of a node ${\\mathscr{N}}_q$ and the nearest centroid\nto that point.\n  \\item ${\\operatorname{lb}}({\\mathscr{N}}_q)$: a lower bound on the distance\nbetween any descendant point of a node ${\\mathscr{N}}_q$ and the nearest pruned\ncentroid.\n  \\item ${\\operatorname{pruned}}({\\mathscr{N}}_q)$: the number of centroids pruned\nduring traversal for ${\\mathscr{N}}_q$.\n  \\item ${\\operatorname{closest}}({\\mathscr{N}}_q)$: if ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, this\nholds the owner of all descendant points of ${\\mathscr{N}}_q$.\n  \\item ${\\operatorname{canchange}}({\\mathscr{N}}_q)$: whether or not\n${\\mathscr{N}}_q$ can change owners next iteration.\n  \\item ${\\operatorname{ub}}(p_q)$: an upper bound on the distance between point\n$p_q$ and its nearest centroid.\n  \\item ${\\operatorname{lb}}(p_q)$: a lower bound on the distance between point\n$p_q$ and its second nearest centroid.\n  \\item ${\\operatorname{closest}}(p_q)$: the closest centroid to $p_q$ (this is\nalso the owner of $p_q$).\n  \\item ${\\operatorname{canchange}}(p_q)$: whether or not $p_q$ can change owners\nnext iteration.\n\\end{itemize}\n\\vspace*{-0.8em}\n\nAt the beginning of the algorithm, each upper bound is initialized to $\\infty$,\neach lower bound is initialized to $\\infty$, ${\\operatorname{pruned}}(\\cdot)$\nis initialized to $0$ for each node, and\n${\\operatorname{closest}}(\\cdot)$ is initialized to an invalid centroid for each\nnode and point.  ${\\operatorname{canchange}}(\\cdot)$ is set to {\\tt\ntrue} for each node and point.  Thus line\n6 does nothing on the first iteration.\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} dataset $S \\in \\mathcal{R}^{N \\times d}$, initial\ncentroids $C \\in \\mathcal{R}^{k \\times d}$.\n  \\STATE {\\bf Output:} converged centroids $C$.\n  \\medskip\n  \\STATE $\\mathscr{T} \\gets$ a tree built on $S$\n  \\WHILE{centroids $C$ not converged}\n    \\STATE \\COMMENT{Remove nodes in the tree if possible.}\n    \\STATE $\\mathscr{T} \\gets \\mathtt{CoalesceNodes(}\\mathscr{T}\\mathtt{)}$\n    \\STATE $\\mathscr{T}_c \\gets$ a tree built on $C$\n    \\medskip\n    \\STATE \\COMMENT{Call dual-tree algorithm.}\n    \\STATE Perform a dual-tree recursion with $\\mathscr{T}$, $\\mathscr{T}_c$,\n\\texttt{BaseCase()}, and \\texttt{Score()}.\n    \\medskip\n    \\STATE \\COMMENT{Restore the tree to its non-coalesced form.}\n    \\STATE $\\mathscr{T} \\gets \\mathtt{DecoalesceNodes(\\mathscr{T})}$\n    \\medskip\n    \\STATE \\COMMENT{Update centroids and bounding information.}\n    \\STATE $C \\gets \\mathtt{UpdateCentroids(}\\mathscr{T}\\mathtt{)}$\n    \\STATE $\\mathscr{T} \\gets \\mathtt{UpdateTree(}\\mathscr{T}\\mathtt{)}$\n  \\ENDWHILE\n  \\STATE {\\bf return} $C$\n\\end{algorithmic}\n\\caption{High-level outline of dual-tree $k$-means.}\n\\label{alg:high_level}\n\\end{algorithm}\n\nFirst, consider the dual-tree algorithm called on line\n9.  As detailed earlier, we can describe a dual-tree\nalgorithm as a combination of tree type, traversal, and point-to-point\n\\texttt{BaseCase()} and node-to-node \\texttt{Score()} functions.  Thus, we\nneed only present \\texttt{BaseCase()} (Algorithm \\ref{alg:base_case}) and\n\\texttt{Score()} (Algorithm \\ref{alg:score})\\footnote{In these algorithms, we\nassume that any point present in a node ${\\mathscr{N}}_i$ will also be present in at least\none child ${\\mathscr{N}}_c \\in \\mathscr{C}_i$.  It is possible to fully\ngeneralize to any tree type, but the exposition is significantly more complex,\nand our assumption covers most standard tree types anyway.}.\n\nThe \\texttt{BaseCase()} function is simple: given a point $p_q$ and a\ncentroid $c_r$, the distance $d(p_q, c_r)$ is calculated; ${\\operatorname{ub}}(p_q)$,\n${\\operatorname{lb}}(p_q)$, and ${\\operatorname{closest}}(p_q)$ are updated if needed.\n\n\\texttt{Score()} is more complex.  The first stanza (lines 4--6) takes the\nvalues of ${\\operatorname{pruned}}(\\cdot)$ and ${\\operatorname{lb}}(\\cdot)$ from the parent node of ${\\mathscr{N}}_q$; this\nis necessary to prevent ${\\operatorname{pruned}}(\\cdot)$ from undercounting.  Next, we prune if\nthe owner of ${\\mathscr{N}}_q$ is already\nknown (line 7).  If the minimum distance between any descendant point of ${\\mathscr{N}}_q$\nand any descendant centroid of ${\\mathscr{N}}_r$ is greater than ${\\operatorname{ub}}({\\mathscr{N}}_q)$,\nthen we may prune the combination (line 16).  In that case we may also improve\nthe lower bound (line 14).  Note the special handling in line 15: our definition\nof tree allows points to be held in more than one node; thus, we must avoid\ndouble-counting clusters that we prune.\\footnote{For trees like the $kd$-tree\nand the metric tree, which do not hold points in more than one node, no special\nhandling is required: we will never prune a cluster twice for a given query node\n${\\mathscr{N}}_q$.}.  If the node combination cannot be pruned in this way, an attempt is\nmade to update the upper bound (lines 17--20).  Instead of using $d_{\\max}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$, we may use a tighter upper bound: select any\ndescendant centroid $c$ from ${\\mathscr{N}}_r$ and use $d_{\\max}({\\mathscr{N}}_q, c)$.  This still\nprovides a valid upper bound, and in practice is generally smaller than\n$d_{\\max}({\\mathscr{N}}_q, {\\mathscr{N}}_r)$.  We simply set ${\\operatorname{closest}}({\\mathscr{N}}_q)$ to $c$ (line 20);\n${\\operatorname{closest}}({\\mathscr{N}}_q)$ only holds the owner of ${\\mathscr{N}}_q$ if all centroids\nexcept one are pruned---in which case the owner {\\it must} be $c$.\n\n\\setlength{\\textfloatsep}{0.4em}\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} query point $p_q$, reference centroid $c_r$\n  \\STATE {\\bf Output:} distance between $p_q$ and $c_r$\n  \\medskip\n  \\IF{$d(p_q, c_r) < {\\operatorname{ub}}(p_q)$}\n    \\STATE ${\\operatorname{lb}}(p_q) \\gets {\\operatorname{ub}}(p_q)$\n    \\STATE ${\\operatorname{ub}}(p_q) \\gets d(p_q, c_r)$\n    \\STATE ${\\operatorname{closest}}(p_q) \\gets c_r$\n  \\ELSIF{$d(p_q, c_r) < {\\operatorname{lb}}(p_q)$}\n    \\STATE ${\\operatorname{lb}}(p_q) \\gets d(p_q, c_r)$\n  \\ENDIF\n  \\medskip\n  \\STATE {\\bf return} $d(p_q, c_r)$\n\\end{algorithmic}\n\\caption{\\texttt{BaseCase()} for dual-tree $k$-means.}\n\\label{alg:base_case}\n\\end{algorithm}\n\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} query node ${\\mathscr{N}}_q$, reference node ${\\mathscr{N}}_r$\n  \\STATE {\\bf Output:} score for node combination $({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$, or $\\infty$ if the combination can be pruned\n  \\medskip\n  \\STATE \\COMMENT{Update the number of pruned nodes, if needed.}\n  \\IF{${\\mathscr{N}}_q$ not yet visited and is not the root node}\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_q) \\gets\n{\\operatorname{parent}}({\\mathscr{N}}_q)$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_q) \\gets\n{\\operatorname{lb}}({\\operatorname{parent}}({\\mathscr{N}}_q))$\n  \\ENDIF\n  \\STATE{{\\bf if} ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$ {\\bf then return} $\\infty$}\n  \\medskip\n  \\STATE $s \\gets d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r)$\n  \\STATE $c \\gets \\mathrm{any\\ descendant\\ cluster\\ centroid\\ of } {\\mathscr{N}}_r$\n  \\IF{$d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r) >\n{\\operatorname{ub}}({\\mathscr{N}}_q)$}\n    \\STATE \\COMMENT{This cluster node owns no descendant points.}\n    \\IF{$d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_r) <\n{\\operatorname{lb}}({\\mathscr{N}}_q)$}\n      \\STATE \\COMMENT{Improve the lower bound for pruned nodes.}\n      \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_q) \\gets d_{\\min}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$\n    \\ENDIF\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_q) \\mathrel{+}= |\\mathscr{D}^p_r \\setminus \\{ \\textrm{clusters\nnot pruned} \\}|$\n    \\STATE $s \\gets \\infty$\n  \\medskip\n  \\ELSIF{$d_{\\max}({\\mathscr{N}}_q, c) <\n{\\operatorname{ub}}({\\mathscr{N}}_q)$}\n    \\STATE \\COMMENT{We may improve the upper bound.}\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_q) \\gets d_{\\max}({\\mathscr{N}}_q,\n{\\mathscr{N}}_r)$\n    \\STATE ${\\operatorname{closest}}({\\mathscr{N}}_q) \\gets c$\n  \\ENDIF\n  \\medskip\n  \\STATE \\COMMENT{Check if all clusters (except one) are pruned.}\n  \\STATE {\\bf if} ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$ {\\bf then return} $\\infty$\n  \\medskip\n  \\STATE {\\bf return} $s$\n\\end{algorithmic}\n\\caption{\\texttt{Score()} for dual-tree $k$-means.}\n\\label{alg:score}\n\\end{algorithm}\n\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} tree $\\mathscr{T}$ built on dataset $S$\n  \\STATE {\\bf Output:} new centroids $C$\n  \\medskip\n  \\STATE $C := \\{ c_0, \\ldots, c_{k - 1} \\} \\gets \\bm{0}^{k \\times d}$; \\ $n =\n\\bm{0}^k$\n  \\medskip\n  \\STATE \\COMMENT{$s$ is a stack.}\n  \\STATE $s \\gets \\{ \\operatorname{root}(\\mathscr{T}) \\}$\n  \\WHILE{$|s| > 0$}\n    \\STATE ${\\mathscr{N}}_i \\gets s\\mathtt{.pop()}$\n    \\IF{${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$}\n      \\STATE \\COMMENT{The node is entirely owned by a cluster.}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n      \\STATE $c_j \\gets c_j + |\\mathscr{D}^p_i|\n\\operatorname{centroid}({\\mathscr{N}}_i)$\n      \\STATE $n_j \\gets n_j + |\\mathscr{D}^p_i|$\n    \\ELSE\n      \\STATE \\COMMENT{The node is not entirely owned by a cluster.}\n      \\STATE {{\\bf if} $|\\mathscr{C}_i| > 0$ {\\bf then}\n$s\\mathtt{.push(}\\mathscr{C}_i\\mathtt{)}$}\n      \\STATE {\\bf else}\n        \\STATE {\\ \\ \\ \\ {\\bf for} $p_i \\in \\mathscr{P}_i$ not yet considered}\n          \\STATE \\ \\ \\ \\ \\ \\ \\ $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n          \\STATE \\ \\ \\ \\ \\ \\ \\ $c_j \\gets c_j + p_i$; \\ \\ $n_j \\gets n_j + 1$\n    \\ENDIF\n  \\ENDWHILE\n  \\medskip\n  \\STATE{{\\bf for} $c_i \\in C${\\bf, if} $n_i > 0$ {\\bf then} $c_i \\gets c_i /\nn_i$}\n  \\STATE {\\bf return} $C$\n\\end{algorithmic}\n\\caption{\\texttt{UpdateCentroids()}.}\n\\label{alg:update_centroids}\n\\end{algorithm}\n\nThus, at the end of the dual-tree algorithm, we know the owner of every node (if\nit exists) via ${\\operatorname{closest}}(\\cdot)$ and ${\\operatorname{pruned}}(\\cdot)$, and we know the owner of\nevery point via ${\\operatorname{closest}}(\\cdot)$.  A simple\nalgorithm to do this is given here as Algorithm \\ref{alg:update_centroids}\n(\\texttt{UpdateCentroids()}); it\nis a depth-first recursion through the tree that terminates a branch when a node\nis owned by a single cluster.\n\n\n\n\n\n\n\n\n\nNext is updating the bounds in the tree and determining if nodes and\npoints can change owners next iteration; this work is encapsulated in the\n\\texttt{UpdateTree()} algorithm, which is an implementation of strategies 3 and\n4 (see the appendix for details).  Once\n\\texttt{UpdateTree()} sets the correct value of ${\\operatorname{canchange}}(\\cdot)$ for every\npoint and node, we coalesce the tree for the next iteration with the\n\\texttt{CoalesceTree()} function.  Coalescing the tree is straightforward:\nwe simply remove any nodes from the tree where ${\\operatorname{canchange}}(\\cdot)$\nis \\texttt{false}.  This leaves a smaller tree with no nodes where\n${\\operatorname{canchange}}(\\cdot)$ is \\texttt{false}.\n\n\n\n\nDecoalescing the tree (\\texttt{DecoalesceTree()}) is done by restoring\nthe tree to its original state.  See the appendix for more details.\n\n\\vspace*{-0.7em}\n\\section{Theoretical results}\n\\label{sec:theory}\n\\vspace*{-0.4em}\n\nSpace constraints allow us to only provide proof sketches for the first two\ntheorems here.  Detailed proofs are given in the appendix.\n\n\\begin{thm}\nA single iteration of dual-tree $k$-means as given in Algorithm\n\\ref{alg:high_level} will produce exactly the same results as the\nbrute-force $O(kN)$ implementation.\n\\end{thm}\n\\vspace*{-1.7em}\n\\begin{proof}\n(Sketch.)  First, we show that the dual-tree algorithm (line 9) produces correct\nresults for ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$\nfor every point and node.  Next, we show that \\texttt{UpdateTree()} maintains\nthe correctness of those four quantities and only marks ${\\operatorname{canchange}}(\\cdot)$ to\n\\texttt{false} when the node or point truly cannot change owner.  Next, it is\neasily shown that \\texttt{CoalesceTree()} and \\texttt{DecoalesceTree()} do not\naffect the results of the dual-tree algorithm because the only nodes and points\nremoved are those where ${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$.  Lastly, we show\nthat \\texttt{UpdateCentroids()} produces centroids correctly.\n\\end{proof}\n\\vspace*{-1.0em}\n\nNext, we consider the runtime of the algorithm.  Our results are with respect to\nthe {\\it expansion constant} $c_k$ of the centroids \\cite{langford2006}, which\nis a measure of intrinsic dimension.  $c_{qk}$ is a related quantity:\nthe largest expansion constant of $C$ plus any point in the dataset.  Our\nresults also depend on the imbalance of the tree $i_t(\\mathscr{T})$, which in\npractice generally scales linearly in $N$ \\cite{curtin2015plug}.  As with the\nother theoretical results, more detail on each of these quantities is available\nin the appendix.\n\n\\begin{thm}\nWhen cover trees are used, a single iteration of dual-tree $k$-means as in\nAlgorithm \\ref{alg:high_level} can be performed in $O(c_k^4 c_{qk}^5 (N +\ni_t(\\mathscr{T})) + c_k^9 k \\log k)$ time.\n\\end{thm}\n\\vspace*{-1.0em}\n\\begin{proof}\n(Sketch.)  Cover trees have $O(N)$ nodes \\cite{langford2006}; because\n\\texttt{CoalesceTree()}, \\texttt{DecoalesceTree()}, \\texttt{UpdateCentroids()},\nand \\texttt{UpdateTree()} can be performed in one pass of the tree, these steps\nmay each be completed in $O(N)$ time.  Building a tree on the centroids takes\n$O(c_k^6 k \\log k)$ time, where $c_k$ is the expansion constant of the\ncentroids.  Recent results show that dual-tree algorithms that use the cover\ntree may have their runtime easily bounded \\cite{curtin2015plug}.  We may\nobserve that our pruning rules are at least as tight as nearest neighbor search;\nthis means that the dual-tree algorithm (line 11) may be performed in\n$O(c_{kr}^9 (N + i_t(\\mathscr{T})))$ time.  Also, we must perform nearest\nneighbor search on the centroids, which costs $O(c_k^9 (k +\ni_t(\\mathscr{T_c})))$ time.  This gives a total per-iteration runtime of\n$O(c_{kr}^9 (N + i_t(\\mathscr{T})) + c_k^6 k \\log k + c_k^9\ni_t(\\mathscr{T}_k))$.\n\\end{proof}\n\n\\vspace*{-1.0em}\n\n\n\n\n\nThis result holds intuitively.  By building a tree on the centroids, we are able\nto prune many centroids at once, and as a result the amortized cost of finding\nthe nearest centroid to a point is $O(1)$.  This meshes with earlier theoretical\nresults \\cite{langford2006, curtin2015plug, ram2009} and earlier empirical\nresults \\cite{gray2003nonparametric, gray2001nbody} that suggest that an answer\ncan be obtained for a single query point in $O(1)$ time.  Note that this\nworst-case bound depends on the intrinsic dimension (the expansion constant)\nof the centroids, $c_k$, and the related quantity $c_{qk}$.  If the intrinsic\ndimension of the centroids is low---that is, if the centroids are distributed\nfavorably---the dual-tree algorithm will be more efficient.\n\nHowever, this bound is generally quite loose in practice.  First, runtime bounds\nfor cover trees are known to be loose \\cite{curtin2015plug}.  Second, this\nparticular bound does not consider the effect of coalescing the tree.\nIn any given iteration, especially toward the end of the $k$-means\nclustering, most points will have $\\operatorname{canchange}(\\cdot) =\n\\mathtt{false}$ and thus the coalesced tree\nwill be far smaller than the full tree built on all $N$ points.\n\n\\begin{thm}\nAlgorithm \\ref{alg:high_level} uses no more than $O(N + k)$ memory when cover\ntrees are used.\n\\end{thm}\n\\vspace*{-1.0em}\n\\begin{proof}\nThis proof is straightforward.  A cover tree on $N$ points takes $O(N)$\nspace.  So the trees and associated bounds take $O(N)$ and $O(k)$ space.  Also,\nthe dataset and centroids take $O(N)$ and $O(k)$ space.\n\\end{proof}\n\n\\vspace*{-1.3em}\n\\section{Experiments}\n\\label{sec:empirical}\n\\vspace*{-0.3em}\n\n\\setlength{\\textfloatsep}{1.2em}\n\\begin{table}[t!]\n{\\small\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n & & & \\multicolumn{2}{|c|}{\\bf tree build time} \\\\\n{\\bf Dataset} & $N$ & $d$ & $kd$-tree & cover tree \\\\\n\\hline\ncloud & 2048 & 10 & 0.001s & 0.005s \\\\\ncup98b & 95413 & 56 & 1.640s & 32.41s \\\\\nbirch3 & 100000 & 2 & 0.037s & 2.125s \\\\\nphy & 150000 & 78 & 4.138s & 22.99s \\\\\n\npower & 2075259 & 7 & 7.342s & 1388s \\\\\nlcdm & 6000000 & 3 & 4.345s & 6214s \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n}\n\\vspace*{-1.0em}\n\\caption{Dataset information.}\n\\label{tab:datasets}\n\\end{table}\n\n\\begin{table*}\n\\begin{center}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|r|l|l|l|l|l|l|}\n\\hline\n & & & \\multicolumn{6}{|c|}{\\bf avg. per-iteration runtime (distance\ncalculations)} \\\\\n{\\bf dataset} & $k$ & {\\bf iter.} & {\\tt elkan}                 & {\\tt hamerly}           & {\\tt yinyang}               & {\\tt blacklist}             & {\\tt dualtree-kd}           & {\\tt dualtree-ct}\n\\\\\n\\hline\ncloud         & 3   & 8           & 1.50e-4s (867)              & 1.11e-4s (1.01k)        & 1.11e-1s (2.00k)             & {\\bf 4.68e-5s} (302)        & 1.27e-4s ({\\bf 278})        & 2.77e-4s (443)    \\\\\ncloud         & 10  & 14          & 2.09e-4s ({\\bf 1.52k})      & 1.92e-4s (4.32k)        & 7.66e-2s (9.55k)             & {\\bf 1.55e-4s} (2.02k)      & 3.69e-4s (1.72k)            & 5.36e-4s (2.90k)  \\\\\ncloud         & 50  & 19          & 5.87e-4s ({\\bf 2.57k})      & {\\bf 5.30e-4s} (21.8k)  & 9.66e-3s (15.6k)            & 8.20e-4s (12.6k)            & 1.23e-3s (5.02k)            & 1.09e-3s (9.84k)  \\\\\n\\hline\ncup98b        & 50  & 224         & 0.0445s ({\\bf 25.9k})       & 0.0557s (962k)          & 0.0465s (313k)              & {\\bf 0.0409s} (277k)        & 0.0955s (254k)              & 0.1089s (436k) \\\\\ncup98b        & 250 & 168         & 0.1972s ({\\bf 96.8k})       & 0.4448s (8.40M)         & {\\bf 0.1417s} (898k)        & 0.2033s (1.36M)             & 0.4585s (1.38M)             & 0.3237s (2.73M)   \\\\\ncup98b        & 750 & 116         & 1.1719s ({\\bf 373k})        & 1.8778s (36.2M)         & {\\bf 0.2653s} (1.26M)       & 0.6365s (4.11M)             & 1.2847s (4.16M)             & 0.8056s (81.4M)   \\\\\n\\hline\nbirch3        & 50  & 129         & 0.0194s ({\\bf 24.2k})       & 0.0093s (566k)          & 0.0378s (399k)              & {\\bf 0.0030s} (42.7k)        & 0.0082s (37.4k)             & 0.0378s (67.9k) \\\\\nbirch3        & 250 & 812         & 0.0895s ({\\bf 42.8k})       & 0.0314s (2.59M)         & 0.0711s (239k)              & {\\bf 0.0164s} (165k)         & 0.0183s (79.7k)             & 0.0485s (140k)    \\\\\nbirch3        & 750 & 373         & 0.3253s (292k)              & 0.0972s (8.58M)         & 0.1423s (476k)              & 0.0554s (450k)               & {\\bf 0.02989s} ({\\bf 126k}) & 0.0581s (235k)    \\\\\n\\hline\nphy           & 50  & 34          & 0.0668s (82.3k)             & 0.1064s (1.38M)         & 0.1072s (808k)              & {\\bf 0.0081s} ({\\bf 33.0k})  & 0.02689s (67.8k)            & 0.0945s (188k)    \\\\\nphy           & 250 & 38          & 0.1627s (121k)              & 0.4634s (6.83M)         & 0.2469s (2.39M)             & {\\bf 0.0249s} (104k)         & 0.0398s ({\\bf 90.4k})       & 0.1023s (168k)    \\\\\nphy           & 750 & 35          & 0.7760s ({\\bf 410k})        & 2.9192s (43.8M)         & 0.6418s (5.61M)             & {\\bf 0.2478s} (1.19M)        & 0.2939s (1.10M)             & 0.3330s (1.84M)   \\\\\n\\hline\n\n\n\n\n\n\n\n\n\n\npower         & 25  & 4           & 0.3872s (2.98M)             & 0.2880s (12.9M)         & 1.1257s (33.5M)             & {\\bf 0.0301s} (216k)         & 0.0950s ({\\bf 87.4k})       & 0.6658s (179k)    \\\\\npower         & 250 & 101         & 2.6532s (425k)              & 0.1868s (7.83M)         & 1.2684s (10.3M)             & 0.1504s (1.13M)              & {\\bf 0.1354s} ({\\bf 192k})  & 0.6405s (263k)    \\\\\npower         & 1000& 870         & {\\it out of memory}         & 6.2407s (389M)          & 4.4261s (9.41M)             & 0.6657s (2.98M)              & {\\bf 0.4115s} ({\\bf 1.57M}) & 1.1799s (4.81M) \\\\\npower         & 5000& 504         & {\\it out of memory}         & 29.816s (1.87B)         & 22.7550s (58.6M)            & 4.1597s (11.7M)              & {\\bf 1.0580s} ({\\bf 3.85M}) & 1.7070s (12.3M)   \\\\\npower        & 15000& 301         & {\\it out of memory}         & 111.74s (6.99B)         & {\\it out of memory}         & {\\it out of memory}          & {\\bf 2.3708s} ({\\bf 8.65M}) & 2.9472s (30.9M)   \\\\\n\\hline\nlcdm          & 500 & 507         & {\\it out of memory}         & 6.4084s (536M)          & 8.8926s (44.5M)             & 0.9347s (4.20M)              & {\\bf 0.7574s} ({\\bf 3.68M}) & 2.9428s (7.03M) \\\\\nlcdm          & 1000& 537         & {\\it out of memory}         & 16.071s (1.31B)         & 18.004s (74.7M)             & 2.0345s (5.93M)              & {\\bf 0.9827s} ({\\bf 5.11M}) & 3.3482s (10.0M)   \\\\\nlcdm          & 5000& 218         & {\\it out of memory}         & 64.895s (5.38B)         & {\\it out of memory}         & 12.909s (16.2M)              & {\\bf 1.8972s} ({\\bf 8.54M}) & 3.9110s (19.0M)   \\\\\nlcdm          &20000& 108         & {\\it out of memory}         & 298.55s (24.7B)         & {\\it out of memory}         & {\\it out of memory}          & {\\bf 4.1911s} ({\\bf 17.8M}) & 5.5771s (43.2M)   \\\\\n\\hline\n\n\\end{tabular}\n}\n\\end{center}\n\\vspace*{-1.0em}\n\\caption{Empirical results for $k$-means.}\n\\label{tab:runtime}\n\\vspace*{-1.0em}\n\\end{table*}\n\nThe next thing to consider is the empirical performance of the algorithm.  We\nuse the publicly available \\texttt{kmeans} program in {\\bf mlpack}\n\\cite{mlpack2013}; in our experiments, we run it as follows:\n\n\\vspace*{-0.5em}\n\\begin{verbatim}\n$ kmeans -i dataset.csv -I centroids.csv -c\n    $k -v -e -a $algorithm\n\\end{verbatim}\n\\vspace*{-0.5em}\n\n\\noindent where \\texttt{\\$k} is the number of clusters and \\texttt{\\$algorithm}\nis the algorithm to be used.  Each algorithm is implemented in C++.  For the\n{\\tt yinyang} algorithm, we use the authors' implementation.  We use a variety\nof $k$ values on mostly real-world datasets; details are shown in Table\n\\ref{tab:datasets} \\cite{uci, birch3, lcdm}.  The table also contains the time\ntaken to build a $kd$-tree (for \\texttt{blacklist} and \\texttt{dualtree-kd}) and\na cover tree (for \\texttt{dualtree-ct}).  Cover trees are far more complex to\nbuild than $kd$-trees; this explains the long cover tree build time.  Even so,\nthe tree only needs to be built once during the $k$-means run.  If results are\nrequired for multiple values of $k$---such as in the X-means algorithm\n\\cite{pelleg2000x}---then the tree built on the points may be re-used.\n\nClusters were initialized using the Bradley-Fayyad refined start procedure\n\\yrcite{bradley1998refining}; however, this was too slow for the very large\ndatasets, so in those cases points were randomly sampled as the initial\ncentroids.  $k$-means was then run until convergence on each dataset.  These\nsimulations were performed on a modest consumer desktop with an\nIntel i5 with 16GB RAM, using {\\bf mlpack}'s benchmarking system\n\\cite{edel2014automatic}.\n\nAverage runtime per iteration results are shown in Table \\ref{tab:runtime}.\nThe amount of work that is being pruned away is somewhat unclear from the\nruntime results, because the \\texttt{elkan} and \\texttt{hamerly} algorithms\naccess points linearly and thus benefit from cache effects; this is not true of\nthe tree-based algorithms.  Therefore, the average number of distance\ncalculations per iteration are also included in the results.\n\nIt is immediately clear that for large datasets, \\texttt{dualtree-kd} is\nfastest, and \\texttt{dualtree-ct} is almost as fast.\nThe \\texttt{elkan} algorithm, because it\nholds $kN$ bounds, is able to prune away a huge amount of work and is very fast\nfor small datasets; however,\nmaintaining all of these bounds becomes prohibitive with large $k$ and the\nalgorithm exhausts all\navailable memory.  The \\texttt{blacklist} algorithm has the same issue: on the\nlargest datasets, with the largest $k$ values, the space required to maintain\nall the blacklists is too much.  This is also true of the \\texttt{yinyang}\nalgorithm, which must maintain bounds between each point and each group of\ncentroids.  For large $k$, this burden becomes too much and the algorithm fails.\nThe \\texttt{hamerly} and dual-tree algorithms, on the other hand, are the\nbest-behaved with memory usage and do not have any issues with large $N$ or\nlarge $k$; however, the \\texttt{hamerly} algorithm is very slow on large\ndatasets because it is not able to prune many points at once.\n\nSimilar to the observations about the \\texttt{blacklist} algorithm, the\ntree-based approaches are less effective in higher dimensions\n\\cite{pelleg1999accelerating}.  This is an important point: the performance of\ntree-based approaches suffer in high dimensions in part because the bound \n$d_{\\min}(\\cdot, \\cdot)$ generally becomes looser as dimension increases.\nThis is partly because the volume of nodes in high dimensions is much higher;\nconsider that a ball has volume that is exponential in the dimension.\n\nEven so, in our results, we see speedup in reasonable dimensions (for example,\nthe {\\tt phy} dataset has 78 dimensions).  Further, because our algorithm is\ntree-independent, we may use tree structures that are tailored to\nhigh-dimensional data \\cite{arya1998optimal}---including ones that\nhave not yet been developed.  From our results we believe\nas a rule of thumb that the dual-tree $k$-means algorithm can be effective up to\na hundred dimensions or more.\n\nAnother clear observation is that when $k$ is scaled on a single dataset, the\n\\texttt{dualtree-kd} and \\texttt{dualtree-ct} algorithms nearly always scale\nbetter (in terms of runtime) than the other algorithms.  These results show that\nour algorithm satisfies its original goals: to be able to scale effectively to\nlarge $k$ and $N$.\n\n\\vspace*{-0.6em}\n\\section{Conclusion and future directions}\n\\label{sec:conclusion}\n\\vspace*{-0.2em}\n\nUsing four pruning strategies, we have developed a flexible,\ntree-independent dual-tree $k$-means algorithm that is the best-performing\nalgorithm for large datasets and large $k$ in small-to-medium dimensions.  It\nis theoretically favorable, has a small memory footprint, and may be used in\nconjunction with initial point selection and approximation schemes for\n additional speedup.\n\nThere are still interesting future directions to pursue, though.  The first\ndirection is parallelism: because our dual-tree algorithm is agnostic to the\ntype of traversal used, we may use a parallel traversal \\cite{curtin2013tree},\nsuch as an adapted version of a recent parallel dual-tree algorithm\n\\cite{lee2012distributed}.  The second direction is kernel $k$-means and other\nspectral clustering techniques: our algorithm may be merged with the\nideas of \\citet{curtin2014dual} to perform kernel $k$-means.  The third\ndirection is theoretical.  Recently, more general notions of intrinsic\ndimensionality have been proposed \\cite{houle2013dimensionality,\namsaleg2015estimating}; these may enable tighter and more descriptive runtime\nbounds.  Our work thus provides a useful and fast $k$-means algorithm and also\nopens promising avenues to further accelerated clustering algorithms.\n\n\\nocite{ram2009rank}\n\\nocite{march2010euclidean}\n\n\\bibliographystyle{icml2016}\n\\bibliography{kmeans}\n\n\\appendix\n\n\\section{Supplementary material}\n\nUnfortunately, space constraints prevent adequate explanation of each of the\npoints in the main paper.  This supplementary material is meant to clarify all\nof the parts of the dual-tree $k$-means algorithm that space did not permit in\nthe main paper.\n\n\\subsection{Updating the tree}\n\nIn addition to updating the centroids, the bounding information contained within\nthe tree must be updated according to pruning strategies 3 and 4.\nUnfortunately, this yields a particularly complex recursive algorithm, given in\nAlgorithm \\ref{alg:update_tree}.\n\n\\begin{algorithm*}\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} node ${\\mathscr{N}}_i$, ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$,\n${\\operatorname{pruned}}(\\cdot)$, ${\\operatorname{closest}}(\\cdot)$, ${\\operatorname{canchange}}(\\cdot)$, centroid movements $m$\n  \\STATE {\\bf Output:} updated ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$,\n${\\operatorname{canchange}}(\\cdot)$\n  \\medskip\n  \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{true}$\n  \\IF{${\\mathscr{N}}_i$ has a parent and ${\\operatorname{canchange}}({\\operatorname{parent}}({\\mathscr{N}}_i)) = \\mathtt{false}$}\n    \\STATE \\COMMENT{Use the parent's bounds.}\n    \\STATE ${\\operatorname{closest}}({\\mathscr{N}}_i) \\gets {\\operatorname{closest}}({\\operatorname{parent}}({\\mathscr{N}}_i))$\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets {\\operatorname{lb}}({\\mathscr{N}}_i) + \\max_i m_i$\n    \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n  \\ELSIF{${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$}\n    \\STATE \\COMMENT{${\\mathscr{N}}_i$ is owned by a single cluster.  Can that owner change\nnext iteration?}\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets \\max \\left({\\operatorname{lb}}({\\mathscr{N}}_i) - \\max_i m_i, \\min_{k \\ne j}\nd(c_k, c_j) / 2 \\right)$\n    \\IF{${\\operatorname{ub}}({\\mathscr{N}}_i) < {\\operatorname{lb}}({\\mathscr{N}}_i)$}\n      \\STATE \\COMMENT{The owner cannot change next iteration.}\n      \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n    \\ELSE\n      \\STATE \\COMMENT{Tighten the upper bound and try to prune again.}\n      \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets \\min \\left({\\operatorname{ub}}({\\mathscr{N}}_i), d_{\\max}({\\mathscr{N}}_i, c_j)\\right)$\n      \\STATE {\\bf if} ${\\operatorname{ub}}({\\mathscr{N}}_i) < {\\operatorname{lb}}({\\mathscr{N}}_i)$ {\\bf then} ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets\n\\mathtt{false}$\n    \\ENDIF\n  \\ELSE\n    \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}({\\mathscr{N}}_i)$\n    \\STATE ${\\operatorname{ub}}({\\mathscr{N}}_i) \\gets {\\operatorname{ub}}({\\mathscr{N}}_i) + m_j$\n    \\STATE ${\\operatorname{lb}}({\\mathscr{N}}_i) \\gets {\\operatorname{lb}}({\\mathscr{N}}_i) - \\max_k m_k$\n  \\ENDIF\n  \\STATE \\COMMENT{Recurse into each child.}\n  \\STATE{{\\bf for each} child ${\\mathscr{N}}_c$ of ${\\mathscr{N}}_i${\\bf , call}\n\\texttt{UpdateTree(${\\mathscr{N}}_c$)}}\n  \\STATE \\COMMENT{Try to determine points whose owner cannot change if ${\\mathscr{N}}_i$\ncan change owners.}\n  \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{true}$}\n    \\FOR{$p_i \\in \\mathscr{P}_i$}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n      \\STATE ${\\operatorname{ub}}(p_i) \\gets {\\operatorname{ub}}(p_i) + m_j$\n      \\STATE ${\\operatorname{lb}}(p_i) \\gets \\min\\left( {\\operatorname{lb}}(p_i) - \\max_k m_k, \\min_{k \\ne j}\nd(c_k, c_j) / 2 \\right)$\n      \\IF{${\\operatorname{ub}}(p_i) < {\\operatorname{lb}}(p_i)$}\n        \\STATE ${\\operatorname{canchange}}(p_i) \\gets \\mathtt{false}$\n      \\ELSE\n        \\STATE \\COMMENT{Tighten the upper bound and try again.}\n        \\STATE ${\\operatorname{ub}}(p_i) \\gets \\min\\left( {\\operatorname{ub}}(p_i), d(p_i, c_j) \\right)$\n        \\IF{${\\operatorname{ub}}(p_i) < {\\operatorname{lb}}(p_i)$} \n          \\STATE ${\\operatorname{canchange}}(p_i) \\gets \\mathtt{false}$\n        \\ELSE\n          \\STATE \\COMMENT{Point cannot be pruned.}\n          \\STATE ${\\operatorname{ub}}(p_i) \\gets \\infty$\n          \\STATE ${\\operatorname{lb}}(p_i) \\gets \\infty$\n        \\ENDIF\n      \\ENDIF\n    \\ENDFOR\n  \\ELSE\n    \\FOR{$p_i \\in \\mathscr{P}_i$ where ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$}\n      \\STATE \\COMMENT{Maintain upper and lower bounds for points whose owner\ncannot change.}\n      \\STATE $j \\gets \\mathrm{index\\ of } {\\operatorname{closest}}(p_i)$\n      \\STATE ${\\operatorname{ub}}(p_i) \\gets {\\operatorname{ub}}(p_i) + m_j$\n      \\STATE ${\\operatorname{lb}}(p_i) \\gets {\\operatorname{lb}}(p_i) - \\max_k m_k$\n    \\ENDFOR\n  \\ENDIF\n  \\IF{${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$ for all children ${\\mathscr{N}}_c$ of\n${\\mathscr{N}}_i$ and all points $p_i \\in \\mathscr{P}_i$}\n    \\STATE ${\\operatorname{canchange}}({\\mathscr{N}}_i) \\gets \\mathtt{false}$\n  \\ENDIF\n  \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{true}$}\n    \\STATE ${\\operatorname{pruned}}({\\mathscr{N}}_i) \\gets 0$\n  \\ENDIF\n\\end{algorithmic}\n\\caption{\\texttt{UpdateTree()} for dual-tree $k$-means.}\n\\label{alg:update_tree}\n\\end{algorithm*}\n\nThe first if statement (lines 4--10) catches the case where the parent cannot\nchange owner next iteration; in this case, the parent's upper bound and lower\nbound can be taken as valid bounds.  In addition, the upper and lower bounds are\nadjusted to account for cluster movement between iterations, so that the bounds\nare valid for next iteration.\n\nIf the node $\\mathscr{N}_i$ has an owner, the algorithm then attempts to use the\npruning rules established in Equations 4 and 6 in the main paper,\nto determine if the owner of $\\mathscr{N}_i$ can change next iteration.  If not,\n${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is set to \\texttt{false} (line 18).  On the other hand, if\nthe pruning check fails, the upper bound is tightened and the pruning check is\nperformed a second time.  It is worth noting that $d_{\\max}(\\mathscr{N}_i, c_j)$\nmay not actually be less than the current value of ${\\operatorname{ub}}({\\mathscr{N}}_i)$, which is why the\n$\\min$ is necessary.\n\nAfter recursing into the children of ${\\mathscr{N}}_i$, if ${\\mathscr{N}}_i$ could have an owner\nchange, each point is individually checked using the same approach (lines\n31--45).  However, there is a slight difference: if a point's owner can change,\nthe upper and lower bounds must be set to $\\infty$ (lines 44--45).  This is only\nnecessary with points; \\texttt{BaseCase()} does not take bounding information\nfrom previous iterations into account, because no work can be avoided in that\nway.\n\nThen, we may set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} if every point in ${\\mathscr{N}}_i$\nand every child of ${\\mathscr{N}}_i$ cannot change owners (and the points and nodes do not\nnecessarily have to have the same owner).  Otherwise, we must set\n${\\operatorname{pruned}}({\\mathscr{N}}_i)$ to $0$ for the next iteration.\n\n\\subsection{Coalescing the tree}\n\n\\begin{algorithm}[t!]\n\\begin{algorithmic}[1]\n  \\STATE {\\bf Input:} tree $\\mathscr{T}$\n  \\STATE {\\bf Output:} coalesced tree $\\mathscr{T}$\n  \\medskip\n  \\STATE \\COMMENT{A depth-first recursion to hide nodes where\n${\\operatorname{canchange}}(\\cdot)$ is \\texttt{false}.}\n  \\STATE $s \\gets \\{ \\operatorname{root}(\\mathscr{T}) \\}$\n  \\WHILE{$|s| > 0$}\n    \\STATE ${\\mathscr{N}}_i \\gets s\\mathtt{.pop()}$\n    \\medskip\n    \\STATE \\COMMENT{Special handling is required for leaf nodes and the root\nnode.}\n    \\IF{$|\\mathscr{C}_i| = 0$}\n      \\STATE {\\bf continue}\n    \\ELSIF{${\\mathscr{N}}_i$ is the root node}\n      \\FOR{${\\mathscr{N}}_c \\in \\mathscr{C}_i$}\n        \\STATE $s\\mathtt{.push(}{\\mathscr{N}}_c\\mathtt{)}$\n      \\ENDFOR\n    \\ENDIF\n    \\medskip\n    \\STATE \\COMMENT{See if children can be removed.}\n    \\FOR{${\\mathscr{N}}_c \\in \\mathscr{C}_i$}\n      \\IF{${\\operatorname{canchange}}({\\mathscr{N}}_c) = \\mathtt{false}$}\n        \\STATE remove child ${\\mathscr{N}}_c$\n      \\ELSE\n        \\STATE $s\\mathtt{.push(}{\\mathscr{N}}_c\\mathtt{)}$\n      \\ENDIF\n    \\ENDFOR\n    \\medskip\n    \\STATE \\COMMENT{If only one child is left, then this node is unnecessary.}\n    \\IF{$|\\mathscr{C}_i| = 1$}\n      \\STATE add child to ${\\operatorname{parent}}({\\mathscr{N}}_i)$\n      \\STATE remove ${\\mathscr{N}}_i$ from ${\\operatorname{parent}}({\\mathscr{N}}_i)$'s children\n    \\ENDIF\n  \\ENDWHILE\n  \\medskip\n  \\STATE {\\bf return} $\\mathscr{T}$\n\\end{algorithmic}\n\\caption{\\texttt{CoalesceTree()} for dual-tree $k$-means.}\n\\label{alg:coalesce}\n\\end{algorithm}\n\nAfter \\texttt{UpdateTree()} is called, the tree must be coalesced to remove any\nnodes where ${\\operatorname{canchange}}(\\cdot) = \\mathtt{false}$.  This can be accomplished via\na single pass over the tree.  A simple implementation is given in Algorithm\n\\ref{alg:coalesce}.  \\texttt{DecoalesceTree()} may be implemented by\nsimply restoring a pristine copy of the tree which was cached right before\n\\texttt{CoalesceTree()} is called.\n\n\\subsection{Correctness proof}\n\nAs mentioned in the main document, a correctness proof is possible but\ndifficult.  We will individually prove the correctness of various pieces of the\ndual-tree $k$-means algorithm, and then we will prove the main correctness\nresult.  For precision, we must introduce the exact definition of a space tree\nand a pruning dual-tree traversal, as given by Curtin et~al.\n\\cite{curtin2013tree}.\n\n\\begin{defn}\nA \\textbf{space tree} on a dataset $S \\in \\Re^{N \\times D}$ is an undirected,\nconnected, acyclic, rooted simple graph with the following properties:\n\n\\vspace*{-0.5em}\n\\begin{itemize}\n  \\item Each \\textit{node} (or vertex), holds a number of points (possibly\nzero) and is connected to one parent node and a number of child nodes (possibly\nzero).\n  \\item There is one node in every space tree with no parent; this\nis the \\textit{root node} of the tree.\n  \\item Each point in $S$ is contained in at least one node.\n  \\item Each node $\\mathscr{N}$ has a convex subset of\n$\\Re^{D}$ containing each point in that node and also the convex\nsubsets represented by each child of the node.\n\\end{itemize}\n\\end{defn}\n\n\\begin{defn}\n\\label{def:dtpt}\nA {\\it pruning dual-tree traversal} is a process that, given two space trees\n$\\mathscr{T}_q$ (the query tree, built on the query set $S_q$) and\n$\\mathscr{T}_r$ (the reference tree, built on the reference set $S_r$), will\nvisit combinations of nodes $(\\mathscr{N}_q, \\mathscr{N}_r)$ such that\n$\\mathscr{N}_q \\in \\mathscr{T}_q$ and $\\mathscr{N}_r \\in \\mathscr{T}_r$ no more\nthan once, and call a function \\texttt{Score($\\mathscr{N}_q$, $\\mathscr{N}_r$)}\nto assign a score to that node.  If the score is $\\infty$ (or above some bound),\nthe combination is pruned and no combinations ($\\mathscr{N}_{qc}$,\n$\\mathscr{N}_{rc}$) such that $\\mathscr{N}_{qc} \\in \\mathscr{D}^n_q$ and\n$\\mathscr{N}_{rc} \\in \\mathscr{D}^n_r$ are visited.  Otherwise, for every\ncombination of points ($p_q$, $p_r$) such that $p_q \\in \\mathscr{P}_q$ and $p_r\n\\in \\mathscr{P}_r$, a function \\texttt{BaseCase($p_q$, $p_r$)} is called.  If no\nnode combinations are pruned during the traversal, \\texttt{BaseCase($p_q$,\n$p_r$)} is called at least once on each combination of $p_q \\in S_q$ and $p_r\n\\in S_r$.\n\\end{defn}\n\nFor more description and clarity on these definitions, refer to\n\\cite{curtin2013tree}.\n\n\\begin{lemma}\nA pruning dual-tree traversal which uses \\texttt{BaseCase()} as given in\nAlgorithm 2 in the main paper and \\texttt{Score()} as given in Algorithm\n3 in the main paper which starts with valid ${\\operatorname{ub}}(\\cdot)$, ${\\operatorname{lb}}(\\cdot)$,\n${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$ for each node\n${\\mathscr{N}}_i \\in \\mathscr{T}$, and ${\\operatorname{ub}}(p_q) = {\\operatorname{lb}}(p_q) = \\infty$ for each point $p_q\n\\in S$, will satisfy the following conditions upon completion:\n\n\\vspace*{-0.4em}\n\\begin{itemize}\n  \\item For every $p_q \\in S$ that is a descendant of a node ${\\mathscr{N}}_i$ that has\nbeen pruned (${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$), ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is an upper bound on the\ndistance between $p_q$ and its closest centroid, and ${\\operatorname{closest}}({\\mathscr{N}}_i)$ is the\nowner of $p_q$.\n\n  \\item For every $p_q \\in S$ that is not a descendant of any node that has been\npruned, ${\\operatorname{ub}}(p_q)$ is an upper bound on the distance between $p_q$ and its\nclosest centroid, and ${\\operatorname{closest}}(p_q)$ is the owner of $p_q$.\n\n  \\item For every $p_q \\in S$ that is a descendant of a node ${\\mathscr{N}}_i$ that has\nbeen pruned (${\\operatorname{pruned}}({\\mathscr{N}}_i) = k - 1$), ${\\operatorname{lb}}({\\mathscr{N}}_i)$ is a lower bound on the\ndistance between $p_q$ and its second closest centroid.\n\n  \\item For every $p_q \\in S$ that is not a descendant of any node that has been\npruned, $\\min({\\operatorname{lb}}(p_q), {\\operatorname{lb}}({\\mathscr{N}}_q))$ where ${\\mathscr{N}}_q$ is a node such that $p_q \\in\n\\mathscr{P}_q$ is a lower bound on the distance between $p_q$ and its second\nclosest centroid.\n\\end{itemize}\n\\label{lem:dt_correct}\n\\vspace*{-0.4em}\n\\end{lemma}\n\n\\begin{proof}\nIt is easiest to consider each condition individually.  Thus, we will first\nconsider the upper bound on the distance to the closest cluster centroid.\nConsider some $p_q$ and suppose that the closest cluster centroid to $p_q$ is\n$c^*$.\n\nNow, suppose first that the point $p_q$ is a descendant point of a node ${\\mathscr{N}}_q$\nthat has been pruned.  We must show, then, that $c^*$ is ${\\operatorname{closest}}({\\mathscr{N}}_q)$.  Take\n$R = \\{ {\\mathscr{N}}_{r0}, {\\mathscr{N}}_{r1}, \\ldots, {\\mathscr{N}}_{rj} \\}$ to be the set of reference nodes\nvisited during the traversal with ${\\mathscr{N}}_q$ as a query node; that is, the\ncombinations $({\\mathscr{N}}_q, {\\mathscr{N}}_{ri})$ were visited for all ${\\mathscr{N}}_{ri} \\in R$.  Any\n${\\mathscr{N}}_{ri}$ is pruned only if\n\n\\vspace*{-0.8em}\n\n", "index": 13, "text": "\\begin{equation}\nd_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_{ri}) > {\\operatorname{ub}}({\\mathscr{N}}_i)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"d_{\\min}({\\mathscr{N}}_{q},{\\mathscr{N}}_{ri})&gt;{\\operatorname{ub}}({\\mathscr{N%&#10;}}_{i})\" display=\"block\"><mrow><mrow><msub><mi>d</mi><mi>min</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>q</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mrow><mi>r</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mo>ub</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1em}\n\nWe may therefore conclude that ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is correct for the root of the tree,\nbecause line 8 can never be reached.  Reasoning\nrecursively, we can see that any upper bound passed from the parent must be\nvalid.  Therefore, the first item of the lemma holds.\n\nNext, we will consider the lower bound, using a similar strategy.  We must show\nthat\n\n\\vspace*{-0.9em}\n\n", "itemtype": "equation", "pos": 79362, "prevtext": "\n\\vspace*{-0.8em}\n\n\\noindent according to line 10 of \\texttt{Score()}.  Thus, as\nlong as ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is a valid upper bound on the closest cluster distance for\nevery descendant point in ${\\mathscr{N}}_q$, then no nodes are incorrectly pruned.  It is\neasy to see that the upper bound is valid: initially, it is valid by assumption;\neach time the bound is updated with some node ${\\mathscr{N}}_{ri}$ (on lines\n19 and 20), it is set to\n$d_{\\max}({\\mathscr{N}}_i, c)$ where $c$ is some descendant centroid of ${\\mathscr{N}}_{ri}$.  This is\nclearly a valid upper bound, since $c$ cannot be any closer to any descendant\npoint of ${\\mathscr{N}}_i$ than $c^*$.  We may thus conclude that no node is incorrectly\npruned from $R$; we may apply this reasoning recursively to the ${\\mathscr{N}}_q$'s\nancestors to see that no reference node is incorrectly pruned.\n\nWhen a node is pruned from $R$, the number of pruned clusters for ${\\mathscr{N}}_q$ is\nupdated: the count of all clusters not previously pruned by ${\\mathscr{N}}_q$ (or its\nancestors) is added.  We cannot double-count the pruning of a cluster; thus the\nonly way that ${\\operatorname{pruned}}({\\mathscr{N}}_q)$ can be equal to $k - 1$ is if every centroid\nexcept one is pruned.  The centroid which is not pruned will be the nearest\ncentroid $c^*$, regardless of if ${\\operatorname{closest}}({\\mathscr{N}}_q)$ was set during this traversal\nor still holds its initial value, and therefore it must be true that ${\\operatorname{ub}}({\\mathscr{N}}_q)$\nis an upper bound on the distance between $p_q$ and $c^*$, and ${\\operatorname{closest}}({\\mathscr{N}}_q) =\nc^*$.\n\nThis allows us to finally conclude that if $p_q$ is a descendant of a node\n${\\mathscr{N}}_q$ that has been pruned, then ${\\operatorname{ub}}({\\mathscr{N}}_q)$ contains a valid upper bound on\nthe distance between $p_q$ and its closest cluster centroid, and\n${\\operatorname{closest}}({\\mathscr{N}}_q)$ is that closest cluster centroid.\n\nNow, consider the other case, where $p_q$ is not a descendant of any node that\nhas been pruned.  Take ${\\mathscr{N}}_i$ to be any node containing $p_q$\\footnote{Note that\nthe meaning here is not that $p_q$ is a descendant of ${\\mathscr{N}}_i$ ($p_i \\in\n\\mathscr{D}^p_i$), but instead that $p_q$ is held directly in ${\\mathscr{N}}_i$: $p_q \\in\n\\mathscr{P}_i$.}.  We have already reasoned that any cluster centroid node that\ncould possibly contain the closest cluster centroid to $p_q$ cannot have been\npruned; therefore, by the definition of pruning dual-tree traversal, we are\nguaranteed that \\texttt{BaseCase()} will be called with $p_q$ as the query point\nand the closest cluster centroid as the reference point.  This will then cause\n${\\operatorname{ub}}(p_q)$ to hold the distance to the closest cluster centroid---assuming\n${\\operatorname{ub}}(p_q)$ is always valid, which it is even at the beginning of the traversal\nbecause it is initialized to $\\infty$---and ${\\operatorname{closest}}(p_q)$ to hold the closest\ncluster centroid.\n\nTherefore, the first two conditions are proven.  The third and fourth\nconditions, for the lower bounds, require a slightly different strategy.\n\nThere are two ways ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is modified: first, at line\n14, when a node combination is pruned, and second, at\nline 6 when the lower bound is taken from the parent.\nAgain, consider the set $R = \\{ {\\mathscr{N}}_{r0}, {\\mathscr{N}}_{r1}, \\ldots, {\\mathscr{N}}_{rj} \\}$ which is\nthe set of reference nodes visited during the traversal with ${\\mathscr{N}}_q$ as a query\nnode.  Call the set of reference nodes that were pruned $R^p$.  At the end of\nthe traversal, then,\n\n\\vspace*{-1.3em}\n\\begin{eqnarray}\n{\\operatorname{lb}}({\\mathscr{N}}_q) &\\le& \\min_{{\\mathscr{N}}_{ri} \\in R^p} d_{\\min}({\\mathscr{N}}_q, {\\mathscr{N}}_{ri}) \\\\\n  &\\le& \\min_{c_k \\in C^p} d_{\\min}({\\mathscr{N}}_q, c_k)\n\\end{eqnarray}\n\\vspace*{-1.3em}\n\n\\noindent where $C^p$ is the set of centroids that are descendants of nodes in\n$R^p$.  Applying this reasoning recursively to the ancestors of ${\\mathscr{N}}_q$ shows\nthat at the end of the dual-tree traversal, ${\\operatorname{lb}}({\\mathscr{N}}_q)$ will contain a lower\nbound on the distance between any descendant point of ${\\mathscr{N}}_q$ and any pruned\ncentroid.  Thus, if ${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$, then ${\\operatorname{lb}}({\\mathscr{N}}_q)$ will contain a\nlower bound on the distance between any descendant point in ${\\mathscr{N}}_q$ and its\nsecond closest centroid.  So if we consider some point $p_q$ which is a\ndescendant of ${\\mathscr{N}}_q$ and ${\\mathscr{N}}_q$ is pruned (${\\operatorname{pruned}}({\\mathscr{N}}_q) = k - 1$), then\n${\\operatorname{lb}}({\\mathscr{N}}_q)$ is indeed a lower bound on the distance between $p_q$ and its second\nclosest centroid.\n\nNow, consider the case where $p_q$ is not a descendant of any node that has been\npruned, and take ${\\mathscr{N}}_q$ to be some node that owns $p_q$ (that is, $p_q \\in\n\\mathscr{P}_q$).  In this case, \\texttt{BaseCase()} will be called with every\ncentroid that has not been pruned.  So ${\\operatorname{lb}}({\\mathscr{N}}_q)$ is a lower bound on the\ndistance between $p_q$ and every pruned centroid, and ${\\operatorname{lb}}(p_q)$ will be a lower\nbound on the distance between $p_q$ and the second-closest non-pruned centroid,\ndue to the structure of the \\texttt{BaseCase()} function.  Therefore,\n$\\min({\\operatorname{lb}}(p_q), {\\operatorname{lb}}({\\mathscr{N}}_q))$ must be a lower bound on the distance between $p_q$\nand its second closest centroid.\n\nFinally, we may conclude that each item in the theorem holds.\n\\end{proof}\n\nNext, we must prove that \\texttt{UpdateTree()} functions correctly.\n\n\\begin{lemma}\nIn the context of Algorithm 1 in the main paper, given a\ntree $\\mathscr{T}$ with all associated bounds ${\\operatorname{ub}}(\\cdot)$ and ${\\operatorname{lb}}(\\cdot)$ and\ninformation ${\\operatorname{pruned}}(\\cdot)$, ${\\operatorname{closest}}(\\cdot)$, and ${\\operatorname{canchange}}(\\cdot)$, a run\nof \\texttt{UpdateTree()} as given in Algorithm \\ref{alg:update_tree} will have\nthe following effects:\n\n\\begin{itemize}\n  \\item For every node ${\\mathscr{N}}_i$, ${\\operatorname{ub}}({\\mathscr{N}}_i)$ will be a valid upper bound on the\ndistance between any descendant point of ${\\mathscr{N}}_i$ and its nearest centroid next\niteration.\n\n  \\item For every node ${\\mathscr{N}}_i$, ${\\operatorname{lb}}({\\mathscr{N}}_i)$ will be a valid lower bound on the\ndistance between any descendant point of ${\\mathscr{N}}_i$ and any pruned centroid next\niteration.\n\n  \\item A node ${\\mathscr{N}}_i$ will only have ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ if the\nowner of any descendant point of ${\\mathscr{N}}_i$ cannot change next iteration.\n\n  \\item A point $p_i$ will only have ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$ if the\nowner of $p_i$ cannot change next iteration.\n\n  \\item Any point $p_i$ with ${\\operatorname{canchange}}(p_i) = \\mathtt{true}$ that does not\nbelong to any node ${\\mathscr{N}}_i$ with ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ will have\n${\\operatorname{ub}}(p_i) = {\\operatorname{lb}}(p_i) = \\infty$, as required by the dual-tree traversal.\n\n  \\item Any node ${\\mathscr{N}}_i$ with ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$ at the end of\n\\texttt{UpdateTree()} will have ${\\operatorname{pruned}}({\\mathscr{N}}_i) = 0$.\n\\end{itemize}\n\\vspace*{-0.6em}\n\\label{lem:update_correct}\n\\end{lemma}\n\n\\begin{proof}\nEach point is best considered individually.  It is important to remember during\nthis proof that the centroids have been updated, but the bounds have not.  So\nany cluster centroid $c_i$ is already set for next iteration.  Take $c^l_i$ to\nmean the cluster centroid $c_i$ {\\it before} adjustment (that is, the old\ncentroid).  Also take ${\\operatorname{ub}}^l(\\cdot)$, ${\\operatorname{lb}}^l(\\cdot)$, ${\\operatorname{pruned}}^l(\\cdot)$, and\n${\\operatorname{canchange}}^l(\\cdot)$ to be the values at the time \\texttt{UpdateTree()} is\ncalled, before any of those values are changed.  Due to the assumptions in the\nstatement of the lemma, each of these quantities is valid.\n\nSuppose that for some node ${\\mathscr{N}}_i$, ${\\operatorname{closest}}({\\mathscr{N}}_i)$ is some cluster $c_j$.  For\n${\\operatorname{ub}}({\\mathscr{N}}_i)$ to be valid for next iteration, we must guarantee that ${\\operatorname{ub}}({\\mathscr{N}}_i)\n\\ge \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c_j)$ at the end of\n\\texttt{UpdateTree()}.  There are four ways ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is updated: it may be\ntaken from the parent and adjusted (line 8), it may be adjusted before a prune\nattempt (line 14), it may be tightened after a failed prune attempt (line 21),\nor it may be adjusted without a prune attempt (line 25).  If we can show that\neach of these four ways always results in ${\\operatorname{ub}}({\\mathscr{N}}_i)$ being valid, then the\nfirst condition of the theorem holds.\n\nIf ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is adjusted in line 14 or 25, the resulting value of ${\\operatorname{ub}}({\\mathscr{N}}_i)$,\nassuming ${\\operatorname{closest}}({\\mathscr{N}}_i) = c_j$, is\n\n\\vspace*{-1em}\n\\begin{eqnarray}\n{\\operatorname{ub}}({\\mathscr{N}}_i) &=& {\\operatorname{ub}}^l({\\mathscr{N}}_i) + m_j \\\\\n  &\\ge& \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c^l_j) + m_j \\\\\n  &\\ge& \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c_j)\n\\end{eqnarray}\n\\vspace*{-1em}\n\n\\noindent where the last step follows by the triangle inequality: $d(c_j, c^l_j)\n= m_j$.  Therefore those two updates to ${\\operatorname{ub}}({\\mathscr{N}}_i)$ result in valid upper bounds\nfor next iteration.  If ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is recalculated, in line\n21, then we are guaranteed that ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is valid because\n\n\\vspace*{-1em}\n\n", "index": 15, "text": "\\begin{equation}\nd_{\\max}({\\mathscr{N}}_i, c_j) \\ge \\max_{p_q \\in \\mathscr{D}^p_q} d(p_q, c_j).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"d_{\\max}({\\mathscr{N}}_{i},c_{j})\\geq\\max_{p_{q}\\in\\mathscr{D}^{p}_{q}}d(p_{q}%&#10;,c_{j}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>d</mi><mi>max</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>p</mi><mi>q</mi></msub><mo>\u2208</mo><msubsup><mi class=\"ltx_font_mathscript\">\ud835\udc9f</mi><mi>q</mi><mi>p</mi></msubsup></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-0.9em}\n\n\\noindent where $C_p$ is the set of centroids pruned by ${\\mathscr{N}}_i$ and ancestors\nduring the last dual-tree traversal.  The lower bound can be taken from the\nparent in line 9 and adjusted, it can be adjusted before a prune attempt in line\n15 or in a similar way without a prune attempt in line 26.  The last adjustment\ncan easily be shown to be valid:\n\n\\begin{eqnarray}\n{\\operatorname{lb}}({\\mathscr{N}}_i) &=& {\\operatorname{lb}}^l({\\mathscr{N}}_i) - \\max_k m_k \\\\\n  &\\le& \\left( \\min_{p_q \\in \\mathscr{D}^p_q} \\min_{c_p \\in C_p} d(p_q, c^l_p)\n\\right) - \\max_k m_k \\\\\n  &\\le& \\min_{p_q \\in \\mathscr{D}^p_q} \\min_{c_p \\in C_p} d(p_q, c_p)\n\\end{eqnarray}\n\\vspace*{-0.8em}\n\n\\noindent which follows by the triangle inequality: $d(c^l_p, c_p) \\le \\max_k\nm_k$.  Line 15 is slightly more complex; we must also consider the term $\\min_{k\n\\ne j} d(c_k, c_j) / 2$.  Suppose that\n\n\\vspace*{-0.8em}\n\n", "itemtype": "equation", "pos": 79878, "prevtext": "\n\\vspace*{-1em}\n\nWe may therefore conclude that ${\\operatorname{ub}}({\\mathscr{N}}_i)$ is correct for the root of the tree,\nbecause line 8 can never be reached.  Reasoning\nrecursively, we can see that any upper bound passed from the parent must be\nvalid.  Therefore, the first item of the lemma holds.\n\nNext, we will consider the lower bound, using a similar strategy.  We must show\nthat\n\n\\vspace*{-0.9em}\n\n", "index": 17, "text": "\\begin{equation}\n{\\operatorname{lb}}({\\mathscr{N}}_i) \\le \\min_{p_q \\in \\mathscr{D}^p_q} \\min_{c_p \\in C_p} d(p_q, c_p)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{lb}}({\\mathscr{N}}_{i})\\leq\\min_{p_{q}\\in\\mathscr{D}^{p}_{q}}%&#10;\\min_{c_{p}\\in C_{p}}d(p_{q},c_{p})\" display=\"block\"><mrow><mrow><mo>lb</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><munder><mi>min</mi><mrow><msub><mi>p</mi><mi>q</mi></msub><mo>\u2208</mo><msubsup><mi class=\"ltx_font_mathscript\">\ud835\udc9f</mi><mi>q</mi><mi>p</mi></msubsup></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><msub><mi>c</mi><mi>p</mi></msub><mo>\u2208</mo><msub><mi>C</mi><mi>p</mi></msub></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>q</mi></msub><mo>,</mo><msub><mi>c</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-0.8em}\n\nWe may use the triangle inequality ($d(p_q, c_k) \\le d(c_j, c_k) + d(p_q, c_j)$)\nto show that if this is true, the second closest centroid $c_k$ is such that\n$d(p_q, c_k) > 2 d(c_k, c_j)$ and therefore $\\min_{k \\ne j} d(c_k, c_j) / 2$ is\nalso a valid lower bound.  We can lastly use the same recursive argument from\nthe upper bound case to show that the second item of the lemma holds.\n\nShowing the correctness of ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is straightforward: we know that\n${\\operatorname{ub}}({\\mathscr{N}}_i)$ and ${\\operatorname{lb}}({\\mathscr{N}}_i)$ are valid for next iteration by the time\nany checks to set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} happens, due to the\ndiscussion above.  The situations where ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is set to\n\\texttt{false}, in line 18 and 22, are simply applications of Equations\n4 and 6 in the main paper, and are therefore valid.  There are\ntwo other ways ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ can be set to \\texttt{false}.  The first is on\nline 10, and this is easily shown to be valid: if\na parent's owner cannot change, then a child's owner cannot change either.  The\nother way to set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} is in line 53.  This is only possible if all\npoints in $\\mathscr{P}_i$ and all children of ${\\mathscr{N}}_i$ have ${\\operatorname{canchange}}(\\cdot)$\nset to \\texttt{false}; thus, no descendant point of ${\\mathscr{N}}_i$ can change owner next\niteration, and we may set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false}.\n\nNext, we must show that ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$ only if the owner of\n$p_i$ cannot change next iteration.  If ${\\operatorname{canchange}}^l(p_i) = \\mathtt{true}$,\nthen due to Lemma \\ref{lem:dt_correct}, ${\\operatorname{ub}}^l(p_i)$ and ${\\operatorname{lb}}^l(p_i)$ will be valid\nbounds.  In this case, we may use similar reasoning to show that ${\\operatorname{ub}}(p_i)$ and\n${\\operatorname{lb}}(p_i)$ are valid, and then we may see that the pruning attempts at line\n35 and 40 are valid.  Now, consider the other\ncase, where ${\\operatorname{canchange}}^l(p_i) = \\mathtt{false}$.  Then, ${\\operatorname{ub}}^l(p_i)$ and\n${\\operatorname{lb}}^l(p_i)$ will not have been modified by the dual-tree traversal, and will\nhold the values set in the previous run of \\texttt{UpdateTree()}.  As long as\nthose values are valid, then the fourth item holds.\n\nThe checks to see if ${\\operatorname{canchange}}(p_i)$ can be set to \\texttt{false} (from lines\n31 to 45) are only reached if ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is\n\\texttt{true}.  We already have shown that ${\\operatorname{ub}}(p_i)$ and ${\\operatorname{lb}}(p_i)$ are set\ncorrectly in that stanza.  The other case is if ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is\n\\texttt{false}.  In this case, lines 47 to 51 are reached.  It is easy to see\nusing similar reasoning to all previous cases that these lines result in valid\n${\\operatorname{ub}}(p_i)$ and ${\\operatorname{lb}}(p_i)$.  Therefore, the fourth item does hold.\n\nThe fifth item is taken care of in line 44 and 45.  Given some point $p_i$ with\n${\\operatorname{canchange}}(p_i) = \\mathtt{true}$, and where $p_i$ does not belong to any node\n${\\mathscr{N}}_i$ where ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$, these two lines must be\nreached, and therefore the fifth item holds.\n\nThe last item holds trivially---any node ${\\mathscr{N}}_i$ where ${\\operatorname{canchange}}({\\mathscr{N}}_i) =\n\\mathtt{true}$ has ${\\operatorname{pruned}}({\\mathscr{N}}_i)$ set to $0$ on line 55.\n\\end{proof}\n\nShowing that \\texttt{CoalesceTree()}, \\texttt{DecoalesceTree()}, and\n\\texttt{UpdateCentroids()} function correctly follows directly from the\nalgorithm descriptions.  Therefore, we are ready to show the main correctness\nresult.\n\n\\begin{thm}\nA single iteration of dual-tree $k$-means as given in Algorithm\n1 in the main paper will produce exactly the same results as the standard\nbrute-force $O(kN)$ implementation.\n\\end{thm}\n\n\\begin{proof}\nWe may use the previous lemmas to flesh out our earlier proof sketch.\n\nFirst, we know that the dual-tree algorithm (line\n9) produces correct results for ${\\operatorname{ub}}(\\cdot)$,\n${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$ for every point and node,\ndue to Lemma \\ref{lem:dt_correct}.\nNext, we know that \\texttt{UpdateTree()} maintains the correctness of those four\nquantities and only marks ${\\operatorname{canchange}}(\\cdot)$ to \\texttt{false} when the node or\npoint truly cannot change owner, due to Lemma \\ref{lem:update_correct}.  Next,\nwe know from earlier discussion that \\texttt{CoalesceTree()} and\n\\texttt{DecoalesceTree()} do not affect the results of the dual-tree algorithm\nbecause the only nodes and points removed are those where ${\\operatorname{canchange}}(\\cdot) =\n\\mathtt{false}$.  We also know that \\texttt{UpdateCentroids()} produces\ncentroids correctly.  Therefore, the results from Algorithm 1 in the main paper\nare identical to those of a brute-force $O(kN)$ $k$-means implementation.\n\\end{proof}\n\n\\subsection{Runtime bound proof}\n\nWe can use adaptive algorithm analysis techniques in order to bound the running\ntime of Algorithm 1 in the main paper, based on \\cite{curtin2015plug} and\n\\cite{langford2006}.  This analysis depends on the {\\it expansion constant},\nwhich is a measure of intrinsic dimension defined below, originally from\n\\cite{karger2002finding}.\n\n\\begin{defn}\n\\label{def:int_dim}\nLet $B_S(p, \\Delta)$ be the set of points in $S$ within a closed ball of radius\n$\\Delta$ around some $p \\in S$ with respect to a metric $d$:\n\n\\vspace*{-1.2em}\n\n", "itemtype": "equation", "pos": 80925, "prevtext": "\n\\vspace*{-0.9em}\n\n\\noindent where $C_p$ is the set of centroids pruned by ${\\mathscr{N}}_i$ and ancestors\nduring the last dual-tree traversal.  The lower bound can be taken from the\nparent in line 9 and adjusted, it can be adjusted before a prune attempt in line\n15 or in a similar way without a prune attempt in line 26.  The last adjustment\ncan easily be shown to be valid:\n\n\\begin{eqnarray}\n{\\operatorname{lb}}({\\mathscr{N}}_i) &=& {\\operatorname{lb}}^l({\\mathscr{N}}_i) - \\max_k m_k \\\\\n  &\\le& \\left( \\min_{p_q \\in \\mathscr{D}^p_q} \\min_{c_p \\in C_p} d(p_q, c^l_p)\n\\right) - \\max_k m_k \\\\\n  &\\le& \\min_{p_q \\in \\mathscr{D}^p_q} \\min_{c_p \\in C_p} d(p_q, c_p)\n\\end{eqnarray}\n\\vspace*{-0.8em}\n\n\\noindent which follows by the triangle inequality: $d(c^l_p, c_p) \\le \\max_k\nm_k$.  Line 15 is slightly more complex; we must also consider the term $\\min_{k\n\\ne j} d(c_k, c_j) / 2$.  Suppose that\n\n\\vspace*{-0.8em}\n\n", "index": 19, "text": "\\begin{equation}\n\\min_{k \\ne j} d(c_k, c_j) / 2 > {\\operatorname{lb}}^l({\\mathscr{N}}_i) + \\max_k m_k.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\min_{k\\neq j}d(c_{k},c_{j})/2&gt;{\\operatorname{lb}}^{l}({\\mathscr{N}}_{i})+\\max%&#10;_{k}m_{k}.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>k</mi><mo>\u2260</mo><mi>j</mi></mrow></munder><mo>\u2061</mo><mi>d</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>k</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mn>2</mn></mrow><mo>&gt;</mo><mrow><mrow><msup><mo>lb</mo><mi>l</mi></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><munder><mi>max</mi><mi>k</mi></munder><mo>\u2061</mo><msub><mi>m</mi><mi>k</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.2em}\n\nThen, the {\\bf expansion constant} of $S$ with respect to the metric $d$ is the\nsmallest $c \\ge 2$ such that\n\n\\vspace*{-1.2em}\n\n", "itemtype": "equation", "pos": 86790, "prevtext": "\n\\vspace*{-0.8em}\n\nWe may use the triangle inequality ($d(p_q, c_k) \\le d(c_j, c_k) + d(p_q, c_j)$)\nto show that if this is true, the second closest centroid $c_k$ is such that\n$d(p_q, c_k) > 2 d(c_k, c_j)$ and therefore $\\min_{k \\ne j} d(c_k, c_j) / 2$ is\nalso a valid lower bound.  We can lastly use the same recursive argument from\nthe upper bound case to show that the second item of the lemma holds.\n\nShowing the correctness of ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is straightforward: we know that\n${\\operatorname{ub}}({\\mathscr{N}}_i)$ and ${\\operatorname{lb}}({\\mathscr{N}}_i)$ are valid for next iteration by the time\nany checks to set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} happens, due to the\ndiscussion above.  The situations where ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is set to\n\\texttt{false}, in line 18 and 22, are simply applications of Equations\n4 and 6 in the main paper, and are therefore valid.  There are\ntwo other ways ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ can be set to \\texttt{false}.  The first is on\nline 10, and this is easily shown to be valid: if\na parent's owner cannot change, then a child's owner cannot change either.  The\nother way to set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false} is in line 53.  This is only possible if all\npoints in $\\mathscr{P}_i$ and all children of ${\\mathscr{N}}_i$ have ${\\operatorname{canchange}}(\\cdot)$\nset to \\texttt{false}; thus, no descendant point of ${\\mathscr{N}}_i$ can change owner next\niteration, and we may set ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ to \\texttt{false}.\n\nNext, we must show that ${\\operatorname{canchange}}(p_i) = \\mathtt{false}$ only if the owner of\n$p_i$ cannot change next iteration.  If ${\\operatorname{canchange}}^l(p_i) = \\mathtt{true}$,\nthen due to Lemma \\ref{lem:dt_correct}, ${\\operatorname{ub}}^l(p_i)$ and ${\\operatorname{lb}}^l(p_i)$ will be valid\nbounds.  In this case, we may use similar reasoning to show that ${\\operatorname{ub}}(p_i)$ and\n${\\operatorname{lb}}(p_i)$ are valid, and then we may see that the pruning attempts at line\n35 and 40 are valid.  Now, consider the other\ncase, where ${\\operatorname{canchange}}^l(p_i) = \\mathtt{false}$.  Then, ${\\operatorname{ub}}^l(p_i)$ and\n${\\operatorname{lb}}^l(p_i)$ will not have been modified by the dual-tree traversal, and will\nhold the values set in the previous run of \\texttt{UpdateTree()}.  As long as\nthose values are valid, then the fourth item holds.\n\nThe checks to see if ${\\operatorname{canchange}}(p_i)$ can be set to \\texttt{false} (from lines\n31 to 45) are only reached if ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is\n\\texttt{true}.  We already have shown that ${\\operatorname{ub}}(p_i)$ and ${\\operatorname{lb}}(p_i)$ are set\ncorrectly in that stanza.  The other case is if ${\\operatorname{canchange}}({\\mathscr{N}}_i)$ is\n\\texttt{false}.  In this case, lines 47 to 51 are reached.  It is easy to see\nusing similar reasoning to all previous cases that these lines result in valid\n${\\operatorname{ub}}(p_i)$ and ${\\operatorname{lb}}(p_i)$.  Therefore, the fourth item does hold.\n\nThe fifth item is taken care of in line 44 and 45.  Given some point $p_i$ with\n${\\operatorname{canchange}}(p_i) = \\mathtt{true}$, and where $p_i$ does not belong to any node\n${\\mathscr{N}}_i$ where ${\\operatorname{canchange}}({\\mathscr{N}}_i) = \\mathtt{false}$, these two lines must be\nreached, and therefore the fifth item holds.\n\nThe last item holds trivially---any node ${\\mathscr{N}}_i$ where ${\\operatorname{canchange}}({\\mathscr{N}}_i) =\n\\mathtt{true}$ has ${\\operatorname{pruned}}({\\mathscr{N}}_i)$ set to $0$ on line 55.\n\\end{proof}\n\nShowing that \\texttt{CoalesceTree()}, \\texttt{DecoalesceTree()}, and\n\\texttt{UpdateCentroids()} function correctly follows directly from the\nalgorithm descriptions.  Therefore, we are ready to show the main correctness\nresult.\n\n\\begin{thm}\nA single iteration of dual-tree $k$-means as given in Algorithm\n1 in the main paper will produce exactly the same results as the standard\nbrute-force $O(kN)$ implementation.\n\\end{thm}\n\n\\begin{proof}\nWe may use the previous lemmas to flesh out our earlier proof sketch.\n\nFirst, we know that the dual-tree algorithm (line\n9) produces correct results for ${\\operatorname{ub}}(\\cdot)$,\n${\\operatorname{lb}}(\\cdot)$, ${\\operatorname{pruned}}(\\cdot)$, and ${\\operatorname{closest}}(\\cdot)$ for every point and node,\ndue to Lemma \\ref{lem:dt_correct}.\nNext, we know that \\texttt{UpdateTree()} maintains the correctness of those four\nquantities and only marks ${\\operatorname{canchange}}(\\cdot)$ to \\texttt{false} when the node or\npoint truly cannot change owner, due to Lemma \\ref{lem:update_correct}.  Next,\nwe know from earlier discussion that \\texttt{CoalesceTree()} and\n\\texttt{DecoalesceTree()} do not affect the results of the dual-tree algorithm\nbecause the only nodes and points removed are those where ${\\operatorname{canchange}}(\\cdot) =\n\\mathtt{false}$.  We also know that \\texttt{UpdateCentroids()} produces\ncentroids correctly.  Therefore, the results from Algorithm 1 in the main paper\nare identical to those of a brute-force $O(kN)$ $k$-means implementation.\n\\end{proof}\n\n\\subsection{Runtime bound proof}\n\nWe can use adaptive algorithm analysis techniques in order to bound the running\ntime of Algorithm 1 in the main paper, based on \\cite{curtin2015plug} and\n\\cite{langford2006}.  This analysis depends on the {\\it expansion constant},\nwhich is a measure of intrinsic dimension defined below, originally from\n\\cite{karger2002finding}.\n\n\\begin{defn}\n\\label{def:int_dim}\nLet $B_S(p, \\Delta)$ be the set of points in $S$ within a closed ball of radius\n$\\Delta$ around some $p \\in S$ with respect to a metric $d$:\n\n\\vspace*{-1.2em}\n\n", "index": 21, "text": "\\begin{equation}\nB_S(p, \\Delta) = \\{ r \\in S \\colon d(p, r) \\leq \\Delta \\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"B_{S}(p,\\Delta)=\\{r\\in S\\colon d(p,r)\\leq\\Delta\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>B</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>r</mi><mo>\u2208</mo><mi>S</mi></mrow><mo>:</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\vspace*{-1.2em}\n\\end{defn}\n\nThe expansion constant is a bound on the number of points which fall into balls\nof increasing sizes.  A low expansion constant generally means that search tasks\nlike nearest neighbor search can be performed quickly with trees, whereas a high\nexpansion constant implies a difficult dataset.  Thus, if we assume a bounded\nexpansion constant like in previous theoretical works \\cite{langford2006,\nram2009, karger2002finding, curtin2014dual, curtin2015plug}, we may assemble a\nruntime bound that reflects the difficulty of the dataset.\n\nOur theoretical analysis will concern the cover tree in particular.  The cover\ntree is a complex data structure with\nappealing theoretical properties.  We will only summarize the relevant\nproperties here.  Interested readers should consult the original cover tree\npaper \\cite{langford2006} and later analyses \\cite{ram2009, curtin2015plug} for\na complete understanding.\n\nA cover tree is a leveled tree; that is, each cover tree node $\\mathscr{N}_i$ is\nassociated with an integer scale $s_i$.  The node with largest scale is the root\nof the tree; each node's scale is greater than its children's.  Each node\n$\\mathscr{N}_i$ holds one point $p_i$, and every descendant point of\n$\\mathscr{N}_i$ is contained in the ball centered at $p_i$ with radius $2^{s_r +\n1}$.  Further, every cover tree satisfies the following three invariants\n\\cite{langford2006}:\n\n\\begin{itemize}\n\n\\item {\\it (Nesting.)}  When a point $p_i$ is held in a node at some scale\n$s_i$, then each smaller scale will also have a node containing $p_i$.\n\n\\item {\\it (Covering tree.)}  For every point $p_i$ held in a node\n$\\mathscr{N}_i$ at scale $s_i$, there exists a node with point $p_j$ and scale\n$s_i + 1$ which is the parent of $\\mathscr{N}_i$, and $d(p_i, p_j) < 2^{s_i +\n1}$.\n\n\\item {\\it (Separation.)}  Given distinct nodes $\\mathscr{N}_i$ holding $p_i$\nand $\\mathscr{N}_j$ holding $p_j$ both at scale $s_i$, $d(p_i, p_j) > 2^{s_i}$.\n\\end{itemize}\n\nA useful result shows there are $O(N)$ points in a cover tree (Theorem 1,\n\\cite{langford2006}).  Another measure of importance of a cover tree is the {\\it\ncover tree imbalance}, which aims to capture how well the data is distributed\nthroughout the cover tree.  For instance, consider a tree where the root, with\nscale $s_r$, has two\nnodes; one node corresponds to a single point and has scale $-\\infty$, and the\nother node has scale $s_r - 1$ and contains every other point in the dataset as\na descendant.  This is very imbalanced, and a tree with many situations like\nthis will not perform well for search tasks.  Below, we reiterate the definition\nof cover tree imbalance from \\cite{curtin2015plug}.\n\n\\begin{defn}\nThe {\\it cover node imbalance} $i_n(\\mathscr{N}_i)$ for a cover tree node\n$\\mathscr{N}_i$ with scale $s_i$ in the cover tree $\\mathscr{T}$ is defined as\nthe cumulative number of missing levels between the node and its parent\n$\\mathscr{N}_p$ (which has scale $s_p$).  If\nthe node is a leaf child (that is, $s_i = -\\infty$), then number of missing\nlevels is defined as the difference between $s_p$ and $s_{\\min} - 1$ where\n$s_{\\min}$ is the smallest scale of a non-leaf node in $\\mathscr{T}$.  If\n$\\mathscr{N}_i$ is the root of the tree, then the cover node imbalance is 0.\nExplicitly written, this calculation is\n\n\n", "itemtype": "equation", "pos": 87026, "prevtext": "\n\\vspace*{-1.2em}\n\nThen, the {\\bf expansion constant} of $S$ with respect to the metric $d$ is the\nsmallest $c \\ge 2$ such that\n\n\\vspace*{-1.2em}\n\n", "index": 23, "text": "\\begin{equation}| B_S(p, 2 \\Delta) | \\le c | B_S(p, \\Delta) |\\ \\forall\\ p \\in S,\\\n\\forall\\ \\Delta > 0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"|B_{S}(p,2\\Delta)|\\leq c|B_{S}(p,\\Delta)|\\ \\forall\\ p\\in S,\\ \\forall\\ \\Delta&gt;0.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>B</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>B</mi><mi>S</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi mathvariant=\"normal\">\u0394</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo rspace=\"7.5pt\">\u2200</mo><mi>p</mi></mrow></mrow><mo>\u2208</mo><mi>S</mi></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mo rspace=\"7.5pt\">\u2200</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>&gt;</mo><mn>0</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\end{defn}\n\nThis simple definition of cover node imbalance is easy to calculate, and using\nit, we can generalize to a measure of imbalance for the full tree.\n\n\\begin{defn}\n\\label{def:imbalance}\nThe {\\it cover tree imbalance} $i_t(\\mathscr{T})$ for a cover tree $\\mathscr{T}$\nis defined as the cumulative number of missing levels in the tree.  This can be\nexpressed as a function of cover node imbalances easily:\n\n\n", "itemtype": "equation", "pos": 90454, "prevtext": "\n\\vspace*{-1.2em}\n\\end{defn}\n\nThe expansion constant is a bound on the number of points which fall into balls\nof increasing sizes.  A low expansion constant generally means that search tasks\nlike nearest neighbor search can be performed quickly with trees, whereas a high\nexpansion constant implies a difficult dataset.  Thus, if we assume a bounded\nexpansion constant like in previous theoretical works \\cite{langford2006,\nram2009, karger2002finding, curtin2014dual, curtin2015plug}, we may assemble a\nruntime bound that reflects the difficulty of the dataset.\n\nOur theoretical analysis will concern the cover tree in particular.  The cover\ntree is a complex data structure with\nappealing theoretical properties.  We will only summarize the relevant\nproperties here.  Interested readers should consult the original cover tree\npaper \\cite{langford2006} and later analyses \\cite{ram2009, curtin2015plug} for\na complete understanding.\n\nA cover tree is a leveled tree; that is, each cover tree node $\\mathscr{N}_i$ is\nassociated with an integer scale $s_i$.  The node with largest scale is the root\nof the tree; each node's scale is greater than its children's.  Each node\n$\\mathscr{N}_i$ holds one point $p_i$, and every descendant point of\n$\\mathscr{N}_i$ is contained in the ball centered at $p_i$ with radius $2^{s_r +\n1}$.  Further, every cover tree satisfies the following three invariants\n\\cite{langford2006}:\n\n\\begin{itemize}\n\n\\item {\\it (Nesting.)}  When a point $p_i$ is held in a node at some scale\n$s_i$, then each smaller scale will also have a node containing $p_i$.\n\n\\item {\\it (Covering tree.)}  For every point $p_i$ held in a node\n$\\mathscr{N}_i$ at scale $s_i$, there exists a node with point $p_j$ and scale\n$s_i + 1$ which is the parent of $\\mathscr{N}_i$, and $d(p_i, p_j) < 2^{s_i +\n1}$.\n\n\\item {\\it (Separation.)}  Given distinct nodes $\\mathscr{N}_i$ holding $p_i$\nand $\\mathscr{N}_j$ holding $p_j$ both at scale $s_i$, $d(p_i, p_j) > 2^{s_i}$.\n\\end{itemize}\n\nA useful result shows there are $O(N)$ points in a cover tree (Theorem 1,\n\\cite{langford2006}).  Another measure of importance of a cover tree is the {\\it\ncover tree imbalance}, which aims to capture how well the data is distributed\nthroughout the cover tree.  For instance, consider a tree where the root, with\nscale $s_r$, has two\nnodes; one node corresponds to a single point and has scale $-\\infty$, and the\nother node has scale $s_r - 1$ and contains every other point in the dataset as\na descendant.  This is very imbalanced, and a tree with many situations like\nthis will not perform well for search tasks.  Below, we reiterate the definition\nof cover tree imbalance from \\cite{curtin2015plug}.\n\n\\begin{defn}\nThe {\\it cover node imbalance} $i_n(\\mathscr{N}_i)$ for a cover tree node\n$\\mathscr{N}_i$ with scale $s_i$ in the cover tree $\\mathscr{T}$ is defined as\nthe cumulative number of missing levels between the node and its parent\n$\\mathscr{N}_p$ (which has scale $s_p$).  If\nthe node is a leaf child (that is, $s_i = -\\infty$), then number of missing\nlevels is defined as the difference between $s_p$ and $s_{\\min} - 1$ where\n$s_{\\min}$ is the smallest scale of a non-leaf node in $\\mathscr{T}$.  If\n$\\mathscr{N}_i$ is the root of the tree, then the cover node imbalance is 0.\nExplicitly written, this calculation is\n\n\n", "index": 25, "text": "\\begin{equation}\ni_n(\\mathscr{N}_i) = \\begin{dcases*}\n  s_p - s_i - 1 & if $\\mathscr{N}_i$ is not a \\\\\n  & leaf and not \\\\\n  & the root node \\\\\n  \\max(s_p - s_{\\min} - 1, \\; 0) & if $\\mathscr{N}_i$ is a leaf \\\\\n  0 & if $\\mathscr{N}_i$ is the root. \\\\\n  \\end{dcases*}\n  \\label{eqn_node_imbalance}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"i_{n}(\\mathscr{N}_{i})=\\begin{dcases}s_{p}-s_{i}-1&amp;if $\\mathscr{N}_{i}$ is not%&#10; a\\\\&#10;&amp;leaf and not\\\\&#10;&amp;the root node\\\\&#10;\\max(s_{p}-s_{\\min}-1,\\;0)&amp;if $\\mathscr{N}_{i}$ is a leaf\\\\&#10;0&amp;if $\\mathscr{N}_{i}$ is the root.\\\\&#10;\\end{dcases}\" display=\"block\"><mrow><mrow><msub><mi>i</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>-</mo><msub><mi>s</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mtext>\u00a0is not a</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mtext>leaf and not</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mi/></mtd><mtd columnalign=\"left\"><mtext>the root node</mtext></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>-</mo><msub><mi>s</mi><mi>min</mi></msub><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mn>\u20040</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mtext>\u00a0is a leaf</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mtext>\u00a0is the root.</mtext></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\end{defn}\n\nBounding $i_t(\\mathscr{T})$ is non-trivial, but empirical results suggest that\nimbalance scales linearly with the size of the dataset, when the expansion\nconstant is well-behaved.  A bound on $i_t(\\mathscr{T})$ is still an open\nproblem at the time of this writing.\n\nWith these terms introduced, we may introduce a slightly adapted result from\n\\cite{curtin2015plug}, which bounds the running time of nearest neighbor search.\n\n\\begin{thm}\n(Theorem 2, \\cite{curtin2015plug}.)  Using cover trees, the standard cover tree\npruning dual-tree traversal, and the\nnearest neighbor search \\texttt{BaseCase()} and \\texttt{Score()} as given in\nAlgorithms 2 and 3 of \\cite{curtin2015plug}, respectively, and also\ngiven a reference set $S_r$ with expansion constant $c_r$, and a query set\n$S_q$, where the range of pairwise distances in $S_r$ is completely contained in\nthe range of pairwise distances in $S_q$, the running time of nearest neighbor\nsearch is bounded by $O(c_r^4 c_{qr}^5 (N + i_t(\\mathscr{T}_q)))$, where\n$c_{qr} = \\max((\\max_{p_q \\in S_q} c_r'), c_r)$, where $c_r'$ is the expansion\nconstant of the set $S_r \\cup \\{ p_q \\}$.\n\\label{thm:nns}\n\\end{thm}\n\nNow, we may adapt this result slightly.\n\n\\begin{thm}\nThe dual-tree $k$-means algorithm with \\texttt{BaseCase()} as in Algorithm\n2 in the main paper and \\texttt{Score()} as in Algorithm 3 in the main paper, with a\npoint set $S_q$ that has expansion constant $c_q$ and size $N$, and $k$ centroids\n$C$ with expansion constant $c_k$, takes no more than $O(c_k^4 c_{qk}^5 (N +\ni_t(\\mathscr{T}_q)))$ time.\n\\label{thm:dtkm}\n\\end{thm}\n\n\\begin{proof}\nBoth \\texttt{Score()} and \\texttt{BaseCase()} for dual-tree $k$-means can be\nperformed in $O(1)$ time.  In addition, the pruning of \\texttt{Score()} for\ndual-tree $k$-means is at least as tight as \\texttt{Score()} for nearest\nneighbor search: the pruning rule in Equation 2 in the main paper is equivalent to\nthe pruning rule for nearest neighbor search.  Therefore, dual-tree $k$-means\ncan visit no more nodes than nearest neighbor search would with query set $S_q$\nand reference set $C$.  Lastly, note that the range of pairwise distances of $C$\nwill be entirely contained in the range of pairwise distances in $S_q$, to see\nthat we can use the result of Theorem \\ref{thm:nns}.  Adapting that result,\nthen, yields the statement of the algorithm.\n\\end{proof}\n\nThe expansion constant of the centroids, $c_k$, may be understood as the\nintrinsic dimensionality of the centroids $C$.  During each iteration, the\ncentroids change, so those iterations that have centroids with high intrinsic\ndimensionality cannot be bounded as tightly.  More general measures of intrinsic\ndimensionality, such as those recently proposed by Houle\n\\cite{houle2013dimensionality}, may make the connection between $c_q$ and $c_k$\nclear.\n\nNext, we turn to bounding the entire algorithm.\n\n\\begin{thm}\nA single iteration of the dual-tree $k$-means algorithm on a dataset $S_q$ using\nthe cover tree $\\mathscr{T}$, the standard cover tree pruning dual-tree\ntraversal, \\texttt{BaseCase()} as given in Algorithm 2 in the main paper,\n\\texttt{Score()} as given in Algorithm 3 in the main paper, will take no more than\n\n\\vspace*{-0.3em}\n\n", "itemtype": "equation", "pos": 91179, "prevtext": "\n\\end{defn}\n\nThis simple definition of cover node imbalance is easy to calculate, and using\nit, we can generalize to a measure of imbalance for the full tree.\n\n\\begin{defn}\n\\label{def:imbalance}\nThe {\\it cover tree imbalance} $i_t(\\mathscr{T})$ for a cover tree $\\mathscr{T}$\nis defined as the cumulative number of missing levels in the tree.  This can be\nexpressed as a function of cover node imbalances easily:\n\n\n", "index": 27, "text": "\\begin{equation}\ni_t(\\mathscr{T}) = \\sum_{\\mathscr{N}_i \\in \\mathscr{T}} i_n(\\mathscr{N}_i).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"i_{t}(\\mathscr{T})=\\sum_{\\mathscr{N}_{i}\\in\\mathscr{T}}i_{n}(\\mathscr{N}_{i}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathscript\">\ud835\udcaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathscript\">\ud835\udcaf</mi></mrow></munder><mrow><msub><mi>i</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathscript\">\ud835\udca9</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\n\\noindent time, where $c_k$ is the expansion constant of the centroids, $c_{qk}$\nis defined as in Theorem \\ref{thm:dtkm}, and $i_t(\\mathscr{T})$ is the imbalance of\nthe tree as defined in Definition \\ref{def:imbalance}.\n\\end{thm}\n\n\\begin{proof}\nConsider each of the steps of the algorithm individually:\n\n\\begin{itemize}\n  \\item \\texttt{CoalesceNodes()} can be performed in a single pass of the cover\ntree $\\mathscr{N}$, which takes $O(N)$ time.\n\n  \\item Building a tree on the centroids ($\\mathscr{T}_c$) takes $O(c_k^6 k \\log\nk)$ time due to the result for cover tree construction time \\cite{langford2006}.\n\n  \\item The dual-tree algorithm takes $O(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T})))$\ntime due to Theorem \\ref{thm:dtkm}.\n\n  \\item \\texttt{DecoalesceNodes()} can be performed in a single pass of the\ncover tree $\\mathscr{N}$, which takes $O(N)$ time.\n\n  \\item \\texttt{UpdateCentroids()} can be performed in a single pass of the\ncover tree $\\mathscr{N}$, so it also takes $O(N)$ time.\n\n  \\item \\texttt{UpdateTree()} depends on the calculation of how much each\ncentroid has moved; this costs $O(k)$ time.  In addition, we must find the\nnearest centroid of every centroid; this is nearest neighbor search, and we may\nuse the runtime bound for monochromatic nearest neighbor search for cover trees\nfrom \\cite{ram2009}, so this costs $O(c_k^9 k)$ time.  Lastly, the actual tree\nupdate visits each node once and iterates over each point in the node.  Cover\ntree nodes only hold one point, so each visit costs $O(1)$ time, and with $O(N)$\nnodes, the entire update process costs $O(N)$ time.  When we consider the\npreprocessing cost too, the total cost of \\texttt{UpdateTree()} per iteration is\n$O(c_k^9 k + N)$.\n\\end{itemize}\n\nWe may combine these into a final result:\n\n\\vspace*{-1em}\n\\begin{eqnarray}\nO(N) + O(c_k^6 k \\log k) + O(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T}))) + \\nonumber \\\\\n\\ \\ O(N) + \nO(N) + O(c_k^9 k + N)\n\\end{eqnarray}\n\\vspace*{-1em}\n\n\\noindent and after simplification, we get the statement of the theorem:\n\n\\vspace*{-1em}\n\n", "itemtype": "equation", "pos": 94498, "prevtext": "\n\\end{defn}\n\nBounding $i_t(\\mathscr{T})$ is non-trivial, but empirical results suggest that\nimbalance scales linearly with the size of the dataset, when the expansion\nconstant is well-behaved.  A bound on $i_t(\\mathscr{T})$ is still an open\nproblem at the time of this writing.\n\nWith these terms introduced, we may introduce a slightly adapted result from\n\\cite{curtin2015plug}, which bounds the running time of nearest neighbor search.\n\n\\begin{thm}\n(Theorem 2, \\cite{curtin2015plug}.)  Using cover trees, the standard cover tree\npruning dual-tree traversal, and the\nnearest neighbor search \\texttt{BaseCase()} and \\texttt{Score()} as given in\nAlgorithms 2 and 3 of \\cite{curtin2015plug}, respectively, and also\ngiven a reference set $S_r$ with expansion constant $c_r$, and a query set\n$S_q$, where the range of pairwise distances in $S_r$ is completely contained in\nthe range of pairwise distances in $S_q$, the running time of nearest neighbor\nsearch is bounded by $O(c_r^4 c_{qr}^5 (N + i_t(\\mathscr{T}_q)))$, where\n$c_{qr} = \\max((\\max_{p_q \\in S_q} c_r'), c_r)$, where $c_r'$ is the expansion\nconstant of the set $S_r \\cup \\{ p_q \\}$.\n\\label{thm:nns}\n\\end{thm}\n\nNow, we may adapt this result slightly.\n\n\\begin{thm}\nThe dual-tree $k$-means algorithm with \\texttt{BaseCase()} as in Algorithm\n2 in the main paper and \\texttt{Score()} as in Algorithm 3 in the main paper, with a\npoint set $S_q$ that has expansion constant $c_q$ and size $N$, and $k$ centroids\n$C$ with expansion constant $c_k$, takes no more than $O(c_k^4 c_{qk}^5 (N +\ni_t(\\mathscr{T}_q)))$ time.\n\\label{thm:dtkm}\n\\end{thm}\n\n\\begin{proof}\nBoth \\texttt{Score()} and \\texttt{BaseCase()} for dual-tree $k$-means can be\nperformed in $O(1)$ time.  In addition, the pruning of \\texttt{Score()} for\ndual-tree $k$-means is at least as tight as \\texttt{Score()} for nearest\nneighbor search: the pruning rule in Equation 2 in the main paper is equivalent to\nthe pruning rule for nearest neighbor search.  Therefore, dual-tree $k$-means\ncan visit no more nodes than nearest neighbor search would with query set $S_q$\nand reference set $C$.  Lastly, note that the range of pairwise distances of $C$\nwill be entirely contained in the range of pairwise distances in $S_q$, to see\nthat we can use the result of Theorem \\ref{thm:nns}.  Adapting that result,\nthen, yields the statement of the algorithm.\n\\end{proof}\n\nThe expansion constant of the centroids, $c_k$, may be understood as the\nintrinsic dimensionality of the centroids $C$.  During each iteration, the\ncentroids change, so those iterations that have centroids with high intrinsic\ndimensionality cannot be bounded as tightly.  More general measures of intrinsic\ndimensionality, such as those recently proposed by Houle\n\\cite{houle2013dimensionality}, may make the connection between $c_q$ and $c_k$\nclear.\n\nNext, we turn to bounding the entire algorithm.\n\n\\begin{thm}\nA single iteration of the dual-tree $k$-means algorithm on a dataset $S_q$ using\nthe cover tree $\\mathscr{T}$, the standard cover tree pruning dual-tree\ntraversal, \\texttt{BaseCase()} as given in Algorithm 2 in the main paper,\n\\texttt{Score()} as given in Algorithm 3 in the main paper, will take no more than\n\n\\vspace*{-0.3em}\n\n", "index": 29, "text": "\\begin{equation}\nO(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T})) + c_k^9 k \\log k)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"O(c_{k}^{4}c_{qk}^{5}(N+i_{t}(\\mathscr{T}))+c_{k}^{9}k\\log k)\" display=\"block\"><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>c</mi><mi>k</mi><mn>4</mn></msubsup><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>q</mi><mo>\u2062</mo><mi>k</mi></mrow><mn>5</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathscript\">\ud835\udcaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>c</mi><mi>k</mi><mn>9</mn></msubsup><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>k</mi></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03754.tex", "nexttext": "\n\\end{proof}\n\nTherefore, we see that under some assumptions on the data, we can bound the\nruntime of the dual-tree $k$-means algorithm to something tighter than $O(kN)$\nper iteration.  As expected, we are able to amortize the cost of $k$ across all\n$N$ nodes, giving amortized $O(1)$ search for the nearest centroid per point in\nthe dataset.  This is similar to the results for nearest neighbor search, which\nobtain amortized $O(1)$ search for a single query point.  Also similar to the\nresults for nearest neighbor search is that the search time may, in the worst\ncase, degenerate to $O(kN + k^2)$ when the assumptions on the dataset are not\nsatisfied.  However, empirical results \\cite{ram2009rank, gray2001nbody,\nmarch2010euclidean, langford2006} show that well-behaved datasets are common in\nthe real world, and thus degeneracy of the search time is uncommon.\n\nComparing this bound with the bounds for other algorithms is somewhat difficult;\nfirst, none of the other algorithms have bounds which are adaptive to the\ncharacteristics of the dataset.  It is possible that the blacklist algorithm\ncould be refactored to use the cover tree, but even if that was done it is not\ncompletely clear how the running time could be bounded.  How to apply the\nexpansion constant to an analysis of Hamerly's algorithm and Elkan's algorithm\nis also unclear at the time of this writing.\n\nLastly, the bound we have shown above is potentially loose.  We have reduced\ndual-tree $k$-means to the problem of nearest neighbor search, but our pruning\nrules are tighter.  Dual-tree nearest neighbor search assumes that every query\nnode will be visited (this is where the $O(N)$ in the bound comes from), but\ndual-tree $k$-means can prune a query node entirely if all but one cluster is\npruned (Strategy 2).  These bounds do not take this pruning strategy into\naccount, and they also do not consider the fact that coalescing the tree can\ngreatly reduce its size.  These would be interesting directions for future\ntheoretical work.\n\n\n", "itemtype": "equation", "pos": 96624, "prevtext": "\n\n\\noindent time, where $c_k$ is the expansion constant of the centroids, $c_{qk}$\nis defined as in Theorem \\ref{thm:dtkm}, and $i_t(\\mathscr{T})$ is the imbalance of\nthe tree as defined in Definition \\ref{def:imbalance}.\n\\end{thm}\n\n\\begin{proof}\nConsider each of the steps of the algorithm individually:\n\n\\begin{itemize}\n  \\item \\texttt{CoalesceNodes()} can be performed in a single pass of the cover\ntree $\\mathscr{N}$, which takes $O(N)$ time.\n\n  \\item Building a tree on the centroids ($\\mathscr{T}_c$) takes $O(c_k^6 k \\log\nk)$ time due to the result for cover tree construction time \\cite{langford2006}.\n\n  \\item The dual-tree algorithm takes $O(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T})))$\ntime due to Theorem \\ref{thm:dtkm}.\n\n  \\item \\texttt{DecoalesceNodes()} can be performed in a single pass of the\ncover tree $\\mathscr{N}$, which takes $O(N)$ time.\n\n  \\item \\texttt{UpdateCentroids()} can be performed in a single pass of the\ncover tree $\\mathscr{N}$, so it also takes $O(N)$ time.\n\n  \\item \\texttt{UpdateTree()} depends on the calculation of how much each\ncentroid has moved; this costs $O(k)$ time.  In addition, we must find the\nnearest centroid of every centroid; this is nearest neighbor search, and we may\nuse the runtime bound for monochromatic nearest neighbor search for cover trees\nfrom \\cite{ram2009}, so this costs $O(c_k^9 k)$ time.  Lastly, the actual tree\nupdate visits each node once and iterates over each point in the node.  Cover\ntree nodes only hold one point, so each visit costs $O(1)$ time, and with $O(N)$\nnodes, the entire update process costs $O(N)$ time.  When we consider the\npreprocessing cost too, the total cost of \\texttt{UpdateTree()} per iteration is\n$O(c_k^9 k + N)$.\n\\end{itemize}\n\nWe may combine these into a final result:\n\n\\vspace*{-1em}\n\\begin{eqnarray}\nO(N) + O(c_k^6 k \\log k) + O(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T}))) + \\nonumber \\\\\n\\ \\ O(N) + \nO(N) + O(c_k^9 k + N)\n\\end{eqnarray}\n\\vspace*{-1em}\n\n\\noindent and after simplification, we get the statement of the theorem:\n\n\\vspace*{-1em}\n\n", "index": 31, "text": "\\begin{equation}\nO(c_k^4 c_{qk}^5 (N + i_t(\\mathscr{T})) + c_k^9 k \\log k).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"O(c_{k}^{4}c_{qk}^{5}(N+i_{t}(\\mathscr{T}))+c_{k}^{9}k\\log k).\" display=\"block\"><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>c</mi><mi>k</mi><mn>4</mn></msubsup><mo>\u2062</mo><msubsup><mi>c</mi><mrow><mi>q</mi><mo>\u2062</mo><mi>k</mi></mrow><mn>5</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathscript\">\ud835\udcaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mi>c</mi><mi>k</mi><mn>9</mn></msubsup><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>k</mi></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]