[{"file": "1601.05900.tex", "nexttext": "\nwhere $\\oplus$ denotes the logical XOR operation.  \n\nThat is, the difference is the number of edges that disagree, being in-cluster in one of the clusterings and between-cluster in the other. The maximum distance between clusterings is when the Hamming distance is $1$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, we formally define clustering functions. \n\n\\begin{definition}[Clustering function]\\label{clustering function}\nA \\emph{clustering function} is a function $\\mathcal{F}$\nthat takes as input a pair $(X,d)$ and a parameter $1 \\leq k \\leq |X|$, and outputs a $k$-clustering of the domain $X$.\n\n\\end{definition}\n\n\\section{Perturbation robustness as a property of an algorithm}\\label{impossibility}\n\nWhenever a user is faced with the task of clustering faulty data, it would be natural to select an algorithm that is robust to perturbations of pairwise dissimilarities. As such, we begin our study of perturbation robustness by casting it as a property of an algorithm. If we could classify algorithms based on whether or not (or to what degree) they are perturbation robust, then clustering users could incorporate this information when making decisions regarding which algorithms to apply on their data. \n\nFirst, we define what it means to perturb a dissimilarity function.\n\n\\begin{definition}[$\\alpha$-multiplicative perturbation of a dissimilarity function]\\label{multiplicativeperturbationdissimilarity}\nGiven a pair of dissimilarity functions $d$ and $d'$ over a domain $X$, $d'$ is an \\emph{$\\alpha$-multiplicative-perturbation} of $d$, for $\\alpha>1$, if for all $x,y \\in X$, $\\alpha ^{-1} d(x,y)\\leq d'(x,y) \\leq \\alpha d(x,y)$.\n\\end{definition}\n\n\nAdditive perturbation of a dissimilarity function is defined analogously. \n\n\\begin{definition}[$\\epsilon$-additive perturbation of a dissimilarity function]\\label{additiveperturbationdissimilarity}\nGiven a pair of dissimilarity functions $d$ and $d'$ over a domain $X$, $d'$ is an \\emph{$\\epsilon$-additive perturbation} of $d$, for $\\epsilon>0$, if for all $x,y \\in X$, $d(x,y)-\\epsilon \\leq d'(x,y) \\leq d(x,y)+\\epsilon$.\n\\end{definition}\n\nIt is important to note that all of our results hold for both multiplicative and additive perturbation robustness. Perturbation robust algorithms should be invariant to data perturbations; that is, if data is perturbed, then the output of the algorithm shouldn't change. This view of perturbation robustness is not only intuitive, but is also based on previous formulations \\cite{Reyzin,Bilu,Awasthi} (This can be formalized as a property of clustering functions by setting $\\delta = 0$ in Definition~\\ref{mult-function} below). \n\n\n\n\nHowever, requiring that the partitioning be identical before and after perturbation is provably too strict a requirement for clustering algorithms, as it can only hold for functions that effectively ignore all pairwise distances. That is, this notion of perturbation robustness only holds for clustering functions that, given any domain set $X$ and integer $1 \\leq k \\leq |X|$, produce the same partitioning regardless of the setting of $d$ (See Section 1 of the Appendix for details). \n\nAs such, we introduce a relaxation that allows some error in the output of the algorithm on perturbed data. From a practical point of view, it is likely that a user who has only a perturbation of the true data set is likely to be satisfied with an approximately correct solution. \n\n\n\\begin{definition}\\label{mult-function}\nA clustering function $\\mathcal{F}$ is \\emph{$(\\alpha, \\delta)$-multiplicative perturbation robust} if, given any data set $(X,d)$ and $1 \\leq k \\leq |X|$, whenever $d'$ is an $\\alpha$-multiplicative perturbation of $d$,  \n", "itemtype": "equation", "pos": 8533, "prevtext": "\n\\maketitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{abstract}\nClustering is a fundamental data mining tool that aims to divide data into groups of similar items.  Generally, intuition about clustering reflects the ideal case -- exact data sets endowed with flawless dissimilarity between individual instances. \n\nIn practice however, these cases are in the minority, and clustering applications are typically characterized by noisy data sets with approximate pairwise dissimilarities. As such, the efficacy of clustering methods in practical applications necessitates robustness to perturbations. \n\nIn this paper, we perform a formal analysis of perturbation robustness, revealing that the extent to which algorithms can exhibit this desirable characteristic is inherently limited, and identifying the types of structures that allow popular clustering paradigms to discover meaningful clusters in spite of faulty data. \n\\end{abstract}\n\n\n\n\\section{Introduction}\n\nClustering is a popular data mining tool, due in no small part to its general and intuitive goal of dividing data into groups of similar items. Yet in spite of this seemingly simple task, successful application of clustering techniques in practice is oftentimes challenging. In particular, there are inherent difficulties in the data collection process and design of pairwise dissimilarity measures, both of which significantly impact the behavior of clustering algorithms.\n\n\n\n\nIntuition about clustering often reflects the ideal case -- flawless data sets with well-suited dissimilarity between individual instances. In practice however, these cases are rare. Errors are introduced into a data set for a wide variety of reasons; from precision of instruments (a student's ruler to the Large Hadron Collider alike have a set precision), to human error when data is user-reported (common in the social sciences). Additionally, the dissimilarity between pairwise instances is often based on heuristic measures, particularly when non-numeric attributes are present. Furthermore, the dynamic nature of prominent clustering applications (such as personalization for recommendation systems) implies that by the time the data has been clustered, it has already changed. \n\n\nThe ubiquity of flawed input poses a serious challenge. If clustering is to operate strictly under the assumption of ideal data, its applicability would be reduced to fairly rare applications where such data can be attained. As such, it would be desirable for clustering algorithms to provide some qualitative guarantees about their output when partitioning noisy data. This leads us to explore whether there are any algorithms for which such guarantees can be provided. \n\n\nAlthough data can be faulty in a variety of ways, our focus here is on inaccuracies of pairwise distances. At a minimum, small perturbation to data should not radically affect the output of an algorithm. It would be natural to expect that some clustering techniques are more robust than others, allowing users to rely on perturbation robust techniques when pairwise distances are inexact.\n\n\n\nHowever, our investigation reveals that no reasonable clustering algorithm exhibits this desirable characteristic. In fact, both additive and multiplicative perturbation robustness are unrealistic requirements. We show that no clustering algorithm can satisfy robustness to perturbation without violating even more fundamental requirements. Not only do existing methods lack this desirable characteristic, but our findings also preclude the possibility of designing novel perturbation robust clustering methods. \n\nPerhaps it is already surprising that no reasonable clustering algorithm can be perfectly perturbation robust, but our results go further. Instead of requiring that the clustering remain unchanged following a perturbation, we allow up to \\emph{two-thirds} of all pairwise distances to change (from in-cluster to between-cluster, or vice-versa). It turns our that this substantial relaxation doesn't overcome our impossibility theorem.\n\n\n\n\n\nLuckily, further exploration paints a more optimistic picture. A careful examination of this issue requires a look back to the underlying goal of clustering, which is to discover clustering structure in data \\emph{when such structure is present}. Our investigation suggests that sensitivity to small perturbations is inevitable only on unclusterable instances, for which clustering is inherently ill-suited. As such, it can be argued that whether an algorithm exhibits robustness on such data is inconsequential. \n\nOn the other hand, we show that when data is endowed with inherent structure, existing methods can often successfully reveal that structure even on faulty (perturbed) data. We investigate the type of cluster structures required for the success of popular clustering techniques, showing that the robustness of $k$-means and related methods is directly proportional to the degree of inherent cluster structure. Similarly, we show that popular linkage-based techniques are robust when clusters are well-separated. Furthermore, different cluster structures are necessary for different algorithms to exhibit robustness to perturbations. \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Previous work}\n\nThis work follows a line of research on theoretical foundations of clustering. \nEfforts in the field began as early as the 1970s with the pioneering work of Wright~\\cite{Wright} on axioms of clustering, as well analysis of clustering properties  by Fisher et al~\\cite{fisher1971admissible} and Jardine et al~\\cite{Jardine}, among others. This field saw a renewed surge of activity following Kleinberg's~\\cite{Kleinberg} famous impossibility theorem, when he showed that no clustering function can simultaneously satisfy three simple properties. Also related to our work is a framework for selecting clustering methods based on differences in their input-output behavior~\\cite{NIPS2010,ackerman2011weighted,Jardine,Reza,COLT2010,AISTATS2013}\nas well as research on clusterability, which aims to quantify the degree of inherent cluster structure in data~\\cite{ben2015computational,AISTATS2009,Balcan,blum,Ostrovsky}.\n\n\n\n \n\n\nPrevious work on perturbation robustness studies it from a computational perspective by identifying new efficient algorithms for robust instances \\cite{Bilu,AISTATS2009,Awasthi}. Ben-David and Reyzin~\\cite{ben2014data} recently studied corresponding NP-hardness lower bounds. \n\nIn this paper, we take a fresh look at perturbation robustness. We begin our investigation by asking when perturbation robustness is possible to attain. After proving that robustness to perturbations cannot be achieved as a data-independent property of an algorithm, we seek to understand when popular clustering paradigms satisfy this requirement. Our analysis of established methods is an essential complement to efforts in algorithmic development, as the need for understanding established methods is amplified by the fact that most clustering users rely on a small number of well-known techniques. Our results demonstrate the type of cluster structures required for robustness of popular clustering paradigms. \n\n\\section{Definitions and notation}\n\n\nClustering is a wide and heterogeneous domain. For most of this paper, we focus on a basic sub-domain where the input to a clustering function is a finite set of points endowed with a between-points  dissimilarity function, and the number of clusters ($k$), and the output is a partition of that domain. \n\n\n\nA \\emph{dissimilarity function} is a symmetric function $d: X \\times X \\rightarrow\nR^+$, such that $d(x,x) = 0$ for all $x \\in X$. The data sets that we consider are pairs $(X,d)$, where $X$ is some finite domain set and $d$ is a dissimilarity function over $X$.\n\nA \\emph{$k$-clustering} $C = \\{C_1, C_2, \\ldots, C_k\\}$ of a data set $X$ is a\npartition of $X$ into $k$ disjoint subsets (or, clusters) of $X$ (so, $\\displaystyle \\bigcup_{i} C_i =\nX$). A \\emph{clustering} of $X$ is a $k$-clustering of $X$ for some $1 \\leq k\n\\leq |X|$.\n\nFor a clustering $C$, let $|C|$ denote the number of clusters in $C$ and $|C_i|$ denote the number of points in a cluster $C_i$. For a domain $X$, $|X|$ denotes the number of points in $X$, which we denote by $n$ when the domain is clear from context. We write $x \\sim_{\\mathcal{C}} y$ if $x$ and $y$ are both in some cluster $C_j$; and $x \\not\\sim_{\\mathcal{C}} y$ otherwise. This is an equivalence relation. \n\nThe \\emph{Hamming distance} between clusterings $C$ and $C'$ of the same domain set $X$ is defined by \n", "index": 1, "text": "\n\\[\n\\Delta(C, C') = \\frac{ |\\{\\{x,y\\} \\subset X \\mid (x \\sim_C y) \\oplus (x \\sim_{C'} y )\\}|}{\\binom{|X|}{2}},\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\Delta(C,C^{\\prime})=\\frac{|\\{\\{x,y\\}\\subset X\\mid(x\\sim_{C}y)\\oplus(x\\sim_{C^%&#10;{\\prime}}y)\\}|}{\\binom{|X|}{2}},\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2282</mo><mi>X</mi><mo>\u2223</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><msub><mo>\u223c</mo><mi>C</mi></msub><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2295</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><msub><mo>\u223c</mo><msup><mi>C</mi><mo>\u2032</mo></msup></msub><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mrow><mo stretchy=\"false\">|</mo><mi>X</mi><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></mfrac><mo>)</mo></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{definition}\n\nAdditive perturbation robustness is defined analogously, by replacing the $\\alpha$-multiplicative perturbation of the dissimilarity function with an $\\epsilon$-additive perturbation. \n\n\n\n\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=.75]{3body}\n\\end{center}\n\\caption{An illustration of the three-body rule. Based on the main objective of clustering, which is to group similar items together, this rule requires that whenever an algorithm is given exactly three points and the number of clusters is two, then it groups the two closest elements.}\n\\label{fig:3body}\n\\end{figure}\n\n\n\nDespite this substantial relaxation, perturbation robustness \\emph{for $\\delta$ as high as 2/3} is inherently incompatible with even more elementary requirements, as shown below. \n\n\\subsection{Impossibility theorem for clustering functions}\\label{k-impossibility}\n\nWe now proceed to show that perturbation robustness is too strong a requirement for clustering algorithms, and as such neither existing nor novel techniques can have this desirable characteristic. \n\nParticularly notable is that the impossibility results persists when $\\delta$ is as high as $2/3$, meaning that a perturbation is allowed to change up to two-thirds of all pairwise distances from in-cluster to between-cluster, or vise-versa. As such, we show that no reasonable clustering algorithm can preserve more than a third of its pairwise distances after a perturbation. \n\nThe following impossibility result derives from the pioneering work of Wright~\\cite{Wright} on axioms of clustering. Wright originally proposed his axioms in Euclidean space, here we generalize them for arbitrary pairwise dissimilarities.   \n\n\nThe first axiom we discuss follows from Wright's 11th axiom, and captures the very essence of clustering: to group similar items. This property considers an elementary scenario, requiring that given exactly three points, an algorithm asked for two clusters should group the two closest elements. See Figure \\ref{fig:3body} for an illustration. A special case of this rule occurs when the three elements lie on the real line, in which case the furthest endpoint should be placed in its own cluster. \n\n\\begin{definition}[Three-body rule]\nGiven a data set $X=\\{a,b,c\\}$, if $d(a,b)>d(b,c)$ and $d(a,c)>d(b,c)$, then $\\mathcal{F}(X,d,2) = \\{\\{a\\}, \\{b,c\\}\\}$. \n\\end{definition}\n\nWright's 6th axiom, and the final one we consider here, requires that replicating all data points by the same number should not change the clustering output. Outside of Euclidean space, we \\emph{replicate} a point $x$ by adding a new element $x'$ and setting $d(x',y) = d(x,y)$ for all $y \\in X$. \n \n\n\\begin{definition}[Replication invariance]\nGiven any positive integer $r$, if all points are replicated $r$ times,\nthen the partitioning of the original data is unchanged and all replicas lie in the same cluster as their original element. \n\\end{definition}\n\nNot only are these two axioms natural, as violating them leads to counterintuitive behavior, but they also hold for common techniques. It is easy to show that they are satisfied by common clustering paradigms, including cost-based methods such as $k$-means, $k$-median, and $k$-medoids, as well as linkage-based techniques, such as single-linkage, average-linkage and complete-linkage. \n\n\nWe now prove that no clustering function that satisfies the three-body rule and replication invariance can be perturbation robust. Furthermore, our result holds for all values of $\\delta \\leq 2/3$. Note that the following result applies to arbitrarily large data sets, for both multiplicative and additive perturbations.\n\n\n\\begin{theorem}\nFor any $\\delta \\leq 2/3$, $\\alpha > 1$, and $\\epsilon>0$, there is no clustering function that satisfies\n\\begin{enumerate}\n\\item  $(\\alpha, \\delta)$-multiplicative perturbation robustness, replication invariance, and the three-body rule, and\n\\item  $(\\epsilon, \\delta)$-additive perturbation robustness, replication invariance, and the three-body rule.\n\nFurther, the result holds for arbitrarily large data. \n\\end{enumerate}\n\\end{theorem}\n\\begin{proof}\nWe proceed by contradiction, assuming that there exists a clustering function $\\mathcal{F}$ that is replication invariant, adheres to the three-body rule, and is $(\\alpha, \\delta)$-multiplicative perturbation robust for some $\\delta \\leq 2/3$. \n\nConsider a data set $X=\\{a, b, c\\}$ with a distance function $d$ such that $d(b, c)<d(a, b)<d(a, c)$ and $d(a, b) = \\alpha d(b, c)$. By the three-body rule,  $\\mathcal{F}(X,d,2) = \\{\\{b, c\\}, \\{a\\}\\}$. We now replicate each point an arbitrary number of times, $r$, creating three sets $A, B, C$ such that all points that are replicas of the point $a$ and $a$ itself belong to $A$, all points that are replications of the point $b$ and $b$ itself belong to $B$, and similarly for $C$. By replication invariance, $\\mathcal{F}(A\\cup B\\cup C,d,2) = \\{B \\cup C, A\\}$.\n\n\nNext, we apply an $\\alpha$-multiplicative perturbation, creating distance function $d'$ such that $d'(a, b)<d'(b, c)<d'(a, c)$ and $d'(c, b) = \\alpha \\dot d'(b, a)$. By the three-body rule, $\\mathcal{F}(A\\cup B\\cup C,d',2) = \\{B \\cup A, C\\}$, and yet $(\\alpha, 2/3)$-multiplicative perturbation robustness requires that the Hamming distance between $\\mathcal{F}(A\\cup B\\cup C,d,2)$ and $\\mathcal{F}(A\\cup B\\cup C,d',2)$  must be less than $2/3$. But as the Hamming distance between $\\{B \\cup C, A\\}$ and $\\{B \\cup A, C\\}$ is exactly $2/3$, we reach a contradiction. \n\nFor additive perturbation, set $d$ so that  $d(b, c)<d(a, b)<d(a, c)$ and $d(a, b) =  d(b, c) + 0.5 \\epsilon$. By the three-body rule,  $\\mathcal{F}(X,d,2) = \\{\\{b, c\\}, \\{a\\}\\}$. As for the multiplicative case, we replicate each point $r$ times, creating three sets $A, B, C$. By replication invariance, $\\mathcal{F}(A\\cup B\\cup C,d,2) = \\{B \\cup C, A\\}$. We apply an $\\epsilon$-additive perturbation to make distance function $d'$ such that $d'(a, b)<d'(b, c)<d'(a, c)$ and $d'(c, b) = d'(b, a) + 0.5 \\epsilon$. By the three-body rule, $\\mathcal{F}(A\\cup B\\cup C,d',2) = \\{B \\cup A, C\\}$, and yet $(\\alpha, 2/3)$-additive perturbation robustness requires that the Hamming distance between $\\mathcal{F}(A\\cup B\\cup C,d,2)$ and $\\mathcal{F}(A\\cup B\\cup C,d',2)$  must be less than $2/3$, reaching a contradiction. \n\\end{proof}\n\n\n\n\nNote that the above result holds if the data is in Euclidean space.  This allows us to view perturbations as small movements in space, required to satisfy certain constraints such as the triangle inequality as well as adhering to the dissimilarity constraints required by Definitions~\\ref{multiplicativeperturbationdissimilarity} and \\ref{additiveperturbationdissimilarity}. See supplementary material for details. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Perturbation robustness as a property of data}\n\nThe above section demonstrates an inherent limitation of perturbation robustness as a property of clustering algorithms, showing that no reasonable clustering algorithm can exhibit this desirable characteristic. However, it turns out that perturbation robustness is possible to achieve when we restrict our attention to data endowed with inherent structure. \n\nAs such, perturbation robustness becomes a property of both an algorithm and a specific data set. We introduce a definition of perturbation robustness that directly addresses the underlying data.  \n\n\\begin{definition}[$(\\alpha, \\delta)$-multiplicative perturbation robustness of data]\nA data set $(X,d)$ satisfies \\emph{$(\\alpha, \\delta)$-multiplicative perturbation robustness} with respect to clustering function $\\mathcal{F}$ and $1 \\leq k \\leq |X|$, if for any $d'$ that is an $\\alpha$-multiplicative perturbation of $d$, \n", "itemtype": "equation", "pos": 12306, "prevtext": "\nwhere $\\oplus$ denotes the logical XOR operation.  \n\nThat is, the difference is the number of edges that disagree, being in-cluster in one of the clusterings and between-cluster in the other. The maximum distance between clusterings is when the Hamming distance is $1$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, we formally define clustering functions. \n\n\\begin{definition}[Clustering function]\\label{clustering function}\nA \\emph{clustering function} is a function $\\mathcal{F}$\nthat takes as input a pair $(X,d)$ and a parameter $1 \\leq k \\leq |X|$, and outputs a $k$-clustering of the domain $X$.\n\n\\end{definition}\n\n\\section{Perturbation robustness as a property of an algorithm}\\label{impossibility}\n\nWhenever a user is faced with the task of clustering faulty data, it would be natural to select an algorithm that is robust to perturbations of pairwise dissimilarities. As such, we begin our study of perturbation robustness by casting it as a property of an algorithm. If we could classify algorithms based on whether or not (or to what degree) they are perturbation robust, then clustering users could incorporate this information when making decisions regarding which algorithms to apply on their data. \n\nFirst, we define what it means to perturb a dissimilarity function.\n\n\\begin{definition}[$\\alpha$-multiplicative perturbation of a dissimilarity function]\\label{multiplicativeperturbationdissimilarity}\nGiven a pair of dissimilarity functions $d$ and $d'$ over a domain $X$, $d'$ is an \\emph{$\\alpha$-multiplicative-perturbation} of $d$, for $\\alpha>1$, if for all $x,y \\in X$, $\\alpha ^{-1} d(x,y)\\leq d'(x,y) \\leq \\alpha d(x,y)$.\n\\end{definition}\n\n\nAdditive perturbation of a dissimilarity function is defined analogously. \n\n\\begin{definition}[$\\epsilon$-additive perturbation of a dissimilarity function]\\label{additiveperturbationdissimilarity}\nGiven a pair of dissimilarity functions $d$ and $d'$ over a domain $X$, $d'$ is an \\emph{$\\epsilon$-additive perturbation} of $d$, for $\\epsilon>0$, if for all $x,y \\in X$, $d(x,y)-\\epsilon \\leq d'(x,y) \\leq d(x,y)+\\epsilon$.\n\\end{definition}\n\nIt is important to note that all of our results hold for both multiplicative and additive perturbation robustness. Perturbation robust algorithms should be invariant to data perturbations; that is, if data is perturbed, then the output of the algorithm shouldn't change. This view of perturbation robustness is not only intuitive, but is also based on previous formulations \\cite{Reyzin,Bilu,Awasthi} (This can be formalized as a property of clustering functions by setting $\\delta = 0$ in Definition~\\ref{mult-function} below). \n\n\n\n\nHowever, requiring that the partitioning be identical before and after perturbation is provably too strict a requirement for clustering algorithms, as it can only hold for functions that effectively ignore all pairwise distances. That is, this notion of perturbation robustness only holds for clustering functions that, given any domain set $X$ and integer $1 \\leq k \\leq |X|$, produce the same partitioning regardless of the setting of $d$ (See Section 1 of the Appendix for details). \n\nAs such, we introduce a relaxation that allows some error in the output of the algorithm on perturbed data. From a practical point of view, it is likely that a user who has only a perturbation of the true data set is likely to be satisfied with an approximately correct solution. \n\n\n\\begin{definition}\\label{mult-function}\nA clustering function $\\mathcal{F}$ is \\emph{$(\\alpha, \\delta)$-multiplicative perturbation robust} if, given any data set $(X,d)$ and $1 \\leq k \\leq |X|$, whenever $d'$ is an $\\alpha$-multiplicative perturbation of $d$,  \n", "index": 3, "text": "$$\\Delta(\\mathcal{F}(X,d,k), \\mathcal{F}(X,d',k))<\\delta.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\Delta(\\mathcal{F}(X,d,k),\\mathcal{F}(X,d^{\\prime},k))&lt;\\delta.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>\u03b4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{definition}\n\nAdditive perturbation robustness of data is defined analogously. \n\nThis perspective at perturbation robustness raises a natural question: On what types of data are algorithms perturbation robust? Next, we explore the type of structures that allow popular cost-based paradigms and linkage-based methods to uncover meaningful clusters even when data is faulty. \n\n\n\n\\subsection{Perturbation robustness of $k$-means, $k$-medoids, and min-sum}\n\nWe begin our study of data-dependent perturbation robustness by considering cluster structures required for perturbation robustness of some of the most popular clustering functions: $k$-means, $k$-medoids and min-sum. \n\nRecall that $k$-means~\\cite{steinley2006k} finds the clustering $C = \\{C_1,\\ldots,C_k\\}$ that minimizes $\\sum_{i=1} ^k \\sum_{x \\in C_i}d(x,c_i)^2,$ where $c_i$ is the center of mass of cluster $C_i$. An equivalent formulation that does not rely on centers of mass appears in \\cite{Ostrovsky}. A closely related clustering function is $k$-medoids, where centers are required to be part of the data. Formally, the $k$-medoids cost of $C$ is $\\sum_{i=1} ^k \\sum_{x \\in C_i}d(x,c_i),$ where $c_i \\in C_i$ is chosen to minimize the objective. Lastly, the min-sum~\\cite{sahni1976p} clustering function is the sum of all in-cluster distances, $\\sum_{i=1} ^k \\sum_{x, y \\in C_i}d(x, y)$. \n\nMany different notions of clusterability have been proposed in prior work~\\cite{AISTATS2009, ben2015computational}. Although they all aim to quantify the same tendency, it has been proven that notions of clusterability are often pairwise inconsistent~\\cite{AISTATS2009}. As such, care must be taken when selecting amongst them. \n\n\nIn order to analyze $k$-means and related functions, we turn our attention to an intuitive cost-based notion, which requires that clusterings of near-optimal cost be structurally similar to the optimal solution. That is, this notion characterizes clusterable data as that which has a unique optimal solution in a strong sense, by excluding the possibility of having radically different clusterings of similar cost.  \nSee Figure~\\ref{fig:UO} for an illustration.\n\nThis property, called ``uniqueness of optimum''\\footnote{This notion of clusterability appeared under several different names. The term ``uniqueness of optimum'' was coined by Ben-David~\\cite{ben2015computational}.} and closely related variations were investigated by \\cite{Balcan}, \\cite{Ostrovsky}, \\cite{agarwal2013k} and \\cite{AISTATS2013}, among others. See~\\cite{Balcan} for a detailed exposition.  \n\n\\begin{definition}[Uniqueness of optimum]\nGiven a clustering function $\\mathcal{F}$, a data set $(X,d)$ is \\emph{$(\\delta, c,c_0,k)$-uniquely optimal} if for every $k$-clustering $C$ of $X$ where $cost(C) \\leq c \\cdot cost(\\mathcal{F}(X,d,k))+c_0$, \n\n", "itemtype": "equation", "pos": 20067, "prevtext": "\n\\end{definition}\n\nAdditive perturbation robustness is defined analogously, by replacing the $\\alpha$-multiplicative perturbation of the dissimilarity function with an $\\epsilon$-additive perturbation. \n\n\n\n\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=.75]{3body}\n\\end{center}\n\\caption{An illustration of the three-body rule. Based on the main objective of clustering, which is to group similar items together, this rule requires that whenever an algorithm is given exactly three points and the number of clusters is two, then it groups the two closest elements.}\n\\label{fig:3body}\n\\end{figure}\n\n\n\nDespite this substantial relaxation, perturbation robustness \\emph{for $\\delta$ as high as 2/3} is inherently incompatible with even more elementary requirements, as shown below. \n\n\\subsection{Impossibility theorem for clustering functions}\\label{k-impossibility}\n\nWe now proceed to show that perturbation robustness is too strong a requirement for clustering algorithms, and as such neither existing nor novel techniques can have this desirable characteristic. \n\nParticularly notable is that the impossibility results persists when $\\delta$ is as high as $2/3$, meaning that a perturbation is allowed to change up to two-thirds of all pairwise distances from in-cluster to between-cluster, or vise-versa. As such, we show that no reasonable clustering algorithm can preserve more than a third of its pairwise distances after a perturbation. \n\nThe following impossibility result derives from the pioneering work of Wright~\\cite{Wright} on axioms of clustering. Wright originally proposed his axioms in Euclidean space, here we generalize them for arbitrary pairwise dissimilarities.   \n\n\nThe first axiom we discuss follows from Wright's 11th axiom, and captures the very essence of clustering: to group similar items. This property considers an elementary scenario, requiring that given exactly three points, an algorithm asked for two clusters should group the two closest elements. See Figure \\ref{fig:3body} for an illustration. A special case of this rule occurs when the three elements lie on the real line, in which case the furthest endpoint should be placed in its own cluster. \n\n\\begin{definition}[Three-body rule]\nGiven a data set $X=\\{a,b,c\\}$, if $d(a,b)>d(b,c)$ and $d(a,c)>d(b,c)$, then $\\mathcal{F}(X,d,2) = \\{\\{a\\}, \\{b,c\\}\\}$. \n\\end{definition}\n\nWright's 6th axiom, and the final one we consider here, requires that replicating all data points by the same number should not change the clustering output. Outside of Euclidean space, we \\emph{replicate} a point $x$ by adding a new element $x'$ and setting $d(x',y) = d(x,y)$ for all $y \\in X$. \n \n\n\\begin{definition}[Replication invariance]\nGiven any positive integer $r$, if all points are replicated $r$ times,\nthen the partitioning of the original data is unchanged and all replicas lie in the same cluster as their original element. \n\\end{definition}\n\nNot only are these two axioms natural, as violating them leads to counterintuitive behavior, but they also hold for common techniques. It is easy to show that they are satisfied by common clustering paradigms, including cost-based methods such as $k$-means, $k$-median, and $k$-medoids, as well as linkage-based techniques, such as single-linkage, average-linkage and complete-linkage. \n\n\nWe now prove that no clustering function that satisfies the three-body rule and replication invariance can be perturbation robust. Furthermore, our result holds for all values of $\\delta \\leq 2/3$. Note that the following result applies to arbitrarily large data sets, for both multiplicative and additive perturbations.\n\n\n\\begin{theorem}\nFor any $\\delta \\leq 2/3$, $\\alpha > 1$, and $\\epsilon>0$, there is no clustering function that satisfies\n\\begin{enumerate}\n\\item  $(\\alpha, \\delta)$-multiplicative perturbation robustness, replication invariance, and the three-body rule, and\n\\item  $(\\epsilon, \\delta)$-additive perturbation robustness, replication invariance, and the three-body rule.\n\nFurther, the result holds for arbitrarily large data. \n\\end{enumerate}\n\\end{theorem}\n\\begin{proof}\nWe proceed by contradiction, assuming that there exists a clustering function $\\mathcal{F}$ that is replication invariant, adheres to the three-body rule, and is $(\\alpha, \\delta)$-multiplicative perturbation robust for some $\\delta \\leq 2/3$. \n\nConsider a data set $X=\\{a, b, c\\}$ with a distance function $d$ such that $d(b, c)<d(a, b)<d(a, c)$ and $d(a, b) = \\alpha d(b, c)$. By the three-body rule,  $\\mathcal{F}(X,d,2) = \\{\\{b, c\\}, \\{a\\}\\}$. We now replicate each point an arbitrary number of times, $r$, creating three sets $A, B, C$ such that all points that are replicas of the point $a$ and $a$ itself belong to $A$, all points that are replications of the point $b$ and $b$ itself belong to $B$, and similarly for $C$. By replication invariance, $\\mathcal{F}(A\\cup B\\cup C,d,2) = \\{B \\cup C, A\\}$.\n\n\nNext, we apply an $\\alpha$-multiplicative perturbation, creating distance function $d'$ such that $d'(a, b)<d'(b, c)<d'(a, c)$ and $d'(c, b) = \\alpha \\dot d'(b, a)$. By the three-body rule, $\\mathcal{F}(A\\cup B\\cup C,d',2) = \\{B \\cup A, C\\}$, and yet $(\\alpha, 2/3)$-multiplicative perturbation robustness requires that the Hamming distance between $\\mathcal{F}(A\\cup B\\cup C,d,2)$ and $\\mathcal{F}(A\\cup B\\cup C,d',2)$  must be less than $2/3$. But as the Hamming distance between $\\{B \\cup C, A\\}$ and $\\{B \\cup A, C\\}$ is exactly $2/3$, we reach a contradiction. \n\nFor additive perturbation, set $d$ so that  $d(b, c)<d(a, b)<d(a, c)$ and $d(a, b) =  d(b, c) + 0.5 \\epsilon$. By the three-body rule,  $\\mathcal{F}(X,d,2) = \\{\\{b, c\\}, \\{a\\}\\}$. As for the multiplicative case, we replicate each point $r$ times, creating three sets $A, B, C$. By replication invariance, $\\mathcal{F}(A\\cup B\\cup C,d,2) = \\{B \\cup C, A\\}$. We apply an $\\epsilon$-additive perturbation to make distance function $d'$ such that $d'(a, b)<d'(b, c)<d'(a, c)$ and $d'(c, b) = d'(b, a) + 0.5 \\epsilon$. By the three-body rule, $\\mathcal{F}(A\\cup B\\cup C,d',2) = \\{B \\cup A, C\\}$, and yet $(\\alpha, 2/3)$-additive perturbation robustness requires that the Hamming distance between $\\mathcal{F}(A\\cup B\\cup C,d,2)$ and $\\mathcal{F}(A\\cup B\\cup C,d',2)$  must be less than $2/3$, reaching a contradiction. \n\\end{proof}\n\n\n\n\nNote that the above result holds if the data is in Euclidean space.  This allows us to view perturbations as small movements in space, required to satisfy certain constraints such as the triangle inequality as well as adhering to the dissimilarity constraints required by Definitions~\\ref{multiplicativeperturbationdissimilarity} and \\ref{additiveperturbationdissimilarity}. See supplementary material for details. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Perturbation robustness as a property of data}\n\nThe above section demonstrates an inherent limitation of perturbation robustness as a property of clustering algorithms, showing that no reasonable clustering algorithm can exhibit this desirable characteristic. However, it turns out that perturbation robustness is possible to achieve when we restrict our attention to data endowed with inherent structure. \n\nAs such, perturbation robustness becomes a property of both an algorithm and a specific data set. We introduce a definition of perturbation robustness that directly addresses the underlying data.  \n\n\\begin{definition}[$(\\alpha, \\delta)$-multiplicative perturbation robustness of data]\nA data set $(X,d)$ satisfies \\emph{$(\\alpha, \\delta)$-multiplicative perturbation robustness} with respect to clustering function $\\mathcal{F}$ and $1 \\leq k \\leq |X|$, if for any $d'$ that is an $\\alpha$-multiplicative perturbation of $d$, \n", "index": 5, "text": "$$\\Delta(\\mathcal{F}(X,d,k), \\mathcal{F}(X,d',k)) < \\delta.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\Delta(\\mathcal{F}(X,d,k),\\mathcal{F}(X,d^{\\prime},k))&lt;\\delta.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>\u03b4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{definition}\n\nWe show that whenever data satisfies the uniqueness of optimum notion of clusterability, $k$-means, $k$-medoids, and min-sum are perturbation robust. Furthermore, the degree of robustness depends on the extent to which the data is clusterable. \n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=0.32]{UO}\n\\end{center}\n\\caption{An illustration of the uniqueness of optimum notion of clusterability for two clusters. Consider $k$-means, $k$-medoids, or min-sum. The highly-clusterable data depicted in (a) has a unique optimal solution, with no structurally different clusterings of near-optimal cost. In contrast, (b) displays data with two radically different clusterings of near-optimal cost, making this data poorly-clusterable for $k=2$.}\n\\label{fig:UO}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the following proofs we will use $cost_d(C)$ to denote the cost of clustering $C$ with the distance function $d$. We now show the relationships between uniqueness of optimum and perturbation robustness for $k$-means. \n\n\n\n\n\\newpage\n\n\\begin{theorem}\nConsider the $k$-means clustering function and a data set $(X,d)$. If $(X,d)$ is \\emph{$(\\delta, c, c_0, k)$-uniquely optimal}, then it is also $(\\epsilon, \\delta,k)$-additive perturbation robust for all $\\epsilon <min(\\frac{c-1}{2},\\frac{- M + \\sqrt{ M ^2 + 4 M C_0 }} {2 M})$, where $M = \\binom{n}{2}$.\n\n\\end{theorem}\n\\begin{proof}\nConsider a data set $(X, d)$, and let $d'$ be any $\\epsilon$-additive perturbation of $d$. \n\nFirst, we argue that $cost_{d'}(\\mathcal{F}(X,d,k))$ is close to $cost_{d'}(\\mathcal{F}(X,d',k))$. Let $C = \\mathcal{F}(X,d,k)$.\nFirst, note that $cost_{d'}(\\mathcal{F}(X,d',k)) \\leq cost_{d'}(C)$. This is because $\\mathcal{F}$ finds the optimal solution on $(X,d')$, and so the clustering it selects can only have lower or equal to cost than the cost of $C$ when evaluated with $d'$. \n\nSo, we calculate the $k$-means cost of $C$ on $(X,d')$. The $k$-means objective function is equivalent to $\\sum_{i=1}^k \\frac{1}{|C_i|}\\sum_{x,y \\in C_i}d(x,y)^2$\\cite{Ostrovsky}. After an additive perturbation, any pairwise distance, $d(x, y)$, is bounded by $d(x, y) + \\epsilon$. In addition, the contribution of any in-cluster pairwise distance to the total cost of the clustering is proportional to the magnitude of the distance. It therefore follows\n\n\n \n\n\n\n\n\\begin{subequations}\n\\label{eq:leqList}\n\n", "itemtype": "equation", "pos": 22940, "prevtext": "\n\\end{definition}\n\nAdditive perturbation robustness of data is defined analogously. \n\nThis perspective at perturbation robustness raises a natural question: On what types of data are algorithms perturbation robust? Next, we explore the type of structures that allow popular cost-based paradigms and linkage-based methods to uncover meaningful clusters even when data is faulty. \n\n\n\n\\subsection{Perturbation robustness of $k$-means, $k$-medoids, and min-sum}\n\nWe begin our study of data-dependent perturbation robustness by considering cluster structures required for perturbation robustness of some of the most popular clustering functions: $k$-means, $k$-medoids and min-sum. \n\nRecall that $k$-means~\\cite{steinley2006k} finds the clustering $C = \\{C_1,\\ldots,C_k\\}$ that minimizes $\\sum_{i=1} ^k \\sum_{x \\in C_i}d(x,c_i)^2,$ where $c_i$ is the center of mass of cluster $C_i$. An equivalent formulation that does not rely on centers of mass appears in \\cite{Ostrovsky}. A closely related clustering function is $k$-medoids, where centers are required to be part of the data. Formally, the $k$-medoids cost of $C$ is $\\sum_{i=1} ^k \\sum_{x \\in C_i}d(x,c_i),$ where $c_i \\in C_i$ is chosen to minimize the objective. Lastly, the min-sum~\\cite{sahni1976p} clustering function is the sum of all in-cluster distances, $\\sum_{i=1} ^k \\sum_{x, y \\in C_i}d(x, y)$. \n\nMany different notions of clusterability have been proposed in prior work~\\cite{AISTATS2009, ben2015computational}. Although they all aim to quantify the same tendency, it has been proven that notions of clusterability are often pairwise inconsistent~\\cite{AISTATS2009}. As such, care must be taken when selecting amongst them. \n\n\nIn order to analyze $k$-means and related functions, we turn our attention to an intuitive cost-based notion, which requires that clusterings of near-optimal cost be structurally similar to the optimal solution. That is, this notion characterizes clusterable data as that which has a unique optimal solution in a strong sense, by excluding the possibility of having radically different clusterings of similar cost.  \nSee Figure~\\ref{fig:UO} for an illustration.\n\nThis property, called ``uniqueness of optimum''\\footnote{This notion of clusterability appeared under several different names. The term ``uniqueness of optimum'' was coined by Ben-David~\\cite{ben2015computational}.} and closely related variations were investigated by \\cite{Balcan}, \\cite{Ostrovsky}, \\cite{agarwal2013k} and \\cite{AISTATS2013}, among others. See~\\cite{Balcan} for a detailed exposition.  \n\n\\begin{definition}[Uniqueness of optimum]\nGiven a clustering function $\\mathcal{F}$, a data set $(X,d)$ is \\emph{$(\\delta, c,c_0,k)$-uniquely optimal} if for every $k$-clustering $C$ of $X$ where $cost(C) \\leq c \\cdot cost(\\mathcal{F}(X,d,k))+c_0$, \n\n", "index": 7, "text": "$$\\Delta(\\mathcal{F}(X,d,k), C) < \\delta.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\Delta(\\mathcal{F}(X,d,k),C)&lt;\\delta.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>\u03b4</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\nBy distributing the summation in the inequality \\ref{eq:leqList:leq4} we come to:\n\\begin{subequations}\n\\label{eq:sumExpansion}\n\n", "itemtype": "equation", "pos": 25365, "prevtext": "\n\\end{definition}\n\nWe show that whenever data satisfies the uniqueness of optimum notion of clusterability, $k$-means, $k$-medoids, and min-sum are perturbation robust. Furthermore, the degree of robustness depends on the extent to which the data is clusterable. \n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=0.32]{UO}\n\\end{center}\n\\caption{An illustration of the uniqueness of optimum notion of clusterability for two clusters. Consider $k$-means, $k$-medoids, or min-sum. The highly-clusterable data depicted in (a) has a unique optimal solution, with no structurally different clusterings of near-optimal cost. In contrast, (b) displays data with two radically different clusterings of near-optimal cost, making this data poorly-clusterable for $k=2$.}\n\\label{fig:UO}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the following proofs we will use $cost_d(C)$ to denote the cost of clustering $C$ with the distance function $d$. We now show the relationships between uniqueness of optimum and perturbation robustness for $k$-means. \n\n\n\n\n\\newpage\n\n\\begin{theorem}\nConsider the $k$-means clustering function and a data set $(X,d)$. If $(X,d)$ is \\emph{$(\\delta, c, c_0, k)$-uniquely optimal}, then it is also $(\\epsilon, \\delta,k)$-additive perturbation robust for all $\\epsilon <min(\\frac{c-1}{2},\\frac{- M + \\sqrt{ M ^2 + 4 M C_0 }} {2 M})$, where $M = \\binom{n}{2}$.\n\n\\end{theorem}\n\\begin{proof}\nConsider a data set $(X, d)$, and let $d'$ be any $\\epsilon$-additive perturbation of $d$. \n\nFirst, we argue that $cost_{d'}(\\mathcal{F}(X,d,k))$ is close to $cost_{d'}(\\mathcal{F}(X,d',k))$. Let $C = \\mathcal{F}(X,d,k)$.\nFirst, note that $cost_{d'}(\\mathcal{F}(X,d',k)) \\leq cost_{d'}(C)$. This is because $\\mathcal{F}$ finds the optimal solution on $(X,d')$, and so the clustering it selects can only have lower or equal to cost than the cost of $C$ when evaluated with $d'$. \n\nSo, we calculate the $k$-means cost of $C$ on $(X,d')$. The $k$-means objective function is equivalent to $\\sum_{i=1}^k \\frac{1}{|C_i|}\\sum_{x,y \\in C_i}d(x,y)^2$\\cite{Ostrovsky}. After an additive perturbation, any pairwise distance, $d(x, y)$, is bounded by $d(x, y) + \\epsilon$. In addition, the contribution of any in-cluster pairwise distance to the total cost of the clustering is proportional to the magnitude of the distance. It therefore follows\n\n\n \n\n\n\n\n\\begin{subequations}\n\\label{eq:leqList}\n\n", "index": 9, "text": "\\begin{align}\n \\label{eq:leqList:leq1} cost_{d'}(\\mathcal{F}(X,d', k)) & \\leq cost_{d'}(C)\\\\\n\\label{eq:leqList:leq2} &\\leq \\sum ^k _{i=1} \\frac{1}{|C_i|} \\sum _{\\{x, y\\} \\subseteq C_i} (d(x, y) + \\epsilon) ^2 \\\\\n\\label{eq:leqList:leq3} &\\leq \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i}\\frac{1}{|C_i|} (d(x, y) + \\epsilon)^2\\\\\n\\label{eq:leqList:leq4} &\\leq \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} \\left[ d(x, y)^2 + 2d(x, y)\\epsilon + \\epsilon ^2 \\right]\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle cost_{d^{\\prime}}(\\mathcal{F}(X,d^{\\prime},k))\" display=\"inline\"><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq cost_{d^{\\prime}}(C)\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum^{k}_{i=1}\\frac{1}{|C_{i}|}\\sum_{\\{x,y\\}\\subseteq C_{i}}(%&#10;d(x,y)+\\epsilon)^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}(%&#10;d(x,y)+\\epsilon)^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}%&#10;\\left[d(x,y)^{2}+2d(x,y)\\epsilon+\\epsilon^{2}\\right]\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo>[</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>+</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\nThe first term, \\ref{eq:sumExpansion:term1}, is equivalent to $cost_d(\\mathcal{F}(X, d, k))$. We deal with the second term, \\ref{eq:sumExpansion:term2}, by defining two sets $S_1$ and $S_2$. To define $S_1$, we first define $S_{1i}$. $S_{1i} = \\{ \\{x, y\\} \\subseteq C_i | d(x, y) > 1\\}$. Then $S_{1} = \\{S_{1i} | 1\\leq i \\leq k\\}$. Similarly $S_{2i} = \\{ \\{x, y\\} \\subseteq C_i | d(x, y) \\leq 1\\}$, and $S_2 = \\{S_{2i} | 1\\leq i \\leq k\\}$. \n\\begin{subequations}\n\\label{eq:dxy1}\n\n", "itemtype": "equation", "pos": 26001, "prevtext": "\n\\end{subequations}\nBy distributing the summation in the inequality \\ref{eq:leqList:leq4} we come to:\n\\begin{subequations}\n\\label{eq:sumExpansion}\n\n", "index": 11, "text": "\\begin{align}\n  \\label{eq:sumExpansion:term1} cost_{d'}(\\mathcal{F}(X, d', k))& \\leq \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} d(x, y)^2  \\\\\n\\label{eq:sumExpansion:term2} &+ \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)\\epsilon \\\\\n\\label{eq:sumExpansion:term3} &+  \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|}\\epsilon ^2\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle cost_{d^{\\prime}}(\\mathcal{F}(X,d^{\\prime},k))\" display=\"inline\"><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}d%&#10;(x,y)^{2}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2d(x%&#10;,y)\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}%&#10;\\epsilon^{2}\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\nBecause for all $\\{ x, y \\} \\in S_{1i}$ for all $1 \\leq i \\leq k$, $d(x, y) > 1$, we can square the  $d(x, y)$ value in term \\ref{eq:dxy1:1} while only increasing the total value. Likewise, we can replace the $d(x, y)$ value in term \\ref{eq:dxy1:2}  with 1 while only increasing the total value.\n\n\\begin{subequations}\n\\label{eq:dxy2}\n\n", "itemtype": "equation", "pos": 26893, "prevtext": "\n\\end{subequations}\nThe first term, \\ref{eq:sumExpansion:term1}, is equivalent to $cost_d(\\mathcal{F}(X, d, k))$. We deal with the second term, \\ref{eq:sumExpansion:term2}, by defining two sets $S_1$ and $S_2$. To define $S_1$, we first define $S_{1i}$. $S_{1i} = \\{ \\{x, y\\} \\subseteq C_i | d(x, y) > 1\\}$. Then $S_{1} = \\{S_{1i} | 1\\leq i \\leq k\\}$. Similarly $S_{2i} = \\{ \\{x, y\\} \\subseteq C_i | d(x, y) \\leq 1\\}$, and $S_2 = \\{S_{2i} | 1\\leq i \\leq k\\}$. \n\\begin{subequations}\n\\label{eq:dxy1}\n\n", "index": 13, "text": "\\begin{align}\n\\label{eq:dxy1:1} \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)\\epsilon & \\leq \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\in S_{1i}} \\frac{1}{|C_i|} 2d(x, y)\\epsilon \\\\\n\\label{eq:dxy1:2} &+ \\leq \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\in S_{2i}} \\frac{1}{|C_i|} 2d(x, y)\\epsilon\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2d(x,%&#10;y)\\epsilon\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\in S_{1i}}\\frac{1}{|C_{i}|}2d(x,y)\\epsilon\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2208</mo><msub><mi>S</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\in S_{2i}}\\frac{1}{|C_{i}|}2d(x,%&#10;y)\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mo>\u2264</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2208</mo><msub><mi>S</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mn>2</mn><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mi>\u03f5</mi></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\n\nSince $S_{1i}$ and $S_{2i}$ both consist of point pairs in $C_i$ and we are looking for an upper bound:\n\\begin{subequations}\n\\label{eq:dxy3}\n\n", "itemtype": "equation", "pos": 27557, "prevtext": "\n\\end{subequations}\nBecause for all $\\{ x, y \\} \\in S_{1i}$ for all $1 \\leq i \\leq k$, $d(x, y) > 1$, we can square the  $d(x, y)$ value in term \\ref{eq:dxy1:1} while only increasing the total value. Likewise, we can replace the $d(x, y)$ value in term \\ref{eq:dxy1:2}  with 1 while only increasing the total value.\n\n\\begin{subequations}\n\\label{eq:dxy2}\n\n", "index": 15, "text": "\\begin{align}\n\\label{eq:dxy2:1} \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)\\epsilon & \\leq   \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\in S_{1i}} \\frac{1}{|C_i|} 2d(x, y)^2\\epsilon \\\\\n\\label{eq:dxy2:2} & +  \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\in S_{2i}}  \\frac{1}{|C_i|} 2\\epsilon\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2d(x,%&#10;y)\\epsilon\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\in S_{1i}}\\frac{1}{|C_{i}|}2d(x,y%&#10;)^{2}\\epsilon\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2208</mo><msub><mi>S</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\in S_{2i}}\\frac{1}{|C_{i}|}2\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2208</mo><msub><mi>S</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\nNote that $\\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)^2\\epsilon$ is equivalent to $2\\epsilon cost_d(\\mathcal{F}(X, d)$.  \nWe can now return to the original inequality.\\\\\n\n\\begin{subequations}\n\\label{eq:sumExpansion3}\n\n", "itemtype": "equation", "pos": 28024, "prevtext": "\n\\end{subequations}\n\nSince $S_{1i}$ and $S_{2i}$ both consist of point pairs in $C_i$ and we are looking for an upper bound:\n\\begin{subequations}\n\\label{eq:dxy3}\n\n", "index": 17, "text": "\\begin{align}\n\\label{eq:dxy3:1} \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)\\epsilon & \\leq  \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)^2\\epsilon \\\\\n\\label{eq:dxy3:2} & + \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2\\epsilon\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2d(x,%&#10;y)\\epsilon\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2%&#10;d(x,y)^{2}\\epsilon\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\nWhile we cannot know the size of individual clusters in the general case we do know $\\frac{1}{|C_i|}$ is upper bounded by 1. Therefore we can substitute $\\frac{1}{|C_i|}$ with 1 in terms \\ref{eq:sumExpansion3:3} and \\ref{eq:sumExpansion3:4} while only increasing the value. For the same reasons we do not know the number of in-cluster point pairs in the general case in terms \\ref{eq:sumExpansion3:3} and \\ref{eq:sumExpansion3:4}. However we do know the number of in-cluster point pairs is bounded by the total number of point pairs, namely $\\binom{n}{2}$ which can be substituted in the same way while only increasing the value.\n\\begin{subequations}\n\\label{eq:sumExpansion4}\n\n", "itemtype": "equation", "pos": 28597, "prevtext": "\n\\end{subequations}\nNote that $\\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2d(x, y)^2\\epsilon$ is equivalent to $2\\epsilon cost_d(\\mathcal{F}(X, d)$.  \nWe can now return to the original inequality.\\\\\n\n\\begin{subequations}\n\\label{eq:sumExpansion3}\n\n", "index": 19, "text": "\\begin{align}\n\\label{eq:sumExpansion3:1} cost_{d'}(\\mathcal{F}(X, d', k)) & \\leq cost_d(\\mathcal{F}(X, d, k))  \\\\\n\\label{eq:sumExpansion3:2} & + 2\\epsilon cost_d(\\mathcal{F}(X, d, k)\\\\\n \\label{eq:sumExpansion3:3} &+ \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} 2\\epsilon\\\\\n\\label{eq:sumExpansion3:4} & + \\sum _{i=1} ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} \\epsilon ^2\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle cost_{d^{\\prime}}(\\mathcal{F}(X,d^{\\prime},k))\" display=\"inline\"><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq cost_{d}(\\mathcal{F}(X,d,k))\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+2\\epsilon cost_{d}(\\mathcal{F}(X,d,k)\" display=\"inline\"><mrow><mo>+</mo><mn>2</mn><mi>\u03f5</mi><mi>c</mi><mi>o</mi><mi>s</mi><msub><mi>t</mi><mi>d</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}2\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{i=1}^{k}\\sum_{\\{x,y\\}\\subseteq C_{i}}\\frac{1}{|C_{i}|}%&#10;\\epsilon^{2}\" display=\"inline\"><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2286</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\\end{subequations}\n\nTherefore we know: \n", "itemtype": "equation", "pos": 29699, "prevtext": "\n\\end{subequations}\nWhile we cannot know the size of individual clusters in the general case we do know $\\frac{1}{|C_i|}$ is upper bounded by 1. Therefore we can substitute $\\frac{1}{|C_i|}$ with 1 in terms \\ref{eq:sumExpansion3:3} and \\ref{eq:sumExpansion3:4} while only increasing the value. For the same reasons we do not know the number of in-cluster point pairs in the general case in terms \\ref{eq:sumExpansion3:3} and \\ref{eq:sumExpansion3:4}. However we do know the number of in-cluster point pairs is bounded by the total number of point pairs, namely $\\binom{n}{2}$ which can be substituted in the same way while only increasing the value.\n\\begin{subequations}\n\\label{eq:sumExpansion4}\n\n", "index": 21, "text": "\\begin{align}\n\\label{eq:sumExpansion4:1} cost_{d'}(\\mathcal{F}(X, d', k)) & \\leq cost_d(\\mathcal{F}(X, d, k))  \\\\\n\\label{eq:sumExpansion4:2} & + 2\\epsilon cost_d(\\mathcal{F}(X, d, k))\\\\\n \\label{eq:sumExpansion4:3} &+ \\binom{n}{2} 2\\epsilon\\\\\n\\label{eq:sumExpansion4:4} & + \\binom{n}{2} \\epsilon ^2\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle cost_{d^{\\prime}}(\\mathcal{F}(X,d^{\\prime},k))\" display=\"inline\"><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq cost_{d}(\\mathcal{F}(X,d,k))\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+2\\epsilon cost_{d}(\\mathcal{F}(X,d,k))\" display=\"inline\"><mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\binom{n}{2}2\\epsilon\" display=\"inline\"><mrow><mo>+</mo><mrow><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\binom{n}{2}\\epsilon^{2}\" display=\"inline\"><mrow><mo>+</mo><mrow><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac linethickness=\"0pt\"><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>)</mo></mrow><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05900.tex", "nexttext": "\n\nThen, $c\\geq 1+2 \\epsilon $, so $\\epsilon \\leq \\frac{c-1}{2}$. Similarly, $c_0 \\geq M(\\epsilon^2 + 2\\epsilon)$,  so $\\epsilon \\leq \\frac{- M + \\sqrt{ M ^2 + 4 M C_0 }} {2 M}$ where $M = \\binom{n}{2}$. So, $\\epsilon <min(\\frac{c-1}{2},\\frac{- M + \\sqrt{ M ^2 + 4 M C_0 }} {2 M})$. \n\\end{proof}\n\n\\begin{theorem}\nConsider the $k$-means clustering function and a data set $(X,d)$. If $(X,d)$ is \\emph{$(\\delta, c, c_0, k)$-uniquely optimal}, then it is also $(\\alpha, \\delta,k)$-multiplicative perturbation robust for all $\\alpha<\\sqrt{c}$.\n\n\n\\end{theorem}\n\n\\begin{proof}\nConsider a data set $(X,d)$, and let $d'$ be any $\\alpha$-multiplicative perturbation of $d$. \n\nFirst, we argue that $cost_{d'}(\\mathcal{F}(X,d, k))$ is close to $cost_{d'}(\\mathcal{F}(X,d', k))$. Let $C = \\mathcal{F}(X,d, k)$.\nFirst, note that $cost_{d'}(\\mathcal{F}(X,d', k)) \\leq cost_{d'}(C)$. This is because $\\mathcal{F}$ finds the optimal solution on $(X,d')$, and so the clustering it selects can only have lower cost than the cost of $C$ when evaluated with $d'$. \n\nSo, we calculate the cost of $C$ on $(X,d')$. The $k$-means cost function is bounded by $\\sum_{i=1}^k \\sum_{x,y \\in C_i} \\frac{1}{|C_i|} d(x,y)^2.$ After a multiplicative perturbation, the contribution of an edge of length $d(x, y)$, which used to contribute at most $d(x, y)^2$ to the cost of the function, contributes at most $(\\alpha \\cdot d(x, y))^2$, and so the contribution increases by at most a factor of $\\alpha ^2$. $cost_{d'}(\\mathcal{F}(X,d')) \\leq cost_{d'}(C) \\leq \\sum_i ^k \\sum _{\\{x, y\\} \\subseteq C_i} \\frac{1}{|C_i|} (\\alpha \\cdot d(x, y) )^2 \\leq \\alpha ^2 cost_{d}(C)$. \n\nThen, $c\\geq \\alpha ^2$, so $\\alpha \\leq \\sqrt{c}$.\n\\end{proof}\n\n\nThe proofs for $k$-medoid and min-sum follow similarly and are included in the appendix.\n\n\\subsection{Perturbation robustness of Linkage-Based algorithms}\n\nWe now move onto Linkage-Based algorithms, which in contrast to the methods studied in the previous section, do not seek to optimize an explicit objective function. Instead, they perform a series of merges, combining clusters according to their own measure of between-cluster distance. \n\nGiven clusters $A, B \\subseteq X$, the following are the between-cluster distances of some of the most popular Linkage-Based algorithms:\n\\begin{itemize}\n\\item \\textbf{Single linkage:} $\\min_{a \\in A, b \\in B} d(a,b)$\n\\item \\textbf{Average linkage:} $\\sum_{a\\in A, b \\in B} \\frac{d(a,b)}{(|A|\\cdot|B|)}$\n\\item \\textbf{Complete linkage:} $\\max_{a \\in A, b \\in B} d(a,b)$\n\\end{itemize}\n\nWe consider Linkage-Based algorithms with the $k$-stopping criterion, which terminate an algorithm when $k$ clusters remain, and return the resulting partitioning. \n\n\n\n\n\n\n\n\n\n\nBecause no explicit objective functions are used, we cannot rely on the uniqueness of optimum notion of clusterability. To define the type of cluster structure on which Linkage-Based algorithms exhibit perturbation robustness, we introduce a natural measure of clusterability based on a definition by Balcan et al~\\cite{blum}. The original notion required data to contain a clustering where every element is closer to all elements in its cluster than to all other points. This notion was also used in ~\\cite{ackerman2011weighted}, \\cite{Reyzin}, and \\cite{ackerman2014incremental}. See Figure~\\ref{fig:nice} for an illustration. \n\n\n\n\\begin{definition}[$(\\alpha,k)$-strictly separable]\nA data set $(X,d)$ is $(\\alpha,k)$-\\emph{strictly separable} if there exists a unique clustering $C = \\{C_1, \\ldots, C_k\\}$ of $X$ so that for all $i \\neq j$ and all $x,y \\in C_i$, $z \\in C_j$, $\\alpha d(x, y) \\leq d(x, z)$. \n\\end{definition}\n\nThe definition for $(\\epsilon,k)$-strictly additive separable is analogous.\n\n\\begin{definition}[$(\\epsilon,k)$-Strictly Additive Separable]\nA data set $(X,d)$ is $(\\epsilon,k)$-\\emph{Strictly Additive Separable} if there exists a unique clustering $C = \\{C_1, \\ldots, C_k\\}$ of $X$ so that for all $i \\neq j$ and all $x,y \\in C_i$, $z \\in C_j$, $ d(x, y) + \\epsilon \\leq d(x, z)$. \n\\end{definition}\n\nBefore moving on to our results for Linkage-Based algorithms, we show that the above notions of clusterability are not sufficient to show that data is perturbation robust for $k$-means and similar methods. This indicates that different algorithms require different cluster structures in order to exhibit perturbation robustness. We show this results for $(\\alpha, k)$-strictly separable data. The proof for $(\\epsilon,k)$-strictly additive separable data is in the supplementary material.\n\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=0.4]{multGroup}\n\\end{center}\n\\caption{An illustration of the data set in the proof of Theorem~\\ref{bigthm} for $k=3$. The original data consists of a dense region, ``cloud'' of points, and $k-1$ outliers. Each of the $A_i$s, $B_i$s and $C_i$s consist of $\\frac{n}{k^2}$ points. Before the perturbation, the data in the cloud is clustered $\\{\\{A_1\\cup B_1\\cup C_1\\}, \\{A_2\\cup B_2\\cup C_2\\}, \\{A_3\\cup B_3\\cup C_3\\}\\} $, whereas after  the perturbation it is partitioned as  $\\{\\{A_1\\cup A_2\\cup A_3\\}, \\{B_1\\cup B_2\\cup B_3\\}, \\{C_1\\cup C_2\\cup C_3\\}\\} $, leading to a large Hamming distance.}\n\n\\label{fig:multGroup}\n\\end{figure}\n\n\\begin{theorem}\\label{bigthm}\nLet $\\mathcal{F}$ be any one of $k$-means, $k$-medoids, or min-sum. Then for any $\\alpha >1, \\: \\delta < \\frac{2(k-1)n}{k^2 (n-1)}$ , there exists an $(\\alpha, k)$-strictly separable data set on which $\\mathcal{F}$ is not $(\\alpha, \\delta)$-multiplicative perturbation robust.\n\\end{theorem}\n\n\\begin{proof}\n\n\nWe construct such a data set $(X,d)$ such that there is one cloud of points densely packed together and  $k-1$ singleton points far away from all other points. Further, in this construction $n|k^2$ and $n>>k$.  The strictly separable clustering of $X$ will consist of the $k-1$ singleton points being separate clusters and  the cloud being in one cluster. \n\nArrange the cloud of points such that the ratio of the largest to smallest in-cloud distance is less than $\\alpha$, the ratio of the largest to smallest cloud point to singleton point distance is less than $\\alpha$, and all points are separated such that $\\mathcal{F}$ splits the cloud evenly into $k$ separate clusters and the singleton points go into separate clusters.\n\nBecause the ratio between the largest and smallest in-cloud distance is less than $\\alpha$, an $\\alpha$-multiplicative perturbation can radically change the structure of the points in the cloud. \nIf a point is identified by its distances to all other data, then perturbing the distance function can cause points to switch with one another. This ability to change applies similarly to the distance between cloud and singleton points. Because points can be arbitrarily made to act like other points, we perturb the data set such that the maximum number of in/between-cluster relationships are changed (with the restriction of never switching points from being in the cloud to being a singleton point and vice versa, because a multiplicative perturbation cannot necessarily make this switch).\n\n\n\nWe maximize the possible Hamming distance under the previous assumptions by constructing the following two data sets: First, divide the points of the cloud evenly into $k$ clusters. This is our first clustering. Then taking that clustering, re-cluster the points by grouping the points in each cluster into groups of $n/k^2$. Finally, form the new clusters by selecting one group from each previous cluster to be in a new cluster. See figure ~\\ref{fig:multGroup} for an example.\n\nTo find the Hamming distance between these two clusterings we first find the number of pairwise relationships that were formerly between-cluster that are now in-cluster. First, remember that a group contains $n/k^2$ points and there will be $k  \\binom{k}{2}$ group pairs that were formerly between-cluster that are now in cluster. Next, each point in a group will contribute $n/k^2$ to the Hamming distance per group pair. This gives the amount contributed to the Hamming distance by points relationships that were formerly between-cluster that are now in-cluster as $k  \\frac{n^2}{k^4}  \\binom{k}{2}$.\n\nWe now find the number of pairwise dissimilarities that were formerly in-cluster that are now between-cluster. Similar to before, each group contains $n/k^2$ points and there will be $k \\binom{k}{2}$ groups that were formerly in-cluster that are now between-cluster. This gives the total Hamming distance as $2k \\frac{n^2}{k^4}\\binom{k}{2}\\binom{n}{2}^{-1}$, which reduces to $\\frac{2(k-1)n}{k^2 (n-1)}$.\n\n\n\n\\end{proof}\n\n\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[scale=0.32]{nice}\n\\end{center}\n\\caption{An example of strictly separable data. Note that it may include clusters with different diameters, as long as the dissimilarity between any two clusters scales as the larger diameter of the two.}\n\\label{fig:nice}\n\\end{figure}\n\nWe now show that whenever data is strictly separable, then it is also perturbation robust with respect to some of the most popular Linkage-Based algorithms. An analogous result for additive perturbation robustness appears in the supplementary material. \n\n\n\\begin{theorem}\nSingle-Linkage, Average-Linkage, and Complete-Linkage are $(\\alpha,0)$-multiplicative perturbation robust on all $(\\alpha ^2 ,k)$-strictly separable  data sets. \n\\end{theorem}\n\\begin{proof}\n\nWe begin by showing that whenever data is $(1,k)$-strictly separable, then these Linkage-Based algorithms identify the underlying cluster structure. This result was previously shown for Single-Linkage~\\cite{blum} and Average-Linkage~\\cite{ackerman2011weighted}. We now prove this for complete-linkage. \n\nFirst, we introduce the concept of a \\emph{refinement}. A clustering $C'$ is a \\emph{refinement} of clustering $C^*$ if $C^*$ can be obtained by merging clusters in $C'$. The proof proceeds by induction on the number of iterations, showing that at each step of the algorithm, the current clustering is a refinement of the strictly separable $k$-clustering $C$. Since Linkage-Based algorithms start by placing each point in its own cluster, the clustering formed in the first step is a refinement of $C$. Assuming that the hypothesis holds at step $i$ of the algorithm, we show that it is retained  in the following step. Consider any $C_1$ and $C_2$ that are a subset of the same cluster in $C$, and any $C_3$ that is a subset of a different cluster in $C$. Let $(x,y) = argmax_{x\\in C_1, y\\in C_2}d(x,y)$. Then, the dissimilarity between $x$ and any point in $C_3$ is greater than $d(x,y)$ since the data is $(1,k)$-strictly separable. Then Complete-Linkage merges $C_1$ with $C_2$ before merging $C_1$ with $C_3$. \n\nLastly, observe that data that is $(\\alpha ^2 ,k)$-strictly separable is also  $(1,k)$-strictly separable, and remains so after an $\\alpha$-perturbation, as shown in  Lemma 1 in the appendix. It follows that single, average, and complete linkage are $(\\alpha,0)$-Multiplicative Perturbation Robust on $(\\alpha ^2,k)$-strictly separable  data. \n\\end{proof}\n\n\n\n\n\n\\vspace{-3mm}\n\\section{Conclusions}\n\\vspace{-2mm}\n\nAs a property of an algorithm, perturbation robustness fails in a strong sense, contradicting even more fundamental requirements of clustering functions. As such, no  algorithm can exhibit this desirable characteristic on all data sets. Notably, this result persists even if we allow two-thirds of all pairwise distance to change following a perturbation. \n\nHowever, a more optimistic picture emerges when considering clusterable data, and we show that popular  paradigms are able to discover some cluster structures even on faulty data. Further, different clustering techniques are perturbation robust on different cluster structures. This has important implications for the ``user's dilemma,'' which is the problem of selecting a suitable clustering algorithm for a given task. Faced with the challenge of clustering data with imprecise dissimilarities between pairwise entities, a user cannot simply elect to apply a perturbation robust technique as no such methods exist, and as such the selection of suitable methods calls for some insight on the underlying structure of the data. \n\nFuture work will investigate robustness of heuristics, such as Lloyd's method, for which preliminary analysis suggests that the cluster structure required for perturbation robustness depends on the method of initialization. \n\n\n\n\n\n\n\n\n\n\n\n\\newpage\n\\include{Appendix}\n\\newpage\n\\bibliographystyle{abbrvnat}\n\\bibliography{clustering}\n\n", "itemtype": "equation", "pos": 30048, "prevtext": "\n\\end{subequations}\n\nTherefore we know: \n", "index": 23, "text": "$$cost_{d'}(\\mathcal{F}(X, d', k)) \\leq (1 + 2\\epsilon)cost_d(\\mathcal{F}(X, d, k)) + \\binom{n}{2} (2\\epsilon + \\epsilon ^2)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"cost_{d^{\\prime}}(\\mathcal{F}(X,d^{\\prime},k))\\leq(1+2\\epsilon)cost_{d}(%&#10;\\mathcal{F}(X,d,k))+\\binom{n}{2}(2\\epsilon+\\epsilon^{2})\" display=\"block\"><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><msup><mi>d</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>d</mi><mo>\u2032</mo></msup><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mi>d</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mfrac linethickness=\"0pt\"><mi>n</mi><mn>2</mn></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>+</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]