[{"file": "1601.00372.tex", "nexttext": "\nWhile standard {{{\\textsc{Seq2Seq}}}\\xspace} models thus capture the unidirectional dependency from source to target, i.e., $p(y|x)$,\nthey ignore $p(x|y)$, the dependency from the target to the source,\nwhich has long been an important feature in phrase-based translation \n\\cite{OchNey02,shen2010string}.\nPhrase based systems that combine $p(x|y)$, $p(y|x)$ and other features\nlike sentence length yield significant performance boost. \n\n\n\n\n\n\n\n\nWe propose to incorporate this bi-directional dependency and model the maximum mutual information (MMI) between source and target\ninto  {{{\\textsc{Seq2Seq}}}\\xspace} models.\n\n\n\n \n\nAs \\newcite{li2015diversity} recently showed in the context of\nconversational response generation, \nthe MMI based objective function \n is equivalent\nto linearly combining $p(x|y)$ and $p(y|x)$.\nWith a tuning weight $\\lambda$, \nsuch a loss function can be written as :\n\n", "itemtype": "equation", "pos": 2213, "prevtext": "\n\\maketitle\n\n\n\\begin{abstract}\nSequence-to-sequence neural translation models\nlearn semantic and syntactic relations between sentence\npairs by optimizing the likelihood of the target given the source, i.e., $p(y|x)$,\nan objective that ignores other potentially useful sources of information.\nWe introduce an alternative objective function for neural MT that maximizes the mutual information \nbetween the source and target sentences,\nmodeling the bi-directional dependency of sources and targets.\nWe implement the model with a simple re-ranking method,\nand also introduce a decoding algorithm that increases diversity in the N-best\nlist produced by the first pass.\nApplied to the WMT German/English and French/English tasks,\nboth mechanisms offer a consistent performance boost\non both standard LSTM and attention-based neural MT architectures.\nThe result is the best published performance for\na single (non-ensemble) neural MT system,\nas well as the potential application of our diverse decoding algorithm to other NLP re-ranking tasks. \n\n\n\\end{abstract}\n\\section{Introduction}\nSequence-to-sequence models for machine translation ({{{\\textsc{Seq2Seq}}}\\xspace}) \\cite{sutskever2014sequence,bahdanau2014neural,cho2014learning,kalchbrenner2013recurrent,li2015hierarchical} are of growing interest  for their \ncapacity to learn semantic and\nsyntactic relations between sequence pairs, capturing contextual\ndependencies in a more continuous way than phrase-based SMT approaches. \n{{{\\textsc{Seq2Seq}}}\\xspace} models require minimal domain knowledge, can be trained\nend-to-end, have a much smaller memory footprint than the\nlarge phrase tables needed for phrase-based SMT,\nand achieve  state-of-the-art performance in large-scale tasks like English to French \\cite{luong2015addressing} and English to German  \\cite{luong2015effective,jean2014using} translation.\n\n{{{\\textsc{Seq2Seq}}}\\xspace} models are implemented as an encoder-decoder network, in which\na source sequence input $x$ is mapped (encoded) to a continuous  vector representation\nfrom which  a target output $y$ will be generated (decoded).\nThe framework \nis optimized through maximizing the log-likelihood of observing the paired output  $y$ given $x$:\n\n", "index": 1, "text": "\\begin{equation}\n\\text{Loss}= -\\log p(y|x)\n\n\\label{eqseq2seq}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\text{Loss}=-\\log p(y|x)\\par&#10;\" display=\"block\"><mrow><mtext>Loss</mtext><mo>=</mo><mo>-</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nBut as also discussed in \\newcite{li2015diversity}, direct decoding from \\eqref{eqseq2seq} \nis infeasible because computing $p(x|y)$ can't be done until the \ntarget has been computed\\footnote{\nAs demonstrated in \\cite{li2015diversity}\n\n", "itemtype": "equation", "pos": 3181, "prevtext": "\nWhile standard {{{\\textsc{Seq2Seq}}}\\xspace} models thus capture the unidirectional dependency from source to target, i.e., $p(y|x)$,\nthey ignore $p(x|y)$, the dependency from the target to the source,\nwhich has long been an important feature in phrase-based translation \n\\cite{OchNey02,shen2010string}.\nPhrase based systems that combine $p(x|y)$, $p(y|x)$ and other features\nlike sentence length yield significant performance boost. \n\n\n\n\n\n\n\n\nWe propose to incorporate this bi-directional dependency and model the maximum mutual information (MMI) between source and target\ninto  {{{\\textsc{Seq2Seq}}}\\xspace} models.\n\n\n\n \n\nAs \\newcite{li2015diversity} recently showed in the context of\nconversational response generation, \nthe MMI based objective function \n is equivalent\nto linearly combining $p(x|y)$ and $p(y|x)$.\nWith a tuning weight $\\lambda$, \nsuch a loss function can be written as :\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{aligned}\n\\hat{y}&={\\operatorname{arg\\,max}}_{y} \\log\\frac{p(x,y)}{p(x)p(y)^{\\lambda}} \\\\\n&= {\\operatorname{arg\\,max}}_{y} (1-\\lambda) \\log p(y|x)+\\lambda p(x|y) \n\n\\label{eqseq2seq}\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{y}\" display=\"inline\"><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{arg\\,max}}_{y}\\log\\frac{p(x,y)}{p(x)p(y)^{\\lambda}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mi>y</mi></msub><mo>\u2061</mo><mi>log</mi></mrow><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mi>\u03bb</mi></msup></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{arg\\,max}}_{y}(1-\\lambda)\\log p(y|x)+\\lambda p(x|%&#10;y)\\par&#10;\" display=\"inline\"><mrow><mo>=</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mi>y</mi></msub><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nEqu. \\ref{eqseq2seq} can be immediately achieved by applying bayesian rules \n\n", "itemtype": "equation", "pos": 3650, "prevtext": "\nBut as also discussed in \\newcite{li2015diversity}, direct decoding from \\eqref{eqseq2seq} \nis infeasible because computing $p(x|y)$ can't be done until the \ntarget has been computed\\footnote{\nAs demonstrated in \\cite{li2015diversity}\n\n", "index": 5, "text": "\\begin{equation}\n\\log\\frac{p(x,y)}{p(x)p(y)^\\lambda}=\\log p(y|x)-\\lambda p(y)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\log\\frac{p(x,y)}{p(x)p(y)^{\\lambda}}=\\log p(y|x)-\\lambda p(y)\" display=\"block\"><mrow><mi>log</mi><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mi>\u03bb</mi></msup></mrow></mfrac><mo>=</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03bb</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\n}. \n\nTo avoid this enormous search space, we propose\nto use a reranking approach to approximate\nthe mutual information between source and target\nin neural machine translation models.\nWe separately trained two {{{\\textsc{Seq2Seq}}}\\xspace} models, one for $p(y|x)$ and one for $p(x|y)$.\nThe $p(y|x)$ model is used to\ngenerate N-best lists from the source sentence $x$.  The lists are followed by a reranking process \nusing the second term of the\nobjective function, $p(x|y)$. \n\nBecause reranking approaches are dependent on having a diverse\nN-best list to rerank, we also propose a diversity-promoting decoding model\ntailored to neural MT systems. \nWe tested the  mutual information objective function and the diversity-promoting\ndecoding model on English$\\rightarrow$French, English$\\rightarrow$German\nand German$\\rightarrow$English  \n translation tasks,\nusing both standard LSTM settings and the more advanced Attention-model based settings\nthat have recently shown to result in higher performance.\n \nAs we will show, each of our two models\nyields a consistent performance boost on neural MT, \nand the combined system achieves\nwhat is to our knowledge the best published BLEU score from a single (non-ensemble) neural MT system.\n\nThe next section presents related work, followed by a background section 3 introducing \nLSTM/Attention machine translation models. Our proposed model will be described in detail in Sections 4,\nwith datasets and experimental results in Section 6 followed by conclusions.\n\n\\section{Related Work}\n\nThis paper draws on three prior lines of research: {{{\\textsc{Seq2Seq}}}\\xspace} models, modeling mutual information,\nand promoting translation diversity.\n\n\\paragraph{{{{\\textsc{Seq2Seq}}}\\xspace} Models} \n{{{\\textsc{Seq2Seq}}}\\xspace} models map source sequences to vector space representations, \nfrom which a target sequence is then generated. They yield good\nperformance in a variety of NLP generation tasks including \nconversational response generation\n\\cite{vinyals2015neural,serban2015building,li2015diversity}, and\nparsing \\cite{vinyals2014grammar}.\n\nA neural machine translation system \n\nuses distributed representations to\nmodel the conditional probability of targets given sources,\nusing two components, an encoder and a decoder. \n\nKalchbrenner and Blunsom \\shortcite{kalchbrenner2013recurrent} used an encoding model akin to convolutional networks for encoding\nand standard hidden unit recurrent nets for decoding.\nSimilar convolutional networks are used in \\cite{meng2015encoding} for encoding. \n\\newcite{sutskever2014sequence,luong2015effective} employed a stacking LSTM model for both encoding and decoding. \n\\newcite{bahdanau2014neural,jean2014using} adopted bi-directional recurrent nets for the encoder. \n\n\n\\paragraph{Maximum Mutual Information}\nMaximum Mutual Information (MMI) was introduced in speech recognition \\cite{bahl1986maximum}\nas a way of measuring\nthe mutual dependence between inputs (acoustic feature vectors) and outputs (words)\nand improving discriminative training \\cite{WoodlandPovey02}.\n\n\n\n\n\n\n\n\\newcite{li2015diversity} showed that MMI could solve\nan important problem in {{{\\textsc{Seq2Seq}}}\\xspace} conversational response generation.\nPrior {{{\\textsc{Seq2Seq}}}\\xspace} models tended to generate highly generic, dull responses (e.g., {\\em I don't know}) regardless of the inputs\n\\cite{sordoni2015neural,vinyals2015neural,serban2015survey}.  Li\net al. \\shortcite{li2015diversity} shows that modeling\nthe mutual dependency between messages and response promotes the\ndiversity of response outputs.\n\nOur goal, distinct from these previous uses of MMI,\nis to see whether the mutual information objective\nimproves translation by bidirectionally modeling source-target dependencies.\nIn that sense our work is designed to incorporate into {{{\\textsc{Seq2Seq}}}\\xspace} models\nfeatures that have proved useful in phrase-based MT,\nlike the reverse translation probability or sentence length\n\\cite{OchNey02,shen2010string}.\n\n\\paragraph{Generating Diverse Translations}\n\nVarious algorithms have been proposed for generated diverse translations in phrase-based MT,\n\nincluding compact representations like lattices and hypergraphs\n\\cite{macherey2008lattice,tromble2008lattice,kumar2004minimum},\n``traits'' like translation length \n\\cite{devlin2012trait}, bagging/boosting\n\\cite{xiao2013bagging}, or multiple systems \\cite{cer2013positive}.\n\\newcite{gimpel2013systematic,batra2012diverse}, \nproduce diverse N-best lists by adding a dissimilarity function\nbased on N-gram overlaps, distancing the current translation from already-generated ones\nby choosing translations that are highly-scoring but distinct from previous ones.\nWhile we draw on these intuitions, these\nexisting diversity promoting algorithms are tailored to phrase-based translation frameworks and not easily transplanted to neural MT decoding.  For example the \\cite{gimpel2013systematic} approach\ncan only be evaluated after translations are constructed.\nWe will propose an on-line algorithm that can \npromote diversity during beam search.\n\n\\section{Background: LSTM \\& Attention Models} \nNeural machine translation models map source $x=\\{x_1,x_2,...x_{N_x}\\}$ to a continuous  vector representation, from which \ntarget output $y=\\{y_1,y_2,...,y_{N_y}\\}$ is to be generated. \n\n\\subsection{LSTM Models}\n A long-short term memory model \\cite{hochreiter1997long}\n associates each time step with an input gate, a memory gate and an output gate, \ndenoted respectively as $i_t$, $f_t$ and $o_t$.\nLet $e_{t}$ denote the vector for the current word $w_t$, $h_t$ the vector computed by the LSTM model at time $t$ by combining $e_t$ and $h_{t-1}$.,\n$c_t$ the cell state vector at time $t$,\nand $\\sigma$ the sigmoid function. The vector representation $h_t$ for each time step $t$ is given by:\n\n\\begin{eqnarray}\ni_t=\\sigma (W_i\\cdot [h_{t-1},e_{t}])\\\\\nf_t=\\sigma (W_f\\cdot [h_{t-1},e_{t}])\\\\\no_t=\\sigma (W_o\\cdot [h_{t-1},e_{t}])\\\\\nl_t=\\text{tanh}(W_l\\cdot [h_{t-1},e_{t}])\\\\\nc_t=f_t\\cdot c_{t-1}+i_t\\cdot l_t\\\\\nh_{t}^s=o_t\\cdot \\text{tanh}(c_t)\n\\end{eqnarray}\nwhere $W_i$, $W_f$, $W_o$, $W_l \\in \\mathbb{R}^{K\\times 2K}$.\n\nThe LSTM defines a distribution over outputs $T$ and sequentially predicts tokens using a softmax function:\n\n", "itemtype": "equation", "pos": 3820, "prevtext": "\nEqu. \\ref{eqseq2seq} can be immediately achieved by applying bayesian rules \n\n", "index": 7, "text": "$$\\log p(y)=\\log p(y|x)+\\log p(x)-\\log p(x|y)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\log p(y)=\\log p(y|x)+\\log p(x)-\\log p(x|y)\" display=\"block\"><mrow><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nwhere $f(h_{t-1}, e_{y_t})$ denotes the activation function between $h_{t-1}$ and $e_{w_t}$, where $h_{t-1}$ is the representation output from the LSTM at time \\mbox{$t-1$}. \nEach sentence concludes with a special end-of-sentence symbol {{\\it EOS}\\xspace}. \nCommonly, the input and output each use different LSTMs with separate sets of compositional parameters to capture different compositional patterns. \nDuring decoding, the algorithm terminates when an {{\\it EOS}\\xspace} token is predicted.\n\\subsection{Attention Models}\nAttention models adopt a look-back strategy that\nlinks the current decoding stage with input time steps\nto represent which portions of the\ninput are most responsible for the current decoding state \\cite{xu2015show,luong2015addressing,bahdanau2014neural}. \n\nLet $H=\\{\\hat{h}_1,\\hat{h}_2,...,\\hat{h}_{N_x} \\}$ be the collection of hidden vectors outputted from LSTMs during encoding.  \nEach element in $H$ contains information\nabout the input sequences, focusing\non the parts surrounding each specific token. \nLet  $h_{t-1}$ be the LSTM outputs for decoding at time $t-1$. \nAttention models link the current-step decoding information, i.e., $h_{t}$ with each of the \nrepresentations at decoding step $\\hat{h}_{t'}$ using a weight variable $a_t$.\n$a_t$ can be constructed from different scoring functions such as\nthe {\\it  dot product} between the two vectors, i.e., $h_{t-1}^T\\cdot \\hat{h}_t$, a {\\it general} model akin to tensor operation i.e., $h_{t-1}^T\\cdot W\\cdot \\hat{h}_t$, and the {\\it concatenation} model by concatenating the two vectors i.e., $U^T\\cdot$\\text{tanh}$(W\\cdot [h_{t-1}, \\hat{h}_t]$). \nThe behavior of different attention scoring functions have been extensively studied in \\cite{luong2015effective}.\nFor all experiments in this paper, we adopt the {\\it general} strategy where the relevance score between \nthe current step of the decoding representation and the encoding representation is given by:\n\n", "itemtype": "equation", "pos": 10108, "prevtext": "\n}. \n\nTo avoid this enormous search space, we propose\nto use a reranking approach to approximate\nthe mutual information between source and target\nin neural machine translation models.\nWe separately trained two {{{\\textsc{Seq2Seq}}}\\xspace} models, one for $p(y|x)$ and one for $p(x|y)$.\nThe $p(y|x)$ model is used to\ngenerate N-best lists from the source sentence $x$.  The lists are followed by a reranking process \nusing the second term of the\nobjective function, $p(x|y)$. \n\nBecause reranking approaches are dependent on having a diverse\nN-best list to rerank, we also propose a diversity-promoting decoding model\ntailored to neural MT systems. \nWe tested the  mutual information objective function and the diversity-promoting\ndecoding model on English$\\rightarrow$French, English$\\rightarrow$German\nand German$\\rightarrow$English  \n translation tasks,\nusing both standard LSTM settings and the more advanced Attention-model based settings\nthat have recently shown to result in higher performance.\n \nAs we will show, each of our two models\nyields a consistent performance boost on neural MT, \nand the combined system achieves\nwhat is to our knowledge the best published BLEU score from a single (non-ensemble) neural MT system.\n\nThe next section presents related work, followed by a background section 3 introducing \nLSTM/Attention machine translation models. Our proposed model will be described in detail in Sections 4,\nwith datasets and experimental results in Section 6 followed by conclusions.\n\n\\section{Related Work}\n\nThis paper draws on three prior lines of research: {{{\\textsc{Seq2Seq}}}\\xspace} models, modeling mutual information,\nand promoting translation diversity.\n\n\\paragraph{{{{\\textsc{Seq2Seq}}}\\xspace} Models} \n{{{\\textsc{Seq2Seq}}}\\xspace} models map source sequences to vector space representations, \nfrom which a target sequence is then generated. They yield good\nperformance in a variety of NLP generation tasks including \nconversational response generation\n\\cite{vinyals2015neural,serban2015building,li2015diversity}, and\nparsing \\cite{vinyals2014grammar}.\n\nA neural machine translation system \n\nuses distributed representations to\nmodel the conditional probability of targets given sources,\nusing two components, an encoder and a decoder. \n\nKalchbrenner and Blunsom \\shortcite{kalchbrenner2013recurrent} used an encoding model akin to convolutional networks for encoding\nand standard hidden unit recurrent nets for decoding.\nSimilar convolutional networks are used in \\cite{meng2015encoding} for encoding. \n\\newcite{sutskever2014sequence,luong2015effective} employed a stacking LSTM model for both encoding and decoding. \n\\newcite{bahdanau2014neural,jean2014using} adopted bi-directional recurrent nets for the encoder. \n\n\n\\paragraph{Maximum Mutual Information}\nMaximum Mutual Information (MMI) was introduced in speech recognition \\cite{bahl1986maximum}\nas a way of measuring\nthe mutual dependence between inputs (acoustic feature vectors) and outputs (words)\nand improving discriminative training \\cite{WoodlandPovey02}.\n\n\n\n\n\n\n\n\\newcite{li2015diversity} showed that MMI could solve\nan important problem in {{{\\textsc{Seq2Seq}}}\\xspace} conversational response generation.\nPrior {{{\\textsc{Seq2Seq}}}\\xspace} models tended to generate highly generic, dull responses (e.g., {\\em I don't know}) regardless of the inputs\n\\cite{sordoni2015neural,vinyals2015neural,serban2015survey}.  Li\net al. \\shortcite{li2015diversity} shows that modeling\nthe mutual dependency between messages and response promotes the\ndiversity of response outputs.\n\nOur goal, distinct from these previous uses of MMI,\nis to see whether the mutual information objective\nimproves translation by bidirectionally modeling source-target dependencies.\nIn that sense our work is designed to incorporate into {{{\\textsc{Seq2Seq}}}\\xspace} models\nfeatures that have proved useful in phrase-based MT,\nlike the reverse translation probability or sentence length\n\\cite{OchNey02,shen2010string}.\n\n\\paragraph{Generating Diverse Translations}\n\nVarious algorithms have been proposed for generated diverse translations in phrase-based MT,\n\nincluding compact representations like lattices and hypergraphs\n\\cite{macherey2008lattice,tromble2008lattice,kumar2004minimum},\n``traits'' like translation length \n\\cite{devlin2012trait}, bagging/boosting\n\\cite{xiao2013bagging}, or multiple systems \\cite{cer2013positive}.\n\\newcite{gimpel2013systematic,batra2012diverse}, \nproduce diverse N-best lists by adding a dissimilarity function\nbased on N-gram overlaps, distancing the current translation from already-generated ones\nby choosing translations that are highly-scoring but distinct from previous ones.\nWhile we draw on these intuitions, these\nexisting diversity promoting algorithms are tailored to phrase-based translation frameworks and not easily transplanted to neural MT decoding.  For example the \\cite{gimpel2013systematic} approach\ncan only be evaluated after translations are constructed.\nWe will propose an on-line algorithm that can \npromote diversity during beam search.\n\n\\section{Background: LSTM \\& Attention Models} \nNeural machine translation models map source $x=\\{x_1,x_2,...x_{N_x}\\}$ to a continuous  vector representation, from which \ntarget output $y=\\{y_1,y_2,...,y_{N_y}\\}$ is to be generated. \n\n\\subsection{LSTM Models}\n A long-short term memory model \\cite{hochreiter1997long}\n associates each time step with an input gate, a memory gate and an output gate, \ndenoted respectively as $i_t$, $f_t$ and $o_t$.\nLet $e_{t}$ denote the vector for the current word $w_t$, $h_t$ the vector computed by the LSTM model at time $t$ by combining $e_t$ and $h_{t-1}$.,\n$c_t$ the cell state vector at time $t$,\nand $\\sigma$ the sigmoid function. The vector representation $h_t$ for each time step $t$ is given by:\n\n\\begin{eqnarray}\ni_t=\\sigma (W_i\\cdot [h_{t-1},e_{t}])\\\\\nf_t=\\sigma (W_f\\cdot [h_{t-1},e_{t}])\\\\\no_t=\\sigma (W_o\\cdot [h_{t-1},e_{t}])\\\\\nl_t=\\text{tanh}(W_l\\cdot [h_{t-1},e_{t}])\\\\\nc_t=f_t\\cdot c_{t-1}+i_t\\cdot l_t\\\\\nh_{t}^s=o_t\\cdot \\text{tanh}(c_t)\n\\end{eqnarray}\nwhere $W_i$, $W_f$, $W_o$, $W_l \\in \\mathbb{R}^{K\\times 2K}$.\n\nThe LSTM defines a distribution over outputs $T$ and sequentially predicts tokens using a softmax function:\n\n", "index": 9, "text": "\\begin{equation*}\n\\begin{aligned}\np(y|x)=\\prod_{t=1}^{n_T}\\frac{\\exp(f(h_{t-1},e_{y_t}))}{\\sum_{w'}\\exp(f(h_{t-1},e_{w'}))}\n\n\n\n\\end{aligned}\n\\label{equ-lstm}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle p(y|x)=\\prod_{t=1}^{n_{T}}\\frac{\\exp(f(h_{t-1},e_{y_{t}}))}{\\sum%&#10;_{w^{\\prime}}\\exp(f(h_{t-1},e_{w^{\\prime}}))}\\par&#10;\\par&#10;\\par&#10;\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>T</mi></msub></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><msub><mi>y</mi><mi>t</mi></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><msup><mi>w</mi><mo>\u2032</mo></msup></msub><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><msup><mi>w</mi><mo>\u2032</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nThe attention vector  is created by averaging weights over all input time-steps: \n\n", "itemtype": "equation", "pos": 12229, "prevtext": "\nwhere $f(h_{t-1}, e_{y_t})$ denotes the activation function between $h_{t-1}$ and $e_{w_t}$, where $h_{t-1}$ is the representation output from the LSTM at time \\mbox{$t-1$}. \nEach sentence concludes with a special end-of-sentence symbol {{\\it EOS}\\xspace}. \nCommonly, the input and output each use different LSTMs with separate sets of compositional parameters to capture different compositional patterns. \nDuring decoding, the algorithm terminates when an {{\\it EOS}\\xspace} token is predicted.\n\\subsection{Attention Models}\nAttention models adopt a look-back strategy that\nlinks the current decoding stage with input time steps\nto represent which portions of the\ninput are most responsible for the current decoding state \\cite{xu2015show,luong2015addressing,bahdanau2014neural}. \n\nLet $H=\\{\\hat{h}_1,\\hat{h}_2,...,\\hat{h}_{N_x} \\}$ be the collection of hidden vectors outputted from LSTMs during encoding.  \nEach element in $H$ contains information\nabout the input sequences, focusing\non the parts surrounding each specific token. \nLet  $h_{t-1}$ be the LSTM outputs for decoding at time $t-1$. \nAttention models link the current-step decoding information, i.e., $h_{t}$ with each of the \nrepresentations at decoding step $\\hat{h}_{t'}$ using a weight variable $a_t$.\n$a_t$ can be constructed from different scoring functions such as\nthe {\\it  dot product} between the two vectors, i.e., $h_{t-1}^T\\cdot \\hat{h}_t$, a {\\it general} model akin to tensor operation i.e., $h_{t-1}^T\\cdot W\\cdot \\hat{h}_t$, and the {\\it concatenation} model by concatenating the two vectors i.e., $U^T\\cdot$\\text{tanh}$(W\\cdot [h_{t-1}, \\hat{h}_t]$). \nThe behavior of different attention scoring functions have been extensively studied in \\cite{luong2015effective}.\nFor all experiments in this paper, we adopt the {\\it general} strategy where the relevance score between \nthe current step of the decoding representation and the encoding representation is given by:\n\n", "index": 11, "text": "\\begin{equation}\n\\begin{aligned}\n&v_{t'}=h_{t-1}^T\\cdot W\\cdot \\hat{h}_t\\\\\n&a_i=\\frac{\\exp(v_{t^*})}{\\sum_{t*}\\exp(v_{t^*})}\\\\\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle v_{t^{\\prime}}=h_{t-1}^{T}\\cdot W\\cdot\\hat{h}_{t}\" display=\"inline\"><mrow><msub><mi>v</mi><msup><mi>t</mi><mo>\u2032</mo></msup></msub><mo>=</mo><mrow><msubsup><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo>\u22c5</mo><mi>W</mi><mo>\u22c5</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle a_{i}=\\frac{\\exp(v_{t^{*}})}{\\sum_{t*}\\exp(v_{t^{*}})}\" display=\"inline\"><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><msup><mi>t</mi><mo>*</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>\u2063</mo><mo>*</mo></mrow></msub><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><msup><mi>t</mi><mo>*</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nAttention models predict subsequent tokens based on the combination of the last step outputted LSTM vectors $h_{t-1}$ and attention vectors $m_t$:\n\n", "itemtype": "equation", "pos": 12467, "prevtext": "\nThe attention vector  is created by averaging weights over all input time-steps: \n\n", "index": 13, "text": "\\begin{equation}\nm_t=\\sum_{t'\\in[1,N_S]}a_i \\hat{h}_{t'}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"m_{t}=\\sum_{t^{\\prime}\\in[1,N_{S}]}a_{i}\\hat{h}_{t^{\\prime}}\" display=\"block\"><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>t</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msub><mi>N</mi><mi>S</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></munder><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">^</mo></mover><msup><mi>t</mi><mo>\u2032</mo></msup></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nwhere $W_c \\in \\mathbb{R}^{K\\times 2K}$, $W_s\\in \\mathbb{R}^{V\\times K}$ with V denoting vocabulary size. \n\\cite{luong2015effective} reported a significant performance boost by integrating $\\vec{h}_{t-1}$ into the next step LSTM hidden state computation (referred to as the {\\it input-feeding} model), making LSTM compositions in decoding as follows:\n\n", "itemtype": "equation", "pos": 12686, "prevtext": "\nAttention models predict subsequent tokens based on the combination of the last step outputted LSTM vectors $h_{t-1}$ and attention vectors $m_t$:\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{aligned}\n&\\vec{h}_{t-1}=\\text{tanh}(W_c\\cdot [h_{t-1}, m_t])\\\\\n&p(y_t|y_<,x)=\\text{softmax}(W_s\\cdot\\vec{h}_{t-1})\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\vec{h}_{t-1}=\\text{tanh}(W_{c}\\cdot[h_{t-1},m_{t}])\" display=\"inline\"><mrow><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mtext>tanh</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>c</mi></msub><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>m</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle p(y_{t}|y_{&lt;},x)=\\text{softmax}(W_{s}\\cdot\\vec{h}_{t-1})\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mo>&lt;</mo></msub><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mi>s</mi></msub><mo>\u22c5</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nwhere $W_i$, $W_f$, $W_o$, $W_l \\in \\mathbb{R}^{K\\times 3K}$.\nFor the attention models implemented in this work, we adopt the {\\it input-feeding} strategy.\n \n\n \\subsection{Unknown Word Replacements}\nOne of the major issues in neural MT models is the computational complexity of the softmax function for target word prediction, which requires summing over all tokens in the vocabulary. \nNeural models tend to keep a shortlist of 50,00-80,000 most frequent words and use an unknown ({\\tt UNK}) token to represent all infrequent tokens, which significantly impairs BLEU scores. Recent work has proposed to deal with this issue:\n\\cite{luong2015addressing} adopt a post-processing strategy based on aligner from IBM models, while\n\\cite{jean2014using} approximates softmax functions by selecting a small subset of target vocabulary. \n\nIn this paper, we use a strategy similar to that of \\newcite{jean2014using}, thus avoiding the reliance \non external IBM model word aligner. \nFrom the attention models, we obtain word alignments from the training dataset, \nfrom which a bilingual dictionary is extracted. \nAt test time, we first generate target sequences. Once a translation is generated, we link the generated {\\tt UNK} tokens back to positions in the source inputs, and replace each {\\tt UNK} token with the  translation word of \nits correspondent source token using the pre-constructed dictionary. \n\nAs the unknown word replacement mechanism relies on automatic word alignment extraction which is not explicitly modeled in vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models, it can not be immediately applied to vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models.  However, since unknown word replacement can  be viewed as a post-processing technique, we can apply a pre-trained attention-model to any given translation. For {{{\\textsc{Seq2Seq}}}\\xspace} models, we first generate translations and replace {\\tt UNK} tokens within the translations using the pre-trained attention models to post-process the translations. \n\n\\section{Mutual Information via Reranking}\n\\begin{figure*}\n\\includegraphics[width=2in]{1.png}\n\\includegraphics[width=2.6in]{2.png}\n\\centering\n\\caption{Illustration of Standard Beam Search and proposed diversity promoting Beam Search. }\n\\end{figure*}\nAs discussed in \\newcite{li2015diversity},\ndirect decoding from \\eqref{eqseq2seq} is infeasible since  the second part, $p(x|y)$,\nrequires completely generating the target before it can be computed. \n\nWe therefore use an approximation approach:\n\\begin{enumerate}\n\\item Train $p(y|x)$ and $p(x|y)$ separately using vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models or Attention models.\n\\item Generate N-best lists from $p(y|x)$.\n\\item Rerank the N-best list by linearly adding $p(x|y)$. \n\\end{enumerate}\n\n\n\n\n\n\n\n\\subsection{Standard Beam Search for N-best lists} \n\nN-best lists  are generated using a beam search decoder with beam size set to 200 from $p(y|x)$ models.\nAs illustrated in Figure 1, at time step $t-1$ in decoding, we keep record of $K$ hypotheses based on score $S(Y_{t-1}|x)=\\log p(y_1,y_2,...,y_{t-1}|x)$. As we move on to time step $t$, \nwe expand each of the K hypotheses\n(denoted as $Y_{t-1}^k=\\{y_1^k,y_2^k,...,y_{t-1}^k\\}$, $k\\in [1,K]$), \n by selecting top $K$  of the translations, denoted as  $y_t^{k,k'}$, $k'\\in [1,K]$,  \n  leading to the construction of $K\\times K$ new hypotheses:\n  \n", "itemtype": "equation", "pos": 13205, "prevtext": "\nwhere $W_c \\in \\mathbb{R}^{K\\times 2K}$, $W_s\\in \\mathbb{R}^{V\\times K}$ with V denoting vocabulary size. \n\\cite{luong2015effective} reported a significant performance boost by integrating $\\vec{h}_{t-1}$ into the next step LSTM hidden state computation (referred to as the {\\it input-feeding} model), making LSTM compositions in decoding as follows:\n\n", "index": 17, "text": "\\begin{equation}\n\\begin{aligned}\n&i_t=\\sigma (W_i\\cdot [h_{t-1},e_{t},\\vec{h}_{t-1}])\\\\\n&f_t=\\sigma (W_f\\cdot [h_{t-1},e_{t},\\vec{h}_{t-1}])\\\\\n&o_t=\\sigma (W_o\\cdot [h_{t-1},e_{t},\\vec{h}_{t-1}])\\\\\n&l_t=\\text{tanh}(W_l\\cdot [h_{t-1},e_{t},\\vec{h}_{t-1}])\\\\\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle i_{t}=\\sigma(W_{i}\\cdot[h_{t-1},e_{t},\\vec{h}_{t-1}])\" display=\"inline\"><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{t}=\\sigma(W_{f}\\cdot[h_{t-1},e_{t},\\vec{h}_{t-1}])\" display=\"inline\"><mrow><msub><mi>f</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>f</mi></msub><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle o_{t}=\\sigma(W_{o}\\cdot[h_{t-1},e_{t},\\vec{h}_{t-1}])\" display=\"inline\"><mrow><msub><mi>o</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>o</mi></msub><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle l_{t}=\\text{tanh}(W_{l}\\cdot[h_{t-1},e_{t},\\vec{h}_{t-1}])\" display=\"inline\"><mrow><msub><mi>l</mi><mi>t</mi></msub><mo>=</mo><mrow><mtext>tanh</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>l</mi></msub><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>e</mi><mi>t</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">\u2192</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nThe score for each of the $K\\times K$ hypotheses is computed as follows:\n\n", "itemtype": "equation", "pos": 16854, "prevtext": "\nwhere $W_i$, $W_f$, $W_o$, $W_l \\in \\mathbb{R}^{K\\times 3K}$.\nFor the attention models implemented in this work, we adopt the {\\it input-feeding} strategy.\n \n\n \\subsection{Unknown Word Replacements}\nOne of the major issues in neural MT models is the computational complexity of the softmax function for target word prediction, which requires summing over all tokens in the vocabulary. \nNeural models tend to keep a shortlist of 50,00-80,000 most frequent words and use an unknown ({\\tt UNK}) token to represent all infrequent tokens, which significantly impairs BLEU scores. Recent work has proposed to deal with this issue:\n\\cite{luong2015addressing} adopt a post-processing strategy based on aligner from IBM models, while\n\\cite{jean2014using} approximates softmax functions by selecting a small subset of target vocabulary. \n\nIn this paper, we use a strategy similar to that of \\newcite{jean2014using}, thus avoiding the reliance \non external IBM model word aligner. \nFrom the attention models, we obtain word alignments from the training dataset, \nfrom which a bilingual dictionary is extracted. \nAt test time, we first generate target sequences. Once a translation is generated, we link the generated {\\tt UNK} tokens back to positions in the source inputs, and replace each {\\tt UNK} token with the  translation word of \nits correspondent source token using the pre-constructed dictionary. \n\nAs the unknown word replacement mechanism relies on automatic word alignment extraction which is not explicitly modeled in vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models, it can not be immediately applied to vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models.  However, since unknown word replacement can  be viewed as a post-processing technique, we can apply a pre-trained attention-model to any given translation. For {{{\\textsc{Seq2Seq}}}\\xspace} models, we first generate translations and replace {\\tt UNK} tokens within the translations using the pre-trained attention models to post-process the translations. \n\n\\section{Mutual Information via Reranking}\n\\begin{figure*}\n\\includegraphics[width=2in]{1.png}\n\\includegraphics[width=2.6in]{2.png}\n\\centering\n\\caption{Illustration of Standard Beam Search and proposed diversity promoting Beam Search. }\n\\end{figure*}\nAs discussed in \\newcite{li2015diversity},\ndirect decoding from \\eqref{eqseq2seq} is infeasible since  the second part, $p(x|y)$,\nrequires completely generating the target before it can be computed. \n\nWe therefore use an approximation approach:\n\\begin{enumerate}\n\\item Train $p(y|x)$ and $p(x|y)$ separately using vanilla {{{\\textsc{Seq2Seq}}}\\xspace} models or Attention models.\n\\item Generate N-best lists from $p(y|x)$.\n\\item Rerank the N-best list by linearly adding $p(x|y)$. \n\\end{enumerate}\n\n\n\n\n\n\n\n\\subsection{Standard Beam Search for N-best lists} \n\nN-best lists  are generated using a beam search decoder with beam size set to 200 from $p(y|x)$ models.\nAs illustrated in Figure 1, at time step $t-1$ in decoding, we keep record of $K$ hypotheses based on score $S(Y_{t-1}|x)=\\log p(y_1,y_2,...,y_{t-1}|x)$. As we move on to time step $t$, \nwe expand each of the K hypotheses\n(denoted as $Y_{t-1}^k=\\{y_1^k,y_2^k,...,y_{t-1}^k\\}$, $k\\in [1,K]$), \n by selecting top $K$  of the translations, denoted as  $y_t^{k,k'}$, $k'\\in [1,K]$,  \n  leading to the construction of $K\\times K$ new hypotheses:\n  \n", "index": 19, "text": "$$[Y_{t-1}^k, y_t^{k,k'}], k\\in [1,K], k'\\in [1,K]$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"[Y_{t-1}^{k},y_{t}^{k,k^{\\prime}}],k\\in[1,K],k^{\\prime}\\in[1,K]\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mi>k</mi></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>K</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo><mrow><msup><mi>k</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>K</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nIn a standard beam search model, the top $K$ \nhypotheses are selected\n(from the $K\\times K$  hypotheses computed in the last step) based on \nthe score $S(Y_{t-1}^k,y_t^{k,k'}|x)$. The remaining hypotheses are ignored as we proceed to the next time step. \n\nWe set a maximum length to 1.5 times the length of sources. \nAs decoding proceeds, sentences that are generated with a predicted {{\\it EOS}\\xspace} token are stored for later reranking. \n\\subsection{Generating a Diverse N-best List}\nUnfortunately, the N-best lists outputted from standard beam search are a poor surrogate \nfor the entire search space \\cite{finkel2006solving,huang2008forest}. \nThe beam search algorithm can only keep a small proportion of candidates in the search space and\nmost of the generated translations in N-best list are similar, differing  only by punctuation\nor minor morphological variations, with most of the words overlapping. \nBecause this lack of diversity in the N-best list will significantly decrease the impact of \nour reranking process,  it is important to find a way to  generate a more diverse N-best list.\n\nWe propose  to change\nthe way $S(Y_{t-1}^k,y_t^{k,k'}|x)$ is computed in an attempt to promote diversity,\nas shown in Figure 1. \nFor each of the  hypotheses $Y_{t-1}^k$ ($he$ and $it$),  \nwe generate the top $K$ translations, \n $y_t^{k,k'}$, $k'\\in [1,K]$ as in the standard beam search model.\nNext we  rank the $K$ translated tokens generated from the same parental hypothesis\nbased on $p(y_t^{k,k'}|x,Y_{t-1}^k)$ \nin descending order: {\\it he is} ranks the first among {\\it he is} and {\\it he has}, and {\\it he has} ranks second; \nsimilarly for {\\it it is} and {\\it it has}.\n \nNext we rewrite the score for $[Y_{t-1}^k, y_t^{k,k'}]$ by adding an additional part $\\gamma k'$, where $k'$ denotes the ranking of the current hypothesis among its siblings, \nwhich is first for {\\it he is} and {\\it it is}, second for {\\it he has} and {\\it it has}. \n\n", "itemtype": "equation", "pos": 16980, "prevtext": "\nThe score for each of the $K\\times K$ hypotheses is computed as follows:\n\n", "index": 21, "text": "\\begin{equation}\nS(Y_{t-1}^k,y_t^{k,k'}|x)=S(Y_{t-1}^k|x)+\\log p(y_t^{k,k'}|x,Y_{t-1}^k)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"S(Y_{t-1}^{k},y_{t}^{k,k^{\\prime}}|x)=S(Y_{t-1}^{k}|x)+\\log p(y_{t}^{k,k^{%&#10;\\prime}}|x,Y_{t-1}^{k})\" display=\"block\"><mrow><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></msubsup><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></msubsup><mo stretchy=\"false\">|</mo><mi>x</mi><mo>,</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nThe top $K$ hypothesis are selected based on $\\hat{S}(Y_{t-1}^k,y_t^{k,k'}|x)$ as we move on to the next time step. \nBy adding the additional term $\\gamma k'$, \nthe model punishes bottom ranked hypotheses among siblings (hypotheses descended from the same parent).\nWhen we compare newly generated hypotheses descended from different ancestors, the model gives more credit to  top hypotheses from each of different ancestors. \nFor instance, even though the original score for {\\it it is} is lower than {\\it he is},\nthe model favors the former as the latter is more severely punished by the intra-sibling ranking part $\\gamma k'$. \nThe model thus generally favors choosing hypotheses from diverse parents, leading to a more diverse N-best list. \n\n\nThe proposed model is straightforwardly implemented with minor adjustment to the standard beam search \nmodel\\footnote{Decoding for neural based MT model using large batch-size can be expensive resulted from softmax word prediction function. The proposed model is tailored to decoding in chunk using GPU, significantly speed up decoding process than other diversity fostering models tailored to phrase based MT systems. }. \n\nWe employ the diversity evaluation metrics in \\cite{li2015diversity}\nto evaluate the degree of diversity of the N-best lists:\ncalculating the average number of distinct unigrams {\\it  distinct-1} and bigrams {\\it  distinct-2} in the N-best list given each source sentence, \nscaled by the total number of tokens. \nBy employing the diversity promoting model with $\\gamma$ tuned from the development set based on BLEU score,  the value of {\\it  distinct-1}  increases from $0.54\\%$ to \n$0.95\\%$, and {\\it distinct-2} increases from $1.55\\%$ to $2.84\\%$ for English-German translation. Similar phenomenon are observed from English-French translation tasks and details are omitted for brevity. \n\n\\subsection{Reranking}\nThe generated N-best list is then reranked  by linearly combining $p(y|x)$ with $p(x|y)$. \nThe score of the source given each generated translation can be immediately computed from the previously trained $p(x|y)$. \nWe also consider an additional term that takes into account the length of targets \n(denotes as $L_T$) in decoding.\nWe thus linearly combine the three parts, making the final ranking score for a given target candidate $y$ as follows:\n \n", "itemtype": "equation", "pos": 19032, "prevtext": "\nIn a standard beam search model, the top $K$ \nhypotheses are selected\n(from the $K\\times K$  hypotheses computed in the last step) based on \nthe score $S(Y_{t-1}^k,y_t^{k,k'}|x)$. The remaining hypotheses are ignored as we proceed to the next time step. \n\nWe set a maximum length to 1.5 times the length of sources. \nAs decoding proceeds, sentences that are generated with a predicted {{\\it EOS}\\xspace} token are stored for later reranking. \n\\subsection{Generating a Diverse N-best List}\nUnfortunately, the N-best lists outputted from standard beam search are a poor surrogate \nfor the entire search space \\cite{finkel2006solving,huang2008forest}. \nThe beam search algorithm can only keep a small proportion of candidates in the search space and\nmost of the generated translations in N-best list are similar, differing  only by punctuation\nor minor morphological variations, with most of the words overlapping. \nBecause this lack of diversity in the N-best list will significantly decrease the impact of \nour reranking process,  it is important to find a way to  generate a more diverse N-best list.\n\nWe propose  to change\nthe way $S(Y_{t-1}^k,y_t^{k,k'}|x)$ is computed in an attempt to promote diversity,\nas shown in Figure 1. \nFor each of the  hypotheses $Y_{t-1}^k$ ($he$ and $it$),  \nwe generate the top $K$ translations, \n $y_t^{k,k'}$, $k'\\in [1,K]$ as in the standard beam search model.\nNext we  rank the $K$ translated tokens generated from the same parental hypothesis\nbased on $p(y_t^{k,k'}|x,Y_{t-1}^k)$ \nin descending order: {\\it he is} ranks the first among {\\it he is} and {\\it he has}, and {\\it he has} ranks second; \nsimilarly for {\\it it is} and {\\it it has}.\n \nNext we rewrite the score for $[Y_{t-1}^k, y_t^{k,k'}]$ by adding an additional part $\\gamma k'$, where $k'$ denotes the ranking of the current hypothesis among its siblings, \nwhich is first for {\\it he is} and {\\it it is}, second for {\\it he has} and {\\it it has}. \n\n", "index": 23, "text": "\\begin{equation}\n\\hat{S}(Y_{t-1}^k,y_t^{k,k'}|x)=S(Y_{t-1}^k,y_t^{k,k'}|x)-\\gamma k'\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\hat{S}(Y_{t-1}^{k},y_{t}^{k,k^{\\prime}}|x)=S(Y_{t-1}^{k},y_{t}^{k,k^{\\prime}}%&#10;|x)-\\gamma k^{\\prime}\" display=\"block\"><mrow><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></msubsup><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>S</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>Y</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>t</mi><mrow><mi>k</mi><mo>,</mo><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></msubsup><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b3</mi><msup><mi>k</mi><mo>\u2032</mo></msup></mrow></math>", "type": "latex"}, {"file": "1601.00372.tex", "nexttext": "\nWe applied grid search to achieve the combination value for $\\eta$, $\\lambda$ and $\\gamma$. \nHyperparameters are tuned via BLEU score \\cite{papineni2002bleu} on the development set. \n \n\\section{Experiments} \nOur models are trained on the WMT'14 training dataset containing 4.5 million pairs for English-German \nand German-English translation, and 12 million pairs for English-French translation.\nFor English-German translation, we limit our vocabularies to\nthe top 50K most frequent words for both languages. For English-French translation, we keep \nthe top 200K most frequent words for the source language and 80K for the target language. \nWords that are not in the vocabulary list are noted as the universal unknown token. \n\nFor the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU  \\cite{papineni2002bleu} on newstest2014 (2737) sentences. \nFor English-French translation, we concatenate news-test-2012 and news-test-2014\nto make a development set (6,003 pairs in total) and \nevaluate the models on news-test-2014 with 3,003 pairs\\footnote{As in \\cite{luong2015effective}. All texts are tokenized with tokenizer.perl and\nBLEU scores are computed with multi-bleu.perl}.\n\\begin{table*}[!ht]\n\\centering\n\\begin{tabular}{lll}\\hline\nLanguage Pairs&Model&BLEU scores \\\\ \\hline\nEnglish$\\rightarrow$German& Standard& 13.6 \\\\\nEnglish$\\rightarrow$German& Standard ({\\it mutual})& 15.0 (+1.4) \\\\\nEnglish$\\rightarrow$German& Standard ({\\it mutual+diversity})& 15.7 (+2.1) \\\\\\hline\\hline\nEnglish$\\rightarrow$German& Standard+{\\it UnkRep}& 14.6 \\\\\nEnglish$\\rightarrow$German& Standard ({\\it mutual})+{\\it UnkRep}& 15.7 (+1.1) \\\\\nEnglish$\\rightarrow$German& Standard ({\\it mutual+diversity})+{\\it UnkRep}& 16.6 (+2.0) \\\\\\hline\\hline\nEnglish$\\rightarrow$German& Attention+{\\it UnkRep}& 20.4  \\\\\nEnglish$\\rightarrow$German& Attention ({\\it mutual})+{\\it UnkRep}& 21.5  (+1.1)\\\\\nEnglish$\\rightarrow$German& Attention ({\\it mutual+diversity})+{\\it UnkRep}&22.1 (+1.7) \\\\\\hline\\hline\nEnglish$\\rightarrow$German& Buck et al., 2014& 20.7\\\\\\hline\nEnglish$\\rightarrow$German& Jean et al., 2015 (without {\\it ensemble})&19.4\\\\\nEnglish$\\rightarrow$German& Jean et al., 2015 (with {\\it ensemble})&21.6\\\\\\hline\nEnglish$\\rightarrow$German& Thang et al., 2015  (with {\\it UnkRep}, without {\\it ensemble})&20.9\\\\\nEnglish$\\rightarrow$German& Thang et al., 2015  (with {\\it UnkRep}, with {\\it ensemble})&23.0\\\\\\hline\\hline\n \\end{tabular}\n\\caption{BLEU scores from different models for on WMT14 English-German results. \n{\\it UnkRep} denotes applying unknown word replacement strategy. \n{\\it diversity} indicates diversity-promoting model for decoding being adopted. \nBaselines performances are reprinted from \\newcite{buck2014n,luong2015effective,jean2014using}. \n}\n\\label{tab1}\n\\end{table*}\n\\subsection{Training Details for $p(x|y)$ and $p(y|x)$}\nWe  trained neural models on Standard {{{\\textsc{Seq2Seq}}}\\xspace} Models and  Attention Models.\nWe trained $p(y|x)$ following the standard training protocols described in \\cite{sutskever2014sequence}.\n$p(x|y)$ is trained identically but with sources and targets swapped. \n\nWe adopt a deep structure with four LSTM\nlayers for encoding and four LSTM layers for decoding,\neach of which consists of a different set of parameters. \nWe followed the detailed protocols from \\newcite{luong2015effective}:\neach LSTM layer consists of 1,000 hidden\nneurons, and the dimensionality of word embeddings\nis set to 1,000. Other training details include: \nLSTM parameters and word embeddings are\ninitialized from a uniform distribution between [-0.1,0.1]; \nFor English-German translation, we run 12 epochs in total. \nAfter 8 epochs, we start halving the learning rate after each epoch;\nfor English-French translation, the total number of epochs is set to 8,\nand we start halving the learning rate after 5 iterations. \nBatch size is set to 128; gradient clipping is adopted by scaling gradients\nwhen the norm exceeded a threshold of 5. Inputs are reversed. \n\nOur implementation on a single GPU\\footnote{Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores.}  processes \napproximately 800-1200 tokens per second. \nTraining for the English-German dataset (4.5 million pairs) takes roughly 12-15 days.\nFor the French-English dataset, comprised of 12 million pairs, training takes roughly 4-6 weeks. \n\\begin{table*}[!ht]\n\\centering\n\\begin{tabular}{lll}\\hline\nLanguage Pairs&Model&BLEU scores \\\\ \\hline\nGerman$\\rightarrow$English& Standard+{\\it UnkRep}& 15.2 \\\\\nGerman$\\rightarrow$English& Standard ({\\it mutual+UnkRep})& 16.5 (+1.3) \\\\\nGerman$\\rightarrow$English& Standard ({\\it mutual+diversity+UnkRep})& 18.0 (+1.8) \\\\\\hline\\hline\nGerman$\\rightarrow$English& Attention+{\\it UnkRep}& 19.6  \\\\\nGerman$\\rightarrow$English& Attention ({\\it mutual})+{\\it UnkRep}& 20.8  (+1.2)\\\\\nGerman$\\rightarrow$English& Attention ({\\it mutual+diversity+UnkRep})&21.2 (+1.6) \\\\\\hline\\hline\n \\end{tabular}\n\\caption{BLEU scores from different models for on WMT'14 German-English results. }\n\\label{English-to-Geman}\n\\end{table*}\n\n\\begin{table*}[!ht]\n\\centering\n\\begin{tabular}{lll}\\hline\nLanguage Pairs&Model&BLEU scores \\\\ \\hline\nFrench$\\rightarrow$English& Standard& 29.4 \\\\\nFrench$\\rightarrow$English& Standard ({\\it mutual})& 31.0 (+1.6) \\\\\nFrench$\\rightarrow$English& Standard ({\\it mutual+diversity})& 32.2 (+2.8) \\\\\\hline\\hline\nFrench$\\rightarrow$English& Standard+{\\it UnkRep}& 31.2 \\\\\nFrench$\\rightarrow$English& Standard ({\\it mutual})+{\\it UnkRep}& 32.7 (+1.5) \\\\\nFrench$\\rightarrow$English& Standard ({\\it mutual+diversity})+{\\it UnkRep}& 33.7 (+2.5) \\\\\\hline\\hline\nFrench$\\rightarrow$English& Attention+{\\it UnkRep}& 33.6  \\\\\nFrench$\\rightarrow$English& Attention ({\\it mutual})+{\\it UnkRep}& 34.8  (+1.2)\\\\\nFrench$\\rightarrow$English& Attention ({\\it mutual+diversity})+{\\it UnkRep}&35.8 (+2.2) \\\\\\hline\\hline\nFrench$\\rightarrow$English&LSTM (Google) (without ensemble)&30.6 \\\\\nFrench$\\rightarrow$English&LSTM (Google) (with ensemble)&33.0 \\\\\\hline\\hline\nFrench$\\rightarrow$English&Luong {\\em et al.} (2015), {\\it UnkRep} (without ensemble) &32.7\\\\\nFrench$\\rightarrow$English&Luong {\\em et al.} (2015), {\\it UnkRep} (with ensemble) &37.5\\\\\\hline\\hline\n \\end{tabular}\n\\caption{BLEU scores from different models for on WMT'14 English-French results. Google\nis the LSTM-based model proposed in Sutskever {\\em et al.} (2014). Luong {\\em et al.} (2015) is the extension of Google models with unknown token replacements. }\n\\label{English-to-French}\n\\end{table*}\n\n\\subsection{English-German Results}\nResults for different models on WMT2014 are shown in Figure \\ref{tab1}. \nAs can be seen, the mutual information reranking models result in improved performance:\n+1.4 and +1.3 for standard {{{\\textsc{Seq2Seq}}}\\xspace} models without and with\nunknown word replacement, +0.9 for attention models. \nWe see\nthe benefit from our diverse N-best list by comparing  {\\it mutual+diversity} models with {\\it diversity} models.\nOn top of the improvements from standard beam search due to mutual information reranking, \nthe {\\it diversity} models introduce additional gains of +0.7, +0.9 and +0.6, \nleading the total gains roughly up to +2.0.  \nThe unknown token replacement technique yields significant gains,\nin line with observations from \\newcite{jean2014using,luong2015effective}. \n\nWe compare our English-German system with various others:\n(1)\n The winning system in WMT2014 \\cite{buck2014n} where language models were trained on a huge monolingual dataset.\n(2) The end-to-end \nneural\nMT system from \\newcite{jean2014using} using a large vocabulary size.\n(3) Models from \\newcite{luong2015effective} that combines different attention models. \nFor the models described in \\cite{jean2014using} and \\cite{luong2015effective},\nwe reprint their results from both the single model setting\nand the {\\it ensemble} setting, which a set of \n(usually 8) neural models that differ in  random initializations and the order of minibatches are trained, the combination of which jointly contributes in the decoding process.\nThe {\\it ensemble}  procedure is known to  result in improved performance\n\\cite{luong2015effective,jean2014using,sutskever2014sequence}.\n\nNote that the reported results from  the\nstandard {{{\\textsc{Seq2Seq}}}\\xspace} models and attention models in Table 1\n(those without considering mutual information) are \nfrom models  identical in structure to the corresponding\nmodels described in \\cite{luong2015effective}, and\nachieve similar performances (13.6 vs 14.0 for standard {{{\\textsc{Seq2Seq}}}\\xspace} models and 20.4 vs 20.7 for attention models). \n\nTo the best of our knowledge, our proposed mutual information model\nachieves the best published performance from a single neural model:\n+2.1 when compared with \\newcite{jean2014using} and +1.2 when compared\nwith \\newcite{luong2015effective}.\n(Due to time and computational constraints, we did not implement an ensemble mechanism,\nmaking our results incomparable to the ensemble mechanisms in these papers.)\n\n\\subsection{German-English Results}\nWe carried out similar set of experiments for the WMT'15 \nGerman to English\ntranslation task. Mutual information reranking again results in  gains in BLEU,\nas demonstrated in Table \\ref{English-to-Geman}. \nThe  mutual information model gives +1.3 and +0.9 performance gains, on top of which we obtain another boost of up to +0.5-0.7 form the {\\it diversity} decoding mechanism.\n\n\n\n\\subsection{French-English Results}\n\nResults from the WMT'14 French-English\ndatasets  are shown in Table~\\ref{English-to-French}, along with results\nreprinted from \\newcite{sutskever2014sequence,luong2015addressing}.\nWe again observe that applying mutual information\nyields better performance than the corresponding standard neural MT models.\n\nRelative to the English-German dataset, the English-French translation task shows\na larger gap between our new\nmodel and vanilla models where mutual information is not considered;\nour models respectively yield up to +2.8, +2.5, +2.2 boost in BLEU compared to\nstandard neural models without and with unknown word replacement,\nand Attention models.\n\n\\section{Discussion}\n\nIn this paper, we introduce a new objective for neural MT\nbased on the mutual dependency between the source and target sentences,\ninspired by recent work in neural conversation generation \\cite{li2015diversity}.\nWe build an approximate implementation of our model using reranking, and then\nto make reranking more powerful we introduce a new decoding\nmethod that promotes diversity in the first-pass N-best list. \n\n\nOn English$\\rightarrow$French and English$\\rightarrow$German\ntranslation tasks, we show that the neural machine\ntranslation models trained using the proposed\nmethod perform better than corresponding standard models,\nand that both the mutual information objective and the\ndiversity-increasing decoding methods contribute to the performance boost..\n\nThe new models come with the advantages of easy implementation with sources and targets interchanged,\nand of offering a general solution that can be integrated into any\nneural generation models with minor adjustments. \nIndeed, our diversity-enhancing decoder can be applied to \ngenerate more diverse N-best lists for any NLP reranking task.\n\nFinding a way to introduce mutual information based decoding directly into a first-pass\ndecoder without reranking naturally constitutes our future work. \n\n\n\n \n\n\n~~\\\\\n\\noindent {\\bf {\\large Acknowledgements}}\nWe would especially want to \nthank Thang Luong Minh for insightful discussions and releasing relevant code,\nas well as Will Monroe, Sida Wang, Chris Manning and other members from Stanford NLP group for helpful comments and suggestions. \nThe authors also want to thank Michel Galley, Bill Dolan, Chris Brockett, Jianfeng Gao \nand other members of the NLP group at Microsoft research \nfor helpful discussions. \nJiwei Li is very grateful to be supported\nby a Facebook Fellowship.\n\n\\bibliographystyle{acl2012}\n\\bibliography{MMI_MT}\n\n\n", "itemtype": "equation", "pos": 21465, "prevtext": "\nThe top $K$ hypothesis are selected based on $\\hat{S}(Y_{t-1}^k,y_t^{k,k'}|x)$ as we move on to the next time step. \nBy adding the additional term $\\gamma k'$, \nthe model punishes bottom ranked hypotheses among siblings (hypotheses descended from the same parent).\nWhen we compare newly generated hypotheses descended from different ancestors, the model gives more credit to  top hypotheses from each of different ancestors. \nFor instance, even though the original score for {\\it it is} is lower than {\\it he is},\nthe model favors the former as the latter is more severely punished by the intra-sibling ranking part $\\gamma k'$. \nThe model thus generally favors choosing hypotheses from diverse parents, leading to a more diverse N-best list. \n\n\nThe proposed model is straightforwardly implemented with minor adjustment to the standard beam search \nmodel\\footnote{Decoding for neural based MT model using large batch-size can be expensive resulted from softmax word prediction function. The proposed model is tailored to decoding in chunk using GPU, significantly speed up decoding process than other diversity fostering models tailored to phrase based MT systems. }. \n\nWe employ the diversity evaluation metrics in \\cite{li2015diversity}\nto evaluate the degree of diversity of the N-best lists:\ncalculating the average number of distinct unigrams {\\it  distinct-1} and bigrams {\\it  distinct-2} in the N-best list given each source sentence, \nscaled by the total number of tokens. \nBy employing the diversity promoting model with $\\gamma$ tuned from the development set based on BLEU score,  the value of {\\it  distinct-1}  increases from $0.54\\%$ to \n$0.95\\%$, and {\\it distinct-2} increases from $1.55\\%$ to $2.84\\%$ for English-German translation. Similar phenomenon are observed from English-French translation tasks and details are omitted for brevity. \n\n\\subsection{Reranking}\nThe generated N-best list is then reranked  by linearly combining $p(y|x)$ with $p(x|y)$. \nThe score of the source given each generated translation can be immediately computed from the previously trained $p(x|y)$. \nWe also consider an additional term that takes into account the length of targets \n(denotes as $L_T$) in decoding.\nWe thus linearly combine the three parts, making the final ranking score for a given target candidate $y$ as follows:\n \n", "index": 25, "text": "\\begin{equation}\n Score(y)=(1-\\lambda)p(y|x)+\\lambda p(x|y)+\\eta L_T\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"Score(y)=(1-\\lambda)p(y|x)+\\lambda p(x|y)+\\eta L_{T}\" display=\"block\"><mrow><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">|</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03b7</mi><msub><mi>L</mi><mi>T</mi></msub></mrow></math>", "type": "latex"}]