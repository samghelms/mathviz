[{"file": "1601.03288.tex", "nexttext": "\n\n\nInstead of calculating a distance between vectors, it is also possible to consider the vector as a probability distribution of tokens occurring in a corpus. Similarity between probability distributions can be calculated with e.g the Kullback-Leibler divergence (KL; \\cite{Kullback51}):\n\n\n", "itemtype": "equation", "pos": 19863, "prevtext": "\n\n\n\n\n\\title{Predicting the Effectiveness of Self-Training: Application to Sentiment Classification}\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Vincent~Van~Asch,\n        Walter~Daelemans\n\\thanks{Both authors are with CLiPS, University of Antwerp, Belgium}\n\\thanks{For contact information, see www.clips.uantwerpen.be}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nThe goal of this paper is to investigate the connection between the\nperformance gain that can be obtained by self-training and the\nsimilarity between the corpora used in this approach.  Self-training is a\nsemi-supervised technique designed to increase the performance of machine\nlearning algorithms by automatically classifying instances of a task and\nadding these as additional training material to the same classifier.\nIn the context of language processing tasks, this training material is\nmostly an (annotated) corpus. Unfortunately self-training does not always lead to a\nperformance increase and whether it will is largely unpredictable. We\nshow that the similarity between\ncorpora can be used to identify those setups for which\nself-training can be beneficial. We consider this research as a step\nin the process of developing a classifier that is able to adapt itself\nto each new test corpus that it is presented with.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\section{Introduction}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEPARstart{W}{hen} developing and testing techniques to increase\nthe performance of natural language processing systems, the choice of\nthe corpora to test the technique may influence the efficacy of the new\ntechnique. For this reason it is important to introduce a second dimension, \napart from the performance scores, that can describe the corpora that \nhave been selected. We have chosen to introduce similarity scores in a self-training\nsetup in order to identify those setups for which the self-training technique is useful.   \n\n\n\n\n\\subsection{Self-training procedure}\n\\label{sec:procedure}\n\nSelf-training is a semi-supervised machine learning method developed\nmainly to enhance the performance of a machine learner on corpora that\nare more dissimilar from the training corpus than one would prefer.\nWhen train and test data distributions are too different, models\ntrained on the training data will not generalize to the test data.\n\nThe self-training procedure is, in its plain form, a two-step\ntechnique. Variants of the self-training procedure include an instance\nselection step to increase the probability of adding only informative\ninstances to the training data. We do not address instance selection in this paper.\n\nThree corpora are needed for self-training: a labeled training corpus, a labeled test corpus, and an unlabeled additional corpus \\cite{Charniak97}. During self-training, a model is learned from the training data and it is applied to the unlabeled data.\nThus, the additional training data is created by automatically labeling unlabeled data. Next, the (partially incorrectly) labeled additional data is appended to the original training data (\\emph{self-training step 1}). \nThis first labeling step is followed by a second training phase using\nthe original training data plus the newly labeled data. The model\nresulting from this phase is then used to label the test data\n(\\emph{self-training step 2}). The expectation is that labeling the\ntest data in \\emph{self-training step 2} yields more correct labels\nthan simply labeling the test data depending only on the information\npresent in the original training data.\n\nIt remains controversial whether self-training is a useful method; it\nis not shown to lead to performance gain for every experimental setup.\nReference \\cite{Sagae10}  argues that self-training is only beneficial in\nthose situations for which the training and test data are sufficiently\ndissimilar, but other factors -- most obviously labeling accuracy of the unlabeled data -- may have an influence too. Rather than dismissing the self-training technique as dysfunctional, we want to identify those setups for which performance gain can be expected.\n\nSince self-training is a technique to enhance the performance in\nsituations of data sparseness, it can be linked to the notion of\ndomain adaptation. For an introduction to domain adaptation see \\cite{Daume06}. In a domain adaptation context the test corpus and the training corpus originate from different domains. Unfortunately, the term \\emph{domain} is a vague concept and defining a domain in an objective manner is a contentious task. Reference \\cite{DLee01}, drawing upon \\cite{Biber88} and the EAGLES initiative \\cite{Sinclair96}, mentions two types of parameters to categorize corpora: external (intended audience, purpose, and setting) and internal (lexical or grammatical (co)occurrence features). A domain would then be an external parameter, \\emph{viz.}\\ the subject field, like finances, mathematics, or linguistics.\n\nHowever, in machine learning, the only requirement of a domain is that\nthe underlying distribution of the words of a corpus coming from that\ndomain is different from the distribution of other domains. This\nbroadens the scope of a domain, but it still remains unclear how to\nexpress the difference between the distributions and, as a\nconsequence, there are no objective boundaries to a domain. In\npractice, domain corpora are straightforwardly gathered from different\nsubject fields, but it would be equally valid to collect texts using different registers, like newswire\ndata, e-mail correspondence and novels, all on the same subject.\n\nIrrespective of how domain corpora are collected, the basic assumption\nis that cross-domain machine learning will suffer from sub-optimal\ngeneralization.\n\nThe objective of this study is to investigate the foundations of a\nself-adapting classifier. Consider a system that has access to one\nlabeled training corpus and a collection of unlabeled corpora coming\nfrom varying domains. If an unseen, unlabeled text is presented to\nthis system, this system should be able to select from the unlabeled\ncorpora the corpus (or corpora) that would facilitate the labeling\nprocess most. In this manner, the classifier would be able to adapt itself to each newly presented text. In this paper, two assumptions of this scenario are tested: the assumption that it is possible to select an unlabeled corpus that increases the performance and the assumption that it is better to select an unlabeled corpus than it is to include all additional unlabeled data. The first assumption corresponds to the assumption that self-training can be helpful if a suitable combination of corpora is involved. \n\nTo be able to select the best-suited unlabeled corpus, the classifier in this study relies on similarity measures. A similarity measure expresses the similarity of two corpora and various implementations are available. Common implementations are unknown word ratio and Kullback-Leibler (see Section~\\ref{sec:simmeas}). Combining the concept of self-training and domain similarities may enable us to make a distinction between useful and harmful self-training setups. \n\nThe bulk of his paper tackles the question when self-training is helpful. It starts with an overview of related research (Section~\\ref{sec:rs}) followed by an overview of the different elements that are used in the experiments (Section~\\ref{sec:ede}), experimental results (Section~\\ref{sec:exps}), discussion and additional experiments (Section~\\ref{sec:dis}), and conclusions (Section~\\ref{sec:con}).\n\n\n\\section {Related research}\n\\label{sec:rs}\n The concepts of self-training were first introduced by \\cite{Charniak97}, but more recent examples are given by \\cite{Jiang07}, \\cite{McClosky10}, and \\cite{Sagae10}. There are many variations on self-training. Reference \\cite{Dong2011} combines the self-training concept with ensemble learning for citation classification. Thus, instead of using a single classifier that has to be trained, they use an ensemble of classifiers. Another interesting approach is the combination of self-training with active learning \\cite{Liu2013}. Instead of using the entire training set to label the unlabeled data, a portion is kept apart. This held-out data is labeled together with the unlabeled data in \\emph{self-training step 1}. The most confidently classified instances of the unlabeled data are added to the training corpus together with the least confidently labeled held-out data. The intention is to select useful data in a more rigorous manner.\n\nIt is possible to use similarities between corpora as such, using\ntheir outcome to draw inferences about them \\cite{Verspoor09,Biber10},\nbut an additional interesting usage involves applying similarities in a machine learning setup.\n\nSimilarities are used in natural language processing in various situations ranging from feature selection \\cite{Pietra97,Mitra02} and measuring the similarity between two language models \\cite{Lee01,Gao02} to training corpus creation \\cite{Chen09,McClosky10,Moore2010,Plank11}. An example of training corpus creation is presented by \\cite{Mansour09}. They employ the R\\'enyi divergence \\cite{Renyi61} to create a combination of different training corpora in order to increase the labeling performance for a specific test corpus.\n\nSimilarities have also been used to predict the performance of machine learners \\cite{Zhang09,Asch10}. A good example of such an application is the prediction of parsing accuracy \\cite{Ravi08}.\nAlthough there are issues when similarity measures are used (see Section~\\ref{sec:issues}), good results have been obtained for these tasks. Apart from the R\\'enyi divergence, some of the similarities that are used are perplexity, Kullback-Leibler divergence \\cite{Kullback51}, and the Skew divergence \\cite{Lee99}. \n\n\nDespite the fact that authors have shown that a similarity \\cite{Asch10,Plank11} or a linear combination of similarities \\cite{McClosky10} can be successfully used to link the similarity between domains to the performance of a natural language processing system, no consensus exists about which similarity or combination of similarities is best suited for the task. The best similarity is not selected on theoretical grounds but by testing a range of similarities and selecting the best one. For the research of\\cite{Yuan2005}, measuring the cross-entropy between two domains offered the best results when adapting a baseline language model to a new domain.\n\nThe bag-of-words approaches presented above are not the only manner of computing similarities automatically. It is possible to obtain a richer comparison of texts using the semantics. Current research focuses on semantic textual similarity (STS) \\cite{Agirre2013}. Most of these similarities draw from various sources like dependency parsing, part-of-speech tagging and/or latent Dirichlet allocation. An interesting software package in this context is the DKPro Similarity package \\cite{Bar2013}, which implements various semantic similarities in addition to less complex string matching similarities.  \n\n\nSimilarity measures have been used on a range of corpora and currently we know of two papers that carry out domain similarity research on the same corpus that we use \\cite{Ponomareva12,Remus12}.\n\n\n\n\nReference \\cite{Ponomareva12} shows that instead of exploiting the correlation\nbetween the test/training similarity and the accuracy, one could also\nuse the correlation between the similarity and the accuracy drop. The\naccuracy drop is the difference between the accuracy of an in-domain\nexperiment, using the test corpus, and the accuracy of a cross-domain\nexperiment. In this setup, the in-domain accuracy is considered to be\nan upper bound. Reference \\cite{Ponomareva12} also introduces the notion of domain complexity, which may be expressed by the percentage of \\emph{rare} words in a domain. Examining the corpus of \\cite{Blitzer07}, they observe that less complex source domains tend to give a smaller accuracy drop on more complex target domains. They prefer to use $\\chi^2$ in combination with inverse document frequencies (IDF) to measure domain similarity.  \n\nReference \\cite{Remus12} investigates instance selection for the corpus of \\cite{Blitzer07}. He strives for the identification of the most helpful instances using the Jensen-Shannon divergence. \n\n\n\\section{Experimental design}\n\\label{sec:ede}\n\n\\subsection{Definitions}\n\nThere are two levels of evaluation in this paper. The first level is the\nself-training experiment and the second level is the evaluation of how\nwell self-training gain can be predicted. For maximal clarity, some explicit definitions are given here first.\n\n\n\\textbf{similarity} A number expressing the degree to which two corpora are similar. The higher this number, the less similar the corpora are.\\footnote{Because higher values express less similarity this is sometimes called \\emph{distance}. However, the term distance entails certain mathematical properties that we do not demand from a similarity measure.} The exact meaning of \\emph{similar} depends on the similarity measure that is used.\\\\\n\\indent \\textbf{labeling experiment} A labeling experiment consists of labeling instances. Often this involves a training phase using a labeled training corpus and a test phase during which the unlabeled instances of the test corpus are assigned a label. This is a standard classification experiment. We use the term \\emph{labeling} to be able to differentiate this experiment from the second level classification experiment that involves the classification of self-training setups according to the self-training gain. \\\\\n\\indent \\textbf{labeling performance} The performance of a labeling experiment. Labeling performance can be quantified using accuracy, precision, recall, F-score, etc. and these scores are calculated using the gold standard of the test data.\n\\\\ \\indent \\textbf{self-training experiment} Each self-training experiment is linked to a particular setup involving three corpora: a labeled training corpus, a labeled test corpus and unlabeled additional data. This is an experiment as described in Section~\\ref{sec:procedure}.\n\\\\ \\indent \\textbf{self-training gain} The performance gain obtained when the labeling performance for a given test and training set is compared with and without the introduction of self-training. The inverse is self-training loss.\n\\\\ \\indent\\textbf{self-training gain prediction} Apart from the labeling experiments, which are the basis of the self-training experiments, this is the second type of classification experiment discussed in this paper. It can be regarded as a \\emph{second level} experiment, meaning that first a range of self-training experiments is conducted before self-training gain is predicted. The classification  consists of separating the setups that lead to self-training gain from the others.\n\\\\ \\indent \\textbf{prediction performance} The performance of self-training gain prediction. A self-training experiment counts as a true positive if it is correctly predicted to result in self-training gain. Prediction performance can be quantified using accuracy, precision, recall, F-score, etc.\n\n\n\\subsection{Corpus and labeling task}\n\nSelf-training can be applied to every supervised machine learning problem. In this paper, the labeling task consists of a binary sentiment classification task. The goal is to label an instance based on a product review according to the sentiment expressed in the review: Is the review favorable to the product or not? \n\n\n\nThe instances for this task are bag-of-word instances coming from the sentiment classification corpus of \\cite{Blitzer07}.\\footnote{Multi-Domain Sentiment Dataset (v.\\ 2.0) in April 2013 retrieved from\\\\ \\url{www.cs.jhu.edu/~mdredze/datasets/sentiment/unprocessed.tar.gz} .} The main reason why we chose this corpus is that it contains data for various domains which is labeled for the same task.\n\nTo minimize corpus size effects, the corpora for the different domains are normalized to a size of 2,500 instances. We chose this number as a trade-off between sufficient corpus size and sufficient number of domains that have more than 2,500 instances to sample from.   \nIn the end, 13 domains meet the corpus size constraints: \\emph{beauty, baby, camera \\& photo, sports \\& outdoors, health \\& personal care, apparel, toys \\& games, video, kitchen \\& housewares, electronics, dvd, books, and music}.\n\nAfter randomly sampling the instances, a script is used to convert the instances into a format fit for the machine learner, i.e.\\ SVMLight v6.02 \\cite{Joachims99}. The machine learner is used with default settings.\n\nFor each self-training experiment, three different corpora are needed. With 13 domains, a total of 1,716 distinct setups ($= 13 \\times 12 \\times 11$) are conceivable. Of these 1,716 self-training experiments, 94\\% of the setups lead to performance loss. It is clear that with only 106 setups in the positive class, self-training -- with little additional data -- is more often detrimental to performance than it is helpful. This illustrates that being able to identify setups leading to decreased accuracy can be of help when the use of  self-training is considered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe briefly discuss a few general observations involving the data. When carrying out regular cross-domain labeling experiments, the average macro-averaged F1-score is 60.61\\%, equalling an accuracy of 85.44\\%. Although the experiments are not directly comparable, the cross-domain accuracy is in the same region as reported by \\cite{Blitzer07} meaning that the machine learner is not underperforming. \n\n\n\n\nCarrying out the 1,716 self-training experiments results in an average F1-score loss of 3.5\\%. If performance increases, an average of 0.6\\% is added to the score. \nThe differences in F1-score are rather small, but for the best self-training setups the difference is statistically significant at the 1\\% confidence level.\\footnote{Using approximate randomization testing \\cite{Noreen89,Yeh00}.} \nAlso note that in a regular self-training setup, considerably more additional unlabeled data is added, which may lead to more self-training gain. We conducted a small experiment with dvd (training), video (test), and apparel as additional data. Using all apparel data (8,940 instances instead of 2,500) leads to a labeling performance F-score increase of 4.98\\% instead of 4.29\\%. This small experiment shows that adding more additional data, as one would normally do, may indeed lead to a higher self-training gain. Although the self-training gain in our experiments is rather small, more data may lead to more gain thus making the selection of the right corpora more relevant.\n\n\n\\subsection{Similarity measures}\n\\label{sec:simmeas}\n\nAn important issue is how the similarity between corpora can be measured. The features of the corpus of \\cite{Blitzer07} consist of tokens. \nAn instance can be considered a bag-of-words and can be converted into a vector. The values in the vector indicate whether a given token occurs in the sample text or not. In this manner, an entire corpus can be reduced to a single vector, namely the centroid of all instance-based vectors in that corpus. If we have a vector for the test corpus and one for the training corpus, for example, cosine similarity between the two domain vectors can be computed. During the experiments, the cosine similarity and the Euclidean distance between the two vectors are computed. To make the similarity independent of sample size, the actual values in the corpus-based vectors are not the raw counts but the point-wise mutual information (pmi) values. Point-wise mutual information also smoothes down the influence of large token count differences. Given the two raw count vectors of the two corpora that are compared, the pmi-value of token $t$ in vector $v$ becomes: \n\n\n", "index": 1, "text": "\\begin{align}\npmi(t,v) &= log \\big ( n \\frac{c^{v}_{t}}{c^{*}_{t}c^{v}_{*}}  \\big) \\\\\nc^{v}_{t} &: \\text{count of token $t$ in vector $v$} \\nonumber \\\\\nc^{*}_{t} &: \\text{sum of all counts of token $t$} \\nonumber \\\\\nc^{c}_{*} &: \\text{sum of the counts in vector $v$} \\nonumber \\\\\nn &: \\text{sum of all counts} \\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle pmi(t,v)\" display=\"inline\"><mrow><mi>p</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=log\\big{(}n\\frac{c^{v}_{t}}{c^{*}_{t}c^{v}_{*}}\\big{)}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>n</mi><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>c</mi><mi>t</mi><mi>v</mi></msubsup><mrow><msubsup><mi>c</mi><mi>t</mi><mo>*</mo></msubsup><mo>\u2062</mo><msubsup><mi>c</mi><mo>*</mo><mi>v</mi></msubsup></mrow></mfrac></mstyle></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle c^{v}_{t}\" display=\"inline\"><msubsup><mi>c</mi><mi>t</mi><mi>v</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\text{count of token $t$ in vector $v$}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mtext>count of token\u00a0</mtext><mi>t</mi><mtext>\u00a0in vector\u00a0</mtext><mi>v</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle c^{*}_{t}\" display=\"inline\"><msubsup><mi>c</mi><mi>t</mi><mo>*</mo></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\text{sum of all counts of token $t$}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mtext>sum of all counts of token\u00a0</mtext><mi>t</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle c^{c}_{*}\" display=\"inline\"><msubsup><mi>c</mi><mo>*</mo><mi>c</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\text{sum of the counts in vector $v$}\" display=\"inline\"><mrow><mi/><mo>:</mo><mrow><mtext>sum of the counts in vector\u00a0</mtext><mi>v</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle n\" display=\"inline\"><mi>n</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle:\\text{sum of all counts}\" display=\"inline\"><mrow><mi/><mo>:</mo><mtext>sum of all counts</mtext></mrow></math>", "type": "latex"}, {"file": "1601.03288.tex", "nexttext": "\n\n\\noindent with $p_k$ the value in the vector of the test corpus $P$ at position $k$ and $q_k$ the value of the vector of the training corpus $Q$ at position $k$.\\footnote{When $q_k=0$, smoothing is applied by setting $q_k = 2^{-52}$.}   \n\nApart from the Kullback-Leibler divergence, we also implemented the Jensen-Shannon divergence (JS; \\cite{Lin91}):\n\n\n\n", "itemtype": "equation", "pos": 20485, "prevtext": "\n\n\nInstead of calculating a distance between vectors, it is also possible to consider the vector as a probability distribution of tokens occurring in a corpus. Similarity between probability distributions can be calculated with e.g the Kullback-Leibler divergence (KL; \\cite{Kullback51}):\n\n\n", "index": 3, "text": "\\begin{equation} KL(P; Q) = \\sum_k p_k log_2\\Big( \\frac{p_k}{q_k}\\Big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"KL(P;Q)=\\sum_{k}p_{k}log_{2}\\Big{(}\\frac{p_{k}}{q_{k}}\\Big{)}\" display=\"block\"><mrow><mrow><mi>K</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>;</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mrow><msub><mi>p</mi><mi>k</mi></msub><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><msub><mi>g</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><msub><mi>p</mi><mi>k</mi></msub><msub><mi>q</mi><mi>k</mi></msub></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03288.tex", "nexttext": "\n\n\n\\noindent Using the same notation as for the KL-divergence. The JS-divergence can be considered as a symmetric version of KL.\n\nA fifth similarity measure that has been used is the simple Unknown Word Ratio (sUWR; \\cite{Zhang09,Plank10}). The sUWR is the proportion of tokens $t$ in the test corpus $P$ that are not seen in the training corpus $Q$:\n\n\n", "itemtype": "equation", "pos": 20927, "prevtext": "\n\n\\noindent with $p_k$ the value in the vector of the test corpus $P$ at position $k$ and $q_k$ the value of the vector of the training corpus $Q$ at position $k$.\\footnote{When $q_k=0$, smoothing is applied by setting $q_k = 2^{-52}$.}   \n\nApart from the Kullback-Leibler divergence, we also implemented the Jensen-Shannon divergence (JS; \\cite{Lin91}):\n\n\n\n", "index": 5, "text": "\\begin{align}\n\n\n\\scriptstyle{JS(P; Q)} &\\scriptstyle{= \\frac{1}{2}\\bigg[  KL\\Big (P; \\frac{P + Q}{2}\\Big ) + KL\\Big (Q; \\frac{P + Q}{2} \\Big)  \\bigg]}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;\\par&#10;\\scriptstyle{JS(P;Q)}\" display=\"inline\"><mrow><mi mathsize=\"70%\">J</mi><mo>\u2062</mo><mi mathsize=\"70%\">S</mi><mo>\u2062</mo><mrow><mo maxsize=\"70%\" minsize=\"70%\">(</mo><mi mathsize=\"70%\">P</mi><mo mathsize=\"70%\" stretchy=\"false\">;</mo><mi mathsize=\"70%\">Q</mi><mo maxsize=\"70%\" minsize=\"70%\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\scriptstyle{=\\frac{1}{2}\\bigg{[}KL\\Big{(}P;\\frac{P+Q}{2}\\Big{)}+%&#10;KL\\Big{(}Q;\\frac{P+Q}{2}\\Big{)}\\bigg{]}}\" display=\"inline\"><mrow><mi/><mo mathsize=\"70%\" stretchy=\"false\">=</mo><mrow><mstyle scriptlevel=\"+1\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mrow><mrow><mi mathsize=\"70%\">K</mi><mo>\u2062</mo><mi mathsize=\"70%\">L</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mi mathsize=\"70%\">P</mi><mo mathsize=\"70%\" stretchy=\"false\">;</mo><mstyle scriptlevel=\"+1\"><mfrac><mrow><mi>P</mi><mo>+</mo><mi>Q</mi></mrow><mn>2</mn></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow><mo mathsize=\"70%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"70%\">K</mi><mo>\u2062</mo><mi mathsize=\"70%\">L</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mi mathsize=\"70%\">Q</mi><mo mathsize=\"70%\" stretchy=\"false\">;</mo><mstyle scriptlevel=\"+1\"><mfrac><mrow><mi>P</mi><mo>+</mo><mi>Q</mi></mrow><mn>2</mn></mfrac></mstyle><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03288.tex", "nexttext": "\n\nIn summary, in this study, we evaluate five similarity measures on their usefulness to predict self-training gain (i.e. their prediction performance).  \n\n\\section{Experiments}\n\\label{sec:exps}\n\\subsection{Baseline systems}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor our experiments, four different baselines are computed: two one-class-prediction baselines and two uncomplicated learner baselines.\nAll results tables contain the precision on self-training gain prediction, macro-averaged F-score for performance prediction, and the accuracy of the performance prediction.\n\nWe include the accuracy because it gives a general insight in the correct predictions. Because the majority class has more influence on the accuracy and because the accuracy ignores the precision, we also include the macro-averaged F-score. This score gives the best sense of how well a system performs.\n\nIn a practical situation, a system developer may be most interested in the precision of self-training gain prediction. Indeed, if a self-training setup is predicted to lead to a performance gain, the developer wants this prediction to be trustworthy. All other evaluation scores, like recall on gain, can be computed from the scores that are reported in this paper.\\footnote{An online tool is available at \\url{www.clips.uantwerpen.be/cgi-bin/vincent/scoreconverter.html}.} \n\n\\paragraph{One-class prediction}\nThe baseline of the self-training gain prediction can be set to predicting that self-training will always increase labeling performance (the \\texttt{POS} baseline) or that it will never increase labeling performance (the \\texttt{NEG} baseline). The precision on gain, the macro-averaged F-score and the accuracy are reported in Table~\\ref{tab:unsup}. As a consequence of the nature of the baselines, the accuracy of a baseline system equals the precision of the class label that is predicted.\n\nGiven the nature of the corpus, only 106 out of 1,716 setups result in self-training gain, it can be expected that always predicting a loss after self-training produces better overall scores.\n\n\\paragraph{Uncomplicated prediction}\n\nAs we will see later, self-training gain, using the sentiment prediction corpus, is highly dependent on the choice of the test and training corpus, regardless of the nature of the additional data. For this reason, using information about the outcome of previous test/training combinations will produce another type of baseline. One such baseline system may predict gain if at least one similar test/training combination in the training corpus leads to self-training gain (\\texttt{ONCE}). The second may predict gain if the majority of the similar test/training instances lead to self-training gain (\\texttt{MAJ}). For both baseline systems, the precision on gain, the macro-averaged F-score and the accuracy are reported in Table~\\ref{tab:sup1}.\n\n\n\\subsection{Self-training gain prediction}\n\nIn this section, we will discuss two methods to tackle the prediction of setups that lead to gain after self-training. The unsupervised method consists of calculating an indicator based on the similarities between the corpora that are involved. The supervised method consists of training a machine learner on the similarities between the corpora. \n\n\\subsubsection{Unsupervised}\n\nThe unsupervised way of predicting self-training gain is based on the performance indicator $\\delta$ developed by \\cite{Asch10}. This indicator is defined as:\n\n\n", "itemtype": "equation", "pos": 21441, "prevtext": "\n\n\n\\noindent Using the same notation as for the KL-divergence. The JS-divergence can be considered as a symmetric version of KL.\n\nA fifth similarity measure that has been used is the simple Unknown Word Ratio (sUWR; \\cite{Zhang09,Plank10}). The sUWR is the proportion of tokens $t$ in the test corpus $P$ that are not seen in the training corpus $Q$:\n\n\n", "index": 7, "text": "\\begin{equation}sUWR(P; Q) = \\frac{| \\{t | t \\notin Q \\land t \\in P \\}|}{| \\{ t | t \\in P \\} | }\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"sUWR(P;Q)=\\frac{|\\{t|t\\notin Q\\land t\\in P\\}|}{|\\{t|t\\in P\\}|}\" display=\"block\"><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>;</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>\u2209</mo><mrow><mi>Q</mi><mo>\u2227</mo><mi>t</mi></mrow><mo>\u2208</mo><mi>P</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>\u2208</mo><mi>P</mi></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03288.tex", "nexttext": "\n \nThis indicator weighs the similarity between test and training corpus relative to the similarity between test corpus and the additional data. Its design is such that $\\delta$ is +1 if gain is expected from self-training; otherwise the value is -1.\n\n\\begin{table}[!t]\n\\centering\n\\begin{threeparttable}[b]\n\\caption{Unsupervised performance prediction.}\n\\label{tab:unsup}\n\\begin{tabular}{lrrr}\n\\hline\ntype &  precision & macro-avg. & accuracy \\\\\n &  on gain & F-score &  \\\\\n\\hline\n\\textbf{Systems} & & & \\\\\nCosine & 7.58 & 39.85 & 51.40 \\\\\nEuclidean & 10.72 & 43.74 & 54.55 \\\\ \nKL & 6.99 & 39.13 & 50.82 \\\\\n\n\nJS & 9.44 & 42.15 & 53.26 \\\\\nsUWR & 7.34 & 39.56 & 51.17 \\\\\n\\emph{optimized} &&&\\\\\nEuclidean & 17.17 & 57.69 & 82.46 \\\\ \n\\multicolumn{2}{l}{\\textbf{One-class baselines}} && \\\\\n\\texttt{NEG} & 0 & 48.41 & 93.82 \\\\\n\\texttt{POS} & 6.18 & 5.82 & 6.18 \\\\\n\\hline\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item The one-class-prediction baselines are also given.\n\\item Scores are expressed as percentages.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\nFor the prediction performance, all 1,716 self-training experiments are carried out and the performance is measured.\nThe results, substituting the five similarity measures in Equation~\\ref{eq:delta}, are given in Table~\\ref{tab:unsup}. \n\nThe performance indicator $\\delta$ is a simplification. As a consequence, the indicator can be optimized by varying the -1 in Equation~\\ref{eq:delta}. Ideally, one would determine the required value using a separate development partition. Because of the limited data, we did not carry out these experiments. We optimized using the test partition, which leads to overfitting. Nevertheless, we report on one such system, \\emph{optimized} Euclidean, in Table~\\ref{tab:unsup}. The optimized value is -1.1. We include these scores because they reveal that the precision on gain remains low even after improper optimization.\n\nFrom our observations, we conclude that it is unlikely to obtain reasonable prediction performance by using the performance indicator. \nThe default choice is to say that self-training will never lead to gain, i.e.\\ the \\texttt{NEG} baseline. Only if one is anxious to discover a setup that leads to self-training gain, using the performance indicator can be helpful to at least narrow down the amount of setups that need to be tested. In this case, the Euclidean distance and the Jensen-Shannon divergence seem to be the best options to compute the similarities.\n\n \n \n\\subsubsection{Supervised}\n\\label{sec:sup}\nFor supervised performance prediction, three similarity values are taken as the features: test/train, additional/train, and test/additional.\n\n\\paragraph{Leave-one-out cross-validation}\n\nOf the 1,716 self-training experiments, no two setups are the same. The outcome of a leave-one-out cross-validation experiment can be an estimate of how well an unseen setup can be labeled as leading to gain or loss. We choose a $k$NN-based machine learner to estimate self-training gain \\footnote{TiMBL 6.4.2 -- \\url{http://ilk.uvt.nl/timbl}}. The feature metric, the metric that defines how close neighbors are, is set to the Euclidean distance because the default feature metric is more appropriate for categorical features.\n\n\n\n\n\\begin{table}[!t]\n\\centering\n\\begin{threeparttable}[b]\n\\caption{Supervised performance prediction using leave-one-out cross-validation.}\n\\label{tab:sup1}\n\\begin{tabular}{lrrr}\n\\hline\ntype &  precision & macro-avg. & accuracy \\\\\n &  on gain & F-score &  \\\\\n\\hline\n\\textbf{Systems} & & & \\\\\nCosine & 38.10 & 66.92 & 92.37 \\\\\nEuclidean & 49.53 & 73.22 & 93.76 \\\\ \nKL & 75.53 & 84.60 & 96.62 \\\\\n\n\nJS & 71.56 & 85.36 & 96.56 \\\\ \nsUWR & 60.38 & 78.88 & 95.10 \\\\ \n\\multicolumn{2}{l}{\\textbf{Uncomplicated baselines}} && \\\\\n\\texttt{ONCE} & 41.90 & 77.13 & 91.43  \\\\\n\\texttt{MAJ} & 100 & 95.08 & 98.95  \\\\\n\\hline\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item The uncomplicated-prediction baselines are also given.\n\\item Scores are expressed as percentages.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\nThe scores are given in Table~\\ref{tab:sup1}. The values in Table~\\ref{tab:sup1} are better than those in the previous table. Predicting with around 70\\% precision whether a setup will be profitable seems acceptable and it may convince a system developer to carry out self-training. However, there are two remarks that have to be made. Pragmatically, when designing a system, one does not always have the data to train a machine learner to predict the self-training gain. Secondly, looking at the setups that lead to self-training gain reveals a weakness of the classification. We will expand on this observation.\n\n\nEach test/training/additional data combination is unique, but if only the test and training data are taken into account, 11 setups include the same test and training data. Examining which setups lead to self-training gain reveals that once a test/train pair experiences an advantage from self-training, the advantage is often present irrespective of the nature of the additional data. This means that there is information leakage using this leave-one-out cross-validation setup. Indeed, 10 test/train pairs similar to the test instance are present in the training split. Because the 10 pairs included in the training data are very likely a correct indication of the self-training gain/loss for the one pair in the test partition, the 70\\% prediction precision is no surprise. Evaluation would be more relevant if the prediction performance for an entirely new setup is measured. \n\n\n\n\n\nBased on the results of Table~\\ref{tab:sup1}, we can conclude that once it is known that carrying out self-training can increase the labeling performance, the source of the unlabeled data is less important. This observation raises questions about the nature of the differences between the test and training corpus. Performance loss due to a specific, yet unidentified, kind of difference can be mediated by adding additional information through self-training. If this difference between test and training corpus is not sufficiently prominent, self-training will be of no help. Often domain adaptation is needed without having access to knowledge about previous self-training experiments. In the next section, we will investigate what can be done best if a given test/training combination has not been seen before.\n\n\n\n\\paragraph{Tailored leave-one-out} In the previous paragraph, we have shown that knowledge about previous outcomes of a test/training combination leads to high scores. However, sometimes a given test/training combination may not have been tested yet. To examine how well self-training gain can be predicted in that situation, a tailored leave-on-out routine is implemented.\n\nFor each instance, three corpora are involved: training, test and the unlabeled data (extra). The instance contains three numbers: the train/test similarity, the train/extra similarity and the test/extra similarity. Any instance that contains any pair of corpora from the test instance is excluded from the training partition (60 instances). Also, any instance containing the same corpus as the unlabeled corpus in the test instance is excluded from the training partition (396 instances).\n\nAs a result, we have 1,716 folds with 1 instance in the test partition and 1,260 instances in the training partition. This split ensures that the similarities of the test instance are not present in the training partition.\n\n\nThe results are presented in Table~\\ref{tab:tlo}.\n\n\n\\begin{table}[!t]\n\\centering\n\\begin{threeparttable}[b]\n\\caption{Supervised performance prediction using tailored leave-one-out cross-validation.}\n\\label{tab:tlo}\n\\begin{tabular}{lrrr}\n\\hline\ntype &  precision & macro-avg. & accuracy \\\\\n &  on gain & F-score &  \\\\\n\\hline\nCosine & 1.43 & 47.90 & 89.86 \\\\\nEuclidean & 6.12 & 49.97 & 88.81 \\\\ \nKL & 29.63 & 60.69 & 91.90 \\\\\n\n\nJS & 41.90 & 68.94 & 92.83 \\\\ \nsUWR & 2.67 & 48.38 & 89.69 \\\\ \n\\hline\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item Scores are expressed as percentages.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\nThe scores in Table~\\ref{tab:tlo} are clearly lower than the scores in Table~\\ref{tab:sup1}. This could be expected because this setup emulates classifying an entirely new combination of domains. However, apart from the precision on self-training gain for the non-probability-distribution-based similarities, these scores are higher than the scores for the unsupervised method in Table~\\ref{tab:unsup}. \n\n\n\nThe experiments of this section confirm the observation that prior knowledge about the outcome of self-training experiments is the best predictor for new self-training experiments. Although this may not come as a surprising conclusion, it also holds when the tested combination of corpora was not seen before, meaning that the performance prediction classifier was able to learn from setups unrelated to the test setup. This is an indication that the similarity scores capture useful information about the corpora, in the context of sentiment classification.\n\n\n\n\n\\section{Discussion}\n\\label{sec:dis}\n\n\\subsection{Limitations of similarity scores}\n\\label{sec:issues}\n\nPredicting self-training gain appears to work best if information about a collection of self-training experiments is available. Although this is a restriction on the practicability of the technique, performance prediction can be the working method of choice in selected situations. It can be useful to sum up some limitations that should be taken into account when similarity measures are used.   \n\n\\paragraph{Class label independence} Similarity measures do not use the class labels. Consider two corpora that are completely disjunct with respect to class labels but very similar in the feature space. A similarity measure will probably underestimate the difference between the two corpora and overestimate the labeling performance. Luckily there are labeling tasks (part-of-speech tagging, sentiment prediction, \\ldots) for which this extreme situation is not likely to occur, but it still means that the linearity between similarity and performance should be assessed for each new task. \n\\paragraph{Corpus size} Similarity measures are corpus size dependent. In our experiments all corpora are of the same size, but in a real situation this may not be the case. For example, the overlap measure quantifies the number of unseen elements in the test set given a training set. If a larger test set of the same test domain is taken, the number of unseen elements will probably decrease. The decrease is the result of chance and does not stem from a higher similarity between the domains. \n\\paragraph{Similarity measure} It can be difficult to find a similarity measure fit for the task. In addition, substantiating why a given measure works well can be unfeasible.   \n\n\n\\subsection{Different experimental setups}\n\nThe nature of self-training and self-training gain prediction is complex and many design choices have to be made when setting up experiments. In this section, we explore two different setups that aim at obtaining more insight in the importance of the nature of the additional data. \n\nA first research question is whether the additional data of the self-training in previous experiments is large enough. One may expect that more additional data would more easily lead to self-training gain.\n\nA second experimental design change tackles the need for the similarities based on the additional data. Indeed, much information is already present in the test and training corpus. Is it useful to include extra information?\n\nBecause we change the experimental design, the number of self-training experiments is different for each change in setup. More details are given in the following paragraphs.\n\n\\paragraph{Concatenated additional data set}\n\nIn the setup of the previous self-training experiments, the unlabeled, additional data that is added is limited to the data of a single domain. The research question that is addressed in this paragraph is: Why limit the additional data to a single domain? Can the concatenated data of all domains lead to the same results?\n\nTo address this question, we set up a collection of 156 self-training experiments. The training and test corpora are the same as in the previous experiments, the additional corpus is different. The additional corpus consists of the concatenation of all 11 domains that are neither the training nor the test corpus.   \n\nThe overlap in setups that lead to self-training gain are given in Fig.~\\ref{fig:grid}. The grid shows that there are many grey cells. These cells are the training/test combinations that lead at least once to self-training gain if a selected domain is added as unlabeled data, but there is no benefit if the concatenated data is used during self-training. This indicates that it is useful to select the right domain to add as unlabeled data, because adding all domains together eliminates the self-training gain. Since not all data is good data, it is interesting to be able to predict self-training gain. \nFig.~\\ref{fig:grid} also contains four dotted cells. These are the setups for which only the concatenated data is helpful. Including the {\\footnotesize\\textsc{BULK}} approach in the experiments of Section~\\ref{sec:exps} would be a helpful extension to the research in this paper. We did not do this for corpus size reasons explained in Section~\\ref{sec:issues}.\n\n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[scale=0.42]{vanas1}\n\\caption{A grid showing the self-training gain difference between a setup in which the additional data is a single domain (DOMAIN) and a setup in which the additional data is the concatenated data from all other domains (BULK). If a given training/test combination leads to self-training gain in both approaches, the cell is crossed. If only the DOMAIN approach leads to gain, the cell is greyed. If only the BULK approach leads to gain, the cell is dotted. If there is no gain, the cell is blank. Note that for the DOMAIN approach, each cell represents 11 self-training experiments and if at least one of these 11 setups leads to gain, the cell is colored. For example, using \\emph{music} as training corpus and \\emph{apparel} as test corpus, leads to self-training gain for both DOMAIN and BULK.}\n\\label{fig:grid}\n\\end{figure}\n\n\n\n\\paragraph{Only test/train similarity as a feature}\n\n\\begin{table}\n\\centering\n\\begin{threeparttable}[b]\n\\caption{Supervised performance prediction gain using tailored leave-one-out cross-validation.}\n\\label{tab:onlytt}\n\\begin{tabular}{lrrr}\n\\hline\ntype &  precision & macro-avg. & accuracy \\\\\n &  on gain & F-score &  \\\\\n\\hline\nCosine & 2.63 & 48.54 & 91.72 \\\\\nEuclidean & 6.52 & 49.75 & 91.49 \\\\ \nKL & 42.11 & 68.03 & 92.95 \\\\\n\n\nJS & 45.31 & 65.34 & 93.47 \\\\ \nsUWR & 0 & 46.61 & 87.30 \\\\ \n\\hline\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item Using only the test/training similarity.\n\\item Scores are expressed as percentages.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\nIn Section~\\ref{sec:exps}, the experiments are carried out using all three similarities: test/train, test/additional, and additional/train. In Section~\\ref{sec:sup}, it is shown that knowledge of the the test/train combination is already very informative. For this reason, we carried out tailored leave-one-out experiments, using only the test/train similarities as a feature. The results are given in Table~\\ref{tab:onlytt}. Comparison of these results with Table~\\ref{tab:tlo} reveal that the scores are very similar.\n\n\nThe differences between the usage of one or three similarities come mainly from the fact that using three features leads to more predictions of self-training gain, i.e. self-training setups labeled as \\texttt{POS}. For the Cosine distance, no extra true positives for the \\texttt{POS}-class are predicted, only false positives, leading to a lower macro-averaged F-score. But in general, the extra true positives smooth out the effect of the extra false positives, leading to an increase in F-score. The only exception is the Kullback-Leibler divergence. Adding extra features leads to the prediction of less \\texttt{POS}-labels. We consider this as a property of the divergence and have no explanation for this different behavior.\n\nBased on the precision gain, using only the test/training similarity for prediction appears to be the best option. Even when the given test/training combination has previously not been seen. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:con}\n\nIn this paper, we showed that self-training can be a performance boosting technique for a strict selection of setups. \n\nConsider a system developer who wants to implement an online labeling tool. He has one tagged corpus and a range of unlabeled corpora at his disposal. He does not know which data a user will be submitting and, as a consequence, he does not know which unlabeled corpus to add to his tagged corpus. It is also very likely that the submitted corpus will be an unseen corpus. For the binary sentiment classification data, we have shown that he could assess the similarity between his training corpus, his unlabeled corpora and the unseen test corpus and predict whether he should add the unlabeled data to his training corpus before tagging the test corpus.   \n\nFor unsupervised prediction, the best thing to do is always assume self-training loss. If one wants to be able to predict a self-training gain setup, the performance indicator can be used.\n\nFor supervised prediction, if previously computed outcomes are available for identical corpora, the best thing to do is to predict the same outcome as the majority of the identical training/test combinations. If no previous information is available for identical corpora, the similarities between the test and the training can be used to predict the outcome of the self-training experiment. The test/additional and additional/training similarities can be added as extra features, but if precision on gain is the selection criterion, this extension seems to be unnecessary.   \n\nWe do not make claims about the best choice of similarity measure because all measures have their disadvantages. Nevertheless, based on the experiments it appears that probability-distribution-based measures are more precise, and more specific: i.e.\\ the Jensen-Shannon divergence.\n\nA general conclusion regarding similarity measures is that the similarity between domains is important when evaluating domain adaptation techniques. A domain adaptation technique may lead to better results on nearby domains than on domains that are further apart. Or vice versa. Not knowing how similar domains are may lead to unjustified conclusions when comparing different domain adaptation techniques that are tested an a variety of corpora.\n\nFor this reason, a widely accepted technique to measure domain similarity would be an important addition to domain adaptation research. If further research could provide such a similarity measure, judging the applicability of domain adaptation techniques would become a lot more objective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendices\n\\appendix{}\nWe refer to two online resources that are not essential to a clear understanding of this article:\n\\begin{itemize}\n\\item \\url{www.clips.uantwerpen.be/~vincent/self-training/scripts.html}\\\\ For reference, the experimental implementations that are used in this study are made available online. Because the creation of a standalone application is not one of the goals of this research, the implementations are a collection of scripts rather than a mature software package.\n\\item \\url{www.clips.uantwerpen.be/~vincent/self-training}\\\\ Additional insight into the structure of the corpus based on the three similarity measures as it is used in Section~\\ref{sec:sup} can be gained from this vector space visualization.\n\\end{itemize}\n\n\n\n\\section*{Acknowledgment}\nThis research is funded by the Research Foundation Flanders (FWO-project G.0478.10 -- Statistical Relational Learning of Natural Language)\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{IEEEabrv,bibliography}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{IEEEbiographynophoto}{Vincent Van Asch}\nhas started his research during a project on natural language understanding, aiming to produce graphic output. After a project on biomedical text mining he obtained a PhD in Computational Linguistics at the University of Antwerp, studying the application of similarity measures on text corpora.\n\\end{IEEEbiographynophoto}\n\n\n\n\\begin{IEEEbiographynophoto}{Walter Daelemans}\nstudied linguistics and psycholinguistics at the University of Antwerp and the University of Leuven where he also received a PhD in Computational Linguistics in 1987. He held research and teaching appointments at the University of Nijmegen, the Brussels AI-Lab, and Tilburg University. Since 2005 he is full professor of Computational Linguistics at the University of Antwerp, where he also directs the CLiPS research centre. His research interests are in Computational Language Learning, Natural Language Understanding, and Computational Stylometry.\n\\end{IEEEbiographynophoto}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 24993, "prevtext": "\n\nIn summary, in this study, we evaluate five similarity measures on their usefulness to predict self-training gain (i.e. their prediction performance).  \n\n\\section{Experiments}\n\\label{sec:exps}\n\\subsection{Baseline systems}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor our experiments, four different baselines are computed: two one-class-prediction baselines and two uncomplicated learner baselines.\nAll results tables contain the precision on self-training gain prediction, macro-averaged F-score for performance prediction, and the accuracy of the performance prediction.\n\nWe include the accuracy because it gives a general insight in the correct predictions. Because the majority class has more influence on the accuracy and because the accuracy ignores the precision, we also include the macro-averaged F-score. This score gives the best sense of how well a system performs.\n\nIn a practical situation, a system developer may be most interested in the precision of self-training gain prediction. Indeed, if a self-training setup is predicted to lead to a performance gain, the developer wants this prediction to be trustworthy. All other evaluation scores, like recall on gain, can be computed from the scores that are reported in this paper.\\footnote{An online tool is available at \\url{www.clips.uantwerpen.be/cgi-bin/vincent/scoreconverter.html}.} \n\n\\paragraph{One-class prediction}\nThe baseline of the self-training gain prediction can be set to predicting that self-training will always increase labeling performance (the \\texttt{POS} baseline) or that it will never increase labeling performance (the \\texttt{NEG} baseline). The precision on gain, the macro-averaged F-score and the accuracy are reported in Table~\\ref{tab:unsup}. As a consequence of the nature of the baselines, the accuracy of a baseline system equals the precision of the class label that is predicted.\n\nGiven the nature of the corpus, only 106 out of 1,716 setups result in self-training gain, it can be expected that always predicting a loss after self-training produces better overall scores.\n\n\\paragraph{Uncomplicated prediction}\n\nAs we will see later, self-training gain, using the sentiment prediction corpus, is highly dependent on the choice of the test and training corpus, regardless of the nature of the additional data. For this reason, using information about the outcome of previous test/training combinations will produce another type of baseline. One such baseline system may predict gain if at least one similar test/training combination in the training corpus leads to self-training gain (\\texttt{ONCE}). The second may predict gain if the majority of the similar test/training instances lead to self-training gain (\\texttt{MAJ}). For both baseline systems, the precision on gain, the macro-averaged F-score and the accuracy are reported in Table~\\ref{tab:sup1}.\n\n\n\\subsection{Self-training gain prediction}\n\nIn this section, we will discuss two methods to tackle the prediction of setups that lead to gain after self-training. The unsupervised method consists of calculating an indicator based on the similarities between the corpora that are involved. The supervised method consists of training a machine learner on the similarities between the corpora. \n\n\\subsubsection{Unsupervised}\n\nThe unsupervised way of predicting self-training gain is based on the performance indicator $\\delta$ developed by \\cite{Asch10}. This indicator is defined as:\n\n\n", "index": 9, "text": "\\begin{equation}\n\\delta = \\frac{\\big|\\frac{\\text{test/training similarity}}{\\text{test/additional data similarity}} - 1 \\big|}{\\frac{\\text{test/training similarity}}{\\text{test/additional data similarity}}-1} \n\\label{eq:delta}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\delta=\\frac{\\big{|}\\frac{\\text{test/training similarity}}{\\text{test/%&#10;additional data similarity}}-1\\big{|}}{\\frac{\\text{test/training similarity}}{%&#10;\\text{test/additional data similarity}}-1}\" display=\"block\"><mrow><mi>\u03b4</mi><mo>=</mo><mfrac><mrow><mo maxsize=\"120%\" minsize=\"120%\">|</mo><mrow><mfrac><mtext>test/training similarity</mtext><mtext>test/additional data similarity</mtext></mfrac><mo>-</mo><mn>1</mn></mrow><mo maxsize=\"120%\" minsize=\"120%\">|</mo></mrow><mrow><mfrac><mtext>test/training similarity</mtext><mtext>test/additional data similarity</mtext></mfrac><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>", "type": "latex"}]