[{"file": "1601.05458.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 34994, "prevtext": "\n\n\\title{Efficient Compilation to Event-Driven Task Programs}\n\\author{Beno{\\^\\i}t Meister, Muthu Baskaran, Beno{\\^\\i}t Pradelle \\\\\n  Tom Henretty, Richard Lethin \\\\ Reservoir Labs \\\\ \\url{lastname@reservoir.com}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\\begin{abstract}\nAs illustrated by the emergence of a class of new languages and runtimes, it is\nexpected that a large portion of the programs to run on extreme scale computers\nwill need to be written as graphs of event-driven tasks (EDTs).\nEDT runtime systems, which schedule such collections of tasks, enable more\nconcurrency than traditional runtimes by reducing the amount of inter-task\nsynchronization, improving dynamic load balancing and making more operations\nasynchronous.\n\nWe present an efficient technique to generate such task graphs from a\npolyhedral representation of a program, both in terms of compilation time and\nasymptotic execution time.\nTask dependences become materialized in different forms, depending upon the\nsynchronization model available with the targeted runtime.\n\nWe explore the different ways of programming EDTs using each synchronization\nmodel, and identify important sources of overhead associated with them. \n\n\n\nWe evaluate these programming schemes according to the cost they entail in\nterms of sequential start-up, in-flight task management, space used for\nsynchronization objects, and garbage collection of these objects. \n\nWhile our implementation and evaluation take place in a polyhedral compiler,\nthe presented overhead cost analysis is useful in the more general context of\nautomatic code generation.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Problem and Context}\n\nThe race for hardware speed and low-power is bringing computers from embedded to\nlarge scale into the ``Extreme scale'' era, in which high numbers of cores react\nheterogeneously to their environment, and are constrained by their\nglobal energy consumption. \nThis imposes tall requirements on the software, which must be as parallel as\npossible to take advantage of the cores, and also adaptable to changing core\ncapabilities and avoid wasting energy. \n\nOne way to address this problem is to depart from the Bulk-Synchronous\nProgramming (BSP) model.\nIronically, while BSP has historically promoted parallelism by enabling simple\nprogramming models such as loop parallelism and Single-Program Multiple Data\n(SPMD) computations, the model now seems to stand in the way of the \namounts of parallelism sought out.\nFirst, bulk synchronizations (across iterations of a for loop, for instance) \noften express an over-approximation of the actual dependences among computation\ninstances (whether they are tasks or loop iterations).\nAlso, synchrony often results in a loss of parallelism and a waste of energy,\nsince cores spend a portion of their time waiting for some condition to happen\n(e.g., a barrier to be reached by other cores, a spawned task to return). \n\nThus, it is commonly believed that the next generation of parallel software will\nbe asynchronous and non-bulk. \nIn other words, programs will be expressed as a graph of tasks, in which tasks\nare sent for asynchronous execution (``scheduled''), and they become runnable\nwhenever their input data is ready.\nIn this model, the more accurate the inter-task dependences are with respect to\nthe semantics of the program, the more parallelism is exposed.\n\nSome programming models support the expression of parallel programs as recursive\ntasks, with for instance Cilk \\cite{Blumofe:ACM:1995}, X10 \\cite{x10}, and\nHabanero \\cite{habaneroj}. In these models, each task can only depend on one\n(parent) task, or on the set of tasks scheduled by a sequential predecessor. \nA major advantage of these models is that they offer provable\nnon-deadlock guarantees.  \nHowever, this comes at the cost of being less general than other systems which\nexpress programs as acyclic graphs of tasks \\cite{ocr-spec-100, swarm, CnC, kong14taco,\n  legion, ompss, parsec}\\footnote{Since dependences represent constraints on the\n  task exectution order, a task graph needs to be acyclic for a valid execution\n  order of its tasks to exist.}.\nWhile these graphs exist in the literature under a variety of names, here we are\nusing the name ``Event-Driven Tasks'' (EDT) to refer to them. \nAn event here represents the satisfaction of a dependence. \n\nSince with more generality and performance also come higher programming\ndifficulty, we have worked on tools to automatically {\\em generate} such\nprograms, when they can be modeled with the polyhedral model. \nWith this model, geared towards compute-intensive loop codes, the parallelization\ntool is provided with a precise representation of the program, whose task graph\ncan be generated statically, as described in \\cite{Baskaran2009,kong14taco}. \n\nSince dependences are determined statically at parallelization time, relying\non systems that discover dependences at runtime \\cite{legion, ompss} would be\nwasteful and is hence not considered in this paper.\n\nOne of the intrinsic challenges of automatic parallelization is to define\nprogrammatic ways of generating tasks and dependences and of using the target\nsystem capabilities without introducing too much overhead.\nAnother important challenge with polyhedral representations is to maintain\noptimization time tractable, which requires the use of nimble operations on\npolyhedra representing the tasks and their dependences.\n\nThis paper offers two main contributions related to the automatic generation of\nEDT codes, and in particular the dependence relationships among tasks. \n\nAfter comparing run-time overheads implied by implementation strategies based\non a set of basic synchronization models available in current EDT runtimes,\nwe propose a nearly-optimal strategy based on a slight improvement of one of the\nmodels, in section \\ref{sec:deps-comparison}. \n\nThen, focusing on the case of the polyhedral model, we present a novel, scalable\ntechnique for automatically generating tasks and dependences in section\n\\ref{sec:scalable}.\n\nIn section \\ref{sec:deps-codegen}, we show how this model can be used to\ngenerate EDT codes with the discussed synchronization models, along with further \ncode optimizations. \nFinally, we evaluate the benefits of using our techniques on a set of\nbenchmarks in section \\ref{sec:exp}, discuss related work in section\n\\ref{sec:related} and summarize our findings in section \\ref{sec:conclusion}.\n\n\n\\section{A comparison of synchronization models} \\label{sec:deps-comparison}\n\n\n\nThroughout this paper, we care about the automatic, optimal generation of\nEDT-based codes, and the cost of using various synchronization models. \nWe are excluding other questions such as the per-task overhead of the\nruntime, which boil down to the constraint of making the tasks large enough\n(thousands to tens of thousands of operations per task seems to be the norm on\nx86-based platforms).\n\nExpressing a program as a graph of event-driven tasks (EDTs) requires some\namount of bookkeeping.\nWe are interested in overheads that such bookkeeping would entail, and in their\nbehavior as the number of tasks grows.\nWe illustrate these overheads by referring to the system proposed by\nBaskaran et al \\cite{Baskaran2009}. \nWhile we remind the reader that its authors did {\\em not} intend it for a\nlarge-scale system, it has been used in larger-scale works for automatic\nparallelization to task graphs using the polyhedral model since then\n\\cite{dathathri14dataflow, kong14taco}. \n\nLet $n$ be the number of vertices in the task graph. \nOne of the advantages of the reference method is its simplicity. \nA master thread sets up a graph of tasks linked by dependences; \nthen, an on-line list scheduling algorithm defines when and where tasks get\nexecuted. \n\nOne obvious bookkeeping overhead in this scheme is that it requires a setup\nphase {\\em before} the program can actually run in parallel.  \nAmdahl's law dictates that the cost of sequential part of bookkeeping\ntasks must be insignificant as compared to the execution time of a task. \nThe importance of this {\\bf sequential start-up overhead} becomes greater as the\navailable parallelism grows, and is hence crucial to minimize.\n\nAlso, the spatial cost of representing the dependences has a major impact on\nscalability, and even feasibility of generating executable programs. \nFor instance, the baseline method represents inter-task dependences explicitly,\nall at once, and hence has an $O(n^2)$ {\\bf spatial overhead}.\n\nAnother source of overhead relates to the amount of tasks and dependencies the\nruntime has to manage at a given point in time. \nEDT-based runtimes make it possible to run tasks asynchronously, i.e., a task\ncan {\\em schedule} other tasks, even before its inputs are ready and without\nwaiting for its completion. \nIt is the user's responsibility to let the runtime know when a task's inputs are\nready (i.e., when the task can be executed) through the synchronization constructs\nprovided by the runtime API. \nThe amount of tasks that are scheduled before they are ready to execute\nis referred to here as {\\bf in-flight task overhead}.\nThe number of unresolved dependence objects that need to be managed by the\nruntime at any time is the {\\bf in-flight dependence overhead}. \n\nFinally, garbage collection of objects created for the purpose of running a\ntask graph may entail large overheads, especially if the garbage collection must\nbe done only after a large set of tasks has completed. \nHere, we measure {\\bf garbage collection overhead} by the number of objects that\nare not useful anymore but are not destroyed at any point in the execution.\n\nIn the next section, we go through the synchronization paradigms we have\nexperimented with, and examine the overheads induced by their use in automatic\ncode generation. \n\n\n\n\n\\subsection{Synchronization constructs} \\label{sec:sync-paradigms}\nWe have implemented task-graph code generation based on three synchronization\nmodels that we observed in existing task-graph-based runtimes.\nIn one of them, a task (often called ``prescriber task'') sets up input\ndependences for a task before these dependences can be satisfied by other\ntasks. \nHere we call it the {\\em prescribed} synchronization model. \n\nAlternatively, dependence satisfaction information can go through {\\em tags},\nobjects that tasks can {\\em get} from and {\\em put} into a thread-safe table.\nAt a high level, tags are identified with inter-task dependences, and tasks\ncheck that their input dependences are satisfied by {\\em get}ting the\ncorresponding tag.\nWhen a task satisfies another task's input dependence, it puts the corresponding\ntag into the table. \nThe structure of the tasks our tool generates using tags is defined by a\nsequence of {\\em get}s, then the task's computation, followed by a sequence of\n{\\em put}s.\nThe tag table mechanism is currently available in the SWift Adaptive Runtime\nMachine~\\cite{swarm} (SWARM) runtime.\nIt has also been proposed for future implementation in the Open Community\nRuntime~\\cite{ocr-spec-100} (OCR) 1.0.1 specification.\n\nFinally, tasks may be associated with a counter, and scheduled upon the counter\nreaching zero. \nIn this case, only the number of dependences are represented, as opposed to a\nset of dependences. \nA {\\em counted dependence} is a synchronization construct that associates the\nscheduling of a task with a counter. \nThe task is scheduled whenever the counter reaches zero. \nWe extended this construct to one that safely creates a counted dependence that\nneeds to be decremented if it does not exist yet, and called this construct\n``autodec.''  \n\nTable \\ref{tab:sync-paradigms} presents a breakdown of the synchronization\nconstructs available in the Exascale-oriented runtimes studied in this paper. \n\\begin{table} \n\\begin{center}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n{\\bf Runtime} & Prescribed & Tags      & Counted    \\\\ \\hline\nOCR           & $\\times$   &           & $\\times$   \\\\ \\hline\nSWARM         &            & $\\times$  & $\\times$   \\\\ \\hline\n\n\\end{tabular}\n\\caption{Task synchronization models, and examples of runtimes implementing\n  them.} \\label{tab:sync-paradigms}\n\\end{center}\n\\end{table}\n\nIn OpenStream, a main thread sets up ``streams'', which are both a\nsynchronization and communication queue between tasks. The order in which tasks\nthat write to a stream are scheduled by the main task defines which readers of\nthe stream depend on which writers. In that sense, the synchronization model here\nis akin to a prescribed model. \nTags are available in Intel's implementation of the CnC coordination\nlanguage \\cite{CnC}, as well as prescription.\nThe \\verb|async| and \\verb|finish| constructs available in Cilk,\nX10 and Habanero can be implemented using counted dependences.\nFutures can be supported as well, for instance by adding prescription or tags. \n\nThe next section studies the asymptotic overheads linked with the use of these\nsynchronization models. \n\n\\subsection{Comparative overheads}\nIn this section, we go through four synchronization models and find out the\nimplied overheads when using them in automatic code generation. \nDifferent ways of using them are considered, when these lead to different\noverheads. \nOur analysis is summarized in Table \\ref{tab:overhead-summary}, where $n$ is the\nnumber of tasks in the graph, and $r$ is the maximum number of tasks that are\nready to run simultaneously in any possible execution of the graph.\n$d$ is the complexity of computing the number of predecessors to a task, \nand $o$ is the maximum number of dependences going out of a task (the maximum\nout-degree in the task graph). \n\\begin{table*}\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n ~      & Start-up & Spatial & In-flight tasks & In-flight deps & Garbage collection \\\\ \\hline\nPrescribed      & $O(n^2)$   & $O(n^2)$ & $O(n)$    & $O(n^2)$ & $O(n)$      \\\\ \\hline\nTags Method 1   & $O(1)$     & $O(n^2)$ & $O(n)$    & $O(n^2)$ & $O(1)$      \\\\ \\hline\nTags Method 2   & $O(1)$     & $O(n)$   & $O(n)$    & $O(n)$   & $O(n)$      \\\\ \\hline\nCounted         & $O(n.d)$   & $O(n)$   & $O(n)$    & $O(n)$   & $O(1)$      \\\\ \\hline\nAutodec w/o src & $O(1)$     & $O(n)$   & $O(n)$    & $O(r.o)$ & $O(1)$      \\\\ \\hline\nAutodec w/ src  & $O(1)$     & $O(r.o)$ & $O(r)$    & $O(r.o)$ & $O(1)$      \\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\caption{Overheads associated with task graph synchronization\n  models}\\label{tab:overhead-summary}\n\\end{table*}\n\nThe following sections (\\ref{ssec:prescribed} through \\ref{ssec:autodec}) go\nthrough a detailed analysis for each synchronization model.\nThey ultimately show that optimal overheads can be obtained by using autodecs.\n\n\\subsubsection{Prescribed synchronization} \\label{ssec:prescribed}\nTo support our discussion, let us consider a particular task in a task graph,\nand let us refer to it as the {\\em target task}.\nWe also define $n$ as the number of tasks in the graph to run on the \nruntime system.\nFor simplicity, we also assume a ``master'' thread/task/worker, which is\nable to schedule tasks.\n \nWith prescribed synchronization, the target task is created and its input\ndependences are set up by a task that precedes it in the task graph. \nThis method is straightforward for task graphs that are trees, but less so in\ncases where tasks may have more than one direct predecessor. \nTo illustrate this, consider the case when a task has more than one\npredecessor, illustrated with a ``diamond'' pattern in Figure\n\\ref{fig:diamond}, and in which Task 3 is the target task. \nIn this toy example, dependences do not define a particular order of execution\nbetween Tasks 1 and 2. \nTask 3 needs to be created, and its input dependences set up by one of its\npredecessors. \nWithout further synchronization, it is impossible for Task 2 to know whether\nTask 1 has created Task 3 already, or if it needs to be created, and conversely\nfor Task 1. \n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=0.3\\columnwidth]{diamond.eps}\n\\end{center}\n\\caption{Simple example where a successor task has two predecessors}\\label{fig:diamond}\n\\end{figure}\nWhen the target task has more than one predecessor, and without further\nsynchronization possibilities, there are three methods available: \n\\begin{enumerate} \n\\item[1.] A task that dominates all the transitive predecessors of the target task can\n  be responsible for the creation of the target task. \n  The only dominator in our example is Task 0. \n  In our example, Task 0 would indeed be responsible for the setup of all the\n  other tasks.   \n\\item[2.] Alternatively, one of the transitive predecessors to the target task is\n  chosen to set up the target task, and additional dependences are introduced\n  between the chosen task and the transitive predecessors to the target task\n  that don't precede the chosen task.  \n  The additional dependences materialize the fact that the created target task\n  becomes an input dependence to the transitive predecessors. \n  In our example, we could for instance choose Task 1 as the creator and add a\n  dependence between Tasks 1 and 2, making the task graph entirely sequential.\n\\item[3.] Finally, a special prescriber task can be added for the specific\n  purpose of setting up the target task. \n  Dependences representing the created task object are added between the\n  prescriber task and a set of tasks that transitively precede and\n  collectively dominate the target task. \n  One instance of such set is the set of direct predecessors to the target\n  task. \n\\end{enumerate}\n\nSince one of the fundamental goals of EDT runtimes is to increase the\namount of available parallelism, the loss of parallelism induced by the\nintroduction of sequentializing dependences~\\footnote{\n  ``sequentializing dependences'' are dependences that are not required by the\n  semantics of the program and that reduce parallelism.} \nin Method 2 excludes it from the set of acceptable methods.\n\nThe worst case for Method 1 is when the task graph is dominated by one task, as\nin the diamond example.\nThis occurs fairly frequently, including in many stencil computations as\nparallelized through polyhedral techniques.\nIn this case, the dominator task is responsible for setting up all tasks in the\ngraph and their dependences, before any other task can start. \nAn equivalent case is when all tasks are dominated by a set of tasks, in which\ncase the host is the only common dominator of all tasks. \nBoth cases result in a $O(n^2)$ sequential overhead, which accounts for the\nsetting up of all the dependences in the graph. \n\nA naive implementation of Method 3 would generate a prescriber task for each\ntarget task in the graph that has more than one predecessor, and introduce\ndependences between the prescriber task and a dominating set of predecessors to\nthe target task. \nNotice that this process adds an input dependence to the predecessors, which\nmay have only had one predecessor before adding the prescriber task. \nThese predecessor tasks now fall into the original problem, and themselves\nrequire a prescriber task, which must precede the initial prescriber.\nThis results in a transitive construction of prescriber tasks.\n\nIn the worst case, the number of such tasks grows as a polynomial of $n$,\n$pr(n)$. \nThis is illustrated in Figure \\ref{fig:meth3-scale}, where the number of\nprescriber tasks grows in $O(n^2)$. \nThe number of dependences for these prescriber tasks is then expected to be in\n$O(pr(n)^2)$. \n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=\\columnwidth]{method3.eps}\n\\end{center}\n\\caption{Growth of prescriber tasks as introduced by Method 3.} \\label{fig:meth3-scale}\n\\end{figure}\n \nIn the same example, while some tasks may start before all the prescribing tasks\nhave completed, the number of prescriber tasks that need to complete\nbefore any single non-prescriber task in the graph equals $\\frac{n-1}{2}$, i.e.,\na $O(n)$ sequential start-up overhead.\nThe complexity of the sequential start-up overhead appears to be a polynomial\nof degree less than $pr(n)$. \n\nSuch an approach would be impractical at large scale, not only because the\npotentially high complexity of its incurred sequential overhead, but also\nbecause many additional tasks may need to be created. \n\nA response to both these issues is to group the prescriber tasks into\n``macro-''' prescriber tasks, which are responsible for setting up several\ntasks. \nWithout any particular knowledge of the graph structure, the optimal grouping\nfor these tasks is when creating one single prescriber task that sets up all the\ngraph. \nThis solution is exactly equivalent to Method 1, and it has the same overheads. \n\nAnalyzing the graph structure in the hope of finding more than one group with\nlower overheads also has a $O(n)$ overhead. \nHence Method 3 is at best as good as Method 1. \n\nWe implemented Method 1 using OCR.\nIts spatial overhead is $O(n^2)$ because all dependences are represented explicitly.\nThe number of tasks to be handled by the scheduler at once (in-flight task\noverhead) is $O(n)$, the number of in-flight dependences is $O(n^2)$ and the\ninput dependence objects for each task can be garbage-collected when the task\nstarts.\n\n\\subsubsection{Tag-based synchronization} \\label{ssec:tags}\nNotionally, a tag is a key in an associative table.\nA predecessor and its successor tasks synchronize through a tag that\nrepresents the completion of the predecessor task to the successor task. \nThe predecessor signifies its completion by putting the tag in the table. \nThe successor can wait for tags to be available in the table by getting the\ntag. \nControl returns to the task whenever all the tags for which a \\verb|get| was issued\nhave been put in the table. \nTo avoid deadlocks, \\verb|get|s are typically asynchronous.\nWhen a tag is put, all the tasks that did \\verb|get| the tag are considered for\nexecution. \n\nWe have found two meaningful tag-based methods to perform inter-task\nsynchronization.\n\\begin{enumerate}\n\\item [1.] In the first method, each pair of tasks linked by a dependence is\n  mapped to a tag. \n\\item [2.] In the second method, independently developed in\n  \\cite{vasilache13tale}, one tag is associated with each predecessor task.\n  Before completion, each task puts a tag that signifies that it has completed.\n  Its successors all get the same tag from the table.\n\\end{enumerate}\n\nA clear advantage of these methods, as compared to prescribed\nsynchronization methods, is that they have no sequential scheduling overhead,\nsince all tasks can be scheduled in parallel, virtually at any point in time. \n\nThe cost of storing synchronization objects (spatial overhead) differentiates\nboth methods. \nIt is $O(n^2)$ for Method 1 (one tag per dependence), and $O(n)$ for Method 2\n(one tag per task). \nA subtlety here is that while only completed tasks perform a\n\\verb|put| in the tag table, all the other tasks may have performed at least one\n\\verb|get|, which needs to be tracked by the runtime. \nHowever, Method 1 has an advantage in terms of garbage-collection of its tags,\nsince a tag can be disposed of as soon as its unique \\verb|get| has executed. \nThe SWARM runtime offers one-use tags, where the disposal of a tag is performed\nby the runtime after a \\verb|get| was done on the tag. \nIn Method 2, without further sequentializing synchronization, successor tasks\ndon't know whether they are the last task to \\verb|get| the tag. \nHence the tag objects can only be disposed once a post-dominator of the task\ngraph has started, or when the entire task graph has completed. \n\nIn terms of in-flight task overhead, a straightforward implementation of both\nmethods consists in starting all the tasks upfront and letting them synchronize\nwith each other.\nTo reduce the number of tasks to be managed by the scheduler simultaneously,\ntasks should be scheduled by their predecessors. \nThe tightest bound on the number of in-flight tasks is obtained when each task\nis scheduled by one of its predecessors. \nIn this case, the number of in-flight scheduled tasks is exactly the number of\ntasks that are ready to run. \nThe main obstacle to achieving this appears again with tasks that have more than\none predecessor, in which case one of the predecessors must be chosen to\nschedule the successor task. \nThis cannot be performed dynamically, using tags only, without introducing\nsequentializing synchronizations. \n\nMethods for statically electing a task within a set of tasks are available in\nthe context of automatic parallelization using the polyhedral model. \nFor any given successor task, the method consists in considering a total order\namong the set of predecessor tasks, and defining the task that minimizes the\norder \\cite{feautrier88parametric, isl} as the one that schedules the successor\ntask.\nUnfortunately, except for simple cases, the computation of such a minimum\ndoesn't scale well with the number of nested loops in the program,\nleading to potentially intractable execution times of the automatic\nparallelization tool. \nAdditionally, this solution is specific to the polyhedral model, and in this\nsection we are discussing synchronization models in the general context of\nautomatic generation of EDT codes. \n\nHence, here we consider the straightforward solution as the only generally viable\noption, with a $O(n)$ in-flight task overhead. \nMethod 2, proposed by \\cite{vasilache13tale}, is superior to Method 1 across the\nboard, except for its garbage collection overhead. \n\n\\subsubsection{Counted dependences} \\label{ssec:counted}\nCounted dependences have similarities with both prescribed synchronizations and\ntags.\nLike prescribed synchronization, they require a task to initialize them. \nA counted dependence naturally represents the number of unsatisfied input\ndependences of a successor task.\nIt is decremented by each predecessor of the task, at completion.\nWithout further synchronization, counted dependences have an $O(n^2)$\nsequential overhead -- like prescribed synchronizations -- if the input\ndependences need to be enumerated, or $O(n.d)$ if there exists an analytic\nfunction that computes them in time $d$.\n\nSuch a function can be generated in the case of polyhedral code generation. \nWe show in section \\ref{sec:deps-codegen} that task dependences can be\nrepresented with a polyhedron, which scans the predecessors (resp. successors)\nof a task as a function of the task's own runtime parameters. \nWe use polyhedral counting techniques to compute the number of predecessors to a\ntask, either by evaluating the enumerator of the\npolyhedron~\\cite{clauss97deriving,1275134}, or by scanning the polyhedron as a\nloop and incrementing the count by one for each iteration.\nThe best choice of a counting loop versus an enumerator depends upon the shape of the\npolyhedron. \nComplex shapes result in complex enumerators, which can be costlier to\nevaluate in practice than with a counting loop, especially if the count is low.\n\nThe fact that the program starts with $n$ tasks to schedule implies an in-flight\ntask overhead of $O(n)$.\nOnly one counted dependence is required for each task, giving a spatial overhead\nand an in-flight overhead of $O(n)$. \nGarbage collection of the counted dependence associated with each task can be\nperformed as soon as the task starts. \n\nThe set of useful in-flight scheduled tasks should be the ones that are\nready to run, i.e., the ones whose input dependences are satisfied, plus the\nones that are already running. \nLet $r$ be the maximum number of such tasks in any execution of the task\ngraph.\nHaving less than $r$ in-flight tasks would reduce parallelism and is hence not\ndesirable.\nAn ideal task graph runtime scheme would have $O(r)$ in-flight task overhead\nand, accordingly, an $O(r)$ spatial overhead.\n\n\\subsubsection{Autodecs} \\label{ssec:autodec}\nSequential overhead results from the inability to determine, for a given\nsuccessor task with multiple predecessors, a unique predecessor that can set up\nthe successor task (let us call such task the successor's creator).\n\nAs we saw in previous sections, we assume that there is no general, viable way\nto resolve this statically. \nWe propose a dynamic resolution based on counted dependences, which does not\nintroduce sequentializing dependences. \nAgain, this is different from dynamic dependence discovery as performed by some\nruntimes, since the dependences here are defined by the compiler. \nIn our proposed dynamic resolution, the first task to be able to decrement a\nsuccessor task's counter becomes its unique creator. \nWe call such decrement with automatic creation an \\verb|autodec| operation. \n\nCreation of a unique counted dependence -- and hence a unique successor task --\ncan be ensured for instance using an atomic operation, which deals with the\npresumably rare case when two predecessors would complete at the exact same\ntime.\n\nAs a result, only tasks that don't have a predecessor need to be scheduled by\nthe master task (no sequential start-up overhead). \nThe tasks that have predecessors are scheduled upon the first completion of one\nof their predecessors, resulting in an $O(o.r)$ in-flight dependence overhead. \nTasks are only scheduled when all their dependences are satisfied, resulting in\nan $O(r)$ in-flight task overhead. \n\nAn $O(r)$ spatial overhead can be obtained by storing the counted dependences in\na map (for instance a hash map), at the price of more complex synchronization\nmechanisms.\n\nConsider the case where the set of tasks without predecessors is unknown\nstatically. \nSince we know the set of predecessors for each task, one solution would be to\nidentify this set by scanning all the tasks and collecting the ones with zero\npredecessors. \nUnfortunately, this would entail a worst-case sequential start-up overhead of\n$O(n)$, which can be dramatically optimized in the case of the polyhedral model,\nas presented in section \\ref{ssec:codegen-autodec}. \n\nTo avoid this need, we introduce a \\verb|preschedule| operation, in which a\ncounted dependence is atomically initialized -- as in autodecs -- but not\ndecremented.\nThe fact that the same mechanism is used by autodec and preschedule operations\nguarantees that no counted dependence will be created more than once, and that\nno task will be executed more than once. \nHence, the order in which the master task preschedules tasks and tasks\nauto-decrement their successors does not matter, and preschedule operations can\nexecute concurrently with the tasks, resulting in a $O(1)$ sequential start-up\noverhead. \n\n\\paragraph{Porting autodec principles to the tag-based model:}\nA similar synchronization combined with task initialization could be implemented\non top of Tags Method 1, using an ``auto-put'' operation, through which\nthe first predecessor to a task also sets up the task. \nUnfortunately, this method would still suffer from a higher spatial overhead\n($O(r^2)$), since one tag is associated with each dependence. \n\nIt is clear that counting could be used in Tag Method 2 \\cite{vasilache13tale}\nto reduce its garbage collection overhead to $O(r)$. \nHowever, we do not see a way around the $O(n)$ overhead for spatial occupancy\nand number of in-flight tasks. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Scalable task dependence generation} \\label{sec:scalable}\n\n\n\n\n\nAutomatic extraction of task parallelism is an attractive proposition. \nUnfortunately, existing techniques based on the polyhedral model\nweaken this proposition, because their practicality is limited by\ntheir poor algorithmic tractability.\n\nFor the sake of simplicity, in this section we assume that each tile defines a\ntask, and we use both words interchangeably.\nIn practice, a task is defined either as a tile or as a set of tiles. \nAlso, a useful guideline is that no synchronization should happen inside a \ntask, which enables the scheduler to prevent any active wait. \n\nThe base technique used by \\cite{Baskaran2009, dathathri14dataflow, kong14taco}\nto compute tiles and tile dependences is as follows.\nThe authors form dependence relationships among pairs of tiled references.\nThe dependence domain is expressed in the Cartesian product of the tiled\niteration\ndomains of the source and destination (polyhedral) statements\nThe task dependences are obtained by projecting out the intra-tile dimensions in\nboth source and destination iteration spaces. \nIn \\cite{kong14taco}, the transformations are actually expressed in terms of a\ntransformation from the iteration domain to a multi-dimensional time range\ncalled the schedule. \nUseful schedules being bijective functions, descriptions\nbased on the domain and the schedule are equivalent in practice. \nHere, we choose to use the domain-based description because it is simpler. \n\nUnfortunately, the base technique does not scale well because it relies on the\nprojection of a high-dimensional polyhedron (or of integer-valued points in the\npolyhedron).\nProjection is know to scale poorly with the number of dimensions of the\nsource polyhedron.\nThis is true even when the rational relaxation of the source polyhedron\nis considered, a valid and slightly conservative approximation in the case of\ndependences. \n\nHere, we present a technical solution which does not require the computation of\na high-dimensional dependence domain, and also does not rely on projections. \nOur technique assumes that iteration space tiling partitions computations\ninto parallelotopes. \nIn current polyhedral parallelization, hyperplanes that define the shape\nof the parallelotopes are defined by scheduling hyperplanes. \nTogether, they form a schedule, which defines a transformation of the domain. \nTiling is then performed along these hyperplanes. \nSince we are eliding the schedule in this description, parallelotope tiling\nhence corresponds to applying the transformation defined by the scheduling\nhyperplanes to the iteration domain, followed by orthogonal tiling. \nHence, without loss of generality and for the sake of clarity, we are describing\nour method assuming orthogonal tiling, where the tiling hyperplanes are defined\nby canonical vectors of the iteration space. \n\nThe main idea is to start with a {\\em pre-tiling} dependence (i.e., among\nnon-tiled iterations), and to derive the inter-tile dependences by expressing\nthe tile iteration spaces using a linear compression of the pre-tiling iteration\nspaces.\n\nSets of integer-valued points are represented in the polyhedral model by\na rational relaxation, which can be represented compactly as the integer points\nof a (rational) polyhedron.\nWe first explain our technique on a polyhedron $D$, for which we consider\na tiling transformation defined by a matrix $G$. \nWe show how to precisely define the set of tile indices that correspond to\ntiles that contain integer points in $D$.\nMore specifically, let the integer diagonal matrix with positive diagonal\nelements $G \\in {\\mathbb{Z}}^{n\\times n}$ represent the orthogonal tiling transformation\n being applied to the space of index $I\\in {\\mathbb{Z}}^n$ in which $D$ is immersed.\n \nThe relationship between an iteration $I$ and the inter-tile $T\\in{\\mathbb{Z}}^n$ and\nintra-tile $X\\in {\\mathbb{Z}}^n$ dimensions obtained by tiling $I$ according to $G$ is:\n\n", "index": 1, "text": "\\begin{equation} \\label{eq:compression}\nI = GT + X\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"I=GT+X\" display=\"block\"><mrow><mi>I</mi><mo>=</mo><mrow><mrow><mi>G</mi><mo>\u2062</mo><mi>T</mi></mrow><mo>+</mo><mi>X</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n, where $diag(G)$ is the vector made of the diagonal elements of $G$, and\n${\\vec{1}}$ is a $n$-vector of coefficients 1. \n\n$G$ being invertible, (\\ref{eq:compression}) can also be written as:\n\n", "itemtype": "equation", "pos": 35060, "prevtext": "\n\n", "index": 3, "text": "\\begin{equation} \\label{eq:modulo}\n0 \\leq X \\leq diag(G)-{\\vec{1}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"0\\leq X\\leq diag(G)-{\\vec{1}}\" display=\"block\"><mrow><mn>0</mn><mo>\u2264</mo><mi>X</mi><mo>\u2264</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mover accent=\"true\"><mn>1</mn><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nAnd (\\ref{eq:modulo}) can be written as:\n", "itemtype": "equation", "pos": 35334, "prevtext": "\n, where $diag(G)$ is the vector made of the diagonal elements of $G$, and\n${\\vec{1}}$ is a $n$-vector of coefficients 1. \n\n$G$ being invertible, (\\ref{eq:compression}) can also be written as:\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:compression2}\nT = G^{-1}I - G^{-1}X,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"T=G^{-1}I-G^{-1}X,\" display=\"block\"><mrow><mrow><mi>T</mi><mo>=</mo><mrow><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>I</mi></mrow><mo>-</mo><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>X</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nwhere elementwise division is used. \n\nLet $U$ be defined as: \n\n", "itemtype": "equation", "pos": 35453, "prevtext": "\nAnd (\\ref{eq:modulo}) can be written as:\n", "index": 7, "text": "\n\\[\n0 \\leq G^{-1}X \\leq \\frac{diag(G)-{\\vec{1}}}{diag(G)}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"0\\leq G^{-1}X\\leq\\frac{diag(G)-{\\vec{1}}}{diag(G)}\" display=\"block\"><mrow><mn>0</mn><mo>\u2264</mo><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>\u2264</mo><mfrac><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mover accent=\"true\"><mn>1</mn><mo stretchy=\"false\">\u2192</mo></mover></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n\nFrom Equation \\ref{eq:compression2}, any $T$ corresponding to an integer point\n$I$ in $D$ is defined by: \n\n", "itemtype": "equation", "pos": 35576, "prevtext": "\nwhere elementwise division is used. \n\nLet $U$ be defined as: \n\n", "index": 9, "text": "\\begin{equation} \\label{eq:U}\n\\{Y\\in {\\mathbb{Q}}^n: Y = -G^{-1}X, 0 \\leq X \\leq diag(G)-{\\vec{1}} \\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\{Y\\in{\\mathbb{Q}}^{n}:Y=-G^{-1}X,0\\leq X\\leq diag(G)-{\\vec{1}}\\}\" display=\"block\"><mrow><mo stretchy=\"false\">{</mo><mrow><mi>Y</mi><mo>\u2208</mo><msup><mi>\u211a</mi><mi>n</mi></msup></mrow><mo>:</mo><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mo>-</mo><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>X</mi></mrow></mrow></mrow><mo>,</mo><mrow><mn>0</mn><mo>\u2264</mo><mi>X</mi><mo>\u2264</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mover accent=\"true\"><mn>1</mn><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nHence, the set of values of $T$ corresponding to integer points in $D$ is given\nby\n\n", "itemtype": "equation", "pos": 35800, "prevtext": "\n\nFrom Equation \\ref{eq:compression2}, any $T$ corresponding to an integer point\n$I$ in $D$ is defined by: \n\n", "index": 11, "text": "\\begin{equation}\nT = G^{-1}I + Y, Y \\in U\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"T=G^{-1}I+Y,Y\\in U\" display=\"block\"><mrow><mrow><mi>T</mi><mo>=</mo><mrow><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>I</mi></mrow><mo>+</mo><mi>Y</mi></mrow></mrow><mo>,</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi>U</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nwhere $\\oplus$ represents the polyhedral direct sum operator. \nThis set is {\\em exact} for any given $D$ and tiling $G$ of $D$'s space. \n\nWe can apply the same method for a dependence $\\Delta(I_s, I_t)$ linking the\niteration spaces $I_s$ and $I_t$ of a source statement $s$ and a target\nstatememnt $t$. \nLet us consider tiling $G_s$ for $s$ and tiling $G_t$ for $t$. \nThe corresponding inter-tile dependence $\\Delta_T$ is the set of inter-tile\nindices $T_s$ and $T_t$ that correspond to an integer point $(I_s, I_t)$ in\n$\\Delta$. \n\nWe consider the combined compression transformation $G_{s,t}$ which applies $G_s$ to\nthe $I_s$ space and $G_t$ to the $I_t$ space:\n", "itemtype": "equation", "pos": 35940, "prevtext": "\nHence, the set of values of $T$ corresponding to integer points in $D$ is given\nby\n\n", "index": 13, "text": "\\begin{equation} \\label{eq:comp-n-plus}\nT \\in image(D, G^{-1}) \\oplus U\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"T\\in image(D,G^{-1})\\oplus U\" display=\"block\"><mrow><mi>T</mi><mo>\u2208</mo><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo>,</mo><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2295</mo><mi>U</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nLet $X_s$ and $X_t$ be the intra-tile dimensions defined by $G_{s,t}$.\nWe get a definition of $U$ in the combined source-target space as in Equation\n\\ref{eq:U}:\n\n", "itemtype": "equation", "pos": 36689, "prevtext": "\nwhere $\\oplus$ represents the polyhedral direct sum operator. \nThis set is {\\em exact} for any given $D$ and tiling $G$ of $D$'s space. \n\nWe can apply the same method for a dependence $\\Delta(I_s, I_t)$ linking the\niteration spaces $I_s$ and $I_t$ of a source statement $s$ and a target\nstatememnt $t$. \nLet us consider tiling $G_s$ for $s$ and tiling $G_t$ for $t$. \nThe corresponding inter-tile dependence $\\Delta_T$ is the set of inter-tile\nindices $T_s$ and $T_t$ that correspond to an integer point $(I_s, I_t)$ in\n$\\Delta$. \n\nWe consider the combined compression transformation $G_{s,t}$ which applies $G_s$ to\nthe $I_s$ space and $G_t$ to the $I_t$ space:\n", "index": 15, "text": "\n\\[\nG_{s,t} = \\left( \\begin{array}{{cc}} {G_s & 0 \\\\ 0 & G_t} \\end{array}\\right)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"G_{s,t}=\\left(\\begin{array}[]{{cc}}{G_{s}\\hfil&amp;0\\\\&#10;0&amp;G_{t}\\end{array}\\right)}\" display=\"block\"><mrow><msub><mi>G</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\" columnspan=\"2\"><mrow><msub><mi>G</mi><mi>s</mi></msub><mo>\u2062</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><msub><mi>G</mi><mi>t</mi></msub></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n\nHere too, since $G_{s,t}$ is invertible, we define $P = image(\\Delta,\nG_{s,t}^{-1})$ and we have:\n\n", "itemtype": "equation", "pos": 36934, "prevtext": "\nLet $X_s$ and $X_t$ be the intra-tile dimensions defined by $G_{s,t}$.\nWe get a definition of $U$ in the combined source-target space as in Equation\n\\ref{eq:U}:\n\n", "index": 17, "text": "\\begin{equation} \\label{eq:sum1}\n-G^{-1}(X_s, X_t)^T \\in U_{s,t}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"-G^{-1}(X_{s},X_{t})^{T}\\in U_{s,t}\" display=\"block\"><mrow><mrow><mo>-</mo><mrow><msup><mi>G</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>s</mi></msub><mo>,</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo>\u2208</mo><msub><mi>U</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n\nWe can hence define an exact inter-tile dependence relationship without\nresorting to forming high-dimensional polyhedra and, more importantly, without\nhaving to project any high-dimensional polyhedron. \nThe only operations we have used are a linear, invertible compression, and a\npolyhedral direct sum. \n\nWhile already much more scalable (and we will validate this later on), we could\nstill look for an even more scalable solution. \nIn particular, the direct sum of a polyhedron with a hyper-rectangle is that it\nwill results in a polyhedron with many vertices, which could reduce the\nscalability of further operations.\nThis can be addressed using a cheap, constraints-oriented way of computing a\nslight over-approximation of this particular type of direct sums, presented in\nthe next section.\n\n\\subsection{Preventing vertex explosion}\nThe following technique for reducing vertices in the task dependence polyhedron\nrelies on slightly shifting the constraints of $P$ outwards, until the\nmodified $P$ contains all the points of $P\\oplus U_{s,t}$. \nWe call this operation an {\\em inflation} of $P$ w.r.t $U_{s,t}$.\n\nAs stated above, the $U$ polyhedron defined in (\\ref{eq:U}) can be written (in\nthe $T$ space) as:\n", "itemtype": "equation", "pos": 37113, "prevtext": "\n\nHere too, since $G_{s,t}$ is invertible, we define $P = image(\\Delta,\nG_{s,t}^{-1})$ and we have:\n\n", "index": 19, "text": "\\begin{equation} \\label{eq:sum2}\n\\Delta_T = P \\oplus U_{s,t}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\Delta_{T}=P\\oplus U_{s,t}\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">\u0394</mi><mi>T</mi></msub><mo>=</mo><mrow><mi>P</mi><mo>\u2295</mo><msub><mi>U</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n$U$ is a hyper-rectangle. \nIts vertices are defined by vector $(g')$, where \n", "itemtype": "equation", "pos": 38401, "prevtext": "\n\nWe can hence define an exact inter-tile dependence relationship without\nresorting to forming high-dimensional polyhedra and, more importantly, without\nhaving to project any high-dimensional polyhedron. \nThe only operations we have used are a linear, invertible compression, and a\npolyhedral direct sum. \n\nWhile already much more scalable (and we will validate this later on), we could\nstill look for an even more scalable solution. \nIn particular, the direct sum of a polyhedron with a hyper-rectangle is that it\nwill results in a polyhedron with many vertices, which could reduce the\nscalability of further operations.\nThis can be addressed using a cheap, constraints-oriented way of computing a\nslight over-approximation of this particular type of direct sums, presented in\nthe next section.\n\n\\subsection{Preventing vertex explosion}\nThe following technique for reducing vertices in the task dependence polyhedron\nrelies on slightly shifting the constraints of $P$ outwards, until the\nmodified $P$ contains all the points of $P\\oplus U_{s,t}$. \nWe call this operation an {\\em inflation} of $P$ w.r.t $U_{s,t}$.\n\nAs stated above, the $U$ polyhedron defined in (\\ref{eq:U}) can be written (in\nthe $T$ space) as:\n", "index": 21, "text": "\n\\[\n-\\frac{g_i-1}{g_i} \\leq T_i \\leq 0\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"-\\frac{g_{i}-1}{g_{i}}\\leq T_{i}\\leq 0\" display=\"block\"><mrow><mrow><mo>-</mo><mfrac><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><msub><mi>g</mi><mi>i</mi></msub></mfrac></mrow><mo>\u2264</mo><msub><mi>T</mi><mi>i</mi></msub><mo>\u2264</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n\nConsider a constraint of $P$, written as $aT+b \\geq 0$, where $b$ may contain\nparametric expressions. \nWe are looking for an offset $c$ such that $aT+b + c \\geq 0$ contains all the\npoints of $P \\oplus U$. \n\nIn other words, \n", "itemtype": "equation", "pos": 38519, "prevtext": "\n$U$ is a hyper-rectangle. \nIts vertices are defined by vector $(g')$, where \n", "index": 23, "text": "\n\\[\ng_i' = \\left\\{ \\begin{array}{{c}} {   0 \\mbox{ or } \\\\   -\\frac{g_i-1}{g_i} } \\end{array}\\right., i \\in [1, n]\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"g_{i}^{\\prime}=\\left\\{\\begin{array}[]{{c}}{0\\mbox{ or }\\hfil\\\\&#10;-\\frac{g_{i}-1}{g_{i}}\\end{array}\\right.,i\\in[1,n]}\" display=\"block\"><mrow><mrow><msubsup><mi>g</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>=</mo><mrow><mo>{</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0or\u00a0</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mo>-</mo><mfrac><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><msub><mi>g</mi><mi>i</mi></msub></mfrac></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\nThis relationship is respected whenever $c \\geq -a(g')$. \nThe maximum value for the right-hand side occurs when $g_i'=\\frac{g_i-1}{g_i}$\nwhenever $a_i$ is positive. \nHence the maximum required value for $c$ is:\n", "itemtype": "equation", "pos": 38861, "prevtext": "\n\nConsider a constraint of $P$, written as $aT+b \\geq 0$, where $b$ may contain\nparametric expressions. \nWe are looking for an offset $c$ such that $aT+b + c \\geq 0$ contains all the\npoints of $P \\oplus U$. \n\nIn other words, \n", "index": 25, "text": "\n\\[\na(T+(g')) + b + c \\geq 0 \\Leftrightarrow aT + a(g') + b + c \\geq 0\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"a(T+(g^{\\prime}))+b+c\\geq 0\\Leftrightarrow aT+a(g^{\\prime})+b+c\\geq 0\" display=\"block\"><mrow><mrow><mrow><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>g</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi></mrow><mo>\u2265</mo><mn>0</mn></mrow><mo>\u21d4</mo><mrow><mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>T</mi></mrow><mo>+</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>g</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi></mrow><mo>\u2265</mo><mn>0</mn></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05458.tex", "nexttext": "\n\nThe inflated polyhedron is then defined by replacing the constant offset $b$\nwith $b + c_{max}(a)$ for every constraint of $P$. \nOf course, the $U$ considered for tile dependences is $U_{s,t}$ and the $G$ is\n$G_{s,t}$.\nSince the inflated task dependence polyhedron is obtained only by shifting\nconstraints of $P$, it has the exact same combinatorial structure, i.e., we\nhaven't incresased the number of vertices or constraints through inflation.\n\n\\section{Generating dependence code}\n\\label{sec:deps-codegen}\n\n\n\n\\subsection{Prescribed Dependences}\n\nIn the case of prescribed dependences, the tasks and all their dependences\nmust be declared before starting the program execution. A task is then executed\nby the runtime layer as soon as all its dependences are satisfied.  Polyhedral\ncompilers maintain during the whole compilation the exact set of iterations and\ndependences as polyhedra, which is translated into the required\nsynchronization APIs.\n\n\\begin{figure}\n\\begin{center}\n    \\includegraphics[width=1\\columnwidth]{codegen_prescribed.eps}\n\\end{center}\n\\caption{For prescribed dependences, the task creation loop is generated from\nthe tile iteration domain. Task dependences are also declared using the\ntile dependence polyhedra. The control code generated to handle cases\nwhere \\texttt{N} is not divisible by 8 is not represented to simplify the\nnotation.}\\label{fig:codegen_prescribed}\n\\end{figure}\n\nTask-based parallelization in the polyhedral model relies on forming\ntasks from tiled loop nest. Here again, for simplicity we assume a task is\nassociated with each instance of a tile, i.e., each value of the inter-tile\niterations corresponds to a task instance. \nTasks are generated as functions (or their equivalent in the targeted runtime),\nwhose parameters include their inter-tile coordinates. \nLet the ``tile iteration domain'' of the tiled statements be the set of valid\ninter-tile iterations, which corresponds to the set of non-empty tasks. \nThe tile iteration domain can be formed by using the\ncompression method of section \\ref{sec:scalable} or by projecting the iteration\ndomain on its inter-tile dimensions. \nAs illustrated in the top of Figure~\\ref{fig:codegen_prescribed},\nthe tile iteration domain is then assigned to a task initialization primitive.\nA similar tile creation loop nest is created for every\ndistinct tiled loop nest in the program, which results in the initialization\ncode for all the program tasks.\n\n\n\nOnce tasks are known to the runtime, task dependences are declared. \nAs described in section \\ref{sec:scalable}, dependences are formed as a\npolyhedron in the Cartesian product of the tile iteration spaces of the source\nand destination tiles.\nA dependence polyhedron defines a relationship between the inter-tile\ncoordinates of its source tasks and the inter-tile coordinates of its\ndestination tasks.\nSince, in the prescribed model, the role of such polyhedron is to declare the\nexistence of a dependence, in this section we call it the {\\em declarative\n  dependence polyhedron}.\nHence, as explained in \\cite{Baskaran2009}, they can naturally be generated as\nloop nests that scan all the (source task, destination task) pairs that are\nconnected by a dependence.\nA function call is generated as the body of these loops, which declares the\nexistence of the dependence for each such pair, as illustrated in\nFigure~\\ref{fig:codegen_prescribed}. \nThe loop indices are used\nas the coordinates of the task at the origin and at the destination of the\ndependence.  As shown in our example, the generated loop nest benefits from any\nof the loop optimizations applied during code generation, including their\nsimplification.\n\n\\subsection{Tags}\n\nIt is possible to generate code for Tag Methods 1 and 2 from the\ndeclarative form defined above. \n\nIn Method 1, each task first gets a tag from each of their predecessors,\nperforms computations, and puts a tag for each of their successors. \nThe get and put loops can be derived directly from the declarative dependence\nrelationship, by mapping the destination inter-tile loops of the dependence\npolyhedron to the parameters of the task. \nThe task performing the \\verb|get|s acts as the destination of the dependence\nrelationship.\nA loop that scans all the coordinate of the predecessors as a function of the\ninter-tile parameters of the task is generated from the resulting polyhedron,\nexecuting the \\verb|get|s.\nSymmetrically, the iteration domain of the \\verb|put| loop is obtained by\nmapping the source inter-tile dimensions of the declarative dependence to the\ninter-tile parameters of the task. \n\nMethod 2 is simpler in that each task runs a single \\verb|put| call, with its\nown inter-tile parameters as parameters to the \\verb|put|. The \\verb|get|s are\nobtained in the same way as for Method 1. \n\n\\begin{figure}\n\\begin{center}\n    \\includegraphics[width=1\\columnwidth]{codegen_tags.eps}\n\\end{center}\n\\caption{For tags (Method 2), each task issues a \\texttt{put} operation for\n  itself and every task waits for its predecessors using a tag \\texttt{get}\n  operation.\nThe control code generated to handle cases where \\texttt{N} is not divisible by\n8 is not represented to simplify the notation.}\\label{fig:codegen_tags}\n\\end{figure}\n\n\nThe process is illustrated in\nFigure~\\ref{fig:codegen_tags}, where the optimizations performed during code\ngeneration simplify the complex loop nest into a single\n\\texttt{get} statement parameterized by the task coordinates \\texttt{iT}.\n\n\\subsection{Autodecs}\\label{ssec:codegen-autodec}\n\n\nAutodecs use counted dependences and atomic task initialization to enable tight\ndynamic task scheduling.\nWith autodecs, only tasks with no predecessor need to be created by the master\nEDT. \nWhen a task ends, it iterates over all its successors in order to decrement its\nnumber unsatisfied dependences. \nSuch a loop is generated precisely like the \\verb|put| loop in Tag Method 1,\nexcept that the function called is \\verb|autodec| instead of \\verb|put|.\n\nThe first task which decrements the incoming dependence counter of any of its\nsuccessors also initializes the successor's counted dependence. \nTo do so, it needs to compute the number of predecessors of said successor task. \nIn order to implement this, a predecessor count function is made available\nspecifically for autodecs by the compiler. \nThis function takes the successor task's inter-tile coordinates and returns the\nnumber of its predecessors.\nThe number of predecessors is defined by the number of integer-valued points \nin the dependence polyhedron, as a function of the successor task's inter-tile\ncoordinates.\n\nThere are two possible ways of generating such a function, and both can be\ndefined from the get loop from Tag Method 1. \nOne way is as a loop, by turning the \\verb|get| calls into increments of a\ncounter. The returned value is the number of iterations in the loop, i.e., the\nnumber of predecessors to the task. \nAnother way consists in computing the enumerator of the \\verb|get| loop nest,\ni.e., an analytic function, which returns the number of integer-valued points in\nthe polyhedron representing the \\verb|get| loop, as a function of the inter-tile\nparameters of the task. \n\nHeuristics to determine which form is best are essentially based on the shape of\nthe polyhedron representing the \\verb|get| loop and an estimate of the number of\niterations.\nEnumerators are not sensitive to the number of iterations, but very much to the\nshape of the dependence polyhedron, while direct iteration counting is\ninsensitive to shape but undesirable when the number of predecessors is high.\n\n\\begin{figure}\n\\begin{center}\n    \\includegraphics[width=1\\columnwidth]{codegen_autodecs.eps}\n\\end{center}\n\\caption{With autodecs, only tasks without predecessors are explicitly\ninitialized. Every task computes its number of predecessors. \nAt completion time, tasks decrement the input dependence counter of their\nsuccessors, and may initialize them. \nThe control code generated to handle cases where \\texttt{N} is not divisible by 8\nis not represented to simplify the notation.}\\label{fig:codegen_autodecs}\n\\end{figure}\n\nThe resulting expression defines the exact number of predecessors for a task as\na function of the task coordinates and problem parameters.  The generated code\nis illustrated in Figure~\\ref{fig:codegen_autodecs}.\n\nAs opposed to the previous methods, with autodecs, the tasks are created\nby one of their predecessors. A loop scanning the tasks without predecessors\nhas to be created for execution by the master task. \n\nTo determine the set of tasks without predecessor, we project the dependence polyhedra\non their destination dimensions. \nThe projected polyhedra represent the coordinates of all the tasks with a\npredecessor. \nThe projected polyhedra are then subtracted from the target statement iteration\ndomain, which results in the set of tasks without predecessors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Experiments} \\label{sec:exp}\nIn order to validate our findings, we compare the computation time of our\npolyhedral dependence computation method with the current state-of-the-art\nmethods \\cite{Baskaran2009, kong14taco} in section \\ref{ssec:exp-compression}.\nThen, in section \\ref{ssec:exp-overhead} we explore\nthe question of the significance of the worst-case complexity analysis we\npresented in section \\ref{sec:deps-comparison} by comparing concrete values for\noverheads with high worst-case complexity.\n\n``Machine A'' is a 12-core, dual-hyperthreaded Xeon\nE5-2620 running at 2.00GHz with 32Gb RAM running Linux Ubuntu 14.04.\nOur version of the parallelizing compiler produces source code, which we compile\nwith GCC 4.8.4. \n``Machine B'' is a 32-core, dual-hyperthreaded Xeon E5-4620 running at 2.20Ghz\nwith 128Gb RAM running Ubuntu Linux 14.04. \n\n\\subsection{Compile-time dependence computation scalability}\n\\label{ssec:exp-compression}\n\nWhile it is hardly debatable that performing a linear compressions of\nlow-dimensional polyhedra is (much) less computationally expensive than\nprojecting roughly half the dimensions of a high-dimensional polyhedron, a\nfew well-chosen experiments could help evaluate the importance of the problem.\n\nIn order to perform a meaningful comparison, we enforced the same behavior of the\npolyhedral optimizations upstream (such as affine scheduling and tiling), by\nrunning the tool with default options. \nWe also turned off the removal of transitive dependences, so as to leave\ndiscussions about trade-offs between compilation time and precision of the\ndependences out of the scope of this paper. \nTransitive dependence removal hardly decreases the dimensionality of the\nproblem and increases the number of dependence polyhedra. \nWe instrument the code in order to measure dependence computation time only over\n143 benchmarks which include linear algebra, radar and signal processing codes\n(including FFT-based), stencil computations, sparse tensor codes, an\nimplementation of the Livermore benchmarks \\cite{livermore-loops}, and a handful\nof synthetic codes.\nThe speedups on Machine A are reported on a logarithmic scale in Figure\n\\ref{fig:dep-speedups}. \n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=1.5\\columnwidth,height=4cm]{dep-speedups.eps}\n\\caption{$log_{10}$ of Speedups of the compression method over the projection method}\n\\label{fig:dep-speedups}\n\\end{center}\n\\end{figure*}\n\nThe high points are as follows.\n\\begin{itemize}\n\\item Two benchmarks exceeded the 3-minute timeout in the naive method. \n  While this is a global compilation time timeout, the bottleneck there was\n  clearly the computation of the task dependences. \n  These two benchmarks were taken out of the measurements below. \n\\item The average speedup is 10.5X, the maximum (excluding timeouts) is\n  135X. In order to improve the readability of Figure \\ref{fig:dep-speedups}, we\n  arbitrarily capped the timed-out compilations to a 200X speedup. \n  These numbers imply great practical compilation time speedups, considering that\n  these operations are typically\n  the computational bottleneck in the parallelization process. These speedups are\n  relatively low, considering the combinatorial nature of the problem.\n  This is explained by the fact that we are only compiling the code with one level\n  of tiling. \n  If tiling (or any other strip-mining-based transformation) were used to target\n  more than one level of processing or memory, the base number of iteration dimensions\n  would increase significantly, and the gap between our compression method and\n  the projective method would increase dramatically. \n\\item There are a few cases where the projection method is slightly faster than\n  the compression method.\n  We looked at these cases, and the simple explanation there is that the\n  projection is very efficient for these iteration domains.\n\\item Some dependences are computed even for codes that are usually seen as\n  ``embarrassingly parallel.'' There are two reasons for this. \n  First, code is often partitioned in different ways than with loop\n  parallelism, in order to create more tasks and increase load balancing. \n  For instance, in matrix multiplication, a task is created for each tile, i.e.,\n  each iteration of the three outer loops (including the reduction loop). \n  Also, we have turned off optimizations that simplify the dependences based on\n  known parallelism information, hence dependences are sometimes built, only to\n  realize later that they are empty.\n\\end{itemize}\n\n\\subsection{Worst-case overheads}\n\\label{ssec:exp-overhead}\n\nThe problem of creating meaningful comparisons among codes generated for\ndifferent synchronization models is somewhat difficult for a few reasons. \n\nFirst, with OCR we can use prescription and autodecs, while with SWARM we can\nuse tags and autodecs. \nComparisons among specific runtimes being out of topic for this paper, we decide\nto only perform execution time comparisons within each runtime. \nWe compare execution times obtained with prescription and autodecs in OCR, and\ncompare those obtained with tags and autodecs in SWARM. \n\nSimilarly, a large space of schedules and tile sizes can be explored in the\nprocess of optimizing EDT codes. \nWhile they have a great impact on performance and are an interesting topic in\nand of themselves, they are not quite relevant to the problem we are addressing\nin this paper. \nHence, instead of comparing best execution times among all the possible compiler\noptimizations we could apply, we chose to pick a particular parallelization\nchoice (the one obtained using the default settings of the compiler), and\ncompare execution times obtained across synchronization models. \n\nThe goal of these comparisons is to evaluate the relevance of the worst-case\noverhead figures we derived in section \\ref{sec:deps-comparison}. \nWith Exascale at the horizon, we are easily convinced that they will eventually\ndo, but estimating their impact on current machines is informative of how soon\nwe should start worrying about them.\n\nWe do not intend to be exhaustive here but just understand trends, and hence we did\nnot reimplement Tag Method 2 \\cite{vasilache13tale}, which was neither the optimal\nnor presenting the most serious overhead behaviors.\n \nWe ran a sample set of benchmarks both in prescribed mode and autodec mode using\nOCR and compared their execution times on machine B. \nWe observed speedups in a majority of the benchmarks (up to 27X for a\nfixed-point Givens QR code), but roughly a third of them have slowdowns, up to\n5X for a synthetic benchmark. \nQuite systematically, the benchmarks for which the autodec version is slower\nhave short execution times, mostly below 0.1 s, suggesting that they\ncorrespond to a small number of tasks.   \n\nWe also ran a sample set of benchmarks both using Tag Method 1 and autodecs in\nSWARM. \nSpeedups are more salient there, as autodec-based versions are up to 75X faster\n(for the \\verb|trisolv| benchmark), and one slowdown is observed (10X for\n\\verb|covcol|).\nThis shows that both the synchronization model and the way it is employed should\nnot be overlooked, as they have a major impact on the generated program. \nAlso, three benchmarks did not finish in the tag-based implementation because\nthey ran out of memory, while they do not run out of memory using autodecs.\nThis shows that the $O(n^2)$ spatial cost can be {\\em already limiting} on a\n32-core machine. \n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Related Work} \\label{sec:related}\n\nThe most directly related work targets task-based parallelism from a polyhedral\nprogram representation. \nIn particular, Baskaran et al proposed a task-based strategy for\nmulticore processors using the polyhedral model~\\cite{Baskaran2009}. The\nstrategy is not intended for large scale systems and requires the full set of\ntasks dependences to be expressed before starting the execution. Our approach\nhas lower memory and computational requirements and is then much more scalable.\nMoreover, even though tile dependences are considered, they are obtained using\npolyhedral projection, which is computationally costly. A similar approach is\nconsidered more recently by Kong et al~\\cite{kong14taco}, whose focus is on the\ngeneration of OpenStream code. The same projection-based method is also used in\nthe distributed dataflow work of Dathathri et al\\cite{dathathri14dataflow}.\nIn our method, the tile dependences are deduced from the original program\nrepresentation, and without requiring any intractable polyhedral projection.\n\nThe various proposed solutions focus on different aspects of the\npolyhedral representation and are often complementary.\n\nAn important point about the compression technique is that it addresses one\nimportant tractability issue in performing computations on polyhedra during\ncompilation.\nTractability is a core problem in polyhedral compilation. \nIt cannot reasonably be ignored in a production compiler. \nHence, much work has been performed to improve tractability of polyhedral\ncompilation in the literature, often at the price of approximations or by\nintroducing extra constraints on the program representation.\nSeveral techniques restrict the set of constraints allowed to define polyhedra.\nSeveral variants of the same techniques exist, each one restricting differently\nthe form of the constraints that can be handled. For instance, Difference\nBound Matrices (DBM) only allows constraints in the form $x_i - x_j \\le k, x_i\n\\ge 0, x_j \\ge 0$~\\cite{Shaham00,Mine01DBM}. Other representations allow more\ncomplex constraints such as Unit Two Variables Per Inequality\n(UTVPI)~\\cite{Bagnara09,Mine06} or Two Variables Per Inequality\n(TVPI)~\\cite{Simon03TVPI,upadrasta12-two-vars-per-ineq-impact,upadrasta13-popl}\nfor instance. The general idea is to restrict the form of the constraints in\norder to use specialized algorithms to handle the polyhedra, usually with\nstrong worst-case complexity guarantees. In a different direction, Mehta and\nYew recently proposed to over-approximate a sequence of statements as a single\nelement called O-molecule~\\cite{Mehta15}. Their approach reduces the number\nof statements considered in a program, which drastically improves the\ncomplexity of several polyhedral operations performed during compilation. A\nsimilar solution was proposed by Feautrier~\\cite{Fea04}, and can also be\nperceived in the work of Kong et al~\\cite{kong14taco}. All the cited\nimprovements are independent from our work and can be combined with the\ndependence analysis based on tiles presented in this paper.\n\nAn alternative, scalable approach to computing tile dependences requires the\nprogrammer to express their program in terms of computation (and data) tiles, as\nin \\cite{parsec}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe second contribution of this paper improves the scalability of the runtime in\ncharge of scheduling the tasks. This is in a context where the runtime is\ngiven all dependences by the programmer, as opposed to runtimes that discover\ndependences as a function of data regions commonly accessed by tasks\n(as in \\cite{legion, starss}). \n\nOur proposed improvement does not rely on new\nlanguage constructs and can be achieved automatically, without involving the\nprogrammer. Moreover, the task dependence management we propose is not\nspecifically related to any runtime system, although some of them are better\ncandidates for an integration. We successfully implemented our optimization for\ntwo different runtimes: SWARM~\\cite{swarm}, and OCR~\\cite{ocr-spec-100}. \nFurthermore, nothing would prevent the implementation of our optimizations on any\n runtime that enables the composition of programs as a graph of tasks.\nOn the other hand, in some executions models Cilk~\\cite{Blumofe:ACM:1995} or\nX10~\\cite{x10}, the task graph is supported by a tree in which tasks can synchronize\nwith their direct or (respectively) transitive children, which is less general\nthan the model we considered.\nSuch models usually provide termination guarantees in exchange for reduced\ngenerality of the task graph model. The polyhedral model provides similar\nguarantees without specifically imposing tasks trees, although its application\ndomain is more limited than what can be written by hand using tree-supported\nlanguages. \nOpenStream~\\cite{PCo13} also provides a less restricted task model, although it\nseems geared towards much finer synchronization granularity based on streams of\ndata.\n\n\n\\section{Conclusion} \\label{sec:conclusion}\nWe presented a truly scalable solution for the generation of event-driven task\n(EDT) graphs from programs in polyhedral representation by a compiler.  We\ninvestigated tractability issues in both the compilation time and the execution\ntime of such generated programs, and offered two main contributions.\n\nFirst, we explored the use of three different synchronization models available\nin current EDT runtimes. \nWe evaluated their overheads in terms of space, in-flight task\nand dependence management, and garbage collection. We found out a way of using\na slight extension of one of the synchronization models to reach\nnear-optimal overheads across the board. \n\nSecond, we contributed a method to dramatically reduce the computation time of\nthe costliest operation required to generate EDT codes in a polyhedral compiler:\nthe generation of inter-task dependences. \nWe also discussed how to generate code for the three synchronization models from\ntheir polyhedral representation. \n\nBoth aspects unlock limitations of EDT code generation for polyhedral\ncompilers. \nWe also believe that our comparative study on synchronization models is useful\nto anyone who would want to implement an automatic code generation framework\nbased on the ones we considered.\n\nThese methods were fully implemented in the R-Stream\ncompiler~\\cite{rstream2011}, using the OCR~\\cite{ocr-spec-100} and SWARM~\\cite{swarm}\nruntimes.\nMore optimizations related to inter-task dependences have an impact on\nperformance. \nSome of them were addressed in the literature, but there are more to be done. \n\nOur polyhedral tile dependence computation method supports most practical\ntilings out-of-the-box, including diamond tiling.\nNevertheless, extensions to more exotic -- but useful -- tilings, such as hexagonal\ntiling or overlapped tiling, would be of interest as well.\n\n\\section{Acknowledgements} \\label{sec:ack}\nThis research was developed with funding from the Defense Advanced Research\nProjects Agency (DARPA).\nThe views and conclusions contained in this document are those of the authors\nand should not be interpreted as representing the official policies, either\nexpressly or implied, of DARPA or the U.S. Government. \n\n\n\n\n\n\n\n\n\n\\bibliographystyle{plain}\n\\bibliography{pca}\n\n", "itemtype": "equation", "pos": 39145, "prevtext": "\nThis relationship is respected whenever $c \\geq -a(g')$. \nThe maximum value for the right-hand side occurs when $g_i'=\\frac{g_i-1}{g_i}$\nwhenever $a_i$ is positive. \nHence the maximum required value for $c$ is:\n", "index": 27, "text": "\n\\[\nc_{max}(a) = \\sum_i a_i.g_{a,i} \\mbox{, where }\ng_{a,i} = \\left\\{ \\begin{array}{{c}} { \\frac{g_i-1}{g_i} \\mbox{ if } a_i >0\\\\ 0 \\mbox{ otherwise} } \\end{array}\\right.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"c_{max}(a)=\\sum_{i}a_{i}.g_{a,i}\\mbox{, where }g_{a,i}=\\left\\{\\begin{array}[]{%&#10;{c}}{\\frac{g_{i}-1}{g_{i}}\\mbox{ if }a_{i}&gt;0\\hfil\\\\&#10;0\\mbox{ otherwise}\\end{array}\\right.}\" display=\"block\"><mrow><mrow><mrow><msub><mi>c</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><msub><mi>a</mi><mi>i</mi></msub></mrow></mrow><mo>.</mo><mrow><mrow><msub><mi>g</mi><mrow><mi>a</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mtext>, where\u00a0</mtext><mo>\u2062</mo><msub><mi>g</mi><mrow><mi>a</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>=</mo><mrow><mo>{</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mfrac><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow><msub><mi>g</mi><mi>i</mi></msub></mfrac><mo>\u2062</mo><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><msub><mi>a</mi><mi>i</mi></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0otherwise</mtext></mrow></mtd></mtr></mtable><mi/></mrow></mrow></mrow></math>", "type": "latex"}]