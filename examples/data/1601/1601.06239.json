[{"file": "1601.06239.tex", "nexttext": "\nwhere the localization weight $W_{h,X_i}$ satisfies $W_{h,X_i}(x)>0$ and\n$\\sum_{i=1}^NW_{h,X_i}(x)=1$. Here, $h>0$ is the so-called\nlocalization parameter. Generally speaking, $W_{h,X_i}(x)$ is ``small'' if $X_i$\nis ``far'' from $x$. Two widely used  examples of LAR are\nNadaraya-Watson kernel (NWK) and $k$ nearest neighbor (KNN)\nestimates.\n\\begin{example}(NWK estimate)\nLet $K: \\mathcal X\\rightarrow\\mathbb R_+$ be a kernel function~\\citep{gyorfi2006distribution}, and $h>0$ be its bandwidth. The NWK estimate is defined by\n\n", "itemtype": "equation", "pos": 9764, "prevtext": "\n\n\\begin{center}\n{\\bf\\Large Divide and Conquer Local Average Regression}\\\\\n\\bigskip\n\nXiangyu Chang$^1$, Shaobo Lin$^{2*}$ and Yao Wang$^3$\n\\begin{footnotetext}[1]\n{$^*$Correspondence Author: sblin1983@gmail.com\n}\n\\end{footnotetext}\n\n{\\it $^1$Center of Data Science and Information Quality, School of Management, Xi'an Jiaotong University, Xi'an, China\\\\\n$^2$Department of Statistics, College of Mathematics and Information Science, Wenzhou University, Wenzhou, China\\\\\n$^3$ Department of Statistics, School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, China}\\\\\n\n\nThis Version: \\today\n\n\\end{center}\n\n\\begin{singlespace}\n\\begin{abstract}\n\nThe divide and conquer strategy, which\nbreaks a massive data set into a series of manageable data blocks, and then combines the independent results of data blocks to obtain a final decision, has been recognized as a state-of-the-art method to overcome challenges of massive data analysis. In this paper, we merge the divide and conquer strategy with local average regression methods to infer the regressive relationship of input-output pairs from a massive data set. After theoretically analyzing the pros and cons,\nwe find that although the divide and conquer local average\nregression can reach the optimal learning rate, the restriction to\nthe number of data blocks is a bit strong, which makes it only feasible\nfor small number of data blocks. We then propose two variants to lessen (or remove) this restriction. Our results show that these variants can\nachieve the optimal learning rate with much milder restriction (or\nwithout such restriction). Extensive experimental studies are carried out to verify our theoretical assertions.\n\\\\\n\n\\noindent {\\bf KEY WORDS: Divide and Conquer Strategy, Local Average Regression, Nadaraya-Watson Estimate,\n$k$ Nearest Neighbor Estimate}\n\n\n\\end{abstract}\n\\end{singlespace}\n\n\\newpage\n\n\\section{Introduction}\n\nRapid expansion of capacity in the automatic data generation and\nacquisition has made a profound impact on statistics and machine learning, as it\nbrings data in unprecedented size and complexity. {These data are}\ngenerally called as the ``massive data'' or ``big data''\n\\citep{Wu2014}. Massive data bring new opportunities of\ndiscovering subtle population patterns and heterogeneities which are\nbelieved to embody rich values and impossible to be found in\nrelatively small data sets. It, however, simultaneously leads to a\nseries of challenges such as the storage bottleneck, efficient\ncomputation, etc.~\\citep{Zhou2014}.\n\n\nTo attack the aforementioned challenges, a  divide and conquer\nstrategy was suggested and widely used in statistics and machine\nlearning communities\n\\citep{li2013statistical,Mann2009,Zhang2013,Zhang2014,dwork2010differential,battey2015distributed}.\nThis approach firstly distributes a massive data set into $m$ data\nblocks, then runs a specified learning algorithm on each data\nblock independently to get a {\\it local estimate} $\\hat{f}_{j},\nj=1,\\dots, m$ and at last transmits $m$ local estimates into one\nmachine to synthesize a {\\it global estimate} $\\bar{f}$, which is\nexpected to model the structure of original massive dataset. A\npractical and exclusive synthesizing method is the {\\it average\nmixture} (AVM)\n\\citep{li2013statistical,Mann2009,Zhang2013,Zhang2014,dwork2010differential,battey2015distributed},\ni.e., $\\bar{f} =\\frac{1}{m}\\sum_{j=1}^m\\hat{f}_{j}$.\n\nIn practice, the above divide and conquer strategy has many\napplicable scenarios. We show the following three situations as\nmotivated examples. The first one is using limited primary memory to\nhandle a massive data set. In this situation, the divide and\nconquer strategy is employed as a two-stage procedure. In the first\nstage, it reads the whole data set sequentially {block by block},\neach having a manageable sample size for the limited primary memory,\nand derives a {local estimate} based on each  block. In the\nsecond stage, it  averages {local estimates} to build up a global estimate~\\citep{li2013statistical}. The second motivated\nexample is using distributed data management systems to tackle\nmassive data. In this situation, distributed data management systems\n(e.g., Hadoop) are designed by the  divide and conquer strategy.\nThey can load the whole data set into the systems and tackle\ncomputational tasks separably and automatically. \\citet{rhipe2012}\nhas developed an integrated programing environment of R and Hadoop\n(called RHIPE) for expedient and efficient statistical computing. The third\nmotivated example is the massive data privacy. In this situation, it\ndivides a massive data set into several small pieces and combining\nthe estimates derived from these pieces for keeping the data\nprivacy~\\citep{dwork2010differential}.\n\nFor nonparametric regression, the aforementioned\n divide and conquer strategy has been shown to be efficient and\nfeasible for global modeling methods such as the kernel ridge\nregression \\citep{Zhang2014} and conditional maximum entropy model\n\\citep{Mann2009}. Compared with the global modeling methods, local\naverage regression (LAR)~\\citep{gyorfi2006distribution,fan2000prospects,tsybakov2008introduction}, such as the Nadaraya-Watson kernel (NWK) and $k$ nearest neighbor (KNN)\n estimates, benefits in computation and therefore, is also widely used in image processing~\\citep{takeda2007kernel}, power\nprediction~\\citep{kramer2010power}, recommendation\nsystem~\\citep{biau2010statistical} and financial\nengineering~\\citep{Kato2012}. LAR is by definition a learning scheme\nthat averages outputs whose corresponding inputs  satisfy certain\nlocalization assumptions. To tackle massive data  regression\nproblems, we combine the divide and conquer approach with LAR to\nproduce a new learning scheme, average mixture local average\nregression (AVM-LAR), just as \\cite{Zhang2014} did for kernel ridge\nregression.\n\n\nOur first purpose is to analyze the performance of AVM-LAR. After\nproviding the optimal learning rate of LAR,  we show that AVM-LAR\ncan also achieve this rate, provided the number of data blocks, $m$,\nis relatively small. We also prove that the restriction\nconcerning $m$ cannot be essentially improved. In a word, we provide\nboth the optimal learning rate and the essential restriction\nconcerning $m$ to guarantee the optimal rate of AVM-LAR. It should\nbe highlighted that this essential restriction is a bit strong and\nmakes AVM-LAR feasible only for small $m$. Therefore, compared with LAR, AVM-LAR does not bring the essential improvement, since we must pay much attention to\ndetermine an appropriate $m$.\n\nOur second purpose is to pursue other divide and conquer strategies to equip LAR efficiently. In particular, we provide two concrete variants of AVM-LAR in this\npaper. The first variant is motivated by the distinction between KNN\nand NWK. In our experiments, we note that the range of $m$ to\nguarantee the optimal learning rate of AVM-KNN is much larger than\nAVM-NWK. Recalling that the localization parameter of KNN depends\non data, while it doesn't hold for NWK, we propose a variant of\nAVM-LAR such that localization parameters of each data block depend on data. We establish the optimal learning\nrate of this variant with  mild restriction to $m$ and verify its\nfeasibility by numerical simulations.\n The second variant is based\non the definitions of AVM and LAR. It follows from the definition of\nLAR that the predicted value of a new input depends  on samples near\nthe input. If there are not such samples in a specified data block,\nthen this data block doesn't affect the prediction of LAR. However,\nAVM averages {local estimates} directly, neglecting the concrete\nvalue of a specified {local estimate}, which may lead to an\ninaccurate prediction. Based on this observation, we propose another\nvariant of AVM-LAR by distinguishing whether a specified data block\naffects the prediction. We provide the optimal learning rate of this\nvariant without any restriction to $m$ and also present the\nexperimental verifications.\n\nTo complete the above missions, the rest of paper is organized as\nfollows. In Section \\ref{section2}, we  present optimal learning\nrates of LAR and  AVM-LAR and analyze the pros and cons of\nAVM-LAR. In Section \\ref{section3}, we propose two new modified\nAVM-LARs to improve the performance of AVM-LAR. A set of simulation\nstudies to support the correctness of our assertions are given in\nSection \\ref{section4}. In Section \\ref{section5}, we\ndetailedly justify all the theorems. In Section \\ref{section6}, we present the conclusion and some useful remarks. \n\n\\section{Divide and Conquer Local Average Regression}\\label{section2}\n\nIn this section, after introducing some basic concepts of LAR, we\npresent a baseline of our analysis, where an optimal minimax\nlearning rate of LAR is derived. Then, we deduce the  learning rate\n  of AVM-LAR and analyze its pros and cons.\n\n\n\\subsection{Local Average Regression}\n\nLet $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be the data set where $X_i\\in\\mathcal\nX\\subseteq\\mathbb{R}^d$ is a covariant and $Y_i\\in [-M,M]$ is the\nreal-valued response. We always assume $\\mathcal X$ is a compact\nset. Suppose that samples are drawn independently and\nidentically according to an unknown joint distribution $\\rho$ over\n$\\mathcal X\\times [-M,M]$. Then the main aim of nonparametric\nregression is to construct a function $\\hat{f}:\\mathcal X\\rightarrow\n[-M,M]$ that can describe future responses based on new inputs. The\nquality of the estimate $\\hat{f}$ is measured in terms of the {\\it\nmean-squared prediction error} $\\mathbf E\\{\\hat{f}(X)-Y\\}^2$, which is\nminimized by the so-called {\\it regression function}\n$f_{\\rho}(x)=\\mathbf E\\left\\{Y|X=x\\right\\}$.\n\n\n\n\n\nLAR, as one of the most widely used\nnonparametric regression approaches, constructs an estimate formed\nas\n\n", "index": 1, "text": "\\begin{equation}\\label{local_estimation}\nf_{D,h}(x)=\\sum_{i=1}^NW_{h,X_i}(x)Y_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"f_{D,h}(x)=\\sum_{i=1}^{N}W_{h,X_{i}}(x)Y_{i},\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nand therefore,\n\n", "itemtype": "equation", "pos": 10386, "prevtext": "\nwhere the localization weight $W_{h,X_i}$ satisfies $W_{h,X_i}(x)>0$ and\n$\\sum_{i=1}^NW_{h,X_i}(x)=1$. Here, $h>0$ is the so-called\nlocalization parameter. Generally speaking, $W_{h,X_i}(x)$ is ``small'' if $X_i$\nis ``far'' from $x$. Two widely used  examples of LAR are\nNadaraya-Watson kernel (NWK) and $k$ nearest neighbor (KNN)\nestimates.\n\\begin{example}(NWK estimate)\nLet $K: \\mathcal X\\rightarrow\\mathbb R_+$ be a kernel function~\\citep{gyorfi2006distribution}, and $h>0$ be its bandwidth. The NWK estimate is defined by\n\n", "index": 3, "text": "\\begin{equation}\\label{Watson kernel estimate}\n            \\hat{f}_h(x)=\\frac{\\sum_{i=1}^NK\\left(\\frac{x-X_i}{h}\\right)Y_i}{\\sum_{i=1}^NK\\left(\\frac{x-X_i}{h}\\right)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}_{h}(x)=\\frac{\\sum_{i=1}^{N}K\\left(\\frac{x-X_{i}}{h}\\right)Y_{i}}{\\sum_%&#10;{i=1}^{N}K\\left(\\frac{x-X_{i}}{h}\\right)},\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nTwo popular kernel functions are the naive kernel,\n$K(x)={I}_{\\{\\|x\\|\\leq 1\\}}$ and Gaussian kernel\n$K(x)=\\exp\\left(-\\|x\\|^2\\right)$, where $ I_{\\{\\|x\\|\\leq 1\\}}$ is an indicator function with the feasible domain $\\|x\\|\\leq 1$ and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\\end{example}\n\n\\begin{example}(KNN estimate)\nFor $x\\in \\mathcal{X}$, let $\\{(X_{(i)}(x),Y_{(i)}(x))\\}_{i=1}^N$ be\na permutation of $\\{(X_i,Y_i)\\}_{i=1}^N$ such that\n\n", "itemtype": "equation", "pos": 10584, "prevtext": "\nand therefore,\n\n", "index": 5, "text": "$$\n            W_{h,X_i}(x)=\\frac{\n            K\\left(\\frac{x-X_i}{h}\\right)}{\\sum_{i=1}^NK\\left(\\frac{x-X_i}{h}\\right)}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"W_{h,X_{i}}(x)=\\frac{K\\left(\\frac{x-X_{i}}{h}\\right)}{\\sum_{i=1}^{N}K\\left(%&#10;\\frac{x-X_{i}}{h}\\right)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mi>h</mi></mfrac><mo>)</mo></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThen the KNN estimate is defined by\n\n", "itemtype": "equation", "pos": 11145, "prevtext": "\nTwo popular kernel functions are the naive kernel,\n$K(x)={I}_{\\{\\|x\\|\\leq 1\\}}$ and Gaussian kernel\n$K(x)=\\exp\\left(-\\|x\\|^2\\right)$, where $ I_{\\{\\|x\\|\\leq 1\\}}$ is an indicator function with the feasible domain $\\|x\\|\\leq 1$ and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\\end{example}\n\n\\begin{example}(KNN estimate)\nFor $x\\in \\mathcal{X}$, let $\\{(X_{(i)}(x),Y_{(i)}(x))\\}_{i=1}^N$ be\na permutation of $\\{(X_i,Y_i)\\}_{i=1}^N$ such that\n\n", "index": 7, "text": "$$\n            \\|x-X_{(1)}(x)\\|\\leq\\cdots\\leq \\|x-X_{(N)}(x)\\|.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\|x-X_{(1)}(x)\\|\\leq\\cdots\\leq\\|x-X_{(N)}(x)\\|.\" display=\"block\"><mrow><mrow><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><mrow><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mo>\u2264</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2264</mo><mrow><mo>\u2225</mo><mrow><mi>x</mi><mo>-</mo><mrow><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nAccording to (\\ref{local_estimation}), we have\n\n", "itemtype": "equation", "pos": 11248, "prevtext": "\nThen the KNN estimate is defined by\n\n", "index": 9, "text": "\\begin{equation}\\label{KNNE estimate}\n         \\hat{f}_k(x)=\\frac1k\\sum_{i=1}^k Y_{(i)}(x).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}_{k}(x)=\\frac{1}{k}\\sum_{i=1}^{k}Y_{(i)}(x).\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>Y</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nHere we denote the weight of KNN  as $W_{h,X_i}$ instead of\n$W_{k,X_i}$ in order to unify the notation and $h$ in KNN  depends\n  on the distribution of points and $k$.\n\\end{example}\n\n\n\n\\subsection{Optimal Learning Rate of LAR}\n\n\nThe weakly universal consistency and optimal learning rates of\nsome specified LARs have been provided by\n\\citet{gyorfi2006distribution}. In particular, \\citet[Theorem\n4.1]{gyorfi2006distribution} presented a sufficient condition to\nguarantee the weakly universal consistency of LAR. \\citet[Theorem\n5.2, Theorem 6.2]{gyorfi2006distribution} deduced  optimal learning\nrates of NWK  and KNN.\n\n\n\n\n\n\nThe aim of this subsection is to present some sufficient conditions to guarantee optimal learning rates of general LAR.\n\n\n\n\nFor $r,c_0>0$, let $\\mathcal\nF^{c_0,r}=\\{f|f:\\mathcal{X}\\rightarrow\\mathcal{Y},|f(x)-f(x')|\\leq\nc_0\\|x-x'\\|^r,\\forall x, x'\\in\\mathcal{X}\\}$. We suppose in this\npaper that $f_{\\rho} \\in \\mathcal F^{c_0,r}$. This is a commonly\naccepted prior assumption of regression function which is employed\nin\n~\\citep{tsybakov2008introduction,gyorfi2006distribution,Zhang2014}.\nThe following  Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} is our first main result.\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}\nLet $f_{D,h}$ be defined by (\\ref{local_estimation}).\nAssume that:\n\n(A) there exists a positive number $c_1$ such that\n\n", "itemtype": "equation", "pos": 11402, "prevtext": "\nAccording to (\\ref{local_estimation}), we have\n\n", "index": 11, "text": "$$\n          W_{h,X_i}(x)=\\left\\{\\begin{array}{cc} 1/k, &\n          \\mbox{if}\\  X_i\\in\\{X_{(1)},\\dots,X_{(k)}\\},\\\\\n            0, & \\mbox{otherwise.}\n            \\end{array}\\right.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"W_{h,X_{i}}(x)=\\left\\{\\begin{array}[]{cc}1/k,&amp;\\mbox{if}\\ X_{i}\\in\\{X_{(1)},%&#10;\\dots,X_{(k)}\\},\\\\&#10;0,&amp;\\mbox{otherwise.}\\end{array}\\right.\" display=\"block\"><mrow><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mrow><mrow><mpadded width=\"+5pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mtext>otherwise.</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\n(B) there exists a positive number  $c_2$  such that\n\n", "itemtype": "equation", "pos": 12999, "prevtext": "\nHere we denote the weight of KNN  as $W_{h,X_i}$ instead of\n$W_{k,X_i}$ in order to unify the notation and $h$ in KNN  depends\n  on the distribution of points and $k$.\n\\end{example}\n\n\n\n\\subsection{Optimal Learning Rate of LAR}\n\n\nThe weakly universal consistency and optimal learning rates of\nsome specified LARs have been provided by\n\\citet{gyorfi2006distribution}. In particular, \\citet[Theorem\n4.1]{gyorfi2006distribution} presented a sufficient condition to\nguarantee the weakly universal consistency of LAR. \\citet[Theorem\n5.2, Theorem 6.2]{gyorfi2006distribution} deduced  optimal learning\nrates of NWK  and KNN.\n\n\n\n\n\n\nThe aim of this subsection is to present some sufficient conditions to guarantee optimal learning rates of general LAR.\n\n\n\n\nFor $r,c_0>0$, let $\\mathcal\nF^{c_0,r}=\\{f|f:\\mathcal{X}\\rightarrow\\mathcal{Y},|f(x)-f(x')|\\leq\nc_0\\|x-x'\\|^r,\\forall x, x'\\in\\mathcal{X}\\}$. We suppose in this\npaper that $f_{\\rho} \\in \\mathcal F^{c_0,r}$. This is a commonly\naccepted prior assumption of regression function which is employed\nin\n~\\citep{tsybakov2008introduction,gyorfi2006distribution,Zhang2014}.\nThe following  Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} is our first main result.\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}\nLet $f_{D,h}$ be defined by (\\ref{local_estimation}).\nAssume that:\n\n(A) there exists a positive number $c_1$ such that\n\n", "index": 13, "text": "$$\n       \\mathbf E\\left\\{\\sum_{i=1}^NW^2_{h,X_i}(X)\\right\\}\\leq\n       \\frac{c_1}{Nh^d};\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{i=1}^{N}W^{2}_{h,X_{i}}(X)\\right\\}\\leq\\frac{c_{1}}{Nh^{%&#10;d}};\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>1</mn></msub><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nIf $h\\sim N^{-1/(2r+d)}$, then there exist  constants $C_0$ and\n$C_1$ depending only on $d$, $r$, $c_0$, $c_1$ and $c_2$ such that\n\n\n\n", "itemtype": "equation", "pos": 13146, "prevtext": "\n\n(B) there exists a positive number  $c_2$  such that\n\n", "index": 15, "text": "$$\n      \\mathbf\n      E\\left\\{ \\sum_{i=1}^NW_{h,X_i}(X)I_{\\{\\|X-X_i\\|>h\\}}\\right\\}\n      \\leq\n       \\frac{c_2}{\\sqrt{Nh^d}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{i=1}^{N}W_{h,X_{i}}(X)I_{\\{\\|X-X_{i}\\|&gt;h\\}}\\right\\}\\leq%&#10;\\frac{c_{2}}{\\sqrt{Nh^{d}}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mi>X</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>h</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>2</mn></msub><msqrt><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></msqrt></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\\end{theorem}\n\nTheorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}\npresents sufficient conditions of the localization weights to ensure the\noptimal learning rate of LAR. There are totally four constraints\nof the weights $W_{h,X_i}(\\cdot)$. The first one is the averaging\nconstraint $\\sum_{i=1}^NW_{h,X_i}(x)=1, \\ \\mbox{for all}\\\nX_i,x\\in\\mathcal\n      X.$ It essentially reflects the word ``average'' in LAR.\n\n\nThe second one is the non-negative constraint. We regard it as a\nmild constraint  as it holds for all the widely used LAR such as\nNWK and KNN. The third constraint is  condition (A), which devotes\nto controlling the scope of the weights. It aims at avoiding the\nextreme case that there is a very large weight near 1 and others are\nalmost 0. The last constraint is condition (B), which implies the\nlocalization property of LAR.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should highlight that Theorem \\ref{THEOREM:SUFFICIENT CONDITION\nFOR LOCAL ESTIMATE} is significantly  important for our analysis. On\nthe one hand, it is obvious that the AVM-LAR estimate (see Section \\ref{AVM-LARsection}) is also a new LAR estimate. Thus, Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} provides a\ntheoretical tool to derive  optimal learning rates of AVM-LAR. On\nthe other hand, Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} also provides a sanity-check that an efficient AVM-LAR\nestimate should possess the similar learning rate as\n(\\ref{theorem1}).\n\n\n\n\n\\subsection{AVM-LAR}\\label{AVM-LARsection}\n\nThe AVM-LAR estimate, which is a marriage of the classical AVM\nstrategy \\citep{Mann2009,Zhang2013,Zhang2014} and LAR, can be formulated in the  following Algorithm \\ref{AVM-LAR}.\n\n\\begin{algorithm}\\caption{AVM-LAR}\\label{AVM-LAR}\n\\begin{algorithmic}[!h]\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks, $h$ be the bandwidth\nparameter.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\overline{f}_h$.}\n\n\n\\STATE{{\\bf Division}:  Randomly divide $D$ into $m$ data blocks\n$D_1,D_2,\\dots,D_m$ such that $D=\\mathop{\\bigcup}\\limits_{j=1}^{m}\nD_j, D_i\\cap D_j=\\varnothing, i\\neq j$ and\n$|D_1|=\\dots=|D_m|=n=N/m$.}\n\n \\STATE{{\\bf Local\nprocessing}:\n           For  $j=1,2,\\dots,m$, implement LAR for the\ndata block $D_j$ to get the $j$th {\\it local estimate}\n\n", "itemtype": "equation", "pos": 13409, "prevtext": "\nIf $h\\sim N^{-1/(2r+d)}$, then there exist  constants $C_0$ and\n$C_1$ depending only on $d$, $r$, $c_0$, $c_1$ and $c_2$ such that\n\n\n\n", "index": 17, "text": "\\begin{equation}\\label{theorem1}\n           C_0N^{-2r/(2r+d)}\\leq \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|f_{D,h}-f_\\rho\\|_\\rho^2\\}\n           \\leq\n           C_1N^{-2r/(2r+d)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"C_{0}N^{-2r/(2r+d)}\\leq\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|f_%&#10;{D,h}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq C_{1}N^{-2r/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>C</mi><mn>0</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow><mo>\u2264</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m$ {\\it local estimates}\n$f_{j,h}$ to a machine, getting a {\\it global estimate} defined by\n\n", "itemtype": "equation", "pos": 15954, "prevtext": "\n\\end{theorem}\n\nTheorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}\npresents sufficient conditions of the localization weights to ensure the\noptimal learning rate of LAR. There are totally four constraints\nof the weights $W_{h,X_i}(\\cdot)$. The first one is the averaging\nconstraint $\\sum_{i=1}^NW_{h,X_i}(x)=1, \\ \\mbox{for all}\\\nX_i,x\\in\\mathcal\n      X.$ It essentially reflects the word ``average'' in LAR.\n\n\nThe second one is the non-negative constraint. We regard it as a\nmild constraint  as it holds for all the widely used LAR such as\nNWK and KNN. The third constraint is  condition (A), which devotes\nto controlling the scope of the weights. It aims at avoiding the\nextreme case that there is a very large weight near 1 and others are\nalmost 0. The last constraint is condition (B), which implies the\nlocalization property of LAR.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should highlight that Theorem \\ref{THEOREM:SUFFICIENT CONDITION\nFOR LOCAL ESTIMATE} is significantly  important for our analysis. On\nthe one hand, it is obvious that the AVM-LAR estimate (see Section \\ref{AVM-LARsection}) is also a new LAR estimate. Thus, Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} provides a\ntheoretical tool to derive  optimal learning rates of AVM-LAR. On\nthe other hand, Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} also provides a sanity-check that an efficient AVM-LAR\nestimate should possess the similar learning rate as\n(\\ref{theorem1}).\n\n\n\n\n\\subsection{AVM-LAR}\\label{AVM-LARsection}\n\nThe AVM-LAR estimate, which is a marriage of the classical AVM\nstrategy \\citep{Mann2009,Zhang2013,Zhang2014} and LAR, can be formulated in the  following Algorithm \\ref{AVM-LAR}.\n\n\\begin{algorithm}\\caption{AVM-LAR}\\label{AVM-LAR}\n\\begin{algorithmic}[!h]\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks, $h$ be the bandwidth\nparameter.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\overline{f}_h$.}\n\n\n\\STATE{{\\bf Division}:  Randomly divide $D$ into $m$ data blocks\n$D_1,D_2,\\dots,D_m$ such that $D=\\mathop{\\bigcup}\\limits_{j=1}^{m}\nD_j, D_i\\cap D_j=\\varnothing, i\\neq j$ and\n$|D_1|=\\dots=|D_m|=n=N/m$.}\n\n \\STATE{{\\bf Local\nprocessing}:\n           For  $j=1,2,\\dots,m$, implement LAR for the\ndata block $D_j$ to get the $j$th {\\it local estimate}\n\n", "index": 19, "text": "$$\n        f_{j,h}(x)=\\sum_{(X_i,Y_i)\\in D_j}W_{X_i,h}(x)Y_i.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"f_{j,h}(x)=\\sum_{(X_{i},Y_{i})\\in D_{j}}W_{X_{i},h}(x)Y_{i}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><msub><mi>W</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\nIn Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, we show that this\nsimple generalization of LAR achieves the optimal learning rate\nwith a rigorous condition concerning $m$. We also show that this condition is essential.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{THEOREM DISTRBUTED LAE POSITIVE}\nLet $\\overline{f}_h$ be defined by (\\ref{global_RERM}) and $h_{D_j}$\n be the mesh norm of the data block $D_j$ defined by $\n                 h_{D_j}:=\\max\\limits_{X\\in\\mathcal X}\\min\\limits_{X_i\\in\n                 D_j}\\|X-X_i\\|.\n$\n Suppose that\n\n(C) for all $D_1,\\dots,D_m$, there exists a positive number $c_3$\nsuch that\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m$ {\\it local estimates}\n$f_{j,h}$ to a machine, getting a {\\it global estimate} defined by\n\n", "index": 21, "text": "\\begin{equation}\\label{global_RERM}\n            \\overline{f}_h=\\frac{1}{m}\\sum_{j=1}^mf_{j,h}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\overline{f}_{h}=\\frac{1}{m}\\sum_{j=1}^{m}f_{j,h}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\n\n(D) for all $D_1,\\dots,D_m$, there holds almost surely\n\n", "itemtype": "equation", "pos": 16927, "prevtext": "}\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\nIn Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, we show that this\nsimple generalization of LAR achieves the optimal learning rate\nwith a rigorous condition concerning $m$. We also show that this condition is essential.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}\\label{THEOREM DISTRBUTED LAE POSITIVE}\nLet $\\overline{f}_h$ be defined by (\\ref{global_RERM}) and $h_{D_j}$\n be the mesh norm of the data block $D_j$ defined by $\n                 h_{D_j}:=\\max\\limits_{X\\in\\mathcal X}\\min\\limits_{X_i\\in\n                 D_j}\\|X-X_i\\|.\n$\n Suppose that\n\n(C) for all $D_1,\\dots,D_m$, there exists a positive number $c_3$\nsuch that\n\n", "index": 23, "text": "$$\n       \\mathbf E\\left\\{\\sum_{(X_i,Y_i)\\in D_j}W^2_{h,X_i}(X)\\right\\}\\leq\n       \\frac{c_3}{nh^d};\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{(X_{i},Y_{i})\\in D_{j}}W^{2}_{h,X_{i}}(X)\\right\\}\\leq%&#10;\\frac{c_{3}}{nh^{d}};\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><msubsup><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>3</mn></msub><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nIf $h\\sim N^{-1/(2r+d)}$, and the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j$\\} holds, then there exists a constant $C_2$ depending only on $d,r,M,c_0,c_3$ and $c_4$ such that \n\n\n", "itemtype": "equation", "pos": 17088, "prevtext": "\n\n\n(D) for all $D_1,\\dots,D_m$, there holds almost surely\n\n", "index": 25, "text": "$$\n          W_{X_i,h}I_{\\{\\|x-X_i\\|>h\\}}=0.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"W_{X_{i},h}I_{\\{\\|x-X_{i}\\|&gt;h\\}}=0.\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>h</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nOtherwise, if the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} dose not\nhold, then for arbitrary $h\\geq\\frac12(n+2)^{-1/d}$,  there exists a\ndistribution  $\\rho$ such that\n\n", "itemtype": "equation", "pos": 17309, "prevtext": "\nIf $h\\sim N^{-1/(2r+d)}$, and the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j$\\} holds, then there exists a constant $C_2$ depending only on $d,r,M,c_0,c_3$ and $c_4$ such that \n\n\n", "index": 27, "text": "\\begin{equation}\\label{theorem1.11111}\n           C_0N^{-2r/(2r+d)}\\leq \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|\\overline{f}_{h}-f_\\rho\\|_\\rho^2\\}\n           \\leq\n           C_2N^{-2r/(2r+d)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"C_{0}N^{-2r/(2r+d)}\\leq\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|%&#10;\\overline{f}_{h}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq C_{2}N^{-2r/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>C</mi><mn>0</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow><mo>\u2264</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>C</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\n\\end{theorem}\n\nThe assertions in Theorem \\ref{THEOREM DISTRBUTED LAE\nPOSITIVE} can be divided into two parts. The first one is the\npositive assertion, which means that if some  conditions of the\nweights and  an extra constraint of the data blocks are imposed, then\nthe AVM-LAR estimate (\\ref{global_RERM}) possesses the same learning\nrate as that in (\\ref{theorem1}) by taking the same localization\nparameter $h$ (ignoring constants). This means that the proposed\n divide and conquer operation in\n Algorithm \\ref{AVM-LAR} doesn't affect the learning rate under this circumstance.  In fact,\nwe can relax the restriction (D) for the bound\n(\\ref{theorem1.11111}) to the following condition (D$^*$).\n\n (D$^*$)  For all $D_1,\\dots,D_m$, there exists a positive number\n$c_4$ such that\n\n", "itemtype": "equation", "pos": 17703, "prevtext": "\nOtherwise, if the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} dose not\nhold, then for arbitrary $h\\geq\\frac12(n+2)^{-1/d}$,  there exists a\ndistribution  $\\rho$ such that\n\n", "index": 29, "text": "\\begin{equation}\\label{theorem1.22222222222}\n            \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|\\overline{f}_{h}-f_\\rho\\|_\\rho^2\\}\n           \\geq \\frac{M^2\\{(2h)^{-d}-2\\}}{3n}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|\\overline{f}_{h}-f_{\\rho}%&#10;\\|_{\\rho}^{2}\\}\\geq\\frac{M^{2}\\{(2h)^{-d}-2\\}}{3n}.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2265</mo><mfrac><mrow><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>h</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mi>d</mi></mrow></msup><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><mn>3</mn><mo>\u2062</mo><mi>n</mi></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nTo guarantee the optimal minimax learning rate of AVM-LAR, condition\n(C) is the same as   condition (A) by noting that there are only $n$\nsamples in each $D_j$. Moreover,  condition (D$^*$) is a bit\nstronger than condition (B) as there are totally $n$ samples in\n$D_j$ but the localization bound of it is $c_4/(\\sqrt{Nh^d})$.\nHowever, we should point out that such a restriction is also  mild,\nsince in almost all   widely used LAR, the localization bound either\nis  0 (see NWK with naive kernel, and KNN) or decreases\nexponentially (such as NWK with Gaussian kernel). All the above\nmethods satisfy conditions (C) and (D$^*$).\n\nThe negative assertion, however, shows that if the event \\{there is\na $D_j$ such that $h_{D_j}>h$\\} holds, then for any\n$h\\geq\\frac12(n+2)^{-1/d}$, the learning rate of AVM-LAR isn't\nfaster than $\\frac{1}{nh^d}$. It follows from Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} that the best\nlocalization parameter to guarantee the optimal  learning rate\nsatisfies {$h\\sim N^{-1/(2r+d)}$}. The condition\n$h\\geq\\frac12(n+2)^{-1/d}$ implies that if the best parameter is\nselected, then $m$ should satisfy $m\\leq \\mathcal O(N^{2r/(2r+d)})$.\n\n\n\n\n\n\n\nUnder this condition, from\n  (\\ref{theorem1.22222222222}), we have\n\n", "itemtype": "equation", "pos": 18695, "prevtext": "\n\n\\end{theorem}\n\nThe assertions in Theorem \\ref{THEOREM DISTRBUTED LAE\nPOSITIVE} can be divided into two parts. The first one is the\npositive assertion, which means that if some  conditions of the\nweights and  an extra constraint of the data blocks are imposed, then\nthe AVM-LAR estimate (\\ref{global_RERM}) possesses the same learning\nrate as that in (\\ref{theorem1}) by taking the same localization\nparameter $h$ (ignoring constants). This means that the proposed\n divide and conquer operation in\n Algorithm \\ref{AVM-LAR} doesn't affect the learning rate under this circumstance.  In fact,\nwe can relax the restriction (D) for the bound\n(\\ref{theorem1.11111}) to the following condition (D$^*$).\n\n (D$^*$)  For all $D_1,\\dots,D_m$, there exists a positive number\n$c_4$ such that\n\n", "index": 31, "text": "$$\n      \\mathbf\n      E\\left\\{\\sum_{(X_i,Y_i)\\in D_j}|W_{h,X_i}(X)|I_{\\{\\|X-X_i\\|>h\\}}\\right\\}\n      \\leq\n       \\frac{c_4}{\\sqrt{Nh^d}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{(X_{i},Y_{i})\\in D_{j}}|W_{h,X_{i}}(X)|I_{\\{\\|X-X_{i}\\|%&#10;&gt;h\\}}\\right\\}\\leq\\frac{c_{4}}{\\sqrt{Nh^{d}}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mi>X</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>h</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>4</mn></msub><msqrt><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></msqrt></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThis means, if we select $h\\sim N^{-1/(2r+d)}$ and $m\\leq \\mathcal\nO(N^{2r/(2r+d)})$, then the learning rate of AVM-LAR is essentially\nslower than that in (\\ref{theorem1}). If we select a smaller $h$,\nthen the above inequality also yields the similar conclusion. If we\nselect a larger $h$, however, the\n approximation error (see the proof of Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE})\n is $\\mathcal O(h^{2r})$ which\ncan be larger than the learning rate in (\\ref{theorem1}). In a word,\nif the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} does not hold, then\nthe AVM operation essentially degrades the optimal learning rate of\nLAR.\n\nAt last, we should discuss the probability of the event \\{$\nh_{D_j}\\leq h$ for all $D_j$\\}. As $\\mathbf P \\{\n      h_{D_j}\\leq h  \\ \\mbox{for all}\\  D_j \\} =1-m\\mathbf\n      P\\{h_{D_1}>h\\},$\n and it can   be found in \\citep[P.93-94]{gyorfi2006distribution} that\n$\\mathbf P\\{h_{D_1}>h\\} \\leq \\frac{c}{nh^d}$, we have $\\mathbf P \\{\n      h_{D_j}\\leq h  \\ \\mbox{for all}\\  D_j \\} \\geq\n      1-\\frac{m}{nh^d}.$\nWhen $h\\sim (mn)^{-1/(2r+d)}$, we have\n\n", "itemtype": "equation", "pos": 20090, "prevtext": "\nTo guarantee the optimal minimax learning rate of AVM-LAR, condition\n(C) is the same as   condition (A) by noting that there are only $n$\nsamples in each $D_j$. Moreover,  condition (D$^*$) is a bit\nstronger than condition (B) as there are totally $n$ samples in\n$D_j$ but the localization bound of it is $c_4/(\\sqrt{Nh^d})$.\nHowever, we should point out that such a restriction is also  mild,\nsince in almost all   widely used LAR, the localization bound either\nis  0 (see NWK with naive kernel, and KNN) or decreases\nexponentially (such as NWK with Gaussian kernel). All the above\nmethods satisfy conditions (C) and (D$^*$).\n\nThe negative assertion, however, shows that if the event \\{there is\na $D_j$ such that $h_{D_j}>h$\\} holds, then for any\n$h\\geq\\frac12(n+2)^{-1/d}$, the learning rate of AVM-LAR isn't\nfaster than $\\frac{1}{nh^d}$. It follows from Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} that the best\nlocalization parameter to guarantee the optimal  learning rate\nsatisfies {$h\\sim N^{-1/(2r+d)}$}. The condition\n$h\\geq\\frac12(n+2)^{-1/d}$ implies that if the best parameter is\nselected, then $m$ should satisfy $m\\leq \\mathcal O(N^{2r/(2r+d)})$.\n\n\n\n\n\n\n\nUnder this condition, from\n  (\\ref{theorem1.22222222222}), we have\n\n", "index": 33, "text": "$$\n         \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|\\overline{f}_{h}-f_\\rho\\|_\\rho^2\\}\n           \\geq  \\frac{C}{nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|\\overline{f}_{h}-f_{\\rho}%&#10;\\|_{\\rho}^{2}\\}\\geq\\frac{C}{nh^{d}}.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2265</mo><mfrac><mi>C</mi><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThe above quantity is small when $m$ is large, which means\n that the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} has a significant\nchance to be broken down.\n By\nusing the method in \\citep[Problem 2.4]{gyorfi2006distribution}, we\ncan  show that  the above estimate for the confidence is essential\nin the sense that for the uniform distribution, the equality holds\nfor some constant $c'$.\n\n\n\n\n\\section{Modified AVM-LAR}\\label{section3}\n\nAs shown in Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, if\n$h_{D_j}\\leq h$ does not hold for some $D_j$, then AVM-LAR cannot\nreach the optimal learning rate. In this section, we propose two\nvariants of AVM-LAR such that they can achieve the optimal learning\nrate under mild conditions.\n\n\\subsection{AVM-LAR with data-dependent parameters}\n\nThe event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} essentially implies\nthat for arbitrary $x$, there is at least one sample in the ball\n$B_{h}(x):=\\{x'\\in \\mathbb{R}^d: \\|x-x'\\|\\leq h$\\}. This condition holds for KNN  as the parameter $h$ in KNN changes\nwith respect to  samples. However, for NWK and other local average\nmethods (e.g., partition estimation~\\citep{gyorfi2006distribution}), such a condition usually fails.  Motivated by KNN, it is\nnatural to select a sample-dependent localization $h$ to ensure the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j$\\}. Therefore, we propose a\nvariant of AVM-LAR with data-dependent parameters in Algorithm\n\\ref{AVM-LAR-alg2}.\n\n\\begin{algorithm}\\caption{AVM-LAR with data-dependent parameters}\\label{AVM-LAR-alg2}\n\\begin{algorithmic}[!h]\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\tilde{f}_{\\tilde{h}}$.}\n\n\n\\STATE{{\\bf Division}: Randomly divide $D$ into $m$ data blocks\n$D_1,D_2,\\dots,D_m$ such that $D=\\mathop{\\bigcup}\\limits_{j=1}^{m}\nD_j, D_i\\cap D_j=\\varnothing, i\\neq j$ and $|D_1|=\\dots=|D_m|=n=N/m$.\nCompute the mesh norms $h_{D_1},\\dots, h_{D_m}$, and select\n$\\tilde{h}\\geq h_{D_j}$, $j=1,2,\\dots,m$.\n  }\n\\STATE{{\\bf Local processing}:\n           For any $j=1,2,\\dots,m$, implement   LAR with bandwidth parameter $\\tilde{h}$ for the\ndata block $D_j$ to get the $j$th {\\it local estimate}\n\n", "itemtype": "equation", "pos": 21324, "prevtext": "\nThis means, if we select $h\\sim N^{-1/(2r+d)}$ and $m\\leq \\mathcal\nO(N^{2r/(2r+d)})$, then the learning rate of AVM-LAR is essentially\nslower than that in (\\ref{theorem1}). If we select a smaller $h$,\nthen the above inequality also yields the similar conclusion. If we\nselect a larger $h$, however, the\n approximation error (see the proof of Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE})\n is $\\mathcal O(h^{2r})$ which\ncan be larger than the learning rate in (\\ref{theorem1}). In a word,\nif the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} does not hold, then\nthe AVM operation essentially degrades the optimal learning rate of\nLAR.\n\nAt last, we should discuss the probability of the event \\{$\nh_{D_j}\\leq h$ for all $D_j$\\}. As $\\mathbf P \\{\n      h_{D_j}\\leq h  \\ \\mbox{for all}\\  D_j \\} =1-m\\mathbf\n      P\\{h_{D_1}>h\\},$\n and it can   be found in \\citep[P.93-94]{gyorfi2006distribution} that\n$\\mathbf P\\{h_{D_1}>h\\} \\leq \\frac{c}{nh^d}$, we have $\\mathbf P \\{\n      h_{D_j}\\leq h  \\ \\mbox{for all}\\  D_j \\} \\geq\n      1-\\frac{m}{nh^d}.$\nWhen $h\\sim (mn)^{-1/(2r+d)}$, we have\n\n", "index": 35, "text": "$$\n      \\mathbf P \\{\n      h_{D_j}\\leq h  \\ \\mbox{for all}\\  D_j \\} \\geq\n      1-c' \\frac{m^{(2r+2d)/(2r+d)}}{n^{2r/(2r+d)}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{P}\\{h_{D_{j}}\\leq h\\ \\mbox{for all}\\ D_{j}\\}\\geq 1-c^{\\prime}\\frac{m^{%&#10;(2r+2d)/(2r+d)}}{n^{2r/(2r+d)}}.\" display=\"block\"><mrow><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>h</mi><msub><mi>D</mi><mi>j</mi></msub></msub><mo>\u2264</mo><mpadded width=\"+5pt\"><mi>h</mi></mpadded><mpadded width=\"+5pt\"><mtext>for all</mtext></mpadded><msub><mi>D</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo>\u2265</mo><mn>1</mn><mo>-</mo><msup><mi>c</mi><mo>\u2032</mo></msup><mfrac><msup><mi>m</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>d</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><msup><mi>n</mi><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m$ {\\it local estimates}\n$f_{j,\\tilde{h}}$ to a machine, getting a {\\it global estimate}\ndefined by\n\n", "itemtype": "equation", "pos": 23670, "prevtext": "\nThe above quantity is small when $m$ is large, which means\n that the event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} has a significant\nchance to be broken down.\n By\nusing the method in \\citep[Problem 2.4]{gyorfi2006distribution}, we\ncan  show that  the above estimate for the confidence is essential\nin the sense that for the uniform distribution, the equality holds\nfor some constant $c'$.\n\n\n\n\n\\section{Modified AVM-LAR}\\label{section3}\n\nAs shown in Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, if\n$h_{D_j}\\leq h$ does not hold for some $D_j$, then AVM-LAR cannot\nreach the optimal learning rate. In this section, we propose two\nvariants of AVM-LAR such that they can achieve the optimal learning\nrate under mild conditions.\n\n\\subsection{AVM-LAR with data-dependent parameters}\n\nThe event \\{$ h_{D_j}\\leq h$ for all $D_j$\\} essentially implies\nthat for arbitrary $x$, there is at least one sample in the ball\n$B_{h}(x):=\\{x'\\in \\mathbb{R}^d: \\|x-x'\\|\\leq h$\\}. This condition holds for KNN  as the parameter $h$ in KNN changes\nwith respect to  samples. However, for NWK and other local average\nmethods (e.g., partition estimation~\\citep{gyorfi2006distribution}), such a condition usually fails.  Motivated by KNN, it is\nnatural to select a sample-dependent localization $h$ to ensure the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j$\\}. Therefore, we propose a\nvariant of AVM-LAR with data-dependent parameters in Algorithm\n\\ref{AVM-LAR-alg2}.\n\n\\begin{algorithm}\\caption{AVM-LAR with data-dependent parameters}\\label{AVM-LAR-alg2}\n\\begin{algorithmic}[!h]\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\tilde{f}_{\\tilde{h}}$.}\n\n\n\\STATE{{\\bf Division}: Randomly divide $D$ into $m$ data blocks\n$D_1,D_2,\\dots,D_m$ such that $D=\\mathop{\\bigcup}\\limits_{j=1}^{m}\nD_j, D_i\\cap D_j=\\varnothing, i\\neq j$ and $|D_1|=\\dots=|D_m|=n=N/m$.\nCompute the mesh norms $h_{D_1},\\dots, h_{D_m}$, and select\n$\\tilde{h}\\geq h_{D_j}$, $j=1,2,\\dots,m$.\n  }\n\\STATE{{\\bf Local processing}:\n           For any $j=1,2,\\dots,m$, implement   LAR with bandwidth parameter $\\tilde{h}$ for the\ndata block $D_j$ to get the $j$th {\\it local estimate}\n\n", "index": 37, "text": "$$\n        f_{j,\\tilde{h}}(x)=\\sum_{(X_i,Y_i)\\in D_j}W_{X_i,\\tilde{h}}(x)Y_i.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"f_{j,\\tilde{h}}(x)=\\sum_{(X_{i},Y_{i})\\in D_{j}}W_{X_{i},\\tilde{h}}(x)Y_{i}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><msub><mi>W</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "}\n\\end{algorithmic}\n\\end{algorithm}\n\nComparing with AVM-LAR in Algorithm \\ref{AVM-LAR}, the only\ndifference of Algorithm \\ref{AVM-LAR-alg2} is  the division step,\nwhere we select the bandwidth parameter to be  greater than all\n$h_{D_j}, j =1,\\dots,m$. The following  Theorem \\ref{THEOREM:\nAVM-LAE} states the theoretical merit of AVM-LAR with\ndata-dependent parameters.\n\n\\begin{theorem}\\label{THEOREM: AVM-LAE}\nLet $r<d/2$, $\\tilde{f}_{\\tilde{h}}$ be defined by\n(\\ref{global_AVM_LAR1}). Assume (C) and (D$^*$) hold. Suppose\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m$ {\\it local estimates}\n$f_{j,\\tilde{h}}$ to a machine, getting a {\\it global estimate}\ndefined by\n\n", "index": 39, "text": "\\begin{equation}\\label{global_AVM_LAR1}\n            \\tilde{f}_{\\tilde{h}}=\\frac{1}{m}\\sum_{j=1}^mf_{j,\\tilde{h}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\tilde{f}_{\\tilde{h}}=\\frac{1}{m}\\sum_{j=1}^{m}f_{j,\\tilde{h}}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nand\n$m\\leq\\left(\\frac{\nc_0^2(2r+d)+8d(c_3+2c_4^2)M^2}{4r(c_0^2+2)}\\right)^{d/(2r)}N^{2r/(2r+d)},\n$ then there exists a constant $C_3$ depending only on $c_0,c_3,c_4,r,d$ and $M$ such that\n\n\n", "itemtype": "equation", "pos": 24547, "prevtext": "}\n\\end{algorithmic}\n\\end{algorithm}\n\nComparing with AVM-LAR in Algorithm \\ref{AVM-LAR}, the only\ndifference of Algorithm \\ref{AVM-LAR-alg2} is  the division step,\nwhere we select the bandwidth parameter to be  greater than all\n$h_{D_j}, j =1,\\dots,m$. The following  Theorem \\ref{THEOREM:\nAVM-LAE} states the theoretical merit of AVM-LAR with\ndata-dependent parameters.\n\n\\begin{theorem}\\label{THEOREM: AVM-LAE}\nLet $r<d/2$, $\\tilde{f}_{\\tilde{h}}$ be defined by\n(\\ref{global_AVM_LAR1}). Assume (C) and (D$^*$) hold. Suppose\n\n", "index": 41, "text": "$$\n\\tilde{h}=\\max\\{m^{-1/(2r+d)}\\max_j\\{h_{D_j}^{d/(2r+d)}\\},\\max_j\\{h_{D_j}\\}\\},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h}=\\max\\{m^{-1/(2r+d)}\\max_{j}\\{h_{D_{j}}^{d/(2r+d)}\\},\\max_{j}\\{h_{D_{%&#10;j}}\\}\\},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>m</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup><mo>\u2062</mo><mrow><munder><mi>max</mi><mi>j</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>h</mi><msub><mi>D</mi><mi>j</mi></msub><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo><mrow><munder><mi>max</mi><mi>j</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>h</mi><msub><mi>D</mi><mi>j</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\\end{theorem}\n\nTheorem \\ref{THEOREM: AVM-LAE} shows that if the localization parameter\nis selected elaborately, then AVM-LAR can achieve the optimal\nlearning rate under mild conditions concerning $m$. It should be\nnoted that there is an additional restriction to the smoothness\ndegree, $r<d/2$. We highlight that this condition cannot be removed.\nIn fact, without this condition, (\\ref{theorem3}) may not hold for\nsome marginal distribution $\\rho_X$. For example, let $d=1$, it can\nbe deduced from \\citep[Problem 6.1]{gyorfi2006distribution} that\nthere exists a $\\rho_{X}$ such that (\\ref{theorem3}) doesn't hold.\nHowever, if we don't aim at deriving a distribution free result, we\ncan fix this condition by using the technique in \\citep[Problem\n6.7]{gyorfi2006distribution}. Actually, for $d\\leq 2r$,  assume the\nmarginal distribution $\\rho_X$ satisfies that there exist\n$\\varepsilon_0>0$, a nonnegative function $g$ such that for all\n$x\\in\\mathcal X$, and $0<\\varepsilon\\leq \\varepsilon_0$ satisfying\n$\\rho_X(B_\\varepsilon(x))>g(x)\\varepsilon^d,$ and $\\int_{\\mathcal\n         X}\\frac{1}{g^{2/d}(x)}d\\rho_X<\\infty,$\nthen (\\ref{theorem3}) holds for arbitrary $r$ and $d$. It is obvious\nthat the   uniform distribution satisfies the above conditions.\n\nInstead of imposing a restriction to $h_{D_j}$, Theorem\n\\ref{THEOREM: AVM-LAE} states that after using the data-dependent\nparameter $\\tilde{h}$, AVM-LAR doesn't degrade the learning rate\nfor a large range of $m$. We should illustrate that the derived\nbound of $m$ cannot be improved further. Indeed, it can be found in\nour proof that  the bias of AVM-LAR can be bounded by $C\\mathbf\nE\\{\\tilde{h}^{2r}\\}$. Under the conditions of Theorem \\ref{THEOREM:\nAVM-LAE}, if $m\\sim N^{(2r+\\varepsilon)/(2r+d)}$, then for arbitrary\n$D_j$, there holds\n$\\mathbf E\\{ {h_{D_j}}\\}\\leq Cn^{-1/d}\n      = C(N/m)^{-1/d}\\leq CN^{(d-\\varepsilon)/(2r+d)}.$\nThus, it is easy to check that $\\mathbf E\\{\\tilde{h}^{2r}\\}\\leq\nCN^{(-2r+\\varepsilon)/(2r+d)}$, which implies a learning rate slower\nthan $N^{-2r/(2r+d)}$.\n\n\\subsection{Qualified AVM-LAR}\n\nAlgorithm \\ref{AVM-LAR-alg2} provided an intuitive way to improve\nthe performance of AVM-LAR. However,  Algorithm \\ref{AVM-LAR-alg2}\n  increases the computational complexity of AVM-LAR, because we\nhave to compute the mesh norm $h_{D_j},j=1,\\dots, m$. A natural\nquestion is whether we can avoid this procedure while maintaining\nthe learning performance. The following Algorithm \\ref{AVM-LAR-alg3}\nprovides a possible way to tackle this question.\n\n\n\\begin{algorithm}\\caption{Qualified AVM-LAR}\\label{AVM-LAR-alg3}\n\\begin{algorithmic}\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks, $h$ be the bandwidth parameter.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\hat{f}_h$.}\n\n\n\\STATE {{\\bf Division}: Randomly divide $D$ into $m$ data blocks,\ni.e. $D=\\cup_{j=1}^mD_j$ with $D_j\\cap D_k=\\varnothing$ for $k\\neq\nj$ and $|D_1|=\\dots=|D_m|=n$.}\n\n\\STATE {{\\bf Qualification}: For a test input $x$, if there exists\nan $X_{0}^j\\in D_j$ such that $|x-X_0^j|\\leq h,$ then we qualify\n$D_j$ as an active data block for the {\\it local estimate}. Rewrite\nall the active data blocks as $T_1,\\dots,T_{m_0}$.}\n\n\\STATE {{\\bf Local processing }:  For arbitrary data block $T_j$,\n$j=1,\\dots,m_0$, define\n\n", "itemtype": "equation", "pos": 24820, "prevtext": "\nand\n$m\\leq\\left(\\frac{\nc_0^2(2r+d)+8d(c_3+2c_4^2)M^2}{4r(c_0^2+2)}\\right)^{d/(2r)}N^{2r/(2r+d)},\n$ then there exists a constant $C_3$ depending only on $c_0,c_3,c_4,r,d$ and $M$ such that\n\n\n", "index": 43, "text": "\\begin{equation}\\label{theorem3}\n           C_0N^{-2r/(2r+d)}\\leq \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|\\tilde{f}_{\\tilde{h}}-f_\\rho\\|_\\rho^2\\}\n           \\leq\n           C_3N^{-2r/(2r+d)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"C_{0}N^{-2r/(2r+d)}\\leq\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|%&#10;\\tilde{f}_{\\tilde{h}}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq C_{3}N^{-2r/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>C</mi><mn>0</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow><mo>\u2264</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>C</mi><mn>3</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m_0$ {\\it local estimates}\n$f_{j,{h}}$ to a machine, getting a {\\it global estimate} defined by\n\n", "itemtype": "equation", "pos": 28361, "prevtext": "\n\\end{theorem}\n\nTheorem \\ref{THEOREM: AVM-LAE} shows that if the localization parameter\nis selected elaborately, then AVM-LAR can achieve the optimal\nlearning rate under mild conditions concerning $m$. It should be\nnoted that there is an additional restriction to the smoothness\ndegree, $r<d/2$. We highlight that this condition cannot be removed.\nIn fact, without this condition, (\\ref{theorem3}) may not hold for\nsome marginal distribution $\\rho_X$. For example, let $d=1$, it can\nbe deduced from \\citep[Problem 6.1]{gyorfi2006distribution} that\nthere exists a $\\rho_{X}$ such that (\\ref{theorem3}) doesn't hold.\nHowever, if we don't aim at deriving a distribution free result, we\ncan fix this condition by using the technique in \\citep[Problem\n6.7]{gyorfi2006distribution}. Actually, for $d\\leq 2r$,  assume the\nmarginal distribution $\\rho_X$ satisfies that there exist\n$\\varepsilon_0>0$, a nonnegative function $g$ such that for all\n$x\\in\\mathcal X$, and $0<\\varepsilon\\leq \\varepsilon_0$ satisfying\n$\\rho_X(B_\\varepsilon(x))>g(x)\\varepsilon^d,$ and $\\int_{\\mathcal\n         X}\\frac{1}{g^{2/d}(x)}d\\rho_X<\\infty,$\nthen (\\ref{theorem3}) holds for arbitrary $r$ and $d$. It is obvious\nthat the   uniform distribution satisfies the above conditions.\n\nInstead of imposing a restriction to $h_{D_j}$, Theorem\n\\ref{THEOREM: AVM-LAE} states that after using the data-dependent\nparameter $\\tilde{h}$, AVM-LAR doesn't degrade the learning rate\nfor a large range of $m$. We should illustrate that the derived\nbound of $m$ cannot be improved further. Indeed, it can be found in\nour proof that  the bias of AVM-LAR can be bounded by $C\\mathbf\nE\\{\\tilde{h}^{2r}\\}$. Under the conditions of Theorem \\ref{THEOREM:\nAVM-LAE}, if $m\\sim N^{(2r+\\varepsilon)/(2r+d)}$, then for arbitrary\n$D_j$, there holds\n$\\mathbf E\\{ {h_{D_j}}\\}\\leq Cn^{-1/d}\n      = C(N/m)^{-1/d}\\leq CN^{(d-\\varepsilon)/(2r+d)}.$\nThus, it is easy to check that $\\mathbf E\\{\\tilde{h}^{2r}\\}\\leq\nCN^{(-2r+\\varepsilon)/(2r+d)}$, which implies a learning rate slower\nthan $N^{-2r/(2r+d)}$.\n\n\\subsection{Qualified AVM-LAR}\n\nAlgorithm \\ref{AVM-LAR-alg2} provided an intuitive way to improve\nthe performance of AVM-LAR. However,  Algorithm \\ref{AVM-LAR-alg2}\n  increases the computational complexity of AVM-LAR, because we\nhave to compute the mesh norm $h_{D_j},j=1,\\dots, m$. A natural\nquestion is whether we can avoid this procedure while maintaining\nthe learning performance. The following Algorithm \\ref{AVM-LAR-alg3}\nprovides a possible way to tackle this question.\n\n\n\\begin{algorithm}\\caption{Qualified AVM-LAR}\\label{AVM-LAR-alg3}\n\\begin{algorithmic}\n\\STATE {{\\bf Initialization}: Let $D=\\{(X_i,Y_i)\\}_{i=1}^N$ be $N$\nsamples, $m$ be the number of data blocks, $h$ be the bandwidth parameter.}\n\n\\STATE{ {\\bf Output}: The global estimate $\\hat{f}_h$.}\n\n\n\\STATE {{\\bf Division}: Randomly divide $D$ into $m$ data blocks,\ni.e. $D=\\cup_{j=1}^mD_j$ with $D_j\\cap D_k=\\varnothing$ for $k\\neq\nj$ and $|D_1|=\\dots=|D_m|=n$.}\n\n\\STATE {{\\bf Qualification}: For a test input $x$, if there exists\nan $X_{0}^j\\in D_j$ such that $|x-X_0^j|\\leq h,$ then we qualify\n$D_j$ as an active data block for the {\\it local estimate}. Rewrite\nall the active data blocks as $T_1,\\dots,T_{m_0}$.}\n\n\\STATE {{\\bf Local processing }:  For arbitrary data block $T_j$,\n$j=1,\\dots,m_0$, define\n\n", "index": 45, "text": "$$\n           f_{j,h}(x)=\\sum_{(X_i,Y_i)\\in T_j}W_{X_i,h}(x)Y_i.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"f_{j,h}(x)=\\sum_{(X_{i},Y_{i})\\in T_{j}}W_{X_{i},h}(x)Y_{i}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>T</mi><mi>j</mi></msub></mrow></munder><mrow><msub><mi>W</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n}\n\n\\end{algorithmic}\n\\end{algorithm}\n\nComparing with Algorithms \\ref{AVM-LAR} and \\ref{AVM-LAR-alg2}, the\nonly difference of Algorithm \\ref{AVM-LAR-alg3} is the qualification\nstep which essentially doesn't need extra   computation. In fact,\nthe qualification and local processing steps can be implemented\nsimultaneously.  It should be further mentioned that the\nqualification step actually eliminates   the data blocks which have\na chance to break down the event \\{$h_{D_j}\\leq h$ for all $D_j$\\}.\nAlthough, this strategy may loss a  part of information, we show\nthat the qualified AVM-LAR can achieve the optimal learning rate\nwithout any restriction to $m$.\n\n\\begin{theorem}\\label{THEOREM: DLAE}\nLet $\\hat{f}_{h}$ be defined by (\\ref{New AVM}).  Assume\n(C) holds and\n\n(E) for all $D_1,\\dots,D_m$, there exists a positive number\n$c_5$ such that\n\n", "itemtype": "equation", "pos": 28569, "prevtext": "\n}\n\n\\STATE{{\\bf Synthesization}: Transmit   $m_0$ {\\it local estimates}\n$f_{j,{h}}$ to a machine, getting a {\\it global estimate} defined by\n\n", "index": 47, "text": "\\begin{equation}\\label{New AVM}\n            \\hat{f}_h=\\frac1{m_0}\\sum_{j=1}^{m_0}f_{j,h}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\hat{f}_{h}=\\frac{1}{m_{0}}\\sum_{j=1}^{m_{0}}f_{j,h}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>m</mi><mn>0</mn></msub></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mn>0</mn></msub></munderover><msub><mi>f</mi><mrow><mi>j</mi><mo>,</mo><mi>h</mi></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\n\nIf $h\\sim N^{-1/(2r+d)}$, then there exists a constant $C_4$\ndepending only on $c_0,c_1,c_3,c_5,r,d$ and $M$ such that\n\n", "itemtype": "equation", "pos": 29520, "prevtext": "\n}\n\n\\end{algorithmic}\n\\end{algorithm}\n\nComparing with Algorithms \\ref{AVM-LAR} and \\ref{AVM-LAR-alg2}, the\nonly difference of Algorithm \\ref{AVM-LAR-alg3} is the qualification\nstep which essentially doesn't need extra   computation. In fact,\nthe qualification and local processing steps can be implemented\nsimultaneously.  It should be further mentioned that the\nqualification step actually eliminates   the data blocks which have\na chance to break down the event \\{$h_{D_j}\\leq h$ for all $D_j$\\}.\nAlthough, this strategy may loss a  part of information, we show\nthat the qualified AVM-LAR can achieve the optimal learning rate\nwithout any restriction to $m$.\n\n\\begin{theorem}\\label{THEOREM: DLAE}\nLet $\\hat{f}_{h}$ be defined by (\\ref{New AVM}).  Assume\n(C) holds and\n\n(E) for all $D_1,\\dots,D_m$, there exists a positive number\n$c_5$ such that\n\n", "index": 49, "text": "$$\n      \\mathbf\n      E\\left\\{ \\sum_{i=1}^n|W_{h,X_i}(X)|I_{\\{\\|X-X_i\\|>h\\}}\\right\\}\n      \\leq\n       \\frac{c_{5}}{m\\sqrt{nh^d}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{i=1}^{n}|W_{h,X_{i}}(X)|I_{\\{\\|X-X_{i}\\|&gt;h\\}}\\right\\}%&#10;\\leq\\frac{c_{5}}{m\\sqrt{nh^{d}}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mi>X</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>h</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>5</mn></msub><mrow><mi>m</mi><mo>\u2062</mo><msqrt><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></msqrt></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\\end{theorem}\n\nIn Theorem \\ref{THEOREM: AVM-LAE}, we declare that AVM-LAR with\ndata-dependent parameter doesn't slow down  the learning rate of\nLAR. However, the bound of $m$ in Theorem \\ref{THEOREM: AVM-LAE}\ndepends on the smoothness of the regression function, which is\nusually unknown in the real world applications. This makes $m$ be a\npotential parameter in AVM-LAR with data-dependent parameter, as we\ndo not know which $m$ definitely works. However, Theorem\n\\ref{THEOREM: DLAE} states that we can avoid this problem by\nintroducing a qualification step. The theoretical price of such an\nimprovement is only to use  condition (E) to take place condition\n(D$^*$). As shown above, all the widely used LARs such as the\npartition estimate, NWK with naive kernel, NWK  with Gaussian kernel\nand KNN satisfy condition (E) (with a logarithmic term for NWK\nwith Gaussian kernel).\n\n\\section{Experiments}\\label{section4}\nIn this section, we report experimental studies on synthetic data\nsets to demonstrate the\nperformances of  AVM-LAR and its variants. We employ three criteria\nfor the comparison purpose. The first criterion is the {\\it global\nerror} (GE) which is the mean square error of testing set when $N$\nsamples are used as a training set. We use  GE as a baseline that\ndoes not change with respect to $m$. The second criterion is the\n{\\it local error} (LE) which is the mean square error of testing set\nwhen we use only one data block ($n$ samples) as a training set. The\nthird criterion is the {\\it average error} (AE) which is the mean\nsquare error of AVM-LAR (including Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}).\n\\subsection{Simulation 1}\nWe use a fixed total number of samples\n$N=10,000$, but assume that the number of data blocks $m$ (the data block size\n$n=N/m$) and dimensionality $d$ are varied. The\nsimulation results are based on the average values of 20 trails.\n\nWe generate data from the following regression models $\n            y = g_j(x)+\\varepsilon, \\ j=1,2,\n$ where $\\varepsilon$ is the Gaussian noise $\\mathcal{N}(0,0.1)$,\n\n", "itemtype": "equation", "pos": 29776, "prevtext": "\n\n\nIf $h\\sim N^{-1/(2r+d)}$, then there exists a constant $C_4$\ndepending only on $c_0,c_1,c_3,c_5,r,d$ and $M$ such that\n\n", "index": 51, "text": "\\begin{equation}\\label{theorem dlae}\n           C_0N^{-2r/(2r+d)}\\leq \\sup_{f_\\rho\\in\\mathcal\n           F^{c_0,r}}\\mathbf E\\{\\|\\hat{f}_{h}-f_\\rho\\|_\\rho^2\\}\n           \\leq\n           C_4N^{-2r/(2r+d)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"C_{0}N^{-2r/(2r+d)}\\leq\\sup_{f_{\\rho}\\in\\mathcal{F}^{c_{0},r}}\\mathbf{E}\\{\\|%&#10;\\hat{f}_{h}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq C_{4}N^{-2r/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>C</mi><mn>0</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow><mo>\u2264</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><msub><mi>c</mi><mn>0</mn></msub><mo>,</mo><mi>r</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">^</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><msub><mi>C</mi><mn>4</mn></msub><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 32067, "prevtext": "\n\\end{theorem}\n\nIn Theorem \\ref{THEOREM: AVM-LAE}, we declare that AVM-LAR with\ndata-dependent parameter doesn't slow down  the learning rate of\nLAR. However, the bound of $m$ in Theorem \\ref{THEOREM: AVM-LAE}\ndepends on the smoothness of the regression function, which is\nusually unknown in the real world applications. This makes $m$ be a\npotential parameter in AVM-LAR with data-dependent parameter, as we\ndo not know which $m$ definitely works. However, Theorem\n\\ref{THEOREM: DLAE} states that we can avoid this problem by\nintroducing a qualification step. The theoretical price of such an\nimprovement is only to use  condition (E) to take place condition\n(D$^*$). As shown above, all the widely used LARs such as the\npartition estimate, NWK with naive kernel, NWK  with Gaussian kernel\nand KNN satisfy condition (E) (with a logarithmic term for NWK\nwith Gaussian kernel).\n\n\\section{Experiments}\\label{section4}\nIn this section, we report experimental studies on synthetic data\nsets to demonstrate the\nperformances of  AVM-LAR and its variants. We employ three criteria\nfor the comparison purpose. The first criterion is the {\\it global\nerror} (GE) which is the mean square error of testing set when $N$\nsamples are used as a training set. We use  GE as a baseline that\ndoes not change with respect to $m$. The second criterion is the\n{\\it local error} (LE) which is the mean square error of testing set\nwhen we use only one data block ($n$ samples) as a training set. The\nthird criterion is the {\\it average error} (AE) which is the mean\nsquare error of AVM-LAR (including Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}).\n\\subsection{Simulation 1}\nWe use a fixed total number of samples\n$N=10,000$, but assume that the number of data blocks $m$ (the data block size\n$n=N/m$) and dimensionality $d$ are varied. The\nsimulation results are based on the average values of 20 trails.\n\nWe generate data from the following regression models $\n            y = g_j(x)+\\varepsilon, \\ j=1,2,\n$ where $\\varepsilon$ is the Gaussian noise $\\mathcal{N}(0,0.1)$,\n\n", "index": 53, "text": "\\begin{equation}\\label{simulation1}\ng_1(x)=\\left\\{\n\\begin{array}{cc}\n(1-2x)_+^3(1+6x), & 0<x\\leq 0.5 \\\\\n0 & x>0.5\n\\end{array}\n\\right.,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"g_{1}(x)=\\left\\{\\begin{array}[]{cc}(1-2x)_{+}^{3}(1+6x),&amp;0&lt;x\\leq 0.5\\\\&#10;0&amp;x&gt;0.5\\end{array}\\right.,\" display=\"block\"><mrow><mrow><mrow><msub><mi>g</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mn>3</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mn>6</mn><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mn>0</mn><mo>&lt;</mo><mi>x</mi><mo>\u2264</mo><mn>0.5</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><mrow><mi>x</mi><mo>&gt;</mo><mn>0.5</mn></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n\\citet{Wendland2005} revealed that $g_1$ and $g_2$ are the so-called\nWendland functions with the property $g_1,g_2\\in\n\\mathcal{F}^{c_0,1}$ and $g_1,g_2\\notin \\mathcal{F}^{c_0,2}$ for\nsome absolute constant $c_0$. The simulated $N$ samples are drawn\ni.i.d. according to the uniform distribution on the (hyper-)cube\n$[0,1]^d$. We also generate 1000 test samples $(X_i',Y_i')$ with\n$X_i'$ drawn i.i.d. according to the uniform distribution and\n$Y_i'=g_j(X_i'), j=1,2$.\n\n\nOn the basis of above setting,  we illustrate two\nsimulation results. The first one is to compare the learning\nperformance between Algorithm \\ref{AVM-LAR} and LAR. Both NWK  and\nKNN  are  considered. The second one is to show how  Algorithms\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}   overcome Algorithm\n\\ref{AVM-LAR}'s weakness. Because Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} are the same for KNN, KNN is not considered in this part. The detailed implementation of\nNWK and KNN is specified as follows.\n\\begin{itemize}\n\n\\item NWK: In Algorithm 1\nand Algorithm \\ref{AVM-LAR-alg3}, for each $m\\in\\{5,10,\\dots,350\\}$,  the bandwidth\n parameter satisfies  $h\\sim N^{-\\frac{1}{2r+d}}$ according to\n Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE} and Theorem \\ref{THEOREM: DLAE}.\nIn Algorithm \\ref{AVM-LAR-alg2}, we set $\\tilde{h}\\sim\n \\max\\{m^{-1/(2r+d)}\\max_j\\{h_{D_j}^{d/(2r+d)}\\},\\max_j\\{h_{D_j}\\}\\}$\naccording to Theorem \\ref{THEOREM: AVM-LAE}.\n\n\\item KNN: According to Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, the parameter $k$ is set to $k\\sim\\frac{N^{\\frac{2r}{2r+d}}}{m}$. However,\nas $k\\geq 1$,  the range of $m$ should satisfy\n$m\\in\\{1,2,\\dots,N^{\\frac{2r}{2r+d}}\\}$.\n\\end{itemize}\nTo present   proper constants for the localization parameters (e.g.,\n$h=cN^{-\\frac{1}{2r+d}}$), we use the 5-fold cross-validation method in simulations. Based on these strategies, we obtain the simulation results in the following Figures \\ref{simulations1_NWK}, \\ref{simulations1_KNN} and \\ref{simulations2_NWK}.\n\nAs shown in  Figure \\ref{simulations1_NWK}, AEs are smaller than\nLEs, which means  AVM-NWK outperforms NWK  with only one data block.\nFurthermore, AEs of NWK are comparable with GEs when $m$ is not\ntoo big and  there exists an upper bound  of the number of data\nblocks, $m'$, lager than which  the curve of AE increases\ndramatically. Moreover, $m'$ decreases when $d$ increases.\n\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=400pt]{SIM1.pdf}\n  \\vspace{-15mm}\n\\caption{\\emph{The first row shows AEs, LEs and GEs of NWK for\ndifferent $m$. The second row shows the number of inactive machines\nwhich satisfy $h_{D_j}>h$. The vertical axis of the second row of\nFigure \\ref{simulations1_NWK} is the number of inactive data blocks\nwhich break  down the condition $h_{D_j}\\leq h$.\n}}\\label{simulations1_NWK}\n  \\end{center}\n\\end{figure*}\n\nLet us explain these above phenomena. If only one data block is\nutilized, then it follows from Theorem \\ref{THEOREM:SUFFICIENT\nCONDITION FOR LOCAL ESTIMATE} that $\\min\\limits_{j=1,\\dots,m}\\mathbf\nE\\{\\|{f}_{j,h}-f_\\rho\\|_\\rho^2\\}=\\mathcal{O}(n^{-\\frac{2r}{2r+d}})$,\nwhich is far larger than $\\mathcal{O}(N^{-\\frac{2r}{2r+d}})$ for\nAVM-NWK due to Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. Thus,\nAEs are  smaller than LEs.  Moreover, Theorem \\ref{THEOREM\nDISTRBUTED LAE POSITIVE}  implies that AEs are comparable with GE as\nlong as the  event \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\}\nholds. To verify this assertion, we record the number of data blocks\nbreaking down the condition $h_{D_j}\\leq h$ for different $m$ in the\nsecond row of Figure \\ref{simulations1_NWK}. It can be observed that the\ndramatically increasing time of the number of inactive data blocks\nand AEs are almost same. This result is extremely consistent with\nthe negative part of Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}.\nBecause   large $d$ and  $m$ lead to a higher probability to break\ndown the event \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\}.\nThus   $m'$ decreases when $d$ increases.\n\n\\begin{figure*}[ht]\n\\begin{center}\n \\includegraphics[width=430pt]{SIM1_KNN.pdf}\n  \\vspace{-15mm}\n\\caption{\\emph{ AEs, LEs and GEs of KNN for different\n$m$.}}\\label{simulations1_KNN}\n  \\end{center}\n\\end{figure*}\n\n\nCompared with AVM-NWK, AVM-KNN shows significant different results\nin Figure \\ref{simulations1_KNN}. In fact, there isn't a similar\n$m'$ to guarantee comparable AEs and GE for AVM-KNN. The reason is\n   KNN selects a data-dependent bandwidth $h$ which makes the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\} always holds.\nThis result is extremely consistent with the positive part of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. However, we find that\nAVM-KNN has a design deficiency. To be detailed, the range of $m$\nmust be in $\\{1,2,\\dots,N^{\\frac{2r}{2r+d}}\\}$ due to $k\\geq 1$.\n\n\n\n\n\n\n\nIn Figure \\ref{simulations2_NWK}, AEs of Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} which are denoted by\nAE-A1, AE-A2 and AE-A3. We can find that AE-A1, AE-A2 and AE-A3 have\nsimilar  values which are comparable with GE when $m\\leq m'$. The\nreason is that the small number of data blocks can guarantee the\nevent \\{$h_{D_j}\\leq h$ for all $D_j, j=1,\\dots,m$\\}. Under this\ncircumstance, Theorems \\ref{THEOREM DISTRBUTED LAE POSITIVE},\n\\ref{THEOREM: AVM-LAE} and \\ref{THEOREM: DLAE} yield that all these\n  estimates can reach  optimal learning rates. As $m$ increasing,\nthe event \\{$h_{D_j}>h$ for some $j$\\} inevitably happens, then\nAlgorithm \\ref{AVM-LAR} fails according to the negative part of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. At the same time,\nAE-A1 begins to increase dramatically. As Algorithms\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}  are designed  to avoid\nthe weakness of Algorithm \\ref{AVM-LAR}, the curves of AE-A2 and\nAE-A3 are always below that of AE-A1 when $m>m'$. Another\ninteresting fact is that AE-A3 is smaller than AE-A2,\nalthough both of them all can achieve the same learning rate in  theory.\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=450pt]{S2.pdf}\n  \\vspace{-25mm}\n\\caption{\\emph{AE-A1, AE-A2, AE-A3 and GE of the simulation. The curves of AE-A2 and AE-A3 are always below AE-A1's to illustrate the improved capability of modified AVM-LARs.}}\\label{simulations2_NWK}\n  \\end{center}\n\\end{figure*}\n\n\\subsection{Simulation 2}\nWe make use of the same simulation study which is conducted by\n\\citet{Zhang2014} for comparing the learning performance of AVM-NWK\n(including Algorithms \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and\n\\ref{AVM-LAR-alg3}) and the divide and conquer kernel ridge regression\n(DKRR for short).\n\nWe generate data from the regression model $y=g_3(x)+\\epsilon$,\nwhere $g_3(x)=\\min(x,1-x)$, the noise variable $\\epsilon$ is\nnormally distributed with mean 0 and variance $\\sigma^2=1/5$, and\n$X_i,i=1,\\dots,N$ are simulated from a uniform distribution in\n$[0,1]$ independently. In \\citet{Zhang2014}'s simulation, DKRR used\nthe kernel function $K(x,x')=1+\\min\\{x,x'\\}$, and regularization\nparameter $\\lambda = N^{-2/3}$ due to $g_3\\in \\mathcal{F}^{c_0,1}$\nfor some absolute constant $c_0$. We also use $N=10,000$ training\nsamples, and 1,000 test samples. The parameter setting of Algorithms\n\\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} is the same\nas that in Simulation 1.\n\n\\vspace{5mm}\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=300pt]{s2comp.pdf}\n\n\\caption{\\emph{AEs of Algorithm \\ref{AVM-LAR},  \\ref{AVM-LAR-alg2},\n\\ref{AVM-LAR-alg3} and DKRR.\n}}\\label{simulations2_com}\n  \\end{center}\n\\end{figure*}\n\nIn Figure \\ref{simulations2_com}, we plot AEs of Algorithms\n\\ref{AVM-LAR}, \\ref{AVM-LAR-alg2}, \\ref{AVM-LAR-alg3} and DKRR for\n$m\\in\\{2^3,2^4,\\dots, 2^{11}\\}$. Figure \\ref{simulations2_com} shows\nAEs of Algorithm \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2},\n\\ref{AVM-LAR-alg3} and DKRR are comparable when $m<256$. As long as\n$m>256$, AEs of Algorithm \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and DKRR\nincrease dramatically. However, AEs of Algorithm \\ref{AVM-LAR-alg3}\nare  stable. The reason is that, to keep the optimal learning rates,\nDKRR needs $m = \\mathcal{O}(N^{1/3})$~\\citep{Zhang2014}, and\nAlgorithm \\ref{AVM-LAR-alg2} needs $m=\\mathcal{O}(N^{2/3})$, while\nAlgorithm \\ref{AVM-LAR-alg3} holds for all $m$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Proofs}\\label{section5}\n\n\n\n\n\\subsection{Proof of Theorem 1}\nLet $f_{\\rho,h}(x)= \\sum_{i=1}^NW_{h,X_i}(x)f_\\rho(X_i).$\nThen, it is obvious that $f_{\\rho,h}(x)=\\mathbf E^*\\{f_{D,h}(x)\\},$\nwhere $\\mathbf  E^*\\{\\cdot\\}=\\mathbf E\\{\\cdot|X_1,X_2,\\dots,X_n\\}$.\nTherefore, we can deduce\n\n", "itemtype": "equation", "pos": 32221, "prevtext": "\nand\n\n", "index": 55, "text": "\\begin{equation}\\label{simulation2}\ng_2(x)=\\left\\{\n\\begin{array}{cc}\n(1-\\|x\\|)_+^5(1+5\\|x\\|) + \\frac{1}{5}\\|x\\|^2, & 0<\\|x\\|\\leq 1, x\\in\\mathbb{R}^5 \\\\\n \\frac{1}{5}\\|x\\|^2 & \\|x\\|>1\n\\end{array}\n\\right..\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"g_{2}(x)=\\left\\{\\begin{array}[]{cc}(1-\\|x\\|)_{+}^{5}(1+5\\|x\\|)+\\frac{1}{5}\\|x%&#10;\\|^{2},&amp;0&lt;\\|x\\|\\leq 1,x\\in\\mathbb{R}^{5}\\\\&#10;\\frac{1}{5}\\|x\\|^{2}&amp;\\|x\\|&gt;1\\end{array}\\right..\" display=\"block\"><mrow><mrow><mrow><msub><mi>g</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mrow><msubsup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mn>5</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mn>5</mn><mo>\u2062</mo><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>5</mn></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mrow><mn>0</mn><mo>&lt;</mo><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mo>\u2264</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mn>5</mn></msup></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mfrac><mn>1</mn><mn>5</mn></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mtd><mtd columnalign=\"center\"><mrow><mrow><mo>\u2225</mo><mi>x</mi><mo>\u2225</mo></mrow><mo>&gt;</mo><mn>1</mn></mrow></mtd></mtr></mtable><mi/></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThat is,\n\n", "itemtype": "equation", "pos": 41034, "prevtext": "\n\\citet{Wendland2005} revealed that $g_1$ and $g_2$ are the so-called\nWendland functions with the property $g_1,g_2\\in\n\\mathcal{F}^{c_0,1}$ and $g_1,g_2\\notin \\mathcal{F}^{c_0,2}$ for\nsome absolute constant $c_0$. The simulated $N$ samples are drawn\ni.i.d. according to the uniform distribution on the (hyper-)cube\n$[0,1]^d$. We also generate 1000 test samples $(X_i',Y_i')$ with\n$X_i'$ drawn i.i.d. according to the uniform distribution and\n$Y_i'=g_j(X_i'), j=1,2$.\n\n\nOn the basis of above setting,  we illustrate two\nsimulation results. The first one is to compare the learning\nperformance between Algorithm \\ref{AVM-LAR} and LAR. Both NWK  and\nKNN  are  considered. The second one is to show how  Algorithms\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}   overcome Algorithm\n\\ref{AVM-LAR}'s weakness. Because Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} are the same for KNN, KNN is not considered in this part. The detailed implementation of\nNWK and KNN is specified as follows.\n\\begin{itemize}\n\n\\item NWK: In Algorithm 1\nand Algorithm \\ref{AVM-LAR-alg3}, for each $m\\in\\{5,10,\\dots,350\\}$,  the bandwidth\n parameter satisfies  $h\\sim N^{-\\frac{1}{2r+d}}$ according to\n Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE} and Theorem \\ref{THEOREM: DLAE}.\nIn Algorithm \\ref{AVM-LAR-alg2}, we set $\\tilde{h}\\sim\n \\max\\{m^{-1/(2r+d)}\\max_j\\{h_{D_j}^{d/(2r+d)}\\},\\max_j\\{h_{D_j}\\}\\}$\naccording to Theorem \\ref{THEOREM: AVM-LAE}.\n\n\\item KNN: According to Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}, the parameter $k$ is set to $k\\sim\\frac{N^{\\frac{2r}{2r+d}}}{m}$. However,\nas $k\\geq 1$,  the range of $m$ should satisfy\n$m\\in\\{1,2,\\dots,N^{\\frac{2r}{2r+d}}\\}$.\n\\end{itemize}\nTo present   proper constants for the localization parameters (e.g.,\n$h=cN^{-\\frac{1}{2r+d}}$), we use the 5-fold cross-validation method in simulations. Based on these strategies, we obtain the simulation results in the following Figures \\ref{simulations1_NWK}, \\ref{simulations1_KNN} and \\ref{simulations2_NWK}.\n\nAs shown in  Figure \\ref{simulations1_NWK}, AEs are smaller than\nLEs, which means  AVM-NWK outperforms NWK  with only one data block.\nFurthermore, AEs of NWK are comparable with GEs when $m$ is not\ntoo big and  there exists an upper bound  of the number of data\nblocks, $m'$, lager than which  the curve of AE increases\ndramatically. Moreover, $m'$ decreases when $d$ increases.\n\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=400pt]{SIM1.pdf}\n  \\vspace{-15mm}\n\\caption{\\emph{The first row shows AEs, LEs and GEs of NWK for\ndifferent $m$. The second row shows the number of inactive machines\nwhich satisfy $h_{D_j}>h$. The vertical axis of the second row of\nFigure \\ref{simulations1_NWK} is the number of inactive data blocks\nwhich break  down the condition $h_{D_j}\\leq h$.\n}}\\label{simulations1_NWK}\n  \\end{center}\n\\end{figure*}\n\nLet us explain these above phenomena. If only one data block is\nutilized, then it follows from Theorem \\ref{THEOREM:SUFFICIENT\nCONDITION FOR LOCAL ESTIMATE} that $\\min\\limits_{j=1,\\dots,m}\\mathbf\nE\\{\\|{f}_{j,h}-f_\\rho\\|_\\rho^2\\}=\\mathcal{O}(n^{-\\frac{2r}{2r+d}})$,\nwhich is far larger than $\\mathcal{O}(N^{-\\frac{2r}{2r+d}})$ for\nAVM-NWK due to Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. Thus,\nAEs are  smaller than LEs.  Moreover, Theorem \\ref{THEOREM\nDISTRBUTED LAE POSITIVE}  implies that AEs are comparable with GE as\nlong as the  event \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\}\nholds. To verify this assertion, we record the number of data blocks\nbreaking down the condition $h_{D_j}\\leq h$ for different $m$ in the\nsecond row of Figure \\ref{simulations1_NWK}. It can be observed that the\ndramatically increasing time of the number of inactive data blocks\nand AEs are almost same. This result is extremely consistent with\nthe negative part of Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}.\nBecause   large $d$ and  $m$ lead to a higher probability to break\ndown the event \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\}.\nThus   $m'$ decreases when $d$ increases.\n\n\\begin{figure*}[ht]\n\\begin{center}\n \\includegraphics[width=430pt]{SIM1_KNN.pdf}\n  \\vspace{-15mm}\n\\caption{\\emph{ AEs, LEs and GEs of KNN for different\n$m$.}}\\label{simulations1_KNN}\n  \\end{center}\n\\end{figure*}\n\n\nCompared with AVM-NWK, AVM-KNN shows significant different results\nin Figure \\ref{simulations1_KNN}. In fact, there isn't a similar\n$m'$ to guarantee comparable AEs and GE for AVM-KNN. The reason is\n   KNN selects a data-dependent bandwidth $h$ which makes the\nevent \\{$ h_{D_j}\\leq h$ for all $D_j, j=1,\\dots, m$\\} always holds.\nThis result is extremely consistent with the positive part of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. However, we find that\nAVM-KNN has a design deficiency. To be detailed, the range of $m$\nmust be in $\\{1,2,\\dots,N^{\\frac{2r}{2r+d}}\\}$ due to $k\\geq 1$.\n\n\n\n\n\n\n\nIn Figure \\ref{simulations2_NWK}, AEs of Algorithms \\ref{AVM-LAR},\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} which are denoted by\nAE-A1, AE-A2 and AE-A3. We can find that AE-A1, AE-A2 and AE-A3 have\nsimilar  values which are comparable with GE when $m\\leq m'$. The\nreason is that the small number of data blocks can guarantee the\nevent \\{$h_{D_j}\\leq h$ for all $D_j, j=1,\\dots,m$\\}. Under this\ncircumstance, Theorems \\ref{THEOREM DISTRBUTED LAE POSITIVE},\n\\ref{THEOREM: AVM-LAE} and \\ref{THEOREM: DLAE} yield that all these\n  estimates can reach  optimal learning rates. As $m$ increasing,\nthe event \\{$h_{D_j}>h$ for some $j$\\} inevitably happens, then\nAlgorithm \\ref{AVM-LAR} fails according to the negative part of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}. At the same time,\nAE-A1 begins to increase dramatically. As Algorithms\n\\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3}  are designed  to avoid\nthe weakness of Algorithm \\ref{AVM-LAR}, the curves of AE-A2 and\nAE-A3 are always below that of AE-A1 when $m>m'$. Another\ninteresting fact is that AE-A3 is smaller than AE-A2,\nalthough both of them all can achieve the same learning rate in  theory.\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=450pt]{S2.pdf}\n  \\vspace{-25mm}\n\\caption{\\emph{AE-A1, AE-A2, AE-A3 and GE of the simulation. The curves of AE-A2 and AE-A3 are always below AE-A1's to illustrate the improved capability of modified AVM-LARs.}}\\label{simulations2_NWK}\n  \\end{center}\n\\end{figure*}\n\n\\subsection{Simulation 2}\nWe make use of the same simulation study which is conducted by\n\\citet{Zhang2014} for comparing the learning performance of AVM-NWK\n(including Algorithms \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and\n\\ref{AVM-LAR-alg3}) and the divide and conquer kernel ridge regression\n(DKRR for short).\n\nWe generate data from the regression model $y=g_3(x)+\\epsilon$,\nwhere $g_3(x)=\\min(x,1-x)$, the noise variable $\\epsilon$ is\nnormally distributed with mean 0 and variance $\\sigma^2=1/5$, and\n$X_i,i=1,\\dots,N$ are simulated from a uniform distribution in\n$[0,1]$ independently. In \\citet{Zhang2014}'s simulation, DKRR used\nthe kernel function $K(x,x')=1+\\min\\{x,x'\\}$, and regularization\nparameter $\\lambda = N^{-2/3}$ due to $g_3\\in \\mathcal{F}^{c_0,1}$\nfor some absolute constant $c_0$. We also use $N=10,000$ training\nsamples, and 1,000 test samples. The parameter setting of Algorithms\n\\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and \\ref{AVM-LAR-alg3} is the same\nas that in Simulation 1.\n\n\\vspace{5mm}\n\\begin{figure*}[ht]\n\\begin{center}\n  \\includegraphics[width=300pt]{s2comp.pdf}\n\n\\caption{\\emph{AEs of Algorithm \\ref{AVM-LAR},  \\ref{AVM-LAR-alg2},\n\\ref{AVM-LAR-alg3} and DKRR.\n}}\\label{simulations2_com}\n  \\end{center}\n\\end{figure*}\n\nIn Figure \\ref{simulations2_com}, we plot AEs of Algorithms\n\\ref{AVM-LAR}, \\ref{AVM-LAR-alg2}, \\ref{AVM-LAR-alg3} and DKRR for\n$m\\in\\{2^3,2^4,\\dots, 2^{11}\\}$. Figure \\ref{simulations2_com} shows\nAEs of Algorithm \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2},\n\\ref{AVM-LAR-alg3} and DKRR are comparable when $m<256$. As long as\n$m>256$, AEs of Algorithm \\ref{AVM-LAR}, \\ref{AVM-LAR-alg2} and DKRR\nincrease dramatically. However, AEs of Algorithm \\ref{AVM-LAR-alg3}\nare  stable. The reason is that, to keep the optimal learning rates,\nDKRR needs $m = \\mathcal{O}(N^{1/3})$~\\citep{Zhang2014}, and\nAlgorithm \\ref{AVM-LAR-alg2} needs $m=\\mathcal{O}(N^{2/3})$, while\nAlgorithm \\ref{AVM-LAR-alg3} holds for all $m$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Proofs}\\label{section5}\n\n\n\n\n\\subsection{Proof of Theorem 1}\nLet $f_{\\rho,h}(x)= \\sum_{i=1}^NW_{h,X_i}(x)f_\\rho(X_i).$\nThen, it is obvious that $f_{\\rho,h}(x)=\\mathbf E^*\\{f_{D,h}(x)\\},$\nwhere $\\mathbf  E^*\\{\\cdot\\}=\\mathbf E\\{\\cdot|X_1,X_2,\\dots,X_n\\}$.\nTherefore, we can deduce\n\n", "index": 57, "text": "$$\n        \\mathbf E^*\\{(f_{D,h}(x)-f_\\rho(x))\\}=\\mathbf E^*\n        \\{(f_{D,h}(x)-f_{\\rho,h}(x))^2\\}+(f_{\\rho,h}(x)-f_\\rho(x))^2.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}^{*}\\{(f_{D,h}(x)-f_{\\rho}(x))\\}=\\mathbf{E}^{*}\\{(f_{D,h}(x)-f_{\\rho%&#10;,h}(x))^{2}\\}+(f_{\\rho,h}(x)-f_{\\rho}(x))^{2}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>\ud835\udc04</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>\ud835\udc04</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThe first and second terms are referred to the {\\it sample error} and {\\it\n approximation error}, respectively. To bound the sample error, noting\n$\\mathbf E^*\\{Y_i\\}=f_\\rho(X_i)$,   we have\n\\begin{eqnarray*}\n           &&\\mathbf E^*\n        \\{(f_{D,h}(x)-f_{\\rho,h}(x))^2\\}\n          =\n         \\mathbf E^*\n        \\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(x)(Y_i-f_\\rho(X_i))\\right)^2\n        \\right\\}\\\\\n        &\\leq&\n         \\mathbf E^*\\left\\{\\sum_{i=1}^N\\left(W_{h,X_i}(x)(Y_i-f_\\rho(X_i))\\right)^2\n        \\right\\}\n        \\leq\n         4M^2\\sum_{i=1}^NW^2_{h,X_i}(x).\n\\end{eqnarray*}\nTherefore we can use (A) to bound the sample error as\n\n", "itemtype": "equation", "pos": 41177, "prevtext": "\nThat is,\n\n", "index": 59, "text": "$$\n     \\mathbf E\\{\\|f_{D,h}-f_\\rho\\|_\\rho^2\\}\n     =\n     \\int_{\\mathcal X}\\mathbf E\\{\\mathbf E^*\n        \\{(f_{D,h}(X)-f_{\\rho,h}(X))^2\\}\\}d\\rho_X+\\int_{\\mathcal X}\n        \\mathbf E\\{(f_{\\rho,h}(X)-f_\\rho(X))^2\\}d\\rho_X.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|f_{D,h}-f_{\\rho}\\|_{\\rho}^{2}\\}=\\int_{\\mathcal{X}}\\mathbf{E}\\{%&#10;\\mathbf{E}^{*}\\{(f_{D,h}(X)-f_{\\rho,h}(X))^{2}\\}\\}d\\rho_{X}+\\int_{\\mathcal{X}}%&#10;\\mathbf{E}\\{(f_{\\rho,h}(X)-f_{\\rho}(X))^{2}\\}d\\rho_{X}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></msub><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mi>\ud835\udc04</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03c1</mi><mi>X</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></msub><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">}</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03c1</mi><mi>X</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nNow, we turn to bound the approximation error. Let $B_h(x)$   be the\n$l^2$ ball with center $x$ and radius $h$, we have\n\\begin{eqnarray*}\n        &&\\mathbf E\\{(f_{\\rho,h}(X)-f_\\rho(X))^2\\}\n        =\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)f_\\rho(X_i)-f_\\rho(X)\\right)^2\\right\\}\\\\\n        &=&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\\right)^2\\right\\}\\\\\n         &=&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\n        \\right)^2I_{\\{B_h(X)\\cap D=\\varnothing\\}}\n        \\right\\}\\\\\n        &+&\n        \\mathbf E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)\n        -f_\\rho(X))\\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}.\n\\end{eqnarray*}\nIt follows from \\citep[P.66]{gyorfi2006distribution} and\n$\\sum_{i=1}^NW_{h,X_i}(X)=1$ that\n\n", "itemtype": "equation", "pos": 42042, "prevtext": "\nThe first and second terms are referred to the {\\it sample error} and {\\it\n approximation error}, respectively. To bound the sample error, noting\n$\\mathbf E^*\\{Y_i\\}=f_\\rho(X_i)$,   we have\n\\begin{eqnarray*}\n           &&\\mathbf E^*\n        \\{(f_{D,h}(x)-f_{\\rho,h}(x))^2\\}\n          =\n         \\mathbf E^*\n        \\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(x)(Y_i-f_\\rho(X_i))\\right)^2\n        \\right\\}\\\\\n        &\\leq&\n         \\mathbf E^*\\left\\{\\sum_{i=1}^N\\left(W_{h,X_i}(x)(Y_i-f_\\rho(X_i))\\right)^2\n        \\right\\}\n        \\leq\n         4M^2\\sum_{i=1}^NW^2_{h,X_i}(x).\n\\end{eqnarray*}\nTherefore we can use (A) to bound the sample error as\n\n", "index": 61, "text": "$$\n        \\mathbf E\\{(f_{D,h}(X)-f_{\\rho,h}(X))^2\\}\\leq4M^2\\mathbf E\n        \\left\\{\\sum_{i=1}^NW^2_{h,X_i}(X)\\right\\}\\leq\n        \\frac{4c_1M^2}{Nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{(f_{D,h}(X)-f_{\\rho,h}(X))^{2}\\}\\leq 4M^{2}\\mathbf{E}\\left\\{\\sum_{%&#10;i=1}^{N}W^{2}_{h,X_{i}}(X)\\right\\}\\leq\\frac{4c_{1}M^{2}}{Nh^{d}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>4</mn><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><msub><mi>c</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nFurthermore,\n\\begin{eqnarray*}\n        &&\\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\n        \\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}\\\\\n        &\\leq&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{\\|X_i-X\\|\\leq h}W_{h,X_i}(X)|f_\\rho(X_i)-f_\\rho(X)|\n        \\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}\\\\\n        &+&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{\\|X_i-X\\|> h}W_{h,X_i}(X)|f_\\rho(X_i)-f_\\rho(X)|\n        \\right)^2\n        \\right\\}\\\\\n        &\\leq&\n        c_0^2h^{2r}+\\frac{4c^2_2M^2}{Nh^d},\n\\end{eqnarray*}\nwhere the last inequality is deduced by $f_\\rho\\in\\mathcal\nF^{c_0,r}$, condition (B) and Jensen's inequality.\n Under this circumstance, we get\n\n", "itemtype": "equation", "pos": 43054, "prevtext": "\nNow, we turn to bound the approximation error. Let $B_h(x)$   be the\n$l^2$ ball with center $x$ and radius $h$, we have\n\\begin{eqnarray*}\n        &&\\mathbf E\\{(f_{\\rho,h}(X)-f_\\rho(X))^2\\}\n        =\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)f_\\rho(X_i)-f_\\rho(X)\\right)^2\\right\\}\\\\\n        &=&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\\right)^2\\right\\}\\\\\n         &=&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\n        \\right)^2I_{\\{B_h(X)\\cap D=\\varnothing\\}}\n        \\right\\}\\\\\n        &+&\n        \\mathbf E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)\n        -f_\\rho(X))\\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}.\n\\end{eqnarray*}\nIt follows from \\citep[P.66]{gyorfi2006distribution} and\n$\\sum_{i=1}^NW_{h,X_i}(X)=1$ that\n\n", "index": 63, "text": "$$\n         \\mathbf E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)\n        -f_\\rho(X))\\right)^2I_{\\{B_h(X)\\cap D=\\varnothing\\}}\n        \\right\\}\n        \\leq \\frac{16M^2}{Nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\left(\\sum_{i=1}^{N}W_{h,X_{i}}(X)(f_{\\rho}(X_{i})-f_{\\rho}(X%&#10;))\\right)^{2}I_{\\{B_{h}(X)\\cap D=\\varnothing\\}}\\right\\}\\leq\\frac{16M^{2}}{Nh^{%&#10;d}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><msup><mrow><mo>(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>X</mi><mi>i</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><mi>D</mi><mo>=</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><mrow><mn>16</mn><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nIf we set $h=\\left(\\frac{4(c_1+c_2^2+4)M^2}{c_0^2N}\\right)^{-1/(2r+d)},$\nthen\n\n", "itemtype": "equation", "pos": 43973, "prevtext": "\nFurthermore,\n\\begin{eqnarray*}\n        &&\\mathbf\n        E\\left\\{\\left(\\sum_{i=1}^NW_{h,X_i}(X)(f_\\rho(X_i)-f_\\rho(X))\n        \\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}\\\\\n        &\\leq&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{\\|X_i-X\\|\\leq h}W_{h,X_i}(X)|f_\\rho(X_i)-f_\\rho(X)|\n        \\right)^2I_{\\{B_h(X)\\cap D\\neq\\varnothing\\}}\n        \\right\\}\\\\\n        &+&\n        \\mathbf\n        E\\left\\{\\left(\\sum_{\\|X_i-X\\|> h}W_{h,X_i}(X)|f_\\rho(X_i)-f_\\rho(X)|\n        \\right)^2\n        \\right\\}\\\\\n        &\\leq&\n        c_0^2h^{2r}+\\frac{4c^2_2M^2}{Nh^d},\n\\end{eqnarray*}\nwhere the last inequality is deduced by $f_\\rho\\in\\mathcal\nF^{c_0,r}$, condition (B) and Jensen's inequality.\n Under this circumstance, we get\n\n", "index": 65, "text": "$$\n     \\mathbf E\\{\\|f_{D,h}-f_\\rho\\|_\\rho^2\\}\n     \\leq\n     c_0^2h^{2r}+\\frac{4(c_1+c_2^2+4)M^2}{Nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|f_{D,h}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq c_{0}^{2}h^{2r}+\\frac{4(c_{1%&#10;}+c_{2}^{2}+4)M^{2}}{Nh^{d}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup></mrow><mo>+</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThis together with \\citep[Theorem 3.2]{gyorfi2006distribution} finishes the proof\nof Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}. $\\square$\n\n\n\n\\subsection{Proof of Theorem 2}\nSince $\\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|_\\rho^2\\right\\}\n     =\n     \\mathbf E\\left\\{\\|\\overline{f}_h-\\mathbf\n     E\\{\\overline{f}_h\\}+\\mathbf\n     E\\{\\overline{f}_h\\}-f_\\rho\\|^2_\\rho\\right\\}$\nand\n$\\mathbf E\\{ \\overline{f}_h \\}=\\mathbf\n          E\\{ f_{j,h}\\}, \\quad j=1,\\dots,m,$\nwe get\n\\begin{eqnarray}\\label{distribution error decomposition}\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n            &=&\n            \\frac1{m^2}\\mathbf E\\left\\{\\sum_{j=1}^m\\left(\\|f_{j,h }-\\mathbf\n            E\\{f_{j,h }\\}\\|_\\rho^2+\\|\\mathbf\n            E\\{f_{j,h }\\}-f_\\rho\\|^2_\\rho\\right)\\right. \\nonumber \\\\\n            &+&\n            \\left.2\\sum_{j=1}^m\\sum_{k\\neq j}\\langle f_{j,h}-\\mathbf\n            E\\{f_{j,h}\\},f_{k,h}-\\mathbf\n            E\\{f_{k,h}\\}\\rangle_\\rho\\right\\} \\nonumber\\\\\n            &=&\n           \\frac1m\\mathbf E\\{\\| f_{1,h} -\\mathbf\n            E\\{f_{1,h}\\}\\|^2_\\rho\\}+ \\|\\mathbf\n            E\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho\\\\\n            &\\leq&\n            \\frac2m\\mathbf E\\{\\| f_{1,h} -f_\\rho\\}\\|^2_\\rho\\}+ 2\\|\\mathbf\n            E\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho. \\nonumber\n\\end{eqnarray}\nAs $h\\geq h_{D_j}$ for all $1\\leq j\\leq m$, we obtain that\n$B_h(x)\\cap D_j\\neq\\varnothing$ for all $x\\in\\mathcal X$ and $1\\leq\nj\\leq m$. Then, using the same method as that in the proof of\nTheorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}, (C)\nand (D$^*$) yield  that\n\n", "itemtype": "equation", "pos": 44160, "prevtext": "\nIf we set $h=\\left(\\frac{4(c_1+c_2^2+4)M^2}{c_0^2N}\\right)^{-1/(2r+d)},$\nthen\n\n", "index": 67, "text": "$$\n           \\mathbf E\\{\\|f_{D,h}-f_\\rho\\|_\\rho^2\\}\\leq\n           c_0^{2d/(2r+d)}(4(c_1+c_2^2+4)M^2)^{2r/(2r+d)}N^{-2r/(2r+d)}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|f_{D,h}-f_{\\rho}\\|_{\\rho}^{2}\\}\\leq c_{0}^{2d/(2r+d)}(4(c_{1}+c_%&#10;{2}^{2}+4)M^{2})^{2r/(2r+d)}N^{-2r/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>f</mi><mrow><mi>D</mi><mo>,</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2264</mo><mrow><msubsup><mi>c</mi><mn>0</mn><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>d</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>\u2062</mo><msup><mi>N</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nDue to the Jensen's inequality, we have\n\n", "itemtype": "equation", "pos": 45890, "prevtext": "\nThis together with \\citep[Theorem 3.2]{gyorfi2006distribution} finishes the proof\nof Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}. $\\square$\n\n\n\n\\subsection{Proof of Theorem 2}\nSince $\\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|_\\rho^2\\right\\}\n     =\n     \\mathbf E\\left\\{\\|\\overline{f}_h-\\mathbf\n     E\\{\\overline{f}_h\\}+\\mathbf\n     E\\{\\overline{f}_h\\}-f_\\rho\\|^2_\\rho\\right\\}$\nand\n$\\mathbf E\\{ \\overline{f}_h \\}=\\mathbf\n          E\\{ f_{j,h}\\}, \\quad j=1,\\dots,m,$\nwe get\n\\begin{eqnarray}\\label{distribution error decomposition}\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n            &=&\n            \\frac1{m^2}\\mathbf E\\left\\{\\sum_{j=1}^m\\left(\\|f_{j,h }-\\mathbf\n            E\\{f_{j,h }\\}\\|_\\rho^2+\\|\\mathbf\n            E\\{f_{j,h }\\}-f_\\rho\\|^2_\\rho\\right)\\right. \\nonumber \\\\\n            &+&\n            \\left.2\\sum_{j=1}^m\\sum_{k\\neq j}\\langle f_{j,h}-\\mathbf\n            E\\{f_{j,h}\\},f_{k,h}-\\mathbf\n            E\\{f_{k,h}\\}\\rangle_\\rho\\right\\} \\nonumber\\\\\n            &=&\n           \\frac1m\\mathbf E\\{\\| f_{1,h} -\\mathbf\n            E\\{f_{1,h}\\}\\|^2_\\rho\\}+ \\|\\mathbf\n            E\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho\\\\\n            &\\leq&\n            \\frac2m\\mathbf E\\{\\| f_{1,h} -f_\\rho\\}\\|^2_\\rho\\}+ 2\\|\\mathbf\n            E\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho. \\nonumber\n\\end{eqnarray}\nAs $h\\geq h_{D_j}$ for all $1\\leq j\\leq m$, we obtain that\n$B_h(x)\\cap D_j\\neq\\varnothing$ for all $x\\in\\mathcal X$ and $1\\leq\nj\\leq m$. Then, using the same method as that in the proof of\nTheorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}, (C)\nand (D$^*$) yield  that\n\n", "index": 69, "text": "$$\n          \\mathbf E\\{\\| f_{1,h} -f_\\rho\\}\\|^2_\\rho\\}\n          \\leq c_0^2h^{2r}+\\frac{4(c_3+c_4^2)M^2}{nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|f_{1,h}-f_{\\rho}\\}\\|^{2}_{\\rho}\\}\\leq c_{0}^{2}h^{2r}+\\frac{4(c_%&#10;{3}+c_{4}^{2})M^{2}}{nh^{d}}.\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mi>h</mi></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">}</mo></mrow><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo><mo>\u2264</mo><mi>c</mi><msub><mi/><mn>0</mn></msub><msup><mi/><mn>2</mn></msup><mi>h</mi><msup><mi/><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup><mo>+</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nNoting $B_h(X)\\cap D_j\\neq\\varnothing$ almost surely, the same\nmethod as that in the proof of Theorem \\ref{THEOREM:SUFFICIENT\nCONDITION FOR LOCAL ESTIMATE} together with (D) yields that\n\n", "itemtype": "equation", "pos": 46046, "prevtext": "\nDue to the Jensen's inequality, we have\n\n", "index": 71, "text": "$$\n       \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|_\\rho^2\\right\\}\n       \\leq\n       \\frac{2c_0^2h^{2r}}{m}+\\frac{8(c_3+c_4^2)M^2}{mnh^d}+\n       2\\mathbf E\\left\\{\\|\\mathbf\n            E^*\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho\\right\\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\overline{f}_{h}-f_{\\rho}\\|_{\\rho}^{2}\\right\\}\\leq\\frac{2c_%&#10;{0}^{2}h^{2r}}{m}+\\frac{8(c_{3}+c_{4}^{2})M^{2}}{mnh^{d}}+2\\mathbf{E}\\left\\{\\|%&#10;\\mathbf{E}^{*}\\{f_{1,h}\\}-f_{\\rho}\\|^{2}_{\\rho}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow><mo>\u2264</mo><mrow><mfrac><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup></mrow><mi>m</mi></mfrac><mo>+</mo><mfrac><mrow><mn>8</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>\ud835\udc04</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mi>h</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThus,\n\n", "itemtype": "equation", "pos": 46461, "prevtext": "\nNoting $B_h(X)\\cap D_j\\neq\\varnothing$ almost surely, the same\nmethod as that in the proof of Theorem \\ref{THEOREM:SUFFICIENT\nCONDITION FOR LOCAL ESTIMATE} together with (D) yields that\n\n", "index": 73, "text": "$$\n\\mathbf E\\left\\{\\|\\mathbf\n            E^*\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho\\right\\}\\leq\n            c_0^2h^{2r}+\\frac{8c_4^2M^2}{mnh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\mathbf{E}^{*}\\{f_{1,h}\\}-f_{\\rho}\\|^{2}_{\\rho}\\right\\}\\leq&#10;c%&#10;_{0}^{2}h^{2r}+\\frac{8c_{4}^{2}M^{2}}{mnh^{d}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>\ud835\udc04</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mi>h</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup></mrow><mo>+</mo><mfrac><mrow><mn>8</mn><mo>\u2062</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThis finishes the proof of (\\ref{theorem1.11111}) by taking  $\nh=\\left(\\frac{16(c_3+c_4^2)M^2}{3c_0^2nm}\\right)^{-1/(2r+d)}$ into\naccount.\n\n\n Now, we turn to prove (\\ref{theorem1.22222222222}). According to (\\ref{distribution error\ndecomposition}), we have\n\n", "itemtype": "equation", "pos": 46603, "prevtext": "\nThus,\n\n", "index": 75, "text": "$$\n            \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|_\\rho^2\\right\\}\n            \\leq\n           \\frac{16(c_3+c_4^2)M^2}{mnh^d}\n            +3c_0^2h^{2r}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\overline{f}_{h}-f_{\\rho}\\|_{\\rho}^{2}\\right\\}\\leq\\frac{16(%&#10;c_{3}+c_{4}^{2})M^{2}}{mnh^{d}}+3c_{0}^{2}h^{2r}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow><mo>\u2264</mo><mrow><mfrac><mrow><mn>16</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac><mo>+</mo><mrow><mn>3</mn><mo>\u2062</mo><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><msup><mi>h</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nWithout loss of generality, we assume $h<h_{D_1}$. It then follows\nfrom the definition of the mesh norm that   there exits\n$X\\in\\mathcal X$ which is not in $B_h(X_i)$, $X_i\\in D_1$. Define the\nseparation radius of a set of points\n$S=\\{\\zeta_i\\}_{i=1}^n\\subset\\mathcal{X}$\n  via\n\n", "itemtype": "equation", "pos": 47021, "prevtext": "\nThis finishes the proof of (\\ref{theorem1.11111}) by taking  $\nh=\\left(\\frac{16(c_3+c_4^2)M^2}{3c_0^2nm}\\right)^{-1/(2r+d)}$ into\naccount.\n\n\n Now, we turn to prove (\\ref{theorem1.22222222222}). According to (\\ref{distribution error\ndecomposition}), we have\n\n", "index": 77, "text": "$$\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n           \\geq\n            \\|\\mathbf\n            E\\{f_{1,h}\\}-f_\\rho\\|^2_\\rho\n            =\\int_{\\mathcal X}\\left(\\mathbf\n            E\\left\\{\\sum_{i=1}^nW_{X_i,h}(X)f_\\rho(X_i)-f_\\rho(X)\\right\\}\\right)^2\n            d\\rho_X.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\overline{f}_{h}-f_{\\rho}\\|^{2}_{\\rho}\\right\\}\\geq\\|\\mathbf%&#10;{E}\\{f_{1,h}\\}-f_{\\rho}\\|^{2}_{\\rho}=\\int_{\\mathcal{X}}\\left(\\mathbf{E}\\left\\{%&#10;\\sum_{i=1}^{n}W_{X_{i},h}(X)f_{\\rho}(X_{i})-f_{\\rho}(X)\\right\\}\\right)^{2}d%&#10;\\rho_{X}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow><mo>\u2265</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mi>h</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></msub><mrow><msup><mrow><mo>(</mo><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>W</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><mi>h</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>\u03c1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\u03c1</mi><mi>X</mi></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n   The mesh ratio $\n               \\tau_{_{S}}:=\\frac{h_{S}}{q_{_{S}}}\\geq1\n$ provides a measure of how uniformly points in $S$ are distributed\non $\\mathcal X$. If $\\tau_{_{S}}\\leq 2$, we then call $S$ as the\nquasi-uniform point set. Let $\\Xi_l=\\{\\xi_1,\\dots,\\xi_l\\}$ be\n$l=\\lfloor(2h)^{-d}\\rfloor$ quasi-uniform points \\citep{Wendland2005}\nin $\\mathcal X$. That is\n$\\tau_{_{\\Xi_l}}=\\frac{h_{_{\\Xi_l}}}{q_{_{\\Xi_l}}}\\leq 2.$ Since\n$h_{_{\\Xi_l}}\\geq l^{-1/d}$, we have $q_{_{\\Xi_l}}\\geq\n\\frac1{2l^{1/d}}\\geq h.$ Then,\n\\begin{eqnarray}\\nonumber\n \\mathbf\n           E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n           &=&\n           \\|\\mathbf E\\{f_{1,h}\\}-f_\\rho\\|_\\rho^2\\\\\\nonumber\n           &\\geq&\n            \\sum_{k=1}^l\\int_{B_{q_{_{\\Xi_l}}}(\\xi_k)}\\left(\\mathbf\n            E\\left\\{\\sum_{i=1}^nW_{X_i,h}(X)f_\\rho(X_i)-f_\\rho(X)\\right\\}\\right)^2\n            d\\rho_X.\n\\end{eqnarray}\n\n{If $f_\\rho(x)=M$, then\n\\begin{eqnarray*}\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n          & \\geq&\n            M^2\\sum_{k=1}^l\\int_{B_{q_{_{\\Xi_l}}}(\\xi_k)}\\left(\\mathbf\n            E\\left\\{I_{\\{D_1\\cap B_{q_{_{\\Xi_l}}}(\\xi_k)=\\varnothing\\}}\\right\\}\\right)^2\n            d\\rho_X\\\\\n            &\\geq&\n            M^2\\sum_{k=1}^l\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k))\\mathbf P\\{D_1\\cap\n            B_{q_{_{\\Xi_l}}}(\\xi_k)=\\varnothing\\}\\\\\n            &=&\n            M^2\\sum_{k=1}^l\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k))(1-\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k)))^n.\n\\end{eqnarray*}\nSince $h\\geq\\frac12(n+2)^{-1/d}$, we can let $\\rho_X$ be the\nmarginal distribution satisfying\n\n", "itemtype": "equation", "pos": 47600, "prevtext": "\nWithout loss of generality, we assume $h<h_{D_1}$. It then follows\nfrom the definition of the mesh norm that   there exits\n$X\\in\\mathcal X$ which is not in $B_h(X_i)$, $X_i\\in D_1$. Define the\nseparation radius of a set of points\n$S=\\{\\zeta_i\\}_{i=1}^n\\subset\\mathcal{X}$\n  via\n\n", "index": 79, "text": "$$\n                 q_{_{S}}:=\\frac12\\min_{j\\neq k}\\|\\zeta_j-\\zeta_k\\|.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"q_{{}_{S}}:=\\frac{1}{2}\\min_{j\\neq k}\\|\\zeta_{j}-\\zeta_{k}\\|.\" display=\"block\"><mrow><mrow><msub><mi>q</mi><msub><mi/><mi>S</mi></msub></msub><mo>:=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><munder><mi>min</mi><mrow><mi>j</mi><mo>\u2260</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><mo>\u2225</mo><mrow><msub><mi>\u03b6</mi><mi>j</mi></msub><mo>-</mo><msub><mi>\u03b6</mi><mi>k</mi></msub></mrow><mo>\u2225</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThen\n\n", "itemtype": "equation", "pos": 49250, "prevtext": "\n   The mesh ratio $\n               \\tau_{_{S}}:=\\frac{h_{S}}{q_{_{S}}}\\geq1\n$ provides a measure of how uniformly points in $S$ are distributed\non $\\mathcal X$. If $\\tau_{_{S}}\\leq 2$, we then call $S$ as the\nquasi-uniform point set. Let $\\Xi_l=\\{\\xi_1,\\dots,\\xi_l\\}$ be\n$l=\\lfloor(2h)^{-d}\\rfloor$ quasi-uniform points \\citep{Wendland2005}\nin $\\mathcal X$. That is\n$\\tau_{_{\\Xi_l}}=\\frac{h_{_{\\Xi_l}}}{q_{_{\\Xi_l}}}\\leq 2.$ Since\n$h_{_{\\Xi_l}}\\geq l^{-1/d}$, we have $q_{_{\\Xi_l}}\\geq\n\\frac1{2l^{1/d}}\\geq h.$ Then,\n\\begin{eqnarray}\\nonumber\n \\mathbf\n           E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n           &=&\n           \\|\\mathbf E\\{f_{1,h}\\}-f_\\rho\\|_\\rho^2\\\\\\nonumber\n           &\\geq&\n            \\sum_{k=1}^l\\int_{B_{q_{_{\\Xi_l}}}(\\xi_k)}\\left(\\mathbf\n            E\\left\\{\\sum_{i=1}^nW_{X_i,h}(X)f_\\rho(X_i)-f_\\rho(X)\\right\\}\\right)^2\n            d\\rho_X.\n\\end{eqnarray}\n\n{If $f_\\rho(x)=M$, then\n\\begin{eqnarray*}\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n          & \\geq&\n            M^2\\sum_{k=1}^l\\int_{B_{q_{_{\\Xi_l}}}(\\xi_k)}\\left(\\mathbf\n            E\\left\\{I_{\\{D_1\\cap B_{q_{_{\\Xi_l}}}(\\xi_k)=\\varnothing\\}}\\right\\}\\right)^2\n            d\\rho_X\\\\\n            &\\geq&\n            M^2\\sum_{k=1}^l\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k))\\mathbf P\\{D_1\\cap\n            B_{q_{_{\\Xi_l}}}(\\xi_k)=\\varnothing\\}\\\\\n            &=&\n            M^2\\sum_{k=1}^l\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k))(1-\\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k)))^n.\n\\end{eqnarray*}\nSince $h\\geq\\frac12(n+2)^{-1/d}$, we can let $\\rho_X$ be the\nmarginal distribution satisfying\n\n", "index": 81, "text": "$$\n         \\rho_X(B_{q_{_{\\Xi_l}}}(\\xi_k))=1/n, \\ k=1,2,\\dots, l-1.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\rho_{X}(B_{q_{{}_{\\Xi_{l}}}}(\\xi_{k}))=1/n,\\ k=1,2,\\dots,l-1.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>\u03c1</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>B</mi><msub><mi>q</mi><msub><mi/><msub><mi mathvariant=\"normal\">\u039e</mi><mi>l</mi></msub></msub></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\n  This finishes the proof of Theorem \\ref{THEOREM DISTRBUTED LAE\n  POSITIVE}. $\\square$\n}\n\\subsection{Proof of Theorem 3}\nWithout loss of generality, we assume $h_{D_1}=\\max_j\\{h_{D_j}\\}$.\nIt follows from (\\ref{distribution error decomposition}) that\n\n", "itemtype": "equation", "pos": 49327, "prevtext": "\nThen\n\n", "index": 83, "text": "$$\n           \\mathbf E\\left\\{\\|\\overline{f}_h-f_\\rho\\|^2_\\rho\\right\\}\n           \\geq\n           M^2\\sum_{k=1}^{l-1}\\frac1n(1-1/n)^n\n           \\geq\\frac{M^2((2h)^{-d}-2)}{3n}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\overline{f}_{h}-f_{\\rho}\\|^{2}_{\\rho}\\right\\}\\geq M^{2}%&#10;\\sum_{k=1}^{l-1}\\frac{1}{n}(1-1/n)^{n}\\geq\\frac{M^{2}((2h)^{-d}-2)}{3n}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>f</mi><mo>\u00af</mo></mover><mi>h</mi></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub></mrow><mo>\u2225</mo></mrow><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow></mrow><mo>\u2265</mo><mrow><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup></mrow></mrow></mrow><mo>\u2265</mo><mfrac><mrow><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>h</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mi>d</mi></mrow></msup><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>3</mn><mo>\u2062</mo><mi>n</mi></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nWe first bound $\\mathbf E\\{\\| f_{1,\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}$.\nAs $\\tilde{h}\\geq h_{D_1}$, the same method as that in the proof of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE} yields that\n\n", "itemtype": "equation", "pos": 49759, "prevtext": "\n  This finishes the proof of Theorem \\ref{THEOREM DISTRBUTED LAE\n  POSITIVE}. $\\square$\n}\n\\subsection{Proof of Theorem 3}\nWithout loss of generality, we assume $h_{D_1}=\\max_j\\{h_{D_j}\\}$.\nIt follows from (\\ref{distribution error decomposition}) that\n\n", "index": 85, "text": "$$\n           \\mathbf E\\left\\{\\|\\tilde{f}_{\\tilde{h}}-f_\\rho\\|^2_\\rho\\right\\}\n            \\leq\n            \\frac2m\\mathbf E\\{\\| f_{1,\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}+ 2\\|\\mathbf\n            E\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\|\\tilde{f}_{\\tilde{h}}-f_{\\rho}\\|^{2}_{\\rho}\\right\\}\\leq%&#10;\\frac{2}{m}\\mathbf{E}\\{\\|f_{1,\\tilde{h}}-f_{\\rho}\\}\\|^{2}_{\\rho}\\}+2\\|\\mathbf{%&#10;E}\\{f_{1,\\tilde{h}}\\}-f_{\\rho}\\|^{2}_{\\rho}.\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo>{</mo><mo>\u2225</mo><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo>}</mo></mrow><mo>\u2264</mo><mfrac><mn>2</mn><mi>m</mi></mfrac><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">}</mo></mrow><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo><mo>+</mo><mn>2</mn><mo>\u2225</mo><mi>\ud835\udc04</mi><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo stretchy=\"false\">}</mo><mo>-</mo><mi>f</mi><msub><mi/><mi>\u03c1</mi></msub><mo>\u2225</mo><msup><mi/><mn>2</mn></msup><msub><mi/><mi>\u03c1</mi></msub><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nTo bound $\\|\\mathbf\n            E\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho$, we use the same\n            method as that in the proof of Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}\nagain. As $\\tilde{h}\\geq m^{-1/(2r+d)}h_{D_1}^{d/(2r+d)}$, it is\neasy to deduce that\n\\begin{eqnarray*}\n          \\|\\mathbf\n            E\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho\n            &\\leq&\n            \\mathbf E\\left\\{\\|\\mathbf\n            E^*\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho\\right\\}\n            \\leq\n            c_0^2\\mathbf E\\{\\tilde{h}^{2r}\\}+\\mathbf\n            E\\left\\{\\frac{8c_4^2M^2}{mn\\tilde{h}^d}\\right\\}\\\\\n            &\\leq&\n            c_0^2m^{-2r/(2r+d)}\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n            +\n            c_0^2\\mathbf E\\{h_{D_1}^{2r}\\}\\\\\n            &+&\n            8c_4^2M^2(mn)^{-1}\\mathbf E\\{m^{d/(2r+d)}h_{D_1}^{-d^2/(2r+d)}\\}.\n\\end{eqnarray*}\nThus\n\\begin{eqnarray*}\n        \\mathbf E\\{\\| \\tilde{f}_{\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}\n        &\\leq&\n        c_0^2m^{-2r/(2r+d)}\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n            +\n            (c_0^2+2)\\mathbf E\\{h_{D_1}^{2r}\\}\\\\\n            &+&\n            8(c_3+2c_4^2)M^2(mn)^{-1}m^{d/(2r+d)}\\mathbf E\\{h_{D_1}^{-d^2/(2r+d)}\\}.\n\\end{eqnarray*}\nTo bound $\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}$, we note that for\narbitrary $\\varepsilon>0$, there holds\n\n", "itemtype": "equation", "pos": 50184, "prevtext": "\nWe first bound $\\mathbf E\\{\\| f_{1,\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}$.\nAs $\\tilde{h}\\geq h_{D_1}$, the same method as that in the proof of\nTheorem \\ref{THEOREM DISTRBUTED LAE POSITIVE} yields that\n\n", "index": 87, "text": "$$\n        \\mathbf E\\{\\| f_{1,\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}\n        \\leq\n        c_0^2\\mathbf E\\{\\tilde{h}^{2r}\\}+\\mathbf E\\left\\{\n        \\frac{4M^2(c_3+c_4^2)}{n\\tilde{h}^d}\\right\\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|f_{1,\\tilde{h}}-f_{\\rho}\\}\\|^{2}_{\\rho}\\}\\leq c_{0}^{2}\\mathbf{E%&#10;}\\{\\tilde{h}^{2r}\\}+\\mathbf{E}\\left\\{\\frac{4M^{2}(c_{3}+c_{4}^{2})}{n\\tilde{h}%&#10;^{d}}\\right\\}.\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mi>f</mi><mrow><mn>1</mn><mo>,</mo><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></mrow></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">}</mo></mrow><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo><mo>\u2264</mo><mi>c</mi><msub><mi/><mn>0</mn></msub><msup><mi/><mn>2</mn></msup><mi>\ud835\udc04</mi><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msup><mo stretchy=\"false\">}</mo><mo>+</mo><mi>\ud835\udc04</mi><mo>{</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>n</mi><mo>\u2062</mo><msup><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mi>d</mi></msup></mrow></mfrac><mo>}</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nLet $t_1,\\dots,t_l$ be the quasi-uniform points  of $\\mathcal X$.\nThen it follows from \\citep[P.93]{gyorfi2006distribution} that\n\n", "itemtype": "equation", "pos": 51669, "prevtext": "\nTo bound $\\|\\mathbf\n            E\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho$, we use the same\n            method as that in the proof of Theorem \\ref{THEOREM DISTRBUTED LAE POSITIVE}\nagain. As $\\tilde{h}\\geq m^{-1/(2r+d)}h_{D_1}^{d/(2r+d)}$, it is\neasy to deduce that\n\\begin{eqnarray*}\n          \\|\\mathbf\n            E\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho\n            &\\leq&\n            \\mathbf E\\left\\{\\|\\mathbf\n            E^*\\{f_{1,\\tilde{h}}\\}-f_\\rho\\|^2_\\rho\\right\\}\n            \\leq\n            c_0^2\\mathbf E\\{\\tilde{h}^{2r}\\}+\\mathbf\n            E\\left\\{\\frac{8c_4^2M^2}{mn\\tilde{h}^d}\\right\\}\\\\\n            &\\leq&\n            c_0^2m^{-2r/(2r+d)}\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n            +\n            c_0^2\\mathbf E\\{h_{D_1}^{2r}\\}\\\\\n            &+&\n            8c_4^2M^2(mn)^{-1}\\mathbf E\\{m^{d/(2r+d)}h_{D_1}^{-d^2/(2r+d)}\\}.\n\\end{eqnarray*}\nThus\n\\begin{eqnarray*}\n        \\mathbf E\\{\\| \\tilde{f}_{\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}\n        &\\leq&\n        c_0^2m^{-2r/(2r+d)}\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n            +\n            (c_0^2+2)\\mathbf E\\{h_{D_1}^{2r}\\}\\\\\n            &+&\n            8(c_3+2c_4^2)M^2(mn)^{-1}m^{d/(2r+d)}\\mathbf E\\{h_{D_1}^{-d^2/(2r+d)}\\}.\n\\end{eqnarray*}\nTo bound $\\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}$, we note that for\narbitrary $\\varepsilon>0$, there holds\n\n", "index": 89, "text": "$$\n         \\mathbf P\\{h_{D_1}>\\varepsilon\\}\n         =\\mathbf P\\{\\max_{x\\in\\mathcal X}\\min_{X_i\\in D_1}\\|x-X_i\\|>\\varepsilon\\}\n         \\leq  \\max_{x\\in\\mathcal X}\n         \\mathbf E\\{(1-\\rho_X(B_\\varepsilon(x)))^n\\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{P}\\{h_{D_{1}}&gt;\\varepsilon\\}=\\mathbf{P}\\{\\max_{x\\in\\mathcal{X}}\\min_{X_%&#10;{i}\\in D_{1}}\\|x-X_{i}\\|&gt;\\varepsilon\\}\\leq\\max_{x\\in\\mathcal{X}}\\mathbf{E}\\{(1%&#10;-\\rho_{X}(B_{\\varepsilon}(x)))^{n}\\}.\" display=\"block\"><mrow><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>h</mi><msub><mi>D</mi><mn>1</mn></msub></msub><mo>&gt;</mo><mi>\u03b5</mi><mo stretchy=\"false\">}</mo></mrow><mo>=</mo><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">{</mo><munder><mi>max</mi><mrow><mi>x</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder><munder><mi>min</mi><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi>D</mi><mn>1</mn></msub></mrow></munder><mo>\u2225</mo><mi>x</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>\u03b5</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2264</mo><munder><mi>max</mi><mrow><mi>x</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><msub><mi>\u03c1</mi><mi>X</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>B</mi><mi>\u03b5</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mo stretchy=\"false\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThen, we have\n\\begin{eqnarray*}\n    &&\n    \\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n    =\n    \\int_0^\\infty\\mathbf\n    P\\{h_{D_1}^{2rd/(2r+d)}>\\varepsilon\\}d\\varepsilon\n     =\n    \\int_0^\\infty\\mathbf\n    P\\{h_{D_1}>\\varepsilon^{(2r+d)/(2rd)}\\}d\\varepsilon\\\\\n    &\\leq&\n    \\int_0^{n^{-2r/(2r+d)}}1d\\varepsilon+\n    \\int_{n^{-2r/(2r+d)}}^\\infty\\mathbf\n    P\\{h_{D_1}>\\varepsilon^{(2r+d)/(2rd)}\\}d\\varepsilon\\\\\n    &\\leq&\n    n^{-2r/(2r+d)}+\\frac1n\\int_{n^{-2r/(2r+d)}}^\\infty\\varepsilon^{-(2r+d)/(2r)}d\\varepsilon\n    \\leq\n    \\frac{2r+d}dn^{-2r/(2r+d)}.\n\\end{eqnarray*}\nTo bound $\\mathbf E\\{h_{D_1}^{2r}\\}$, we can use the above method\nagain and $r<d/2$ to derive\n\n", "itemtype": "equation", "pos": 52020, "prevtext": "\nLet $t_1,\\dots,t_l$ be the quasi-uniform points  of $\\mathcal X$.\nThen it follows from \\citep[P.93]{gyorfi2006distribution} that\n\n", "index": 91, "text": "$$\n        \\mathbf P\\{h_{D_1}>\\varepsilon\\}\\leq\n        \\frac{1}{n\\varepsilon^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{P}\\{h_{D_{1}}&gt;\\varepsilon\\}\\leq\\frac{1}{n\\varepsilon^{d}}.\" display=\"block\"><mrow><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>h</mi><msub><mi>D</mi><mn>1</mn></msub></msub><mo>&gt;</mo><mi>\u03b5</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2264</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>\u03b5</mi><mi>d</mi></msup></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nTo bound $\\mathbf E\\{h_{D_1}^{-d^2/(2r+d)}\\}$, we use the fact\n $h_{D_1}\\geq n^{-1/d}$ almost surely to obtain\n\n", "itemtype": "equation", "pos": 52763, "prevtext": "\nThen, we have\n\\begin{eqnarray*}\n    &&\n    \\mathbf E\\{h_{D_1}^{2rd/(2r+d)}\\}\n    =\n    \\int_0^\\infty\\mathbf\n    P\\{h_{D_1}^{2rd/(2r+d)}>\\varepsilon\\}d\\varepsilon\n     =\n    \\int_0^\\infty\\mathbf\n    P\\{h_{D_1}>\\varepsilon^{(2r+d)/(2rd)}\\}d\\varepsilon\\\\\n    &\\leq&\n    \\int_0^{n^{-2r/(2r+d)}}1d\\varepsilon+\n    \\int_{n^{-2r/(2r+d)}}^\\infty\\mathbf\n    P\\{h_{D_1}>\\varepsilon^{(2r+d)/(2rd)}\\}d\\varepsilon\\\\\n    &\\leq&\n    n^{-2r/(2r+d)}+\\frac1n\\int_{n^{-2r/(2r+d)}}^\\infty\\varepsilon^{-(2r+d)/(2r)}d\\varepsilon\n    \\leq\n    \\frac{2r+d}dn^{-2r/(2r+d)}.\n\\end{eqnarray*}\nTo bound $\\mathbf E\\{h_{D_1}^{2r}\\}$, we can use the above method\nagain and $r<d/2$ to derive\n\n", "index": 93, "text": "$$\n        \\mathbf E\\{h_{D_1}^{2r}\\}\n        \\leq 4rd^{-1}n^{-2r/d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{h_{D_{1}}^{2r}\\}\\leq 4rd^{-1}n^{-2r/d}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>h</mi><msub><mi>D</mi><mn>1</mn></msub><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>4</mn><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msup><mi>d</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mi>d</mi></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nHence\n\n", "itemtype": "equation", "pos": 52946, "prevtext": "\nTo bound $\\mathbf E\\{h_{D_1}^{-d^2/(2r+d)}\\}$, we use the fact\n $h_{D_1}\\geq n^{-1/d}$ almost surely to obtain\n\n", "index": 95, "text": "$$\n         \\mathbf E\\{h_{D_1}^{-d^2/(2r+d)}\\}\\leq n^{d/(2r+d)}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{h_{D_{1}}^{-d^{2}/(2r+d)}\\}\\leq n^{d/(2r+d)}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>h</mi><msub><mi>D</mi><mn>1</mn></msub><mrow><mo>-</mo><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2264</mo><msup><mi>n</mi><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nSince\n\n", "itemtype": "equation", "pos": 53020, "prevtext": "\nHence\n\n", "index": 97, "text": "$$\n        \\mathbf E\\{\\| \\tilde{f}_{\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}\n        \\leq\n        \\left(\\frac{c_0^2(2r+d)}d+8(c_3+2c_4^2)M^2\\right)N^{-2r/(2r+d)}\n            +\n            \\frac{4r(c_0^2+2)}{d}n^{-2r/d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|\\tilde{f}_{\\tilde{h}}-f_{\\rho}\\}\\|^{2}_{\\rho}\\}\\leq\\left(\\frac{c%&#10;_{0}^{2}(2r+d)}{d}+8(c_{3}+2c_{4}^{2})M^{2}\\right)N^{-2r/(2r+d)}+\\frac{4r(c_{0%&#10;}^{2}+2)}{d}n^{-2r/d}.\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">}</mo></mrow><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo><mo>\u2264</mo><mo>(</mo><mfrac><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mi>d</mi></mfrac><mo>+</mo><mn>8</mn><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><mn>2</mn><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><msup><mi>M</mi><mn>2</mn></msup><mo>)</mo><mi>N</mi><msup><mi/><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup><mo>+</mo><mfrac><mrow><mn>4</mn><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mi>d</mi></mfrac><mi>n</mi><msup><mi/><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mi>d</mi></mrow></mrow></msup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nwe have\n\n", "itemtype": "equation", "pos": 53240, "prevtext": "\nSince\n\n", "index": 99, "text": "$$\n          m\\leq\n          \\left(\\frac{ c_0^2(2r+d)+8d(c_3+2c_4^2)M^2}{4r(c_0^2+2)}\n          \\right)^{d/(2r)}N^{2r/(2r+d)},\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex37.m1\" class=\"ltx_Math\" alttext=\"m\\leq\\left(\\frac{c_{0}^{2}(2r+d)+8d(c_{3}+2c_{4}^{2})M^{2}}{4r(c_{0}^{2}+2)}%&#10;\\right)^{d/(2r)}N^{2r/(2r+d)},\" display=\"block\"><mrow><mrow><mi>m</mi><mo>\u2264</mo><mrow><msup><mrow><mo>(</mo><mfrac><mrow><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>8</mn><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>M</mi><mn>2</mn></msup></mrow></mrow><mrow><mn>4</mn><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow><mrow><mi>d</mi><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>\u2062</mo><msup><mi>N</mi><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nwhich finishes the proof of (\\ref{theorem3}). $\\square$\n\n\n\\subsection{Proof of Theorem 4}\n\n{\\bf Proof.} From the definition, it follows that\n\n", "itemtype": "equation", "pos": 53378, "prevtext": "\nwe have\n\n", "index": 101, "text": "$$\n        \\mathbf E\\{\\| \\tilde{f}_{\\tilde{h}} -f_\\rho\\}\\|^2_\\rho\\}\n        \\leq\n        2\\left(\\frac{c_0^2(2r+d)}d+8(c_3+2c_4^2)M^2\\right)N^{-2r/(2r+d)}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex38.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{\\|\\tilde{f}_{\\tilde{h}}-f_{\\rho}\\}\\|^{2}_{\\rho}\\}\\leq 2\\left(\\frac%&#10;{c_{0}^{2}(2r+d)}{d}+8(c_{3}+2c_{4}^{2})M^{2}\\right)N^{-2r/(2r+d)}\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><msub><mover accent=\"true\"><mi>f</mi><mo stretchy=\"false\">~</mo></mover><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>-</mo><msub><mi>f</mi><mi>\u03c1</mi></msub><mo stretchy=\"false\">}</mo></mrow><msubsup><mo>\u2225</mo><mi>\u03c1</mi><mn>2</mn></msubsup><mo stretchy=\"false\">}</mo><mo>\u2264</mo><mn>2</mn><mo>(</mo><mfrac><mrow><msubsup><mi>c</mi><mn>0</mn><mn>2</mn></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mi>d</mi></mfrac><mo>+</mo><mn>8</mn><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mn>3</mn></msub><mo>+</mo><mn>2</mn><msubsup><mi>c</mi><mn>4</mn><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><msup><mi>M</mi><mn>2</mn></msup><mo>)</mo><mi>N</mi><msup><mi/><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>r</mi></mrow><mo>+</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nWe then use Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} to consider a new local estimate with\n\n", "itemtype": "equation", "pos": 53676, "prevtext": "\nwhich finishes the proof of (\\ref{theorem3}). $\\square$\n\n\n\\subsection{Proof of Theorem 4}\n\n{\\bf Proof.} From the definition, it follows that\n\n", "index": 103, "text": "$$\n         \\hat{f_h}(x)=\\sum_{j=1}^m\\frac{I_{\\{B_h(x)\\cap D_j\\neq\n         \\varnothing\\}}}{\\sum_{j=1}^mI_{\\{B_h(x)\\cap D_j\\neq\n         \\varnothing\\}}}\\sum_{(X_i^j,Y_i^j)\\in D_j}W_{h,X_{i}^j}(x)Y_{i}^j.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex39.m1\" class=\"ltx_Math\" alttext=\"\\hat{f_{h}}(x)=\\sum_{j=1}^{m}\\frac{I_{\\{B_{h}(x)\\cap D_{j}\\neq\\varnothing\\}}}{%&#10;\\sum_{j=1}^{m}I_{\\{B_{h}(x)\\cap D_{j}\\neq\\varnothing\\}}}\\sum_{(X_{i}^{j},Y_{i}%&#10;^{j})\\in D_{j}}W_{h,X_{i}^{j}}(x)Y_{i}^{j}.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><msub><mi>f</mi><mi>h</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><msub><mi>D</mi><mi>j</mi></msub><mo>\u2260</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow></msub><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><msub><mi>D</mi><mi>j</mi></msub><mo>\u2260</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup><mo>,</mo><msubsup><mi>Y</mi><mi>i</mi><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>Y</mi><mi>i</mi><mi>j</mi></msubsup></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nWe first prove (A) holds. To this end, we have\n\\begin{eqnarray*}\n         \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}(W^*_{h,X_i^j}(X))^2\\right\\}\n           &\\leq&\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W^*_{h,X_i^j}(X))^2 \\right\\}\\\\\n           &+&\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}(W^*_{h,X_i^j}(X))^2\\right\\},\n\\end{eqnarray*}\nwhere we define $\\sum_{\\varnothing}=0$. To bound the first term in\nthe right part of the above inequality, it is easy to see that if\n$I_{\\{X_{i}^j\\in B_h(X)\\}}=1$, then $I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}=1$, vice versa. Thus, it follows from (C)\n         that\n\\begin{eqnarray*}\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W^*_{h,X_i^j}(X))^2  \\right\\}\n           &=&\n           \\frac1{m^2}\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac1m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac1m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac{c_{3}}{Nh^d}\n\\end{eqnarray*}\nTo bound the second term, we have\n\\begin{eqnarray*}\n           &&\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}(W^*_{h,X_i^j}(X))^2\\right\\}\\\\\n           &&=\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}\\left(\\frac{I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}W_{h,X_{i}^j}(X)}{\\sum_{j=1}^mI_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}}\\right)^2\\right\\}\n\\end{eqnarray*}\nAt first, the same method as that in the proof of Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} yields that\n\n", "itemtype": "equation", "pos": 53995, "prevtext": "\nWe then use Theorem \\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL\nESTIMATE} to consider a new local estimate with\n\n", "index": 105, "text": "$$\n      W^*_{h,X_{i}^j}(x)=\\frac{I_{\\{B_h(x)\\cap D_j\\neq\n         \\varnothing\\}}W_{h,X_{i}^j}(x)}{\\sum_{j=1}^mI_{\\{B_h(x)\\cap D_j\\neq\n         \\varnothing\\}}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex40.m1\" class=\"ltx_Math\" alttext=\"W^{*}_{h,X_{i}^{j}}(x)=\\frac{I_{\\{B_{h}(x)\\cap D_{j}\\neq\\varnothing\\}}W_{h,X_{%&#10;i}^{j}}(x)}{\\sum_{j=1}^{m}I_{\\{B_{h}(x)\\cap D_{j}\\neq\\varnothing\\}}}.\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><msub><mi>D</mi><mi>j</mi></msub><mo>\u2260</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow></msub><mo>\u2062</mo><msub><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><msub><mi>D</mi><mi>j</mi></msub><mo>\u2260</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nTherefore, we have\n\\begin{eqnarray*}\n          &&\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}\\left(\\frac{I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}W_{h,X_{i}^j}(X)}{\\sum_{j=1}^mI_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}}\\right)^2\\right\\}\\\\\n         &\\leq&\n         \\frac{4}{Nh^d}+m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}\\left(W_{h,X_{i}^j}(X)I_{\\|X-X_i^j\\|>h}\\right)\\right\\}\\\\\n           &\\leq&\n           \\frac{4+c_{3}+c_5}{Nh^d}.\n\\end{eqnarray*}\nNow, we turn to prove (B) holds. This can be deduced directly by\nusing the similar method as the last inequality and the condition\n(E). That is,\n\n", "itemtype": "equation", "pos": 56157, "prevtext": "\nWe first prove (A) holds. To this end, we have\n\\begin{eqnarray*}\n         \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}(W^*_{h,X_i^j}(X))^2\\right\\}\n           &\\leq&\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W^*_{h,X_i^j}(X))^2 \\right\\}\\\\\n           &+&\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}(W^*_{h,X_i^j}(X))^2\\right\\},\n\\end{eqnarray*}\nwhere we define $\\sum_{\\varnothing}=0$. To bound the first term in\nthe right part of the above inequality, it is easy to see that if\n$I_{\\{X_{i}^j\\in B_h(X)\\}}=1$, then $I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}=1$, vice versa. Thus, it follows from (C)\n         that\n\\begin{eqnarray*}\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W^*_{h,X_i^j}(X))^2  \\right\\}\n           &=&\n           \\frac1{m^2}\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac1m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\in B_h(X)}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac1m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}(W_{h,X_i^j}(X))^2  \\right\\}\\\\\n           &\\leq&\n           \\frac{c_{3}}{Nh^d}\n\\end{eqnarray*}\nTo bound the second term, we have\n\\begin{eqnarray*}\n           &&\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}(W^*_{h,X_i^j}(X))^2\\right\\}\\\\\n           &&=\n           \\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}\\left(\\frac{I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}W_{h,X_{i}^j}(X)}{\\sum_{j=1}^mI_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}}\\right)^2\\right\\}\n\\end{eqnarray*}\nAt first, the same method as that in the proof of Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE} yields that\n\n", "index": 107, "text": "$$\n       \\mathbf E\\{B_h(X)\\cap D=\\varnothing\\}\\leq \\frac{4}{Nh^d}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex41.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\{B_{h}(X)\\cap D=\\varnothing\\}\\leq\\frac{4}{Nh^{d}}.\" display=\"block\"><mrow><mi>\ud835\udc04</mi><mrow><mo stretchy=\"false\">{</mo><msub><mi>B</mi><mi>h</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2229</mo><mi>D</mi><mo>=</mo><mi mathvariant=\"normal\">\u2205</mi><mo stretchy=\"false\">}</mo></mrow><mo>\u2264</mo><mfrac><mn>4</mn><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06239.tex", "nexttext": "\nThen Theorem \\ref{THEOREM: DLAE} follows from Theorem\n\\ref{THEOREM:SUFFICIENT CONDITION FOR LOCAL ESTIMATE}. $\\square$\n\n\n\\section{Conclusion}\\label{section6}\nIn this paper, we combined  the  divide and conquer strategy with\nlocal average regression to provide a new method called\naverage-mixture local average regression (AVM-LAR) to  attack the\nmassive data regression problems. We found that the estimate\nobtained by AVM-LAR can achieve the minimax learning rate under a strict\nrestriction concerning $m$. We then proposed two variants of AVM-LAR to either lessen the restriction or remove it. Theoretical analysis and\nsimulation studies confirmed our assertions.\n\nWe discuss here two interesting  topics for future study.\nFirstly, LAR cannot handle the high-dimensional data due to the\ncurse of\ndimensionality~\\citep{gyorfi2006distribution,fan2000prospects}. How\nto design variants of AVM-LAR to overcome this hurdle can be accommodated as an desirable\nresearch topic. Secondly, equipping  other nonparametric methods\n(e.g., \\citet{fan1994censored,gyorfi2006distribution,tsybakov2008introduction}) with the  divide and conquer\nstrategy can be taken into consideration for massive data analysis.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{asa}\n\\bibliography{distributedlearning}\n\n", "itemtype": "equation", "pos": 56909, "prevtext": "\nTherefore, we have\n\\begin{eqnarray*}\n          &&\\mathbf E\\left\\{\\sum_{j=1}^m\\sum_{(X_i^j,Y_i^j)\\in\n           D_j,X_{i}^j\\notin B_h(X)}\\left(\\frac{I_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}W_{h,X_{i}^j}(X)}{\\sum_{j=1}^mI_{\\{B_h(X)\\cap D_j\\neq\n         \\varnothing\\}}}\\right)^2\\right\\}\\\\\n         &\\leq&\n         \\frac{4}{Nh^d}+m\\max_{1\\leq j\\leq m}\\mathbf E\\left\\{\\sum_{(X_i^j,Y_i^j)\\in\n           D_j}\\left(W_{h,X_{i}^j}(X)I_{\\|X-X_i^j\\|>h}\\right)\\right\\}\\\\\n           &\\leq&\n           \\frac{4+c_{3}+c_5}{Nh^d}.\n\\end{eqnarray*}\nNow, we turn to prove (B) holds. This can be deduced directly by\nusing the similar method as the last inequality and the condition\n(E). That is,\n\n", "index": 109, "text": "$$\n      \\mathbf\n      E\\left\\{ \\sum_{j=1}^m\n      \\sum_{(X_i^j,Y_i^j)\\in D_j}|W^*_{h,X^j_i}(X)|I_{\\{\\|X-X_i\\|>h\\}}\\right\\}\n      \\leq\n       \\frac{c_{5}}{\\sqrt{Nh^d}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex42.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{E}\\left\\{\\sum_{j=1}^{m}\\sum_{(X_{i}^{j},Y_{i}^{j})\\in D_{j}}|W^{*}_{h,%&#10;X^{j}_{i}}(X)|I_{\\{\\|X-X_{i}\\|&gt;h\\}}\\right\\}\\leq\\frac{c_{5}}{\\sqrt{Nh^{d}}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc04</mi><mo>\u2062</mo><mrow><mo>{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup><mo>,</mo><msubsup><mi>Y</mi><mi>i</mi><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>j</mi></msub></mrow></munder><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mi>W</mi><mrow><mi>h</mi><mo>,</mo><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mi>X</mi><mo>-</mo><msub><mi>X</mi><mi>i</mi></msub><mo>\u2225</mo><mo>&gt;</mo><mi>h</mi><mo stretchy=\"false\">}</mo></mrow></msub></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>\u2264</mo><mfrac><msub><mi>c</mi><mn>5</mn></msub><msqrt><mrow><mi>N</mi><mo>\u2062</mo><msup><mi>h</mi><mi>d</mi></msup></mrow></msqrt></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}]