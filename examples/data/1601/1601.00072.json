[{"file": "1601.00072.tex", "nexttext": "\n\n\\noindent with respect to:\n\n\n", "itemtype": "equation", "pos": 4782, "prevtext": "\n\\twocolumn[\n\\begin{@twocolumnfalse}\n\\maketitle\n\n\n\n \n\n \n\n \n \n \n  \n  \n    \n \n\n    \\begin{abstract}\nIn this paper, a fast and practical GPU-based implementation of Fuzzy C-Means (FCM) clustering algorithm for image segmentation is proposed. First, an extensive analysis is conducted to study the dependency among the image pixels in the algorithm for parallelization. The proposed GPU-based FCM has been tested on digital brain simulated dataset to segment white matter(WM), gray matter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The execution time of the sequential FCM is 2798 seconds for an image dataset with the size of 1MB. While the proposed GPU-based FCM requires only 4.2seconds for the similar size of image dataset. An estimated 674-fold superlinear speedup is measured for the data size of 700 KB on a CUDA device that has 448 processors.\n    \\bigskip\n\n    \\noindent Superlinear speedup, Fuzzy C-Means, Parallel algorithms, Graphic Processing Units (GPUs), CUDA \n\n\n    \\end{abstract}\n\\end{@twocolumnfalse}\n]\n    \n    \n\\section{Introduction}\n\\label{intro}\nImage segmentation has been one of the fundamental research areas in image processing. It is a process of partitioning a given image into desired regions according to the chosen image feature information such as intensity or texture. The segmentation is used with application in the field of medical imaging, tumors locating and diagnosis. Over the past few decades, as image segmentation has gained much interest, various segmentation techniques have been proposed, each of which uses different induction principle.\\\\   \n\\indent Clustering is one of the most popular techniques used in image segmentation. In clustering, the goal is to produce coherent clusters of pixels \\cite{Chattopadhyay}. The pixels in a cluster are as similar as possible with respect to the selected image feature information. While the pixels belong in the adjacent clusters are significantly different with respect to the same selected image feature information \\cite{Chattopadhyay}. There are variants of clustering algorithms have been used widely in image segmentation and they are K-Means \\cite{Tou}, Fuzzy C-Means (FCM) \\cite{Bezdek}, and ISODATA \\cite{Ball}. \\\\\n\\indent In the last decades, FCM has been very popularly used to solve the image segmentation problems \\cite{Shen}; \\cite{Vadiveloo}. It is a fuzzy clustering method that allows a single pixel to belong to two or more clusters. The introduction of fuzziness makes this algorithm to able to retain more information from the original image than the crisp or hard clustering algorithms \\cite{Shen}; \\cite{Vadiveloo}. However this sequential FCM becomes computationally intensive when segmenting large image datasets \\cite{Vadiveloo}. In such a case, the algorithm becomes very inefficient.\\\\   \n\\indent One-way to improve the performance of the FCM clustering algorithm is to use parallel computing methods. Initially, Graphic Processing Units (GPUs) were specific-purpose processors that only manipulate and accelerate the creation of images intended for output to a display. However, GPUs have recently shifted to general-purpose processors (GPGPUs) to solve general concerns, such as scientific and engineering problems. Data parallelism on a GPU is a powerful parallel model. In this paper, a fast and practical parallel FCM approach on GPGPU is presented and discussed. \\\\\n\\indent This paper is organized as follows: Section 2 provides a background of FCM algorithm and the parallel technology used. Section 3 presents related works. The proposed method is explained in detail in Section 4. The experimental results are presented and discussed in Section 5. Finally, Section 6 provides the conclusion and suggestions for future works.\n\n\\section{Preliminaries}\n\\label{sec:Preliminaries}\nIn the first sub-section, a brief introduction on Fuzzy C-Means (FCM) algorithm is presented. While in the following sub-section the parallel technology used in this work namely on General Purpose Computing on Graphics Processing Units (GPGPU) data parallelism is discussed. \n\n\\subsection{Fuzzy C-Means Algorithm}\n\\label{sec:fcm}\nFuzzy C-Means was developed by \\cite{Bezdek}. It is an iterative optimization that minimizes the objective function defined in \\ref{eq1111}. The objective function consists of two main components $u$ and $v$.~$u_{ij}$ is the membership function of a pixel, $x_i$. It represents the probability that $x_i$ may belong to a cluster. The $u_{ij}$ is dependent on the distance function, $d_{ij}$. $d_{ij}$ is the Euclidean distance measure between the pixel $x_i$ and each cluster center,  $v_j, d_{ij}=|| x_i - v_j ||$. $m$ is a constant that represents the fuzziness value of the resulting clusters that are to be formed; $1\\leq m\\leq \\infty$.\n\n\n", "index": 1, "text": "\\begin{equation}\\label{eq1111}\n\\large J_{i}= \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^c u_{ij}^m {||x_i - v_j||}^2 \\\\ \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\large J_{i}=\\sum\\limits_{i=1}^{N}\\sum\\limits_{j=1}^{c}u_{ij}^{m}{||x_{i}-v_{j%&#10;}||}^{2}\\\\&#10;\" display=\"block\"><mrow><msub><mi mathsize=\"120%\">J</mi><mi mathsize=\"120%\">i</mi></msub><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">i</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">N</mi></munderover><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">j</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">c</mi></munderover><mrow><msubsup><mi mathsize=\"120%\">u</mi><mrow><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">j</mi></mrow><mi mathsize=\"120%\">m</mi></msubsup><mo>\u2062</mo><msup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi mathsize=\"120%\">x</mi><mi mathsize=\"120%\">i</mi></msub><mo mathsize=\"120%\" stretchy=\"false\">-</mo><msub><mi mathsize=\"120%\">v</mi><mi mathsize=\"120%\">j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn mathsize=\"120%\">2</mn></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00072.tex", "nexttext": "\n\n\\indent In image clustering, the most commonly used feature is the grey level or intensity value of the image being segmented. Therefore, the objective function,$ j_m$ in \\ref{eq1111} is minimized when higher membership value is assigned to pixels with intensity values close to a cluster center of the corresponding cluster, while lower membership value is assigned to pixels whose intensities are far from the cluster center.  \n\n\n", "itemtype": "equation", "pos": 4942, "prevtext": "\n\n\\noindent with respect to:\n\n\n", "index": 3, "text": "\\begin{equation}\\label{eq112}\n\\begin{split}\n\\large \\sum\\limits_{j=1}^c  u_{ij} = 1, 1\\leq i\\leq n \\\\ \n0 < \\sum\\limits_{i=1}^n  u_{ij} < n, 1\\leq j\\leq c \\\\\n\\sum\\limits_{i=1}^c \\sum\\limits_{i=1}^n u_{ij} = n .\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\large\\sum\\limits_{j=1}^{c}u_{ij}=1,1\\leq i\\leq n\\\\&#10;\\displaystyle 0&lt;\\sum\\limits_{i=1}^{n}u_{ij}&lt;n,1\\leq j\\leq c\\\\&#10;\\displaystyle\\sum\\limits_{i=1}^{c}\\sum\\limits_{i=1}^{n}u_{ij}=n.\\end{split}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">j</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">c</mi></munderover><msub><mi mathsize=\"120%\">u</mi><mrow><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">j</mi></mrow></msub></mrow><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mo mathsize=\"120%\" stretchy=\"false\">,</mo><mrow><mn mathsize=\"120%\">1</mn><mo mathsize=\"120%\" stretchy=\"false\">\u2264</mo><mi mathsize=\"120%\">i</mi><mo mathsize=\"120%\" stretchy=\"false\">\u2264</mo><mi mathsize=\"120%\">n</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mn>0</mn><mo>&lt;</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>&lt;</mo><mi>n</mi></mrow><mo>,</mo><mrow><mn>1</mn><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><mi>c</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>u</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>=</mo><mi>n</mi></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.00072.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 5610, "prevtext": "\n\n\\indent In image clustering, the most commonly used feature is the grey level or intensity value of the image being segmented. Therefore, the objective function,$ j_m$ in \\ref{eq1111} is minimized when higher membership value is assigned to pixels with intensity values close to a cluster center of the corresponding cluster, while lower membership value is assigned to pixels whose intensities are far from the cluster center.  \n\n\n", "index": 5, "text": "\\begin{equation}\\label{eq113}\n\\large v_{i}= \\frac{\\sum\\limits_{i=1}^N u_{ij}^m. x_{i}}{\\sum\\limits_{i=1}^N u_{ij}^m} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\large v_{i}=\\frac{\\sum\\limits_{i=1}^{N}u_{ij}^{m}.x_{i}}{\\sum\\limits_{i=1}^{N%&#10;}u_{ij}^{m}}\" display=\"block\"><mrow><msub><mi mathsize=\"120%\">v</mi><mi mathsize=\"120%\">i</mi></msub><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mfrac><mrow><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">i</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">N</mi></munderover><msubsup><mi mathsize=\"120%\">u</mi><mrow><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">j</mi></mrow><mi mathsize=\"120%\">m</mi></msubsup></mrow><mo mathsize=\"120%\" stretchy=\"false\">.</mo><msub><mi mathsize=\"120%\">x</mi><mi mathsize=\"120%\">i</mi></msub></mrow><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">i</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">N</mi></munderover><msubsup><mi mathsize=\"120%\">u</mi><mrow><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">j</mi></mrow><mi mathsize=\"120%\">m</mi></msubsup></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00072.tex", "nexttext": "\n\n\\indent Starting with random initialization of the membership values for each pixel from the manually selected clusters, the clusters are converged by recursively updating the cluster centers and membership function in \\ref{eq113} and \\ref{eq114}. This is to minimize the objective function in \\ref{eq1111}. Convergence stops when the overall difference in the membership function between the current and previous iteration is smaller than a given epsilon value, $\\varepsilon$. After the convergence, deffuzzifaction is applied. Each pixel is assigned to a specific cluster according to the maximal value of its membership function. The steps of the Fuzzy C-Means algorithm are illustrated in Algorithm \\ref{algo1C}. \n\n\n{\\LinesNumberedHidden\n\\begin{algorithm}\n\\DontPrintSemicolon \n\\textbf{Assumptions: Image is transformed into feature space.}\\\\\n\nStep 1: Initialize the number of clusters $c$,  $m =2$, and $\\varepsilon = 0.005$\\\\\nStep 2: Initialize the membership function, $u_{ij}$  randomly.\\\\\nStep 3: \\Repeat {$||u^{k+1}_{ij}-u^{k}_{ij}|| < \\varepsilon $ } {\n Update the cluster center, $v_i$ using Equation \\ref{eq113}\\\\\n Update the membership function $u_{ij}$ using Equation \\ref{eq114}\\\\\n  }\n\\caption{{\\sc Fuzzy C-Means algorithm} }\n\\label{algo1C}\n\\end{algorithm}}\n\n\n\n\\subsection{Data Parallelism on GPGPU}\n\\label{sec:gpu}\nInitially, GPU was a hardware equipped with a processor specifically designed to accelerate graphic processing. Eventually, GPU applications were extended to general-purpose computations. At present, GPGPU is used in many applications typically performed using a CPU, such as analytic, engineering, and scientific applications \\cite{Farber}. With the release of the massively parallel architecture called CUDA in 2007 from NVIDIA, GPUs have become widely accessible \\cite{Kirk2010}.\\\\\n\\indent A GPU is a processor or a multiprocessor device that has hundreds or even thousands of cores called scalar processors (SPs), which are arranged in groups named streaming multiprocessors (SMs), as shown in the left side of Fig. \\ref{executionmodel}. Moreover, GPUs have different kinds of memories: global, local, texture, constant, shared, and register memories. Global, constant, and texture memories are accessible to all threads in the grid. Shared memory is visible to threads within one CUDA block. It is faster than the global memory but is limited by size. Register memory is visible to the thread that initialized the said memory and lasts for the lifetime of that thread.\\\\\n\\indent CUDA is the parallel programming model used for NVIDIA GPGPUs. CUDA can increase the performance by harnessing the power of a GPU device. Thousands of threads can be executed concurrently using CUDA on GPGPU. The execution model of CUDA on NVIDIA devices is shown in Fig. \\ref{executionmodel}.\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.55]{executionmodel.png}\n\\caption{CUDA Execution Model on GPGPU}\n\\label{executionmodel}\n\\end{center}\n\\end{figure}\n\n\n\\section{Related works}\n\\label{sec:Related works}\nLi et al. proposed an Fuzzy C-Means (FCM) algorithm based on GPU \\cite{Li2014}. They modified the sequential FCM algorithm, such that the calculations of the membership and cluster center matrices are not comparable to the sequential one. They have FCM on GPU using CUDA. The empirical results obtained by Li et al. showed that the proposed parallel FCM on GPU is more efficient than the sequential FCM. Instead of efficiency, they claimed that the proposed method exhibits improvement in the quality of the GPU segmented image. The authors achieved a 10-fold speedup with the proposed parallel FCM on NVIDIA GTX 260 device compared with the sequential FCM for natural images sized from 53kb to 101kb.\\\\\n\\indent Mahmoud et al. presented a GPU-based brFCM for medical images segmentation \\cite{Mahmoud2015}. The brFCM is a faster variant of the sequential FCM \\cite{Eschrich2003}. The GPU-based brFCM is implemented on different GPGPU cards. Mahmoud et al  showed that the GPU-based brFCM  has a significant improvement over the parallel FCM in \\cite{2014Soroosh}. The achieved speedup is up tp 23.42 fold faster than parallel FCM in \\cite{2014Soroosh} for medical images of 350x350 and 512x512 dimensions.\\\\\n\\indent Shalom et al. proposed a scalable FCM based on graphic hardware \\cite{Shalom2008}. On two different graphic cards, the results show that the proposed GPU-based FCM algorithm is more efficient and faster than the sequential FCM. The authors succeeded in reaching a 73-fold speedup on NVIDIA GeForce 8500 GT. Amazingly, a 140-fold speedup was achieved on NVIDIA GeForce 8800 GTX compared with sequential FCM for 65k yeast gene expression data set of 79 dimension.\\\\\n\\indent Rowinska and Goclawski proposed a CUDA-based FCM algorithm to accelerate image-segmentation \\cite{Rowi2012}. The proposed method has been tested on polyurethane foam with fungus color images and was compared with the sequential FCM implemented using C++ and MATLAB. The authors achieved a 10-fold speedup of their parallel proposal compared with the FCM implemented in C++ for object area of 310k pixels, and a 50- to 100-fold speedup compared with the FCM implemented in MATLAB for object area of 260k pixels. A comparison of our work and the previous related works is summarized in Table \\ref{relatedworks}. \n\n\\begin{table*}[t]\n\\centering\n\\caption{Comparison of our work and previous related works} \n\\begin{tabular}{c|L|L|c}\n\\textbf{Work by}& \\textbf{Method} & \\textbf{Image dataset} & \\textbf{Speedup}\\\\\\hline \n\n$ \\begin{matrix} \\text{Li et al.} \\\\ \\text{\\cite{Li2014}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Modified the original FCM algorithm and then parallelized it on GPGPU}&  \\multicolumn{1}{m{3cm}|}{Natural images (from 53kB to 101kB)} & 10x  \\\\\\hline\n$ \\begin{matrix} \\text{Mahmoud et al.} \\\\ \\text{\\cite{Mahmoud2015}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Parallelized brFCM the variant of FCM algorithm on GPGPU}&  \\multicolumn{1}{m{3cm}|}{Medical images (Lung CT with the dimesion of 512x512; Knee MRI with the dimension of 350x350)} & \\makecell{23x faster\\\\ than in \\cite{2014Soroosh} } \\\\\\hline\n$ \\begin{matrix} \\text{Shalom et al } \\\\ \\text{\\cite{Shalom2008}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Proposed a scalable FCM GPU-based implementation}&  \\multicolumn{1}{m{3cm}|}{Yeast gene expression data set (79 dimension with 65K genes)} & 140x  \\\\\\hline\n$ \\begin{matrix} \\text{Rowinska et al.} \\\\ \\text{\\cite{Rowi2012}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Presented a CUDA-based FCM algorithm to accelerate image segmentation}&  \\multicolumn{1}{m{3cm}|}{Polyurethane foam with fungus color images (object area of 310k pixels)} & 10x  \\\\\\hline\nThis paper & \\multicolumn{1}{m{3cm}|}{A parallel FCM approach on GPGPU using CUDA  }&  \\multicolumn{1}{m{3cm}|}{Digital brain phantom simulated dataset (from 20kB to 1000kB)} & $ \\begin{matrix} \\text{Superlinear} \\\\ \\text{speedup} \\\\ \\text{up to 674x} \\end{matrix}$    \n\\end{tabular}\n\\label{relatedworks} \n\\end{table*}\n\n\\section{The Proposed Method}\n\\label{sec:FCM Parallel Approach}\nThe sequential FCM algorithm has been subjected to extensive analysis in order to find out where the algorithm exhibits parallelism that we might exploit in the parallel design. The strongest data dependency in the FCM algorithm is the steps where the total summation calculation is required, as illustrated in step 3 in the sequential FCM (Algorithm \\ref{algo1C}). For instance, two sigma operations are needed to calculate the cluster centers as shown in Equation \\ref{eq113}. Such a strong dependency makes parallelizing the sequential algorithm infeasible. According to Bernstein\u00e2\u0080\u0099s conditions \\cite{Bernstein}, this type of dependency is called output dependence. In parallel computing, the reduction method is an efficient approach to remove output dependence.\\\\\n\\indent The proposed parallel FCM design consists of two main parts: a sequential part executed on the CPU (host) and a parallel part executed on the GPU (device). Fig. \\ref{block} shows the block diagram of the proposed work. The following sub-sections discuss each stages of the block diagram in Fig. \\ref{block}. \n\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[scale=0.71]{BlockDiagram1.png}\n\\caption{The Block Diagram of The Proposed Parallel Fuzzy C-Means}\n\\label{block}\n\\end{center}\n\\end{figure*}\n\n\\subsection{Initialization and data transferring}\n\\label{sec:Initialization}\nAs shown in Fig. \\ref{block}, the first two steps are executed sequentially on the host. The membership is randomly initialized. The memories are allocated on the device global memory for the pixels of the image data, membership, and cluster centers. All the arrays are defined in a 1-D pattern.\\\\\n\\indent After defining memories on the device, all the data are transferred from host to device, and then the main program loop is started. Subsequently, the parallel kernels are called concurrently to manipulate the image pixels on the device.\n\n\\subsection{Calculating cluster centers from membership functions}\n\\label{sec:Calculating cluster centers}\nThe host calls four CUDA kernels one after another to calculate the cluster centers from memberships. The first CUDA kernel concurrently handles the heavy calculations, such as exponential, division, and multiplication of floating points for every pixel. At this step, the final summation is not included. The numerator and denominator of Equation \\ref{eq113} are calculated separately for every pixel, and the results are stored in two different arrays in the device global memory. The number of spawned CUDA threads in this kernel is defined to be equal to the number of image pixels, such that every thread will handle one pixel.      \n\n\\indent The second CUDA kernel at this phase is the reduction kernel, which computes the partial summation of the numerator of Equation \\ref{eq113}. The reduction technique is an efficient method to break down the dependency among the data. The computation complexity of the sequential addition of $n$ elements is $O(n)$. However, using parallel computing can significantly improve the computation complexity to $O(log~n)$ \\cite{2014r3}\\cite{2014r6}. Several CUDA reduction methods are available, such as in \\cite{2014r1}\\cite{2014r2}\\cite{2014r3}. The CUDA reduction method used in this work is similar to \\cite{2014r3} and is shown in Algorithm \\ref{algo1}. First, a segment of the input is loaded into the device-shared memory. This device shared memory can facilitate fast access to the image pixels \\cite{2014r4}\\cite{2014r5}. The reduction process is then performed over the shared memory. Each calculated partial sum of every segment stored in the shared memory is loaded to the output in the global memory. As illustrated in Algorithm \\ref{algo1}, the CUDA block ID (blockIdx.x) is used as an index to store the partial sum from the device-shared memory to the global memory. Fig. \\ref{reduction} demonstrates the reduction process performed on GPGPU using shared memory.\n\n\\begin{algorithm*}\n\\DontPrintSemicolon \n\\KwIn{A large set $A=\\{a_1, a_2, \\ldots, a_n\\}$ where $n = pixels$}\n\\KwOut{A reduced small set $B=\\{b_1, b_2, \\ldots, b_m\\}$ where $m = n / blockDim<<1$}\n$global\\_idx \\gets blockIdx.x*blockDim.x + threadIdx.x$\\;\n$local\\_idx \\gets threadIdx.x$\\; \n$start \\gets  2*blockIdx.x*blockDim.x$\\;\n\\_\\_shared\\_\\_  $partialSum[2*MAX\\_THREAD]$\\;\n\\textbf{//Loading segment from the input into the shared memory:}\\;\n\\If{$(start + local\\_idx) < n$}\n    {\n        $partialSum[local\\_idx] = A[start + local\\_idx]$\\;      \n    }\n\\Else\n    {       \n        $partialSum[local\\_idx] = 0.0$\\;\n    }\n\\If{$(start + local\\_idx+ blockDim.x) < n$}\n    {\n        $partialSum[local\\_idx + blockDim.x] = A[start + local\\_idx + blockDim.x]$\\;      \n    }\n\\Else\n    {       \n        $partialSum[local\\_idx + blockDim.x] = 0.0$\\;\n    }\n\\textbf{//Reduction over the device shared memory:}\\;\n\\For{$stride \\gets blockDim.x$ \\textbf{to} $0$ ; $stride /= 2$} {\n  \\If{$local\\_idx < stride$} {\n    $partialSum[local\\_idx] += partialSum[local\\_idx + stride]$\\;\n  }\n}\n\\textbf{//Storing the output into the device global memory:}\\;\n\\If{$local\\_idx == 0 \\&\\& (global\\_idx*2) < n$} {\n    $B[blockIdx.x] = partialSum[local\\_idx]$\\;\n}\n\\caption{{\\sc Sum Reduction on GPGPU Using CUDA} }\n\\label{algo1}\n\\end{algorithm*}\n\nThe actual reduction for the illustrated example in Fig. \\ref{reduction}, reduces the addition operations from adding 16 elements to only 2 elements. Another example from the conducted experiments of this work is an image with a size of 1 MB (1048576 bytes) that was reduced to $(1048576/128<<1)$, which equals 4 KB (4096 bytes).\\\\\n\\indent The third kernel to be called in this phase (calculating cluster centers from the membership function phase) is another reduction kernel that calculates the partial sum of the denominator of Equation \\ref{eq113}. Finally, the last CUDA kernel calculates both final summations from the previous two kernels and computes the final result. Only one thread is defined for this kernel. The reason for this one thread kernel is that instead of transferring the reduced arrays from the previous kernels to the host memory to calculate the final summations, in this proposed method the device is allowed to carry out the final summation only with one thread. Lastly, all the previous four CUDA kernels are called in iterative loops that are equal to the predefined number of clusters. This is to calculate the cluster centers from the membership functions as shown in Fig. \\ref{block}. \n\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.35]{reduction.png}\n\\caption{Sum Reduction Example on GPGPU. There are four CUDA block dimension in this example.}\n\\label{reduction}\n\\end{center}\n\\end{figure}\n   \n\\subsection{Calculating membership functions from cluster centers}\n\\label{sec:Calculating Mfs}\nOnly one CUDA kernel is defined to compute membership functions from the cluster centers. Rather than defining CUDA threads and block dimensions, the implementation in this kernel is quite similar to the sequential algorithm. The spawned CUDA threads are defined equally to the image pixels, which implies fine-grained granularity. Thus, one thread will handle one pixel. In correspondence to the previous phase of the proposed work \\ref{block}, this kernel will be called in an iterative loop equally to the predefined number of clusters. At this stage, the computed new membership function arrays will be transferred to the host. The host will determine if the new membership function satisfies the condition as shown in Fig. \\ref{block}. If the condition is satisfied, finally the cluster center arrays will be transferred back to the host. Defuzzification is performed and the the final segmented image is obtained. \n\n\n\\section{Implementation and Results}\n\\label{sec:results}\nIn this section, the implementation design of the proposed method is introduced in the first subsection. The functionality of the proposed method is proven using both qualitative and quantitative evaluations in the next subsections. The performance analysis is discussed in the final subsection.\n\n\\subsection{Implementation}\n\\label{sec:implementation}\nThe proposed method was implemented using C language and CUDA. First, the sequential FCM algorithm was implemented in C. Our sequential C version was derived from a Java version available online at \\cite{java}. The sequential FCM in C was tested on Intel Core i5-480 CPU, Windows 7 Ultimate platform.\\\\\n\\indent In the proposed parallel FCM, the image pixels, memberships, and cluster center arrays are defined in a 1-D pattern. The reason is to ensure coalesced memory transactions in the GPGPU. In addition, defining those input arrays in 1-D pattern will ease the number of CUDA block and grid sizes calculations. The CUDA block and grid sizes are consequently defined in 1-D patterns corresponding to the input arrays. Therefore, the form of the input has a significant effect on the performance of CUDA kernels because of the coalescing access \\cite{Kirk2010}\\cite{2014c}. Figure \\ref{arrays} illustrates examples on the indices of arrays are modified when converting multi-dimensional arrays to 1-D arrays. In this work, the image array was converted from 2-D to 1-D, and the membership array was converted from 3-D to 1-D. The details of the parallel platform used in this experiment are shown in Table \\ref{platform}.\n\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.35]{arrays.png}\n\\caption{Converting Multidimensional Arrays to One Dimensional}\n\\label{arrays}\n\\end{center}\n\\end{figure}\n\n\\begin{table}[H]\n\\caption{Platform of the experiments} \n\\centering  \n\n\\setlength{\\tabcolsep}{5pt} \n\\begin{tabular}{ l l } \n\n\n\\hline                  \n \n    \\textbf{CPU:}  & AMD Phenom(tm) II X4 810 Processor.  \\\\\n    \\textbf{Kernel:} & Linux  x86\\_64 GNU.  \\\\\n    \\textbf{GPU:} & NVIDIA Tesla C2050.  \\\\\n    \\textbf{CUDA:} & CUDA compilation tools, release 5.0. \\\\\n \n\n\\hline\n\\end{tabular}\n\\label{platform} \n\\end{table}\n\n\\subsection{Functionality Evaluation}\n\\label{sec:Functionality}\nThe proposed GPGPU-based FCM is tested on digital brain phantom simulated dataset from the Brain Web MR Simulator \\cite{Collins} with the size of 20kB to segment white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF) soft tissues regions. Skull stripping \\cite{Dogdas} has been carried out on the brain phantom images to remove skull and other non-brain soft tissues, so that only brain soft tissues are used in the proposed parallel Fuzzy C-Means (FCM) segmentation process. When applying the proposed FCM on the brain soft tissues, four clusters are manually selected to represent the WM, GM, and CSF soft tissues regions and the final cluster represent the background region. Therefore in the proposed parallel FCM, there are four cluster center values being associated with the aforementioned regions. The functionality of the proposed method is then proven using both qualitative and quantitative evaluations in the following subsections. \n\n\\subsubsection{Qualitative evaluation}\n\\label{sec:Qualitative}\nThe qualitative evaluation is performed for both the segmented results of the proposed parallel FCM and the sequential FCM. This is to evaluate the similarity of the segmented result of the proposed parallel FCM with the segmented result produced by the sequential FCM, visually. In Fig. \\ref{qual}, the experiment results are presented. It can be seen that the result of the proposed method is identical to the result of the sequential FCM. \n\n\\begin{figure}[it]\n\\begin{center}\n\\includegraphics[scale=0.35]{qual.png}\n\\caption{Representative Results of The 101\\textsuperscript{st}, 91\\textsuperscript{st} and 96\\textsuperscript{th} Axial Slice of Brain Tissue Phantom Using Sequential Fuzzy C-Means and The Proposed GPGPU-Based Fuzzy C-Means.}\n\\label{qual}\n\\end{center}\n\\end{figure}\n\n\\subsubsection{Quantitative evaluation}\n\\label{sec:Quantitative}\nThe quantitative evaluation is used to compare the results of the proposed parallel Fuzzy C- FCM and sequential FCM. Evaluation metrics such as Dice Coefficient Similarity (DSC) \\cite{Zijdenbos} and performance analysis are used. DSC is used to evaluate if the accuracy of the segmented results of the proposed method is statistically similar to the segmented results of the sequential FCM based on the ground truth. While performance analysis is to compare the execution time and speed up of the proposed method with the sequential FCM. DSC is defined as in Equation \\ref{eq116}.\n\n\n", "itemtype": "equation", "pos": 5744, "prevtext": "\n\n\n", "index": 7, "text": "\\begin{equation}\\label{eq114}\n\\large u_{ij}=\\frac{1}{\\sum\\limits_{k=1}^c \\bigg[{\\frac{||x_{i}-v_{j}||}{||x_{i}-v_{k}||}\\bigg]^\\frac{2}{m-1}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\large u_{ij}=\\frac{1}{\\sum\\limits_{k=1}^{c}\\bigg{[}{\\frac{||x_{i}-v_{j}||}{||%&#10;x_{i}-v_{k}||}\\bigg{]}^{\\frac{2}{m-1}}}}\" display=\"block\"><mrow><msub><mi mathsize=\"120%\">u</mi><mrow><mi mathsize=\"120%\">i</mi><mo>\u2062</mo><mi mathsize=\"120%\">j</mi></mrow></msub><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mfrac><mn mathsize=\"120%\">1</mn><mrow><munderover><mo largeop=\"true\" mathsize=\"120%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"120%\">k</mi><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mn mathsize=\"120%\">1</mn></mrow><mi mathsize=\"120%\">c</mi></munderover><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mfrac><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msub><mi mathsize=\"120%\">x</mi><mi mathsize=\"120%\">i</mi></msub><mo mathsize=\"120%\" stretchy=\"false\">-</mo><msub><mi mathsize=\"120%\">v</mi><mi mathsize=\"120%\">j</mi></msub></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow><mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo><mrow><msub><mi mathsize=\"120%\">x</mi><mi mathsize=\"120%\">i</mi></msub><mo mathsize=\"120%\" stretchy=\"false\">-</mo><msub><mi mathsize=\"120%\">v</mi><mi mathsize=\"120%\">k</mi></msub></mrow><mo fence=\"true\" maxsize=\"142%\" minsize=\"142%\">||</mo></mrow></mfrac><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mfrac><mn mathsize=\"120%\">2</mn><mrow><mi mathsize=\"120%\">m</mi><mo mathsize=\"120%\" stretchy=\"false\">-</mo><mn mathsize=\"120%\">1</mn></mrow></mfrac></msup></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00072.tex", "nexttext": "\n\nWhere $PR$ is the segmented results of each method while $GT$ is the ground truth provided with the dataset \\cite{Collins}. The DSC was implemented in C to be compatible with the implementation of the proposed method. An example of the ground truth is presented in Fig. \\ref{fig:sfig5}.\n\n\\begin{figure}[H]\n\\begin{center}\n\\begin{subfigure}{.15\\textwidth}\n\\centering\n  \\includegraphics[width=.7\\linewidth]{96.jpg}\n  \\caption{}\n  \\label{fig:sfig1}\n\\end{subfigure}\\\\\n\\begin{subfigure}{.15\\textwidth}\n\\centering\n  \\includegraphics[width=.7\\linewidth]{961.jpg}\n  \\caption{}\n  \\label{fig:sfig2}\n\\end{subfigure}\n\\begin{subfigure}{.15\\textwidth}\n\\centering\n  \\includegraphics[width=.7\\linewidth]{962.jpg}\n  \\caption{}\n  \\label{fig:sfig3}\n\\end{subfigure}\n\\begin{subfigure}{.15\\textwidth}\n\\centering\n  \\includegraphics[width=.7\\linewidth]{963.jpg}\n  \\caption{}\n  \\label{fig:sfig4}\n\\end{subfigure}\n\\begin{subfigure}{.15\\textwidth}\n\\centering\n  \\includegraphics[width=.7\\linewidth]{964.jpg}\n  \\caption{}\n  \\label{fig:sfig5}\n\\end{subfigure}\n\\caption{(a) The 96\\textsuperscript{th} Axial Slice of Brain Tissues Phantom and The Corresponding Ground Truth Images (b) White Matter (c) Gray Matter (d) Cerebrospinal Fluid (e) Background.\n}\n\\label{fig:fig}\n\\end{center}\n\\end{figure}\n\n\nFig. \\ref{quan} illustrates the percentage of DSC of the proposed parallel FCM and sequential FCM for white matter (WM), gray matter (GM), cerebrospinal fluid (CSF) and background regions for 91\\textsuperscript{th}, 96\\textsuperscript{th}, 101\\textsuperscript{th} and  111\\textsuperscript{th} axial slices of brain tissues phantom. The accuracy of the segmented results of both the proposed method and sequential FCM are statistically similar. \n\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[scale=0.35]{quan.png}\n\\caption{Percentage of Dice Similarity Coefficient for 91\\textsuperscript{th}, 96\\textsuperscript{th}, 101\\textsuperscript{th}, and 111\\textsuperscript{th} Axial Slices of Brain Tissues Phantom Images}\n\\label{quan}\n\\end{center}\n\\end{figure*}\n\n\\subsection{Performance analysis }\n\\label{sec:Performance}\nOnce the functionality of the parallel approach is confirmed, performance analysis in terms of execution time and speedup was performed. As mentioned in Section \\ref{sec:Calculating Mfs}, fine-grained granularity is adopted in this work in which one CUDA thread is spawned to manipulate one pixel. The total number of the spawned concurrent threads are equal to the image size to be segmented which indicates to the design scalability. The execution time was measured in both sequential and parallel approach for the process of calculating clusters centers and memberships. The initialization process was excluded from the measurements in both approaches. The function gettimeofday() was used to measure the elapsed time. For the sake of verification, the cudaEventRecord() function from CUDA API was also used to test the execution time. Table \\ref{time} presents the execution time of both the sequential FCM and the proposed parallel FCM on GPGPU. The results of the execution time listed in Table \\ref{time} and the corresponding speedup illustrated in Fig. \\ref{speedup}, are the average execution time and speedup of 30 runs.\n\nFrom Table \\ref{time}, it is shown that we have conducted experiments on various sizes of dataset from 20KB up to 1MB. In order to evaluate the execution time of the proposed parallel FCM in larger size dataset, we have enlarged the original phantom dataset 6KB (the original dataset size) up to 1MB. This enlargement is done only on the basis to evaluate the execution time of the proposed method in a larger size dataset.\n\n\n\\begin{table}[H]\n\\caption{The Execution Time of Sequential Fuzzy C-Means and The Proposed Parallel Fuzzy C-Means In Seconds.} \n\\centering  \n\n\\setlength{\\tabcolsep}{3pt} \n\\begin{tabular}{ c  c  c } \n\n\\vtop{\\hbox{\\strut \\textbf  {Dataset Size}}\\hbox{\\strut   ~~~~~(Byte)}}&\\vtop{\\hbox{\\strut \\textbf  {Sequential FCM}}\\hbox{\\strut   ~~~~~~~~~(sec)}} &\\vtop{\\hbox{\\strut \\textbf  {Parallel FCM}}\\hbox{\\strut   ~~~~~~~(sec)}}\\\\ [0.5ex] \n\n\\hline     \n \n    20KB  & 57 & 0.102  \\\\\n    40kB & 114 & 0.195  \\\\\n    60KB & 177 & 0.321  \\\\\n    80KB   & 231 & 0.505  \\\\\n    100KB   & 287 & 0.632  \\\\\n    120KB   & 341 & 0.864  \\\\\n    140KB   & 394 & 0.977  \\\\\n    160KB   & 446 & 0.986  \\\\\n    180KB   & 503 & 1.22  \\\\\n    200KB   & 558 & 1.45  \\\\\n    300KB   & 845 & 2.18  \\\\\n    500KB   & 1420 & 2.4  \\\\\n    700KB   & 1955 & 2.9  \\\\\n    1000KB   & 2798 & 4.2 \\\\ [1ex]      \n\n\n\\end{tabular}\n\\label{time} \n\\end{table}\n\n\\indent Fig. \\ref{speedup} shows the speedup results of the proposed parallel FCM over the sequential FCM. The horizontal red line in the middle of the chart represents the number of processing elements in the GPGPU device (Tesla C2050) used in the experiments. Generally, when the speedup exceeds the number of the utilized processors in parallel computing then this speedup is referred to as superlinear speedup. In fact, the speedup may equal to the number of the parallel processors only in the ideal situation because of the external overheads, such as data transferring, synchronization, and scheduling \\cite{Fayez}. However, superlinear speedup can be achieved in some circumstances, such as in low-level computations because of memory hierarchies and cache effect \\cite{Ananth}.\n\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[scale=0.5]{speedup3.png}\n\\caption{Speed Up of The Proposed Parallel Fuzzy C-Means on Tesla C2050 of 448 Processing Elements}\n\\label{speedup}\n\\end{center}\n\\end{figure*}\n\n\\indent The superlinear speedup obtained in this work can be justified by the high speed of GPGPUs in manipulating floating points compared with CPUs. In fact, GPGPUs are highly optimized for floating-point computations. The Intel i5 CPU used in the experiments can achieve 23 GFLOPs (Giga Floating Point Operations per Second) \\cite{2014r7}. Meanwhile, the Tesla C2050 GPU used in this work can achieve 1030 GFLOPs \\cite{2014r8}.\\\\\n\\indent In Fig. \\ref{speedup}, superlinear speedup is obtained when the data size varies from 20 KB to 80 KB. When the data size is larger than 80 KB up to 300 KB, the speedup varies from 400- to 448-fold. Superlinear speedup occurs again when the data size goes beyond 300 KB.\\\\\n\\indent The results of the proposed parallel FCM shows unusual behavior of the speedup with respect to the number of the spawned threads even though the superlinear speedup provides outstanding performance. This behavior of the speedup poses some open questions. By referring to Fig. \\ref{speedup}, we would like to list down the following open questions:\n\\begin{enumerate}\n\\item When the data size varies from 20KB to 100KB, superlinear speedup of the parallel FCM is achieved although the number of the spawned concurrent threads is considerably small? \n\\item Does the nature of FCM algorithm have any role to play in obtaining superlinear speedup on GPGPU? \n\\item When the data size varies from 100KB to approximately 360KB, the results show no superlinear speedup, why?\n\\item The superlinear speedup happened again when the data size exceeds 360KB and the achieved results are much better compared with the ones mentioned in question 1. What will the speedup behavior be when the data size exceeds 1MB? \n\\item The proposed parallel FCM was tested on NVIDIA Tesla C2050 device of 448 processing elements, will the superlinear speedup and speedup behavior with respect to the spawned threads number occur on other GPGPU devices of different specifications? Or, is the nature of FCM algorithm only fitting Tesla C2050 such that this outstanding performance is achieved? \n\\end{enumerate}\n\n\\section{Conclusion and Future Works}\n\\label{sec:8}\nGPGPUs are vary practical parallel models because they are affordable and not expensive. In this work, we proposed an efficient GPU-based implementation for Fuzzy C-Means algorithm. The functionality of the proposed parallel FCM has been verified and proven by conducting qualitative and quantitative evaluations. The empirical results show that the parallel FCM works precisely as the traditional sequential FCM. In addition, high performance and superlinear speedup of approximately 674 folds have been achieved compared with sequential FCM. \\\\\n\\indent In future, it would be interesting to explore the open questions mentioned in Section \\ref {sec:Performance} and find some answers by using FCM algorithm as a case study on several GPGPU devices. Recently, new CUDA devices have been released featured with the capability of launching dynamic parallel kernels. Generally speaking, dynamic kernels or (nested kernels) enables to multiple levels reduction concurrently. It would be also an interesting topic in the future to implement FCM on such powerful devices.    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{spbasic}      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem[Chattopadhyay(2011)]{Chattopadhyay}\nChattopadhyay S., Pratihar D.K., and Sarkar, S.C.D\n\\newblock {\\em A Comparative Study of Fuzzy C-Means Algorithm and Entropy-Based Fuzzy Clustering Algorithms}.\n\\newblock Computing and Informatics, Vol. 30, pp. 701-720, 2011\n\n\\bibitem[Tou(1974)]{Tou}\nTou, J.T. and Gonzalez R.C. \n\\newblock {\\em Pattern Recognition Principles}.\n\\newblock Addison-Wesley, London.1974\n\n\\bibitem[Bezdek(1981)]{Bezdek}\nBezdek J.C. \n\\newblock {\\em Pattern Recognition with Fuzzy Objective Function Algorithms}.\n\\newblock Kluwer Academic Publishers. 1981\n\n\\bibitem[Ball(1967)]{Ball}\nBall G. and Hall D. \n\\newblock {\\em A Clustering Technique for Summarizing Multivariate Data}.\n\\newblock Behav Sci 12, pp. 153-155, 1967\n\n\\bibitem[Shen(2009)]{Shen}\nShen Y.  and Li Y-L. \n\\newblock {\\em An automatic fuzzy c-means algorithm for image segmentation}.\n\\newblock Springer-Verlag, Vol. 14, Number 2, pp. 123-128. 2009\n\n\\bibitem[Vadiveloo(2011)]{Vadiveloo}\nVadiveloo M., Abdullah R., Rajeswari M. and Abu-Shareha A.A. \n\\newblock {\\em Image Segmentation With Cyclic Load Balanced Parallel Fuzzy C-Means Cluster Analysis}.\n\\newblock IEEE International Conference on Imaging Systems and Techniques, Malaysia, 2011, pp. 124-129. 2011\n\n\\bibitem[Farber(2012)]{Farber}\nFarber Rob,\n\\newblock {\\em CUDA Application Design and Development}.\n\\newblock Morgan Kaufmann Publishers Inc. ISPN 9780123884268, San Francisco, CA, USA, 1st edition, 2012.\n\n\\bibitem[Kirk et al.(2010)]{Kirk2010}\nDavid~B. Kirk and Wen-mei~W. Hwu.\n\\newblock {\\em Programming Massively Parallel Processors: A Hands-on Approach}.\n\\newblock Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition, 2010.\n\n\\bibitem[Li et al.(2014)]{Li2014}\nHaiyang Li, Zhaofeng Yang, Hongzhou He.\n\\newblock  {\\em An Improved Image Segmentation Algorithm Based on GPU Parallel Computing}. \n\\newblock Journal of Software, Vol 9, No 8 (2014), 1985-1990, Aug 2014.\n\n\\bibitem[Mahmoud et al.(2015)]{Mahmoud2015}\nMahmoud Al-Ayyoub, AnsamM Abu-Dalo, Yaser Jararweh, Moath Jarrah, Mohammad Al Sa'd,\n\\newblock  {\\em A GPU-based implementations of the fuzzy C-means algorithms for medical image segmentation}. \\url{http://dx.doi.org/10.1007/s11227-015-1431-y}.\n\\newblock The Journal of Supercomputing, DOI 10.1007/S11227-015-1431-Y, ISSN 0920-8542, Pages: 1-14, Springer US, 2015.\n\n\\bibitem[Eschrich et al.(2015)]{Eschrich2003}\nEschrich S., Jingwei Ke, Hall L.O., Goldgof D.B.,\n\\newblock  {\\em Fast accurate fuzzy clustering through data reduction}. \\url{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1192702&isnumber=26743}.\n\\newblock In Fuzzy Systems, IEEE Transactions on Vol.11, No.2, Pages: 262-270, DOI 10.1109/TFUZZ.2003.809902, Apr 2003.\n\n\\bibitem[Shalom et al.(2008)]{Shalom2008}\nShalom, S.A.A., Dash, M. and Tue, M.\n\\newblock  {\\em Graphics hardware based efficient and scalable fuzzy c-means clustering}. \\url{http://dx.doi.org/10.1007/s11227-015-1431-y}.\n\\newblock In Proceedings of the 7th Australasian data mining conference, volume 87, Australian Computer Society Inc, Darlinghurst, Australia, AusDM 08, pp 179\u00e2\u0080\u0093186. 2008.\n\n\\bibitem[Rowi et al.(2012)]{Rowi2012}\nRowi \u00cc\u0081 ska Z, Goc\u00c5\u0082awski J \n\\newblock  {\\em Cuda based fuzzy c-means acceleration for the segmentation of n images with fungus grown in foam matrices}. \n\\newblock Image Processing and Communications 17(4):191\u00e2\u0080\u0093200. DOI:10.2478/ V10248-012-0046-7, 2012.\n\n\\bibitem[Bernstein(1966)]{Bernstein}\nBernstein A. J.\n\\newblock {\\em Analysis of Programs for Parallel Processing}.\n\\newblock Electronic Computers, IEEE Transactions on , VOL EC-15, NO.5, Pages:757-763, Oct. 1966. \n\n\\bibitem[reduction3(2014)]{2014r3}\n{\\em CUDA Array Sum with Reduction}, \\url{https://gist.github.com/wh5a/4424992}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[reduction4(2014)]{2014r6}\n{\\em CUDA Optimization}, \\url{http://sbel.wisc.edu/Courses/ME964/2012/Lectures/lecture0313.pdf}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[reduction1(2014)]{2014r1}\n{\\em Faster Parallel Reductions on Kepler}, \\url{http://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[reduction2(2014)]{2014r2}\n{\\em Optimizing Parallel Reduction in CUDA}, \\url{http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86\\_website/projects/reduction/doc/reduction.pdf}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[shared1(2014)]{2014r4}\n{\\em GPU Performance Analysis and Optimization}, \\url{http://on-demand.gputechconf.com/gtc/2012/presentations/S0514-GTC2012-GPU-Performance-Analysis.pdf}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[shared2(2014)]{2014r5}\n{\\em Using Shared Memory in CUDA C/C++}, \\url{http://devblogs.nvidia.com/parallelforall/using-shared-memory-cuda-cc/}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[javacode(2014)]{java}\n{\\em Java Image Processing Cookbook}, \\url{http://www.lac.inpe.br/JIPCookbook/Code/JAIStuff/algorithms/fuzzycmeans/FuzzyCMeansImageClustering.java.txt}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[CUDA2(2014)]{2014c}\n{\\em {H}ow to Access Global Memory Efficiently in CUDA C/C++ Kernels}, \\sloppy{http://devblogs.nvidia.com/parallelforall/how-access-global-memory-efficiently-cuda-c-kernels}.\n\\newblock {R}etrieved 03 Mar 2015.\n\n\\bibitem[Collins(1998)]{Collins}\nD.L. Collins, A.P. Zijdenbos, V. Kollokian, J.G. Sled, N.J. Kabani, C.J. Holmes and A.C. Evans\n\\newblock {\\em Design and Construction of a Realistic Digital Brain Phantom}.\n\\newblock  IEEE Transactions on Medical Imaging, Vol. 17, No. 3, pp. 463-468, (1998)\n\n\\bibitem[Dogdas(2005)]{Dogdas}\nDogdas B., Shattuck D.W., and Leahy R.M. \n\\newblock {\\em Segmentation of Skull and Scalp in 3D Human MRI Using Mathematical Morphology Human Brain Mapping}.\n\\newblock Vol. 26, No.4, pp.273-285. 2005\n\n\\bibitem[Zijdenbos(1994)]{Zijdenbos}\nZijdenbos A.P., Dawant B.M. Margolin R.A. and Palmer A.C. \n\\newblock {\\em Morphometric analysis of white matter lesions in MR images: Method and validation}.\n\\newblock IEEE Transactions on Medical Imaging, 13(4), pp.716. 1994\n\n\n\n\n\n\\bibitem[Fayez(2011)]{Fayez}\nGebali Fayez,\n\\newblock {\\em Algorithms and Parallel Computing}.\n\\newblock Wiley Publishing, ISPN 0470902108, 1st edition, 2011.\n\n\\bibitem[Ananth(2003)]{Ananth}\nAnanth Grama, Anshul Gupta, George Karypis, and Vipin Kumar,\n\\newblock {\\em Introduction to Parallel Computing}.\n\\newblock Addison-Wesley, ISPN 0201648652, 2nd edition, 2003.\n\n\\bibitem[reduction4(2014)]{2014r7}\n{\\em TESLA\u00e2\u0084\u00a2 C2050 / C2070 GPU Computing Processor }, \\url{http://www.nvidia.com/docs/IO/43395/NV_DS_Tesla_C2050_C2070_jul10_lores.pdf}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[reduction4(2014)]{2014r8}\n{\\em Intel\u00c2\u00ae Core i5-400 Processor Series}, \\url{http://download.intel.com/support/processors/corei5/sb/core_i5-400.pdf}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\bibitem[Soroosh4(2014)]{2014Soroosh}\n{\\em Soroosh 129/GPU-FCM}, \\url{https://github.com/Soroosh129/GPU-FCM}.\n\\newblock {R}etrieved 10 May 2015.\n\n\\end{thebibliography}\n\n\n\n    \n", "itemtype": "equation", "pos": 25422, "prevtext": "\n\n\\indent Starting with random initialization of the membership values for each pixel from the manually selected clusters, the clusters are converged by recursively updating the cluster centers and membership function in \\ref{eq113} and \\ref{eq114}. This is to minimize the objective function in \\ref{eq1111}. Convergence stops when the overall difference in the membership function between the current and previous iteration is smaller than a given epsilon value, $\\varepsilon$. After the convergence, deffuzzifaction is applied. Each pixel is assigned to a specific cluster according to the maximal value of its membership function. The steps of the Fuzzy C-Means algorithm are illustrated in Algorithm \\ref{algo1C}. \n\n\n{\\LinesNumberedHidden\n\\begin{algorithm}\n\\DontPrintSemicolon \n\\textbf{Assumptions: Image is transformed into feature space.}\\\\\n\nStep 1: Initialize the number of clusters $c$,  $m =2$, and $\\varepsilon = 0.005$\\\\\nStep 2: Initialize the membership function, $u_{ij}$  randomly.\\\\\nStep 3: \\Repeat {$||u^{k+1}_{ij}-u^{k}_{ij}|| < \\varepsilon $ } {\n Update the cluster center, $v_i$ using Equation \\ref{eq113}\\\\\n Update the membership function $u_{ij}$ using Equation \\ref{eq114}\\\\\n  }\n\\caption{{\\sc Fuzzy C-Means algorithm} }\n\\label{algo1C}\n\\end{algorithm}}\n\n\n\n\\subsection{Data Parallelism on GPGPU}\n\\label{sec:gpu}\nInitially, GPU was a hardware equipped with a processor specifically designed to accelerate graphic processing. Eventually, GPU applications were extended to general-purpose computations. At present, GPGPU is used in many applications typically performed using a CPU, such as analytic, engineering, and scientific applications \\cite{Farber}. With the release of the massively parallel architecture called CUDA in 2007 from NVIDIA, GPUs have become widely accessible \\cite{Kirk2010}.\\\\\n\\indent A GPU is a processor or a multiprocessor device that has hundreds or even thousands of cores called scalar processors (SPs), which are arranged in groups named streaming multiprocessors (SMs), as shown in the left side of Fig. \\ref{executionmodel}. Moreover, GPUs have different kinds of memories: global, local, texture, constant, shared, and register memories. Global, constant, and texture memories are accessible to all threads in the grid. Shared memory is visible to threads within one CUDA block. It is faster than the global memory but is limited by size. Register memory is visible to the thread that initialized the said memory and lasts for the lifetime of that thread.\\\\\n\\indent CUDA is the parallel programming model used for NVIDIA GPGPUs. CUDA can increase the performance by harnessing the power of a GPU device. Thousands of threads can be executed concurrently using CUDA on GPGPU. The execution model of CUDA on NVIDIA devices is shown in Fig. \\ref{executionmodel}.\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.55]{executionmodel.png}\n\\caption{CUDA Execution Model on GPGPU}\n\\label{executionmodel}\n\\end{center}\n\\end{figure}\n\n\n\\section{Related works}\n\\label{sec:Related works}\nLi et al. proposed an Fuzzy C-Means (FCM) algorithm based on GPU \\cite{Li2014}. They modified the sequential FCM algorithm, such that the calculations of the membership and cluster center matrices are not comparable to the sequential one. They have FCM on GPU using CUDA. The empirical results obtained by Li et al. showed that the proposed parallel FCM on GPU is more efficient than the sequential FCM. Instead of efficiency, they claimed that the proposed method exhibits improvement in the quality of the GPU segmented image. The authors achieved a 10-fold speedup with the proposed parallel FCM on NVIDIA GTX 260 device compared with the sequential FCM for natural images sized from 53kb to 101kb.\\\\\n\\indent Mahmoud et al. presented a GPU-based brFCM for medical images segmentation \\cite{Mahmoud2015}. The brFCM is a faster variant of the sequential FCM \\cite{Eschrich2003}. The GPU-based brFCM is implemented on different GPGPU cards. Mahmoud et al  showed that the GPU-based brFCM  has a significant improvement over the parallel FCM in \\cite{2014Soroosh}. The achieved speedup is up tp 23.42 fold faster than parallel FCM in \\cite{2014Soroosh} for medical images of 350x350 and 512x512 dimensions.\\\\\n\\indent Shalom et al. proposed a scalable FCM based on graphic hardware \\cite{Shalom2008}. On two different graphic cards, the results show that the proposed GPU-based FCM algorithm is more efficient and faster than the sequential FCM. The authors succeeded in reaching a 73-fold speedup on NVIDIA GeForce 8500 GT. Amazingly, a 140-fold speedup was achieved on NVIDIA GeForce 8800 GTX compared with sequential FCM for 65k yeast gene expression data set of 79 dimension.\\\\\n\\indent Rowinska and Goclawski proposed a CUDA-based FCM algorithm to accelerate image-segmentation \\cite{Rowi2012}. The proposed method has been tested on polyurethane foam with fungus color images and was compared with the sequential FCM implemented using C++ and MATLAB. The authors achieved a 10-fold speedup of their parallel proposal compared with the FCM implemented in C++ for object area of 310k pixels, and a 50- to 100-fold speedup compared with the FCM implemented in MATLAB for object area of 260k pixels. A comparison of our work and the previous related works is summarized in Table \\ref{relatedworks}. \n\n\\begin{table*}[t]\n\\centering\n\\caption{Comparison of our work and previous related works} \n\\begin{tabular}{c|L|L|c}\n\\textbf{Work by}& \\textbf{Method} & \\textbf{Image dataset} & \\textbf{Speedup}\\\\\\hline \n\n$ \\begin{matrix} \\text{Li et al.} \\\\ \\text{\\cite{Li2014}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Modified the original FCM algorithm and then parallelized it on GPGPU}&  \\multicolumn{1}{m{3cm}|}{Natural images (from 53kB to 101kB)} & 10x  \\\\\\hline\n$ \\begin{matrix} \\text{Mahmoud et al.} \\\\ \\text{\\cite{Mahmoud2015}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Parallelized brFCM the variant of FCM algorithm on GPGPU}&  \\multicolumn{1}{m{3cm}|}{Medical images (Lung CT with the dimesion of 512x512; Knee MRI with the dimension of 350x350)} & \\makecell{23x faster\\\\ than in \\cite{2014Soroosh} } \\\\\\hline\n$ \\begin{matrix} \\text{Shalom et al } \\\\ \\text{\\cite{Shalom2008}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Proposed a scalable FCM GPU-based implementation}&  \\multicolumn{1}{m{3cm}|}{Yeast gene expression data set (79 dimension with 65K genes)} & 140x  \\\\\\hline\n$ \\begin{matrix} \\text{Rowinska et al.} \\\\ \\text{\\cite{Rowi2012}}\\end{matrix}$ & \\multicolumn{1}{m{3cm}|}{Presented a CUDA-based FCM algorithm to accelerate image segmentation}&  \\multicolumn{1}{m{3cm}|}{Polyurethane foam with fungus color images (object area of 310k pixels)} & 10x  \\\\\\hline\nThis paper & \\multicolumn{1}{m{3cm}|}{A parallel FCM approach on GPGPU using CUDA  }&  \\multicolumn{1}{m{3cm}|}{Digital brain phantom simulated dataset (from 20kB to 1000kB)} & $ \\begin{matrix} \\text{Superlinear} \\\\ \\text{speedup} \\\\ \\text{up to 674x} \\end{matrix}$    \n\\end{tabular}\n\\label{relatedworks} \n\\end{table*}\n\n\\section{The Proposed Method}\n\\label{sec:FCM Parallel Approach}\nThe sequential FCM algorithm has been subjected to extensive analysis in order to find out where the algorithm exhibits parallelism that we might exploit in the parallel design. The strongest data dependency in the FCM algorithm is the steps where the total summation calculation is required, as illustrated in step 3 in the sequential FCM (Algorithm \\ref{algo1C}). For instance, two sigma operations are needed to calculate the cluster centers as shown in Equation \\ref{eq113}. Such a strong dependency makes parallelizing the sequential algorithm infeasible. According to Bernstein\u00e2\u0080\u0099s conditions \\cite{Bernstein}, this type of dependency is called output dependence. In parallel computing, the reduction method is an efficient approach to remove output dependence.\\\\\n\\indent The proposed parallel FCM design consists of two main parts: a sequential part executed on the CPU (host) and a parallel part executed on the GPU (device). Fig. \\ref{block} shows the block diagram of the proposed work. The following sub-sections discuss each stages of the block diagram in Fig. \\ref{block}. \n\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[scale=0.71]{BlockDiagram1.png}\n\\caption{The Block Diagram of The Proposed Parallel Fuzzy C-Means}\n\\label{block}\n\\end{center}\n\\end{figure*}\n\n\\subsection{Initialization and data transferring}\n\\label{sec:Initialization}\nAs shown in Fig. \\ref{block}, the first two steps are executed sequentially on the host. The membership is randomly initialized. The memories are allocated on the device global memory for the pixels of the image data, membership, and cluster centers. All the arrays are defined in a 1-D pattern.\\\\\n\\indent After defining memories on the device, all the data are transferred from host to device, and then the main program loop is started. Subsequently, the parallel kernels are called concurrently to manipulate the image pixels on the device.\n\n\\subsection{Calculating cluster centers from membership functions}\n\\label{sec:Calculating cluster centers}\nThe host calls four CUDA kernels one after another to calculate the cluster centers from memberships. The first CUDA kernel concurrently handles the heavy calculations, such as exponential, division, and multiplication of floating points for every pixel. At this step, the final summation is not included. The numerator and denominator of Equation \\ref{eq113} are calculated separately for every pixel, and the results are stored in two different arrays in the device global memory. The number of spawned CUDA threads in this kernel is defined to be equal to the number of image pixels, such that every thread will handle one pixel.      \n\n\\indent The second CUDA kernel at this phase is the reduction kernel, which computes the partial summation of the numerator of Equation \\ref{eq113}. The reduction technique is an efficient method to break down the dependency among the data. The computation complexity of the sequential addition of $n$ elements is $O(n)$. However, using parallel computing can significantly improve the computation complexity to $O(log~n)$ \\cite{2014r3}\\cite{2014r6}. Several CUDA reduction methods are available, such as in \\cite{2014r1}\\cite{2014r2}\\cite{2014r3}. The CUDA reduction method used in this work is similar to \\cite{2014r3} and is shown in Algorithm \\ref{algo1}. First, a segment of the input is loaded into the device-shared memory. This device shared memory can facilitate fast access to the image pixels \\cite{2014r4}\\cite{2014r5}. The reduction process is then performed over the shared memory. Each calculated partial sum of every segment stored in the shared memory is loaded to the output in the global memory. As illustrated in Algorithm \\ref{algo1}, the CUDA block ID (blockIdx.x) is used as an index to store the partial sum from the device-shared memory to the global memory. Fig. \\ref{reduction} demonstrates the reduction process performed on GPGPU using shared memory.\n\n\\begin{algorithm*}\n\\DontPrintSemicolon \n\\KwIn{A large set $A=\\{a_1, a_2, \\ldots, a_n\\}$ where $n = pixels$}\n\\KwOut{A reduced small set $B=\\{b_1, b_2, \\ldots, b_m\\}$ where $m = n / blockDim<<1$}\n$global\\_idx \\gets blockIdx.x*blockDim.x + threadIdx.x$\\;\n$local\\_idx \\gets threadIdx.x$\\; \n$start \\gets  2*blockIdx.x*blockDim.x$\\;\n\\_\\_shared\\_\\_  $partialSum[2*MAX\\_THREAD]$\\;\n\\textbf{//Loading segment from the input into the shared memory:}\\;\n\\If{$(start + local\\_idx) < n$}\n    {\n        $partialSum[local\\_idx] = A[start + local\\_idx]$\\;      \n    }\n\\Else\n    {       \n        $partialSum[local\\_idx] = 0.0$\\;\n    }\n\\If{$(start + local\\_idx+ blockDim.x) < n$}\n    {\n        $partialSum[local\\_idx + blockDim.x] = A[start + local\\_idx + blockDim.x]$\\;      \n    }\n\\Else\n    {       \n        $partialSum[local\\_idx + blockDim.x] = 0.0$\\;\n    }\n\\textbf{//Reduction over the device shared memory:}\\;\n\\For{$stride \\gets blockDim.x$ \\textbf{to} $0$ ; $stride /= 2$} {\n  \\If{$local\\_idx < stride$} {\n    $partialSum[local\\_idx] += partialSum[local\\_idx + stride]$\\;\n  }\n}\n\\textbf{//Storing the output into the device global memory:}\\;\n\\If{$local\\_idx == 0 \\&\\& (global\\_idx*2) < n$} {\n    $B[blockIdx.x] = partialSum[local\\_idx]$\\;\n}\n\\caption{{\\sc Sum Reduction on GPGPU Using CUDA} }\n\\label{algo1}\n\\end{algorithm*}\n\nThe actual reduction for the illustrated example in Fig. \\ref{reduction}, reduces the addition operations from adding 16 elements to only 2 elements. Another example from the conducted experiments of this work is an image with a size of 1 MB (1048576 bytes) that was reduced to $(1048576/128<<1)$, which equals 4 KB (4096 bytes).\\\\\n\\indent The third kernel to be called in this phase (calculating cluster centers from the membership function phase) is another reduction kernel that calculates the partial sum of the denominator of Equation \\ref{eq113}. Finally, the last CUDA kernel calculates both final summations from the previous two kernels and computes the final result. Only one thread is defined for this kernel. The reason for this one thread kernel is that instead of transferring the reduced arrays from the previous kernels to the host memory to calculate the final summations, in this proposed method the device is allowed to carry out the final summation only with one thread. Lastly, all the previous four CUDA kernels are called in iterative loops that are equal to the predefined number of clusters. This is to calculate the cluster centers from the membership functions as shown in Fig. \\ref{block}. \n\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.35]{reduction.png}\n\\caption{Sum Reduction Example on GPGPU. There are four CUDA block dimension in this example.}\n\\label{reduction}\n\\end{center}\n\\end{figure}\n   \n\\subsection{Calculating membership functions from cluster centers}\n\\label{sec:Calculating Mfs}\nOnly one CUDA kernel is defined to compute membership functions from the cluster centers. Rather than defining CUDA threads and block dimensions, the implementation in this kernel is quite similar to the sequential algorithm. The spawned CUDA threads are defined equally to the image pixels, which implies fine-grained granularity. Thus, one thread will handle one pixel. In correspondence to the previous phase of the proposed work \\ref{block}, this kernel will be called in an iterative loop equally to the predefined number of clusters. At this stage, the computed new membership function arrays will be transferred to the host. The host will determine if the new membership function satisfies the condition as shown in Fig. \\ref{block}. If the condition is satisfied, finally the cluster center arrays will be transferred back to the host. Defuzzification is performed and the the final segmented image is obtained. \n\n\n\\section{Implementation and Results}\n\\label{sec:results}\nIn this section, the implementation design of the proposed method is introduced in the first subsection. The functionality of the proposed method is proven using both qualitative and quantitative evaluations in the next subsections. The performance analysis is discussed in the final subsection.\n\n\\subsection{Implementation}\n\\label{sec:implementation}\nThe proposed method was implemented using C language and CUDA. First, the sequential FCM algorithm was implemented in C. Our sequential C version was derived from a Java version available online at \\cite{java}. The sequential FCM in C was tested on Intel Core i5-480 CPU, Windows 7 Ultimate platform.\\\\\n\\indent In the proposed parallel FCM, the image pixels, memberships, and cluster center arrays are defined in a 1-D pattern. The reason is to ensure coalesced memory transactions in the GPGPU. In addition, defining those input arrays in 1-D pattern will ease the number of CUDA block and grid sizes calculations. The CUDA block and grid sizes are consequently defined in 1-D patterns corresponding to the input arrays. Therefore, the form of the input has a significant effect on the performance of CUDA kernels because of the coalescing access \\cite{Kirk2010}\\cite{2014c}. Figure \\ref{arrays} illustrates examples on the indices of arrays are modified when converting multi-dimensional arrays to 1-D arrays. In this work, the image array was converted from 2-D to 1-D, and the membership array was converted from 3-D to 1-D. The details of the parallel platform used in this experiment are shown in Table \\ref{platform}.\n\n\\begin{figure}[H]\n\\begin{center}\n\\includegraphics[scale=0.35]{arrays.png}\n\\caption{Converting Multidimensional Arrays to One Dimensional}\n\\label{arrays}\n\\end{center}\n\\end{figure}\n\n\\begin{table}[H]\n\\caption{Platform of the experiments} \n\\centering  \n\n\\setlength{\\tabcolsep}{5pt} \n\\begin{tabular}{ l l } \n\n\n\\hline                  \n \n    \\textbf{CPU:}  & AMD Phenom(tm) II X4 810 Processor.  \\\\\n    \\textbf{Kernel:} & Linux  x86\\_64 GNU.  \\\\\n    \\textbf{GPU:} & NVIDIA Tesla C2050.  \\\\\n    \\textbf{CUDA:} & CUDA compilation tools, release 5.0. \\\\\n \n\n\\hline\n\\end{tabular}\n\\label{platform} \n\\end{table}\n\n\\subsection{Functionality Evaluation}\n\\label{sec:Functionality}\nThe proposed GPGPU-based FCM is tested on digital brain phantom simulated dataset from the Brain Web MR Simulator \\cite{Collins} with the size of 20kB to segment white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF) soft tissues regions. Skull stripping \\cite{Dogdas} has been carried out on the brain phantom images to remove skull and other non-brain soft tissues, so that only brain soft tissues are used in the proposed parallel Fuzzy C-Means (FCM) segmentation process. When applying the proposed FCM on the brain soft tissues, four clusters are manually selected to represent the WM, GM, and CSF soft tissues regions and the final cluster represent the background region. Therefore in the proposed parallel FCM, there are four cluster center values being associated with the aforementioned regions. The functionality of the proposed method is then proven using both qualitative and quantitative evaluations in the following subsections. \n\n\\subsubsection{Qualitative evaluation}\n\\label{sec:Qualitative}\nThe qualitative evaluation is performed for both the segmented results of the proposed parallel FCM and the sequential FCM. This is to evaluate the similarity of the segmented result of the proposed parallel FCM with the segmented result produced by the sequential FCM, visually. In Fig. \\ref{qual}, the experiment results are presented. It can be seen that the result of the proposed method is identical to the result of the sequential FCM. \n\n\\begin{figure}[it]\n\\begin{center}\n\\includegraphics[scale=0.35]{qual.png}\n\\caption{Representative Results of The 101\\textsuperscript{st}, 91\\textsuperscript{st} and 96\\textsuperscript{th} Axial Slice of Brain Tissue Phantom Using Sequential Fuzzy C-Means and The Proposed GPGPU-Based Fuzzy C-Means.}\n\\label{qual}\n\\end{center}\n\\end{figure}\n\n\\subsubsection{Quantitative evaluation}\n\\label{sec:Quantitative}\nThe quantitative evaluation is used to compare the results of the proposed parallel Fuzzy C- FCM and sequential FCM. Evaluation metrics such as Dice Coefficient Similarity (DSC) \\cite{Zijdenbos} and performance analysis are used. DSC is used to evaluate if the accuracy of the segmented results of the proposed method is statistically similar to the segmented results of the sequential FCM based on the ground truth. While performance analysis is to compare the execution time and speed up of the proposed method with the sequential FCM. DSC is defined as in Equation \\ref{eq116}.\n\n\n", "index": 9, "text": "\\begin{equation}\\label{eq116}\n\\large DSC=\\frac{2(PR \\cap GT)}{PR+GT}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\large DSC=\\frac{2(PR\\cap GT)}{PR+GT}\" display=\"block\"><mrow><mrow><mi mathsize=\"120%\">D</mi><mo>\u2062</mo><mi mathsize=\"120%\">S</mi><mo>\u2062</mo><mi mathsize=\"120%\">C</mi></mrow><mo mathsize=\"120%\" stretchy=\"false\">=</mo><mfrac><mrow><mn mathsize=\"120%\">2</mn><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><mi mathsize=\"120%\">P</mi><mo>\u2062</mo><mi mathsize=\"120%\">R</mi></mrow><mo mathsize=\"120%\" stretchy=\"false\">\u2229</mo><mrow><mi mathsize=\"120%\">G</mi><mo>\u2062</mo><mi mathsize=\"120%\">T</mi></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mrow><mrow><mi mathsize=\"120%\">P</mi><mo>\u2062</mo><mi mathsize=\"120%\">R</mi></mrow><mo mathsize=\"120%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"120%\">G</mi><mo>\u2062</mo><mi mathsize=\"120%\">T</mi></mrow></mrow></mfrac></mrow></math>", "type": "latex"}]