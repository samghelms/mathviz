[{"file": "1601.02093.tex", "nexttext": "\nwith corresponding global image descriptors obtained by simply concatenating the moments for the individual features:\n\n", "itemtype": "equation", "pos": 14671, "prevtext": "\n\n\\begin{titlepage}\n\\includepdf{cbmm_cover}\n\\end{titlepage}\n\n\\title{Group Invariant Deep Representations for Image Instance Retrieval}\n\n\\author{Olivier Mor\\`{e}re{$^{*,1,2}$}, Antoine Veillard{$^{*,1}$}, Jie Lin{$^{2}$}\\\\\nJulie Petta{$^{3}$, Vijay Chandrasekhar{$^{2}$}, Tomaso Poggio{$^{4}$}}\n\\thanks{{$^{*}$} O. Mor\\`ere and A. Veillard contributed equally to this work.}\n\\thanks{{$^{1}$} Universit\\'e Pierre et Marie Curie}\n\\thanks{{$^{2}$} A*STAR Institute for Infocomm Research}\n\\thanks{{$^{3}$} CentraleSup\\'{e}lec}\n\\thanks{{$^{4}$} CBMM, LCSL, IIT and MIT}\n}\n\n\\maketitle\n\n\\begin{abstract}\n\nMost image instance retrieval pipelines are based on comparison of vectors known as global image descriptors between a query image and the database images.\nDue to their success in large scale image classification, representations extracted from Convolutional Neural Networks (CNN) are quickly gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors for image instance retrieval.\nWhile CNN-based descriptors are generally remarked for good retrieval performance at lower bitrates, they nevertheless present a number of drawbacks including the lack of robustness to common object transformations such as rotations compared with their interest point based FV counterparts.\n\nIn this paper, we propose a method for computing invariant global descriptors from CNNs.\nOur method implements a recently proposed mathematical theory for invariance in a sensory cortex modeled as a feedforward neural network.\nThe resulting global descriptors can be made invariant to multiple arbitrary transformation groups while retaining good discriminativeness.\n\nBased on a thorough empirical evaluation using several publicly available datasets, we show that our method is able to significantly and consistently improve retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to overfitting: improvements generalize well across datasets with different properties with regard to invariances.\nFinally, we show that our descriptors are able to compare favourably to other state-of-the-art compact descriptors in similar bitranges, exceeding the highest retrieval results reported in the literature on some datasets.\nA dedicated dimensionality reduction step --quantization or hashing-- may be able to further improve the competitiveness of the descriptors.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nInvariance theory, group invariance, deep learning, convolutional neural networks, image instance retrieval, global image descriptors, compact descriptors.\n\\end{IEEEkeywords}\n\n\\section{Intoduction}\n\\label{sec:introduction}\n\nImage instance retrieval is the discovery of images from a database representing the same object or scene as the one depicted in a query image.\nThe first step of a typical retrieval pipeline starts with the comparison of vectors representing the image contents known as {\\it global image descriptors}.\nGood quality descriptors is key to achieving good retrieval performances.\n\nDeep learning neural networks are fast becoming the dominant approach for image classification due to their remarkable performance for large scale image classification~\\cite{AlexNet,Simonyan2014}. \nIn their recent work, Babenko et al.~\\cite{Yandex} propose using representations extracted from Convolutional Neural Nets (CNN) as a global descriptor for image retrieval, and show promising initial results for the approach.\nIn our recent work~\\cite{CompactGlobal, unsupervisedtriplethashing}, we also show how stacked Restricted Boltzmann Machines (RBM) and supervised fine-tuning can be used for generating extremely compact hashes from global descriptors obtained from CNNs for large scale image-retrieval.\n\nWhile CNN based descriptors are progressively replacing Fisher Vectors (FV)~\\cite{Perronnin_CVPR_10} as state-of-the-art descriptors for image instance retrieval, we have shown in our recent work thoroughly comparing both types of descriptors~\\cite{practicalguide2015} that the use of CNNs still presents a number of significant drawbacks compared with FVs.\nOne of them is the lack of invariance to transformations of the input image such as rotations: the performance of CNN descriptors quickly degrade when the objects in the query and the database image are rotated differently.\n\nIn this paper, we propose a method to produce global image descriptors from CNNs which are both compact and robust to such transformations.\nOur method is inspired from a recent invariance theory (subsequently referred to as \\emph{i-theory}) for information processing in sensory cortex~\\cite{itheory1, itheory2, itheory3}.\nThe theory is an information processing model explaining how feedforward information processing in a sensory cortex can be made robust to various types of signal distorsions.\nIn particular, it provides a practical and mathematically proven way for computing invariant object representations with feedforward neural networks.\n\nAfter showing that CNNs are compatible with the \\emph{i-theory}, we propose a simple and practical way to apply the theory to the construction of global image descriptors which are robust to various types of transformations of the input image at the same time.\nThrough a thorough empirical evaluation based on multiple publicly available datasets, we show that our method is able to significantly consistently improve retrieval results while keeping dimensionality low.\nRotations, translations and scale changes are studied in the scope of this paper but our approach is extensible to other types of transformations.\n\n\\begin{figure*}\n\\includegraphics[width=\\textwidth]{./figures/cbmm_fig123}\n\\caption{(a) A single convolution-pooling operation from a CNN schematized for a single input layer and single output neuron.\nThe parallel with \\emph{i-theory} shows that the universal building block of CNNs is compatible with the incorporation of invariance to local translations of the input according to the theory.\nThe network architecture is responsible for the invariance properties while back-propagation provides a practical way to learn the templates from data.\n(b) A specific succession of convolution and pooling operations learnt by the CNN (depicted in red) computes the \\emph{pool5} feature $f_i$ for each feature map $i$ from the RGB image data.\nA number of transformations $g$ can be applied to the input $x$ in order to vary the response $f_i(g.x)$.\n(c) Our method takes inspiration from the \\emph{i-theory} to create compact and robust global image descriptors from CNNs.  \nStarting with raw \\emph{pool5} descriptors, it can be used to stack-up an arbitrary number of transformation group invariances while keeping the dimensionality under control.\nThe particular sequence of transformation groups and statistical moments represented on the diagram was shown to produce the best performing hashes on average in our study but other arbitrary combinations are also able to improve retrieval results.\n}\n\\label{fig:method}\n\\end{figure*}\n\n\\section{Related Work}\n\\label{sec:related}\n\nSince the winning submission of Krizhevsky et al. in the ImageNet 2012 challenge~\\cite{AlexNet}, deep CNNs are now considered as the mainstream go-to approach for large-scale image classification.\nThey are also known to achieve state-of-the-art results with many other visual recognition tasks such as face recognition~\\cite{deepface,deepid}, pedestrian detection~\\cite{deeppedestrian} and pose estimation~\\cite{deeppose}.\n\nCNN began to be applied to the instance retrieval problem as well, although there is comparatively less work on CNN-based descriptors for instance retrieval compared to large-scale image classification.\nRazavian et al.~\\cite{CNNOffTheShelf} evaluate the performance of CNN model of~\\cite{AlexNet} on a wide range of tasks including instance retrieval, and show initial promising results.\nBabenko et al.~\\cite{Yandex} show that a pre-trained CNN can be fine tuned with domain specific data (objects, scenes, etc.) to improve retrieval performance on relevant data sets.\nThe authors also show that the CNN representations can be compressed more effectively than their FV counterparts for large-scale instance retrieval.\nIn~\\cite{CompactGlobal, unsupervisedtriplethashing}, we show how sparse high-dimensional CNN representations can be hashed to very compact representations (32-1024 bits) for large scale image retrieval with little loss in matching performance.\n\n\nMeanwhile, we showed in our recent evaluation work~\\cite{practicalguide2015} that CNN-based descriptors still suffer from a number of drawbacks including the lack of robustness to certain transformations of the input data.\nThe \\emph{i-theory} proposed as the guideline method in this paper was recently used to successfully compute robust representations for face recognition \\cite{poggioface} and music classification \\cite{poggiomusic}.\n\nThe contributions of our work can be summarized as follows.\n\\begin{itemize}\n\\item A method based on \\emph{i-theory} for creating robust and compact global image descriptors from CNNs.\n\\item The ability to iteratively incorporate different group invariances, each new addition leading to consistent and significant improvements in retrieval results.\n\\item A set of highly competitive global descriptors for image instance retrieval compared with other state-of-the-art compact descriptors at similar bitrates.\n\\item A low risk of overfitting: few parameters and many reasonable settings which can generalize well across all datasets.\n\\item A thorough empirical study based on several publicly available datasets.\n\\end{itemize}\n\n\n\\section{Invariant Global Image Descriptors}\n\\label{sec:method}\n\n\\subsection{I-theory in an Nutshell}\n\\label{sec:method1}\n\nMany common classes of image transformations such as rotations, translations and scale changes can be modeled by the action of a group $G$ over the set $E$ of images.\nLet $x \\in E$ and a group $G$ of transformations acting over $E$ with group action $G \\times E \\rightarrow E$ denoted with a dot ($.$).\nThe orbit of $x$ by $G$ is the subset of $E$ defined as $O_x = \\{ g.x \\in E | g \\in G \\}$.\nIt can be easily shown that $O_x$ is globally invariant to the action of any element of $G$ and thus any descriptor computed directly from $O_x$ would be globally invariant to $G$.\n\nThe \\emph{i-theory} predicts that an invariant descriptor for a given object $x \\in E$ is computed in relation with a predefined template $t \\in E$ from the distribution of the dot products $D_{x,t} = \\{< g.x , t >  \\in \\mathbb{R} | g \\in G \\} = \\{< x , g.t > \\in \\mathbb{R} | g \\in G \\}$ over the orbit. \nOne may note that the transformation can be applied either on the image or the template indifferently.\nThe proposed invariant descriptor extracted from the pipeline should be a histogram representation of the distribution with a specific bin configuration.\nSuch a representation is mathematically proven to have proper invariance and selectivity properties provided that the group is compact or at least locally compact \\cite{itheory1}.\n\nIn practice, while a compact group (e.g. rotations) or locally-compact group (e.g. translations, scale changes) is required for the theory to be mathematically provable, the authos of \\cite{itheory1} suggest that the theory extends well (with approximate invariance) to non-locally compact groups and even to continuous non-group transformations (e.g. out-of-plane rotations, elastic deformations) provided that proper class-specific templates can be provided.\nRecent work on face verification \\cite{poggioface} and music classification \\cite{poggiomusic} apply the theory to non-compact groups with good results.\nAdditionally, the histograms can also be effectively replaced by statistical moments (e.g. mean, min, max, standard deviation, etc.).\n\n\\subsection{CNNs are i-theory Compliant Networks}\n\\label{sec:method2}\n\nAll popular CNN architectures designed for image classification such as \\emph{AlexNet}~\\cite{AlexNet} and \\emph{OxfordNet}~\\cite{Simonyan2014} share a common building block: a succession of convolution-pooling operations designed to model increasingly high-level visual representations of the data.\nThe highest level visual features may then be fed into fully connected layers acting as a classifiers.\n\nAs shown in detail on Figure~\\ref{fig:method}~(a), the succession of convolution and pooling operations in a typical CNN is in fact a way to incorporate local translation invariance strictly compliant with the framework proposed by the \\emph{i-theory}.\nThe network architecture provides the robustness such as predicted by the invariance theory while training via back propagation ensures a proper choice of templates.\nIn general, multiple convolution-pooling steps are applied (5 times in both \\emph{AlexNet} and \\emph{OxfordNet}) resulting in increased robustness and higher level templates.\nNote that the iterative composition of local translation invariance approximately translates into robustness to local elastic distortions for the features at the \\emph{pool5} layer.\n\nIn this study, instead of the popular first fully-connected layer (\\emph{fc6}) which is on average the best single CNN layer to use as a global out-of-the-box descriptor for image retrieval \\cite{practicalguide2015}, we decide to use the locally invariant \\emph{pool5} as a starting representation for our own global descriptors and further enhance their robustness to selected transformation groups in a way inspired from \\emph{i-theory}.\n\n\\subsection{Transformation Invariant CNN Descriptors}\n\\label{sec:method3}\n\nFor every feature map $i$ of the \\emph{pool5} layer ($0 \\leq i < 512$ in the case of the presently used {\\it OxfordNet}), we denote $f_i(x)$ the corresponding feature obtained from the RGB image data through a succession of convolution-pooling operations.\nNote that the transformation $f_i$ is non-linear and thus not strictly a mathematical dot product with a template but can still be viewed as an inner product.\n\nIn this study, we propose to further improve the invariance of \\emph{pool5} CNN descriptors by incorporating global invariance to several transformation groups.\nThe specific transformation groups considered in this study are translations $G_T$, rotations $G_R$ and scale changes $G_S$.\nAs shown on Figure~\\ref{fig:method}~(b), transformations $g$ are applied on the input image $x$ varying the output of the \\emph{pool5} feature $f_i(g.x)$ accordingly.\n\nThe invariant statistical moments computed from the distributions $\\{ f_i(g.x) | g \\in G \\}$ with $G \\in \\{ G_T, G_R, G_S \\}$ are averages, maxima and standard deviations, respectively:\n\n", "index": 1, "text": "\\begin{align} \n&{\\mathcal{A}}_{G,i}(x) = \\displaystyle \\frac{1}{\\int_G dg}\\int_G f_i(g.x) dg \\label{eqn1}\\\\\n&{\\mathcal{M}}_{G,i}(x) = \\displaystyle \\max_G \\left( f_i(g.x) \\right) \\label{eqn2}\\\\\n&{\\mathcal{S}}_{G,i}(x) = \\displaystyle \\frac{1}{\\int_G dg} \\sqrt{ \\int_G f_i(g.x)^2 dg - (\\int_G f_i(g.x) dg)^2} \\label{eqn3}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{A}}_{G,i}(x)=\\displaystyle\\frac{1}{\\int_{G}dg}\\int_{G}f%&#10;_{i}(g.x)dg\" display=\"inline\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>G</mi></msub><mrow><mo>\ud835\udc51</mo><mi>g</mi></mrow></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>G</mi></msub></mstyle><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><mi>g</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{M}}_{G,i}(x)=\\displaystyle\\max_{G}\\left(f_{i}(g.x)\\right)\" display=\"inline\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mi>max</mi><mi>G</mi></munder><mrow><mo>(</mo><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{S}}_{G,i}(x)=\\displaystyle\\frac{1}{\\int_{G}dg}\\sqrt{%&#10;\\int_{G}f_{i}(g.x)^{2}dg-(\\int_{G}f_{i}(g.x)dg)^{2}}\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>G</mi></msub><mrow><mo>\ud835\udc51</mo><mi>g</mi></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><msqrt><mrow><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>G</mi></msub></mstyle><msub><mi>f</mi><mi>i</mi></msub><msup><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mi>d</mi><mi>g</mi><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>G</mi></msub></mstyle><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><mi>g</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02093.tex", "nexttext": "\nIn principle, $G$ is always measurable and of finite measure as required since $G_T$ and $G_S$ must be restricted to compact subsets due to image border effects.\n\nAn interesting aspect of the \\emph{i-theory} is the possibility in practice to chain multiple types of group invariances one after the other as already demonstrated in~\\cite{poggiomusic}.\nIn this study, we construct descriptors invariant to several transformation groups by successively applying the method to different transformation groups as shown on Figure~\\ref{fig:method}~(c).\nFor instance, following scale invariance with average by translation invariance with standard deviation for feature $i$ would correspond to:\n\n", "itemtype": "equation", "pos": 15122, "prevtext": "\nwith corresponding global image descriptors obtained by simply concatenating the moments for the individual features:\n\n", "index": 3, "text": "\\begin{align} \n&{\\mathcal{A}}_G(x) = ( {\\mathcal{A}}_{G,i}(x) )_{0 \\leq i < 512}\\\\\n&{\\mathcal{M}}_G(x) = ( {\\mathcal{M}}_{G,i}(x) )_{0 \\leq i < 512}\\\\\n&{\\mathcal{S}}_G(x) = ( {\\mathcal{S}}_{G,i}(x) )_{0 \\leq i < 512}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{A}}_{G}(x)=({\\mathcal{A}}_{G,i}(x))_{0\\leq i&lt;512}\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>0</mn><mo>\u2264</mo><mi>i</mi><mo>&lt;</mo><mn>512</mn></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{M}}_{G}(x)=({\\mathcal{M}}_{G,i}(x))_{0\\leq i&lt;512}\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>0</mn><mo>\u2264</mo><mi>i</mi><mo>&lt;</mo><mn>512</mn></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{S}}_{G}(x)=({\\mathcal{S}}_{G,i}(x))_{0\\leq i&lt;512}\" display=\"inline\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mi>G</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>0</mn><mo>\u2264</mo><mi>i</mi><mo>&lt;</mo><mn>512</mn></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.02093.tex", "nexttext": "\nOne may note that the operations are sometimes commutable (e.g. $\\mathcal{A}_G$ and $\\mathcal{A}_{G'}$) and sometimes not (e.g. $\\mathcal{A}_G$ and $\\mathcal{M}_{G'}$) depending on the specific combination of moments.\n\n\\section{Image Instance Retrieval}\n\\label{sec:results}\n\n\\subsection{Evaluation Framework}\n\nWe evaluate our invariant image descriptors in the context of image instance retrieval.\nAs our starting representation, we use the \\emph{pool5} layer from the 16 layers \\emph{OxfordNet} \\cite{Simonyan2014} with a total dimensionality of $25088$ organized in $512$ feature maps of size $7 \\times 7$.\n\nSimilarly to our evaluation work in \\cite{practicalguide2015}, the rotated input images are padded with the mean pixel value from the ImageNet data set.\nThe step size for rotations is 10 degrees yielding 36 rotated images per orbit.\nFor scale changes, 10 different center crops geometrically spanning from 100\\% to 50\\% of the total image have been taken.\nFor translations, the entire feature map is used for every feature, resulting in an orbit size of $7 \\times 7 = 49$.\n\nWe evaluate the performances of the descriptors against four popular data sets: {\\it Holidays}, {\\it Oxford buildings (Oxbuild)}, {\\it UKBench (UKB)} and {\\it Graphics}.\nThe four datasets are chosen for the diversity of data they provide: {\\it UKBench} and {\\it Graphics} are object-centric featuring close-up shots of objects in indoor environments. \n{\\it Holidays} and {\\it Oxbuild} are scene-centric datasets consisting primarily of outdoor buildings and scenes.\n\n{\\textbf{INRIA Holidays.}\nThe INRIA Holidays dataset~\\cite{Jegou08} consist of personal holiday pictures. \nThe dataset includes a large variety of outdoor scene types: natural, man-made, water and fire effects. \nThere are 500 queries and 991 database images.\nVariations in lighting conditions are rare in this data set as the pictures from the same location are taken at the same time.\n\n\\textbf{Oxbuild.}\nThe Oxford Buildings Dataset~\\cite{Philbin07} consists of 5062 images collected from Flickr representing landmark buildings in Oxford. \nThe collection has been manually annotated to generate a comprehensive ground truth for 11 different landmarks, each represented by 5 possible queries. \nNote that the set contains 55 queries only. \n\n{\\textbf{UKBench.}}\nThe University of Kentucky (UKY) data set~\\cite{Nister06} consists of 2550 groups of common objects.\nThere are 4 images representing each.\nOnly the object of interest present in each image.\nThus, there is no foreground or background clutter within this data set.\nAll 10200 images are used as queries.\n\n{\\textbf{Graphics.}}\nThe Graphics data set is part of the Stanford Mobile Visual Search data set~\\cite{SVMSDataSet}, which notably was used in the MPEG standard: Compact Descriptors for Visual Search (CDVS)~\\cite{MPEGDataset2}.\nThe data set contains different categories of objects like CDs, DVDs, books, software products, business cards, etc.\nThe query images include foreground and background clutter that would be considered typical in real-world scenarii, e.g., a picture of a CD might contain other CDs in the background.\nThis data set distinguishes from the other ones as it contains images of rigid objects captured under widely varying lighting conditions, perspective distortion, foreground and background clutter. \nQuery images are taken with heterogeneous phone cameras. \nEach query has two relevant images.\nThere are 500 unique objects, 1500 queries, and 1000 database images.\n\n\\subsection{Pairwise Matching Distance}\n\nImage instance retrieval starts with the construction of a list of database images ordered according to their pairwise matching distance with the query image.\nWith CNN descriptors, the matching distance is strongly affected by commonly encountered image transformations.\nAs shown in our previous evaluation work~\\cite{practicalguide2015}, a rotation of the query image by 10 degrees or more causing a sharp drop in results.\nThis particular issue is much less pronounced with the popular Fisher vectors, largely due to the use of interest point detectors.\n\nFigure~\\ref{fig:dists} provides an insight on how adding different types of invariance with our proposed method will affect the matching distance on different image pairs of matching objects.\nWith the incorporation of each new transformation group, we notice that the relative reduction in matching distance is the most significant with the image pair which is the most affected by the transformation group. \n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=.5\\textwidth]{./figures/dists_nolegend.pdf}\n\\begin{tabular}{@{}c@{}cc@{}cc@{}cc@{} c}\n\\includegraphics[width=.6cm]{./figures/legend1}&\\emph{pool5}&\n\\includegraphics[width=.6cm]{./figures/legend2}&$\\mathcal{A}_{G_S}$&\n\\includegraphics[width=.6cm]{./figures/legend3}&$\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$&\n\\includegraphics[width=.6cm]{./figures/legend4} &$\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_R}$\n\\end{tabular}\n\\caption{Distances for 3 matching pairs from \\emph{UKBench}. \nFor each pair, 4 pairwise distances ($L_2$-normalized) are computed corresponding to the following descriptors: \\emph{pool5}, $\\mathcal{A}_{G_S}$, $\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$ and  $\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_R}$. \n  Adding scale invariance makes the most difference on (b), translation invariance on (c), and rotation on (a) which is consistent with the scenarii suggested by the images.}\n  \\label{fig:dists}\n\\end{figure}\n\n\n\\subsection{Transformations, Order and Moments}\n\n\\begin{table}[ht]\n\\caption{Retrieval results (mAP) for different sequences of transformation groups and moments.\n}\n\\label{tab:res} \n{\n\\centering\n\\ra{1.2}\n{\\footnotesize \\singlespacing \n\\begin{adjustbox}{max width=\\textwidth,center}\n\\begin{tabular}{@{}lrrrrr@{}}\n\\toprule\n{\\sc Sequence} & {\\sc Dims} & \\multicolumn{4}{c}{\\sc Dataset} \\\\\n\\cmidrule{3-6}\n & & Oxbuild & Holidays & UKB & Graphics\\\\\n\\midrule\n\\emph{pool5} & 25088 & 0.427 & 0.707 & 0.823(3.105) & 0.315\\\\\n\\emph{fc6} & 4096 & 0.461 & 0.782 & 0.910(3.494) & 0.312\\\\\n\\midrule\n$\\mathcal{A}_{G_S}$ & 25088 & 0.430 & 0.716 & 0.828(3.122) & 0.394\\\\\n$\\mathcal{A}_{G_T}$ & 512 & 0.477 & 0.800 & 0.924(3.564) & 0.322\\\\\n$\\mathcal{A}_{G_R}$ & 25088 & 0.462 & 0.779 & 0.954(3.718) & 0.500\\\\\n$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_R}$ & 512 & 0.418 & 0.796 & 0.955(3.725) & 0.417\\\\\n$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_S}$ & 512 & 0.537 & 0.811 & 0.931(3.605) & 0.430\\\\\n$\\mathcal{A}_{G_R}$-$\\mathcal{A}_{G_S}$ & 25088 & 0.494 & 0.815 & 0.959(3.752) & 0.552\\\\\n$\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_R}$ & 512 & 0.484 & 0.833 & 0.971(3.819) & 0.509\\\\\n\\midrule\n$\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ & 512 & \\textbf{0.592} & \\textbf{0.838} & \\textbf{0.975(3.842)} & \\textbf{0.589}\\\\\n$\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ & 512 {\\bf bits} & 0.523 & 0.787 & 0.958(3.741)  & 0.552\\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n}\n}\n\\vspace*{2mm}\\\\\n\\footnotesize\nResults are computed with the mean average precision (mAP) metric.\nFor reference, 4$\\times$Recall@4 results are also provided for UKBench (between parentheses).\n$G_T$, $G_R$, $G_S$ denote the groups of translations, rotations and scale changes respectively.\nNote that averages commute with other averages so the sequence order of the composition does not matter when only averages are involved.\nBest results are achieved by choosing specific moments.\n$\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ corresponds to the best average performer for which a binarized version is also given.\n\\emph{fc6} and \\emph{pool5} are provided as a baseline.\n\\end{table}\n\n\\begin{figure}[ht]\n  \\centering\n\\includegraphics[width=.42\\textwidth]{./figures/depth.pdf}\n  \\caption{\n    Results from Table~\\ref{tab:res} for the 7 strategies using averages only (rows 3 to 9) expressed in terms of improvement in mAP over \\emph{pool5}, and aggregated by number of invariance groups.\n Improvements range from +5\\% on Oxbuild using 1 transformation to +83.5\\% on UKBench using 3 transformations.\n On all 4 datasets, results clearly improve with the amount of groups considered.}\n  \\label{fig:depth}\n\\end{figure}\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=.49\\textwidth]{./figures/error_improvement.pdf}\n  \\caption{\n  Results from Table~\\ref{tab:res} expressed in terms of improvement in mAP over \\emph{pool5}.\nMost strategies yield significant improvements over \\emph{pool5} on most datasets.\nThe average improvement is 68\\% for the best strategy. }\n  \\label{fig:error_improvement}\n\\end{figure}\n\nOur first set of results summarized in Table~\\ref{tab:res} study the effects of incorporating various transformation groups and using different moments.\n\\emph{Pool5} which is the starting point of our descriptors and \\emph{fc6} which is considered the best off-the-shelf descriptor \\cite{practicalguide2015,CNNOffTheShelf} are provided as baselines. \nTable~\\ref{tab:res} also provides results for all possible combinations of transformation groups for average pooling (order does not matter as averages commute) and for the single best performer which is $\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ (order matters).\n\nFirst, we can immediately point out the high potential of \\emph{pool5}.\nAlthough it performs notably worse than \\emph{fc6} as-is, a simple average pooling over the space of translations \\emph{$\\mathcal{A}_{G_T}$} makes it both better and 8 times more compact than \\emph{fc6}.\nAs shown in Figure~\\ref{fig:depth}, accuracy increases with the number of transformation groups involved.\nOn average, single transformation schemes perform 21\\% better  compared to \\emph{pool5}, 2-transformations schemes perform 34\\% better, and the 3-transformations scheme performs 41\\% better.\n\nSelecting statistical moments different than averages can further improve the retrieval results.\nIn Figure~\\ref{fig:error_improvement}, we observe that $\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ performs roughly 17\\% better (average results over all datasets) than $\\mathcal{A}_{G_S}$-$\\mathcal{A}_{G_T}$-$\\mathcal{A}_{G_R}$.\nNotably, the best combination corresponds to an increase in the orders of the moments: $\\mathcal{A}$ being a first-order moment, $\\mathcal{S}$ second order and $\\mathcal{M}$ of infinite order.\nA different way of stating this fact is that a more invariant representation requires a higher order of pooling.\n\nOverall, $\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ improves results over \\emph{pool5} by 29\\% (Oxbuild) to 86\\% (UKBench) with large discrepancies according to the dataset.\nBetter improvements with UKBench can be explained with the presence of many rotations in the dataset (smaller objects taken under different angles) while Oxbuild consisting mainly of upright buildings is not significantly helped by incorporating rotation invariance.\n\n\n\\subsection{Compact Binary Hashes}\n\n\\begin{table}[ht]\n\\caption{Retrieval performance (mAP) comparing our method to other state-of-the-art methods.}\n\\label{tab:stateoftheart} \n\\centering\n\\ra{1.2}\n{\\footnotesize \\singlespacing \n\\begin{adjustbox}{max width=\\textwidth,center}\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n{\\sc Method} & \\multicolumn{1}{c}{\\sc rep. size} & \\multicolumn{3}{c}{\\sc Dataset} \\\\\n\\cmidrule{3-5}\n&  \\#dim (size in bits) & Oxbuild & Holidays & UKB\\\\\n\\midrule\n{\\textbf{Our results}} & 512 (512) & 0.523 & \\textbf{0.787} & \\textbf{0.958}\\\\\n{OxfordNet \\cite{sharif2015baseline}} & 256 (1024) & \\textbf{0.533} & 0.716 & 0.842\\\\\n{OxfordNet  \\cite{sharif2015baseline}} & 256 (256) & 0.436 & 0.578 & 0.693\\\\\n{T-embedding \\cite{jegou2014triangulation}} & 256 (2048) & 0.472 & 0.657 & 0.863\\\\\n{T-embedding \\cite{jegou2014triangulation}} & 128 (1024) & 0.433 & 0.617 & 0.850\\\\\n{VLAD+CSurf \\cite{spyromitros2014comprehensive}} & 128 (1024) & 0.293 & 0.738 & 0.830\\\\\n{mVLAD+Surf  \\cite{spyromitros2014comprehensive}} & 128 (1024) & 0.387 & 0.718 & 0.875\\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n}\n\\vspace*{2mm}\\\\\n\\footnotesize\nOnly methods within a comparable range of bitrates are selected.\n\\end{table}\n\nAs shown in Table~\\ref{tab:res}, a simple binarization strategy (thresholding at dataset mean) applied to our best performing descriptor $\\mathcal{A}_{G_S}$-$\\mathcal{S}_{G_T}$-$\\mathcal{M}_{G_R}$ degrades retrieval performance only very marginally and is in fact sufficient to produce a hash that compares favourably with other state-of-the-art methods.\nOn average over the four datasets, the hash performs 39\\% better than \\emph{pool5}, while having a 1568 times smaller representation size. Compared to \\emph{fc6} feature, the hash performs 26\\% better while being 256 times smaller.\n\nAs mentioned, the invariant hash also performs well compared to other state-of-the-art.\nIn Table~\\ref{tab:stateoftheart}, we compare our invariant hash against other approaches designed to produce compact representations with comparable bit sizes, 512 being considered fairly compact by current standards \\cite{sharif2015baseline}.\nIn addition, note that the hashing scheme used in this experiment is very naive and better hashes (better retrieval results and/or higher compression rates) could most certainly be obtained by applying more sophisticated hashing methods such as in our previous work~\\cite{deephash, unsupervisedtriplethashing} based on stacked RBMs and metric finetuning.\n\n\\section{Conclusion}\n\nWe proposed a novel method based on \\emph{i-theory} for creating robust and compact global image descriptors from CNNs for image instance retrieval.\nThrough a thorough empirical study, we show that the incorporation of every new group invariance property following the method leads to consistent and significant improvements in retrieval results.\n\nOur method has a number of parameters (sequence of the invariances and choice of statistical moments) but experiments show that many default and reasonable settings produce results which can generalise well across all datasets meaning that the risk of overfitting is low.\n\nThis study also confirms the high potential of the feature pyramid (\\emph{pool5}) as a starting representation for high-performance compact hashes instead of the more commonly used first fully connected layer (\\emph{fc6}).\nOur method produces a set of descriptors able to compare favourably with other state-of-the-art compact descriptors at similar bitrates even without dedicated compression/hashing step.\n\n\\bibliography{../main}   \n\\bibliographystyle{IEEEtran}\n\n\n", "itemtype": "equation", "pos": 16038, "prevtext": "\nIn principle, $G$ is always measurable and of finite measure as required since $G_T$ and $G_S$ must be restricted to compact subsets due to image border effects.\n\nAn interesting aspect of the \\emph{i-theory} is the possibility in practice to chain multiple types of group invariances one after the other as already demonstrated in~\\cite{poggiomusic}.\nIn this study, we construct descriptors invariant to several transformation groups by successively applying the method to different transformation groups as shown on Figure~\\ref{fig:method}~(c).\nFor instance, following scale invariance with average by translation invariance with standard deviation for feature $i$ would correspond to:\n\n", "index": 5, "text": "\\begin{align} \n&\\displaystyle \\max_{{g_t} \\in G_T} \\left( \\frac{1}{\\int_{g_s \\in G_S} dg_s}\\int_{g_s \\in G_S} f_i(g_tg_s.x) dg_s \\right)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\displaystyle\\max_{{g_{t}}\\in G_{T}}\\left(\\frac{1}{\\int_{g_{s}\\in&#10;G%&#10;_{S}}dg_{s}}\\int_{g_{s}\\in G_{S}}f_{i}(g_{t}g_{s}.x)dg_{s}\\right)\" display=\"inline\"><mrow><munder><mi>max</mi><mrow><msub><mi>g</mi><mi>t</mi></msub><mo>\u2208</mo><msub><mi>G</mi><mi>T</mi></msub></mrow></munder><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>g</mi><mi>s</mi></msub><mo>\u2208</mo><msub><mi>G</mi><mi>S</mi></msub></mrow></msub><mrow><mo>\ud835\udc51</mo><msub><mi>g</mi><mi>s</mi></msub></mrow></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>g</mi><mi>s</mi></msub><mo>\u2208</mo><msub><mi>G</mi><mi>S</mi></msub></mrow></msub></mstyle><msub><mi>f</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>g</mi><mi>t</mi></msub><msub><mi>g</mi><mi>s</mi></msub><mo>.</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mi>d</mi><msub><mi>g</mi><mi>s</mi></msub><mo>)</mo></mrow></mrow></math>", "type": "latex"}]