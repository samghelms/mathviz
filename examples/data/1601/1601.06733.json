[{"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 19295, "prevtext": "\n\\maketitle\n\\begin{abstract}\n  Teaching machines to process text with psycholinguistic insights is a challenging task. We propose an attentive machine reader that reads text \n  from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long Short-Term Memory architecture, which different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. In addition, we propose a new attentional encoder-decoder that explicitly stores inter alignment information in the memory of the decoder, from which we reap significant gains. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference tasks. For the natural language inference, we achieved 89.1\\% on the test set, outperforming the previous state-of-the-art by 3\\%.\n\\end{abstract}\n\n\n\n\\section{Introduction}\n We are generally interested in the task of developing intelligent machines with inspirations drawn from psycholinguistics, in order to understand text and encode it properly. It has been studied in psycholinguistics that humans process text by shifting their \\textit{fixation point} in a sequential order \\cite{rayner2009eye}. When reading from left to right, a human reader forms a complete dynamic representation of the text portions up to the current fixation point \\cite{pickering2006syntactic}. Modern language modeling tools, such as recurrent neural networks (RNN), agree with this cognitive behavior. RNNs treat each sentence as a sequence of words and recursively compose each word with its previous \\textit{memory}, until a meaning representation of the whole sentence has been derived.\n \n However, natural language is more complicated than pure sequences. While reading a sentence, human readers implicitly analyze the words they have read so far and infer dependency relations among them---which is known as parsing or language comprehension in psycholinguistics. This meaning acquisition step has also been largely studied in computational linguistics but from a crucially different perspective. While psycholinguistic models \\textit{incrementally} build up the syntactic and semantic structures on the fly of reading, most natural language parsers construct a parse tree with a \\textit{whole} sentence as the input \\cite{abney1989computational}.\n \n We propose a novel machine reader which offers a middle ground between the recurrent neural networks and psycholinguistic models. The reader processes texts from left to right while implicitly capturing the dependencies among tokens. It is designed by enforcing an additional psycholinguistic prior in the architecture of a RNN---as a way not only to better simulate the human behavior of language comprehension, but also to address the intrinsic limitations of the RNN architecture for long sequence modeling in an interactive environment.\n \n Although proven Turing complete \\cite{siegelmann1995computational}, RNNs have at least two limiting factors in processing long input sequences. One concerns training and has to do with the vanishing and exploding gradient problem \\cite{hochreiter1991untersuchungen,bengio1994learning}, which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) \\cite{hochreiter1997long}, and the gradient clipping trick \\cite{pascanu2012difficulty}. The other limitation of RNNs relates to the memory compression effect. As an input sequence gets compressed into a single dense vector, it requires a sufficiently large memory capacity to store all the past information---which indicates poor generalization ability to long sequences as well as a significant memory wastage for shorter ones. This mixture effect also prevents further structural analysis or adaptive modulation within the memories.\n \n There have been a few recent papers on storing and querying sequence information with external memories \\cite{weston2014memory,sukhbaatar2015end,grefenstette2015learning}. In this work multiple memory slots are used to memorize a sequence in streams, and the read and write operations to each slot are modeled as an attention mechanism depending on the current input token and the state of a neural controller. Inspired by this work, we equip our machine reader with a memory tape whose size grows with the input sequence\\footnote{It is possible to restrict the size of the tape to simulate a limited memory span.}. As a major difference, we embed the memory tape within an LSTM unit, enabling the model to recurrently read texts without any external state controller. \n \n The resulting model is a Long Short-Term Memory-Network (LSTMN) machine reader, which can be used for any sequence processing task. While reading a text from left to right, the LSTMN performs implicit dependency analysis of its tokens with soft attention, learning how to modulate the memory and extracting meanings (shown in Figure \\ref{example}). We further explain how the internal attention complements with the external widely used between an encoder and a decoder. We validate the performance of our model in three tasks, including language modeling, sentiment analysis and natural language inference.\n \n \\begin{figure}[t]\n \t\\centering\n \t\\scriptsize\n \t\\begin{tabular}{|l|}\n \t\t\\hline \n \t\t{  \\color{red}The} dog is chasing a cat on the ground . \\\\\n \t\t\n \t\t    \\tikz[baseline=(A.base)]{\\node[opacity=-1](A) {The};\n \t\t \t\\shade[inner color=blue!40] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} {\\color{red}dog} is chasing a cat on the ground . \\\\\n \t\t \n \t\t    \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};}  {\\color{red}is} chasing a cat on the ground . \\\\\n \t\t \n \t\t    \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!35] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!40] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!40] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} {\\color{red}chasing} a cat on the ground . \\\\\n \t\t \n \t\t    \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!10] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t\t \t\\shade[inner color=blue!50] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} {\\color{red}a} cat on the ground . \\\\\n \t\t \n \t\t    \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t    \\shade[inner color=blue!10] (A.south east) rectangle (A.north west);\n \t\t    \\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {a};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{a}};} {\\color{red}cat} on the ground . \\\\\n \t\t \n \t\t \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!10] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {a};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{a}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {cat};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{cat}};} {\\color{red}on} the ground . \\\\\n \t\t \n \t\t \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!10] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {a};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{a}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {cat};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{cat}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {on};\n \t\t \t\\shade[inner color=blue!50] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{on}};} {\\color{red}the} ground . \\\\\n \t\t \n \t     \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t     \t\\shade[inner color=blue!10] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t     \t\\shade[inner color=blue!50] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t     \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t     \t\\shade[inner color=blue!40] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {a};\n \t     \t\\shade[inner color=blue!50] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{a}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {cat};\n \t     \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{cat}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {on};\n \t     \t\\shade[inner color=blue!60] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{on}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {the};\n \t     \t\\shade[inner color=blue!40] (A.south east) rectangle (A.north west);\n \t     \t\\path (A.center) \\pgfextra{\\pgftext{the}};} {\\color{red}ground} . \\\\\n \t\t\n \t\t \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {The};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{The}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {dog};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{dog}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {is};\n \t\t \t\\shade[inner color=blue!15] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{is}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {chasing};\n \t\t \t\\shade[inner color=blue!30] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{chasing}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {a};\n \t\t \t\\shade[inner color=blue!15] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{a}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {cat};\n \t\t \t\\shade[inner color=blue!35] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{cat}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {on};\n \t\t \t\\shade[inner color=blue!20] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{on}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {the};\n \t\t \t\\shade[inner color=blue!15] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{the}};} \\tikz[baseline=(A.base)]{\\node[opacity=0](A) {ground};\n \t\t \t\\shade[inner color=blue!45] (A.south east) rectangle (A.north west);\n \t\t \t\\path (A.center) \\pgfextra{\\pgftext{ground}};} {\\color{red}.} \\\\\n \t\t\\hline\n \t\\end{tabular}\n \t\\caption{An example explains how the machine reader works. Color \\textit{red} represents the current fixation point and \\textit{blue} represents memories. Darkness indicates the degree of memory activation.}\n \t\\label{example}\n \t\\vspace{-2ex}\n \\end{figure}\n \n\\section{Related Work}\n\\textbf{Machine Reading.}\nDeveloping intelligent machines that read text is a long-standing goal in artificial intelligence. Most of the previous research in this area analyzes text with traditional NLP pipelines such as tagging and parsing, which are substantially different from the human reading behavior. These systems require a considerable amount of manual engineering and labeled training examples, restricting their application domain to the specific task and data distribution \\cite{poon2010machine}. In fact, machine reading is inherently an \\textit{unsupervised} task, with the relations encountered during reading implicitly parsed on the fly \\cite{etzioni2006machine}. While end-to-end machine reading systems are still rare, a few recent works leverage recurrent neural networks to understand text from scratch \\cite{mikolov2010recurrent,bahdanau2014neural}. \n\n\\textbf{Recurrent Sequence Modeling.}\nRecurrent Neural Networks (RNNs) are powerful sequence modeling tools that can principally capture long input histories. There has been a surge of works on using RNNs to solve both single sequence modeling and sequence-to-sequence transduction. The later has assumed several guises in tasks like machine translation \\cite{bahdanau2014neural}, sentence compression \\cite{rush2015neural}, program execution \\cite{zaremba2014learning}, to name just a few. In practice, training RNNs to capture long term dependencies is challenging due to the well-known exploding or vanishing gradient problem \\cite{bengio1994learning}. This motivates the development of models with gated activation functions such as the Long Short-Term Memory \\cite{hochreiter1997long}, Gated Recurrent Unit \\cite{cho2014learning}, and more advanced architectures that enhance the information flow within the network \\cite{koutnik2014clockwork,chung2015gated}. Another problem relating RNNs is the memory compression effect. Since the model recursively combines all inputs into a single memory which is typically too small, RNNs are known to have difficulty in memorizing sequences well \\cite{zaremba2014learning}, which becomes a bottle neck for downstream tasks \\cite{bahdanau2014neural}. In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism to learn a soft alignment \\textit{between} the encoding and decoding states \\cite{bahdanau2014neural}. To the best of our knowledge, no attempts have been made to model attention \\textit{within} a sequence encoder.\n\n\\textbf{Differentiable Memories.} Recently, there has been a resurgence of methods with external storage of differentiable memories, which obviate both long-term dependency and memory compression. This idea dates back to an early work of \\newcite{das1992learning}, who connect a recurrent neural network state controller with an external stack memory for learning context free grammars. Very recently, \\newcite{weston2014memory} propose Memory Networks to explicitly segregate the memory storage from the computation of a neural network. This model can be trained end-to-end with a memory addressing mechanism closely related to soft attention \\cite{sukhbaatar2015end}. \\newcite{meng2015deep} apply a variant of the memory network in neural machine translation. Their model explicitly stores the source sequence in the memory, applies multiple steps of read-write transformations and finally decode the target sequence based on the transformed memory. \\newcite{grefenstette2015learning} define a set of differentiable data structures (stacks, queues and dequeues) as memories controlled by a recurrent neural network. Their model has shown promising results in simple sequence transduction tasks, such as copying. Concurrent to our work is the pre-print \\textit{Recurrent Memory Networks} \\cite{ke2016memory}, which equips an LSTM with an external memory block interacting with the hidden state of the LSTM.  To the best of our knowledge, these models introduce \\textit{external} memories that interact with a neural controller. In comparison, our work directly enhances the \\textit{internal} memory of an LSTM.\n \n\n\n\\section{The Machine Reader}\nIn this section we propose a novel machine reader inspired by psycholinguistics. The core of the reader is a Long Short-Term Memory recurrent neural network with an extended memory tape that explicitly simulates the human memory span. The reader performs implicit dependency analysis with an attention-based memory addressing mechanism at every input time step. In the following we first review the standard Long Short-Term Memory unit.\n\\subsection{Long Short-Term Memory}\nA Long Short-Term Memory (LSTM) recurrent neural network processes a variable-length sequence $x=(x_1, x_2, \\cdots, x_n)$ by incrementally adding up new content into a single slot of maintained memory, with gates controlling the extent to which new content should be memorized, old content should be erased and current content should be exposed. At time step $t$, the memory $c_t$ and the hidden state $h_t$ are updated with the following equations\n\n", "index": 1, "text": "\\begin{equation}\n\\begin{bmatrix}\ni_t\\\\ f_t\\\\ o_t\\\\ \\hat{c}_t\n\\end{bmatrix} =\n\\begin{bmatrix} \\sigma\\\\ \\sigma\\\\ \\sigma\\\\ \\tanh\n\\end{bmatrix} W\\cdot [h_{t-1}, \\, s_t]\n\\label{beginlstm}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}i_{t}\\\\&#10;f_{t}\\\\&#10;o_{t}\\\\&#10;\\hat{c}_{t}\\end{bmatrix}=\\begin{bmatrix}\\sigma\\\\&#10;\\sigma\\\\&#10;\\sigma\\\\&#10;\\tanh\\end{bmatrix}W\\cdot[h_{t-1},\\,s_{t}]\" display=\"block\"><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>i</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>f</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>o</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>tanh</mi></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mi>W</mi></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo rspace=\"4.2pt\">,</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 19493, "prevtext": "\n\n", "index": 3, "text": "\\begin{equation} c_t = f_t \\odot c_{t-1} +\ni_t \\odot \\hat{c}_t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"c_{t}=f_{t}\\odot c_{t-1}+i_{t}\\odot\\hat{c}_{t}\" display=\"block\"><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mi>c</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": " \nwhere $i$, $f$ and $o$ are gate activations. Compared to the standard RNN, LSTM separates the memory $c$ from the hidden state $h$ that actually interacts with the environment when computing the output. This network principally memorizes long input histories, conditioned on which future predictions can be made. In the case of language modeling, the model estimates the probability distribution of the next word as follows:\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n", "index": 5, "text": "\\begin{equation} h_t = o_t \\odot tanh(c_t)\n\\label{endlstm}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"h_{t}=o_{t}\\odot tanh(c_{t})\" display=\"block\"><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>o</mi><mi>t</mi></msub><mo>\u2299</mo><mi>t</mi></mrow><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\nwhere $w$ is a word in the vocabulary $V$, and $u$ is a scoring function that relates $w$ to the current hidden state $h_t$. $Z = \\sum_{w' \\in V} \\exp(u(w', h_t))$ is the normalization constant.\n\nA limitation of the standard LSTM/RNN lies in the memory compression effect. Since all the past inputs get recursively compressed into a single fixed-size vector, the model is potentially vulnerable to suffer from information loss or inefficient usage of the past information. \\newcite{cho2014learning} show that the performance of a sequence-to-sequence model drops significantly as the input sequence becomes longer. \n\n\\subsection{Long Short-Term Memory-Network}\nWe aim at developing a machine reader that explicitly stores memory segments so that it learns how to analyze and modulate past information in order to facilitate the understanding of the present input. To this end, we modify the standard LSTM structure by replacing the memory cell with a memory network, whose size grows with the input sequence. The resulting Long Short-Term Memory-Network (LSTMN) stores the input at each time step with a unique memory slot, obviating the problem of information compression whilst enabling adaptive modulation within the memories. While it is feasible to apply both read and write operations to the memory network based on the current input token, we focus on the $read$ operation only---as a way to attentively link the current input token to previous contents in the memory and accordingly select useful memory contents when processing the current token. Although it is not the focus of this work, the significance of the $write$ operation can be similarly justified---as a way to incrementally update the previous memories to correct wrong understandings when processing for example \\textit{garden path sentences}. \n\nThe architecture of the LSTMN is shown in Figure \\ref{lstmn}. At each time step, the model computes the memory activation based on the present input token and the previous attention vector. Then it uses the adaptively weighted hidden contents to compute various gate activations like the LSTM. Finally it mixes the adaptively weighted memory contents with the current input token to obtain the new input memory. In this model, each input token corresponds to one memory slot, which stores the transformed input representation under the given context.  Formally given the current input  $x_t$, previous memory tape $C_{t-1} = (c_1, \\cdots, c_{t-1})$ and previous hidden states $H_{t-1} = (h_1, \\cdots, h_{t-1})$, the model computes at the time step $t$ the values of $c_t$ and $h_t$ as follows: \n\n", "itemtype": "equation", "pos": 20071, "prevtext": " \nwhere $i$, $f$ and $o$ are gate activations. Compared to the standard RNN, LSTM separates the memory $c$ from the hidden state $h$ that actually interacts with the environment when computing the output. This network principally memorizes long input histories, conditioned on which future predictions can be made. In the case of language modeling, the model estimates the probability distribution of the next word as follows:\n\n", "index": 7, "text": "\\begin{equation}\np(x_{t+1} = w| x_1, \\cdots, x_t) =\\exp(u(w, h_t)) / Z\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"p(x_{t+1}=w|x_{1},\\cdots,x_{t})=\\exp(u(w,h_{t}))/Z\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>w</mi><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo>,</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mi>Z</mi></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 22771, "prevtext": "\nwhere $w$ is a word in the vocabulary $V$, and $u$ is a scoring function that relates $w$ to the current hidden state $h_t$. $Z = \\sum_{w' \\in V} \\exp(u(w', h_t))$ is the normalization constant.\n\nA limitation of the standard LSTM/RNN lies in the memory compression effect. Since all the past inputs get recursively compressed into a single fixed-size vector, the model is potentially vulnerable to suffer from information loss or inefficient usage of the past information. \\newcite{cho2014learning} show that the performance of a sequence-to-sequence model drops significantly as the input sequence becomes longer. \n\n\\subsection{Long Short-Term Memory-Network}\nWe aim at developing a machine reader that explicitly stores memory segments so that it learns how to analyze and modulate past information in order to facilitate the understanding of the present input. To this end, we modify the standard LSTM structure by replacing the memory cell with a memory network, whose size grows with the input sequence. The resulting Long Short-Term Memory-Network (LSTMN) stores the input at each time step with a unique memory slot, obviating the problem of information compression whilst enabling adaptive modulation within the memories. While it is feasible to apply both read and write operations to the memory network based on the current input token, we focus on the $read$ operation only---as a way to attentively link the current input token to previous contents in the memory and accordingly select useful memory contents when processing the current token. Although it is not the focus of this work, the significance of the $write$ operation can be similarly justified---as a way to incrementally update the previous memories to correct wrong understandings when processing for example \\textit{garden path sentences}. \n\nThe architecture of the LSTMN is shown in Figure \\ref{lstmn}. At each time step, the model computes the memory activation based on the present input token and the previous attention vector. Then it uses the adaptively weighted hidden contents to compute various gate activations like the LSTM. Finally it mixes the adaptively weighted memory contents with the current input token to obtain the new input memory. In this model, each input token corresponds to one memory slot, which stores the transformed input representation under the given context.  Formally given the current input  $x_t$, previous memory tape $C_{t-1} = (c_1, \\cdots, c_{t-1})$ and previous hidden states $H_{t-1} = (h_1, \\cdots, h_{t-1})$, the model computes at the time step $t$ the values of $c_t$ and $h_t$ as follows: \n\n", "index": 9, "text": "\\begin{equation}\na_i^t = v^T tanh(W_h h_i + W_x x_t + W_{\\tilde{h}} \\tilde{h}_{t-1})\n\\label{intraatt}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"a_{i}^{t}=v^{T}tanh(W_{h}h_{i}+W_{x}x_{t}+W_{\\tilde{h}}\\tilde{h}_{t-1})\" display=\"block\"><mrow><msubsup><mi>a</mi><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><msup><mi>v</mi><mi>T</mi></msup><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>W</mi><mi>h</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msub><mi>W</mi><mi>x</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>W</mi><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 22888, "prevtext": "\n\n", "index": 11, "text": "\\begin{equation}\ns_i^t = softmax (a_i^t)\n\\label{softmax}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"s_{i}^{t}=softmax(a_{i}^{t})\" display=\"block\"><mrow><msubsup><mi>s</mi><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>a</mi><mi>i</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 22960, "prevtext": "\n\n", "index": 13, "text": "\\begin{equation}\n\\begin{bmatrix}\n\\tilde{h}_t\\\\ \\tilde{c}_t\n\\end{bmatrix} = \\sum\\limits_{i=1}^{t-1} s_i^t \\cdot\n\\begin{bmatrix}\nh_i\\\\ c_i\n\\end{bmatrix} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}\\tilde{h}_{t}\\\\&#10;\\tilde{c}_{t}\\end{bmatrix}=\\sum\\limits_{i=1}^{t-1}s_{i}^{t}\\cdot\\begin{bmatrix%&#10;}h_{i}\\\\&#10;c_{i}\\end{bmatrix}\" display=\"block\"><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msubsup><mi>s</mi><mi>i</mi><mi>t</mi></msubsup><mo>\u22c5</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>h</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>c</mi><mi>i</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 23127, "prevtext": "\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{bmatrix}\ni_t\\\\ f_t\\\\ o_t\\\\ \\hat{c}_t\n\\end{bmatrix} =\n\\begin{bmatrix} \\sigma\\\\ \\sigma\\\\ \\sigma\\\\ \\tanh\n\\end{bmatrix} W \\cdot [\\tilde{h}_t, \\, x_t]\n\\label{gates}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}i_{t}\\\\&#10;f_{t}\\\\&#10;o_{t}\\\\&#10;\\hat{c}_{t}\\end{bmatrix}=\\begin{bmatrix}\\sigma\\\\&#10;\\sigma\\\\&#10;\\sigma\\\\&#10;\\tanh\\end{bmatrix}W\\cdot[\\tilde{h}_{t},\\,x_{t}]\" display=\"block\"><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>i</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>f</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>o</mi><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\u03c3</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>tanh</mi></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mi>W</mi></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi>h</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub><mo rspace=\"4.2pt\">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 23326, "prevtext": "\n\n", "index": 17, "text": "\\begin{equation} c_t = f_t \\odot \\tilde{c}_t +\ni_t \\odot \\hat{c}_t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"c_{t}=f_{t}\\odot\\tilde{c}_{t}+i_{t}\\odot\\hat{c}_{t}\" display=\"block\"><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": " \nwhere $v$, $W_h$ and $W_x$ are weights of the network. Compared to the standard LSTM in Equation \\ref{beginlstm} to \\ref{endlstm}, we additionally introduce an attention layer to compute the adaptive memory representation $\\tilde{c}_t$ and hidden representation (i.e., the attention vector) $\\tilde{h}_t$, which are then used in computing the gated activations $i_t$, $f_t$, $o_t$ and the transformed memory $\\hat{c}_t$. \n\n\\subsection{Deep LSTMNs}\nIt is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM \\cite{graves2013generating}, or from another perspective, a multi-hop memory network \\cite{sukhbaatar2015end}. This is achieved by feeding the attention vector $\\tilde{h}_t^k$ of the $k$ layer as input to the $(k+1)$th layer. We find skip-connections \\cite{graves2013generating} across layers important for easing the training and information flow. \n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\t\\begin{center}\n\t\t\\includegraphics[width=0.45\\textwidth]{\"lstmn\".jpg}\n\t\\end{center}\n\t\\caption{\\label{lstmn} Long Short-Term Memory-Network.}\n\t\\vspace{-2ex}\n\\end{figure}\n\n\n\\section{LSTMNs for Dual Sequence Modeling}\nPractical natural language tasks, such as machine translation, are concerned more with dual sequences rather than a single sequence. Central to these tasks is a dual sequence processor (e.g., an encoder-decoder), where the second sequence (i.e., \\textit{target}) is processed \\textit{conditioned on} the first one (i.e., \\textit{source}). In this section we draw connections between the inherent attention mechanism of the LSTMN and that widely used in between two sequences. We then explain how an LSTMN can be used in the dual sequence processing task.\n\nIn general, the intra attention within a sequence and the inter attention between two sequences complement each other. While inter attention derives the alignment between the source and target tokens, intra attention provides implicit dependency analysis within a sequence, resulting in enhanced memories that could benefit subsequent inter alignment. In the following we propose two ways of using the LSTMN in a dual architecture, shown in Figure \\ref{shallow} and \\ref{deep} respectively.\n\n\\textbf{Shallow Attention Fusion.} Shallow fusion treats LSTMN as a separate module that is readily plugged into a dual architecture (e.g., an encoder-decoder), in place of a standard RNN or LSTM. In the complete picture, both of the two sequence processors are modeled as LSTMNs with intra-attention. Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the \\emph{RNNSearch} \\cite{bahdanau2014neural}.\n\n\\textbf{Deep Attention Fusion.} Deep fusion aims to fuse inter and intra attention initiated by the target sequence processor, in a way that both previous memories of the target and the entire memories of the source are used to compute the current inputting memory. Denote source memories and hidden states with $A=[\\alpha_1, \\cdots, \\alpha_m]$ and $Y=[\\gamma_1, \\cdots, \\gamma_m]$ respectively ,where $m$ is the length of the source sequence conditioned on. We can compute the inter attention at time step $t$ when processing the target sequence as follows\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation} h_t = o_t \\odot tanh(c_t)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"h_{t}=o_{t}\\odot tanh(c_{t})\" display=\"block\"><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>o</mi><mi>t</mi></msub><mo>\u2299</mo><mi>t</mi></mrow><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 26700, "prevtext": " \nwhere $v$, $W_h$ and $W_x$ are weights of the network. Compared to the standard LSTM in Equation \\ref{beginlstm} to \\ref{endlstm}, we additionally introduce an attention layer to compute the adaptive memory representation $\\tilde{c}_t$ and hidden representation (i.e., the attention vector) $\\tilde{h}_t$, which are then used in computing the gated activations $i_t$, $f_t$, $o_t$ and the transformed memory $\\hat{c}_t$. \n\n\\subsection{Deep LSTMNs}\nIt is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM \\cite{graves2013generating}, or from another perspective, a multi-hop memory network \\cite{sukhbaatar2015end}. This is achieved by feeding the attention vector $\\tilde{h}_t^k$ of the $k$ layer as input to the $(k+1)$th layer. We find skip-connections \\cite{graves2013generating} across layers important for easing the training and information flow. \n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\t\\begin{center}\n\t\t\\includegraphics[width=0.45\\textwidth]{\"lstmn\".jpg}\n\t\\end{center}\n\t\\caption{\\label{lstmn} Long Short-Term Memory-Network.}\n\t\\vspace{-2ex}\n\\end{figure}\n\n\n\\section{LSTMNs for Dual Sequence Modeling}\nPractical natural language tasks, such as machine translation, are concerned more with dual sequences rather than a single sequence. Central to these tasks is a dual sequence processor (e.g., an encoder-decoder), where the second sequence (i.e., \\textit{target}) is processed \\textit{conditioned on} the first one (i.e., \\textit{source}). In this section we draw connections between the inherent attention mechanism of the LSTMN and that widely used in between two sequences. We then explain how an LSTMN can be used in the dual sequence processing task.\n\nIn general, the intra attention within a sequence and the inter attention between two sequences complement each other. While inter attention derives the alignment between the source and target tokens, intra attention provides implicit dependency analysis within a sequence, resulting in enhanced memories that could benefit subsequent inter alignment. In the following we propose two ways of using the LSTMN in a dual architecture, shown in Figure \\ref{shallow} and \\ref{deep} respectively.\n\n\\textbf{Shallow Attention Fusion.} Shallow fusion treats LSTMN as a separate module that is readily plugged into a dual architecture (e.g., an encoder-decoder), in place of a standard RNN or LSTM. In the complete picture, both of the two sequence processors are modeled as LSTMNs with intra-attention. Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the \\emph{RNNSearch} \\cite{bahdanau2014neural}.\n\n\\textbf{Deep Attention Fusion.} Deep fusion aims to fuse inter and intra attention initiated by the target sequence processor, in a way that both previous memories of the target and the entire memories of the source are used to compute the current inputting memory. Denote source memories and hidden states with $A=[\\alpha_1, \\cdots, \\alpha_m]$ and $Y=[\\gamma_1, \\cdots, \\gamma_m]$ respectively ,where $m$ is the length of the source sequence conditioned on. We can compute the inter attention at time step $t$ when processing the target sequence as follows\n\n", "index": 21, "text": "\\begin{equation}\nb_j^t = u^T tanh(W_{\\gamma} {\\gamma}_j + W_x x_t + W_{\\tilde{\\gamma}} \\tilde{\\gamma}_{t-1})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"b_{j}^{t}=u^{T}tanh(W_{\\gamma}{\\gamma}_{j}+W_{x}x_{t}+W_{\\tilde{\\gamma}}\\tilde%&#10;{\\gamma}_{t-1})\" display=\"block\"><mrow><msubsup><mi>b</mi><mi>j</mi><mi>t</mi></msubsup><mo>=</mo><mrow><msup><mi>u</mi><mi>T</mi></msup><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>W</mi><mi>\u03b3</mi></msub><mo>\u2062</mo><msub><mi>\u03b3</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><msub><mi>W</mi><mi>x</mi></msub><mo>\u2062</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>W</mi><mover accent=\"true\"><mi>\u03b3</mi><mo stretchy=\"false\">~</mo></mover></msub><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\u03b3</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 26824, "prevtext": "\n\n", "index": 23, "text": "\\begin{equation}\np_j^t = softmax (b_j^t)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"p_{j}^{t}=softmax(b_{j}^{t})\" display=\"block\"><mrow><msubsup><mi>p</mi><mi>j</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>b</mi><mi>j</mi><mi>t</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\nand then we can cast the source memory to the target with another gating operation:\n\n", "itemtype": "equation", "pos": 26880, "prevtext": "\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{bmatrix}\n\\tilde{\\gamma}_t \\\\ \\tilde{\\alpha}_t\n\\end{bmatrix} = \\sum\\limits_{j=1}^{m} p_j^t \\cdot\n\\begin{bmatrix}\n\\gamma_j \\\\ \\alpha_j\n\\end{bmatrix} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}\\tilde{\\gamma}_{t}\\\\&#10;\\tilde{\\alpha}_{t}\\end{bmatrix}=\\sum\\limits_{j=1}^{m}p_{j}^{t}\\cdot\\begin{%&#10;bmatrix}\\gamma_{j}\\\\&#10;\\alpha_{j}\\end{bmatrix}\" display=\"block\"><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>\u03b3</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mover accent=\"true\"><mi>\u03b1</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msubsup><mi>p</mi><mi>j</mi><mi>t</mi></msubsup><mo>\u22c5</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\u03b3</mi><mi>j</mi></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>\u03b1</mi><mi>j</mi></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 27151, "prevtext": "\nand then we can cast the source memory to the target with another gating operation:\n\n", "index": 27, "text": "\\begin{equation}\nr_t = \\sigma (W_r [\\tilde{\\gamma}_t, x_t])\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"r_{t}=\\sigma(W_{r}[\\tilde{\\gamma}_{t},x_{t}])\" display=\"block\"><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mover accent=\"true\"><mi>\u03b3</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 27226, "prevtext": "\n\n", "index": 29, "text": "\\begin{equation} c_t = f_t \\odot \\tilde{c}_{t-1} +\ni_t \\odot \\hat{c}_t + r_t \\odot \\tilde{\\alpha}_t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"c_{t}=f_{t}\\odot\\tilde{c}_{t-1}+i_{t}\\odot\\hat{c}_{t}+r_{t}\\odot\\tilde{\\alpha}%&#10;_{t}\" display=\"block\"><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>f</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>\u2299</mo><msub><mover accent=\"true\"><mi>\u03b1</mi><mo stretchy=\"false\">~</mo></mover><mi>t</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06733.tex", "nexttext": " \nwhere $v$, $W_h$ and $W_x$ are weights of the network. Compared to the standard LSTM in Equation \\ref{beginlstm} to \\ref{endlstm}, we additionally introduce an attention layer to compute the adaptive memory representation $\\tilde{c}_t$ and hidden representation (i.e., the attention vector) $\\tilde{h}_t$, which are then used in computing the gated activations $i_t$, $f_t$, $o_t$ and the transformed memory $\\hat{c}_t$. \n\n\\subsection{Deep LSTMNs}\nIt is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM \\cite{graves2013generating}, or from another perspective, a multi-hop memory network \\cite{sukhbaatar2015end}. This is achieved by feeding the attention vector $\\tilde{h}_t^k$ of the $k$ layer as input to the $(k+1)$th layer. We find skip-connections \\cite{graves2013generating} across layers important for easing the training and information flow. \n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\t\\begin{center}\n\t\t\\includegraphics[width=0.45\\textwidth]{\"lstmn\".jpg}\n\t\\end{center}\n\t\\caption{\\label{lstmn} Long Short-Term Memory-Network.}\n\t\\vspace{-2ex}\n\\end{figure}\n\n\n\\section{LSTMNs for Dual Sequence Modeling}\nPractical natural language tasks, such as machine translation, are concerned more with dual sequences rather than a single sequence. Central to these tasks is a dual sequence processor (e.g., an encoder-decoder), where the second sequence (i.e., \\textit{target}) is processed \\textit{conditioned on} the first one (i.e., \\textit{source}). In this section we draw connections between the inherent attention mechanism of the LSTMN and that widely used in between two sequences. We then explain how an LSTMN can be used in the dual sequence processing task.\n\nIn general, the intra attention within a sequence and the inter attention between two sequences complement each other. While inter attention derives the alignment between the source and target tokens, intra attention provides implicit dependency analysis within a sequence, resulting in enhanced memories that could benefit subsequent inter alignment. In the following we propose two ways of using the LSTMN in a dual architecture, shown in Figure \\ref{shallow} and \\ref{deep} respectively.\n\n\\textbf{Shallow Attention Fusion.} Shallow fusion treats LSTMN as a separate module that is readily plugged into a dual architecture (e.g., an encoder-decoder), in place of a standard RNN or LSTM. In the complete picture, both of the two sequence processors are modeled as LSTMNs with intra-attention. Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the \\emph{RNNSearch} \\cite{bahdanau2014neural}.\n\n\\textbf{Deep Attention Fusion.} Deep fusion aims to fuse inter and intra attention initiated by the target sequence processor, in a way that both previous memories of the target and the entire memories of the source are used to compute the current inputting memory. Denote source memories and hidden states with $A=[\\alpha_1, \\cdots, \\alpha_m]$ and $Y=[\\gamma_1, \\cdots, \\gamma_m]$ respectively ,where $m$ is the length of the source sequence conditioned on. We can compute the inter attention at time step $t$ when processing the target sequence as follows\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation} h_t = o_t \\odot tanh(c_t)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"h_{t}=o_{t}\\odot tanh(c_{t})\" display=\"block\"><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>o</mi><mi>t</mi></msub><mo>\u2299</mo><mi>t</mi></mrow><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]