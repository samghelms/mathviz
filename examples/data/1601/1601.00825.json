[{"file": "1601.00825.tex", "nexttext": "\nwhere $P^+$ is number of points earned for a correct click, $A$ is the area in pixels of the clicked object, and $t$ is the number of consecutive clicks within a 30$\\times$30 pixels region. In practice, $\\frac{A}{20}$ is the maximum score awarded for a click on a certain object, but this score is progressively reduced if the user keeps clicking on the same spot: when salient objects are in the scene, this reduction forces users to vary their clicking pattern to get more points, while at the same time helps to provide data on as many objects as possible. Conversely, users are subtracted points if they click too far from the objects in the visualized frame; the penalty is computed as:\n\n", "itemtype": "equation", "pos": 18347, "prevtext": "\n\n\n\n\n\n\n\n\\title{Gamifying Video Object Segmentation}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Simone~Palazzo,\n        Concetto~Spampinato,~\\IEEEmembership{Member,~IEEE} and Daniela~Giordano,~\\IEEEmembership{Member,~IEEE}\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem S. Palazzo, C. Spampinato and D. Giordano are with the Department\nof Electrical, Electronics and Computer Engineering, University of Catania, Italy.\\protect\\\\\n\n\nE-mail: see http://perceive.dieei.unict.it}\n\\thanks{Manuscript submitted on January 05, 2016}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\n\n\n\n\\begin{abstract}\nVideo object segmentation can be considered as one of the most challenging computer vision problems. Indeed, so far, no existing solution is able to effectively deal with the peculiarities of real-world videos, especially in cases of articulated motion and object occlusions; limitations that appear more evident when we compare their performance with the human one. However, manually segmenting objects in videos is largely impractical as it requires a lot of human time and concentration. \nTo address this problem, in this paper we propose an interactive video object segmentation method, which exploits, on one hand, the capability of humans to identify correctly objects in visual scenes, and on the other hand, the collective human brainpower to solve challenging tasks. In particular, our method relies on a web game to collect human inputs on object locations, followed by an accurate segmentation phase achieved by optimizing an energy function encoding spatial and temporal constraints between object regions as well as human-provided input. \nPerformance analysis carried out on challenging video datasets with some users playing the game demonstrated that our method shows a better trade-off between annotation times and segmentation accuracy than interactive video annotation and automated video object segmentation approaches.\n\n\\end{abstract}\n\n\n\n\\begin{IEEEkeywords}\nInteractive video annotation, Games with a purpose, Human in the Loop, Spatio-temporal superpixel segmentation\n\\end{IEEEkeywords}\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec:introduction}\n\\IEEEPARstart{T}{he} generation and collection of massive amount of videos has become an easy task due to the progress in low-cost digital imaging systems as well as storage services. Indeed, every day several petabytes of videos are routinely generated for disparate applications ranging from video-surveillance to news broadcasting to entertainment. This is also highlighted in the recent ``Forecast and Methodology 2014-2019'' report\\footnote{http://www.cisco.com/c/en/us/solutions/collateral/service-provider/ip-ngn-ip-next-generation-network/white\\_paper\\_c11-481360.html}  by CISCO that has estimated that consumer internet video traffic will be 80 percent of all consumer Internet traffic in 2019. Nevertheless, this video data deluge needs automated methods able to extract meaningful information for data indexing, preservation and understanding, since it is unrealistic and unfeasible (as stated in the same CISCO report, {\\it it would take an individual over 5 million years to watch the amount of video that will cross global IP networks each month in 2019}) to assume people can do this work manually. This is the main reason why all the  existing video datasets have only a few frames annotated.\nOne of the upstream modules for video understanding is object segmentation, which aims at discriminating accurately foreground objects from the background. \nThere is a large (past and present) bulk of literature of methods for video object segmentation. Background modeling/subtraction~\\cite{Giordano2015superpixel, HanKDESVM},  motion analysis~\\cite{6682905,baieccv2010}, object ranking~\\cite{Zhang2013video,Lee:2011:KVO:2355573.2356444} and clustering point tracks~\\cite{Ochs2011,broxeccv10} and, recently, combination of CNN-based moving object detectors~\\cite{Fragkiadaki_2015_CVPR} are among the most common methods. However, the accuracy and performance of these techniques are still not satisfactory, especially in cases of articulated motion, cluttered scenes and object occlusions. \n\nTherefore, so far, there exist no valid alternatives to the classic automated video segmentation methods and the only possible solution might be to include effectively and efficiently humans in the analysis and learning process. ``Human in the loop'' is a recent trend in machine learning which tries to learn discriminative patterns by proactively involving people in the annotation process. This is the case of ``games with a purpose'' that channel collective human brainpower through computer games~\\cite{vonAhn2008}. The underlying idea is to engage people in solving unconsciously complex tasks while playing computer games. The combination of these games to crowdsourcing strategies may be an extremely powerful tool to solve tasks at large scale. \nWhile there exist several games with purpose to support automated image analysis~\\cite{Morrison:2009,Hacker:2009,vonAhn2004,vonAhn:2006,vonAhn2006}  and some for video tagging~\\cite{eps271071,ontogame} (which, however, share the same philosophy of the image annotation ones), to the best of our knowledge, none have been adopted for video object segmentation, which would particularly benefit from this approach as annotating videos, in terms of object segmentation, requires much more human time and concentration than identifying image classes.\nUnder this scenario, in this paper we propose a human-guided video object segmentation method, built upon a web game and able to effectively and accurately extract moving objects from videostreams by greatly reducing human intervention. \nThe contribution of the paper is threefold:\n\\begin{itemize}\n\\item First, we present and release a web game to collect human input (in the form of clicks) that can be used with any kind of videos, thus representing a powerful tool for computer vision scientists (and not only) to get their own videos automatically annotated;\n\\item We propose an interactive video object segmentation approach based on the optimization of an energy function which is able to encode spatio-temporal constraints between object regions as well as human-provided priors. The method can be combined with other sources of human input (e.g., eye-gaze data~\\cite{Palazzo2015using}) to annotate automatically videos.\n\\item We demonstrate that the collective action of players, despite providing noisy and inaccurate data, results in better segmentation accuracy -- with much less human effort --  than state-of-the-art automated video object solutions as well as interactive video annotation methods.\n\\end{itemize}\n\n\n\n\n\\section{Related Work}\n\\label{sec:related}\nOur work shares the same end goal of automated video object segmentation~\\cite{Giordano2015superpixel,Barnich2011vibe,Spampinato2014texton,HanKDESVM,Shengcai2010, Zhang2011,Papazoglou2013fast,eccv}, but our approach is more inline with research that places humans in the loop (including games with purpose)~\\cite{Deng2013,Maji2012,Sorokin,Parikh2012,Parkash,Parikh,Vijayanarasimhan2014,Salvador2013,Morrison:2009,Hacker:2009,vonAhn2004,vonAhn:2006,vonAhn2006,eps271071,ontogame} and interactive video segmentation~\\cite{Badrinarayanan2010,Ochs2011,Fathi2011,Budvytis2011,NSB15,Price2009,Badrinarayanan2013,Li2005}.\n\nUnsupervised video segmentation has gained a lot of attention in the last decades~\\cite{Barnich2011vibe,Spampinato2014texton,HanKDESVM, Shengcai2010, Zhang2011} and recently it has been thought mainly in terms of spatio-temporal superpixel modeling~\\cite{Giordano2015superpixel,Papazoglou2013fast, eccv}. The key idea behind these methods is the one of grouping pixels which are appearance- and motion-wise--consistent. Despite the performance increase due to superpixel segmentation, all these methods suffer from oversegmentation, especially in cases of camera motion and object occlusions.\nTherefore, manual or semi-manual video annotation can be considered the only reliable way for obtaining precise object segmentation, but as argued earlier, this tedious and labor-intensive process is extremely costly and unfeasible at large scale. Semi-supervised video segmentation approaches~\\cite{Badrinarayanan2010,Ochs2011,Fathi2011,Budvytis2011,NSB15,Price2009}, that differently from the semi-supervised image segmentation ones (e.g., the popular Grabcut~\\cite{grabcut}) have not received much attention, usually require a short intervention by humans in terms of object annotations that are then propagated automatically over time. Most of these methods rely either on optical flow~\\cite{Fathi2011} or on temporal connections ~\\cite{Price2009, Budvytis2011} between frames usually modeled by Markov chains, but they work well only in simple cases failing mainly in precisely representing object boundaries. In~\\cite{Li2005}, the authors propose an interactive video object segmentation method using a 3D graph- cut--based segmentation followed by a tracking-based local refinement. In~\\cite{NSB15} video object segmentation is formulated as a spatio-temporal MRF optimization problem, with a cost function including user input, motion and appearance cues, spatio-temporal consistency similarly to the one proposed in this work.  In addition, superpixel segmentation is also largely employed as it allows us to reduce processing time ensuring at the same time spatio-temporal coherency among pixels. In some work~\\cite{jain2014}, temporal linking between superpixels is done manually. Despite these methods are able to alleviate human effort for video annotation, at large scale they are ineffective and still time-consuming for human operators.\nAnother option to support low-level computer vision tasks is to understand how human perform them and to seek how human inference/reasoning can be integrated into computer programs. Examples are the ones that ask people to provide explicitly annotation rationales \\cite{Donahue2011} or to elicit the visual features employed to discriminate between image/object classes~\\cite{Vedaldi2014,Deng2013}. Nevertheless, unlike computers, humans need incentives, either monetary or for entertainment, to carry out specific tasks. Under this scenario, on-line games represent an effective mechanism to involve people in solving challenging problems. \nTwo of the most common approaches exploiting web-games for collecting human feedback for machine learning methods are the ESP Game~\\cite{vonAhn2004} and Peekaboom~\\cite{vonAhn2006}. Both approaches use the collective intelligence of human brains for gathering key information for image classification. Since their release,  many thousands of people have played them, generating millions of labels. However, these games are devised only for image analysis and, moreover, cannot be played by everyone: for instance, children, who usually are very passionate with games, would have difficulty in getting engaged by them.  In addition, to the best of our knowledge, there exist no games employed for supporting video object segmentation, except the one used in this work.\n\nThe approach proposed in this paper draws inspiration from both interactive video object segmentation approaches and human-computation using web-games combining both strategies in a smart way for accurate object segmentation in videos at large scale. More specifically, we propose an interactive video segmentation approach formulated as a spatio-temporal superpixel labeling by taking into account user input and spatio-temporal consistency of motion and appearance features. In addition, user feedback is gathered through a web game in the form of clicks, instead of strokes (as in most of the interactive video object segmentation methods),  leveraging on multiple users to obtain more consistent and structured feedback for automated segmentation. Also, the web-game is designed to being playable by any person of any age, thus increasing its possible audience (and with it the amount of gathered data) and improving the accuracy of the generated object segmentations.  \n\n\n\\section{Method}\n\\label{sec:methods}\nThe proposed interactive video object segmentation approach can be seen as a two-step spatio-temporal MRF optimization problem: the first one with a cost function exploiting spatial information at the frame level and encoding user input and appearance cues in order to extract homogeneous object regions in video frames; and the second one enforcing spatio-temporal consistency between the segmented object regions in consecutive frames, thus refining the preliminary segmentation. \nThree are the main modules of the whole approach:\n\\begin{itemize}\n \\item \\emph{The game}: The starting point of the whole process, our game is thought to gather user clicks in correspondence of objects of interest in videos. The game is designed to be challenging and competitive, so that users are encouraged to play: while this helps keeping the competition between users, game difficulty often reflects on the noisiness of the generated data.\n \\item \\emph{Superclick extraction}: The initial stage of our algorithm converts the noisy set of clicks into a set of more accurate ``clicked superpixels'', or \\emph{superclicks}. Posing the problem in terms of superpixels rather than pixels 1) reduces the  numerical complexity of the task and, 2) enforces spatial coherency between clicked object regions. On top of this viewpoint, we identify and group together superclicks through MRF optimization. \n \\item \\emph{Temporal smoothing}: Single-frame superclick extraction produces a fairly accurate segmentation of the objects in the scene, however it ignores temporal consistency between frames which can be exploited to further improve the segmentation. Based on the superclicks extracted from a span of consecutive frames, a three-dimensional (across time) MRF is designed in order to transfer information on the labels assigned to corresponding superpixels at different frames.\n\\end{itemize}\n\n\\subsection{The Game}\n\\label{sec:game}\nWe reused the game presented in \\cite{6595949} (where users had to perform a similar task but for a different objective, i.e., ground-truth generation) by adapting and modifying it according to our objectives: players are instructed to click on moving objects in a set of videos (one for each game level). Each correct click awards points, and each level is successfully completed if the user sums up a certain amount of points (increasing by level). \n\n\\textbf{User interface.} Fig.~\\ref{fig:game_interface} shows an example of a typical in-game screenshot. The video for the current level is, of course, the most important element of the interface and takes up most of the space; the current score obtained by the user is shown at the top; the remaining time before the level's end is shown on the top-left corner (indicated by the \\textsc{oxygen} icon---a legacy from the original underwater-oriented application of the game), and the number of points needed to pass the current level is at the bottom of the screen. The mouse cursor is shaped like a camera reticle, and at each click the taken ``photo'' is shown  at the bottom-left corner of the screen (this is done for future object classification purposes, which are beyond the scope of this paper). When the user clicks correctly on a target, points are awarded, shown as upward-floating bubbles (``+81'' in the example image). Finally, further option buttons are shown at the top-right corner of the screen.\n\n\\begin{figure}\n \\centering\n \\includegraphics[width=0.48\\textwidth]{figures/game_interface.png}\n \\caption{In-game screenshot of the user interface.}\n \\label{fig:game_interface}\n\\end{figure}\n\n\\textbf{Levels.} Each level in the game is associated to an input video, which is supposed to be at least 30 seconds long at 10 frames per second (if longer, only the first 30 seconds will be shown). In order to pass a level, a certain amount of points must be scored, starting from 4000 at the first level, and increasing by 2000 at each successive level. As the game is actually relatively simple, this increase represents the main challenge, since it makes it more and more difficult to achieve the required points.\n\nLevel-video association is done randomly, i.e., in different game sessions, the video ordering is never the same in order to avoid players to know in advance where objects might be located. This is necessary since in games, players often tend to maximize their scores also using tricks.\n\nOne related issue was the \\emph{saliency bias} of some objects with respect to others, which caused users to click always on the same objects (the most salient ones) in a scene even if several others were present. To reduce this phenomenon, we applied an \\emph{inhibition of return} mechanism by blurring videos in areas where clicks (by all users) accumulate: this reduced the saliency of underlying objects and led users to click on other objects in the scene. Video blurring was performed using all gathered clicks, and not just the current user's, although saliency is partly a subjective process: we found this also helped to avoid gathering too many clicks on objects for which we already had enough data (see paragraph~\\ref{sec:results_users_time}). Fig.~\\ref{fig:ior_blur} shows an example of the click distribution in a frame and the corresponding blurred version.\n\n\\begin{figure}\n \\centering\n \\includegraphics[width=0.4\\textwidth]{figures/example_ior_clicks.png} \\\\ \\vspace{0.2cm}\n \\includegraphics[width=0.4\\textwidth]{figures/example_ior_blurred.png}\n \\caption{{\\bf Left}: user clicks (blue dots) in a frame; {\\bf Right}: saliency inhibition by blurring clicked regions.}\n \\label{fig:ior_blur}\n\\end{figure}\n\n\\textbf{Points.} As in any gamification process, it is necessary to pose the task as a competitive one, providing the users with a feedback on how good they are with respect to their previous results or their friends. We employ a point-based system to reflect users' performance on the game, and keep an all-time ranking of the best scores.\nPoints are awarded by clicking correctly on an object of interest, depending on the size of the object and on previous clicks: bigger objects are awarded more points, but successive clicks in the same area earn the user less and less points, according to the formula:\n\n", "index": 1, "text": "\\begin{equation}\nP^+ = \\frac{A}{20}\\left(1 - \\frac{t}{10}\\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"P^{+}=\\frac{A}{20}\\left(1-\\frac{t}{10}\\right)\" display=\"block\"><mrow><msup><mi>P</mi><mo>+</mo></msup><mo>=</mo><mrow><mfrac><mi>A</mi><mn>20</mn></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mi>t</mi><mn>10</mn></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\nwhere $P^-$ is the amount of points subtracted to the current score due to a wrong click, and $t$ is the number of consecutive clicks falling further than 200 pixels from the closest correct object. Penalties prevent users from clicking randomly across the frame and force them to be as more accurate as possible.\n\nHowever, to award points to players we need object segmentation (not necessarily highly accurate) on the input videos to tell whether clicks hit or miss objects. In order to have a reference signal --- {\\it score video segmentation}--- according to which we assign points to players, we use the \\emph{output of the system itself}. When the system is first set up and no data is available yet, the initial video object segmentation is obtained by running a classic background modeling method (\\cite{Barnich2011vibe} in our case); although in the beginning this may not be enough to cover all and only objects in the scene, it still provides an adequate base for setting the game up. After users have started to play, the object segmentation is simply updated based on users' clicks by running the algorithm presented in this paper. It is not strictly necessary for {\\it score video segmentation} to be extremely accurate: scores are only provided for the benefit of users, in order to keep them interested by means of competition.\n\n\\textbf{Click quality.} We also estimate the ``quality'' (in the sense of ``accuracy of clicks with respect to objects'') of the data provided by users while playing the game. Quality scores are computed on a per-level and per-user basis as the fraction of user clicks hitting the objects in the level. We assume that all clicked pixels in a game level by a user gets the same quality score computed as above. \nWe could have computed a global quality score for a single game (i.e., the sequence of levels a user plays before completing the game) or for the user, however different levels may return completely different quality scores even within the same game session, due to each video's scene and object characteristics, so a global score would become too generic to describe individual click quality in a level's context.\n\n\n\n\\subsection{Superclick Extraction}\n\\label{sec:superclick}\n\n\nThe clicks collected through the web game are used to extract information on the location of objects in the scene for each video frame and to carry out a preliminary object segmentation. We pose the problem as a binary segmentation task (background and foreground) by means of the minimization of an energy function defining the cost of a segmentation. Like some of the most recent methods for video object segmentation~\\cite{Giordano2015superpixel,Papazoglou2013fast, eccv}, we use \\emph{superpixels} (computed by SLIC \\cite{Achanta2012slic}) as basic image parts instead of pixels as they provide two main advantages: 1) reducing the number of variables greatly speeds up the minimization algorithm (the number of variables is scaled down by a factor of 30-50, depending on superpixel settings); 2) the initial segmentation provided by superpixels is usually effective in detecting edges, which allows to simply focus on finding the optimal aggregation, taking boundary detection for granted.\n\nThe first processing step, given our target frame $F$, consists of \\emph{superclick extraction}, where a \\emph{superclick} is the intuitive extension of the concept of clicks to superpixels. This step is necessary to be able to pose the problem in terms of superpixels only, by ``converting'' point data (e.g., clicks) to superpixel-oriented ones. Of course, the principle behind this operation is that superpixels containing clicked pixels should be more likely to be marked as superclicks, which are then converted into constraints for MRF optimization. However, clicks are generally noisy, thus other factors, such as click density, click quality (as defined previously), closeness to other clicked superpixels need to be taken into account for superclick identification.\n\nBefore explaining how superclicks are computed, a more basic question is: \\emph{what clicks should we use to analyze a certain frame?} Depending on video frame rate and target speed, users' reaction times may introduce a delay which results in a shift between the frame at which the user clicks on an object and the frame at which the user \\emph{intended} to click. Fig.~\\ref{fig:click_delay} shows a few examples: it is possible to notice that the delay effect is more visible on some videos than others according to mainly objects' speed. Since this issue involves complex biological phenomena~\\cite{reaction_times}, which are out of the scope of the paper, we adopt a simple but effective empirical approach: we assume that all clicks are delayed by a constant number of frames for all videos. \nIn detail, the results in Sect.~\\ref{sec:results} shows that shifting all clicks back by 2 frames (although the optimal delay may vary from 1 to 4 frames which depends on several factors, one above all the video frame rate) represents a good trade-off between accuracy and complexity.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_1.png}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_2.png}\\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_3.png}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_4.png}\n\\caption{Due to users' reaction times, clicks may be delayed with respect to the ``intended'' frame. It is possible to notice that this phenomenon may be more or less evident even within the same image, depending not only on the user but also on the objects in the scene.}\n\\label{fig:click_delay}\n\\end{figure}\n\nLet $C = \\left\\{ c_1, c_2, \\dots, c_{n_C}\\right\\} = \\left\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_{n_C}, y_{n_C})\\right\\}$ be the players' clicks for frame $F$, with corresponding quality scores $Q = \\left\\{q_{c_1}, q_{c_2}, \\dots, q_{c_{N_C}}\\right\\}$ (each click gets the quality score assigned to the user who did it on a per-level basis). We define a graph-representable energy function~\\cite{Kolmogorov2004what} over the set of $F$'s superpixels $S = \\left\\{s_1, s_2, \\dots, s_{n_S} \\right\\}$, with a cost function able to model the ``clickedness'' of each superpixel independently, and at the same time, to enforce constraints on visual smoothness and click continuity. \nOur main assumptions are:\n\\begin{enumerate}\n \\item Superpixels containing a large number of clicks should be marked as superclicks (and vice versa), i.e., they can be seen as hard constraints for segmentation. \n \\item Clicked pixels should be weighted by the relative quality when evaluating their contribution to a superclick.\n \\item Unclicked superpixels which are close to clicked and visually-similar superclicks should be marked as superclicks as well, since they are likely to belong to the same object.\n \\item Isolated clicked superpixels (even if in small groups) should be ignored as being likely noise.\n\\end{enumerate}\nTranslating these assumptions into energy potentials, we obtain the following cost function for energy optimization:\n\n", "itemtype": "equation", "pos": 19119, "prevtext": "\nwhere $P^+$ is number of points earned for a correct click, $A$ is the area in pixels of the clicked object, and $t$ is the number of consecutive clicks within a 30$\\times$30 pixels region. In practice, $\\frac{A}{20}$ is the maximum score awarded for a click on a certain object, but this score is progressively reduced if the user keeps clicking on the same spot: when salient objects are in the scene, this reduction forces users to vary their clicking pattern to get more points, while at the same time helps to provide data on as many objects as possible. Conversely, users are subtracted points if they click too far from the objects in the visualized frame; the penalty is computed as:\n\n", "index": 3, "text": "\\begin{equation}\nP^- = 20t\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"P^{-}=20t\" display=\"block\"><mrow><msup><mi>P</mi><mo>-</mo></msup><mo>=</mo><mrow><mn>20</mn><mo>\u2062</mo><mi>t</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\nwhere $\\mathcal{L} = \\left\\{ l_{s_1}, l_{s_2}, \\dots, l_{s_{n_S}}\\right\\}$ is the superclick label assignment ($l_{s_i}$ is the binary superclick label for superpixel $s_i$), $\\mathcal{N}(S)$ is the set of pairs of neighbor superpixels (that is, having part of boundary in common; we will also use the notation $\\mathcal{N}(s)$ to denote the set of neighbors of the single superpixel $s$), and $\\alpha_1$ is a weighing factor.\n\nUnary potential $V_1$ models whether superpixel $s$ is likely to be a superclick or not. This ``likeliness'' depends on the number and quality of clicks inside the superpixel's region and on the vicinity to clicked superpixels\\footnote{The reader might think that ``vicinity to clicked superpixels'' should be modeled as a pairwise potential, rather than unary. In fact, it should be modeled as unary because it is not an indication of whether two elements should be assigned the same label (which is what pairwise potentials represent); instead, it uses local information to indicate whether that item, \\emph{individually}, is more likely to be assigned to a specific label ($1$ for ``superclick'' or $0$ for ``not superclick'')}. Therefore, $V_1$ is given by two contributions:\n\\begin{itemize}\n \\item \\textbf{\\emph{Clickedness} $K_s$}: the more (high-quality) clicks a superpixel has received, the more it is likely to be a good candidate superclick. The clickedness score $K_s$ for superpixel $s$ is:\n \n", "itemtype": "equation", "pos": 26285, "prevtext": "\nwhere $P^-$ is the amount of points subtracted to the current score due to a wrong click, and $t$ is the number of consecutive clicks falling further than 200 pixels from the closest correct object. Penalties prevent users from clicking randomly across the frame and force them to be as more accurate as possible.\n\nHowever, to award points to players we need object segmentation (not necessarily highly accurate) on the input videos to tell whether clicks hit or miss objects. In order to have a reference signal --- {\\it score video segmentation}--- according to which we assign points to players, we use the \\emph{output of the system itself}. When the system is first set up and no data is available yet, the initial video object segmentation is obtained by running a classic background modeling method (\\cite{Barnich2011vibe} in our case); although in the beginning this may not be enough to cover all and only objects in the scene, it still provides an adequate base for setting the game up. After users have started to play, the object segmentation is simply updated based on users' clicks by running the algorithm presented in this paper. It is not strictly necessary for {\\it score video segmentation} to be extremely accurate: scores are only provided for the benefit of users, in order to keep them interested by means of competition.\n\n\\textbf{Click quality.} We also estimate the ``quality'' (in the sense of ``accuracy of clicks with respect to objects'') of the data provided by users while playing the game. Quality scores are computed on a per-level and per-user basis as the fraction of user clicks hitting the objects in the level. We assume that all clicked pixels in a game level by a user gets the same quality score computed as above. \nWe could have computed a global quality score for a single game (i.e., the sequence of levels a user plays before completing the game) or for the user, however different levels may return completely different quality scores even within the same game session, due to each video's scene and object characteristics, so a global score would become too generic to describe individual click quality in a level's context.\n\n\n\n\\subsection{Superclick Extraction}\n\\label{sec:superclick}\n\n\nThe clicks collected through the web game are used to extract information on the location of objects in the scene for each video frame and to carry out a preliminary object segmentation. We pose the problem as a binary segmentation task (background and foreground) by means of the minimization of an energy function defining the cost of a segmentation. Like some of the most recent methods for video object segmentation~\\cite{Giordano2015superpixel,Papazoglou2013fast, eccv}, we use \\emph{superpixels} (computed by SLIC \\cite{Achanta2012slic}) as basic image parts instead of pixels as they provide two main advantages: 1) reducing the number of variables greatly speeds up the minimization algorithm (the number of variables is scaled down by a factor of 30-50, depending on superpixel settings); 2) the initial segmentation provided by superpixels is usually effective in detecting edges, which allows to simply focus on finding the optimal aggregation, taking boundary detection for granted.\n\nThe first processing step, given our target frame $F$, consists of \\emph{superclick extraction}, where a \\emph{superclick} is the intuitive extension of the concept of clicks to superpixels. This step is necessary to be able to pose the problem in terms of superpixels only, by ``converting'' point data (e.g., clicks) to superpixel-oriented ones. Of course, the principle behind this operation is that superpixels containing clicked pixels should be more likely to be marked as superclicks, which are then converted into constraints for MRF optimization. However, clicks are generally noisy, thus other factors, such as click density, click quality (as defined previously), closeness to other clicked superpixels need to be taken into account for superclick identification.\n\nBefore explaining how superclicks are computed, a more basic question is: \\emph{what clicks should we use to analyze a certain frame?} Depending on video frame rate and target speed, users' reaction times may introduce a delay which results in a shift between the frame at which the user clicks on an object and the frame at which the user \\emph{intended} to click. Fig.~\\ref{fig:click_delay} shows a few examples: it is possible to notice that the delay effect is more visible on some videos than others according to mainly objects' speed. Since this issue involves complex biological phenomena~\\cite{reaction_times}, which are out of the scope of the paper, we adopt a simple but effective empirical approach: we assume that all clicks are delayed by a constant number of frames for all videos. \nIn detail, the results in Sect.~\\ref{sec:results} shows that shifting all clicks back by 2 frames (although the optimal delay may vary from 1 to 4 frames which depends on several factors, one above all the video frame rate) represents a good trade-off between accuracy and complexity.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_1.png}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_2.png}\\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_3.png}\n\\includegraphics[width=.24\\textwidth]{figures/click_delay_4.png}\n\\caption{Due to users' reaction times, clicks may be delayed with respect to the ``intended'' frame. It is possible to notice that this phenomenon may be more or less evident even within the same image, depending not only on the user but also on the objects in the scene.}\n\\label{fig:click_delay}\n\\end{figure}\n\nLet $C = \\left\\{ c_1, c_2, \\dots, c_{n_C}\\right\\} = \\left\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_{n_C}, y_{n_C})\\right\\}$ be the players' clicks for frame $F$, with corresponding quality scores $Q = \\left\\{q_{c_1}, q_{c_2}, \\dots, q_{c_{N_C}}\\right\\}$ (each click gets the quality score assigned to the user who did it on a per-level basis). We define a graph-representable energy function~\\cite{Kolmogorov2004what} over the set of $F$'s superpixels $S = \\left\\{s_1, s_2, \\dots, s_{n_S} \\right\\}$, with a cost function able to model the ``clickedness'' of each superpixel independently, and at the same time, to enforce constraints on visual smoothness and click continuity. \nOur main assumptions are:\n\\begin{enumerate}\n \\item Superpixels containing a large number of clicks should be marked as superclicks (and vice versa), i.e., they can be seen as hard constraints for segmentation. \n \\item Clicked pixels should be weighted by the relative quality when evaluating their contribution to a superclick.\n \\item Unclicked superpixels which are close to clicked and visually-similar superclicks should be marked as superclicks as well, since they are likely to belong to the same object.\n \\item Isolated clicked superpixels (even if in small groups) should be ignored as being likely noise.\n\\end{enumerate}\nTranslating these assumptions into energy potentials, we obtain the following cost function for energy optimization:\n\n", "index": 5, "text": "\\begin{equation}\n E_1(\\mathcal{L}) = \\alpha_1 \\sum_{s \\in S} V_1(s, l_s, C) + \\sum_{(s_1,s_2) \\in \\mathcal{N}(S)} V_2(s_1,s_2, l_{s_1}, l_{s_2})\n \\label{eq:superclick_energy}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"E_{1}(\\mathcal{L})=\\alpha_{1}\\sum_{s\\in S}V_{1}(s,l_{s},C)+\\sum_{(s_{1},s_{2})%&#10;\\in\\mathcal{N}(S)}V_{2}(s_{1},s_{2},l_{s_{1}},l_{s_{2}})\" display=\"block\"><mrow><mrow><msub><mi>E</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>\u03b1</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><msub><mi>V</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>l</mi><mi>s</mi></msub><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><msub><mi>V</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n where $C \\cap s$ is the set of clicks hitting superpixel $s$ and $\\left|\\cdot\\right|$ is set cardinality. The first (unreduced) version explains more clearly what this formula is meant for: Eq. term (\\ref{eq:superclick_energy_clickedness}a) indicates how many clicks, superpixel $s$ contains with respect to the superpixel containing most clicks in the processed frame; Eq. term (\\ref{eq:superclick_energy_clickedness}b) is, instead, the average quality of clicks inside $s$, and encodes quality information in the score. The way this score is computed thus addresses items 1 and 2 of the above design principles.\n \\item \\textbf{Proximity to clicked superpixels $V_s$}: if $s$ has not received many clicks but is close to superpixels which did, we might want to take it into consideration as a potential superclick.  \n Of course, being close to clicked superpixels by itself is not enough: any superpixel just outside an object's boundary satisfies this requirement; this issue will be addressed by the pairwise potential $V_2$.\\\\\n Our proximity score $V_s$ is computed as the fraction of neighbor superpixels with clickedness score $K_{s_n} > 0.5$, with $s_n \\in \\mathcal{N}(s)$:\n \n", "itemtype": "equation", "pos": 27908, "prevtext": "\nwhere $\\mathcal{L} = \\left\\{ l_{s_1}, l_{s_2}, \\dots, l_{s_{n_S}}\\right\\}$ is the superclick label assignment ($l_{s_i}$ is the binary superclick label for superpixel $s_i$), $\\mathcal{N}(S)$ is the set of pairs of neighbor superpixels (that is, having part of boundary in common; we will also use the notation $\\mathcal{N}(s)$ to denote the set of neighbors of the single superpixel $s$), and $\\alpha_1$ is a weighing factor.\n\nUnary potential $V_1$ models whether superpixel $s$ is likely to be a superclick or not. This ``likeliness'' depends on the number and quality of clicks inside the superpixel's region and on the vicinity to clicked superpixels\\footnote{The reader might think that ``vicinity to clicked superpixels'' should be modeled as a pairwise potential, rather than unary. In fact, it should be modeled as unary because it is not an indication of whether two elements should be assigned the same label (which is what pairwise potentials represent); instead, it uses local information to indicate whether that item, \\emph{individually}, is more likely to be assigned to a specific label ($1$ for ``superclick'' or $0$ for ``not superclick'')}. Therefore, $V_1$ is given by two contributions:\n\\begin{itemize}\n \\item \\textbf{\\emph{Clickedness} $K_s$}: the more (high-quality) clicks a superpixel has received, the more it is likely to be a good candidate superclick. The clickedness score $K_s$ for superpixel $s$ is:\n \n", "index": 7, "text": "\\begin{equation}\n  K_s = \\underbrace{\\frac{\\left|C \\cap s\\right|}{\\max\\limits_{t \\in S} \\left| C \\cap t\\right|}}_\\text{(\\ref{eq:superclick_energy_clickedness}a)} \\underbrace{\\frac{1}{\\left|C \\cap s\\right|} \\sum_{c \\in C \\cap s} q_c}_\\text{(\\ref{eq:superclick_energy_clickedness}b)} = \\frac{\\sum\\limits_{c \\in C \\cap s} q_c}{\\max\\limits_{t \\in S} \\left|C \\cap t\\right|}\n  \\label{eq:superclick_energy_clickedness}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"K_{s}=\\underbrace{\\frac{\\left|C\\cap s\\right|}{\\max\\limits_{t\\in S}\\left|C\\cap t%&#10;\\right|}}_{\\text{(\\ref{eq:superclick_energy_clickedness}a)}}\\underbrace{\\frac{%&#10;1}{\\left|C\\cap s\\right|}\\sum_{c\\in C\\cap s}q_{c}}_{\\text{(\\ref{eq:superclick_%&#10;energy_clickedness}b)}}=\\frac{\\sum\\limits_{c\\in C\\cap s}q_{c}}{\\max\\limits_{t%&#10;\\in S}\\left|C\\cap t\\right|}\" display=\"block\"><mrow><msub><mi>K</mi><mi>s</mi></msub><mo>=</mo><mrow><munder><munder accentunder=\"true\"><mfrac><mrow><mo movablelimits=\"false\">|</mo><mrow><mi>C</mi><mo movablelimits=\"false\">\u2229</mo><mi>s</mi></mrow><mo movablelimits=\"false\">|</mo></mrow><mrow><munder><mi>max</mi><mrow><mi>t</mi><mo movablelimits=\"false\">\u2208</mo><mi>S</mi></mrow></munder><mo movablelimits=\"false\">\u2061</mo><mrow><mo movablelimits=\"false\">|</mo><mrow><mi>C</mi><mo movablelimits=\"false\">\u2229</mo><mi>t</mi></mrow><mo movablelimits=\"false\">|</mo></mrow></mrow></mfrac><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mtext>(</mtext><mtext><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_ref_self\" style=\"font-size:70%;\"><span class=\"ltx_text ltx_ref_tag\">4</span></span></mtext><mtext>a)</mtext></mrow></munder><mo>\u2062</mo><munder><munder accentunder=\"true\"><mrow><mfrac><mn>1</mn><mrow><mo movablelimits=\"false\">|</mo><mrow><mi>C</mi><mo movablelimits=\"false\">\u2229</mo><mi>s</mi></mrow><mo movablelimits=\"false\">|</mo></mrow></mfrac><mo movablelimits=\"false\">\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>c</mi><mo movablelimits=\"false\">\u2208</mo><mrow><mi>C</mi><mo movablelimits=\"false\">\u2229</mo><mi>s</mi></mrow></mrow></munder><msub><mi>q</mi><mi>c</mi></msub></mrow></mrow><mo movablelimits=\"false\">\u23df</mo></munder><mrow><mtext>(</mtext><mtext><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_ref_self\" style=\"font-size:70%;\"><span class=\"ltx_text ltx_ref_tag\">4</span></span></mtext><mtext>b)</mtext></mrow></munder></mrow><mo>=</mo><mfrac><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>c</mi><mo>\u2208</mo><mrow><mi>C</mi><mo>\u2229</mo><mi>s</mi></mrow></mrow></munder><msub><mi>q</mi><mi>c</mi></msub></mrow><mrow><munder><mi>max</mi><mrow><mi>t</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mo>\u2061</mo><mrow><mo>|</mo><mrow><mi>C</mi><mo>\u2229</mo><mi>t</mi></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n \n Analogously, if $s$ gets enough clicks but is isolated, $V_s$ will be low and  $K_s$ won't suffice to label it as a superclick. Thus, $V_s$ balances items 3 and 4 of our design principles.\n \n \\item \\textbf{Unclicked regularizer}: the point of introducing the $V_s$ score is to allow a superpixel with few or no clicks to be labeled as superclick if its neighborhood hints that it should; however, if an unclicked superpixel is not adjacent to any clicked superpixels, its $V_1$ potential is zero, which is something we want to avoid. Consider, for example, the case of an object consisting of a large uniform region with a non-uniform users' click distribution (which is actually often the case, as users tend to click at the center of objects): by setting unclicked superpixels to a low (but not null) potential, we allow labels to ``spread'' from superclicks (as per item 3 of our design principles above)---as long as uniformity requirements, defined by potential $V_2$, apply.\\\\\n For this reason, we add a constant $U_s$ term to the $V_1$ potential, which should be small enough not to ``push'' too much toward the ``superclick'' label (since clickedness and vicinity clues suggest it should not be), but not so small that it cannot ever be labeled as such.\\\\\n \n\\end{itemize}\n\nThe definitions of $K_s$, $V_s$ and $U_s$ have been chosen so that the sum of those terms (clipped to 1 if necessary) can be interpreted as the probability that superpixel $s$ belongs to class ``superclick'', $P_{s,1} = P(l_s = 1 | C,S) = \\min\\left(K_s + V_s + U_s, 1\\right)$. Similarly, the complementary probability $P_{s,0} = P(l_s = 0 | C,S) = 1 - P_{s,1}$ is the probability that $s$ is ``not a superclick''. In the energy function, $V_1$ is meant to represent the cost of assigning a certain label to each superpixel: such costs can be computed as the negative log-likelihood of the two probabilities above:\n\n", "itemtype": "equation", "pos": 29519, "prevtext": "\n where $C \\cap s$ is the set of clicks hitting superpixel $s$ and $\\left|\\cdot\\right|$ is set cardinality. The first (unreduced) version explains more clearly what this formula is meant for: Eq. term (\\ref{eq:superclick_energy_clickedness}a) indicates how many clicks, superpixel $s$ contains with respect to the superpixel containing most clicks in the processed frame; Eq. term (\\ref{eq:superclick_energy_clickedness}b) is, instead, the average quality of clicks inside $s$, and encodes quality information in the score. The way this score is computed thus addresses items 1 and 2 of the above design principles.\n \\item \\textbf{Proximity to clicked superpixels $V_s$}: if $s$ has not received many clicks but is close to superpixels which did, we might want to take it into consideration as a potential superclick.  \n Of course, being close to clicked superpixels by itself is not enough: any superpixel just outside an object's boundary satisfies this requirement; this issue will be addressed by the pairwise potential $V_2$.\\\\\n Our proximity score $V_s$ is computed as the fraction of neighbor superpixels with clickedness score $K_{s_n} > 0.5$, with $s_n \\in \\mathcal{N}(s)$:\n \n", "index": 9, "text": "\\begin{equation}\n  V_s = \\frac{\\left| \\left\\{ s_n \\in \\mathcal{N}(s) : K_{s_n} > 0.5 \\right\\} \\right|}{\\left|\\mathcal{N}(s)\\right|}\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"V_{s}=\\frac{\\left|\\left\\{s_{n}\\in\\mathcal{N}(s):K_{s_{n}}&gt;0.5\\right\\}\\right|}{%&#10;\\left|\\mathcal{N}(s)\\right|}\" display=\"block\"><mrow><msub><mi>V</mi><mi>s</mi></msub><mo>=</mo><mfrac><mrow><mo>|</mo><mrow><mo>{</mo><mrow><msub><mi>s</mi><mi>n</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><msub><mi>K</mi><msub><mi>s</mi><mi>n</mi></msub></msub><mo>&gt;</mo><mn>0.5</mn></mrow><mo>}</mo></mrow><mo>|</mo></mrow><mrow><mo>|</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>|</mo></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\nPairwise potential $V_2$ is the cost of assigning different labels to two adjacent superpixels $s_1$ and $s_2$: ideally, it should be large for ``similar'' superpixels (so that they are assigned the same label) and small for superpixels for which no evidence exists that they should belong to the same class. Although in general this function could depend on the specific labels being assigned (so that, for example, the cost of assigning labels $(l_{s_1} = 1, l_{s_2} = 0)$ might be different than the cost of assigning labels $(l_{s_1} = 0, l_{s_2} = 1)$), in our case we focus only on estimating the optimal separation point between the ``superclick''/``non-superclick'' regions, based on visual similarity.\\\\\nTherefore, potential $V_2$ is simply expressed as follows:\n\n", "itemtype": "equation", "pos": 31565, "prevtext": "\n \n Analogously, if $s$ gets enough clicks but is isolated, $V_s$ will be low and  $K_s$ won't suffice to label it as a superclick. Thus, $V_s$ balances items 3 and 4 of our design principles.\n \n \\item \\textbf{Unclicked regularizer}: the point of introducing the $V_s$ score is to allow a superpixel with few or no clicks to be labeled as superclick if its neighborhood hints that it should; however, if an unclicked superpixel is not adjacent to any clicked superpixels, its $V_1$ potential is zero, which is something we want to avoid. Consider, for example, the case of an object consisting of a large uniform region with a non-uniform users' click distribution (which is actually often the case, as users tend to click at the center of objects): by setting unclicked superpixels to a low (but not null) potential, we allow labels to ``spread'' from superclicks (as per item 3 of our design principles above)---as long as uniformity requirements, defined by potential $V_2$, apply.\\\\\n For this reason, we add a constant $U_s$ term to the $V_1$ potential, which should be small enough not to ``push'' too much toward the ``superclick'' label (since clickedness and vicinity clues suggest it should not be), but not so small that it cannot ever be labeled as such.\\\\\n \n\\end{itemize}\n\nThe definitions of $K_s$, $V_s$ and $U_s$ have been chosen so that the sum of those terms (clipped to 1 if necessary) can be interpreted as the probability that superpixel $s$ belongs to class ``superclick'', $P_{s,1} = P(l_s = 1 | C,S) = \\min\\left(K_s + V_s + U_s, 1\\right)$. Similarly, the complementary probability $P_{s,0} = P(l_s = 0 | C,S) = 1 - P_{s,1}$ is the probability that $s$ is ``not a superclick''. In the energy function, $V_1$ is meant to represent the cost of assigning a certain label to each superpixel: such costs can be computed as the negative log-likelihood of the two probabilities above:\n\n", "index": 11, "text": "\\begin{equation}\n V_1(s, l_s, C) =\n \\begin{cases}\n  -\\log P_{s,1} & \\text{if}~l_s = 1 \\\\\n  -\\log P_{s,0} & \\text{if}~l_s = 0\n \\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"V_{1}(s,l_{s},C)=\\begin{cases}-\\log P_{s,1}&amp;\\text{if}~{}l_{s}=1\\\\&#10;-\\log P_{s,0}&amp;\\text{if}~{}l_{s}=0\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>V</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>l</mi><mi>s</mi></msub><mo>,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>P</mi><mrow><mi>s</mi><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+3.3pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><msub><mi>l</mi><mi>s</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>P</mi><mrow><mi>s</mi><mo>,</mo><mn>0</mn></mrow></msub></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+3.3pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><msub><mi>l</mi><mi>s</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\nwhere $\\chi^2(\\cdot,\\cdot)$ is the Chi-square distance, $H_{s_i}$ is the RGB color histogram of superpixel $s_i$, $\\beta_1$ is a constant, and $\\mathcal{I}$ is an indicator function which returns 1 if the arguments is true, and 0 otherwise (this ensures that $V_2$ is a submodular function, thus making the whole energy function graph-representable~\\cite{Kolmogorov2004what}). Using a simple similarity measure such as the color histogram has a twofold justification: 1) by construction, superpixels have very little internal structure, so using more complex descriptors is unnecessary; 2) since the function has to be evaluated for all pairs of adjacent superpixels, it is important to perform as efficient operations as possible, in order to keep computation times reasonable.\n\nOnce $E_1(\\mathcal{L})$ has been minimized by means of graph cut, the extracted superclicks already provide a good approximated segmentation of the objects of interest in the scene, as shown by the examples in Fig.~\\ref{fig:superclicks}. Nevertheless, output images at this stage can show segmentation errors, e.g., holes, oversegmentations, etc, and further processing by taking into account motion information is carried out to refine the obtained segmentation masks. \n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_04_f_265_notime.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_masked.jpg} \\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_masked.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_masked.jpg}\\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_notime.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_masked.jpg}\n\n\n\n\n\n\n\\caption{Output examples for superclick identification: blue dots are users' clicks while green regions show the yielded segmentation masks. Segmentation refinement is carried out by including temporal constraints.}\n\\label{fig:superclicks}\n\\end{figure}\n\n\n\n\n\n\\subsection{Temporal Smoothing}\n\\label{sec:temporal}\n\nThe superclick extraction step turns a set of noisy clicks into a set of spatial coherent superclicks per frame, but ignores any temporal information which, instead, is necessary in video streams. \nTherefore, the next step for segmentation refinement consists in exploiting the \\emph{temporal consistency} between consecutive frames to ``transfer'' labels across segmentations. \nThe idea is that if a set of consecutive (in time) segmentations all mark a certain object as ``interesting'', then it is likely that they are correct; similarly, if no (or only few) segmentations include that object, it is probably safer to ignore it in the final output, especially if it is relatively isolated from other potential foreground objects. \nTwo issues arise when trying to implement the above criterion: first, superclick segmentations are defined in terms of superpixel labels, and superpixel segmentation is not consistent in presence of motion; second, objects in a video typically move, thus the notion of ``a certain object'' across several frames implies the employment of a object/point tracking method. \n\nOur approach addresses both issues: 1) we define a \\emph{temporal linking} between superclicks by extending the energy function, employed for superclick extraction, thus taking into account visual similarity between spatio-temporal regions; 2) we employ optical flow \\cite{Liu2009opticalflow} to estimate where superpixels in frame  $t$ may have moved in frame $t+1$: in practice, we introduce pairwise potentials on all pairs of superpixels $\\left\\{s_t, s_{t+1}\\right\\}$ such that $s_t$ contains at least one pixel $p_t$ whose projection $p^{v_{p_t}}_{t\\rightarrow {t+1}}=p_t+v_{p_t}$ into frame $t+1$ under the motion vector $v_{p_t}$ (i.e., $v_{p_t}$ is the motion vector computed between frame $t$ and frame $t+1$ for location $p_t$) is part of superpixel $s_{t+1}$ in frame $t+1$. Of course, it is unlikely that each superpixel will appear in only one such link, which allows to better ``explore'' the space around the estimated motion area, thus reducing the amount of error due to the optical flow and performing a more comprehensive analysis on the surrounding superpixels.\\\\\nIn the definition of the cost function employed for the temporal smoothing across frames $t-T$ and $t+T$, where $t$ is the current processed frame and $T$ is a constant which affects the number of frames involved in the temporal smoothing (i.e., $2T+1$), we assume to have identified superclicks for all the involved frames. In particular, we will refer to the same quantities as defined in Section~\\ref{sec:superclick} and add an apex relative to the frame they refer to: for example, $l_s^t$ is the superclick label for superpixel $s$ in frame $t$, $S^{t+1}$ is the set of superpixels in frame $t+1$, and so on. The output label set will be identified by $\\mathcal{L}$, and each label by $l_s$, without the temporal apex and they refer to the segmentation of the current processed frame. We can now introduce the energy function used for the final segmentation:\n\n\n", "itemtype": "equation", "pos": 32491, "prevtext": "\n\nPairwise potential $V_2$ is the cost of assigning different labels to two adjacent superpixels $s_1$ and $s_2$: ideally, it should be large for ``similar'' superpixels (so that they are assigned the same label) and small for superpixels for which no evidence exists that they should belong to the same class. Although in general this function could depend on the specific labels being assigned (so that, for example, the cost of assigning labels $(l_{s_1} = 1, l_{s_2} = 0)$ might be different than the cost of assigning labels $(l_{s_1} = 0, l_{s_2} = 1)$), in our case we focus only on estimating the optimal separation point between the ``superclick''/``non-superclick'' regions, based on visual similarity.\\\\\nTherefore, potential $V_2$ is simply expressed as follows:\n\n", "index": 13, "text": "\\begin{equation}\n V_2(s_1,s_2, l_{s_1}, l_{s_2}) = \\text{exp}\\left[-\\beta_1 \\chi^2(H_{s_1}, H_{s_2})\\right]\\mathcal{I}(l_{s_1} \\neq l_{s_2})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"V_{2}(s_{1},s_{2},l_{s_{1}},l_{s_{2}})=\\text{exp}\\left[-\\beta_{1}\\chi^{2}(H_{s%&#10;_{1}},H_{s_{2}})\\right]\\mathcal{I}(l_{s_{1}}\\neq l_{s_{2}})\" display=\"block\"><mrow><msub><mi>V</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mtext>exp</mtext><mrow><mo>[</mo><mo>-</mo><msub><mi>\u03b2</mi><mn>1</mn></msub><msup><mi>\u03c7</mi><mn>2</mn></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>H</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>H</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow><mi class=\"ltx_font_mathcaligraphic\">\u2110</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>\u2260</mo><msub><mi>l</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\n\n\n\n\n\nThe first two lines of the cost function includes single-frame potentials, which consist of, respectively, unary potentials for each identified superpixel (first line) and pairwise potentials (second line) for each pair of superpixels belonging to the same frame. The last term (third line) enforces temporal smoothing, and consists of pairwise potentials computed over the set $\\mathcal{N}_T(\\cup_{\\tau = t-T}^{t+T} S^\\tau)$, which represents all pairs of superpixels (from all the frames in the considered time interval) satisfying the ``temporal linking'' described above, i.e., such that the two superpixels in each pair belong to temporally consecutive frames, and that at least one pixel belonging to one of them is projected onto the other by means of optical flow.\n\nSimilarly to $V_1$ (defined in Sect.~\\ref{sec:superclick} ), unary potential $W_1$ models whether superpixel $s$ is more likely to be assigned to background or foreground \\emph{per se}. In this stage, we simply assign a constant value to the potential depending on whether it had been identified, at the previous stage (see Sect.~\\ref{sec:superclick}), as a superclick or not (i.e., depending on $l_s^\\tau$).  In detail, given superpixel $s$, we set corresponding \\emph{foreground cost} and \\emph{background cost}; the value of each cost depends on $l_s^\\tau$: if $s$ was labeled as a superclick, we expect it to be more likely that it is foreground, so the background cost should be higher, and vice versa. $W_1$ is therefore computed as follows:\n\n", "itemtype": "equation", "pos": 37832, "prevtext": "\nwhere $\\chi^2(\\cdot,\\cdot)$ is the Chi-square distance, $H_{s_i}$ is the RGB color histogram of superpixel $s_i$, $\\beta_1$ is a constant, and $\\mathcal{I}$ is an indicator function which returns 1 if the arguments is true, and 0 otherwise (this ensures that $V_2$ is a submodular function, thus making the whole energy function graph-representable~\\cite{Kolmogorov2004what}). Using a simple similarity measure such as the color histogram has a twofold justification: 1) by construction, superpixels have very little internal structure, so using more complex descriptors is unnecessary; 2) since the function has to be evaluated for all pairs of adjacent superpixels, it is important to perform as efficient operations as possible, in order to keep computation times reasonable.\n\nOnce $E_1(\\mathcal{L})$ has been minimized by means of graph cut, the extracted superclicks already provide a good approximated segmentation of the objects of interest in the scene, as shown by the examples in Fig.~\\ref{fig:superclicks}. Nevertheless, output images at this stage can show segmentation errors, e.g., holes, oversegmentations, etc, and further processing by taking into account motion information is carried out to refine the obtained segmentation masks. \n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_04_f_265_notime.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_masked.jpg} \\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_masked.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_masked.jpg}\\\\ \\vspace{0.2cm}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_notime.jpg}\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_masked.jpg}\n\n\n\n\n\n\n\\caption{Output examples for superclick identification: blue dots are users' clicks while green regions show the yielded segmentation masks. Segmentation refinement is carried out by including temporal constraints.}\n\\label{fig:superclicks}\n\\end{figure}\n\n\n\n\n\n\\subsection{Temporal Smoothing}\n\\label{sec:temporal}\n\nThe superclick extraction step turns a set of noisy clicks into a set of spatial coherent superclicks per frame, but ignores any temporal information which, instead, is necessary in video streams. \nTherefore, the next step for segmentation refinement consists in exploiting the \\emph{temporal consistency} between consecutive frames to ``transfer'' labels across segmentations. \nThe idea is that if a set of consecutive (in time) segmentations all mark a certain object as ``interesting'', then it is likely that they are correct; similarly, if no (or only few) segmentations include that object, it is probably safer to ignore it in the final output, especially if it is relatively isolated from other potential foreground objects. \nTwo issues arise when trying to implement the above criterion: first, superclick segmentations are defined in terms of superpixel labels, and superpixel segmentation is not consistent in presence of motion; second, objects in a video typically move, thus the notion of ``a certain object'' across several frames implies the employment of a object/point tracking method. \n\nOur approach addresses both issues: 1) we define a \\emph{temporal linking} between superclicks by extending the energy function, employed for superclick extraction, thus taking into account visual similarity between spatio-temporal regions; 2) we employ optical flow \\cite{Liu2009opticalflow} to estimate where superpixels in frame  $t$ may have moved in frame $t+1$: in practice, we introduce pairwise potentials on all pairs of superpixels $\\left\\{s_t, s_{t+1}\\right\\}$ such that $s_t$ contains at least one pixel $p_t$ whose projection $p^{v_{p_t}}_{t\\rightarrow {t+1}}=p_t+v_{p_t}$ into frame $t+1$ under the motion vector $v_{p_t}$ (i.e., $v_{p_t}$ is the motion vector computed between frame $t$ and frame $t+1$ for location $p_t$) is part of superpixel $s_{t+1}$ in frame $t+1$. Of course, it is unlikely that each superpixel will appear in only one such link, which allows to better ``explore'' the space around the estimated motion area, thus reducing the amount of error due to the optical flow and performing a more comprehensive analysis on the surrounding superpixels.\\\\\nIn the definition of the cost function employed for the temporal smoothing across frames $t-T$ and $t+T$, where $t$ is the current processed frame and $T$ is a constant which affects the number of frames involved in the temporal smoothing (i.e., $2T+1$), we assume to have identified superclicks for all the involved frames. In particular, we will refer to the same quantities as defined in Section~\\ref{sec:superclick} and add an apex relative to the frame they refer to: for example, $l_s^t$ is the superclick label for superpixel $s$ in frame $t$, $S^{t+1}$ is the set of superpixels in frame $t+1$, and so on. The output label set will be identified by $\\mathcal{L}$, and each label by $l_s$, without the temporal apex and they refer to the segmentation of the current processed frame. We can now introduce the energy function used for the final segmentation:\n\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{aligned}\n E_2(\\mathcal{L}) = &  \\sum_{\\tau = t-T}^{t+T} \\left[ \\alpha_2 \\sum_{s \\in S^\\tau} W_1(s, l_s, l_s^\\tau) \\right] +\\\\\n + & \\sum_{\\tau = t-T}^{t+T} \\left[\\sum_{(s_1,s_2) \\in \\mathcal{N}(S^\\tau)} V_2(s_1,s_2, l_{s_1}, l_{s_2}) \\right] + \\\\\n+ & \\sum_{(s_1,s_2) \\in \\mathcal{N}_T(\\cup_{\\tau = t-T}^{t+T} S^\\tau)} V_2(s_1,s_2, l_{s_1}, l_{s_2})\n\\end{aligned}\n\\label{eq:time_energy}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle E_{2}(\\mathcal{L})=\" display=\"inline\"><mrow><mrow><msub><mi>E</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{\\tau=t-T}^{t+T}\\left[\\alpha_{2}\\sum_{s\\in S^{\\tau}}W_{1}(s,%&#10;l_{s},l_{s}^{\\tau})\\right]+\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\u03c4</mi><mo>=</mo><mrow><mi>t</mi><mo>-</mo><mi>T</mi></mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>T</mi></mrow></munderover></mstyle><mrow><mo>[</mo><mrow><msub><mi>\u03b1</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><msup><mi>S</mi><mi>\u03c4</mi></msup></mrow></munder></mstyle><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>l</mi><mi>s</mi></msub><mo>,</mo><msubsup><mi>l</mi><mi>s</mi><mi>\u03c4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\" display=\"inline\"><mo>+</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{\\tau=t-T}^{t+T}\\left[\\sum_{(s_{1},s_{2})\\in\\mathcal{N}(S^{%&#10;\\tau})}V_{2}(s_{1},s_{2},l_{s_{1}},l_{s_{2}})\\right]+\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\u03c4</mi><mo>=</mo><mrow><mi>t</mi><mo>-</mo><mi>T</mi></mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>T</mi></mrow></munderover></mstyle><mrow><mo>[</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>S</mi><mi>\u03c4</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><msub><mi>V</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\" display=\"inline\"><mo>+</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{(s_{1},s_{2})\\in\\mathcal{N}_{T}(\\cup_{\\tau=t-T}^{t+T}S^{%&#10;\\tau})}V_{2}(s_{1},s_{2},l_{s_{1}},l_{s_{2}})\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>T</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo>\u222a</mo><mrow><mi>\u03c4</mi><mo>=</mo><mrow><mi>t</mi><mo>-</mo><mi>T</mi></mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>T</mi></mrow></msubsup><msup><mi>S</mi><mi>\u03c4</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><msub><mi>V</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>l</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\nwith $\\gamma_1 < \\gamma_2$.\n\nPairwise potential $V_2$ is defined as in Section~\\ref{sec:superclick}, but in the last term (third line of Formula~\\ref{eq:time_energy}) of $E_2$ we employ it to evaluate the similarity not only between adjacent superpixels in the same frame, but also ``temporally-adjacent'' (according to $\\mathcal{N}_T$) superpixels in consecutive frames. In order to deal with errors in optical flow computation, we do not simply assign a constant based on the presence or absence of a temporal link between two superpixels in consecutive frames, but also verify that they are visually similar and in fact refer to the same object/region in both frames. Thus, we manage to enforce the criteria according to which superpixels overlaying the same region across different frames should all be assigned the same label.\n\n\nBoth $E_1$ and $E_2$ are binary pairwise energy functions with submodular pairwise potentials, and as such we minimize exactly them by graph-cuts in order to get the final segmentation for frame $t$.\nSome qualitative examples are shown in Fig.~\\ref{fig:superclicks1} (compared to those obtained by using only superclick extraction shown in Fig.~\\ref{fig:superclicks}): it is easy to notice the difference in terms of segmentation quality achieved by analyzing a single frame only and by employing temporal smoothing, which is able to extract much better objects' shapes. It should also be noted that most of the processing (e.g. superpixel extraction and optical flow) can be shared when processing frames one after another, thus reducing the main processing time to superpixel segmentation and computation of optical flow for a single frame only.\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_notime.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_masked.png}\\\\ \\vspace{0.2cm}\n\t\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_masked.jpg}\n\t\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_notime.png}\n\t\n\t\n\t\n\t\n\t\n\t\n\t\\caption{Qualitative comparison between segmentations obtained when excluding (first column, see Fig.~\\ref{fig:superclicks}) and including temporal smoothing (second column).}\n\t\\label{fig:superclicks1}\n\\end{figure}\n\n\n\n\n\\section{Experimental results}\n\\label{sec:results}\nIn this section we present the experimental results obtained by testing our gamification approach and link them to the state of the art on interactive video annotation and automated video object segmentation methods.\n\n\\subsection{Datasets}\n\nFor testing the accuracy of our method we created 7 game levels (each 300 frames long) from the following four datasets:\n\\begin{itemize}\n \\item Fish dataset \\cite{Spampinato2014texton}: we created two levels from the \\emph{ComplexBkg1} and \\emph{Standard1} video sequences, by choosing frame intervals with high density of ground truth frames and with several visible objects.\n \\item I2R \\cite{Li2003I2R}: we created two levels from the \\emph{bootstrap} and \\emph{shopping\\_mall} videos, which are the most suitable ones for being used as game levels, since most videos in the dataset feature very few moving objects, in favor of dynamic background. The levels have been edited by selecting only parts with higher activity.\n \\item ETH BIWI \\cite{Ess2008ethbiwi}: we created two levels from the \\emph{BAHNHOF} and \\emph{JELMOLI} sequences created for urban multi-person tracking. These videos were particularly interesting because of the large number of moving targets appearing at the same time; the disadvantage was that annotations were provided only at the bounding-box level, which we nevertheless treated as pixel-wise segmentations (which causes the reported performance to be lower than they actually are, as our method performs pixel-level segmentation).\n \\item SegTrack v2 \\cite{Li2013segtrack2}: we created only one level by incorporating a subset of videos from the SegTrack v2 dataset (namely, \\emph{birdfall}, \\emph{bmx}, \\emph{cheetah}, \\emph{drift}, \\emph{hummingbird}, \\emph{monkey} and \\emph{monkeydog}) into a single sequence, since many of them were just few dozen frames long. We favored sequences with multiple objects, and excluded videos where the target, though moving, appeared at the same frame location due to camera motion (e.g. the \\emph{girl} sequence).\n\\end{itemize}\n\nWhen we compared our methods to existing interactive video annotation methods and automated video object segmentation approaches we used the whole SegTrack v2.\n\n\\subsection{Collected data}\n\nOur experiments involved only five players (two of them were children) outside of our research team, who were simply asked to compete with each other by achieving the highest possible score. The following information describes the amount of collected data and playing statistics:\n\\begin{itemize}\n \\item \\textbf{Level time: 30 seconds.}\n \\item \\textbf{Game time (7 levels): 3:30 minutes.}\n \\item \\textbf{75 games played}.\n \\item \\textbf{Total play time: 9:18 hours}. On average, each participant played for 1:52 hours, which may seem a lot. In fact, in a real public gaming scenario the amount of collected data corresponds to having about 160 users playing just one game, which is easily achievable by publishing the game with very little advertising on a social network.\n \\item \\textbf{Total number of clicks: 235,799}; the average number of clicks per frame was 52.4.\n\\end{itemize}\n\n\\subsection{Algorithm parameters}\n\nIn Sect.~\\ref{sec:methods}, we introduced some parameters which control the trade-off between clicks and visual regularity in the segmentation process. We empirically set the values for those parameters, as follows: $\\alpha_1 = 1/4$, $\\alpha_2 = 1/5$, $U_s$ = 0.4, $\\beta_1 = 5$, $T = 2 $, $\\gamma_1 = 0.1$, $\\gamma_2 = 0.9$ ($W_1$).\n\nThese parameter values were used to compute the results shown in the next section. It is important to note that the same parameters were used for all videos, although they have distinct differences in scenery, type of targets, motion patterns, motion speed, frame rate, etc. It is foreseeable that applying the same method to videos belonging to a more homogeneous set of videos would yield higher accuracy.\n\n\\subsection{Segmentation results}\n\nThis paper describes a interactive video object segmentation method based on spatio-temporal MRF optimization using the collaborative effort of multiple users. Thus, we first evaluated the role of temporal smoothing followed by the analysis of segmentation accuracy w.r.t to game play time, number of players, click delay and click quality. \nThen, we compared our method in terms of interaction times and segmentation accuracy with, respectively, recent state-of-the-art interactive video annotation methods and automated video object segmentation approaches.\nThe metrics employed for performance analysis were pixel-level precision ($Pr$), recall ($Rec$) and F-measure ($F_1$) and average Pascal Overlap Measure ($POM$: intersection over union between $T$ ground truth $GT$ masks  and output segmentations $S$) defined as:\n\n\n", "itemtype": "equation", "pos": 39784, "prevtext": "\n\n\n\n\n\n\nThe first two lines of the cost function includes single-frame potentials, which consist of, respectively, unary potentials for each identified superpixel (first line) and pairwise potentials (second line) for each pair of superpixels belonging to the same frame. The last term (third line) enforces temporal smoothing, and consists of pairwise potentials computed over the set $\\mathcal{N}_T(\\cup_{\\tau = t-T}^{t+T} S^\\tau)$, which represents all pairs of superpixels (from all the frames in the considered time interval) satisfying the ``temporal linking'' described above, i.e., such that the two superpixels in each pair belong to temporally consecutive frames, and that at least one pixel belonging to one of them is projected onto the other by means of optical flow.\n\nSimilarly to $V_1$ (defined in Sect.~\\ref{sec:superclick} ), unary potential $W_1$ models whether superpixel $s$ is more likely to be assigned to background or foreground \\emph{per se}. In this stage, we simply assign a constant value to the potential depending on whether it had been identified, at the previous stage (see Sect.~\\ref{sec:superclick}), as a superclick or not (i.e., depending on $l_s^\\tau$).  In detail, given superpixel $s$, we set corresponding \\emph{foreground cost} and \\emph{background cost}; the value of each cost depends on $l_s^\\tau$: if $s$ was labeled as a superclick, we expect it to be more likely that it is foreground, so the background cost should be higher, and vice versa. $W_1$ is therefore computed as follows:\n\n", "index": 17, "text": "\\begin{equation}\n W_1(s, l_s, l_s^\\tau) =\n \\begin{cases}\n  \\gamma_1 & \\text{if}~(l_s = 1 \\land l_s^\\tau = 1) \\lor (l_s = 0 \\land l_s^\\tau = 0)\\\\\n  \\gamma_2 & \\text{otherwise}\n \\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"W_{1}(s,l_{s},l_{s}^{\\tau})=\\begin{cases}\\gamma_{1}&amp;\\text{if}~{}(l_{s}=1\\land l%&#10;_{s}^{\\tau}=1)\\lor(l_{s}=0\\land l_{s}^{\\tau}=0)\\\\&#10;\\gamma_{2}&amp;\\text{otherwise}\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>l</mi><mi>s</mi></msub><mo>,</mo><msubsup><mi>l</mi><mi>s</mi><mi>\u03c4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>\u03b3</mi><mn>1</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mpadded width=\"+3.3pt\"><mtext>if</mtext></mpadded><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>s</mi></msub><mo>=</mo><mn>1</mn><mo>\u2227</mo><msubsup><mi>l</mi><mi>s</mi><mi>\u03c4</mi></msubsup><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2228</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mi>s</mi></msub><mo>=</mo><mn>0</mn><mo>\u2227</mo><msubsup><mi>l</mi><mi>s</mi><mi>\u03c4</mi></msubsup><mo>=</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>\u03b3</mi><mn>2</mn></msub></mtd><mtd columnalign=\"left\"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 47646, "prevtext": "\nwith $\\gamma_1 < \\gamma_2$.\n\nPairwise potential $V_2$ is defined as in Section~\\ref{sec:superclick}, but in the last term (third line of Formula~\\ref{eq:time_energy}) of $E_2$ we employ it to evaluate the similarity not only between adjacent superpixels in the same frame, but also ``temporally-adjacent'' (according to $\\mathcal{N}_T$) superpixels in consecutive frames. In order to deal with errors in optical flow computation, we do not simply assign a constant based on the presence or absence of a temporal link between two superpixels in consecutive frames, but also verify that they are visually similar and in fact refer to the same object/region in both frames. Thus, we manage to enforce the criteria according to which superpixels overlaying the same region across different frames should all be assigned the same label.\n\n\nBoth $E_1$ and $E_2$ are binary pairwise energy functions with submodular pairwise potentials, and as such we minimize exactly them by graph-cuts in order to get the final segmentation for frame $t$.\nSome qualitative examples are shown in Fig.~\\ref{fig:superclicks1} (compared to those obtained by using only superclick extraction shown in Fig.~\\ref{fig:superclicks}): it is easy to notice the difference in terms of segmentation quality achieved by analyzing a single frame only and by employing temporal smoothing, which is able to extract much better objects' shapes. It should also be noted that most of the processing (e.g. superpixel extraction and optical flow) can be shared when processing frames one after another, thus reducing the main processing time to superpixel segmentation and computation of optical flow for a single frame only.\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_09_f_025_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_13_f_135_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_masked.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_042_notime.png}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_notime.jpg}\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_181_masked.png}\\\\ \\vspace{0.2cm}\n\t\n\t\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_masked.jpg}\n\t\n\\includegraphics[width=.24\\textwidth]{figures/mrf1/config_1_v_15_f_245_notime.png}\n\t\n\t\n\t\n\t\n\t\n\t\n\t\\caption{Qualitative comparison between segmentations obtained when excluding (first column, see Fig.~\\ref{fig:superclicks}) and including temporal smoothing (second column).}\n\t\\label{fig:superclicks1}\n\\end{figure}\n\n\n\n\n\\section{Experimental results}\n\\label{sec:results}\nIn this section we present the experimental results obtained by testing our gamification approach and link them to the state of the art on interactive video annotation and automated video object segmentation methods.\n\n\\subsection{Datasets}\n\nFor testing the accuracy of our method we created 7 game levels (each 300 frames long) from the following four datasets:\n\\begin{itemize}\n \\item Fish dataset \\cite{Spampinato2014texton}: we created two levels from the \\emph{ComplexBkg1} and \\emph{Standard1} video sequences, by choosing frame intervals with high density of ground truth frames and with several visible objects.\n \\item I2R \\cite{Li2003I2R}: we created two levels from the \\emph{bootstrap} and \\emph{shopping\\_mall} videos, which are the most suitable ones for being used as game levels, since most videos in the dataset feature very few moving objects, in favor of dynamic background. The levels have been edited by selecting only parts with higher activity.\n \\item ETH BIWI \\cite{Ess2008ethbiwi}: we created two levels from the \\emph{BAHNHOF} and \\emph{JELMOLI} sequences created for urban multi-person tracking. These videos were particularly interesting because of the large number of moving targets appearing at the same time; the disadvantage was that annotations were provided only at the bounding-box level, which we nevertheless treated as pixel-wise segmentations (which causes the reported performance to be lower than they actually are, as our method performs pixel-level segmentation).\n \\item SegTrack v2 \\cite{Li2013segtrack2}: we created only one level by incorporating a subset of videos from the SegTrack v2 dataset (namely, \\emph{birdfall}, \\emph{bmx}, \\emph{cheetah}, \\emph{drift}, \\emph{hummingbird}, \\emph{monkey} and \\emph{monkeydog}) into a single sequence, since many of them were just few dozen frames long. We favored sequences with multiple objects, and excluded videos where the target, though moving, appeared at the same frame location due to camera motion (e.g. the \\emph{girl} sequence).\n\\end{itemize}\n\nWhen we compared our methods to existing interactive video annotation methods and automated video object segmentation approaches we used the whole SegTrack v2.\n\n\\subsection{Collected data}\n\nOur experiments involved only five players (two of them were children) outside of our research team, who were simply asked to compete with each other by achieving the highest possible score. The following information describes the amount of collected data and playing statistics:\n\\begin{itemize}\n \\item \\textbf{Level time: 30 seconds.}\n \\item \\textbf{Game time (7 levels): 3:30 minutes.}\n \\item \\textbf{75 games played}.\n \\item \\textbf{Total play time: 9:18 hours}. On average, each participant played for 1:52 hours, which may seem a lot. In fact, in a real public gaming scenario the amount of collected data corresponds to having about 160 users playing just one game, which is easily achievable by publishing the game with very little advertising on a social network.\n \\item \\textbf{Total number of clicks: 235,799}; the average number of clicks per frame was 52.4.\n\\end{itemize}\n\n\\subsection{Algorithm parameters}\n\nIn Sect.~\\ref{sec:methods}, we introduced some parameters which control the trade-off between clicks and visual regularity in the segmentation process. We empirically set the values for those parameters, as follows: $\\alpha_1 = 1/4$, $\\alpha_2 = 1/5$, $U_s$ = 0.4, $\\beta_1 = 5$, $T = 2 $, $\\gamma_1 = 0.1$, $\\gamma_2 = 0.9$ ($W_1$).\n\nThese parameter values were used to compute the results shown in the next section. It is important to note that the same parameters were used for all videos, although they have distinct differences in scenery, type of targets, motion patterns, motion speed, frame rate, etc. It is foreseeable that applying the same method to videos belonging to a more homogeneous set of videos would yield higher accuracy.\n\n\\subsection{Segmentation results}\n\nThis paper describes a interactive video object segmentation method based on spatio-temporal MRF optimization using the collaborative effort of multiple users. Thus, we first evaluated the role of temporal smoothing followed by the analysis of segmentation accuracy w.r.t to game play time, number of players, click delay and click quality. \nThen, we compared our method in terms of interaction times and segmentation accuracy with, respectively, recent state-of-the-art interactive video annotation methods and automated video object segmentation approaches.\nThe metrics employed for performance analysis were pixel-level precision ($Pr$), recall ($Rec$) and F-measure ($F_1$) and average Pascal Overlap Measure ($POM$: intersection over union between $T$ ground truth $GT$ masks  and output segmentations $S$) defined as:\n\n\n", "index": 19, "text": "\\begin{equation}\n\\text{Pr} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\text{Pr}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\" display=\"block\"><mrow><mtext>Pr</mtext><mo>=</mo><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 47728, "prevtext": "\n\n", "index": 21, "text": "\\begin{equation}\n\\text{Rec} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\text{Rec}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\" display=\"block\"><mrow><mtext>Rec</mtext><mo>=</mo><mfrac><mtext>TP</mtext><mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\n\nThese performance metrics were computed by summing up the number of true positives, false positives and false negatives of each video category (i.e., without averaging across frames).\n\n\n\n", "itemtype": "equation", "pos": 47811, "prevtext": "\n\n", "index": 23, "text": "\\begin{equation}\n\\text{F}_1 = 2\\cdot\\frac{\\text{Pr}\\cdot \\text{Rec}}{\\text{Pr}+\\text{Rec}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\text{F}_{1}=2\\cdot\\frac{\\text{Pr}\\cdot\\text{Rec}}{\\text{Pr}+\\text{Rec}}\" display=\"block\"><mrow><msub><mtext>F</mtext><mn>1</mn></msub><mo>=</mo><mrow><mn>2</mn><mo>\u22c5</mo><mfrac><mrow><mtext>Pr</mtext><mo>\u22c5</mo><mtext>Rec</mtext></mrow><mrow><mtext>Pr</mtext><mo>+</mo><mtext>Rec</mtext></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00825.tex", "nexttext": "\n\n\\subsubsection{Role of spatio-temporal segmentation refinement}\n\nFig.~\\ref{fig:superclicks1} shows a qualitative comparison in terms of segmentation outputs when employing  only superclick extraction phase (see Sect.~\\ref{sec:superclick}) and when exploiting the temporal consistency between consecutive frames of superclicks. \nTable~\\ref{res:temporal_smoothing} reports quantitatively how including spatio-temporal based refinement enhanced the segmentation accuracy. It can be noted that in some cases the accuracy gain was lower (ETH BIWI and I2R) than in others (Fish and SegTrack v2). This depends on the dynamics of the video sequences, i.e., in ETH BIWI and I2R the objects move slowly so users were able to click accurately on objects, while SegTrack v2 and Fish are characterized by  strong camera and object motion resulting in much noisier input data, and the spatio-temporal based refinement proved effective to recover users' failures in identifying objects.\n\nThe lower performance achieved in ETH BIWI and I2R than the ones in Fish and SegTrack v2, instead, can be explained by the number of objects in the scene. Indeed, some ETH BIWI and I2R videos depicted very crowded scenes and given the limited number of users playing the game, not all objects present in the scene were accurately identified.\n\nFor all the following evaluations, we used the method including the spatio-temporal segmentation refinement.\n\n\\begin{table*}\n\t\\centering\n\t\\caption{Average segmentation accuracy for the video categories employed in our game in terms of precision, recall and F-measure, when we employ only superclick extraction (first row) and when we refine the output segmentation by means of spatio-temporal linking between superclicks in consecutive frames (second row).}\n\t\\begin{tabular}{lcccccccccccc}\n\t\t\\toprule\n\t\t\\multirow{2}{*}{\\textbf{}} & \\multicolumn{3}{c}{\\textbf{Fish}} & \\multicolumn{3}{c}{\\textbf{ETH BIWI}} & \\multicolumn{3}{c}{\\textbf{I2R}}  & \\multicolumn{3}{c}{\\textbf{SegTrack v2}} \\\\\n\t\t\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n\t\t{\\bf Method} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} \\\\\n\t\t\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n\t\t\t Superclick extraction & 0.639\t& 0.726 & \\underline{0.680} & 0.597\t& 0.629 & \\underline{0.613} & 0.602 & 0.661 & \\underline{0.6304} & 0.673 & 0.763 & \\underline{0.716}\\\\\n\t\t\tSpatio-temporal refinement& 0.684 & 0.821 & \\underline{0.746} & 0.647 & 0.636 & \\underline{0.642} & 0.606 & 0.696 & \\underline{0.6484} & 0.739 & 0.835 & \\underline{0.784} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{res:temporal_smoothing}\n\\end{table*}\n\n\n\n\\subsubsection{Accuracy w.r.t. users' play time}\n\\label{sec:results_users_time}\n\nTable~\\ref{tab:overall_accuracy} shows how segmentation results vary in relation to the amount of users' play time in terms of pixel-level precision, recall and F-measure.\n\n\\begin{table*}\n\\centering\n\\caption{Average segmentation accuracy for the video categories employed in our game in terms of precision, recall and F-measure, for different values of cumulative users' play time.}\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Playtime (hours)}} & \\multicolumn{3}{c}{\\textbf{Fish}} & \\multicolumn{3}{c}{\\textbf{ETH BIWI}} & \\multicolumn{3}{c}{\\textbf{I2R}}  & \\multicolumn{3}{c}{\\textbf{SegTrack v2}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n& \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n9 h & 0.684 & 0.821 & \\underline{0.746} & 0.647 & 0.636 & \\underline{0.641} & 0.606 & 0.696 & \\underline{0.648} & 0.739 & 0.835 & \\underline{0.784} \\\\\n8 h & 0.711 & 0.814 & \\underline{0.759} & 0.632 & 0.670 & \\underline{0.650} & 0.609 & 0.720 & \\underline{0.660} & 0.721 & 0.827 & \\underline{0.770} \\\\\n7 h & 0.735 & 0.787 & \\underline{0.760} & 0.672 & 0.612 & \\underline{0.640} & 0.663 & 0.670 & \\underline{0.666} & 0.770 & 0.819 & \\underline{0.794} \\\\\n6 h & 0.738 & 0.760 & \\underline{0.749} & 0.655 & 0.644 & \\underline{0.649} & 0.681 & 0.677 & \\underline{0.679} & 0.758 & 0.796 & \\underline{0.776} \\\\\n5 h & 0.707 & 0.592 & \\underline{0.644} & 0.689 & 0.564 & \\underline{0.620} & 0.756 & 0.571 & \\underline{0.650} & 0.816 & 0.758 & \\underline{0.786} \\\\\n3 h & 0.780 & 0.372 & \\underline{0.504} & 0.716 & 0.472 & \\underline{0.569} & 0.812 & 0.368 & \\underline{0.507} & 0.883 & 0.617 & \\underline{0.727} \\\\\n1 h & 0.869 & 0.149 & \\underline{0.254} & 0.816 & 0.339 & \\underline{0.479} & 0.909 & 0.228 & \\underline{0.365} & 0.929 & 0.450 & \\underline{0.607} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:overall_accuracy}\n\\end{table*}\n\nIt is easy to notice that, at the moment we interrupted the experiment, the accuracy had already started to slightly decrease as the participants played more games (i.e., as more clicks were being collected). Indeed, it is intuitive that as soon as enough clicks have been collected which allow to sufficiently highlight superclicks from the background, additional clicks are more likely to be noisy than to actually cover any missing foreground region. Of course, in a large-scale application scenario with hundreds or thousands of videos, the amount of play time needed to reach this ``saturation point'' is much higher and would require a very large user base and/or users' time.\n\n\\subsubsection{Accuracy w.r.t. number of users}\n\nAmong the advantages of exploiting gamification to solve a complex or large-scale task, one of the most important is the variety of data patterns contributed by different users of the system. This is especially important in multi-target tasks such as segmentation with multiple objects, as single users tend to be biased to select the same object across multiple games. We evaluated this tendency by comparing the segmentation accuracy obtained by taking into consideration the clicks from a single user (single-player scenario) with the accuracy obtained by the other four players (many-players scenario). The chosen user for the single-player scenario is the one who played most games (23); in order to compute the accuracy for the many-players scenario, we randomly sampled several sets of clicks such that each set had the same number of clicks per video as the chosen single player and averaged the results. Table~\\ref{tab:num_players_accuracy} show the comparison between the two scenarios.\n\n\\begin{table*}\n\\centering\n\\caption{Average segmentation accuracy for the video categories employed in our game in terms of precision, recall and F-measure, for the single-player and the many-players scenarios.}\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Number of players}} & \\multicolumn{3}{c}{\\textbf{Fish}} & \\multicolumn{3}{c}{\\textbf{ETH BIWI}} & \\multicolumn{3}{c}{\\textbf{I2R}}  & \\multicolumn{3}{c}{\\textbf{SegTrack v2}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n& \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\nSingle player & 0.577 & 0.203 & \\underline{0.301} & 0.562 & 0.286 & \\underline{0.379} & 0.781 & 0.305 & \\underline{0.438} & 0.797 & 0.493 & \\underline{0.609} \\\\\nMultiple players  & 0.917 & 0.261 & \\underline{0.407} & 0.797 & 0.415 & \\underline{0.546} & 0.902 & 0.300 & \\underline{0.451} & 0.870 & 0.543 & \\underline{0.668} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:num_players_accuracy}\n\\end{table*}\n\nThe comparison clearly shows that using clicks coming from many users yielded markedly better performance than having a single user gather the same amount of data. Of course, the reported performance is lower than the best accuracy in Table~\\ref{tab:overall_accuracy} because the click sets on which Table~\\ref{tab:num_players_accuracy} was computed amount to less than 3 hours' cumulative play time.\n\nTo test the importance of integrating data from several participants in multi-target tasks, we compared the accuracy of the single-player and many-players with respect to the number of objects in a frame, across all videos. The results in Table~\\ref{tab:num_players_objects} show that the difference in accuracy increases with the number of objects in a frame, hinting that, indeed, the variability introduced by a higher number of players positively affects a multi-target task with respect to fewer people working on it.\n\n\\begin{table*}\n\\centering\n\\caption{Average segmentation accuracy in terms of precision, recall and F-measure, for the single-player and the many-players scenarios, with respect to the number of objects in a frame.}\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Number of players}} & \\multicolumn{3}{c}{\\textbf{1-2 objects}} & \\multicolumn{3}{c}{\\textbf{3-5 objects}} & \\multicolumn{3}{c}{\\textbf{6+ objects}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n& \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\nSingle player & 0.798 & 0.458 & \\underline{0.582} & 0.564 & 0.219 & \\underline{0.315} & 0.687 & 0.200 & \\underline{0.310} \\\\\nMultiple players  & 0.868 & 0.502 & \\underline{0.636} & 0.760 & 0.285 & \\underline{0.415} & 0.852 & 0.290 & \\underline{0.432} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:num_players_objects}\n\\end{table*}\n\n\\subsubsection{Accuracy w.r.t. click delay}\n\nWhile most algorithm parameters have a strictly mathematical meaning, click delay (i.e., the number of frames by which we shift user clicks to take human reaction delay into account) is particularly interesting to analyze, as it can be an important design choice  which should be made by taking into consideration several factors such as user base, target speed, frame rate, etc. We evaluated segmentation accuracy for different click delay values, and the results are shown in Table~\\ref{tab:click_delay}. The specific value we used for our evaluations (2) is the one yielding the best average F-measure score, although the other values are quite close. It is interesting to see that the chosen value is not the optimal one for all videos -- for example, it is not for the ETH BIWI and I2R videos: indeed, those are the video categories with the lowest dynamics, requiring fewer sudden reactions and thus making it easier for users to follow the objects.\n\n\\begin{table*}\n\\centering\n\\caption{Average segmentation accuracy for the video categories employed in our game in terms of precision, recall and F-measure, for different click delay values.}\n\\begin{tabular}{lcccccccccccc|c}\n\\toprule\n\\multirow{2}{*}{\\textbf{Click delay}} & \\multicolumn{3}{c}{\\textbf{Fish}} & \\multicolumn{3}{c}{\\textbf{ETH BIWI}} & \\multicolumn{3}{c}{\\textbf{I2R}}  & \\multicolumn{3}{c}{\\textbf{SegTrack v2}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n& \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Average F${}_1$} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n0 & 0.613 & 0.743 & \\underline{0.672} & 0.611 & 0.692 & \\underline{0.649} & 0.613 & 0.692 & \\underline{0.650} & 0.664 & 0.789 & \\underline{0.721} & \\underline{0.673} \\\\\n1 & 0.667 & 0.794 & \\underline{0.725} & 0.613 & 0.678 & \\underline{0.644} & 0.600 & 0.711 & \\underline{0.650} & 0.688 & 0.815 & \\underline{0.746} & \\underline{0.692} \\\\\n2 & 0.684 & 0.821 & \\underline{0.746} & 0.647 & 0.636 & \\underline{0.641} & 0.606 & 0.696 & \\underline{0.648} & 0.739 & 0.835 & \\underline{0.784} & \\underline{0.705} \\\\\n3 & 0.667 & 0.791 & \\underline{0.724} & 0.613 & 0.675 & \\underline{0.643} & 0.598 & 0.711 & \\underline{0.650} & 0.688 & 0.815 & \\underline{0.746} & \\underline{0.691} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:click_delay}\n\\end{table*}\n\n\\subsubsection{Accuracy w.r.t. click quality}\n\nIn order to measure the effectiveness of our click quality assessment approach, we evaluated segmentation accuracy using subsets of clicks belonging to different quality ranges (namely, [0--0.6[, [0.6--0.8[, [0.8--1]), as shown in Table~\\ref{tab:quality_accuracy}. Unlike our tests concerning the variation of accuracy with respect to the number of players, we did not normalize the size of the click sets, as the three ranges were chosen so that they would approximately contain the same number of clicks. It can be seen that our measurement for click quality reflects the accuracy of the resulting segmentations; it should be noted that, as in Table~\\ref{tab:num_players_accuracy}, the reported results are sensibly lower than the best results (see Table~\\ref{tab:overall_accuracy}) due to a lower number of used clicks. This demonstrates that click quality is necessary to achieve accurate segmentantions but the number of clicks is more important, i.e., it's more important to have many users than few high quality ones playing the game. \n\n\\begin{table*}\n\\centering\n\\caption{Average segmentation accuracy for the video categories employed in our game in terms of precision, recall and F-measure, for different ranges of click quality. Along with each quality range, we report the fraction of clicks included in that range with respect to the total number of clicks.}\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Quality range}} & \\multicolumn{3}{c}{\\textbf{Fish}} & \\multicolumn{3}{c}{\\textbf{ETH BIWI}} & \\multicolumn{3}{c}{\\textbf{I2R}}  & \\multicolumn{3}{c}{\\textbf{SegTrack v2}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n& \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} & \\textbf{Pr} & \\textbf{Rec} & \\textbf{F${}_1$} \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n0.8 -- 1   (32.7\\%) & 0.817 & 0.423 & \\underline{0.558} & 0.777 & 0.477 & \\underline{0.591} & 0.872 & 0.391 & \\underline{0.540} & 0.896 & 0.633 & \\underline{0.742} \\\\\n0.6 -- 0.8 (32.9\\%) & 0.771 & 0.415 & \\underline{0.540} & 0.762 & 0.482 & \\underline{0.590} & 0.841 & 0.389 & \\underline{0.532} & 0.872 & 0.640 & \\underline{0.738} \\\\\n0   -- 0.6 (34.4\\%) & 0.392 & 0.223 & \\underline{0.284} & 0.370 & 0.267 & \\underline{0.310} & 0.407 & 0.319 & \\underline{0.358} & 0.493 & 0.335 & \\underline{0.399} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:quality_accuracy}\n\\end{table*}\n\n\\begin{table*}\n\t\\centering\n\t\\caption{Comparison in terms of segmentation accuracy - measured as average POM in percentage, respectively, achieved within the first 50 secs of annotation (first row) and maximun value (with the related interaction times)- between our approach and other interactive video annotation methods on a subset of 10 frames extracted from SegTrack v2 video sequences}\n\t\\begin{tabular}{lcccc}\n\t\t\\toprule\n\t\t& \\cite{Sch13} & \\cite{jain2014}   & \\cite{NSB15} & Our method\\\\\n\t\t\\midrule\n\t\tPOM within 50 secs & 39.8 & 42.2 & 61.5 & {\\bf 72.4} \\\\\n\t\tMaximum POM &56.6 ($\\sim$1,200 secs) & 65.2 ($\\sim$500 secs) & 84.3 ($\\sim$1,400 secs)& {\\bf 72.4} ($\\sim$50 sec) \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:interactive}\n\\end{table*}\n\n\\subsubsection{Comparison with the state of the art interactive video annotation methods}\n\n\nWe also compared our method to existing interactive video segmentation ones~\\cite{Sch13,jain2014,NSB15} in terms of interaction times and accuracy. It has to be noted that for a single user the interaction time of our method depends only on the video length, i.e., there is a linear dependency between number of annotated frames and annotation times, while existing interactive methods~\\cite{Sch13,jain2014,NSB15} show a non-linear (exponential-like) dependency. Nevertheless, using data generated by a single user in a single played game  would result in poor accuracy performance (as discussed in the previous section), thus we consider, for comparison with the state of the art, the cumulative time spent by a single user in multiple game sessions (generally one frame is shown for 0.2 seconds in our game, thus if we select the clicks of one user in five game sessions, the interaction time would be of 1 second.). \nWe selected randomly 10 frames from SegTrack V2 and we assessed how the segmentation accuracy changed w.r.t. to the interaction times. For our method, we considered the clicks of the user who played most games in several game sessions. Fig.~\\ref{fig:interaction_times} shows the interaction times of users against the achieved segmentation quality for our method and \n~\\cite{Sch13,jain2014,NSB15}. Our approach yielded a reasonably accuracy over the 10 considered frames after approximately 50 seconds of playtime (i.e., about 20 game sessions) while the other approaches needed much more time. Quantitatively, Table~\\ref{tab:interactive} reports the achieved segmentation accuracy (expressed by POM in percentage) over the 10 considered frames by all the comparing methods in two cases: a) within 50 seconds of annotation and b) the maximum achieved accuracy.\nLet us recall that the interaction time (50 seconds corresponding to about 20 game sessions) for our method is given by the playtime of a single user, and that the same value can be achieved by involving 20 users, in parallel, playing only one game session, i.e., with a very little human annotation effort as opposed to the existing other solutions ~\\cite{Sch13,jain2014,NSB15}.\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=.68\\textwidth]{figures/interaction_times_new}\n\t\\caption{Interaction times vs segmentation accuracy. The figure shows that with the proposed approach we get a fairly good segmentation quality just after 30 seconds. When we allowed users to spend more time on the annotation task, the method in~\\cite{NSB15} achieved the best performance with a POM of about 0.85 with an interaction time of about 1,400 seconds.}\n\t\\label{fig:interaction_times}\n\\end{figure*}\n\n\n\n\n\n\n\\subsubsection{Comparison with the state of the art automated video segmentation methods}\n\nDespite the proposed method is more inline with the research on interactive video annotation, it can be seen as a video object segmentation approach (with very little human intervention) and as such it is useful to link its performance with the state of the art on the automated methods. \nThe comparison was again performed on the SegTrack v2 dataset (largely employed as a benchmarking dataset for video object segmentation), and we selected as comparing methods those ones posing the video object segmentation task as a superpixel labeling problem  using spatio-temporal MRF optimization, namely, \\cite{Papazoglou2013fast,eccv,Giordano2015superpixel}. \nThe results, in terms of average POM in percentage, are reported in Table~\\ref{tab:segtrack}.\n\n\\begin{table}\n\t\\centering\n\t\\caption{Comparison in terms of segmentation accuracy - measured as average POM in percentage- between our approach and automated video object segmentation methods on SegTrack v2}\n\t\\begin{tabular}{lcccc}\n\t\t\\toprule\n\t\t& \\cite{Papazoglou2013fast} & \\cite{eccv} & \\cite{Giordano2015superpixel} & Our method\\\\\n\t\t\\midrule\n\t\tAverage POM & 53.5 & 59.3 & 64.4  & {\\bf 71.7} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:segtrack}\n\\end{table}\n\n\nTables~\\ref{tab:interactive} and \\ref{tab:segtrack} indicate that our method performs better than automated video object segmentation methods and slightly worse than interactive video annotation approaches. This is not surprising since interactive video annotation tools require users to spend more time in providing accurate annotation but are barely usable in case of large video datasets. This makes our method a very good trade-off between accuracy and annotation times.\nHowever, when we used only points (uniformly taken from ground truth masks) within objects of interest (this is might be a typical interactive video annotation scenario where users might be asked to accurately select foreground pixels) we obtain a much higher accuracy with an average POM of about 0.85 as shown in Fig.~\\ref{fig:good_gt}. \n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_050_notime.jpg}\n\t\\includegraphics[width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_088_notime.jpg}\n\t\\includegraphics[height=4.5cm,width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_127_notime.jpg} \\\\ \\vspace{0.2cm}\n\t\\includegraphics[height=4.5cm,width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_185_notime.jpg}\n\t\\includegraphics[width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_223_notime.jpg}\n\t\\includegraphics[width=.33\\textwidth]{figures/gt_segmentation/config_1_v_15_f_274_notime.jpg}\n\t\\caption{Output segmentations when using only points within objects (i.e., taken from ground truth segmentation masks) of interest: blue dots are ground truth points while green regions show the yielded segmentation masks.}\n\t\\label{fig:good_gt}\n\\end{figure*}\n\n\nFig.~\\ref{fig:sample_results} shows some failure cases of the proposed method: user clicks diverged substantially from objects' position hitting also background regions that were then classified as foreground. Our spatio-temporal refinement module was not able to recover such failures since in the previous phase (i.e., superclick extraction) several wrong superpixels were marked as superclicks. \n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=.4\\textwidth]{figures/fails/fast_movement_and_camera_motion.pdf}\n\t\\includegraphics[width=.4\\textwidth]{figures/fails/fast_movement_and_multiple_targets.pdf}\\\\ \\vspace{0.2cm}\n\t\\includegraphics[width=.4\\textwidth]{figures/fails/uncut_background_1.pdf}\n\t\\includegraphics[width=.4\\textwidth]{figures/fails/uncut_background_2.pdf}\n\t\\caption{Failure cases: User clicks were extremely inaccurate resulting in wrong object segmentations.}\n\t\\label{fig:sample_results}\n\\end{figure*}\n\n\\subsubsection{Running times}\n\nUsing $T = 2$, processing a single frame requires solving five MRFs for superclick extraction and one temporal MRF for accurate segmentation. Our Matlab implementation, run on a PC with a quad-core i7 CPU and 8 GB RAM, takes 3 seconds for superclick extraction in a single frame and 30 seconds for the temporal MRF (actually, for the four optical flow computations which link superpixels in time; MRF solving time is negligible), which would amount to 45 seconds in total. However, after the initial bootstrap phase, segmenting a new frame can benefit from already-extracted superpixels and computed the optical flow for previous frames, so processing is reduced to a single superpixel extraction and a single optical flow computation, resulting in a frame processing time of 10.5 seconds.\n\n\\subsubsection{Available Resources}\n\nIn the authors' webpage, the source code for object segmentation taking user clicks as input together with the videos used as game levels are available. We do not release the source code for the game, instead, we provide a web service where interested people can set up their game using their own videos and get the output segmentations as well as raw data (e.g., user clicks).\n\n\n\n\\section{Concluding Remarks}\n\\label{sec:conclusions}\nIn this paper we have described an interactive video object segmentation method \nwhich combines effectively games with a purpose strategy with collaborative human  efforts. The performance analysis showed that our method outperforms in terms of segmentation accuracy state-of-the-art automated video object segmentation methods and is more suitable for large scale video annotation than classic interactive video annotation tools. \nWe also demonstrated how including spatio-temporal regularization enhances greatly performance than using only spatial information and user-provided hard constraints.  We also release the source code for human-guided video object segmentation as well as a web service that enables interested people to set up their game (with their own videos) and download user generated data. \nAs future work, we plan to carry out experiments at a larger scale involving more users (as in~\\cite{6595949}) and videos. In addition as inhibition of return and click delay are two important aspects of the approach, we plan to make them adaptive according to, respectively, play time (thus to enforce users to identify as many objects as possible) and video motion characteristics.\n\n\n\n\\bibliographystyle{elsarticle-num}\n\\begin{thebibliography}{10}\n\t\\expandafter\\ifx\\csname url\\endcsname\\relax\n\t\\def\\url#1{\\texttt{#1}}\\fi\n\t\\expandafter\\ifx\\csname urlprefix\\endcsname\\relax\\def\\urlprefix{URL }\\fi\n\t\\expandafter\\ifx\\csname href\\endcsname\\relax\n\t\\def\\href#1#2{#2} \n\\def\\path#1{#1}\\fi\n\n\t\n\t\\bibitem{Giordano2015superpixel}\n\tC.~S. {Daniela Giordano, Francesca Murabito, Simone Palazzo}, {Superpixel-based\n\t\tVideo Object Segmentation using Perceptual Organization and Location Prior},\n\tin: Computer Vision and Pattern Recognition, 2015.\n\t\n\t\\bibitem{HanKDESVM}\n\tB.~Han, L.~S. Davis, {Density-Based Multifeature Background Subtraction with\n\t\tSupport Vector Machine}, IEEE Transactions on Pattern Analysis and Machine\n\tIntelligence 34~(5) (2012) 1017--1023.\n\t\\newblock \\href {http://dx.doi.org/10.1109/TPAMI.2011.243}\n\t{\\path{doi:10.1109/TPAMI.2011.243}}.\n\t\n\t\\bibitem{6682905}\n\tP.~Ochs, J.~Malik, T.~Brox, {Segmentation of Moving Objects by Long Term Video\n\t\tAnalysis}, IEEE Transactions on Pattern Analysis and Machine Intelligence\n\t36~(6) (2014) 1187--1200.\n\t\n\t\\bibitem{baieccv2010}\n\tX.~Bai, J.~Wang, G.~Sapiro, {Dynamic Color Flow: A Motion-Adaptive Color Model\n\t\tfor Object Segmentation in Video}, in: ECCV 2010, 2010, pp. 617--630.\n\t\n\t\\bibitem{Zhang2013video}\n\tD.~Zhang, O.~Javed, M.~Shah, {Video Object Segmentation through Spatially\n\t\tAccurate and Temporally Dense Extraction of Primary Object Regions}, 2013\n\tIEEE Conference on Computer Vision and Pattern Recognition 0 (2013) 628--635.\n\t\n\t\\bibitem{Lee:2011:KVO:2355573.2356444}\n\tY.~J. Lee, J.~Kim, K.~Grauman, {Key-segments for Video Object Segmentation},\n\tin: Proceedings of the 2011 International Conference on Computer Vision, ICCV\n\t'11, 2011, pp. 1995--2002.\n\t\n\t\\bibitem{Ochs2011}\n\tP.~Ochs, T.~Brox, {Object segmentation in video: A hierarchical variational\n\t\tapproach for turning point trajectories into dense regions}, Iccv (2011)\n\t1583--1590.\n\t\n\t\\bibitem{broxeccv10}\n\tT.~Brox, J.~Malik, {Object Segmentation by Long Term Analysis of Point\n\t\tTrajectories}, in: ECCV 2010, 2010, pp. 282--295.\n\t\n\t\\bibitem{Fragkiadaki_2015_CVPR}\n\tK.~Fragkiadaki, P.~Arbelaez, P.~Felsen, J.~Malik, Learning to segment moving\n\tobjects in videos, in: CVPR, 2015.\n\t\n\t\\bibitem{vonAhn2008}\n\tL.~von Ahn, L.~Dabbish, {Designing Games with a Purpose}, Commun. ACM 51~(8)\n\t(2008) 58--67.\n\t\\newblock \\href {http://dx.doi.org/10.1145/1378704.1378719}\n\t{\\path{doi:10.1145/1378704.1378719}}.\n\t\n\t\\bibitem{Morrison:2009}\n\tD.~Morrison, S.~Marchand-Maillet, {\\'{E}}.~Bruno, {TagCaptcha: Annotating\n\t\tImages with CAPTCHAs}, in: Proceedings of the ACM SIGKDD Workshop on Human\n\tComputation, HCOMP '09, ACM, New York, NY, USA, 2009, pp. 44--45.\n\t\n\t\\bibitem{Hacker:2009}\n\tS.~Hacker, L.~von Ahn, {Matchin: Eliciting User Preferences with an Online\n\t\tGame}, in: Proceedings of the SIGCHI Conference on Human Factors in Computing\n\tSystems, CHI '09, ACM, New York, NY, USA, 2009, pp. 1207--1216.\n\t\n\t\\bibitem{vonAhn2004}\n\tL.~von Ahn, L.~Dabbish, {Labeling images with a computer game}, in: ACM\n\tConference on Human Factors in Computing Systems, 2004, pp. 319 -- 326.\n\t\n\t\\bibitem{vonAhn:2006}\n\tL.~von Ahn, S.~Ginosar, M.~Kedia, R.~Liu, M.~Blum, {Improving Accessibility of\n\t\tthe Web with a Computer Game}, in: Proceedings of the SIGCHI Conference on\n\tHuman Factors in Computing Systems, CHI '06, ACM, New York, NY, USA, 2006,\n\tpp. 79--82.\n\t\n\t\\bibitem{vonAhn2006}\n\tL.~{Von Ahn}, R.~Liu, M.~Blum, {Peekaboom: a game for locating objects in\n\t\timages}, Most pages (2006) 55--64.\n\t\\newblock \\href {http://dx.doi.org/10.1145/1124772.1124782}\n\t{\\path{doi:10.1145/1124772.1124782}}.\n\t\n\t\\bibitem{eps271071}\n\tM.~Addis, L.~Boch, W.~Allasia, F.~Gallo, W.~Bailer, R.~Wright, {100 Million\n\t\tHours of Audiovisual Content: Digital Preservation and Access in the\n\t\tPrestoPRIME Project}, in: First International Digital Preservation\n\tInteroperability Framework (DPIF) Symposium, ACM, 2010.\n\t\n\t\\bibitem{ontogame}\n\tK.~Siorpaes, M.~Hepp, {OntoGame: Weaving the Semantic Web by Online Games}, in:\n\tS.~Bechhofer, M.~Hauswirth, J.~Hoffmann, M.~Koubarakis (Eds.), The Semantic\n\tWeb: Research and Applications, Vol. 5021 of Lecture Notes in Computer\n\tScience, Springer Berlin Heidelberg, 2008, pp. 751--766.\n\t\n\t\\bibitem{Palazzo2015using}\n\tS.~Palazzo, C.~Spampinato, D.~Giordano, F.~Murabito, {Using the eyes to \"see\"\n\t\tthe objects}, in: ACMMM, 2015.\n\t\n\t\\bibitem{Barnich2011vibe}\n\tO.~Barnich, M.~{Van Droogenbroeck}, {ViBe: a universal background subtraction\n\t\talgorithm for video sequences.}, IEEE Transactions on Image processing 20~(6)\n\t(2011) 1709--1724.\n\t\\newblock \\href {http://dx.doi.org/10.1109/TIP.2010.2101613}\n\t{\\path{doi:10.1109/TIP.2010.2101613}}.\n\t\n\t\\bibitem{Spampinato2014texton}\n\tC.~Spampinato, S.~Palazzo, I.~Kavasidis, {A texton-based kernel density\n\t\testimation approach for background modeling under extreme conditions},\n\tComputer Vision and Image Understanding 122 (2014) 74--83.\n\t\n\t\\bibitem{Shengcai2010}\n\tS.~Liao, G.~Zhao, V.~Kellokumpu, M.~Pietikainen, S.~Z. Li, {Modeling pixel\n\t\tprocess with scale invariant local patterns for background subtraction in\n\t\tcomplex scenes}, in: 2010 IEEE Conference on Computer Vision and Pattern\n\tRecognition (CVPR), 2010, pp. 1301--1306.\n\t\n\t\\bibitem{Zhang2011}\n\tB.~Zhang, Y.~Gao, S.~Zhao, B.~Zhong, {Kernel Similarity Modeling of Texture\n\t\tPattern Flow for Motion Detection in Complex Background}, IEEE Transactions\n\ton Circuits and Systems for Video Technology 21~(1) (2011) 29--38.\n\t\n\t\\bibitem{Papazoglou2013fast}\n\tA.~Papazoglou, V.~Ferrari, {Fast object segmentation in unconstrained video},\n\tProceedings of the IEEE International Conference on Computer Vision (2013)\n\t1777--1784.\n\t\n\t\\bibitem{eccv}\n\tJ.~Lim, B.~Han, {Generalized Background Subtraction Using Superpixels with\n\t\tLabel Integrated Motion Estimation}, in: ECCV 2014, 2014, pp. 173--187.\n\t\n\t\\bibitem{Deng2013}\n\tJ.~Deng, J.~Krause, L.~Fei-Fei, {Fine-grained crowdsourcing for fine-grained\n\t\trecognition}, Proceedings of the IEEE Computer Society Conference on Computer\n\tVision and Pattern Recognition (2013) 580--587\\href\n\t{http://dx.doi.org/10.1109/CVPR.2013.81} {\\path{doi:10.1109/CVPR.2013.81}}.\n\t\n\t\\bibitem{Maji2012}\n\tS.~Maji, {Discovering a lexicon of parts and attributes}, Lecture Notes in\n\tComputer Science (including subseries Lecture Notes in Artificial\n\tIntelligence and Lecture Notes in Bioinformatics) 7585 LNCS~(PART 3) (2012)\n\t21--30.\n\t\n\t\\bibitem{Sorokin}\n\tA.~Sorokin, D.~Forsyth, {Utility data annotaton with Amazon Mechanical Turk},\n\tProceedings of the 1st IEEE Workshop on Internet Vision at CVPR 08~(c) (2008)\n\t1--8.\n\t\n\t\\bibitem{Parikh2012}\n\tD.~Parikh, D.~Crandall, K.~Grauman, {Discovering localized attributes for\n\t\tfine-grained recognition}, 2012 IEEE Conference on Computer Vision and\n\tPattern Recognition (2012) 3474--3481.\n\t\n\t\\bibitem{Parkash}\n\tA.~Parkash, D.~Parikh, {Attributes for classifier feedback}, Lecture Notes in\n\tComputer Science (including subseries Lecture Notes in Artificial\n\tIntelligence and Lecture Notes in Bioinformatics) 7574 LNCS~(PART 3) (2012)\n\t354--368.\n\t\n\t\\bibitem{Parikh}\n\tD.~Parikh, C.~L. Zitnick, {Human-Debugging of Machines}, Neural Information\n\tProcessing Systems (2011) 1--5.\n\t\n\t\\bibitem{Vijayanarasimhan2014}\n\tS.~Vijayanarasimhan, K.~Grauman, {Large-Scale Live Active Learning: Training\n\t\tObject Detectors with Crawled Data and Crowds}, International Journal of\n\tComputer Vision 108~(1-2) (2014) 97--114.\n\t\n\t\\bibitem{Salvador2013}\n\tA.~Salvador, A.~Carlier, X.~Giro-i Nieto, O.~Marques, V.~Charvillat,\n\t{Crowdsourced object segmentation with a game}, Proceedings of the 2nd ACM\n\tinternational workshop on Crowdsourcing for multimedia - CrowdMM '13 (2013)\n\t15--20.\n\t\n\t\\bibitem{Badrinarayanan2010}\n\tV.~Badrinarayanan, F.~Galasso, R.~Cipolla, {Label propagation in video\n\t\tsequences}, 2010 IEEE Computer Society Conference on Computer Vision and\n\tPattern Recognition (2010) 3265--3272.\n\t\n\t\\bibitem{Fathi2011}\n\tA.~Fathi, M.~F. Balcan, X.~Ren, J.~M. Rehg, {Combining Self Training and Active\n\t\tLearning for Video Segmentation}, Procedings of the British Machine Vision\n\tConference 2011 (2011) 78.1--78.11\\href {http://dx.doi.org/10.5244/C.25.78}\n\t{\\path{doi:10.5244/C.25.78}}.\n\t\n\t\\bibitem{Budvytis2011}\n\tI.~Budvytis, V.~Badrinarayanan, R.~Cipolla, {Semi-supervised video segmentation\n\t\tusing tree structured graphical models} 35~(11) (2011) 2257--2264.\n\t\n\t\\bibitem{NSB15}\n\tN.~S. Nagaraja, F.~Schmidt, T.~Brox, {Video Segmentation with Just a Few\n\t\tStrokes}, in: IEEE International Conference on Computer Vision (ICCV), 2015.\n\t\n\t\\bibitem{Price2009}\n\tB.~L. Price, B.~S. Morse, S.~Cohen, {LIVEcut: Learning-based interactive video\n\t\tsegmentation by evaluation of multiple propagated cues}, IEEE 12th\n\tInternational Conference on Computer Vision (ICCV)~(Iccv) (2009) 779--786.\n\t\n\t\\bibitem{Badrinarayanan2013}\n\tV.~Badrinarayanan, I.~Budvytis, R.~Cipolla, {Mixture of Trees Probabilistic\n\t\tGraphical Model for Video Segmentation}, International Journal of Computer\n\tVision (2013) 1--16.\n\t\n\t\\bibitem{Li2005}\n\tY.~Li, J.~Sun, H.-Y. Shum, {Video object cut and paste}, ACM Transactions on\n\tGraphics 24~(3) (2005) 595.\n\t\n\t\\bibitem{grabcut}\n\tC.~Rother, V.~Kolmogorov, A.~Blake, {\"GrabCut\": interactive foreground\n\t\textraction using iterated graph cuts}, ACM Trans. Graph. 23~(3) (2004)\n\t309--314.\n\t\n\t\\bibitem{jain2014}\n\tS.~Jain, K.~Grauman, {Supervoxel-Consistent Foreground Propagation in Video},\n\tin: D.~Fleet, T.~Pajdla, B.~Schiele, T.~Tuytelaars (Eds.), Computer Vision\n\t\u00e2\u0080\u0093 ECCV 2014, Vol. 8692 of Lecture Notes in Computer Science, Springer\n\tInternational Publishing, 2014, pp. 656--671.\n\t\n\t\\bibitem{Donahue2011}\n\tJ.~Donahue, K.~Grauman, {Annotator rationales for visual recognition}, 2011\n\tInternational Conference on Computer Vision (2011) 1395--1402.\n\t\n\t\\bibitem{Vedaldi2014}\n\tA.~Vedaldi, S.~Mahendran, S.~Tsogkas, S.~Maji, R.~Girshick, J.~Kannala,\n\tE.~Rahtu, I.~Kokkinos, M.~B. Blaschko, D.~Weiss, B.~Taskar, K.~Simonyan,\n\tN.~Saphra, S.~Mohamed, {Understanding Objects in Detail with Fine-Grained\n\t\tAttributes}, in: 2014 IEEE Conference on Computer Vision and Pattern\n\tRecognition, 2014, pp. 3622--3629.\n\t\n\t\\bibitem{6595949}\n\tI.~Kavasidis, C.~Spampinato, D.~Giordano, {Generation of Ground Truth for\n\t\tObject Detection While Playing an Online Game: Productive Gaming or\n\t\tRecreational Working?}, in: Computer Vision and Pattern Recognition Workshops\n\t(CVPRW), 2013 IEEE Conference on, 2013, pp. 694--699.\n\t\n\t\\bibitem{Achanta2012slic}\n\tR.~Achanta, A.~Shaji, K.~Smith, A.~Lucchi, P.~Fua, S.~Susstrunk, {SLIC\n\t\tSuperpixels}, EPFL Technical Report 149300~(June) (2010) 15.\n\t\\newblock \\href {http://dx.doi.org/10.1109/TPAMI.2012.120}\n\t{\\path{doi:10.1109/TPAMI.2012.120}}.\n\t\n\t\\bibitem{reaction_times}\n\tA.~Jain, R.~Bansal, A.~Kumar, K.~D. Singh, {{A} comparative study of visual and\n\t\tauditory reaction times on the basis of gender and physical activity levels\n\t\tof medical first year students}, Int J Appl Basic Med Res 5~(2) (2015)\n\t124--127.\n\t\n\t\\bibitem{Kolmogorov2004what}\n\tV.~Kolmogorov, R.~Zabih, {What Energy Functions Can Be Minimized via Graph\n\t\tCuts?}, IEEE Transactions on Pattern Analysis and Machine Intelligence 26~(2)\n\t(2004) 147--159.\n\t\n\t\\bibitem{Liu2009opticalflow}\n\tC.~Liu, W.~Adviser-Freeman, E.~Adviser-Adelson, {Beyond pixels: exploring new\n\t\trepresentations and applications for motion analysis}, Proceedings of the\n\t10th European Conference on Computer Vision: Part III (2009) 28--42.\n\t\n\t\\bibitem{Li2003I2R}\n\tL.~Li, W.~Huang, I.~Y.~H. Gu, Q.~Tian, {Foreground object detection from videos\n\t\tcontaining complex background}, Proceedings of the eleventh ACM international\n\tconference on Multimedia MULTIMEDIA 03 03 (2003) 2.\n\t\\newblock \\href {http://dx.doi.org/10.1145/957013.957017}\n\t{\\path{doi:10.1145/957013.957017}}.\n\t\n\t\\bibitem{Ess2008ethbiwi}\n\tA.~Ess, B.~Leibe, K.~Schindler, L.~{Van Gool}, {A mobile vision system for\n\t\trobust multi-person tracking}, Computer Vision and Pattern Recognition, 2008.\n\tCVPR 2008. IEEE Conference on (2008) 1--8.\n\t\n\t\\bibitem{Li2013segtrack2}\n\tF.~Li, T.~Kim, A.~Humayun, D.~Tsai, J.~M. Rehg, {Video segmentation by tracking\n\t\tmany figure-ground segments}, Proceedings of the IEEE International\n\tConference on Computer Vision (2013) 2192--2199.\n\t\n\t\\bibitem{Sch13}\n\tL.~Gorelick, F.~R. Schmidt, Y.~Boykov, {Fast Trust Region for Segmentation},\n\tin: IEEE International Conference on Computer Vision and Pattern Recognition\n\t(CVPR), 2013.\n\t\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 48105, "prevtext": "\n\n\nThese performance metrics were computed by summing up the number of true positives, false positives and false negatives of each video category (i.e., without averaging across frames).\n\n\n\n", "index": 25, "text": "\\begin{equation}\n\\text{POM(GT, S)} = \\frac{1}{T}\\sum_{i=1}^{T}\\frac{|GT_i \\cap S_i|}{|GT_i \\cup S_i|}\n\\label{form:pom}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\text{POM(GT, S)}=\\frac{1}{T}\\sum_{i=1}^{T}\\frac{|GT_{i}\\cap S_{i}|}{|GT_{i}%&#10;\\cup S_{i}|}\" display=\"block\"><mrow><mtext>POM(GT, S)</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>G</mi><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub></mrow><mo>\u2229</mo><msub><mi>S</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>G</mi><mo>\u2062</mo><msub><mi>T</mi><mi>i</mi></msub></mrow><mo>\u222a</mo><msub><mi>S</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}]