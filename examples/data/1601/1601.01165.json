[{"file": "1601.01165.tex", "nexttext": "\nHere the Dispersion Measure \\textit{DM} represents the\nprojected number of free electrons between the source and the receiver.\nIn incoherent dedispersion,  the lower frequencies\n are shifted in time and realigned  with the higher ones, thus\n approximating the original signal.\n\nIn a survey, the incoming signal must be brute-force dedispersed for\nthousands of possible DM values.\nAs every telescope pointing direction or\n\\textit{beam} can be processed independently, performance of the\ndedispersion algorithm can be improved by means of large-scale parallelization.\n\n\n\n\n\\subsection{Sequential Algorithm}\n\\label{sub:sequential_algorithm}\n\nThe input of this algorithm is a frequency-channelized time series,\nrepresented as a $c \\times t$ matrix,\nwith $c$ frequency channels and $t$ time samples\nneeded to dedisperse one second of data.\nThe output is a set of\n $d$ dedispersed trial-DM time-series, each of length $s$ samples per second, represented\nas a $d \\times s$ matrix.  All data are single precision floating point numbers; their real-life rates are e.g.\n36~GB/s input and 72~GB/s output for the pulsar search with Apertif on the Westerbork telescope \\citep{leeu14}.\n\n\nDedispersion (sequential pseudocode shown in Algorithm~\\ref{alg:Dedispersion}) then consists of three nested loops, and\nevery output element is the sum of $c$ samples: one for each frequency\nchannel.  Which samples are part of each sum depends on the applied\ndelay (i.e. $\\Delta$) that, as we know from\nEquation~\\ref{eq:Dispersion}, is a non-linear function of frequency\nand DM\\@.  These delays\ncan be computed in advance, so they do not contribute to the\nalgorithm's complexity.  Therefore, the complexity of this algorithm is $O(d \\times s \\times c)$.\n\n\\begin{algorithm}\n\\caption{Pseudocode of the brute-force dedispersion algorithm.}\n\\label{alg:Dedispersion}\n\\footnotesize{\\begin{algorithmic}\n\\FOR{dm = 0 $\\to$ d}\n  \\FOR{sample = 0 $\\to$ s}\n    \\STATE{dSample = 0}\n    \\FOR{chan = 0 $\\to$ c}\n      \\STATE{dSample += input[chan][sample + $\\Delta$(chan, dm)]}\n    \\ENDFOR\n    \\STATE{output[dm][sample] = dSample}\n  \\ENDFOR\n\\ENDFOR\n\\end{algorithmic}}\n\\end{algorithm}\n\n\nIn the context of many-core accelerators, there is another, extremely important algorithmic\ncharacteristic: the \\textit{Arithmetic\n  Intensity} (AI), i.e.\\ the ratio between the performed\nfloating-point operations and the number of bytes accessed in global\nmemory.  In many-core\narchitectures the gap between computational capabilities and memory\nbandwidth is wide, and a high AI is thus a prerequisite for high\nperformance \\citep{williams2009}.  Unfortunately,\nthe AI for Algorithm~\\ref{alg:Dedispersion} is\ninherently low, with only one floating point operation per four bytes loaded from global memory.\nFor dedispersion,\n\n", "itemtype": "equation", "pos": 6060, "prevtext": "\n\n\\title{Real-Time Dedispersion for Fast Radio Transient Surveys, using Auto Tuning on Many-Core Accelerators}\n\n\\author[vu,astron]{Alessio Sclocco}\n\\ead{a.sclocco@vu.nl}\n\\author[astron,api]{Joeri van Leeuwen}\n\\ead{leeuwen@astron.nl}\n\\author[vu]{Henri E. Bal}\n\\ead{h.e.bal@vu.nl}\n\\author[nlesc]{Rob V. van Nieuwpoort}\n\\ead{r.vannieuwpoort@esciencecenter.nl}\n\n\\address[vu]{Faculty of Sciences, Vrije Universiteit Amsterdam, Amsterdam, the Netherlands}\n\\address[astron]{ASTRON, the Netherlands Institute for Radio Astronomy, Dwingeloo, the Netherlands}\n\\address[api]{Astronomical Institute ``Anton Pannekoek'', University of Amsterdam, Amsterdam, the Netherlands}\n\\address[nlesc]{NLeSC, Netherlands eScience Center, Amsterdam, the Netherlands}\n\n\\begin{abstract}\nDedispersion, the removal of deleterious smearing of\n  impulsive signals by the interstellar matter, is one of the most\n  intensive processing steps in any radio survey for pulsars and fast\n  transients.  We here present a study of the parallelization of this\n  algorithm on many-core accelerators, including GPUs\n  from AMD and NVIDIA, and the Intel Xeon Phi. We find that\n  dedispersion is inherently memory-bound. Even in a perfect scenario,\n  hardware limitations keep the arithmetic intensity low, thus\n  limiting performance. We next exploit auto-tuning to adapt\n  dedispersion to different accelerators, observations, and even\n  telescopes. We demonstrate that the optimal settings differ between\n  observational setups, and that auto-tuning significantly improves\n  performance. This impacts time-domain surveys from Apertif to SKA\\@.\n\\end{abstract}\n\n\\begin{keyword}\nPulsars: general -- Astronomical instrumentation, methods and techniques -- Techniques: miscellaneous\n\\end{keyword}\n\n\\maketitle\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nAstronomical phenomena such as pulsars \\citep{hbp+68} and fast radio\nbursts \\citep[FRBs; ][]{lbm+07} consist\nof millisecond-duration impulsive signals over a broad radio-frequency\nrange.\nAs the electromagnetic waves propagate through the interstellar\nmaterial (ISM) between us and the source, they are dispersed \\citep{lorimer2005}.\nThis causes lower radio frequencies to arrive progressively later, and without correction this results in a loss of signal-to-noise, that often makes the source undetectable when integrating over a wide observing bandwidth.\nThis frequency-dependent delay can be removed in a process called\n\\textit{dedispersion}. Complete removal can be achieved by reverting\nall phases through a convolution of the signal with the inverse of the modeled\nISM \\citep[coherent dedispersion; ][]{hr75}. Incomplete but much\nfaster removal, especially when many dispersion measure trials are\nrequired, can be achieved by appropriately shifting in time the signal\nfrequency channels (incoherent dedispersion; from now on referred to plainly as dedispersion).\nThis dedispersion is a basic algorithm in high-time-resolution radio\nastronomy, and one of the building blocks of surveys for fast\nphenomena with modern radio telescopes\nsuch as the Low Frequency Array \\citep[LOFAR; ][]{ls10, sha+11} and the\nSquare Kilometer Array \\citep[SKA; ][]{carilli2004}.\nIn these surveys, the dispersion measures are a priori unknown, and\ncan only be determined in a brute-force search.\nThis search generally runs on off-site\nsupercomputers. These range from e.g., the CM-200 in the \\citet{fcwa95}\nArecibo survey, to\ngSTAR for the Parkes HTRU \\citep{kjs+10}, and Cartesius for\nthe LOFAR LOTAAS \\citep{clh+14} surveys. In the latter the\ndedispersion step amounts to over half of\nall required pulsar-search processing. For the SKA, this processing\nwill amount to many PFLOPS for both SKA-Mid \\citep[cf.\\,][]{magr14} and\n  SKA-Low \\citep{kbk+14}.\n\nAbove and beyond these pure compute requirements, the similar and\noften simultaneous search for FRBs demands that this dedispersion is\ndone near real time. Only then can these fleeting events be immediately\nfollowed up by telescopes at other energies \\citep[][]{pbb+15}.\n\nWe aim to achieve the required performance by parallelizing this\nalgorithm for many-core accelerators.  Compared to similar attempts\nmade by~\\cite{barsdell2012b} and~\\cite{armour2012}, we\npresent a performance analysis that is more complete, and\nintroduce a novel many-core algorithm that can be tuned for different platforms and\nobservational setups.  To our knowledge, this is the first attempt at\ndesigning a brute-force dedispersion algorithm that is highly portable and not\nfine-tuned for a specific platform or telescope.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo summarize our contributions, in this paper we: (1) provide an\nin-depth analysis of the arithmetic intensity (AI) of brute-force dedispersion,\nfinding  analytically and  empirically that it is\nmemory bound; (2) show that auto-tuning can\nadapt the algorithm to different platforms, telescopes, and\nobservational setups; (3) demonstrate that\nmany-core accelerators can achieve real-time performance; (4) quantify the\nimpact that auto-tuning has on performance; (5) compare different\nplatforms using a real-world scientific application; and (6) compare\nthe performance of OpenCL and OpenMP for the Intel Xeon devices.\n\nIn Section~\\ref{sec:dedispersion_kernel} we describe the\nbrute-force dedispersion algorithm, our parallel implementation and\nits optimizations; and the theoretical analysis of\nthe dedispersion AI\\@.  We next present our experiments (Section~\\ref{sec:experimental_setup}),  results (Section~\\ref{sec:results}) and  further\ndiscussion (Section~\\ref{sec:discussion}). Finally, relevant\nliterature is discussed in Section~\\ref{sec:related_works}, and\nSection~\\ref{sec:conclusions} summarizes our conclusions.\n\n\n\n\\section{The Brute-Force Dedispersion Algorithm}\n\\label{sec:dedispersion_kernel}\n\\label{sub:dispersion}\n\nIn dispersion \\citep{lorimer2005}, the highest frequency\nin a certain band\n$f_{h}$ is received at time $t_{x}$, while lower simultaneously emitted frequency components $f_{i}$ arrive at $t_{x} + k$.\nFor frequencies expressed in MHz this delay in seconds is:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:Dispersion}\nk \\approx 4150 \\times DM \\times \\left(\\frac{1}{f_{i}^{2}} - \\frac{1}{f_{h}^{2}}\\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"k\\approx 4150\\times DM\\times\\left(\\frac{1}{f_{i}^{2}}-\\frac{1}{f_{h}^{2}}\\right)\" display=\"block\"><mrow><mi>k</mi><mo>\u2248</mo><mrow><mrow><mrow><mn>4150</mn><mo>\u00d7</mo><mi>D</mi></mrow><mo>\u2062</mo><mi>M</mi></mrow><mo>\u00d7</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><msubsup><mi>f</mi><mi>i</mi><mn>2</mn></msubsup></mfrac><mo>-</mo><mfrac><mn>1</mn><msubsup><mi>f</mi><mi>h</mi><mn>2</mn></msubsup></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01165.tex", "nexttext": "\nwhere\n$\\epsilon$ represents the effect of accessing the delay table and\nwriting the output.\nThis low AI shows that brute-force dedispersion\nis memory bound on most architectures. Its\nperformance is thus limited not by computational\ncapabilities, but by memory bandwidth.  One way to increase\nAI and thus improve\nperformance, is to reduce the number of reads from global memory, by\nimplementing some form of data reuse.  In\nAlgorithm~\\ref{alg:Dedispersion} some data reuse appears possible.\nFor some frequencies, the delay\nis the same for two close DMs, $dm_{i}$ and $dm_{j}$, so that\n$\\Delta(c, dm_{i}) = \\Delta(c, dm_{j})$.  Then, one input\nelement provides two different sums. With data reuse,\n\n", "itemtype": "equation", "pos": 8967, "prevtext": "\nHere the Dispersion Measure \\textit{DM} represents the\nprojected number of free electrons between the source and the receiver.\nIn incoherent dedispersion,  the lower frequencies\n are shifted in time and realigned  with the higher ones, thus\n approximating the original signal.\n\nIn a survey, the incoming signal must be brute-force dedispersed for\nthousands of possible DM values.\nAs every telescope pointing direction or\n\\textit{beam} can be processed independently, performance of the\ndedispersion algorithm can be improved by means of large-scale parallelization.\n\n\n\n\n\\subsection{Sequential Algorithm}\n\\label{sub:sequential_algorithm}\n\nThe input of this algorithm is a frequency-channelized time series,\nrepresented as a $c \\times t$ matrix,\nwith $c$ frequency channels and $t$ time samples\nneeded to dedisperse one second of data.\nThe output is a set of\n $d$ dedispersed trial-DM time-series, each of length $s$ samples per second, represented\nas a $d \\times s$ matrix.  All data are single precision floating point numbers; their real-life rates are e.g.\n36~GB/s input and 72~GB/s output for the pulsar search with Apertif on the Westerbork telescope \\citep{leeu14}.\n\n\nDedispersion (sequential pseudocode shown in Algorithm~\\ref{alg:Dedispersion}) then consists of three nested loops, and\nevery output element is the sum of $c$ samples: one for each frequency\nchannel.  Which samples are part of each sum depends on the applied\ndelay (i.e. $\\Delta$) that, as we know from\nEquation~\\ref{eq:Dispersion}, is a non-linear function of frequency\nand DM\\@.  These delays\ncan be computed in advance, so they do not contribute to the\nalgorithm's complexity.  Therefore, the complexity of this algorithm is $O(d \\times s \\times c)$.\n\n\\begin{algorithm}\n\\caption{Pseudocode of the brute-force dedispersion algorithm.}\n\\label{alg:Dedispersion}\n\\footnotesize{\\begin{algorithmic}\n\\FOR{dm = 0 $\\to$ d}\n  \\FOR{sample = 0 $\\to$ s}\n    \\STATE{dSample = 0}\n    \\FOR{chan = 0 $\\to$ c}\n      \\STATE{dSample += input[chan][sample + $\\Delta$(chan, dm)]}\n    \\ENDFOR\n    \\STATE{output[dm][sample] = dSample}\n  \\ENDFOR\n\\ENDFOR\n\\end{algorithmic}}\n\\end{algorithm}\n\n\nIn the context of many-core accelerators, there is another, extremely important algorithmic\ncharacteristic: the \\textit{Arithmetic\n  Intensity} (AI), i.e.\\ the ratio between the performed\nfloating-point operations and the number of bytes accessed in global\nmemory.  In many-core\narchitectures the gap between computational capabilities and memory\nbandwidth is wide, and a high AI is thus a prerequisite for high\nperformance \\citep{williams2009}.  Unfortunately,\nthe AI for Algorithm~\\ref{alg:Dedispersion} is\ninherently low, with only one floating point operation per four bytes loaded from global memory.\nFor dedispersion,\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:DedispersionAI}\nAI = \\frac{1}{4 + \\epsilon} < \\frac{1}{4}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"AI=\\frac{1}{4+\\epsilon}&lt;\\frac{1}{4}\" display=\"block\"><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>I</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>4</mn><mo>+</mo><mi>\u03f5</mi></mrow></mfrac><mo>&lt;</mo><mfrac><mn>1</mn><mn>4</mn></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01165.tex", "nexttext": "\nThe bound from Equation~\\ref{eq:DedispersionAIp} theoretically goes toward\ninfinity, but in\npractice the non-linear delay function diverges rapidly at lower\nfrequencies. There is\nnever enough data reuse to approach the upper bound of\nEquation~\\ref{eq:DedispersionAIp}; for a more in-depth discussion see~\\cite{sclocco2014}.\nWe thus categorize the algorithm as memory-bound.\nIn this conclusion we differ from previous literature such as\n\\cite{barsdell2010} and~\\cite{barsdell2012b}.\nThe importance of the above mentioned data reuse in dedispersion was identified early on and implemented in e.g.\\ the tree dedispersion algorithm \\citep{tay74}.\nThat fast implementation has the drawback of assuming the dispersion sweep is linear.\nSeveral modern pulsar and FRB surveys with large fractional bandwidths have used modified tree algorithms that sum over the \\emph{quadratic} nature of the sweep (e.g.\\ \\cite{mlc+01}, \\cite{masui2015}).\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Parallelization}\n\\label{sub:parallelization}\n\nWe first determine how to divide and organise the work of different\nthreads, and describe these in  OpenCL terminology.\nWe identify three main algorithm dimensions: DM, time and\nfrequency. Time and DM\nare independent, and\nideal for parallelization, avoiding any inter- and\nintra-thread dependency.  In our implementation, each OpenCL work-item\n(i.e.\\ thread) is associated with a different (DM, time) pair and it\nexecutes the innermost loop of Algorithm~\\ref{alg:Dedispersion}.  An\nOpenCL work-group (i.e.\\ group of threads) combines work-items that\nare associated with the same DM, but with different time samples.\n\nThis proposed organization also simplifies memory access, using\ncoalesced reads and writes. Different small\nrequests can then be combined in one bigger operation. This well-known\noptimization is a\nperformance requisite for many-core architectures, especially for\nmemory-bound algorithms like dedispersion.  Our output\nelements are written to adjacent, and aligned, memory locations. The\n\\emph{reads} from global memory are also coalesced but, due to the shape of\nthe delay function, are not always aligned.\nThe worst-case memory overhead is at most a factor two \\citep{sclocco2014}.\n\nTo exploit data reuse, we compute more than\none DM per work-group.  So, the final structure of our many-core\ndedispersion algorithm consists of two-dimensional work-groups.  In\nthis way a work-group is associated with more than one DM, so that its\nwork-items can either collaborate to load the necessary elements from\nglobal to \\textit{local memory} (a fast memory area that is shared\nbetween the work-items of a same work-group) or rely on the cache.\n\nThe general structure of the algorithm can be specifically\ninstantiated by configuring five user-controlled parameters.  Two\nparameters control the number of work-items per work-group\nin the time and DM dimensions, thus regulating\nparallelism. Two parameters control the number of\nelements per work-item. The last\nparameter specifies whether local memory or\ncache are employed for data reuse. These parameters determine source code generation at run time.\nBecause we do not know, a priori, the optimal configuration of these parameters, we rely on auto-tuning, i.e.\\ we try all meaningful combinations and select the best performing one.\n\n\n\n\n\n\\section{Experimental Setup}\n\\label{sec:experimental_setup}\n\nIn this section we describe how the experiments are carried out; all\ninformation necessary for replication is detailed in \\citet{sclocco2014}.\nTable~\\ref{tab:Platforms} lists  the platforms  used, and reports\nbasic details such as number of OpenCL Compute Elements (CEs) (i.e.\\ cores), peak performance, peak memory bandwidth and thermal design power (TDP).\n\n\\begin{table}\n\\centering\n\\footnotesize{\\begin{tabular}{| l | r | r | r | r |}\n\\hline\n\\textbf{Platform} & \\textbf{CEs} & \\textbf{GFLOP} & \\textbf{GB} & \\textbf{Watt} \\\\\n\\textbf{} & \\textbf{}    & \\textbf{/s} & \\textbf{/s} & \\textbf{} \\\\\n\\hline\nAMD HD7970 & $64 \\times 32$ & 3,788 & 264 & 250 \\\\\n\\hline\nNVIDIA GTX 680 & $192 \\times 8$ & 3,090 & 192 & 195 \\\\\n\\hline\nNVIDIA GTX Titan & $192 \\times 14$ & 4,500 & 288 & 250 \\\\\n\\hline\nNVIDIA K20 & $192 \\times 13$ & 3,519 & 208 & 225 \\\\\n\\hline\nIntel Xeon Phi 5110P & $2 \\times 60$ & 2,022 & 320 & 225 \\\\\n\\hline\nIntel Xeon E5-2620 & $6 \\times 1$ & 192 & 42 & 95 \\\\\n\\hline\n\\end{tabular}}\n\\caption{Characteristics of the used platforms.}\n\\label{tab:Platforms}\n\\end{table}\n\nWe run the same code on every many-core accelerator; the source code\\footnote{https://github.com/isazi/Dedispersion} is implemented in C++ and OpenCL\\@.\nThe accelerators are part of the Distributed ASCI Supercomputer 4 (DAS-4)~\\footnote{http://www.cs.vu.nl/das4/}.\nAs dedispersion is usually part of a larger pipeline,  input and output\nare assumed to reside on the device. We thus do not measure data transfers over the PCI-e bus.\n\n\\begin{table}\n\\centering\n\\footnotesize{\\begin{tabular}{| l | r | r | r | r |}\n\\hline\n\\textbf{Survey} & \\textbf{s (Hz)}& \\textbf{BW (MHz)} &  \\textbf{n$_{chan}$} & \\textbf{f (MHz)}  \\\\\n\\hline\nLOFAR   & 200,000 & 6     & 32 & 138$-$145 \\\\\n\\hline\nApertif & 20,000  & 300 & 1,024 & 1,420$-$1,729 \\\\\n\\hline\n\\end{tabular}}\n\\caption{Survey name, sampling rate $s$, bandwidth $BW$, total number of\n  channels n$_{chan}$ and frequency range $f$ for the two experimental setups.}\n\\label{tab:Surveys}\n\\end{table}\n\n\nThe experimental set ups (Table~\\ref{tab:Surveys}) are based on two different pulsar surveys,\none for a hypothetical high-time resolution LOFAR survey comparable to the LOFAR Pilot Pulsar\nSurvey \\citep{clh+14} and one for the planned Apertif pulsar/FRB survey \\citep{leeu14}.\nFor both, trial DMs start at 0 and increment by  0.25 $pc/cm^{3}$.\nThese two setups stress different characteristics of the algorithm --\nthe Apertif setup, at 20~MFLOP per DM, is three times more compute\nintensive than LOFAR at 6~MFLOP per DM\\@.\nYet the higher Apertif frequencies cause reduced  delays, with more potential for data reuse.\nWe thus try two complementary scenarios: one is more computationally intensive, but potentially offers more data reuse, and one that is less computationally intensive, but precludes almost any data reuse.\n\nWe auto-tune the five algorithm parameters described in\nSection~\\ref{sec:dedispersion_kernel}, for each of the six platforms of Table~\\ref{tab:Platforms}, in both observational setups.\nWe use 12 different input instances between 2$-$4,096\n\\citep{sclocco2014}, and measure our performance metric, the number of single precision floating-point operations per second.\n\n\n\n\\section{Results}\n\\label{sec:results}\n\n\n\\subsection{Auto-Tuning}\n\\label{sub:auto_tuning}\n\nFor the Apertif case,\n the optimal number of work-items per work-group identified by\nauto-tuning is the highest for the GTX 680 (1,024). The other three GPUs require between 256 and 512, while the two Intel platforms\nrequire the lowest number (i.e.\\ between 16 and 128). As detailed in\n\\citet{sclocco2014}, the optimal configuration is more variable with\nsmaller input instances, where optimization\nspace is small.\n\nResults for the LOFAR setup are more stable. It has less available\ndata reuse, easing the identification of the optimum. The number of  work-items per work-group\nstay similar for the GPUs, and is somewhat lowered for the Intel platforms.\n\nEven for similar total numbers of work-items per work-group, the two\nunderlying parameters (Section~\\ref{sec:dedispersion_kernel}) may\ndiffer.  For e.g.\\ the HD7970, the Apertif work-group is as $8 \\times\n32$ work-items, while LOFAR it is composed by $64 \\times 4$\nwork-items.  In the Apertif setup the auto-tuning identifies a\nconfiguration that intensively exploits the available data reuse,\nwhile in the LOFAR setup the optimal configuration relies more on the\ndevice occupancy.\n\nThis result is important:\naccelerated dedispersion algorithm has no single\noptimal configuration -- it must be adapted to the exact observational\nsetup.\n\nThe subsequent auto tuning the amount of work per work-item\nagain exploits each accelerator's advantage, such as the high number of registers in the K20 and\nTitan, adapting the algorithm to different scenarios \\citep{sclocco2014}.\n\nThe last tunable parameter is the explicit use of local memory,\npresent on the GPUs, to exploit data reuse, over just hardware\ncache. Again auto-tuning adjusted the interaction of different parameters\nfor highest performance dedispersion in each platform.\n\nIn summary, we find that optimal configurations cannot be identified a\npriori, and that auto-tuning is the only feasible way to properly\nconfigure the dedispersion algorithm, because of the number and\ninteraction of parameters, and their impact on AI\\@.\n\n\n\n\\subsection{Impact of Auto-Tuning on Performance}\n\\label{sub:impact_of_auto_tuning}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{PerfApertif}\n\\caption{Performance of auto-tuned dedispersion, Apertif (higher is better).}\n\\label{fig:PerfApertif}\n\\end{figure}\n\nIn Fig.~\\ref{fig:PerfApertif} we show the performance achieved by\nauto-tuned dedispersion for the Apertif case.\nAll platforms show a performance increase with the dimension of the\ninput instance, and plateauing afterwards.\nNote how the tuned algorithm scales better than linearly up to this maximum, and then scales linearly.\nThe HD7970 achieves highest performance, the Intel CPU and the Xeon\nPhi are at the bottom, and the three NVIDIA GPUs occupy the middle of this figure.\nOn average, the HD7970 is 2.7 times faster than the NVIDIA GPUs, and 11.3 times faster than the Intel CPU and Xeon Phi; the Phi is 1.5 times faster than the multi-core CPU.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{PerfLOFAR}\n\\caption{Performance of auto-tuned dedispersion, LOFAR (higher is better).}\n\\label{fig:PerfLOFAR}\n\\end{figure}\n\nThe results for the LOFAR scenario (Fig.~\\ref{fig:PerfLOFAR}) are different.\nAbsolute performance is lower.\nThis is caused by the reduced available data reuse, resulting in lower\nalgorithm AI\\@.\nNext, the GPUs are closer in performance; now the HD7970 is only 1.4\ntimes faster than the GTX Titan.\nWith less data reuse available, memory bandwidth becomes increasingly\nmore important, leveling the differences. On average, the HD7970 is just 1.9 times faster than the NVIDIA GPUs, and 6 times faster than the Intel CPU and Xeon Phi.\n\nIn both figures, only scenarios above the ``\\textit{real-time}'' line can dedisperse one second of data in less than one second of computation.\nThis is a fundamental requirement for modern radio telescopes, whose\n extreme data rate precludes data storage and off-line\nprocessing. We show that GPUs can scale to bigger instances and/or to a multi-beam scenario, and still satisfy the real-time constraints.\n\n\n\nWhat was the impact of auto-tuning on performance?\nFigure~\\ref{fig:HD7970HistApertif} shows the histogram of performance in the optimization space.\nThe optimum lies far from the typical configuration.\nBecause optimal configurations depends on multiple parameters, like the platform used to execute the algorithm, and specific observational parameters, it will not be trivial, even for an experienced user, to manually select the best configuration without extensive tuning.\nThis claim is also supported by the high signal-to-noise ratio of the optimal configurations, as detailed in \\citet{sclocco2014}.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{HD7970HistApertif}\n\\caption{Example of a performance histogram, for the case or Apertif 2,048 DMs.}\n\\label{fig:HD7970HistApertif}\n\\end{figure}\n\nIn summary, we satisfy a realistic real-time\nconstraint in almost every test case, which allows for massive\nmulti-beaming.\nOptimal configurations are difficult to guess in this optimization space, and therefore auto-tuning has a critical impact on the performance.\n\n\n\n\\subsection{Data Reuse and Performance Limits}\n\\label{sub:data_reuse_and_performance_limits}\n\n\nTo simulate a scenario with perfect data reuse, we tune and measure the performance of dedispersion using the value zero for all DM trials.\nIn the case of\nFor Apertif, the difference is negligible (cf.~Fig.~11 in\n\\citealt{sclocco2014}). Data reuse was already maximized.\nFor the LOFAR setup (cf.~Fig.~12 in \\citealt{sclocco2014}), the performance improves, approaching the Apertif setup.\nThe increased data reuse is exploited until the hardware is saturated.\n\nPerformance is predominantly determined by the amount of possible data reuse, which\nis a function of real-life frequencies and DM values.\nThis tests our hypothesis that even\nwith perfect data reuse,  high AI cannot be  achieved  because of the\nlimitations of real hardware (in contrast with the conclusions of~\\citealt{barsdell2010}).\nWe therefore conclude that brute-force dedispersion is memory-bound for every practical and realistic scenario.\n\n\n\n\\section{Discussion}\n\\label{sec:discussion}\n\nWe first compare the performance of the \\emph{auto-tuned} versus a \\textit{generically tuned}  algorithm.\nIn the latter, tuning is confined to a mono-dimensional configuration of the work-groups and the work-items compute only one DM value.\nThis strategy is widely applied by programmers; but as the\nalgorithm's AI is unaffected, data reuse is not optimized.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{SpeedupNoReuseApertif}\n\\caption{Speedup over a generically tuned configuration, Apertif\n  (higher is better). The HD7970 GPU is missing because\n  there are no valid configurations of the\n  algorithm that do not exploit data reuse.\n}\n\\label{fig:SpeedupNoReuseApertif}\n\\end{figure}\n\nWe find that \\emph{auto-tuned} dedispersion on GPUs is $\\sim$5 times faster than\n\\emph{generically tuned} in the Apertif setup (Fig.~\\ref{fig:SpeedupNoReuseApertif}), and still 1.5--2.5 times faster in the LOFAR setup.\nEffectively exploiting data reuse is a main performance driver for a high-performance dedispersion algorithm, especially for those platforms where the gap between floating point peak performance and memory bandwidth is wide.\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{SpeedupLOFAR}\n\\caption{Speedup over a parallel CPU implementation, LOFAR (higher is better).}\n\\label{fig:SpeedupLOFAR}\n\\end{figure}\n\nWe also compare the performance of our auto-tuned algorithm to an optimized CPU version.\nThis CPU version of the algorithm is parallelized using OpenMP instead of OpenCL, with different threads computing different chunks of DM values and time samples.\nIt includes all meaningful optimizations described in Section~\\ref{sub:parallelization}, and is vectorized using Intel's Advanced Vector Exensions (AVX) intrinsics.\nLike the OpenCL version, it is automatically auto-tuned.\nFor both observational setups (cf.~Fig.~\\ref{fig:SpeedupLOFAR} for LOFAR), the many-core implementation is considerably faster than a highly optimized, and tuned, CPU implementation.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{XeonComparisonApertif}\n\\caption{Comparing OpenCL and OpenMP implementations on the CPU and Xeon Phi, Apertif (higher is better).}\n\\label{fig:XeonComparisonApertif}\n\\end{figure}\n\n\nThe OpenMP code generator was also extended to generate native code for the Xeon Phi, and we compared the Intel CPU and Xeon Phi, using both OpenCL and OpenMP\\@.\nIn the Apertif case (Figure~\\ref{fig:XeonComparisonApertif}) the OpenCL implementation is faster than the OpenMP one for smaller inputs, while the opposite is true for larger ones, and this behavior holds for both platforms.\nIn the LOFAR case, however, this same behavior applies only to the Xeon Phi, with the Xeon E5-2620 showing a performance degradation for bigger inputs that is consistent with the experiments of~\\cite{armour2012}.\nOverall improvements to the Intel OpenCL compiler could improve the Xeon Phi's performance, as we were able to achieve a 51\\% improvement in performance on this platform using a tuned OpenMP implementation.\n\n\\subsection{Multi-Beam Performance}\n\nIn the conclusions of Section~\\ref{sub:impact_of_auto_tuning} we highlighted how dedispersion performance can allow for  multi-beam scenarios.\nApertif will be need to dedisperse 2,000 DMs in real-time for 450 different beams, thus requiring 18.5 TFLOP/s just for dedispersion.\nUsing our best performing accelerator, the AMD HD7970, it is possible to dedisperse 2,000 DMs in 0.08 seconds; 9 beams per GPU could be dedispersed in real-time. Enough memory is available.\nTherefore, the 18.5 TFLOP/s dedispersion instrument for Apertif could be implemented using 2015 technology with $\\sim$50 GPUs.\nWith a HD7970 model with double the memory,  12 beams could be dedispersed at once, reducing the system size to only 38 GPUs.\n\nFor its real-life impact, we compute an upper bound on the power necessary for dedispersion on Apertif from the TDPs (Table~\\ref{tab:Platforms}): the proposed GPU solution requires 12.5~kW while the traditional CPU solution requires 46.5~kW.\nWe can therefore conclude that, in this scenario, using many-core accelerators does not only provide a 9.8 times reduction in the size of the system, when compared to a traditional CPU-based solution, but it also lowers the power consumption by a factor of 3.7.\nThis improvement in energy efficiency is in part architectural, and in part caused by auto-tuning.\nUsing the results of Section~\\ref{sub:impact_of_auto_tuning} it is possible to determine that the power that would be required by an average configuration is 22.5 kW:\\@ we can thus conclude that auto-tuning is responsible for around 50\\% of the total power savings.\n\n\n\\section{Related Work}\n\\label{sec:related_works}\n\nIn the literature, auto-tuning is considered a viable technique to achieve performance that is both high and portable.\nIn particular, \\cite{li2009} show that it is possible to use auto-tuning to improve performance of even highly-tuned algorithms, and \\cite{datta2008} affirm that application specific auto-tuning is the most practical way to achieve high performance on multi-core systems.\nHighly relevant here is the work of~\\cite{du2012}, with whom we  agree  that auto-tuning can be used as a performance portability tool, especially with OpenCL\\@.\nAnother attempt at achieving performance portability on heterogeneous systems has been made by~\\cite{phothilimthana2013}, and while their approach focuses on the compiler, it still relies on auto-tuning to map algorithms and heterogeneous platforms in an optimal way.\nIn recent years, we have been working on parallelizing and implementing radio astronomy kernels on multi and many-core platforms \\citep{nieuwpoort2011}, and using auto-tuning to achieve high performance on applications like the LOFAR beam former \\citep{sclocco2012}.\nWhat makes our current work different, is that we do not only use auto-tuning to achieve high performance, but measure its impact, and show that the optimal configurations are difficult to guess without thorough tuning.\n\nWhile there are no previous attempts at auto-tuning dedispersion for many cores, there are a few previous GPU implementations documented in literature.\nFirst, in \\cite{barsdell2010} dedispersion is listed as a possible candidate for acceleration, together with other astronomy algorithms.\nWhile we agree that dedispersion is a potentially good candidate for many-core acceleration because of its inherently parallel structure, we believe their performance analysis to be too optimistic, and the AI estimate in \\citet{barsdell2010} to be unrealistically high.\nIn fact, we showed in this paper that dedispersion's AI is low in all realistic scenarios, and that the algorithm is inherently memory-bound.\nThe same authors implemented, in a follow-up paper~\\citep{barsdell2012b}, dedispersion for NVIDIA GPUs, using CUDA as their implementation framework.\nHowever, we do not completely agree with the performance results presented in \\citet{barsdell2012b} for two reasons: first, they do not completely exploit data reuse, and we have shown here how important data reuse is for performance; and second, part of their results are not experimental, but derived from performance models.\n\nAnother GPU implementation of the dedispersion algorithm is presented in~\\cite{magro2011}.\nAlso in this case there is no mention of exploiting data reuse.\nIn fact, some of the authors of this paper published, shortly after~\\cite{magro2011}, another short paper \\citep{armour2012} in which they affirm that their previously developed algorithm does not perform well enough because it does not exploit data reuse.\nUnfortunately, this paper does not provide sufficient detail on either the algorithm or on experimental details, such as frequencies and time resolution, for us to repeat their experiment.\nTherefore, we cannot verify the claimed 50\\% of theoretical peak performance.\nHowever, we believe this claim to be unrealistic because dedispersion has an inherently low AI, and it cannot take advantage of fused multiply-adds, which by itself already limits the theoretical upper bound to 50\\% of the peak on the used GPUs.\n\nA different approach to both brute-force and tree based dedispersion has been proposed by \\citet{zackay2014}.\nThis new algorithm has lower computational complexity than brute-force dedispersion, and appears to be faster than the implementations of both \\citet{magro2011} and \\citet{barsdell2012b}.\nAlthough the experimental results highlighted in our paper show higher performance than the results presented in \\citet{zackay2014}, we believe this algorithm has potential; and that it, too, could benefit from auto-tuning to further improve performance in real-life implementations.\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nIn this paper, we analyzed the brute-force dedispersion algorithm, and presented  our many-core implementation. We analytically proved that dedispersion is a memory-bound algorithm and that, in any real world scenario, its performance is limited by low arithmetic intensity.\nWith the experiments presented in this paper, we also demonstrated that by using auto-tuning it is possible to obtain high performance for dedispersion.\nEven more important, we showed that auto-tuning makes the algorithm portable between different platforms and different observational setups.\nFurthermore, we highlighted how auto-tuning permits to automatically exploit the architectural specificities of different platforms.\n\nMeasuring the performance of the tuned algorithm, we verified that it scales linearly with the number of DMs for every tested platform and observational setup.\nSo far, the most suitable platform to run dedispersion among the ones we tested, is a GPU from AMD, the HD7970.\nThis GPU performs 2--9 times better than the other accelerators when extensive data reuse is available, and remains 1.4--6 times faster even in less optimal scenarios.\nOverall, the GPUs are 4.9--7.5 and 3.8 times faster than the CPU and Xeon Phi in the two scenarios analyzed in this work.\nWe conclude that, at the moment, GPUs are the best candidate for brute-force dedispersion.\n\nIn this paper, we showed that using the HD7970 GPU it would be possible to\nbuild the 18.5 TFLOP/s dedispersion instrument for Apertif using just $\\sim$50 accelerators, thus reducing the system size of a factor 9.8 and the power requirements of a factor 3.7.\nThese improvements are only in part architectural, and mostly a result of optimal tuning of the algorithm.\nWe will continue to compare platforms as the design date for SKA approaches.\n\nAnother important contribution of this paper is the quantitative evidence of the impact that auto-tuning has on performance.\nWith our experiments, we showed that the optimal configuration is difficult to find manually and lies far from the average.\nMoreover, we showed that the auto-tuned algorithm is faster than a generically tuned version of the same algorithm on all platforms, and is an order of magnitude faster than an optimized CPU implementation.\n\nFinally, our last contribution was to provide further empirical proof that brute-force dedispersion is a memory-bound algorithm, limited by low AI\\@.\nIn particular, we showed that achievable performance is limited by the amount of data reuse that dedispersion can exploit, and the available data reuse is affected by parameters like the DM space and the frequency interval.\nWe also showed that, even in a perfect scenario where data reuse is unrealistically high, the performance of dedispersion is limited by the constraints imposed by real hardware, and approaching the theoretical AI bound of the algorithm becomes impossible.\n\n\n\n\\bibliographystyle{elsarticle-num-names}\n\\bibliography{Bibliography}\n\n\n", "itemtype": "equation", "pos": 9763, "prevtext": "\nwhere\n$\\epsilon$ represents the effect of accessing the delay table and\nwriting the output.\nThis low AI shows that brute-force dedispersion\nis memory bound on most architectures. Its\nperformance is thus limited not by computational\ncapabilities, but by memory bandwidth.  One way to increase\nAI and thus improve\nperformance, is to reduce the number of reads from global memory, by\nimplementing some form of data reuse.  In\nAlgorithm~\\ref{alg:Dedispersion} some data reuse appears possible.\nFor some frequencies, the delay\nis the same for two close DMs, $dm_{i}$ and $dm_{j}$, so that\n$\\Delta(c, dm_{i}) = \\Delta(c, dm_{j})$.  Then, one input\nelement provides two different sums. With data reuse,\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:DedispersionAIp}\n\\footnotesize{AI < \\frac{d \\times s \\times c}{4 \\times ((s \\times c) + (d \\times s) + (d \\times c))}  = \\frac{1}{4 \\times (\\frac{1}{d} + \\frac{1}{s} + \\frac{1}{c})}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\footnotesize{AI&lt;\\frac{d\\times s\\times c}{4\\times((s\\times c)+(d\\times s)+(d%&#10;\\times c))}=\\frac{1}{4\\times(\\frac{1}{d}+\\frac{1}{s}+\\frac{1}{c})}}\" display=\"block\"><mrow><mrow><mi mathsize=\"80%\">A</mi><mo>\u2062</mo><mi mathsize=\"80%\">I</mi></mrow><mo mathsize=\"80%\" stretchy=\"false\">&lt;</mo><mfrac><mrow><mi mathsize=\"80%\">d</mi><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mi mathsize=\"80%\">s</mi><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mi mathsize=\"80%\">c</mi></mrow><mrow><mn mathsize=\"80%\">4</mn><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mi mathsize=\"80%\">s</mi><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mi mathsize=\"80%\">c</mi></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mi mathsize=\"80%\">d</mi><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mi mathsize=\"80%\">s</mi></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mi mathsize=\"80%\">d</mi><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mi mathsize=\"80%\">c</mi></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mfrac><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mfrac><mn mathsize=\"80%\">1</mn><mrow><mn mathsize=\"80%\">4</mn><mo mathsize=\"80%\" stretchy=\"false\">\u00d7</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mfrac><mn mathsize=\"80%\">1</mn><mi mathsize=\"80%\">d</mi></mfrac><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mfrac><mn mathsize=\"80%\">1</mn><mi mathsize=\"80%\">s</mi></mfrac><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mfrac><mn mathsize=\"80%\">1</mn><mi mathsize=\"80%\">c</mi></mfrac></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}]