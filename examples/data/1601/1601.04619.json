[{"file": "1601.04619.tex", "nexttext": "\n\n\\noindent{in which $p_x(k)$ and $p_y(k)$ are the gradients of the $k^{th}$ pixel in the patch $w$ on $x$ and $y$ directions. The SVD of the gradient matrix, $G$, is defined as }\n\n\n", "itemtype": "equation", "pos": 10348, "prevtext": "\n\n\\title{Comparison-based Image Quality Assessment for Parameter Selection}\n\n\n\\author{\\IEEEauthorblockN{Haoyi Liang,}{\\huge {\\tiny }}\n\\IEEEauthorblockA{\\textit{Student Member, IEEE,} and}\n\\and\n\\IEEEauthorblockN{Daniel S. Weller,}\n\\IEEEauthorblockA{\\textit{Member, IEEE}}\n\n\\thanks{Haoyi Liang and Daniel S. Weller are with the Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA 22904 USA (email: hl2uc@virginia.edu, dweller@virginia.edu).}\n}\n\n\\maketitle\n\n\\begin{abstract}\nImage quality assessment (IQA) is traditionally classified into full-reference (FR) IQA and no-reference (NR) IQA according to whether the original image is required. Although NR-IQA is widely used in practical applications, room for improvement still remains because of the lack of the reference image. Inspired by the fact that in many applications, such as parameter selection, a series of distorted images are available, the authors propose a novel comparison-based image quality assessment (C-IQA) method. The new comparison-based framework parallels FR-IQA by requiring two input images, and resembles NR-IQA by not using the original image. As a result, the new comparison-based approach has more application scenarios than FR-IQA does, and takes greater advantage of the accessible information than the traditional single-input NR-IQA does. Further, C-IQA is compared with other state-of-the-art NR-IQA methods on two widely used IQA databases. Experimental results show that C-IQA outperforms the other NR-IQA methods for parameter selection, and the parameter trimming framework combined with C-IQA saves the computation of iterative image reconstruction up to 80\\%.\n\n\\textit{Index Terms}---Image distortion, image quality assessment (IQA), human visual system (HVS), comparison-based image quality assessment (C-IQA), parameter selection\n\n\n\\end{abstract}\n\n\n\\section{Introduction}\nObtaining an image with high perceptual quality is the ultimate goal of many image processing problems, such as image reconstruction, denoising and inpaiting. However, measuring the perceptual image quality by subjective experiment is time-consuming and expensive, so designing an image quality assessment (IQA) algorithm that agrees with the human visual system (HVS)\\cite{HSV_neural,HSV_IQA1,HSV_IQA2,HSV_IQA3,HSV_IQA4} is a foundational image processing objective. Moreover, most image restoration algorithms require one or more parameters to regulate the restoration process, and no-reference IQA methods can be used to guide selecting the parameters. For instance, the regularization parameter of image reconstruction\\cite{parameter_trimming} is selected by a no-reference image quality index\\cite{MetricQ}. However, most existing no-reference IQA algorithms output the estimated image quality based on a single distorted image, ignoring that different degraded images can provide more information together to the quality estimation of each degraded image. This observation inspires us to develop a comparison-based IQA method to fill the gap between the increasing need of parameter selection for image processing algorithms and the lack of such a NR-IQA algorithm that makes full use of the available information.\n\nIQA algorithms are classified based on whether the reference image (the distortion-free image) is required: full-reference (FR),  reduced-reference (RR) and no-reference (NR). FR-IQA\\cite{FR_IQA_review_OSU,SSIM,FR_IQA3,FR_IQA4,FR_IQA5,FR_IQA6} is a relatively well-studied area. Traditional methods like mean squared error (MSE) and signal-to-noise ratio (SNR) are used as the standard signal fidelity indices\\cite{IQA_review}. A more sophisticated FR-IQA algorithm, Structural Similarity Index Method (SSIM)\\cite{SSIM}, considers the structure information in images and performs well in different studies\\cite{IQA_review, SSIM_cite1,SSIM_cite2,SVD_IQA}. RR-IQA algorithms\\cite{RR_IQA1,RR_IQA2,RR_IQA3} require some statistical features of the reference image, such as the power spectrum, and measure the similarity of these features from the reference image and the distorted image. NR-IQA algorithms adopt two different approaches. The first kind of NR-IQA\\cite{DIVIINE,BRISQUE,anisotropy,NR_IQA4,NR_IQA5,NR_IQA6,NR_IQA7} has a similar approach to the RR-IQA. The difference is that rather than extracting the features from the reference images, this kind of NR-IQA extracts statistical features from a training set. The second kind of NR-IQA algorithms\\cite{MetricQ,SVD_IQA} adopts a local approach to quantifying structure as a surrogate for quality. A common implementation of the second approach calculates local scores by analyzing the quality of gradient. The overall score is synthesized by taking the average of the local scores.\n\nAmong these three kinds of IQA algorithms, speed and accuracy generally decrease from FR-IQA, RR-IQA to NR-IQA progressively. Unfortunately, reference images do not exist in many cases. Noticing that in many applications, including parameter selection and comparison of different restoration algorithms, we compare a set of distorted images with the same image content, a new comparison-based IQA (C-IQA) method is proposed in this paper. The prototype of comparison-based IQA is reflected in \\cite{anisotropy}. However, the concept of comparison in \\cite{anisotropy} is just implicitly mentioned by sorting the overall image qualities of a series of distorted images. In our work, the comparison-based framework is built from low level image structures, and the final output is a graph that can illustrate the local relative quality. \n\nAfter proposing the comparison-based IQA method, we demonstrate one of its applications, parameter selection. The framework of parameter trimming, first proposed in \\cite{parameter_trimming}, is designed to boost the parameter selection by combining NR-IQA with parameter selection. In \\cite{parameter_trimming}, parameters that do not show the potential to obtain the best result are cut during the convergence process.\n\nThe rest of the paper is organized as follows. Section \\ref{NR-IQA} introduces and compares different NR-IQA methods. Section \\ref{C-IQA} elaborates on the implementation details of C-IQA. The framework of parameter trimming and the technique used for image reconstruction are introduced in Section \\ref{parameter_trimming}. In Section \\ref{experiments} experiments are conducted on two widely used IQA databases, LIVE\\cite{LIVE_dataset} and CSIQ\\cite{FR_IQA_review_OSU}, to verify the accuracy of C-IQA and demonstrate the effectiveness of parameter trimming combined with C-IQA. Section \\ref{conclusion} reviews the novelty and experimental results of C-IQA and suggests extensions to comparison-based IQA. \n\n\n\\section{Existing NR-IQA methods}  \n\\label{NR-IQA}\nExisting NR-IQA algorithms can be classified into two types\\cite{SVD_IQA}: global approaches and local approaches. The output of global approaches is a scalar number that indicates the overall quality of the image. The local approaches estimate the quality in each local patch, and an overall quality index is obtained by taking the average of the local quality indices.\n\n\\subsection{Global Approach}\nThe rationale behind global approaches\\cite{DIVIINE,BRISQUE,anisotropy,NR_IQA4,NR_IQA5,NR_IQA6,NR_IQA7} is that the distributions of natural scene statistics (NSS) share certain common characteristics among distortion-free images, and distortions will change these characteristics. For example, it is widely-accepted that the wavelet coefficients of a natural image can be modeled by a generalized Gaussian distribution (GGD)\\cite{GGD1,GGD2}.  \n\nBecause the NSS are extracted from the whole image, the final output of global approaches is a scalar number that indicates the overall quality. The advantage of global NR-IQA algorithms is that most of them are not dedicated to a specific distortion since the NSS features are a high-dimensional vector designed to be sensitive to various distortions. However, because of the high dimensionality of the statistical feature space, it is difficult to individually interpret and analyze these features quantitatively, and thus feature selection is largely an empirical work. In BRISQUE\\cite{BRISQUE}, the authors treat this approach like a black box. Another drawback of the global approach is that computing these NSS features is usually time-consuming. \n\n\\subsection{Local Approach}\nThe output of the local approach\\cite{MetricQ,anisotropy,SVD_IQA} can be a graph, which illustrates the local image quality, or a scalar number by taking the average of the graph. Because in most cases the perceived distortion varies across regions in an image, an innate advantage of the local approach is that they are able to highlight the areas where distortion is most significant. At the same time, an overall quality index is easily obtained from the local quality index. \nBecause human eyes are highly sensitive to the gradient in images, and the information in images can be well represented by their gradient\\cite{SSIM,MetricQ,Gradient_IQA}, the local quality index is usually evaluated using the spatial gradient information. However, the amount of the gradient, or total variation, itself is not a stable indicator of the quality\\cite{SVD_IQA}. Previous works\\cite{MetricQ, SVD_IQA,Structure_IQA} have shown that assessing the quality of the gradient in an image can be a promising way to evaluate the image quality. Among these works, MetricQ\\cite{MetricQ} shows encouraging results choosing denoising parameters. The underlying rationale of MetricQ is that the more concentrated the gradient direction is, the better the quality of the patch is. It is a reasonable assumption since both of the two most common distortions, noise and blurring, disperse the distributions of the gradient direction. Because C-IQA makes use of the quality index defined in MetricQ, we introduce MetricQ in detail in the next paragraphs.\n\nThe local quality index used by MetricQ is based on singular values of the local gradient matrix, which have been widely used as low level features in different image processing problems, such as tracking feature selection\\cite{SVD_tracking}, recognition\\cite{SVD_rec} and image quality assessment \\cite{SVD_IQA}. For each $n\\times n$ local patch ($w$), the gradient matrix is\n\n\n", "index": 1, "text": "\\begin{equation}\nG =\n\\left[ {\\begin{array}{cc}\n\\vdots & \\vdots \\\\\np_x(k) & p_y(k) \\\\\n\\vdots & \\vdots\n\\end{array} } \\right],\n\\label{gradient_mtx}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"G=\\left[{\\begin{array}[]{cc}\\vdots&amp;\\vdots\\\\&#10;p_{x}(k)&amp;p_{y}(k)\\\\&#10;\\vdots&amp;\\vdots\\end{array}}\\right],\" display=\"block\"><mrow><mrow><mi>G</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>p</mi><mi>y</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\noindent{where $U$ and $V$ are both orthonormal matrices. Vector $V_1$ is of size $2\\times 1$ and corresponds to the dominant direction of the local gradient; $V_2$ is orthogonal to $V_1$ and thus represents the edge direction. Singular values, $s_1$ and $s_2$, represent the luminance variances on $V_1$ and $V_2$ respectively. Intuitively, a large $s_1$ and a small $s_2$ indicate a prominent edge in the local patch.}\n\nIn MetricQ\\cite{MetricQ}, two indices reflect the quality of a local patch: Image Content Index and Coherence Index. Image Content Index is defined as\n\n", "itemtype": "equation", "pos": 10688, "prevtext": "\n\n\\noindent{in which $p_x(k)$ and $p_y(k)$ are the gradients of the $k^{th}$ pixel in the patch $w$ on $x$ and $y$ directions. The SVD of the gradient matrix, $G$, is defined as }\n\n\n", "index": 3, "text": "\\begin{equation}\nG = USV^T = U\n\\left[ {\\begin{array}{cc}\ns_1 & 0 \\\\\n 0 & s_2\n\\end{array} } \\right]\n{\\left[ {\\begin{array}{cc}\nV_1 & V_2 \\\\\n\\end{array} } \\right]}^T,\n\\label{SVD_definiation}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"G=USV^{T}=U\\left[{\\begin{array}[]{cc}s_{1}&amp;0\\\\&#10;0&amp;s_{2}\\end{array}}\\right]{\\left[{\\begin{array}[]{cc}V_{1}&amp;V_{2}\\\\&#10;\\end{array}}\\right]}^{T},\" display=\"block\"><mrow><mrow><mi>G</mi><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>s</mi><mn>1</mn></msub></mtd><mtd columnalign=\"center\"><mn>0</mn></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><msub><mi>s</mi><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><msup><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"center\"><msub><mi>V</mi><mn>1</mn></msub></mtd><mtd columnalign=\"center\"><msub><mi>V</mi><mn>2</mn></msub></mtd></mtr></mtable><mo>]</mo></mrow><mi>T</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\noindent{and Coherence Index is defined as}\n\n", "itemtype": "equation", "pos": 11466, "prevtext": "\n\\noindent{where $U$ and $V$ are both orthonormal matrices. Vector $V_1$ is of size $2\\times 1$ and corresponds to the dominant direction of the local gradient; $V_2$ is orthogonal to $V_1$ and thus represents the edge direction. Singular values, $s_1$ and $s_2$, represent the luminance variances on $V_1$ and $V_2$ respectively. Intuitively, a large $s_1$ and a small $s_2$ indicate a prominent edge in the local patch.}\n\nIn MetricQ\\cite{MetricQ}, two indices reflect the quality of a local patch: Image Content Index and Coherence Index. Image Content Index is defined as\n\n", "index": 5, "text": "\\begin{equation}\nQ = s_1\\frac{s_1-s_2}{s_1+s_2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"Q=s_{1}\\frac{s_{1}-s_{2}}{s_{1}+s_{2}},\" display=\"block\"><mrow><mrow><mi>Q</mi><mo>=</mo><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>\u2062</mo><mfrac><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>-</mo><msub><mi>s</mi><mn>2</mn></msub></mrow><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>+</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\n$Q$ reflects the structure prominence in a local patch and $R$ is used to determine whether a local patch is dominated by noise. \nThe overall score of an image is calculated by\n\n", "itemtype": "equation", "pos": 11575, "prevtext": "\n\\noindent{and Coherence Index is defined as}\n\n", "index": 7, "text": "\\begin{equation}\nR = \\frac{s_1-s_2}{s_1+s_2}.\n\\label{img_content_ind}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"R=\\frac{s_{1}-s_{2}}{s_{1}+s_{2}}.\" display=\"block\"><mrow><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>-</mo><msub><mi>s</mi><mn>2</mn></msub></mrow><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>+</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\noindent{where $M\\times N$ is the size of the image and $\\tau$ is the threshold to decide whether a local patch is dominated by noise. $Q(i,j)$ and $R(i,j)$ are the Image Content Index and Coherence Index of the local patch centered at $(i,j)$ in the image. A simplified interpretation of \\eqref{overall_MQ} is that $AQ$ is the average structure index of local patches that have meaningful image content.}\n\n\n\n However, because the view of the local approaches is constrained by the patch size, local approaches tend to confuse the sharpness with the blocking artifacts. \nFortunately, specialized IQA algorithms aimed at blocking artifacts can be used to evaluate this special distortion\\cite{NR_IQA6,blocking_IQA}.\n\n\n\n\n\\section{Comparison-based image quality assessment}\n\\label{C-IQA}\nPrevious works on IQA\\cite{SSIM,anisotropy,FR_IQA5,FR_IQA7,HSV_IQA4,HSV_IQA3,HSV_IQA2,HSV_IQA1} show that IQA performance can be significantly improved by taking advantage of the characteristics of HVS. For example, the structural information that human eyes are highly sensitive to is made use by SSIM\\cite{SSIM}. Traditional NR-IQA algorithms also try to exploit HVS features and make reasonable assumptions about the natural scene images, but one important aspect of HVS is ignored: comparison. In the subjective IQA experiment\\cite{FR_IQA_review_OSU}, volunteers are required to evaluate the quality of an image by comparing it with a reference image, rather than giving an absolute score for the image. Although in most image processing applications, the reference image does not exist, a set of differently degraded images are available. In these cases, extending existing state-of-the-art FR-IQA algorithms to comparison-based NR-IQA algorithms is a natural thought. However, different from FR-IQA algorithms, neither of the two input image qualities is known in the comparison-based IQA framework. As a result, in a comparison-based NR-IQA algorithm, we not only measure the difference between two input images, but also assess the quality of the difference. \n\n\n\n\n\n\n\n\\subsection{Framework of C-IQA}\n\n\\begin{figure*}[t]\n\t\\centering{\\includegraphics[width=\\linewidth,trim = {0 6cm 0 3.6cm}, clip]{flowchat}}\n\t\\caption{Flow Chart of Comparison-based IQA: $P_1$ and $P_2$ are local patches from input images, $I_1$ and $I_2$, at the same location respectively. The Content Detection module determines whether there is a meaningful structure in the difference patch; the Contribution module calculates which patch mainly contributes to the difference patch; the Texture Compensation module compensates the distortion sensitivity difference of patches with various texture complexities. The output, comparison-based index, indicates the relative quality of $P_1$ based on $P_2$.}\n\t\\label{C-IQA flowchat}\n\\end{figure*}\n\n\nAs shown in Fig. \\ref{C-IQA flowchat}, C-IQA has two input images, $I_1$ and $I_2$, and the output indicates the relative quality of $I_1$ based on $I_2$. We refer to the second image in C-IQA as the base image to distinguish it from the reference image in FR-IQA. C-IQA consists of two basic modules: Content Detection and Contribution. The third module, Texture Compensation, is optional and its description is deferred to Section \\ref{sec_textureCompensation}. In the rest of the paper, we refer to the comparison-based IQA variation composed by the two basic modules as C-IQA and the variation with three modules as CT-IQA. Content Detection determines whether the difference between two input images contains any meaningful structure, and Contribution decides which image mainly contributes to the difference. C-IQA composes these two modules by the criterion that the input image that contributes to a structured difference is better and the input image that contributes to a random difference is worse. The Texture Compensation module added in CT-IQA adjusts the distortion sensitivity difference of patches with different texture complexity\\cite{SSIM,sensitivity_texture_complexity}.\n\n\\subsubsection{Content Detection}\nThe Content Detection module is based on the Image Content Index put forward in MetricQ\\cite{MetricQ}. Different from MetricQ, this index is calculated with the difference image between two input images in C-IQA. In MetricQ, limited by the information provided by single input image, the algorithm does not know the texture complexity in the original image, and it is hard for an algorithm to tell how concentrated the gradient should be. However, by mimicking the comparative way HVS works, C-IQA removes the main image content in the images by taking the difference, and thus the Content Detection module is less influenced by the texture complexity in images.\n\n\\begin{algorithm}\n\\caption{Content Detection}\\label{content_detection}\n\\begin{algorithmic}\n\\State{$D_p = P_1 - P_2$}\n\\State{$G = [d_x(D_p)\\ d_y(D_p)]$} \n\\State{$USV^T=SVD(G)$}  \n\\State{$C_{ind} = \\frac{s_1-s_2}{s_1+s_2}$} \\Comment{$s_1 > s_2$}\n\\If{$C_{ind} > C_{thresh}$}\n\t\\State $is\\_stru = 1$  \\Comment{structure}\n\\Else\n\t\\State $is\\_stru = -1$ \\Comment{noise}\n\\EndIf\n\\end{algorithmic}\n\\end{algorithm}\n\nIn Alg. \\ref{content_detection}, $P_1$ and $P_2$ are two patches of size $n\\times n$ from $I_1$ and $I_2$ respectively, $G$ is the same 2-column gradient matrix defined in \\eqref{gradient_mtx}, $SVD(G)$ represents taking the SVD operation on $G$, and $s_1$ and $s_2$ are the singular values of $G$. $C_{thresh}$ is a constant threshold to binarize $C_{ind}$. The binary output $is\\_stru$ indicates whether there is a meaningful structure in the difference of local patches.\n\n\\subsubsection{Contribution}\nOnce the difference is classified into noise or structure, the Contribution module is designed to find out which of the two input images mainly contributes to the difference image. In our implementation, the luminance-normalized covariance between the input image and the difference image is used to measure the contribution. \n\n\\begin{algorithm}\n\\caption{Contribution}\\label{contribution_alg}\n\\begin{algorithmic}\n\\State $D_p = P_1 - P_2$\n\\State $M_p = max(\\frac{mean(P_1)+mean(P_2)}{2},\\frac{1}{n\\times n})$\n\\State $ctri1 = cov(P_1,D_p)$  \n\\State $ctri2 = cov(P_2,-D_p)$\n\\State $ctri = \\frac{ctri1 - ctri2}{M_p}$\n\\end{algorithmic}\n\\end{algorithm}\nIn Alg. \\ref{contribution_alg}, $mean(P_i)$ calculates the average of the local patch, and $cov(x_1,x_2)$ calculates the covariance between two input patches, \n", "itemtype": "equation", "pos": 11838, "prevtext": "\n\n$Q$ reflects the structure prominence in a local patch and $R$ is used to determine whether a local patch is dominated by noise. \nThe overall score of an image is calculated by\n\n", "index": 9, "text": "\\begin{equation}\nAQ = \\frac{1}{MN}\\sum_{i,j:R(i,j)>\\tau}Q(i,j),\n\\label{overall_MQ}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"AQ=\\frac{1}{MN}\\sum_{i,j:R(i,j)&gt;\\tau}Q(i,j),\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>Q</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>M</mi><mo>\u2062</mo><mi>N</mi></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>:</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mi>\u03c4</mi></mrow></mrow></munder><mrow><mi>Q</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\noindent{$x_1$ and $x_2$ are vectorized patches of size $n^2\\times 1$.}\\\\\n\nThe comparative quality index for each local patch is calculated by\n", "itemtype": "equation", "pos": 18349, "prevtext": "\n\\noindent{where $M\\times N$ is the size of the image and $\\tau$ is the threshold to decide whether a local patch is dominated by noise. $Q(i,j)$ and $R(i,j)$ are the Image Content Index and Coherence Index of the local patch centered at $(i,j)$ in the image. A simplified interpretation of \\eqref{overall_MQ} is that $AQ$ is the average structure index of local patches that have meaningful image content.}\n\n\n\n However, because the view of the local approaches is constrained by the patch size, local approaches tend to confuse the sharpness with the blocking artifacts. \nFortunately, specialized IQA algorithms aimed at blocking artifacts can be used to evaluate this special distortion\\cite{NR_IQA6,blocking_IQA}.\n\n\n\n\n\\section{Comparison-based image quality assessment}\n\\label{C-IQA}\nPrevious works on IQA\\cite{SSIM,anisotropy,FR_IQA5,FR_IQA7,HSV_IQA4,HSV_IQA3,HSV_IQA2,HSV_IQA1} show that IQA performance can be significantly improved by taking advantage of the characteristics of HVS. For example, the structural information that human eyes are highly sensitive to is made use by SSIM\\cite{SSIM}. Traditional NR-IQA algorithms also try to exploit HVS features and make reasonable assumptions about the natural scene images, but one important aspect of HVS is ignored: comparison. In the subjective IQA experiment\\cite{FR_IQA_review_OSU}, volunteers are required to evaluate the quality of an image by comparing it with a reference image, rather than giving an absolute score for the image. Although in most image processing applications, the reference image does not exist, a set of differently degraded images are available. In these cases, extending existing state-of-the-art FR-IQA algorithms to comparison-based NR-IQA algorithms is a natural thought. However, different from FR-IQA algorithms, neither of the two input image qualities is known in the comparison-based IQA framework. As a result, in a comparison-based NR-IQA algorithm, we not only measure the difference between two input images, but also assess the quality of the difference. \n\n\n\n\n\n\n\n\\subsection{Framework of C-IQA}\n\n\\begin{figure*}[t]\n\t\\centering{\\includegraphics[width=\\linewidth,trim = {0 6cm 0 3.6cm}, clip]{flowchat}}\n\t\\caption{Flow Chart of Comparison-based IQA: $P_1$ and $P_2$ are local patches from input images, $I_1$ and $I_2$, at the same location respectively. The Content Detection module determines whether there is a meaningful structure in the difference patch; the Contribution module calculates which patch mainly contributes to the difference patch; the Texture Compensation module compensates the distortion sensitivity difference of patches with various texture complexities. The output, comparison-based index, indicates the relative quality of $P_1$ based on $P_2$.}\n\t\\label{C-IQA flowchat}\n\\end{figure*}\n\n\nAs shown in Fig. \\ref{C-IQA flowchat}, C-IQA has two input images, $I_1$ and $I_2$, and the output indicates the relative quality of $I_1$ based on $I_2$. We refer to the second image in C-IQA as the base image to distinguish it from the reference image in FR-IQA. C-IQA consists of two basic modules: Content Detection and Contribution. The third module, Texture Compensation, is optional and its description is deferred to Section \\ref{sec_textureCompensation}. In the rest of the paper, we refer to the comparison-based IQA variation composed by the two basic modules as C-IQA and the variation with three modules as CT-IQA. Content Detection determines whether the difference between two input images contains any meaningful structure, and Contribution decides which image mainly contributes to the difference. C-IQA composes these two modules by the criterion that the input image that contributes to a structured difference is better and the input image that contributes to a random difference is worse. The Texture Compensation module added in CT-IQA adjusts the distortion sensitivity difference of patches with different texture complexity\\cite{SSIM,sensitivity_texture_complexity}.\n\n\\subsubsection{Content Detection}\nThe Content Detection module is based on the Image Content Index put forward in MetricQ\\cite{MetricQ}. Different from MetricQ, this index is calculated with the difference image between two input images in C-IQA. In MetricQ, limited by the information provided by single input image, the algorithm does not know the texture complexity in the original image, and it is hard for an algorithm to tell how concentrated the gradient should be. However, by mimicking the comparative way HVS works, C-IQA removes the main image content in the images by taking the difference, and thus the Content Detection module is less influenced by the texture complexity in images.\n\n\\begin{algorithm}\n\\caption{Content Detection}\\label{content_detection}\n\\begin{algorithmic}\n\\State{$D_p = P_1 - P_2$}\n\\State{$G = [d_x(D_p)\\ d_y(D_p)]$} \n\\State{$USV^T=SVD(G)$}  \n\\State{$C_{ind} = \\frac{s_1-s_2}{s_1+s_2}$} \\Comment{$s_1 > s_2$}\n\\If{$C_{ind} > C_{thresh}$}\n\t\\State $is\\_stru = 1$  \\Comment{structure}\n\\Else\n\t\\State $is\\_stru = -1$ \\Comment{noise}\n\\EndIf\n\\end{algorithmic}\n\\end{algorithm}\n\nIn Alg. \\ref{content_detection}, $P_1$ and $P_2$ are two patches of size $n\\times n$ from $I_1$ and $I_2$ respectively, $G$ is the same 2-column gradient matrix defined in \\eqref{gradient_mtx}, $SVD(G)$ represents taking the SVD operation on $G$, and $s_1$ and $s_2$ are the singular values of $G$. $C_{thresh}$ is a constant threshold to binarize $C_{ind}$. The binary output $is\\_stru$ indicates whether there is a meaningful structure in the difference of local patches.\n\n\\subsubsection{Contribution}\nOnce the difference is classified into noise or structure, the Contribution module is designed to find out which of the two input images mainly contributes to the difference image. In our implementation, the luminance-normalized covariance between the input image and the difference image is used to measure the contribution. \n\n\\begin{algorithm}\n\\caption{Contribution}\\label{contribution_alg}\n\\begin{algorithmic}\n\\State $D_p = P_1 - P_2$\n\\State $M_p = max(\\frac{mean(P_1)+mean(P_2)}{2},\\frac{1}{n\\times n})$\n\\State $ctri1 = cov(P_1,D_p)$  \n\\State $ctri2 = cov(P_2,-D_p)$\n\\State $ctri = \\frac{ctri1 - ctri2}{M_p}$\n\\end{algorithmic}\n\\end{algorithm}\nIn Alg. \\ref{contribution_alg}, $mean(P_i)$ calculates the average of the local patch, and $cov(x_1,x_2)$ calculates the covariance between two input patches, \n", "index": 11, "text": "\n\\[cov(x_1,x_2) = \\frac{{(x_1-mean(x_1))}^T(x_2-mean(x_2))}{n^2-1},\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"cov(x_{1},x_{2})=\\frac{{(x_{1}-mean(x_{1}))}^{T}(x_{2}-mean(x_{2}))}{n^{2}-1},\" display=\"block\"><mrow><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>-</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>-</mo><mn>1</mn></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\noindent{The overall comparative quality of $I_1$ based on $I_2$ is}\n", "itemtype": "equation", "pos": 18562, "prevtext": "\n\\noindent{$x_1$ and $x_2$ are vectorized patches of size $n^2\\times 1$.}\\\\\n\nThe comparative quality index for each local patch is calculated by\n", "index": 13, "text": "\n\\[C_Q = is\\_stru \\cdot ctri .\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"C_{Q}=is\\_stru\\cdot ctri.\" display=\"block\"><mrow><mrow><msub><mi>C</mi><mi>Q</mi></msub><mo>=</mo><mrow><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>\u22c5</mo><mi>c</mi></mrow><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>i</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": " where $C_Q(i,j)$ is the local comparative quality index centered at $(i,j)$ in the image, $n\\times n$ is the size of the local patch and $M\\times N$ is the size of the image. Pixels that are on the margin of the image do not have $C_Q$ and thus are not included for the overall quality. A positive $CIQA(I_1,I_2)$ means $I_1$ is better than $I_2$, and the absolute value quantifies the quality difference. Due to the anti-symmetric design of the algorithm, $CIQA(I_1,I_2) = -CIQA(I_2,I_1)$.\n\n\n\\subsection{Justification of C-IQA}\n\\label{proof_C_IQA}\n\nInspired Li's work\\cite{3errorSources} which claims that an IQA model should be based on three quantities: edge sharpness, random noise level and structure noise, \nwe classify the distortions by residual images, the difference between a distorted image and the original image. In our new classification, distortions can be categorized into two types: introducing a random residual image, or introducing a structured residual image. In most cases, random residual images correspond to noise-like distortions and structured residual images correspond to blurring-like distortions. In this part, we prove how C-IQA works under these two distortions. \n\nAssume $I_{true}$ is the original image and $I_1, I_2$ are two distorted images. The residual images are calculated by,\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\noindent{The overall comparative quality of $I_1$ based on $I_2$ is}\n", "index": 15, "text": "\n\\[CIQA(I_1,I_2) = \\frac{1}{M\\times N}\\sum_{i,j=(n/2):(M-n/2)}{C_Q(i,j)},\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"CIQA(I_{1},I_{2})=\\frac{1}{M\\times N}\\sum_{i,j=(n/2):(M-n/2)}{C_{Q}(i,j)},\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mn>1</mn></msub><mo>,</mo><msub><mi>I</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>M</mi><mo>\u00d7</mo><mi>N</mi></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>/</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>M</mi><mo>-</mo><mrow><mi>n</mi><mo>/</mo><mn>2</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></munder><mrow><msub><mi>C</mi><mi>Q</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\nSimilarly, for each patch we have\n", "itemtype": "equation", "pos": 20058, "prevtext": " where $C_Q(i,j)$ is the local comparative quality index centered at $(i,j)$ in the image, $n\\times n$ is the size of the local patch and $M\\times N$ is the size of the image. Pixels that are on the margin of the image do not have $C_Q$ and thus are not included for the overall quality. A positive $CIQA(I_1,I_2)$ means $I_1$ is better than $I_2$, and the absolute value quantifies the quality difference. Due to the anti-symmetric design of the algorithm, $CIQA(I_1,I_2) = -CIQA(I_2,I_1)$.\n\n\n\\subsection{Justification of C-IQA}\n\\label{proof_C_IQA}\n\nInspired Li's work\\cite{3errorSources} which claims that an IQA model should be based on three quantities: edge sharpness, random noise level and structure noise, \nwe classify the distortions by residual images, the difference between a distorted image and the original image. In our new classification, distortions can be categorized into two types: introducing a random residual image, or introducing a structured residual image. In most cases, random residual images correspond to noise-like distortions and structured residual images correspond to blurring-like distortions. In this part, we prove how C-IQA works under these two distortions. \n\nAssume $I_{true}$ is the original image and $I_1, I_2$ are two distorted images. The residual images are calculated by,\n", "index": 17, "text": "\n\\[e_i=I_i-I_{true},\\ i = 1,2.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"e_{i}=I_{i}-I_{true},\\ i=1,2.\" display=\"block\"><mrow><mrow><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>I</mi><mi>i</mi></msub><mo>-</mo><msub><mi>I</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi></mrow></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\n\n\n\\paragraph{Random residual image}\nResidual images behave like noise in this case. If we assume $I_1$ is more severely distorted than $I_2$, then we have $E[{\\|e_{P1}\\|}_2^2]>E[{\\|e_{P2}\\|}_2^2]$.\nThe expectation of the local comparative quality index is\n\\begin{eqnarray}\nE[C_Q]&=&E[ctri\\cdot is\\_stru] \\nonumber \\\\\n\t  &=&E[(ctri1 - ctri2)\\cdot is\\_stru] \\nonumber \\\\\n\t  &=&E[cov(P_1,P_1-P_2) - cov(P_2,P_2-P_1)] \\nonumber\\\\\n\t  &\\quad & \\cdot\\ E[is\\_stru] \\nonumber \\\\\n\t  &=&-E[cov(P_{true}+e_{P1},e_{P1}-e_{P2}) \\nonumber\\\\\n\t  &\\quad& -\\ cov(P_{true}+e_{P2},e_{P2}-e_{P1})]\\nonumber \\\\\t  \n\t  &=&-E[2 \\cdot cov(P_{true},e_{P1}-e_{P2}) \\nonumber\\\\\n\t  &\\quad& +\\ cov(e_{P1},e_{P1})-cov(e_{P2},e_{P2})] \\nonumber \\\\\n\t  &=&-E[cov(e_{P1},e_{P1})]+E[cov(e_{P2},e_{P2})] \\nonumber \\\\\n\t  &\\quad& < 0 .\\nonumber\n\\label{def_contri}\n\\end{eqnarray}\nThe three most important properties in the derivation are the irrelevance between $P_{true}$ and $e_{Pi}$, the randomness of $e_{Pi}$, and independence of $is\\_stru$ and $ctri1$, $ctri2$. The result $E[C_Q] < 0$ agrees with our assumption that $I_1$ is more severely distorted than $I_2$ and when $I_2$ is more severely distorted, the same proof shows $E[C_Q] > 0$.\n\n\\paragraph{Structured residual image}\nIf the residual images show structured information, the most probable reason is that the image is distorted by a blurring-like distortion. Because the blurring filter acts as a low-pass filter, the residual images show a structure that is inversely related to the original image\\cite{NL_denoise} to smoothen the high contrast on the edges.\n\nWithout loss of generality, we assume more blurring happens in $I_1$ than $I_2$, which means $E[|e_{P1}|]>E[|e_{P2}|]$. The expectation of the local comparative quality index is\n\\begin{eqnarray}\nE[C_Q] &=&E[ctri\\cdot is\\_stru] \\nonumber \\\\\n       &=&E[(ctri1 - ctri2)\\cdot is\\_stru] \\nonumber \\\\\n\t   &=&E[cov(P_1,P_1-P_2) - cov(P_2,P_2-P_1)] \\nonumber \\\\\n\t   &=&E[cov(P_{true}+e_{P1},e_{P1}-e_{P2}) \\nonumber \\\\\n\t   &\\quad& -\\ cov(P_{true}+e_{P2},e_{P2}-e_{P1})] \\nonumber \\\\\t         \n       &=& E[cov(2 \\cdot P_{true},e_{P1}-e_{P2}) \\nonumber\\\\\n       &\\quad& \\quad +\\ cov(e_{P1} + e_{P2},e_{P1}-e_{P2})] \\nonumber \\\\\n\t   &=& E[cov(2 \\cdot P_{true}+e_{P1}+e_{P2},e_{P1}-e_{P2}) ] \\nonumber \\\\\n\t   &\\quad& < 0 . \\nonumber\n\\end{eqnarray}\nThe most important step in this derivation is the last step. Since  $E[|e_{P1}|]>E[|e_{P2}|]$, $e_{P1}-e_{P2}$ also demonstrates a structure that is inversely related to the original image as $e_{Pi}$. As long as the distortion is not severe enough to remove the structure in the original image, $2\\cdot P_{true}+e_{P1}+e_{P2}= P_1+P_2$ is positively related to the original image. As a result, $E[cov(2 \\cdot P_{true}+e_{P1}+e_{P2},e_{P1}-e_{P2}) ] < 0$, which agrees with our assumption that $I_1$ is more severely distorted than $I_2$. Following the same steps, we can show $E[C_Q] > 0$ if $I_2$ is more severe distorted than $I_1$.\n\n\n\n\\subsection{Texture Compensation}\n\\label{sec_textureCompensation}\nWe have proven that only with Content Detection and Contribution, the C-IQA can give correct results if both of the two input images are distorted by one distortion, either noise-like distortion or blurring-like distortion. However, another important property of HVS is missed in C-IQA: the response of HVS to the same distortion is texture-dependent. One example of this HVS property is that after being distorted by the same amount of Gaussian noise, the distortion in the image with simpler texture is more obvious. \nIn this part, we first investigate such texture-based response of C-IQA and then design a compensation algorithm to adjust the sensitivity of C-IQA to different textures. We refer to the improved C-IQA as CT-IQA.\n\nIn C-IQA, Content Detection is a qualitative module that detects the meaningful structure and the Contribution module quantifies the relative quality. Therefore, the Contribution module may implicitly include compensation. We design an experiment to explore the relation between the texture complexity and the output of Contribution, $ctri$. In this experiment, 140 patches of size $101\\times 101$ with homogeneous texture are selected from LIVE\\cite{LIVE_dataset} and CSIQ\\cite{FR_IQA_review_OSU}, and six samples of these patches are shown in Fig. \\ref{patch_sample}. As the representatives of blurring-like and noise-like distortions, bilateral filter and Gaussian noise with the same parameters are applied to each patch. According to the Weber-Fechner law\\cite{weber}, we use luminance-normalized total variation as the perceived texture complexity, $T\\_ind = \\frac{TV(P)}{mean(P)}$, where $TV(P)$ is the total variation in the original patch and $mean(P)$ is the average of the original patch. The relation between $T\\_ind$ and $ctri$ are plotted in Fig. \\ref{contri}, in which each circle represents a patch. From Fig. \\ref{contri}, it is clear that $ctri$ is almost linear related to texture complexity, $T\\_ind$, when blurring happens. On the contrary, $T\\_ind$ shows no relation with $ctri$ when the distortion is noise. The reason for this is that blurring is a highly image-dependent distortion, and the residual image is more prominent at areas where total variation is high. After figuring out the blurring sensitivity compensation mechanism in C-IQA, we need to design an algorithm to compensate the sensitivity difference to noise.\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{.97\\linewidth}\n\t\t\\begin{minipage}[t]{.24\\linewidth}\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{womanhat_2}\t\n\t\t\\end{minipage}\t\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{redwood_2}\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{ocean_1}\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{bridge_1}\t\t\n\t\t\\end{minipage}\n\t\\end{minipage}\t\n\t\\begin{minipage}[t]{.97\\linewidth}\n\t\t\\begin{minipage}[t]{.24\\linewidth}\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{cemetry_2}\t\t\n\t\t\\end{minipage}\t\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{cactus_1}\t\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{boston_2}\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{1600_1}\t\t\n\t\t\\end{minipage}\n\t\\end{minipage}\t\n\t\\caption{Patch samples are selected from LIVE\\cite{LIVE_dataset} and CSIQ\\cite{FR_IQA_review_OSU} to verify the texture compensation in C-IQA.}\n\t\\label{patch_sample}\n\\end{figure}\n\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=4.0cm]{TV_Contri_blur}\n\t\t(a) Blurring ($\\rho=-0.92$)\n\t\\end{minipage}\t\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=4.0cm]{TV_Contri_noise}\n\t\t(b) Noise ($\\rho=0.10$)\n\t\\end{minipage}\n\t\\caption{Relations between $ctri$ and $T\\_ind$. Each circle in the figure represents a sample patch. All the sample patches are degraded by the same amount of distortion for blurring and noise.}\n\t\\label{contri}\n\\end{figure}\n\n\\begin{algorithm}\t\n\t\\caption{Texture Compensation}\t\n\t\\label{Textureness_Compensation}\n\t\\begin{algorithmic}\n\t\t\\State{$T1\\_ind = \\frac{TV(P_1)}{mean(P_1)}$} \n\t\t\\State{$T2\\_ind = \\frac{TV(P_2)}{mean(P_2)}$} \n\t\t\\If{is\\_stru = 1}\n\t\t\\State$T\\_ind = max\\{T1\\_ind,T2\\_ind\\}$;\t\n\t\t\\Else\n\t\t\\State$T\\_ind = min\\{T1\\_ind,T2\\_ind\\}$;\t\n\t\t\\EndIf\n\t\t\\State$S\\_ind = log(1+\\frac{1}{C_1\\times T\\_ind})$ \n\t\t\n\t\t\\If{is\\_stru = 1} \n\t\t\\State$weight =1 $  \n\t\t\\Else\n\t\t\\State $weight = -S\\_ind$ \n\t\t\\EndIf\t\n\t\\end{algorithmic}\n\\end{algorithm}\n\nBecause noise-like distortion tends to increase the total variation while blurring-like distortion tends to decrease the total variation, Alg. \\ref{Textureness_Compensation} uses the output of Content Detection to synthesize $T1\\_ind$ and $T2\\_ind$ into $T\\_ind$. After texture complexity estimation, we transfer $T\\_ind$ to the smoothness index, $S_{ind}$, and compensate the sensitivity to noise.\n\nIn CT-IQA, the comparative quality index for each local patch is\n", "itemtype": "equation", "pos": 20124, "prevtext": "\nSimilarly, for each patch we have\n", "index": 19, "text": "\n\\[e_{Pi} = P_i - P_{true},\\ i = 1,2.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"e_{Pi}=P_{i}-P_{true},\\ i=1,2.\" display=\"block\"><mrow><mrow><mrow><msub><mi>e</mi><mrow><mi>P</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>-</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi></mrow></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": " The overall comparative quality of $I_1$ based on $I_2$ is calculated by taking the average of local comparative quality index as C-IQA does.\n\\subsection{Comparison between CT-IQA and SSIM} \nSSIM consists of three components: structure (loss of correlation), luminance (mean distortion) and contrast (variance distortion). In CT-IQA, the outputs of Content Detection and Texture Compensation provide a ``reference image'' (the difference image) and the quality of the ``reference image''. The luminance and the contrast of an input image together determine the contribution of the input image to the ``reference image''. Therefore, Content Detection and Texture Compensation of CT-IQA together play the role of the structure part in SSIM. The difference is that without knowing which image has the better quality, CT-IQA has to analyze the quality of the structure in the ``reference image'', rather than only measuring the structure distance as SSIM does. The Contribution module in CT-IQA is similar to the functions of luminance and contrast parts together in SSIM.  \n\n\n\\section{Parameter Selection}\n\\label{parameter_trimming}\nAs the motivation of C-IQA mentioned in the introduction, most image processing algorithms contain user-defined parameters (these image processing algorithms are referred as ``target algorithms'' in the following to differ from IQA algorithms). Parameter selection \\cite{parameter_select1,parameter_select2,parameter_select3,parameter_select4,parameter_select5,parameter_select6,parameter_select7,MetricQ,parameter_trimming} is of importance to these target algorithms. By parameter selection, some of these target algorithms\\cite{parameter_select5,parameter_select6} achieve a faster convergence rate; some \\cite{parameter_select3,parameter_select4} obtain a better restored image.\n\nA traditional approach to parameter selection\\cite{parameter_select1,parameter_select2,parameter_select3,parameter_select4} is selecting the parameters after the convergence of all the target algorithm instances with a perceptual quality monitor, usually a NR-IQA algorithm. However, since either the target algorithms converge quickly \\cite{parameter_select4, MetricQ} or the NR-IQA algorithm is time-consuming \\cite{parameter_select3}, computational efficiency is not considered in previous works. For instance, the denoising parameter selection in \\cite{MetricQ} involves experiments with 30 parameter candidates and 20 iterations/candidate. In situations where target algorithms converge slowly or the set of parameter candidates is large, assessing image qualities and selecting the best parameter after all the algorithm instances converge would be too time-consuming to be practical.\nInstead of placing the quality monitor at the output end, a novel parameter trimming framework proposed in \\cite{parameter_trimming} integrates the quality monitor into the target algorithms. By doing so, parameters that do not have the potential to achieve good results are trimmed before convergence. In this section, we use image reconstruction as the application to illustrate the parameter trimming framework because a regularized iterative algorithm is usually adopted to obtain superior reconstructed results.\n\n\\subsection{Image Reconstruction}\n\\label{image_reconstruction}\nTotal variation (TV) reconstruction\\cite{TV_recon} is aimed at minimizing the cost function,\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\n\n\\paragraph{Random residual image}\nResidual images behave like noise in this case. If we assume $I_1$ is more severely distorted than $I_2$, then we have $E[{\\|e_{P1}\\|}_2^2]>E[{\\|e_{P2}\\|}_2^2]$.\nThe expectation of the local comparative quality index is\n\\begin{eqnarray}\nE[C_Q]&=&E[ctri\\cdot is\\_stru] \\nonumber \\\\\n\t  &=&E[(ctri1 - ctri2)\\cdot is\\_stru] \\nonumber \\\\\n\t  &=&E[cov(P_1,P_1-P_2) - cov(P_2,P_2-P_1)] \\nonumber\\\\\n\t  &\\quad & \\cdot\\ E[is\\_stru] \\nonumber \\\\\n\t  &=&-E[cov(P_{true}+e_{P1},e_{P1}-e_{P2}) \\nonumber\\\\\n\t  &\\quad& -\\ cov(P_{true}+e_{P2},e_{P2}-e_{P1})]\\nonumber \\\\\t  \n\t  &=&-E[2 \\cdot cov(P_{true},e_{P1}-e_{P2}) \\nonumber\\\\\n\t  &\\quad& +\\ cov(e_{P1},e_{P1})-cov(e_{P2},e_{P2})] \\nonumber \\\\\n\t  &=&-E[cov(e_{P1},e_{P1})]+E[cov(e_{P2},e_{P2})] \\nonumber \\\\\n\t  &\\quad& < 0 .\\nonumber\n\\label{def_contri}\n\\end{eqnarray}\nThe three most important properties in the derivation are the irrelevance between $P_{true}$ and $e_{Pi}$, the randomness of $e_{Pi}$, and independence of $is\\_stru$ and $ctri1$, $ctri2$. The result $E[C_Q] < 0$ agrees with our assumption that $I_1$ is more severely distorted than $I_2$ and when $I_2$ is more severely distorted, the same proof shows $E[C_Q] > 0$.\n\n\\paragraph{Structured residual image}\nIf the residual images show structured information, the most probable reason is that the image is distorted by a blurring-like distortion. Because the blurring filter acts as a low-pass filter, the residual images show a structure that is inversely related to the original image\\cite{NL_denoise} to smoothen the high contrast on the edges.\n\nWithout loss of generality, we assume more blurring happens in $I_1$ than $I_2$, which means $E[|e_{P1}|]>E[|e_{P2}|]$. The expectation of the local comparative quality index is\n\\begin{eqnarray}\nE[C_Q] &=&E[ctri\\cdot is\\_stru] \\nonumber \\\\\n       &=&E[(ctri1 - ctri2)\\cdot is\\_stru] \\nonumber \\\\\n\t   &=&E[cov(P_1,P_1-P_2) - cov(P_2,P_2-P_1)] \\nonumber \\\\\n\t   &=&E[cov(P_{true}+e_{P1},e_{P1}-e_{P2}) \\nonumber \\\\\n\t   &\\quad& -\\ cov(P_{true}+e_{P2},e_{P2}-e_{P1})] \\nonumber \\\\\t         \n       &=& E[cov(2 \\cdot P_{true},e_{P1}-e_{P2}) \\nonumber\\\\\n       &\\quad& \\quad +\\ cov(e_{P1} + e_{P2},e_{P1}-e_{P2})] \\nonumber \\\\\n\t   &=& E[cov(2 \\cdot P_{true}+e_{P1}+e_{P2},e_{P1}-e_{P2}) ] \\nonumber \\\\\n\t   &\\quad& < 0 . \\nonumber\n\\end{eqnarray}\nThe most important step in this derivation is the last step. Since  $E[|e_{P1}|]>E[|e_{P2}|]$, $e_{P1}-e_{P2}$ also demonstrates a structure that is inversely related to the original image as $e_{Pi}$. As long as the distortion is not severe enough to remove the structure in the original image, $2\\cdot P_{true}+e_{P1}+e_{P2}= P_1+P_2$ is positively related to the original image. As a result, $E[cov(2 \\cdot P_{true}+e_{P1}+e_{P2},e_{P1}-e_{P2}) ] < 0$, which agrees with our assumption that $I_1$ is more severely distorted than $I_2$. Following the same steps, we can show $E[C_Q] > 0$ if $I_2$ is more severe distorted than $I_1$.\n\n\n\n\\subsection{Texture Compensation}\n\\label{sec_textureCompensation}\nWe have proven that only with Content Detection and Contribution, the C-IQA can give correct results if both of the two input images are distorted by one distortion, either noise-like distortion or blurring-like distortion. However, another important property of HVS is missed in C-IQA: the response of HVS to the same distortion is texture-dependent. One example of this HVS property is that after being distorted by the same amount of Gaussian noise, the distortion in the image with simpler texture is more obvious. \nIn this part, we first investigate such texture-based response of C-IQA and then design a compensation algorithm to adjust the sensitivity of C-IQA to different textures. We refer to the improved C-IQA as CT-IQA.\n\nIn C-IQA, Content Detection is a qualitative module that detects the meaningful structure and the Contribution module quantifies the relative quality. Therefore, the Contribution module may implicitly include compensation. We design an experiment to explore the relation between the texture complexity and the output of Contribution, $ctri$. In this experiment, 140 patches of size $101\\times 101$ with homogeneous texture are selected from LIVE\\cite{LIVE_dataset} and CSIQ\\cite{FR_IQA_review_OSU}, and six samples of these patches are shown in Fig. \\ref{patch_sample}. As the representatives of blurring-like and noise-like distortions, bilateral filter and Gaussian noise with the same parameters are applied to each patch. According to the Weber-Fechner law\\cite{weber}, we use luminance-normalized total variation as the perceived texture complexity, $T\\_ind = \\frac{TV(P)}{mean(P)}$, where $TV(P)$ is the total variation in the original patch and $mean(P)$ is the average of the original patch. The relation between $T\\_ind$ and $ctri$ are plotted in Fig. \\ref{contri}, in which each circle represents a patch. From Fig. \\ref{contri}, it is clear that $ctri$ is almost linear related to texture complexity, $T\\_ind$, when blurring happens. On the contrary, $T\\_ind$ shows no relation with $ctri$ when the distortion is noise. The reason for this is that blurring is a highly image-dependent distortion, and the residual image is more prominent at areas where total variation is high. After figuring out the blurring sensitivity compensation mechanism in C-IQA, we need to design an algorithm to compensate the sensitivity difference to noise.\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{.97\\linewidth}\n\t\t\\begin{minipage}[t]{.24\\linewidth}\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{womanhat_2}\t\n\t\t\\end{minipage}\t\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{redwood_2}\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{ocean_1}\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{bridge_1}\t\t\n\t\t\\end{minipage}\n\t\\end{minipage}\t\n\t\\begin{minipage}[t]{.97\\linewidth}\n\t\t\\begin{minipage}[t]{.24\\linewidth}\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{cemetry_2}\t\t\n\t\t\\end{minipage}\t\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{cactus_1}\t\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{boston_2}\t\t\n\t\t\\end{minipage}\n\t\t\\begin{minipage}[t]{0.24\\linewidth}\t\t\n\t\t\t\\includegraphics[width= 0.99\\linewidth]{1600_1}\t\t\n\t\t\\end{minipage}\n\t\\end{minipage}\t\n\t\\caption{Patch samples are selected from LIVE\\cite{LIVE_dataset} and CSIQ\\cite{FR_IQA_review_OSU} to verify the texture compensation in C-IQA.}\n\t\\label{patch_sample}\n\\end{figure}\n\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=4.0cm]{TV_Contri_blur}\n\t\t(a) Blurring ($\\rho=-0.92$)\n\t\\end{minipage}\t\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width=4.0cm]{TV_Contri_noise}\n\t\t(b) Noise ($\\rho=0.10$)\n\t\\end{minipage}\n\t\\caption{Relations between $ctri$ and $T\\_ind$. Each circle in the figure represents a sample patch. All the sample patches are degraded by the same amount of distortion for blurring and noise.}\n\t\\label{contri}\n\\end{figure}\n\n\\begin{algorithm}\t\n\t\\caption{Texture Compensation}\t\n\t\\label{Textureness_Compensation}\n\t\\begin{algorithmic}\n\t\t\\State{$T1\\_ind = \\frac{TV(P_1)}{mean(P_1)}$} \n\t\t\\State{$T2\\_ind = \\frac{TV(P_2)}{mean(P_2)}$} \n\t\t\\If{is\\_stru = 1}\n\t\t\\State$T\\_ind = max\\{T1\\_ind,T2\\_ind\\}$;\t\n\t\t\\Else\n\t\t\\State$T\\_ind = min\\{T1\\_ind,T2\\_ind\\}$;\t\n\t\t\\EndIf\n\t\t\\State$S\\_ind = log(1+\\frac{1}{C_1\\times T\\_ind})$ \n\t\t\n\t\t\\If{is\\_stru = 1} \n\t\t\\State$weight =1 $  \n\t\t\\Else\n\t\t\\State $weight = -S\\_ind$ \n\t\t\\EndIf\t\n\t\\end{algorithmic}\n\\end{algorithm}\n\nBecause noise-like distortion tends to increase the total variation while blurring-like distortion tends to decrease the total variation, Alg. \\ref{Textureness_Compensation} uses the output of Content Detection to synthesize $T1\\_ind$ and $T2\\_ind$ into $T\\_ind$. After texture complexity estimation, we transfer $T\\_ind$ to the smoothness index, $S_{ind}$, and compensate the sensitivity to noise.\n\nIn CT-IQA, the comparative quality index for each local patch is\n", "index": 21, "text": "\\[CT_Q = is\\_stru \\cdot ctri \\cdot weight.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"CT_{Q}=is\\_stru\\cdot ctri\\cdot weight.\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><msub><mi>T</mi><mi>Q</mi></msub></mrow><mo>=</mo><mrow><mrow><mrow><mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>\u22c5</mo><mi>c</mi></mrow><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u22c5</mo><mi>w</mi></mrow><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\nwhere $x$ is the reconstructed image, $y$ is the observed incomplete data set, $S$ is the system matrix, $D$ represents the difference matrix, and the TV regularizer ${\\|Dx\\|}_1$ combines gradients on two directions isotropically. In our implementation, $S = R\\mathcal{F}$, where $R$ represents the subsampling matrix and $\\mathcal{F}$ represents the Fourier transform matrix. The regularization parameter $\\beta$ controls the sharpness of the reconstructed result. Large $\\beta$ will oversmooth the reconstructed image, while small $\\beta$ will leave residual noise. A proper $\\beta$ is crucial to the performance of TV reconstruction. Split Bregman iteration\\cite{split_bregman} is used to solve \\eqref{recon_target}. By making the replacement $d \\leftarrow Dx$ and introducing the dual variable $b$, the split formulation of \\eqref{recon_target} becomes:\n\n", "itemtype": "equation", "pos": 31703, "prevtext": " The overall comparative quality of $I_1$ based on $I_2$ is calculated by taking the average of local comparative quality index as C-IQA does.\n\\subsection{Comparison between CT-IQA and SSIM} \nSSIM consists of three components: structure (loss of correlation), luminance (mean distortion) and contrast (variance distortion). In CT-IQA, the outputs of Content Detection and Texture Compensation provide a ``reference image'' (the difference image) and the quality of the ``reference image''. The luminance and the contrast of an input image together determine the contribution of the input image to the ``reference image''. Therefore, Content Detection and Texture Compensation of CT-IQA together play the role of the structure part in SSIM. The difference is that without knowing which image has the better quality, CT-IQA has to analyze the quality of the structure in the ``reference image'', rather than only measuring the structure distance as SSIM does. The Contribution module in CT-IQA is similar to the functions of luminance and contrast parts together in SSIM.  \n\n\n\\section{Parameter Selection}\n\\label{parameter_trimming}\nAs the motivation of C-IQA mentioned in the introduction, most image processing algorithms contain user-defined parameters (these image processing algorithms are referred as ``target algorithms'' in the following to differ from IQA algorithms). Parameter selection \\cite{parameter_select1,parameter_select2,parameter_select3,parameter_select4,parameter_select5,parameter_select6,parameter_select7,MetricQ,parameter_trimming} is of importance to these target algorithms. By parameter selection, some of these target algorithms\\cite{parameter_select5,parameter_select6} achieve a faster convergence rate; some \\cite{parameter_select3,parameter_select4} obtain a better restored image.\n\nA traditional approach to parameter selection\\cite{parameter_select1,parameter_select2,parameter_select3,parameter_select4} is selecting the parameters after the convergence of all the target algorithm instances with a perceptual quality monitor, usually a NR-IQA algorithm. However, since either the target algorithms converge quickly \\cite{parameter_select4, MetricQ} or the NR-IQA algorithm is time-consuming \\cite{parameter_select3}, computational efficiency is not considered in previous works. For instance, the denoising parameter selection in \\cite{MetricQ} involves experiments with 30 parameter candidates and 20 iterations/candidate. In situations where target algorithms converge slowly or the set of parameter candidates is large, assessing image qualities and selecting the best parameter after all the algorithm instances converge would be too time-consuming to be practical.\nInstead of placing the quality monitor at the output end, a novel parameter trimming framework proposed in \\cite{parameter_trimming} integrates the quality monitor into the target algorithms. By doing so, parameters that do not have the potential to achieve good results are trimmed before convergence. In this section, we use image reconstruction as the application to illustrate the parameter trimming framework because a regularized iterative algorithm is usually adopted to obtain superior reconstructed results.\n\n\\subsection{Image Reconstruction}\n\\label{image_reconstruction}\nTotal variation (TV) reconstruction\\cite{TV_recon} is aimed at minimizing the cost function,\n\n", "index": 23, "text": "\\begin{equation}\n\\label{recon_target}\nE_\\beta(x)=\\beta{\\|Dx\\|}_1+\\frac{1}{2}{\\|Sx-y\\|}_2^2\\ ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"E_{\\beta}(x)=\\beta{\\|Dx\\|}_{1}+\\frac{1}{2}{\\|Sx-y\\|}_{2}^{2}\\ ,\" display=\"block\"><mrow><mrow><mrow><msub><mi>E</mi><mi>\u03b2</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mpadded width=\"+5pt\"><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\nThe Split Bregman iteration solution to \\eqref{recon_sub} is Alg. \\ref{Split_Bregman}.\nIn Alg. \\ref{Split_Bregman} we use the notation $K=(R^TR-\\mu\\mathcal{F}D^TD\\mathcal{F}^{-1})$, $L_k=(\\mathcal{F}^TR^Ty+\\mu D^T (d^k-b^k ))$ and  $s^k=\\sqrt{{|Dx^k+b^k|}^2 }$. $\\mu$ is set as $0.01\\beta$ to ensure a fast convergence rate.\n\\begin{algorithm}\n\\caption{Split Bregman}\\label{Split_Bregman}\n\\begin{algorithmic}\n\\State{Initialize: $x^0=0,d^0=b^0=0$}\n\\While{stop criterion is not satisfied}\n", "itemtype": "equation", "pos": 32670, "prevtext": "\nwhere $x$ is the reconstructed image, $y$ is the observed incomplete data set, $S$ is the system matrix, $D$ represents the difference matrix, and the TV regularizer ${\\|Dx\\|}_1$ combines gradients on two directions isotropically. In our implementation, $S = R\\mathcal{F}$, where $R$ represents the subsampling matrix and $\\mathcal{F}$ represents the Fourier transform matrix. The regularization parameter $\\beta$ controls the sharpness of the reconstructed result. Large $\\beta$ will oversmooth the reconstructed image, while small $\\beta$ will leave residual noise. A proper $\\beta$ is crucial to the performance of TV reconstruction. Split Bregman iteration\\cite{split_bregman} is used to solve \\eqref{recon_target}. By making the replacement $d \\leftarrow Dx$ and introducing the dual variable $b$, the split formulation of \\eqref{recon_target} becomes:\n\n", "index": 25, "text": "\\begin{equation}\n\\label{recon_sub} \n\\begin{split}\n\\min_{x,d}\\beta \\|d\\|_1 + \\frac{1}{2}\\|Sy&-y\\|_2^2 + \\frac{\\mu}{2}\\|d-Dx-b\\|_2^2\\ ,\\\\\n&s.t.\\ d=Dx.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{x,d}\\beta\\|d\\|_{1}+\\frac{1}{2}\\|Sy&amp;%&#10;\\displaystyle-y\\|_{2}^{2}+\\frac{\\mu}{2}\\|d-Dx-b\\|_{2}^{2}\\ ,\\\\&#10;&amp;\\displaystyle s.t.\\ d=Dx.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><munder><mi>min</mi><mrow><mi>x</mi><mo>,</mo><mi>d</mi></mrow></munder><mo>\u2061</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mo>\u2225</mo><mrow><msub><mrow><mi>d</mi><mo fence=\"true\">\u2225</mo></mrow><mn>1</mn></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2225</mo></mrow><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>y</mi></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mo>-</mo><msubsup><mrow><mi>y</mi><mo fence=\"true\">\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mfrac><mi>\u03bc</mi><mn>2</mn></mfrac><mo>\u2062</mo><mpadded width=\"+5pt\"><msubsup><mrow><mo>\u2225</mo><mrow><mi>d</mi><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mi>b</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mpadded></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi>s</mi><mo>.</mo><mi>t</mi><mo rspace=\"7.5pt\">.</mo><mrow><mi>d</mi><mo>=</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>x</mi></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n", "itemtype": "equation", "pos": 33332, "prevtext": "\n\nThe Split Bregman iteration solution to \\eqref{recon_sub} is Alg. \\ref{Split_Bregman}.\nIn Alg. \\ref{Split_Bregman} we use the notation $K=(R^TR-\\mu\\mathcal{F}D^TD\\mathcal{F}^{-1})$, $L_k=(\\mathcal{F}^TR^Ty+\\mu D^T (d^k-b^k ))$ and  $s^k=\\sqrt{{|Dx^k+b^k|}^2 }$. $\\mu$ is set as $0.01\\beta$ to ensure a fast convergence rate.\n\\begin{algorithm}\n\\caption{Split Bregman}\\label{Split_Bregman}\n\\begin{algorithmic}\n\\State{Initialize: $x^0=0,d^0=b^0=0$}\n\\While{stop criterion is not satisfied}\n", "index": 27, "text": "\n\\[x^{k+1} = \\mathcal{F}^{-1}K^{-1}L_k\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"x^{k+1}=\\mathcal{F}^{-1}K^{-1}L_{k}\" display=\"block\"><mrow><msup><mi>x</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>K</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>L</mi><mi>k</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n", "itemtype": "equation", "pos": 33372, "prevtext": "\n", "index": 29, "text": "\n\\[d_{k+1}=\\max(s^k-\\frac{1}{\\mu},0)\\frac{Dx^k+b^k}{s^k}\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"d_{k+1}=\\max(s^{k}-\\frac{1}{\\mu},0)\\frac{Dx^{k}+b^{k}}{s^{k}}\" display=\"block\"><mrow><msub><mi>d</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>s</mi><mi>k</mi></msup><mo>-</mo><mfrac><mn>1</mn><mi>\u03bc</mi></mfrac></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mfrac><mrow><mrow><mi>D</mi><mo>\u2062</mo><msup><mi>x</mi><mi>k</mi></msup></mrow><mo>+</mo><msup><mi>b</mi><mi>k</mi></msup></mrow><msup><mi>s</mi><mi>k</mi></msup></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width= 0.99\\linewidth]{brain1}}\t\n\t\t(a)\t\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain2}}\n\t\t(b)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain3}}\n\t\t(c)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain4}}\n\t\t(d)\n\t\\end{minipage}\n\t\\caption{(a): original Brain image\\cite{Brain}; (b): reconstructed result with $\\beta = 1.22\\times 10^{-6}$; (c): reconstruction result with  $\\beta = 4.46\\times 10^{-1}$; (d): reconstructed result with  $\\beta = 10$.}\n\t\\label{recon_examples}\n\\end{figure}\n\nTo illustrate the necessity of parameter selection of TV reconstruction, the Brain image\\cite{Brain} is reconstructed with 30 values of  $\\beta$. These candidate values of $\\beta$ are uniformly sampled from $1.22 \\times 10^{-6}$ to $10$ in logarithmic scale and three of the reconstructed results are shown in Fig. \\ref{recon_examples}.\nThe image quality indices during the convergence process are plotted in Fig. \\ref{recon_converge_line}(a), and each line corresponds to one parameter candidate. The final reconstructed image qualities are plotted in Fig. \\ref{recon_converge_line}(b). \n\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering{\\includegraphics[width=5cm,trim = {0.5cm 0 0.5cm 0.5cm},clip]{reconConver}}\n(a)\n\\end{minipage}\n\\begin{minipage}[t]{0.28\\linewidth}\n\\centering{\\includegraphics[width=5cm,trim = {0.4cm 0  0.5cm 0.5cm},clip]{recon_converged}}\n\\centering{(b)}\n\\end{minipage}\n\\caption{(a): Each line corresponds to an algorithm instance with a parameter candidate. (b): Reconstructed result qualities of different parameter candidates after 160 iterations.}\n\\label{recon_converge_line}\n\\end{figure}\n\n\n\\subsection{Parameter trimming}\nIn \\cite{parameter_trimming}, we first proposed a parameter trimming framework that combines the quality index with target algorithms to carry out the parameter selection before convergence. Assume $I_m^i$ is the reconstructed result of the $m^{th}$ parameter candidate at the $i^{th}$ iteration. The trimming decision is made based on three indices, $s_m^i$, $g_m^i$ and $p_m^i$, which are the reconstructed quality, the quality increasing gradient and the prediction of the quality of $I_m^i$ respectively. Because the image quality index we use here is a comparison-based index, the definitions of the these three indices are modified to fit CT-IQA into the parameter trimming framework in \\cite{parameter_trimming}. Denoting the best reconstructed result at the $i^{th}$ iteration is $best_i$, it satisfies $CTIQA(I_{best_i}^i,I_{best_i-1}^i) \\ge 0$ and $CTIQA(I_{best_i}^i,I_{best_i+1}^i) \\ge 0$. The three indices used for parameter trimming, $s_m^i$, $g_m^i$ and $p_m^i$, are defined as,\n", "itemtype": "equation", "pos": 33430, "prevtext": "\n", "index": 31, "text": "\n\\[b^{k+1}=b^k+(Dx^k-d^{k+1})\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"b^{k+1}=b^{k}+(Dx^{k}-d^{k+1})\" display=\"block\"><mrow><msup><mi>b</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msup><mi>b</mi><mi>k</mi></msup><mo>+</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>D</mi><mo>\u2062</mo><msup><mi>x</mi><mi>k</mi></msup></mrow><mo>-</mo><msup><mi>d</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n", "itemtype": "equation", "pos": 36430, "prevtext": "\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\\begin{figure}[t]\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width= 0.99\\linewidth]{brain1}}\t\n\t\t(a)\t\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain2}}\n\t\t(b)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain3}}\n\t\t(c)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.24\\linewidth}\n\t\t\\centering{\\includegraphics[width=0.99\\linewidth]{brain4}}\n\t\t(d)\n\t\\end{minipage}\n\t\\caption{(a): original Brain image\\cite{Brain}; (b): reconstructed result with $\\beta = 1.22\\times 10^{-6}$; (c): reconstruction result with  $\\beta = 4.46\\times 10^{-1}$; (d): reconstructed result with  $\\beta = 10$.}\n\t\\label{recon_examples}\n\\end{figure}\n\nTo illustrate the necessity of parameter selection of TV reconstruction, the Brain image\\cite{Brain} is reconstructed with 30 values of  $\\beta$. These candidate values of $\\beta$ are uniformly sampled from $1.22 \\times 10^{-6}$ to $10$ in logarithmic scale and three of the reconstructed results are shown in Fig. \\ref{recon_examples}.\nThe image quality indices during the convergence process are plotted in Fig. \\ref{recon_converge_line}(a), and each line corresponds to one parameter candidate. The final reconstructed image qualities are plotted in Fig. \\ref{recon_converge_line}(b). \n\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering{\\includegraphics[width=5cm,trim = {0.5cm 0 0.5cm 0.5cm},clip]{reconConver}}\n(a)\n\\end{minipage}\n\\begin{minipage}[t]{0.28\\linewidth}\n\\centering{\\includegraphics[width=5cm,trim = {0.4cm 0  0.5cm 0.5cm},clip]{recon_converged}}\n\\centering{(b)}\n\\end{minipage}\n\\caption{(a): Each line corresponds to an algorithm instance with a parameter candidate. (b): Reconstructed result qualities of different parameter candidates after 160 iterations.}\n\\label{recon_converge_line}\n\\end{figure}\n\n\n\\subsection{Parameter trimming}\nIn \\cite{parameter_trimming}, we first proposed a parameter trimming framework that combines the quality index with target algorithms to carry out the parameter selection before convergence. Assume $I_m^i$ is the reconstructed result of the $m^{th}$ parameter candidate at the $i^{th}$ iteration. The trimming decision is made based on three indices, $s_m^i$, $g_m^i$ and $p_m^i$, which are the reconstructed quality, the quality increasing gradient and the prediction of the quality of $I_m^i$ respectively. Because the image quality index we use here is a comparison-based index, the definitions of the these three indices are modified to fit CT-IQA into the parameter trimming framework in \\cite{parameter_trimming}. Denoting the best reconstructed result at the $i^{th}$ iteration is $best_i$, it satisfies $CTIQA(I_{best_i}^i,I_{best_i-1}^i) \\ge 0$ and $CTIQA(I_{best_i}^i,I_{best_i+1}^i) \\ge 0$. The three indices used for parameter trimming, $s_m^i$, $g_m^i$ and $p_m^i$, are defined as,\n", "index": 33, "text": "\n\\[s_m^i = CTIQA(I_m^i,I_{best_i}^i),\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"s_{m}^{i}=CTIQA(I_{m}^{i},I_{best_{i}}^{i}),\" display=\"block\"><mrow><mrow><msubsup><mi>s</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>I</mi><mi>m</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>I</mi><mrow><mi>b</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n", "itemtype": "equation", "pos": 36469, "prevtext": "\n", "index": 35, "text": "\n\\[g_m^i = CTIQA(I_m^i,I_{best_{i-1}}^{i-1}) - CTIQA(I_m^{i-1},I_{best_{i-1}}^{i-1}),\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"g_{m}^{i}=CTIQA(I_{m}^{i},I_{best_{i-1}}^{i-1})-CTIQA(I_{m}^{i-1},I_{best_{i-1%&#10;}}^{i-1}),\" display=\"block\"><mrow><mrow><msubsup><mi>g</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>I</mi><mi>m</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>I</mi><mrow><mi>b</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>C</mi><mo>\u2062</mo><mi>T</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>Q</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>I</mi><mi>m</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>I</mi><mrow><mi>b</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\nWe set $pre_{len} = 4$ in all the experiments. More examples of the reconstruction process and the changing of these three indices during the trimming process are shown in Section \\ref{experiments}.\n\n\\section{Experiments}\n\\label{experiments}\n\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[height = 3.2cm]{caps}\n(a) ``caps''\n\\label{sample_images_case_study_caps}\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[height = 3.2cm]{coinsinfountain}\n(b) ``coinsinfountain''\n\\end{minipage}\n\\caption{Images from LIVE\\cite{LIVE_dataset} used for case study in Section \\ref{case_study}}\n\\label{sample_images_case_study}\n\\end{figure}\n\nWe first introduce two key properties (consistency and minimum resolution) of C-IQA/CT-IQA in Section \\ref{case_study}. In the next two parts, more comprehensive experiments on two databases are conducted to verify the effectiveness of C-IQA/CT-IQA and their applications to parameter selection.\n\nThe other NR-IQA algorithms that we use to compare C-IQA/CT-IQA with are DIIVINE (DII)\\cite{DIVIINE}, BRISQUE (BRI)\\cite{BRISQUE}, MetricQ (MQ)\\cite{MetricQ} and Anisotropy (Ani)\\cite{anisotropy}. A widely accepted FR-IQA algorithm, SSIM\\cite{SSIM}, is used as the ground truth to evaluate the performance of different NR-IQA algorithms. Two IQA databases used in the experiments are LIVE\\cite{LIVE_dataset} and CISQ\\cite{FR_IQA_review_OSU}. Parameters in C-IQA/CT-IQA are set as $C_{thresh}=0.12$, $C_1 = 4.6$ and the size of local patch is $9\\times 9$.\n\n\\subsection{Case study to explore two key properties of C-IQA/CT-IQA}\n\\label{case_study}\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.99\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim={7cm 0cm 18cm 0cm},clip]{consistence}\n\\centering\n\\end{minipage}\n\\caption{Consistency of different IQA algorithms on noise and blurring. The scores of Anisotropy and DIIVINE are inconsistent on Gaussian noise.}\n\\label{consistence_fig}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7cm 0cm 0.5cm 4cm},clip]{minimum_resolution_neighbor}\n(a) \n\\end{minipage}\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7.5cm 0cm 0cm 3cm},clip]{minimum_resolution_caps}\n(b) \n\\end{minipage}\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7.5cm 0cm 0cm 3cm},clip]{minimum_resolution_coins}\n(b)\n\\end{minipage}\n\\caption{Minimum resolution of Comparison-based IQA algorithm: (a) CT-IQA scores of denoised images compared with their previous images (series1) and the one before previous images (series2); (b) Accumulated CT-IQA scores in (a) and SSIM scores of ``caps''; (c) Accumulated CT-IQA scores in (a) and SSIM scores of ``coinsinfountain''}\n\\label{minimum_resolution}\n\\end{figure*}\n\nSince the comparison-based IQA is a brand-new approach, some new properties arise. In this section, we illustrate these properties and corresponding solutions based on two images from LIVE\\cite{LIVE_dataset} as shown in Fig. \\ref{sample_images_case_study}.\n\n\n\\subsubsection{Consistency on single kind of distortion}\n\\label{consistence_part}\nConsistency on a single kind of distortion is one of the basic requirements of an IQA algorithm. Because distortions can be generally classified into two categories\\cite{denoise_review}, noise and blur, we check the consistency of different NR-IQA algorithms on Gaussian noise and blurring with the Gaussian kernel respectively. \n\n\nFor each distortion, a series of increasingly distorted images, whose SSIM indices uniformly range from 0.85 to 1, are evaluated by different NR-IQA algorithms. Since we are interested in the trend of each IQA algorithm, scores obtained from different IQA algorithms are normalized between [0, 1] and the original images are used as the base images for C-IQA/CT-IQA. It is clear from Fig. \\ref{consistence_fig} that all of NR-IQA algorithms produce consistent results for blurring, but in the noise case, DIIVINE and Anisotropy are inconsistence. \n\n\n\n\\subsubsection{Minimum resolution}\n\\label{minimum_resolution_part}\nSimilar to HVS, IQA algorithms are not able to make a convincing quality comparison between images whose difference is sufficiently small.\n\nIn this part, we define the minimum mean squared difference (MSD) between two images required to make a convincing quality comparison as the minimum resolution. It is worth noticing that minimum resolutions vary over different distortions and different IQA algorithms.\n\nFor the traditional single-image-input NR-IQA algorithms, minimum resolutions can be regarded as the minimum MSD required to ensure consistency on a series of increasingly distorted images. The unwanted fluctuations of DIIVINE and Anisotropy in Fig. \\ref{consistence_fig}(a) indicate that the MSD between the adjacent images is less than their minimum resolutions of Gaussian noise.\n\nHowever, under the comparison-based framework, a distorted image has different scores compared with different base images. We cannot refer to the consistency to define the minimum resolution for a comparison-based IQA algorithm. The minimum resolution for comparison-based IQA is defined as the minimum MSD required to preserve transitivity among a series of distorted images. We conduct an experiment on the images in Fig. \\ref{sample_images_case_study} to demonstrate the transitivity. Assume $I_{org}$ is the original image, and $I_1$ is created by adding Gaussian noise to $I_{org}$. A series of gradually filtered images, $(I_1,I_2,\\cdots,I_N)$, are denoised by bilateral filters $BF_{(r,d)}$, where $r$ and $d$ are the variances of Gaussian range kernel for smoothing differences in intensities and Guassian spatial kernel for smoothing differences in coordinates. For simplicity, we reduce the parameters of bilateral filters to one by fixing the ratio between $r$ and $d$, $BF_{k} = BF_{(0.1k,3k)}$. In Fig. \\ref{minimum_resolution}(a), we show the CT-IQA scores of each image compared with its previous one in the denoised sequence ($series1$) and the CT-IQA scores compared with the one before its previous one ($series2$). We can see that CT-IQA scores in $series1$ are always positive, but pass $0$ in $series2$, which means the denoised image qualities are monotonically increasing in $series1$, but reach a peak in $series2$. In Fig. \\ref{minimum_resolution}(b) and Fig. \\ref{minimum_resolution}(c), we plot the cumulated CT-IQA scores in $series1$ and $series2$. It is clear that CT-IQA fails to characterize the trend of image quality in $series1$, but successfully reflects the peak in $series2$. In this example, the MSD between adjacent images in $series1$ is below the minimum resolution of the bilateral filter, but the MSD between adjacent images in $series2$ is above the minimum resolution of the bilateral filter. \n\n\n\n\n\n\n\nThere are two ways to avoid the unwanted result of operating below minimum resolution. First, increase the difference between adjacent images by increasing the parameter steps. Second, avoid comparing the adjacent images in a series of increasingly distorted images. The Key Image algorithm introduced in the next part is an implementation of the second way.\n\n\n\n\\subsection{Experiment verification on databases}\n\nIn this part, we evaluate the performance of different NR-IQA algorithms by comparing their balance abilities between noise and blurring on two databases. \n\n\\subsubsection{Balance ability among different distortions}\nIn this experiment, four distortions are applied to each original image and four series of increasingly distorted images are created: independent and identically distributed Gaussian noise (IID-GN), Zero-mean Gaussian noise with an intensity-dependent variance (ID-GN), blurring with Gaussian kernel (GB) and blurring with bilateral filter (BB). We reduce the parameters of the bilateral filter to one parameter the same way as we did in Section \\ref{case_study}. For each image under each kind of distortion, we first search the distortion parameter to ensure the SSIM index of the distorted image is between $0.85\\pm 0.01$, and then uniformly sample the other 14 parameters between 0 and the searched distortion parameter. In Fig. \\ref{blanace_separately_example}, we show the IQA scores of 60 distorted ``caps'' by different IQA algorithms. All the scores are also normalized to $[0,1]$ for each IQA algorithm.\n\n\\begin{figure}\t\n\t\\includegraphics[width = 0.99\\linewidth,trim = {2.6cm 0 2.6cm 0}, clip]{blanace_separately_example}\n\t\\caption{IQAs scores of 60 degraded ``caps'' by four distortions}\n\t\\label{blanace_separately_example}\n\\end{figure}\n\n\nSince it is justified for each IQA algorithm to have its own sensitivity properties at different distortion levels, we evaluate the balance ability among different distortions of each NR-IQA algorithm at 14 distortion levels. Eight distorted images with adjacent distortion parameters of four distortions are combined into an image set. The average SSIM index of all the eight images is the distortion level of this set. Therefore, we have 14 sets of distorted images and rank the eight distorted images in each set according to different IQA algorithms. The weighted inversion numbers\\cite{inversion_number} between the ranking results by NR-IQA algorithms and by SSIM are used to evaluate the performance of different NR-IQA algorithms. Assume $(I_1,\\cdots,I_N)$ is the ranking sequence according to a NR-IQA algorithm from low quality to high quality, the weighted inversion number in our experiment is defined as\n", "itemtype": "equation", "pos": 36556, "prevtext": "\n", "index": 37, "text": "\n\\[p_m^i = s_m^i + pre_{len}\\cdot g_m^i.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"p_{m}^{i}=s_{m}^{i}+pre_{len}\\cdot g_{m}^{i}.\" display=\"block\"><mrow><mrow><msubsup><mi>p</mi><mi>m</mi><mi>i</mi></msubsup><mo>=</mo><mrow><msubsup><mi>s</mi><mi>m</mi><mi>i</mi></msubsup><mo>+</mo><mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msub><mi>e</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi></mrow></msub></mrow><mo>\u22c5</mo><msubsup><mi>g</mi><mi>m</mi><mi>i</mi></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04619.tex", "nexttext": "\n\n\n\n\nSince C-IQA/CT-IQA are rank-based algorithms, distorted images in one set are sorted by the bubble sort algorithm. The bubble sort algorithm is the same as the traditional one, except that we use C-IQA/CT-IQA to compare each adjacent pair of images in the ranking sequence. In our experiment, the final ranking results are not sensitive to the initial ranking and we start the bubble sort from a random rank. Fig. \\ref{LIVE_OSU_balance} shows weighted inversion numbers at 14 distortion levels on two databases. \n\n\n\\begin{figure}[t]\t\n\t\\begin{minipage}[t]{0.99\\linewidth}\n\t\t\\centering\n\t\t \\includegraphics[width = 0.99\\linewidth,trim = {8cm 1cm 9cm 2cm}, clip]{sep_ranking_inv}\n\t \\end{minipage}\t \n\\caption{Weighted inversion numbers of different NR-IQA methods at 14 distortion levels}\n\\label{LIVE_OSU_balance}\n \\end{figure}\n\nIn Table \\ref{LIVE_OSU_ability_balance}, we provide the average weighted inversion numbers over 14 distortion levels of different NR-IQA algorithms. It is clear C-IQA and CT-IQA are two of the best NR-IQA algorithms.\n\n \\begin{table*}\n\t \\centering\n\t \\caption{Average weighted inversion numbers of different NR-IQA algorithms}\n\t \\begin{tabular}{|c|c|c|c|c|c|c|} \\hline\n\t\t  & DIIVINE & BRISQUE & MetricQ & Anisotropy & C-IQA (ranking) & CT-IQA (ranking) \\\\ \\hline\t  \t\t \t\n\t\t  LIVE  & 0.2328 & 0.2032 & 0.2205 & 0.2064  & \\textbf{0.1001} & 0.1026 \\\\ \\hline\t\t \n\t\t  CSIQ &  0.2137 & 0.2168  & 0.2283 & 0.2274 & \\textbf{0.1207} & 0.1362 \\\\ \\hline\n \\end{tabular} \n\t \\label{LIVE_OSU_ability_balance}\n \\end{table*}\n\n\n\\subsubsection{Balance ability for bilateral filter}\nA series of increasingly denoised images by bilateral filtering, $I_1, I_2,\\cdots, I_{30}$, is created for each image the same as the previous part. The SSIM index of the most oversmoothed image $I_{30}$ is between $0.85\\pm 0.01$. Because the MSD between the adjacent images are below minimum resolution, Alg. \\ref{key_image} is adopted to select the best result. Key images are a set of images among which the MSD is greater than the minimum resolution. Alg. \\ref{key_image} first separates the 30 increasingly distorted images into a few parts by key images. Images in the two parts next to the best key image are evaluated based on the two key images on the ends. By doing so, we avoid comparing the adjacent images directly. We set $K_{thresh} = 3.0$ in this experiment.\n\n\\begin{algorithm}\n\\caption{Key Image}\\label{key_image}\n\\begin{algorithmic}\n\n\\State{\\hspace{-1em}\\textbf{Key Images Selection;}}\n\\State{$key\\_img = [1]$}\n\\State{$key_{num} = 1$}\n\\For{$i = 1:N$}\n\\If{$MSE(I_i,I_{pre_{key}(key_{num})})>K_{thresh}$}\n\t\\State{$key\\_img = [key\\_img,i]$}\n\t\\State{$key_{num} = key_{num}+1$}\n\\EndIf\n\\EndFor\n\n\n\\bigskip\n\\State{\\hspace{-1em}\\textbf{Key Images Comparision;}}\n\n\\For{$i = 2:(key_{num}-1)$}\n\t\\If{$CQ(I_{key\\_img(i)},I_{key\\_img(i-1)})>0 $ and\\\\\n\t\t\\quad\\quad\\quad$CQ(I_{key\\_img(i)},I_{key\\_img(i+1)})>0$\n\t\t}\n\t\t\\State{$best_{key} = i$}\n\t\t\\State{\\it{break;}}\n\t\\EndIf\n\t\n\\EndFor\n\n\\bigskip\n\\State{\\hspace{-1em}\\textbf{Best Image Selection;}}\n\\State{$start_{num} = key\\_img(best_{key}-1)$}\n\\State{$end_{num} = key\\_img(best_{key}+1)$}\n\\For{$i=start_{num}:end_{num}$}\n\t\\State{$score_{start}(i) = CQ(I(i),I(start_{num}))$}\n\t\\State{$score_{end}(i) = CQ(I(i),I(end_{num}))$}\n\\EndFor\n\\State{$best_{img} = max(score_{start}+score_{end})$}\n\n\\end{algorithmic}\n\\end{algorithm}\n\nThe SSIM index difference between the best images chosen by a NR-IQA method and the one chosen by SSIM is used to evaluate NR-IQA methods. There are 59 original images in the two databases and 1770 distorted images in our experiment. \n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim = {0cm 0cm 0cm 0cm},clip]{LIVE_bilateral}\n(a) LIVE\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim = {0cm 0cm 0cm 0cm},clip]{CSIQ_bilateral}\n(b) CSIQ\n\\end{minipage}\n\\caption{The SSIM differences between the best images chosen by IQA method and by SSIM from denoised images}\n\\label{bilateral_database}\n\\end{figure}\nFrom Fig. \\ref{bilateral_database}, we can see that MetricQ and CT-IQA are two of the best NR-IQA algorithms. Table \\ref{bilatral_database_table} provides more quantitative evaluations of different NR-IQA algorithms.\n\\begin{table*}\n\\centering\n\\caption{Balancing abilities of blurring and noise on a series of denoised images of different IQAs}\n\\begin{tabular}{|cc|c|c|c|c|c|c|} \\hline\n & & DIIVINE & BRISQUE & MetricQ & Anisotropy & C-IQA & C-T-IQA \\\\ \\hline\n\\multirow{2}{*}{LIVE}  \n\t& \\multicolumn{1}{ |c| }{median of all SSIM differences} & $1.06\\times10^{-1}$ & $1.07\\times10^{-1}$ &  $\\mathbf{2.36\\times10^{-3}}$  & $6.95\\times10^{-2}$ &  $1.43\\times10^{-2}$ & $3.80\\times10^{-3}$  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of all SSIM differences} & $1.22\\times10^{-1}$ &  $1.25\\times10^{-1}$   & $6.73\\times10^{-3}$ & $8.23\\times10^{-2}$  &  $1.43\\times10^{-2}$  & {$\\mathbf{6.05\\times10^{-3}}$}  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of non-outliers} &  $1.22\\times10^{-1}$  & $1.25\\times10^{-1}$  & $5.67\\times10^{-3}$ & $8.23\\times10^{-2}$  & $1.12\\times10^{-2}$  & {$\\mathbf{3.93\\times10^{-3}}$}   \\\\ \\hline\n\\multirow{2}{*}{CSIQ}  \n\t& \\multicolumn{1}{ |c| }{median of all SSIM differences} & $8.81\\times10^{-2}$ & $8.68\\times10^{-2}$ & $\\mathbf{2.83\\times10^{-3}}$ & $2.18\\times10^{-2}$ & $5.28\\times10^{-3}$ &  $4.05\\times10^{-3}$  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of all SSIM differences} & $9.11\\times10^{-2}$ & $8.95\\times10^{-2}$ &  $7.65\\times10^{-3}$ & $4.30\\times10^{-2}$ &  $1.09\\times10^{-2}$ & {$\\mathbf{6.24\\times10^{-3}}$}  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of non-outliers} &  $8.54\\times10^{-2}$ & $8.06\\times10^{-2}$ & $6.34\\times10^{-3}$ & $3.17\\times10^{-2}$ & $6.36\\times10^{-3}$ & {$\\mathbf{5.32\\times10^{-3}}$}       \\\\ \\hline\n \\end{tabular} \n\\label{bilatral_database_table}\n\\end{table*}\n \nIn order to have a better understanding of Comparison-based IQA, we show two outliers of C-IQA/CT-IQA in Fig. \\ref{outliers}. Without the knowledge of the contents in the scenes, fine textures are regarded as noise in these two images, and both C-IQA/CT-IQA choose oversmoothed images as the best denoised results.\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.58\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth]{stream}\n(a) ``stream'' (from LIVE)\n\\end{minipage}\n\\begin{minipage}[t]{0.38\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth]{geckos}\n(b) ``geckos'' (from CSIQ)\n\\end{minipage}\n\\caption{Outliers of parameter selection}\n\\label{outliers}\n\\end{figure}\n\n\\subsubsection{Balance ability for TV reconstruction}\n\\label{balancity_recon_sec}\nThe algorithm used for image reconstruction is introduced in Section \\ref{image_reconstruction}. In the experiment, $70\\%$ Fourier transform data are used to reconstruct the image and in order to be more realistic, Fourier transform data are distorted by Gaussian noise. The SNR is kept at 20 dB in all reconstruction experiments. All 30 regularization parameter candidates are uniformly selected between $[10^{-5},10^{-1}]$ in logarithmic scale. Because in this experiment, the MSD between adjacent images is above the minimum resolution of C-IQA/CT-IQA, we choose the best image simply by comparing the adjacent images.\n\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim = {0cm 0cm 0cm 0cm},clip]{LIVE_recon}\n(a) LIVE\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim = {0cm 0cm 0cm 0cm},clip]{CSIQ_recon}\n(b) CSIQ\n\\end{minipage}\n\\caption{The SSIM differences between the best images chosen by NR-IQA method and the by SSIM from reconstructed images}\n\\label{recon_database}\n\\end{figure}\n\nTh SSIM difference of each IQA algorithm is plotted in Fig. \\ref{recon_database}. C-IQA and CT-IQA are the two best IQA algorithms. In Table \\ref{recon_database_table}, we provide quantitative evaluation of different NR-IQA algorithms.\nThe two outliers in TV reconstruction are the same images as shown in Fig. \\ref{outliers}.\n\\begin{table*}\n\\centering\n\\caption{Balancing abilities of blurring and noise on a series of reconstructed images of different IQAs}\n\\begin{tabular}{|cc|c|c|c|c|c|c|} \\hline\n & & DIIVINE & BRISQUE & MetricQ & Anisotropy & C-IQA & C-T-IQA \\\\ \\hline\n\\multirow{2}{*}{LIVE}  \n\t& \\multicolumn{1}{ |c| }{median of all SSIM differences} & $1.67\\times10^{-1}$ & $1.33\\times10^{-1}$ &  $2.42\\times10^{-2}$ & $8.88\\times10^{-2}$ &  $2.97\\times10^{-3}$ & $\\mathbf{0}$  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of all SSIM difference} & $1.85\\times10^{-1}$ &  $1.33\\times10^{-1}$   & $5.07\\times10^{-2}$ & $1.09\\times10^{-1}$  &  $9.92\\times10^{-3}$  & {$\\mathbf{7.77\\times10^{-3}}$}  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of non-outliers} &  $1.74\\times10^{-1}$  & $1.33\\times10^{-1}$  & $5.07\\times10^{-2}$ & $1.09\\times10^{-1}$  & $7.02\\times10^{-3}$  & {$\\mathbf{2.07\\times10^{-3}}$}   \\\\ \\hline\n\\multirow{2}{*}{CSIQ}  \n\t& \\multicolumn{1}{ |c| }{median of all SSIM difference} & $1.24\\times10^{-1}$ & $1.05\\times10^{-1}$ & $2.44\\times10^{-2}$ & $3.66\\times10^{-2}$ & $\\mathbf{1.73\\times10^{-3}}$ &  $\\mathbf{1.73\\times10^{-3}}$  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of all SSIM difference} & $1.43\\times10^{-1}$ & $1.23\\times10^{-1}$ &  $4.25\\times10^{-2}$ & $4.30\\times10^{-2}$ &  $\\mathbf{1.12\\times10^{-2}}$ & {$1.12\\times10^{-2}$}  \\\\ \\cline{2-8} \n\t& \\multicolumn{1}{ |c| }{average of non-outliers} &  $1.31\\times10^{-1}$ & $1.23\\times10^{-1}$ & $2.19\\times10^{-2}$ & $3.81\\times10^{-2}$ & $6.02\\times10^{-3}$ & {$\\mathbf{5.38\\times10^{-3}}$}       \\\\ \\hline\n \\end{tabular} \n\\label{recon_database_table}\n\\end{table*}\n\n\\begin{figure}\t\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth]{buildings}\n\t\t(a) ``buildings'' (from LIVE)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0.5cm 2.5cm 10cm},clip]{trimming_example1}\n\t\t(b) Convergence without parameter trimming\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 2.5cm 10cm},clip]{trimming_example2}\n\t\t(c) Convergence with parameter trimming\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 2.5cm 10cm},clip]{currentIQA}\n\t\t(d) Current CT-IQA scores\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 2.5cm 10cm},clip]{gradientIQA}\n\t\t(e) Current CT-IQA gradients\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.49\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 2.5cm 10cm},clip]{predictIQA}\n\t\t(f) Predicted CT-IQA scores\n\t\\end{minipage}\n\t\n\t\\caption{Comparison between convergence with and without 1-D parameter trimming on ``buildings''}\n\t\\label{trimming_examples}\n\\end{figure}\n\\begin{figure}\t\n\t\\begin{minipage}[t]{0.25\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth]{aerial_city}\n\t\t(a) ``aerial\\_city'' (from CSIQ)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.36\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 3cm 9.5cm},clip]{2Dbeta}\n\t\t(b) Convergence without parameter trimming (different $\\gamma$)\n\t\\end{minipage}\n\t\\begin{minipage}[t]{0.36\\linewidth}\n\t\t\\centering\n\t\t\\includegraphics[width = 0.99\\linewidth,trim = {23.5cm 0cm 3cm 9.5cm},clip]{2Dgamma}\n\t\t(c) Convergence with parameter trimming (different $\\beta$)\n\t\\end{minipage}\n\t\n\t\\caption{Comparison between convergence with and without 2-D parameter trimming on ``aerial city''}\n\t\\label{2Dtrimming_examples}\n\\end{figure}\n\n\\subsection{Application in iterative framework}\nIn this section, we combine CT-IQA with the parameter trimming framework and show that considerable computation can be saved while preserving the accuracy of parameter selection. \n\\subsubsection{1-D parameter trimming}\nIn this part, all the parameter settings are the same as the settings in Section \\ref{balancity_recon_sec}.\nOn LIVE\\cite{LIVE_dataset}, all the parameters selected with parameter trimming are the same as the parameters selected after convergence; on CSIQ\\cite{FR_IQA_review_OSU}, only one of the best parameter selected by parameter trimming is different from the one selected after convergence. Fig. \\ref{trimming_examples}(a) is one example image in parameter trimming. In Fig. \\ref{trimming_examples}(b) and Fig. \\ref{trimming_examples}(c), we show the effectiveness of parameter trimming with SSIM as the quality index (SSIM is only used to demonstrated the trimming process here); Fig. \\ref{trimming_examples}(d) -Fig. \\ref{trimming_examples}(f) show the changes of three CT-IQA related indices that we actually use to make the trimming decision. From Fig. \\ref{trimming_examples}, we can see that the trimming decision achieves the goal of terminating iteration of parameters that is far from the best choice and thus saving computation.\n\n\n\nIn Table \\ref{parameter_trimming_table}, we provide the average numbers of iterations with and without parameter trimming per image on two databases.\n\n\\begin{table}\n\\centering\n\\caption{Computation saved by parameter trimming}\n\\begin{tabular}{|p{0.6cm}|p{2.4cm}|p{2.4cm}|p{1cm}|} \\hline\n & {Ave \\# of iteration without parameter trimming} & Ave \\# of iteration with parameter trimming& saved computation (\\%)\\\\  \\hline\nLIVE& \t4651.9\t&\t\t847.7\t\t\t&\t81.78\t\t\\\\\t\\hline\nCSIQ&\t\t4565.6\t&\t941.1\t\t&\t79.39\t\\\\\t\\hline\n\\end{tabular}\n\\label{parameter_trimming_table}\n\\end{table}\n\n\n\n\\subsubsection{2-D parameters trimming}\nWe include two regularizers, TV and Haar wavelet, for the reconstruction algorithm in this part. Two regularization parameters, $\\beta$ and $\\gamma$, are for TV and Haar wavelet respectively and both have six parameter candidates. Regularization parameters, $\\beta$ and $\\gamma$, are uniformly sampled between $[10^{-5},10^{-1}]$ and $[10^{-8},10^{-1}]$ in logarithmic scale. One image from CSIQ\\cite{FR_IQA_review_OSU} is tested for 2-D parameter trimming and the best set of parameters is correctly selected by parameter trimming. In Fig. \\ref{2Dtrimming_examples}, parameter trimming is illustrated by changing one parameter while fixing the other parameter as the best choice.\n\n\n\\section{Conclusion}\n\\label{conclusion}\nIn this paper, we focused on the NR-IQA method and discussed the advantages and drawbacks of two different approaches to the NR-IQA problem, global and local approaches. Inspired by some key concepts put forward in the previous works \\cite{anisotropy,MetricQ}, for the first time we designed a comparison-based IQA method and analyzed important properties unique to comparison-based IQA, such as minimum resolution. The novel C-IQA/CT-IQA method includes three primary modules, Content Detection, Contribution and Texture Compensation. At last, the comparison-based IQA compares favorably with other NR-IQA algorithms on two widely used databases\\cite{LIVE_dataset, FR_IQA_review_OSU}.\n\nIn the experiment (Section \\ref{experiments}), we showed that when fine texture with small granularity appears in the image, C-IQA/CT-IQA tends to select the suboptimal result. Integrating the global texture information with local gradient-based structure information is a possible solution to improve the robustness of comparison-based IQA and other NR-IQA algorithms.\n\nWe take C-IQA/CT-IQA as a specific implementation of the comparison-based IQA method. By exploiting the extra available information in many image quality assessment applications, other comparison-based IQA methods can be designed for different application scenarios.\n\n\n\n\\section*{Acknowledgment}\nThe authors would like to thank Xiang Zhu for providing the MetricQ\\cite{MetricQ} code and Anush Krishna Moorthy for explaining the usage of DIIVINE\\cite{DIVIINE} code.\n\\label{sec:ref}\n\\bibliographystyle{IEEEbib}\n\\bibliography{refs}\n\n", "itemtype": "equation", "pos": 46204, "prevtext": "\nWe set $pre_{len} = 4$ in all the experiments. More examples of the reconstruction process and the changing of these three indices during the trimming process are shown in Section \\ref{experiments}.\n\n\\section{Experiments}\n\\label{experiments}\n\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[height = 3.2cm]{caps}\n(a) ``caps''\n\\label{sample_images_case_study_caps}\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\linewidth}\n\\centering\n\\includegraphics[height = 3.2cm]{coinsinfountain}\n(b) ``coinsinfountain''\n\\end{minipage}\n\\caption{Images from LIVE\\cite{LIVE_dataset} used for case study in Section \\ref{case_study}}\n\\label{sample_images_case_study}\n\\end{figure}\n\nWe first introduce two key properties (consistency and minimum resolution) of C-IQA/CT-IQA in Section \\ref{case_study}. In the next two parts, more comprehensive experiments on two databases are conducted to verify the effectiveness of C-IQA/CT-IQA and their applications to parameter selection.\n\nThe other NR-IQA algorithms that we use to compare C-IQA/CT-IQA with are DIIVINE (DII)\\cite{DIVIINE}, BRISQUE (BRI)\\cite{BRISQUE}, MetricQ (MQ)\\cite{MetricQ} and Anisotropy (Ani)\\cite{anisotropy}. A widely accepted FR-IQA algorithm, SSIM\\cite{SSIM}, is used as the ground truth to evaluate the performance of different NR-IQA algorithms. Two IQA databases used in the experiments are LIVE\\cite{LIVE_dataset} and CISQ\\cite{FR_IQA_review_OSU}. Parameters in C-IQA/CT-IQA are set as $C_{thresh}=0.12$, $C_1 = 4.6$ and the size of local patch is $9\\times 9$.\n\n\\subsection{Case study to explore two key properties of C-IQA/CT-IQA}\n\\label{case_study}\n\\begin{figure}[t]\n\\begin{minipage}[t]{0.99\\linewidth}\n\\centering\n\\includegraphics[width = 0.99\\linewidth,trim={7cm 0cm 18cm 0cm},clip]{consistence}\n\\centering\n\\end{minipage}\n\\caption{Consistency of different IQA algorithms on noise and blurring. The scores of Anisotropy and DIIVINE are inconsistent on Gaussian noise.}\n\\label{consistence_fig}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7cm 0cm 0.5cm 4cm},clip]{minimum_resolution_neighbor}\n(a) \n\\end{minipage}\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7.5cm 0cm 0cm 3cm},clip]{minimum_resolution_caps}\n(b) \n\\end{minipage}\n\\begin{minipage}[t]{0.32\\linewidth}\n\\centering\n\\includegraphics[width=0.99\\linewidth,trim ={7.5cm 0cm 0cm 3cm},clip]{minimum_resolution_coins}\n(b)\n\\end{minipage}\n\\caption{Minimum resolution of Comparison-based IQA algorithm: (a) CT-IQA scores of denoised images compared with their previous images (series1) and the one before previous images (series2); (b) Accumulated CT-IQA scores in (a) and SSIM scores of ``caps''; (c) Accumulated CT-IQA scores in (a) and SSIM scores of ``coinsinfountain''}\n\\label{minimum_resolution}\n\\end{figure*}\n\nSince the comparison-based IQA is a brand-new approach, some new properties arise. In this section, we illustrate these properties and corresponding solutions based on two images from LIVE\\cite{LIVE_dataset} as shown in Fig. \\ref{sample_images_case_study}.\n\n\n\\subsubsection{Consistency on single kind of distortion}\n\\label{consistence_part}\nConsistency on a single kind of distortion is one of the basic requirements of an IQA algorithm. Because distortions can be generally classified into two categories\\cite{denoise_review}, noise and blur, we check the consistency of different NR-IQA algorithms on Gaussian noise and blurring with the Gaussian kernel respectively. \n\n\nFor each distortion, a series of increasingly distorted images, whose SSIM indices uniformly range from 0.85 to 1, are evaluated by different NR-IQA algorithms. Since we are interested in the trend of each IQA algorithm, scores obtained from different IQA algorithms are normalized between [0, 1] and the original images are used as the base images for C-IQA/CT-IQA. It is clear from Fig. \\ref{consistence_fig} that all of NR-IQA algorithms produce consistent results for blurring, but in the noise case, DIIVINE and Anisotropy are inconsistence. \n\n\n\n\\subsubsection{Minimum resolution}\n\\label{minimum_resolution_part}\nSimilar to HVS, IQA algorithms are not able to make a convincing quality comparison between images whose difference is sufficiently small.\n\nIn this part, we define the minimum mean squared difference (MSD) between two images required to make a convincing quality comparison as the minimum resolution. It is worth noticing that minimum resolutions vary over different distortions and different IQA algorithms.\n\nFor the traditional single-image-input NR-IQA algorithms, minimum resolutions can be regarded as the minimum MSD required to ensure consistency on a series of increasingly distorted images. The unwanted fluctuations of DIIVINE and Anisotropy in Fig. \\ref{consistence_fig}(a) indicate that the MSD between the adjacent images is less than their minimum resolutions of Gaussian noise.\n\nHowever, under the comparison-based framework, a distorted image has different scores compared with different base images. We cannot refer to the consistency to define the minimum resolution for a comparison-based IQA algorithm. The minimum resolution for comparison-based IQA is defined as the minimum MSD required to preserve transitivity among a series of distorted images. We conduct an experiment on the images in Fig. \\ref{sample_images_case_study} to demonstrate the transitivity. Assume $I_{org}$ is the original image, and $I_1$ is created by adding Gaussian noise to $I_{org}$. A series of gradually filtered images, $(I_1,I_2,\\cdots,I_N)$, are denoised by bilateral filters $BF_{(r,d)}$, where $r$ and $d$ are the variances of Gaussian range kernel for smoothing differences in intensities and Guassian spatial kernel for smoothing differences in coordinates. For simplicity, we reduce the parameters of bilateral filters to one by fixing the ratio between $r$ and $d$, $BF_{k} = BF_{(0.1k,3k)}$. In Fig. \\ref{minimum_resolution}(a), we show the CT-IQA scores of each image compared with its previous one in the denoised sequence ($series1$) and the CT-IQA scores compared with the one before its previous one ($series2$). We can see that CT-IQA scores in $series1$ are always positive, but pass $0$ in $series2$, which means the denoised image qualities are monotonically increasing in $series1$, but reach a peak in $series2$. In Fig. \\ref{minimum_resolution}(b) and Fig. \\ref{minimum_resolution}(c), we plot the cumulated CT-IQA scores in $series1$ and $series2$. It is clear that CT-IQA fails to characterize the trend of image quality in $series1$, but successfully reflects the peak in $series2$. In this example, the MSD between adjacent images in $series1$ is below the minimum resolution of the bilateral filter, but the MSD between adjacent images in $series2$ is above the minimum resolution of the bilateral filter. \n\n\n\n\n\n\n\nThere are two ways to avoid the unwanted result of operating below minimum resolution. First, increase the difference between adjacent images by increasing the parameter steps. Second, avoid comparing the adjacent images in a series of increasingly distorted images. The Key Image algorithm introduced in the next part is an implementation of the second way.\n\n\n\n\\subsection{Experiment verification on databases}\n\nIn this part, we evaluate the performance of different NR-IQA algorithms by comparing their balance abilities between noise and blurring on two databases. \n\n\\subsubsection{Balance ability among different distortions}\nIn this experiment, four distortions are applied to each original image and four series of increasingly distorted images are created: independent and identically distributed Gaussian noise (IID-GN), Zero-mean Gaussian noise with an intensity-dependent variance (ID-GN), blurring with Gaussian kernel (GB) and blurring with bilateral filter (BB). We reduce the parameters of the bilateral filter to one parameter the same way as we did in Section \\ref{case_study}. For each image under each kind of distortion, we first search the distortion parameter to ensure the SSIM index of the distorted image is between $0.85\\pm 0.01$, and then uniformly sample the other 14 parameters between 0 and the searched distortion parameter. In Fig. \\ref{blanace_separately_example}, we show the IQA scores of 60 distorted ``caps'' by different IQA algorithms. All the scores are also normalized to $[0,1]$ for each IQA algorithm.\n\n\\begin{figure}\t\n\t\\includegraphics[width = 0.99\\linewidth,trim = {2.6cm 0 2.6cm 0}, clip]{blanace_separately_example}\n\t\\caption{IQAs scores of 60 degraded ``caps'' by four distortions}\n\t\\label{blanace_separately_example}\n\\end{figure}\n\n\nSince it is justified for each IQA algorithm to have its own sensitivity properties at different distortion levels, we evaluate the balance ability among different distortions of each NR-IQA algorithm at 14 distortion levels. Eight distorted images with adjacent distortion parameters of four distortions are combined into an image set. The average SSIM index of all the eight images is the distortion level of this set. Therefore, we have 14 sets of distorted images and rank the eight distorted images in each set according to different IQA algorithms. The weighted inversion numbers\\cite{inversion_number} between the ranking results by NR-IQA algorithms and by SSIM are used to evaluate the performance of different NR-IQA algorithms. Assume $(I_1,\\cdots,I_N)$ is the ranking sequence according to a NR-IQA algorithm from low quality to high quality, the weighted inversion number in our experiment is defined as\n", "index": 39, "text": "\n\\[WInv_{num}=\\sum_{i=1:N}{\\sum_{j=i+1:N}{max(0,SSIM(I_i)-SSIM(I_j))}}.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"WInv_{num}=\\sum_{i=1:N}{\\sum_{j=i+1:N}{max(0,SSIM(I_{i})-SSIM(I_{j}))}}.\" display=\"block\"><mrow><mrow><mrow><mi>W</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>m</mi></mrow></msub></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mo>:</mo><mi>N</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>j</mi><mo>=</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>:</mo><mi>N</mi></mrow></munder><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]