[{"file": "1601.05519.tex", "nexttext": "\n\n\n\\noindent\nAt this point, one should take note of the fact that the net synergy can be both \npositive ($\\Delta I > 0$) and negative ($\\Delta I < 0$) valued. Information \ntheoretically, \npositive net synergy implies synergistic aspect of the target variables is prevalent \nover the extent of redundant character. Negative net synergy conveys precisely the \nopposite implication. Positive net synergy indicates the information shared \nbetween the source S and the targets (X, Y) taken together as a single \ntarget is more than the sum of the information shared between S and the targets \nX, Y considered individually in turn. Negative net synergy indicates separately the \ntargets share more information with the source than while considered together as a \nsingle target. Zero net synergy ($\\Delta I = 0$) is an interesting case to mention \nas it means that \nthe targets X and Y are informationally independent of each other since the \ninformation shared between the source and the targets does not depend on \nwhether one takes the targets individually or together as a single target to compute \ndifferent mutual information terms. As per the definition of the net synergy stated in \nEq.~(\\ref{eq1}), it is maximum while the total information is synergistic and\nassumes a minimum value when the total information is purely redundant in \nnature. In this study, we do not quantify the separate identification of synergy \nand redundancy. Rather, we conceive the quantity of interest here as redundant \nsynergy. Information theoretically, the net synergy serves as a quantification marker \nof information independence between the random variables often characterized \nas source and target variables \\cite{Schneidman2003}. Although, the tags of \nsource(s) and target(s) are not strict enough in information theoretic sense but \nwhile dealing with cascades or directed networks, in general, such classifications \ncan be done suitably as the phenomenology or experimental realities demand. \n\n\nTo understand the redundancy in the information transmission in biochemical\nprocesses we undertake the kinetics associated with a generic two step cascade\n$\\text{S} \\rightarrow \\text{X} \\rightarrow \\text{Y}$.\nThe dynamics related to the two step cascade is linear in nature and\nprovides analytical solution at steady state within the purview of Gaussian\nnoise processes. In this connection, it is important to mention theoretical analysis\nperformed on different biochemical motifs obeying Gaussian noise processes \n\\cite{Ziv2007,Ronde2010,Tanase2006,Bruggeman2009,Tostevin2010,Maity2014,\nMaity2015,Grima2015,Maity2016}.\nIn this set of works, the theoretical analysis was performed using linear noise \napproximation \\cite{Kampen2007} that provides exact expressions of second \nmoments \\cite{Warren2006,Grima2015}.\n\n\n\n\n\n\nThe set of Langevin equations governing the dynamics of a two step \ncascade motif can be written as,\n\\begin{eqnarray}\n\\label{eq2}\n\\frac{ds}{dt} & = & k_s - \\mu_s s + \\sqrt{\\alpha_s} \\xi_s(t), \\\\\n\\label{eq3}\n\\frac{dx}{dt} & = & k_x s - \\mu_x x + \\sqrt{\\alpha_x} \\xi_x(t), \\\\\n\\label{eq4}\n\\frac{dy}{dt} & = & k_y x - \\mu_y y + \\sqrt{\\alpha_y} \\xi_y(t),\n\\end{eqnarray}\n\n\n\\noindent\nwhere  $s$, $x$ and $y$ are three nodes of the cascade representing three \ndifferent gene products. Here $k_i$-s and $\\mu_i$-s ($i=s, x, y$) are the \nsynthesis and degradation rates of the components S, X and Y, \nrespectively. The noise terms $\\xi_s(t)$, $\\xi_x(t)$ and $\\xi_y(t)$ are of\nGaussian white type with properties\n$\\langle \\xi_s(t) \\rangle = \\langle \\xi_x(t) \\rangle = \\langle \\xi_y(t) \\rangle = 0$\nand \n$\\langle \\xi_s(t) \\xi_s(t') \\rangle =  \\langle \\xi_x(t) \\xi_x(t') \\rangle =\n\\langle \\xi_y(t) \\xi_y(t') \\rangle = \\delta (t-t')$. Also,\nthe different noise processes are uncorrelated, \n$\\langle \\xi_i(t) \\xi_j(t') \\rangle =\\delta_{ij} \\delta(t-t')$.\nThe noise strengths $\\alpha_i$-s ($i = s, x, y$) are defined as\n$\\alpha_s = 2 \\mu_s \\langle s \\rangle$, $\\alpha_x = 2 \\mu_x \\langle x \\rangle$, \n$\\alpha_y = 2 \\mu_y \\langle y \\rangle$. The quantities $\\langle s \\rangle, \n\\langle x \\rangle$ and $ \\langle y \\rangle$ are the  steady state values of \nthe respective species and are meassured as \n$\\langle s \\rangle=k_s / \\mu_s$, \n$\\langle x \\rangle= (k_x / \\mu_x) \\langle s \\rangle$ and\n$\\langle y \\rangle=(k_y / \\mu_y) \\langle x \\rangle$, respectively.\n\nWe now expand Eqs.~(\\ref{eq2}-\\ref{eq4}) around steady state\n$\\delta z(t) = z(t) - \\langle z \\rangle$, where $\\langle z \\rangle$ is the\naverage population of $z$ at steady state and obtain,\n\n", "itemtype": "equation", "pos": 3720, "prevtext": "\n\n\\title{Redundancy in the information transmission in a two-step cascade}\n\n\\author{Ayan Biswas}\n\\email{ayanbiswas@jcbose.ac.in}\n\\affiliation{Department of Chemistry, Bose Institute, 93/1 A P C Road, Kolkata 700009, India}\n\n\\author{Suman K Banik}\n\\email[Corresponding author: ]{skbanik@jcbose.ac.in}\n\\affiliation{Department of Chemistry, Bose Institute, 93/1 A P C Road, Kolkata 700009, India}\n\n\\begin{abstract}\nWe present a stochastic framework to study signal transmission in a \ngeneric two-step cascade $\\text{S} \\rightarrow \\text{X} \\rightarrow \\text{Y}$. \nStarting from a set of Langevin equations obeying Gaussian noise \nprocesses we calculate the variance and covariance associated with \nthe system components. These quantities are then used to calculate \nthe net synergy within the purview of partial information decomposition. \nWe show that redundancy in information transmission is essentially an\nimportant consequence of Markovian property of the two-step cascade\nmotif.\n\\end{abstract}\n\n\\pacs{87.10.-e, 05.40.-a, 87.18.Tt, 87.18.Vf}\n\n\\date{\\today}\n\n\\maketitle\n\n\n\n\n\nA living system sustains in the diverse and continuously changing environment. \nIn order to respond to the changes made in the surroundings, every living \nspecies developed complex signal transmission networks over the\nevolutionary time scale \\cite{Alon2006,Alon2007}. The main purpose of these \nnetworks is\nto transmit the extracellular changes reliably and efficiently to the cell. In\naddition, these networks take care of different biochemical changes that are taking\nplace within the cell. A typical signaling cascade comprises of one or \nseveral components of biochemical origin. The interactions between these \ncomponents are probabilistic in nature, thus giving rise to stochastic kinetics. \nOne of the tools to figure out the signal transmission mechanism in a fluctuating \nenvironment is information theory \\cite{Shannon1948,Cover1991}. The \nformalism of information theory provides a quantification of information \ntransfer between the source (signal) and the target (response). The \nmeasure of information transmission is characterized by mutual information \n(MI), that quantifies the common information content of the source and the \ntarget. Moreover, information theory provides a measure of fidelity of the \nsignaling pathway \\cite{Borst1999,Mitra2001,Ziv2007,Tostevin2010,Cheong2011,\nTkacik2011,Bowsher2013,Maity2015,Mahon2014}.\n\n\nThe notion of MI is conceptualized as the intersection of the entropy spaces \nof two stochastic variables \\cite{Shannon1948,Cover1991}. Hence, MI signifies \nthe average reduction in the uncertainty of prediction of one random variable \nwhen knowledge of another random variable is available. However, this \ninformation theoretic measure is symmetric in its argument random variables \nsignifying the `mutual' attribution linked to it. For a generic two step cascade\n$\\text{S} \\rightarrow \\text{X} \\rightarrow \\text{Y}$, although, MI between three \nvariables is an ill defined concept, its usage can be validated if one considers \n$I(s; x,y)$ to be the MI that the source variable S  shares with the pair of target \nvariables X and Y. It is thus interesting to investigate whether the three variable \nMI can be divided into more fundamental units, thus shedding light on various \ntypes of informational relationship among more than two stochastic variables. \nOne prevalent approach in this respect is known as partial information \ndecomposition (PID) \\cite{Williams2010,Barrett2015}. \n\nFollowing the formalism of PID one can define an information theoretic measure, \nthe net synergy $\\Delta I(s; x,y)$, in terms of two and three variable MI-s\n\\cite{Schneidman2003,Williams2010,Barrett2015,Maity2016}\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq1}\n\\Delta I(s; x,y) = I(s; x,y) - I(s; x) - I(s; y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\Delta I(s;x,y)=I(s;x,y)-I(s;x)-I(s;y).\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>;</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>;</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>;</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>;</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05519.tex", "nexttext": "\n\n\n\\noindent where\n\\begin{eqnarray*}\n\\delta \\mathbf{A} = \\left (\n\\begin{array}{c}\n\\delta s \\\\\n\\delta x \\\\\n\\delta y\n\\end{array} \n\\right ),\n\\mathbf{J} = \\left (\n\\begin{array}{ccc}\n-\\mu_s & 0 & 0 \\\\\nk_x & -\\mu_x & 0 \\\\\n0 & k_y & -\\mu_y\n\\end{array} \n\\right ),\n\\mathbf{\\Theta} = \\left (\n\\begin{array}{c}\n\\xi_s \\\\\n\\xi_x \\\\\n\\xi_y\n\\end{array} \n\\right ).\n\\end{eqnarray*}\n\n\n\\noindent\nHere $\\delta \\mathbf{A}$ and $\\mathbf{\\Theta}$ are the fluctuations matrix \nand the noise matrix, respectively. $\\mathbf{J}$ is the Jacobian matrix \nevaluated at steady state. To calculate the variance and covariance of \ndifferent species of the TSC motif, we use the Lyapunov equation at\nsteady state\n\\cite{Keizer1987,Elf2003,Paulsson2004,Paulsson2005}\n\n", "itemtype": "equation", "pos": 8349, "prevtext": "\n\n\n\\noindent\nAt this point, one should take note of the fact that the net synergy can be both \npositive ($\\Delta I > 0$) and negative ($\\Delta I < 0$) valued. Information \ntheoretically, \npositive net synergy implies synergistic aspect of the target variables is prevalent \nover the extent of redundant character. Negative net synergy conveys precisely the \nopposite implication. Positive net synergy indicates the information shared \nbetween the source S and the targets (X, Y) taken together as a single \ntarget is more than the sum of the information shared between S and the targets \nX, Y considered individually in turn. Negative net synergy indicates separately the \ntargets share more information with the source than while considered together as a \nsingle target. Zero net synergy ($\\Delta I = 0$) is an interesting case to mention \nas it means that \nthe targets X and Y are informationally independent of each other since the \ninformation shared between the source and the targets does not depend on \nwhether one takes the targets individually or together as a single target to compute \ndifferent mutual information terms. As per the definition of the net synergy stated in \nEq.~(\\ref{eq1}), it is maximum while the total information is synergistic and\nassumes a minimum value when the total information is purely redundant in \nnature. In this study, we do not quantify the separate identification of synergy \nand redundancy. Rather, we conceive the quantity of interest here as redundant \nsynergy. Information theoretically, the net synergy serves as a quantification marker \nof information independence between the random variables often characterized \nas source and target variables \\cite{Schneidman2003}. Although, the tags of \nsource(s) and target(s) are not strict enough in information theoretic sense but \nwhile dealing with cascades or directed networks, in general, such classifications \ncan be done suitably as the phenomenology or experimental realities demand. \n\n\nTo understand the redundancy in the information transmission in biochemical\nprocesses we undertake the kinetics associated with a generic two step cascade\n$\\text{S} \\rightarrow \\text{X} \\rightarrow \\text{Y}$.\nThe dynamics related to the two step cascade is linear in nature and\nprovides analytical solution at steady state within the purview of Gaussian\nnoise processes. In this connection, it is important to mention theoretical analysis\nperformed on different biochemical motifs obeying Gaussian noise processes \n\\cite{Ziv2007,Ronde2010,Tanase2006,Bruggeman2009,Tostevin2010,Maity2014,\nMaity2015,Grima2015,Maity2016}.\nIn this set of works, the theoretical analysis was performed using linear noise \napproximation \\cite{Kampen2007} that provides exact expressions of second \nmoments \\cite{Warren2006,Grima2015}.\n\n\n\n\n\n\nThe set of Langevin equations governing the dynamics of a two step \ncascade motif can be written as,\n\\begin{eqnarray}\n\\label{eq2}\n\\frac{ds}{dt} & = & k_s - \\mu_s s + \\sqrt{\\alpha_s} \\xi_s(t), \\\\\n\\label{eq3}\n\\frac{dx}{dt} & = & k_x s - \\mu_x x + \\sqrt{\\alpha_x} \\xi_x(t), \\\\\n\\label{eq4}\n\\frac{dy}{dt} & = & k_y x - \\mu_y y + \\sqrt{\\alpha_y} \\xi_y(t),\n\\end{eqnarray}\n\n\n\\noindent\nwhere  $s$, $x$ and $y$ are three nodes of the cascade representing three \ndifferent gene products. Here $k_i$-s and $\\mu_i$-s ($i=s, x, y$) are the \nsynthesis and degradation rates of the components S, X and Y, \nrespectively. The noise terms $\\xi_s(t)$, $\\xi_x(t)$ and $\\xi_y(t)$ are of\nGaussian white type with properties\n$\\langle \\xi_s(t) \\rangle = \\langle \\xi_x(t) \\rangle = \\langle \\xi_y(t) \\rangle = 0$\nand \n$\\langle \\xi_s(t) \\xi_s(t') \\rangle =  \\langle \\xi_x(t) \\xi_x(t') \\rangle =\n\\langle \\xi_y(t) \\xi_y(t') \\rangle = \\delta (t-t')$. Also,\nthe different noise processes are uncorrelated, \n$\\langle \\xi_i(t) \\xi_j(t') \\rangle =\\delta_{ij} \\delta(t-t')$.\nThe noise strengths $\\alpha_i$-s ($i = s, x, y$) are defined as\n$\\alpha_s = 2 \\mu_s \\langle s \\rangle$, $\\alpha_x = 2 \\mu_x \\langle x \\rangle$, \n$\\alpha_y = 2 \\mu_y \\langle y \\rangle$. The quantities $\\langle s \\rangle, \n\\langle x \\rangle$ and $ \\langle y \\rangle$ are the  steady state values of \nthe respective species and are meassured as \n$\\langle s \\rangle=k_s / \\mu_s$, \n$\\langle x \\rangle= (k_x / \\mu_x) \\langle s \\rangle$ and\n$\\langle y \\rangle=(k_y / \\mu_y) \\langle x \\rangle$, respectively.\n\nWe now expand Eqs.~(\\ref{eq2}-\\ref{eq4}) around steady state\n$\\delta z(t) = z(t) - \\langle z \\rangle$, where $\\langle z \\rangle$ is the\naverage population of $z$ at steady state and obtain,\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq5}\n\\frac{d \\delta \\mathbf{A}}{dt} =\n\\mathbf{J}_{A = \\langle A \\rangle} \\delta \\mathbf{A} (t) + \\mathbf{\\Theta} (t),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\frac{d\\delta\\mathbf{A}}{dt}=\\mathbf{J}_{A=\\langle A\\rangle}\\delta\\mathbf{A}(t%&#10;)+\\mathbf{\\Theta}(t),\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mi>\ud835\udc00</mi></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow></mfrac><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc09</mi><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>A</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></msub><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mi>\ud835\udc00</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udeaf</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05519.tex", "nexttext": "\n\n\n\\noindent\nHere, $\\mathbf{\\Sigma}$ is the covariance matrix and \n$\\mathbf{D} = \\langle \\mathbf{\\Theta} \\mathbf{\\Theta}^T \\rangle$ is the\ndiffusion matrix due to different noise strength $\\alpha_i$ ($i=s,x,y$).\nThe notation $\\langle \\cdots \\rangle$ stands for  ensemble average\nand $T$ is the transpose of a matrix. Solution of Eq.~(\\ref{eq6})\nprovides analytical expressions of variance and covariance associated\nwith $s$, $x$ and $y$\n\\begin{eqnarray*}\n\\Sigma (s) & =& \\langle s \\rangle,\n\\Sigma (s,x) = \\frac{k_x \\langle s \\rangle}{\\mu_s + \\mu_x}, \\\\\n\\Sigma (s,y) &=& \\frac{k_y k_x \\langle s \\rangle}{(\\mu_s + \\mu_x) (\\mu_s + \\mu_y)}, \\\\\n\\Sigma (x) & =& \\langle x \\rangle + \\frac{k_x^2 \\langle s \\rangle}{\\mu_x (\\mu_s + \\mu_x)}, \\\\\n\\Sigma (x,y) &=& \\frac{k_y}{\\mu_x + \\mu_y} \\Sigma (x) \n+ \\frac{k_x}{\\mu_x + \\mu_y} \\Sigma (s,y), \\\\\n\\Sigma (y) &=& \\langle y \\rangle + \\frac{k_y}{\\mu_y} \\Sigma (x,y).\n\\end{eqnarray*}\n\n\n\n\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.80\\columnwidth,angle=-90]{tsc-net-syn-fig1.eps}\n\n\\end{center}\n\\caption{The net synergy $\\Delta I(s; x,y)$ profile as a function of $\\mu_s$.\nThe solid line is drawn using Eq.~(\\ref{eq7}) and the symbols are \ngenerated from stochastic simulation \\cite{Gillespie1977}.\nThe simulation results are average of $10^5$ trajectories.\nThe parameters used are $k_s = 10$ min$^{-1}$, $k_x = 10$ min$^{-1}$, \n$k_y = 1$ min$^{-1}$, $\\mu_x = 0.5$ min$^{-1}$ and $\\mu_y = 5$ min$^{-1}$ \n\\cite{Ronde2010}.\n}\n\\label{fig1}\n\\end{figure}\n\n\nWe now quantify the MI-s associated with the linear signaling cascade \n$\\text{S} \\rightarrow \\text{X} \\rightarrow \\text{Y}$. \n\nFor $s$, $x$ and $y$ assumed to be Gaussian variables one can express\nthe net synergy associated with TSC motif as follows\n\\cite{Barrett2015}\n\\begin{eqnarray}\n\\label{eq7}\n\\Delta I(s;x,y) & = & \\frac{1}{2} \n\\left [\n\\log_2 \\left ( \\frac{\\det\\Sigma(s)}{\\det\\Sigma (s|x,y)} \\right ) \\right. \\nonumber \\\\\n&& - \\log_2 \\left ( \\frac{\\det\\Sigma(s)}{\\det\\Sigma(s|x)} \\right )  \\nonumber \\\\\n&& \\left. - \\log_2 \\left ( \\frac{\\det\\Sigma(s)}{\\det\\Sigma(s|y)} \\right )\n\\right ] .\n\\end{eqnarray}\n\n\n\\noindent\nThe usage of base 2 in the logarithm functions suggests that net synergy is \ncalculated in the units of `bits'. The first, second and the third term on the right \nhand side of Eq.~(\\ref{eq7}) corresponds to $I(s; x,y)$, $I(s; x)$ and $I(s; y)$,\nrespectively. The definitions of various conditional variances used in \nEq.~(\\ref{eq7}) are\n\\begin{eqnarray}\n\\label{eq8}\n\\Sigma(s|x) & =: & \\Sigma(s)-\\Sigma(s,x)(\\Sigma(x))^{-1}\\Sigma(x,s), \\\\\n\\label{eq9}\n\\Sigma(s|y) & =: & \\Sigma(s)-\\Sigma(s,y)(\\Sigma(y))^{-1}\\Sigma(y,s), \\\\\n\\label{eq10}\n\\Sigma(s|x,y) & =: & \\Sigma(s) \n- \\left( \\begin{array}{ccc} \\Sigma(s,x) & \\Sigma(s,y) \\end{array} \\right) \\nonumber \\\\\n&& \\times\n\\left( \\begin{array}{ccc} \\Sigma(x)& \\Sigma(x,y) \\\\ \n\\Sigma(y,x)&\\Sigma(y) \\end{array} \\right)^{-1} \n\\left( \\begin{array}{ccc} \\Sigma(s,x) \\\\ \n\\Sigma(s,y) \\end{array} \\right). \\nonumber \\\\\n\\end{eqnarray}\n\n\n\\noindent \nThe expression of $\\Sigma(s|x,y)$ after completing the matrix multiplication \nyields\n\\begin{eqnarray}\n\\label{eq11}\n\\Sigma(s|x,y) & = &\\Sigma(s) - (1/{\\cal D})\n[ \\Sigma(y)(\\Sigma(s,x))^2 \\nonumber \\\\\n&& - 2\\Sigma(s,x)\\Sigma(s,y)\\Sigma(x,y) \\nonumber \\\\\n&& + \\Sigma(x)(\\Sigma(s,y))^2 ],\n\\end{eqnarray}\n\n\\noindent\nwith ${\\cal D}=:\\Sigma(x)\\Sigma(y)-(\\Sigma(x,y))^2$. \n\n\n\n\n\n\n\n\n\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.80\\columnwidth,angle=-90]{tsc-net-syn-fig2.eps}\n\n\\end{center}\n\\caption{$I(s; x,y)$, $I(s; x)$ and $I(s; y)$ profiles as functions of $\\mu_s$.\nThe solid lines are theoretical results and the symbols are \ngenerated from stochastic simulation \\cite{Gillespie1977}.\nThe simulation results are average of $10^5$ trajectories.\nThe parameters used are same as in Fig.~\\ref{fig1}.\n}\n\\label{fig2}\n\\end{figure}\n\n\nIn Fig.~\\ref{fig1}, we show the profile of net synergy $\\Delta I(s; x,y)$ as a \nfunction of input relaxation rate constant $\\mu_s$, where the solid line is \ndue to Eq.~(\\ref{eq7}) and the symbols are generated from stochastic \nsimulation \\cite{Gillespie1977}. The net synergy profile grows as $\\mu_s$ \nis increased and moves towards $\\Delta I = 0$.\n\nAs mentioned earlier, we are only interested in the difference between \nsynergy and redundancy since their individual values can not be determined \nwithin the purview of PID adopted in the present work.  If at all one tries \nto infer synergy from the net synergy, one may consider the net synergy \nas redundant synergy of some kind \\cite{Williams2010}. \n\nTo understand the nature of the net synergy we look at the profiles of its \nthree ingredients, viz., $I(s;x,y)$, $I(s;x)$ and $I(s;y)$ as functions of \n$\\mu_s$. In Fig.~\\ref{fig2}, $I(s;x,y)$ and $I(s;x)$ are nearly equal while \n$I(s;y)$ assumes a lower value compared to the other two expressions \nof MI. This result suggests that in the TSC motif the relevant information \nis lost and is never regained while getting transduced from the source to\nthe output. This loss cannot be undone or compensated for by any kind \nof manipulation in the signal transduction pathway. From the information\ntheoretic point of view, this is a consequence of data processing inequality\n(DPI) \\cite{Cover1991}. Within the framework of Markov chain property\nwe have $I(s;x,y) = I(s;x)$ and $I(s;x) \\geqslant I(s;y)$ where the inequality\nexpression arises due to DPI.  From Fig.~\\ref{fig2} it \nis clear that $I(s;x,y) \\approx I(s;x)$ and $I(s;x) > I(s;y)$. Recalling the \nexpression of the net synergy, we notice that the first two terms ($I(s;x,y)$ \nand $I(s;x)$) nearly cancel each other and we are left with \n$\\Delta I (s;x,y) \\approx - I(s;y)$ that generates the net synergy profile shown \nin Fig.~\\ref{fig1}.\n\n\n\n\n\n\nTo summarize, we have investigated how different constituents of a \ngeneric TSC motif are related to each other in information theoretic \nsense. To investigate such mutual dependencies, we explored the \nconcept of net synergy, an essential information theoretic measure \ndue to the formalism of partial information decomposition. The two \nvariable and three variable mutual information quantities have been \ncomputed in terms of variance and covariance of the Gaussian random \nvariables representing the components of the TSC motif. In this study, \nwe have tuned the signal by changing the degradation rate of the \nsource ($\\mu_s$) and have quantified the three MI-s and the net synergy. \nOur result shows that $I(s;x,y)$ and $I(s;x)$ are nearly equal. As a \nconseqeunce Markov chain proerty, the net synergy $\\Delta I(s;x,y)$ \npicks up contribution mostly from $I(s;y)$, thus showing redundancy.\n\n\n\n\n\nWe thank Alok Kumar Maity for fruitful discussion.\nFinancial support from Institutional Programme VI - Development of Systems \nBiology, Bose Institute, Kolkata is thankfully acknowledged.\n\n\n\n\n\n\\begin{thebibliography}{99}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\\expandafter\\ifx\\csname bibnamefont\\endcsname\\relax\n  \\def\\bibnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname bibfnamefont\\endcsname\\relax\n  \\def\\bibfnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname citenamefont\\endcsname\\relax\n  \\def\\citenamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname url\\endcsname\\relax\n  \\def\\url#1{\\texttt{#1}}\\fi\n\\expandafter\\ifx\\csname urlprefix\\endcsname\\relax\\def\\urlprefix{URL }\\fi\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\eprint}[2][]{\\url{#2}}\n\n\\bibitem[{\\citenamefont{Alon}(2006)}]{Alon2006}\n\\bibinfo{author}{\\bibfnamefont{U.}~\\bibnamefont{Alon}},\n  \\emph{\\bibinfo{title}{An Introduction to Systems Biology: Design Principles\n  of Biological Circuits}} (\\bibinfo{publisher}{CRC Press, Boca Raton},\n  \\bibinfo{year}{2006}).\n\n\\bibitem[{\\citenamefont{Alon}(2007)}]{Alon2007}\n\\bibinfo{author}{\\bibfnamefont{U.}~\\bibnamefont{Alon}}, \\bibinfo{journal}{Nat.\n  Rev. Genet.} \\textbf{\\bibinfo{volume}{8}}, \\bibinfo{pages}{450}\n  (\\bibinfo{year}{2007}).\n\n\\bibitem[{\\citenamefont{Shannon}(1948)}]{Shannon1948}\n\\bibinfo{author}{\\bibfnamefont{C.~E.} \\bibnamefont{Shannon}},\n  \\bibinfo{journal}{Bell. Syst. Tech. J} \\textbf{\\bibinfo{volume}{27}},\n  \\bibinfo{pages}{379} (\\bibinfo{year}{1948}).\n\n\\bibitem[{\\citenamefont{Cover and Thomas}(1991)}]{Cover1991}\n\\bibinfo{author}{\\bibfnamefont{T.~M.} \\bibnamefont{Cover}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.~A.} \\bibnamefont{Thomas}},\n  \\emph{\\bibinfo{title}{{E}lements of {I}nformation {T}heory}}\n  (\\bibinfo{publisher}{Wiley-Interscience, New York}, \\bibinfo{year}{1991}).\n\n\\bibitem[{\\citenamefont{Borst and Theunissen}(1999)}]{Borst1999}\n\\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Borst}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{F.~E.} \\bibnamefont{Theunissen}},\n  \\bibinfo{journal}{Nat. Neurosci.} \\textbf{\\bibinfo{volume}{2}},\n  \\bibinfo{pages}{947} (\\bibinfo{year}{1999}).\n\n\\bibitem[{\\citenamefont{Mitra and Stark}(2001)}]{Mitra2001}\n\\bibinfo{author}{\\bibfnamefont{P.~P.} \\bibnamefont{Mitra}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.~B.} \\bibnamefont{Stark}},\n  \\bibinfo{journal}{Nature} \\textbf{\\bibinfo{volume}{411}},\n  \\bibinfo{pages}{1027} (\\bibinfo{year}{2001}).\n\n\\bibitem[{\\citenamefont{Ziv et~al.}(2007)\\citenamefont{Ziv, Nemenman, and\n  Wiggins}}]{Ziv2007}\n\\bibinfo{author}{\\bibfnamefont{E.}~\\bibnamefont{Ziv}},\n  \\bibinfo{author}{\\bibfnamefont{I.}~\\bibnamefont{Nemenman}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{C.~H.} \\bibnamefont{Wiggins}},\n  \\bibinfo{journal}{PLoS ONE} \\textbf{\\bibinfo{volume}{2}},\n  \\bibinfo{pages}{e1077} (\\bibinfo{year}{2007}).\n\n\\bibitem[{\\citenamefont{Tostevin and ten Wolde}(2010)}]{Tostevin2010}\n\\bibinfo{author}{\\bibfnamefont{F.}~\\bibnamefont{Tostevin}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{P.~R.} \\bibnamefont{ten Wolde}},\n  \\bibinfo{journal}{Phys. Rev. E} \\textbf{\\bibinfo{volume}{81}},\n  \\bibinfo{pages}{061917} (\\bibinfo{year}{2010}).\n\n\\bibitem[{\\citenamefont{Cheong et~al.}(2011)\\citenamefont{Cheong, Rhee, Wang,\n  Nemenman, and Levchenko}}]{Cheong2011}\n\\bibinfo{author}{\\bibfnamefont{R.}~\\bibnamefont{Cheong}},\n  \\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Rhee}},\n  \\bibinfo{author}{\\bibfnamefont{C.~J.} \\bibnamefont{Wang}},\n  \\bibinfo{author}{\\bibfnamefont{I.}~\\bibnamefont{Nemenman}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Levchenko}},\n  \\bibinfo{journal}{Science} \\textbf{\\bibinfo{volume}{334}},\n  \\bibinfo{pages}{354} (\\bibinfo{year}{2011}).\n\n\\bibitem[{\\citenamefont{Tka{\\v c}ik and Walczak}(2011)}]{Tkacik2011}\n\\bibinfo{author}{\\bibfnamefont{G.}~\\bibnamefont{Tka{\\v c}ik}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{A.~M.} \\bibnamefont{Walczak}},\n  \\bibinfo{journal}{J. Phys. Condens. Matter} \\textbf{\\bibinfo{volume}{23}},\n  \\bibinfo{pages}{153102} (\\bibinfo{year}{2011}).\n\n\\bibitem[{\\citenamefont{Bowsher et~al.}(2013)\\citenamefont{Bowsher, Voliotis,\n  and Swain}}]{Bowsher2013}\n\\bibinfo{author}{\\bibfnamefont{C.~G.} \\bibnamefont{Bowsher}},\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{Voliotis}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{P.~S.} \\bibnamefont{Swain}},\n  \\bibinfo{journal}{PLoS Comput. Biol.} \\textbf{\\bibinfo{volume}{9}},\n  \\bibinfo{pages}{e1002965} (\\bibinfo{year}{2013}).\n\n\\bibitem[{\\citenamefont{Maity et~al.}(2015{\\natexlab{a}})\\citenamefont{Maity,\n  Chaudhury, and Banik}}]{Maity2015}\n\\bibinfo{author}{\\bibfnamefont{A.~K.} \\bibnamefont{Maity}},\n  \\bibinfo{author}{\\bibfnamefont{P.}~\\bibnamefont{Chaudhury}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{S.~K.} \\bibnamefont{Banik}},\n  \\bibinfo{journal}{PLoS ONE} \\textbf{\\bibinfo{volume}{10}},\n  \\bibinfo{pages}{e0123242} (\\bibinfo{year}{2015}{\\natexlab{a}}).\n\n\\bibitem[{\\citenamefont{Mc~Mahon et~al.}(2014)\\citenamefont{Mc~Mahon, Sim,\n  Filippi, Johnson, Liepe, Smith, and Stumpf}}]{Mahon2014}\n\\bibinfo{author}{\\bibfnamefont{S.~S.} \\bibnamefont{Mc~Mahon}},\n  \\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Sim}},\n  \\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{Filippi}},\n  \\bibinfo{author}{\\bibfnamefont{R.}~\\bibnamefont{Johnson}},\n  \\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Liepe}},\n  \\bibinfo{author}{\\bibfnamefont{D.}~\\bibnamefont{Smith}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.~P.} \\bibnamefont{Stumpf}},\n  \\bibinfo{journal}{Semin. Cell Dev. Biol.} \\textbf{\\bibinfo{volume}{35}},\n  \\bibinfo{pages}{98} (\\bibinfo{year}{2014}).\n\n\\bibitem[{\\citenamefont{Williams and Beer}(2010)}]{Williams2010}\n\\bibinfo{author}{\\bibfnamefont{P.~L.} \\bibnamefont{Williams}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{R.~D.} \\bibnamefont{Beer}}\n  (\\bibinfo{year}{2010}), \\eprint{arXiv: cs.IT/1004.2515}.\n\n\\bibitem[{\\citenamefont{Barrett}(2015)}]{Barrett2015}\n\\bibinfo{author}{\\bibfnamefont{A.~B.} \\bibnamefont{Barrett}},\n  \\bibinfo{journal}{Phys. Rev. E} \\textbf{\\bibinfo{volume}{91}},\n  \\bibinfo{pages}{052802} (\\bibinfo{year}{2015}).\n\n\\bibitem[{\\citenamefont{Schneidman et~al.}(2003)\\citenamefont{Schneidman,\n  Bialek, and Berry}}]{Schneidman2003}\n\\bibinfo{author}{\\bibfnamefont{E.}~\\bibnamefont{Schneidman}},\n  \\bibinfo{author}{\\bibfnamefont{W.}~\\bibnamefont{Bialek}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.~J.} \\bibnamefont{Berry}},\n  \\bibinfo{journal}{J. Neurosci.} \\textbf{\\bibinfo{volume}{23}},\n  \\bibinfo{pages}{11539} (\\bibinfo{year}{2003}).\n\n\\bibitem[{\\citenamefont{Maity et~al.}(2015{\\natexlab{b}})\\citenamefont{Maity,\n  Chaudhury, and Banik}}]{Maity2016}\n\\bibinfo{author}{\\bibfnamefont{A.~K.} \\bibnamefont{Maity}},\n  \\bibinfo{author}{\\bibfnamefont{P.}~\\bibnamefont{Chaudhury}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{S.~K.} \\bibnamefont{Banik}}\n  (\\bibinfo{year}{2015}{\\natexlab{b}}), \\eprint{arXiv: q-bio.MN/1510.04799}.\n\n\\bibitem[{\\citenamefont{de~Ronde et~al.}(2010)\\citenamefont{de~Ronde, Tostevin,\n  and ten Wolde}}]{Ronde2010}\n\\bibinfo{author}{\\bibfnamefont{W.~H.} \\bibnamefont{de~Ronde}},\n  \\bibinfo{author}{\\bibfnamefont{F.}~\\bibnamefont{Tostevin}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{P.~R.} \\bibnamefont{ten Wolde}},\n  \\bibinfo{journal}{Phys. Rev. E} \\textbf{\\bibinfo{volume}{82}},\n  \\bibinfo{pages}{031914} (\\bibinfo{year}{2010}).\n\n\\bibitem[{\\citenamefont{T{\\u a}nase-Nicola et~al.}(2006)\\citenamefont{T{\\u\n  a}nase-Nicola, Warren, and ten Wolde}}]{Tanase2006}\n\\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{T{\\u a}nase-Nicola}},\n  \\bibinfo{author}{\\bibfnamefont{P.~B.} \\bibnamefont{Warren}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{P.~R.} \\bibnamefont{ten\n  Wolde}}, \\bibinfo{journal}{Phys. Rev. Lett.} \\textbf{\\bibinfo{volume}{97}},\n  \\bibinfo{pages}{068102} (\\bibinfo{year}{2006}).\n\n\\bibitem[{\\citenamefont{Bruggeman et~al.}(2009)\\citenamefont{Bruggeman,\n  Bl{\\\"u}thgen, and Westerhoff}}]{Bruggeman2009}\n\\bibinfo{author}{\\bibfnamefont{F.~J.} \\bibnamefont{Bruggeman}},\n  \\bibinfo{author}{\\bibfnamefont{N.}~\\bibnamefont{Bl{\\\"u}thgen}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{H.~V.}\n  \\bibnamefont{Westerhoff}}, \\bibinfo{journal}{PLoS Comput. Biol.}\n  \\textbf{\\bibinfo{volume}{5}}, \\bibinfo{pages}{e1000506}\n  (\\bibinfo{year}{2009}).\n\n\\bibitem[{\\citenamefont{Maity et~al.}(2014)\\citenamefont{Maity, Bandyopadhyay,\n  Chaudhury, and Banik}}]{Maity2014}\n\\bibinfo{author}{\\bibfnamefont{A.~K.} \\bibnamefont{Maity}},\n  \\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Bandyopadhyay}},\n  \\bibinfo{author}{\\bibfnamefont{P.}~\\bibnamefont{Chaudhury}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{S.~K.} \\bibnamefont{Banik}},\n  \\bibinfo{journal}{Phys. Rev. E} \\textbf{\\bibinfo{volume}{89}},\n  \\bibinfo{pages}{032713} (\\bibinfo{year}{2014}).\n\n\\bibitem[{\\citenamefont{Grima}(2015)}]{Grima2015}\n\\bibinfo{author}{\\bibfnamefont{R.}~\\bibnamefont{Grima}},\n  \\bibinfo{journal}{Phys. Rev. E} \\textbf{\\bibinfo{volume}{92}},\n  \\bibinfo{pages}{042124} (\\bibinfo{year}{2015}).\n\n\\bibitem[{\\citenamefont{van Kampen}(2007)}]{Kampen2007}\n\\bibinfo{author}{\\bibfnamefont{N.~G.} \\bibnamefont{van Kampen}},\n  \\emph{\\bibinfo{title}{Stochastic Processes in Physics and Chemistry, 3rd\n  ed.}} (\\bibinfo{publisher}{North-Holland, Amsterdam}, \\bibinfo{year}{2007}).\n\n\\bibitem[{\\citenamefont{Warren et~al.}(2006)\\citenamefont{Warren, T{\\u\n  a}nase-Nicola, and ten Wolde}}]{Warren2006}\n\\bibinfo{author}{\\bibfnamefont{P.~B.} \\bibnamefont{Warren}},\n  \\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{T{\\u a}nase-Nicola}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{P.~R.} \\bibnamefont{ten\n  Wolde}}, \\bibinfo{journal}{J. Chem. Phys.} \\textbf{\\bibinfo{volume}{125}},\n  \\bibinfo{pages}{144904} (\\bibinfo{year}{2006}).\n\n\\bibitem[{\\citenamefont{Keizer}(1987)}]{Keizer1987}\n\\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Keizer}},\n  \\emph{\\bibinfo{title}{Statistical Thermodynamics of Nonequilibrium\n  Processes}} (\\bibinfo{publisher}{Springer-Verlag, Berlin},\n  \\bibinfo{year}{1987}).\n\n\\bibitem[{\\citenamefont{Elf and Ehrenberg}(2003)}]{Elf2003}\n\\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Elf}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{Ehrenberg}},\n  \\bibinfo{journal}{Genome Res.} \\textbf{\\bibinfo{volume}{13}},\n  \\bibinfo{pages}{2475} (\\bibinfo{year}{2003}).\n\n\\bibitem[{\\citenamefont{Paulsson}(2004)}]{Paulsson2004}\n\\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Paulsson}},\n  \\bibinfo{journal}{Nature} \\textbf{\\bibinfo{volume}{427}},\n  \\bibinfo{pages}{415} (\\bibinfo{year}{2004}).\n\n\\bibitem[{\\citenamefont{Paulsson}(2005)}]{Paulsson2005}\n\\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Paulsson}},\n  \\bibinfo{journal}{Phys Life Rev} \\textbf{\\bibinfo{volume}{2}},\n  \\bibinfo{pages}{157} (\\bibinfo{year}{2005}).\n\n\\bibitem[{\\citenamefont{Gillespie}(1977)}]{Gillespie1977}\n\\bibinfo{author}{\\bibfnamefont{D.~T.} \\bibnamefont{Gillespie}},\n  \\bibinfo{journal}{J. Phys. Chem.} \\textbf{\\bibinfo{volume}{81}},\n  \\bibinfo{pages}{2340} (\\bibinfo{year}{1977}).\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 9233, "prevtext": "\n\n\n\\noindent where\n\\begin{eqnarray*}\n\\delta \\mathbf{A} = \\left (\n\\begin{array}{c}\n\\delta s \\\\\n\\delta x \\\\\n\\delta y\n\\end{array} \n\\right ),\n\\mathbf{J} = \\left (\n\\begin{array}{ccc}\n-\\mu_s & 0 & 0 \\\\\nk_x & -\\mu_x & 0 \\\\\n0 & k_y & -\\mu_y\n\\end{array} \n\\right ),\n\\mathbf{\\Theta} = \\left (\n\\begin{array}{c}\n\\xi_s \\\\\n\\xi_x \\\\\n\\xi_y\n\\end{array} \n\\right ).\n\\end{eqnarray*}\n\n\n\\noindent\nHere $\\delta \\mathbf{A}$ and $\\mathbf{\\Theta}$ are the fluctuations matrix \nand the noise matrix, respectively. $\\mathbf{J}$ is the Jacobian matrix \nevaluated at steady state. To calculate the variance and covariance of \ndifferent species of the TSC motif, we use the Lyapunov equation at\nsteady state\n\\cite{Keizer1987,Elf2003,Paulsson2004,Paulsson2005}\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq6}\n\\mathbf{J} \\mathbf{\\Sigma} + \\mathbf{\\Sigma} \\mathbf{J}^T \n+ \\mathbf{D} = 0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{J}\\mathbf{\\Sigma}+\\mathbf{\\Sigma}\\mathbf{J}^{T}+\\mathbf{D}=0.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>\ud835\udc09</mi><mo>\u2062</mo><mi>\ud835\udeba</mi></mrow><mo>+</mo><mrow><mi>\ud835\udeba</mi><mo>\u2062</mo><msup><mi>\ud835\udc09</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>\ud835\udc03</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}]