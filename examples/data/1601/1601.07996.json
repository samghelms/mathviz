[{"file": "1601.07996.tex", "nexttext": "\nwhere $SC$ is a function that measures the utility of feature ${\\bf {f}}$, ${\\bf {\\hat{f}}}'$ and ${\\bf {\\hat{S}}}$ are the normalized feature and refined affinity matrix obtained from ${\\bf {f}}$ and ${\\bf {S}}$, respectively. The maximization problem in Eq.~(\\ref{eq:similarityFramework}) shows that we would select a subset of features from $\\mathcal{F}$ such that they can well preserve the data similarity structures defined in ${\\bf {\\hat{S}}}$. This problem is usually solved by greedily selecting the top $k$ features that maximize their individual utility ${\\bf {\\hat{f}}}'{\\bf {S}}{\\bf {\\hat{f}}}$. Methods in this category vary in the way the similarity matrix ${\\bf {S}}$ is designed. We subsequently discuss about the original formulations of some representative algorithms in this group and then introduce how they can be reformulated under the unified framework.\n\n\\subsubsection{Laplacian Score~\\citep{he2005laplacian} (Unsupervised)}\n\nLaplacian Score is an unsupervised feature selection algorithm which selects features that can best preserve the data manifold structure. It consists of three phases.\nFirst, it construct a nearest neighbor graph $\\mathcal{G}$ with $n$ nodes where the $i$-th node corresponds to $x_{i}$. If $x_{i}$ is among the $p$ nearest neighbors of $x_{j}$ or $x_{j}$ is among the $p$ nearest neighbors of $x_{i}$, nodes $i$ and $j$ are connected in $\\mathcal{G}$ ($p$ is a predefined number). Second, if nodes $i$ and $j$ are connected, the entry in the affinity matrix ${\\bf {S}}_{ij}$ is ${\\bf {S}}(i,j)=e^{-\\frac{||{\\bf {x}}_{i}-{\\bf {x}}_{j}||^{2}}{t}}$, where $t$ is a constant, otherwise ${\\bf {S}}(i,j)=0$. The diagonal matrix ${\\bf {D}}$ is defined as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$ and the laplacian matrix ${\\bf {L}}$ is ${\\bf {L}}={\\bf {D}}-{\\bf {S}}$~\\citep{chung1997spectral}. Lastly, the Laplacian Score of each feature $f_{i}$ is computed as:\n\n", "itemtype": "equation", "pos": 30775, "prevtext": "\n\n\\title{Feature Selection: A Data Perspective}\n\n\\author{\\name Jundong Li \\email jundongl@asu.edu \\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n       \\AND\n       \\name Kewei Cheng \\email kcheng18@asu.edu \\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n       \\AND\n       \\name Suhang Wang \\email swang187@asu.edu \\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n       \\AND\n       \\name Fred Morstatter \\email fmorstat@asu.edu\\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n       \\AND\n       \\name Robert P. Trevino \\email rptrevin@asu.edu\\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n       \\AND\n       \\name Jiliang Tang \\email jlt@yahoo-inc.com\\\\\n       \\addr Yahoo! Labs, Sunnyvale, CA, 94085, USA\n       \\AND\n       \\name Huan Liu \\email huanliu@asu.edu\\\\\n       \\addr Arizona State University, Tempe, AZ 85281, USA\n}\n\n\\editor{}\n\n\\maketitle\n\n\\begin{abstract}\nFeature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (\\url{http://featureselection.asu.edu/scikit-feast/}). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research.\n\\end{abstract}\n\n\\begin{keywords}\nFeature Selection\n\\end{keywords}\n\n\\section{Introduction}\n\nWe are now in the era of big data, where massive amounts of high dimensional data has become ubiquitous in our daily life, such as social media, e-commerce, health care, bioinformatics, transportation, online education, etc. Figure~(\\ref{fig:UCI}) shows an example by plotting the growth trend of UCI machine learning repository~\\citep{bache2013uci}. Rapid growth of data presents challenges for effective and efficient data management. Therefore, it is desirable and of great importance to apply data mining and machine learning techniques to automatically discover knowledge from these data.\n\\begin{figure}[!htbp]\n\\centering\n\\begin{minipage}{0.45\\textwidth}\n\\centering\n\\subfigure[attribute number growth]\n{\\includegraphics[width=\\textwidth]{NoFeatGrowth.eps}}\n\\end{minipage}\n\\begin{minipage}{0.45\\textwidth}\n\\centering\n\\subfigure[sample size growth]\n{\\includegraphics[width=\\textwidth]{SampleSizeGrowth.eps}}\n\\end{minipage}\n\\centering\n\\caption{Number of samples and number of features growth trend during the past thirty years in UCI machine learning repository.}\n\\label{fig:UCI}\n\\end{figure}\n\nWhen applying data mining and machine learning algorithms on high dimensional data, a critical issue is known as curse of dimensionality~\\citep{hastie2005elements}. It refers to the phenomenon that data becomes sparser in high dimensional space, adversely affecting algorithms designed for low dimensional space. In addition, with the existence of a large number of features, learning models tend to overfit which may cause performance degradation on unseen data. Moreover, data of high dimension significantly increases the memory storage requirements and computational costs for data analytics.\n\nDimensionality reduction is one of the most powerful tools to address the previously described issues. It can be categorized mainly into into two main components: feature extraction and feature selection. Feature extraction projects original high dimensional feature space to a new feature space with low dimensionality. The new constructed feature space is usually a linear or nonlinear combination of the original feature space. Examples of feature extraction methods include Principle Component Analysis (PCA)~\\citep{jolliffe2002principal}, Linear Discriminant Analysis (LDA)~\\citep{scholkopft1999fisher}, Canonical Correlation Analysis (CCA)~\\citep{hardoon2004canonical}, Singular Value Decomposition~\\citep{golub2012matrix}, ISOMAP~\\citep{tenenbaum2000global} and Locally Linear Embedding (LLE)~\\citep{roweis2000nonlinear}. Feature selection, on the other hand, directly selects a subset of relevant features for the use model construction. Lasso~\\citep{tibshirani1996regression}, Information Gain~\\citep{cover2012elements}, Relief~\\citep{kira1992feature}, MRMR~\\citep{peng2005feature}, Fisher Score~\\citep{duda2012pattern}, Laplacian Score~\\citep{he2005laplacian}, and SPEC~\\citep{zhao2007spectral} are some of the well known feature selection techniques.\n\nBoth feature extraction and feature selection have the advantage of improving learning performance, increasing computational efficiency, decreasing memory storage requirements, and building better generalization models. However, since feature extraction builds a set of new features, further analysis is problematic as we cannot get the physical meaning of these features in the transformed space. In contrast, by keeping some original features, feature selection maintains physical meanings of original features, and gives models better readability and interpretability. Therefore, feature selection is often preferred in many real-world applications such as text mining and genetic analysis compared to feature extraction.\n\nReal-world data is usually imperfect, containing some irrelevant and redundant features. Removing these features by feature selection reduces storage and computational cost while avoiding significant loss of information or negative degradation of learning performance. For example, in Figure~(\\ref{fig:featureIllustration-a}), feature $f_{1}$ is a relevant feature which is able to discriminate two classes (clusters). However, given feature $f_{1}$, feature $f_{2}$ in Figure~(\\ref{fig:featureIllustration-b}) is redundant as $f_{2}$ is strongly correlated with $f_{1}$. In Figure~(\\ref{fig:featureIllustration-c}), feature $f_{3}$ is an irrelevant feature as it cannot separate two classes (clusters) at all. Therefore, the removal of $f_{2}$ and $f_{3}$ will not negatively impact the learning performance.\n\n\\begin{figure}[!htbp]\n\\centering\n\\begin{minipage}{0.32\\textwidth}\n\\centering\n\\subfigure[relevant feature $f_{1}$\\label{fig:featureIllustration-a}]\n{\\includegraphics[width=\\textwidth]{Relevant.eps}}\n\\end{minipage}\n\\begin{minipage}{0.32\\textwidth}\n\\centering\n\\subfigure[redundant feature $f_{2}$\\label{fig:featureIllustration-b}]\n{\\includegraphics[width=\\textwidth]{Redundant.eps}}\n\\end{minipage}\n\\begin{minipage}{0.32\\textwidth}\n\\centering\n\\subfigure[irrelevant feature $f_{3}$\\label{fig:featureIllustration-c}]\n{\\includegraphics[width=\\textwidth]{Irrelevant.eps}}\n\\end{minipage}\n\\centering\n\\caption{A toy example of relevant, irrelevant and redundant features. In Figure~(\\ref{fig:featureIllustration-a}), feature $f_{1}$ is a relevant feature which can discriminate two classes (clusters), i.e., the blue points and the red points. In Figure~(\\ref{fig:featureIllustration-b}) and Figure~(\\ref{fig:featureIllustration-c}), feature $f_{2}$ and feature $f_{3}$ are irrelevant and redundant w.r.t. feature $f_{1}$, respectively.}\n\\label{fig:featureIllustration}\n\\end{figure}\n\nIn the following subsections, we first review traditional categorizations of feature selection algorithms from the availability of labels and from the search strategy perspectives in Section 1.1. In Section 1.2, we revisit feature selection from a data perspective motivated by challenges and opportunities from big data. Meanwhile, we discuss the necessity for a comprehensive and structured overview of current advances on feature selection, and our efforts to build a open source machine learning repository to cover state-of-the-art feature selection algorithms. In Section 1.3, we give an outline and organization of the survey.\n\n\\subsection{Traditional Categorizations of Feature Selection Algorithms}\n\n\\subsubsection{Label Perspective}\nAccording to the availability of label information, feature selection algorithms can be broadly classified as supervised, unsupervised and semi-supervised methods.\n\n\\paragraph{Supervised Feature Selection}\\mbox{}\\\\\nSupervised feature selection is generally designed for classification or regression problems. It aims to select a subset of features that are able to discriminate samples from different classes. With the existence of class labels, the feature relevance is usually assessed via its correlation with class labels. A general framework of supervised feature selection is illustrated in Figure~(\\ref{fig:ClassificationFramework}). The training phase of the classification highly depends on feature selection. After splitting the data into training and testing sets, classifiers are trained based on a subset of features selected by supervised feature selection. Note that the feature selection phase can either be independent of the learning algorithm (filter methods), or it may iteratively take advantage of the learning performance of a classifier to assess the quality of selected features so far (wrapper methods). Finally, the trained classifier predicts class labels of samples in the test set on the selected features.\n\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.9\\textwidth]{ClassificationFramework.eps}\n      \\caption{A General Framework of Supervised Feature Selection.}\n\\label{fig:ClassificationFramework}\n\\end{figure}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.9\\textwidth]{ClusteringFramework.eps}\n      \\caption{A General Framework of Unsupervised Feature Selection.}\n\\label{fig:ClusteringFramework}\n\\end{figure}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.9\\textwidth]{SemiSupervisedFramework.eps}\n      \\caption{A General Framework of Semi-Supervised Feature Selection.}\n\\label{fig:SemiSupervisedFramework}\n\\end{figure}\n\n\\paragraph{Unsupervised Feature Selection}\\mbox{}\\\\\nUnsupervised feature selection is generally designed for clustering problems. Since acquiring labeled data is particularly expensive in both time and effort, unsupervised feature selection on unlabeled data has recently gained considerable attention recently. Due to the lack of label information to evaluate the importance of features, unsupervised feature selection methods seek alternative criteria to define the relevance of features such as data similarity and local discriminative information. A general framework of unsupervised feature selection is illustrated in Figure~(\\ref{fig:ClusteringFramework}). Different from supervised feature selection, unsupervised feature selection usually uses all instances are available in the feature selection phase. The feature selection phase is either be independent of the unsupervised learning algorithms (filter methods), or it relies on the learning algorithms to iteratively improve the quality of selected features (wrapper methods). After the feature selection phase, it outputs the cluster structure of all data samples on the selected features by using a typical clustering algorithm.\n\n\\paragraph{Semi-Supervised Feature Selection}\\mbox{}\\\\\nSupervised feature selection works when sufficient label information is available while unsupervised feature selection algorithms do not require any label information. However, in many real-world applications,  we usually have a small number of labeled samples and a large number of unlabeled samples. Both supervised and unsupervised feature selection algorithms cannot fully take advantage of all samples in this scenario. For supervised methods, the small number of labeled samples may be insufficient to provide correlation information of features; while unsupervised methods totally ignore class labels which could provide useful information to discriminate different classes. Therefore, it is desirable to develop semi-supervised methods by exploiting both labeled and unlabeled samples. We provide a general framework of semi-supervised feature selection in Figure~(\\ref{fig:SemiSupervisedFramework}), it is similar to the framework of supervised feature selection except that in semi-supervised methods only partial label information is available.\n\n\\subsubsection{Search Strategy Perspective}\nWith respect to different selection strategies, feature selection methods can be categorized as wrapper, filter and embedded methods.\n\n\\paragraph{Wrapper Methods}\nWrapper methods rely on the predictive performance of a predefined learning algorithm to evaluate the quality of selected features. Given a specific learning algorithm, a typical wrapper method performs two steps: (1) Searches for a subset of features and (2) evaluate selected features. It repeats (1) and (2) until some stopping criteria are satisfied or the desired learning performance is obtained. The workflow of wrapper methods is illustrated in Figure~(\\ref{fig:WrapperFramework}). It can be observed that the feature set search component first generates a subset of features, then the learning algorithm acts as a black box to evaluate the quality of these features based on the learning performance. The whole process works iteratively until the highest learning performance is achieved. The feature subset that gives the highest learning performance is output as the selected features. Unfortunately, a known issue of wrapper methods is that the search space for $d$ features is $2^d$, which makes the exhaustive search impractical when $d$ is large. Therefore, many different search strategies such as sequential search~\\citep{guyon2003introduction}, hill-climbing search, best-first search~\\citep{kohavi1997wrappers}, branch-and-bound search~\\citep{narendra1977branch}, genetic algorithms~\\citep{golberg1989genetic} are proposed to yield a local optimum learning performance. However, the search space is still extremely large for high dimensional datasets. As a result, wrapper methods are seldom used in practice.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.7\\textwidth]{WrapperFramework.eps}\n      \\caption{A General Framework of Wrapper Feature Selection Methods.}\n\\label{fig:WrapperFramework}\n\\end{figure}\n\n\\paragraph{Filter Methods}\nFilter methods are independent of any learning algorithms. They rely on certain characteristics of data to assess the importance of features. Filter methods are typically more efficient than wrapper methods. However, due to the lack of a specific learning algorithm guiding the feature selection phase,  the selected features may not be optimal for the target learning algorithms. A typical filter method consists of two steps. In the first step, feature importance is ranked by a feature score according to some feature evaluation criteria. The feature importance evaluation process can be either univariate or multivariate. In the univariate scheme, each feature is ranked individually regardless of other features, while the multivariate scheme ranks multiple features in a batch way. In the second step of a typical filter method, low ranking features are filtered out and the remaining features are selected. In the past decades, many different evaluation criteria for filter methods have been proposed. Some representative criteria include feature discriminative ability to separate samples~\\citep{kira1992practical,robnik2003theoretical}, feature correlation~\\citep{koller1995toward,guyon2003introduction}, mutual information~\\citep{yu2003feature,peng2005feature}, feature ability to preserve data manifold structure~\\citep{he2005laplacian,gu2011generalized,zhao2007spectral}, and feature ability to reconstruct the original data~\\citep{masaeli2010convex,farahat2011efficient}.\n\n\\paragraph{Embedded Methods}\nFilter methods select features that are independent of any learning algorithms, therefore they are computational efficient. However, they fail to consider the bias of the learning algorithms, and the selected features may not be optimal for the learning tasks. On the contrast, wrapper methods evaluate the importance of features by the given learning algorithms iteratively and can obtain better predictive accuracy for that specific learning algorithm. However, due to the exponential search space, it is computational intractable in many applications when the feature dimension is high. Embedded methods provide a trade-off solution between filter and wrapper methods which embed the feature selection with the model learning, thus they inherit the merits of wrapper and filter methods -- (1) they include the interactions with the learning algorithm; and (2) they are far more efficient than the wrapper methods since they do not need to evaluate feature sets iteratively. The most widely used embedded methods are the regularization models which targets to fit a learning model by minimizing the fitting errors and forcing the feature coefficients to be small (or exact zero) simultaneously. Afterwards, both the regularization model and selected feature sets are output as results.\n\n\\subsection{Feature Selection Algorithms from A Data Perspective}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.95\\textwidth]{Categorization.eps}\n      \\caption{Feature Selection Algorithms from the Data Perspective.}\n\\label{fig:Categorization}\n\\end{figure}\n\nThe recent popularity of big data presents some challenges for the traditional feature selection task. Meanwhile, some characteristics of big data like velocity and variety tend to promote the development of novel feature selection algorithms. Here we briefly present and discuss some major concerns when we apply feature selection algorithms.\n\n\\paragraph{Streaming Data and Features}\nStreaming data and features have become more prevalent in real world applications.  This poses a significant challenge to traditional feature selection algorithms, which assume static datasets with fixed features. For example in Twitter, new data like posts and new features like slang words are continuously being generated. It is impractical to apply traditional batch-mode feature selection algorithms to find relevant features at each round when new data or new feature arrives. In addition, the volume data may sometimes be too large to be loaded into memory directly with a single data scan.  This is especially a problem when a second pass is either unavailable or very expensive. Due to aforementioned reasons, it is more appealing to apply feature selection in a streaming fashion to dynamically maintain a best set of feature from all features and data seen up to that point.\n\n\\paragraph{Heterogeneous Data}\nMost existing algorithms of feature selection are designed to handle tasks with single data source and always assume that the data is independent and identically distributed (\\emph{i.i.d.}). However, multi-source data is quite prevalent in many domains. For example, in social media, data come from heterogeneous sources such as text, images, tags. In addition, linked data is ubiquitous and presents itself in various forms such as user-post relations and user-user relations. The availability of multiple data sources brings unprecedented opportunities as we can leverage shared intrinsic characteristics and correlations to find more relevant features.  However, challenges are also unequivocally presented with additional data sources. For instance, with the existence of link information, the widely adopted \\emph{i.i.d.} assumption in most machine learning algorithms does not hold. How to appropriately utilize link information for feature selection is still a challenging problem.\n\n\\paragraph{Structures Between Features}\nSometimes, features can exhibit certain types of structures in many real-world applications. Some well-known structures among features are group structure, tree structure, graph structure, etc. When performing feature selection, if the feature structure is not taken into consideration, the intrinsic dependencies may not be captured and the selected features may not be suitable for the data. Incorporating the prior knowledge of feature structures can possibly help select relevant features to greatly improve the learning performance.\n\\paragraph{}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.7\\textwidth]{CategorizationTraditional.eps}\n      \\caption{Categorization of Traditional Feature Selection Algorithms According to The Adopted Techniques.}\n\\label{fig:CategorizationTraditional}\n\\end{figure}\n\nThe aforementioned reasons motivate us to investigate feature selection algorithms from a different view to include recent advances and frontiers about feature selection. In this survey, we revisit feature selection algorithms from a data perspective, the categorization is illustrated in Figure~(\\ref{fig:Categorization}). It is shown that data consists of static data and streaming data. First, for static data, it can be further grouped into generic data and heterogeneous data. In generic data, features can either be flat or possess some intrinsic structures. Traditional feature selection algorithms are proposed to deal with these flat features in which features are considered to be independent. The past few decades have witnessed hundreds of feature selection algorithms. Based on their technical characteristics, we propose to classify them into four groups, i.e., similarity based methods, information-theoretic based methods, sparse learning based methods and statistical based methods as shown in Figure~(\\ref{fig:CategorizationTraditional}). It should be noted that this categorization only involves filter methods and embedded methods while the wrapper methods are excluded. The reason for excluding wrapper methods is that they are computationally expensive and, therefore, are seldom used in practice. More details about these four categories will be investigated later. When features express some structures, specific feature selection algorithms for structural features are more desirable. On the other hand, data can be heterogeneous, in many real-world applications we are often encompassed by linked, multi-source or multi-view data, we also show how well-designed feature selection algorithms are designed to deal with these situations. Second, in the streaming settings, data arrive sequentially in a streaming fashion where the size of data instances is unknown, feature selection algorithms that make only one pass over the data is proposed to tackle streaming data. Similarly, in a orthogonal setting, features can also be generated dynamically -- new features are sequentially added and the size of features is even unknown in some cases. Streaming feature selection algorithms are designed to determine if accepting the newly added features and if removing existing but outdated features.\n\nCurrently, there exist a number of feature selection surveys~\\citep{guyon2003introduction,alelyani2013feature,chandrashekar2014survey,tang2014feature}. These surveys either focus on traditional feature selection algorithms or detailed learning task like classification and clustering. However, none of them provide a comprehensive and structured overview of traditional feature selection algorithms in conjunction with recent advances in feature selection from a data perspective. In this survey, we will introduce representative feature selection algorithms to cover all components mentioned in Figure~(\\ref{fig:Categorization}) and Figure~(\\ref{fig:CategorizationTraditional}). We also release a feature selection repository in Python named \\emph{scikit-feast} which is built upon the widely used machine learning package \\emph{scikit-learn}\\footnote{http://scikit-learn.org/stable/} and two scientific computing packages \\emph{Numpy}\\footnote{http://www.numpy.org/} and \\emph{Scipy}\\footnote{http://www.scipy.org/}. It includes more than 40 representative feature selection algorithms. The website of the repository is available at \\url{http://featureselection.asu.edu/scikit-feast/}.\n\n\\subsubsection{Organization of the Survey}\nWe present this survey in five parts and the covered topics are listed as follows:\n\\begin{enumerate}\n  \\item Feature Selection with Generic Data (Section 2)\n  \\begin{enumerate}\n    \\item Similarity based Feature Selection Methods\n    \\item Information Theoretical based Feature Selection Methods\n    \\item Sparse Learning based Feature Selection Methods\n    \\item Statistical based Feature Selection Methods\n  \\end{enumerate}\n  \\item Feature Selection with Structure Features (Section 3)\n  \\begin{enumerate}\n    \\item Feature Selection Algorithms with Group Structure Features\n    \\item Feature Selection Algorithms with Tree Structure Features\n    \\item Feature Selection Algorithms with Graph Structure Features\n  \\end{enumerate}\n  \\item Feature Selection with Heterogeneous Data (Section 4)\n  \\begin{enumerate}\n    \\item Feature Selection Algorithms with Linked Data\n    \\item Feature Selection Algorithms with Multi-Source Data\n    \\item Feature Selection Algorithms with Multi-View Data\n  \\end{enumerate}\n  \\item Feature Selection with Streaming Data (Section 5)\n    \\begin{enumerate}\n    \\item Feature Selection Algorithms with Data Streams\n    \\item Feature Selection Algorithms with Feature Streams\n  \\end{enumerate}\n  \\item Performance Evaluation (Section 6)\n  \\item Open Problems and Challenges (Section 7)\n  \\item Summary of the Survey (Section 8)\n\\end{enumerate}\n\\vskip 0.2in\n\n\\section{Feature Selection on Generic Data}\n\nOver the past two decades, hundreds of feature selection algorithms have been proposed. In this Section, we broadly group traditional feature selection algorithms for generic data into four categories: similarity based, information theoretical based, sparse learning based and statistical based methods according to the techniques they adopt during the feature selection process. In the following subsections, we will briefly review each category with some representative algorithms.\n\nWe summarize some common symbols used throughout this survey in Table~\\ref{table:symbols}. We use bold uppercase characters for matrices (e.g. ${\\bf {A}}$), bold lowercase characters for vectors (e.g. ${\\bf {a}}$), calligraphic fonts for sets (e.g. $\\mathcal{F}$). We follow the matrix settings in Matlab to represent $i$-th row of matrix ${\\bf {A}}$ as ${\\bf {A}}(i,:)$, $j$-th column of ${\\bf {A}}$ as ${\\bf {A}}(:,j)$, $(i,j)$-th entry of ${\\bf {A}}$ as ${\\bf {A}}(i,j)$, transpose of ${\\bf {A}}$ as ${\\bf {A}}'$, and the trace of ${\\bf {A}}$ as $tr({\\bf {A}})$. For any matrix ${\\bf {A}}\\in \\mathbb{R}^{n\\times d}$, its Frobenius norm is defined as $||{\\bf {A}}||_{F}=\\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{d}{\\bf {A}}(i,j)^{2}}$, and its $\\ell_{2,1}$-norm is $||{\\bf {A}}||_{2,1}=\\sum_{i=1}^{n}\\sqrt{\\sum_{j=1}^{d}{\\bf {A}}(i,j)^{2}}$. For any vector ${\\bf {a}}=[a_{1},a_{2},...,a_{n}]'$, its $\\ell_{2}$-norm is defined as $||{\\bf {a}}||_{2}=\\sqrt{\\sum_{i=1}^{n}a_{i}^{2}}$, and its $\\ell_{1}$-norm is $||{\\bf {a}}||_{1}=\\sum_{i=1}^{n}|a_{i}|$. ${\\bf {I}}$ is an identity matrix and ${\\bf {1}}$ is a vector whose elements are all 1.\n\n\\begin{table}[!htbp]\n\\centering\n\\begin{tabular}{|c|c|} \\hline\nNotations& Definitions or Descriptions \\\\ \\hline \\hline\n$n$ & number of instances in the data \\\\ \\hline\n$d$ & number of features in the data\\\\ \\hline\n$k$ & number of selected features \\\\ \\hline\n$c$ & number of classes (if exist) \\\\ \\hline\n$\\mathcal{F}$ & original feature set which contains $d$ features \\\\ \\hline\n$\\mathcal{S}$ & selected feature set which contains $k$ selected features\\\\ \\hline\n$\\{i_{1},i_{2},...,i_{k}\\}$ & index of k selected features in $\\mathcal{S}$ \\\\ \\hline\n$f_{1},f_{2},...,f_{d}$ & $d$ features \\\\ \\hline\n$f_{i_{1}},f_{i_{2}},...,f_{i_{k}}$ & $k$ selected features \\\\ \\hline\n$x_{1},x_{2},...,x_{n}$ & $n$ data instances \\\\ \\hline\n${\\bf {f}}_{1},{\\bf {f}}_{2},...,{\\bf {f}}_{d}$ & $d$ feature vectors corresponding to $f_{1},f_{2},...,f_{d}$ \\\\ \\hline\n${\\bf {f}}_{i_{1}},{\\bf {f}}_{i_{2}},...,{\\bf {f}}_{i_{k}}$ & $k$ feature vectors corresponding to $f_{i_{1}},f_{i_{2}},...,f_{i_{k}}$ \\\\ \\hline\n${\\bf {x}}_{1},{\\bf {x}}_{2},...,{\\bf {x}}_{n}$ & $n$ data vectors corresponding to $x_{1},x_{2},...,x_{n}$ \\\\ \\hline\n$y_{1},y_{2},...,y_{n}$ & class labels of all $n$ instances (if exist) \\\\ \\hline\n${\\bf {X}}\\in \\mathbb{R}^{n\\times d}$ & data matrix with $n$ instances and $d$ features \\\\ \\hline\n${\\bf {X}}_{\\mathcal{F}}\\in \\mathbb{R}^{n\\times k}$ & data matrix on the selected $k$ features \\\\ \\hline\n${\\bf {y}}\\in \\mathbb{R}^{n}$ & class label vector for all $n$ instances (if exist) \\\\ \\hline\n\\end{tabular}\n\\caption{Symbols.}\n\\label{table:symbols}\n\\end{table}\n\n\\subsection{Similarity based Methods}\nDifferent feature selection algorithms exploit various types of criteria to define the relevance of features such as distance, separability, information, correlation, dependency, and reconstruction error. Among them, there is a family of methods assessing the importance of features by its ability to preserve data similarity. We call these kinds of methods to be similarity based feature selection methods. For supervised feature selection, data similarity can be derived from label information; while for unsupervised feature selection methods, most methods take advantage of different distance metric measures to obtain data similarity.\n\nGiven a dataset ${\\bf {X}}\\in \\mathbb{R}^{n\\times d}$ with $n$ instances and $d$ features, the pairwise similarity among instances can be encoded in an affinity matrix ${\\bf {S}}\\in\\mathbb{R}^{n\\times n}$. The affinity matrix ${\\bf {S}}$ is symmetric and its $(i,j)$-th entry indicates the similarity between the $i$-th instance ${\\bf {x}}_{i}$ and the $j$-th instance ${\\bf {x}}_{j}$, the larger the value of ${\\bf {S}}_{i,j}$ is, the more similarity ${\\bf {x}}_{i}$ and ${\\bf {x}}_{j}$ share. Suppose we want to select $k$ most relevant features from $\\mathcal{F}$, then the utility of these $k$ features is maximized as follows:\n\n", "index": 1, "text": "\\begin{equation}\n\\max_{\\mathcal{F}}\\sum_{f\\in\\mathcal{F}}SC(f)=\\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}{\\bf {\\hat{f}}}'{\\bf {\\hat{S}}}{\\bf {\\hat{f}}},\n\\label{eq:similarityFramework}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\mathcal{F}}\\sum_{f\\in\\mathcal{F}}SC(f)=\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}%&#10;\\in\\mathcal{F}}{\\bf{\\hat{f}}}^{\\prime}{\\bf{\\hat{S}}}{\\bf{\\hat{f}}},\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><mi>S</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><msup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nSince Laplacian Score evaluates the importance of each feature individually, the task of selecting the $k$ features can be solved by greedily picking the top $k$ features with the smallest Laplacian Scores.\n\nThe Laplacian Score of each feature can be reformulated as follows:\n\n", "itemtype": "equation", "pos": 32892, "prevtext": "\nwhere $SC$ is a function that measures the utility of feature ${\\bf {f}}$, ${\\bf {\\hat{f}}}'$ and ${\\bf {\\hat{S}}}$ are the normalized feature and refined affinity matrix obtained from ${\\bf {f}}$ and ${\\bf {S}}$, respectively. The maximization problem in Eq.~(\\ref{eq:similarityFramework}) shows that we would select a subset of features from $\\mathcal{F}$ such that they can well preserve the data similarity structures defined in ${\\bf {\\hat{S}}}$. This problem is usually solved by greedily selecting the top $k$ features that maximize their individual utility ${\\bf {\\hat{f}}}'{\\bf {S}}{\\bf {\\hat{f}}}$. Methods in this category vary in the way the similarity matrix ${\\bf {S}}$ is designed. We subsequently discuss about the original formulations of some representative algorithms in this group and then introduce how they can be reformulated under the unified framework.\n\n\\subsubsection{Laplacian Score~\\citep{he2005laplacian} (Unsupervised)}\n\nLaplacian Score is an unsupervised feature selection algorithm which selects features that can best preserve the data manifold structure. It consists of three phases.\nFirst, it construct a nearest neighbor graph $\\mathcal{G}$ with $n$ nodes where the $i$-th node corresponds to $x_{i}$. If $x_{i}$ is among the $p$ nearest neighbors of $x_{j}$ or $x_{j}$ is among the $p$ nearest neighbors of $x_{i}$, nodes $i$ and $j$ are connected in $\\mathcal{G}$ ($p$ is a predefined number). Second, if nodes $i$ and $j$ are connected, the entry in the affinity matrix ${\\bf {S}}_{ij}$ is ${\\bf {S}}(i,j)=e^{-\\frac{||{\\bf {x}}_{i}-{\\bf {x}}_{j}||^{2}}{t}}$, where $t$ is a constant, otherwise ${\\bf {S}}(i,j)=0$. The diagonal matrix ${\\bf {D}}$ is defined as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$ and the laplacian matrix ${\\bf {L}}$ is ${\\bf {L}}={\\bf {D}}-{\\bf {S}}$~\\citep{chung1997spectral}. Lastly, the Laplacian Score of each feature $f_{i}$ is computed as:\n\n", "index": 3, "text": "\\begin{equation}\nlaplacian\\_score(f_{i})=\\frac{\\tilde{{\\bf {f}}}_{i}'{\\bf {L}}\\tilde{{\\bf {f}}}_{i}}{\\tilde{{\\bf {f}}}_{i}'{\\bf {D}}\\tilde{{\\bf {f}}}_{i}}, \\mbox{   where   } \\tilde{{\\bf {f}}_{i}}={\\bf {f}}_{i}-\\frac{{\\bf {f}}_{i}'{\\bf {D}}{\\bf {1}}}{{\\bf {1}}'{\\bf {D}}{\\bf {1}}}{\\bf {1}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"laplacian\\_score(f_{i})=\\frac{\\tilde{{\\bf{f}}}_{i}^{\\prime}{\\bf{L}}\\tilde{{\\bf%&#10;{f}}}_{i}}{\\tilde{{\\bf{f}}}_{i}^{\\prime}{\\bf{D}}\\tilde{{\\bf{f}}}_{i}},\\mbox{ %&#10;where }\\tilde{{\\bf{f}}_{i}}={\\bf{f}}_{i}-\\frac{{\\bf{f}}_{i}^{\\prime}{\\bf{D}}{%&#10;\\bf{1}}}{{\\bf{1}}^{\\prime}{\\bf{D}}{\\bf{1}}}{\\bf{1}}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc0b</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc03</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow></mfrac></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0where\u00a0</mtext><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover></mrow><mo>=</mo><mrow><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo>-</mo><mrow><mfrac><mrow><msubsup><mi>\ud835\udc1f</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc03\ud835\udfcf</mi></mrow><mrow><msup><mn>\ud835\udfcf</mn><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc03\ud835\udfcf</mi></mrow></mfrac><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nSince $\\tilde{{\\bf {f}}_{i}}'{\\bf {D}}\\tilde{{\\bf {f}}_{i}}$ is the weighted data variance of feature $f_{i}$ (denoted as $\\sigma_{i}^{2}$), $||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||$ is the standard data variance (denoted as $\\sigma_{i}$), and the term $\\tilde{{\\bf {f}}}_{i}/||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||$ is interpreted as a normalized feature vector $\\hat{{\\bf {f}}_{i}}=({\\bf {f}}_{i}-\\mu_{i}{\\bf {1}})/\\sigma_{i}$. Therefore, Laplacian Score feature selection can be reformulated by maximizing the following term:\n\n", "itemtype": "equation", "pos": 33474, "prevtext": "\nSince Laplacian Score evaluates the importance of each feature individually, the task of selecting the $k$ features can be solved by greedily picking the top $k$ features with the smallest Laplacian Scores.\n\nThe Laplacian Score of each feature can be reformulated as follows:\n\n", "index": 5, "text": "\\begin{equation}\n\\begin{split}\nlaplacian\\_score(f_{i})&=\\frac{\\tilde{{\\bf {f}}}_{i}'{\\bf {L}}\\tilde{{\\bf {f}}}_{i}}{\\tilde{{\\bf {f}}}_{i}'{\\bf {D}}\\tilde{{\\bf {f}}}_{i}}=\\frac{\\tilde{{\\bf {f}}}_{i}'({\\bf {D}}-{\\bf {S}})\\tilde{{\\bf {f}}}_{i}}{\\tilde{{\\bf {f}}}_{i}'{\\bf {D}}\\tilde{{\\bf {f}}}_{i}}=1-\\frac{\\tilde{{\\bf {f}}}_{i}'{\\bf {S}}\\tilde{{\\bf {f}}}_{i}}{\\tilde{{\\bf {f}}}_{i}'{\\bf {D}}\\tilde{{\\bf {f}}}_{i}}\\\\\n&=1-\\left(\\frac{\\tilde{{\\bf {f}}}_{i}}{||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||}\\right)'{\\bf {S}}\\left(\\frac{\\tilde{{\\bf {f}}}_{i}}{||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||}\\right).\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle laplacian\\_score(f_{i})&amp;\\displaystyle=\\frac{\\tilde{%&#10;{\\bf{f}}}_{i}^{\\prime}{\\bf{L}}\\tilde{{\\bf{f}}}_{i}}{\\tilde{{\\bf{f}}}_{i}^{%&#10;\\prime}{\\bf{D}}\\tilde{{\\bf{f}}}_{i}}=\\frac{\\tilde{{\\bf{f}}}_{i}^{\\prime}({\\bf{%&#10;D}}-{\\bf{S}})\\tilde{{\\bf{f}}}_{i}}{\\tilde{{\\bf{f}}}_{i}^{\\prime}{\\bf{D}}\\tilde%&#10;{{\\bf{f}}}_{i}}=1-\\frac{\\tilde{{\\bf{f}}}_{i}^{\\prime}{\\bf{S}}\\tilde{{\\bf{f}}}_%&#10;{i}}{\\tilde{{\\bf{f}}}_{i}^{\\prime}{\\bf{D}}\\tilde{{\\bf{f}}}_{i}}\\\\&#10;&amp;\\displaystyle=1-\\left(\\frac{\\tilde{{\\bf{f}}}_{i}}{||{\\bf{D}}^{\\frac{1}{2}}%&#10;\\tilde{{\\bf{f}}}_{i}||}\\right)^{\\prime}{\\bf{S}}\\left(\\frac{\\tilde{{\\bf{f}}}_{i%&#10;}}{||{\\bf{D}}^{\\frac{1}{2}}\\tilde{{\\bf{f}}}_{i}||}\\right).\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mfrac><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc0b</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc03</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc03</mi><mo>-</mo><mi>\ud835\udc12</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc03</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc12</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc03</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow></mfrac></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><msup><mrow><mo>(</mo><mfrac><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow></mfrac><mo>)</mo></mrow><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nand it is a special case of the unified framework of similarity based feature selection.\n\n\\subsubsection{SPEC~\\citep{zhao2007spectral} (Unsupervised and Supervised)}\nSPEC is an extension of Laplacian Score that work for both supervised and unsupervised scenarios. For example, in the unsupervised scenario, without label information, the data similarity is measured by the RBF kernel function:\n\n", "itemtype": "equation", "pos": 34660, "prevtext": "\nSince $\\tilde{{\\bf {f}}_{i}}'{\\bf {D}}\\tilde{{\\bf {f}}_{i}}$ is the weighted data variance of feature $f_{i}$ (denoted as $\\sigma_{i}^{2}$), $||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||$ is the standard data variance (denoted as $\\sigma_{i}$), and the term $\\tilde{{\\bf {f}}}_{i}/||{\\bf {D}}^{\\frac{1}{2}}\\tilde{{\\bf {f}}}_{i}||$ is interpreted as a normalized feature vector $\\hat{{\\bf {f}}_{i}}=({\\bf {f}}_{i}-\\mu_{i}{\\bf {1}})/\\sigma_{i}$. Therefore, Laplacian Score feature selection can be reformulated by maximizing the following term:\n\n", "index": 7, "text": "\\begin{equation}\n\\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}{\\bf {\\hat{f}}}'{\\bf {\\hat{S}}}{\\bf {\\hat{f}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}\\in\\mathcal{F}}{\\bf{\\hat{f}}}^{\\prime}{\\bf{%&#10;\\hat{S}}}{\\bf{\\hat{f}}},\" display=\"block\"><mrow><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><msup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nin the supervised scenario, using label information, data similarity can be defined by:\n\n", "itemtype": "equation", "pos": 35182, "prevtext": "\nand it is a special case of the unified framework of similarity based feature selection.\n\n\\subsubsection{SPEC~\\citep{zhao2007spectral} (Unsupervised and Supervised)}\nSPEC is an extension of Laplacian Score that work for both supervised and unsupervised scenarios. For example, in the unsupervised scenario, without label information, the data similarity is measured by the RBF kernel function:\n\n", "index": 9, "text": "\\begin{equation}\n{\\bf {S}}(i,j)=e^{-\\frac{||{\\bf {x}}_{i}-{\\bf {x}}_{j}||}{2\\sigma^{2}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\bf{S}}(i,j)=e^{-\\frac{||{\\bf{x}}_{i}-{\\bf{x}}_{j}||}{2\\sigma^{2}}},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{l}$ is the number of data samples in the class $l$. Afterwards the construction of affinity matrix ${\\bf {S}}$, the diagonal matrix ${\\bf {D}}$ is defined as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$ and the normalized laplacian matrix ${\\bf {L}}_{norm}$ is ${\\bf {L}}_{norm}={\\bf {D}}^{-\\frac{1}{2}}({\\bf {D}}-{\\bf {S}}){\\bf {D}}^{-\\frac{1}{2}}$~\\citep{chung1997spectral}. The basic idea of SPEC is similar to Laplacian Score, a feature that\nis consistent with the data manifold structure assigns similar values to instances that are near each other. The feature relevance are measured by three different criteria:\n\n", "itemtype": "equation", "pos": 35375, "prevtext": "\nin the supervised scenario, using label information, data similarity can be defined by:\n\n", "index": 11, "text": "\\begin{align}\n\\bold{S}(i,j)=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{n_{l}}& \\text{if } y_{i}=y_{j}=l\\\\\n0& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{S}(i,j)=\\left\\{\\begin{array}[]{ll}\\frac{1}{n_{l}}&amp;\\text{%&#10;if }y_{i}=y_{j}=l\\\\&#10;0&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><mi>\ud835\udd4a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>n</mi><mi>l</mi></msub></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>=</mo><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mi>l</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIn the above three equations, $\\hat{{\\bf {f}}_{i}}={\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}/||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}||$; $(\\lambda_{j},\\xi_{j})$ is the $j$-th eigenpair of the normalized laplacian matrix ${\\bf {L}}_{norm}$; $\\alpha_{j}=\\cos\\theta_{j}$, $\\theta_{j}$ is the angle between $\\xi_{j}$ and ${\\bf {f}}_{i}$; $\\gamma(.)$ is an increasing function to penalize high frequency components of the eigensystem to reduce noise. If the data is noise free, the function $\\gamma(.)$ can be removed and $\\gamma(x)=x$. When the second evaluation criteria $SPEC\\_score2(f_{i})$ is used, SPEC is equivalent to Laplacian Score. For $SPEC\\_score3(f_{i})$, it uses the top $m$ eigenpairs to evaluate the importance of feature $f_{i}$.\n\nNext we show how SPEC can be reduced to the generalized similarity based feature selection framework. Selecting top $k$ features in SPEC with three different criteria in Eq.~(\\ref{eq:SPECCriteria}) can be reformulated as:\n\n", "itemtype": "equation", "pos": 36158, "prevtext": "\nwhere $n_{l}$ is the number of data samples in the class $l$. Afterwards the construction of affinity matrix ${\\bf {S}}$, the diagonal matrix ${\\bf {D}}$ is defined as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$ and the normalized laplacian matrix ${\\bf {L}}_{norm}$ is ${\\bf {L}}_{norm}={\\bf {D}}^{-\\frac{1}{2}}({\\bf {D}}-{\\bf {S}}){\\bf {D}}^{-\\frac{1}{2}}$~\\citep{chung1997spectral}. The basic idea of SPEC is similar to Laplacian Score, a feature that\nis consistent with the data manifold structure assigns similar values to instances that are near each other. The feature relevance are measured by three different criteria:\n\n", "index": 13, "text": "\\begin{equation}\n\\begin{split}\nSPEC\\_score1(f_{i}) &= \\hat{{\\bf {f}}_{i}}'\\gamma({\\bf {L}}_{norm})\\hat{{\\bf {f}}_{i}} = \\sum_{j=1}^{n}\\alpha_{j}^{2}\\gamma(\\lambda_{j})\\\\\nSPEC\\_score2(f_{i}) &= \\frac{\\hat{{\\bf {f}}_{i}}'\\gamma({\\bf {L}}_{norm})\\hat{{\\bf {f}}_{i}}}{1-(\\hat{{\\bf {f}}_{i}}'\\xi_{1})^{2}} = \\frac{\\sum_{j=2}^{n}\\alpha_{j}^{2}\\gamma(\\lambda_{j})}{\\sum_{j=2}^{n}\\alpha_{j}^{2}}\\\\\nSPEC\\_score3(f_{i}) &= \\sum_{j=1}^{m}(\\gamma(2)-\\gamma(\\lambda_{j}))\\alpha_{j}^{2}.\n\\end{split}\n\\label{eq:SPECCriteria}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle SPEC\\_score1(f_{i})&amp;\\displaystyle=\\hat{{\\bf{f}}_{i}%&#10;}^{\\prime}\\gamma({\\bf{L}}_{norm})\\hat{{\\bf{f}}_{i}}=\\sum_{j=1}^{n}\\alpha_{j}^{%&#10;2}\\gamma(\\lambda_{j})\\\\&#10;\\displaystyle SPEC\\_score2(f_{i})&amp;\\displaystyle=\\frac{\\hat{{\\bf{f}}_{i}}^{%&#10;\\prime}\\gamma({\\bf{L}}_{norm})\\hat{{\\bf{f}}_{i}}}{1-(\\hat{{\\bf{f}}_{i}}^{%&#10;\\prime}\\xi_{1})^{2}}=\\frac{\\sum_{j=2}^{n}\\alpha_{j}^{2}\\gamma(\\lambda_{j})}{%&#10;\\sum_{j=2}^{n}\\alpha_{j}^{2}}\\\\&#10;\\displaystyle SPEC\\_score3(f_{i})&amp;\\displaystyle=\\sum_{j=1}^{m}(\\gamma(2)-%&#10;\\gamma(\\lambda_{j}))\\alpha_{j}^{2}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>1</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msup><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc0b</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>\u03b1</mi><mi>j</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mfrac><mrow><msup><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc0b</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>m</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover></mrow><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mover accent=\"true\"><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\u03be</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>\u03b1</mi><mi>j</mi><mn>2</mn></msubsup><mo>\u2062</mo><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></msubsup><msubsup><mi>\u03b1</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>3</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bb</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\u03b1</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {U}}$ and ${\\bf {\\Sigma}}$ contain the singular vectors and singular values of the normalized laplacian matrix ${\\bf {L}}_{norm}$, ${\\bf {L}}_{norm}={\\bf {U}}{\\bf {\\Sigma}}{\\bf {U}}'$.\n\n\\subsubsection{Fisher Score~\\citep{duda2012pattern} (Supervised)}\nFisher Score is a supervised feature selection algorithm. Suppose the class labels of $n$ samples ${\\bf {y}}=\\{y_{1},y_{2},...,y_{n}\\}$ come from $c$ classes, Fisher Score selects the features such that the feature values of the samples within the same class are small while the feature values of the samples from different classes are large. The Fisher score of each feature $f_{i}$ is evaluated as follows:\n\n", "itemtype": "equation", "pos": 37643, "prevtext": "\nIn the above three equations, $\\hat{{\\bf {f}}_{i}}={\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}/||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}||$; $(\\lambda_{j},\\xi_{j})$ is the $j$-th eigenpair of the normalized laplacian matrix ${\\bf {L}}_{norm}$; $\\alpha_{j}=\\cos\\theta_{j}$, $\\theta_{j}$ is the angle between $\\xi_{j}$ and ${\\bf {f}}_{i}$; $\\gamma(.)$ is an increasing function to penalize high frequency components of the eigensystem to reduce noise. If the data is noise free, the function $\\gamma(.)$ can be removed and $\\gamma(x)=x$. When the second evaluation criteria $SPEC\\_score2(f_{i})$ is used, SPEC is equivalent to Laplacian Score. For $SPEC\\_score3(f_{i})$, it uses the top $m$ eigenpairs to evaluate the importance of feature $f_{i}$.\n\nNext we show how SPEC can be reduced to the generalized similarity based feature selection framework. Selecting top $k$ features in SPEC with three different criteria in Eq.~(\\ref{eq:SPECCriteria}) can be reformulated as:\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{split}\n& \\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}{\\bf {\\hat{f}}}_{i}'{\\bf {\\hat{S}}}{\\bf {\\hat{f}}}_{i}\\\\\n\\mbox{in } SPEC\\_score1: {\\bf {\\hat{f}}}_{i} & = {\\bf {f}}_{i}||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}||, {\\bf {\\hat{S}}}={\\bf {D}}^{\\frac{1}{2}}{\\bf {U}}({\\bf {I}}-\\gamma({\\bf {\\Sigma}})){\\bf {U}}'{\\bf {D}}^{\\frac{1}{2}}\\\\\n\\mbox{in } SPEC\\_score2: {\\bf {\\hat{f}}}_{i} & = ({\\bf {f}}_{i}-\\mu{\\bf {1}})/||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}||, {\\bf {\\hat{S}}}={\\bf {D}}^{\\frac{1}{2}}{\\bf {U}}({\\bf {I}}-\\gamma({\\bf {\\Sigma}})){\\bf {U}}'{\\bf {D}}^{\\frac{1}{2}}\\\\\n\\mbox{in } SPEC\\_score3: {\\bf {\\hat{f}}}_{i} & = {\\bf {f}}_{i}||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}_{i}||, {\\bf {\\hat{S}}}={\\bf {D}}^{\\frac{1}{2}}{\\bf {U}}_{m}(\\gamma(2{\\bf {I}})-\\gamma({\\bf {\\Sigma}}_{m})){\\bf {U}}_{m}'{\\bf {D}}^{\\frac{1}{2}},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}\\in\\mathcal{F}}{\\bf%&#10;{\\hat{f}}}_{i}^{\\prime}{\\bf{\\hat{S}}}{\\bf{\\hat{f}}}_{i}\\\\&#10;\\displaystyle\\mbox{in }SPEC\\_score1:{\\bf{\\hat{f}}}_{i}&amp;\\displaystyle={\\bf{f}}_%&#10;{i}||{\\bf{D}}^{\\frac{1}{2}}{\\bf{f}}_{i}||,{\\bf{\\hat{S}}}={\\bf{D}}^{\\frac{1}{2}%&#10;}{\\bf{U}}({\\bf{I}}-\\gamma({\\bf{\\Sigma}})){\\bf{U}}^{\\prime}{\\bf{D}}^{\\frac{1}{2%&#10;}}\\\\&#10;\\displaystyle\\mbox{in }SPEC\\_score2:{\\bf{\\hat{f}}}_{i}&amp;\\displaystyle=({\\bf{f}}%&#10;_{i}-\\mu{\\bf{1}})/||{\\bf{D}}^{\\frac{1}{2}}{\\bf{f}}_{i}||,{\\bf{\\hat{S}}}={\\bf{D%&#10;}}^{\\frac{1}{2}}{\\bf{U}}({\\bf{I}}-\\gamma({\\bf{\\Sigma}})){\\bf{U}}^{\\prime}{\\bf{%&#10;D}}^{\\frac{1}{2}}\\\\&#10;\\displaystyle\\mbox{in }SPEC\\_score3:{\\bf{\\hat{f}}}_{i}&amp;\\displaystyle={\\bf{f}}_%&#10;{i}||{\\bf{D}}^{\\frac{1}{2}}{\\bf{f}}_{i}||,{\\bf{\\hat{S}}}={\\bf{D}}^{\\frac{1}{2}%&#10;}{\\bf{U}}_{m}(\\gamma(2{\\bf{I}})-\\gamma({\\bf{\\Sigma}}_{m})){\\bf{U}}_{m}^{\\prime%&#10;}{\\bf{D}}^{\\frac{1}{2}},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><msubsup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mtext>in\u00a0</mtext><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>1</mn></mrow><mo>:</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow></mrow></mrow><mo>,</mo><mrow><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><mi>\ud835\udc14</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeba</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc14</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mtext>in\u00a0</mtext><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>2</mn></mrow><mo>:</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo>-</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow></mrow></mrow><mo>,</mo><mrow><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><mi>\ud835\udc14</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeba</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc14</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mtext>in\u00a0</mtext><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>E</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mn>3</mn></mrow><mo>:</mo><msub><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow></mrow></mrow><mo>,</mo><mrow><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udeba</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc14</mi><mi>m</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{j}$, $\\mu_{i}$, $\\mu_{i,j}$ and $\\sigma_{i,j}^{2}$ indicate the number of samples in class $j$, mean value of feature $f_{i}$, mean value of feature $f_{i}$ for samples in class $j$, variance value of feature $f_{i}$ for samples in class $j$, respectively. Similar to Laplacian Score, the top $k$ features can be obtained by greedily selecting the features with the largest Fisher Scores.\n\nAccording to~\\citep{he2005laplacian}, Fisher Score can be considered as a special case of Laplacian Score as long as the affinity matrix is as follows:\n\n", "itemtype": "equation", "pos": 39183, "prevtext": "\nwhere ${\\bf {U}}$ and ${\\bf {\\Sigma}}$ contain the singular vectors and singular values of the normalized laplacian matrix ${\\bf {L}}_{norm}$, ${\\bf {L}}_{norm}={\\bf {U}}{\\bf {\\Sigma}}{\\bf {U}}'$.\n\n\\subsubsection{Fisher Score~\\citep{duda2012pattern} (Supervised)}\nFisher Score is a supervised feature selection algorithm. Suppose the class labels of $n$ samples ${\\bf {y}}=\\{y_{1},y_{2},...,y_{n}\\}$ come from $c$ classes, Fisher Score selects the features such that the feature values of the samples within the same class are small while the feature values of the samples from different classes are large. The Fisher score of each feature $f_{i}$ is evaluated as follows:\n\n", "index": 17, "text": "\\begin{equation}\nfisher\\_score(f_{i})=\\frac{\\sum_{j=1}^{c}n_{j}(\\mu_{i,j}-\\mu_{i})^{2}}{\\sum_{j=1}^{c}n_{j}\\sigma(i,j)^{2}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"fisher\\_score(f_{i})=\\frac{\\sum_{j=1}^{c}n_{j}(\\mu_{i,j}-\\mu_{i})^{2}}{\\sum_{j%&#10;=1}^{c}n_{j}\\sigma(i,j)^{2}},\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></msubsup><mrow><msub><mi>n</mi><mi>j</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>\u03bc</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></msubsup><mrow><msub><mi>n</mi><mi>j</mi></msub><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{l}$ is the number of data samples in the class $l$. With this specific affinity matrix, we can get the following relationship between Fisher Score and Laplacian Score as follows:\n\n", "itemtype": "equation", "pos": 39875, "prevtext": "\nwhere $n_{j}$, $\\mu_{i}$, $\\mu_{i,j}$ and $\\sigma_{i,j}^{2}$ indicate the number of samples in class $j$, mean value of feature $f_{i}$, mean value of feature $f_{i}$ for samples in class $j$, variance value of feature $f_{i}$ for samples in class $j$, respectively. Similar to Laplacian Score, the top $k$ features can be obtained by greedily selecting the features with the largest Fisher Scores.\n\nAccording to~\\citep{he2005laplacian}, Fisher Score can be considered as a special case of Laplacian Score as long as the affinity matrix is as follows:\n\n", "index": 19, "text": "\\begin{align}\n\\bold{S}(i,j)=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{n_{l}}& \\text{if } y_{i}=y_{j}=l\\\\\n0& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\label{eq:fisherSimilarity}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{S}(i,j)=\\left\\{\\begin{array}[]{ll}\\frac{1}{n_{l}}&amp;\\text{%&#10;if }y_{i}=y_{j}=l\\\\&#10;0&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><mi>\ud835\udd4a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>n</mi><mi>l</mi></msub></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>=</mo><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mi>l</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nTherefore, the computation of Fisher Score can be reduced to the unified framework of similarity based feature selection.\n\nFisher Score measures the relevance of each feature individually as Laplacian Score and SPEC. This leads to a suboptimal subset of features that is incapable of removing redundant features. To tackle this issue, a Generalized Fisher Score method~\\cite{gu2011generalized} is proposed to jointly select features. It aims to find a subset of features by maximizing the lower bound of Fisher Score, which results in the following objective function:\n\n", "itemtype": "equation", "pos": 40245, "prevtext": "\nwhere $n_{l}$ is the number of data samples in the class $l$. With this specific affinity matrix, we can get the following relationship between Fisher Score and Laplacian Score as follows:\n\n", "index": 21, "text": "\\begin{equation}\nfisher\\_score(f_{i})=1-\\frac{1}{laplacian\\_score(f_{i})}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"fisher\\_score(f_{i})=1-\\frac{1}{laplacian\\_score(f_{i})}.\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mrow><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n${\\bf {p}}$ is a feature indicator vector to indicate whether the feature is selected or not, $\\alpha$ is a regularization parameter, ${\\bf {H}}\\in \\mathbb{R}^{n\\times c}$ is a label matrix, its $(i,j)$-th entry is given by:\n\n", "itemtype": "equation", "pos": 40904, "prevtext": "\nTherefore, the computation of Fisher Score can be reduced to the unified framework of similarity based feature selection.\n\nFisher Score measures the relevance of each feature individually as Laplacian Score and SPEC. This leads to a suboptimal subset of features that is incapable of removing redundant features. To tackle this issue, a Generalized Fisher Score method~\\cite{gu2011generalized} is proposed to jointly select features. It aims to find a subset of features by maximizing the lower bound of Fisher Score, which results in the following objective function:\n\n", "index": 23, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {W}},{\\bf {p}}} & \\quad \\frac{1}{2}||{\\bf {X}}diag({\\bf {p}}){\\bf {W}}-{\\bf {H}}||_{F}^{2}+\\frac{\\alpha}{2}||{\\bf {W}}||_{F}^{2}\\\\\ns.t. & \\quad {\\bf {p}}\\in\\{0,1\\}^{d}, {\\bf {p}}'{\\bf {1}}=k.\n\\end{split}\n\\label{eq:generalizedFisher}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{W}},{\\bf{p}}}&amp;\\displaystyle\\quad\\frac{1}{%&#10;2}||{\\bf{X}}diag({\\bf{p}}){\\bf{W}}-{\\bf{H}}||_{F}^{2}+\\frac{\\alpha}{2}||{\\bf{W%&#10;}}||_{F}^{2}\\\\&#10;\\displaystyle s.t.&amp;\\displaystyle\\quad{\\bf{p}}\\in\\{0,1\\}^{d},{\\bf{p}}^{\\prime}{%&#10;\\bf{1}}=k.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mrow><mi>\ud835\udc16</mi><mo>,</mo><mi>\ud835\udc29</mi></mrow></munder></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mpadded><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc29</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow><mo>-</mo><mi>\ud835\udc07</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mfrac><mi>\u03b1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mi>s</mi><mo>.</mo><mi>t</mi></mrow><mo>.</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc29</mi></mpadded><mo>\u2208</mo><msup><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><mi>d</mi></msup></mrow><mo>,</mo><mrow><mrow><msup><mi>\ud835\udc29</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo>=</mo><mi>k</mi></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{j}$ is the number of data instances in class $j$.\n\n\\subsubsection{Trace Ratio Criterion~\\citep{nie2008trace} (Supervised)}\nRecently, the trace ratio criterion has been proposed to directly select the global optimal feature subset based on the corresponding score, which is computed in a trace ratio norm. It builds two affinity matrices ${\\bf {S}}_{w}$ and ${\\bf {S}}_{b}$ to characterize within-class (local affinity) and between-class (global affinity) data similarity. Their corresponding diagonal matrices and laplacian matrices are defined as ${\\bf {D}}_{w}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}_{w}(i,j)$, ${\\bf {D}}_{b}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}_{b}(i,j)$, ${\\bf {L}}_{w}={\\bf {D}}_{w}-{\\bf {S}}_{w}$, ${\\bf {L}}_{b}={\\bf {D}}_{b}-{\\bf {S}}_{b}$, respectively. Let ${\\bf {W}}=[{\\bf {w}}_{i_{1}},{\\bf {w}}_{i_{2}},...,{\\bf {w}}_{i_{k}}]\\in\\mathbb{R}^{d\\times k}$ be the selection indicator matrix such that only the $i_{j}$-th entry in ${\\bf {w}}_{i_{j}}$ is 1 and all the other entries are 0. With these, the trace ratio criterion of all $k$ features in $\\mathcal{F}$ is:\n\n", "itemtype": "equation", "pos": 41419, "prevtext": "\n${\\bf {p}}$ is a feature indicator vector to indicate whether the feature is selected or not, $\\alpha$ is a regularization parameter, ${\\bf {H}}\\in \\mathbb{R}^{n\\times c}$ is a label matrix, its $(i,j)$-th entry is given by:\n\n", "index": 25, "text": "\\begin{align}\n\\bold{H}(i,j)=\\left\\{\n\\begin{array}{ll}\n\\sqrt{\\frac{n}{n_{j}}}-\\sqrt{\\frac{n_{j}}{n}} & \\text{if } y_{i}=j\\\\\n-\\sqrt{\\frac{n_{j}}{n}}& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{H}(i,j)=\\left\\{\\begin{array}[]{ll}\\sqrt{\\frac{n}{n_{j}}}-%&#10;\\sqrt{\\frac{n_{j}}{n}}&amp;\\text{if }y_{i}=j\\\\&#10;-\\sqrt{\\frac{n_{j}}{n}}&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><mi>\u210d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msqrt><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><msub><mi>n</mi><mi>j</mi></msub></mfrac></mstyle></msqrt><mo>-</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mi>j</mi></msub><mi>n</mi></mfrac></mstyle></msqrt></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mi>j</mi></msub><mi>n</mi></mfrac></mstyle></msqrt></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe basic idea is to maximize the data similarity for the instances from the same class (or close to each other) while minimizing the data similarity for the instances from different classes (or far away from each other). The larger the score, the more important the feature set is.\n\n\n\n\n\n\n\n\n\n\nThe trace ratio criterion score in Eq.~(\\ref{eq:traceRatio}) provides a general framework for feature selection. Different between-class and within-class similarity matricse ${\\bf {S}}_{b}$ and ${\\bf {S}}_{w}$ lead to different feature selection algorithms such as batch-mode Lalpacian Score and batch-mode Fisher Score; all of them can be considered as special cases of the general similarity based feature selection framework. For example, in batch-mode Fisher Score, the within-class data similarity and the between-class data similarity are defined as follows:\n\n", "itemtype": "equation", "pos": 42702, "prevtext": "\nwhere $n_{j}$ is the number of data instances in class $j$.\n\n\\subsubsection{Trace Ratio Criterion~\\citep{nie2008trace} (Supervised)}\nRecently, the trace ratio criterion has been proposed to directly select the global optimal feature subset based on the corresponding score, which is computed in a trace ratio norm. It builds two affinity matrices ${\\bf {S}}_{w}$ and ${\\bf {S}}_{b}$ to characterize within-class (local affinity) and between-class (global affinity) data similarity. Their corresponding diagonal matrices and laplacian matrices are defined as ${\\bf {D}}_{w}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}_{w}(i,j)$, ${\\bf {D}}_{b}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}_{b}(i,j)$, ${\\bf {L}}_{w}={\\bf {D}}_{w}-{\\bf {S}}_{w}$, ${\\bf {L}}_{b}={\\bf {D}}_{b}-{\\bf {S}}_{b}$, respectively. Let ${\\bf {W}}=[{\\bf {w}}_{i_{1}},{\\bf {w}}_{i_{2}},...,{\\bf {w}}_{i_{k}}]\\in\\mathbb{R}^{d\\times k}$ be the selection indicator matrix such that only the $i_{j}$-th entry in ${\\bf {w}}_{i_{j}}$ is 1 and all the other entries are 0. With these, the trace ratio criterion of all $k$ features in $\\mathcal{F}$ is:\n\n", "index": 27, "text": "\\begin{equation}\ntrace\\_ratio(\\mathcal{F})=\\frac{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{b}{\\bf {X}}{\\bf {W}})}{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{w}{\\bf {X}}{\\bf {W}})}.\n\\label{eq:traceRatio}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"trace\\_ratio(\\mathcal{F})=\\frac{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{\\bf{L}}_%&#10;{b}{\\bf{X}}{\\bf{W}})}{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{\\bf{L}}_{w}{\\bf{X}%&#10;}{\\bf{W}})}.\" display=\"block\"><mrow><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>b</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>w</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 43761, "prevtext": "\nThe basic idea is to maximize the data similarity for the instances from the same class (or close to each other) while minimizing the data similarity for the instances from different classes (or far away from each other). The larger the score, the more important the feature set is.\n\n\n\n\n\n\n\n\n\n\nThe trace ratio criterion score in Eq.~(\\ref{eq:traceRatio}) provides a general framework for feature selection. Different between-class and within-class similarity matricse ${\\bf {S}}_{b}$ and ${\\bf {S}}_{w}$ lead to different feature selection algorithms such as batch-mode Lalpacian Score and batch-mode Fisher Score; all of them can be considered as special cases of the general similarity based feature selection framework. For example, in batch-mode Fisher Score, the within-class data similarity and the between-class data similarity are defined as follows:\n\n", "index": 29, "text": "\\begin{align}\n{\\bf {S}}_w(i,j)=\\left\\{\n\\begin{array}{ll}\n1/n_{l}& \\text{if } y_{i}=y_{j}=l\\\\\n0& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf{S}}_{w}(i,j)=\\left\\{\\begin{array}[]{ll}1/n_{l}&amp;\\text{if }y_{%&#10;i}=y_{j}=l\\\\&#10;0&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mi>w</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>/</mo><msub><mi>n</mi><mi>l</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>=</mo><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mi>l</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{l}$ is the number of instances in class $l$. The trace ratio criterion for the feature subset $\\mathcal{F}$ can be calculated as:\n\n", "itemtype": "equation", "pos": 43909, "prevtext": "\n\n", "index": 31, "text": "\\begin{align}\n{\\bf {S}}_b(i,j)=\\left\\{\n\\begin{array}{ll}\n1/n-1/n_{l}& \\text{if } y_{i}=y_{j}=l\\\\\n1/n& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf{S}}_{b}(i,j)=\\left\\{\\begin{array}[]{ll}1/n-1/n_{l}&amp;\\text{if %&#10;}y_{i}=y_{j}=l\\\\&#10;1/n&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mi>b</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>n</mi><mi>l</mi></msub></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>=</mo><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mi>l</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nMaximizing the score in above equation is also equivalent to maximize the following term:\n\n", "itemtype": "equation", "pos": 44203, "prevtext": "\nwhere $n_{l}$ is the number of instances in class $l$. The trace ratio criterion for the feature subset $\\mathcal{F}$ can be calculated as:\n\n", "index": 33, "text": "\\begin{equation}\ntrace\\_ratio\\_fisher(\\mathcal{F})=\\frac{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{b}{\\bf {X}}{\\bf {W}})}{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{w}{\\bf {X}}{\\bf {W}})}=\\frac{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'{\\bf {S}}_{w}{\\bf {f}}_{i_{s}}}{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'({\\bf {I}}-{\\bf {S}}_{w}){\\bf {f}}_{i_{s}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"trace\\_ratio\\_fisher(\\mathcal{F})=\\frac{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{%&#10;\\bf{L}}_{b}{\\bf{X}}{\\bf{W}})}{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{\\bf{L}}_{w%&#10;}{\\bf{X}}{\\bf{W}})}=\\frac{\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}{\\bf{S}}_{w}{%&#10;\\bf{f}}_{i_{s}}}{\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}({\\bf{I}}-{\\bf{S}}_{w}%&#10;){\\bf{f}}_{i_{s}}}.\" display=\"block\"><mrow><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>b</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>w</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nSince ${\\bf {X}}_{\\mathcal{F}}'{\\bf {X}}_{\\mathcal{F}}$ is constant and the above maximization problem can be reduced to the unified similarity based feature selection framework:\n\n", "itemtype": "equation", "pos": 44626, "prevtext": "\nMaximizing the score in above equation is also equivalent to maximize the following term:\n\n", "index": 35, "text": "\\begin{equation}\n\\frac{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'{\\bf {S}}_{w}{\\bf {f}}_{i_{s}}}{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'{\\bf {f}}_{i_{s}}}=\\frac{{\\bf {X}}_{\\mathcal{F}}'{\\bf {S}}_{w}{\\bf {X}}_{\\mathcal{F}}}{{\\bf {X}}_{\\mathcal{F}}'{\\bf {X}}_{\\mathcal{F}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}{\\bf{S}}_{w}{\\bf{f}}_{i_{s}}}{%&#10;\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}{\\bf{f}}_{i_{s}}}=\\frac{{\\bf{X}}_{%&#10;\\mathcal{F}}^{\\prime}{\\bf{S}}_{w}{\\bf{X}}_{\\mathcal{F}}}{{\\bf{X}}_{\\mathcal{F}%&#10;}^{\\prime}{\\bf{X}}_{\\mathcal{F}}}.\" display=\"block\"><mrow><mrow><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mi>\ud835\udc17</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc17</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub></mrow><mrow><msubsup><mi>\ud835\udc17</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc17</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIn batch-mode Laplacian Score, the within-class data similarity and the between-class data similarity are defined as follows:\n\n", "itemtype": "equation", "pos": 45076, "prevtext": "\nSince ${\\bf {X}}_{\\mathcal{F}}'{\\bf {X}}_{\\mathcal{F}}$ is constant and the above maximization problem can be reduced to the unified similarity based feature selection framework:\n\n", "index": 37, "text": "\\begin{equation}\n\\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}{\\bf {\\hat{f}}}'{\\bf {\\hat{S}}}{\\bf {\\hat{f}}}, \\mbox{ where } {\\bf {\\hat{f}}}={\\bf {f}}/||{\\bf {f}}|| \\mbox{ and } {\\bf {\\hat{S}}}={\\bf {S}}_{w}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}\\in\\mathcal{F}}{\\bf{\\hat{f}}}^{\\prime}{\\bf{%&#10;\\hat{S}}}{\\bf{\\hat{f}}},\\mbox{ where }{\\bf{\\hat{f}}}={\\bf{f}}/||{\\bf{f}}||%&#10;\\mbox{ and }{\\bf{\\hat{S}}}={\\bf{S}}_{w}.\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><msup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow><mo>,</mo><mrow><mtext>\u00a0where\u00a0</mtext><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow><mo>=</mo><mrow><mrow><mi>\ud835\udc1f</mi><mo>/</mo><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc1f</mi><mo fence=\"true\">||</mo></mrow></mrow><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 45429, "prevtext": "\nIn batch-mode Laplacian Score, the within-class data similarity and the between-class data similarity are defined as follows:\n\n", "index": 39, "text": "\\begin{align}\n{\\bf {S}}_w(i,j)=\\left\\{\n\\begin{array}{ll}\ne^{-\\frac{||{\\bf {x}}_{i}-{\\bf {x}}_{j}||^{2}}{t}}& \\text{if } {\\bf {x}}_{i}\\in \\mathcal{N}_{p}({\\bf {x}}_{j}) \\text{ or }{\\bf {x}}_{j}\\in \\mathcal{N}_{p}({\\bf {x}}_{i})\\\\\n0& \\text{otherwise,}\\\\\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf{S}}_{w}(i,j)=\\left\\{\\begin{array}[]{ll}e^{-\\frac{||{\\bf{x}}_%&#10;{i}-{\\bf{x}}_{j}||^{2}}{t}}&amp;\\text{if }{\\bf{x}}_{i}\\in\\mathcal{N}_{p}({\\bf{x}}_%&#10;{j})\\text{ or }{\\bf{x}}_{j}\\in\\mathcal{N}_{p}({\\bf{x}}_{i})\\\\&#10;0&amp;\\text{otherwise,}\\\\&#10;\\end{array}\\right.\" display=\"inline\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mi>w</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msup><mi>e</mi><mrow><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mn>2</mn></msup><mi>t</mi></mfrac></mrow></msup></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0or\u00a0</mtext><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise,</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe trace ratio criterion score can be computed by:\n\n", "itemtype": "equation", "pos": 45714, "prevtext": "\n\n\n", "index": 41, "text": "\\begin{equation}\n{\\bf {S}}_{b}=({\\bf {1}}'{\\bf {D}}_{w}{\\bf {1}})^{-1}{\\bf {D}}_{w}{\\bf {1}}{\\bf {1}}'{\\bf {D}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"{\\bf{S}}_{b}=({\\bf{1}}^{\\prime}{\\bf{D}}_{w}{\\bf{1}})^{-1}{\\bf{D}}_{w}{\\bf{1}}{%&#10;\\bf{1}}^{\\prime}{\\bf{D}}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mi>b</mi></msub><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mn>\ud835\udfcf</mn><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc03</mi><mi>w</mi></msub><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc03</mi><mi>w</mi></msub><mo>\u2062</mo><msup><mn>\ud835\udfcf\ud835\udfcf</mn><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc03</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nMaximizing the above trace ratio criterion score is also equivalent to solve the following problem:\n\n", "itemtype": "equation", "pos": 45894, "prevtext": "\nThe trace ratio criterion score can be computed by:\n\n", "index": 43, "text": "\\begin{equation}\ntrace\\_ratio\\_laplacian(\\mathcal{F})=\\frac{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{b}{\\bf {X}}{\\bf {W}})}{tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}_{w}{\\bf {X}}{\\bf {W}})}=\n\\frac{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'{\\bf {D}}_{w}{\\bf {f}}_{i_{s}}}{\\sum_{s=1}^{k}{\\bf {f}}_{i_{s}}'({\\bf {D}}_{w}-{\\bf {S}}_{w}){\\bf {f}}_{i_{s}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"trace\\_ratio\\_laplacian(\\mathcal{F})=\\frac{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{%&#10;\\prime}{\\bf{L}}_{b}{\\bf{X}}{\\bf{W}})}{tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{%&#10;\\bf{L}}_{w}{\\bf{X}}{\\bf{W}})}=\\frac{\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}{%&#10;\\bf{D}}_{w}{\\bf{f}}_{i_{s}}}{\\sum_{s=1}^{k}{\\bf{f}}_{i_{s}}^{\\prime}({\\bf{D}}_%&#10;{w}-{\\bf{S}}_{w}){\\bf{f}}_{i_{s}}}.\" display=\"block\"><mrow><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>b</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>w</mi></msub><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc03</mi><mi>w</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mrow><msubsup><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc03</mi><mi>w</mi></msub><mo>-</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc1f</mi><msub><mi>i</mi><mi>s</mi></msub></msub></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\ntherefore it is a special case of the unified framework.\n\n\\subsubsection{ReliefF~\\citep{robnik2003theoretical} (Supervised)}\nRelief and its multi-class variant ReliefF are supervised filter algorithms that select features to separate instances from different classes. Assume that $l$ data instances are randomly selected among all $n$ instances, then the feature score of $f_{i}$ in Relief is defined as follows:\n\n", "itemtype": "equation", "pos": 46335, "prevtext": "\nMaximizing the above trace ratio criterion score is also equivalent to solve the following problem:\n\n", "index": 45, "text": "\\begin{equation}\n\\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}{\\bf {\\hat{f}}}'{\\bf {\\hat{S}}}{\\bf {\\hat{f}}}, \\mbox{ where } {\\bf {\\hat{f}}}={\\bf {f}}/||{\\bf {D}}^{\\frac{1}{2}}{\\bf {f}}|| \\mbox{ and } {\\bf {\\hat{S}}}={\\bf {S}}_{w},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}\\in\\mathcal{F}}{\\bf{\\hat{f}}}^{\\prime}{\\bf{%&#10;\\hat{S}}}{\\bf{\\hat{f}}},\\mbox{ where }{\\bf{\\hat{f}}}={\\bf{f}}/||{\\bf{D}}^{%&#10;\\frac{1}{2}}{\\bf{f}}||\\mbox{ and }{\\bf{\\hat{S}}}={\\bf{S}}_{w},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder><mrow><msup><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mrow><mo>,</mo><mrow><mtext>\u00a0where\u00a0</mtext><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc1f</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow><mo>=</mo><mrow><mrow><mi>\ud835\udc1f</mi><mo>/</mo><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc03</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow><mo fence=\"true\">||</mo></mrow></mrow><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><msub><mi>\ud835\udc12</mi><mi>w</mi></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $NM(j)$ and $NH(j)$ indicates the nearest data instances to $x_{j}$ with the same class label and different class, respectively. $d(.)$ is a distance metric which is usually set to be Euclidean distance. Relief only works for binary classification task. To tackle the multiclass classification problem, the feature score in Eq.~(\\ref{eq:relief}) is extended in ReliefF:\n\n", "itemtype": "equation", "pos": 46998, "prevtext": "\ntherefore it is a special case of the unified framework.\n\n\\subsubsection{ReliefF~\\citep{robnik2003theoretical} (Supervised)}\nRelief and its multi-class variant ReliefF are supervised filter algorithms that select features to separate instances from different classes. Assume that $l$ data instances are randomly selected among all $n$ instances, then the feature score of $f_{i}$ in Relief is defined as follows:\n\n", "index": 47, "text": "\\begin{equation}\nRelief\\_score(f_{i})=\\frac{1}{2}\\sum_{j=1}^{l}d({\\bf {X}}(j,i)-{\\bf {X}}(NM(j),i))-d({\\bf {X}}(j,i)-{\\bf {X}}(NH(j),i)),\n\\label{eq:relief}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"Relief\\_score(f_{i})=\\frac{1}{2}\\sum_{j=1}^{l}d({\\bf{X}}(j,i)-{\\bf{X}}(NM(j),i%&#10;))-d({\\bf{X}}(j,i)-{\\bf{X}}(NH(j),i)),\" display=\"block\"><mrow><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $NH(j)$ and $NM(j,y)$ indicate the nearest data instances to $x_{j}$ in the same class and a different class $y$, respectively, and their sizes are $h_{jy}$ and $m_{j}$. $p(y)$ is the ratio of instances with class label $y$.\n\nReliefF is equivalent to selecting features that preserve a special form of data similarity matrix which can be derived from the class labels. Assume that the dataset has the same number of instances in each of the $c$ classes, there are $q$ instances in both $NM(j)$ and $NH(j,y)$, the Euclidean distance is used and each feature vector has been normalized. Then according to~\\citep{zhao2007spectral}, the criterion of ReliefF is equivalent to the following with above assumptions:\n\n", "itemtype": "equation", "pos": 47545, "prevtext": "\nwhere $NM(j)$ and $NH(j)$ indicates the nearest data instances to $x_{j}$ with the same class label and different class, respectively. $d(.)$ is a distance metric which is usually set to be Euclidean distance. Relief only works for binary classification task. To tackle the multiclass classification problem, the feature score in Eq.~(\\ref{eq:relief}) is extended in ReliefF:\n\n", "index": 49, "text": "\\begin{align}\nReliefF\\_score(f_{i})=&\\frac{1}{c}\\sum_{j=1}^{l}\\left(-\\frac{1}{m_{j}}\\sum_{x_{r}\\in NH(j)}d({\\bf {X}}(j,i)-{\\bf {X}}(r,i))\\right.\\\\\n+&\\sum_{y\\neq y_{j}}\\frac{1}{h_{jy}}\\frac{p(y)}{1-p(y)}\\left.\\sum_{x_{r}\\in NM(j,y)}d({\\bf {X}}(j,i)-{\\bf {X}}(r,i))\\right),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle ReliefF\\_score(f_{i})=\" display=\"inline\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{c}\\sum_{j=1}^{l}\\left(-\\frac{1}{m_{j}}\\sum_{x_{r}\\in NH(%&#10;j)}d({\\bf{X}}(j,i)-{\\bf{X}}(r,i))\\right.\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>c</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover></mstyle><mrow><mo>(</mo><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>m</mi><mi>j</mi></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>r</mi></msub><mo>\u2208</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle+\" display=\"inline\"><mo>+</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{y\\neq y_{j}}\\frac{1}{h_{jy}}\\frac{p(y)}{1-p(y)}\\left.\\sum_{%&#10;x_{r}\\in NM(j,y)}d({\\bf{X}}(j,i)-{\\bf{X}}(r,i))\\right),\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>y</mi><mo>\u2260</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>h</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>y</mi></mrow></msub></mfrac></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>1</mn><mo>-</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>r</mi></msub><mo>\u2208</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mi>d</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>r</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>)</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $NM(j)_{s}$ denote the $s$-th nearest hit of $x_{j}$ and $NH(j,y)_{s}$ denote the $s$-th nearest miss of $x_{j}$ in class $y$. It is easy to show that maximizing the ReliefF score in Eq.~(\\ref{eq:reliefReformulation}) is equivalent to the following optimization problem:\n\n", "itemtype": "equation", "pos": 48544, "prevtext": "\nwhere $NH(j)$ and $NM(j,y)$ indicate the nearest data instances to $x_{j}$ in the same class and a different class $y$, respectively, and their sizes are $h_{jy}$ and $m_{j}$. $p(y)$ is the ratio of instances with class label $y$.\n\nReliefF is equivalent to selecting features that preserve a special form of data similarity matrix which can be derived from the class labels. Assume that the dataset has the same number of instances in each of the $c$ classes, there are $q$ instances in both $NM(j)$ and $NH(j,y)$, the Euclidean distance is used and each feature vector has been normalized. Then according to~\\citep{zhao2007spectral}, the criterion of ReliefF is equivalent to the following with above assumptions:\n\n", "index": 51, "text": "\\begin{align}\nReliefF\\_score(f_{i})=&\\sum_{j=1}^{n}\\left(\\sum_{s=1}^{q}\\frac{1}{q}({\\bf {X}}(j,i)-{\\bf {X}}(NM(j)_{s}))^{2}\\right.\\\\\n-&\\left.\\sum_{y\\neq y_{j}}\\frac{\\sum_{s=1}^{q}{\\bf {X}}(j,i)-{\\bf {X}}(NH(j,y)_{s})^{2}}{(c-1)q}\\right),\n\\label{eq:reliefReformulation}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle ReliefF\\_score(f_{i})=\" display=\"inline\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{j=1}^{n}\\left(\\sum_{s=1}^{q}\\frac{1}{q}({\\bf{X}}(j,i)-{\\bf{%&#10;X}}(NM(j)_{s}))^{2}\\right.\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>q</mi></mfrac></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mi>M</mi><msub><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-\" display=\"inline\"><mo>-</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left.\\sum_{y\\neq y_{j}}\\frac{\\sum_{s=1}^{q}{\\bf{X}}(j,i)-{\\bf{X}%&#10;}(NH(j,y)_{s})^{2}}{(c-1)q}\\right),\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>y</mi><mo>\u2260</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></msubsup><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mi>s</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>q</mi></mrow></mfrac></mstyle><mo>)</mo><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the affinity matrix is defined as:\n\n", "itemtype": "equation", "pos": 49102, "prevtext": "\nwhere $NM(j)_{s}$ denote the $s$-th nearest hit of $x_{j}$ and $NH(j,y)_{s}$ denote the $s$-th nearest miss of $x_{j}$ in class $y$. It is easy to show that maximizing the ReliefF score in Eq.~(\\ref{eq:reliefReformulation}) is equivalent to the following optimization problem:\n\n", "index": 53, "text": "\\begin{equation}\n\\max_{\\mathcal{F}}\\sum_{{\\bf {f}}\\in\\mathcal{F}}-1+{\\bf {f}}'{\\bf {\\hat{S}}}{\\bf {f}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\mathcal{F}}\\sum_{{\\bf{f}}\\in\\mathcal{F}}-1+{\\bf{f}}^{\\prime}{\\bf{\\hat{S%&#10;}}}{\\bf{f}},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></munder><mo>\u2062</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc1f</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></munder></mrow><mo>-</mo><mn>1</mn></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc1f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mi>\ud835\udc1f</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe optimization can be solved in a greedy manner by selecting the top $k$ features with the highest ReliefF scores.\n\n\\subsection{Information Theoretical based Methods}\nA large family of existing feature selection algorithms are information theoretical based methods. Algorithms in this family mainly exploit different heuristic filter criteria to measure the importance of features. As indicated in~\\citep{duda2012pattern}, many hand-designed information theoretic criteria are proposed to maximize feature relevance and minimize feature redundancy. Since the relevance of a feature is usually measured by its correlation with class labels, most algorithms in this family are performed in a supervised way. In addition, most information theoretic concepts can only be applied on discrete variables. Therefore, feature selection algorithms in this family can only work with discrete data. For numeric feature values, some data discretization techniques~\\citep{dougherty1995supervised,kotsiantis2006discretization} are required. Two decades of research on information theoretic criteria can be unified in a conditional likelihood maximization framework, and most algorithms can be reduced to be a specific case of the unified framework~\\citep{brown2012conditional} . In this subsection, we introduce some representative algorithms in this family. First, we show the general forms of the algorithms, and then we show how they can be reduced to the unified framework. Before presenting the detailed algorithms, we first give a brief introduction about basic information theoretic concepts.\n\nThe first concept is called \\emph{entropy}, it is a measure of the uncertainty of a discrete random variable. The entropy of a discrete random variable $X$ is defined as follows:\n\n", "itemtype": "equation", "pos": 49262, "prevtext": "\nwhere the affinity matrix is defined as:\n\n", "index": 55, "text": "\\begin{align}\n{\\bf {\\hat{S}}}(i,j)=\\left\\{\n\\begin{array}{ll}\n1 & \\text{if } i=j\\\\\n-\\frac{1}{q} & x_{j}\\in NH(i)\\\\\n\\frac{1}{(c-1)q} & x_{j}\\in NH(i,y).\n\\end{array}\n\\right.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf{\\hat{S}}}(i,j)=\\left\\{\\begin{array}[]{ll}1&amp;\\text{if }i=j\\\\&#10;-\\frac{1}{q}&amp;x_{j}\\in NH(i)\\\\&#10;\\frac{1}{(c-1)q}&amp;x_{j}\\in NH(i,y).\\end{array}\\right.\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>\ud835\udc12</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>=</mo><mi>j</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>q</mi></mfrac></mstyle></mrow></mtd><mtd columnalign=\"left\"><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2208</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>q</mi></mrow></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>\u2208</mo><mrow><mi>N</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $x_{i}$ denotes a specific value of random variable $X$, $P(x_{i})$ denotes the probability of $x_{i}$ over all possible values of $X$ which can be estimated from the data.\n\nThe second concept is the \\emph{conditional entropy} of $X$ given another discrete random variable $Y$:\n\n", "itemtype": "equation", "pos": 51212, "prevtext": "\nThe optimization can be solved in a greedy manner by selecting the top $k$ features with the highest ReliefF scores.\n\n\\subsection{Information Theoretical based Methods}\nA large family of existing feature selection algorithms are information theoretical based methods. Algorithms in this family mainly exploit different heuristic filter criteria to measure the importance of features. As indicated in~\\citep{duda2012pattern}, many hand-designed information theoretic criteria are proposed to maximize feature relevance and minimize feature redundancy. Since the relevance of a feature is usually measured by its correlation with class labels, most algorithms in this family are performed in a supervised way. In addition, most information theoretic concepts can only be applied on discrete variables. Therefore, feature selection algorithms in this family can only work with discrete data. For numeric feature values, some data discretization techniques~\\citep{dougherty1995supervised,kotsiantis2006discretization} are required. Two decades of research on information theoretic criteria can be unified in a conditional likelihood maximization framework, and most algorithms can be reduced to be a specific case of the unified framework~\\citep{brown2012conditional} . In this subsection, we introduce some representative algorithms in this family. First, we show the general forms of the algorithms, and then we show how they can be reduced to the unified framework. Before presenting the detailed algorithms, we first give a brief introduction about basic information theoretic concepts.\n\nThe first concept is called \\emph{entropy}, it is a measure of the uncertainty of a discrete random variable. The entropy of a discrete random variable $X$ is defined as follows:\n\n", "index": 57, "text": "\\begin{equation}\nH(X)=-\\sum_{x_{i}\\in X}P(x_{i})log(P(x_{i})),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"H(X)=-\\sum_{x_{i}\\in X}P(x_{i})log(P(x_{i})),\" display=\"block\"><mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $P(y_{i})$ is the prior probability of $y_{i}$, while $P(x_{i}|y_{j})$ is the conditional probability of $x_{i}$ given $y_{j}$. The measure of conditional entropy shows the uncertainty of $X$ given another discrete random variable $Y$.\n\nThe concept of \\emph{information gain}~\\citep{shannon2001mathematical} between $X$ and $Y$ is used to measure their dependency with entropy and conditional entropy. Since it measures the amount of information shared by $X$ and $Y$ together, it is often referred as \\emph{mutual information}, which is calculated as:\n\n", "itemtype": "equation", "pos": 51574, "prevtext": "\nwhere $x_{i}$ denotes a specific value of random variable $X$, $P(x_{i})$ denotes the probability of $x_{i}$ over all possible values of $X$ which can be estimated from the data.\n\nThe second concept is the \\emph{conditional entropy} of $X$ given another discrete random variable $Y$:\n\n", "index": 59, "text": "\\begin{equation}\nH(X|Y)=\\sum_{y_{j}\\in Y}P(y_{j})\\sum_{x_{i} \\in X}P(x_{i}|y_{j})log(P(x_{i}|y_{j})),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"H(X|Y)=\\sum_{y_{j}\\in Y}P(y_{j})\\sum_{x_{i}\\in X}P(x_{i}|y_{j})log(P(x_{i}|y_{%&#10;j})),\" display=\"block\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>\u2208</mo><mi>Y</mi></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi>X</mi></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $P(x_{i},y_{j})$ is the joint probability of $x_{i}$ and $y_{j}$. It can be noticed that information gain is symmetric such that $I(X;Y)=I(Y;X)$, and is zero if the discrete variables $X$ and $Y$ are independent.\n\nSimilar to the concept of entropy, the \\emph{conditional information gain} (or \\emph{conditional mutual information}) of discrete variables $X$ and $Y$ given the third discrete variable $Z$ is given as follows:\n\n", "itemtype": "equation", "pos": 52250, "prevtext": "\nwhere $P(y_{i})$ is the prior probability of $y_{i}$, while $P(x_{i}|y_{j})$ is the conditional probability of $x_{i}$ given $y_{j}$. The measure of conditional entropy shows the uncertainty of $X$ given another discrete random variable $Y$.\n\nThe concept of \\emph{information gain}~\\citep{shannon2001mathematical} between $X$ and $Y$ is used to measure their dependency with entropy and conditional entropy. Since it measures the amount of information shared by $X$ and $Y$ together, it is often referred as \\emph{mutual information}, which is calculated as:\n\n", "index": 61, "text": "\\begin{equation}\n\\begin{split}\nI(X;Y)=&H(X)-H(X|Y)\\\\\n=&\\sum_{x_{i}\\in X}\\sum_{y_{j}\\in Y}P(x_{i},y_{j})log\\frac{P(x_{i},y_{j})}{P(x_{i})P(y_{j})},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle I(X;Y)=&amp;\\displaystyle H(X)-H(X|Y)\\\\&#10;\\displaystyle=&amp;\\displaystyle\\sum_{x_{i}\\in X}\\sum_{y_{j}\\in Y}P(x_{i},y_{j})%&#10;log\\frac{P(x_{i},y_{j})}{P(x_{i})P(y_{j})},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mo>=</mo></mtd><mtd columnalign=\"left\"><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi>X</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>\u2208</mo><mi>Y</mi></mrow></munder><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mfrac><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIt shows the amount of mutual information shared by $X$ and $Y$ when the third discrete variable $Z$ is given.\n\nSearching for the global best set of features is NP-hard, thus, most algorithms in this family exploit heuristic sequential search approaches to add/remove features one by one. In this survey, we explain the feature selection problem by forward sequential search such that features are added into the selected feature set one by one. We denote $\\mathcal{S}$ as the current selected feature set that is initially empty. $Y$ represents the class labels. $X_{j}\\in \\mathcal{S}$ is a specific feature in the current $\\mathcal{S}$. $J(.)$ is a feature selection criterion (score) where, generally, the higher the value of $J(X_{k})$, the more important the feature $X_{k}$ is. In the unified conditional likelihood maximization feature selection framework, the selection criterion (score) for a new unselected feature $X_{k}$ is given as follows:\n\n", "itemtype": "equation", "pos": 52855, "prevtext": "\nwhere $P(x_{i},y_{j})$ is the joint probability of $x_{i}$ and $y_{j}$. It can be noticed that information gain is symmetric such that $I(X;Y)=I(Y;X)$, and is zero if the discrete variables $X$ and $Y$ are independent.\n\nSimilar to the concept of entropy, the \\emph{conditional information gain} (or \\emph{conditional mutual information}) of discrete variables $X$ and $Y$ given the third discrete variable $Z$ is given as follows:\n\n", "index": 63, "text": "\\begin{equation}\n\\begin{split}\nI(X;Y|Z)=&H(X|Z)-H(X|Y,Z)\\\\\n=&\\sum_{z_{k}\\in Z}P(z_{k})\\sum_{x_{i}\\in X}\\sum_{y_{j}\\in Y}P(x_{i},y_{j}|z_{k})log\\frac{P(x_{i},y_{j}|z_{k})}{P(x_{i}|z_{k})P(y_{j}|z_{k})}.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle I(X;Y|Z)=&amp;\\displaystyle H(X|Z)-H(X|Y,Z)\\\\&#10;\\displaystyle=&amp;\\displaystyle\\sum_{z_{k}\\in Z}P(z_{k})\\sum_{x_{i}\\in X}\\sum_{y_%&#10;{j}\\in Y}P(x_{i},y_{j}|z_{k})log\\frac{P(x_{i},y_{j}|z_{k})}{P(x_{i}|z_{k})P(y_%&#10;{j}|z_{k})}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mo>=</mo></mtd><mtd columnalign=\"left\"><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>k</mi></msub><mo>\u2208</mo><mi>Z</mi></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2208</mo><mi>X</mi></mrow></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>\u2208</mo><mi>Y</mi></mrow></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $g(.)$ is a function w.r.t. two variables $I(X_{j};X_{k})$ and $I(X_{j};X_{k}|Y)$. If $g(.)$ is a non-linear function w.r.t. these two variables, it is referred as a criterion by linear combinations of Shannon information terms such that:\n\n", "itemtype": "equation", "pos": 54038, "prevtext": "\nIt shows the amount of mutual information shared by $X$ and $Y$ when the third discrete variable $Z$ is given.\n\nSearching for the global best set of features is NP-hard, thus, most algorithms in this family exploit heuristic sequential search approaches to add/remove features one by one. In this survey, we explain the feature selection problem by forward sequential search such that features are added into the selected feature set one by one. We denote $\\mathcal{S}$ as the current selected feature set that is initially empty. $Y$ represents the class labels. $X_{j}\\in \\mathcal{S}$ is a specific feature in the current $\\mathcal{S}$. $J(.)$ is a feature selection criterion (score) where, generally, the higher the value of $J(X_{k})$, the more important the feature $X_{k}$ is. In the unified conditional likelihood maximization feature selection framework, the selection criterion (score) for a new unselected feature $X_{k}$ is given as follows:\n\n", "index": 65, "text": "\\begin{equation}\nJ_{cmi}(X_{k})=I(X_{k};Y)+\\sum_{X_{j}\\in\\mathcal{S}}g[I(X_{j};X_{k}),I(X_{j};X_{k}|Y)],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"J_{cmi}(X_{k})=I(X_{k};Y)+\\sum_{X_{j}\\in\\mathcal{S}}g[I(X_{j};X_{k}),I(X_{j};X%&#10;_{k}|Y)],\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>g</mi><mrow><mo stretchy=\"false\">[</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\beta$ and $\\lambda$ are two nonnegative parameters between zero and one. On the other other hand, if $g(.)$ is a linear function w.r.t. these two variables, it is referred as a criterion by non-linear combinations of Shannon information terms.\n\n\\subsubsection{Mutual Information Maximization (or Information Gain)~\\citep{lewis1992feature}}\nMutual Information Maximization (MIM)(also known as Information Gain) measures the importance of a feature by its correlation with the class label. It assumes that when a feature has a strong correlation with the class label, it can help achieve good classification performance. The Mutual Information score for a new unselected feature $X_{k}$ is:\n\n", "itemtype": "equation", "pos": 54403, "prevtext": "\nwhere $g(.)$ is a function w.r.t. two variables $I(X_{j};X_{k})$ and $I(X_{j};X_{k}|Y)$. If $g(.)$ is a non-linear function w.r.t. these two variables, it is referred as a criterion by linear combinations of Shannon information terms such that:\n\n", "index": 67, "text": "\\begin{equation}\nJ_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m1\" class=\"ltx_Math\" alttext=\"J_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+%&#10;\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b2</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered while the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\n\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\n\n", "itemtype": "equation", "pos": 55255, "prevtext": "\nwhere $\\beta$ and $\\lambda$ are two nonnegative parameters between zero and one. On the other other hand, if $g(.)$ is a linear function w.r.t. these two variables, it is referred as a criterion by non-linear combinations of Shannon information terms.\n\n\\subsubsection{Mutual Information Maximization (or Information Gain)~\\citep{lewis1992feature}}\nMutual Information Maximization (MIM)(also known as Information Gain) measures the importance of a feature by its correlation with the class label. It assumes that when a feature has a strong correlation with the class label, it can help achieve good classification performance. The Mutual Information score for a new unselected feature $X_{k}$ is:\n\n", "index": 69, "text": "\\begin{equation}\nJ_{MIM}(X_{k})=I(X_{k};Y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m1\" class=\"ltx_Math\" alttext=\"J_{MIM}(X_{k})=I(X_{k};Y).\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere both $\\beta$ and $\\lambda$ are equal to zero.\n\n\\subsubsection{Mutual Information Feature Selection~\\citep{battiti1994using}}\nA limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature $X_{k}$ can be formulated as follows:\n\n", "itemtype": "equation", "pos": 55913, "prevtext": "\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered while the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\n\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\n\n", "index": 71, "text": "\\begin{equation}\nJ_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E38.m1\" class=\"ltx_Math\" alttext=\"J_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+%&#10;\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b2</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIn MIFS, the feature relevance of the new feature is evaluated by the first term $I(X_{k};Y)$, while the second term tries to penalize the feature that has a high mutual information with the current selected features such that feature redundancy is minimized. In the original paper, the parameter $\\beta$ is empirically set to be one.\n\nMIFS can also be reduced to be a special case of the linear combination of Shannon information terms:\n\n", "itemtype": "equation", "pos": 56763, "prevtext": "\nwhere both $\\beta$ and $\\lambda$ are equal to zero.\n\n\\subsubsection{Mutual Information Feature Selection~\\citep{battiti1994using}}\nA limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature $X_{k}$ can be formulated as follows:\n\n", "index": 73, "text": "\\begin{equation}\nJ_{MIFS}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{k};X_{j}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E39.m1\" class=\"ltx_Math\" alttext=\"J_{MIFS}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{k};X_{j}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere both $\\beta$ and $\\lambda$ are equal to zero.\n\n\\subsubsection{Mutual Information Feature Selection~\\citep{battiti1994using}}\nA limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature $X_{k}$ can be formulated as follows:\n\n", "itemtype": "equation", "pos": 55913, "prevtext": "\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered while the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\n\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\n\n", "index": 71, "text": "\\begin{equation}\nJ_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E40.m1\" class=\"ltx_Math\" alttext=\"J_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+%&#10;\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b2</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nWith more selected features, the effect of feature redundancy is gradually reduced. The intuition is that with more non-redundant features selected, it is becoming more difficult for new features to be redundant to the features that have already been  in $\\mathcal{S}$. In~\\citep{brown2012conditional}, it gives another interpretation that the pairwise independence between features becomes stronger as more features are added to $\\mathcal{S}$, possibly because of noise information in the data.\n\nThe MRMR criterion is strongly linked to the Conditional likelihood maximization framework:\n\n", "itemtype": "equation", "pos": 57830, "prevtext": "\nwhere $\\beta$ is between zero and one, and $\\lambda$ is set to be zero.\n\n\\subsubsection{Minimum Redundancy Maximum Relevance~\\citep{peng2005feature}}\nUnlike MIFS that empirically sets $\\beta$ to be one,~\\citep{peng2005feature} proposed a Minimum Redundancy Maximum Relevance (MRMR) criterion to set the value of $\\beta$ the reverse of the number of selected features:\n\n", "index": 77, "text": "\\begin{equation}\nJ_{MRMR}(X_{k})=I(X_{k};Y)-\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(X_{k};X_{j}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E41.m1\" class=\"ltx_Math\" alttext=\"J_{MRMR}(X_{k})=I(X_{k};Y)-\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(%&#10;X_{k};X_{j}).\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>M</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>R</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere both $\\beta$ and $\\lambda$ are equal to zero.\n\n\\subsubsection{Mutual Information Feature Selection~\\citep{battiti1994using}}\nA limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature $X_{k}$ can be formulated as follows:\n\n", "itemtype": "equation", "pos": 55913, "prevtext": "\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered while the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\n\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\n\n", "index": 71, "text": "\\begin{equation}\nJ_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E42.m1\" class=\"ltx_Math\" alttext=\"J_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+%&#10;\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b2</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIt can be observed that compared with MIFS, it adds a third term $\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y)$ to maximize the conditional redundancy.\n\nCIFE is also a special case of the linear combination of Shannon information terms:\n\n", "itemtype": "equation", "pos": 59598, "prevtext": "\nif we iteratively revise the value of $\\beta$ to be $\\frac{1}{|\\mathcal{S}|}$, and set the other parameter $\\lambda$ to be zero.\n\n\\subsubsection{Conditional Infomax Feature Extraction~\\citep{lin2006conditional}}\nMIFS and MRMR consider both feature relevance and feature redundancy at the same time. However, some studies~\\cite{lin2006conditional,el2008powerful,guo2009gait} show that in contrast to minimize the feature redundancy, the conditional redundancy between unselected features and already selected features given class labels should be maximized. In other words, as long as the feature redundancy given class labels is stronger than the intra feature redundancy, the feature selection will be affected negatively. A typical feature selection under this argument is Conditional Infomax Feature Extraction (CIFE) criterion, in which the feature score for a new unselected feature $X_{k}$ is:\n\n", "index": 81, "text": "\\begin{equation}\nJ_{CIFE}(X_{k})=I(X_{k};Y)-\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E43.m1\" class=\"ltx_Math\" alttext=\"J_{CIFE}(X_{k})=I(X_{k};Y)-\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\sum_{X_{j%&#10;}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>E</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere both $\\beta$ and $\\lambda$ are equal to zero.\n\n\\subsubsection{Mutual Information Feature Selection~\\citep{battiti1994using}}\nA limitation of MIM feature selection criterion is that it assumes that features are independent of each other. However, in reality, good features should not only be strongly correlated with class labels, but also should not be highly correlated with each other. In other words, the correlation between features should be minimized. Mutual Information Feature Selection (MIFS) criterion considers both the feature relevance and feature redundancy in the feature selection phase, the feature score for a new unselected feature $X_{k}$ can be formulated as follows:\n\n", "itemtype": "equation", "pos": 55913, "prevtext": "\nIt can be observed that in MIM, the scores of features are assessed individually which is independent of other features. Therefore, in MIM, only the feature correlation is considered while the feature redundancy property is completely ignored. After it obtains the MIM feature score for all unselected features, we choose the feature with the highest feature score and add it to the selected feature set. The process repeats until the desired number of selected features are obtained.\n\nIt can also be observed that MIM is a special case of linear combination of Shannon information terms such that:\n\n", "index": 71, "text": "\\begin{equation}\nJ_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E44.m1\" class=\"ltx_Math\" alttext=\"J_{CMI}(X_{k})=I(X_{k};Y)-\\beta\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+%&#10;\\lambda\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y),\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>\u03b2</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>\u03bb</mi><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe basic idea of JMI is that we should include new features that are complementary to the existing features given the class labels.\n\nUnlike previous mentioned approaches that can be directly represented by the linear combination of Shannon information terms, JMI can not be directly reduced to the condition likelihood maximization framework. In~\\citep{brown2012conditional}, the authors demonstrate that with simple manipulations, the JMI criterion can be re-written as:\n\n", "itemtype": "equation", "pos": 60604, "prevtext": "\nby setting $\\beta$ and $\\gamma$ to be one.\n\n\\subsubsection{Joint Mutual Information~\\citep{yang1999data}}\n\nMIFS and MRMR reduce feature redundancy in the feature selection process. An alternative criterion, Joint Mutual Information~\\citep{yang1999data,meyer2008information} was proposed to increase complementary information that are shared between new unselected feature and selected features given the class labels. The feature selection criterion is listed as follows:\n\n", "index": 85, "text": "\\begin{equation}\nJ_{JMI}(X_{k})=\\sum_{X_{j}\\in \\mathcal{S}}I(X_{k},X_{j};Y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E45.m1\" class=\"ltx_Math\" alttext=\"J_{JMI}(X_{k})=\\sum_{X_{j}\\in\\mathcal{S}}I(X_{k},X_{j};Y).\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>J</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>,</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nTherefore, it is also a special case of the linear combination of Shannon information terms by iteratively setting $\\beta$ and $\\lambda$ to be $\\frac{1}{|\\mathcal{S}|}$.\n\n\\subsubsection{Conditional Mutual Information Maximization~\\citep{fleuret2004fast}}\nPreviously mentioned information theoretic feature selection criterion can be reduced to a linear combination of Shannon information terms. Next we show some other algorithms that can only reduce to a non-linear combination of Shannon information terms. Among them, Conditional Mutual Information Maximization (CMIM)~\\citep{vidal2003object,fleuret2004fast} is a criterion which\niteratively selects features which maximize the mutual information with the class labels given the selected features so far. In other words, CMIM does not select the feature that is similar to previous selected ones even though its predictable power for the class labels is strong. Mathematically, during the selection phase, the feature score for each new unselected feature $X_{k}$ can be formulated as follows:\n\n", "itemtype": "equation", "pos": 61169, "prevtext": "\nThe basic idea of JMI is that we should include new features that are complementary to the existing features given the class labels.\n\nUnlike previous mentioned approaches that can be directly represented by the linear combination of Shannon information terms, JMI can not be directly reduced to the condition likelihood maximization framework. In~\\citep{brown2012conditional}, the authors demonstrate that with simple manipulations, the JMI criterion can be re-written as:\n\n", "index": 87, "text": "\\begin{equation}\nJ_{JMI}(X_{k})=I(X_{k};Y)-\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k})+\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E46.m1\" class=\"ltx_Math\" alttext=\"J_{JMI}(X_{k})=I(X_{k};Y)-\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(X%&#10;_{j};X_{k})+\\frac{1}{|\\mathcal{S}|}\\sum_{X_{j}\\in\\mathcal{S}}I(X_{j};X_{k}|Y).\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>J</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nNote that the value of $I(X_{k};Y|X_{j})$ is small if $X_{k}$ is not strongly correlated with the class label $Y$ or if $X_{k}$ is redundant when $\\mathcal{S}$ is known. By selecting the feature that maximizes this minimum value, it can guarantee that the selected feature has strong predictive ability as well as reducing redundancy to the selected features.\n\nThe CMIM criterion is equivalent to the following form after some derivations:\n\n", "itemtype": "equation", "pos": 62405, "prevtext": "\nTherefore, it is also a special case of the linear combination of Shannon information terms by iteratively setting $\\beta$ and $\\lambda$ to be $\\frac{1}{|\\mathcal{S}|}$.\n\n\\subsubsection{Conditional Mutual Information Maximization~\\citep{fleuret2004fast}}\nPreviously mentioned information theoretic feature selection criterion can be reduced to a linear combination of Shannon information terms. Next we show some other algorithms that can only reduce to a non-linear combination of Shannon information terms. Among them, Conditional Mutual Information Maximization (CMIM)~\\citep{vidal2003object,fleuret2004fast} is a criterion which\niteratively selects features which maximize the mutual information with the class labels given the selected features so far. In other words, CMIM does not select the feature that is similar to previous selected ones even though its predictable power for the class labels is strong. Mathematically, during the selection phase, the feature score for each new unselected feature $X_{k}$ can be formulated as follows:\n\n", "index": 89, "text": "\\begin{equation}\nJ_{CMIM}(X_{k})=\\min_{X_{j}\\in \\mathcal{S}}[I(X_{k};Y|X_{j})].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E47.m1\" class=\"ltx_Math\" alttext=\"J_{CMIM}(X_{k})=\\min_{X_{j}\\in\\mathcal{S}}[I(X_{k};Y|X_{j})].\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mi>min</mi><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mo stretchy=\"false\">[</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><msub><mi>X</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nTherefore, CMIM is also a special case of the conditional likelihood maximization framework:\n\n", "itemtype": "equation", "pos": 62940, "prevtext": "\nNote that the value of $I(X_{k};Y|X_{j})$ is small if $X_{k}$ is not strongly correlated with the class label $Y$ or if $X_{k}$ is redundant when $\\mathcal{S}$ is known. By selecting the feature that maximizes this minimum value, it can guarantee that the selected feature has strong predictive ability as well as reducing redundancy to the selected features.\n\nThe CMIM criterion is equivalent to the following form after some derivations:\n\n", "index": 91, "text": "\\begin{equation}\nJ_{CMIM}(X_{k})=I(X_{k};Y)-\\max_{X_{j}\\in\\mathcal{S}}[I(X_{j};X_{k})-I(X_{j};X_{k}|Y)].\n\\label{eq:CMIM}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E48.m1\" class=\"ltx_Math\" alttext=\"J_{CMIM}(X_{k})=I(X_{k};Y)-\\max_{X_{j}\\in\\mathcal{S}}[I(X_{j};X_{k})-I(X_{j};X%&#10;_{k}|Y)].\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><munder><mi>max</mi><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mo stretchy=\"false\">[</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\n\\subsubsection{Informative Fragments~\\citep{vidal2003object}}\nIn~\\citep{vidal2003object}, the authors propose a feature selection criterion called Informative Fragments (IG). The feature score of each new unselected features is given as:\n\n", "itemtype": "equation", "pos": 63169, "prevtext": "\nTherefore, CMIM is also a special case of the conditional likelihood maximization framework:\n\n", "index": 93, "text": "\\begin{equation}\nJ_{cmi}(X_{k})=I(X_{k};Y)+\\sum_{X_{j}\\in\\mathcal{S}}g[I(X_{j};X_{k}),I(X_{j};X_{k}|Y)].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E49.m1\" class=\"ltx_Math\" alttext=\"J_{cmi}(X_{k})=I(X_{k};Y)+\\sum_{X_{j}\\in\\mathcal{S}}g[I(X_{j};X_{k}),I(X_{j};X%&#10;_{k}|Y)].\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>g</mi><mrow><mo stretchy=\"false\">[</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe intuition behind Informative Fragments is that the addition of the new feature $X_{k}$ should maximize the value of conditional mutual information between $X_{k}$ and existing features in $\\mathcal{S}$ over the mutual information between $X_{j}$ and $Y$. An interesting phenomenon of Informative Fragments is that with the chain rule that $I(X_{k}X_{j};Y)=I(X_{j};Y)+I(X_{k};Y|X_{j})$, Informative Fragments has the equivalent form as CMIM, therefore, it can also be reduced to the conditional likelihood maximization framework.\n\n\\subsubsection{Interaction Capping~\\citep{jakulin2005machine}}\nInteraction Capping is a similar feature selection criterion as CMIM in Eq.~(\\ref{eq:CMIM}), but instead of restricting the term $I(X_{j};X_{k})-I(X_{j};X_{k}|Y)$ to be nonnegative:\n\n", "itemtype": "equation", "pos": 63528, "prevtext": "\n\n\\subsubsection{Informative Fragments~\\citep{vidal2003object}}\nIn~\\citep{vidal2003object}, the authors propose a feature selection criterion called Informative Fragments (IG). The feature score of each new unselected features is given as:\n\n", "index": 95, "text": "\\begin{equation}\nJ_{IF}(X_{k})=\\min_{X_{j}\\in\\mathcal{S}}[I(X_{j}X_{k};Y)-I(X_{j};Y)].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E50.m1\" class=\"ltx_Math\" alttext=\"J_{IF}(X_{k})=\\min_{X_{j}\\in\\mathcal{S}}[I(X_{j}X_{k};Y)-I(X_{j};Y)].\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>I</mi><mo>\u2062</mo><mi>F</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>k</mi></msub></mrow><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nApparently, it is a special case of non-linear combination of Shannon information terms by setting the function $g(.)$ to be $\\max[0,I(X_{j};X_{k})-I(X_{j};X_{k}|Y)]$.\n\n\\subsubsection{Double Input Symmetrical Relevance~\\citep{meyer2006use}}\nAnother class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR)~\\citep{meyer2006use} exploits normalization techniques to normalize mutual information~\\citep{guyon2008feature}:\n\n", "itemtype": "equation", "pos": 64409, "prevtext": "\nThe intuition behind Informative Fragments is that the addition of the new feature $X_{k}$ should maximize the value of conditional mutual information between $X_{k}$ and existing features in $\\mathcal{S}$ over the mutual information between $X_{j}$ and $Y$. An interesting phenomenon of Informative Fragments is that with the chain rule that $I(X_{k}X_{j};Y)=I(X_{j};Y)+I(X_{k};Y|X_{j})$, Informative Fragments has the equivalent form as CMIM, therefore, it can also be reduced to the conditional likelihood maximization framework.\n\n\\subsubsection{Interaction Capping~\\citep{jakulin2005machine}}\nInteraction Capping is a similar feature selection criterion as CMIM in Eq.~(\\ref{eq:CMIM}), but instead of restricting the term $I(X_{j};X_{k})-I(X_{j};X_{k}|Y)$ to be nonnegative:\n\n", "index": 97, "text": "\\begin{equation}\nJ_{CMIM}(X_{k})=I(X_{k};Y)-\\sum_{X_{j}\\in\\mathcal{S}}\\max[0,I(X_{j};X_{k})-I(X_{j};X_{k}|Y)].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E51.m1\" class=\"ltx_Math\" alttext=\"J_{CMIM}(X_{k})=I(X_{k};Y)-\\sum_{X_{j}\\in\\mathcal{S}}\\max[0,I(X_{j};X_{k})-I(X%&#10;_{j};X_{k}|Y)].\" display=\"block\"><mrow><msub><mi>J</mi><mrow><mi>C</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>M</mi></mrow></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mi>max</mi><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>;</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">|</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIt is easy to validate that DISR is a non-linear combination of Shannon information terms and can be reduced to the conditional likelihood maximization framework.\n\n\\subsubsection{Fast Correlation Based Filter~\\citep{yu2003feature}}\n\nThere are other information theoretical based feature selection methods that can not be simply be reduced to the unified conditional likelihood maximization framework. Here, we introduce one algorithm named Fast Correlation Based Filter (FCBF)~\\citep{yu2003feature}. It is a filter method that exploits feature-class correlation and feature-feature correlation simultaneously. The algorithm works as follows: (1) given a predefined threshold $\\delta$, it selects a subset of features $\\mathcal{S}$ that is highly correlated with the class label with $SU\\geq \\delta$, where $SU$ is the symmetric uncertainty, the $SU$ between a set of features $X_{\\mathcal{S}}$ and the class label $Y$ is given as follows:\n\n", "itemtype": "equation", "pos": 64994, "prevtext": "\nApparently, it is a special case of non-linear combination of Shannon information terms by setting the function $g(.)$ to be $\\max[0,I(X_{j};X_{k})-I(X_{j};X_{k}|Y)]$.\n\n\\subsubsection{Double Input Symmetrical Relevance~\\citep{meyer2006use}}\nAnother class of information theoretical based methods such as Double Input Symmetrical Relevance (DISR)~\\citep{meyer2006use} exploits normalization techniques to normalize mutual information~\\citep{guyon2008feature}:\n\n", "index": 99, "text": "\\begin{equation}\nJ_{DISR}(X_{k})=\\sum_{X_{j}\\in \\mathcal{S}}\\frac{I(X_{j}X_{k};Y)}{H(X_{j}X_{k}Y)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E52.m1\" class=\"ltx_Math\" alttext=\"J_{DISR}(X_{k})=\\sum_{X_{j}\\in\\mathcal{S}}\\frac{I(X_{j}X_{k};Y)}{H(X_{j}X_{k}Y%&#10;)}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>D</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi>R</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mfrac><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>k</mi></msub></mrow><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>X</mi><mi>k</mi></msub><mo>\u2062</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nA specific feature $X_{k}$ is called predominant iff $SU(X_{k},Y)\\geq \\delta$ and there does not exist a feature $X_{j}\\in\\mathcal{S}(j\\neq k)$ such that $SU(X_{j},X_{k})\\geq SU(X_{k},Y)$. Feature $X_{j}$ is considered to be redundant to feature $X_{k}$ if $SU(X_{j},X_{k})\\geq SU(X_{k},Y)$;  (2) the set of redundant features is denoted as $\\mathcal{S}_{P_{i}}$, which will be further split into $\\mathcal{S}_{P_{i}}^{+}$ and $\\mathcal{S}_{P_{i}}^{-}$ where they contain redundant features to feature $X_{k}$ with $SU(X_{j},Y)>SU(X_{k},Y)$ and $SU(X_{j},Y)<SU(X_{k},Y)$, respectively; and (3) different heuristics are applied on $\\mathcal{S}_{P}$, $\\mathcal{S}_{P_{i}}^{+}$ and $\\mathcal{S}_{P_{i}}^{-}$ to remove reundant features and keep the features that are most relevant features to the class label. Different from feature weighting methods that assign a score to each feature and select the features with the highest score, FCBF is a subset search algorithm which cannot determine the number of selected features.\n\n\\subsection{Sparse Learning based Methods}\n\nFilter feature selection methods select features that are independent of any learning algorithms. However, they do not take into account the bias of the learning algorithms such that the selected features may not be optimal for a specific learning task. To tackle this issue, embedded methods embed the feature selection phase into the learning algorithm construction where these two phases compliment each other. The selected features are suitable for that learning algorithm which can be used for further analysis. There are three main types of embedded feature selection methods: The first type of embedded methods are pruning methods. At the very beginning, they use the whole set of features to train a learning model and then attempt to remove some features by setting the feature coefficients to zero, while maintaining the model performance, one example method in this category is recursive feature elimination methods using support vector machine (SVM)~\\citep{guyon2002gene}. The second type of embedded methods contain a built-in feature selection mechanism such as ID3~\\citep{quinlan1986induction} and C4.5~\\citep{quinlan1993c4}. The third type of methods are sparse learning based methods which aim to minimize the fitting errors along with some sparse regularization terms. The sparse regularizer forces some feature coefficients to be small or exactly zero, and then the corresponding features can be simply eliminated. Sparse learning based methods have received great attention in recent years due to their good performance and interpretability. In the following subsections, we review the sparse learning based feature selection methods in both supervised and unsupervised perspective. We first cover some representative supervised sparse learning based methods and then introduce some unsupervised sparse learning based methods.\n\n\\subsubsection{Feature Selection with $\\ell_{1}$-norm Regularizer (Supervised)~\\citep{tibshirani1996regression,hastie2015statistical}}\n\nFirst, we consider the binary classification ($y_{i}$ is either 0 or 1) or regression problem with only one regression target. Without loss of generality, we only consider linear classification or linear regression model, but it can be easily extended to non-linear problems. The classification label or regression target ${\\bf {y}}$ can be considered as a linear combination of data instances ${\\bf {X}}$ like SVM~\\citep{cortes1995support} and logistic regression~\\citep{hosmer2004applied}. To achieve feature selection, the $\\ell_{1}$-norm penalty term is added on the classification or regression model. One main advantage of $\\ell_{1}$-norm regularization (Lasso)~\\citep{tibshirani1996regression,hastie2015statistical} is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero. This property makes it suitable for feature selection, as we can select features with corresponding non-zero coefficients. Specifically, let ${\\bf {w}}$ denote the model parameter (feature coefficient) that can be obtained by solving following optimization problem:\n\n", "itemtype": "equation", "pos": 66048, "prevtext": "\nIt is easy to validate that DISR is a non-linear combination of Shannon information terms and can be reduced to the conditional likelihood maximization framework.\n\n\\subsubsection{Fast Correlation Based Filter~\\citep{yu2003feature}}\n\nThere are other information theoretical based feature selection methods that can not be simply be reduced to the unified conditional likelihood maximization framework. Here, we introduce one algorithm named Fast Correlation Based Filter (FCBF)~\\citep{yu2003feature}. It is a filter method that exploits feature-class correlation and feature-feature correlation simultaneously. The algorithm works as follows: (1) given a predefined threshold $\\delta$, it selects a subset of features $\\mathcal{S}$ that is highly correlated with the class label with $SU\\geq \\delta$, where $SU$ is the symmetric uncertainty, the $SU$ between a set of features $X_{\\mathcal{S}}$ and the class label $Y$ is given as follows:\n\n", "index": 101, "text": "\\begin{equation}\nSU(X_{\\mathcal{S}},Y)=2\\frac{I(X_{\\mathcal{S}};Y)}{H(X_{\\mathcal{S}})+H(Y)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E53.m1\" class=\"ltx_Math\" alttext=\"SU(X_{\\mathcal{S}},Y)=2\\frac{I(X_{\\mathcal{S}};Y)}{H(X_{\\mathcal{S}})+H(Y)}.\" display=\"block\"><mrow><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mfrac><mrow><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo>;</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>H</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $loss(.)$ is a loss function which denotes a classification or regression model, $penalty({\\bf {w}})$ is a sparse regularization term for feature selection, and $\\alpha$ is a regularization parameter to balance the contribution of the loss function and the regularization term.\n\nSome widely used loss functions $loss(.)$ include least squares, hinge loss and logistic loss. They are defined as follows:\n\\begin{itemize}\n  \\item Least Square Loss:\n  \n", "itemtype": "equation", "pos": 70296, "prevtext": "\nA specific feature $X_{k}$ is called predominant iff $SU(X_{k},Y)\\geq \\delta$ and there does not exist a feature $X_{j}\\in\\mathcal{S}(j\\neq k)$ such that $SU(X_{j},X_{k})\\geq SU(X_{k},Y)$. Feature $X_{j}$ is considered to be redundant to feature $X_{k}$ if $SU(X_{j},X_{k})\\geq SU(X_{k},Y)$;  (2) the set of redundant features is denoted as $\\mathcal{S}_{P_{i}}$, which will be further split into $\\mathcal{S}_{P_{i}}^{+}$ and $\\mathcal{S}_{P_{i}}^{-}$ where they contain redundant features to feature $X_{k}$ with $SU(X_{j},Y)>SU(X_{k},Y)$ and $SU(X_{j},Y)<SU(X_{k},Y)$, respectively; and (3) different heuristics are applied on $\\mathcal{S}_{P}$, $\\mathcal{S}_{P_{i}}^{+}$ and $\\mathcal{S}_{P_{i}}^{-}$ to remove reundant features and keep the features that are most relevant features to the class label. Different from feature weighting methods that assign a score to each feature and select the features with the highest score, FCBF is a subset search algorithm which cannot determine the number of selected features.\n\n\\subsection{Sparse Learning based Methods}\n\nFilter feature selection methods select features that are independent of any learning algorithms. However, they do not take into account the bias of the learning algorithms such that the selected features may not be optimal for a specific learning task. To tackle this issue, embedded methods embed the feature selection phase into the learning algorithm construction where these two phases compliment each other. The selected features are suitable for that learning algorithm which can be used for further analysis. There are three main types of embedded feature selection methods: The first type of embedded methods are pruning methods. At the very beginning, they use the whole set of features to train a learning model and then attempt to remove some features by setting the feature coefficients to zero, while maintaining the model performance, one example method in this category is recursive feature elimination methods using support vector machine (SVM)~\\citep{guyon2002gene}. The second type of embedded methods contain a built-in feature selection mechanism such as ID3~\\citep{quinlan1986induction} and C4.5~\\citep{quinlan1993c4}. The third type of methods are sparse learning based methods which aim to minimize the fitting errors along with some sparse regularization terms. The sparse regularizer forces some feature coefficients to be small or exactly zero, and then the corresponding features can be simply eliminated. Sparse learning based methods have received great attention in recent years due to their good performance and interpretability. In the following subsections, we review the sparse learning based feature selection methods in both supervised and unsupervised perspective. We first cover some representative supervised sparse learning based methods and then introduce some unsupervised sparse learning based methods.\n\n\\subsubsection{Feature Selection with $\\ell_{1}$-norm Regularizer (Supervised)~\\citep{tibshirani1996regression,hastie2015statistical}}\n\nFirst, we consider the binary classification ($y_{i}$ is either 0 or 1) or regression problem with only one regression target. Without loss of generality, we only consider linear classification or linear regression model, but it can be easily extended to non-linear problems. The classification label or regression target ${\\bf {y}}$ can be considered as a linear combination of data instances ${\\bf {X}}$ like SVM~\\citep{cortes1995support} and logistic regression~\\citep{hosmer2004applied}. To achieve feature selection, the $\\ell_{1}$-norm penalty term is added on the classification or regression model. One main advantage of $\\ell_{1}$-norm regularization (Lasso)~\\citep{tibshirani1996regression,hastie2015statistical} is that it forces some feature coefficients to to become smaller and, in some cases, exactly zero. This property makes it suitable for feature selection, as we can select features with corresponding non-zero coefficients. Specifically, let ${\\bf {w}}$ denote the model parameter (feature coefficient) that can be obtained by solving following optimization problem:\n\n", "index": 103, "text": "\\begin{equation}\n{\\bf {w}}={\\arg\\!\\min}_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha \\, penalty({\\bf {w}}),\n\\label{eq:fs_regularization}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E54.m1\" class=\"ltx_Math\" alttext=\"{\\bf{w}}={\\arg\\!\\min}_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha\\,%&#10;penalty({\\bf{w}}),\" display=\"block\"><mrow><mrow><mi>\ud835\udc30</mi><mo>=</mo><mrow><mrow><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  \\item Hinge Loss:\n  \n", "itemtype": "equation", "pos": 70914, "prevtext": "\nwhere $loss(.)$ is a loss function which denotes a classification or regression model, $penalty({\\bf {w}})$ is a sparse regularization term for feature selection, and $\\alpha$ is a regularization parameter to balance the contribution of the loss function and the regularization term.\n\nSome widely used loss functions $loss(.)$ include least squares, hinge loss and logistic loss. They are defined as follows:\n\\begin{itemize}\n  \\item Least Square Loss:\n  \n", "index": 105, "text": "\\begin{equation}\n    loss({\\bf {w}};{\\bf {X}},{\\bf {y}})=\\sum_{i=1}^{n}(y_{i}-{\\bf {w}}'{\\bf {x}}_{i})^{2},\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E55.m1\" class=\"ltx_Math\" alttext=\"loss({\\bf{w}};{\\bf{X}},{\\bf{y}})=\\sum_{i=1}^{n}(y_{i}-{\\bf{w}}^{\\prime}{\\bf{x}%&#10;}_{i})^{2},\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><mrow><msup><mi>\ud835\udc30</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  \\item Logistic Loss:\n  \n", "itemtype": "equation", "pos": 71061, "prevtext": "\n  \\item Hinge Loss:\n  \n", "index": 107, "text": "\\begin{equation}\n    loss({\\bf {w}};{\\bf {X}},{\\bf {y}})=\\sum_{i=1}^{n}\\max(0,1-y_{i}{\\bf {w}}'{\\bf {x}}_{i}),\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E56.m1\" class=\"ltx_Math\" alttext=\"loss({\\bf{w}};{\\bf{X}},{\\bf{y}})=\\sum_{i=1}^{n}\\max(0,1-y_{i}{\\bf{w}}^{\\prime}%&#10;{\\bf{x}}_{i}),\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mrow><mn>1</mn><mo>-</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>\u2062</mo><msup><mi>\ud835\udc30</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\\end{itemize}\n\nAs mentioned above, the most widely used sparse regularization is $\\ell_{1}$-norm regularization, but there are also different types of sparse regularization such as adaptive lasso and elastic net. Next, we briefly introduce these sparse regularization terms.\n\\begin{itemize}\n  \\item Lasso Regularization~\\citep{tibshirani1996regression}:\n  Lasso is short for \\emph{least absolute shrinkage and selection operator}, it is based on the $\\ell_{1}$-norm regularization term on the feature coefficient ${\\bf {w}}$:\n  \n", "itemtype": "equation", "pos": 71214, "prevtext": "\n  \\item Logistic Loss:\n  \n", "index": 109, "text": "\\begin{equation}\n    loss({\\bf {w}};{\\bf {X}},{\\bf {y}})=\\sum_{i=1}^{n}log(1+exp(-y_{i}{\\bf {w}}'{\\bf {x}}_{i})),\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E57.m1\" class=\"ltx_Math\" alttext=\"loss({\\bf{w}};{\\bf{X}},{\\bf{y}})=\\sum_{i=1}^{n}log(1+exp(-y_{i}{\\bf{w}}^{%&#10;\\prime}{\\bf{x}}_{i})),\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>\u2062</mo><msup><mi>\ud835\udc30</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  The $\\ell_{1}$-norm regularization term forces some feature coefficient to be zero and the corresponding features can be simply eliminated since the elimination of these features will not greatly affect the learning performance. After obtaining the feature weight ${\\bf {w}}$ by some optimization algorithms, we can sort the feature importance according to the feature weight -- the higher the feature weight, the more important the feature is.\n  \\item Adaptive Lasso Regularization~\\citep{zou2006adaptive}:\n  The Lasso variable selection phase is consistent if it satisfies non-trivial solutions. However, this condition is difficult to satisfy in some scenarios~\\citep{zhao2006model}. Another critical issue of Lasso is that  the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk~\\citep{fan2001variable}. To tackle these problems, the adaptive Lasso regularization is proposed:\n  \n", "itemtype": "equation", "pos": 71873, "prevtext": "\n\\end{itemize}\n\nAs mentioned above, the most widely used sparse regularization is $\\ell_{1}$-norm regularization, but there are also different types of sparse regularization such as adaptive lasso and elastic net. Next, we briefly introduce these sparse regularization terms.\n\\begin{itemize}\n  \\item Lasso Regularization~\\citep{tibshirani1996regression}:\n  Lasso is short for \\emph{least absolute shrinkage and selection operator}, it is based on the $\\ell_{1}$-norm regularization term on the feature coefficient ${\\bf {w}}$:\n  \n", "index": 111, "text": "\\begin{equation}\n  penalty({\\bf {w}})=||{\\bf {w}}||_{1}=\\sum_{i=1}^{d}|{\\bf {w}}_{i}|.\n  \\label{eq:fs_lassopenalty}\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E58.m1\" class=\"ltx_Math\" alttext=\"penalty({\\bf{w}})=||{\\bf{w}}||_{1}=\\sum_{i=1}^{d}|{\\bf{w}}_{i}|.\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  where ${\\bf {b}}$ is a given weight vector to control the contribution of each feature coefficient in the $\\ell_{1}$-norm penalty term. It is easy to see that adaptive Lasso is a weighted version of Lasso. In~\\citep{zou2006adaptive}, the authors show that the adaptive lasso enjoys the oracle properties and can be solved by the same efficient algorithm for solving the Lasso.\n  \\item Elastic Net Regularization~\\citep{zou2005regularization}:\n  In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications. In addition, in many applications such as bioinformatics, image processing and natural language processing~\\citep{mitra2002unsupervised,segal2003regression,liu2007sparse}, it is common that features may have some strong correlations with each other. However, Lasso tends to randomly select features\n  from a group and discards the others. To handle features with high correlations, Elastic Net regularization~\\citep{zou2005regularization} is proposed as:\n  \n", "itemtype": "equation", "pos": 72970, "prevtext": "\n  The $\\ell_{1}$-norm regularization term forces some feature coefficient to be zero and the corresponding features can be simply eliminated since the elimination of these features will not greatly affect the learning performance. After obtaining the feature weight ${\\bf {w}}$ by some optimization algorithms, we can sort the feature importance according to the feature weight -- the higher the feature weight, the more important the feature is.\n  \\item Adaptive Lasso Regularization~\\citep{zou2006adaptive}:\n  The Lasso variable selection phase is consistent if it satisfies non-trivial solutions. However, this condition is difficult to satisfy in some scenarios~\\citep{zhao2006model}. Another critical issue of Lasso is that  the lasso shrinkage produces biased estimates for the large coefficients, and thus it could be suboptimal in terms of estimation risk~\\citep{fan2001variable}. To tackle these problems, the adaptive Lasso regularization is proposed:\n  \n", "index": 113, "text": "\\begin{equation}\n  penalty({\\bf {w}})=\\sum_{i=1}^{d}\\frac{|{\\bf {w}}_{i}|}{{\\bf {b}}_{i}},\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E59.m1\" class=\"ltx_Math\" alttext=\"penalty({\\bf{w}})=\\sum_{i=1}^{d}\\frac{|{\\bf{w}}_{i}|}{{\\bf{b}}_{i}},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><msub><mi>\ud835\udc1b</mi><mi>i</mi></msub></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  with $0<\\gamma\\leq 1$ and $\\lambda\\geq 0$. The parameters $\\gamma$ and $\\lambda$ are usually set to be 1 such that Elastic Net regularization is simply a combination of $\\ell_{1}$-norm and $\\ell_{2}$-norm regularization.\n\\end{itemize}\n\nIn addition to the different variations of Lasso regularization, another way to obtain a sparse representation of feature coefficients ${\\bf {w}}$ is Dantzig selector; it is based on the normal score equations and controls the correlation of residuals with ${\\bf {X}}$ as:\n\n", "itemtype": "equation", "pos": 74119, "prevtext": "\n  where ${\\bf {b}}$ is a given weight vector to control the contribution of each feature coefficient in the $\\ell_{1}$-norm penalty term. It is easy to see that adaptive Lasso is a weighted version of Lasso. In~\\citep{zou2006adaptive}, the authors show that the adaptive lasso enjoys the oracle properties and can be solved by the same efficient algorithm for solving the Lasso.\n  \\item Elastic Net Regularization~\\citep{zou2005regularization}:\n  In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications. In addition, in many applications such as bioinformatics, image processing and natural language processing~\\citep{mitra2002unsupervised,segal2003regression,liu2007sparse}, it is common that features may have some strong correlations with each other. However, Lasso tends to randomly select features\n  from a group and discards the others. To handle features with high correlations, Elastic Net regularization~\\citep{zou2005regularization} is proposed as:\n  \n", "index": 115, "text": "\\begin{equation}\n  penalty({\\bf {w}})=\\sum_{i=1}^{d}|{\\bf {w}}_{i}|^{\\gamma}+(\\sum_{i=1}^{d}{\\bf {w}}_{i}^{2})^{\\lambda},\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E60.m1\" class=\"ltx_Math\" alttext=\"penalty({\\bf{w}})=\\sum_{i=1}^{d}|{\\bf{w}}_{i}|^{\\gamma}+(\\sum_{i=1}^{d}{\\bf{w}%&#10;}_{i}^{2})^{\\lambda},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mi>\u03b3</mi></msup></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msubsup><mi>\ud835\udc30</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03bb</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $||.||_{\\infty}$ denotes the infinity norm of a vector. Dantzig selector was designed for linear regression models. In~\\citep{candes2007dantzig,james2009dasso}, the authors show that the errors of Dantzig selector is up to a logarithmic factor $log(d)$ ($d$ is the feature dimensionality). Strong theoretical results in~\\citep{james2009dasso} show that LASSO and Dantzig selector are closely related.\n\n\\subsubsection{Feature Selection with $\\ell_{2,1}$-norm Regularizer (Supervised)~\\citep{liu2009multi}}\nAs mentioned above, for binary classification and regression with one target, we can achieve feature selection via $\\ell_{1}$-norm regularization since it will force some feature coefficient to be exact zero. Here, we discuss how to perform feature selection for the general classification problems. The problem is more difficult because of multiple classification or regression targets since we would like the feature selection phase to be consistent over multiple targets. In other words, we want multiple predictive models for different targets to share the similar parameter sparsity patterns -- each feature either has small scores for all data points or has large scores over all data points. This problem can be solved by the $\\ell_{2,1}$-norm regularization which is widely applied in many applications~\\citep{obozinski2007joint,evgeniou2007multi,bi2008improved,zhang2008flexible}. Similar to $\\ell_{1}$-norm regularization, $\\ell_{2,1}$-norm regularization is also convex and a global optimal solution can be achieved. The $\\ell_{2,1}$-norm regularization has strong connections with group lasso~\\citep{yuan2006model} which will be explained later.\n\nAssume that ${\\bf {X}}$ denotes the data matrix, and ${\\bf {y}}$ denotes the label vector such that it contains $s$ different class labels $\\{c_{1},c_{2},...,c_{s}\\}$. First, we can transform the feature vector ${\\bf {y}}$ to one-hot label matrix ${\\bf {Y}}$ such that if ${\\bf {y}}_{i}=c_{j}$ then the only the $j$-th element in the corresponding row vector ${\\bf {Y}}(i,:)$ is 1, other elements are 0. We further assume that the linear classification problem is parameterized by a weight matrix ${\\bf {W}}$ such that the $j$-th column of ${\\bf {W}}$ contains the feature coefficient for the $j$-th class label. If the least square loss function is specified, then the model is formulated as follows:\n\n", "itemtype": "equation", "pos": 74769, "prevtext": "\n  with $0<\\gamma\\leq 1$ and $\\lambda\\geq 0$. The parameters $\\gamma$ and $\\lambda$ are usually set to be 1 such that Elastic Net regularization is simply a combination of $\\ell_{1}$-norm and $\\ell_{2}$-norm regularization.\n\\end{itemize}\n\nIn addition to the different variations of Lasso regularization, another way to obtain a sparse representation of feature coefficients ${\\bf {w}}$ is Dantzig selector; it is based on the normal score equations and controls the correlation of residuals with ${\\bf {X}}$ as:\n\n", "index": 117, "text": "\\begin{equation}\n\\begin{split}\n&\\min_{{\\bf {w}}}||{\\bf {w}}||_{1}\\\\\ns.t. & ||{\\bf {X}}'({\\bf {y}}-{\\bf {Xw}})||_{\\infty}\\leq \\lambda,\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E61.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\min_{{\\bf{w}}}||{\\bf{w}}||_{1}\\\\&#10;\\displaystyle s.t.&amp;\\displaystyle||{\\bf{X}}^{\\prime}({\\bf{y}}-{\\bf{Xw}})||_{%&#10;\\infty}\\leq\\lambda,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><munder><mi>min</mi><mi>\ud835\udc30</mi></munder><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mi>s</mi><mo>.</mo><mi>t</mi></mrow><mo>.</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc32</mi><mo>-</mo><mi>\ud835\udc17\ud835\udc30</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03bb</mi></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the parameter $\\alpha$ is used to control the contribution from the loss function and the regularization term. By solving this optimization problem, we can obtain a sparse matrix ${\\bf {W}}$ where many rows are exact zero or small numbers. The features corresponding to these rows can then be eliminated. By selecting a few number of rows, it achieves joint feature selection among different classification labels.\n\nSimilar to Lasso, we can also exploit different loss functions such as hinge loss and logistic loss. Meanwhile, the $\\ell_{2,1}$-norm regularization can also be modified as adaptive Lasso and elastic net. Generally, the $\\ell_{2,1}$-norm regularization problem can be solved efficiently by the state-of-the-art proximal gradient descent methods~\\citep{Liu:2009:SLEP:manual}. After we obtain the feature coefficient matrix ${\\bf {W}}\\in \\mathbb{R}^{d\\times s}$, we can compute the $\\ell_{2}$-norm of each row vector $||{\\bf {W}}(i,:)||_{2}$ which corresponds to the $i$-th feature -- the larger the value of the $\\ell_{2}$-norm, the more important the feature is.\n\n\\subsubsection{Efficient and Robust Feature Selection (Supervised)~\\citep{nie2010efficient}}\nIn~\\citep{nie2010efficient}, the authors propose an efficient and robust feature selection (REFS) method by employing a joint $\\ell_{2,1}$-norm minimization on both the loss function and the regularization. Their argument is that the $\\ell_{2}$-norm based loss function is sensitive to noisy data while the $\\ell_{2,1}$-norm based loss function is more robust to noise. The reason is that $\\ell_{2,1}$-norm loss function has a rotational invariant property~\\citep{ding2006r}. Consistent with $\\ell_{2,1}$-norm regularized feature selection model, a $\\ell_{2,1}$-norm regularizer is added to the $\\ell_{2,1}$-norm loss function to achieve group sparsity. The objective function of REFS is formulated as follows:\n\n", "itemtype": "equation", "pos": 77302, "prevtext": "\nwhere $||.||_{\\infty}$ denotes the infinity norm of a vector. Dantzig selector was designed for linear regression models. In~\\citep{candes2007dantzig,james2009dasso}, the authors show that the errors of Dantzig selector is up to a logarithmic factor $log(d)$ ($d$ is the feature dimensionality). Strong theoretical results in~\\citep{james2009dasso} show that LASSO and Dantzig selector are closely related.\n\n\\subsubsection{Feature Selection with $\\ell_{2,1}$-norm Regularizer (Supervised)~\\citep{liu2009multi}}\nAs mentioned above, for binary classification and regression with one target, we can achieve feature selection via $\\ell_{1}$-norm regularization since it will force some feature coefficient to be exact zero. Here, we discuss how to perform feature selection for the general classification problems. The problem is more difficult because of multiple classification or regression targets since we would like the feature selection phase to be consistent over multiple targets. In other words, we want multiple predictive models for different targets to share the similar parameter sparsity patterns -- each feature either has small scores for all data points or has large scores over all data points. This problem can be solved by the $\\ell_{2,1}$-norm regularization which is widely applied in many applications~\\citep{obozinski2007joint,evgeniou2007multi,bi2008improved,zhang2008flexible}. Similar to $\\ell_{1}$-norm regularization, $\\ell_{2,1}$-norm regularization is also convex and a global optimal solution can be achieved. The $\\ell_{2,1}$-norm regularization has strong connections with group lasso~\\citep{yuan2006model} which will be explained later.\n\nAssume that ${\\bf {X}}$ denotes the data matrix, and ${\\bf {y}}$ denotes the label vector such that it contains $s$ different class labels $\\{c_{1},c_{2},...,c_{s}\\}$. First, we can transform the feature vector ${\\bf {y}}$ to one-hot label matrix ${\\bf {Y}}$ such that if ${\\bf {y}}_{i}=c_{j}$ then the only the $j$-th element in the corresponding row vector ${\\bf {Y}}(i,:)$ is 1, other elements are 0. We further assume that the linear classification problem is parameterized by a weight matrix ${\\bf {W}}$ such that the $j$-th column of ${\\bf {W}}$ contains the feature coefficient for the $j$-th class label. If the least square loss function is specified, then the model is formulated as follows:\n\n", "index": 119, "text": "\\begin{equation}\n\\min_{{\\bf {W}}}||{\\bf {XW}}-{\\bf {Y}}||_{F}^{2}+\\alpha||{\\bf {W}}||_{2,1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E62.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}}||{\\bf{XW}}-{\\bf{Y}}||_{F}^{2}+\\alpha||{\\bf{W}}||_{2,1},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc18</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe objective function of REFS is difficult to solve since both terms are convex but non-smooth. In~\\citep{nie2010efficient}, an efficient algorithm is proposed to solve this optimization problem with strict convergence analysis.\n\\subsubsection{Multi-Cluster Feature Selection (Unsupervised)~\\citep{cai2010unsupervised}}\nMost of existing sparse learning based approaches build a learning model with the supervision of class labels. The feature selection phase is derived afterwards on the sparse feature coefficients. However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years~\\citep{cai2010unsupervised,yang2011l2,hou2011feature,li2012unsupervised,qian2013robust,liu2014global,du2015unsupervised}. Multi-Cluster Feature Selection (MCFS)~\\citep{cai2010unsupervised} is one of the first unsupervised feature selection algorithm using sparse learning techniques. Without class labels to guide the feature selection process, MCFS proposes to select features that can cover the multi-cluster structure of the data where spectral analysis is used to measure the correlation between different features.\n\nMCFS consists of three steps: (1) the spectral clustering step, (2) sparse coefficient learning step and (3) feature selection step. In the first step, spectral clustering~\\citep{chan1994spectral,ng2002spectral} is applied on the dataset to detect the cluster structure of the data. It first constructs a $k$-nearest neighbor graph to capture the local geometric structure of the data and gets the graph affinity matrix ${\\bf {S}}$, where $k$ is a predefined parameter. There are many different ways to define the affinity matrix. Typical ways include 0-1 weighting, heart kernel weighting and dot-product weighting. For example, if the affinity matrix is built by the heart kernel weighting scheme, ${\\bf {S}}$ is computed as follows:\n\n", "itemtype": "equation", "pos": 79300, "prevtext": "\nwhere the parameter $\\alpha$ is used to control the contribution from the loss function and the regularization term. By solving this optimization problem, we can obtain a sparse matrix ${\\bf {W}}$ where many rows are exact zero or small numbers. The features corresponding to these rows can then be eliminated. By selecting a few number of rows, it achieves joint feature selection among different classification labels.\n\nSimilar to Lasso, we can also exploit different loss functions such as hinge loss and logistic loss. Meanwhile, the $\\ell_{2,1}$-norm regularization can also be modified as adaptive Lasso and elastic net. Generally, the $\\ell_{2,1}$-norm regularization problem can be solved efficiently by the state-of-the-art proximal gradient descent methods~\\citep{Liu:2009:SLEP:manual}. After we obtain the feature coefficient matrix ${\\bf {W}}\\in \\mathbb{R}^{d\\times s}$, we can compute the $\\ell_{2}$-norm of each row vector $||{\\bf {W}}(i,:)||_{2}$ which corresponds to the $i$-th feature -- the larger the value of the $\\ell_{2}$-norm, the more important the feature is.\n\n\\subsubsection{Efficient and Robust Feature Selection (Supervised)~\\citep{nie2010efficient}}\nIn~\\citep{nie2010efficient}, the authors propose an efficient and robust feature selection (REFS) method by employing a joint $\\ell_{2,1}$-norm minimization on both the loss function and the regularization. Their argument is that the $\\ell_{2}$-norm based loss function is sensitive to noisy data while the $\\ell_{2,1}$-norm based loss function is more robust to noise. The reason is that $\\ell_{2,1}$-norm loss function has a rotational invariant property~\\citep{ding2006r}. Consistent with $\\ell_{2,1}$-norm regularized feature selection model, a $\\ell_{2,1}$-norm regularizer is added to the $\\ell_{2,1}$-norm loss function to achieve group sparsity. The objective function of REFS is formulated as follows:\n\n", "index": 121, "text": "\\begin{equation}\n\\min_{{\\bf {W}}}||{\\bf {XW}}-{\\bf {Y}}||_{2,1}+\\alpha||{\\bf {W}}||_{2,1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E63.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}}||{\\bf{XW}}-{\\bf{Y}}||_{2,1}+\\alpha||{\\bf{W}}||_{2,1},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc18</mi></mrow><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {x}}_{i}$ and ${\\bf {x}}_{j}$ are connected instances in the $k$-nearest neighbor graph and the bandwidth $\\sigma$ is a predefined parameter. Then, the graph Laplacian matrix is defined as ${\\bf {L}}={\\bf {D}}-{\\bf {W}}$, where ${\\bf {D}}$ is a diagonal matrix with its diagonal element $D(i,i)=\\sum_{j}{\\bf {W}}(i,j)$. With the graph Laplacian matrix, the flat embedding that unfolds the data manifold can be obtained by solving the following generalized eigen-problem:\n\n", "itemtype": "equation", "pos": 81352, "prevtext": "\nThe objective function of REFS is difficult to solve since both terms are convex but non-smooth. In~\\citep{nie2010efficient}, an efficient algorithm is proposed to solve this optimization problem with strict convergence analysis.\n\\subsubsection{Multi-Cluster Feature Selection (Unsupervised)~\\citep{cai2010unsupervised}}\nMost of existing sparse learning based approaches build a learning model with the supervision of class labels. The feature selection phase is derived afterwards on the sparse feature coefficients. However, since labeled data is costly and time consuming to obtain, unsupervised sparse learning based feature selection has received increasing attention in recent years~\\citep{cai2010unsupervised,yang2011l2,hou2011feature,li2012unsupervised,qian2013robust,liu2014global,du2015unsupervised}. Multi-Cluster Feature Selection (MCFS)~\\citep{cai2010unsupervised} is one of the first unsupervised feature selection algorithm using sparse learning techniques. Without class labels to guide the feature selection process, MCFS proposes to select features that can cover the multi-cluster structure of the data where spectral analysis is used to measure the correlation between different features.\n\nMCFS consists of three steps: (1) the spectral clustering step, (2) sparse coefficient learning step and (3) feature selection step. In the first step, spectral clustering~\\citep{chan1994spectral,ng2002spectral} is applied on the dataset to detect the cluster structure of the data. It first constructs a $k$-nearest neighbor graph to capture the local geometric structure of the data and gets the graph affinity matrix ${\\bf {S}}$, where $k$ is a predefined parameter. There are many different ways to define the affinity matrix. Typical ways include 0-1 weighting, heart kernel weighting and dot-product weighting. For example, if the affinity matrix is built by the heart kernel weighting scheme, ${\\bf {S}}$ is computed as follows:\n\n", "index": 123, "text": "\\begin{equation}\n{\\bf {S}}(i,j)=e^{\\frac{-||{\\bf {x}}_{i}-{\\bf {x}}_{j}||^{2}}{\\sigma}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E64.m1\" class=\"ltx_Math\" alttext=\"{\\bf{S}}(i,j)=e^{\\frac{-||{\\bf{x}}_{i}-{\\bf{x}}_{j}||^{2}}{\\sigma}},\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mfrac><mrow><mo>-</mo><msup><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mn>2</mn></msup></mrow><mi>\u03c3</mi></mfrac></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nLet ${\\bf {E}}=\\{{\\bf {e}}_{1},{\\bf {e}}_{2},...,{\\bf {e}}_{K}\\}$ denote the eigenvectors of the above eigen-problem w.r.t. the smallest $K$ eigenvalues. Each column of ${\\bf {E}}$, i.e., ${\\bf {e}}_{i}$ denotes the $i$-th embedding of the data ${\\bf {X}}$, and $K$ denotes the intrinsic dimensionality of the data that is usually set to the number of clusters if the number of clusters is known in advance.\n\nIn the second step, since the embedding of the data ${\\bf {X}}$ is known, MCFS takes advantage of them to measure the importance of a feature by a regression model with a $\\ell_{1}$-norm regularization. Specifically, given the $i$-th embedding ${\\bf {e}}_{i}$, MCFS regards it as a regression target to minimize the following objective function:\n\n", "itemtype": "equation", "pos": 81939, "prevtext": "\nwhere ${\\bf {x}}_{i}$ and ${\\bf {x}}_{j}$ are connected instances in the $k$-nearest neighbor graph and the bandwidth $\\sigma$ is a predefined parameter. Then, the graph Laplacian matrix is defined as ${\\bf {L}}={\\bf {D}}-{\\bf {W}}$, where ${\\bf {D}}$ is a diagonal matrix with its diagonal element $D(i,i)=\\sum_{j}{\\bf {W}}(i,j)$. With the graph Laplacian matrix, the flat embedding that unfolds the data manifold can be obtained by solving the following generalized eigen-problem:\n\n", "index": 125, "text": "\\begin{equation}\n{\\bf {L}}{\\bf {e}}=\\lambda{\\bf {D}}{\\bf {e}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E65.m1\" class=\"ltx_Math\" alttext=\"{\\bf{L}}{\\bf{e}}=\\lambda{\\bf{D}}{\\bf{e}}.\" display=\"block\"><mrow><mrow><mi>\ud835\udc0b\ud835\udc1e</mi><mo>=</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\ud835\udc03\ud835\udc1e</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {w}}_{i}$ denotes the feature coefficient vector for the $i$-th embedding.\n\nBy solving all $K$ sparse regression problems, MCFS obtains $K$ sparse feature coefficient vectors ${\\bf {W}}=[{\\bf {w}}_{1},...,{\\bf {w}}_{K}]$ and each vector corresponds to one embedding of ${\\bf {X}}$. In the third step, for each feature $f_{j}$, the MCFS score for that feature can be computed as:\n\n", "itemtype": "equation", "pos": 82772, "prevtext": "\nLet ${\\bf {E}}=\\{{\\bf {e}}_{1},{\\bf {e}}_{2},...,{\\bf {e}}_{K}\\}$ denote the eigenvectors of the above eigen-problem w.r.t. the smallest $K$ eigenvalues. Each column of ${\\bf {E}}$, i.e., ${\\bf {e}}_{i}$ denotes the $i$-th embedding of the data ${\\bf {X}}$, and $K$ denotes the intrinsic dimensionality of the data that is usually set to the number of clusters if the number of clusters is known in advance.\n\nIn the second step, since the embedding of the data ${\\bf {X}}$ is known, MCFS takes advantage of them to measure the importance of a feature by a regression model with a $\\ell_{1}$-norm regularization. Specifically, given the $i$-th embedding ${\\bf {e}}_{i}$, MCFS regards it as a regression target to minimize the following objective function:\n\n", "index": 127, "text": "\\begin{equation}\n\\min_{w_{i}}||{\\bf {X}}{\\bf {w}}_{i}-{\\bf {e}}_{i}||_{2}^{2}+\\alpha||{\\bf {w}}_{i}||_{1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E66.m1\" class=\"ltx_Math\" alttext=\"\\min_{w_{i}}||{\\bf{X}}{\\bf{w}}_{i}-{\\bf{e}}_{i}||_{2}^{2}+\\alpha||{\\bf{w}}_{i}%&#10;||_{1},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><msub><mi>w</mi><mi>i</mi></msub></munder><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc17\ud835\udc30</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc1e</mi><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe higher the MCFS score, the more important the feature is.\n\n\\subsubsection{$\\ell_{2,1}$-norm Regularized Discriminative Feature Selection (Unsupervised)~\\citep{yang2011l2}}\nTo perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data~\\citep{he2005laplacian,zhao2007spectral,cai2010unsupervised}. An alternative way is to exploit the discriminative information encoded in the data that has been proven to be effective in many learning tasks~\\citep{fukunaga2013introduction}. In~\\citep{yang2011l2}, the authors propose a new unsupervised feature selection algorithm (UDFS) to select the most discriminative features by exploiting both the discriminative information and feature correlations.\n\nFirst, we briefly introduce discriminative information. Suppose $n$ instances come from $s$ classes and there are $n_{i}$ instances in the $i$-th class. ${\\bf {Y}}\\in\\{0,1\\}^{n\\times s}$ denotes the class label matrix for $n$ instances such that ${\\bf {Y}}(i,j)=1$ if ${\\bf {x}}_{i}$ belongs to the $j$-th class, otherwise ${\\bf {Y}}(i,j)=0$. Let ${\\bf {H}}_{n}={\\bf {I}}_{n}-\\frac{1}{n}{\\bf {1}}_{n}{\\bf {1}}_{n}'$, then the total scatter matrix ${\\bf {S}}_{t}$ and between class scatter matrix ${\\bf {S}}_{b}$ are defined as follows:\n\n", "itemtype": "equation", "pos": 83285, "prevtext": "\nwhere ${\\bf {w}}_{i}$ denotes the feature coefficient vector for the $i$-th embedding.\n\nBy solving all $K$ sparse regression problems, MCFS obtains $K$ sparse feature coefficient vectors ${\\bf {W}}=[{\\bf {w}}_{1},...,{\\bf {w}}_{K}]$ and each vector corresponds to one embedding of ${\\bf {X}}$. In the third step, for each feature $f_{j}$, the MCFS score for that feature can be computed as:\n\n", "index": 129, "text": "\\begin{equation}\nMCFS(j)=\\max_{i}|{\\bf {W}}(j,i)|.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E67.m1\" class=\"ltx_Math\" alttext=\"MCFS(j)=\\max_{i}|{\\bf{W}}(j,i)|.\" display=\"block\"><mrow><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi>i</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>\ud835\udc16</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {\\mu}}=\\frac{{\\bf {x}}_{1}+...+{\\bf {x}}_{n}}{n}$ is the mean of all data instances, ${\\bf {\\mu}}_{i}$ is the mean of all instances in the $i$-th class, $\\tilde{{\\bf {X}}}$ is the centered data matrix such $\\tilde{{\\bf {X}}}={\\bf {H}}_{n}{\\bf {X}}$ and ${\\bf {G}}=[{\\bf {G}}_{1},{\\bf {G}}_{1},...,{\\bf {G}}_{n}]'={\\bf {Y}}({\\bf {Y}}'{\\bf {Y}})^{-\\frac{1}{2}}$ is the weighted label indicator matrix. Linear discriminant analysis aims to obtain a linear transformation matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ that projects ${\\bf {X}}$ from a $d$-dimensional space to a low dimensional space such that ${\\bf {S}}_{t}$ is minimized and ${\\bf {S}}_{b}$ is maximized.\n\nInstead of using global discriminative information, authors in~\\citep{yang2011l2} propose to utilize the local discriminative information~\\citep{sugiyama2006local,yang2010image} to select discriminative features. The advantage of using local discriminative information are two folds. First, it has been demonstrated to be more important than global discriminative information in many classification and clustering tasks. Second, when it considers the local discriminative information, the data manifold structure is also taken into account. For each data instance $x_{i}$, it constructs a $k$-nearest neighbor set for that instance $\\mathcal{N}_{k}(x_{i})=\\{x_{i_{1}},x_{i_{2}},...,x_{i_{k}}\\}$. Let ${\\bf {X}}_{\\mathcal{N}_{k}(i)}=[{\\bf {x}}_{i},{\\bf {x}}_{i_{1}},...,{\\bf {x}}_{i_{k}}]$ denote the local data matrix around $x_{i}$, then the local total scatter matrix ${\\bf {S}}_{t}^{(i)}$ and local between class scatter matrix ${\\bf {S}}_{b}^{(i)}$ are defined as:\n\n", "itemtype": "equation", "pos": 84669, "prevtext": "\nThe higher the MCFS score, the more important the feature is.\n\n\\subsubsection{$\\ell_{2,1}$-norm Regularized Discriminative Feature Selection (Unsupervised)~\\citep{yang2011l2}}\nTo perform unsupervised feature selection, one widely accepted criterion is to select features that best preserve the manifold structure of the data~\\citep{he2005laplacian,zhao2007spectral,cai2010unsupervised}. An alternative way is to exploit the discriminative information encoded in the data that has been proven to be effective in many learning tasks~\\citep{fukunaga2013introduction}. In~\\citep{yang2011l2}, the authors propose a new unsupervised feature selection algorithm (UDFS) to select the most discriminative features by exploiting both the discriminative information and feature correlations.\n\nFirst, we briefly introduce discriminative information. Suppose $n$ instances come from $s$ classes and there are $n_{i}$ instances in the $i$-th class. ${\\bf {Y}}\\in\\{0,1\\}^{n\\times s}$ denotes the class label matrix for $n$ instances such that ${\\bf {Y}}(i,j)=1$ if ${\\bf {x}}_{i}$ belongs to the $j$-th class, otherwise ${\\bf {Y}}(i,j)=0$. Let ${\\bf {H}}_{n}={\\bf {I}}_{n}-\\frac{1}{n}{\\bf {1}}_{n}{\\bf {1}}_{n}'$, then the total scatter matrix ${\\bf {S}}_{t}$ and between class scatter matrix ${\\bf {S}}_{b}$ are defined as follows:\n\n", "index": 131, "text": "\\begin{equation}\n\\begin{split}\n{\\bf {S}}_{t}&=\\sum_{i=1}^{n}({\\bf {x_{i}-{\\bf {\\mu}}}})({\\bf {x_{i}-{\\bf {\\mu}}}})'=\\tilde{{\\bf {X}}}'\\tilde{{\\bf {X}}}\\\\\n{\\bf {S}}_{b}&=\\sum_{i=1}^{c}n_{i}({\\bf {\\mu_{i}-{\\bf {\\mu}}}})({\\bf {\\mu_{i}-{\\bf {\\mu}}}})'=\\tilde{{\\bf {X}}}'{\\bf {G}}{\\bf {G}}'\\tilde{{\\bf {X}}},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E68.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bf{S}}_{t}&amp;\\displaystyle=\\sum_{i=1}^{n}({\\bf{x_{i}%&#10;-{\\bf{\\mu}}}})({\\bf{x_{i}-{\\bf{\\mu}}}})^{\\prime}=\\tilde{{\\bf{X}}}^{\\prime}%&#10;\\tilde{{\\bf{X}}}\\\\&#10;\\displaystyle{\\bf{S}}_{b}&amp;\\displaystyle=\\sum_{i=1}^{c}n_{i}({\\bf{\\mu_{i}-{\\bf{%&#10;\\mu}}}})({\\bf{\\mu_{i}-{\\bf{\\mu}}}})^{\\prime}=\\tilde{{\\bf{X}}}^{\\prime}{\\bf{G}}%&#10;{\\bf{G}}^{\\prime}\\tilde{{\\bf{X}}},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\ud835\udc12</mi><mi>t</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc31</mi><mi>\ud835\udc22</mi></msub><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc31</mi><mi>\ud835\udc22</mi></msub><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><msup><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>\ud835\udc12</mi><mi>b</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mi>\ud835\udc22</mi></msub><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mi>\ud835\udc22</mi></msub><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><msup><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc06\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc17</mi><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\tilde{{\\bf {X}}_{i}}={\\bf {H}}_{k+1}{\\bf {X}}_{i}$ and ${\\bf {G}}_{(i)}=[{\\bf {G}}_{i},{\\bf {G}}_{i_{1}},...,{\\bf {G}}_{i_{k}}]'$. Note that ${\\bf {G}}_{(i)}$ is a subset from ${\\bf {G}}$ and ${\\bf {G}}_{(i)}$ can be obtained by a selection matrix ${\\bf {P}}_{i}\\in\\{0,1\\}^{n\\times (k+1)}$ such that ${\\bf {G}}_{(i)}={\\bf {P}}_{(i)}'{\\bf {G}}$. Without label information in unsupervised feature selection, UDFS assumes that there is a linear classifier ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ to map each data instance ${\\bf {x}}_{i}\\in \\mathbb{R}^{d}$ to a low dimensional space ${\\bf {G}}_{i}\\in \\mathbb{R}^{s}$. Following the definition of global discriminative information~\\citep{yang2010image,fukunaga2013introduction}, the local discriminative score for each instance $x_{i}$ is computed as:\n\n", "itemtype": "equation", "pos": 86649, "prevtext": "\nwhere ${\\bf {\\mu}}=\\frac{{\\bf {x}}_{1}+...+{\\bf {x}}_{n}}{n}$ is the mean of all data instances, ${\\bf {\\mu}}_{i}$ is the mean of all instances in the $i$-th class, $\\tilde{{\\bf {X}}}$ is the centered data matrix such $\\tilde{{\\bf {X}}}={\\bf {H}}_{n}{\\bf {X}}$ and ${\\bf {G}}=[{\\bf {G}}_{1},{\\bf {G}}_{1},...,{\\bf {G}}_{n}]'={\\bf {Y}}({\\bf {Y}}'{\\bf {Y}})^{-\\frac{1}{2}}$ is the weighted label indicator matrix. Linear discriminant analysis aims to obtain a linear transformation matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ that projects ${\\bf {X}}$ from a $d$-dimensional space to a low dimensional space such that ${\\bf {S}}_{t}$ is minimized and ${\\bf {S}}_{b}$ is maximized.\n\nInstead of using global discriminative information, authors in~\\citep{yang2011l2} propose to utilize the local discriminative information~\\citep{sugiyama2006local,yang2010image} to select discriminative features. The advantage of using local discriminative information are two folds. First, it has been demonstrated to be more important than global discriminative information in many classification and clustering tasks. Second, when it considers the local discriminative information, the data manifold structure is also taken into account. For each data instance $x_{i}$, it constructs a $k$-nearest neighbor set for that instance $\\mathcal{N}_{k}(x_{i})=\\{x_{i_{1}},x_{i_{2}},...,x_{i_{k}}\\}$. Let ${\\bf {X}}_{\\mathcal{N}_{k}(i)}=[{\\bf {x}}_{i},{\\bf {x}}_{i_{1}},...,{\\bf {x}}_{i_{k}}]$ denote the local data matrix around $x_{i}$, then the local total scatter matrix ${\\bf {S}}_{t}^{(i)}$ and local between class scatter matrix ${\\bf {S}}_{b}^{(i)}$ are defined as:\n\n", "index": 133, "text": "\\begin{equation}\n\\begin{split}\n{\\bf {S}}_{t}^{(i)}&=\\tilde{{\\bf {X}}_{i}}'\\tilde{{\\bf {X}}_{i}}\\\\\n{\\bf {S}}_{b}^{(i)}&=\\tilde{{\\bf {X}}_{i}}'{\\bf {G}}_{i}{\\bf {G}}_{i}'\\tilde{{\\bf {X}}_{i}},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E69.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bf{S}}_{t}^{(i)}&amp;\\displaystyle=\\tilde{{\\bf{X}}_{i}%&#10;}^{\\prime}\\tilde{{\\bf{X}}_{i}}\\\\&#10;\\displaystyle{\\bf{S}}_{b}^{(i)}&amp;\\displaystyle=\\tilde{{\\bf{X}}_{i}}^{\\prime}{%&#10;\\bf{G}}_{i}{\\bf{G}}_{i}^{\\prime}\\tilde{{\\bf{X}}_{i}},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msubsup><mi>\ud835\udc12</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msup><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msubsup><mi>\ud835\udc12</mi><mi>b</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><mi>i</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc06</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\lambda$ is a small number to make $\\tilde{{\\bf {X}}_{i}}\\tilde{{\\bf {X}}_{i}}'$ invertible. If the instance has a high local discriminative score, it indicates that the linear classifier ${\\bf {W}}$ can discriminate these instances well. Therefore, UDFS tends to train ${\\bf {W}}$ which obtains the highest local discriminative score for all instances in ${\\bf {X}}$; it incorporates a $\\ell_{2,1}$-norm regularizer to achieve feature selection, the objective function is formulated as follows:\n\n", "itemtype": "equation", "pos": 87670, "prevtext": "\nwhere $\\tilde{{\\bf {X}}_{i}}={\\bf {H}}_{k+1}{\\bf {X}}_{i}$ and ${\\bf {G}}_{(i)}=[{\\bf {G}}_{i},{\\bf {G}}_{i_{1}},...,{\\bf {G}}_{i_{k}}]'$. Note that ${\\bf {G}}_{(i)}$ is a subset from ${\\bf {G}}$ and ${\\bf {G}}_{(i)}$ can be obtained by a selection matrix ${\\bf {P}}_{i}\\in\\{0,1\\}^{n\\times (k+1)}$ such that ${\\bf {G}}_{(i)}={\\bf {P}}_{(i)}'{\\bf {G}}$. Without label information in unsupervised feature selection, UDFS assumes that there is a linear classifier ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ to map each data instance ${\\bf {x}}_{i}\\in \\mathbb{R}^{d}$ to a low dimensional space ${\\bf {G}}_{i}\\in \\mathbb{R}^{s}$. Following the definition of global discriminative information~\\citep{yang2010image,fukunaga2013introduction}, the local discriminative score for each instance $x_{i}$ is computed as:\n\n", "index": 135, "text": "\\begin{equation}\n\\begin{split}\nDS_{i}&=tr[({\\bf {S}}_{t}^{(i)}+\\lambda{\\bf {I}}_{d})^{-1}{\\bf {S}}_{b}^{(i)}]\\\\\n&=tr[{\\bf {W}}'{\\bf {X}}'{\\bf {P}}_{(i)}\\tilde{{\\bf {X}}_{i}}'(\\tilde{{\\bf {X}}_{i}}\\tilde{{\\bf {X}}_{i}}'+\\lambda{\\bf {I}}_{d})^{-1}\\tilde{{\\bf {X}}_{i}}{\\bf {P}}_{(i)}'{\\bf {X}}{\\bf {W}}],\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E70.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle DS_{i}&amp;\\displaystyle=tr[({\\bf{S}}_{t}^{(i)}+\\lambda%&#10;{\\bf{I}}_{d})^{-1}{\\bf{S}}_{b}^{(i)}]\\\\&#10;&amp;\\displaystyle=tr[{\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{\\bf{P}}_{(i)}\\tilde{{\\bf{%&#10;X}}_{i}}^{\\prime}(\\tilde{{\\bf{X}}_{i}}\\tilde{{\\bf{X}}_{i}}^{\\prime}+\\lambda{%&#10;\\bf{I}}_{d})^{-1}\\tilde{{\\bf{X}}_{i}}{\\bf{P}}_{(i)}^{\\prime}{\\bf{X}}{\\bf{W}}],%&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>D</mi><mo>\u2062</mo><msub><mi>S</mi><mi>i</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc12</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>d</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc12</mi><mi>b</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2062</mo><msup><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2032</mo></msup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>d</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msubsup><mi>\ud835\udc0f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {G}}_{(i)}'{\\bf {H}}_{k+1}{\\bf {G}}_{(i)}$ is added to avoid overfit and $||{\\bf {W}}||_{2,1}$ will make ${\\bf {W}}$ to be sparse in rows, which is suitable for feature selection, $\\alpha$ is a regularization parameter to control the sparsity of the model. With some mathematical derivation, the objective function in Eq.~(\\ref{eq:UDFS}) can be reformulated as follows:\n\n", "itemtype": "equation", "pos": 88503, "prevtext": "\nwhere $\\lambda$ is a small number to make $\\tilde{{\\bf {X}}_{i}}\\tilde{{\\bf {X}}_{i}}'$ invertible. If the instance has a high local discriminative score, it indicates that the linear classifier ${\\bf {W}}$ can discriminate these instances well. Therefore, UDFS tends to train ${\\bf {W}}$ which obtains the highest local discriminative score for all instances in ${\\bf {X}}$; it incorporates a $\\ell_{2,1}$-norm regularizer to achieve feature selection, the objective function is formulated as follows:\n\n", "index": 137, "text": "\\begin{equation}\n\\min_{{\\bf {W}}'{\\bf {W}}={\\bf {I}}_{d}}\\sum_{i=1}^{n}\\{tr[{\\bf {G}}_{(i)}'{\\bf {H}}_{k+1}{\\bf {G}}_{(i)}-DS_{i}]\\}+\\alpha||{\\bf {W}}||_{2,1},\n\\label{eq:UDFS}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E71.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}^{\\prime}{\\bf{W}}={\\bf{I}}_{d}}\\sum_{i=1}^{n}\\{tr[{\\bf{G}}_{(i)}%&#10;^{\\prime}{\\bf{H}}_{k+1}{\\bf{G}}_{(i)}-DS_{i}]\\}+\\alpha||{\\bf{W}}||_{2,1},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>d</mi></msub></mrow></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy=\"false\">{</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msubsup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc07</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><msub><mi>S</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {M}}={\\bf {X}}'[\\sum_{i=1}^{n}({\\bf {P}}_{i}{\\bf {H}}_{k+1}(\\tilde{{\\bf {X}}_{i}}\\tilde{{\\bf {X}}_{i}}+\\lambda{\\bf {I}}_{k+1})^{-1}{\\bf {H}}_{k+1}{\\bf {S}}_{i}')]{\\bf {X}}$. After we obtain the sparse coefficient matrix, we can rank the features according to its $\\ell_{2}$-norm value and return the top ranked ones.\n\n\\subsubsection{Feature Selection Using Nonnegative Spectral Analysis (Unsupervised)~\\citep{li2012unsupervised}}\nIn addition to UDFS, there are some other ways to exploit discriminative information for unsupervised feature selection. Nonnegative Discriminative Feature Selection (NDFS)~\\citep{hou2011feature} is an algorithm which performs spectral clustering and feature selection simultaneously in a joint framework to select a subset of discriminative features. NDFS assumes that pseudo class label indicators can be obtained by spectral clustering techniques. Different from most existing spectral clustering techniques, NDFS imposes nonnegative and orthogonal constraints during the spectral clustering phase. The argument is that with these constraints, the learnt pseudo class labels are more close to real cluster results. These nonnegative pseudo class labels then act as regression constraints to guide the feature selection phase. Instead of performing these two tasks separately, NDFS incorporates these two phases into a joint framework.\n\nSuppose that these $n$ data instances come from $s$ classes $\\{c_{1},...,c_{s}\\}$ and ${\\bf {Y}}\\in\\{0,1\\}^{n\\times s}$ denotes the class label matrix such that ${\\bf {Y}}(i,j)=1$ if ${\\bf {x}}_{i}$ belongs to $j$-th class, otherwise ${\\bf {Y}}(i,j)=0$. Similar to the definition in UDFS, we use ${\\bf {G}}=[{\\bf {G}}_{1},{\\bf {G}}_{1},...,{\\bf {G}}_{n}]'={\\bf {Y}}({\\bf {Y}}'{\\bf {Y}})^{-\\frac{1}{2}}$ to denote the weight cluster indicator matrix. It is easy to show that for the weight cluster indicator matrix ${\\bf {G}}$, we have ${\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}$. NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved~\\citep{shi2000normalized,yu2003multiclass}. Since the local geometric structure of the dataset can be modeled by a $k$-nearest neighbor graph, the affinity matrix ${\\bf {S}}$ is defined as:\n\n", "itemtype": "equation", "pos": 89076, "prevtext": "\nwhere ${\\bf {G}}_{(i)}'{\\bf {H}}_{k+1}{\\bf {G}}_{(i)}$ is added to avoid overfit and $||{\\bf {W}}||_{2,1}$ will make ${\\bf {W}}$ to be sparse in rows, which is suitable for feature selection, $\\alpha$ is a regularization parameter to control the sparsity of the model. With some mathematical derivation, the objective function in Eq.~(\\ref{eq:UDFS}) can be reformulated as follows:\n\n", "index": 139, "text": "\\begin{equation}\n\\min_{{\\bf {W}}'{\\bf {W}}={\\bf {I}}_{d}}tr({\\bf {W}}'{\\bf {M}}{\\bf {W}})+\\alpha||{\\bf {W}}||_{2,1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E72.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}^{\\prime}{\\bf{W}}={\\bf{I}}_{d}}tr({\\bf{W}}^{\\prime}{\\bf{M}}{\\bf{%&#10;W}})+\\alpha||{\\bf{W}}||_{2,1},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>d</mi></msub></mrow></munder><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0c\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n${\\bf {x}}_{i}$ and ${\\bf {x}}_{j}$ are connected instances in the $k$-nearest neighbor graph, where the bandwidth $\\sigma$ is a predefined parameter. Then the local geometric structure can be fully used by minimizing the following normalized graph Laplacian:\n\n", "itemtype": "equation", "pos": 91483, "prevtext": "\nwhere ${\\bf {M}}={\\bf {X}}'[\\sum_{i=1}^{n}({\\bf {P}}_{i}{\\bf {H}}_{k+1}(\\tilde{{\\bf {X}}_{i}}\\tilde{{\\bf {X}}_{i}}+\\lambda{\\bf {I}}_{k+1})^{-1}{\\bf {H}}_{k+1}{\\bf {S}}_{i}')]{\\bf {X}}$. After we obtain the sparse coefficient matrix, we can rank the features according to its $\\ell_{2}$-norm value and return the top ranked ones.\n\n\\subsubsection{Feature Selection Using Nonnegative Spectral Analysis (Unsupervised)~\\citep{li2012unsupervised}}\nIn addition to UDFS, there are some other ways to exploit discriminative information for unsupervised feature selection. Nonnegative Discriminative Feature Selection (NDFS)~\\citep{hou2011feature} is an algorithm which performs spectral clustering and feature selection simultaneously in a joint framework to select a subset of discriminative features. NDFS assumes that pseudo class label indicators can be obtained by spectral clustering techniques. Different from most existing spectral clustering techniques, NDFS imposes nonnegative and orthogonal constraints during the spectral clustering phase. The argument is that with these constraints, the learnt pseudo class labels are more close to real cluster results. These nonnegative pseudo class labels then act as regression constraints to guide the feature selection phase. Instead of performing these two tasks separately, NDFS incorporates these two phases into a joint framework.\n\nSuppose that these $n$ data instances come from $s$ classes $\\{c_{1},...,c_{s}\\}$ and ${\\bf {Y}}\\in\\{0,1\\}^{n\\times s}$ denotes the class label matrix such that ${\\bf {Y}}(i,j)=1$ if ${\\bf {x}}_{i}$ belongs to $j$-th class, otherwise ${\\bf {Y}}(i,j)=0$. Similar to the definition in UDFS, we use ${\\bf {G}}=[{\\bf {G}}_{1},{\\bf {G}}_{1},...,{\\bf {G}}_{n}]'={\\bf {Y}}({\\bf {Y}}'{\\bf {Y}})^{-\\frac{1}{2}}$ to denote the weight cluster indicator matrix. It is easy to show that for the weight cluster indicator matrix ${\\bf {G}}$, we have ${\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}$. NDFS adopts a strategy to learn the weight cluster matrix such that the local geometric structure of the data can be well preserved~\\citep{shi2000normalized,yu2003multiclass}. Since the local geometric structure of the dataset can be modeled by a $k$-nearest neighbor graph, the affinity matrix ${\\bf {S}}$ is defined as:\n\n", "index": 141, "text": "\\begin{equation}\n{\\bf {S}}(i,j)=e^{\\frac{-||{\\bf {x}}_{i}-{\\bf {x}}_{j}||^{2}}{\\sigma}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E73.m1\" class=\"ltx_Math\" alttext=\"{\\bf{S}}(i,j)=e^{\\frac{-||{\\bf{x}}_{i}-{\\bf{x}}_{j}||^{2}}{\\sigma}}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mfrac><mrow><mo>-</mo><msup><mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo fence=\"true\" maxsize=\"200%\" minsize=\"200%\">||</mo></mrow><mn>2</mn></msup></mrow><mi>\u03c3</mi></mfrac></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {D}}$ is a diagonal matrix with its diagonal element ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$, ${\\bf {L}}={\\bf {D}}^{-\\frac{1}{2}}({\\bf {D}}-{\\bf {S}}){\\bf {D}}^{-\\frac{1}{2}}$ is the normalized Laplacian matrix. Given the pseudo labels ${\\bf {G}}$, NDFS assumes that there exists a linear transformation matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ between the data instances ${\\bf {X}}$ and pseudo labels ${\\bf {G}}$. These pseudo class labels are utilized as constraints to guide the feature selection process by minimizing the following objective function:\n\n", "itemtype": "equation", "pos": 91847, "prevtext": "\n${\\bf {x}}_{i}$ and ${\\bf {x}}_{j}$ are connected instances in the $k$-nearest neighbor graph, where the bandwidth $\\sigma$ is a predefined parameter. Then the local geometric structure can be fully used by minimizing the following normalized graph Laplacian:\n\n", "index": 143, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {G}}}\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}{\\bf {S}}(i,j)&||\\frac{{\\bf {G}}_{i}}{\\sqrt{{\\bf {D}}(i,i)}}-\\frac{{\\bf {G}}_{j}}{\\sqrt{{\\bf {D}}(j,j)}}||_{2}^{2}\\\\\n&=tr({\\bf {G}}'{\\bf {L}}{\\bf {G}}),\n\\end{split}\n\\label{eq:NDFSgraph}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E74.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{G}}}\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{%&#10;n}{\\bf{S}}(i,j)&amp;\\displaystyle||\\frac{{\\bf{G}}_{i}}{\\sqrt{{\\bf{D}}(i,i)}}-\\frac%&#10;{{\\bf{G}}_{j}}{\\sqrt{{\\bf{D}}(j,j)}}||_{2}^{2}\\\\&#10;&amp;\\displaystyle=tr({\\bf{G}}^{\\prime}{\\bf{L}}{\\bf{G}}),\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc06</mi></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mfrac><msub><mi>\ud835\udc06</mi><mi>i</mi></msub><msqrt><mrow><mi>\ud835\udc03</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac><mo>-</mo><mfrac><msub><mi>\ud835\udc06</mi><mi>j</mi></msub><msqrt><mrow><mi>\ud835\udc03</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ is a parameter to control the sparsity of the model. $||{\\bf {W}}||_{2,1}$ ensures that NDFS achieves group sparsity among all $s$ psuedo class labels. Note that NDFS imposes a nonnegative constraint on ${\\bf {G}}$ because with both the orthogonal and nonnegative constraint, there is only one positive element in each row of ${\\bf {G}}$ and all the other elements are zero. In this way, the learnt pseudo class labels ${\\bf {G}}$ are more accurate and can better help to select discriminative features.\n\nBy combining the objective functions in Eq.~(\\ref{eq:NDFSgraph}) and Eq.~(\\ref{eq:NDFSsparse}), the objective function of NDFS is formulated as follows:\n\n", "itemtype": "equation", "pos": 92711, "prevtext": "\nwhere ${\\bf {D}}$ is a diagonal matrix with its diagonal element ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$, ${\\bf {L}}={\\bf {D}}^{-\\frac{1}{2}}({\\bf {D}}-{\\bf {S}}){\\bf {D}}^{-\\frac{1}{2}}$ is the normalized Laplacian matrix. Given the pseudo labels ${\\bf {G}}$, NDFS assumes that there exists a linear transformation matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times s}$ between the data instances ${\\bf {X}}$ and pseudo labels ${\\bf {G}}$. These pseudo class labels are utilized as constraints to guide the feature selection process by minimizing the following objective function:\n\n", "index": 145, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {W}}}&||{\\bf {X}}{\\bf {W}}-{\\bf {G}}||_{F}^{2}+\\alpha||{\\bf {W}}||_{2,1}\\\\\n& \\mbox{s.t.} \\quad {\\bf {{\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}}}, {\\bf {G}}\\geq 0,\n\\end{split}\n\\label{eq:NDFSsparse}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E75.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{W}}}&amp;\\displaystyle||{\\bf{X}}{\\bf{W}}-{\\bf%&#10;{G}}||_{F}^{2}+\\alpha||{\\bf{W}}||_{2,1}\\\\&#10;&amp;\\displaystyle\\mbox{s.t.}\\quad{\\bf{{\\bf{G}}{\\bf{G}}^{\\prime}={\\bf{I}}_{n}}},{%&#10;\\bf{G}}\\geq 0,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mi>\ud835\udc16</mi></munder></mtd><mtd columnalign=\"left\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc06</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>s.t.</mtext><mo separator=\"true\">\u2003</mo><msup><mi>\ud835\udc06\ud835\udc06</mi><mo>\u2032</mo></msup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>\ud835\udc27</mi></msub></mrow><mo>,</mo><mrow><mi>\ud835\udc06</mi><mo>\u2265</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe objective function of NDFS in Eq.~(\\ref{eq:NDFS}) can be solved in an alternating way. After obtaining the feature coefficient matrix ${\\bf {W}}$, NDFS takes the same strategy as UDFS to rank the features according to its $\\ell_{2}$-norm value in ${\\bf {W}}$ and return the top ranked ones.\n\n\\subsubsection{Feature selection via joint embedding learning and sparse regression (Unsupervised)~\\citep{hou2011feature}}\nFeature selection via joint embedding learning and sparse regression (JELSR)~\\citep{hou2011feature} is an unsupervised feature selection that is similar to NDFS. They both embed the pseudo class label learning process into the sparse regression for feature selection. The difference is that NDFS uses graph Laplacian to learn the pseudo class labels, while JELSR utilizes local linear embedding. In addition, NDFS imposes both orthogonal and nonnegative constraints on the pseudo class labels while JELSR only considers the orthogonal constraint.\n\nIn the first step, JELSR builds a $k$-nearest neighbor graph. In the second step, instead of using some explicit affinity matrix ${\\bf {S}}$ like MCFS and NDFS, JELSR takes advantage of the local linear embedding~\\citep{roweis2000nonlinear} to learn the local approximation matrix, i.e., affinity matrix. Specifically, if the $i$-th instance $x_{i}$ and the $j$-th instance $x_{j}$ are connected in the $k$-nearest neighbor graph; its entry in the affinity matrix is positive ${\\bf {S}}(i,j)>0$, otherwise ${\\bf {S}}(i,j)=0$. The nonzero entry can be learnt by solving the following optimization problems:\n\n", "itemtype": "equation", "pos": 93630, "prevtext": "\nwhere $\\alpha$ is a parameter to control the sparsity of the model. $||{\\bf {W}}||_{2,1}$ ensures that NDFS achieves group sparsity among all $s$ psuedo class labels. Note that NDFS imposes a nonnegative constraint on ${\\bf {G}}$ because with both the orthogonal and nonnegative constraint, there is only one positive element in each row of ${\\bf {G}}$ and all the other elements are zero. In this way, the learnt pseudo class labels ${\\bf {G}}$ are more accurate and can better help to select discriminative features.\n\nBy combining the objective functions in Eq.~(\\ref{eq:NDFSgraph}) and Eq.~(\\ref{eq:NDFSsparse}), the objective function of NDFS is formulated as follows:\n\n", "index": 147, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {G}},{\\bf {W}}}\\,tr({\\bf {G}}'{\\bf {L}}{\\bf {G}})&+\\beta(||{\\bf {X}}{\\bf {W}}-{\\bf {G}}||_{F}^{2}+\\alpha||{\\bf {W}}||_{2,1})\\\\\n& \\mbox{s.t.} \\quad {\\bf {{\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}}}, {\\bf {G}}\\geq 0.\n\\end{split}\n\\label{eq:NDFS}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E76.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{G}},{\\bf{W}}}\\,tr({\\bf{G}}^{\\prime}{\\bf{L%&#10;}}{\\bf{G}})&amp;\\displaystyle+\\beta(||{\\bf{X}}{\\bf{W}}-{\\bf{G}}||_{F}^{2}+\\alpha||%&#10;{\\bf{W}}||_{2,1})\\\\&#10;&amp;\\displaystyle\\mbox{s.t.}\\quad{\\bf{{\\bf{G}}{\\bf{G}}^{\\prime}={\\bf{I}}_{n}}},{%&#10;\\bf{G}}\\geq 0.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mrow><mi>\ud835\udc06</mi><mo>,</mo><mi>\ud835\udc16</mi></mrow></munder></mpadded><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc06</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>s.t.</mtext><mo separator=\"true\">\u2003</mo><msup><mi>\ud835\udc06\ud835\udc06</mi><mo>\u2032</mo></msup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>\ud835\udc27</mi></msub></mrow><mo>,</mo><mrow><mi>\ud835\udc06</mi><mo>\u2265</mo><mn>0</mn></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe basic idea is that each instance in ${\\bf {X}}$ can be approximated by a linear combination of some of its $k$-nearest neighbors. JELSR assumes that the low dimensional representation, i.e., pesudo class labels, shares the same local structure as the instances ${\\bf {X}}$ in the high dimensional space, therefore it aims to minimize the following:\n\n", "itemtype": "equation", "pos": 95495, "prevtext": "\nThe objective function of NDFS in Eq.~(\\ref{eq:NDFS}) can be solved in an alternating way. After obtaining the feature coefficient matrix ${\\bf {W}}$, NDFS takes the same strategy as UDFS to rank the features according to its $\\ell_{2}$-norm value in ${\\bf {W}}$ and return the top ranked ones.\n\n\\subsubsection{Feature selection via joint embedding learning and sparse regression (Unsupervised)~\\citep{hou2011feature}}\nFeature selection via joint embedding learning and sparse regression (JELSR)~\\citep{hou2011feature} is an unsupervised feature selection that is similar to NDFS. They both embed the pseudo class label learning process into the sparse regression for feature selection. The difference is that NDFS uses graph Laplacian to learn the pseudo class labels, while JELSR utilizes local linear embedding. In addition, NDFS imposes both orthogonal and nonnegative constraints on the pseudo class labels while JELSR only considers the orthogonal constraint.\n\nIn the first step, JELSR builds a $k$-nearest neighbor graph. In the second step, instead of using some explicit affinity matrix ${\\bf {S}}$ like MCFS and NDFS, JELSR takes advantage of the local linear embedding~\\citep{roweis2000nonlinear} to learn the local approximation matrix, i.e., affinity matrix. Specifically, if the $i$-th instance $x_{i}$ and the $j$-th instance $x_{j}$ are connected in the $k$-nearest neighbor graph; its entry in the affinity matrix is positive ${\\bf {S}}(i,j)>0$, otherwise ${\\bf {S}}(i,j)=0$. The nonzero entry can be learnt by solving the following optimization problems:\n\n", "index": 149, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {S}}}\\sum_{i=1}^{n}&||{\\bf {x}}_{i}-\\sum_{j=1}^{n}{\\bf {S}}(i,j){\\bf {x}}_{j}||_{2}^{2}\\\\\n& \\mbox{s.t.} \\quad \\sum_{j=1}^{n}{\\bf {S}}(i,j)=1.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E77.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{S}}}\\sum_{i=1}^{n}&amp;\\displaystyle||{\\bf{x}%&#10;}_{i}-\\sum_{j=1}^{n}{\\bf{S}}(i,j){\\bf{x}}_{j}||_{2}^{2}\\\\&#10;&amp;\\displaystyle\\mbox{s.t.}\\quad\\sum_{j=1}^{n}{\\bf{S}}(i,j)=1.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><munder><mi>min</mi><mi>\ud835\udc12</mi></munder><mo>\u2062</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mrow></mtd><mtd columnalign=\"left\"><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>s.t.</mtext><mo separator=\"true\">\u2003</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nBy integrating the embedding learning phase into the feature selection phase, the objective function of JELSR is formulated as follows:\n\n", "itemtype": "equation", "pos": 96059, "prevtext": "\nThe basic idea is that each instance in ${\\bf {X}}$ can be approximated by a linear combination of some of its $k$-nearest neighbors. JELSR assumes that the low dimensional representation, i.e., pesudo class labels, shares the same local structure as the instances ${\\bf {X}}$ in the high dimensional space, therefore it aims to minimize the following:\n\n", "index": 151, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {G}}}\\sum_{i=1}^{n}&||{\\bf {G}}_{i}-\\sum_{j=1}^{n}{\\bf {S}}(i,j){\\bf {G}}_{j}||_{2}^{2}=tr({\\bf {G}}'{\\bf {L}}{\\bf {G}})\\\\\n& \\mbox{s.t.} \\quad {\\bf {{\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}}}.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E78.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{G}}}\\sum_{i=1}^{n}&amp;\\displaystyle||{\\bf{G}%&#10;}_{i}-\\sum_{j=1}^{n}{\\bf{S}}(i,j){\\bf{G}}_{j}||_{2}^{2}=tr({\\bf{G}}^{\\prime}{%&#10;\\bf{L}}{\\bf{G}})\\\\&#10;&amp;\\displaystyle\\mbox{s.t.}\\quad{\\bf{{\\bf{G}}{\\bf{G}}^{\\prime}={\\bf{I}}_{n}}}.%&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><munder><mi>min</mi><mi>\ud835\udc06</mi></munder><mo>\u2062</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mrow></mtd><mtd columnalign=\"left\"><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc06</mi><mi>i</mi></msub><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\ud835\udc12</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><mi>j</mi></msub></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>s.t.</mtext><mo separator=\"true\">\u2003</mo><msup><mi>\ud835\udc06\ud835\udc06</mi><mo>\u2032</mo></msup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>\ud835\udc27</mi></msub></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ and $\\beta$ are two balance parameters. The $\\ell_{2,1}$-norm regularization term ensures that ${\\bf {W}}$ is sparse in rows. Similar to UDFS and NDFS, after deriving ${\\bf {W}}$ by some optimization algorithms, it uses $\\ell_{2}$-norm, i.e., ${\\bf {W}}(i,:)$ to rank features. The higher ranked features are relatively more important.\n\n\\subsection{Statistical based Methods}\nAnother category of feature selection algorithms is based on different statistical measures; we group them as statistical based methods in this survey. Since they rely on some statistical measures instead of learning algorithms to evaluate the relevance of features, most of them are filter based methods. In addition, most statistical based algorithms analyze features individually. Hence, feature redundancies is inevitably ignored during the selection phase. We introduce some representative feature selection algorithms in this category. Note that the vast majority algorithms in this category works with discrete data, numerical datasets have to perform discretization first.\n\n\\subsubsection{Low Variance~\\citep{scikit-learn} (Unsupervised)}\nLow Variance is a simple feature selection algorithm which eliminates the feature whose variance is below some threshold. For example, for the features that have the same values on all instances, the variance is 0 and should be removed since it cannot help to discriminate instances from different classes. Suppose that the dataset consists of only boolean features, i.e., the feature values are either 0 and 1, since the boolean features are Bernoulli random variables, its variance value can be computed as:\n\n", "itemtype": "equation", "pos": 96450, "prevtext": "\nBy integrating the embedding learning phase into the feature selection phase, the objective function of JELSR is formulated as follows:\n\n", "index": 153, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {G}},{\\bf {W}}}\\,tr({\\bf {G}}'{\\bf {L}}{\\bf {G}})&+\\beta(||{\\bf {X}}{\\bf {W}}-{\\bf {G}}||_{F}^{2}+\\alpha||{\\bf {W}}||_{2,1})\\\\\n& \\mbox{s.t.} \\quad {\\bf {{\\bf {G}}{\\bf {G}}'={\\bf {I}}_{n}}}.\n\\end{split}\n\\label{eq:JELSR}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E79.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{G}},{\\bf{W}}}\\,tr({\\bf{G}}^{\\prime}{\\bf{L%&#10;}}{\\bf{G}})&amp;\\displaystyle+\\beta(||{\\bf{X}}{\\bf{W}}-{\\bf{G}}||_{F}^{2}+\\alpha||%&#10;{\\bf{W}}||_{2,1})\\\\&#10;&amp;\\displaystyle\\mbox{s.t.}\\quad{\\bf{{\\bf{G}}{\\bf{G}}^{\\prime}={\\bf{I}}_{n}}}.%&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mrow><mi>\ud835\udc06</mi><mo>,</mo><mi>\ud835\udc16</mi></mrow></munder></mpadded><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc06</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc16</mi><mo fence=\"true\">||</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>s.t.</mtext><mo separator=\"true\">\u2003</mo><msup><mi>\ud835\udc06\ud835\udc06</mi><mo>\u2032</mo></msup></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>\ud835\udc27</mi></msub></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $p$ denotes the percentage of instances that take the feature value of 1. After the variance of features are obtained, the feature with a variance score below a predefined threshold can be directly pruned.\n\n\\subsubsection{T-score~\\citep{davis1986statistics} (Supervised)}\n$t$-score is used for binary classification problems. For each feature $f_{i}$, suppose that $\\mu_{1}$ and $\\mu_{2}$ are the mean feature values for the instances from the first class and the second class, $\\sigma_{1}$ and $\\sigma_{2}$ are the corresponding standard deviation values, $n_{1}$ and $n_{2}$ denote the number of instances from these two classes. Then the $t$-score for the feature $f_{i}$ can be computed as:\n\n", "itemtype": "equation", "pos": 98374, "prevtext": "\nwhere $\\alpha$ and $\\beta$ are two balance parameters. The $\\ell_{2,1}$-norm regularization term ensures that ${\\bf {W}}$ is sparse in rows. Similar to UDFS and NDFS, after deriving ${\\bf {W}}$ by some optimization algorithms, it uses $\\ell_{2}$-norm, i.e., ${\\bf {W}}(i,:)$ to rank features. The higher ranked features are relatively more important.\n\n\\subsection{Statistical based Methods}\nAnother category of feature selection algorithms is based on different statistical measures; we group them as statistical based methods in this survey. Since they rely on some statistical measures instead of learning algorithms to evaluate the relevance of features, most of them are filter based methods. In addition, most statistical based algorithms analyze features individually. Hence, feature redundancies is inevitably ignored during the selection phase. We introduce some representative feature selection algorithms in this category. Note that the vast majority algorithms in this category works with discrete data, numerical datasets have to perform discretization first.\n\n\\subsubsection{Low Variance~\\citep{scikit-learn} (Unsupervised)}\nLow Variance is a simple feature selection algorithm which eliminates the feature whose variance is below some threshold. For example, for the features that have the same values on all instances, the variance is 0 and should be removed since it cannot help to discriminate instances from different classes. Suppose that the dataset consists of only boolean features, i.e., the feature values are either 0 and 1, since the boolean features are Bernoulli random variables, its variance value can be computed as:\n\n", "index": 155, "text": "\\begin{equation}\nvariance\\_score(f_{i})=p(1-p),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E80.m1\" class=\"ltx_Math\" alttext=\"variance\\_score(f_{i})=p(1-p),\" display=\"block\"><mrow><mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe basic idea of $t$-score is to assess whether the feature can make the means of two classes to be different statistically by computing the ratio between the mean difference and the variance of two classes. Usually, the higher the $t$-score, the more important the feature is.\n\n\\subsubsection{F-score~\\citep{wright1965interpretation} (Supervised)}\n$t$-score can only be applied for binary classification task, therefore it has some limitations. $f$-score can handle the multi-class situation by testing if a feature is able to well separate samples from different classes. Considering both the within class variance and between class variance, the $f$-score can be computed as follows:\n\n", "itemtype": "equation", "pos": 99138, "prevtext": "\nwhere $p$ denotes the percentage of instances that take the feature value of 1. After the variance of features are obtained, the feature with a variance score below a predefined threshold can be directly pruned.\n\n\\subsubsection{T-score~\\citep{davis1986statistics} (Supervised)}\n$t$-score is used for binary classification problems. For each feature $f_{i}$, suppose that $\\mu_{1}$ and $\\mu_{2}$ are the mean feature values for the instances from the first class and the second class, $\\sigma_{1}$ and $\\sigma_{2}$ are the corresponding standard deviation values, $n_{1}$ and $n_{2}$ denote the number of instances from these two classes. Then the $t$-score for the feature $f_{i}$ can be computed as:\n\n", "index": 157, "text": "\\begin{equation}\nt\\_score(f_{i})=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+\\frac{\\sigma_{2}^{2}}{n_{2}}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E81.m1\" class=\"ltx_Math\" alttext=\"t\\_score(f_{i})=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}}+%&#10;\\frac{\\sigma_{2}^{2}}{n_{2}}}}.\" display=\"block\"><mrow><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>-</mo><msub><mi>\u03bc</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">|</mo></mrow><msqrt><mrow><mfrac><msubsup><mi>\u03c3</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><msubsup><mi>\u03c3</mi><mn>2</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>2</mn></msub></mfrac></mrow></msqrt></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nGiven feature $f_{i}$, $n_{j}$, $\\mu$, $\\mu_{j}$, $\\sigma_{j}$ denote the number of instances from class $j$, the mean feature value, the mean feature value on class $j$, the standard deviation of feature value on class $j$, respectively. Similar to $t$-score, the higher the $t$-score, the more important the feature is.\n\n\\subsubsection{Chi-Square Score~\\citep{liu1995chi2} (Supervised)}\nChi-square score utilizes the test of independence to assess whether the feature is independent of the class label. Given a particular feature with $r$ different feature values, the Chi-square score of that feature can be computed as:\n\n", "itemtype": "equation", "pos": 99966, "prevtext": "\nThe basic idea of $t$-score is to assess whether the feature can make the means of two classes to be different statistically by computing the ratio between the mean difference and the variance of two classes. Usually, the higher the $t$-score, the more important the feature is.\n\n\\subsubsection{F-score~\\citep{wright1965interpretation} (Supervised)}\n$t$-score can only be applied for binary classification task, therefore it has some limitations. $f$-score can handle the multi-class situation by testing if a feature is able to well separate samples from different classes. Considering both the within class variance and between class variance, the $f$-score can be computed as follows:\n\n", "index": 159, "text": "\\begin{equation}\nf\\_score(f_{i})=\\frac{\\sum_{j}\\frac{n_{j}}{c-1}(\\mu_{j}-\\mu)^{2}}{\\frac{1}{n-c}\\sum_{j}(n_{j}-1)\\sigma_{j}^{2}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E82.m1\" class=\"ltx_Math\" alttext=\"f\\_score(f_{i})=\\frac{\\sum_{j}\\frac{n_{j}}{c-1}(\\mu_{j}-\\mu)^{2}}{\\frac{1}{n-c%&#10;}\\sum_{j}(n_{j}-1)\\sigma_{j}^{2}}.\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi></msub><mrow><mfrac><msub><mi>n</mi><mi>j</mi></msub><mrow><mi>c</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bc</mi><mi>j</mi></msub><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>-</mo><mi>c</mi></mrow></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>j</mi></msub><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>n</mi><mi>j</mi></msub><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n_{js}$ is the number of instances with the $j$-th feature value given feature $f_{i}$. In addition, $\\mu_{js}=\\frac{n_{*s}n_{j*}}{n}$, where $n_{j*}$ indicates the number of data instances with the $j$-th feature value given feature $f_{i}$, $n_{*s}$ denotes the number of data instances in class $r$. A higher Chi-square score indicates that the feature is relatively more important.\n\n\\subsubsection{Gini Index~\\citep{gini1912variability} (Supervised)}\nGini index is a statistical measure to quantify if the feature is able to separate instances from different classes. Given a feature $f_{i}$ with $r$ different feature values, for the $j$-th feature value, let $\\mathcal{W}$ denote the set of instances with the feature value smaller than or equal to the $j$-th feature value, let $\\overline{\\mathcal{W}}$ denote the set of instances with the feature value larger than the $j$-th feature value. In other words, the $j$-th feature value can separate the dataset into $\\mathcal{W}$ and $\\overline{\\mathcal{W}}$, then the Gini index score for the feature $f_{i}$ is given as follows:\n\n", "itemtype": "equation", "pos": 100735, "prevtext": "\nGiven feature $f_{i}$, $n_{j}$, $\\mu$, $\\mu_{j}$, $\\sigma_{j}$ denote the number of instances from class $j$, the mean feature value, the mean feature value on class $j$, the standard deviation of feature value on class $j$, respectively. Similar to $t$-score, the higher the $t$-score, the more important the feature is.\n\n\\subsubsection{Chi-Square Score~\\citep{liu1995chi2} (Supervised)}\nChi-square score utilizes the test of independence to assess whether the feature is independent of the class label. Given a particular feature with $r$ different feature values, the Chi-square score of that feature can be computed as:\n\n", "index": 161, "text": "\\begin{equation}\nChi\\_square\\_score(f_{i})=\\sum_{j=1}^{r}\\sum_{s=1}^{c}\\frac{(n_{js}-\\mu_{js})^{2}}{\\mu_{js}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E83.m1\" class=\"ltx_Math\" alttext=\"Chi\\_square\\_score(f_{i})=\\sum_{j=1}^{r}\\sum_{s=1}^{c}\\frac{(n_{js}-\\mu_{js})^%&#10;{2}}{\\mu_{js}},\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>q</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>n</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>s</mi></mrow></msub><mo>-</mo><msub><mi>\u03bc</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>s</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><msub><mi>\u03bc</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>s</mi></mrow></msub></mfrac></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $C_{s}$ indicates that the class label is $s$. $p(.)$ denotes the probability, for instance, $p(C_{s}|\\mathcal{W})$ indicates the conditional probability of class $s$ given the set of $\\mathcal{W}$. In Eq.~(\\ref{eq:giniIndex}), the gini index score is obtained by going through all the possible split $\\mathcal{W}$. Usually for binary classification problem, it can take a maximum value of 0.5, but it can also be used in multi-classification problems. Unlike previous statistical measures, the lower the gini index value, the more relevant the feature is.\n\n\\subsubsection{CFS~\\citep{hall1999feature} (Supervised)}\nThe basic idea of CFS is to use a correlation based heuristic to evaluate the worth of feature subset $\\mathcal{F}$:\n\n", "itemtype": "equation", "pos": 101953, "prevtext": "\nwhere $n_{js}$ is the number of instances with the $j$-th feature value given feature $f_{i}$. In addition, $\\mu_{js}=\\frac{n_{*s}n_{j*}}{n}$, where $n_{j*}$ indicates the number of data instances with the $j$-th feature value given feature $f_{i}$, $n_{*s}$ denotes the number of data instances in class $r$. A higher Chi-square score indicates that the feature is relatively more important.\n\n\\subsubsection{Gini Index~\\citep{gini1912variability} (Supervised)}\nGini index is a statistical measure to quantify if the feature is able to separate instances from different classes. Given a feature $f_{i}$ with $r$ different feature values, for the $j$-th feature value, let $\\mathcal{W}$ denote the set of instances with the feature value smaller than or equal to the $j$-th feature value, let $\\overline{\\mathcal{W}}$ denote the set of instances with the feature value larger than the $j$-th feature value. In other words, the $j$-th feature value can separate the dataset into $\\mathcal{W}$ and $\\overline{\\mathcal{W}}$, then the Gini index score for the feature $f_{i}$ is given as follows:\n\n", "index": 163, "text": "\\begin{equation}\ngini\\_index\\_score(f_{i})=\\min_{\\mathcal{W}}\\left(p(\\mathcal{W})(1-\\sum_{s=1}^{c}p(C_{s}|\\mathcal{W})^{2})+p(\\overline{\\mathcal{W}})(1-\\sum_{s=1}^{c}p(C_{s}|\\overline{\\mathcal{W}})^{2})\\right),\n\\label{eq:giniIndex}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E84.m1\" class=\"ltx_Math\" alttext=\"gini\\_index\\_score(f_{i})=\\min_{\\mathcal{W}}\\left(p(\\mathcal{W})(1-\\sum_{s=1}^%&#10;{c}p(C_{s}|\\mathcal{W})^{2})+p(\\overline{\\mathcal{W}})(1-\\sum_{s=1}^{c}p(C_{s}%&#10;|\\overline{\\mathcal{W}})^{2})\\right),\" display=\"block\"><mrow><mi>g</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mi mathvariant=\"normal\">_</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mi>min</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi></munder><mrow><mo>(</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>p</mi><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi>s</mi></msub><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>-</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>p</mi><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi>s</mi></msub><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb2</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the CFS score shows the heuristic ``merit\" of the feature subset $\\mathcal{F}$ with $k$ features. $\\overline{r_{cf}}$ is the mean feature class correlation and $\\overline{r_{ff}}$ is the average feature-feature intercorrelation. In Eq.~(\\ref{eq:cfsScore}), the numerator indicates the predictive power of the feature set while the denominator shows how much feature redundancy the feature set has. The basic idea of CFS is that a good feature subset should have strong correlation with class labels and are weakly intercorrelated. In order to get the feature-class correlation and feature-feature correlation, CFS uses symmetrical uncertainty~\\citep{vetterling1992numerical} to estimate the degree of associations between two attributes.  Since finding the global optimal feature subset is computational prohibitive, CFS adopts a best-search strategy to find a local optimal feature subset. At the very beginning, it computes the utility of each feature by considering both feature-class and feature-feature correlation with the symmetrical uncertainty measure. It then starts with an empty set and expand the set by the feature with the highest utility until it satisfies a stopping criteria.\n\n\\section{Feature Selection with Structure Features}\n\nExisting feature selection methods for generic data are based on a strong assumption that features are independent of each other while completely ignoring the intrinsic structures among features. For example, these feature selection methods may select the same subset of features even though the features are reshuffled~\\citep{ye2012sparse}. However, in many real applications features also exhibit various kinds of structures, e.g., spatial or temporal smoothness, disjoint groups, overlap groups, trees and graphs~\\citep{tibshirani2005sparsity,jenatton2011structured,yuan2011efficient,huang2011learning,zhou2012modeling}. With the existence of these intrinsic feature structures, feature selection algorithms which incorporate knowledge about the structures of features may help select more relevant features and therefore improve some post-learning tasks such as classification and clustering. One motivating example is from bioinformatics. In the study of arrayCGH such features (the DNA copy numbers along the genome) have some natural spatial order, incorporating such spatial structure can help select more meaningful features and achieve more accurate classification accuracy. Therefore, we discuss some representative feature selection algorithms which explicitly consider feature structures. Specifically, we will focus on group structure, tree structure and graph structure.\n\nSince most existing algorithms in this family are supervised algorithm for binary classification and regression task. Without loss of generality, we focus on linear classification or regression problems such that the class label or regression target ${\\bf {y}}$ can be considered as a linear combination of data instances ${\\bf {X}}$, the linear combination (feature coefficient) is encoded in a vector ${\\bf {w}}$. A popular and successful approach to achieve feature selection with structural features is to minimize a empirical error penalized by a structural regularization term, which can be formulated as:\n\n", "itemtype": "equation", "pos": 102938, "prevtext": "\nwhere $C_{s}$ indicates that the class label is $s$. $p(.)$ denotes the probability, for instance, $p(C_{s}|\\mathcal{W})$ indicates the conditional probability of class $s$ given the set of $\\mathcal{W}$. In Eq.~(\\ref{eq:giniIndex}), the gini index score is obtained by going through all the possible split $\\mathcal{W}$. Usually for binary classification problem, it can take a maximum value of 0.5, but it can also be used in multi-classification problems. Unlike previous statistical measures, the lower the gini index value, the more relevant the feature is.\n\n\\subsubsection{CFS~\\citep{hall1999feature} (Supervised)}\nThe basic idea of CFS is to use a correlation based heuristic to evaluate the worth of feature subset $\\mathcal{F}$:\n\n", "index": 165, "text": "\\begin{equation}\nCFS\\_score(\\mathcal{F})=\\frac{k\\overline{r_{cf}}}{\\sqrt{k+k(k-1)}\\overline{r_{ff}}},\n\\label{eq:cfsScore}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E85.m1\" class=\"ltx_Math\" alttext=\"CFS\\_score(\\mathcal{F})=\\frac{k\\overline{r_{cf}}}{\\sqrt{k+k(k-1)}\\overline{r_{%&#10;ff}}},\" display=\"block\"><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>F</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mover accent=\"true\"><msub><mi>r</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>f</mi></mrow></msub><mo>\u00af</mo></mover></mrow><mrow><msqrt><mrow><mi>k</mi><mo>+</mo><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msqrt><mo>\u2062</mo><mover accent=\"true\"><msub><mi>r</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>f</mi></mrow></msub><mo>\u00af</mo></mover></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\mathcal{G}$ denotes the structures among features and $\\alpha$ is a trade-off parameter between the loss function and the sparse regularization term.\nTo achieve feature selection, $penalty({\\bf {w}},\\mathcal{G})$ is usually set to be a sparse regularization term. Note that the above formulation is similar to that in Eq.~(\\ref{eq:fs_regularization}), the only difference is that for feature selection with structural features, we explicitly consider the structural information $\\mathcal{G}$ among features in the sparse regularization term.\n\n\\subsection{Feature Selection with Group Feature Structures}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.8\\textwidth]{GroupLasso.eps}\n      \\caption{Illustration of Lasso, Group Lasso and Sparse Group Lasso. The whole feature set can be divided into four groups $G_{1}$, $G_{2}$, $G_{3}$ and $G_{4}$. Each column denotes a feature, the column with dark color denotes the selected features while the column with light color denotes the unselected features.}\n\\label{fig:GroupLasso}\n\\end{figure}\n\nIn many real-world applications, features exhibit group structures. One of the most common example is that in multifactor analysis-of-variance (ANOVA), each factor is associated with several groups and can be expressed by a set of dummy features~\\citep{yuan2006model}. Some other examples include different frequency bands represented as groups in signal processing~\\citep{mcauley2005subband} and genes with similar functionalities acting as groups in bioinformatics~\\citep{ma2007supervised}. Therefore, when performing feature selection, it is more appealing to explicitly take into consideration the group structure among features.\n\n\\subsubsection{Group Lasso (Supervised)~\\citep{yuan2006model}}\nGroup Lasso~\\citep{yuan2006model,bach2008consistency,jacob2009group,meier2008group}, which derives feature coefficients from some groups to be exact zero, is a solution to this problem. In other words, it selects or does not select a group of features as a whole. The difference between Lasso and Group Lasso is shown by the toy example in Figure~(\\ref{fig:GroupLasso}). Assume that a total of 25 features come from 4 different groups $\\mathcal{G}=\\{G_{1}$, $G_{2}$, $G_{3}$, $G_{4}\\}$ (each column denotes a feature) and there is no overlap between these groups. Considering the explicit feature structure, we can reorganize the feature coefficients for these 25 features into 4 parts ${\\bf {w}}=\\{{\\bf {w}}_{1};{\\bf {w}}_{2};{\\bf {w}}_{3};{\\bf {w}}_{4}\\}$, where ${\\bf {w}}_{i}$ denotes the feature coefficients for the features from the $i$-th group $G_{i}$. Lasso completely ignores the group structures among features and the selected features (dark column) are across four different groups. In contrast, Group Lasso tends to select or not select the features from different groups as a whole. In the toy example, Group Lasso only selects the second and the fourth group $G_{2}$ and $G_{4}$, features in other two groups $G_{1}$ and $G_{3}$ are not selected. Mathematically, Group Lasso first uses a $\\ell_{2}$-norm regularization term for feature coefficients ${\\bf {w}}_{i}$ in each group $G_{i}$, then it performs a $\\ell_{1}$-norm regularization for previous $\\ell_{2}$-norm terms. The objective function of group lasso is formulated as follows:\n\n", "itemtype": "equation", "pos": 106328, "prevtext": "\nwhere the CFS score shows the heuristic ``merit\" of the feature subset $\\mathcal{F}$ with $k$ features. $\\overline{r_{cf}}$ is the mean feature class correlation and $\\overline{r_{ff}}$ is the average feature-feature intercorrelation. In Eq.~(\\ref{eq:cfsScore}), the numerator indicates the predictive power of the feature set while the denominator shows how much feature redundancy the feature set has. The basic idea of CFS is that a good feature subset should have strong correlation with class labels and are weakly intercorrelated. In order to get the feature-class correlation and feature-feature correlation, CFS uses symmetrical uncertainty~\\citep{vetterling1992numerical} to estimate the degree of associations between two attributes.  Since finding the global optimal feature subset is computational prohibitive, CFS adopts a best-search strategy to find a local optimal feature subset. At the very beginning, it computes the utility of each feature by considering both feature-class and feature-feature correlation with the symmetrical uncertainty measure. It then starts with an empty set and expand the set by the feature with the highest utility until it satisfies a stopping criteria.\n\n\\section{Feature Selection with Structure Features}\n\nExisting feature selection methods for generic data are based on a strong assumption that features are independent of each other while completely ignoring the intrinsic structures among features. For example, these feature selection methods may select the same subset of features even though the features are reshuffled~\\citep{ye2012sparse}. However, in many real applications features also exhibit various kinds of structures, e.g., spatial or temporal smoothness, disjoint groups, overlap groups, trees and graphs~\\citep{tibshirani2005sparsity,jenatton2011structured,yuan2011efficient,huang2011learning,zhou2012modeling}. With the existence of these intrinsic feature structures, feature selection algorithms which incorporate knowledge about the structures of features may help select more relevant features and therefore improve some post-learning tasks such as classification and clustering. One motivating example is from bioinformatics. In the study of arrayCGH such features (the DNA copy numbers along the genome) have some natural spatial order, incorporating such spatial structure can help select more meaningful features and achieve more accurate classification accuracy. Therefore, we discuss some representative feature selection algorithms which explicitly consider feature structures. Specifically, we will focus on group structure, tree structure and graph structure.\n\nSince most existing algorithms in this family are supervised algorithm for binary classification and regression task. Without loss of generality, we focus on linear classification or regression problems such that the class label or regression target ${\\bf {y}}$ can be considered as a linear combination of data instances ${\\bf {X}}$, the linear combination (feature coefficient) is encoded in a vector ${\\bf {w}}$. A popular and successful approach to achieve feature selection with structural features is to minimize a empirical error penalized by a structural regularization term, which can be formulated as:\n\n", "index": 167, "text": "\\begin{equation}\n{\\bf {w}}={\\arg\\!\\min}_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha \\, penalty({\\bf {w}},\\mathcal{G}),\n\\label{eq:fs_structural}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E86.m1\" class=\"ltx_Math\" alttext=\"{\\bf{w}}={\\arg\\!\\min}_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha\\,%&#10;penalty({\\bf{w}},\\mathcal{G}),\" display=\"block\"><mrow><mrow><mi>\ud835\udc30</mi><mo>=</mo><mrow><mrow><mrow><mpadded width=\"-1.7pt\"><mi>arg</mi></mpadded><mo>\u2061</mo><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $g$ is the total number of groups, $h_{i}$ is a weight for the $i$-th group ${\\bf {w}}_{G_{i}}$ which can be considered as a prior to measure the contribution of the $i$-th group in the feature selection process. Through optimizing Eq.~(\\ref{eq:fs_grouplasso}), we obtain the feature coefficients for all the features, these feature coefficients are ranked in a descending order, the higher the value, the more the important the feature is.\n\n\\subsubsection{Sparse Group Lasso (Supervised)~\\citep{friedman2010note,peng2010regularized}}\nOnce Group Lasso selects a group, all the features in the group will be selected. However, for some applications that require the diversity of selected features, Group Lasso is not appropriate anymore. In other words, it is desirable to consider the intrinsic feature structures and select features from different selected groups simultaneously. Sparse Group Lasso~\\citep{friedman2010note} takes advantage of both Lasso and Group Lasso, and it produces a solution with simultaneous intra-group and inter-group sparsity. The sparse regularization term of Sparse Group Lasso is a combination of the penalty term of Lasso and Group Lasso. The formulation of Sparse Group Lasso is formulated as follows:\n\n", "itemtype": "equation", "pos": 109818, "prevtext": "\nwhere $\\mathcal{G}$ denotes the structures among features and $\\alpha$ is a trade-off parameter between the loss function and the sparse regularization term.\nTo achieve feature selection, $penalty({\\bf {w}},\\mathcal{G})$ is usually set to be a sparse regularization term. Note that the above formulation is similar to that in Eq.~(\\ref{eq:fs_regularization}), the only difference is that for feature selection with structural features, we explicitly consider the structural information $\\mathcal{G}$ among features in the sparse regularization term.\n\n\\subsection{Feature Selection with Group Feature Structures}\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.8\\textwidth]{GroupLasso.eps}\n      \\caption{Illustration of Lasso, Group Lasso and Sparse Group Lasso. The whole feature set can be divided into four groups $G_{1}$, $G_{2}$, $G_{3}$ and $G_{4}$. Each column denotes a feature, the column with dark color denotes the selected features while the column with light color denotes the unselected features.}\n\\label{fig:GroupLasso}\n\\end{figure}\n\nIn many real-world applications, features exhibit group structures. One of the most common example is that in multifactor analysis-of-variance (ANOVA), each factor is associated with several groups and can be expressed by a set of dummy features~\\citep{yuan2006model}. Some other examples include different frequency bands represented as groups in signal processing~\\citep{mcauley2005subband} and genes with similar functionalities acting as groups in bioinformatics~\\citep{ma2007supervised}. Therefore, when performing feature selection, it is more appealing to explicitly take into consideration the group structure among features.\n\n\\subsubsection{Group Lasso (Supervised)~\\citep{yuan2006model}}\nGroup Lasso~\\citep{yuan2006model,bach2008consistency,jacob2009group,meier2008group}, which derives feature coefficients from some groups to be exact zero, is a solution to this problem. In other words, it selects or does not select a group of features as a whole. The difference between Lasso and Group Lasso is shown by the toy example in Figure~(\\ref{fig:GroupLasso}). Assume that a total of 25 features come from 4 different groups $\\mathcal{G}=\\{G_{1}$, $G_{2}$, $G_{3}$, $G_{4}\\}$ (each column denotes a feature) and there is no overlap between these groups. Considering the explicit feature structure, we can reorganize the feature coefficients for these 25 features into 4 parts ${\\bf {w}}=\\{{\\bf {w}}_{1};{\\bf {w}}_{2};{\\bf {w}}_{3};{\\bf {w}}_{4}\\}$, where ${\\bf {w}}_{i}$ denotes the feature coefficients for the features from the $i$-th group $G_{i}$. Lasso completely ignores the group structures among features and the selected features (dark column) are across four different groups. In contrast, Group Lasso tends to select or not select the features from different groups as a whole. In the toy example, Group Lasso only selects the second and the fourth group $G_{2}$ and $G_{4}$, features in other two groups $G_{1}$ and $G_{3}$ are not selected. Mathematically, Group Lasso first uses a $\\ell_{2}$-norm regularization term for feature coefficients ${\\bf {w}}_{i}$ in each group $G_{i}$, then it performs a $\\ell_{1}$-norm regularization for previous $\\ell_{2}$-norm terms. The objective function of group lasso is formulated as follows:\n\n", "index": 169, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha \\, \\sum_{i=1}^{g}h_{i}||{\\bf {w}}_{G_{i}}||_{2},\n\\label{eq:fs_grouplasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E87.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha\\,\\sum_{i=1}^{g}h_{i}|%&#10;|{\\bf{w}}_{G_{i}}||_{2},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>g</mi></munderover><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><msub><mi>G</mi><mi>i</mi></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ is parameter between 0 and 1 to balance the contribution of inter-group sparsity and intra-group sparsity for feature selection.\n\nThe differences between Lasso, Group Lasso and Sparse Group Lasso are illustrated in Figure~(\\ref{fig:GroupLasso}):\n\\begin{itemize}\n  \\item Lasso does not consider the group structures among features and selects a subset of relevant features among all groups;\n  \\item Group Lasso performs group selection and selects or not selects a whole group of features;\n  \\item Sparse Group Lasso performs group selection and selects a subset of relevant features in each group.\n\\end{itemize}\n\n\\subsubsection{Overlapping Sparse Group Lasso (Supervised)~\\citep{jacob2009group}}\nAbove methods consider the disjoint group structures among features. However, groups may overlap with each other in some applications~\\citep{jacob2009group,jenatton2011structured,zhao2009composite}. One motivating example is the usage of biologically meaningful gene/protein groups mentioned in~\\citep{ye2012sparse}. Different groups of genes may overlap, i.e., one protein/gene may belong to multiple groups. Under this scenario, Group Lasso and Sparse Group Lasso are not applicable. A general overlapping Sparse Group Lasso regularization is similar to the regularization term of Sparse Group Lasso:\n\n", "itemtype": "equation", "pos": 111227, "prevtext": "\nwhere $g$ is the total number of groups, $h_{i}$ is a weight for the $i$-th group ${\\bf {w}}_{G_{i}}$ which can be considered as a prior to measure the contribution of the $i$-th group in the feature selection process. Through optimizing Eq.~(\\ref{eq:fs_grouplasso}), we obtain the feature coefficients for all the features, these feature coefficients are ranked in a descending order, the higher the value, the more the important the feature is.\n\n\\subsubsection{Sparse Group Lasso (Supervised)~\\citep{friedman2010note,peng2010regularized}}\nOnce Group Lasso selects a group, all the features in the group will be selected. However, for some applications that require the diversity of selected features, Group Lasso is not appropriate anymore. In other words, it is desirable to consider the intrinsic feature structures and select features from different selected groups simultaneously. Sparse Group Lasso~\\citep{friedman2010note} takes advantage of both Lasso and Group Lasso, and it produces a solution with simultaneous intra-group and inter-group sparsity. The sparse regularization term of Sparse Group Lasso is a combination of the penalty term of Lasso and Group Lasso. The formulation of Sparse Group Lasso is formulated as follows:\n\n", "index": 171, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha) \\, \\sum_{i=1}^{g}h_{i}||{\\bf {w}}_{G_{i}}||_{2},\n\\label{eq:fs_grouplasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E88.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i=1}^{g}h_{i}||{\\bf{w}}_{G_{i}}||_{2},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>g</mi></munderover><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><msub><mi>G</mi><mi>i</mi></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe only difference for overlapping Sparse Group Lasso is that different feature groups $G_{i}$ can have a overlap, i.e., there exists at least two groups $G_{i}$ and $G_{j}$ such that $G_{i}\\bigcap G_{j}\\neq \\emptyset$.\n\n\\subsection{Feature Selection with Tree Feature Structures}\nIn addition to group structures, features can also exhibit other kinds of structures such as tree structures. For example, in image processing such as face images, different pixels (features) can be represented a tree, where the root node indicates the whole face, its child nodes can be the different organs, and each specific pixel is considered as a leaf node. In other words, these pixels enjoy a spatial locality structure. Another motivating example is that genes/proteins may form certain hierarchical tree structures~\\citep{liu2010moreau}. Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree~\\citep{kim2010tree,liu2010moreau,jenatton2010proximal}.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.6\\textwidth]{TreeLasso.eps}\n      \\caption{Illustration of tree structure among features. The 8 features form a simple index tree with a depth of 3.}\n\\label{fig:TreeLasso}\n\\end{figure}\n\n\\subsubsection{Tree-guided Group Lasso (Supervised)~\\citep{liu2010moreau}}\nIn Tree-guided Group Lasso, the structure over the features can be represented as a tree with leaf nodes as features. Each internal node denotes a group of features such that the internal node is considered as a root of a subtree and the group of features are considered as leaf nodes. Each internal node in the tree is associated with a weight that represents the height of its subtree, or how tightly the features in this subtree are correlated.\n\nWe follow the definition from~\\citep{liu2010moreau} to define Tree-guided Group Lasso, for an index tree $\\mathcal{G}$ with a depth of $d$, $\\mathcal{G}_{i}=\\{G_{1}^{i},G_{2}^{i},...,G_{n_{i}}^{i}\\}$ denotes the whole set of nodes (features) in the $i$-th level (the root node is defined as the level 0), and $n_{i}$ denotes the number of nodes in the $i$-th level. With these, the nodes in Tree-guided Group Lasso have to satisfy the following two conditions: (1) internal nodes from the same depth level have non-overlapping indices, i.e., $G_{j}^{i}\\bigcap G_{k}^{i}=\\emptyset$, $\\forall i=1,2,...,d$, $j\\neq k$, $i\\leq j, k\\leq n_{i}$; and (2) if $G_{m}^{i-1}$ is the parent node of $G_{j}^{i}$, $G_{j}^{i}\\subseteq G_{m}^{i-1}$.\n\nWe explain these conditions via a toy example in Figure~(\\ref{fig:TreeLasso}). In the figure, we can observe that 8 features are organized in an indexed tree of depth 3. For the internal nodes in each level, we have:\n\n", "itemtype": "equation", "pos": 112737, "prevtext": "\nwhere $\\alpha$ is parameter between 0 and 1 to balance the contribution of inter-group sparsity and intra-group sparsity for feature selection.\n\nThe differences between Lasso, Group Lasso and Sparse Group Lasso are illustrated in Figure~(\\ref{fig:GroupLasso}):\n\\begin{itemize}\n  \\item Lasso does not consider the group structures among features and selects a subset of relevant features among all groups;\n  \\item Group Lasso performs group selection and selects or not selects a whole group of features;\n  \\item Sparse Group Lasso performs group selection and selects a subset of relevant features in each group.\n\\end{itemize}\n\n\\subsubsection{Overlapping Sparse Group Lasso (Supervised)~\\citep{jacob2009group}}\nAbove methods consider the disjoint group structures among features. However, groups may overlap with each other in some applications~\\citep{jacob2009group,jenatton2011structured,zhao2009composite}. One motivating example is the usage of biologically meaningful gene/protein groups mentioned in~\\citep{ye2012sparse}. Different groups of genes may overlap, i.e., one protein/gene may belong to multiple groups. Under this scenario, Group Lasso and Sparse Group Lasso are not applicable. A general overlapping Sparse Group Lasso regularization is similar to the regularization term of Sparse Group Lasso:\n\n", "index": 173, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha) \\, \\sum_{i=1}^{g}h_{i}||{\\bf {w}}_{G_{i}}||_{2}.\n\\label{eq:fs_grouplasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E89.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i=1}^{g}h_{i}||{\\bf{w}}_{G_{i}}||_{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>g</mi></munderover><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><msub><mi>G</mi><mi>i</mi></msub></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\nIt can be observed from the figure that $G_{1}^{0}$ is the root node of the index tree which includes 8 features. In addition, internal nodes from the same level do not overlap while the parent node and the child node have some overlap such that the features of the child node is a subset of these of the parent node.\n\nWith the definition of the index tree, the objective function of Tree-guided Group Lasso is formulated as follows:\n\n", "itemtype": "equation", "pos": 115677, "prevtext": "\nThe only difference for overlapping Sparse Group Lasso is that different feature groups $G_{i}$ can have a overlap, i.e., there exists at least two groups $G_{i}$ and $G_{j}$ such that $G_{i}\\bigcap G_{j}\\neq \\emptyset$.\n\n\\subsection{Feature Selection with Tree Feature Structures}\nIn addition to group structures, features can also exhibit other kinds of structures such as tree structures. For example, in image processing such as face images, different pixels (features) can be represented a tree, where the root node indicates the whole face, its child nodes can be the different organs, and each specific pixel is considered as a leaf node. In other words, these pixels enjoy a spatial locality structure. Another motivating example is that genes/proteins may form certain hierarchical tree structures~\\citep{liu2010moreau}. Recently, Tree-guided Group Lasso is proposed to handle the feature selection for features that can be represented in an index tree~\\citep{kim2010tree,liu2010moreau,jenatton2010proximal}.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.6\\textwidth]{TreeLasso.eps}\n      \\caption{Illustration of tree structure among features. The 8 features form a simple index tree with a depth of 3.}\n\\label{fig:TreeLasso}\n\\end{figure}\n\n\\subsubsection{Tree-guided Group Lasso (Supervised)~\\citep{liu2010moreau}}\nIn Tree-guided Group Lasso, the structure over the features can be represented as a tree with leaf nodes as features. Each internal node denotes a group of features such that the internal node is considered as a root of a subtree and the group of features are considered as leaf nodes. Each internal node in the tree is associated with a weight that represents the height of its subtree, or how tightly the features in this subtree are correlated.\n\nWe follow the definition from~\\citep{liu2010moreau} to define Tree-guided Group Lasso, for an index tree $\\mathcal{G}$ with a depth of $d$, $\\mathcal{G}_{i}=\\{G_{1}^{i},G_{2}^{i},...,G_{n_{i}}^{i}\\}$ denotes the whole set of nodes (features) in the $i$-th level (the root node is defined as the level 0), and $n_{i}$ denotes the number of nodes in the $i$-th level. With these, the nodes in Tree-guided Group Lasso have to satisfy the following two conditions: (1) internal nodes from the same depth level have non-overlapping indices, i.e., $G_{j}^{i}\\bigcap G_{k}^{i}=\\emptyset$, $\\forall i=1,2,...,d$, $j\\neq k$, $i\\leq j, k\\leq n_{i}$; and (2) if $G_{m}^{i-1}$ is the parent node of $G_{j}^{i}$, $G_{j}^{i}\\subseteq G_{m}^{i-1}$.\n\nWe explain these conditions via a toy example in Figure~(\\ref{fig:TreeLasso}). In the figure, we can observe that 8 features are organized in an indexed tree of depth 3. For the internal nodes in each level, we have:\n\n", "index": 175, "text": "\\begin{equation}\n\\begin{split}\nG_{1}^{0}&=\\{f_{1},f_{2},f_{3},f_{4},f_{5},f_{6},f_{7},f_{8}\\}\\\\\nG_{1}^{1}&=\\{f_{1},f_{2}\\},G_{1}^{2}=\\{f_{3},f_{4},f_{5},f_{6},f_{7}\\},G_{1}^{3}=\\{f_{8}\\}\\\\\nG_{1}^{2}&=\\{f_{1},f_{2}\\},G_{2}^{2}=\\{f_{3},f_{4}\\},G_{2}^{2}=\\{f_{5},f_{6},f_{7}\\}.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E90.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle G_{1}^{0}&amp;\\displaystyle=\\{f_{1},f_{2},f_{3},f_{4},f%&#10;_{5},f_{6},f_{7},f_{8}\\}\\\\&#10;\\displaystyle G_{1}^{1}&amp;\\displaystyle=\\{f_{1},f_{2}\\},G_{1}^{2}=\\{f_{3},f_{4},%&#10;f_{5},f_{6},f_{7}\\},G_{1}^{3}=\\{f_{8}\\}\\\\&#10;\\displaystyle G_{1}^{2}&amp;\\displaystyle=\\{f_{1},f_{2}\\},G_{2}^{2}=\\{f_{3},f_{4}%&#10;\\},G_{2}^{2}=\\{f_{5},f_{6},f_{7}\\}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msubsup><mi>G</mi><mn>1</mn><mn>0</mn></msubsup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><msub><mi>f</mi><mn>3</mn></msub><mo>,</mo><msub><mi>f</mi><mn>4</mn></msub><mo>,</mo><msub><mi>f</mi><mn>5</mn></msub><mo>,</mo><msub><mi>f</mi><mn>6</mn></msub><mo>,</mo><msub><mi>f</mi><mn>7</mn></msub><mo>,</mo><msub><mi>f</mi><mn>8</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msubsup><mi>G</mi><mn>1</mn><mn>1</mn></msubsup></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo><mrow><mrow><msubsup><mi>G</mi><mn>1</mn><mn>2</mn></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>3</mn></msub><mo>,</mo><msub><mi>f</mi><mn>4</mn></msub><mo>,</mo><msub><mi>f</mi><mn>5</mn></msub><mo>,</mo><msub><mi>f</mi><mn>6</mn></msub><mo>,</mo><msub><mi>f</mi><mn>7</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo><mrow><msubsup><mi>G</mi><mn>1</mn><mn>3</mn></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>8</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msubsup><mi>G</mi><mn>1</mn><mn>2</mn></msubsup></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo><mrow><mrow><msubsup><mi>G</mi><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>3</mn></msub><mo>,</mo><msub><mi>f</mi><mn>4</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo><mrow><msubsup><mi>G</mi><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>5</mn></msub><mo>,</mo><msub><mi>f</mi><mn>6</mn></msub><mo>,</mo><msub><mi>f</mi><mn>7</mn></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha\\geq 0$ is a regularization parameter and $h_{j}^{i}\\geq 0$ is pre-defined parameter to measure the contribution of the internal node $G_{j}^{i}$. Since parent node is a superset of its child nodes, thus, if a parent node is not selected (i.e., the corresponding model coefficient is zero), all of its child nodes will not be selected. For example, as illustrated in Figure~(\\ref{fig:TreeLasso}), if the internal node $G_{2}^{1}$ is not selected, both of its child nodes $G_{2}^{2}$ and $G_{3}^{2}$ will not be selected.\n\n\\subsection{Feature Selection with Graph Feature Structures}\nIn many real-world applications, features may have strong dependencies. For example, in natural language processing, if we take each word as a feature, we have synonyms and antonyms relationships between different words~\\citep{fellbaum1998wordnet}. Moreover, many biological studies show that genes tend to work in groups according to their biological functions, and there are strong dependencies between some genes. Since features show some dependencies in these cases, we can model the features by an undirected graph, where nodes represent features and edges among nodes show the pairwise dependencies between features. Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features~\\citep{sandler2009regularized,kim2009statistical,yang2012feature}.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.7\\textwidth]{GraphLasso.eps}\n      \\caption{A toy example of graph of 7 features $\\{f_{1},f_{2},...,f_{7}\\}$ and its dependencies information encoded in an adjacency matrix ${\\bf {A}}$.}\n\\label{fig:GraphLasso}\n\\end{figure}\n\nWhen there exists some dependencies among features, we can use a undirected graph $\\mathcal{G}(N,E)$ to encode these dependencies. Assume that there are $n$ nodes $N=\\{N_{1},N_{2},...,N_{n}\\}$ and a set of $E$ edges $\\{E_{1},E_{2},...,E_{e}\\}$ in $\\mathcal{G}(N,E)$. Then node $N_{i}$ corresponds to the $i$-th feature and the pairwise feature dependencies can be represented by an adjacency matrix ${\\bf {A}}\\in\\mathbb{R}^{N_{n}\\times N_{n}}$ of $\\mathcal{G}(N,E)$. Figure~(\\ref{fig:GraphLasso}) shows an example of the graph with 7 features $\\{f_{1},f_{2},...,f_{7}\\}$ and its pairwise dependence information encoded in an adjacency matrix ${\\bf {A}}$. Note that entries in ${\\bf {A}}$ does not necessarily have to be 0 or 1, it can be any numerical numbers that can reflect the correlations between features.\n\n\\subsubsection{Laplacian Lasso (Supervised)~\\citep{ye2012sparse}}\nSince features exhibit graph structures, when two nodes (features) $N_{i}$ and $N_{j}$ are connected by an edge in $\\mathcal{G}(N,E)$, the features $f_{i}$ and $f_{j}$ are more likely to be selected together, and they should have similar feature coefficients. One way to achieve this target is via Graph Lasso -- adding a graph regularizer on the feature graphs on the basis of Lasso. The formulation is as follows:\n\n", "itemtype": "equation", "pos": 116414, "prevtext": "\n\nIt can be observed from the figure that $G_{1}^{0}$ is the root node of the index tree which includes 8 features. In addition, internal nodes from the same level do not overlap while the parent node and the child node have some overlap such that the features of the child node is a subset of these of the parent node.\n\nWith the definition of the index tree, the objective function of Tree-guided Group Lasso is formulated as follows:\n\n", "index": 177, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha\\, \\sum_{i=0}^{d}\\sum_{j=1}^{n_{i}}h_{j}^{i}||{\\bf {w}}_{G_{j}^{i}}||_{2},\n\\label{eq:fs_treelasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E91.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha\\,\\sum_{i=0}^{d}\\sum_{%&#10;j=1}^{n_{i}}h_{j}^{i}||{\\bf{w}}_{G_{j}^{i}}||_{2},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>d</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><mrow><msubsup><mi>h</mi><mi>j</mi><mi>i</mi></msubsup><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc30</mi><msubsup><mi>G</mi><mi>j</mi><mi>i</mi></msubsup></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the first regularization term $\\alpha||{\\bf {w}}||_{1}$ is the same as the regularization term as Lasso while the second term enforces that if a pair of features shows strong dependencies, i.e., large ${\\bf {A}}(i,j)$, their feature coefficients should also be close to each other. We can reformulate above loss function into a matrix format:\n\n", "itemtype": "equation", "pos": 119624, "prevtext": "\nwhere $\\alpha\\geq 0$ is a regularization parameter and $h_{j}^{i}\\geq 0$ is pre-defined parameter to measure the contribution of the internal node $G_{j}^{i}$. Since parent node is a superset of its child nodes, thus, if a parent node is not selected (i.e., the corresponding model coefficient is zero), all of its child nodes will not be selected. For example, as illustrated in Figure~(\\ref{fig:TreeLasso}), if the internal node $G_{2}^{1}$ is not selected, both of its child nodes $G_{2}^{2}$ and $G_{3}^{2}$ will not be selected.\n\n\\subsection{Feature Selection with Graph Feature Structures}\nIn many real-world applications, features may have strong dependencies. For example, in natural language processing, if we take each word as a feature, we have synonyms and antonyms relationships between different words~\\citep{fellbaum1998wordnet}. Moreover, many biological studies show that genes tend to work in groups according to their biological functions, and there are strong dependencies between some genes. Since features show some dependencies in these cases, we can model the features by an undirected graph, where nodes represent features and edges among nodes show the pairwise dependencies between features. Recent studies have shown that the learning performance can be improved if we explicitly take into account the dependency information among features~\\citep{sandler2009regularized,kim2009statistical,yang2012feature}.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.7\\textwidth]{GraphLasso.eps}\n      \\caption{A toy example of graph of 7 features $\\{f_{1},f_{2},...,f_{7}\\}$ and its dependencies information encoded in an adjacency matrix ${\\bf {A}}$.}\n\\label{fig:GraphLasso}\n\\end{figure}\n\nWhen there exists some dependencies among features, we can use a undirected graph $\\mathcal{G}(N,E)$ to encode these dependencies. Assume that there are $n$ nodes $N=\\{N_{1},N_{2},...,N_{n}\\}$ and a set of $E$ edges $\\{E_{1},E_{2},...,E_{e}\\}$ in $\\mathcal{G}(N,E)$. Then node $N_{i}$ corresponds to the $i$-th feature and the pairwise feature dependencies can be represented by an adjacency matrix ${\\bf {A}}\\in\\mathbb{R}^{N_{n}\\times N_{n}}$ of $\\mathcal{G}(N,E)$. Figure~(\\ref{fig:GraphLasso}) shows an example of the graph with 7 features $\\{f_{1},f_{2},...,f_{7}\\}$ and its pairwise dependence information encoded in an adjacency matrix ${\\bf {A}}$. Note that entries in ${\\bf {A}}$ does not necessarily have to be 0 or 1, it can be any numerical numbers that can reflect the correlations between features.\n\n\\subsubsection{Laplacian Lasso (Supervised)~\\citep{ye2012sparse}}\nSince features exhibit graph structures, when two nodes (features) $N_{i}$ and $N_{j}$ are connected by an edge in $\\mathcal{G}(N,E)$, the features $f_{i}$ and $f_{j}$ are more likely to be selected together, and they should have similar feature coefficients. One way to achieve this target is via Graph Lasso -- adding a graph regularizer on the feature graphs on the basis of Lasso. The formulation is as follows:\n\n", "index": 179, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha)\\, \\sum_{i,j}{\\bf {A}}(i,j)({\\bf {w}}_{i}-{\\bf {w}}_{j})^{2},\n\\label{eq:fs_graphlasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E92.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i,j}{\\bf{A}}(i,j)({\\bf{w}}_{i}-{\\bf{w}}_{j})^{2},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {L}}={\\bf {D}}-{\\bf {A}}$ is the Laplacian matrix and ${\\bf {D}}$ is a diagonal matrix with ${\\bf {D}}(i,i)=\\sum_{j}^{N_{i}}{\\bf {A}}(i,j)$. The Laplacian matrix in Eq.~(\\ref{eq:fs_graphlassomatrix}) is positive semi-definite and captures the local geometric structure of features. It can be observed that when the Laplacian matrix is the identity matrix ${\\bf {I}}$, ${\\bf {w}}'{\\bf {L}}{\\bf {w}}=||{\\bf {w}}||_{2}^{2}$, the penalty term in Eq.~(\\ref{eq:fs_graphlassomatrix}) reduces to the elastic net penalty~\\citep{zou2005regularization}. Since the graph regularization term ${\\bf {w}}'{\\bf {L}}{\\bf {w}}$ is convex and differentiable, existing efficient algorithms like LARS~\\citep{efron2004least} and proximal gradient descent methods~\\citep{liu2009efficient} can be directly applied.\n\n\\subsubsection{GFLasso (Supervised)~\\citep{kim2009statistical}}\nIn Eq.~(\\ref{eq:fs_graphlassomatrix}), graph feature structures are represented by an unsigned graph, and it encourages features connected together with similar feature coefficients. However, in many cases, features can also be negatively correlated. In this case, the feature graph $\\mathcal{G}(N,E)$ is represented by a signed graph, with both positive and negative edges. Recently, GFLasso is proposed to explicitly consider both the positive and negative feature correlations, the objective function of GFLasso is formulated as follows:\n\n", "itemtype": "equation", "pos": 120181, "prevtext": "\nwhere the first regularization term $\\alpha||{\\bf {w}}||_{1}$ is the same as the regularization term as Lasso while the second term enforces that if a pair of features shows strong dependencies, i.e., large ${\\bf {A}}(i,j)$, their feature coefficients should also be close to each other. We can reformulate above loss function into a matrix format:\n\n", "index": 181, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha){\\bf {w}}'{\\bf {L}}{\\bf {w}},\n\\label{eq:fs_graphlassomatrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E93.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha){\\bf{w}}^{\\prime}{\\bf{L}}{\\bf{w}},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc30</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc30</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $r_{i,j}$ indicates the correlation between two features $f_{i}$ and $f_{j}$. When two features are positively correlation, we have ${\\bf {A}}(i,j)=1$ and $r_{i,j}>0$, and the penalty term forces the feature coefficients ${\\bf {w}}_{i}$ and ${\\bf {w}}_{j}$ to be similar; on the other hand, if two features are negatively correlated, we have ${\\bf {A}}(i,j)=1$ and $r_{i,j}<0$, and the penalty term makes the feature coefficients ${\\bf {w}}_{i}$ and ${\\bf {w}}_{j}$ to be dissimilar. An major limitation of GFLasso is that it uses pairwise sample correlations to measure feature dependencies, which may lead to additional estimation bias. The feature dependencies cannot be correctly estimated when the sample size is small.\n\n\\subsubsection{GOSCAR (Supervised)~\\citep{yang2012feature}}\nTo address the limitations of GFLasso,~\\citep{yang2012feature} proposed a GOSCAR algorithm by putting a $\\ell_{\\infty}$-norm regularization to enforce the pairwise feature coefficients to be equal if they are connected over the feature graph $\\mathcal{G}(N,E)$. The formulation of GOSCAR is defined as:\n\n", "itemtype": "equation", "pos": 121772, "prevtext": "\nwhere ${\\bf {L}}={\\bf {D}}-{\\bf {A}}$ is the Laplacian matrix and ${\\bf {D}}$ is a diagonal matrix with ${\\bf {D}}(i,i)=\\sum_{j}^{N_{i}}{\\bf {A}}(i,j)$. The Laplacian matrix in Eq.~(\\ref{eq:fs_graphlassomatrix}) is positive semi-definite and captures the local geometric structure of features. It can be observed that when the Laplacian matrix is the identity matrix ${\\bf {I}}$, ${\\bf {w}}'{\\bf {L}}{\\bf {w}}=||{\\bf {w}}||_{2}^{2}$, the penalty term in Eq.~(\\ref{eq:fs_graphlassomatrix}) reduces to the elastic net penalty~\\citep{zou2005regularization}. Since the graph regularization term ${\\bf {w}}'{\\bf {L}}{\\bf {w}}$ is convex and differentiable, existing efficient algorithms like LARS~\\citep{efron2004least} and proximal gradient descent methods~\\citep{liu2009efficient} can be directly applied.\n\n\\subsubsection{GFLasso (Supervised)~\\citep{kim2009statistical}}\nIn Eq.~(\\ref{eq:fs_graphlassomatrix}), graph feature structures are represented by an unsigned graph, and it encourages features connected together with similar feature coefficients. However, in many cases, features can also be negatively correlated. In this case, the feature graph $\\mathcal{G}(N,E)$ is represented by a signed graph, with both positive and negative edges. Recently, GFLasso is proposed to explicitly consider both the positive and negative feature correlations, the objective function of GFLasso is formulated as follows:\n\n", "index": 183, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha)\\, \\sum_{i,j}{\\bf {A}}(i,j)({\\bf {w}}_{i}-\\mbox{sign}(r_{i,j}){\\bf {w}}_{j})^{2},\n\\label{eq:fs_graphlasso}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E94.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i,j}{\\bf{A}}(i,j)({\\bf{w}}_{i}-\\mbox{sign}(r_{i,j}){\\bf{w}}_{j}%&#10;)^{2},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>-</mo><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nIn the above formulation, the $\\ell_{1}$-norm regularization is used for feature selection while the pairwise $\\ell_{\\infty}$-norm term penalizes large coefficients. The pairwise $\\ell_{\\infty}$-norm term can be decomposed as:\n\n", "itemtype": "equation", "pos": 123095, "prevtext": "\nwhere $r_{i,j}$ indicates the correlation between two features $f_{i}$ and $f_{j}$. When two features are positively correlation, we have ${\\bf {A}}(i,j)=1$ and $r_{i,j}>0$, and the penalty term forces the feature coefficients ${\\bf {w}}_{i}$ and ${\\bf {w}}_{j}$ to be similar; on the other hand, if two features are negatively correlated, we have ${\\bf {A}}(i,j)=1$ and $r_{i,j}<0$, and the penalty term makes the feature coefficients ${\\bf {w}}_{i}$ and ${\\bf {w}}_{j}$ to be dissimilar. An major limitation of GFLasso is that it uses pairwise sample correlations to measure feature dependencies, which may lead to additional estimation bias. The feature dependencies cannot be correctly estimated when the sample size is small.\n\n\\subsubsection{GOSCAR (Supervised)~\\citep{yang2012feature}}\nTo address the limitations of GFLasso,~\\citep{yang2012feature} proposed a GOSCAR algorithm by putting a $\\ell_{\\infty}$-norm regularization to enforce the pairwise feature coefficients to be equal if they are connected over the feature graph $\\mathcal{G}(N,E)$. The formulation of GOSCAR is defined as:\n\n", "index": 185, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha)\\, \\sum_{i,j}{\\bf {A}}(i,j)\\mbox{max}(|{\\bf {w}}_{i}|,|{\\bf {w}}_{j}|).\n\\label{eq:fs_goscar}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E95.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i,j}{\\bf{A}}(i,j)\\mbox{max}(|{\\bf{w}}_{i}|,|{\\bf{w}}_{j}|).\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>max</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {u}}$ and ${\\bf {v}}$ are sparse vectors with only two non-zero entries such that ${\\bf {u}}_{i}={\\bf {u}}_{j}=\\frac{1}{2}$, ${\\bf {v}}_{i}=-{\\bf {v}}_{j}=\\frac{1}{2}$. The formulation of the GOSCAR is more general than the OSCAR algorithm which is proposed in~\\citep{bondell2008simultaneous}. OSCAR algorithm assumes that all features form a complete graph whose entries in the adjacency matrix ${\\bf {A}}$ are all 1. In contrast, GOSCAR can deal with an arbitrary undirected graph as long as the adjacency matrix ${\\bf {A}}$ is symmetric.\n\nDifferent signs of feature coefficients can introduce additional penalty in the objective function in Eq.~(\\ref{eq:fs_graphlassomatrix}). Even though GOSCAR is designed to mitigate this side effect, it may still over penalize the feature coefficient ${\\bf {w}}_{i}$ or ${\\bf {w}}_{j}$ due to the property of the max operator. Therefore, a new formulation with non-convex grouping penalty is also proposed in~\\citep{yang2012feature}:\n\n", "itemtype": "equation", "pos": 123536, "prevtext": "\nIn the above formulation, the $\\ell_{1}$-norm regularization is used for feature selection while the pairwise $\\ell_{\\infty}$-norm term penalizes large coefficients. The pairwise $\\ell_{\\infty}$-norm term can be decomposed as:\n\n", "index": 187, "text": "\\begin{equation}\n\\begin{split}\n\\mbox{max}(|{\\bf {w}}_{i}|,|{\\bf {w}}_{j}|)&=\\frac{1}{2}(|{\\bf {w}}_{i}+{\\bf {w}}_{j}|+|{\\bf {w}}_{i}-{\\bf {w}}_{j}|)\\\\\n&=|{\\bf {u}}'{\\bf {w}}|+|{\\bf {v}}'{\\bf {w}}|,\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E96.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\mbox{max}(|{\\bf{w}}_{i}|,|{\\bf{w}}_{j}|)&amp;%&#10;\\displaystyle=\\frac{1}{2}(|{\\bf{w}}_{i}+{\\bf{w}}_{j}|+|{\\bf{w}}_{i}-{\\bf{w}}_{%&#10;j}|)\\\\&#10;&amp;\\displaystyle=|{\\bf{u}}^{\\prime}{\\bf{w}}|+|{\\bf{v}}^{\\prime}{\\bf{w}}|,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mtext>max</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msup><mi>\ud835\udc2e</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msup><mi>\ud835\udc2f</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc30</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the grouping penalty term controls only magnitudes of differences of coefficients ignoring their signs over the feature graph $\\mathcal{G}(N,E)$.\n\nFor feature selection problem with graph structured features, a subset of highly connected features in the graph is more likely to be selected or not selected as a whole. For example, in the toy example in Figure~(\\ref{fig:GraphLasso}), features $\\{f_{5},f_{6},f_{7}\\}$ are selected while features $\\{f_{1},f_{2},f_{3},f_{4}\\}$ are not selected.\n\n\\section{Feature Selection with Heterogeneous Data}\n\nTraditional feature selection algorithms can only work with generic data from a single data source which is based on the data independent and identically distributed (i.i.d.) assumption. However, heterogeneous data are becoming more prevalent in the era of big data. For example, in the medical domain, high dimensional gene features are often considered associated with different types clinical features. Since data of each source can be noisy, partial, or redundant, how to select relevant sources and how to use them together for effective feature selection is a challenging problem. Another example is in social media platforms, instances with high-dimensional features are often linked together, how to integrate link information to guide feature selection is another difficult problem. In this section, we review current feature selection algorithms for heterogeneous data from three aspects: (1) feature selection for linked data; (2) feature selection for multi-source data; and (3) feature selection for multi-view data. Note that multi-source feature selection is similar to multi-view feature selection, but they are different in two ways.  First, multi-source feature selection aims select features from the original feature space by integrating multiple sources while multi-view feature selection select features from different feature spaces for all views simultaneously.  Second, multi-source feature selection normally ignores the correlations among sources while multi-view feature selection exploits the relations among features from different views.\n\n\\subsection{Feature Selection Algorithms with Linked Data}\nLinked data has become ubiquitous in real-world applications such as Twitter\\footnote{https://twitter.com/} (tweets linked through hyperlinks), social networks in Facebook\\footnote{https://www.facebook.com/} (people connected by friendships) and biological networks (protein interaction networks). Since linked data are related to each other by different types of links, they are distinct from traditional attribute value data (or ``flat\" data).\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.5\\textwidth}\n\\centering\n\\subfigure[Linked Data\\label{fig:linkedfeature-a}]\n{\\includegraphics[width=\\textwidth]{examplelinkeddata.eps}}\n\\end{minipage}\n\\begin{minipage}{0.33\\textwidth}\n\\centering\n\\subfigure[Attribute-value Data Representation\\label{fig:linkedfeature-b}]\n{\\includegraphics[width=\\textwidth]{attributerepresentation.eps}}\n\\end{minipage}\n\\begin{minipage}{0.7\\textwidth}\n\\centering\n\\subfigure[Linked Data Representation\\label{fig:linkedfeature-c}]\n{\\includegraphics[width=\\textwidth]{linkedrepresentation.eps}}\n\\end{minipage}\n\\centering\n\\caption{A toy example of Linked Data.}\n\\label{fig:linkedfeature}\n\\end{figure}\nFigure~(\\ref{fig:linkedfeature}) illustrates a toy example of linked data and its two representations. Figure~(\\ref{fig:linkedfeature-a}) shows 8 linked instances ($u_{1}$ to $u_{8}$) while Figure~(\\ref{fig:linkedfeature-b})\nis a conventional representation of attribute-value data such that each row corresponds to one instance and each column corresponds to one feature\\footnote{The data can either be labeled or unlabeled. In the example in Figure~(\\ref{fig:linkedfeature}), the data is unlabeled.}. As mentioned above, in addition to feature information, linked data provides an extra source of information in the form of links, which can be represented by an adjacency matrix, illustrated in Figure~(\\ref{fig:linkedfeature-c}). Many linked data related learning tasks are proposed such as collective classification~\\citep{macskassy2007classification,sen2008collective}, relational learning~\\citep{long2006spectral,long2007probabilistic}, and active learning~\\citep{bilgic2010active,hu2013actnet}, but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information. Until recently, feature selection for linked data receives some attention. Next, we introduce some representative algorithms which leverage link information for feature selection.\n\n\\subsubsection{Feature Selection on Networks (Supervised)~\\citep{gu2011towards}}\nIn~\\citep{gu2011towards}, authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS). In detail, they propose to use linear classifier to capture the relationship between content information and class labels, and incorporate link information by graph regularization. Suppose that ${\\bf {X}}\\in\\mathbb{R}^{n\\times d}$ denotes the content matrix with $n$ instances and each instance is associated with a $d$-dimensional feature vector; ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ denotes the class labels matrix such that ${\\bf {Y}}(i,k)=1$ if the class label for the $i$-th instance is $k$, ${\\bf {Y}}(i,k)=0$ otherwise; ${\\bf {A}}$ denotes the adjacency matrix for all $n$ linked instances. The network can either be directed and undirected. With these notations, LapRLS first attempts to learn a linear classifier ${\\bf {W}}\\in\\mathbb{R}^{d\\times c}$ to map ${\\bf {X}}$ to ${\\bf {Y}}$:\n\n", "itemtype": "equation", "pos": 124748, "prevtext": "\nwhere ${\\bf {u}}$ and ${\\bf {v}}$ are sparse vectors with only two non-zero entries such that ${\\bf {u}}_{i}={\\bf {u}}_{j}=\\frac{1}{2}$, ${\\bf {v}}_{i}=-{\\bf {v}}_{j}=\\frac{1}{2}$. The formulation of the GOSCAR is more general than the OSCAR algorithm which is proposed in~\\citep{bondell2008simultaneous}. OSCAR algorithm assumes that all features form a complete graph whose entries in the adjacency matrix ${\\bf {A}}$ are all 1. In contrast, GOSCAR can deal with an arbitrary undirected graph as long as the adjacency matrix ${\\bf {A}}$ is symmetric.\n\nDifferent signs of feature coefficients can introduce additional penalty in the objective function in Eq.~(\\ref{eq:fs_graphlassomatrix}). Even though GOSCAR is designed to mitigate this side effect, it may still over penalize the feature coefficient ${\\bf {w}}_{i}$ or ${\\bf {w}}_{j}$ due to the property of the max operator. Therefore, a new formulation with non-convex grouping penalty is also proposed in~\\citep{yang2012feature}:\n\n", "index": 189, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}+(1-\\alpha)\\, \\sum_{i,j}{\\bf {A}}(i,j)\\||{\\bf {w}}_{i}|-|{\\bf {w}}_{j}|\\|_{1},\n\\label{eq:fs_goscarnoncovex}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E97.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}+(1-%&#10;\\alpha)\\,\\sum_{i,j}{\\bf{A}}(i,j)\\||{\\bf{w}}_{i}|-|{\\bf{w}}_{j}|\\|_{1},\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mrow><mi>\ud835\udc00</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe regularization term $\\|{\\bf {W}}\\|_{2,1}$ is included to achieve feature selection. It makes ${\\bf {W}}$ be sparse in rows, which makes the feature sparse across all $c$ class labels. The other regularization term $\\|{\\bf {W}}\\|_{F}^{2}$ presents the overfitting of the model and makes the model more robust in the same time.\n\nTill now, FSNet has modeled the content information for feature selection. However, link information is not considered yet. To capture the correlation between link information and content information to select more relevant features, FSNet uses the graph regularization and the basic assumption is that if two instances are linked, their class labels are likely to be similar. Taking into consideration the graph regularization, the objective function of FSNet can be mathematically formulated as:\n\n", "itemtype": "equation", "pos": 130808, "prevtext": "\nwhere the grouping penalty term controls only magnitudes of differences of coefficients ignoring their signs over the feature graph $\\mathcal{G}(N,E)$.\n\nFor feature selection problem with graph structured features, a subset of highly connected features in the graph is more likely to be selected or not selected as a whole. For example, in the toy example in Figure~(\\ref{fig:GraphLasso}), features $\\{f_{5},f_{6},f_{7}\\}$ are selected while features $\\{f_{1},f_{2},f_{3},f_{4}\\}$ are not selected.\n\n\\section{Feature Selection with Heterogeneous Data}\n\nTraditional feature selection algorithms can only work with generic data from a single data source which is based on the data independent and identically distributed (i.i.d.) assumption. However, heterogeneous data are becoming more prevalent in the era of big data. For example, in the medical domain, high dimensional gene features are often considered associated with different types clinical features. Since data of each source can be noisy, partial, or redundant, how to select relevant sources and how to use them together for effective feature selection is a challenging problem. Another example is in social media platforms, instances with high-dimensional features are often linked together, how to integrate link information to guide feature selection is another difficult problem. In this section, we review current feature selection algorithms for heterogeneous data from three aspects: (1) feature selection for linked data; (2) feature selection for multi-source data; and (3) feature selection for multi-view data. Note that multi-source feature selection is similar to multi-view feature selection, but they are different in two ways.  First, multi-source feature selection aims select features from the original feature space by integrating multiple sources while multi-view feature selection select features from different feature spaces for all views simultaneously.  Second, multi-source feature selection normally ignores the correlations among sources while multi-view feature selection exploits the relations among features from different views.\n\n\\subsection{Feature Selection Algorithms with Linked Data}\nLinked data has become ubiquitous in real-world applications such as Twitter\\footnote{https://twitter.com/} (tweets linked through hyperlinks), social networks in Facebook\\footnote{https://www.facebook.com/} (people connected by friendships) and biological networks (protein interaction networks). Since linked data are related to each other by different types of links, they are distinct from traditional attribute value data (or ``flat\" data).\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.5\\textwidth}\n\\centering\n\\subfigure[Linked Data\\label{fig:linkedfeature-a}]\n{\\includegraphics[width=\\textwidth]{examplelinkeddata.eps}}\n\\end{minipage}\n\\begin{minipage}{0.33\\textwidth}\n\\centering\n\\subfigure[Attribute-value Data Representation\\label{fig:linkedfeature-b}]\n{\\includegraphics[width=\\textwidth]{attributerepresentation.eps}}\n\\end{minipage}\n\\begin{minipage}{0.7\\textwidth}\n\\centering\n\\subfigure[Linked Data Representation\\label{fig:linkedfeature-c}]\n{\\includegraphics[width=\\textwidth]{linkedrepresentation.eps}}\n\\end{minipage}\n\\centering\n\\caption{A toy example of Linked Data.}\n\\label{fig:linkedfeature}\n\\end{figure}\nFigure~(\\ref{fig:linkedfeature}) illustrates a toy example of linked data and its two representations. Figure~(\\ref{fig:linkedfeature-a}) shows 8 linked instances ($u_{1}$ to $u_{8}$) while Figure~(\\ref{fig:linkedfeature-b})\nis a conventional representation of attribute-value data such that each row corresponds to one instance and each column corresponds to one feature\\footnote{The data can either be labeled or unlabeled. In the example in Figure~(\\ref{fig:linkedfeature}), the data is unlabeled.}. As mentioned above, in addition to feature information, linked data provides an extra source of information in the form of links, which can be represented by an adjacency matrix, illustrated in Figure~(\\ref{fig:linkedfeature-c}). Many linked data related learning tasks are proposed such as collective classification~\\citep{macskassy2007classification,sen2008collective}, relational learning~\\citep{long2006spectral,long2007probabilistic}, and active learning~\\citep{bilgic2010active,hu2013actnet}, but the task of feature selection is not well studied due to some of its unique challenges: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information. Until recently, feature selection for linked data receives some attention. Next, we introduce some representative algorithms which leverage link information for feature selection.\n\n\\subsubsection{Feature Selection on Networks (Supervised)~\\citep{gu2011towards}}\nIn~\\citep{gu2011towards}, authors propose a supervised feature selection algorithm (FSNet) based on Laplacian Regularized Least Squares (LapRLS). In detail, they propose to use linear classifier to capture the relationship between content information and class labels, and incorporate link information by graph regularization. Suppose that ${\\bf {X}}\\in\\mathbb{R}^{n\\times d}$ denotes the content matrix with $n$ instances and each instance is associated with a $d$-dimensional feature vector; ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ denotes the class labels matrix such that ${\\bf {Y}}(i,k)=1$ if the class label for the $i$-th instance is $k$, ${\\bf {Y}}(i,k)=0$ otherwise; ${\\bf {A}}$ denotes the adjacency matrix for all $n$ linked instances. The network can either be directed and undirected. With these notations, LapRLS first attempts to learn a linear classifier ${\\bf {W}}\\in\\mathbb{R}^{d\\times c}$ to map ${\\bf {X}}$ to ${\\bf {Y}}$:\n\n", "index": 191, "text": "\\begin{equation}\n\\min_{{\\bf {W}}}\\|{\\bf {XW}}-{\\bf {Y}}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}\\|_{2,1}+\\beta\\|{\\bf {W}}\\|_{F}^{2}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E98.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}}\\|{\\bf{XW}}-{\\bf{Y}}\\|_{F}^{2}+\\alpha\\|{\\bf{W}}\\|_{2,1}+\\beta\\|%&#10;{\\bf{W}}\\|_{F}^{2}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc18</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}{\\bf {XW}})$ is the graph regularization, $\\gamma$ controls the contribution of content information and link information. For undirected network, ${\\bf {L}}={\\bf {D}}-{\\bf {A}}$ is the laplacian matrix where ${\\bf {D}}$ is a diagonal matrix with its diagonal entry as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {A}}(i,j)$. For directed network, the laplacian matrix ${\\bf {L}}$ can be obtained either by transforming the network to be undirected ${\\bf {A}}=\\max({\\bf {A}},{\\bf {A}}')$ or by applying the random walk method in~\\citep{zhou2005learning}. The objective function in Eq.~(\\ref{eq:FSNet}) can be solved by proximal gradient descent methods~\\citep{nesterov2004introductory}. Afterwards, NetFS calculates the $\\ell_{2}$-norm value for each feature coefficient vector in ${\\bf {W}}$ and return the top ranked ones.\n\n\\subsubsection{Feature Selection for Social Media Data (Supervised)~\\citep{tang2012feature,tang2014featuresocial}}\nIn ~\\citep{tang2012feature,tang2014featuresocial}, the authors investigate feature selection problem on social media data by evaluating the effects of user-user and user-post relationships as illustrated in Figure~(\\ref{fig:linkedsocialexample}). The target is to perform feature selection for high dimensional posts, and take into account the user-user and user-post relationships manifested in the linked data.\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.35\\textwidth}\n\\centering\n\\subfigure[Social Media Data Example\\label{fig:linkedsocialexample-a}]\n{\\includegraphics[width=\\textwidth]{LinkedSocialExample1.eps}}\n\\end{minipage}\n\\begin{minipage}{0.58\\textwidth}\n\\centering\n\\subfigure[Attribute-value Data Representation\\label{fig:linkedsocialexample-b}]\n{\\includegraphics[width=\\textwidth]{LinkedSocialExample2.eps}}\n\\end{minipage}\n\\caption{A toy example of Linked Data in Social Media.}\n\\label{fig:linkedsocialexample}\n\\end{figure}\n\nIn the proposed supervised feature selection framework (LinkedFS), different social relationships are extracted to enhance the feature selection performance. As illustrated in Figure~(\\ref{fig:linkedFS}), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user $u_{2}$ can have multiple posts ($p_{3}$, $p_{4}$ and $p_{5}$) and these posts are more similar than those randomly selected; (b) CoFollowing - two users $u_{1}$ and $u_{3}$ follow a user $u_{4}$, its counterpart in citation analysis is bibliographic coupling~\\citep{morris2005manifestation}, and their posts are more likely to be of similar topics; (c) CoFollowed - two users $u_{2}$ and $u_{4}$ are followed by a third user $u_{1}$, similar to co-citation relation~\\citep{morris2005manifestation} in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user $u_{1}$ follows another user $u_{2}$, and their posts (e.g., $\\{p_{1},p_{2}\\}$ and $\\{p_{3},p_{4},p_{5}\\}$ are more likely similar in terms of topics. These four hypotheses are supported by social correlation theories such as homophily~\\citep{mcpherson2001birds} and social influence~\\citep{marsden1993network} in explaining the existence of similarity as what these relations suggest. For example, homophily indicates that people with similar interests are more likely to be linked.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.98\\textwidth]{LinkedFSRelations.eps}\n      \\caption{Different Types of Relations Extracted from Social Correlations among Posts\\label{fig:linkedFSRelations}}\n\\label{fig:linkedFS}\n\\end{figure}\n\nNext, we use the CoPost relation as an example to illustrate how these relations can be integrated into feature selection. A comprehensive report of other three relations can be referred to the original\npaper~\\citep{tang2012feature,tang2014featuresocial}. Let ${\\bf {p}}=\\{p_{1},p_{2},...,p_{N}\\}$ be the post set and ${\\bf {X}}\\in \\mathbb{R}^{N\\times d}$ be the matrix representation of these posts where $N$ is the number of posts and each post is associated with a $d$ dimensional feature vector; ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ denotes the class labels matrix where ${\\bf {Y}}(i,k)=1$ if the $i$-th post belongs to class $c_{j}$, otherwise zero; ${\\bf {u}}=\\{u_{1},u_{2},...,u_{n}\\}$ denotes the set of $n$ users and their link information is encoded in an adjacency matrix ${\\bf {A}}$; ${\\bf {P}}\\in\\mathbb{R}^{n\\times N}$ denotes the user-post relationships such that ${\\bf {P}}(i,j)=1$ if $u_{i}$ posts $p_{j}$, otherwise 0. To integrate the CoPost relations among users into the feature selection framework, the authors propose to add a regularization term\nenforces the hypothesis that the class labels (i.e., topics) of posts by the same user are similar. Hence, feature selection with the CoPost hypothesis can be formulated as the following optimization problem:\n\n", "itemtype": "equation", "pos": 131772, "prevtext": "\nThe regularization term $\\|{\\bf {W}}\\|_{2,1}$ is included to achieve feature selection. It makes ${\\bf {W}}$ be sparse in rows, which makes the feature sparse across all $c$ class labels. The other regularization term $\\|{\\bf {W}}\\|_{F}^{2}$ presents the overfitting of the model and makes the model more robust in the same time.\n\nTill now, FSNet has modeled the content information for feature selection. However, link information is not considered yet. To capture the correlation between link information and content information to select more relevant features, FSNet uses the graph regularization and the basic assumption is that if two instances are linked, their class labels are likely to be similar. Taking into consideration the graph regularization, the objective function of FSNet can be mathematically formulated as:\n\n", "index": 193, "text": "\\begin{equation}\n\\min_{{\\bf {W}}}\\|{\\bf {XW}}-{\\bf {Y}}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}\\|_{2,1}+\\beta\\|{\\bf {W}}\\|_{F}^{2}+\\gamma tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}{\\bf {XW}}),\n\\label{eq:FSNet}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E99.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}}\\|{\\bf{XW}}-{\\bf{Y}}\\|_{F}^{2}+\\alpha\\|{\\bf{W}}\\|_{2,1}+\\beta\\|%&#10;{\\bf{W}}\\|_{F}^{2}+\\gamma tr({\\bf{W}}^{\\prime}{\\bf{X}}^{\\prime}{\\bf{L}}{\\bf{XW%&#10;}}),\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc18</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc17\ud835\udc16</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {p}}_{u}$ denotes the whole set of posts by user $u$. The parameter $\\alpha$ controls the sparseness of ${\\bf {W}}$ in rows across all class labels and $\\beta$ controls the contribution of the CoPost relations. The CoPost regularization term indicates that if a user posts multiple posts, the class labels of these posts should be similar. After optimizing the objective function in Eq.~(\\ref{eq:linkedfscopost}), we can obtain the sparse matrix ${\\bf {W}}$ and get the ranking of all $d$ features thereby.\n\n\\subsubsection{Unsupervised Feature Selection for Linked Data (Unsupervised)~\\citep{tang2012unsupervised,tang2014unsupervised}}\nLinked Unsupervised Feature Selection (LUFS) is an unsupervised feature selection framework for linked data and in essence, LUFS investigates how to take advantage of link information for unsupervised feature selection. Generally speaking, feature selection aims to select a subset of relevant features with some constraints, while in supervised feature selection, the class labels play the role of constraints such that distances of instances with the same class labels should be closer than these of instances from different classes. However, in unsupervised feature selection, without class labels to assess feature relevance, some alternative criteria has to be exploited. The problem is formulated as follows: given $n$ linked instances $\\{x_{1},x_{2},...,x_{n}\\}$, their feature information and link information can be represented a feature matrix ${\\bf {X}}\\in\\mathbb{R}^{n\\times d}$ and an adjacency matrix ${\\bf {A}}\\in\\mathbb{R}^{n\\times n}$, respectively, where $d$ denotes the feature dimension. The task is to select a subset of relevant features from all $d$ features by utilizing both feature information ${\\bf {X}}$ and link information ${\\bf {A}}$. LUFS introduces the concept of pseudo-class labels to guide the unsupervised feature selection. Particularly, LUFS assumes the pseudo class labels come from $c$ classes, and uses ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ to denote the label matrix such each row of ${\\bf {Y}}$ has only one nonzero entry. Like most feature selection algorithms, LUFS assumes a linear mapping matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times c}$ exists between feature matrix ${\\bf {X}}$ and pseudo-class label matrix ${\\bf {Y}}$. With these notations, LUFS seeks the pseudo-class labels by extracting constraints from link information and attribute-value information through social dimension approach and spectral analysis, respectively.\n\nFirst, to consider the constraints from link information, LUFS employs social dimension approach to exploit the hidden factors that incur the interdependency among instances. Particularly, it uses modularity maximization~\\citep{newman2004finding} to extract hidden factor matrix ${\\bf {H}}$. Since the extracted hidden factor matrix ${\\bf {H}}$ indicates some affiliations among linked instances, according to the Linear Discriminative Analysis, within, between and total hidden factor scatter matrix ${\\bf {S}}_{w}$, ${\\bf {S}}_{b}$ and ${\\bf {S}}_{t}$ are defined as ${\\bf {S}}_{w}={\\bf {Y}}'{\\bf {Y}}-{\\bf {Y}}'{\\bf {F}}{\\bf {F}}'{\\bf {Y}}$, ${\\bf {S}}_{b}={\\bf {Y}}'{\\bf {F}}{\\bf {F}}'{\\bf {Y}}$, ${\\bf {S}}_{t}={\\bf {Y}}'{\\bf {Y}}$, where ${\\bf {F}}={\\bf {H}}({\\bf {H}}'{\\bf {H}})^{-\\frac{1}{2}}$ is the weighted hidden factor matrix. Considering the fact that instances with similar hidden factors are similar and instances with different hidden factors are dissimilar, the constraint from link information can be incorporated by solving the following maximization problem:\n\n", "itemtype": "equation", "pos": 136802, "prevtext": "\nwhere $tr({\\bf {W}}'{\\bf {X}}'{\\bf {L}}{\\bf {XW}})$ is the graph regularization, $\\gamma$ controls the contribution of content information and link information. For undirected network, ${\\bf {L}}={\\bf {D}}-{\\bf {A}}$ is the laplacian matrix where ${\\bf {D}}$ is a diagonal matrix with its diagonal entry as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {A}}(i,j)$. For directed network, the laplacian matrix ${\\bf {L}}$ can be obtained either by transforming the network to be undirected ${\\bf {A}}=\\max({\\bf {A}},{\\bf {A}}')$ or by applying the random walk method in~\\citep{zhou2005learning}. The objective function in Eq.~(\\ref{eq:FSNet}) can be solved by proximal gradient descent methods~\\citep{nesterov2004introductory}. Afterwards, NetFS calculates the $\\ell_{2}$-norm value for each feature coefficient vector in ${\\bf {W}}$ and return the top ranked ones.\n\n\\subsubsection{Feature Selection for Social Media Data (Supervised)~\\citep{tang2012feature,tang2014featuresocial}}\nIn ~\\citep{tang2012feature,tang2014featuresocial}, the authors investigate feature selection problem on social media data by evaluating the effects of user-user and user-post relationships as illustrated in Figure~(\\ref{fig:linkedsocialexample}). The target is to perform feature selection for high dimensional posts, and take into account the user-user and user-post relationships manifested in the linked data.\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.35\\textwidth}\n\\centering\n\\subfigure[Social Media Data Example\\label{fig:linkedsocialexample-a}]\n{\\includegraphics[width=\\textwidth]{LinkedSocialExample1.eps}}\n\\end{minipage}\n\\begin{minipage}{0.58\\textwidth}\n\\centering\n\\subfigure[Attribute-value Data Representation\\label{fig:linkedsocialexample-b}]\n{\\includegraphics[width=\\textwidth]{LinkedSocialExample2.eps}}\n\\end{minipage}\n\\caption{A toy example of Linked Data in Social Media.}\n\\label{fig:linkedsocialexample}\n\\end{figure}\n\nIn the proposed supervised feature selection framework (LinkedFS), different social relationships are extracted to enhance the feature selection performance. As illustrated in Figure~(\\ref{fig:linkedFS}), LinkedFS extracts four basic types of relations as hypotheses: (a) CoPost - a user $u_{2}$ can have multiple posts ($p_{3}$, $p_{4}$ and $p_{5}$) and these posts are more similar than those randomly selected; (b) CoFollowing - two users $u_{1}$ and $u_{3}$ follow a user $u_{4}$, its counterpart in citation analysis is bibliographic coupling~\\citep{morris2005manifestation}, and their posts are more likely to be of similar topics; (c) CoFollowed - two users $u_{2}$ and $u_{4}$ are followed by a third user $u_{1}$, similar to co-citation relation~\\citep{morris2005manifestation} in citation analysis, and their posts are more likely to be similar to each other; and (d) Following - a user $u_{1}$ follows another user $u_{2}$, and their posts (e.g., $\\{p_{1},p_{2}\\}$ and $\\{p_{3},p_{4},p_{5}\\}$ are more likely similar in terms of topics. These four hypotheses are supported by social correlation theories such as homophily~\\citep{mcpherson2001birds} and social influence~\\citep{marsden1993network} in explaining the existence of similarity as what these relations suggest. For example, homophily indicates that people with similar interests are more likely to be linked.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.98\\textwidth]{LinkedFSRelations.eps}\n      \\caption{Different Types of Relations Extracted from Social Correlations among Posts\\label{fig:linkedFSRelations}}\n\\label{fig:linkedFS}\n\\end{figure}\n\nNext, we use the CoPost relation as an example to illustrate how these relations can be integrated into feature selection. A comprehensive report of other three relations can be referred to the original\npaper~\\citep{tang2012feature,tang2014featuresocial}. Let ${\\bf {p}}=\\{p_{1},p_{2},...,p_{N}\\}$ be the post set and ${\\bf {X}}\\in \\mathbb{R}^{N\\times d}$ be the matrix representation of these posts where $N$ is the number of posts and each post is associated with a $d$ dimensional feature vector; ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ denotes the class labels matrix where ${\\bf {Y}}(i,k)=1$ if the $i$-th post belongs to class $c_{j}$, otherwise zero; ${\\bf {u}}=\\{u_{1},u_{2},...,u_{n}\\}$ denotes the set of $n$ users and their link information is encoded in an adjacency matrix ${\\bf {A}}$; ${\\bf {P}}\\in\\mathbb{R}^{n\\times N}$ denotes the user-post relationships such that ${\\bf {P}}(i,j)=1$ if $u_{i}$ posts $p_{j}$, otherwise 0. To integrate the CoPost relations among users into the feature selection framework, the authors propose to add a regularization term\nenforces the hypothesis that the class labels (i.e., topics) of posts by the same user are similar. Hence, feature selection with the CoPost hypothesis can be formulated as the following optimization problem:\n\n", "index": 195, "text": "\\begin{equation}\n\\min_{{\\bf {W}}}\\|{\\bf {XW}}-{\\bf {Y}}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}\\|_{2,1}+\\beta\\sum_{u\\in{\\bf {u}}}\\sum_{\\{p_{i},p_{j}\\}\\in{\\bf {p}}_{u}}\\|{\\bf {X}}(i,:){\\bf {W}}-{\\bf {X}}(j,:){\\bf {W}}\\|_{2}^{2},\n\\label{eq:linkedfscopost}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E100.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}}\\|{\\bf{XW}}-{\\bf{Y}}\\|_{F}^{2}+\\alpha\\|{\\bf{W}}\\|_{2,1}+\\beta%&#10;\\sum_{u\\in{\\bf{u}}}\\sum_{\\{p_{i},p_{j}\\}\\in{\\bf{p}}_{u}}\\|{\\bf{X}}(i,:){\\bf{W}%&#10;}-{\\bf{X}}(j,:){\\bf{W}}\\|_{2}^{2},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc18</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>u</mi><mo>\u2208</mo><mi>\ud835\udc2e</mi></mrow></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo>\u2208</mo><msub><mi>\ud835\udc29</mi><mi>u</mi></msub></mrow></munder><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mo>:</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow><mo>-</mo><mrow><mi>\ud835\udc17</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mo>:</mo><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nSecond, to take advantage of information from attribute-value part, LUFS obtains the constraints by the spectral analysis~\\citep{von2007tutorial}:\n\n", "itemtype": "equation", "pos": 140661, "prevtext": "\nwhere ${\\bf {p}}_{u}$ denotes the whole set of posts by user $u$. The parameter $\\alpha$ controls the sparseness of ${\\bf {W}}$ in rows across all class labels and $\\beta$ controls the contribution of the CoPost relations. The CoPost regularization term indicates that if a user posts multiple posts, the class labels of these posts should be similar. After optimizing the objective function in Eq.~(\\ref{eq:linkedfscopost}), we can obtain the sparse matrix ${\\bf {W}}$ and get the ranking of all $d$ features thereby.\n\n\\subsubsection{Unsupervised Feature Selection for Linked Data (Unsupervised)~\\citep{tang2012unsupervised,tang2014unsupervised}}\nLinked Unsupervised Feature Selection (LUFS) is an unsupervised feature selection framework for linked data and in essence, LUFS investigates how to take advantage of link information for unsupervised feature selection. Generally speaking, feature selection aims to select a subset of relevant features with some constraints, while in supervised feature selection, the class labels play the role of constraints such that distances of instances with the same class labels should be closer than these of instances from different classes. However, in unsupervised feature selection, without class labels to assess feature relevance, some alternative criteria has to be exploited. The problem is formulated as follows: given $n$ linked instances $\\{x_{1},x_{2},...,x_{n}\\}$, their feature information and link information can be represented a feature matrix ${\\bf {X}}\\in\\mathbb{R}^{n\\times d}$ and an adjacency matrix ${\\bf {A}}\\in\\mathbb{R}^{n\\times n}$, respectively, where $d$ denotes the feature dimension. The task is to select a subset of relevant features from all $d$ features by utilizing both feature information ${\\bf {X}}$ and link information ${\\bf {A}}$. LUFS introduces the concept of pseudo-class labels to guide the unsupervised feature selection. Particularly, LUFS assumes the pseudo class labels come from $c$ classes, and uses ${\\bf {Y}}\\in\\mathbb{R}^{n\\times c}$ to denote the label matrix such each row of ${\\bf {Y}}$ has only one nonzero entry. Like most feature selection algorithms, LUFS assumes a linear mapping matrix ${\\bf {W}}\\in\\mathbb{R}^{d\\times c}$ exists between feature matrix ${\\bf {X}}$ and pseudo-class label matrix ${\\bf {Y}}$. With these notations, LUFS seeks the pseudo-class labels by extracting constraints from link information and attribute-value information through social dimension approach and spectral analysis, respectively.\n\nFirst, to consider the constraints from link information, LUFS employs social dimension approach to exploit the hidden factors that incur the interdependency among instances. Particularly, it uses modularity maximization~\\citep{newman2004finding} to extract hidden factor matrix ${\\bf {H}}$. Since the extracted hidden factor matrix ${\\bf {H}}$ indicates some affiliations among linked instances, according to the Linear Discriminative Analysis, within, between and total hidden factor scatter matrix ${\\bf {S}}_{w}$, ${\\bf {S}}_{b}$ and ${\\bf {S}}_{t}$ are defined as ${\\bf {S}}_{w}={\\bf {Y}}'{\\bf {Y}}-{\\bf {Y}}'{\\bf {F}}{\\bf {F}}'{\\bf {Y}}$, ${\\bf {S}}_{b}={\\bf {Y}}'{\\bf {F}}{\\bf {F}}'{\\bf {Y}}$, ${\\bf {S}}_{t}={\\bf {Y}}'{\\bf {Y}}$, where ${\\bf {F}}={\\bf {H}}({\\bf {H}}'{\\bf {H}})^{-\\frac{1}{2}}$ is the weighted hidden factor matrix. Considering the fact that instances with similar hidden factors are similar and instances with different hidden factors are dissimilar, the constraint from link information can be incorporated by solving the following maximization problem:\n\n", "index": 197, "text": "\\begin{equation}\n\\max_{{\\bf {W}}}tr(({\\bf {S}}_{t})^{-1}{\\bf {S}}_{b}).\n\\label{eq:LUFSconstraint1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E101.m1\" class=\"ltx_Math\" alttext=\"\\max_{{\\bf{W}}}tr(({\\bf{S}}_{t})^{-1}{\\bf{S}}_{b}).\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mi>\ud835\udc16</mi></munder><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc12</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc12</mi><mi>b</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {L}}={\\bf {D}}-{\\bf {S}}$ is the laplacian matrix and ${\\bf {D}}$ is the diagonal matrix with its diagonal entry as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$. ${\\bf {S}}$ denotes the affinity matrix from ${\\bf {X}}$ and LUFS adopts the RBF kernel to get the affinity matrix. Incorporating the constraints from Eq.~(\\ref{eq:LUFSconstraint1}) and Eq.~(\\ref{eq:LUFSconstraint2}), the objective function of LUFS is formulated as follows:\n\n", "itemtype": "equation", "pos": 140922, "prevtext": "\nSecond, to take advantage of information from attribute-value part, LUFS obtains the constraints by the spectral analysis~\\citep{von2007tutorial}:\n\n", "index": 199, "text": "\\begin{equation}\n\\min tr({\\bf {Y}}'{\\bf {L}}{\\bf {Y}}),\n\\label{eq:LUFSconstraint2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E102.m1\" class=\"ltx_Math\" alttext=\"\\min tr({\\bf{Y}}^{\\prime}{\\bf{L}}{\\bf{Y}}),\" display=\"block\"><mrow><mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc18</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc18</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ is a regularization parameter to balance the contribution from these two constraints. To achieve feature selection, LUFS further adds a $\\ell_{2,1}$-norm regularization term on ${\\bf {W}}$, and with spectral relaxation of the pseudo-class label matrix, the objective function in Eq.~(\\ref{eq:LUFS-obj1}) can be eventually represented as:\n\n", "itemtype": "equation", "pos": 141470, "prevtext": "\nwhere ${\\bf {L}}={\\bf {D}}-{\\bf {S}}$ is the laplacian matrix and ${\\bf {D}}$ is the diagonal matrix with its diagonal entry as ${\\bf {D}}(i,i)=\\sum_{j=1}^{n}{\\bf {S}}(i,j)$. ${\\bf {S}}$ denotes the affinity matrix from ${\\bf {X}}$ and LUFS adopts the RBF kernel to get the affinity matrix. Incorporating the constraints from Eq.~(\\ref{eq:LUFSconstraint1}) and Eq.~(\\ref{eq:LUFSconstraint2}), the objective function of LUFS is formulated as follows:\n\n", "index": 201, "text": "\\begin{equation}\n\\min_{W}tr({\\bf {Y}}'{\\bf {L}}{\\bf {Y}})-\\alpha tr(({\\bf {S}}_{t})^{-1}{\\bf {S}}_{b}),\n\\label{eq:LUFS-obj1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E103.m1\" class=\"ltx_Math\" alttext=\"\\min_{W}tr({\\bf{Y}}^{\\prime}{\\bf{L}}{\\bf{Y}})-\\alpha tr(({\\bf{S}}_{t})^{-1}{%&#10;\\bf{S}}_{b}),\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>W</mi></munder><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc18</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc0b\ud835\udc18</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc12</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc12</mi><mi>b</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\beta$ is a parameter to control the sparseness of ${\\bf {W}}$ in rows and $\\lambda{\\bf {I}}_{d}$ is included to make ${\\bf {X}}'{\\bf {X}}+\\lambda{\\bf {I}}_{d}$ invertible. Similar to previous mentioned feature selection algorithms, the ranking of features can be obtained from the sparse matrix ${\\bf {W}}$.\n\n\n\\subsection{Feature Selection Algorithms with Multi-Source Data}\nOver the past few decades, many feature selection algorithms are proposed and they have proven to be effective in handling high dimensional data. However, most of them are designed for single source of data. In many data mining and machine learning tasks, we may have multiple data sources for the same set of data instances. For example, recent advancement in bioinformatics reveal the existence of some non-coding RNA species in addition widely used messenger RNA, these non-coding RNA species functions across a variety of biological process.  The availability of multiple data sources makes it possible to solve some problems unsolvable using a single source since the multi-faceted representations of data can help depict some intrinsic patterns hidden in a single source of data. The task of multi-source feature selection is formulated as follows: given $m$ sources of data depicting the same set of $n$ instances, and their matrix representations ${\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{1}},{\\bf {X}}_{2}\\in\\mathbb{R}^{n\\times d_{2}},...,{\\bf {X}}_{m}\\in\\mathbb{R}^{n\\times d_{m}}$ (where $d_{1},...,d_{m}$ denote the feature dimensions), select a subset of relevant features from a target source (e.g., ${\\bf {X}}_{i}$) by taking advantage of all information in all $m$ sources.\n\n\\subsubsection{Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised)~\\citep{zhao2008multi}}\nTo integrate information from multiple sources, authors in~\\citep{zhao2008multi} propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances~\\citep{lanckriet2004learning}. They introduce a concept of geometry-dependent covariance that enables the usage of the global geometric pattern in covariance analysis for feature selection. Given multiple\nlocal geometric patterns in an affinity matrix ${\\bf {S}}_{i}$ where $i$ denotes the $i$-th data source, a global pattern can be obtained by linearly combining all affinity matrices as ${\\bf {S}}=\\sum_{i=1}^{m}\\alpha_{i}{\\bf {S}}_{i}$, where $\\alpha_{i}$ controls the contribution of the $i$-th source. With the global geometric pattern obtained from multiple data sources, one can build a geometry-dependent sample covariance matrix for the target source ${\\bf {X}}_{i}$ as follows:\n\n", "itemtype": "equation", "pos": 141963, "prevtext": "\nwhere $\\alpha$ is a regularization parameter to balance the contribution from these two constraints. To achieve feature selection, LUFS further adds a $\\ell_{2,1}$-norm regularization term on ${\\bf {W}}$, and with spectral relaxation of the pseudo-class label matrix, the objective function in Eq.~(\\ref{eq:LUFS-obj1}) can be eventually represented as:\n\n", "index": 203, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {W}}}tr({\\bf {W}}'({\\bf {X}}'{\\bf {LX}}&+\\alpha{\\bf {X}}'({\\bf {I}}_{n}-{\\bf {F}}{\\bf {F}}')){\\bf {W}})+\\beta\\|{\\bf {W}}\\|_{2,1}\\\\\n\\mbox{s.t.}& \\quad {\\bf {W}}'({\\bf {X}}'{\\bf {X}}+\\lambda{\\bf {I}}_{d}){\\bf {W}}={\\bf {I}}_{c},\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E104.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{W}}}tr({\\bf{W}}^{\\prime}({\\bf{X}}^{\\prime%&#10;}{\\bf{LX}}&amp;\\displaystyle+\\alpha{\\bf{X}}^{\\prime}({\\bf{I}}_{n}-{\\bf{F}}{\\bf{F}}%&#10;^{\\prime})){\\bf{W}})+\\beta\\|{\\bf{W}}\\|_{2,1}\\\\&#10;\\displaystyle\\mbox{s.t.}&amp;\\displaystyle\\quad{\\bf{W}}^{\\prime}({\\bf{X}}^{\\prime}%&#10;{\\bf{X}}+\\lambda{\\bf{I}}_{d}){\\bf{W}}={\\bf{I}}_{c},\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><munder><mi>min</mi><mi>\ud835\udc16</mi></munder><mi>t</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc16</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mi>\ud835\udc0b\ud835\udc17</mi></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mo>+</mo><mi>\u03b1</mi><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc08</mi><mi>n</mi></msub><mo>-</mo><msup><mi>\ud835\udc05\ud835\udc05</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\ud835\udc16</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>\u03b2</mi><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo><msub><mi/><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>s.t.</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><msup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc16</mi></mpadded><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc17</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc17</mi></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>\ud835\udc08</mi><mi>d</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc16</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {\\Pi}}$ is a diagonal matrix with ${\\bf {\\Pi}}(j,j)=\\|{\\bf {D}}^{\\frac{1}{2}}{\\bf {X}}_{i}(:,j)\\|^{-1}$, and ${\\bf {D}}$ is also a diagonal matrix from ${\\bf {S}}$ with ${\\bf {D}}(k,k)=\\sum_{j=1}^{n}{\\bf {S}}(k,j)$.\n\nAfter getting a geometry-dependent sample covariance matrix, a subsequent question is how to use it effectively for feature selection. Basically, two methods are proposed. The first method, GPCOVvar sorts the diagonal of the covariance matrix and selects the features that have the biggest variances. Selecting features based on this method is equivalent to choose features that are consistent with the global geometry pattern. The basic idea of the first method is similar to Laplacian score~\\citep{he2005laplacian} and SPEC~\\citep{zhao2007spectral} mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually. While on the other hand, the second method, GPCOVspca, applies sparse principle component analysis (SPCA)~\\citep{d2007direct} to select features that is able to retain the total variance maximally, and hence considers the interactions among features and is able to select features with less redundancy.\n\n\\subsection{Feature Selection Algorithms with Multi-View Data}\nMulti-view sources represent different facets of data instances via different feature spaces. These feature spaces are naturally dependent and also high dimensional, which suggests that feature selection is necessary to prepare these sources for effective data mining tasks such as multi-view clustering. A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations. For example, selecting\npixels, tags, and terms about images in Flickr\\footnote{https://www.flickr.com/} simultaneously. Since multi-view feature selection is designed to select features across multiple views by using their relations, they are naturally different from multi-source feature selection. The difference between multi-source feature selection and multi-view feature selection is illustrated in Figure~(\\ref{fig:multi-source-multi-view}).\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.42\\textwidth}\n\\centering\n\\subfigure[Multi-source Feature Selection\\label{fig:Multi-source}]\n{\\includegraphics[width=\\textwidth]{Multi-source.eps}}\n\\end{minipage}\n\\begin{minipage}{0.42\\textwidth}\n\\centering\n\\subfigure[Multi-view Feature Selection\\label{fig:Multi-view}]\n{\\includegraphics[width=\\textwidth]{Multi-view.eps}}\n\\end{minipage}\n\\caption{Differences between multi-source and multi-view feature selection.}\n\\label{fig:multi-source-multi-view}\n\\end{figure}\nFor supervised multi-view feature selection, the most common approach is Sparse Group Lasso~\\citep{friedman2010note,peng2010regularized}. In this subsection, we review some representative algorithms for unsupervised multi-view feature selection.\n\n\\subsubsection{Adaptive Multi-view Feature Selection (Unsupervised)~\\citep{feng2013adaptive}}\nAdaptive unsupervised multi-view feature selection (AUMFS) takes advantages of data cluster structure, data similarity and correlations among views simultaneously for feature selection. More specifically, let ${\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{1}},{\\bf {X}}_{2}\\in\\mathbb{R}^{n\\times d_{2}},...,{\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{m}}$ denote the description of $n$ instances from $m$ different views, ${\\bf {X}}=[{\\bf {X}}_{1},{\\bf {X}}_{2},...,{\\bf {X}}_{m}]\\in\\mathbb{R}^{d}$ denotes the concatenated data, where $d=d_{1}+d_{2}+...+d_{m}$. AUMFS first builds a feature selection model by using a $\\ell_{2,1}$-norm regularized least square loss function:\n\n", "itemtype": "equation", "pos": 144964, "prevtext": "\nwhere $\\beta$ is a parameter to control the sparseness of ${\\bf {W}}$ in rows and $\\lambda{\\bf {I}}_{d}$ is included to make ${\\bf {X}}'{\\bf {X}}+\\lambda{\\bf {I}}_{d}$ invertible. Similar to previous mentioned feature selection algorithms, the ranking of features can be obtained from the sparse matrix ${\\bf {W}}$.\n\n\n\\subsection{Feature Selection Algorithms with Multi-Source Data}\nOver the past few decades, many feature selection algorithms are proposed and they have proven to be effective in handling high dimensional data. However, most of them are designed for single source of data. In many data mining and machine learning tasks, we may have multiple data sources for the same set of data instances. For example, recent advancement in bioinformatics reveal the existence of some non-coding RNA species in addition widely used messenger RNA, these non-coding RNA species functions across a variety of biological process.  The availability of multiple data sources makes it possible to solve some problems unsolvable using a single source since the multi-faceted representations of data can help depict some intrinsic patterns hidden in a single source of data. The task of multi-source feature selection is formulated as follows: given $m$ sources of data depicting the same set of $n$ instances, and their matrix representations ${\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{1}},{\\bf {X}}_{2}\\in\\mathbb{R}^{n\\times d_{2}},...,{\\bf {X}}_{m}\\in\\mathbb{R}^{n\\times d_{m}}$ (where $d_{1},...,d_{m}$ denote the feature dimensions), select a subset of relevant features from a target source (e.g., ${\\bf {X}}_{i}$) by taking advantage of all information in all $m$ sources.\n\n\\subsubsection{Multi-Source Feature Selection via Geometry-Dependent Covariance Analysis (Unsupervised)~\\citep{zhao2008multi}}\nTo integrate information from multiple sources, authors in~\\citep{zhao2008multi} propose an intuitive way to learn a global geometric pattern from all sources that reflects the intrinsic relationships among instances~\\citep{lanckriet2004learning}. They introduce a concept of geometry-dependent covariance that enables the usage of the global geometric pattern in covariance analysis for feature selection. Given multiple\nlocal geometric patterns in an affinity matrix ${\\bf {S}}_{i}$ where $i$ denotes the $i$-th data source, a global pattern can be obtained by linearly combining all affinity matrices as ${\\bf {S}}=\\sum_{i=1}^{m}\\alpha_{i}{\\bf {S}}_{i}$, where $\\alpha_{i}$ controls the contribution of the $i$-th source. With the global geometric pattern obtained from multiple data sources, one can build a geometry-dependent sample covariance matrix for the target source ${\\bf {X}}_{i}$ as follows:\n\n", "index": 205, "text": "\\begin{equation}\n{\\bf {C}}=\\frac{1}{n-1}{\\bf {\\Pi}}{\\bf {X}}_{i}'({\\bf {S}}-\\frac{{\\bf {S}}{\\bf {1}}{\\bf {1}}'{\\bf {S}}}{{\\bf {1}}'{\\bf {S}}{\\bf {1}}}){\\bf {X}}_{i}{\\bf {\\Pi}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E105.m1\" class=\"ltx_Math\" alttext=\"{\\bf{C}}=\\frac{1}{n-1}{\\bf{\\Pi}}{\\bf{X}}_{i}^{\\prime}({\\bf{S}}-\\frac{{\\bf{S}}{%&#10;\\bf{1}}{\\bf{1}}^{\\prime}{\\bf{S}}}{{\\bf{1}}^{\\prime}{\\bf{S}}{\\bf{1}}}){\\bf{X}}_%&#10;{i}{\\bf{\\Pi}},\" display=\"block\"><mrow><mrow><mi>\ud835\udc02</mi><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>\u2062</mo><mi>\ud835\udeb7</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc17</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc12</mi><mo>-</mo><mfrac><mrow><msup><mi>\ud835\udc12\ud835\udfcf\ud835\udfcf</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc12</mi></mrow><mrow><msup><mn>\ud835\udfcf</mn><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc12\ud835\udfcf</mi></mrow></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udeb7</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {F}}\\in\\mathbb{R}^{n\\times c}$ is the pseudo class label matrix. The $\\ell_{2,1}$-norm loss function is imposed since it is robust to outliers in the data instances and $\\ell_{2,1}$-norm regularization selects features across all $c$ pseudo class labels with joint sparsity. Then AUMFS uses spectral clustering on an affinity matrix from different views to learning the shared pseudo class labels. For the data matrix ${\\bf {X}}_{i}$ in each view, they first build an affinity matrix ${\\bf {S}}_{i}$ based on the data similarity on that view and get the the corresponding laplacian matrix ${\\bf {L}}_{i}$. Then it aims to learn the pseudo class label matrix by considering the spectral clustering from all views:\n\n", "itemtype": "equation", "pos": 148852, "prevtext": "\nwhere ${\\bf {\\Pi}}$ is a diagonal matrix with ${\\bf {\\Pi}}(j,j)=\\|{\\bf {D}}^{\\frac{1}{2}}{\\bf {X}}_{i}(:,j)\\|^{-1}$, and ${\\bf {D}}$ is also a diagonal matrix from ${\\bf {S}}$ with ${\\bf {D}}(k,k)=\\sum_{j=1}^{n}{\\bf {S}}(k,j)$.\n\nAfter getting a geometry-dependent sample covariance matrix, a subsequent question is how to use it effectively for feature selection. Basically, two methods are proposed. The first method, GPCOVvar sorts the diagonal of the covariance matrix and selects the features that have the biggest variances. Selecting features based on this method is equivalent to choose features that are consistent with the global geometry pattern. The basic idea of the first method is similar to Laplacian score~\\citep{he2005laplacian} and SPEC~\\citep{zhao2007spectral} mentioned above, hence one limitation is that feature redundancy is not considered since it measures features individually. While on the other hand, the second method, GPCOVspca, applies sparse principle component analysis (SPCA)~\\citep{d2007direct} to select features that is able to retain the total variance maximally, and hence considers the interactions among features and is able to select features with less redundancy.\n\n\\subsection{Feature Selection Algorithms with Multi-View Data}\nMulti-view sources represent different facets of data instances via different feature spaces. These feature spaces are naturally dependent and also high dimensional, which suggests that feature selection is necessary to prepare these sources for effective data mining tasks such as multi-view clustering. A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations. For example, selecting\npixels, tags, and terms about images in Flickr\\footnote{https://www.flickr.com/} simultaneously. Since multi-view feature selection is designed to select features across multiple views by using their relations, they are naturally different from multi-source feature selection. The difference between multi-source feature selection and multi-view feature selection is illustrated in Figure~(\\ref{fig:multi-source-multi-view}).\n\\begin{figure}[!t]\n\\centering\n\\begin{minipage}{0.42\\textwidth}\n\\centering\n\\subfigure[Multi-source Feature Selection\\label{fig:Multi-source}]\n{\\includegraphics[width=\\textwidth]{Multi-source.eps}}\n\\end{minipage}\n\\begin{minipage}{0.42\\textwidth}\n\\centering\n\\subfigure[Multi-view Feature Selection\\label{fig:Multi-view}]\n{\\includegraphics[width=\\textwidth]{Multi-view.eps}}\n\\end{minipage}\n\\caption{Differences between multi-source and multi-view feature selection.}\n\\label{fig:multi-source-multi-view}\n\\end{figure}\nFor supervised multi-view feature selection, the most common approach is Sparse Group Lasso~\\citep{friedman2010note,peng2010regularized}. In this subsection, we review some representative algorithms for unsupervised multi-view feature selection.\n\n\\subsubsection{Adaptive Multi-view Feature Selection (Unsupervised)~\\citep{feng2013adaptive}}\nAdaptive unsupervised multi-view feature selection (AUMFS) takes advantages of data cluster structure, data similarity and correlations among views simultaneously for feature selection. More specifically, let ${\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{1}},{\\bf {X}}_{2}\\in\\mathbb{R}^{n\\times d_{2}},...,{\\bf {X}}_{1}\\in\\mathbb{R}^{n\\times d_{m}}$ denote the description of $n$ instances from $m$ different views, ${\\bf {X}}=[{\\bf {X}}_{1},{\\bf {X}}_{2},...,{\\bf {X}}_{m}]\\in\\mathbb{R}^{d}$ denotes the concatenated data, where $d=d_{1}+d_{2}+...+d_{m}$. AUMFS first builds a feature selection model by using a $\\ell_{2,1}$-norm regularized least square loss function:\n\n", "index": 207, "text": "\\begin{equation}\n\\min_{{\\bf {W}},{\\bf {F}}}\\|{\\bf {X}}{\\bf {W}}-{\\bf {F}}\\|_{2,1}+\\alpha\\|{\\bf {W}}\\|_{2,1},\n\\label{eq:AUMFS-obj1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E106.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}},{\\bf{F}}}\\|{\\bf{X}}{\\bf{W}}-{\\bf{F}}\\|_{2,1}+\\alpha\\|{\\bf{W}}\\|%&#10;_{2,1},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc16</mi><mo>,</mo><mi>\ud835\udc05</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc05</mi></mrow><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere the contribution of each feature for the joint spectral clustering is balanced by a nonnegative weight $\\lambda_{i}$ and the summation of all $\\lambda_{i}$ equals 1. By combing the objective function in Eq.~(\\ref{eq:AUMFS-obj1}) and Eq.~(\\ref{eq:AUMFS-obj2}) together, the final objective function of AUMFS which jointly performs pseudo class label learning and feature selection is as follows:\n\n", "itemtype": "equation", "pos": 149723, "prevtext": "\nwhere ${\\bf {F}}\\in\\mathbb{R}^{n\\times c}$ is the pseudo class label matrix. The $\\ell_{2,1}$-norm loss function is imposed since it is robust to outliers in the data instances and $\\ell_{2,1}$-norm regularization selects features across all $c$ pseudo class labels with joint sparsity. Then AUMFS uses spectral clustering on an affinity matrix from different views to learning the shared pseudo class labels. For the data matrix ${\\bf {X}}_{i}$ in each view, they first build an affinity matrix ${\\bf {S}}_{i}$ based on the data similarity on that view and get the the corresponding laplacian matrix ${\\bf {L}}_{i}$. Then it aims to learn the pseudo class label matrix by considering the spectral clustering from all views:\n\n", "index": 209, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {F}},{\\bf {\\lambda}}}&\\sum_{i=1}^{m}\\lambda_{i}tr({\\bf {F}}'{\\bf {L}}_{i}{\\bf {F}})=\\min tr({\\bf {F}}'\\sum_{i=1}^{m}\\lambda_{i}{\\bf {L}}_{i}{\\bf {F}})\\\\\n\\mbox{s.t. }&\\quad {\\bf {F}}'{\\bf {F}}={\\bf {I}}_{c}, {\\bf {F}}\\geq 0, \\sum_{i=1}^{m}\\lambda_{i}=1, \\lambda_{i}\\geq 0,\n\\end{split}\n\\label{eq:AUMFS-obj2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E107.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{F}},{\\bf{\\lambda}}}&amp;\\displaystyle\\sum_{i=%&#10;1}^{m}\\lambda_{i}tr({\\bf{F}}^{\\prime}{\\bf{L}}_{i}{\\bf{F}})=\\min tr({\\bf{F}}^{%&#10;\\prime}\\sum_{i=1}^{m}\\lambda_{i}{\\bf{L}}_{i}{\\bf{F}})\\\\&#10;\\displaystyle\\mbox{s.t. }&amp;\\displaystyle\\quad{\\bf{F}}^{\\prime}{\\bf{F}}={\\bf{I}}%&#10;_{c},{\\bf{F}}\\geq 0,\\sum_{i=1}^{m}\\lambda_{i}=1,\\lambda_{i}\\geq 0,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mrow><mi>\ud835\udc05</mi><mo>,</mo><mi>\u03bb</mi></mrow></munder></mtd><mtd columnalign=\"left\"><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc05</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc05</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc0b</mi><mi>i</mi></msub><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>s.t.\u00a0</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><msup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc05</mi></mpadded><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc05</mi><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2265</mo><mn>0</mn></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThrough optimizing the objective function in Eq.~(\\ref{eq:AUMFS-obj}) and obtaining the feature coefficient matrix ${\\bf {W}}$. AUMFS takes a commonly accepted way to rank all features according to the value of $\\|{\\bf {W}}(i,:)\\|_{2}^{2}$ in a descending order and return the top ranked ones.\n\n\\subsubsection{Unsupervised Feature Selection for Multi-View Data (Unsupervised)~\\citep{tang2013unsupervised}}\n\nAUMFS~\\citep{feng2013adaptive} learns one feature weight matrix for all features from different views to approximate the pseudo class labels. In~\\citep{tang2013unsupervised}, the authors propose a novel unsupervised feature selection method called Multi-view Feature Selection (MVFS). Similar to AUMFS, MVFS also uses spectral clustering with the affinity matrix from different views to learn the pseudo class labels. But it differs from AUMFS that it learns one feature weight matrix for each view to fit the pesudo class labels by the joint least squared loss and $\\ell_{2,1}$-norm regularization. The optimization problem of MVFS can be formulated as follows:\n\n", "itemtype": "equation", "pos": 150487, "prevtext": "\nwhere the contribution of each feature for the joint spectral clustering is balanced by a nonnegative weight $\\lambda_{i}$ and the summation of all $\\lambda_{i}$ equals 1. By combing the objective function in Eq.~(\\ref{eq:AUMFS-obj1}) and Eq.~(\\ref{eq:AUMFS-obj2}) together, the final objective function of AUMFS which jointly performs pseudo class label learning and feature selection is as follows:\n\n", "index": 211, "text": "\\begin{equation}\n\\begin{split}\n\\min tr({\\bf {F}}'&\\sum_{i=1}^{m}\\lambda_{i}{\\bf {L}}_{i}{\\bf {F}})+\\beta(\\|{\\bf {X}}{\\bf {W}}-{\\bf {F}}\\|_{2,1}+\\alpha\\|{\\bf {W}}\\|_{2,1})\\\\\n\\mbox{s.t. }&\\quad {\\bf {F}}'{\\bf {F}}={\\bf {I}}_{c}, {\\bf {F}}\\geq 0, \\sum_{i=1}^{m}\\lambda_{i}=1, \\lambda_{i}\\geq 0.\n\\end{split}\n\\label{eq:AUMFS-obj}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E108.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min tr({\\bf{F}}^{\\prime}&amp;\\displaystyle\\sum_{i=1}^{m%&#10;}\\lambda_{i}{\\bf{L}}_{i}{\\bf{F}})+\\beta(\\|{\\bf{X}}{\\bf{W}}-{\\bf{F}}\\|_{2,1}+%&#10;\\alpha\\|{\\bf{W}}\\|_{2,1})\\\\&#10;\\displaystyle\\mbox{s.t. }&amp;\\displaystyle\\quad{\\bf{F}}^{\\prime}{\\bf{F}}={\\bf{I}}%&#10;_{c},{\\bf{F}}\\geq 0,\\sum_{i=1}^{m}\\lambda_{i}=1,\\lambda_{i}\\geq 0.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>min</mi><mi>t</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc05</mi><mo>\u2032</mo></msup></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>\ud835\udc0b</mi><mi>i</mi></msub><mi>\ud835\udc05</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>\u03b2</mi><mo stretchy=\"false\">(</mo><mo>\u2225</mo><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc05</mi><msub><mo>\u2225</mo><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>\u03b1</mi><mo>\u2225</mo><mi>\ud835\udc16</mi><msub><mo>\u2225</mo><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>s.t.\u00a0</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><msup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc05</mi></mpadded><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc05</mi><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2265</mo><mn>0</mn></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nSimilar to AUMFS, the orthogonal and nonnegative constraints on the pseudo class label matrix ${\\bf {F}}$ is imposed to guarantee there is one and only one positive entry in each row and other entries are all zero. The parameter $\\lambda_{i}$ is employed to control the contribution of each view and $\\sum_{i=1}^{m}\\lambda_{i}=1$.\n\n\\subsubsection{Multi-view Clustering and Feature Learning via Structured Sparsity~\\citep{wang2013multi}}\nIn some cases, it is possible that features from a certain view contains more discriminative information than features from other views for different clusters. According to~\\citep{wang2013multi}, one example is that in image processing, the color features are more useful than other types of features in identifying stop signs. To address this issue in multi-view feature selection, a novel feature selection algorithm is proposed in~\\citep{wang2013multi} with a joint group $\\ell_{1}$-norm and $\\ell_{2,1}$-norm regularization.\n\nFor the feature weight matrix ${\\bf {W}}_{1},...,{\\bf {W}}_{m}$ from $m$ different views, the group $\\ell_{1}$-norm is defined as $\\|{\\bf {W}}\\|_{G_{1}}=\\sum_{j=1}^{c}\\sum_{i=1}^{m}\\|{\\bf {W}}_{i}(:,j)\\|$. Crucially, the group $\\ell_{1}$-norm regularization term is able to capture the global relations among different views and is able to achieve view-wise sparsity such that only a few views are selected. In addition to group $\\ell_{1}$-norm, a $\\ell_{2,1}$-norm regularizer on ${\\bf {W}}$ is also included to achieve feature sparsity among selected views. It can be observed that the basic idea is very similar to sparse group lasso~\\citep{friedman2010note,peng2010regularized} which also requires intra-group sparsity and inter-group sparsity. Hence, the objective function of the proposed method is formulated as follows:\n\n", "itemtype": "equation", "pos": 151897, "prevtext": "\nThrough optimizing the objective function in Eq.~(\\ref{eq:AUMFS-obj}) and obtaining the feature coefficient matrix ${\\bf {W}}$. AUMFS takes a commonly accepted way to rank all features according to the value of $\\|{\\bf {W}}(i,:)\\|_{2}^{2}$ in a descending order and return the top ranked ones.\n\n\\subsubsection{Unsupervised Feature Selection for Multi-View Data (Unsupervised)~\\citep{tang2013unsupervised}}\n\nAUMFS~\\citep{feng2013adaptive} learns one feature weight matrix for all features from different views to approximate the pseudo class labels. In~\\citep{tang2013unsupervised}, the authors propose a novel unsupervised feature selection method called Multi-view Feature Selection (MVFS). Similar to AUMFS, MVFS also uses spectral clustering with the affinity matrix from different views to learn the pseudo class labels. But it differs from AUMFS that it learns one feature weight matrix for each view to fit the pesudo class labels by the joint least squared loss and $\\ell_{2,1}$-norm regularization. The optimization problem of MVFS can be formulated as follows:\n\n", "index": 213, "text": "\\begin{equation}\n\\begin{split}\n\\min tr({\\bf {F}}'&\\sum_{i=1}^{m}\\lambda_{i}{\\bf {L}}_{i}{\\bf {F}})+\\sum_{i=1}^{m}\\beta(\\|{\\bf {X}}_{i}{\\bf {W}}_{i}-{\\bf {F}}\\|_{2,1}+\\alpha\\|{\\bf {W}}_{i}\\|_{2,1})\\\\\n\\mbox{s.t. }&\\quad {\\bf {F}}'{\\bf {F}}={\\bf {I}}_{c}, {\\bf {F}}\\geq 0, \\sum_{i=1}^{m}\\lambda_{i}=1, \\lambda_{i}\\geq 0.\n\\end{split}\n\\label{eq:AUMFS-obj}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E109.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min tr({\\bf{F}}^{\\prime}&amp;\\displaystyle\\sum_{i=1}^{m%&#10;}\\lambda_{i}{\\bf{L}}_{i}{\\bf{F}})+\\sum_{i=1}^{m}\\beta(\\|{\\bf{X}}_{i}{\\bf{W}}_{%&#10;i}-{\\bf{F}}\\|_{2,1}+\\alpha\\|{\\bf{W}}_{i}\\|_{2,1})\\\\&#10;\\displaystyle\\mbox{s.t. }&amp;\\displaystyle\\quad{\\bf{F}}^{\\prime}{\\bf{F}}={\\bf{I}}%&#10;_{c},{\\bf{F}}\\geq 0,\\sum_{i=1}^{m}\\lambda_{i}=1,\\lambda_{i}\\geq 0.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>min</mi><mi>t</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc05</mi><mo>\u2032</mo></msup></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>\ud835\udc0b</mi><mi>i</mi></msub><mi>\ud835\udc05</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><msub><mi/><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></msub><msup><mi/><mi>m</mi></msup><mi>\u03b2</mi><mo stretchy=\"false\">(</mo><mo>\u2225</mo><msub><mi>\ud835\udc17</mi><mi>i</mi></msub><msub><mi>\ud835\udc16</mi><mi>i</mi></msub><mo>-</mo><mi>\ud835\udc05</mi><msub><mo>\u2225</mo><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>\u03b1</mi><mo>\u2225</mo><msub><mi>\ud835\udc16</mi><mi>i</mi></msub><msub><mo>\u2225</mo><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>s.t.\u00a0</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><msup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc05</mi></mpadded><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow><mo>,</mo><mrow><mrow><mi>\ud835\udc05</mi><mo>\u2265</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2265</mo><mn>0</mn></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ and $\\beta$ are two parameters to control the inter-view sparsity and intra-view sparsity. Through the proposed two regularizer, many features in the discriminative views and a small number of features in the non-discriminative views will output as the final set of features to discriminate cluster structures.\n\n\\section{Feature Selection with Streaming Data}\nMethods introduced in the previous sections assume that all data instances and features are known in advance. However, it is not the case in many real-world applications that we are more likely facing with dynamic data streams and feature streams. In the worst cases, the size of data or the features are unknown or even infinite, thus it is not practical to wait until all data instances or features are available to perform feature selection. For streaming data, one motivating example is that in online spam email detection problem, new emails are constantly arriving, it is not easy to employ batch-mode feature selection methods to select relevant feature in a time manner. On a orthogonal setting, feature selection for streaming features also have its practical significances. For example, Twitter\nproduces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously being generated. These slang words promptly grab users' attention and become popular in a short time. Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes. Recently, there exists some work trying to combine these two dual problems together, the problem is referred as feature selection on Trapezoidal data streams~\\citep{zhangtowards}. In the following two sections, we will review some representative algorithms for these two orthogonal problems, i.e., feature selection for streaming data and feature selection for streaming features.\n\n\\subsection{Feature Selection Algorithms with Feature Streams}\nFor the feature selection problem with streaming features, the number of instances is considered to be constant while candidate features arrive one at a time, the task is to timely select a subset of relevant features from all features seen so far. Instead of searching for the whole feature space which is costly, streaming feature selection (SFS) processes a new feature\nupon its arrival. A general framework of streaming feature selection is presented in Figure~(\\ref{fig:StreamingFS}). At each time step, a typical SFS algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore. Different algorithms have different implementations in the first step that checks newly arrived features. The second step which checks existing features is an optional step for some algorithms.\n\\begin{figure}[!htbp]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{StreamingFS.eps}\n\\caption{A framework of streaming feature selection. It consists of two phases: testing newly arrived feature and testing existing features.}\n\\label{fig:StreamingFS}\n\\end{figure}\n\n\\subsubsection{Grafting Algorithm (Supervised)~\\citep{perkins2003online}}\nThe first attempt to perform streaming feature selection is credited to~\\citep{perkins2003online}. They proposed a streaming feature selection framework based on stagewise gradient descent regularized risk framework~\\citep{perkins2003grafting}. Grafting is a general technique that can deal with a variety of models that are parameterized by a feature weight vector ${\\bf {w}}$ subject to $\\ell_{1}$-norm regularization, such as Lasso:\n\n", "itemtype": "equation", "pos": 154058, "prevtext": "\nSimilar to AUMFS, the orthogonal and nonnegative constraints on the pseudo class label matrix ${\\bf {F}}$ is imposed to guarantee there is one and only one positive entry in each row and other entries are all zero. The parameter $\\lambda_{i}$ is employed to control the contribution of each view and $\\sum_{i=1}^{m}\\lambda_{i}=1$.\n\n\\subsubsection{Multi-view Clustering and Feature Learning via Structured Sparsity~\\citep{wang2013multi}}\nIn some cases, it is possible that features from a certain view contains more discriminative information than features from other views for different clusters. According to~\\citep{wang2013multi}, one example is that in image processing, the color features are more useful than other types of features in identifying stop signs. To address this issue in multi-view feature selection, a novel feature selection algorithm is proposed in~\\citep{wang2013multi} with a joint group $\\ell_{1}$-norm and $\\ell_{2,1}$-norm regularization.\n\nFor the feature weight matrix ${\\bf {W}}_{1},...,{\\bf {W}}_{m}$ from $m$ different views, the group $\\ell_{1}$-norm is defined as $\\|{\\bf {W}}\\|_{G_{1}}=\\sum_{j=1}^{c}\\sum_{i=1}^{m}\\|{\\bf {W}}_{i}(:,j)\\|$. Crucially, the group $\\ell_{1}$-norm regularization term is able to capture the global relations among different views and is able to achieve view-wise sparsity such that only a few views are selected. In addition to group $\\ell_{1}$-norm, a $\\ell_{2,1}$-norm regularizer on ${\\bf {W}}$ is also included to achieve feature sparsity among selected views. It can be observed that the basic idea is very similar to sparse group lasso~\\citep{friedman2010note,peng2010regularized} which also requires intra-group sparsity and inter-group sparsity. Hence, the objective function of the proposed method is formulated as follows:\n\n", "index": 215, "text": "\\begin{equation}\n\\begin{split}\n\\min_{{\\bf {W}},{\\bf {F}}}&\\|{\\bf {XW}}-{\\bf {F}}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}\\|_{2,1}+\\beta\\|{\\bf {W}}\\|_{G_{1}}\\\\\n\\mbox{s.t. }&\\quad {\\bf {F}}'{\\bf {F}}={\\bf {I}}_{c}, {\\bf {F}}\\geq 0,\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E110.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\min_{{\\bf{W}},{\\bf{F}}}&amp;\\displaystyle\\|{\\bf{XW}}-{%&#10;\\bf{F}}\\|_{F}^{2}+\\alpha\\|{\\bf{W}}\\|_{2,1}+\\beta\\|{\\bf{W}}\\|_{G_{1}}\\\\&#10;\\displaystyle\\mbox{s.t. }&amp;\\displaystyle\\quad{\\bf{F}}^{\\prime}{\\bf{F}}={\\bf{I}}%&#10;_{c},{\\bf{F}}\\geq 0,\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><munder><mi>min</mi><mrow><mi>\ud835\udc16</mi><mo>,</mo><mi>\ud835\udc05</mi></mrow></munder></mtd><mtd columnalign=\"left\"><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc17\ud835\udc16</mi><mo>-</mo><mi>\ud835\udc05</mi></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>\ud835\udc16</mi><mo>\u2225</mo></mrow><msub><mi>G</mi><mn>1</mn></msub></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>s.t.\u00a0</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><msup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>\ud835\udc05</mi></mpadded><mo>\u2032</mo></msup><mo>\u2062</mo><mi>\ud835\udc05</mi></mrow><mo>=</mo><msub><mi>\ud835\udc08</mi><mi>c</mi></msub></mrow><mo>,</mo><mrow><mi>\ud835\udc05</mi><mo>\u2265</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n\nIn general, Grafting can work with the following objective function in which the features are assumed to arrive one at a time, the key challenge is how to efficiently update the parameter ${\\bf {w}}$ when new features are continuously coming. The basic idea of Grafting streaming feature selection algorithm is based on the observation -- incorporating a new feature into the model in Eq.~(\\ref{eq:fs_grafting}) involves in adding a new penalty term into the model. For example, at the time step $j$, when a new feature $f_{j}$ arrives, it incurs a regularization penalty of $\\alpha|{\\bf {w}}_{j}|$. Therefore, the addition of the new feature $f_{j}$ reduces the objective function value in Eq.~(\\ref{eq:fs_grafting}) only when the reduction in the loss function part $loss({\\bf {w}};{\\bf {X}},{\\bf {y}})$ outweighs the increase in the $\\ell_{1}$-norm regularization. With this observation, the condition of accepting the new feature $f_{j}$ is as follows:\n\n", "itemtype": "equation", "pos": 158026, "prevtext": "\nwhere $\\alpha$ and $\\beta$ are two parameters to control the inter-view sparsity and intra-view sparsity. Through the proposed two regularizer, many features in the discriminative views and a small number of features in the non-discriminative views will output as the final set of features to discriminate cluster structures.\n\n\\section{Feature Selection with Streaming Data}\nMethods introduced in the previous sections assume that all data instances and features are known in advance. However, it is not the case in many real-world applications that we are more likely facing with dynamic data streams and feature streams. In the worst cases, the size of data or the features are unknown or even infinite, thus it is not practical to wait until all data instances or features are available to perform feature selection. For streaming data, one motivating example is that in online spam email detection problem, new emails are constantly arriving, it is not easy to employ batch-mode feature selection methods to select relevant feature in a time manner. On a orthogonal setting, feature selection for streaming features also have its practical significances. For example, Twitter\nproduces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously being generated. These slang words promptly grab users' attention and become popular in a short time. Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes. Recently, there exists some work trying to combine these two dual problems together, the problem is referred as feature selection on Trapezoidal data streams~\\citep{zhangtowards}. In the following two sections, we will review some representative algorithms for these two orthogonal problems, i.e., feature selection for streaming data and feature selection for streaming features.\n\n\\subsection{Feature Selection Algorithms with Feature Streams}\nFor the feature selection problem with streaming features, the number of instances is considered to be constant while candidate features arrive one at a time, the task is to timely select a subset of relevant features from all features seen so far. Instead of searching for the whole feature space which is costly, streaming feature selection (SFS) processes a new feature\nupon its arrival. A general framework of streaming feature selection is presented in Figure~(\\ref{fig:StreamingFS}). At each time step, a typical SFS algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore. Different algorithms have different implementations in the first step that checks newly arrived features. The second step which checks existing features is an optional step for some algorithms.\n\\begin{figure}[!htbp]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{StreamingFS.eps}\n\\caption{A framework of streaming feature selection. It consists of two phases: testing newly arrived feature and testing existing features.}\n\\label{fig:StreamingFS}\n\\end{figure}\n\n\\subsubsection{Grafting Algorithm (Supervised)~\\citep{perkins2003online}}\nThe first attempt to perform streaming feature selection is credited to~\\citep{perkins2003online}. They proposed a streaming feature selection framework based on stagewise gradient descent regularized risk framework~\\citep{perkins2003grafting}. Grafting is a general technique that can deal with a variety of models that are parameterized by a feature weight vector ${\\bf {w}}$ subject to $\\ell_{1}$-norm regularization, such as Lasso:\n\n", "index": 217, "text": "\\begin{equation}\n\\min_{{\\bf {w}}}\\, loss({\\bf {w}};{\\bf {X}},{\\bf {y}})+\\alpha||{\\bf {w}}||_{1}.\n\\label{eq:fs_grafting}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E111.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{w}}}\\,loss({\\bf{w}};{\\bf{X}},{\\bf{y}})+\\alpha||{\\bf{w}}||_{1}.\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><munder><mi>min</mi><mi>\ud835\udc30</mi></munder></mpadded><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\ud835\udc30</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nOtherwise, the Grafting algorithm will set the feature coefficient ${\\bf {w}}_{j}$ of the new feature $f_{j}$ to be zero. In the second step, when new features like $f_{j}$ are accepted and included in the model, Grafting adopts a conjugate gradient (CG) procedure to optimize the model with respect to all current parameters.\n\n\\subsubsection{Alpha-investing Algorithm (Supervised)~\\citep{zhou2005streaming}}\nAlpha-investing~\\citep{zhou2005streaming} is an adaptive complexity penalty method which dynamically changes the threshold of error reduction that is required to accept a new feature. It is motivated from a desire to control the false discovery rate (FDR) of newly arrived features such that a small portion of spurious features does not greatly affect the model accuracy. The detailed algorithm works as follows:\n\\begin{itemize}\n  \\item Initialize $w_{0}=0$ (probability of false positives), $i=0$ (index of features), selected features in the model $SF=\\emptyset$\n  \\item \\emph{Step 1}: Get a new feature $f_{i}$\n  \\item \\emph{Step 2}: Set $\\alpha_{i}=w_{i}/(2i)$\n  \\item \\emph{Step 3}:\n  \n", "itemtype": "equation", "pos": 159119, "prevtext": "\n\nIn general, Grafting can work with the following objective function in which the features are assumed to arrive one at a time, the key challenge is how to efficiently update the parameter ${\\bf {w}}$ when new features are continuously coming. The basic idea of Grafting streaming feature selection algorithm is based on the observation -- incorporating a new feature into the model in Eq.~(\\ref{eq:fs_grafting}) involves in adding a new penalty term into the model. For example, at the time step $j$, when a new feature $f_{j}$ arrives, it incurs a regularization penalty of $\\alpha|{\\bf {w}}_{j}|$. Therefore, the addition of the new feature $f_{j}$ reduces the objective function value in Eq.~(\\ref{eq:fs_grafting}) only when the reduction in the loss function part $loss({\\bf {w}};{\\bf {X}},{\\bf {y}})$ outweighs the increase in the $\\ell_{1}$-norm regularization. With this observation, the condition of accepting the new feature $f_{j}$ is as follows:\n\n", "index": 219, "text": "\\begin{equation}\n\\left|\\frac{\\partial loss({\\bf {w}};{\\bf {X}},{\\bf {y}})}{\\partial {\\bf {w}}_{j}}\\right|>\\alpha.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E112.m1\" class=\"ltx_Math\" alttext=\"\\left|\\frac{\\partial loss({\\bf{w}};{\\bf{X}},{\\bf{y}})}{\\partial{\\bf{w}}_{j}}%&#10;\\right|&gt;\\alpha.\" display=\"block\"><mrow><mrow><mrow><mo>|</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc30</mi><mo>;</mo><mi>\ud835\udc17</mi><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\ud835\udc30</mi><mi>j</mi></msub></mrow></mfrac><mo>|</mo></mrow><mo>&gt;</mo><mi>\u03b1</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  \\item \\emph{Step 4}: $i=i+1$\n  \\item \\emph{Step 5}: Repeat \\emph{Step 1} to \\emph{Step 4}.\n\\end{itemize}\nThe threshold $\\alpha_{i}$ corresponds to the probability of selecting a spurious feature at the time step $i$. The threshold $\\alpha_{i}$ is adjusted by the wealth $w_{i}$, which denotes acceptable number of false positivly detected features at the current moment. The wealth $w_{i}$ is increased when a feature is added to the model, otherwise it is decreased when a feature is not included to save for future features.\nMore precisely, at each time step, the method calculates the $p$-value by using the fact that $\\Delta$Logliklohood is equivalent to t-statistics, the $p$-value denotes the probability that a feature coefficient could be set to nonzero when it is not (false positively detected). The basic idea of alpha-investing is to adaptively adjust the threshold such that when new features are selected and included into the model, it allows a higher chance of including incorrect features in the future. On the opposite side, each time when a new feature is not included (found to be not statistically significant), the wealth is wasted and lowers the chance of finding more spurious features. However, one major limitation of this method is that it only tests newly arrived features while failing to take into consideration of the feature redundancy for old existing features. In~\\citep{dhillon2010feature}, authors extended the alpha-investing algorithm and proposed a proposed a multiple streamwise feature selection algorithm to the case where there are multiple feature streams.\n\n\\subsubsection{Online Streaming Feature Selection Algorithm (Supervised)~\\citep{wu2010online,wu2013online}}\nDifferent from grafting and alpha-investing, authors studied the streaming feature selection problem from an information theoretic perspective by using the concept of Markov blanket~\\citep{wu2010online,wu2013online}. According to their definition, the whole feature set consists of four types of features: irrelevant features,  redundant feature, weakly relevant but non-redundant features, and strongly relevant features. An optimal feature selection should select non-redundant and strongly relevant features. But features are dynamically arrived in a streaming fashion, it is difficult to find all strongly relevant and non-redundant features. The proposed method, OSFS is able to capture these non-redundant and strongly relevant features via two steps: (1) online relevance analysis, and (2) online redundancy analysis. A general framework of OSFS is listed as follows:\n\\begin{itemize}\n  \\item Initialize selected features in the model $SF=\\emptyset$\n  \\item \\emph{Step 1}: Get a new feature $f_{i}$\n  \\item \\emph{Step 2}: Online relevance analysis\n  \n", "itemtype": "equation", "pos": 160348, "prevtext": "\nOtherwise, the Grafting algorithm will set the feature coefficient ${\\bf {w}}_{j}$ of the new feature $f_{j}$ to be zero. In the second step, when new features like $f_{j}$ are accepted and included in the model, Grafting adopts a conjugate gradient (CG) procedure to optimize the model with respect to all current parameters.\n\n\\subsubsection{Alpha-investing Algorithm (Supervised)~\\citep{zhou2005streaming}}\nAlpha-investing~\\citep{zhou2005streaming} is an adaptive complexity penalty method which dynamically changes the threshold of error reduction that is required to accept a new feature. It is motivated from a desire to control the false discovery rate (FDR) of newly arrived features such that a small portion of spurious features does not greatly affect the model accuracy. The detailed algorithm works as follows:\n\\begin{itemize}\n  \\item Initialize $w_{0}=0$ (probability of false positives), $i=0$ (index of features), selected features in the model $SF=\\emptyset$\n  \\item \\emph{Step 1}: Get a new feature $f_{i}$\n  \\item \\emph{Step 2}: Set $\\alpha_{i}=w_{i}/(2i)$\n  \\item \\emph{Step 3}:\n  \n", "index": 221, "text": "\\begin{align*}\n  w_{i+1}&=w_{i}-\\alpha_{i},SF=SF \\quad & \\mbox{if } p\\_value(f_{i},SF)\\geq \\alpha_{i}\\\\\n  w_{i+1}&=w_{i}+\\alpha_{\\Delta}-\\alpha_{i}, \\, SF=SF\\cup f_{i}\\quad &\\mbox{if } p\\_value(f_{i},SF)< \\alpha_{i}\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w_{i+1}\" display=\"inline\"><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=w_{i}-\\alpha_{i},SF=SF\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo>=</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mbox{if }p\\_value(f_{i},SF)\\geq\\alpha_{i}\" display=\"inline\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w_{i+1}\" display=\"inline\"><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=w_{i}+\\alpha_{\\Delta}-\\alpha_{i},\\,SF=SF\\cup f_{i}\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><msub><mi>\u03b1</mi><mi mathvariant=\"normal\">\u0394</mi></msub></mrow><mo>-</mo><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo>=</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo>\u222a</mo><msub><mi>f</mi><mi>i</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mbox{if }p\\_value(f_{i},SF)&lt;\\alpha_{i}\" display=\"inline\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  \\item \\emph{Step 3}: Online Redundancy Analysis\n  \\item \\emph{Step 4}: Repeat \\emph{Step 1} to \\emph{Step 3} until some stopping criteria are satisfied.\n\\end{itemize}\nIn step 2, the online relevance analysis step, OSFS discovers weakly relevant and strongly relevant features, and these features are added into the best candidate features (BCF). Otherwise, if the newly arrived feature is not relevant to the class label, it is discarded and not considered in the future step. In step 3, the online redundancy analysis step, OSFS dynamically eliminates redundant features in the selected subset using Markov Blanket. For each feature $f_{j}$ in the best candidate set $BCF$, if there exists a subset of $BCF$ making $f_{j}$ and the class label conditionally independent, then $f_{j}$ is removed from $BCF$. The most time consuming part of OSFS is the redundancy analysis phase, therefore, a fast-OSFS is proposed to improve efficiency. Fast-OSFS further divides this phase into inner-redundancy analysis part and outer-redundancy analysis part. In the inner-redundancy analysis part, fast-OSFS only re-examines the feature newly added into BCF, while the outer-redundancy analysis part re-examines each feature of BCF only when the process of generating a feature is stopped.\n\n\\subsubsection{Streaming Feature Selection with Group Structures (Supervised)~\\citep{wang2013online,li2013group}}\nPrevious mentioned streaming feature selection algorithms evaluate new features individually. However, streaming features may also exhibit group structures and current group feature selection algorithms such as Group Lasso cannot handle online processing.\n\nTherefore, in~\\citep{wang2013online,wang2015online}, authors propose an streaming group feature selection algorithm (OGFS) which consists of two parts: online intra-group selection and online inter-group selection.\nIn the online intra-group selection phase, for the streaming features in a specific group, OGFS uses spectral feature selection techniques to assess if the newly arrived feature will increase the ratio between between-class distances and within-class distances or it is a significant feature with discriminative power. If the inclusion of this new feature will increase this ratio or is statistically significant, the new feature is included, otherwise it is discarded. After all feature groups are processed, in the online inter-group step, for the features from different feature groups, OGFS uses Lasso to select a subset of features to obtain an ultimate subset.\n\nIn addition to OGFS, a similar algorithm is proposed in~\\citep{li2013group}. The proposed algorithm, GFSSF also contains two steps: the feature level selection and group level selection. The difference is that it performs feature level selection and group level selection from an information theoretic perspective. In the feature level selection, it only processes features from the same group, and seeks for the best feature subset from the arrived features so far via relevance and redundancy analysis. Then in the group selection phase, it seeks for a set of feature groups that can cover as much uncertainty of the class labels as possible with a minimum cost. Afterwards, it obtains a subset of relevant features that is sparse at both the group level and the individual feature level.\n\n\\subsubsection{Unsupervised Streaming Feature Selection in Social Media (Unsupervised)~\\citep{li2015unsupervised}}\nVast majority of streaming feature selection methods are supervised which utilize label information to guide feature selection process. However, in social media, it is easy to amass vast quantities of unlabeled data, while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data in social media, authors in~\\citep{li2015unsupervised} proposed an USFS algorithm to study unsupervised streaming feature selection. The key idea of USFS is to utilize source information such as link information to enable unsupervised streaming feature selection. The work flow of the proposed framework USFS is shown in Figure 2.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.98\\textwidth]{USFS.eps}\n      \\caption{Workflow of USFS.}\n\\label{fig:USFS}\n\\end{figure}\nUSFS first uncovers hidden social factors from link information by mixed membership stochastic blockmodel~\\citep{airoldi2009mixed}. Suppose that the number of instances is $n$ and each instance is associated with a $k$ dimensional latent factors. After obtaining the social latent factors ${\\bf {\\Pi}}\\in\\mathbb{R}^{n\\times k}$ for each linked instance, USFS takes advantage of them as a constrain to perform selection. At a specific time step $t$, let ${\\bf {X}}^{(t)}$, ${\\bf {W}}^{(t)}$ denote the corresponding feature matrix, feature coefficient matrix. To model feature information, USFS constructs a graph $\\mathcal{G}$ to represent feature similarity and ${\\bf {A}}^{(t)}$ denotes the adjacency matrix of the graph, ${\\bf {L}}^{(t)}$ is the corresponding laplacian matrix. Then the objective function to achieve feature selection at the time step $t$ is given as follows:\n\n", "itemtype": "equation", "pos": 163347, "prevtext": "\n  \\item \\emph{Step 4}: $i=i+1$\n  \\item \\emph{Step 5}: Repeat \\emph{Step 1} to \\emph{Step 4}.\n\\end{itemize}\nThe threshold $\\alpha_{i}$ corresponds to the probability of selecting a spurious feature at the time step $i$. The threshold $\\alpha_{i}$ is adjusted by the wealth $w_{i}$, which denotes acceptable number of false positivly detected features at the current moment. The wealth $w_{i}$ is increased when a feature is added to the model, otherwise it is decreased when a feature is not included to save for future features.\nMore precisely, at each time step, the method calculates the $p$-value by using the fact that $\\Delta$Logliklohood is equivalent to t-statistics, the $p$-value denotes the probability that a feature coefficient could be set to nonzero when it is not (false positively detected). The basic idea of alpha-investing is to adaptively adjust the threshold such that when new features are selected and included into the model, it allows a higher chance of including incorrect features in the future. On the opposite side, each time when a new feature is not included (found to be not statistically significant), the wealth is wasted and lowers the chance of finding more spurious features. However, one major limitation of this method is that it only tests newly arrived features while failing to take into consideration of the feature redundancy for old existing features. In~\\citep{dhillon2010feature}, authors extended the alpha-investing algorithm and proposed a proposed a multiple streamwise feature selection algorithm to the case where there are multiple feature streams.\n\n\\subsubsection{Online Streaming Feature Selection Algorithm (Supervised)~\\citep{wu2010online,wu2013online}}\nDifferent from grafting and alpha-investing, authors studied the streaming feature selection problem from an information theoretic perspective by using the concept of Markov blanket~\\citep{wu2010online,wu2013online}. According to their definition, the whole feature set consists of four types of features: irrelevant features,  redundant feature, weakly relevant but non-redundant features, and strongly relevant features. An optimal feature selection should select non-redundant and strongly relevant features. But features are dynamically arrived in a streaming fashion, it is difficult to find all strongly relevant and non-redundant features. The proposed method, OSFS is able to capture these non-redundant and strongly relevant features via two steps: (1) online relevance analysis, and (2) online redundancy analysis. A general framework of OSFS is listed as follows:\n\\begin{itemize}\n  \\item Initialize selected features in the model $SF=\\emptyset$\n  \\item \\emph{Step 1}: Get a new feature $f_{i}$\n  \\item \\emph{Step 2}: Online relevance analysis\n  \n", "index": 223, "text": "\\begin{align*}\n  \\mbox{Discard feature } f_{i} \\quad & \\mbox{ if } f_{i} \\mbox{ is relevant to the class label}\\\\\n  SF=SF\\cup f_{i}\\quad &\\mbox{otherwise}\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mbox{Discard feature }f_{i}\" display=\"inline\"><mrow><mtext>Discard feature\u00a0</mtext><mo>\u2062</mo><msub><mi>f</mi><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mbox{ if }f_{i}\\mbox{ is relevant to the class label}\" display=\"inline\"><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>f</mi><mi>i</mi></msub><mo>\u2062</mo><mtext>\u00a0is relevant to the class label</mtext></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle SF=SF\\cup f_{i}\" display=\"inline\"><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo>=</mo><mrow><mrow><mi>S</mi><mo>\u2062</mo><mi>F</mi></mrow><mo>\u222a</mo><msub><mi>f</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $\\alpha$ is a sparse regularization parameter, $\\beta$ controls the robustness of the model and $\\gamma$ balances link information and feature information.\n\nAssume at the next time step $t+1$ a new feature arrives, to test new features, USFS takes a similar strategy as Grafting to perform gradient test. Specifically, if the inclusion of the new feature is going to reduce the objective function in Eq.~(\\ref{eq:USFS}) at the new time step, the feature is accepted, otherwise the new feature can be removed. When new features are continuously being generated, they may take place of some existing features and some existing features may become outdated, therefore, USFS also investigates if it is necessary to remove any existing features by re-optimizing the model through A Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasinewton method~\\citep{boyd2004convex}.\n\n\\subsection{Feature Selection Algorithms with Data Streams}\nIn this subsection, we review the problem of feature selection with data streams which is considered as a dual problem of streaming feature selection. Most existing feature selection algorithms assume that all the data instances are available before performing feature selection. However, such assumptions are not always true in real-world applications that data instances are dynamically generated and arrive in a sequential manner. Therefore, it is necessary and urgent to come up with some solutions to deal with sequential data of high dimensionality.\n\n\\subsubsection{Online Feature Selection (Supervised)~\\citep{wang2014online}}\nIn~\\citep{wang2014online}, an online feature selection algorithm (OFS) for binary classification is proposed. Let $\\{{\\bf {x}}_{1},{\\bf {x}}_{2},...,{\\bf {x}}_{t}...\\}$ and $\\{y_{1},y_{2},...,y_{t}...\\}$ denote a sequence of input data instances and input class labels, where each data instance ${\\bf {x}}_{i}\\in\\mathbb{R}^{d}$ is in a $d$-dimensional space and class label $y_{i}\\in\\{-1,+1\\}$. The task of OFS is to learn a linear classifier ${\\bf {w}}^{(t)}\\in\\mathbb{R}^{d}$ that can be used to classify each instance ${\\bf {x}}_{i}$ by a linear function sign(${\\bf {w}}^{(t)}{\\bf {x}}_{i}$). To achieve the feature selection purpose, it requires that the linear classifier ${\\bf {w}}^{(t)}$ has at most $B$-nonzero elements such that $\\|{\\bf {w}}^{(t)}\\|_{0}\\leq B$. It indicates that at most $B$ features will be used for classification. With a regularization parameter $\\lambda$ and a step size $\\eta$, the algorithm of OFS works as follows:\n\\begin{itemize}\n  \\item \\emph{Step 1}: Get a new data instance ${\\bf {x}}_{t}$ and its class label $y_{t}$\n  \\item \\emph{Step 2}: Make a class label prediction sign(${\\bf {w}}^{(t)}{\\bf {x}}_{t}$) for the new instance\n  \\item \\emph{Step 3(a)}: If ${\\bf {x}}_{t}$ is misclassified such that $y_{i}{\\bf {w}}^{(t)}{\\bf {x}}_{t}<0$\n  \n", "itemtype": "equation", "pos": 168632, "prevtext": "\n  \\item \\emph{Step 3}: Online Redundancy Analysis\n  \\item \\emph{Step 4}: Repeat \\emph{Step 1} to \\emph{Step 3} until some stopping criteria are satisfied.\n\\end{itemize}\nIn step 2, the online relevance analysis step, OSFS discovers weakly relevant and strongly relevant features, and these features are added into the best candidate features (BCF). Otherwise, if the newly arrived feature is not relevant to the class label, it is discarded and not considered in the future step. In step 3, the online redundancy analysis step, OSFS dynamically eliminates redundant features in the selected subset using Markov Blanket. For each feature $f_{j}$ in the best candidate set $BCF$, if there exists a subset of $BCF$ making $f_{j}$ and the class label conditionally independent, then $f_{j}$ is removed from $BCF$. The most time consuming part of OSFS is the redundancy analysis phase, therefore, a fast-OSFS is proposed to improve efficiency. Fast-OSFS further divides this phase into inner-redundancy analysis part and outer-redundancy analysis part. In the inner-redundancy analysis part, fast-OSFS only re-examines the feature newly added into BCF, while the outer-redundancy analysis part re-examines each feature of BCF only when the process of generating a feature is stopped.\n\n\\subsubsection{Streaming Feature Selection with Group Structures (Supervised)~\\citep{wang2013online,li2013group}}\nPrevious mentioned streaming feature selection algorithms evaluate new features individually. However, streaming features may also exhibit group structures and current group feature selection algorithms such as Group Lasso cannot handle online processing.\n\nTherefore, in~\\citep{wang2013online,wang2015online}, authors propose an streaming group feature selection algorithm (OGFS) which consists of two parts: online intra-group selection and online inter-group selection.\nIn the online intra-group selection phase, for the streaming features in a specific group, OGFS uses spectral feature selection techniques to assess if the newly arrived feature will increase the ratio between between-class distances and within-class distances or it is a significant feature with discriminative power. If the inclusion of this new feature will increase this ratio or is statistically significant, the new feature is included, otherwise it is discarded. After all feature groups are processed, in the online inter-group step, for the features from different feature groups, OGFS uses Lasso to select a subset of features to obtain an ultimate subset.\n\nIn addition to OGFS, a similar algorithm is proposed in~\\citep{li2013group}. The proposed algorithm, GFSSF also contains two steps: the feature level selection and group level selection. The difference is that it performs feature level selection and group level selection from an information theoretic perspective. In the feature level selection, it only processes features from the same group, and seeks for the best feature subset from the arrived features so far via relevance and redundancy analysis. Then in the group selection phase, it seeks for a set of feature groups that can cover as much uncertainty of the class labels as possible with a minimum cost. Afterwards, it obtains a subset of relevant features that is sparse at both the group level and the individual feature level.\n\n\\subsubsection{Unsupervised Streaming Feature Selection in Social Media (Unsupervised)~\\citep{li2015unsupervised}}\nVast majority of streaming feature selection methods are supervised which utilize label information to guide feature selection process. However, in social media, it is easy to amass vast quantities of unlabeled data, while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data in social media, authors in~\\citep{li2015unsupervised} proposed an USFS algorithm to study unsupervised streaming feature selection. The key idea of USFS is to utilize source information such as link information to enable unsupervised streaming feature selection. The work flow of the proposed framework USFS is shown in Figure 2.\n\\begin{figure}[!htbp]\n  \\centering\n    \\includegraphics[width=0.98\\textwidth]{USFS.eps}\n      \\caption{Workflow of USFS.}\n\\label{fig:USFS}\n\\end{figure}\nUSFS first uncovers hidden social factors from link information by mixed membership stochastic blockmodel~\\citep{airoldi2009mixed}. Suppose that the number of instances is $n$ and each instance is associated with a $k$ dimensional latent factors. After obtaining the social latent factors ${\\bf {\\Pi}}\\in\\mathbb{R}^{n\\times k}$ for each linked instance, USFS takes advantage of them as a constrain to perform selection. At a specific time step $t$, let ${\\bf {X}}^{(t)}$, ${\\bf {W}}^{(t)}$ denote the corresponding feature matrix, feature coefficient matrix. To model feature information, USFS constructs a graph $\\mathcal{G}$ to represent feature similarity and ${\\bf {A}}^{(t)}$ denotes the adjacency matrix of the graph, ${\\bf {L}}^{(t)}$ is the corresponding laplacian matrix. Then the objective function to achieve feature selection at the time step $t$ is given as follows:\n\n", "index": 225, "text": "\\begin{equation}\n\\min_{{\\bf {W}}^{(t)}}\\frac{1}{2}||{\\bf {X}}^{(t)}{\\bf {W}}^{(t)}-\\boldsymbol{\\Pi}||_{F}^{2}+\\alpha\\sum_{i=1}^{k}\\|({\\bf {w}}^{(t)})^{i}\\|_{1}+\\frac{\\beta}{2}||{\\bf {W}}^{(t)}||_{F}^{2}+\\frac{\\gamma}{2}||({\\bf {X}}^{(t)}{\\bf {W}}^{(t)})^{T}({\\bf {L}}^{(t)})^{\\frac{1}{2}}||_{F}^{2},\n\\label{eq:USFS}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E113.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}^{(t)}}\\frac{1}{2}||{\\bf{X}}^{(t)}{\\bf{W}}^{(t)}-\\boldsymbol{\\Pi%&#10;}||_{F}^{2}+\\alpha\\sum_{i=1}^{k}\\|({\\bf{w}}^{(t)})^{i}\\|_{1}+\\frac{\\beta}{2}||%&#10;{\\bf{W}}^{(t)}||_{F}^{2}+\\frac{\\gamma}{2}||({\\bf{X}}^{(t)}{\\bf{W}}^{(t)})^{T}(%&#10;{\\bf{L}}^{(t)})^{\\frac{1}{2}}||_{F}^{2},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>-</mo><mi>\ud835\udeb7</mi></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc30</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msup><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mfrac><mi>\u03b3</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc0b</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\n  \\item \\emph{Step 3(b)}: ${\\bf {w}}_{t+1}=(1-\\lambda\\eta){\\bf {w}}_{t}$\n  \\item \\emph{Step 4}: Repeat \\emph{Step 1} to \\emph{Step 3(a)} or \\emph{Step 3(b)} until no new data instances arrive.\n\\end{itemize}\nIn Step 3(a), each time when a training instance ${\\bf {x}}_{t}$ is misclassified, ${\\bf {w}}_{t}$ is first updated by online gradient descent and then it is projected to a $\\ell_{2}$-norm ball to ensure that the classifier is bounded. After that, the new classifier $\\hat{{\\bf {w}}}_{t+1}$ is truncated by taking the most important $B$ features. A subset of $B$ features is output at each time step. The process repeats until there are no new data instances arrive anymore.\n\n\\subsubsection{Unsupervised Feature Selection on Data Streams (Unsupervised)~\\citep{huang2015unsupervised}}\nOFS assumes that the class labels of continuously generated data streams are available. However, it is not the case in many real-world applications that label information is costly to obtain. To timely select a subset of relevant features when unlabeled data are continuously being generated, authors in~\\citep{huang2015unsupervised} propose a novel unsupervised feature selection method (FSDS) that is able to perform feature selection timely with only one pass of the data and utilize limited storage. The basic idea of FSDS is to use matrix sketching to efficiently maintain a low-rank approximation of the current observed data and then apply regularized regression to obtain the feature coefficients, which can further be used to obtain the importance of features. The authors empirically show that when some orthogonality conditions are satisfied, the ridge regression can replace the Lasso for feature selection, which is more computational efficient. Assume at a specific time step $t$, ${\\bf {X}}^{(t)}\\in\\mathbb{R}^{n_{t}\\times d}$ denotes the data matrix at that time step, its rank-$k$ approximation is ${\\bf {X}}^{(t)}_{k}=\\sum_{i=1}^{k}\\delta_{i}{\\bf {u}}_{i}{\\bf {v}}_{i}'$, where $\\delta_{i}$ ($i\\leq k$) are the top-$k$ singular values, ${\\bf {u}}_{i}$ and ${\\bf {v}}_{i}'$ are the corresponding left and right singular vectors, respectively. With these, the feature coefficients can be obtained by minimizing the following ridge regression problem:\n\n", "itemtype": "equation", "pos": 171802, "prevtext": "\nwhere $\\alpha$ is a sparse regularization parameter, $\\beta$ controls the robustness of the model and $\\gamma$ balances link information and feature information.\n\nAssume at the next time step $t+1$ a new feature arrives, to test new features, USFS takes a similar strategy as Grafting to perform gradient test. Specifically, if the inclusion of the new feature is going to reduce the objective function in Eq.~(\\ref{eq:USFS}) at the new time step, the feature is accepted, otherwise the new feature can be removed. When new features are continuously being generated, they may take place of some existing features and some existing features may become outdated, therefore, USFS also investigates if it is necessary to remove any existing features by re-optimizing the model through A Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasinewton method~\\citep{boyd2004convex}.\n\n\\subsection{Feature Selection Algorithms with Data Streams}\nIn this subsection, we review the problem of feature selection with data streams which is considered as a dual problem of streaming feature selection. Most existing feature selection algorithms assume that all the data instances are available before performing feature selection. However, such assumptions are not always true in real-world applications that data instances are dynamically generated and arrive in a sequential manner. Therefore, it is necessary and urgent to come up with some solutions to deal with sequential data of high dimensionality.\n\n\\subsubsection{Online Feature Selection (Supervised)~\\citep{wang2014online}}\nIn~\\citep{wang2014online}, an online feature selection algorithm (OFS) for binary classification is proposed. Let $\\{{\\bf {x}}_{1},{\\bf {x}}_{2},...,{\\bf {x}}_{t}...\\}$ and $\\{y_{1},y_{2},...,y_{t}...\\}$ denote a sequence of input data instances and input class labels, where each data instance ${\\bf {x}}_{i}\\in\\mathbb{R}^{d}$ is in a $d$-dimensional space and class label $y_{i}\\in\\{-1,+1\\}$. The task of OFS is to learn a linear classifier ${\\bf {w}}^{(t)}\\in\\mathbb{R}^{d}$ that can be used to classify each instance ${\\bf {x}}_{i}$ by a linear function sign(${\\bf {w}}^{(t)}{\\bf {x}}_{i}$). To achieve the feature selection purpose, it requires that the linear classifier ${\\bf {w}}^{(t)}$ has at most $B$-nonzero elements such that $\\|{\\bf {w}}^{(t)}\\|_{0}\\leq B$. It indicates that at most $B$ features will be used for classification. With a regularization parameter $\\lambda$ and a step size $\\eta$, the algorithm of OFS works as follows:\n\\begin{itemize}\n  \\item \\emph{Step 1}: Get a new data instance ${\\bf {x}}_{t}$ and its class label $y_{t}$\n  \\item \\emph{Step 2}: Make a class label prediction sign(${\\bf {w}}^{(t)}{\\bf {x}}_{t}$) for the new instance\n  \\item \\emph{Step 3(a)}: If ${\\bf {x}}_{t}$ is misclassified such that $y_{i}{\\bf {w}}^{(t)}{\\bf {x}}_{t}<0$\n  \n", "index": 227, "text": "\\begin{align*}\n  \\tilde{{\\bf {w}}}_{t+1}&=(1-\\lambda\\eta){\\bf {w}}_{t}+\\eta y_{t}{\\bf {x}}_{t}\\\\\n  \\hat{{\\bf {w}}}_{t+1} &= \\min\\{1,1/\\sqrt{\\lambda}\\|\\tilde{{\\bf {w}}}_{t+1}\\|_{2}\\}\\tilde{{\\bf {w}}}_{t+1}\\\\\n  {\\bf {w}}_{t+1}&=Truncate(\\hat{{\\bf {w}}}_{t+1},B)\n  \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\tilde{{\\bf{w}}}_{t+1}\" display=\"inline\"><msub><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(1-\\lambda\\eta){\\bf{w}}_{t}+\\eta y_{t}{\\bf{x}}_{t}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03b7</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc30</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msub><mi>y</mi><mi>t</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>t</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\bf{w}}}_{t+1}\" display=\"inline\"><msub><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\min\\{1,1/\\sqrt{\\lambda}\\|\\tilde{{\\bf{w}}}_{t+1}\\|_{2}\\}\\tilde{{%&#10;\\bf{w}}}_{t+1}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mrow><mrow><mn>1</mn><mo>/</mo><msqrt><mi>\u03bb</mi></msqrt></mrow><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf{w}}_{t+1}\" display=\"inline\"><msub><mi>\ud835\udc30</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=Truncate(\\hat{{\\bf{w}}}_{t+1},B)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>T</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>\ud835\udc30</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {U}}^{(t)}_{k}\\in\\mathbb{R}^{n_{t}\\times k}$ are the top-$k$ left singular vectors of ${\\bf {X}}^{(t)}$, $\\alpha$ is a regularization parameter to avoid overfitting.\n\nThe bottleneck of Eq.~(\\ref{eq:FSDS_obj1}) is that the singular value decomposition of ${\\bf {X}}^{(t)}$ is computational expensive, especially when $n_{t}$ is very large. Therefore, FSDS utilizes the matrix sketching strategy from~\\citep{liberty2013simple} to maintain a low-rank approximation of ${\\bf {X}}^{(t)}$. Let ${\\bf {B}}^{(t)}\\in\\mathbb{R}^{\\ell\\times d}$ denote the matrix sketch of ${\\bf {X}}^{(t)}$ ($\\ell$). After some mathematical derivations, the objective of ridge regression is reformulated as follows:\n\n", "itemtype": "equation", "pos": 174335, "prevtext": "\n  \\item \\emph{Step 3(b)}: ${\\bf {w}}_{t+1}=(1-\\lambda\\eta){\\bf {w}}_{t}$\n  \\item \\emph{Step 4}: Repeat \\emph{Step 1} to \\emph{Step 3(a)} or \\emph{Step 3(b)} until no new data instances arrive.\n\\end{itemize}\nIn Step 3(a), each time when a training instance ${\\bf {x}}_{t}$ is misclassified, ${\\bf {w}}_{t}$ is first updated by online gradient descent and then it is projected to a $\\ell_{2}$-norm ball to ensure that the classifier is bounded. After that, the new classifier $\\hat{{\\bf {w}}}_{t+1}$ is truncated by taking the most important $B$ features. A subset of $B$ features is output at each time step. The process repeats until there are no new data instances arrive anymore.\n\n\\subsubsection{Unsupervised Feature Selection on Data Streams (Unsupervised)~\\citep{huang2015unsupervised}}\nOFS assumes that the class labels of continuously generated data streams are available. However, it is not the case in many real-world applications that label information is costly to obtain. To timely select a subset of relevant features when unlabeled data are continuously being generated, authors in~\\citep{huang2015unsupervised} propose a novel unsupervised feature selection method (FSDS) that is able to perform feature selection timely with only one pass of the data and utilize limited storage. The basic idea of FSDS is to use matrix sketching to efficiently maintain a low-rank approximation of the current observed data and then apply regularized regression to obtain the feature coefficients, which can further be used to obtain the importance of features. The authors empirically show that when some orthogonality conditions are satisfied, the ridge regression can replace the Lasso for feature selection, which is more computational efficient. Assume at a specific time step $t$, ${\\bf {X}}^{(t)}\\in\\mathbb{R}^{n_{t}\\times d}$ denotes the data matrix at that time step, its rank-$k$ approximation is ${\\bf {X}}^{(t)}_{k}=\\sum_{i=1}^{k}\\delta_{i}{\\bf {u}}_{i}{\\bf {v}}_{i}'$, where $\\delta_{i}$ ($i\\leq k$) are the top-$k$ singular values, ${\\bf {u}}_{i}$ and ${\\bf {v}}_{i}'$ are the corresponding left and right singular vectors, respectively. With these, the feature coefficients can be obtained by minimizing the following ridge regression problem:\n\n", "index": 229, "text": "\\begin{equation}\n\\min_{{\\bf {W}}^{(t)}}\\|{\\bf {X}}^{(t)}{\\bf {W}}^{(t)}-{\\bf {U}}^{(t)}_{k}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}^{(t)}\\|_{F}^{2},\n\\label{eq:FSDS_obj1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E114.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}^{(t)}}\\|{\\bf{X}}^{(t)}{\\bf{W}}^{(t)}-{\\bf{U}}^{(t)}_{k}\\|_{F}^{%&#10;2}+\\alpha\\|{\\bf{W}}^{(t)}\\|_{F}^{2},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>\ud835\udc17</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>-</mo><msubsup><mi>\ud835\udc14</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere ${\\bf {e}}_{i}\\in\\mathbb{R}^{\\ell}$ is a vector with its $i$-th location as 1 and other locations as 0. By solving the optimization problem in Eq.~(\\ref{eq:FSDS_obj2}), the importance of each feature $f_{i}$ can be computed as:\n\n", "itemtype": "equation", "pos": 175209, "prevtext": "\nwhere ${\\bf {U}}^{(t)}_{k}\\in\\mathbb{R}^{n_{t}\\times k}$ are the top-$k$ left singular vectors of ${\\bf {X}}^{(t)}$, $\\alpha$ is a regularization parameter to avoid overfitting.\n\nThe bottleneck of Eq.~(\\ref{eq:FSDS_obj1}) is that the singular value decomposition of ${\\bf {X}}^{(t)}$ is computational expensive, especially when $n_{t}$ is very large. Therefore, FSDS utilizes the matrix sketching strategy from~\\citep{liberty2013simple} to maintain a low-rank approximation of ${\\bf {X}}^{(t)}$. Let ${\\bf {B}}^{(t)}\\in\\mathbb{R}^{\\ell\\times d}$ denote the matrix sketch of ${\\bf {X}}^{(t)}$ ($\\ell$). After some mathematical derivations, the objective of ridge regression is reformulated as follows:\n\n", "index": 231, "text": "\\begin{equation}\n\\min_{{\\bf {W}}^{(t)}}\\|{\\bf {B}}^{(t)}{\\bf {W}}^{(t)}-\\{{\\bf {e}}_{1},...,{\\bf {e}}_{k}\\}\\|_{F}^{2}+\\alpha\\|{\\bf {W}}^{(t)}\\|_{F}^{2},\n\\label{eq:FSDS_obj2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E115.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\bf{W}}^{(t)}}\\|{\\bf{B}}^{(t)}{\\bf{W}}^{(t)}-\\{{\\bf{e}}_{1},...,{\\bf{e}%&#10;}_{k}\\}\\|_{F}^{2}+\\alpha\\|{\\bf{W}}^{(t)}\\|_{F}^{2},\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></munder><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msup><mi>\ud835\udc01</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>-</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>\ud835\udc1e</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>\ud835\udc1e</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nThe higher the feature score, the more important the feature is.\n\n\\section{Performance Evaluation}\nIn this section, we discuss the evaluation of feature selection algorithms, focusing on feature selection algorithms for generic data. We first introduce the developed feature selection repository, then we introduce the algorithms to be evaluated and some publicly available benchmark datasets we collect. At last, we introduce some widely adopted evaluation metrics and present the empirical experimental results.\n\\subsection{Feature Selection Repository}\nFirst, we introduce our efforts in developing a feature selection repository -- \\emph{scikit-feast}. The purpose of this feature selection repository is to collect some widely used feature selection algorithms that have been developed in the feature selection research to serve as a platform for facilitating their application, comparison and joint study. The feature selection repository also effectively assists researchers to achieve more reliable evaluation in the process of developing new feature selection algorithms.\n\nWe develop the open source feature selection repository \\emph{scikit-feast} by one of the most popular programming language -- python. It contains more than 40 popular feature selection algorithms, including most traditional feature selection algorithms mentioned in this survey and some structural and streaming feature selection algorithms. It is built upon one widely used machine learning package \\emph{scikit-learn} and two scientific computing packages \\emph{Numpy} and \\emph{Scipy}. At the same time, we also maintain a website (\\url{http://featureselection.asu.edu/scikit-feast/}) for this project which offers several sources such as public available benchmark datasets, performance evaluation of algorithms, test cases to run each algorithm. The source code of this repository is available at Github (\\url{https://github.com/jundongl/scikit-feast}).\n\n\n\\subsection{Algorithms}\nWe empirically evaluate the performance of feature selection algorithms for generic data provided in the repository. Next, we will provide detailed information how these algorithms are evaluated, including the datasets, evaluation criteria and experimental setup. The selected feature selection algorithms that will be evaluated are listed as follows. We list the following information of each algorithm: (1) supervised or unsupervised; (2) similarity based, information theoretical based, sparse learning based, or statistical based; (3) output: feature weighting or subset selection; (4) feature type: numerical or categorical. The first two items have been mentioned previously. The third item categorizes these algorithms based on the output. Feature weighing algorithms basically give each feature a score for ranking and feature subset algorithms only show which features are selected. The last item shows the feature types the feature selection method can handle with, continuous or discrete. For supervised feature selection methods, we also list if the method can tackle binary-class or multi-class classification problem.\n\\begin{enumerate}\n\\scriptsize\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \\item Fisher Score: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item ReliefF: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item Trace Ratio: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item Laplacian Score: unsupervised, similarity, feature weight, continuous and discrete\n  \\item SPEC: unsupervised, similarity, feature weight, continuous and discrete\n  \\item MIM: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item MIFS: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item MRMR: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item CIFE: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item JMI: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item CMIM: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item ICAP: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item DISR: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item FCBF: supervised, information theoretic, feature subset, discrete(multi-class)\n  \\item RFS: supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item Least square loss ($\\ell_{2,1}$): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item Logistic loss ($\\ell_{2,1}$): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item MCFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item UDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item NDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item Low variance: unsupervised, statistical, feature subset, discrete(binary-class)\n  \\item T-score: supervised, statistical, feature weight, continuous and discrete(binary-class)\n  \\item F-score: supervised, statistical, feature weight, continuous and discrete(multi-class)\n  \\item Chi-square: supervised, statistical, feature weight, discrete(multi-class)\n  \\item Gini Index: supervised, statistical, feature weight, discrete(multi-class)\n\\end{enumerate}\n\n\\subsection{Datasets}\nTo test different algorithms, we collect 25 public available benchmark datasets to evaluate the performance of feature selection algorithms. We list the detailed information of each dataset in Table~\\ref{table:datasets}. We carefully select datasets from different categories, e.g., text data, image data, biological data, and some others. The features in these datasets are either in numerical or categorical values. We also present the number of features, number of instances and the number of classes for each dataset. The heterogeneity of the data is important for exposing the\nstrength and weakness of algorithms in different applications.\n\\begin{table}[!t]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c|c|c} \\hline\nDataset        & Type  & Feature value & $\\#$ of Features & $\\#$ of Instances & $\\#$ of Classes \\\\ \\hline \\hline\nBASEHOCK       & Text  & Continuous & 4862   & 1993 & 2 \\\\ \\hline\nPCMAC          & Text  & Continuous & 3289   & 1943 & 2 \\\\ \\hline\nRELATHE        & Text  & Continuous & 4322   & 1427 & 2 \\\\ \\hline\nCOIL20         & Image & Continuous & 1024   & 1440 & 20\\\\ \\hline\nORL            & Image & Continuous & 1024   & 400  & 40\\\\ \\hline\norlraws10P     & Image & Continuous & 10304  & 100  & 10\\\\ \\hline\npixraw10P      & Image & Continuous & 10000  & 100  & 10\\\\ \\hline\nwarpAR10P      & Image & Continuous & 2400   & 130  & 10\\\\ \\hline\nwarpPIE10P     & Image & Continuous & 2420   & 210  & 10\\\\ \\hline\nYale           & Image & Continuous & 1024   & 165  & 15\\\\ \\hline\nUSPS           & Image & Continuous & 256    & 9298 & 10\\\\ \\hline\nALLAML         & Bio   & Continuous & 7129   & 72   & 2 \\\\ \\hline\nCarcinom       & Bio   & Continuous & 9182   & 174  & 11\\\\ \\hline\nCLL\\_SUB\\_111  & Bio   & Continuous & 11340  & 111  & 3 \\\\ \\hline\ncolon          & Bio   & Discrete & 2000   & 62   & 2 \\\\ \\hline\nGLA\\_BRA\\_180  & Bio   & Continuous   & 49151  & 180  & 4 \\\\ \\hline\nGLI\\_85        & Bio   & Continuous   & 22283  & 85   & 2 \\\\ \\hline\nGLIOMA         & Bio   & Continuous   & 4434   & 50   & 4 \\\\ \\hline\nleukemia       & Bio   & Discrete & 7070   & 72   & 2 \\\\ \\hline\nlung           & Bio   & Continuous   & 3312   & 203  & 5 \\\\ \\hline\nlung\\_small    & Bio   & Discrete & 325    & 73   & 7 \\\\ \\hline\nlymphoma       & Bio   & Discrete & 4026   & 96  & 9 \\\\ \\hline\nnci9           & Bio   & Discrete & 9712   & 60  & 9 \\\\ \\hline\ngisette        & Image & Continuous & 5000 & 7000 & 2 \\\\ \\hline\nProstate\\_GE   & Bio   & Continuous   & 5966   & 102 & 2 \\\\ \\hline\nSMK\\_CAN\\_187  & Bio   & Continuous   & 19993  & 187 & 2 \\\\ \\hline\nTOX\\_171       & Bio   & Continuous   & 5748   & 171 & 4 \\\\ \\hline\narcene         & Mass Spectrometry & Continuous & 10000 & 200 & 2 \\\\ \\hline\nIsolet         & Spoken letter & Continuous & 617 & 1560 & 26 \\\\ \\hline\nmadelon        & Artificial    & Continuous & 500 & 2600 & 2 \\\\ \\hline\n\n\\end{tabular}\n\\caption{Detailed information of benchmark datasets.}\n\\label{table:datasets}\n\\end{table}\n\n\\subsection{Evaluation Metrics}\nNext, we introduce the widely adopted way to evaluate the performance of feature selection algorithms. We have different evaluation metrics for supervised and unsupervised methods. For algorithms of different output types, different evaluation strategies are used:\n\\begin{enumerate}\n\\item If it is a feature weighting method that outputs the feature score for each feature, then the quality of the first $\\{5,10,15,...,295,300\\}$ features are evaluated respectively.\n\\item If it is a feature subset selection method that only outputs which features are selected, then we use all the selected features to perform the evaluation.\n\\end{enumerate}\n\\paragraph{Supervised Methods:}\nTo test performance of the supervised feature selection algorithms, the evaluation framework introduced in Figure~(\\ref{fig:ClassificationFramework}) is used. The whole dataset is usually divided into two parts - the training set $\\mathcal{T}$ and test set $\\mathcal{U}$. Feature selection algorithms will be first applied on the training set $\\mathcal{T}$ to obtain a subset of relevant features $\\mathcal{F}$. Then the test set on the selected features are acting as input to a classification model for the testing purpose. In the experiments, we use classification accuracy to evaluate the classification performance, and three classification models, Linear SVM, Decision Tree, Na\\\"{\\i}ve Bayes are used. To get more reliable results, we adopt 10-fold cross validation, and the final classification performance are reported as an average over 10 folds. The higher the average classification accuracy, the better the feature selection algorithm is.\n\n\\paragraph{Unsupervised Methods:}\nFollowing the standard way to assess unsupervised feature selection, we evaluate the unsupervised feature selection algorithms in terms of clustering performance. Two commonly used clustering performance metrics, i.e., \\emph{normalized mutual information} (NMI) and \\emph{accuracy} (ACC) are used.\n\nSpecifically, Let $C$ and $C'$ denote the clustering results from ground truth class labels and the predicted cluster labels, respectively. The mutual information between two clusters $C$ and $C'$ is:\n\n", "itemtype": "equation", "pos": 175632, "prevtext": "\nwhere ${\\bf {e}}_{i}\\in\\mathbb{R}^{\\ell}$ is a vector with its $i$-th location as 1 and other locations as 0. By solving the optimization problem in Eq.~(\\ref{eq:FSDS_obj2}), the importance of each feature $f_{i}$ can be computed as:\n\n", "index": 233, "text": "\\begin{equation}\nscore(j)=\\max_{i}|{\\bf {W}}^{(t)}(j,i)|.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E116.m1\" class=\"ltx_Math\" alttext=\"score(j)=\\max_{i}|{\\bf{W}}^{(t)}(j,i)|.\" display=\"block\"><mrow><mrow><mrow><mi>s</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mi>i</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $p(c_{i})$ and $p(c'_{j})$ are the probabilities of instances in cluster $c_{i}$ and $c'_{j}$, respectively. $p(c_{i},c'_{j})$ indicates the probability of instances in cluster $c_{i}$ and in $c'_{j}$ at the same time. Then, NMI is defined as:\n\n", "itemtype": "equation", "pos": 186313, "prevtext": "\nThe higher the feature score, the more important the feature is.\n\n\\section{Performance Evaluation}\nIn this section, we discuss the evaluation of feature selection algorithms, focusing on feature selection algorithms for generic data. We first introduce the developed feature selection repository, then we introduce the algorithms to be evaluated and some publicly available benchmark datasets we collect. At last, we introduce some widely adopted evaluation metrics and present the empirical experimental results.\n\\subsection{Feature Selection Repository}\nFirst, we introduce our efforts in developing a feature selection repository -- \\emph{scikit-feast}. The purpose of this feature selection repository is to collect some widely used feature selection algorithms that have been developed in the feature selection research to serve as a platform for facilitating their application, comparison and joint study. The feature selection repository also effectively assists researchers to achieve more reliable evaluation in the process of developing new feature selection algorithms.\n\nWe develop the open source feature selection repository \\emph{scikit-feast} by one of the most popular programming language -- python. It contains more than 40 popular feature selection algorithms, including most traditional feature selection algorithms mentioned in this survey and some structural and streaming feature selection algorithms. It is built upon one widely used machine learning package \\emph{scikit-learn} and two scientific computing packages \\emph{Numpy} and \\emph{Scipy}. At the same time, we also maintain a website (\\url{http://featureselection.asu.edu/scikit-feast/}) for this project which offers several sources such as public available benchmark datasets, performance evaluation of algorithms, test cases to run each algorithm. The source code of this repository is available at Github (\\url{https://github.com/jundongl/scikit-feast}).\n\n\n\\subsection{Algorithms}\nWe empirically evaluate the performance of feature selection algorithms for generic data provided in the repository. Next, we will provide detailed information how these algorithms are evaluated, including the datasets, evaluation criteria and experimental setup. The selected feature selection algorithms that will be evaluated are listed as follows. We list the following information of each algorithm: (1) supervised or unsupervised; (2) similarity based, information theoretical based, sparse learning based, or statistical based; (3) output: feature weighting or subset selection; (4) feature type: numerical or categorical. The first two items have been mentioned previously. The third item categorizes these algorithms based on the output. Feature weighing algorithms basically give each feature a score for ranking and feature subset algorithms only show which features are selected. The last item shows the feature types the feature selection method can handle with, continuous or discrete. For supervised feature selection methods, we also list if the method can tackle binary-class or multi-class classification problem.\n\\begin{enumerate}\n\\scriptsize\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \\item Fisher Score: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item ReliefF: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item Trace Ratio: supervised, similarity, feature weight, continuous and discrete(multi-class)\n  \\item Laplacian Score: unsupervised, similarity, feature weight, continuous and discrete\n  \\item SPEC: unsupervised, similarity, feature weight, continuous and discrete\n  \\item MIM: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item MIFS: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item MRMR: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item CIFE: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item JMI: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item CMIM: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item ICAP: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item DISR: supervised, information theoretic, feature weight, discrete(multi-class)\n  \\item FCBF: supervised, information theoretic, feature subset, discrete(multi-class)\n  \\item RFS: supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item Least square loss ($\\ell_{2,1}$): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item Logistic loss ($\\ell_{2,1}$): supervised, sparse learning, feature weight, continuous and discrete(multi-class)\n  \\item MCFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item UDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item NDFS: unsupervised, sparse learning, feature weight, continuous and discrete\n  \\item Low variance: unsupervised, statistical, feature subset, discrete(binary-class)\n  \\item T-score: supervised, statistical, feature weight, continuous and discrete(binary-class)\n  \\item F-score: supervised, statistical, feature weight, continuous and discrete(multi-class)\n  \\item Chi-square: supervised, statistical, feature weight, discrete(multi-class)\n  \\item Gini Index: supervised, statistical, feature weight, discrete(multi-class)\n\\end{enumerate}\n\n\\subsection{Datasets}\nTo test different algorithms, we collect 25 public available benchmark datasets to evaluate the performance of feature selection algorithms. We list the detailed information of each dataset in Table~\\ref{table:datasets}. We carefully select datasets from different categories, e.g., text data, image data, biological data, and some others. The features in these datasets are either in numerical or categorical values. We also present the number of features, number of instances and the number of classes for each dataset. The heterogeneity of the data is important for exposing the\nstrength and weakness of algorithms in different applications.\n\\begin{table}[!t]\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|c|c|c|c} \\hline\nDataset        & Type  & Feature value & $\\#$ of Features & $\\#$ of Instances & $\\#$ of Classes \\\\ \\hline \\hline\nBASEHOCK       & Text  & Continuous & 4862   & 1993 & 2 \\\\ \\hline\nPCMAC          & Text  & Continuous & 3289   & 1943 & 2 \\\\ \\hline\nRELATHE        & Text  & Continuous & 4322   & 1427 & 2 \\\\ \\hline\nCOIL20         & Image & Continuous & 1024   & 1440 & 20\\\\ \\hline\nORL            & Image & Continuous & 1024   & 400  & 40\\\\ \\hline\norlraws10P     & Image & Continuous & 10304  & 100  & 10\\\\ \\hline\npixraw10P      & Image & Continuous & 10000  & 100  & 10\\\\ \\hline\nwarpAR10P      & Image & Continuous & 2400   & 130  & 10\\\\ \\hline\nwarpPIE10P     & Image & Continuous & 2420   & 210  & 10\\\\ \\hline\nYale           & Image & Continuous & 1024   & 165  & 15\\\\ \\hline\nUSPS           & Image & Continuous & 256    & 9298 & 10\\\\ \\hline\nALLAML         & Bio   & Continuous & 7129   & 72   & 2 \\\\ \\hline\nCarcinom       & Bio   & Continuous & 9182   & 174  & 11\\\\ \\hline\nCLL\\_SUB\\_111  & Bio   & Continuous & 11340  & 111  & 3 \\\\ \\hline\ncolon          & Bio   & Discrete & 2000   & 62   & 2 \\\\ \\hline\nGLA\\_BRA\\_180  & Bio   & Continuous   & 49151  & 180  & 4 \\\\ \\hline\nGLI\\_85        & Bio   & Continuous   & 22283  & 85   & 2 \\\\ \\hline\nGLIOMA         & Bio   & Continuous   & 4434   & 50   & 4 \\\\ \\hline\nleukemia       & Bio   & Discrete & 7070   & 72   & 2 \\\\ \\hline\nlung           & Bio   & Continuous   & 3312   & 203  & 5 \\\\ \\hline\nlung\\_small    & Bio   & Discrete & 325    & 73   & 7 \\\\ \\hline\nlymphoma       & Bio   & Discrete & 4026   & 96  & 9 \\\\ \\hline\nnci9           & Bio   & Discrete & 9712   & 60  & 9 \\\\ \\hline\ngisette        & Image & Continuous & 5000 & 7000 & 2 \\\\ \\hline\nProstate\\_GE   & Bio   & Continuous   & 5966   & 102 & 2 \\\\ \\hline\nSMK\\_CAN\\_187  & Bio   & Continuous   & 19993  & 187 & 2 \\\\ \\hline\nTOX\\_171       & Bio   & Continuous   & 5748   & 171 & 4 \\\\ \\hline\narcene         & Mass Spectrometry & Continuous & 10000 & 200 & 2 \\\\ \\hline\nIsolet         & Spoken letter & Continuous & 617 & 1560 & 26 \\\\ \\hline\nmadelon        & Artificial    & Continuous & 500 & 2600 & 2 \\\\ \\hline\n\n\\end{tabular}\n\\caption{Detailed information of benchmark datasets.}\n\\label{table:datasets}\n\\end{table}\n\n\\subsection{Evaluation Metrics}\nNext, we introduce the widely adopted way to evaluate the performance of feature selection algorithms. We have different evaluation metrics for supervised and unsupervised methods. For algorithms of different output types, different evaluation strategies are used:\n\\begin{enumerate}\n\\item If it is a feature weighting method that outputs the feature score for each feature, then the quality of the first $\\{5,10,15,...,295,300\\}$ features are evaluated respectively.\n\\item If it is a feature subset selection method that only outputs which features are selected, then we use all the selected features to perform the evaluation.\n\\end{enumerate}\n\\paragraph{Supervised Methods:}\nTo test performance of the supervised feature selection algorithms, the evaluation framework introduced in Figure~(\\ref{fig:ClassificationFramework}) is used. The whole dataset is usually divided into two parts - the training set $\\mathcal{T}$ and test set $\\mathcal{U}$. Feature selection algorithms will be first applied on the training set $\\mathcal{T}$ to obtain a subset of relevant features $\\mathcal{F}$. Then the test set on the selected features are acting as input to a classification model for the testing purpose. In the experiments, we use classification accuracy to evaluate the classification performance, and three classification models, Linear SVM, Decision Tree, Na\\\"{\\i}ve Bayes are used. To get more reliable results, we adopt 10-fold cross validation, and the final classification performance are reported as an average over 10 folds. The higher the average classification accuracy, the better the feature selection algorithm is.\n\n\\paragraph{Unsupervised Methods:}\nFollowing the standard way to assess unsupervised feature selection, we evaluate the unsupervised feature selection algorithms in terms of clustering performance. Two commonly used clustering performance metrics, i.e., \\emph{normalized mutual information} (NMI) and \\emph{accuracy} (ACC) are used.\n\nSpecifically, Let $C$ and $C'$ denote the clustering results from ground truth class labels and the predicted cluster labels, respectively. The mutual information between two clusters $C$ and $C'$ is:\n\n", "index": 235, "text": "\\begin{equation}\nMI(C,C')=\\sum_{c_{i}\\in C, c'_{j}\\in C'}p(c_{i},c'_{j})log\\frac{p(c_{i},c'_{j})}{p(c_{i})p(c'_{j})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E117.m1\" class=\"ltx_Math\" alttext=\"MI(C,C^{\\prime})=\\sum_{c_{i}\\in C,c^{\\prime}_{j}\\in C^{\\prime}}p(c_{i},c^{%&#10;\\prime}_{j})log\\frac{p(c_{i},c^{\\prime}_{j})}{p(c_{i})p(c^{\\prime}_{j})}\" display=\"block\"><mrow><mrow><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>\u2208</mo><mi>C</mi></mrow><mo>,</mo><mrow><msubsup><mi>c</mi><mi>j</mi><mo>\u2032</mo></msubsup><mo>\u2208</mo><msup><mi>C</mi><mo>\u2032</mo></msup></mrow></mrow></munder><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>c</mi><mi>j</mi><mo>\u2032</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>c</mi><mi>j</mi><mo>\u2032</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>c</mi><mi>j</mi><mo>\u2032</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $H(C)$ and $H(C')$ represent the entropies of clusterings $C$ and $C'$, respectively.\n\nLet $p_{i}$ and $q_{i}$ be the clustering result and the ground truth label for instance $u_{i}$, respectively. Then, accuracy (ACC) is defined as:\n\n", "itemtype": "equation", "pos": 186695, "prevtext": "\nwhere $p(c_{i})$ and $p(c'_{j})$ are the probabilities of instances in cluster $c_{i}$ and $c'_{j}$, respectively. $p(c_{i},c'_{j})$ indicates the probability of instances in cluster $c_{i}$ and in $c'_{j}$ at the same time. Then, NMI is defined as:\n\n", "index": 237, "text": "\\begin{equation}\nNMI(C,C')=\\frac{MI(C,C')}{max(H(C),H(C')}\n\\label{eq:nmi}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E118.m1\" class=\"ltx_Math\" alttext=\"NMI(C,C^{\\prime})=\\frac{MI(C,C^{\\prime})}{max(H(C),H(C^{\\prime})}\" display=\"block\"><mrow><mrow><mi>N</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>M</mi><mo>\u2062</mo><mi>I</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo>,</mo><msup><mi>C</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>C</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.07996.tex", "nexttext": "\nwhere $n$ is the total number of data instances, $\\delta(.)$ is an indicator function such that $\\delta(x,y)=1$ if $x=y$, otherwise $\\delta(x,y)=0$. $map(x)$ permutes the predicted cluster labels to match the ground truth as much as possible.\n\nEach feature selection algorithm is first applied to select features, then K-means clustering is performed based on the selected features. We repeat the K-means algorithm 20 times and report the average clustering results since K-means may converge to local optimal.\n\n\\subsection{Experimental Results}\nDue to space limit, we do not list the evaluation results here. The experimental results are shown in \\url{http://featureselection.asu.edu/scikit-feast/datasets.php}). For each dataset, we list all applicable feature selection algorithms along with its evaluation on either classification or clustering task.\n\n\\section{Open Problems and Challenges}\nOver the past two decades, there are tremendous amount of work in developing feature selection algorithms for both theoretical analysis and real-world applications. However, we still believe there are more work can be done in this community. Here are several challenges and concerns that we need to mention and discuss.\n\n\\subsection{Scalability}\nWith the tremendous growth of dataset sizes, the scalability of most current feature selection algorithms may be jeopardized. In many scientific and business applications, data are usually measured in terabyte (1TB = $10^{12}$ bytes). Normally, datasets in the scale of terabytes cannot be loaded into the memory directly and therefore limits the usability of most feature selection algorithms. Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very large scale datasets~\\citep{singh2009parallel,zhao2013massively,yamada2014n}. In addition, most feature selection algorithms proposed so far require time complexity proportional to $O(d^{2})$ or even $O(d^{3})$, where $d$ is the feature dimension. Recently, big data of ultrahigh dimensionality has emerged in many real-world applications such as text mining and information retrieval. Most feature selection algorithms does not scale well on the ultrahigh dimensional data, its efficiency deteriorates quickly or is even computational infeasible. In this case, well-designed feature selection algorithms in linear or sub-linear running time is more preferred~\\citep{fan2009ultrahigh,tan2014towards}. Moreover, in some online classification or online clustering tasks, the scalability of feature selection algorithms is also a big issue. For example, the data streams or feature streams may be infinite and cannot be loaded into the memory, therefore we can only make one pass of the data where the second pass is either unavailable or computational expensive. Even though feature selection algorithms can reduce the issue of scalability for online classification or clustering, these methods either require to keep full dimensionality in the memory or require iterative processes to visit data instances more than once, which limit their practical usages. In conclusion, even though there are preliminary work to increase the scalability of feature selection algorithms, we believe that the scalability problem should be given more attention to keep pace with the rapid growth of very large-scale and fast streaming data.\n\n\\subsection{Stability}\nFor supervised feature selection algorithms, their performance are usually evaluated by the classification accuracy. In addition to accuracy, the stability of these algorithms is also an important consideration when developing new feature selection algorithms. A motivating example from the field of bioinformatics shows that domain experts would like to see the same set or similar set of genes (features) to be selected each time when they obtain new samples in the small amount of perturbation. Otherwise domain experts would not trust these algorithms when they get quite different sets of features with small data perturbation. Considering its importance in practical usage, stability of feature selection algorithms has received increasing attention in the community~\\citep{kalousis2007stability,he2010stable}. It is observed that many well-known feature selection algorithms suffer from the low stability problem after the small data perturbation is introduced in the training set. It is also found in~\\citep{alelyani2011effect} that the underlying characteristics of data may greatly affect the stability of feature selection algorithms and the stability issue may also be data dependent. These factors include the dimensionality of feature, number of data instances, etc.\n\nIn against with supervised feature selection, stability of unsupervised feature selection algorithms has not be well studied yet. Studying stability for unsupervised feature selection is much more difficult than that of the supervised methods. The reason is that in unsupervised feature selection, we do not have enough prior knowledge about the cluster structure of the data. Thus we are uncertain that if the new data instance, i.e., the perturbation belongs to any existing clusters or will introduce new clusters. While in supervised feature selection, we have the prior knowledge about the label of each data instance, and a new sample that does not belong to any existing classes will be considered as an outlier and we do not need to modify the selected feature set to adapt to the outliers. In other words, unsupervised feature selection is more sensitive to noise and the noise will affects the stability of the algorithm.\n\n\\subsection{Model Selection}\nFor most feature selection algorithms especially for feature weighting methods, we have to specify the number of selected features. However, it is often unknown what is the optimal number of selected features. With too large number of selected features, it may increase the risk in including some noisy, redundant and irrelevant features which may jeopardize the learning performance. On the other hand, it is also not good to include too small number of selected features, since some relevant features may be eliminated. In practice, we usually adopt a heuristic way to grid search the number of selected features and pick the number that has the best classification or clustering performance, but the whole process is computational expensive. It is still an open and challenging problem to determine the optimal number of selected features.\n\nIn addition to the optimal number of selected features, we also need to specify the number of clusters or pseudo classes for unsupervised feature selection algorithms. In real world problems, we usually have limited knowledge about the clustering structure of the data. Choosing different number of clusters may lead to merging totally different small clusters into one big cluster or splitting one big cluster into smaller ones. As a consequence, it may result in finding totally different subsets of features. Hence, determining the optimal number of clusters is almost impossible. Some work has been done to estimate these tricky parameters. For instance, in~\\citep{tibshirani2001estimating}, a principled way to estimate the number of suitable clusters in a dataset is proposed. However, it is still not clear how to find the best number of clusters for unsupervised feature selection. All in all, we believe that the model selection problem should be paid more attention.\n\n\\section{Conclusion}\nFeature selection is effective in preprocessing data and reducing data dimensionality that is an essential to successful data mining and machine learning applications. Meanwhile, it has been a hot research topic with practical significance in many areas such as statistics, pattern recognition, machine learning, and data mining (including web, text, image, and microarrays). The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and helping prepare, clean, and understand data. The past few years have witnessed the development of hundreds of new feature selection methods. This survey article aims to provide a comprehensive overview about recent advances in feature selection. We first introduce basic concepts of feature selection and emphasis the importance of applying feature selection algorithms to solve practical problems. Then, we classify traditional feature selection algorithms from label perspective and search strategy perspective. Since current categorization cannot meet the rapid development in feature selection, we propose to review recent advances in feature selection algorithms from a data perspective. Following the taxonomy in Figure~(\\ref{fig:Categorization}), we surveyed the family of feature selection algorithms in four parts: (1) feature Selection with generic Data; (2) feature selection with structure features; (3) feature selection with heterogeneous data; and (4) feature selection with streaming data. Specifically, we further classify feature selection algorithms with generic data into similarity based, information theoretical based, sparse learning based and statistical based methods from their properties. For feature selection with structure features, we consider three types of structural features, namely group, tree and graph features. The third part feature selection with heterogeneous data consists of feature selection algorithms for linked data, multi-source and multi-view data. The fourth part consists of feature selection for streaming data and streaming features. To facilitate the research on feature selection, in this survey, we also present a feature selection repository - \\emph{scikit-feast}, which includes some of the most popular feature selection algorithms that have been developed in the past few decades. We also provide some suggestions on how to evaluate these feature selection algorithms, either supervised or unsupervised methods. At the end of the survey, we present some open problems and challenges that need to be paid more attention in the future feature selection research.\n\nIt also should be mentioned that the aim of the survey is not to claim the superiority of some feature selection algorithms over others. On the other hand, our goal is to provide a comprehensive structured list of recent advances of feature selection algorithms and a feature selection repository to promote the research in this community. As a matter of fact, it is at the discretion of users to decide which algorithms or which tools to use in practice.\n\n\\begin{thebibliography}{165}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Airoldi et~al.(2009)Airoldi, Blei, Fienberg, and\n  Xing]{airoldi2009mixed}\nEdoardo~M Airoldi, David~M Blei, Stephen~E Fienberg, and Eric~P Xing.\n\\newblock Mixed membership stochastic blockmodels.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  33--40, 2009.\n\n\\bibitem[Alelyani et~al.(2011)Alelyani, Liu, and Wang]{alelyani2011effect}\nSalem Alelyani, Huan Liu, and Lei Wang.\n\\newblock The effect of the characteristics of the dataset on the selection\n  stability.\n\\newblock In \\emph{Tools with Artificial Intelligence (ICTAI), 2011 23rd IEEE\n  International Conference on}, pages 970--977. IEEE, 2011.\n\n\\bibitem[Alelyani et~al.(2013)Alelyani, Tang, and Liu]{alelyani2013feature}\nSalem Alelyani, Jiliang Tang, and Huan Liu.\n\\newblock Feature selection for clustering: A review.\n\\newblock \\emph{Data Clustering: Algorithms and Applications}, 29, 2013.\n\n\\bibitem[Bach(2008)]{bach2008consistency}\nFrancis~R Bach.\n\\newblock Consistency of the group lasso and multiple kernel learning.\n\\newblock \\emph{The Journal of Machine Learning Research}, 9:\\penalty0\n  1179--1225, 2008.\n\n\\bibitem[Bache and Lichman(2013)]{bache2013uci}\nKevin Bache and Moshe Lichman.\n\\newblock Uci machine learning repository, 2013.\n\n\\bibitem[Battiti(1994)]{battiti1994using}\nRoberto Battiti.\n\\newblock Using mutual information for selecting features in supervised neural\n  net learning.\n\\newblock \\emph{Neural Networks, IEEE Transactions on}, 5\\penalty0\n  (4):\\penalty0 537--550, 1994.\n\n\\bibitem[Bi et~al.(2008)Bi, Xiong, Yu, Dundar, and Rao]{bi2008improved}\nJinbo Bi, Tao Xiong, Shipeng Yu, Murat Dundar, and R~Bharat Rao.\n\\newblock An improved multi-task learning approach with applications in medical\n  diagnosis.\n\\newblock In \\emph{Machine Learning and Knowledge Discovery in Databases},\n  pages 117--132. Springer, 2008.\n\n\\bibitem[Bilgic et~al.(2010)Bilgic, Mihalkova, and Getoor]{bilgic2010active}\nMustafa Bilgic, Lilyana Mihalkova, and Lise Getoor.\n\\newblock Active learning for networked data.\n\\newblock In \\emph{Proceedings of the 27th international conference on machine\n  learning (ICML-10)}, pages 79--86, 2010.\n\n\\bibitem[Bondell and Reich(2008)]{bondell2008simultaneous}\nHoward~D Bondell and Brian~J Reich.\n\\newblock Simultaneous regression shrinkage, variable selection, and supervised\n  clustering of predictors with oscar.\n\\newblock \\emph{Biometrics}, 64\\penalty0 (1):\\penalty0 115--123, 2008.\n\n\\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}\nStephen Boyd and Lieven Vandenberghe.\n\\newblock \\emph{Convex optimization}.\n\\newblock Cambridge university press, 2004.\n\n\\bibitem[Brown et~al.(2012)Brown, Pocock, Zhao, and\n  Luj{\\'a}n]{brown2012conditional}\nGavin Brown, Adam Pocock, Ming-Jie Zhao, and Mikel Luj{\\'a}n.\n\\newblock Conditional likelihood maximisation: a unifying framework for\n  information theoretic feature selection.\n\\newblock \\emph{The Journal of Machine Learning Research}, 13\\penalty0\n  (1):\\penalty0 27--66, 2012.\n\n\\bibitem[Cai et~al.(2010)Cai, Zhang, and He]{cai2010unsupervised}\nDeng Cai, Chiyuan Zhang, and Xiaofei He.\n\\newblock Unsupervised feature selection for multi-cluster data.\n\\newblock In \\emph{Proceedings of the 16th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 333--342. ACM, 2010.\n\n\\bibitem[Candes and Tao(2007)]{candes2007dantzig}\nEmmanuel Candes and Terence Tao.\n\\newblock The dantzig selector: statistical estimation when p is much larger\n  than n.\n\\newblock \\emph{The Annals of Statistics}, pages 2313--2351, 2007.\n\n\\bibitem[Chan et~al.(1994)Chan, Schlag, and Zien]{chan1994spectral}\nPak~K Chan, Martine~DF Schlag, and Jason~Y Zien.\n\\newblock Spectral k-way ratio-cut partitioning and clustering.\n\\newblock \\emph{Computer-Aided Design of Integrated Circuits and Systems, IEEE\n  Transactions on}, 13\\penalty0 (9):\\penalty0 1088--1096, 1994.\n\n\\bibitem[Chandrashekar and Sahin(2014)]{chandrashekar2014survey}\nGirish Chandrashekar and Ferat Sahin.\n\\newblock A survey on feature selection methods.\n\\newblock \\emph{Computers \\& Electrical Engineering}, 40\\penalty0 (1):\\penalty0\n  16--28, 2014.\n\n\\bibitem[Chung(1997)]{chung1997spectral}\nFan~RK Chung.\n\\newblock \\emph{Spectral graph theory}, volume~92.\n\\newblock American Mathematical Soc., 1997.\n\n\\bibitem[Cortes and Vapnik(1995)]{cortes1995support}\nCorinna Cortes and Vladimir Vapnik.\n\\newblock Support-vector networks.\n\\newblock \\emph{Machine learning}, 20\\penalty0 (3):\\penalty0 273--297, 1995.\n\n\\bibitem[Cover and Thomas(2012)]{cover2012elements}\nThomas~M Cover and Joy~A Thomas.\n\\newblock \\emph{Elements of information theory}.\n\\newblock John Wiley \\& Sons, 2012.\n\n\\bibitem[d'Aspremont et~al.(2007)d'Aspremont, El~Ghaoui, Jordan, and\n  Lanckriet]{d2007direct}\nAlexandre d'Aspremont, Laurent El~Ghaoui, Michael~I Jordan, and Gert~RG\n  Lanckriet.\n\\newblock A direct formulation for sparse pca using semidefinite programming.\n\\newblock \\emph{SIAM review}, 49\\penalty0 (3):\\penalty0 434--448, 2007.\n\n\\bibitem[Davis and Sampson(1986)]{davis1986statistics}\nJohn~C Davis and Robert~J Sampson.\n\\newblock \\emph{Statistics and data analysis in geology}, volume 646.\n\\newblock Wiley New York et al., 1986.\n\n\\bibitem[Dhillon et~al.(2010)Dhillon, Foster, and Ungar]{dhillon2010feature}\nParamveer~S Dhillon, Dean~P Foster, and Lyle~H Ungar.\n\\newblock Feature selection using multiple streams.\n\\newblock In \\emph{International Conference on Artificial Intelligence and\n  Statistics}, pages 153--160, 2010.\n\n\\bibitem[Ding et~al.(2006)Ding, Zhou, He, and Zha]{ding2006r}\nChris Ding, Ding Zhou, Xiaofeng He, and Hongyuan Zha.\n\\newblock R 1-pca: rotational invariant l 1-norm principal component analysis\n  for robust subspace factorization.\n\\newblock In \\emph{Proceedings of the 23rd international conference on Machine\n  learning}, pages 281--288. ACM, 2006.\n\n\\bibitem[Dougherty et~al.(1995)Dougherty, Kohavi, Sahami,\n  et~al.]{dougherty1995supervised}\nJames Dougherty, Ron Kohavi, Mehran Sahami, et~al.\n\\newblock Supervised and unsupervised discretization of continuous features.\n\\newblock In \\emph{Machine learning: proceedings of the twelfth international\n  conference}, volume~12, pages 194--202, 1995.\n\n\\bibitem[Du and Shen(2015)]{du2015unsupervised}\nLiang Du and Yi-Dong Shen.\n\\newblock Unsupervised feature selection with adaptive structure learning.\n\\newblock \\emph{arXiv preprint arXiv:1504.00736}, 2015.\n\n\\bibitem[Duda et~al.(2012)Duda, Hart, and Stork]{duda2012pattern}\nRichard~O Duda, Peter~E Hart, and David~G Stork.\n\\newblock \\emph{Pattern classification}.\n\\newblock John Wiley \\& Sons, 2012.\n\n\\bibitem[Efron et~al.(2004)Efron, Hastie, Johnstone, Tibshirani,\n  et~al.]{efron2004least}\nBradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et~al.\n\\newblock Least angle regression.\n\\newblock \\emph{The Annals of statistics}, 32\\penalty0 (2):\\penalty0 407--499,\n  2004.\n\n\\bibitem[El~Akadi et~al.(2008)El~Akadi, El~Ouardighi, and\n  Aboutajdine]{el2008powerful}\nAli El~Akadi, Abdeljalil El~Ouardighi, and Driss Aboutajdine.\n\\newblock A powerful feature selection approach based on mutual information.\n\\newblock \\emph{International Journal of Computer Science and Network\n  Security}, 8\\penalty0 (4):\\penalty0 116, 2008.\n\n\\bibitem[Evgeniou and Pontil(2007)]{evgeniou2007multi}\nA~Evgeniou and Massimiliano Pontil.\n\\newblock Multi-task feature learning.\n\\newblock \\emph{Advances in neural information processing systems},\n  19:\\penalty0 41, 2007.\n\n\\bibitem[Fan and Li(2001)]{fan2001variable}\nJianqing Fan and Runze Li.\n\\newblock Variable selection via nonconcave penalized likelihood and its oracle\n  properties.\n\\newblock \\emph{Journal of the American statistical Association}, 96\\penalty0\n  (456):\\penalty0 1348--1360, 2001.\n\n\\bibitem[Fan et~al.(2009)Fan, Samworth, and Wu]{fan2009ultrahigh}\nJianqing Fan, Richard Samworth, and Yichao Wu.\n\\newblock Ultrahigh dimensional feature selection: beyond the linear model.\n\\newblock \\emph{The Journal of Machine Learning Research}, 10:\\penalty0\n  2013--2038, 2009.\n\n\\bibitem[Farahat et~al.(2011)Farahat, Ghodsi, and Kamel]{farahat2011efficient}\nAhmed~K Farahat, Ali Ghodsi, and Mohamed~S Kamel.\n\\newblock An efficient greedy method for unsupervised feature selection.\n\\newblock In \\emph{Data Mining (ICDM), 2011 IEEE 11th International Conference\n  on}, pages 161--170. IEEE, 2011.\n\n\\bibitem[Fellbaum(1998)]{fellbaum1998wordnet}\nChristiane Fellbaum.\n\\newblock \\emph{WordNet}.\n\\newblock Wiley Online Library, 1998.\n\n\\bibitem[Feng et~al.(2013)Feng, Xiao, Zhuang, and Liu]{feng2013adaptive}\nYinfu Feng, Jun Xiao, Yueting Zhuang, and Xiaoming Liu.\n\\newblock Adaptive unsupervised multi-view feature selection for visual concept\n  recognition.\n\\newblock In \\emph{Computer Vision--ACCV 2012}, pages 343--357. Springer, 2013.\n\n\\bibitem[Fleuret(2004)]{fleuret2004fast}\nFran{\\c{c}}ois Fleuret.\n\\newblock Fast binary feature selection with conditional mutual information.\n\\newblock \\emph{The Journal of Machine Learning Research}, 5:\\penalty0\n  1531--1555, 2004.\n\n\\bibitem[Friedman et~al.(2010)Friedman, Hastie, and\n  Tibshirani]{friedman2010note}\nJerome Friedman, Trevor Hastie, and Robert Tibshirani.\n\\newblock A note on the group lasso and a sparse group lasso.\n\\newblock \\emph{arXiv preprint arXiv:1001.0736}, 2010.\n\n\\bibitem[Fukunaga(2013)]{fukunaga2013introduction}\nKeinosuke Fukunaga.\n\\newblock \\emph{Introduction to statistical pattern recognition}.\n\\newblock Academic press, 2013.\n\n\\bibitem[Gini(1912)]{gini1912variability}\nCW~Gini.\n\\newblock Variability and mutability, contribution to the study of statistical\n  distribution and relaitons.\n\\newblock \\emph{Studi Economico-Giuricici della R}, 1912.\n\n\\bibitem[Golberg(1989)]{golberg1989genetic}\nDavid~E Golberg.\n\\newblock Genetic algorithms in search, optimization, and machine learning.\n\\newblock \\emph{Addion wesley}, 1989, 1989.\n\n\\bibitem[Golub and Van~Loan(2012)]{golub2012matrix}\nGene~H Golub and Charles~F Van~Loan.\n\\newblock \\emph{Matrix computations}, volume~3.\n\\newblock JHU Press, 2012.\n\n\\bibitem[Gu and Han(2011)]{gu2011towards}\nQuanquan Gu and Jiawei Han.\n\\newblock Towards feature selection in network.\n\\newblock In \\emph{Proceedings of the 20th ACM international conference on\n  Information and knowledge management}, pages 1175--1184. ACM, 2011.\n\n\\bibitem[Gu et~al.(2011)Gu, Li, and Han]{gu2011generalized}\nQuanquan Gu, Zhenhui Li, and Jiawei Han.\n\\newblock Generalized fisher score for feature selection.\n\\newblock In \\emph{Proceedings of the 27th Conference on Uncertainty in\n  Artificial Intelligence}, pages 266--273, 2011.\n\n\\bibitem[Guo and Nixon(2009)]{guo2009gait}\nBaofeng Guo and Mark~S Nixon.\n\\newblock Gait feature subset selection by mutual information.\n\\newblock \\emph{Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE\n  Transactions on}, 39\\penalty0 (1):\\penalty0 36--46, 2009.\n\n\\bibitem[Guyon and Elisseeff(2003)]{guyon2003introduction}\nIsabelle Guyon and Andr{\\'e} Elisseeff.\n\\newblock An introduction to variable and feature selection.\n\\newblock \\emph{The Journal of Machine Learning Research}, 3:\\penalty0\n  1157--1182, 2003.\n\n\\bibitem[Guyon et~al.(2002)Guyon, Weston, Barnhill, and Vapnik]{guyon2002gene}\nIsabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik.\n\\newblock Gene selection for cancer classification using support vector\n  machines.\n\\newblock \\emph{Machine learning}, 46\\penalty0 (1-3):\\penalty0 389--422, 2002.\n\n\\bibitem[Guyon et~al.(2008)Guyon, Gunn, Nikravesh, and Zadeh]{guyon2008feature}\nIsabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti~A Zadeh.\n\\newblock \\emph{Feature extraction: foundations and applications}, volume 207.\n\\newblock Springer, 2008.\n\n\\bibitem[Hall and Smith(1999)]{hall1999feature}\nMark~A Hall and Lloyd~A Smith.\n\\newblock Feature selection for machine learning: Comparing a correlation-based\n  filter approach to the wrapper.\n\\newblock In \\emph{FLAIRS conference}, volume 1999, pages 235--239, 1999.\n\n\\bibitem[Hardoon et~al.(2004)Hardoon, Szedmak, and\n  Shawe-Taylor]{hardoon2004canonical}\nDavid~R Hardoon, Sandor Szedmak, and John Shawe-Taylor.\n\\newblock Canonical correlation analysis: An overview with application to\n  learning methods.\n\\newblock \\emph{Neural computation}, 16\\penalty0 (12):\\penalty0 2639--2664,\n  2004.\n\n\\bibitem[Hastie et~al.(2005)Hastie, Tibshirani, Friedman, and\n  Franklin]{hastie2005elements}\nTrevor Hastie, Robert Tibshirani, Jerome Friedman, and James Franklin.\n\\newblock The elements of statistical learning: data mining, inference and\n  prediction.\n\\newblock \\emph{The Mathematical Intelligencer}, 27\\penalty0 (2):\\penalty0\n  83--85, 2005.\n\n\\bibitem[Hastie et~al.(2015)Hastie, Tibshirani, and\n  Wainwright]{hastie2015statistical}\nTrevor Hastie, Robert Tibshirani, and Martin Wainwright.\n\\newblock \\emph{Statistical Learning with Sparsity: The Lasso and\n  Generalizations}.\n\\newblock CRC Press, 2015.\n\n\\bibitem[He et~al.(2005)He, Cai, and Niyogi]{he2005laplacian}\nXiaofei He, Deng Cai, and Partha Niyogi.\n\\newblock Laplacian score for feature selection.\n\\newblock In \\emph{Advances in neural information processing systems}, pages\n  507--514, 2005.\n\n\\bibitem[He and Yu(2010)]{he2010stable}\nZengyou He and Weichuan Yu.\n\\newblock Stable feature selection for biomarker discovery.\n\\newblock \\emph{Computational biology and chemistry}, 34\\penalty0 (4):\\penalty0\n  215--225, 2010.\n\n\\bibitem[Hosmer~Jr and Lemeshow(2004)]{hosmer2004applied}\nDavid~W Hosmer~Jr and Stanley Lemeshow.\n\\newblock \\emph{Applied logistic regression}.\n\\newblock John Wiley \\& Sons, 2004.\n\n\\bibitem[Hou et~al.(2011)Hou, Nie, Yi, and Wu]{hou2011feature}\nChenping Hou, Feiping Nie, Dongyun Yi, and Yi~Wu.\n\\newblock Feature selection via joint embedding learning and sparse regression.\n\\newblock In \\emph{IJCAI Proceedings-International Joint Conference on\n  Artificial Intelligence}, pages 1324--1329. Citeseer, 2011.\n\n\\bibitem[Hu et~al.(2013)Hu, Tang, Gao, and Liu]{hu2013actnet}\nXia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.\n\\newblock Actnet: Active learning for networked texts in microblogging.\n\\newblock In \\emph{SDM}, pages 306--314. Citeseer, 2013.\n\n\\bibitem[Huang et~al.(2015)Huang, Yoo, and\n  Kasiviswanathan]{huang2015unsupervised}\nHao Huang, Shinjae Yoo, and S~Kasiviswanathan.\n\\newblock Unsupervised feature selection on data streams.\n\\newblock In \\emph{Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management}, pages 1031--1040. ACM, 2015.\n\n\\bibitem[Huang et~al.(2011)Huang, Zhang, and Metaxas]{huang2011learning}\nJunzhou Huang, Tong Zhang, and Dimitris Metaxas.\n\\newblock Learning with structured sparsity.\n\\newblock \\emph{The Journal of Machine Learning Research}, 12:\\penalty0\n  3371--3412, 2011.\n\n\\bibitem[Jacob et~al.(2009)Jacob, Obozinski, and Vert]{jacob2009group}\nLaurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert.\n\\newblock Group lasso with overlap and graph lasso.\n\\newblock In \\emph{Proceedings of the 26th annual international conference on\n  machine learning}, pages 433--440. ACM, 2009.\n\n\\bibitem[Jakulin(2005)]{jakulin2005machine}\nAleks Jakulin.\n\\newblock \\emph{Machine learning based on attribute interactions}.\n\\newblock PhD thesis, Univerza v Ljubljani, 2005.\n\n\\bibitem[James et~al.(2009)James, Radchenko, and Lv]{james2009dasso}\nGareth~M James, Peter Radchenko, and Jinchi Lv.\n\\newblock Dasso: connections between the dantzig selector and lasso.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 71\\penalty0 (1):\\penalty0 127--142, 2009.\n\n\\bibitem[Jenatton et~al.(2010)Jenatton, Mairal, Bach, and\n  Obozinski]{jenatton2010proximal}\nRodolphe Jenatton, Julien Mairal, Francis~R Bach, and Guillaume~R Obozinski.\n\\newblock Proximal methods for sparse hierarchical dictionary learning.\n\\newblock In \\emph{Proceedings of the 27th International Conference on Machine\n  Learning (ICML-10)}, pages 487--494, 2010.\n\n\\bibitem[Jenatton et~al.(2011)Jenatton, Audibert, and\n  Bach]{jenatton2011structured}\nRodolphe Jenatton, Jean-Yves Audibert, and Francis Bach.\n\\newblock Structured variable selection with sparsity-inducing norms.\n\\newblock \\emph{The Journal of Machine Learning Research}, 12:\\penalty0\n  2777--2824, 2011.\n\n\\bibitem[Jolliffe(2002)]{jolliffe2002principal}\nIan Jolliffe.\n\\newblock \\emph{Principal component analysis}.\n\\newblock Wiley Online Library, 2002.\n\n\\bibitem[Kalousis et~al.(2007)Kalousis, Prados, and\n  Hilario]{kalousis2007stability}\nAlexandros Kalousis, Julien Prados, and Melanie Hilario.\n\\newblock Stability of feature selection algorithms: a study on\n  high-dimensional spaces.\n\\newblock \\emph{Knowledge and information systems}, 12\\penalty0 (1):\\penalty0\n  95--116, 2007.\n\n\\bibitem[Kim and Xing(2009)]{kim2009statistical}\nSeyoung Kim and Eric~P Xing.\n\\newblock Statistical estimation of correlated genome associations to a\n  quantitative trait network.\n\\newblock \\emph{PLoS Genet}, 5\\penalty0 (8):\\penalty0 e1000587, 2009.\n\n\\bibitem[Kim and Xing(2010)]{kim2010tree}\nSeyoung Kim and Eric~P Xing.\n\\newblock Tree-guided group lasso for multi-task regression with structured\n  sparsity.\n\\newblock In \\emph{Proceedings of the 27th International Conference on Machine\n  Learning (ICML-10)}, pages 543--550, 2010.\n\n\\bibitem[Kira and Rendell(1992{\\natexlab{a}})]{kira1992feature}\nKenji Kira and Larry~A Rendell.\n\\newblock The feature selection problem: Traditional methods and a new\n  algorithm.\n\\newblock In \\emph{AAAI}, volume~2, pages 129--134, 1992{\\natexlab{a}}.\n\n\\bibitem[Kira and Rendell(1992{\\natexlab{b}})]{kira1992practical}\nKenji Kira and Larry~A Rendell.\n\\newblock A practical approach to feature selection.\n\\newblock In \\emph{Proceedings of the ninth international workshop on Machine\n  learning}, pages 249--256, 1992{\\natexlab{b}}.\n\n\\bibitem[Kohavi and John(1997)]{kohavi1997wrappers}\nRon Kohavi and George~H John.\n\\newblock Wrappers for feature subset selection.\n\\newblock \\emph{Artificial intelligence}, 97\\penalty0 (1):\\penalty0 273--324,\n  1997.\n\n\\bibitem[Koller and Sahami(1995)]{koller1995toward}\nDaphne Koller and Mehran Sahami.\n\\newblock Toward optimal feature selection.\n\\newblock In \\emph{In 13th International Conference on Machine Learning}, 1995.\n\n\\bibitem[Kotsiantis and Kanellopoulos(2006)]{kotsiantis2006discretization}\nSotiris Kotsiantis and Dimitris Kanellopoulos.\n\\newblock Discretization techniques: A recent survey.\n\\newblock \\emph{GESTS International Transactions on Computer Science and\n  Engineering}, 32\\penalty0 (1):\\penalty0 47--58, 2006.\n\n\\bibitem[Lanckriet et~al.(2004)Lanckriet, Cristianini, Bartlett, Ghaoui, and\n  Jordan]{lanckriet2004learning}\nGert~RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent~El Ghaoui, and\n  Michael~I Jordan.\n\\newblock Learning the kernel matrix with semidefinite programming.\n\\newblock \\emph{The Journal of Machine Learning Research}, 5:\\penalty0 27--72,\n  2004.\n\n\\bibitem[Lewis(1992)]{lewis1992feature}\nDavid~D Lewis.\n\\newblock Feature selection and feature extraction for text categorization.\n\\newblock In \\emph{Proceedings of the workshop on Speech and Natural Language},\n  pages 212--217. Association for Computational Linguistics, 1992.\n\n\\bibitem[Li et~al.(2013)Li, Wu, Li, and Ding]{li2013group}\nHaiguang Li, Xindong Wu, Zhao Li, and Wei Ding.\n\\newblock Group feature selection with streaming features.\n\\newblock In \\emph{Data Mining (ICDM), 2013 IEEE 13th International Conference\n  on}, pages 1109--1114. IEEE, 2013.\n\n\\bibitem[Li et~al.(2015)Li, Hu, Tang, and Liu]{li2015unsupervised}\nJundong Li, Xia Hu, Jiliang Tang, and Huan Liu.\n\\newblock Unsupervised streaming feature selection in social media.\n\\newblock In \\emph{Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management}, pages 1041--1050. ACM, 2015.\n\n\\bibitem[Li et~al.(2012)Li, Yang, Liu, Zhou, and Lu]{li2012unsupervised}\nZechao Li, Yi~Yang, Jing Liu, Xiaofang Zhou, and Hanqing Lu.\n\\newblock Unsupervised feature selection using nonnegative spectral analysis.\n\\newblock In \\emph{AAAI}, 2012.\n\n\\bibitem[Liberty(2013)]{liberty2013simple}\nEdo Liberty.\n\\newblock Simple and deterministic matrix sketching.\n\\newblock In \\emph{Proceedings of the 19th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 581--588. ACM, 2013.\n\n\\bibitem[Lin and Tang(2006)]{lin2006conditional}\nDahua Lin and Xiaoou Tang.\n\\newblock Conditional infomax learning: an integrated framework for feature\n  extraction and fusion.\n\\newblock In \\emph{Computer Vision--ECCV 2006}, pages 68--82. Springer, 2006.\n\n\\bibitem[Liu and Setiono(1995)]{liu1995chi2}\nHuan Liu and Rudy Setiono.\n\\newblock Chi2: Feature selection and discretization of numeric attributes.\n\\newblock In \\emph{tai}, page 388. IEEE, 1995.\n\n\\bibitem[Liu et~al.(2009{\\natexlab{a}})Liu, Ji, and Ye]{Liu:2009:SLEP:manual}\nJ.~Liu, S.~Ji, and J.~Ye.\n\\newblock \\emph{SLEP: Sparse Learning with Efficient Projections}.\n\\newblock Arizona State University, 2009{\\natexlab{a}}.\n\\newblock URL \\url{http://www.public.asu.edu/~jye02/Software/SLEP}.\n\n\\bibitem[Liu and Ye(2009)]{liu2009efficient}\nJun Liu and Jieping Ye.\n\\newblock Efficient euclidean projections in linear time.\n\\newblock In \\emph{Proceedings of the 26th Annual International Conference on\n  Machine Learning}, pages 657--664. ACM, 2009.\n\n\\bibitem[Liu and Ye(2010)]{liu2010moreau}\nJun Liu and Jieping Ye.\n\\newblock Moreau-yosida regularization for grouped tree structure learning.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  1459--1467, 2010.\n\n\\bibitem[Liu et~al.(2009{\\natexlab{b}})Liu, Ji, and Ye]{liu2009multi}\nJun Liu, Shuiwang Ji, and Jieping Ye.\n\\newblock Multi-task feature learning via efficient l 2, 1-norm minimization.\n\\newblock In \\emph{Proceedings of the twenty-fifth conference on uncertainty in\n  artificial intelligence}, pages 339--348. AUAI Press, 2009{\\natexlab{b}}.\n\n\\bibitem[Liu et~al.(2014)Liu, Wang, Zhang, Yin, and Liu]{liu2014global}\nXinwang Liu, Lei Wang, Jian Zhang, Jianping Yin, and Huan Liu.\n\\newblock Global and local structure preservation for feature selection.\n\\newblock \\emph{Neural Networks and Learning Systems, IEEE Transactions on},\n  25\\penalty0 (6):\\penalty0 1083--1095, 2014.\n\n\\bibitem[Liu et~al.(2007)Liu, Jiang, Tian, Wang, Sato, Meltzer, and\n  Tan]{liu2007sparse}\nZhenqiu Liu, Feng Jiang, Guoliang Tian, Suna Wang, Fumiaki Sato, Stephen~J\n  Meltzer, and Ming Tan.\n\\newblock Sparse logistic regression with lp penalty for biomarker\n  identification.\n\\newblock \\emph{Statistical Applications in Genetics and Molecular Biology},\n  6\\penalty0 (1), 2007.\n\n\\bibitem[Long et~al.(2006)Long, Zhang, Wu, and Yu]{long2006spectral}\nBo~Long, Zhongfei~Mark Zhang, Xiaoyun Wu, and Philip~S Yu.\n\\newblock Spectral clustering for multi-type relational data.\n\\newblock In \\emph{Proceedings of the 23rd international conference on Machine\n  learning}, pages 585--592. ACM, 2006.\n\n\\bibitem[Long et~al.(2007)Long, Zhang, and Yu]{long2007probabilistic}\nBo~Long, Zhongfei~Mark Zhang, and Philip~S Yu.\n\\newblock A probabilistic framework for relational clustering.\n\\newblock In \\emph{Proceedings of the 13th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 470--479. ACM, 2007.\n\n\\bibitem[Ma et~al.(2007)Ma, Song, and Huang]{ma2007supervised}\nShuangge Ma, Xiao Song, and Jian Huang.\n\\newblock Supervised group lasso with applications to microarray data analysis.\n\\newblock \\emph{BMC bioinformatics}, 8\\penalty0 (1):\\penalty0 60, 2007.\n\n\\bibitem[Macskassy and Provost(2007)]{macskassy2007classification}\nSofus~A Macskassy and Foster Provost.\n\\newblock Classification in networked data: A toolkit and a univariate case\n  study.\n\\newblock \\emph{The Journal of Machine Learning Research}, 8:\\penalty0\n  935--983, 2007.\n\n\\bibitem[Marsden and Friedkin(1993)]{marsden1993network}\nPeter~V Marsden and Noah~E Friedkin.\n\\newblock Network studies of social influence.\n\\newblock \\emph{Sociological Methods \\& Research}, 22\\penalty0 (1):\\penalty0\n  127--151, 1993.\n\n\\bibitem[Masaeli et~al.(2010)Masaeli, Yan, Cui, Fung, and\n  Dy]{masaeli2010convex}\nMahdokht Masaeli, Yan Yan, Ying Cui, Glenn Fung, and Jennifer~G Dy.\n\\newblock Convex principal feature selection.\n\\newblock In \\emph{SDM}, pages 619--628. SIAM, 2010.\n\n\\bibitem[McAuley et~al.(2005)McAuley, Ming, Stewart, and\n  Hanna]{mcauley2005subband}\nJames McAuley, Ji~Ming, Darryl Stewart, and Philip Hanna.\n\\newblock Subband correlation and robust speech recognition.\n\\newblock \\emph{Speech and Audio Processing, IEEE Transactions on}, 13\\penalty0\n  (5):\\penalty0 956--964, 2005.\n\n\\bibitem[McPherson et~al.(2001)McPherson, Smith-Lovin, and\n  Cook]{mcpherson2001birds}\nMiller McPherson, Lynn Smith-Lovin, and James~M Cook.\n\\newblock Birds of a feather: Homophily in social networks.\n\\newblock \\emph{Annual review of sociology}, pages 415--444, 2001.\n\n\\bibitem[Meier et~al.(2008)Meier, Van De~Geer, and\n  B{\\\"u}hlmann]{meier2008group}\nLukas Meier, Sara Van De~Geer, and Peter B{\\\"u}hlmann.\n\\newblock The group lasso for logistic regression.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 70\\penalty0 (1):\\penalty0 53--71, 2008.\n\n\\bibitem[Meyer and Bontempi(2006)]{meyer2006use}\nPatrick~E Meyer and Gianluca Bontempi.\n\\newblock On the use of variable complementarity for feature selection in\n  cancer classification.\n\\newblock In \\emph{Applications of Evolutionary Computing}, pages 91--102.\n  Springer, 2006.\n\n\\bibitem[Meyer et~al.(2008)Meyer, Schretter, and\n  Bontempi]{meyer2008information}\nPatrick~Emmanuel Meyer, Colas Schretter, and Gianluca Bontempi.\n\\newblock Information-theoretic feature selection in microarray data using\n  variable complementarity.\n\\newblock \\emph{Selected Topics in Signal Processing, IEEE Journal of},\n  2\\penalty0 (3):\\penalty0 261--274, 2008.\n\n\\bibitem[Mitra et~al.(2002)Mitra, Murthy, and Pal]{mitra2002unsupervised}\nPabitra Mitra, CA~Murthy, and Sankar~K. Pal.\n\\newblock Unsupervised feature selection using feature similarity.\n\\newblock \\emph{IEEE transactions on pattern analysis and machine\n  intelligence}, 24\\penalty0 (3):\\penalty0 301--312, 2002.\n\n\\bibitem[Morris(2005)]{morris2005manifestation}\nSteven~A Morris.\n\\newblock Manifestation of emerging specialties in journal literature: A growth\n  model of papers, references, exemplars, bibliographic coupling, cocitation,\n  and clustering coefficient distribution.\n\\newblock \\emph{Journal of the American Society for Information Science and\n  Technology}, 56\\penalty0 (12):\\penalty0 1250--1273, 2005.\n\n\\bibitem[Narendra and Fukunaga(1977)]{narendra1977branch}\nPatrenahalli~M Narendra and Keinosuke Fukunaga.\n\\newblock A branch and bound algorithm for feature subset selection.\n\\newblock \\emph{Computers, IEEE Transactions on}, 100\\penalty0 (9):\\penalty0\n  917--922, 1977.\n\n\\bibitem[Nesterov(2004)]{nesterov2004introductory}\nYurii Nesterov.\n\\newblock \\emph{Introductory lectures on convex optimization}, volume~87.\n\\newblock Springer Science \\& Business Media, 2004.\n\n\\bibitem[Newman and Girvan(2004)]{newman2004finding}\nMark~EJ Newman and Michelle Girvan.\n\\newblock Finding and evaluating community structure in networks.\n\\newblock \\emph{Physical review E}, 69\\penalty0 (2):\\penalty0 026113, 2004.\n\n\\bibitem[Ng et~al.(2002)Ng, Jordan, Weiss, et~al.]{ng2002spectral}\nAndrew~Y Ng, Michael~I Jordan, Yair Weiss, et~al.\n\\newblock On spectral clustering: Analysis and an algorithm.\n\\newblock \\emph{Advances in neural information processing systems}, 2:\\penalty0\n  849--856, 2002.\n\n\\bibitem[Nie et~al.(2008)Nie, Xiang, Jia, Zhang, and Yan]{nie2008trace}\nFeiping Nie, Shiming Xiang, Yangqing Jia, Changshui Zhang, and Shuicheng Yan.\n\\newblock Trace ratio criterion for feature selection.\n\\newblock In \\emph{AAAI}, volume~2, pages 671--676, 2008.\n\n\\bibitem[Nie et~al.(2010)Nie, Huang, Cai, and Ding]{nie2010efficient}\nFeiping Nie, Heng Huang, Xiao Cai, and Chris~H Ding.\n\\newblock Efficient and robust feature selection via joint \u00e2\u0084\u00932, 1-norms\n  minimization.\n\\newblock In \\emph{Advances in neural information processing systems}, pages\n  1813--1821, 2010.\n\n\\bibitem[Obozinski et~al.(2007)Obozinski, Taskar, and\n  Jordan]{obozinski2007joint}\nGuillaume Obozinski, Ben Taskar, and Michael Jordan.\n\\newblock Joint covariate selection for grouped classification.\n\\newblock Technical report, Technical report, Statistics Department, UC\n  Berkeley, 2007.\n\n\\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,\n  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,\n  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}\nF.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,\n  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,\n  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.\n\\newblock Scikit-learn: Machine learning in {P}ython.\n\\newblock \\emph{Journal of Machine Learning Research}, 12:\\penalty0 2825--2830,\n  2011.\n\n\\bibitem[Peng et~al.(2005)Peng, Long, and Ding]{peng2005feature}\nHanchuan Peng, Fuhui Long, and Chris Ding.\n\\newblock Feature selection based on mutual information criteria of\n  max-dependency, max-relevance, and min-redundancy.\n\\newblock \\emph{Pattern Analysis and Machine Intelligence, IEEE Transactions\n  on}, 27\\penalty0 (8):\\penalty0 1226--1238, 2005.\n\n\\bibitem[Peng et~al.(2010)Peng, Zhu, Bergamaschi, Han, Noh, Pollack, and\n  Wang]{peng2010regularized}\nJie Peng, Ji~Zhu, Anna Bergamaschi, Wonshik Han, Dong-Young Noh, Jonathan~R\n  Pollack, and Pei Wang.\n\\newblock Regularized multivariate regression for identifying master predictors\n  with application to integrative genomics study of breast cancer.\n\\newblock \\emph{The annals of applied statistics}, 4\\penalty0 (1):\\penalty0 53,\n  2010.\n\n\\bibitem[Perkins and Theiler(2003)]{perkins2003online}\nSimon Perkins and James Theiler.\n\\newblock Online feature selection using grafting.\n\\newblock In \\emph{ICML}, pages 592--599, 2003.\n\n\\bibitem[Perkins et~al.(2003)Perkins, Lacker, and Theiler]{perkins2003grafting}\nSimon Perkins, Kevin Lacker, and James Theiler.\n\\newblock Grafting: Fast, incremental feature selection by gradient descent in\n  function space.\n\\newblock \\emph{The Journal of Machine Learning Research}, 3:\\penalty0\n  1333--1356, 2003.\n\n\\bibitem[Qian and Zhai(2013)]{qian2013robust}\nMingjie Qian and Chengxiang Zhai.\n\\newblock Robust unsupervised feature selection.\n\\newblock In \\emph{Proceedings of the Twenty-Third international joint\n  conference on Artificial Intelligence}, pages 1621--1627. AAAI Press, 2013.\n\n\\bibitem[Quinlan(1986)]{quinlan1986induction}\nJ.~Ross Quinlan.\n\\newblock Induction of decision trees.\n\\newblock \\emph{Machine learning}, 1\\penalty0 (1):\\penalty0 81--106, 1986.\n\n\\bibitem[Quinlan(1993)]{quinlan1993c4}\nJ~Ross Quinlan.\n\\newblock \\emph{C4. 5: programs for machine learning}.\n\\newblock Morgan Kaufmann, 1993.\n\n\\bibitem[Robnik-{\\v{S}}ikonja and Kononenko(2003)]{robnik2003theoretical}\nMarko Robnik-{\\v{S}}ikonja and Igor Kononenko.\n\\newblock Theoretical and empirical analysis of relieff and rrelieff.\n\\newblock \\emph{Machine learning}, 53\\penalty0 (1-2):\\penalty0 23--69, 2003.\n\n\\bibitem[Roweis and Saul(2000)]{roweis2000nonlinear}\nSam~T Roweis and Lawrence~K Saul.\n\\newblock Nonlinear dimensionality reduction by locally linear embedding.\n\\newblock \\emph{Science}, 290\\penalty0 (5500):\\penalty0 2323--2326, 2000.\n\n\\bibitem[Sandler et~al.(2009)Sandler, Blitzer, Talukdar, and\n  Ungar]{sandler2009regularized}\nTed Sandler, John Blitzer, Partha~P Talukdar, and Lyle~H Ungar.\n\\newblock Regularized learning with networks of features.\n\\newblock In \\emph{Advances in neural information processing systems}, pages\n  1401--1408, 2009.\n\n\\bibitem[Scholkopft and Mullert(1999)]{scholkopft1999fisher}\nBernhard Scholkopft and Klaus-Robert Mullert.\n\\newblock Fisher discriminant analysis with kernels.\n\\newblock \\emph{Neural networks for signal processing IX}, 1:\\penalty0 1, 1999.\n\n\\bibitem[Segal et~al.(2003)Segal, Dahlquist, and Conklin]{segal2003regression}\nMark~R Segal, Kam~D Dahlquist, and Bruce~R Conklin.\n\\newblock Regression approaches for microarray data analysis.\n\\newblock \\emph{Journal of Computational Biology}, 10\\penalty0 (6):\\penalty0\n  961--980, 2003.\n\n\\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and\n  Eliassi-Rad]{sen2008collective}\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,\n  and Tina Eliassi-Rad.\n\\newblock Collective classification in network data.\n\\newblock \\emph{AI magazine}, 29\\penalty0 (3):\\penalty0 93, 2008.\n\n\\bibitem[Shannon(2001)]{shannon2001mathematical}\nClaude~Elwood Shannon.\n\\newblock A mathematical theory of communication.\n\\newblock \\emph{ACM SIGMOBILE Mobile Computing and Communications Review},\n  5\\penalty0 (1):\\penalty0 3--55, 2001.\n\n\\bibitem[Shi and Malik(2000)]{shi2000normalized}\nJianbo Shi and Jitendra Malik.\n\\newblock Normalized cuts and image segmentation.\n\\newblock \\emph{Pattern Analysis and Machine Intelligence, IEEE Transactions\n  on}, 22\\penalty0 (8):\\penalty0 888--905, 2000.\n\n\\bibitem[Singh et~al.(2009)Singh, Kubica, Larsen, and\n  Sorokina]{singh2009parallel}\nSameer Singh, Jeremy Kubica, Scott Larsen, and Daria Sorokina.\n\\newblock Parallel large scale feature selection for logistic regression.\n\\newblock In \\emph{SDM}, pages 1172--1183. SIAM, 2009.\n\n\\bibitem[Sugiyama(2006)]{sugiyama2006local}\nMasashi Sugiyama.\n\\newblock Local fisher discriminant analysis for supervised dimensionality\n  reduction.\n\\newblock In \\emph{Proceedings of the 23rd international conference on Machine\n  learning}, pages 905--912. ACM, 2006.\n\n\\bibitem[Tan et~al.(2014)Tan, Tsang, and Wang]{tan2014towards}\nMingkui Tan, Ivor~W Tsang, and Li~Wang.\n\\newblock Towards ultrahigh dimensional feature selection for big data.\n\\newblock \\emph{The Journal of Machine Learning Research}, 15\\penalty0\n  (1):\\penalty0 1371--1429, 2014.\n\n\\bibitem[Tang and Liu(2012{\\natexlab{a}})]{tang2012feature}\nJiliang Tang and Huan Liu.\n\\newblock Feature selection with linked data in social media.\n\\newblock In \\emph{SDM}, pages 118--128. SIAM, 2012{\\natexlab{a}}.\n\n\\bibitem[Tang and Liu(2012{\\natexlab{b}})]{tang2012unsupervised}\nJiliang Tang and Huan Liu.\n\\newblock Unsupervised feature selection for linked social media data.\n\\newblock In \\emph{Proceedings of the 18th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 904--912. ACM,\n  2012{\\natexlab{b}}.\n\n\\bibitem[Tang and Liu(2014{\\natexlab{a}})]{tang2014featuresocial}\nJiliang Tang and Huan Liu.\n\\newblock Feature selection for social media data.\n\\newblock \\emph{ACM Transactions on Knowledge Discovery from Data (TKDD)},\n  8\\penalty0 (4):\\penalty0 19, 2014{\\natexlab{a}}.\n\n\\bibitem[Tang and Liu(2014{\\natexlab{b}})]{tang2014unsupervised}\nJiliang Tang and Huan Liu.\n\\newblock An unsupervised feature selection framework for social media data.\n\\newblock \\emph{Knowledge and Data Engineering, IEEE Transactions on},\n  26\\penalty0 (12):\\penalty0 2914--2927, 2014{\\natexlab{b}}.\n\n\\bibitem[Tang et~al.(2013)Tang, Hu, Gao, and Liu]{tang2013unsupervised}\nJiliang Tang, Xia Hu, Huiji Gao, and Huan Liu.\n\\newblock Unsupervised feature selection for multi-view data in social media.\n\\newblock In \\emph{SDM}, pages 270--278, 2013.\n\n\\bibitem[Tang et~al.(2014)Tang, Alelyani, and Liu]{tang2014feature}\nJiliang Tang, Salem Alelyani, and Huan Liu.\n\\newblock Feature selection for classification: A review.\n\\newblock \\emph{Data Classification: Algorithms and Applications}, page~37,\n  2014.\n\n\\bibitem[Tenenbaum et~al.(2000)Tenenbaum, De~Silva, and\n  Langford]{tenenbaum2000global}\nJoshua~B Tenenbaum, Vin De~Silva, and John~C Langford.\n\\newblock A global geometric framework for nonlinear dimensionality reduction.\n\\newblock \\emph{Science}, 290\\penalty0 (5500):\\penalty0 2319--2323, 2000.\n\n\\bibitem[Tibshirani(1996)]{tibshirani1996regression}\nRobert Tibshirani.\n\\newblock Regression shrinkage and selection via the lasso.\n\\newblock \\emph{Journal of the Royal Statistical Society. Series B\n  (Methodological)}, pages 267--288, 1996.\n\n\\bibitem[Tibshirani et~al.(2001)Tibshirani, Walther, and\n  Hastie]{tibshirani2001estimating}\nRobert Tibshirani, Guenther Walther, and Trevor Hastie.\n\\newblock Estimating the number of clusters in a data set via the gap\n  statistic.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 63\\penalty0 (2):\\penalty0 411--423, 2001.\n\n\\bibitem[Tibshirani et~al.(2005)Tibshirani, Saunders, Rosset, Zhu, and\n  Knight]{tibshirani2005sparsity}\nRobert Tibshirani, Michael Saunders, Saharon Rosset, Ji~Zhu, and Keith Knight.\n\\newblock Sparsity and smoothness via the fused lasso.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 67\\penalty0 (1):\\penalty0 91--108, 2005.\n\n\\bibitem[Vetterling et~al.(1992)Vetterling, Teukolsky, and\n  Press]{vetterling1992numerical}\nWilliam~T Vetterling, Saul~A Teukolsky, and William~H Press.\n\\newblock \\emph{Numerical recipes: example book (C)}.\n\\newblock Press Syndicate of the University of Cambridge, 1992.\n\n\\bibitem[Vidal-Naquet and Ullman(2003)]{vidal2003object}\nMichel Vidal-Naquet and Shimon Ullman.\n\\newblock Object recognition with informative features and linear\n  classification.\n\\newblock In \\emph{ICCV}, volume~3, page 281, 2003.\n\n\\bibitem[Von~Luxburg(2007)]{von2007tutorial}\nUlrike Von~Luxburg.\n\\newblock A tutorial on spectral clustering.\n\\newblock \\emph{Statistics and computing}, 17\\penalty0 (4):\\penalty0 395--416,\n  2007.\n\n\\bibitem[Wang et~al.(2013{\\natexlab{a}})Wang, Nie, and Huang]{wang2013multi}\nHua Wang, Feiping Nie, and Heng Huang.\n\\newblock Multi-view clustering and feature learning via structured sparsity.\n\\newblock In \\emph{Proceedings of the 30th International Conference on Machine\n  Learning (ICML-13)}, pages 352--360, 2013{\\natexlab{a}}.\n\n\\bibitem[Wang et~al.(2014)Wang, Zhao, Hoi, and Jin]{wang2014online}\nJialei Wang, Peilin Zhao, Steven~CH Hoi, and Rong Jin.\n\\newblock Online feature selection and its applications.\n\\newblock \\emph{Knowledge and Data Engineering, IEEE Transactions on},\n  26\\penalty0 (3):\\penalty0 698--710, 2014.\n\n\\bibitem[Wang et~al.(2013{\\natexlab{b}})Wang, Zhao, Hu, Cheung, Wang, and\n  Wu]{wang2013online}\nJing Wang, Zhong-Qiu Zhao, Xuegang Hu, Yiu-Ming Cheung, Meng Wang, and Xindong\n  Wu.\n\\newblock Online group feature selection.\n\\newblock In \\emph{Proceedings of the Twenty-Third international joint\n  conference on Artificial Intelligence}, pages 1757--1763. AAAI Press,\n  2013{\\natexlab{b}}.\n\n\\bibitem[Wang et~al.(2015)Wang, Wang, Li, Liu, Zhao, Hu, and\n  Wu]{wang2015online}\nJing Wang, Meng Wang, Peipei Li, Luoqi Liu, Zhongqiu Zhao, Xuegang Hu, and\n  Xindong Wu.\n\\newblock Online feature selection with group structure analysis.\n\\newblock \\emph{IEEE Transactions on Knowledge and Data Engineering},\n  27\\penalty0 (11):\\penalty0 3029--3041, 2015.\n\n\\bibitem[Wright(1965)]{wright1965interpretation}\nSewall Wright.\n\\newblock The interpretation of population structure by f-statistics with\n  special regard to systems of mating.\n\\newblock \\emph{Evolution}, pages 395--420, 1965.\n\n\\bibitem[Wu et~al.(2010)Wu, Yu, Wang, and Ding]{wu2010online}\nXindong Wu, Kui Yu, Hao Wang, and Wei Ding.\n\\newblock Online streaming feature selection.\n\\newblock In \\emph{Proceedings of the 27th international conference on machine\n  learning (ICML-10)}, pages 1159--1166, 2010.\n\n\\bibitem[Wu et~al.(2013)Wu, Yu, Ding, Wang, and Zhu]{wu2013online}\nXindong Wu, Kui Yu, Wei Ding, Hao Wang, and Xingquan Zhu.\n\\newblock Online feature selection with streaming features.\n\\newblock \\emph{Pattern Analysis and Machine Intelligence, IEEE Transactions\n  on}, 35\\penalty0 (5):\\penalty0 1178--1192, 2013.\n\n\\bibitem[Yamada et~al.(2014)Yamada, Saha, Ouyang, Yin, and Chang]{yamada2014n}\nMakoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, and Yi~Chang.\n\\newblock N3lars: Minimum redundancy maximum relevance feature selection for\n  large and high-dimensional data.\n\\newblock \\emph{arXiv preprint arXiv:1411.2331}, 2014.\n\n\\bibitem[Yang and Moody(1999)]{yang1999data}\nHoward~Hua Yang and John~E Moody.\n\\newblock Data visualization and feature selection: New algorithms for\n  nongaussian data.\n\\newblock In \\emph{NIPS}, volume~99, pages 687--693. Citeseer, 1999.\n\n\\bibitem[Yang et~al.(2012)Yang, Yuan, Lai, Shen, Wonka, and\n  Ye]{yang2012feature}\nSen Yang, Lei Yuan, Ying-Cheng Lai, Xiaotong Shen, Peter Wonka, and Jieping Ye.\n\\newblock Feature grouping and selection over an undirected graph.\n\\newblock In \\emph{Proceedings of the 18th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 922--930. ACM, 2012.\n\n\\bibitem[Yang et~al.(2010)Yang, Xu, Nie, Yan, and Zhuang]{yang2010image}\nYi~Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang.\n\\newblock Image clustering using local discriminant models and global\n  integration.\n\\newblock \\emph{Image Processing, IEEE Transactions on}, 19\\penalty0\n  (10):\\penalty0 2761--2773, 2010.\n\n\\bibitem[Yang et~al.(2011)Yang, Shen, Ma, Huang, and Zhou]{yang2011l2}\nYi~Yang, Heng~Tao Shen, Zhigang Ma, Zi~Huang, and Xiaofang Zhou.\n\\newblock l2, 1-norm regularized discriminative feature selection for\n  unsupervised learning.\n\\newblock In \\emph{IJCAI Proceedings-International Joint Conference on\n  Artificial Intelligence}, pages 1589--1594. Citeseer, 2011.\n\n\\bibitem[Ye and Liu(2012)]{ye2012sparse}\nJieping Ye and Jun Liu.\n\\newblock Sparse methods for biomedical data.\n\\newblock \\emph{ACM SIGKDD Explorations Newsletter}, 14\\penalty0 (1):\\penalty0\n  4--15, 2012.\n\n\\bibitem[Yu and Liu(2003)]{yu2003feature}\nLei Yu and Huan Liu.\n\\newblock Feature selection for high-dimensional data: A fast correlation-based\n  filter solution.\n\\newblock In \\emph{ICML}, volume~3, pages 856--863, 2003.\n\n\\bibitem[Yu and Shi(2003)]{yu2003multiclass}\nStella~X Yu and Jianbo Shi.\n\\newblock Multiclass spectral clustering.\n\\newblock In \\emph{Computer Vision, 2003. Proceedings. Ninth IEEE International\n  Conference on}, pages 313--319. IEEE, 2003.\n\n\\bibitem[Yuan et~al.(2011)Yuan, Liu, and Ye]{yuan2011efficient}\nLei Yuan, Jun Liu, and Jieping Ye.\n\\newblock Efficient methods for overlapping group lasso.\n\\newblock In \\emph{Advances in Neural Information Processing Systems}, pages\n  352--360, 2011.\n\n\\bibitem[Yuan and Lin(2006)]{yuan2006model}\nMing Yuan and Yi~Lin.\n\\newblock Model selection and estimation in regression with grouped variables.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 68\\penalty0 (1):\\penalty0 49--67, 2006.\n\n\\bibitem[Zhang et~al.(2008)Zhang, Ghahramani, and Yang]{zhang2008flexible}\nJian Zhang, Zoubin Ghahramani, and Yiming Yang.\n\\newblock Flexible latent variable models for multi-task learning.\n\\newblock \\emph{Machine Learning}, 73\\penalty0 (3):\\penalty0 221--242, 2008.\n\n\\bibitem[Zhang et~al.(2015)Zhang, Zhang, Long, Ding, Zhang, and\n  Wu]{zhangtowards}\nQin Zhang, Peng Zhang, Guodong Long, Wei Ding, Chengqi Zhang, and Xindong Wu.\n\\newblock Towards mining trapezoidal data streams.\n\\newblock In \\emph{Data Mining (ICDM), 2015 IEEE 15th International Conference\n  on}. IEEE, 2015.\n\n\\bibitem[Zhao and Yu(2006)]{zhao2006model}\nPeng Zhao and Bin Yu.\n\\newblock On model selection consistency of lasso.\n\\newblock \\emph{The Journal of Machine Learning Research}, 7:\\penalty0\n  2541--2563, 2006.\n\n\\bibitem[Zhao et~al.(2009)Zhao, Rocha, and Yu]{zhao2009composite}\nPeng Zhao, Guilherme Rocha, and Bin Yu.\n\\newblock The composite absolute penalties family for grouped and hierarchical\n  variable selection.\n\\newblock \\emph{The Annals of Statistics}, pages 3468--3497, 2009.\n\n\\bibitem[Zhao and Liu(2007)]{zhao2007spectral}\nZheng Zhao and Huan Liu.\n\\newblock Spectral feature selection for supervised and unsupervised learning.\n\\newblock In \\emph{Proceedings of the 24th international conference on Machine\n  learning}, pages 1151--1157. ACM, 2007.\n\n\\bibitem[Zhao and Liu(2008)]{zhao2008multi}\nZheng Zhao and Huan Liu.\n\\newblock Multi-source feature selection via geometry-dependent covariance\n  analysis.\n\\newblock In \\emph{FSDM}, pages 36--47, 2008.\n\n\\bibitem[Zhao et~al.(2013)Zhao, Zhang, Cox, Duling, and\n  Sarle]{zhao2013massively}\nZheng Zhao, Ruiwen Zhang, James Cox, David Duling, and Warren Sarle.\n\\newblock Massively parallel feature selection: an approach based on variance\n  preservation.\n\\newblock \\emph{Machine learning}, 92\\penalty0 (1):\\penalty0 195--220, 2013.\n\n\\bibitem[Zhou et~al.(2005{\\natexlab{a}})Zhou, Huang, and\n  Sch{\\\"o}lkopf]{zhou2005learning}\nDengyong Zhou, Jiayuan Huang, and Bernhard Sch{\\\"o}lkopf.\n\\newblock Learning from labeled and unlabeled data on a directed graph.\n\\newblock In \\emph{Proceedings of the 22nd international conference on Machine\n  learning}, pages 1036--1043. ACM, 2005{\\natexlab{a}}.\n\n\\bibitem[Zhou et~al.(2012)Zhou, Liu, Narayan, and Ye]{zhou2012modeling}\nJiayu Zhou, Jun Liu, Vaibhav~A Narayan, and Jieping Ye.\n\\newblock Modeling disease progression via fused sparse group lasso.\n\\newblock In \\emph{Proceedings of the 18th ACM SIGKDD international conference\n  on Knowledge discovery and data mining}, pages 1095--1103. ACM, 2012.\n\n\\bibitem[Zhou et~al.(2005{\\natexlab{b}})Zhou, Foster, Stine, and\n  Ungar]{zhou2005streaming}\nJing Zhou, Dean Foster, Robert Stine, and Lyle Ungar.\n\\newblock Streaming feature selection using alpha-investing.\n\\newblock In \\emph{Proceedings of the eleventh ACM SIGKDD international\n  conference on Knowledge discovery in data mining}, pages 384--393. ACM,\n  2005{\\natexlab{b}}.\n\n\\bibitem[Zou(2006)]{zou2006adaptive}\nHui Zou.\n\\newblock The adaptive lasso and its oracle properties.\n\\newblock \\emph{Journal of the American statistical association}, 101\\penalty0\n  (476):\\penalty0 1418--1429, 2006.\n\n\\bibitem[Zou and Hastie(2005)]{zou2005regularization}\nHui Zou and Trevor Hastie.\n\\newblock Regularization and variable selection via the elastic net.\n\\newblock \\emph{Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology)}, 67\\penalty0 (2):\\penalty0 301--320, 2005.\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 187025, "prevtext": "\nwhere $H(C)$ and $H(C')$ represent the entropies of clusterings $C$ and $C'$, respectively.\n\nLet $p_{i}$ and $q_{i}$ be the clustering result and the ground truth label for instance $u_{i}$, respectively. Then, accuracy (ACC) is defined as:\n\n", "index": 239, "text": "\\begin{equation}\nACC = \\frac{1}{n}\\sum_{i=1}^{n}\\delta(q_{i},map(p_{i}))\n\\label{eq:acc}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E119.m1\" class=\"ltx_Math\" alttext=\"ACC=\\frac{1}{n}\\sum_{i=1}^{n}\\delta(q_{i},map(p_{i}))\" display=\"block\"><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>C</mi><mo>\u2062</mo><mi>C</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>\u03b4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo>,</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}]