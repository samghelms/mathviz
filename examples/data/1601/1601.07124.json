[{"file": "1601.07124.tex", "nexttext": "\n\n\\subsection{Jensen-Shannon divergence}\n\\label{ss_djs}\n\nWe use Jensen-Shannon (JS) divergence to measure the similarity between sentences. Let $w$ be a words\u00e2\u0080\u0099 set in P and Q. P and Q represent\nthe probability distribution between two objects:\ntwo individuals sentences or a sentence and a set of sentences.\nThe divergence will then calculated among these two objects. The \\ac{JS} divergence is symmetric and provides a stable way to measure the difference between two distributions (equation \\ref{DJS}).\n\n\\begin{eqnarray}\n\\label{DJS}\n\nD_{JS}(P||Q) &=& \\frac{1}{2}\\sum_{w \\in W} \\Bigg[ P_w\\log{\\Bigg( \\frac{2 \\times P_w}{P_w+Q_w} \\Bigg)} \\nonumber \\\\\n  &+& Q_w\\log{\\Bigg( \\frac{2 \\times Q_w}{P_w+Q_w} \\Bigg)} \\Bigg]\\\\ \\nonumber\n\n\\end{eqnarray}\n\nThe \\ac{JS} divergence value ranges from $[0,\\infty+)$. It is closer to zero when the distributions are similar and they differ in another case. \n\nIn the case there is a word in a sentence that is missing in another one, a smooth (different weighting) will be used to avoid null values and have a smoother distribution \\cite{smoothBook}. If a word $w$ is not present in the sentence $Q$, then the smooth is calculated by the equation \\ref{smooth}, where $ \\beta = 1.5 \\times voc$, which $voc$ is the number of distinct words in $R$, $\\gamma$ is the variable that controls the relevance of the missing word in the sentence and $N$ is the number of words in $R$ \n\\cite{nenkova}.\n\n\n", "itemtype": "equation", "pos": 9416, "prevtext": "\n\n\\begin{acronym}{}\n\\acro{ATS}{Automatic Text Summarization}\n\\acro{CAS}{Chemical Abstracts Service}\n\\acro{CST}{Cross-document Structure Theory}\n\\acro{JS}{Jensen-Shannon}\n\\acro{KL}{Kullback-Leibler}\n\\acro{MRS}{Multi-document Rhetorical Structure}\n\\acro{NLP}{Natural Language Processing}\n\\acro{ROUGE}{Recall-Oriented Understudy for Gisting Evaluation}\n\\acro{RAG}{\\textsl{R\u00c3\u00a9sumeur Audio-texte \u00c3\u00a0 base de Graphes}}\n\\end{acronym}\n\n\\maketitle\n\\begin{abstract}\nThis paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging.\n\\end{abstract}\n\nKeywords: Automatic Text Summarization, Jensen-Shannon's divergence of probabilities, Speech-to-text summarization, Graph model.\n\n\\section{Introduction}\n\nNowadays, a lot of information is daily generated. It is necessary to have available memory storage because each datum must be processed and the information contained therein analyzed. The manual analysis is impossible because it is necessary a huge number of persons to analyze this information in an available time. The summary is a short text with main ideas of original text \\cite{torres2014automatic} and reduces the read time to analyze these data.\n\nAudio is widely used in daily life on the radio and on the internet, in news, interviews and conversations. A Call Centre Conversation creates a lot of conversations every day. These centers has issues and tasks. It is essential the control of the discussed topics and the results obtained by customers in these calls. One way to analyze and accelerate the data processing is speech summarization, that is different from traditional text summarization because there are other problems in these texts as speech errors, sentences of different sizes and colloquialisms.\n\n\n\\textsl{``Multiling is a community-driven initiative for benchmarking multilingual summarization systems, nurturing further research, and pushing the state-of-the-art in the area''}\\footnote{\\url{http://multiling.iit.demokritos.gr/pages/view/1517/multiling-2015-call-for-participation}}. The MultiLing 2015 initiative features the following tasks: Multilingual Multi-document Summarization, Multilingual Single-document Summarization, Online Forum Summarization and Call Centre Conversation Summarization (CCCS). The CCCS pilot task consists in \\textsl{``creating systems that can analyze call centres conversations and generate written summaries reflecting why the customer is calling, how the agent answers that query, what are the steps to solve the problem and what is the resolution status of the problem\"} \\cite{benoit}.\n\nWe developed the LIA-RAG summarization system based on the RAG system \\cite{rag:15}, coupled with some post-processing rules\nin order to generate a final summary. LIA-RAG uses a graph model to analyze and verify a set of documents (e.g., the conversation transcription) for MultiLing'15 CCCS pilot task. \nLIA-RAG creates a summary computing the relevance of the words and the similarity among the sentences. The system uses a simple post-processing to improve the quality of the final summary. \n\nThe rest of the paper is organized as follows: \nsection \\ref{sc:rt} describes related work on automatic summarization of texts and conversations. Sections \\ref{sc:mod} and \\ref{sc:rag} analyze the graph model and the system used in this work. Section \\ref{sc:results} describes the results obtained for Multiling/DECODA French corpus and section \\ref{sc:conc} concludes this work.\n\n\\section{Related Works}\n\\label{sc:rt}\n\n\\ac{ATS} aims to creates a summary containing the main ideas of a textual document \\cite{mani:mayburi:99,mani:01,torres2014automatic}. The summary can be an extraction or abstraction of a single document or multi-document. The extraction process identifies the most informative sentences of a document and creates a summary by assembling of these sentences \\cite{Luhn,torres2014automatic}. Extraction may be guided (by a query). In this case, the algorithm selects the most relevant information follow a particular topic. The abstraction algorithms create new (or reformulate) sentences from original texts \\cite{seno1,seno2} and the extraction methods use the key sentences of texts \\cite{Barzilay,torres2014automatic}.\n\nWorks about abstraction usually uses syntactic and semantic knowledge of a language to create the summary. This procedure verifies the best construction of a sentence \\cite{Barzilay2}. This type of summarization uses fusion to help the review of information. \\cite{seno1} proposed a method to fusion similar sentences in Brazilian Portuguese based on a symbolic and domain-independent approach. This method allows the fusion by union and by intersection of a document cluster. Fusion by union preserves the overall message of the cluster while fusion by intersection analyses the redundant information considered most important in the cluster. \\cite{seno2} described how to identify common information between sentences in Brazilian Portuguese using lexical \nknowledge, syntactic and semantic rules of paraphrasing.\n\n\\cite{cstsumm} developed a summarizer system based on the CST model (Cross-document Structure Theory). The system proposed analyses redundancy and contradiction among different information sources in Brazilian Portuguese.\n\n\\cite{Barzilay2} developed a method to generate automatic summaries by identifying and synthesizing similar elements in a cluster of documents. This method creates the summary based on similarity between the sentences and topic. \\cite{Barzilay} described an approach to fusion sentences through the text-to-text technique, to synthesize repeated information from multiple documents. This method uses a syntactic alignment in sentences to identify common information. After the identification step, sentences are processed and a new text is generated with the same content.\n\nA way to calculate the similarity between sentences is to use co-occurrence of words. \\cite{He} proposed a fusion method using similarity metrics, co-occurrence skip-bigram and information density to evaluate sentences and to select the most relevant ones. \\cite{Hennig} developed a multi-document model to summarize by analyzing the co-occurrence of sentence-term and sentence-bigram using the \\ac{JS} divergence.\n\nAnother method to obtain relevant sentences uses compression, as reported in \\cite{Pitler}. Pitler uses approaches based on syntactic trees, sentences and \ndiscourse. \\cite{Filippova} describes a multi-sentence compression method using a word-based graph.\n \nThe summarization by extraction does not have the same quality as the summaries produced by abstraction because it uses surface methods based on statistical calculations to verify the sentence relevance. However, the extraction is general and do not require deep analysis of the language \\cite{Barzilay,sasi}.\n\n\\cite{sasi} use Graph theory concomitant with \\ac{JS} divergence to create multi-document summaries by extraction. Their system describes a text model as a graph where the sentences are represented by vertices and the edges connect two similar sentences. Their approach calculates the stable set of the graph aiming creating the summary containing sentences with general information of the cluster and without redundancy. \\cite{glouton} model the text as graph model and use a heuristic (greedy algorithm) to obtain the relevant sentences in the text.\n\nThe speech summarization task is more complex and it involves other problems. It is more difficult to identify utterance boundaries because it may be fragmented,  contain disfluencies and also because speech recognition introduces errors. Meetings involve multi-party conversation with overlapping speakers. The language used is informal and utterances tend to be partial, fragmentary, ungrammatical and include many ellipses and pronouns. However, the speech signal may provides additional information that emphasizes a piece of text as prosody \\cite{Murray}.\n\n\\cite{Mckeown} described some ways to use a text summarization as a speech summarization. They described some work about summarization of broadcast news and meetings. \\cite{Murray} analyzed extractive summarization of multiparty meetings. They described Maximal Marginal Relevance and Latent Semantic Analysis to create the summary based on prosodic and lexical features.\n\n\n\\section{Modeling the problem}\n\\label{sc:mod}\n\nThis paper aims to design a system to summarize several documents by extraction its most important sentences. Statistical techniques were used to build a language independent system. The proposed methods are based on a specific preprocessing of words, a weighting function of sentences and a bag-of-words model to represent the text content.\n\nThis model uses $K$ matrices represented by $S^{K}_{[m \\times n]}$ and constructed from $K$ documents, where $m_a$ is the number of sentences and $n_a$ is the number of distinct words in the document $a$ ($a \\in K$). The cell $s^{a}_{ij}$ of the matrix represents the frequency of word $j$ in the sentence $i$ ($FP_{ij}$) of the document $a$.\nThis stage was constructed using the libraries and algorithms from Cortex summarization system \\cite{torres2002condenses,Torres-Moreno2001}.\n\n\n\n", "index": 1, "text": "\\begin{equation}\n\\begin{split}\n S^{a}=\\left(\n\\begin{array}{cccc}\ns^{a}_{11} & s^{a}_{12} & \\ldots & s^{a}_{1n}\\\\\ns^{a}_{21} & s^{a}_{22} & \\ldots & s^{a}_{2n}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\ns^{a}_{m1} & s^{a}_{m2} & \\ldots & s^{a}_{mn}\\\\\n\\end{array}\n\\right), a \\in K\\\\\ns^{a}_{ij}=\\left\\{\n\\begin{array}{cc}\nFP_{ij}, & \\textrm{if}\\ \\exists\\ \\textrm{word\\ j\\ in\\ sentence\\ i}\\\\\n0, & \\textrm{otherwise}\\\\\n\\end{array}\n\\right.\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle S^{a}=\\left(\\begin{array}[]{cccc}s^{a}_{11}&amp;s^{a}_{%&#10;12}&amp;\\ldots&amp;s^{a}_{1n}\\\\&#10;s^{a}_{21}&amp;s^{a}_{22}&amp;\\ldots&amp;s^{a}_{2n}\\\\&#10;\\vdots&amp;\\vdots&amp;&amp;\\vdots\\\\&#10;s^{a}_{m1}&amp;s^{a}_{m2}&amp;\\ldots&amp;s^{a}_{mn}\\\\&#10;\\end{array}\\right),a\\in K\\\\&#10;\\displaystyle s^{a}_{ij}=\\left\\{\\begin{array}[]{cc}FP_{ij},&amp;\\textrm{if}\\ %&#10;\\exists\\ \\textrm{word\\ j\\ in\\ sentence\\ i}\\\\&#10;0,&amp;\\textrm{otherwise}\\\\&#10;\\end{array}\\right.\\end{split}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mrow><msup><mi>S</mi><mi>a</mi></msup><mo>=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msubsup><mi>s</mi><mn>11</mn><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mn>12</mn><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>n</mi></mrow><mi>a</mi></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mi>s</mi><mn>21</mn><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mn>22</mn><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>n</mi></mrow><mi>a</mi></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd/><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mi>s</mi><mrow><mi>m</mi><mo>\u2062</mo><mn>1</mn></mrow><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mrow><mi>m</mi><mo>\u2062</mo><mn>2</mn></mrow><mi>a</mi></msubsup></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><msubsup><mi>s</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow><mi>a</mi></msubsup></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mi>a</mi><mo>\u2208</mo><mi>K</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>a</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mrow><mi>F</mi><mo>\u2062</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mrow><mpadded width=\"+5pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><mrow><mo rspace=\"7.5pt\">\u2203</mo><mtext>word\u00a0j\u00a0in\u00a0sentence\u00a0i</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"center\"><mtext>otherwise</mtext></mtd></mtr></mtable><mi/></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.07124.tex", "nexttext": "\n\n\\subsection{Term Frequency-Inverse Sentence Frequency (TF-ISF)}\n\\label{sc_tfisf}\n\nOne way to verify the initial relevance of a word and a sentence to the text is through the TF-ISF. This metric is based on term frequency in the text and it is calculated by the equation \\ref{tfisf}.\n\n\n", "itemtype": "equation", "pos": 11290, "prevtext": "\n\n\\subsection{Jensen-Shannon divergence}\n\\label{ss_djs}\n\nWe use Jensen-Shannon (JS) divergence to measure the similarity between sentences. Let $w$ be a words\u00e2\u0080\u0099 set in P and Q. P and Q represent\nthe probability distribution between two objects:\ntwo individuals sentences or a sentence and a set of sentences.\nThe divergence will then calculated among these two objects. The \\ac{JS} divergence is symmetric and provides a stable way to measure the difference between two distributions (equation \\ref{DJS}).\n\n\\begin{eqnarray}\n\\label{DJS}\n\nD_{JS}(P||Q) &=& \\frac{1}{2}\\sum_{w \\in W} \\Bigg[ P_w\\log{\\Bigg( \\frac{2 \\times P_w}{P_w+Q_w} \\Bigg)} \\nonumber \\\\\n  &+& Q_w\\log{\\Bigg( \\frac{2 \\times Q_w}{P_w+Q_w} \\Bigg)} \\Bigg]\\\\ \\nonumber\n\n\\end{eqnarray}\n\nThe \\ac{JS} divergence value ranges from $[0,\\infty+)$. It is closer to zero when the distributions are similar and they differ in another case. \n\nIn the case there is a word in a sentence that is missing in another one, a smooth (different weighting) will be used to avoid null values and have a smoother distribution \\cite{smoothBook}. If a word $w$ is not present in the sentence $Q$, then the smooth is calculated by the equation \\ref{smooth}, where $ \\beta = 1.5 \\times voc$, which $voc$ is the number of distinct words in $R$, $\\gamma$ is the variable that controls the relevance of the missing word in the sentence and $N$ is the number of words in $R$ \n\\cite{nenkova}.\n\n\n", "index": 3, "text": "\\begin{equation}\n \\label{smooth}\n  Q_w = \\left( \\frac{P_w + \\gamma}{N + \\gamma \\times \\beta} \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"Q_{w}=\\left(\\frac{P_{w}+\\gamma}{N+\\gamma\\times\\beta}\\right)\" display=\"block\"><mrow><msub><mi>Q</mi><mi>w</mi></msub><mo>=</mo><mrow><mo>(</mo><mfrac><mrow><msub><mi>P</mi><mi>w</mi></msub><mo>+</mo><mi>\u03b3</mi></mrow><mrow><mi>N</mi><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u00d7</mo><mi>\u03b2</mi></mrow></mrow></mfrac><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07124.tex", "nexttext": "\n\n\\noindent where $tf(w)$ is frequency of term $w$, $n$ is total number of documents and $n_w$ is number of documents that contain the term $w$.\n\n\\section{The LIA-RAG system}\n\\label{sc:rag}\n\nIn general lines, a text consists of several sentences with different topics. The text can be divided into several groups and each of them describes one step/idea in the text. If a group is large, then it is relevant to the text. It is possible to choose the sentences of the largest group and obtains the most relevant content.\n\nThe main ideas of a text are generally analyzed and discussed several times. The vertices with higher degree have more similar sentences and then, are important to the text. However, it is not necessary to have a lot of similar sentences to be a relevant one.\n\n\\ac{RAG} is a summarizer system by sentence extraction, which selects the main sentences of a text and uses a post-processing to remove some errors and make the text more concise and compact.\n\n\\subsection{The RAG algorithm}\n\\label{ssc:rag_desc}\n\n\\ac{RAG} uses Graph theory and divergence metrics to calculate the similarity and to group the sentences. Initially, the system performs a filtering process to remove the brackets. Then, it performs a segmentation, filtering and stemming processes to remove stopwords and reduce the words to their roots. RAG accomplished this preprocessing and matrix transformation based on \\cite{Torres-Moreno2001}. It calculates the relevance of each sentence based on TF-ISF metric (equation \\ref{tfisf}) and removes the less relevant sentences.\n\nThe system creates a graph $G$ which each vertex represents a sentence previously selected. The text is analyzed and modeled as a sentence graph (vertices). Based on equation \\ref{tfisf}, it calculates the similarity between sentences. If the similarity between two sentences is less than 0.16 (threshold obtained by empirical testing), then the system creates an edge between them. So, the vertices with higher degrees have the most relevant content of the text. However, some sentences may have a small degree, but they may contain important information.\n\nRAG combines the TF-ISF and degree sentences to analyze the relevance of them. The relevance of the sentence $i$ is defined by:\n\n\n", "itemtype": "equation", "pos": 11691, "prevtext": "\n\n\\subsection{Term Frequency-Inverse Sentence Frequency (TF-ISF)}\n\\label{sc_tfisf}\n\nOne way to verify the initial relevance of a word and a sentence to the text is through the TF-ISF. This metric is based on term frequency in the text and it is calculated by the equation \\ref{tfisf}.\n\n\n", "index": 5, "text": "\\begin{equation}\n \\label{tfisf}\n  tf\\_isf(w) = tf(w) \\times \\log \\left( \\frac{n}{n_{w}} \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"tf\\_isf(w)=tf(w)\\times\\log\\left(\\frac{n}{n_{w}}\\right)\" display=\"block\"><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00d7</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo>(</mo><mfrac><mi>n</mi><msub><mi>n</mi><mi>w</mi></msub></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07124.tex", "nexttext": "\n\n\\noindent where $degree(i)$ is the degree of vertex $i$ and $rel(i)$ is the relevance of the sentence $i$.\nAfter, the system creates a summary with the higher score sentences, excluding similar (or redundant) sentences based on Dice's coefficient \\cite{dice}. \n\nThe figure \\ref{fig:rag} describes the RAG system.\n\n\\begin{figure}[!h] \n\\begin{center} \n\\includegraphics[scale=0.42]{rag.pdf}\n\\end{center} \n\\caption{Architecture of the RAG system.} \n\\label{fig:rag} \n\\end{figure}\n\n\\subsection{LIA-RAG: RAG with a specific speech post-processing}\n\\label{ssc:rag_post}\n\nThe speech recognition process produces a text that contains several grammatical problems (slang, colloquialisms, expressions and speech recognition errors). An extraction summary algorithm selects the relevant sentences, however the sentences may have some grammatical problems. So, it is necessary to perform a treatment of this summary.\n\nThe main analyzed aspects in this process are: \n\\begin{itemize}\n\\item Colloquialisms, \n\\item Speech expressions and \n\\item Dates. \n\\end{itemize}\n\nLIA-RAG system receives the summary as an input.\nIn this input, some speech expressions are used to connect ideas or concepts in oral conversations.\nLIA-RAG removes these expressions, because often they are incorrectly transcripted (a noise source). Also, the system eliminates several colloquialisms and the duplicated words. The system replaces some mistaken words by its correct form. The figure \\ref{fig:lia-rag} shows the architecture of the LIA-RAG system.\n\n\\begin{figure}[!h] \n\\begin{center} \n\\includegraphics[scale=0.38]{lia-rag.pdf}\n\\end{center} \n\\caption{Architecture of the LIA-RAG system.} \n\\label{fig:lia-rag} \n\\end{figure}\n\n\\section{Results}\n\\label{sc:results}\n\nThe tests were carried on a computer with i5@2.6 GHz processor and 4 GB of RAM on GNU/Linux Debian 64-bit operating system. The algorithms of RAG were implemented using the Perl language.\n\nWe used the French DECODA corpus \\cite{BECHET12.684}. The systems have to generate textual summaries with the main idea of each conversation belonging to the corpus. \\textsl{``The conversations topics range\nfrom itinerary and schedule requests, to lost and found, to complaints (the calls were recorded during strikes)''}\\cite{benoit}. Each summary has 7\\% of the number of words of each conversation transcription. We compared LIA-RAG and RAG systems with two baseline systems (random and  first lead base). \n\n\nIn order to evaluate the quality of the summaries, Multiling CCCS used the system \\ac{ROUGE}\\footnote{The options for running ROUGE 1.5.5 are -a -l 10000 -n 4 -x -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0}, which determines the quality of an automatic summary based on the intersection of the $n$-grams of a candidate summary and the $n$-grams of a set of reference summaries. More specifically, we used ROUGE-N and ROUGE-SU measures. ROUGE-N, N $\\in\\ [1, 2]$. ROUGE is an $n$-gram recall measure \\cite{rouge}\\footnote{\\url{http://www.berouge.com/Pages/default.aspx}}. The values of these metrics belongs to $[0, 1]$, 1 for the best result.\n\nThe table \\ref{tb:rouge_training} shows the results obtained using the systems over the training corpus.\nThis corpus contains 50 conversations transcription with 23,363 words and 115 summaries. Both versions of RAG provided the best results. The RAG system identified the main sentences discussed in conversations. \nHowever, the errors and speech expressions decreased the informativeness. \nThe post-processing of LIA-RAG allowed to improve the results. \nThis process reduces errors and generates a more informative and concise summary.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{|c|ccc|}\n\\hline\n \\textbf{Systems} \t& \\small \n\\textbf{ROUGE-1}    \t& \\small \\textbf{ROUGE-2}    \t& \\small \\textbf{ROUGE-4}     \t\\\\ \\hline\n \\textbf{LIA-RAG:1}\t\t\t&   \\textbf{0.1893}  \t&   \\textbf{0.0628}  \t& \\textbf{0.0683} \\\\ \n \\textit{RAG}\t\t\t&   \\textit{0.1833}  \t&   \\textit{0.0614}  \t& \n \\textit{0.0654} \\\\ \n Base-first \t&   0.1578  \t&   0.0556  \t& 0.0583 \\\\ \n Base-rand \t\t&   0.1170  \t&   0.0310  \t& 0.0371 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\label{tb:rouge_training} Evaluation of training corpus.}\n\\end{table}\n\nThe French test corpus has 100 conversations transcription with 42,130 words and 212 summaries. The ROUGE-2 official performance for the systems participating to CCCS pilot task is showed in table \\ref{tb:rouge_test} \\cite{benoit}. The LIA-RAG system  obtained the best results.\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{|c|c|}\n\\hline\n \\textbf{Systems} \t& \\textbf{ROUGE-2} \t\\\\ \\hline\n \\textbf{LIA-RAG:1}\t\t&  \\textbf{0.037} \\\\\n NTNU:1 \t\t&  0.035 \\\\ \n NTNU:3\t\t\t&  0.034 \\\\ \n NTNU:2 \t\t&  0.027 \\\\ \n \\hline\n\\end{tabular}\n\\caption{\\label{tb:rouge_test} Evaluation of test corpus.}\n\\end{table}\n\n\\section{Conclusion and perspectives}\n\\label{sc:conc}\n \nDivergence of probabilities in a graph model to extract key sentences in French speech-to-text summarization was very interesting. LIA-RAG system uses very few language resources (stopwords and stemming) and has achieved good results. \nNevertheless, the system is easily adaptable to other languages with only some modifications in the preprocessing stage.\n\nAn interesting perspective of this work consists in the utilization of the speech TAGs markers to improve the computation of the sentences score. \nIn addition, it is necessary to improve the post-processing in order to increase the quality of the final summary.\nFinally, the verification of the grammaticality and readability of the extracted key sentences can help to produce more realistic abstracts.\n\n\n\\section*{Acknowledgments}\nThis project was partially founded by a scholarship from FUNCAP-CE (Brazil).\n\n\\newpage\n\n\n\\bibliographystyle{acl}\n\\bibliography{multi}\n\n\n", "itemtype": "equation", "pos": 14051, "prevtext": "\n\n\\noindent where $tf(w)$ is frequency of term $w$, $n$ is total number of documents and $n_w$ is number of documents that contain the term $w$.\n\n\\section{The LIA-RAG system}\n\\label{sc:rag}\n\nIn general lines, a text consists of several sentences with different topics. The text can be divided into several groups and each of them describes one step/idea in the text. If a group is large, then it is relevant to the text. It is possible to choose the sentences of the largest group and obtains the most relevant content.\n\nThe main ideas of a text are generally analyzed and discussed several times. The vertices with higher degree have more similar sentences and then, are important to the text. However, it is not necessary to have a lot of similar sentences to be a relevant one.\n\n\\ac{RAG} is a summarizer system by sentence extraction, which selects the main sentences of a text and uses a post-processing to remove some errors and make the text more concise and compact.\n\n\\subsection{The RAG algorithm}\n\\label{ssc:rag_desc}\n\n\\ac{RAG} uses Graph theory and divergence metrics to calculate the similarity and to group the sentences. Initially, the system performs a filtering process to remove the brackets. Then, it performs a segmentation, filtering and stemming processes to remove stopwords and reduce the words to their roots. RAG accomplished this preprocessing and matrix transformation based on \\cite{Torres-Moreno2001}. It calculates the relevance of each sentence based on TF-ISF metric (equation \\ref{tfisf}) and removes the less relevant sentences.\n\nThe system creates a graph $G$ which each vertex represents a sentence previously selected. The text is analyzed and modeled as a sentence graph (vertices). Based on equation \\ref{tfisf}, it calculates the similarity between sentences. If the similarity between two sentences is less than 0.16 (threshold obtained by empirical testing), then the system creates an edge between them. So, the vertices with higher degrees have the most relevant content of the text. However, some sentences may have a small degree, but they may contain important information.\n\nRAG combines the TF-ISF and degree sentences to analyze the relevance of them. The relevance of the sentence $i$ is defined by:\n\n\n", "index": 7, "text": "\\begin{equation}\n\\label{eq:score}\n  rel(i) = degree(i) \\times tf\\_isf(i)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"rel(i)=degree(i)\\times tf\\_isf(i)\" display=\"block\"><mrow><mrow><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u00d7</mo><mi>t</mi></mrow><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]