[{"file": "1601.01566.tex", "nexttext": "\n\nUsing the calculated transformation matrix, the 3D points detected in Kinect V2 color image and depth data can be transformed from the camera coordinate system to the robot's base coordinate system.\n\n\\subsection{Robot Motion Planning}\n\nRobot arm control in Cartesian coordinates is being used in the project, given the relatively simple movements, as well as limited workspace. Multiple motion planning algorithms included in \\textit{MoveIt!} framework~\\cite{sucan2013moveit} were tested. the RRT-connect approach~\\cite{kuffner2000rrt}, based on the Rapidly exploring random tree, was found to be suitable for the task. We were using an implementation from the from the Open Motion Planning Library (OMPL)~\\cite{sucan2012open}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{./figures/ReprojectionError2.pdf}\n\\caption{Reprojection error shown in the color image and depth point cloud overlay. The offset in the left image is caused by imprecisely defined relative positions between the color and infrared cameras in the 3D camera. Internal camera calibration compensates for this error. The result is seen in the image on the right side, where the offset is reduced.}\n\\label{fig:reprojection_error}\n\\vspace{-0.5cm}\n\\end{figure}\n\nIn order to achieve a high-quality Kinect V2 internal calibration, the samples should include the checkerboard positioned in the majority of the camera's field of view and for least at two distances. Furthermore, tilting the checkerboard at different angles in relation to the camera is also beneficial~\\cite{iaikinect2}.\n\nIn order to simplify the Kinect V2 internal calibration, it was decided to collect all the data at once and then the calibration using the whole dataset was calculated. However, this meant that lens distortion was still present during the data collection. Furthermore, a reprojection error occurs, which is an offset between the color image and depth data, shown in Figure~\\ref{fig:reprojection_error}. These issues caused the Eye-to-Hand transformation to be imprecise, especially for points closer to the edge of the camera image, where lens distortion is more significant.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{./figures/RobotMoveTrajectory.pdf}\n\\caption{Robot movement trajectory as seen in the 3D camera image. It is split into multiple stages by the positions calculated at increasing distance from the center point of the image. Movements are done stage-by-stage, while improving the Eye-to-Hand transformation accuracy at each step. This figure shows just a two stage example.}\n\\label{fig:robot_move_stages}\n\\vspace{-0.2cm}\n\\end{figure}\n\nA robot movement trajectory was chosen with small offsets from the starting point. It can be split into multiple stages by the positions calculated at increasing distance from the center point of the image. The number of stages depend on the selected overlap of the checkerboard in the camera image, size of the checkerboard, reach-ability of the robot arm as well as the size of the area covered by the camera. At each new position, the detected checkerboard intersection points are accumulated and Eye-to-Hand calibration was recalculated to continually improve the accuracy. The example two-stage robot movement trajectory is shown in Figure \\ref{fig:robot_move_stages}. However, this data is not considered for the final Eye-to-Hand calibration, because the 3D camera sensor itself is still not calibrated at this point.\n\nThis approach has shown to reduce the robot movement error and by the time positions close to the image edges are chosen, the transformation is accurate enough not to exit the camera's field of view, where checkerboard cannot be detected anymore.\n\nAt each position, the checkerboard is tilted by defining changing roll, pitch and yaw of the end effector ranging from -45{\\degree} to 45{\\degree} with 45{\\degree} steps. Images and detected 3D corner positions in color and infrared camera images are saved at each pose. If the desired point is outside the robot workspace, it is automatically identified by the planning algorithm and skipped. The same trajectory is performed with a 20 cm depth offset further away from the camera in order to have data at different distances from the sensor calibrated.\n\nOnce the planned movement trajectory is completed, internal Kinect V2 camera calibration is performed using the collected data.\n\n\\subsection{Repeated Eye-to-Hand Calibration}\n\nAfter the internal calibration of each 3D camera, the accuracy of the Eye-to-Hand calibration is not precise because of compensated lens distortion and adjustments to reduce the reprojection error. The simplified move sequence, without the tilting, is repeated with the robot moving to previously visited positions by reusing the same coordinates and just Eye-to-Hand calibration recalculated. Once this process is finished, the sensor in the system is fully calibrated.\n\n\\subsection{Checkerboard Observation}\n\nWhile one 3D camera is being calibrated, any other sensors included in the system are observing the robot and running the simplified checkerboard detection algorithm. If the checkerboard is detected, the pose of the checkerboard and the pose of the robot, which is being streamed on the network by the robot controlling node, are being saved. In any subsequent checkerboard detection instances, the position is compared to the position of the previous detection, and if the current one is closer to the center of the color image, the poses are updated. Once the current calibration of a 3D camera is completed, the request is sent to the robot to move to the detected position and start the calibration procedure for the other sensor.\n\n\n\\section{Experiments and Results}\n\\label{sec:experiments}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presented calibration process was successfully performed provided that the checkerboard was detected by the 3D camera to be calibrated. Otherwise, the robot was repositioned manually to make sure that the checkerboard was within the field of view of the camera.\n\nGiven a close to autonomous operation of our framework, we conducted experiments to analyze the number of checkerboard positions recorded versus the achieved calibration accuracy. As the process is identical for each of the 3D cameras, for easier comparison, the results from one 3D camera is presented in the experiments section, with plans to publish experiment-oriented follow-up work based on variety of different setups and corrected by sensor fusion. Results are divided into two sections according to the calibration type, each one requiring an independent set of robot moves and data collection:\n\\begin{enumerate}\n  \\item Internal camera calibration\n  \\item Eye-to-Hand calibration\n\\end{enumerate}\n\n\\subsection{Internal Camera Calibration}\n\nThe first iteration of movements was made in order to calibrate the 3D camera internally, using the robot trajectory explained in Figure~\\ref{fig:robot_move_stages}. Because the field of view of the color camera and the infrared camera in the sensor are different, the checkerboard was not always visible or successfully detected in both cameras at the same time. This explains the varying number of detections in each of the sensor's cameras, as well as simultaneously in both, which we refer to as \\textit{combined}. There were 9 experiments conducted in total. Experiments 1-2 had large overlap in checkerboard positions and tilting, experiments 3-6 had no overlap anymore and experiments 7-9, no more tilting. Experiment data is summarized in Table~\\ref{table:internal_calib_summary}.\n\n\\begin{table}[h]\n\\caption{Experiment data for internal sensor calibration.}\n\\label{table:internal_calib_summary}\n\\centering\n\n\\begin{tabular}{ |p{0.7cm}||p{0.75cm}|p{0.75cm}|p{1.1cm}|p{0.8cm}|p{0.65cm}|p{0.8cm}|}\n \\hline\n Exp \\# & Color Frames & IR Frames & Combined Frames & Overlap & Tilting & Time (sec)\\\\\n \\hline\n Exp 1 & 234 & 215 & 158 & Yes & Yes & 613\\\\\n Exp 2 & 120 & 109 & 81 & Yes & Yes & 338\\\\\n Exp 3 & 78 & 72 & 55 & No & Yes & 218\\\\\n Exp 4 & 57 & 54 & 45 & No & Yes & 176\\\\\n Exp 5 & 44 & 41 & 33 & No & Yes & 142\\\\\n Exp 6 & 39 & 35 & 26 & No & Yes & 128\\\\\n Exp 7 & 15 & 14 & 14 & No & No & 57\\\\\n Exp 8 & 10 & 9 & 7 & No & No & 45\\\\\n Exp 9 & 5 & 5 & 5 & No & No & 36\\\\\n \\hline\n\\end{tabular}\n\\vspace{-0.2cm}\n\\end{table}\n\nFigure~\\ref{fig:results_chart} shows the calibration results by analyzing the average error in pixels of each of the sensor's cameras and the reprojection error for each of the experiments. Errors were calculated using the known geometry and size of the checkerboard and comparing the calibrated sensor estimation of the checkerboard dimensions according to the square intersection points to the known geometry. The higher the error, the lower the calibration accuracy. \n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.42\\textwidth]{./figures/ErrorRateGraph2.pdf}\n\\caption{Calibration accuracy results by showing the errors of internal 3D camera calibration. Color camera, IR errors define averages of each sensor's cameras. Reprojection error defines the average error of the offset in the images between the color image and the depth information, seen in Figure~\\ref{fig:reprojection_error}. It has to be noted that left Y axis for error rates is in log scale.}\n\\label{fig:results_chart}\n\\vspace{-0.2cm}\n\\end{figure}\n\n\\subsection{Eye-to-Hand Calibration}\n\n\\begin{table}[h]\n\\caption{Experiment data for Eye-to-Hand calibration.}\n\\label{table:eyehand_calib_summary}\n\\centering\n\n\\begin{tabular}{ |p{0.7cm}||p{1.0cm}|p{1.0cm}|p{1.2cm}|p{1.9cm}|}\n \\hline\n Exp \\# & Frames & Overlap & Time (sec) \\\\\n \\hline\n Exp 1 & 82 & Yes & 154\\\\\n Exp 2 & 42 & Yes & 88\\\\\n Exp 3 & 14 & No & 37\\\\\n Exp 4 & 9 & No & 23\\\\\n Exp 5 & 5 & No & 15\\\\\n \\hline\n\\end{tabular}\n\\vspace{-0.2cm}\n\\end{table}\n\nThe second iteration of moves were performed for Eye-to-Hand calibration, while using the most accurate internal 3D camera calibration mentioned previously. In this part, tilting was not performed and the calibration checkerboard was kept at a constant angle, parallel to the camera image plane. 5 experiments were conducted in total, using different number of frames, as seen in Table~\\ref{table:eyehand_calib_summary}. Experiment 1 had a large overlap in checkerboard positions, in experiment 2 there was a small overlap, while in the rest there was no overlap and even some gaps between the positions.\n\n\\subsection{Result Analysis}\n\nFor the internal calibration, it can be seen that in the first 6 experiments, even with a significantly lower number of frames used, the error in all of the sensor's cameras did not increase much. However, experiments 7 to 9, where the calibration checkerboard was present only in the part of the camera's field of view and was not tilted, show a significant increase in errors. It can be concluded that the most important part to achieve good internal calibration accuracy is to cover the field of view of the camera and perform tilting, but overlapping same areas with the checkerboard is not mandatory.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{./figures/SecondPartErrorRateGraph.pdf}\n\\caption{Eye-to-Hand calibration accuracy results. Overall position error (in cm) as well as each axis separately are shown by comparing the actual robot position versus the predicted robot position from the 3D camera sensor.}\n\\label{fig:results_chart2}\n\\vspace{-0.2cm}\n\\end{figure}\n\nFigure~\\ref{fig:results_chart2} shows that the average error rate of Eye-to-Hand calibration has inverse correlation to the number of frames used. The larger area of the camera's field of view is used, the more accurate calibration is achieved. Looking at the overall average error, experiment 3 seems to be the most optimal choice when considering the number of frames used and accuracy achieved. Interesting effect of Z-axis having a significantly larger error compared to X and Y was observed. It is likely to be caused by the noisy depth data from 3D camera, which should be compensated using more specific methods. On the other hand, it proves that position estimation in X and Y-axes only have even lower error compared to our indicated overall error of the calibration.\n\n\\section{CONCLUSION AND FUTURE WORK}\n\\label{sec:conclusion}\n\nA simple and flexible calibration method for systems containing a robot and one or more 3D cameras was presented. It is based on the robot moving a standard calibration checkerboard and being guided by the information sent from each of the cameras to cover the largest possible area in the field of view, to ensure an accurate calibration.\n\nA full calibration, including a sensor internal calibration together with an Eye-to-Hand calibration can be done, or just the second part separately, given that the sensor was already calibrated internally. Modular design ensures that sensors can be added or removed easily, as well as hardware components interchanged without any modifications to the algorithm.\n\nAccording to experiment results, achieving good calibration requires the robot to cover the majority part of the field of view of the 3D camera to achieve a good accuracy. Using our system, a good accuracy calibration of one 3D sensor taken just out of the box, can be achieved in just a few minutes with minimal supervision by the operator.\n\nThe framework will be further tested with a variety of physical setups and different 3D cameras and multiple robot arm types. We plan to open source the code, making it accessible to researchers allowing further testing and development.\n\nAlgorithm improvements will include a simultaneous calibration of multiple cameras provided that the calibration checkerboard is within the field of view. Furthermore, if only part of the field of view of the camera will be used in the operation, it could be defined by the user and instead of calibrating the whole image area, only the area of interest would be used. Automatic accuracy detection upon start-up of the system will be added to determine if any of the sensor was either moved or lost accuracy by simply moving the robot arm to a multiple known locations and comparing actual position to the estimated position by the camera.\n\n\\addtolength{\\textheight}{-12cm}   \n                                  \n                                  \n                                  \n                                  \n                                  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{IEEEexample}\n\n\n\n\n\n", "itemtype": "equation", "pos": 15365, "prevtext": "\n\n\n\n\\maketitle\n\\thispagestyle{empty}\n\\pagestyle{empty}\n\n\n\n\\begin{abstract}\n\nWith 3D sensing becoming cheaper, environment-aware robot arms capable of safely working in collaboration with humans will become common. However, a reliable calibration is needed, both for camera internal calibration, as well as Eye-to-Hand calibration, to make sure the whole system functions correctly. We present a framework, using a novel combination of well proven methods, allowing a quick automatic calibration of the system consisting of the robot and a varying number of 3D cameras by using a standard checkerboard calibration grid. It is based on a modular design for an easy adaptation after any hardware or algorithm changes. Our approach allows a quick camera-to-robot recalibration after any changes to the setup, for example when cameras or robot were repositioned. The framework has been proven to work by practical experiments to analyze the quality of the calibration versus the number of positions of the checkerboard used for each of the calibration procedures.\n\n\\end{abstract}\n\n\n\n\\section{INTRODUCTION}\n\nIn many practical applications, industrial robots are still working \"blind\" with hard-coded trajectories. This results in the workspace for robots and humans being strictly divided in order to avoid any accidents, which, unfortunately, sometimes still occur. It is often more common to have collision detection systems, which do not always work as expected, rather than collision prevention methods~\\cite{ur5collision}. However, \\textit{environment-aware robots}~\\cite{flacco2012depth}~\\cite{rakprayoon2011kinect} are becoming more common, both developed in research and by robot manufacturers themselves, e.g. Baxter by Rethink Robotics~\\cite{fitzgerald2013developing}. \n\nLow-cost and high-accuracy 3D cameras, also called RGB-D sensors, like Kinect V2~\\cite{Fankhauser2015KinectV2ForMobileRobotNavigation}, are already available. They are suitable for a precise environment sensing in the workspace of a robot, providing both color image and depth information~\\cite{smisek20133d}. However, external sensors are commonly used in \\textit{fixed positions} around the robot and are normally not allowed to be moved. After any reconfiguration in the setup, the whole system has to be calibrated, usually by a skilled engineer. Camera calibration can be divided into two main stages:\n\\begin{itemize}\n\\item Internal camera parameters, like lens distortion, focal length, optical center, and for RGB-D cameras, color and depth image offsets~\\cite{opencvchessboard}~\\cite{Fankhauser2015KinectV2ForMobileRobotNavigation}.\n\\item External camera parameters: the pose (position and orientation) of a camera in a reference coordinate frame. It is commonly called Eye-to-Hand calibration~\\cite{ma2014hand}~\\cite{horaud1995hand}. The Eye-to-Hand calibration, or transformation from the camera coordinate system to the robot base coordinate system is shown in Figure~\\ref{fig:system_setup}.\n\\end{itemize}\n\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.35\\textwidth]{./figures/RobotCameraFigure.pdf}\n\\caption{System Setup with two Kinect V2 depth sensors aimed at the robot end effector at approximately 45{\\degree}  viewpoints. In the system, Eye-to-Hand calibration is represented by the Affine transformation matrix $T_{C}^{R}$, which transforms the coordinate system of each camera to the coordinate system of the robot base, making it common for the whole setup.}\n\\label{fig:system_setup}\n\\vspace{-0.2cm}\n\\end{figure}\n\nNormally, it is sufficient to perform an internal camera parameter calibration only once per sensor unless the lens or sensor itself will be changed or modified. Reliable calibration methods already exist, which are widely used~\\cite{tsai1987versatile}~\\cite{iaikinect2}~\\cite{foix2011lock}~\\cite{amon2014evaluation}.\n\nEye-to-Hand calibration, on the other hand, is more application specific and crucial for precise environment sensing by the robot or vision guided robot control (visual servoing)~\\cite{lippiello2005eye}. Some work has been successful in calibrating multiple cameras and a robot using a custom-made target object placed in a common field of view for all the sensors in the workspace~\\cite{heikkila2000flexible}. Another method calibrated multiple cameras fixed on a rig using structure-of-motion method to estimate relative positions between the cameras~\\cite{esquivel2007calibration}. A similar approach was used for calibrating a network of Kinect sensors aimed at robotic inspection of large work-spaces, where sensors are in fixed positions~\\cite{macknojia2013calibration}. Robot arm mounted camera calibration by moving it to the points of the calibration grid, which is in a fixed position was also proposed~\\cite{zhuang1995simultaneous}~\\cite{dornaika1998simultaneous}. However, most of the presented work is either aimed at the very specific setup or requires a large amount of manual placement of calibration grids, making it time-consuming.\n\nThis paper presents a framework to be used for an automatic combined internal camera parameter and Eye-to-Hand calibration by utilizing a robot arm manipulator to actively move around the standard checkerboard calibration grid. The framework is using existing and reliable calibration approaches, but is based on a novel combination of methods to make the calibration process fully automatic and adaptable to as few or as many external 3D cameras as needed. The whole system is based on the Robot Operating System (ROS) and making use of the modular design and existing integration for a large amount of robot and sensor types~\\cite{quigley2009ros}. This allows the actual hardware to be interchangeable as well as the reconfiguration of the camera placement to be undertaken with a simple and quick automatic recalibration requiring only minimal supervision.\n\nThis paper is organized as follows. We present the system setup in Section~\\ref{sec:system_setup}. Then we explain the method in Section~\\ref{sec:method}. We provide experimental results in Section~\\ref{sec:experiments}, followed by relevant conclusions and future work in Section~\\ref{sec:conclusion}.\n\n\n\\section{SYSTEM SETUP}\n\\label{sec:system_setup}\n\nThe system setup consists of two main hardware elements: a robot arm manipulator and one or more depth 3D sensors with a visual camera, in our case Kinect V2.\n\nWith the main goal of achieving an environment-aware robot arm manipulator, the robot is thought to be in the center of the setup with sensors observing it from surrounding angles. Positions of the sensors do not need to be fixed, however, in case one of them is being repositioned, Eye-to-Hand part of the calibration process has to be repeated.\n\nIn the described setup, two RGB-D depth sensors were used, observing the robot arm end effector from two viewpoints, each angled at approximately 45{\\degree}. The setup can be seen in Figure \\ref{fig:system_setup}. However, the number of sensors is flexible, and only one, or as many as needed can be used as long as sufficient computing power is provided.\n\n\\subsection{Calibration Checkerboard}\n\\label{ssec:calibration_checkerboard}\n\nA custom end-effector mount to hold a checkerboard, with an extension to reduce the number of robot self-collisions, was 3D printed and attached to the end-effector, shown in Figure \\ref{fig:cb_mount}. The checkerboard contains 7 by 5 squares, each one of 30 mm by 30 mm size, printed on an A4 paper sheet, which is mounted on hard plexiglass surface to prevent any deformation. One of the side squares is modified to be hollow, as shown in Figure \\ref{fig:checkerboard}, and is used to identify correct orientation as described in Section \\ref{sec:method}.\n\n\\begin{figure}[h]\n\\centering\n\n\\begin{subfigure}[t]{0.23\\textwidth}\n    \\includegraphics[width=\\textwidth]{./figures/BoardAttachment.jpg}\n    \\caption{A custom end-effector mount with a rigid plexiglass base for holding a checkerboard.}\n    \\label{fig:cb_mount}\n\\end{subfigure}\n~\n\\begin{subfigure}[t]{0.23\\textwidth}\n    \\includegraphics[width=\\textwidth]{./figures/checkerboard.pdf}\n    \\caption{Detected square intersection points are marked in red and a hollow square in the top-left corner, for orientation detection.}\n    \\label{fig:checkerboard}\n\\end{subfigure}\n\\caption{Checkerboard and a custom robot mount.}\n\\label{fig:checkerboard_on_robot}\n\\vspace{-0.5cm}\n\\end{figure}\n\n\\subsection{Robot}\n\nThe robotic manipulator being used is UR5 from Universal Robots with \\textit{6 degrees of freedom}, a working radius of \\textit{850 mm} and a maximum payload of \\textit{5 kg}. The repeatability of the robot movements is \\textit{0.1 mm}.\n\n\\subsection{Sensors}\n\nThe depth sensor being used is the novel low-cost Kinect V2~\\cite{Fankhauser2015KinectV2ForMobileRobotNavigation}. It has been shown to achieve a significantly higher accuracy compared to its predecessor Kinect V1~\\cite{amon2014evaluation}. Kinect V2 is based on \\textit{time-of-flight (ToF)} approach, using a different modulation frequency for each camera, thus allowing multiple ToF cameras to observe the same object without any interference~\\cite{foix2011lock}. Kinect V2 has $1920$x$1080$ pixel resolution color camera operating at $30$ Hz and $512$x$424$ pixel resolution depth sensor with $0.5$ to $4.5$ \\textit{meters} depth range operating at $30$ Hz.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{./figures/RobotSetupPhoto.jpg}\n\\caption{Picture of the actual robot setup used. A checkerboard with a hollow square to allow the detection of its orientation is attached to the robot.}\n\\label{fig:robot_setup_photo}\n\\vspace{-0.4cm}\n\\end{figure}\n\n\\subsection{Software}\n\nThe whole system software is based on the Robot Operating System (ROS), an open-source meta-operating system running on top of Ubuntu 14.04~\\cite{quigley2009ros}. The main advantage of using ROS is its modular design allowing the algorithm to be divided into separate smaller modules performing separate tasks and sharing the results over the network. The workload in our setup was divided over multiple machines, one for each of the 3D cameras and a central one coordinating all the modules and controlling the robot.\n\nKinect V2 is not officially supported on Linux, however, open-source drivers including a bridge to ROS were found to function well, including the GPU utilisation to improve the processing speed of large amounts of data produced by sensors~\\cite{iaikinect2}.\n\nThe modular design allows for interchanging any of the modules without the need to make any modifications to the rest of the system. For example, any of the depth sensors can be exchanged to another model, or another robotic manipulator can be used, as long as the inter-modular message format is kept the same. Furthermore, addition of extra depth sensors to the system only requires adding an extra message topic for the coordinating module to listen to.\n\n\n\\section{METHOD}\n\\label{sec:method}\n\nOur proposed automatic calibration approach consists of a number of modules working together to achieve the desired accuracy of calibration. The calibration can be divided into two main parts:\n\\begin{enumerate}\n  \\item Sensor internal parameter calibration\n  \\item Eye-to-Hand calibration\n\\end{enumerate}\n\nWe first present the general overview of the system functionality and then go into details of each of the processes.\n\n\\subsection{Overview of the Whole System Functionality}\n\nThe structure of the whole calibration framework is shown in Figure~\\ref{fig:framework_overview}. A specific processing is performed by each module and the information between modules is exchanged using custom messages. Instead of having one central unit, each module publishes messages on defined topics to which other modules can subscribe to, resulting in an asynchronous direct peer-to-peer communication. Each message has a time-stamp to allow synchronization and ignoring out-of-date messages. Updating or interchanging modules can be done even at run time as long as the message format is kept identical. Additional sensors can be added in a similar manner, with the new sensor's message topic added to the configuration file, so that it is seen by the rest of the system.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{./figures/AlgorithmGraph.pdf}\n\\caption{The whole framework overview including all the modules and the sequence of the processes. Drivers are marked in blue, image analysis and move planning modules are marked in green and actual calibration modules are marked in yellow. A possibility to add additional 3D cameras to the system is represented by the objects in dashed lines.}\n\\label{fig:framework_overview}\n\\vspace{-0.2cm}\n\\end{figure}\n\n\\subsection{Checkerboard Detection}\n\nExisting algorithms included in OpenCV library were used for checkerboard detection in both color and depth data~\\cite{bradski2008learning}~\\cite{opencvchessboard}. Real-time performance is achieved with X and Y coordinates of identified intersection points of squares on the checkerboard, defined as corners, shown in Figure~\\ref{fig:checkerboard}, and depth value obtained from the depth data. Given the noisy depth data, a normalized value from the surrounding area of 10 pixels over 5 frames is taken and a median value was calculated to reduce the effects of the sensor noise.\n\nPositions in 3D coordinates of the same checkerboard corners are simultaneously calculated using the robot encoder data, corrected with the offset of the checkerboard mounting. Both the data from Kinect V2 and from robot encoders are fully synchronised according to the timestamps of when it was captured to reduce any accuracy issues.\n\nGiven the four possible orientations of the checkerboard, the modified corner square of the checkerboard, seen in the top left of the checkerboard in Figure~\\ref{fig:checkerboard}, is detected using binary thresholding method and the orientation is noted. With the collected data, the corresponding checkerboard corner data points can be matched.\n\n\\subsection{Sensor Internal Parameter Calibration}\n\nRGB-D cameras are calibrated for internal parameters using the method proposed by Zhang~\\cite{zhang2000flexible} in order to compensate for the following systematic errors:\n\\begin{enumerate}\n  \\item Color camera lens distortion\n  \\item Infrared (IR) camera lens distortion\n  \\item Reprojection error, or color to depth image offset\n  \\item Depth distortion\n\\end{enumerate}\n\nOther non-systematic and random errors like amplitude-related errors or temperature-related errors are not discussed or analysed in this paper, because standard internal camera parameter calibration procedure does not compensate for them, and they are not crucial in current application~\\cite{Fankhauser2015KinectV2ForMobileRobotNavigation}~\\cite{smisek20133d}.\n\n\\subsection{Eye-to-Hand Calibration}\n\nUsing the corresponding 3D corner points of the calibration checkerboard, a 3D Affine transformation matrix is estimated~\\cite{opencvchessboard}. With some likelihood of imprecise detection of checkerboard corners, the outlier detection based on Random Sample Consensus (RANSAC) method is being used on the inputs~\\cite{fischler1981random}. The outcome of the estimator is a 3x4 Affine transformation matrix seen in Equation ~\\ref{eq:affine_matrix}, where $R$ is a $3x3$ rotation matrix and $t$ is $3x1$ translation vector.\n\n\n\n", "index": 1, "text": "\\begin{equation}\n T_{C}^{R} = \n\\left \\{\n  \\begin{tabular}{cc}\n  $R_{3x3}$ & $t_{3x1}$ \\\\\n  $0_{1x3}$ & $1$\n  \\end{tabular}\n\\right \\}\n\\space \n\\label{eq:affine_matrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"T_{C}^{R}=\\left\\{\\begin{tabular}[]{cc}$R_{3x3}$&amp;$t_{3x1}$\\\\&#10;$0_{1x3}$&amp;$1$\\end{tabular}\\right\\}\" display=\"block\"><mrow><msubsup><mi>T</mi><mi>C</mi><mi>R</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>R</mi><msub><mi/><mrow><mn>3</mn><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mn>3</mn></mrow></msub></mtd><mtd columnalign=\"center\"><mi>t</mi><msub><mi/><mrow><mn>3</mn><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn><msub><mi/><mrow><mn>1</mn><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mn>3</mn></mrow></msub></mtd><mtd columnalign=\"center\"><mn>1</mn></mtd></mtr></mtable><mo>}</mo></mrow></mrow></math>", "type": "latex"}]