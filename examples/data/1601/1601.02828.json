[{"file": "1601.02828.tex", "nexttext": "\nwhich can approximate $f^*$ with an arbitrarily small error $\\epsilon$ with respect to a distance measure such as mean square error (provided $n$ is sufficiently large):\n\n", "itemtype": "equation", "pos": 11709, "prevtext": "\n\n\\title{Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation}\n\n\\author{Pawel~Swietojanski,~\\IEEEmembership{Student Member,~IEEE,}\\thanks{P Swietojanski and S Renals are with the Centre for Speech Technology Research, University of Edinburgh, UK, email: \\{p.swietojanski,s.renals\\}@ed.ac.uk}\n  Jinyu~Li,~\\IEEEmembership{Member,~IEEE,}\\thanks{J Li is with Microsoft Corporation. One Microsoft Way, WA, USA, email: jinyu.li@microsoft.com}\n  and~Steve~Renals,~\\IEEEmembership{Fellow,~IEEE}\\thanks{PS and SR were supported by EPSRC Programme Grant grant EP/I031022/1 (Natural Speech Technology). One of the K40 GPGPU boards used in this research was donated by NVIDA Corporation.}\n}\n\n\n\\markboth{Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing}\n{Shell \\MakeLowercase{\\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nThis work presents a broad study on the adaptation of neural network acoustic models by means of learning hidden unit contributions (LHUC) -- a method that linearly re-combines hidden units in a speaker- or environment-dependent manner using small amounts of unsupervised adaptation data. We also extend LHUC to a speaker adaptive training (SAT) framework that leads to more adaptable DNN acoustic model, which can work in both a speaker-dependent and a speaker-independent manner, without the requirement to maintain auxiliary speaker-dependent feature extractors or to introduce significant speaker-dependent changes to the DNN structure. Through a series of experiments on four different speech recognition benchmarks (TED talks, Switchboard, AMI meetings and Aurora4) and over 270 test speakers we show that LHUC in both its test-only and SAT variants results in consistent word error rate reductions ranging from 5\\% to 23\\% relative depending on the task and the degree of mismatch between training and test data. In addition we have investigated the effect of the amount of adaptation data per speaker, the quality of adaptation targets when estimating transforms in an unsupervised manner, the complementarity to other adaptation techniques, one-shot adaptation, and an extension to adapting DNNs trained in a sequence discriminative manner.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction and Summary}\n\n\\IEEEPARstart{S}{peech} recognition accuracies have improved substantially  over the past several years through the use of (deep) neural network (DNN) acoustic models. Hinton et al \\cite{Hinton2012} report word error rate (WER) reductions between 10--32\\% across a wide variety of tasks, compared with discriminatively trained \n\nGaussian mixture model (GMM) based systems.  These results use neural networks as part of both hybrid DNN/HMM (hidden Markov model) systems~\\cite{Bourlard1994,Renals1994, Seide2011, Dahl2012, Hinton2012} in which the neural network provides a scaled likelihood estimate to replace the GMM, and as tandem or bottleneck feature systems~\\cite{Hermansky2000,Grezl2007} in which the neural network is used as a discriminative feature extractor for a GMM-based system.  For many tasks it has been observed that GMM-based systems (with tandem or bottleneck features) that have been adapted to the talker are more accurate than unadapted hybrid DNN/HMM systems~\\cite{Sainath2013_cnns, Swietojanski2013, Woodland2015}, indicating that the adaptation of DNN acoustic models is an important topic that merits investigation.\n\n\nAcoustic model adaptation~\\cite{woodland2001} aims to normalise the mismatch between training and runtime data distributions that arises owing to the acoustic variability across speakers, as well as other distortions introduced by the channel or acoustic environment. \n\nIn this paper we investigate unsupervised model-based adaptation of DNN acoustic models to speakers and to acoustic environments, using a recently introduced method called  \\emph{Learning Hidden Unit Contributions} ({\\texttt{LHUC}\\xspace})~\\cite{Abdel-Hamid2013_is, Swietojanski2014_lhuc, Swietojanski:ICASSP16}.  We present the {\\texttt{LHUC}\\xspace} approach both in the context of test-only adaptation, and an extension to speaker-adaptive training (SAT), referred to as {\\texttt{SAT-LHUC}\\xspace}~\\cite{Swietojanski:ICASSP16}.  We present an extensive experimental analysis using four standard corpora: TED talks~\\cite{Cettolo2012}, AMI~\\cite{Carletta_LRE2007}, Switchboard~\\cite{godfrey1992switchboard} and Aurora4~\\cite{aurora4}.  These experiments include:  adaptation of both cross-entropy and sequence trained DNN acoustic models (Sec.~\\ref{ssec:base}--\\ref{ssec:sequence}); an analysis in terms of the quality of adaptation targets, quality of adaptation data and the amount of adaptation data (Sec.~\\ref{ssec:quality}); complementarity with feature-space adaptation techniques based on maximum likelihood linear regression~\\cite{Gales1998} (Sec.~\\ref{ssec:complementarity}); and application to combined speaker and environment adaptation (Sec.~\\ref{sec:factorisation}).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Review of Neural Network Acoustic Adaptation} \\label{ssec:adapt}\nApproaches to the adaptation of neural network acoustic models can be considered as operating either in the feature space, or in the model space, or as a hybrid approach in which speaker-, utterance-, or environment-dependent auxiliary features are appended to the standard acoustic features.\n\n\n\n\n\n\nThe dominant technique for estimating \\emph{feature space transforms} is constrained (feature-space) MLLR, referred to as CMLLR or fMLLR~\\cite{Gales1998}.  fMLLR is an adaptation method developed for GMM-based acoustic models, in which an affine transform of the input acoustic features is estimated by maximising the log-likelihood that the model generates the adaptation data based on first pass alignments. To use fMLLR with a DNN-based system, it is first necessary to train a complete GMM-based system, which is then used to estimate a single input transform per speaker.  The transformed feature vectors are then used to train a DNN in a speaker adaptive manner and another set of transforms is estimated (using the GMM) during evaluation for unseen speakers. This technique has been shown to be effective in reducing WER across several different data sets, in both hybrid and tandem approaches~\\cite{Mohamed2011, Seide2011, Hain2012, Hinton2012, Sainath2012, Sainath2013_cnns, Bell2013_mlan, Swietojanski2013}.  Similar techniques have also been developed to operate directly on neural networks. The linear input network (LIN)~\\cite{Neto1995, Abrash1995} defines an additional speaker-dependent layer between the input features and the first hidden layer, and thus has a similar effect to fMLLR.  This technique has been further developed to include the use of a tied variant of LIN in which each of the input frames is constrained to have the same linear transform -- feature-space discriminative linear regression (fDLR)~\\cite{Seide2011,Yao2012}. LIN or fDLR have been mostly used in test-only adaptation schemes; to make use of fMLLR transforms one needs to perform SAT training, which can usually better compensate against variability in acoustic space. \n\n\nAn alternative speaker-adaptive training approach -- \\emph{auxiliary features} -- augments the acoustic feature vectors with additional speaker-specific features computed for each speaker at both training and test stages.  There has been considerable recent work exploring the use of i-vectors~\\cite{Dehak2010} for this purpose.  I-vectors, which can be regarded as  basis vectors which span a subspace of speaker variability, were first used for adaptation in a GMM framework by Karafiat et al~\\cite{Karafiat2011}.  Saon et al~\\cite{Saon2013} used i-vectors to augment the input features of DNN-based  acoustic models, and showed that augmenting the input features with 100-dimensional i-vectors for each speaker resulted in a 10\\% relative reduction in WER on Switchboard (and a 6\\% reduction when the input features had been transformed using fMLLR). Gupta et al~\\cite{Gupta2014} obtained similar results, and Karanasou et al~\\cite{Karanasou2014} presented an approach in which the i-vectors were factorised into speaker and environment parts. Miao et al~\\cite{Miao2015} proposed to transform i-vectors using an auxiliary DNN which produced speaker-specific transforms of the original feature vectors, similar to fMLLR. Other examples of auxiliary features include the use of speaker-specific bottleneck features obtained from a speaker separation DNN used in a distant speech recognition task~\\cite{Liu2014}, the use of out-of-domain tandem features~\\cite{Bell2013_mlan}, and speaker codes~\\cite{Bridle1990, Abdel-Hamid2013, Xue2014_scodes} in which a specific set of units for each speaker is optimised. Speaker codes  require speaker adaptive (re-)training, owing to the additional connection weights between codes and hidden units.\n\n\n\n\\emph{Model-based adaptation} relies on a direct update of DNN parameters.  Liao~\\cite{Liao2013}  investigated supervised and unsupervised adaptation of different weight subsets using a few minutes of adaptation data.  On a large net (60M weights), up to 5\\% relative improvement was observed for unsupervised adaptation when all weights were adapted.  Yu et al~\\cite{Yu2013} have explored the use of regularisation for adapting the weights of a DNN,  using the Kullback-Liebler (KL) divergence between the speaker-independent output distribution and the speaker-adapted output distributions, resulting in a 3\\% relative improvement on Switchboard.  This approach was also recently used to adapt all parameters of sequence-trained models~\\cite{Huang2015}.  A variant of this approach reduces the number of speaker-specific parameters through a factorisation based on singular value decomposition~\\cite{Xue2014}.  Ochiai et al~\\cite{Ochiai2014} have also explored regularised speaker adaptive training with a speaker-dependent layer.\n\nDirectly adapting the weights of a large DNN results in extremely large speaker-dependent parameter sets, and a computationally intensive adaptation process.  Smaller subsets of the DNN weights may be modified, including output layer biases \\cite{Yao2012}, the bias and slope of hidden units~\\cite{Zhao2015_slopes} or training the models with differentiable pooling operators~\\cite{Swietojanski:ICASSP15}, which are then adapted in SD fashion. Siniscalchi et al \\cite{Sini2013} also investigated the use of Hermite polynomial activation functions, whose parameters are estimated in a speaker adaptive fashion.  One can also adapt the top layer in a Bayesian fashion resulting in a maximum a posteriori (MAP) approach~\\cite{huang2015maximum}, or address the sparsity of context-dependent tied-states when few adaptation data-points are available by using multi-task adaptation, using monophones to adapt the context-dependent output layer~\\cite{Huang2015_mt, Swietojanski2015_mt}. A similar approach, but using a hierarchical output layer (tied-states followed by monophones) rather than multi-task adaptation, has also been proposed~\\cite{Price2014}.\n\n\n\n\n\n\n\n\\section{Learning Hidden Unit Contributions (LHUC)} \\label{sec:lhuc}\n\nA DNN may be viewed as a set of \\emph{adaptive basis functions}, $\\psi(\\cdot)$. Under certain assumptions on the family of target functions $f^*$ (as well as on the model structure itself) the neural network can act as an universal approximator~\\cite{hornik1989multilayer, Hornik1991, Barron1993}. That is, given some vector of input random variables ${\\mathbf{x}} \\in {\\mathbb R}^{d}$ then there exists a neural network $f_n({\\mathbf{x}}) : {\\mathbb R}^{d} \\rightarrow {\\mathbb R}$ of the form\n\n", "index": 1, "text": "\\begin{equation} \\label{eq:basis}\nf_n({\\mathbf{x}}) = \\sum_{k=1}^n r_k\\psi({\\mathbf{w}}^\\top_k{\\mathbf{x}} + b_k) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"f_{n}({\\mathbf{x}})=\\sum_{k=1}^{n}r_{k}\\psi({\\mathbf{w}}^{\\top}_{k}{\\mathbf{x}%&#10;}+b_{k})\" display=\"block\"><mrow><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>r</mi><mi>k</mi></msub><mo>\u2062</mo><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>k</mi><mo>\u22a4</mo></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>+</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nIn~\\eqref{eq:basis} $\\psi : {\\mathbb R} \\rightarrow {\\mathbb R}$ is an element-wise non-linear operation applied after an affine transformation which forms an adaptive basis function parametrised by a set of biases $b_k \\in {\\mathbb R}$ and a weight vector ${\\mathbf{w}}_k\\in{\\mathbb R}^{d_{{\\mathbf{x}}}}$. The target approximation may then be constructed as a linear combination of the basis functions, each weighted by $r\\in {\\mathbb R}$. The formulation can be extended to $m$-dimensional mappings $f_n({\\mathbf{x}}) : {\\mathbb R}^{d} \\rightarrow {\\mathbb R}^m$ simply by splicing the models in~\\eqref{eq:basis} $m$ times.  The properties also hold true when considering deeper (nested) models \\cite{hornik1989multilayer} (Corollaries 2.6 and 2.7).\n\n\n\nDNN training results in the hidden units learning a joint representation of the target function and becoming specialised and complementary to each other.  Generalisation corresponds to the learned combination of basis functions continuing to approximate the target function when applied to unseen test data.  This interpretation motivates the idea of using {\\texttt{LHUC}\\xspace} -- Learning Hidden Unit Contributions -- for test-set adaptation.  In {\\texttt{LHUC}\\xspace} the network's basis functions, previously estimated using a large amount of training data, are kept fixed.  Adaptation involves modifying the combination of hidden units in order to minimise the adaptation loss based on the adaptation data.  Fig.~\\ref{fig:lhuc_si}  illustrates this approach for a regression problem, where the adaptation is performed by linear re-combination of basis functions changing only the $r$ parameters from eq.~\\eqref{eq:basis}.\n\nThe key idea of {\\texttt{LHUC}\\xspace} is to explicitly parametrise the amplitudes of each hidden unit, using a speaker-dependent amplitude function.  Let $h^{l,s}_j$ denote the $j$-th hidden unit activation (basis) in layer $l$, and let $r^{l,s}_j \\in {\\mathbb R}{}$ denote the $s$-th speaker-dependent amplitude function:\n\n\n\n\n", "itemtype": "equation", "pos": 12009, "prevtext": "\nwhich can approximate $f^*$ with an arbitrarily small error $\\epsilon$ with respect to a distance measure such as mean square error (provided $n$ is sufficiently large):\n\n", "index": 3, "text": "\\begin{equation}\n||f^*({\\mathbf{x}})-f_n({\\mathbf{x}})||_2 \\leq \\epsilon .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"||f^{*}({\\mathbf{x}})-f_{n}({\\mathbf{x}})||_{2}\\leq\\epsilon.\" display=\"block\"><mrow><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mi>f</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mi>\u03f5</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nThe amplitude is modelled using a function $\\xi : {\\mathbb R} \\rightarrow {\\mathbb R}^+$  -- typically a sigmoid with range $(0,2)$~\\cite{Swietojanski2014_lhuc}, but an identity function could be used~\\cite{Zhang2015}. ${\\mathbf{w}}^l_j$ is the $j$th column of the corresponding weight matrix ${\\mathbf{W}}^l \\in {\\mathbb R}^{d_{{\\mathbf{x}}}\\times d_{{\\mathbf{h}}}}$, $b_j^l$ denotes the bias, $\\psi$ is the hidden unit activation function (unless stated otherwise, this is assumed to be sigmoid), and $\\circ$ denotes a Hadamard product.  $\\xi$ constrains the  range of the hidden unit amplitude scaling (compare with Fig.~\\ref{fig:lhuc_si}) hence directly affecting the adaptation transform capacity -- this may be desirable when adapting with potentially noisy unsupervised targets (see Sec.~\\ref{ssec:base}).  {\\texttt{LHUC}\\xspace} adaptation progresses setting the speaker-specific amplitude parameters $r^{l,s}_j$ using gradient descent with targets provided by the adaptation data.\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=1.0\\columnwidth]{lhuc_si}\n\n\\caption{Example illustration of how {\\texttt{LHUC}\\xspace} performs adaptation (best viewed in color). Top: A ``bump'' model (eq. \\ref{eq:basis}) with two hidden units can approximate ``bump'' functions. Middle: To learn function $f_2$ given training data $f_1$ (middle), we splice two ``bump'' functions together (4 hidden units, one input/output) to learn an approximation of function $f_1$. Bottom: {\\texttt{LHUC}\\xspace} adaptation of the model optimised to $f_1$ and adapted to $f_2$ using {\\texttt{LHUC}\\xspace} scaling parameters. Image reproduced from \\cite{Swietojanski:ICASSP16}.}\n\\label{fig:lhuc_si}\n\\end{figure}\n\n The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin~\\cite{Trentin2001}, and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang~\\cite{Abdel-Hamid2013_is}. The approach was extended to unsupervised adaptation, non-sigmoid non-linearities, and large vocabulary speech recognition by Swietojanski and Renals~\\cite{Swietojanski2014_lhuc}. Other adaptive transfer function approaches for speaker adaptation have also been proposed~\\cite{Sini2013,Zhao2015_slopes}, as have ``basis'' approaches~\\cite{Wu2015, Tian2015, Decloirx2015}. However, the basis in the latter works involved re-tuning parallel models on some pre-defined clusters (gender, speaker, environment) in a supervised manner; the adaptation then relied on learning linear combination coefficients for those sub-models on adaptation data.\n\n\n\n\n\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=\\columnwidth]{lhuc_basis}\n\\caption{A 4-hidden-unit model trained to $f1 (a)$ and $f1 (b))$ for an SI approach (top) and an adapted representation (middle) keeping the resulting basis functions fixed (bottom).  (Best viewed in color.)}\n\\label{fig:lhuc_basis}\n\\end{figure}\n\n\\section{Speaker Adaptive Training LHUC (SAT-LHUC)}  \\label{sec:satlhuc}\nWhen {\\texttt{LHUC}\\xspace} is applied as a test-only adaptation it assumes that the set of speaker-independent basis functions estimated on the training data provides a good starting point for further tuning to the underlying data distribution of the adaptation data (Fig.~\\ref{fig:lhuc_si}). However, one can derive a counter-example where this assumption fails: the top plot of Fig.~\\ref{fig:lhuc_basis} shows example training data uniformly drawn from two competing distributions $f1(a)$ and $f1(b)$ where the linear recombination of the resulting basis in the average model (Fig~\\ref{fig:lhuc_basis} bottom), provides a poor approximation of adaptation data. \n \nThis motivates combining {\\texttt{LHUC}\\xspace} with speaker adaptive training (SAT)~\\cite{Anastasakos1996} in which the hidden units are trained to capture both good average representations and speaker-specific representations, by estimating speaker-specific hidden unit amplitudes for each training speaker.  This is visualised in Fig.~\\ref{fig:satlhuc_basis} where, given the prior knowledge of which data-point comes from which distribution, we estimate a set of parallel {\\texttt{LHUC}\\xspace} transforms (one per distribution) as well as one extra transform which is responsible for modelling average properties. The top of Fig.~\\ref{fig:satlhuc_basis} shows the same experiment as in Fig~\\ref{fig:lhuc_basis} but with three {\\texttt{LHUC}\\xspace} transforms -- one can see that the 4-hidden-unit MLP in this scenario was able to capture each of the underlying distributions as well as the average aspect well, given the {\\texttt{LHUC}\\xspace} transform. At the same time, the resulting basis functions (Fig~\\ref{fig:satlhuc_basis}, bottom) are a better starting point for the adaptation (Fig.~\\ref{fig:satlhuc_basis}, middle).\n\nThe examples presented in Figs.~\\ref{fig:lhuc_basis} and~\\ref{fig:satlhuc_basis} could be solved by breaking the symmetry through rebalancing the number of  training data-points for each function, resulting in less trivial and hence more adaptable basis functions in the average model. However, as we will show experimentally later,  similar effects are also present in high-dimensional speech data, and  {\\texttt{SAT-LHUC}\\xspace} training allows more tunable canonical acoustic models to be built, that can be better tailored to particular speakers through adaptation.\n\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=0.9\\columnwidth]{satlhuc_basis}\n\\caption{Learned solutions using three different {\\texttt{SAT-LHUC}\\xspace} transforms  and shared basis functions: {\\texttt{LHUC}\\xspace}-0 learns to provide a good average fit to both distributions $f1 (a)$ and $f1 (b))$ at the same time, while {\\texttt{LHUC}\\xspace}-a and {\\texttt{LHUC}\\xspace}-b are tasked to fit either $f1 (a)$ or $f1 (b)$, respectively. The bottom  plot shows the resulting basis functions (activations of 4 hidden units) of the {\\texttt{SAT-LHUC}\\xspace} training approach - one can observe {\\texttt{SAT-LHUC}\\xspace} provides a richer set of basis function which can fit  the data well on average, and can also capture some underlying characteristics necessary to reconstruct target training data -- using different {\\texttt{LHUC}\\xspace} transforms, this property is also visualised in the middle plot. \n\n(Best viewed in color.)}\n\\label{fig:satlhuc_basis}\n\\end{figure}\n\nFor {\\texttt{SAT-LHUC}\\xspace}, test-only adaptation remains the same as for {\\texttt{LHUC}\\xspace}, that is, the set of speaker-dependent {\\texttt{LHUC}\\xspace} parameters $\\theta^s_{LHUC}=\\{\\{r^{l,s}_j\\}_{j=1}^{d^l_{{\\mathbf{h}}}}\\}_{l=1}^L$ is inserted for each of test speakers and their values optimised from unsupervised adaptation data. We also use a set of {\\texttt{LHUC}\\xspace} transforms $\\theta^s_{LHUC}$, where $s=1\\ldots S$,  for the training speakers  which are jointly optimised with the speaker-independent parameters $\\theta_{SI}=\\{{\\mathbf{W}}^l, {\\mathbf{b}}^l\\}_{l=1}^L$. There is an additional speaker-independent LHUC transform, denoted by $\\theta^0_{LHUC}$, which allows the model to be used in speaker-independent fashion, for example, to produce first pass adaptation targets.\n\nTo perform SAT training with {\\texttt{LHUC}\\xspace}, we use the negative log likelihood and maximise the posterior probability of obtaining the correct context-dependent tied-state $c_t$ given observation vector ${\\mathbf{x}}_t$ at time $t$:\n\n", "itemtype": "equation", "pos": 14112, "prevtext": "\nIn~\\eqref{eq:basis} $\\psi : {\\mathbb R} \\rightarrow {\\mathbb R}$ is an element-wise non-linear operation applied after an affine transformation which forms an adaptive basis function parametrised by a set of biases $b_k \\in {\\mathbb R}$ and a weight vector ${\\mathbf{w}}_k\\in{\\mathbb R}^{d_{{\\mathbf{x}}}}$. The target approximation may then be constructed as a linear combination of the basis functions, each weighted by $r\\in {\\mathbb R}$. The formulation can be extended to $m$-dimensional mappings $f_n({\\mathbf{x}}) : {\\mathbb R}^{d} \\rightarrow {\\mathbb R}^m$ simply by splicing the models in~\\eqref{eq:basis} $m$ times.  The properties also hold true when considering deeper (nested) models \\cite{hornik1989multilayer} (Corollaries 2.6 and 2.7).\n\n\n\nDNN training results in the hidden units learning a joint representation of the target function and becoming specialised and complementary to each other.  Generalisation corresponds to the learned combination of basis functions continuing to approximate the target function when applied to unseen test data.  This interpretation motivates the idea of using {\\texttt{LHUC}\\xspace} -- Learning Hidden Unit Contributions -- for test-set adaptation.  In {\\texttt{LHUC}\\xspace} the network's basis functions, previously estimated using a large amount of training data, are kept fixed.  Adaptation involves modifying the combination of hidden units in order to minimise the adaptation loss based on the adaptation data.  Fig.~\\ref{fig:lhuc_si}  illustrates this approach for a regression problem, where the adaptation is performed by linear re-combination of basis functions changing only the $r$ parameters from eq.~\\eqref{eq:basis}.\n\nThe key idea of {\\texttt{LHUC}\\xspace} is to explicitly parametrise the amplitudes of each hidden unit, using a speaker-dependent amplitude function.  Let $h^{l,s}_j$ denote the $j$-th hidden unit activation (basis) in layer $l$, and let $r^{l,s}_j \\in {\\mathbb R}{}$ denote the $s$-th speaker-dependent amplitude function:\n\n\n\n\n", "index": 5, "text": "\\begin{align} \\label{eq:lhuc}\n  h^{l,s}_j = \\xi(r^{l,s}_j)\\circ \\psi_j \\left ({\\mathbf{w}}^{l\\top}_{j} {\\mathbf{x}} + b^l_j \\right) .\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle h^{l,s}_{j}=\\xi(r^{l,s}_{j})\\circ\\psi_{j}\\left({\\mathbf{w}}^{l%&#10;\\top}_{j}{\\mathbf{x}}+b^{l}_{j}\\right).\" display=\"inline\"><mrow><mrow><msubsup><mi>h</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup><mo>=</mo><mrow><mrow><mrow><mi>\u03be</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2218</mo><msub><mi>\u03c8</mi><mi>j</mi></msub></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\ud835\udc30</mi><mi>j</mi><mrow><mi>l</mi><mo>\u22a4</mo></mrow></msubsup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>+</mo><msubsup><mi>b</mi><mi>j</mi><mi>l</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nwhere $s$ denotes the $s$th speaker, $m_t\\in\\{0,s\\}$ selects the SI or SD {\\texttt{LHUC}\\xspace} transforms from $\\theta_{SD}\\in\\{\\theta^0_{LHUC},\\ldots,\\theta^S_{LHUC}\\}$ based on a Bernoulli distribution:\n\n", "itemtype": "equation", "pos": 21657, "prevtext": "\nThe amplitude is modelled using a function $\\xi : {\\mathbb R} \\rightarrow {\\mathbb R}^+$  -- typically a sigmoid with range $(0,2)$~\\cite{Swietojanski2014_lhuc}, but an identity function could be used~\\cite{Zhang2015}. ${\\mathbf{w}}^l_j$ is the $j$th column of the corresponding weight matrix ${\\mathbf{W}}^l \\in {\\mathbb R}^{d_{{\\mathbf{x}}}\\times d_{{\\mathbf{h}}}}$, $b_j^l$ denotes the bias, $\\psi$ is the hidden unit activation function (unless stated otherwise, this is assumed to be sigmoid), and $\\circ$ denotes a Hadamard product.  $\\xi$ constrains the  range of the hidden unit amplitude scaling (compare with Fig.~\\ref{fig:lhuc_si}) hence directly affecting the adaptation transform capacity -- this may be desirable when adapting with potentially noisy unsupervised targets (see Sec.~\\ref{ssec:base}).  {\\texttt{LHUC}\\xspace} adaptation progresses setting the speaker-specific amplitude parameters $r^{l,s}_j$ using gradient descent with targets provided by the adaptation data.\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=1.0\\columnwidth]{lhuc_si}\n\n\\caption{Example illustration of how {\\texttt{LHUC}\\xspace} performs adaptation (best viewed in color). Top: A ``bump'' model (eq. \\ref{eq:basis}) with two hidden units can approximate ``bump'' functions. Middle: To learn function $f_2$ given training data $f_1$ (middle), we splice two ``bump'' functions together (4 hidden units, one input/output) to learn an approximation of function $f_1$. Bottom: {\\texttt{LHUC}\\xspace} adaptation of the model optimised to $f_1$ and adapted to $f_2$ using {\\texttt{LHUC}\\xspace} scaling parameters. Image reproduced from \\cite{Swietojanski:ICASSP16}.}\n\\label{fig:lhuc_si}\n\\end{figure}\n\n The idea of directly learning hidden unit amplitudes was proposed in the context of an adaptive learning rate schedule by Trentin~\\cite{Trentin2001}, and was later applied to supervised speaker adaptation by Abdel-Hamid and Jiang~\\cite{Abdel-Hamid2013_is}. The approach was extended to unsupervised adaptation, non-sigmoid non-linearities, and large vocabulary speech recognition by Swietojanski and Renals~\\cite{Swietojanski2014_lhuc}. Other adaptive transfer function approaches for speaker adaptation have also been proposed~\\cite{Sini2013,Zhao2015_slopes}, as have ``basis'' approaches~\\cite{Wu2015, Tian2015, Decloirx2015}. However, the basis in the latter works involved re-tuning parallel models on some pre-defined clusters (gender, speaker, environment) in a supervised manner; the adaptation then relied on learning linear combination coefficients for those sub-models on adaptation data.\n\n\n\n\n\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=\\columnwidth]{lhuc_basis}\n\\caption{A 4-hidden-unit model trained to $f1 (a)$ and $f1 (b))$ for an SI approach (top) and an adapted representation (middle) keeping the resulting basis functions fixed (bottom).  (Best viewed in color.)}\n\\label{fig:lhuc_basis}\n\\end{figure}\n\n\\section{Speaker Adaptive Training LHUC (SAT-LHUC)}  \\label{sec:satlhuc}\nWhen {\\texttt{LHUC}\\xspace} is applied as a test-only adaptation it assumes that the set of speaker-independent basis functions estimated on the training data provides a good starting point for further tuning to the underlying data distribution of the adaptation data (Fig.~\\ref{fig:lhuc_si}). However, one can derive a counter-example where this assumption fails: the top plot of Fig.~\\ref{fig:lhuc_basis} shows example training data uniformly drawn from two competing distributions $f1(a)$ and $f1(b)$ where the linear recombination of the resulting basis in the average model (Fig~\\ref{fig:lhuc_basis} bottom), provides a poor approximation of adaptation data. \n \nThis motivates combining {\\texttt{LHUC}\\xspace} with speaker adaptive training (SAT)~\\cite{Anastasakos1996} in which the hidden units are trained to capture both good average representations and speaker-specific representations, by estimating speaker-specific hidden unit amplitudes for each training speaker.  This is visualised in Fig.~\\ref{fig:satlhuc_basis} where, given the prior knowledge of which data-point comes from which distribution, we estimate a set of parallel {\\texttt{LHUC}\\xspace} transforms (one per distribution) as well as one extra transform which is responsible for modelling average properties. The top of Fig.~\\ref{fig:satlhuc_basis} shows the same experiment as in Fig~\\ref{fig:lhuc_basis} but with three {\\texttt{LHUC}\\xspace} transforms -- one can see that the 4-hidden-unit MLP in this scenario was able to capture each of the underlying distributions as well as the average aspect well, given the {\\texttt{LHUC}\\xspace} transform. At the same time, the resulting basis functions (Fig~\\ref{fig:satlhuc_basis}, bottom) are a better starting point for the adaptation (Fig.~\\ref{fig:satlhuc_basis}, middle).\n\nThe examples presented in Figs.~\\ref{fig:lhuc_basis} and~\\ref{fig:satlhuc_basis} could be solved by breaking the symmetry through rebalancing the number of  training data-points for each function, resulting in less trivial and hence more adaptable basis functions in the average model. However, as we will show experimentally later,  similar effects are also present in high-dimensional speech data, and  {\\texttt{SAT-LHUC}\\xspace} training allows more tunable canonical acoustic models to be built, that can be better tailored to particular speakers through adaptation.\n\n\n\\begin{figure}[t]\n\\center\n  \\includegraphics[width=0.9\\columnwidth]{satlhuc_basis}\n\\caption{Learned solutions using three different {\\texttt{SAT-LHUC}\\xspace} transforms  and shared basis functions: {\\texttt{LHUC}\\xspace}-0 learns to provide a good average fit to both distributions $f1 (a)$ and $f1 (b))$ at the same time, while {\\texttt{LHUC}\\xspace}-a and {\\texttt{LHUC}\\xspace}-b are tasked to fit either $f1 (a)$ or $f1 (b)$, respectively. The bottom  plot shows the resulting basis functions (activations of 4 hidden units) of the {\\texttt{SAT-LHUC}\\xspace} training approach - one can observe {\\texttt{SAT-LHUC}\\xspace} provides a richer set of basis function which can fit  the data well on average, and can also capture some underlying characteristics necessary to reconstruct target training data -- using different {\\texttt{LHUC}\\xspace} transforms, this property is also visualised in the middle plot. \n\n(Best viewed in color.)}\n\\label{fig:satlhuc_basis}\n\\end{figure}\n\nFor {\\texttt{SAT-LHUC}\\xspace}, test-only adaptation remains the same as for {\\texttt{LHUC}\\xspace}, that is, the set of speaker-dependent {\\texttt{LHUC}\\xspace} parameters $\\theta^s_{LHUC}=\\{\\{r^{l,s}_j\\}_{j=1}^{d^l_{{\\mathbf{h}}}}\\}_{l=1}^L$ is inserted for each of test speakers and their values optimised from unsupervised adaptation data. We also use a set of {\\texttt{LHUC}\\xspace} transforms $\\theta^s_{LHUC}$, where $s=1\\ldots S$,  for the training speakers  which are jointly optimised with the speaker-independent parameters $\\theta_{SI}=\\{{\\mathbf{W}}^l, {\\mathbf{b}}^l\\}_{l=1}^L$. There is an additional speaker-independent LHUC transform, denoted by $\\theta^0_{LHUC}$, which allows the model to be used in speaker-independent fashion, for example, to produce first pass adaptation targets.\n\nTo perform SAT training with {\\texttt{LHUC}\\xspace}, we use the negative log likelihood and maximise the posterior probability of obtaining the correct context-dependent tied-state $c_t$ given observation vector ${\\mathbf{x}}_t$ at time $t$:\n\n", "index": 7, "text": "\\begin{align}\n  {\\mathcal{L}}_{SAT}&({\\mathbf{\\theta}_{SI}},{\\mathbf{\\theta}_{SD}}) = -\\sum_{t \\in D} \\log P(c_t|{\\mathbf x}^{s}_t;{\\mathbf{\\theta}_{SI}};{\\mathbf{\\theta}^{m_t}_{LHUC}})\n  \\label{eq:sat_lhuc}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{L}}_{SAT}\" display=\"inline\"><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle({\\mathbf{\\theta}_{SI}},{\\mathbf{\\theta}_{SD}})=-\\sum_{t\\in D}%&#10;\\log P(c_{t}|{\\mathbf{x}}^{s}_{t};{\\mathbf{\\theta}_{SI}};{\\mathbf{\\theta}^{m_{%&#10;t}}_{LHUC}})\" display=\"inline\"><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mo>,</mo><msub><mi>\u03b8</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>D</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mo>-</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>\u2208</mo><mi>D</mi></mrow></munder></mstyle><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msubsup><mi>\ud835\udc31</mi><mi>t</mi><mi>s</mi></msubsup><mo>;</mo><msub><mi>\u03b8</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>I</mi></mrow></msub><mo>;</mo><msubsup><mi>\u03b8</mi><mrow><mi>L</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><mi>C</mi></mrow><msub><mi>m</mi><mi>t</mi></msub></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nwhere $\\gamma$ is a hyper-parameter specifying the probability the given example is treated as SI. The SI/SD split (determined by equations~\\eqref{eq:binomial1} and~\\eqref{eq:binomial}) can be performed at speaker, utterance or frame level. We further investigate this aspect in section ~\\ref{ssec:sat}. The {\\texttt{SAT-LHUC}\\xspace} model structure is depicted in Fig~\\ref{fig:sat}; notice the alternative routes of forward and backward passes for different speakers.\n\n\\begin{figure}\n\\center\n\n\\includegraphics[width=\\columnwidth]{lhuc-pic2}\n\\caption{Schematic of SAT-LHUC training, with a data point from speaker $s=1$. Dashed line indicates an alternative route through the SI LHUC transform.}\n\\label{fig:sat}\n\\end{figure}\n\nDenote by $\\partial {\\mathcal{L}}_{SAT} / \\partial h^{l,s}_j$ the error back-propagated to the $j$th unit at the $l$th layer (eq.~\\eqref{eq:lhuc}). To back propagate through the transform one needs to element-wise multiply it by the transform itself, as follows:\n\n", "itemtype": "equation", "pos": 22084, "prevtext": "\nwhere $s$ denotes the $s$th speaker, $m_t\\in\\{0,s\\}$ selects the SI or SD {\\texttt{LHUC}\\xspace} transforms from $\\theta_{SD}\\in\\{\\theta^0_{LHUC},\\ldots,\\theta^S_{LHUC}\\}$ based on a Bernoulli distribution:\n\n", "index": 9, "text": "\\begin{align}\nk_t &\\sim\\ \\mbox{Bernoulli}(\\gamma) \\label{eq:binomial1}\\\\\nm_t &=\n\\begin{cases}\n    s       & \\quad \\text{if } k_t = 0 \\\\\n    0       & \\quad \\text{if } k_t = 1 \\\\\n\\end{cases}\n\\label{eq:binomial}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle k_{t}\" display=\"inline\"><msub><mi>k</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sim\\ \\mbox{Bernoulli}(\\gamma)\" display=\"inline\"><mrow><mi/><mo rspace=\"7.5pt\">\u223c</mo><mrow><mtext>Bernoulli</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b3</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m_{t}\" display=\"inline\"><msub><mi>m</mi><mi>t</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\begin{cases}s&amp;\\quad\\text{if }k_{t}=0\\\\&#10;0&amp;\\quad\\text{if }k_{t}=1\\\\&#10;\\end{cases}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mi>s</mi></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mtext>if\u00a0</mtext></mpadded><mo>\u2062</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded lspace=\"10pt\" width=\"+10pt\"><mtext>if\u00a0</mtext></mpadded><mo>\u2062</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nTo obtain the gradient with respect to $r^{l,s}_j$:\n\n", "itemtype": "equation", "pos": 23296, "prevtext": "\nwhere $\\gamma$ is a hyper-parameter specifying the probability the given example is treated as SI. The SI/SD split (determined by equations~\\eqref{eq:binomial1} and~\\eqref{eq:binomial}) can be performed at speaker, utterance or frame level. We further investigate this aspect in section ~\\ref{ssec:sat}. The {\\texttt{SAT-LHUC}\\xspace} model structure is depicted in Fig~\\ref{fig:sat}; notice the alternative routes of forward and backward passes for different speakers.\n\n\\begin{figure}\n\\center\n\n\\includegraphics[width=\\columnwidth]{lhuc-pic2}\n\\caption{Schematic of SAT-LHUC training, with a data point from speaker $s=1$. Dashed line indicates an alternative route through the SI LHUC transform.}\n\\label{fig:sat}\n\\end{figure}\n\nDenote by $\\partial {\\mathcal{L}}_{SAT} / \\partial h^{l,s}_j$ the error back-propagated to the $j$th unit at the $l$th layer (eq.~\\eqref{eq:lhuc}). To back propagate through the transform one needs to element-wise multiply it by the transform itself, as follows:\n\n", "index": 11, "text": "\\begin{equation}\n   \\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial \\psi^l_j} = \\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_j} \\circ \\xi(r^{l,s}_j) \\, .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{\\mathcal{L}}_{SAT}}{\\partial\\psi^{l}_{j}}=\\frac{\\partial{%&#10;\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_{j}}\\circ\\xi(r^{l,s}_{j})\\,.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>\u03c8</mi><mi>j</mi><mi>l</mi></msubsup></mrow></mfrac><mo>=</mo><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>\u2218</mo><mi>\u03be</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nWhen performing mini-batch SAT training one needs to explicitly take account of the fact that different data-points may flow through different transforms: hence the resulting gradient for $r^{l,s}_j$ for the $s$th speaker is the sum of the partial gradients belonging to speaker $s$:\n\n", "itemtype": "equation", "pos": 23522, "prevtext": "\nTo obtain the gradient with respect to $r^{l,s}_j$:\n\n", "index": 13, "text": "\\begin{equation}\n   \\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial r^{l,s}_j} = \\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_j} \\circ \\frac{\\partial \\xi(r^{l,s}_j)}{\\partial r^{l,s}_j} \\circ \\psi^l_j \\, .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{\\mathcal{L}}_{SAT}}{\\partial r^{l,s}_{j}}=\\frac{\\partial{%&#10;\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_{j}}\\circ\\frac{\\partial\\xi(r^{l,s}_{j})}{%&#10;\\partial r^{l,s}_{j}}\\circ\\psi^{l}_{j}\\,.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>=</mo><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>\u2218</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03be</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>\u2218</mo><mpadded width=\"+1.7pt\"><msubsup><mi>\u03c8</mi><mi>j</mi><mi>l</mi></msubsup></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": "\nor 0 in case no data-points for $s$th speaker in the given mini-batch were selected. All adaptation methods studied in this paper require first-pass decoding to obtain adaptation targets to either estimate fMLLR transforms for unseen speakers or to perform DNN speaker-dependent parameter update. \n\n\\section{Experimental setups}  \\label{sec:setups}\n\nWe experimentally investigated {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} using four different corpora: the TED talks corpus~\\cite{Cettolo2012} following the IWSLT evaluation protocol (\\url{www.iwslt.org}); the Switchboard corpus of conversational telephone speech~\\cite{godfrey1992switchboard} (\\url{ldc.upenn.edu}); the AMI meetings corpus \\cite{Carletta_LRE2007, Renals_ASRU2007} (\\url{corpus.amiproject.org}); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments~\\cite{aurora4} (\\url{catalog.elra.info}). Unless explicitly stated otherwise, the models share similar structure across the tasks -- DNNs with 6 hidden layers (2,048 units in each) using a sigmoid non-linearity.  The output logistic regression layer models the distribution of context-dependent clustered tied states~\\cite{Dahl2012}. The features are presented in 11 ($\\pm5$) frame long context windows. All the adaptation experiments, unless explicitly stated otherwise, were performed unsupervised.\n\nBelow, we briefly describe each of the above corpora and its specific experimental configurations. The collective summary of adaptation-related statistics for each corpora are given in Table~\\ref{tab:corpora}.  Note that we adapt to the headset or the side of a conversation, rather than the actual speaker (unless stated otherwise). As a result, the actual number of clusters (or estimated transforms) during training may differ from the number of physical speakers in the data.\n\n\\begin{table}[t]\n\\small\n\n\\caption{Corpus statistics related to SAT and adaptation. In parentheses we give the actual number of speakers.}\n\\label{tab:corpora}\n\\centerline{\n\\begin{tabular}{l|c|c||c|c}\n& \\multicolumn{2}{c||}{Training} & \\multicolumn{2}{c}{Test}\\\\ \\cline{2-5}\nCorpora & \\#Clusters & Time (h) & \\#Clusters & Time (h)\\\\ \\hline \\hline\nAurora4 & 83 (83) & 15 & 8 (8) &  9 \\\\\nAMI & 547 (155) & 80 & 135 (36) & 18\\\\\nTED & 788 (788) & 143 & 39 (39) & 6 \\\\\nSWBD & 4804 (4000) & 283 & 80 (80) & 4\\\\\n\\hline\\hline\n\\end{tabular}}\n\\end{table}\n\n\\textbf{TED}: We carried out experiments using a corpus of publicity available TED talks (\\url{www.ted.com})  following the IWSLT ASR evaluation protocol~\\cite{Federico2012_iwslt} (\\url{iwslt.org}). The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe~\\cite{Swietojanski2013}. In this work however, compared to our previous works~\\cite{Swietojanski2013, Swietojanski2014_lhuc, Swietojanski:ICASSP15}, our systems employ more accurate language models developed for our IWSLT--2014 systems~\\cite{Bell2014}: in particular, the final reported results use a 4-gram language model estimated from 751~million words. The baseline TED acoustic models are trained on unadapted PLP features with first and second order time derivatives. We present results on four predefined IWSLT test sets:  {\\texttt{dev2010}\\xspace}, {\\texttt{tst2010}\\xspace}, {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace} containing 8, 11, 8 and 28 ten-minute talks respectively. We use {\\texttt{tst2010}\\xspace} and/or {\\texttt{tst2013}\\xspace} to perform more detailed analyses. A collective summary of results on all TED test-sets is reported in Sec.~\\ref{ssec:summary}. \n\n\\textbf{AMI}: We follow the Kaldi GMM recipe described in~\\cite{Swietojanski_ASRU13} and use acoustics from either Individual Headset Microphone (IHM) or Single Distant Microphone (SDM). On this corpus, in addition to cepstral features, we also train a separate set of models using 40 mel-filter-bank (FBANK) features for which fMLLR transforms cannot be easily obtained, so {\\texttt{LHUC}\\xspace} offers an interesting adaptation alternative. For AMI, we also evaluate the effectiveness of {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} on AMs that utilise convolutional layers \\cite{Lecun1998, Abdel-Hamid2012, Sainath2013_cnns}, where the CNN networks were trained as described in~\\cite{Swietojanski:SPL14} but with 300 convolutional filters. We decode with a pruned 3-gram language model estimated from 800k words of AMI training transcripts interpolated with an LM trained on Fisher conversational telephone speech transcripts (1M words).\n\n\\textbf{Switchboard}: We use the Kaldi GMM recipe~\\cite{Vesely:IS13, Kaldi:ASRU11}, using Switchboard--1 Release 2 (LDC97S62). Our baseline unadapted acoustic models were trained on LDA/MLLT features. The results are reported on the full Hub5 \u00e2\u0080\u009900 set (LDC2002S09) to which we will refer as {\\texttt{eval2000}\\xspace}. The {\\texttt{eval2000}\\xspace} contains two types of data,  Switchboard (SWBD) -- which is better matched to the training data -- and CallHome English (CHE). Our reported results use 3-gram LMs estimated from Switchboard and Fisher data.\n\n\\textbf{Aurora4}: The Aurora 4 task is a small scale, medium vocabulary noise and channel ASR robustness task based on the Wall Street Journal corpus \\cite{aurora4}. We train our ASR models using the multi-condition training set. One half of the training utterances were recorded using a primary Sennheiser microphone, and the other half was collected using one of 18 other secondary microphones. The multi-condition set contains noisy utterances corrupted with one of six different noise types (airport, babble, car, restaurant, street traffic and train station) at 10-20 dB SNR. The standard Aurora 4 test set (\\texttt{eval92}) consists of 330 utterances, which are used in 14 test conditions (4620 utterances in total). The same six noise types used during training are used to create noisy test utterances with SNRs ranging from 5-15dB SNR, resulting in a total of 14 test sets. These test sets are commonly grouped into 4 subsets -- clean (group A, 1 test case), noisy (group B, 6 test cases), clean with channel distortion (group C, 1 test case) and noisy with channel distortion (group D, 6 test cases). We decode with the standard task's bigram LM.\n \n\n\\section{Results} \\label{sec:results}\n\n\\subsection{LHUC hyperparameters} \\label{ssec:base}\n\nOur initial study concerned the hyper-parameters used with {\\texttt{LHUC}\\xspace} adaptation. First, we used the TED talks to investigate how the word error rate (WER) is affected by adapting different layers in the model using {\\texttt{LHUC}\\xspace} transforms.  The results, graphed in Fig.~\\ref{fig:training_stats} (a), indicated that adapting only the bottom layer brings the largest drop in WER; however, adapting more layers further improves the accuracy for both {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} approaches (adapting the other way round -- starting from the top layer -- is much less effective \\cite{Swietojanski2014_lhuc}). Since obtaining the gradients for the $r$ parameters at each layer is inexpensive compared to the overall back-propagation, and we want to adapt at least the bottom layer, we apply {\\texttt{LHUC}\\xspace} to each layer for the rest of this work.\n\nFig.~\\ref{fig:training_stats} (b) shows WERs for the number of adaptation iterations. The results indicate that one sweep over the adaptation data (in this case {\\texttt{tst2010}\\xspace}) is sufficient and, more importantly, the model does not overfit when adapting with more iterations (despite the adaptation objective consistently improving -- Fig.~\\ref{fig:training_stats} (c)). This suggests that it is not necessary to carefully regularise the model -- for example, by Kullback-Leibler divergence training \\cite{Yu2013} which is usually required when adapting the weights of one or more layers in a network.\n\nFinally, we explored how the form of the {\\texttt{LHUC}\\xspace} re-parametrisation function $\\xi$ affects the WER and frame error rate (FER) (Fig.~\\ref{fig:training_stats} (c) and Table~\\ref{tab:lhucreparm}). For test-only adaptation only a small WER  difference (0.1\\% absolute) is observed, regardless of the large difference in frame accuracies. This supports our previous observation that {\\texttt{LHUC}\\xspace} is robust against over-fitting. For {\\texttt{SAT-LHUC}\\xspace} training, a less constrained parametrisation was found to give better WERs for the SI model. Based on our control experiments, during {\\texttt{SAT-LHUC}\\xspace} training, setting $\\xi$  to be the identity function (linear $r$) gave similar results to  $\\xi(r)=\\max(0,r)$ and $\\xi(r)=\\exp(r)$ and all were better than re-parametrising with $\\xi(r)=2/(1+\\exp(-r))$. This is expected as for full training the last approach constrains the range of back-propagated gradients. From now on, if not stated otherwise, we will use $\\xi(r)=\\exp(r)$ in the remainder of this paper.\n\nWe adapt our all models with the learning rate set to $0.8$ (regardless of $\\xi(\\cdot)$) and the basic training of both the SI and the {\\texttt{SAT-LHUC}\\xspace} models was performed with the initial learning rate set to $0.08$ and was later adjusted according to the \\texttt{newbob} learning scheme \\cite{Renals1992}.\n\n\\begin{figure*}[t]\n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_layers}\n}\n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_iters}\n} \n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_reparm}\n} \n\\vspace{-3mm}\n\\caption{WER(\\%) on TED {\\texttt{tst2010}\\xspace} as a function of: a) number of adapted layers; and b) number of adaptation iterations; c) FER for re-parameterisation functions ($\\xi$) used in adaptation.}\n\\label{fig:training_stats}\n\\end{figure*}\n\n\\begin{table}[t]\n\\small\n\n\\caption{\\label{tab:lhucreparm} WER(\\%) for different re-parametrisation functions for {\\texttt{LHUC}\\xspace} transforms on TED {\\texttt{tst2010}\\xspace}. Unadapted baseline WER is 15.0\\%.}\n\\centerline{\n\\begin{tabular}{c|c|c|c}\n\n$r$ & $2/(1+\\exp(-r))$ & $\\exp(r)$ & $\\max(0, r)$ \\\\ \n\\hline \\hline\n 12.8 & 12.8 & 12.7 & 12.7  \\\\ \n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection {SAT-LHUC} \\label{ssec:sat}\n\nAs described in section~\\ref{sec:satlhuc}, {\\texttt{SAT-LHUC}\\xspace} training aims to regularise the hidden unit feature receptors so that they capture not just the average characteristics of training data, but also specific features of the different distributions the data was drawn from (for example, different training speakers). As a result, the model can be better tailored to unseen speakers by putting more importance to those units that were useful for training speakers with similar characteristics.\n\nPrior to {\\texttt{SAT-LHUC}\\xspace} training we need to decide on how and which data should be used to estimate speaker-dependent and speaker-independent transforms. In this work we train {\\texttt{SAT-LHUC}\\xspace} models with frame-level~\\cite{Swietojanski:ICASSP16}, segment-level and speaker-level clusters. For speaker- and segment-level transforms we decide which speakers or segments are going to be treated as SI or SD prior to training. For the frame-level {\\texttt{SAT-LHUC}\\xspace} approach, the SI/SD decisions are made separately for each data-point during training. In either scenario we ensure that the overall SD/SI ratio determined by $\\gamma$ parameter is satisfied. The WER results for each of these three approaches ($\\gamma=0.5$) are reported in Table~\\ref{tab:satlevels}. Speaker-level {\\texttt{SAT-LHUC}\\xspace} training provides the highest WERs for both SI and SD decodes. Segment-level and frame-level {\\texttt{SAT-LHUC}\\xspace} training result in similar WERs for SI decodes, with a small advantage (0.1\\% abs.) for the frame-level approach after adaptation.\n\nFig.~\\ref{fig:ted_as_p2} gives more insight on how the ratio of SI and SD data (determined by $\\gamma$) affects the WER of the first-pass and adapted systems on TED {\\texttt{tst2013}\\xspace}\\footnote{These results are compatible with further {\\texttt{SAT-LHUC}\\xspace} results using this {\\texttt{tst2013}\\xspace} set in \\cite{Swietojanski:ICASSP16}}. The SI/SD split mainly affects the first pass accuracies with a substantial increase in SI WER when less than 30\\% of the data is used to estimate the SI {\\texttt{LHUC}\\xspace} transforms. However, once adapted, all variants obtained lower WERs compared to the baseline SI and {\\texttt{LHUC}\\xspace} adapted model. For instance, when $\\gamma=0.5$ the {\\texttt{SAT-LHUC}\\xspace} systems operating in SI mode obtained similar accuracies to the baseline SI model (22\\%WER); however, the adapted {\\texttt{SAT-LHUC}\\xspace} model gave around 1\\% absolute (6\\% relative) decrease in WER compared with the SI baseline test-only adapted {\\texttt{LHUC}\\xspace} model. The adaptation results for speaker-level {\\texttt{SAT-LHUC}\\xspace} training were worse by around 0.4\\% absolute compared to segment- or frame-level {\\texttt{SAT-LHUC}\\xspace} training.  However, the difference, as shown experimentally in~\\cite{Swietojanski:ICASSP16}, is mostly due to  poorer quality  adaptation targets resulting from the corresponding first pass {\\texttt{SAT-LHUC}\\xspace} systems rather than the differences in learned representations.  Managing a good trade-off between SI and SD ratios for {\\texttt{SAT-LHUC}\\xspace} is nevertheless an important aspect to take into account, and in our experience using around 50--60\\% of data for the SI transform is a good task-independent setting. If different models for SI and SD decodes are acceptable, then further small gains in accuracy are observed~\\cite{Swietojanski:ICASSP16}.\n\n\\begin{table}[t]\n\\small\n\n\\caption{\\label{sat_strategies} WER(\\%) for different sampling strategies and {\\texttt{SAT-LHUC}\\xspace} training (TED {\\texttt{tst2013}\\xspace})}\n\\label{tab:satlevels}\n\\centerline{\n\\begin{tabular}{c|c||c|c|c}\n\n& & \\multicolumn{3}{c}{WER (\\%) for sampling strategies} \\\\ \\cline{3-5}\nModel & Baseline & Per Speaker & Per Segment & Per Frame \\\\ \n\\hline \\hline\nSI & 22.1 & 23.0 & 22.0 & 22.0  \\\\ \nSD & 19.1 & 18.6 & 18.1 & 18.0  \\\\ \n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.0\\columnwidth]{ted_as_p2}\n\\caption{WER(\\%) for different sampling strategies \\{per frame, per segment, per speaker\\} \nfor SAT-LHUC training and SI and SD decodes on TED {\\texttt{tst2013}\\xspace}.}\n\\label{fig:ted_as_p2}\n\\end{figure}\n\nWe report the baseline {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} comparisons on TED and AMI data in Tables ~\\ref{tab:tedwers1} and ~\\ref{tab:amiwers1}, respectively (further results, including a comparison to fMLLR transforms and on Switchboard data are in the next sections).  On TED (Table~\\ref{tab:tedwers1}), {\\texttt{SAT-LHUC}\\xspace} models operating in SI mode ($\\gamma=0.6$) have comparable WERs to SI models; however, adaptation resulted in a WER reduction of  0.3--1.1\\% absolute (2--6\\% relative) compared to test-only adaptation of the SI models. Similar results were observed on the AMI data (Table~\\ref{tab:amiwers1}) where for both DNN and CNN models trained on FBANK features {\\texttt{LHUC}\\xspace} adaptation decreased the WER by 2\\% absolute (7\\% relative) and {\\texttt{SAT-LHUC}\\xspace} training improved this result by 4\\% relative for DNN models. As expected, the {\\texttt{SAT-LHUC}\\xspace} gain for CNNs was smaller when compared to DNN models, since the CNN layer can learn different patterns for different speakers which may be selected through the max-pooling operator at run-time.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on TED talks ({\\texttt{tst2010}\\xspace} and {\\texttt{tst2013}\\xspace}).}\n\\label{tab:tedwers1}\n\\centerline{\n\\begin{tabular}{c|c||c|c}\n  \\multicolumn{2}{ c|| }{System}    & \\multicolumn{2}{c}{IWSLT Test set} \\\\ \\hline\n  Training & Decoding   &  {\\texttt{tst2010}\\xspace} & {\\texttt{tst2013}\\xspace}  \\\\ \n\\hline \\hline\n\\multicolumn{4}{ l }{Baseline speaker-independent systems} \\\\ \\hline\nSI  & SI\t     & 15.0 & 22.1  \\\\  \nSAT-LHUC & SI    & 15.1 & 22.0  \\\\ \n\\hline\n\\multicolumn{4}{ l }{Adapted systems} \\\\ \\hline\n\nSI  & {\\texttt{LHUC}\\xspace} \t & 12.7 & 19.1 \\\\\n\n\n\n\n{\\texttt{SAT-LHUC}\\xspace} & {\\texttt{LHUC}\\xspace}  & 12.4 & 18.0 \\\\ \n\n\\hline\n\\hline\n\\end{tabular}}\n\\vspace{-0.1cm}\n\\end{table}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM}\n\\label{tab:amiwers1}\n\\centerline{\n\\begin{tabular}{l|c||c|c}\n\nModel & Features & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline\n\n\n\n\n\n\n\n\n\n\n\n\n\\hline \n\n\nDNN   \t&  \tFBANK & 26.8 \t\t& 29.1   \\\\ \n+{\\texttt{LHUC}\\xspace} \t&  \tFBANK & 25.6 \t& 27.1   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 24.9        & 26.1   \\\\ \\hline\n\n\n\nCNN  &  \tFBANK & 25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  \tFBANK & 24.3 \t\t& 25.3   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 23.9        & 24.8   \\\\\n\n\n\\hline \\hline\n\\end{tabular}}\n\\vspace{-0.3cm}\n\\end{table}\n\n\n\\subsection {Sequence model adaptation} \\label{ssec:sequence}\n\nModel-based adaptation of sequence-trained DNNs (SE-DNN) is more challenging compared to adapting networks trained using cross-entropy: a mismatched adaptation objective (here cross-entropy) can easily erase sequence information from the weight matrices due to the well-known effect of catastrophic forgetting~\\cite{French1999} in neural networks. Indeed Huang and Gong~\\cite{Huang2015} report no gain from adapting SE-DNN models with a KL divergence regularised~\\cite{Yu2013} cross-entropy adaptation objective and supervised adaptation targets. In those experiments, all weights in the model were updated and one needs to perform KL divergence regularised sequence level adaptation to further improve on top of SE-DNN. It remains to be answered if one can get similar improvements using SE-DNN adaptation and first-pass transcripts.\n\nIn this work we adapt state-level minimum Bayes risk (sMBR) \\cite{Kaiser2000, Kingsbury2009} sequence-trained models with {\\texttt{LHUC}\\xspace} approach and report  results on TED {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace} in Table~\\ref{tab:seqwers}. We kept all the {\\texttt{LHUC}\\xspace} adaptation hyper-parameters the same as for CE models and obtained around 2\\% absolute (10.9\\% relative) WER reductions on {\\texttt{tst2013}\\xspace} for both SI and fMLLR SAT adapted SE-DNN systems. Interestingly, the obtained adaptation gain was similar to the cross-entropy models and {\\texttt{LHUC}\\xspace} adaptation did not seem to disrupt the learned model's sequence representation.\n\\begin{table}[t]\n\\small\n\n\\caption{Summary of WER results of adapted sequence models on TED {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace}}\n\\label{tab:seqwers} \n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\nDNN-CE    & 12.1 & 22.1  \\\\ \\hline\nDNN-sMBR    & 10.3 & 20.2  \\\\ \n+{\\texttt{LHUC}\\xspace}      & 9.5 & 18.0 \\\\ \\hline\n+fMLLR      & 9.6 & 18.9 \\\\ \n++{\\texttt{LHUC}\\xspace} \t& 8.9 & 15.8 \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\nWe compared also our adaptation results to the most accurate system of the IWSLT--2013 TED transcription evaluation, which performed both feature- and model-space speaker adaptation~\\cite{Huang2013_iwslt}. For model-space adaptation that system used a method which adapts DNNs with a speaker-dependent layer~\\cite{Ochiai2014}. The results are reported in Table~\\ref{tab:iwsltwers} where in the first block one can see a standard sequence-trained feature-space adapted system build from TED and 150 hours of out-of-domain data scoring 15.7\\% WER, similar to the WER of our TED system (15.4\\%), which also for IWSLT utilised 100 hours of out-of-domain AMI data. The 0.3\\% difference could be explained by characteristics of the out-of-domain data used ({\\texttt{tst2013}\\xspace} is characterised by a large proportion of non-native speakers which is also typical for AMI data, hence benefits more our baseline systems).  When comparing both adaptation approaches operating in an unsupervised manner one can see that {\\texttt{LHUC}\\xspace} gives much bigger improvements in WER compared to speaker-dependent layer, 2.1\\% vs. 0.6\\% absolute (13.6\\% vs. 4.3\\% relative) on {\\texttt{tst2013}\\xspace}. This allows our single-model system to match a considerably more sophisticated post-processing pipeline~\\cite{Huang2013_iwslt}, as outlined in Table~\\ref{tab:iwsltwers}. For less mismatched data ({\\texttt{tst2011}\\xspace}) adaptation is less important and our system has a WER 0.8\\% absolute higher compared with the more sophisticated system.\n\nFrom these experiments we conclude that {\\texttt{LHUC}\\xspace} is an effective way to adapt sequence models in unsupervised manner using a cross-entropy objective function, without risk of removing learned sequence information.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WERs for adapted sequence-trained models used in IWSLT evaluation.  Note, the results are not directly comparable to those reported on TED in Table~\\ref{tab:seqwers} due different training data and feature pre-processing pipelines (see referenced papers for system details).}\n\\label{tab:iwsltwers}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\n\\multicolumn{3}{l}{IWSLT2013 winner system (numbers taken from \\cite{Huang2013_iwslt})} \\\\ \\hline\nDNN (sMBR) + HUB4 + WSJ & - & 15.7 \\\\ \n+ Six ROVER subsystems\t& - & 14.8 \\\\ \n++ Automatic segmentation& - & 14.3 \\\\\n+++ LM adapt. + RNN resc.        & - & 14.1 \\\\\n+++++ SAT on DNN \\cite{Ochiai2014}& 7.7 & 13.5 \\\\ \\hline\n\\multicolumn{3}{l}{Our system \\cite{Bell2014}} \\\\ \\hline\nDNN (sMBR) + AMI data  & 9.0 & 15.4 \\\\ \n+{\\texttt{LHUC}\\xspace} \t& 8.5 & 13.3 \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\\subsection {Other aspects of adaptation} \\label{ssec:quality}\n\n\\textbf{Amount of adaptation data}: Fig~\\ref{fig:complementarity_lhuc} shows the effect of the amount of adaptation data on WER for {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adapted models. As little as 10s of unsupervised adaptation data is already able to substantially decrease WERs (by 0.5--0.8\\% absolute). The improvement for {\\texttt{SAT-LHUC}\\xspace} adaptation  compared with {\\texttt{LHUC}\\xspace} is considerably larger -- roughly by a factor of two up to 30s adaptation data. As the duration of adaptation data increases the difference gets smaller; however {\\texttt{SAT-LHUC}\\xspace} results in consistently lower WERs than {\\texttt{LHUC}\\xspace} in all cases (including full two pass adaptation).\n\nWe also investigated supervised (oracle) adaptation by aligning the acoustics with the reference transcriptions (dashed lines). Given supervised adaptation targets,  {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} further substantially decrease WERs, with {\\texttt{SAT-LHUC}\\xspace} giving a consistent advantage over {\\texttt{LHUC}\\xspace}. \n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.\\columnwidth]{complementarity_lhuc}\n\\caption{WER(\\%) for unsupervised and oracle adaptation data on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:complementarity_lhuc}\n\\end{figure}\n\n\\textbf{Quality of adaptation targets}: Since our approach relies on a first-pass decoding, we investigated the extent to which {\\texttt{LHUC}\\xspace} is sensitive to the quality of the adaptation targets. In this experiment we explored the differences resulting from different language models, and assumed that the first pass adaptation data was generated by either an SI or a {\\texttt{SAT-LHUC}\\xspace} model operating in SI mode. The main results are shown in Fig~\\ref{fig:as_data_quality} where the solid lines show WERs obtained with a pruned 3-gram LM and different types of adaptation targets resulting from re-scoring the adaptation data with stronger LMs. One can see there is not much difference unless the adaptation data was re-scored with the largest 4-gram LM. This improvement diminishes in the final adapted system after re-scoring.  This suggests that the technique is not very sensitive to the quality of adaptation targets.  This trend holds regardless of the amount of data used for adaptation (ranging from 10s to several minutes per speaker). In related work~\\cite{Miao2015} {\\texttt{LHUC}\\xspace} was employed using alignments obtained from an SI-GMM system with a 8.1\\% absolute higher WER than the corresponding SI DNN, and substantial gains were obtained over the unadapted SI DNN baseline -- although the WER reduction was considerably smaller (1\\% absolute) compared to adaptation with alignments obtained with the corresponding SI DNN.\n\n\\textbf{Quality of data}: We also investigated how the quality of the acoustic data itself affects the adaptation accuracies, keeping the other ASR components fixed.  We performed an experiment on the AMI corpus using speech captured by individual headset microphones (IHM) and a single distant tabletop microphone (SDM). In case of IHM we adapt to the headset; in this experiment we assume we have speaker labels for the SDM data\\footnote{In a real scenario for SDM data one would have to perform speaker diarisation in order to obtain speaker labels.}. The results are reported in  Table~\\ref{tab:lhuc_ami_ihm_sdm}: {\\texttt{LHUC}\\xspace} adaptation improves the accuracy in both experiments, although the gain for the SDM condition is smaller; however, the SDM system is characterised by twice as large WERs.\n\n\\begin{figure}[t]\n  \\includegraphics[width=1.0\\columnwidth]{as_lm_sigm}\n\\caption{WER(\\%) for different qualities of adaptation targets on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:as_data_quality}\n\\end{figure}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM and AMI--SDM using adapted CNNs.}\n\\label{tab:lhuc_ami_ihm_sdm}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nCNN (IHM) & 25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  24.3 \t\t& 25.3   \\\\ \\hline\nCNN (SDM) & 49.8 \t\t& 54.4   \\\\ \n+{\\texttt{LHUC}\\xspace} &  48.8\t\t& 53.1   \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\textbf{One-shot adaptation}: By one-shot adaptation we mean the scenario in which {\\texttt{LHUC}\\xspace} transforms were estimated once for a held-out speaker and then used many times in a single pass system for this speaker.  We performed those experiments on AMI IHM data, and report results on {\\texttt{dev}\\xspace} and {\\texttt{eval}\\xspace} which contain 21 and 16 unique speakers taking part in 18 and 16 different meetings, respectively. Each speaker participates in multiple meetings: to some degree, adapting to a speaker in one meeting, then applying the adaptation transform to the same speaker in the other meetings simulates a real-life condition where it is possible to assume the speaker identity without necessity of performing speaker diarisation (e.g. personal devices).\n\n\n\n\n\nThe results of this experiment (Table~\\ref{tab:oneshot}) indicate that {\\texttt{LHUC}\\xspace} retains the accuracies of two-pass systems by providing almost identical results when comparing {\\texttt{LHUC}\\xspace} estimated in a full two-pass system and when the transforms are re-used in the {\\texttt{LHUC}\\xspace}.\\texttt{one-shot} experiment.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM and one-shot adaptation}\n\\label{tab:oneshot}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nCNN &  \t25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  \t24.3 \t\t& 25.3   \\\\ \n+{\\texttt{LHUC}\\xspace}.\\texttt{one-shot}  & 24.3 \t\t& 25.4   \\\\\n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\subsection{Complementarity to feature normalisation} \\label{ssec:complementarity}\nFeature-space adaptation using fMLLR is probably the most reliable current form of speaker adaptation, so it is of great interest to explore how complementary the proposed approaches are to SAT training with fMLLR transforms.\\footnote{Due to space constraints we do not make an explicit comparisons to other techniques such as auxiliary i-vector features or speaker-codes; however, the literature suggest that the use of i-vectors give similar \\cite{Saon2013} results when compared to fMLLR trained models. Related recent studies also show {\\texttt{LHUC}\\xspace} is at least as good as the standard use of i-vector features \\cite{Miao2015, Samarakoon:ICASSP16}.}\n\nWe compared {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} to SAT-fMLLR training using TED {\\texttt{tst2010}\\xspace} (Fig~\\ref{fig:complementarity_fmllr}, red curves).  We also compared both techniques, including a comparison in terms of the amount of data used to estimate each type of transform.  fMLLR transforms estimated on 10s of unsupervised data result in an increase in WER compared with the SI-trained baseline (16.1\\% vs. 15.0\\%).  When combined with {\\texttt{LHUC}\\xspace} or {\\texttt{SAT-LHUC}\\xspace} some of this deterioration  was recovered (similar results using  {\\texttt{LHUC}\\xspace} alone were reported in Fig~\\ref{fig:complementarity_lhuc}). For more adaptation data (30s or more) fMLLR improved the accuracies by around 1--2\\% absolute and  combination with {\\texttt{LHUC}\\xspace} (or {\\texttt{SAT-LHUC}\\xspace}) resulted in an additional 1\\% reduction in WER (see also Table~\\ref{tab:tedwers} in the next section for further results).\n\nWe also investigated (in a rather unrealistic experiment) how much mismatch in feature space one can normalise in model space with {\\texttt{LHUC}\\xspace}. To do so, we used a SAT-fMLLR trained model with unadapted PLP features which gave a large increase in WER (26\\% vs 15\\%). Then, using unsupervised adaptation targets obtained from the feature-mismatched decoding both {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} were applied. The results (also presented in Fig.~\\ref{fig:complementarity_fmllr}) indicate that a very large portion of the WER increase can be effectively compensated in model space -- more than 8\\% absolute.  As found before, test-only re-parametrisation functions ($\\exp(r)$ vs. $2/(1+\\exp(-r))$) have negligible impact on the adaptation results, and {\\texttt{SAT-LHUC}\\xspace} again provides better results.\n\n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.\\columnwidth]{complementarity_fmllr}\n\\caption{WER(\\%) for {\\texttt{LHUC}\\xspace}, {\\texttt{SAT-LHUC}\\xspace}, and SAT-fMLLR (and combinations) on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:complementarity_fmllr}\n\\end{figure}\n\n\\subsection{Adaptation Summary} \\label{ssec:summary}\n\nIn this section we summarise our results, applying {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} to TED, AMI, and Switchboard.  Table~\\ref{tab:tedwers} contains results for four IWSLT test sets ({\\texttt{dev2010}\\xspace}, {\\texttt{tst2010}\\xspace}, {\\texttt{tst2011}\\xspace}, and {\\texttt{tst2013}\\xspace}): in most scenarios {\\texttt{SAT-LHUC}\\xspace} results in a lower WER than {\\texttt{LHUC}\\xspace} and both techniques are complementary with SAT-fMLLR training.  \n\nSimilar conclusions can be drawn from experiments on AMI (Table~\\ref{tab:amiwers}) where {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} were found to effectively adapt DNN and CNN models trained on FBANK features. {\\texttt{SAT-LHUC}\\xspace} trained DNN models gave the same final results as the more complicated SAT-fMLLR+{\\texttt{LHUC}\\xspace} system.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER (\\%)  on various TED development and test sets from IWSLT12 and IWSLT13 evaluations.}\n\\label{tab:tedwers}\n\\centerline{\n\\begin{tabular}{l||c|c|c|c}\n\nModel & {\\texttt{dev2010}\\xspace} & {\\texttt{tst2010}\\xspace} & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\n\n\n\n\n\n\n\n\n\nDNN\t\t\t& 15.4 & 15.0  & 12.1 & 22.1  \\\\ \n+{\\texttt{LHUC}\\xspace}      & 14.5 & 12.8  & 10.9 & 19.1 \\\\ \n+{\\texttt{SAT-LHUC}\\xspace}   & 14.0 & 12.4  & 10.9 & 18.0 \\\\ \\hline\n+fMLLR      & 14.5 & 12.9  & 10.9 & 20.8 \\\\ \n++{\\texttt{LHUC}\\xspace}     & 14.1 & 11.8  & 10.3 & 18.4 \\\\ \n++{\\texttt{SAT-LHUC}\\xspace}   & 13.7   & 11.6  & 9.9 & 17.6 \\\\ \n\n\n\n\n\n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM}\n\\label{tab:amiwers}\n\\centerline{\n\\begin{tabular}{l|c||c|c}\n\nModel & Features & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nDNN  \t&  \tFMLLR & 26.2 \t\t& 27.3   \\\\ \n+{\\texttt{LHUC}\\xspace}  \t&  \tFMLLR & 25.6 \t\t& 26.2   \\\\\n\\hline \nDNN   \t&  \tFBANK & 26.8 \t\t& 29.1   \\\\ \n+{\\texttt{LHUC}\\xspace} \t&  \tFBANK & 25.6 \t& 27.1   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 24.9        & 26.1   \\\\ \\hline\n\n\n\n\\hline\n\\end{tabular}}\n\\end{table}\n\nOn Switchboard, in contrast to other corpora, we observed that test-only LHUC does not match the WERs obtained from SAT-fMLLR models (Table~\\ref{tab:swbd_wers}).  The SI system has a WER o 21.7\\% compared with 20.7\\% for the test-only LHUC and 20.2\\% for the SAT-fMLLR system.  The improvement obtained using test-only LHUC is comparable to that obtained with other test-only adaptation techniques, e.g. feature-space discriminative linear regression (fDLR) \\cite{Seide2011}, but neither of these matches  SAT trained feature transform models.  This could be due to the fact Switchboard data is narrow-band and as such contains less information for discrimination between speakers~\\cite{Wester_IS2015}, especially when estimating relevant statistics from small amounts of unsupervised adaptation data.  Another potential reason could be related to the fact that the Switchboard part of ${\\texttt{eval2000}\\xspace}$ is characterised by a large overlap between training and test speakers -- 36 out of 40 test speakers are observed in training \\cite{fiscus2000}, which limits the need for adaptation, but also enables models to learn much more accurate speaker-characteristics during supervised speaker adaptive training.\n\nAdaptation using {\\texttt{SAT-LHUC}\\xspace} (20.3\\% WER) almost matches SAT-fMLLR (20.2\\%). We also observe that {\\texttt{LHUC}\\xspace} performs relatively better under more mismatched conditions (the Callhome (CHE) subset of {\\texttt{eval2000}\\xspace}), similar to what we observed on TED. \n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on Switchboard Hub5'00.}\n\\label{tab:swbd_wers}\n\\centerline{\n\\begin{tabular}{l||c|c|c}\n\n      & \\multicolumn{3}{c}{Hub5'00} \\\\ \\cline{2-4}\nModel & SWB & CHE & TOTAL \\\\ \n\\hline \\hline\n\nDNN \t& 15.2 \t & 28.2 \t\t&  21.7 \\\\\n\n+{\\texttt{LHUC}\\xspace}  \t&  \t14.7 & 26.6    & 20.7 \\\\ \n++{\\texttt{SAT-LHUC}\\xspace}  \t&  \t14.6 &  25.9   & 20.3 \\\\ \\hline\n\n+fMLLR  \t  & 14.2 & 26.2 \t& 20.2  \\\\\n++{\\texttt{LHUC}\\xspace} \t &  14.2 & 25.6 & 19.9  \\\\\n++{\\texttt{SAT-LHUC}\\xspace}   & 14.1 & 25.6  & 19.9 \\\\\n\n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\nFinally, in Fig~\\ref{fig:summary} we show the WERs obtained for  200 speakers across the TED, AMI, and SWBD test sets.  We observe that for 89\\% of speakers {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adaptation reduced the WER, and that {\\texttt{SAT-LHUC}\\xspace} gives a consistent reduction over {\\texttt{LHUC}\\xspace}.\n\n\\begin{figure*}[ht!]\n\\center\n  \\includegraphics[width=1.0\\textwidth]{summary}\n\\caption{Summary of WERs(\\%) obtained with {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adaptation techniques on test speakers of TED, SWBD and AMI corpora (results are sorted in descending WER order for the SI system). For {\\texttt{LHUC}\\xspace} the average observed improvement per speaker was at 1.6\\% absolute (7.0\\% relative). The same statistic for {\\texttt{SAT-LHUC}\\xspace} was at 2.3\\% absolute (9.7\\% relative). The maximum observed WER decrease per speaker was 11.4\\% absolute (32.7\\% relative) and 16.0\\% absolute (50\\% relative) for {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace}, respectively. WERs decreased for 89\\% of speakers using {\\texttt{LHUC}\\xspace} adaptation.}\n\\label{fig:summary}\n\\end{figure*}\n\n\\section{LHUC for Factorisation} \\label{sec:factorisation}\n\n\n\n\n\n\n\nWe have applied {\\texttt{LHUC}\\xspace} to adapt to both the speaker and the acoustic environment.  If multi-condition data is available for a speaker, then it is possible to define a set of joint speaker-environment {\\texttt{LHUC}\\xspace} transforms.  Alternatively, we can estimate two set of transforms -- for speaker ${\\mathbf r}_S$ and for environment ${\\mathbf r}_E$ -- and then linearly interpolate them to derive a combined transform ${\\mathbf {\\hat{r}}}_{SE}$ as follows:\n\n", "itemtype": "equation", "pos": 24032, "prevtext": "\nWhen performing mini-batch SAT training one needs to explicitly take account of the fact that different data-points may flow through different transforms: hence the resulting gradient for $r^{l,s}_j$ for the $s$th speaker is the sum of the partial gradients belonging to speaker $s$:\n\n", "index": 15, "text": "\\begin{equation}\n   \\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial r^{l,s}_j} = \\sum_{t,m_t=s}\\frac{\\partial {\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_j}\\circ\\frac{\\partial \\xi(r^{l,s}_j)}{\\partial r^{l,s}_j} \\circ \\psi^l_j \\, ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial{\\mathcal{L}}_{SAT}}{\\partial r^{l,s}_{j}}=\\sum_{t,m_{t}=s}\\frac%&#10;{\\partial{\\mathcal{L}}_{SAT}}{\\partial h^{l,s}_{j}}\\circ\\frac{\\partial\\xi(r^{l%&#10;,s}_{j})}{\\partial r^{l,s}_{j}}\\circ\\psi^{l}_{j}\\,,\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>t</mi><mo>,</mo><msub><mi>m</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>s</mi></mrow></munder><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>S</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>T</mi></mrow></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>h</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>\u2218</mo><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03be</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>r</mi><mi>j</mi><mrow><mi>l</mi><mo>,</mo><mi>s</mi></mrow></msubsup></mrow></mfrac><mo>\u2218</mo><mpadded width=\"+1.7pt\"><msubsup><mi>\u03c8</mi><mi>j</mi><mi>l</mi></msubsup></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02828.tex", "nexttext": " \nThe idea is similar to {\\texttt{LHUC}\\xspace} applied to channel normalisation between distant and close talking scenarios~\\cite{himawan2015towards}, except we use two independently estimated transforms.\n\nWe adapted baseline multi-condition trained DNN models~\\cite{Seltzer2013}  to the speaker (${\\mathbf r}_S$) and the environment (${\\mathbf r}_E$).  The ${\\mathbf r}_S$ transforms were estimated only on \\emph{clean} speech; similarly the environment transforms were estimated for each scenario (one set of ${\\mathbf r}_E$ per scenario) using multiple speakers (hence, we have 7 different environmental transforms).  To avoid learning joint speaker-environment transforms the target speaker's data was removed from environment adaptation material (e.g. when estimating transforms for the \\texttt{restaurant} environment, we use all restaurant data except the one for the target speaker). \n\nThe results (Table~\\ref{tab:mdnn}) show that both standalone speaker or environment adaptation {\\texttt{LHUC}\\xspace} adaptation improve over an unadapted system (13.1\\%($S$) and 13.3\\%($E$) vs. 13.9\\%) but, as expected, a single transform estimated jointly on the target speaker and environment has a lower WER (12.4\\%). However, when interpolated with $\\alpha=0.7$ the result of the factorised model improves to 12.7\\% WER, although still higher that joint estimation.  However, adaptation data for joint speaker-environment adaptation is not available in many scenarios, and the factorised adaptation based on interpolation is more flexible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table} \n\\caption{Results on Aurora 4. Multi-condition DNN model.}\n\\label{tab:mdnn}\n\\center{\n\\begin{tabular}{ l | c | c | c | c | c }\n  Model & A & B & C & D & AVG \\\\ \\hline \\hline\n  DNN \t\t& 5.1 & 9.3 & 9.3 & 20.8 & 13.9\\\\\n  DNN + ${\\mathbf r}_S$ & 4.3 & 9.3 & 6.9 & 19.3 & 13.1 \\\\\n  DNN + ${\\mathbf r}_E$ & 5.0     & 9.0   & 8.5   & 19.8  & 13.3 \\\\\n  DNN + ${\\mathbf r}_{SE\\;JOINT}$  & 4.5  & 8.6   & 7.4     & 18.3  & 12.4 \\\\ \\hline\n  DNN + ${\\mathbf {\\hat{r}}}_{SE}$, $\\alpha=0.5$ & 4.6  & 8.9   & 7.7    & 19.1  & 12.9 \\\\\n  \n  DNN +  ${\\mathbf {\\hat{r}}}_{SE}$, $\\alpha=0.7$ & 4.5     & 8.8   & 7.2   & 18.9  & {\\textbf{{12.7}}} \\\\ \n  \n  \\hline \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe also train more competitive models following Rennie et al~\\cite{Rennie2014}:  Maxout~\\cite{Goodfellow2013} CNN models were trained using annealed dropout. However, in contrast to \\cite{Rennie2014}, in this work we are use alignments obtained by aligning a corresponding multi-condition model as ground-truth labels, rather than replicating clean alignments to multi-condition data: this is likely to explain differences in the reported baselines (10.9\\% compared with 10.5\\% in \\cite{Rennie2014}). The results for the joint optimisation are reported in  Table~\\ref{tab:maxnn} where one can notice large improvements with unsupervised {\\texttt{LHUC}\\xspace} adaptation.\n\n\\begin{table}\n\\caption{Results on Aurora 4. Multi-condition Maxout-CNN model, with and without annealed dropout (AD).}\n\\label{tab:maxnn}\n\\center{\n\\begin{tabular}{ l | c | c | c | c | c }\n  Model & A & B & C & D & AVG \\\\ \\hline \\hline\n  MaxCNN & 4.2     & 7.7   & 7.9   & 17.4  & 11.6 \\\\ \n  MaxCNN +  ${\\mathbf r}_{SE\\;JOINT}$ & 3.7 & 6.3   & 5.4   & 14.3  & 9.5 \\\\ \\hline\n  AD MaxCNN & 4.3     & 7.7   & 7.2   & 15.6  & 10.9 \\\\ \n  AD MaxCNN +  ${\\mathbf r}_{SE\\;JOINT}$ & 3.4 & 5.7   & 6.1   & 13.4  & 8.6 \\\\ \\hline \\hline\n\\end{tabular}\n}\n\\vspace{-0.1cm}\n\\end{table}\n\nFinally, we  visualise the top hidden layer activations of the annealed dropout Maxout CNN using stochastic neighbourhood embedding (tSNE)~\\cite{Maaten2008_tsne} for one utterance recorded under clean and noisy (restaurant) conditions (Fig.~\\ref{fig:tsnea4}). \n\n\\begin{figure}[t]\n\\centering\n\\subfigure[]{\n  \\includegraphics[width=0.68\\columnwidth]{tsne_lhuc_all_clean3}\n}\n\\vspace{-3mm}\n \\subfigure[]{\n   \\includegraphics[width=0.68\\columnwidth]{tsne_lhuc_all_noisy3}\n } \n\\caption{tSNE plots (best viewed in color) of the top hidden layer before and after adaptation for an utterance recorded in (a) clean and (b) noisy (restaurant) environment, using the annealed dropout maxout CNN. The model can normalise the phonetic space between conditions (brown color), keeping two different spaces for non-speech frames (blue color) under clean and noisy conditions. The effect of {\\texttt{LHUC}\\xspace} is mostly visible for non-speech frames.}\n\\label{fig:tsnea4}\n\\vspace{-0.3cm}\n\\end{figure}\n\n\\section{Conclusions}\nWe have presented the {\\texttt{LHUC}\\xspace} approach to unsupervised adaptation of neural network acoustic models in both test-only ({\\texttt{LHUC}\\xspace}) and SAT  ({\\texttt{SAT-LHUC}\\xspace}) frameworks, evaluating them using four standard speech recognition corpora: TED talks as used in the IWSLT evaluations, AMI, Switchboard, and Aurora4.  Our experimental results indicate that both {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} can provide significant improvements in WER (5--23\\% relative depending on test set and task).  {\\texttt{LHUC}\\xspace} adaptation works well unsupervised and with small amounts of data (as little as 10s),  is complementary to feature space normalisation transforms such as SAT-fMLLR, and can be used for unsupervised adaptation of sequence-trained DNN acoustic models using a cross-entropy adaptation objective function.  Furthermore we have demonstrated that it can be applied in a factorised way, estimating and interpolating separate transforms for adaptation to the acoustic environment and speaker.\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\\bibliographystyle{IEEEbib}\n\\bibliography{master}\n\n\\begin{IEEEbiography}{Pawel Swietojanski}\nBiography text here.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}{Jinyu Li}\nBiography text here.\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}{Steve Renals}\nBiography text here.\n\\end{IEEEbiography}\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nor 0 in case no data-points for $s$th speaker in the given mini-batch were selected. All adaptation methods studied in this paper require first-pass decoding to obtain adaptation targets to either estimate fMLLR transforms for unseen speakers or to perform DNN speaker-dependent parameter update. \n\n\\section{Experimental setups}  \\label{sec:setups}\n\nWe experimentally investigated {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} using four different corpora: the TED talks corpus~\\cite{Cettolo2012} following the IWSLT evaluation protocol (\\url{www.iwslt.org}); the Switchboard corpus of conversational telephone speech~\\cite{godfrey1992switchboard} (\\url{ldc.upenn.edu}); the AMI meetings corpus \\cite{Carletta_LRE2007, Renals_ASRU2007} (\\url{corpus.amiproject.org}); and the Aurora4 corpus of read speech with artificially corrupted acoustic environments~\\cite{aurora4} (\\url{catalog.elra.info}). Unless explicitly stated otherwise, the models share similar structure across the tasks -- DNNs with 6 hidden layers (2,048 units in each) using a sigmoid non-linearity.  The output logistic regression layer models the distribution of context-dependent clustered tied states~\\cite{Dahl2012}. The features are presented in 11 ($\\pm5$) frame long context windows. All the adaptation experiments, unless explicitly stated otherwise, were performed unsupervised.\n\nBelow, we briefly describe each of the above corpora and its specific experimental configurations. The collective summary of adaptation-related statistics for each corpora are given in Table~\\ref{tab:corpora}.  Note that we adapt to the headset or the side of a conversation, rather than the actual speaker (unless stated otherwise). As a result, the actual number of clusters (or estimated transforms) during training may differ from the number of physical speakers in the data.\n\n\\begin{table}[t]\n\\small\n\n\\caption{Corpus statistics related to SAT and adaptation. In parentheses we give the actual number of speakers.}\n\\label{tab:corpora}\n\\centerline{\n\\begin{tabular}{l|c|c||c|c}\n& \\multicolumn{2}{c||}{Training} & \\multicolumn{2}{c}{Test}\\\\ \\cline{2-5}\nCorpora & \\#Clusters & Time (h) & \\#Clusters & Time (h)\\\\ \\hline \\hline\nAurora4 & 83 (83) & 15 & 8 (8) &  9 \\\\\nAMI & 547 (155) & 80 & 135 (36) & 18\\\\\nTED & 788 (788) & 143 & 39 (39) & 6 \\\\\nSWBD & 4804 (4000) & 283 & 80 (80) & 4\\\\\n\\hline\\hline\n\\end{tabular}}\n\\end{table}\n\n\\textbf{TED}: We carried out experiments using a corpus of publicity available TED talks (\\url{www.ted.com})  following the IWSLT ASR evaluation protocol~\\cite{Federico2012_iwslt} (\\url{iwslt.org}). The training data consisted of 143 hours of speech (813 talks) and the systems follow our previously described recipe~\\cite{Swietojanski2013}. In this work however, compared to our previous works~\\cite{Swietojanski2013, Swietojanski2014_lhuc, Swietojanski:ICASSP15}, our systems employ more accurate language models developed for our IWSLT--2014 systems~\\cite{Bell2014}: in particular, the final reported results use a 4-gram language model estimated from 751~million words. The baseline TED acoustic models are trained on unadapted PLP features with first and second order time derivatives. We present results on four predefined IWSLT test sets:  {\\texttt{dev2010}\\xspace}, {\\texttt{tst2010}\\xspace}, {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace} containing 8, 11, 8 and 28 ten-minute talks respectively. We use {\\texttt{tst2010}\\xspace} and/or {\\texttt{tst2013}\\xspace} to perform more detailed analyses. A collective summary of results on all TED test-sets is reported in Sec.~\\ref{ssec:summary}. \n\n\\textbf{AMI}: We follow the Kaldi GMM recipe described in~\\cite{Swietojanski_ASRU13} and use acoustics from either Individual Headset Microphone (IHM) or Single Distant Microphone (SDM). On this corpus, in addition to cepstral features, we also train a separate set of models using 40 mel-filter-bank (FBANK) features for which fMLLR transforms cannot be easily obtained, so {\\texttt{LHUC}\\xspace} offers an interesting adaptation alternative. For AMI, we also evaluate the effectiveness of {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} on AMs that utilise convolutional layers \\cite{Lecun1998, Abdel-Hamid2012, Sainath2013_cnns}, where the CNN networks were trained as described in~\\cite{Swietojanski:SPL14} but with 300 convolutional filters. We decode with a pruned 3-gram language model estimated from 800k words of AMI training transcripts interpolated with an LM trained on Fisher conversational telephone speech transcripts (1M words).\n\n\\textbf{Switchboard}: We use the Kaldi GMM recipe~\\cite{Vesely:IS13, Kaldi:ASRU11}, using Switchboard--1 Release 2 (LDC97S62). Our baseline unadapted acoustic models were trained on LDA/MLLT features. The results are reported on the full Hub5 \u00e2\u0080\u009900 set (LDC2002S09) to which we will refer as {\\texttt{eval2000}\\xspace}. The {\\texttt{eval2000}\\xspace} contains two types of data,  Switchboard (SWBD) -- which is better matched to the training data -- and CallHome English (CHE). Our reported results use 3-gram LMs estimated from Switchboard and Fisher data.\n\n\\textbf{Aurora4}: The Aurora 4 task is a small scale, medium vocabulary noise and channel ASR robustness task based on the Wall Street Journal corpus \\cite{aurora4}. We train our ASR models using the multi-condition training set. One half of the training utterances were recorded using a primary Sennheiser microphone, and the other half was collected using one of 18 other secondary microphones. The multi-condition set contains noisy utterances corrupted with one of six different noise types (airport, babble, car, restaurant, street traffic and train station) at 10-20 dB SNR. The standard Aurora 4 test set (\\texttt{eval92}) consists of 330 utterances, which are used in 14 test conditions (4620 utterances in total). The same six noise types used during training are used to create noisy test utterances with SNRs ranging from 5-15dB SNR, resulting in a total of 14 test sets. These test sets are commonly grouped into 4 subsets -- clean (group A, 1 test case), noisy (group B, 6 test cases), clean with channel distortion (group C, 1 test case) and noisy with channel distortion (group D, 6 test cases). We decode with the standard task's bigram LM.\n \n\n\\section{Results} \\label{sec:results}\n\n\\subsection{LHUC hyperparameters} \\label{ssec:base}\n\nOur initial study concerned the hyper-parameters used with {\\texttt{LHUC}\\xspace} adaptation. First, we used the TED talks to investigate how the word error rate (WER) is affected by adapting different layers in the model using {\\texttt{LHUC}\\xspace} transforms.  The results, graphed in Fig.~\\ref{fig:training_stats} (a), indicated that adapting only the bottom layer brings the largest drop in WER; however, adapting more layers further improves the accuracy for both {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} approaches (adapting the other way round -- starting from the top layer -- is much less effective \\cite{Swietojanski2014_lhuc}). Since obtaining the gradients for the $r$ parameters at each layer is inexpensive compared to the overall back-propagation, and we want to adapt at least the bottom layer, we apply {\\texttt{LHUC}\\xspace} to each layer for the rest of this work.\n\nFig.~\\ref{fig:training_stats} (b) shows WERs for the number of adaptation iterations. The results indicate that one sweep over the adaptation data (in this case {\\texttt{tst2010}\\xspace}) is sufficient and, more importantly, the model does not overfit when adapting with more iterations (despite the adaptation objective consistently improving -- Fig.~\\ref{fig:training_stats} (c)). This suggests that it is not necessary to carefully regularise the model -- for example, by Kullback-Leibler divergence training \\cite{Yu2013} which is usually required when adapting the weights of one or more layers in a network.\n\nFinally, we explored how the form of the {\\texttt{LHUC}\\xspace} re-parametrisation function $\\xi$ affects the WER and frame error rate (FER) (Fig.~\\ref{fig:training_stats} (c) and Table~\\ref{tab:lhucreparm}). For test-only adaptation only a small WER  difference (0.1\\% absolute) is observed, regardless of the large difference in frame accuracies. This supports our previous observation that {\\texttt{LHUC}\\xspace} is robust against over-fitting. For {\\texttt{SAT-LHUC}\\xspace} training, a less constrained parametrisation was found to give better WERs for the SI model. Based on our control experiments, during {\\texttt{SAT-LHUC}\\xspace} training, setting $\\xi$  to be the identity function (linear $r$) gave similar results to  $\\xi(r)=\\max(0,r)$ and $\\xi(r)=\\exp(r)$ and all were better than re-parametrising with $\\xi(r)=2/(1+\\exp(-r))$. This is expected as for full training the last approach constrains the range of back-propagated gradients. From now on, if not stated otherwise, we will use $\\xi(r)=\\exp(r)$ in the remainder of this paper.\n\nWe adapt our all models with the learning rate set to $0.8$ (regardless of $\\xi(\\cdot)$) and the basic training of both the SI and the {\\texttt{SAT-LHUC}\\xspace} models was performed with the initial learning rate set to $0.08$ and was later adjusted according to the \\texttt{newbob} learning scheme \\cite{Renals1992}.\n\n\\begin{figure*}[t]\n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_layers}\n}\n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_iters}\n} \n\\subfigure[]{\n  \\includegraphics[width=0.33\\textwidth]{ted_reparm}\n} \n\\vspace{-3mm}\n\\caption{WER(\\%) on TED {\\texttt{tst2010}\\xspace} as a function of: a) number of adapted layers; and b) number of adaptation iterations; c) FER for re-parameterisation functions ($\\xi$) used in adaptation.}\n\\label{fig:training_stats}\n\\end{figure*}\n\n\\begin{table}[t]\n\\small\n\n\\caption{\\label{tab:lhucreparm} WER(\\%) for different re-parametrisation functions for {\\texttt{LHUC}\\xspace} transforms on TED {\\texttt{tst2010}\\xspace}. Unadapted baseline WER is 15.0\\%.}\n\\centerline{\n\\begin{tabular}{c|c|c|c}\n\n$r$ & $2/(1+\\exp(-r))$ & $\\exp(r)$ & $\\max(0, r)$ \\\\ \n\\hline \\hline\n 12.8 & 12.8 & 12.7 & 12.7  \\\\ \n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection {SAT-LHUC} \\label{ssec:sat}\n\nAs described in section~\\ref{sec:satlhuc}, {\\texttt{SAT-LHUC}\\xspace} training aims to regularise the hidden unit feature receptors so that they capture not just the average characteristics of training data, but also specific features of the different distributions the data was drawn from (for example, different training speakers). As a result, the model can be better tailored to unseen speakers by putting more importance to those units that were useful for training speakers with similar characteristics.\n\nPrior to {\\texttt{SAT-LHUC}\\xspace} training we need to decide on how and which data should be used to estimate speaker-dependent and speaker-independent transforms. In this work we train {\\texttt{SAT-LHUC}\\xspace} models with frame-level~\\cite{Swietojanski:ICASSP16}, segment-level and speaker-level clusters. For speaker- and segment-level transforms we decide which speakers or segments are going to be treated as SI or SD prior to training. For the frame-level {\\texttt{SAT-LHUC}\\xspace} approach, the SI/SD decisions are made separately for each data-point during training. In either scenario we ensure that the overall SD/SI ratio determined by $\\gamma$ parameter is satisfied. The WER results for each of these three approaches ($\\gamma=0.5$) are reported in Table~\\ref{tab:satlevels}. Speaker-level {\\texttt{SAT-LHUC}\\xspace} training provides the highest WERs for both SI and SD decodes. Segment-level and frame-level {\\texttt{SAT-LHUC}\\xspace} training result in similar WERs for SI decodes, with a small advantage (0.1\\% abs.) for the frame-level approach after adaptation.\n\nFig.~\\ref{fig:ted_as_p2} gives more insight on how the ratio of SI and SD data (determined by $\\gamma$) affects the WER of the first-pass and adapted systems on TED {\\texttt{tst2013}\\xspace}\\footnote{These results are compatible with further {\\texttt{SAT-LHUC}\\xspace} results using this {\\texttt{tst2013}\\xspace} set in \\cite{Swietojanski:ICASSP16}}. The SI/SD split mainly affects the first pass accuracies with a substantial increase in SI WER when less than 30\\% of the data is used to estimate the SI {\\texttt{LHUC}\\xspace} transforms. However, once adapted, all variants obtained lower WERs compared to the baseline SI and {\\texttt{LHUC}\\xspace} adapted model. For instance, when $\\gamma=0.5$ the {\\texttt{SAT-LHUC}\\xspace} systems operating in SI mode obtained similar accuracies to the baseline SI model (22\\%WER); however, the adapted {\\texttt{SAT-LHUC}\\xspace} model gave around 1\\% absolute (6\\% relative) decrease in WER compared with the SI baseline test-only adapted {\\texttt{LHUC}\\xspace} model. The adaptation results for speaker-level {\\texttt{SAT-LHUC}\\xspace} training were worse by around 0.4\\% absolute compared to segment- or frame-level {\\texttt{SAT-LHUC}\\xspace} training.  However, the difference, as shown experimentally in~\\cite{Swietojanski:ICASSP16}, is mostly due to  poorer quality  adaptation targets resulting from the corresponding first pass {\\texttt{SAT-LHUC}\\xspace} systems rather than the differences in learned representations.  Managing a good trade-off between SI and SD ratios for {\\texttt{SAT-LHUC}\\xspace} is nevertheless an important aspect to take into account, and in our experience using around 50--60\\% of data for the SI transform is a good task-independent setting. If different models for SI and SD decodes are acceptable, then further small gains in accuracy are observed~\\cite{Swietojanski:ICASSP16}.\n\n\\begin{table}[t]\n\\small\n\n\\caption{\\label{sat_strategies} WER(\\%) for different sampling strategies and {\\texttt{SAT-LHUC}\\xspace} training (TED {\\texttt{tst2013}\\xspace})}\n\\label{tab:satlevels}\n\\centerline{\n\\begin{tabular}{c|c||c|c|c}\n\n& & \\multicolumn{3}{c}{WER (\\%) for sampling strategies} \\\\ \\cline{3-5}\nModel & Baseline & Per Speaker & Per Segment & Per Frame \\\\ \n\\hline \\hline\nSI & 22.1 & 23.0 & 22.0 & 22.0  \\\\ \nSD & 19.1 & 18.6 & 18.1 & 18.0  \\\\ \n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.0\\columnwidth]{ted_as_p2}\n\\caption{WER(\\%) for different sampling strategies \\{per frame, per segment, per speaker\\} \nfor SAT-LHUC training and SI and SD decodes on TED {\\texttt{tst2013}\\xspace}.}\n\\label{fig:ted_as_p2}\n\\end{figure}\n\nWe report the baseline {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} comparisons on TED and AMI data in Tables ~\\ref{tab:tedwers1} and ~\\ref{tab:amiwers1}, respectively (further results, including a comparison to fMLLR transforms and on Switchboard data are in the next sections).  On TED (Table~\\ref{tab:tedwers1}), {\\texttt{SAT-LHUC}\\xspace} models operating in SI mode ($\\gamma=0.6$) have comparable WERs to SI models; however, adaptation resulted in a WER reduction of  0.3--1.1\\% absolute (2--6\\% relative) compared to test-only adaptation of the SI models. Similar results were observed on the AMI data (Table~\\ref{tab:amiwers1}) where for both DNN and CNN models trained on FBANK features {\\texttt{LHUC}\\xspace} adaptation decreased the WER by 2\\% absolute (7\\% relative) and {\\texttt{SAT-LHUC}\\xspace} training improved this result by 4\\% relative for DNN models. As expected, the {\\texttt{SAT-LHUC}\\xspace} gain for CNNs was smaller when compared to DNN models, since the CNN layer can learn different patterns for different speakers which may be selected through the max-pooling operator at run-time.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on TED talks ({\\texttt{tst2010}\\xspace} and {\\texttt{tst2013}\\xspace}).}\n\\label{tab:tedwers1}\n\\centerline{\n\\begin{tabular}{c|c||c|c}\n  \\multicolumn{2}{ c|| }{System}    & \\multicolumn{2}{c}{IWSLT Test set} \\\\ \\hline\n  Training & Decoding   &  {\\texttt{tst2010}\\xspace} & {\\texttt{tst2013}\\xspace}  \\\\ \n\\hline \\hline\n\\multicolumn{4}{ l }{Baseline speaker-independent systems} \\\\ \\hline\nSI  & SI\t     & 15.0 & 22.1  \\\\  \nSAT-LHUC & SI    & 15.1 & 22.0  \\\\ \n\\hline\n\\multicolumn{4}{ l }{Adapted systems} \\\\ \\hline\n\nSI  & {\\texttt{LHUC}\\xspace} \t & 12.7 & 19.1 \\\\\n\n\n\n\n{\\texttt{SAT-LHUC}\\xspace} & {\\texttt{LHUC}\\xspace}  & 12.4 & 18.0 \\\\ \n\n\\hline\n\\hline\n\\end{tabular}}\n\\vspace{-0.1cm}\n\\end{table}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM}\n\\label{tab:amiwers1}\n\\centerline{\n\\begin{tabular}{l|c||c|c}\n\nModel & Features & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline\n\n\n\n\n\n\n\n\n\n\n\n\n\\hline \n\n\nDNN   \t&  \tFBANK & 26.8 \t\t& 29.1   \\\\ \n+{\\texttt{LHUC}\\xspace} \t&  \tFBANK & 25.6 \t& 27.1   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 24.9        & 26.1   \\\\ \\hline\n\n\n\nCNN  &  \tFBANK & 25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  \tFBANK & 24.3 \t\t& 25.3   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 23.9        & 24.8   \\\\\n\n\n\\hline \\hline\n\\end{tabular}}\n\\vspace{-0.3cm}\n\\end{table}\n\n\n\\subsection {Sequence model adaptation} \\label{ssec:sequence}\n\nModel-based adaptation of sequence-trained DNNs (SE-DNN) is more challenging compared to adapting networks trained using cross-entropy: a mismatched adaptation objective (here cross-entropy) can easily erase sequence information from the weight matrices due to the well-known effect of catastrophic forgetting~\\cite{French1999} in neural networks. Indeed Huang and Gong~\\cite{Huang2015} report no gain from adapting SE-DNN models with a KL divergence regularised~\\cite{Yu2013} cross-entropy adaptation objective and supervised adaptation targets. In those experiments, all weights in the model were updated and one needs to perform KL divergence regularised sequence level adaptation to further improve on top of SE-DNN. It remains to be answered if one can get similar improvements using SE-DNN adaptation and first-pass transcripts.\n\nIn this work we adapt state-level minimum Bayes risk (sMBR) \\cite{Kaiser2000, Kingsbury2009} sequence-trained models with {\\texttt{LHUC}\\xspace} approach and report  results on TED {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace} in Table~\\ref{tab:seqwers}. We kept all the {\\texttt{LHUC}\\xspace} adaptation hyper-parameters the same as for CE models and obtained around 2\\% absolute (10.9\\% relative) WER reductions on {\\texttt{tst2013}\\xspace} for both SI and fMLLR SAT adapted SE-DNN systems. Interestingly, the obtained adaptation gain was similar to the cross-entropy models and {\\texttt{LHUC}\\xspace} adaptation did not seem to disrupt the learned model's sequence representation.\n\\begin{table}[t]\n\\small\n\n\\caption{Summary of WER results of adapted sequence models on TED {\\texttt{tst2011}\\xspace} and {\\texttt{tst2013}\\xspace}}\n\\label{tab:seqwers} \n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\nDNN-CE    & 12.1 & 22.1  \\\\ \\hline\nDNN-sMBR    & 10.3 & 20.2  \\\\ \n+{\\texttt{LHUC}\\xspace}      & 9.5 & 18.0 \\\\ \\hline\n+fMLLR      & 9.6 & 18.9 \\\\ \n++{\\texttt{LHUC}\\xspace} \t& 8.9 & 15.8 \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\nWe compared also our adaptation results to the most accurate system of the IWSLT--2013 TED transcription evaluation, which performed both feature- and model-space speaker adaptation~\\cite{Huang2013_iwslt}. For model-space adaptation that system used a method which adapts DNNs with a speaker-dependent layer~\\cite{Ochiai2014}. The results are reported in Table~\\ref{tab:iwsltwers} where in the first block one can see a standard sequence-trained feature-space adapted system build from TED and 150 hours of out-of-domain data scoring 15.7\\% WER, similar to the WER of our TED system (15.4\\%), which also for IWSLT utilised 100 hours of out-of-domain AMI data. The 0.3\\% difference could be explained by characteristics of the out-of-domain data used ({\\texttt{tst2013}\\xspace} is characterised by a large proportion of non-native speakers which is also typical for AMI data, hence benefits more our baseline systems).  When comparing both adaptation approaches operating in an unsupervised manner one can see that {\\texttt{LHUC}\\xspace} gives much bigger improvements in WER compared to speaker-dependent layer, 2.1\\% vs. 0.6\\% absolute (13.6\\% vs. 4.3\\% relative) on {\\texttt{tst2013}\\xspace}. This allows our single-model system to match a considerably more sophisticated post-processing pipeline~\\cite{Huang2013_iwslt}, as outlined in Table~\\ref{tab:iwsltwers}. For less mismatched data ({\\texttt{tst2011}\\xspace}) adaptation is less important and our system has a WER 0.8\\% absolute higher compared with the more sophisticated system.\n\nFrom these experiments we conclude that {\\texttt{LHUC}\\xspace} is an effective way to adapt sequence models in unsupervised manner using a cross-entropy objective function, without risk of removing learned sequence information.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WERs for adapted sequence-trained models used in IWSLT evaluation.  Note, the results are not directly comparable to those reported on TED in Table~\\ref{tab:seqwers} due different training data and feature pre-processing pipelines (see referenced papers for system details).}\n\\label{tab:iwsltwers}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\n\\multicolumn{3}{l}{IWSLT2013 winner system (numbers taken from \\cite{Huang2013_iwslt})} \\\\ \\hline\nDNN (sMBR) + HUB4 + WSJ & - & 15.7 \\\\ \n+ Six ROVER subsystems\t& - & 14.8 \\\\ \n++ Automatic segmentation& - & 14.3 \\\\\n+++ LM adapt. + RNN resc.        & - & 14.1 \\\\\n+++++ SAT on DNN \\cite{Ochiai2014}& 7.7 & 13.5 \\\\ \\hline\n\\multicolumn{3}{l}{Our system \\cite{Bell2014}} \\\\ \\hline\nDNN (sMBR) + AMI data  & 9.0 & 15.4 \\\\ \n+{\\texttt{LHUC}\\xspace} \t& 8.5 & 13.3 \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\n\n\n\n\n\n\\subsection {Other aspects of adaptation} \\label{ssec:quality}\n\n\\textbf{Amount of adaptation data}: Fig~\\ref{fig:complementarity_lhuc} shows the effect of the amount of adaptation data on WER for {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adapted models. As little as 10s of unsupervised adaptation data is already able to substantially decrease WERs (by 0.5--0.8\\% absolute). The improvement for {\\texttt{SAT-LHUC}\\xspace} adaptation  compared with {\\texttt{LHUC}\\xspace} is considerably larger -- roughly by a factor of two up to 30s adaptation data. As the duration of adaptation data increases the difference gets smaller; however {\\texttt{SAT-LHUC}\\xspace} results in consistently lower WERs than {\\texttt{LHUC}\\xspace} in all cases (including full two pass adaptation).\n\nWe also investigated supervised (oracle) adaptation by aligning the acoustics with the reference transcriptions (dashed lines). Given supervised adaptation targets,  {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} further substantially decrease WERs, with {\\texttt{SAT-LHUC}\\xspace} giving a consistent advantage over {\\texttt{LHUC}\\xspace}. \n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.\\columnwidth]{complementarity_lhuc}\n\\caption{WER(\\%) for unsupervised and oracle adaptation data on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:complementarity_lhuc}\n\\end{figure}\n\n\\textbf{Quality of adaptation targets}: Since our approach relies on a first-pass decoding, we investigated the extent to which {\\texttt{LHUC}\\xspace} is sensitive to the quality of the adaptation targets. In this experiment we explored the differences resulting from different language models, and assumed that the first pass adaptation data was generated by either an SI or a {\\texttt{SAT-LHUC}\\xspace} model operating in SI mode. The main results are shown in Fig~\\ref{fig:as_data_quality} where the solid lines show WERs obtained with a pruned 3-gram LM and different types of adaptation targets resulting from re-scoring the adaptation data with stronger LMs. One can see there is not much difference unless the adaptation data was re-scored with the largest 4-gram LM. This improvement diminishes in the final adapted system after re-scoring.  This suggests that the technique is not very sensitive to the quality of adaptation targets.  This trend holds regardless of the amount of data used for adaptation (ranging from 10s to several minutes per speaker). In related work~\\cite{Miao2015} {\\texttt{LHUC}\\xspace} was employed using alignments obtained from an SI-GMM system with a 8.1\\% absolute higher WER than the corresponding SI DNN, and substantial gains were obtained over the unadapted SI DNN baseline -- although the WER reduction was considerably smaller (1\\% absolute) compared to adaptation with alignments obtained with the corresponding SI DNN.\n\n\\textbf{Quality of data}: We also investigated how the quality of the acoustic data itself affects the adaptation accuracies, keeping the other ASR components fixed.  We performed an experiment on the AMI corpus using speech captured by individual headset microphones (IHM) and a single distant tabletop microphone (SDM). In case of IHM we adapt to the headset; in this experiment we assume we have speaker labels for the SDM data\\footnote{In a real scenario for SDM data one would have to perform speaker diarisation in order to obtain speaker labels.}. The results are reported in  Table~\\ref{tab:lhuc_ami_ihm_sdm}: {\\texttt{LHUC}\\xspace} adaptation improves the accuracy in both experiments, although the gain for the SDM condition is smaller; however, the SDM system is characterised by twice as large WERs.\n\n\\begin{figure}[t]\n  \\includegraphics[width=1.0\\columnwidth]{as_lm_sigm}\n\\caption{WER(\\%) for different qualities of adaptation targets on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:as_data_quality}\n\\end{figure}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM and AMI--SDM using adapted CNNs.}\n\\label{tab:lhuc_ami_ihm_sdm}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nCNN (IHM) & 25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  24.3 \t\t& 25.3   \\\\ \\hline\nCNN (SDM) & 49.8 \t\t& 54.4   \\\\ \n+{\\texttt{LHUC}\\xspace} &  48.8\t\t& 53.1   \\\\ \n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\textbf{One-shot adaptation}: By one-shot adaptation we mean the scenario in which {\\texttt{LHUC}\\xspace} transforms were estimated once for a held-out speaker and then used many times in a single pass system for this speaker.  We performed those experiments on AMI IHM data, and report results on {\\texttt{dev}\\xspace} and {\\texttt{eval}\\xspace} which contain 21 and 16 unique speakers taking part in 18 and 16 different meetings, respectively. Each speaker participates in multiple meetings: to some degree, adapting to a speaker in one meeting, then applying the adaptation transform to the same speaker in the other meetings simulates a real-life condition where it is possible to assume the speaker identity without necessity of performing speaker diarisation (e.g. personal devices).\n\n\n\n\n\nThe results of this experiment (Table~\\ref{tab:oneshot}) indicate that {\\texttt{LHUC}\\xspace} retains the accuracies of two-pass systems by providing almost identical results when comparing {\\texttt{LHUC}\\xspace} estimated in a full two-pass system and when the transforms are re-used in the {\\texttt{LHUC}\\xspace}.\\texttt{one-shot} experiment.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM and one-shot adaptation}\n\\label{tab:oneshot}\n\\centerline{\n\\begin{tabular}{l||c|c}\n\nModel & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nCNN &  \t25.2 \t\t& 27.1   \\\\ \n+{\\texttt{LHUC}\\xspace} &  \t24.3 \t\t& 25.3   \\\\ \n+{\\texttt{LHUC}\\xspace}.\\texttt{one-shot}  & 24.3 \t\t& 25.4   \\\\\n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\subsection{Complementarity to feature normalisation} \\label{ssec:complementarity}\nFeature-space adaptation using fMLLR is probably the most reliable current form of speaker adaptation, so it is of great interest to explore how complementary the proposed approaches are to SAT training with fMLLR transforms.\\footnote{Due to space constraints we do not make an explicit comparisons to other techniques such as auxiliary i-vector features or speaker-codes; however, the literature suggest that the use of i-vectors give similar \\cite{Saon2013} results when compared to fMLLR trained models. Related recent studies also show {\\texttt{LHUC}\\xspace} is at least as good as the standard use of i-vector features \\cite{Miao2015, Samarakoon:ICASSP16}.}\n\nWe compared {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} to SAT-fMLLR training using TED {\\texttt{tst2010}\\xspace} (Fig~\\ref{fig:complementarity_fmllr}, red curves).  We also compared both techniques, including a comparison in terms of the amount of data used to estimate each type of transform.  fMLLR transforms estimated on 10s of unsupervised data result in an increase in WER compared with the SI-trained baseline (16.1\\% vs. 15.0\\%).  When combined with {\\texttt{LHUC}\\xspace} or {\\texttt{SAT-LHUC}\\xspace} some of this deterioration  was recovered (similar results using  {\\texttt{LHUC}\\xspace} alone were reported in Fig~\\ref{fig:complementarity_lhuc}). For more adaptation data (30s or more) fMLLR improved the accuracies by around 1--2\\% absolute and  combination with {\\texttt{LHUC}\\xspace} (or {\\texttt{SAT-LHUC}\\xspace}) resulted in an additional 1\\% reduction in WER (see also Table~\\ref{tab:tedwers} in the next section for further results).\n\nWe also investigated (in a rather unrealistic experiment) how much mismatch in feature space one can normalise in model space with {\\texttt{LHUC}\\xspace}. To do so, we used a SAT-fMLLR trained model with unadapted PLP features which gave a large increase in WER (26\\% vs 15\\%). Then, using unsupervised adaptation targets obtained from the feature-mismatched decoding both {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} were applied. The results (also presented in Fig.~\\ref{fig:complementarity_fmllr}) indicate that a very large portion of the WER increase can be effectively compensated in model space -- more than 8\\% absolute.  As found before, test-only re-parametrisation functions ($\\exp(r)$ vs. $2/(1+\\exp(-r))$) have negligible impact on the adaptation results, and {\\texttt{SAT-LHUC}\\xspace} again provides better results.\n\n\n\\begin{figure}[ht!]\n\\center\n  \\includegraphics[width=1.\\columnwidth]{complementarity_fmllr}\n\\caption{WER(\\%) for {\\texttt{LHUC}\\xspace}, {\\texttt{SAT-LHUC}\\xspace}, and SAT-fMLLR (and combinations) on TED {\\texttt{tst2010}\\xspace}.}\n\\label{fig:complementarity_fmllr}\n\\end{figure}\n\n\\subsection{Adaptation Summary} \\label{ssec:summary}\n\nIn this section we summarise our results, applying {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} to TED, AMI, and Switchboard.  Table~\\ref{tab:tedwers} contains results for four IWSLT test sets ({\\texttt{dev2010}\\xspace}, {\\texttt{tst2010}\\xspace}, {\\texttt{tst2011}\\xspace}, and {\\texttt{tst2013}\\xspace}): in most scenarios {\\texttt{SAT-LHUC}\\xspace} results in a lower WER than {\\texttt{LHUC}\\xspace} and both techniques are complementary with SAT-fMLLR training.  \n\nSimilar conclusions can be drawn from experiments on AMI (Table~\\ref{tab:amiwers}) where {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} were found to effectively adapt DNN and CNN models trained on FBANK features. {\\texttt{SAT-LHUC}\\xspace} trained DNN models gave the same final results as the more complicated SAT-fMLLR+{\\texttt{LHUC}\\xspace} system.\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER (\\%)  on various TED development and test sets from IWSLT12 and IWSLT13 evaluations.}\n\\label{tab:tedwers}\n\\centerline{\n\\begin{tabular}{l||c|c|c|c}\n\nModel & {\\texttt{dev2010}\\xspace} & {\\texttt{tst2010}\\xspace} & {\\texttt{tst2011}\\xspace} & {\\texttt{tst2013}\\xspace} \\\\ \n\\hline \\hline\n\n\n\n\n\n\n\n\n\nDNN\t\t\t& 15.4 & 15.0  & 12.1 & 22.1  \\\\ \n+{\\texttt{LHUC}\\xspace}      & 14.5 & 12.8  & 10.9 & 19.1 \\\\ \n+{\\texttt{SAT-LHUC}\\xspace}   & 14.0 & 12.4  & 10.9 & 18.0 \\\\ \\hline\n+fMLLR      & 14.5 & 12.9  & 10.9 & 20.8 \\\\ \n++{\\texttt{LHUC}\\xspace}     & 14.1 & 11.8  & 10.3 & 18.4 \\\\ \n++{\\texttt{SAT-LHUC}\\xspace}   & 13.7   & 11.6  & 9.9 & 17.6 \\\\ \n\n\n\n\n\n\\hline \\hline\n\\end{tabular}}\n\\end{table}\n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on AMI--IHM}\n\\label{tab:amiwers}\n\\centerline{\n\\begin{tabular}{l|c||c|c}\n\nModel & Features & {\\texttt{dev}\\xspace} & {\\texttt{eval}\\xspace} \\\\ \n\\hline \\hline\nDNN  \t&  \tFMLLR & 26.2 \t\t& 27.3   \\\\ \n+{\\texttt{LHUC}\\xspace}  \t&  \tFMLLR & 25.6 \t\t& 26.2   \\\\\n\\hline \nDNN   \t&  \tFBANK & 26.8 \t\t& 29.1   \\\\ \n+{\\texttt{LHUC}\\xspace} \t&  \tFBANK & 25.6 \t& 27.1   \\\\ \n+{\\texttt{SAT-LHUC}\\xspace} & FBANK & 24.9        & 26.1   \\\\ \\hline\n\n\n\n\\hline\n\\end{tabular}}\n\\end{table}\n\nOn Switchboard, in contrast to other corpora, we observed that test-only LHUC does not match the WERs obtained from SAT-fMLLR models (Table~\\ref{tab:swbd_wers}).  The SI system has a WER o 21.7\\% compared with 20.7\\% for the test-only LHUC and 20.2\\% for the SAT-fMLLR system.  The improvement obtained using test-only LHUC is comparable to that obtained with other test-only adaptation techniques, e.g. feature-space discriminative linear regression (fDLR) \\cite{Seide2011}, but neither of these matches  SAT trained feature transform models.  This could be due to the fact Switchboard data is narrow-band and as such contains less information for discrimination between speakers~\\cite{Wester_IS2015}, especially when estimating relevant statistics from small amounts of unsupervised adaptation data.  Another potential reason could be related to the fact that the Switchboard part of ${\\texttt{eval2000}\\xspace}$ is characterised by a large overlap between training and test speakers -- 36 out of 40 test speakers are observed in training \\cite{fiscus2000}, which limits the need for adaptation, but also enables models to learn much more accurate speaker-characteristics during supervised speaker adaptive training.\n\nAdaptation using {\\texttt{SAT-LHUC}\\xspace} (20.3\\% WER) almost matches SAT-fMLLR (20.2\\%). We also observe that {\\texttt{LHUC}\\xspace} performs relatively better under more mismatched conditions (the Callhome (CHE) subset of {\\texttt{eval2000}\\xspace}), similar to what we observed on TED. \n\n\\begin{table}[t]\n\\small\n\n\\caption{WER(\\%) on Switchboard Hub5'00.}\n\\label{tab:swbd_wers}\n\\centerline{\n\\begin{tabular}{l||c|c|c}\n\n      & \\multicolumn{3}{c}{Hub5'00} \\\\ \\cline{2-4}\nModel & SWB & CHE & TOTAL \\\\ \n\\hline \\hline\n\nDNN \t& 15.2 \t & 28.2 \t\t&  21.7 \\\\\n\n+{\\texttt{LHUC}\\xspace}  \t&  \t14.7 & 26.6    & 20.7 \\\\ \n++{\\texttt{SAT-LHUC}\\xspace}  \t&  \t14.6 &  25.9   & 20.3 \\\\ \\hline\n\n+fMLLR  \t  & 14.2 & 26.2 \t& 20.2  \\\\\n++{\\texttt{LHUC}\\xspace} \t &  14.2 & 25.6 & 19.9  \\\\\n++{\\texttt{SAT-LHUC}\\xspace}   & 14.1 & 25.6  & 19.9 \\\\\n\n \\hline \\hline\n\\end{tabular}}\n\\end{table}\n\nFinally, in Fig~\\ref{fig:summary} we show the WERs obtained for  200 speakers across the TED, AMI, and SWBD test sets.  We observe that for 89\\% of speakers {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adaptation reduced the WER, and that {\\texttt{SAT-LHUC}\\xspace} gives a consistent reduction over {\\texttt{LHUC}\\xspace}.\n\n\\begin{figure*}[ht!]\n\\center\n  \\includegraphics[width=1.0\\textwidth]{summary}\n\\caption{Summary of WERs(\\%) obtained with {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace} adaptation techniques on test speakers of TED, SWBD and AMI corpora (results are sorted in descending WER order for the SI system). For {\\texttt{LHUC}\\xspace} the average observed improvement per speaker was at 1.6\\% absolute (7.0\\% relative). The same statistic for {\\texttt{SAT-LHUC}\\xspace} was at 2.3\\% absolute (9.7\\% relative). The maximum observed WER decrease per speaker was 11.4\\% absolute (32.7\\% relative) and 16.0\\% absolute (50\\% relative) for {\\texttt{LHUC}\\xspace} and {\\texttt{SAT-LHUC}\\xspace}, respectively. WERs decreased for 89\\% of speakers using {\\texttt{LHUC}\\xspace} adaptation.}\n\\label{fig:summary}\n\\end{figure*}\n\n\\section{LHUC for Factorisation} \\label{sec:factorisation}\n\n\n\n\n\n\n\nWe have applied {\\texttt{LHUC}\\xspace} to adapt to both the speaker and the acoustic environment.  If multi-condition data is available for a speaker, then it is possible to define a set of joint speaker-environment {\\texttt{LHUC}\\xspace} transforms.  Alternatively, we can estimate two set of transforms -- for speaker ${\\mathbf r}_S$ and for environment ${\\mathbf r}_E$ -- and then linearly interpolate them to derive a combined transform ${\\mathbf {\\hat{r}}}_{SE}$ as follows:\n\n", "index": 17, "text": "\\begin{equation} \\label{eq:intrp}\n\\xi \\left ( {\\mathbf {\\hat{r}}}^l_{SE} \\right ) = \\alpha \\xi \\left ( {\\mathbf r}^l_S \\right ) + (1-\\alpha) \\xi \\left ( {\\mathbf r}^l_E \\right )\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\xi\\left({\\mathbf{\\hat{r}}}^{l}_{SE}\\right)=\\alpha\\xi\\left({\\mathbf{r}}^{l}_{S%&#10;}\\right)+(1-\\alpha)\\xi\\left({\\mathbf{r}}^{l}_{E}\\right)\" display=\"block\"><mrow><mrow><mi>\u03be</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mover accent=\"true\"><mi>\ud835\udc2b</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>S</mi><mo>\u2062</mo><mi>E</mi></mrow><mi>l</mi></msubsup><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>\u03be</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc2b</mi><mi>S</mi><mi>l</mi></msubsup><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03be</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc2b</mi><mi>E</mi><mi>l</mi></msubsup><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]