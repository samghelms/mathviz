[{"file": "1601.05557.tex", "nexttext": "\nOur upper bound is obtained by an application of our reduction-based approach, while our lower bound is proved directly\nusing information-theoretic arguments.\n\nWe note that previous testers for independence were suboptimal up to polynomial factors in both $n$ and $1/{\\epsilon}$, \neven for the case of $d=2.$ Specifically, Batu {\\em et al.}~\\cite{BFFKRW:01} gave an independence tester over $[n] \\times [m]$\nwith sample complexity $\\widetilde{O}(n^{2/3} m^{1/3}) \\cdot {\\mathrm{poly}}(1/{\\epsilon})$, for $n \\ge m.$ \nOn the lower bound side, Levi, Ron, and Rubinfeld \\cite{LRR11} showed a sample complexity lower bound of \n$\\Omega(\\sqrt{n m})$ (for all $n \\ge m$), and $\\Omega(n^{2/3} m^{1/3})$ (for $n = \\Omega(m \\log m))$. \nMore recently, Acharya {\\em et al.}~\\cite{ADK15} gave an upper bound of $O((({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/2}+{\\mathop{\\textstyle \\sum}}_{i=1}^d n_i)/{\\epsilon}^2),$ \nwhich is optimal up to constant factors for the very special case that all the $n_i$'s are the same. In summary, we\nresolve the sample complexity of this problem in any dimension $d$, up to a constant factor, as a function of all relevant parameters.\n\n\n\\item We obtain the first sample-optimal algorithms for testing equivalence for collections of distributions~\\cite{LRR11} \nin both the sampling and the oracle model. Our results improve upon the bounds from~\\cite{LRR11} by polynomial factors.\n\nIn the sampling model, we observe that the problem is equivalent to (a variant of) two-dimensional independence testing.\nIn fact, in the unknown-weights case, the problem is identical.\nIn the known-weights case, the problem is equivalent to two-dimensional independence testing,\nwhere the algorithm is given explicit access to one of the marginals (say, the marginal distribution on $[m]$).\nFor this setting, we give a sample-optimal tester with sample complexity \n$O(\\max(\\sqrt{nm}/{\\epsilon}^2,n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$\\footnote{It should be noted that, \nwhile this is the same form as the sample complexity for independence testing in two dimensions, \nthere is a crucial difference. In this setting, the parameter $m$ represents the support size of the marginal that is explicitly given to us,\nrather than the marginal with smaller support size.}.\n\nIn the query model, we give a sample-optimal closeness tester for $m$ distributions \nover $[n]$ with sample complexity $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/{\\epsilon}^{4/3}))$.\nNote that this bound is independent of $m$ and matches the bound for testing closeness \nbetween two unknown distributions.\n\n\\item As a final application of our techniques, we study the problem of \ntesting whether a distribution belongs in a given ``structured family''~\\cite{ADK15, CDGR15}.\nMore specifically, we focus on the property of being a $k$-histogram over $[n],$ \ni.e., that the probability mass function is piecewise constant with at most $k$ known interval pieces.\nThis is a natural hypothesis testing problem of particular interest in the context of model selection.\nFor $k=1,$ the problem is tantamount to uniformity testing, while for $k = \\Omega(n)$ it is easily seen to be \nequivalent to testing closeness between two unknown distributions over a domain of size $\\Omega(n).$\nUsing our framework, we design a tester for the property of being a $k$-histogram (with respect to a given set of intervals) with sample complexity \n$O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples. We also prove that this bound is \ninformation-theoretically optimal, up to constant factors.\n\\end{enumerate}\n\n\n\\subsection{Prior Techniques and Overview of our Approach} \\label{ssec:overview}\nThe work of Batu {\\em et al.}~\\cite{BFR+:00, Batu13} designs and analyzes a natural $\\ell_2$-closeness tester,\nand then appropriately reduces $\\ell_1$-closeness testing to $\\ell_2$-closeness testing.\nThe main problem with the $\\ell_2 \\rightarrow \\ell_1$ testing approach of ~\\cite{BFR+:00, Batu13}\narises when  the underlying distributions\nhave ``heavy'' bins (domain elements) (hence, large $\\ell_2$-norm), which their $\\ell_1$-tester handles separately.\nRoughly speaking,~\\cite{BFR+:00, Batu13} first learn the subdistribution on the heavy elements, and then \napply an $\\ell_2$-tester on the subdistribution over the light elements.\nAs pointed out in~\\cite{CDVV14}, this approach (of explicitly learning the heavy elements) \ninherently leads to a suboptimal sample complexity.\n\nWe remark that the standard $\\ell_2$-closeness testers are sample-efficient for the corresponding \n$\\ell_1$-testing problem, as long as {\\em at least one} of the distributions in question has small $\\ell_2$-norm.\nTo circumvent the aforementioned inefficiency of the~\\cite{BFR+:00, Batu13} reduction, roughly speaking,\nwe proceed as follows: We show that it is generally possible to (randomly) transform the initial distributions under consideration \n(i.e., the distributions we are given sample access to) to new distributions (over a potentially larger support) \nsuch that (at least) one of the new distributions has appropriately small $\\ell_2$-norm.\nThe transformation we define preserves the $\\ell_1$-norm, and is such that we can easily simulate samples from the new distributions.\nThe main idea is to artificially subdivide the heavy bins of one distribution in question\n(which, if needed, can be detected by drawing samples) into multiple bins\nin order to bring the $\\ell_2$-norm to a more manageable size. This procedure decreases the $\\ell_2$-norm while increasing\nthe domain size. By balancing these two quantities, we obtain sample-optimal testers for a wide variety of properties.\n\n\nOur lower bounds proceed by constructing explicit distributions $\\mathcal{D}$ and $\\mathcal{D'}$\nover (sets of) distributions, so that a random distribution $p$ drawn from $\\mathcal{D}$ satisfies the property,\na random distribution $p$ from $\\mathcal{D'}$ is far from satisfying the property (with high probability), and\nit is hard to distinguish between the two cases given a small number of samples.\nOur analysis is based on classical information-theoretic notions \nand is significantly different from previous approaches in this context.\nInstead of using techniques involving matching moments~\\cite{PValiant:08},\nwe are able to directly prove that the mutual information between the set of samples \ntaken and the distribution that $p$ was drawn from is small. Appropriately bounding the mutual information \nis perhaps somewhat technical, but quite manageable only requiring elementary calculations.\nWe believe that this technique is more flexible than the techniques of~\\cite{PValiant:08} \n(for example, it is not restricted to symmetric properties), \nand may prove useful in future problems.\n\n\n\n\n\n\\subsection{Organization}\n\nThe structure of this paper is as follows: In Section~\\ref{sec:upper}, we describe our $\\ell_2 \\rightarrow \\ell_1$ reduction and exploit\nit to obtain our optimal testers for a variety of problems. In Section~\\ref{sec:lb}, we illustrate our lower bound approach by proving\ntight lower bounds for independence testing and related problems.\n\n\n\\section{Our Reduction and its Algorithmic Applications} \\label{sec:upper}\n\nIn Section~\\ref{ssec:red}, we describe our basic reduction from $\\ell_1$ to $\\ell_2$ testing.\nIn Section~\\ref{ssec:apps}, we apply our reduction to a variety of concrete distribution testing problems.\n\n\n\\subsection{Reduction of $\\ell_1$-testing to $\\ell_2$-testing} \\label{ssec:red}\nThe starting point of our reduction-based approach is a ``basic tester'' for the identity between two unknown distributions \nwith respect to the $\\ell_2$-norm.\nWe emphasize that a simple and natural tester turns out to be optimal in this setting.\nMore specifically, we will use the following simple lemma (that follows, e.g., from Proposition 3.1 in~\\cite{CDVV14}):\n\n\\begin{lemma}\\label{L2TesterLem}\nLet $p$ and $q$ be two unknown distributions on $[n]$.\nThere exists an algorithm that {{on input $n$,  ${\\epsilon}>0,$ and $b \\geq \\max\\{\\|p\\|_2, \\|q\\|_2 \\}$}}\ndraws $O(bn/{\\epsilon}^2)$ samples from each of $p$ and $q$,\nand with probability at least $2/3$ distinguishes between the cases that $p=q$ and $\\|p-q\\|_1 > {\\epsilon}.$\n\\end{lemma}\n\n\\begin{remark} \\label{rem:l2} \n{\\em {{We remark that Proposition 3.1 of~\\cite{CDVV14} provides a somewhat stronger guarantee than the one of Lemma~\\ref{L2TesterLem}. Specifically, it yields a {\\em robust} $\\ell_2$-closeness tester with the following performance guarantee: Given $O(bn/{\\epsilon}^2)$ samples from distributions $p, q$ over $[n]$, where $b \\geq \\max\\{\\|p\\|_2, \\|q\\|_2 \\}$, the algorithm distinguishes (with probability at least $2/3$) between the cases that $\\|p-q\\|_2 \\leq {\\epsilon}/(2\\sqrt{n})$  and $\\|p-q\\|_2 \\geq {\\epsilon}/\\sqrt{n}.$ The soundness guarantee of Lemma~\\ref{L2TesterLem} follows from the Cauchy-Schwarz inequality.}}\n}\n\\end{remark}\n\nObserve that if $\\|p\\|_2$ and $\\|q\\|_2$ are both small, the algorithm of Lemma~\\ref{L2TesterLem} is in fact sample-efficient.\nFor example, if both are $O(1/\\sqrt{n}),$\nits sample complexity is an optimal $O(\\sqrt{n}/\\epsilon^2).$\nOn the other hand, the performance of this algorithm degrades as $\\|p\\|_2$ or $\\|q\\|_2$ increases.\nFortunately, there are some convenient reductions that simplify matters.\nTo begin with, we note that it suffices that only one of $\\|p\\|_2$ and $\\|q\\|_2$ is small.\nThis is essentially because if there is a large difference between the two, this is easy to detect.\n\n\\begin{lemma}\\label{L2TesterImprovedLem}\nLet $p$ and $q$ be two unknown distributions on $[n]$.\nThere exists an algorithm that \n {{on input $n$,  ${\\epsilon}>0,$ and $b \\geq \\min \\{\\|p\\|_2, \\|q\\|_2 \\}$}}\ndraws $O(bn/{\\epsilon}^2)$ samples\nfrom each of $p$ and $q$ and, with probability at least $2/3$,\ndistinguishes between the cases that $p=q$ and $\\|p-q\\|_1 > {\\epsilon}.$\n\\end{lemma}\n\\begin{proof}\nThe basic idea is to first test if $\\|p\\|_2 = \\Theta(\\|q\\|_2),$ and if so to run the tester of Lemma~\\ref{L2TesterLem}.\nTo test whether $\\|p\\|_2 = \\Theta(\\|q\\|_2)$, we estimate $\\|p\\|_2$ and $\\|q\\|_2$ up to a multiplicative constant factor.\nIt is known~\\cite{GR00, BFFKRW:01} that this can be done with\n $O(\\sqrt{n})=O(\\min (\\|p\\|_2, \\|q\\|_2) n)$ samples. If $\\|p\\|_2$ and $\\|q\\|_2$ do not agree to within a constant factor,\n we can conclude that $p\\neq q.$ Otherwise, we use the tester from Lemma \\ref{L2TesterLem}, and note\n that the number of required samples is $O(\\|p\\|_2n/{\\epsilon}^2).$\n\\end{proof}\n\n\\noindent In our applications of Lemma~\\ref{L2TesterImprovedLem}, \nwe take the parameter $b$ to be equal  to our upper bound on $\\min \\{\\|p\\|_2, \\|q\\|_2 \\}.$\nIn all our algorithms in Section~\\ref{ssec:apps} this upper bound will be clear from the context.\n\n\nIf both our initial distributions have large $\\ell_2$ norm, we describe a new way to reduce them\nby splitting the large weight bins (domain elements) into pieces. \nThe following key definition is the basis for our reduction:\n\n\\begin{definition}\nGiven a probability distribution $p$ on $[n]$ and a multiset $S$ of elements of $[n]$, define the \\emph{split distribution} $p_S$ on $[n+|S|]$ as follows:\nFor $1\\leq i\\leq n$, let $a_i$ denote $1$ plus the number of elements of $S$ that are equal to $i$.\nThus, $\\sum_{i=1}^n a_i = n+|S|.$ We can therefore associate the elements of $[n+|S|]$ to elements of the set\n$B=\\{(i,j):i\\in [n], 1\\leq j \\leq a_i\\}$.\nWe now define a distribution $p_S$ with support $B$, by letting a random sample from $p_S$ be given by $(i,j)$,\nwhere $i$ is drawn randomly from $p$ and $j$ is drawn randomly from $[a_i]$.\n\\end{definition}\n\nWe now show two basic facts about split distributions:\n\\begin{fact}\\label{splitDistributionFactsLem}\nLet $p$ and $q$ be probability distributions on $[n]$, and $S$ a given multiset of $[n]$. Then\n\\begin{itemize}\n\\item We can simulate a sample from $p_S$ or $q_S$ by taking a single sample from $p$ or $q$, respectively.\n\\item $\\|p_S-q_S\\|_1 = \\|p-q\\|_1$.\n\\end{itemize}\n\\end{fact}\nFact~\\ref{splitDistributionFactsLem} implies that it suffices to be able to test\nthe closeness of $p_S$ and $q_S$, for some $S$.\nIn particular, we want to find an $S$ so that $\\|p_S\\|_2$ and $\\|q_S\\|_2$ are small.\nWe show that this can be achieved as follows:\n\\begin{lemma}\\label{splitL2Lem}\nLet $p$ be a distribution on $[n]$.\n\\begin{itemize}\n\\item For any multisets $S\\subseteq S'$ of $[n]$, $\\|p_{S'}\\|_2 \\leq \\|p_S\\|_2$.\n\\item If $S$ is obtained by taking $\\mathrm{Poi}(m)$ samples from $p$, then ${\\mathbb{E}}[\\|p_S\\|_2^2] \\leq 1/m$.\n\\end{itemize}\n\\end{lemma}\n\\begin{proof}\n\nLet $a_i$ equal one plus the number of copies of $i$ in $S$,\nand $a_i'$ equal one plus the number of copies of $i$ in $S'$.\nWe note that $p_S=(i,j)$ with probability $p_i/a_i$. Therefore, we have that\n\n", "itemtype": "equation", "pos": 11461, "prevtext": "\n\n\\maketitle\n\n\n\n\\begin{abstract}\nWe study problems in distribution property testing:\nGiven sample access to one or more unknown discrete distributions,\nwe want to determine whether they have some global property or are ${\\epsilon}$-far\nfrom having the property in $\\ell_1$ distance.\nIn this paper, we provide a simple and general approach to obtain upper bounds in this setting,\nby reducing $\\ell_1$-testing to $\\ell_2$-testing.\nOur reduction yields optimal $\\ell_1$-testers, by using a standard $\\ell_2$-tester as a black-box.\n\nUsing our framework, we obtain sample--optimal and computationally efficient estimators for\na wide variety of $\\ell_1$ distribution testing problems, including the following: identity testing to a fixed distribution,\ncloseness testing between two unknown distributions (with equal/unequal sample sizes),\nindependence testing (in any number of dimensions), closeness testing for collections of distributions, and testing\n$k$-histograms. For most of these problems, we give the first optimal testers in the literature.\nMoreover, our estimators are significantly simpler to state and analyze compared to previous approaches.\n\nAs our second main contribution, we provide a direct general approach for proving distribution testing lower bounds,\nby bounding the mutual information. Our lower bound approach is not restricted to symmetric properties,\nand we use it to prove tight lower bounds for the aforementioned problems.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\\section{Introduction}  \\label{sec:intro}\n\n\\subsection{Motivation and Background}\n\nThe problem of determining whether an unknown object fits a model\nbased on observed data is of fundamental scientific importance.\nWe study the following formalization of this problem:\nGiven a few samples from one or more probability distributions, can we\ndetermine whether the distributions in question satisfy a certain global property?\nThis is the prototypical question in the area of statistical hypothesis testing~\\cite{NeymanP, lehmann2005testing}.\nDuring the past two decades, this broad question has received considerable attention by the TCS community\nin the framework of {\\em property testing}~\\cite{RS96, GGR98}, with a focus on discrete probability distributions.\n\nThe area of distribution property testing~\\cite{BFR+:00, Batu13}\nhas developed into a mature research field with connections to information theory, learning and statistics.\nThe generic inference problem in this field is the following: given sample access to one or more unknown distributions,\ndetermine whether they have some global property or are ``far''\n(in statistical distance or, equivalently, $\\ell_1$ norm) from having the property.\nThe goal is to obtain statistically and computationally efficient testing algorithms,\ni.e., algorithms that use the information-theoretically\nminimum sample size and run in polynomial time.\nSee~\\cite{GR00, BFR+:00, BFFKRW:01, Batu01, BDKR:02, BKR:04,  Paninski:08, PV11sicomp,\nDDSVV13, DJOP11, LRR11, ILR12, CDVV14, VV14, DKN:15, DKN:15:FOCS, ADK15, CDGR15} for a sample of works\nand~\\cite{Rub12, Canonne15} for two recent surveys.\n\nAfter almost two decades of intensive investigation, a number of tools and techniques have been developed, that\nin many cases, have led to a good understanding of the number of samples required for\nvarious distribution properties. Alas, the optimal sample complexity of several estimation tasks has remained open.\n\nIn this work, we describe a novel and general approach for distribution testing. \nAs a consequence of our unifying framework, we\nresolve the sample complexity of a wide variety of testing problems.\nSpecifically, we describe a method that yields both sample--optimal testers and matching information-theoretic lower bounds (up to constant factors).\nAll our upper bounds are obtained via a modular {\\em reduction}. Our reduction provides a simple way to obtain {\\em optimal} testers under the $\\ell_1$ distance,\nby using a standard {\\em $\\ell_2$--identity tester} as a black-box. {{Roughly speaking, for a variety of properties, we show that a sample--optimal $\\ell_1$-tester  can be obtained by applying a simple (randomized) transformation to a basic $\\ell_2$-identity tester.}}\n\nIt should be noted that the idea of reducing $\\ell_1$-testing to $\\ell_2$-testing has been used before in this context.\nIn particular, it was already used in the seminal work of Batu {\\em et al.}~\\cite{BFR+:00} on testing closeness between two unknown distributions.\nOne reason that an $\\ell_2 \\rightarrow \\ell_1$ reduction is natural is because the task of $\\ell_2$-testing appears to be significantly easier than $\\ell_1$-testing.\nIn particular, for the problem of $\\ell_2$-closeness testing,\nthe obvious unbiased estimator (with a straightforward analysis)\nis known to be optimal~\\cite{BFR+:00, CDVV14}.\n\nAlas, the reduction approach of~\\cite{BFR+:00} inherently gives suboptimal results for $\\ell_1$-testing,\nand it was suggested~\\cite{CDVV14} that a different, more direct approach may be necessary to achieve sample-optimal bounds.\nThis led researchers to consider different approaches to $\\ell_1$-testing \n(e.g., appropriately rescaled versions of the chi-squared test~\\cite{Orlitsky:colt12, CDVV14, VV14, BV15, ADK15}) \nthat, although shown optimal for a couple of cases, lead to somewhat ad-hoc \nestimators that come with a highly-nontrivial analysis.\n\nWe show here that there exists an extremely simple optimal reduction of $\\ell_1$-testing to $\\ell_2$-identity testing,\nand we use it to resolve a number of open problems in the literature. In addition to precisely pinning--down the \nsample complexity of a variety of distribution testing problems, a key contribution of our work is methodological.\n{{Our approach implies that we do not need an inherently different statistic for each particular testing problem. In contrast, all our testing algorithms follow the same pattern: They  are obtained by applying a simple transformation to a basic statistic -- one that tests  the identity between two distributions in $\\ell_2$-norm -- in a black-box manner. Following this scheme, we obtain the first sample-optimal testers for a variety of properties,  resolving a number of open problems. Importantly, our testers  can be stated in a few lines and their analysis fits in a short paragraph.}}\n\n\n{{ As our second main contribution, we provide a direct, elementary approach to prove sample complexity lower bounds for distribution testing problems. Given a  candidate hard instance, our proof  proceeds by bounding from above the mutual information between appropriate random variables.  Our analysis leads to new, optimal lower bounds for a variety of problems,  including testing closeness, testing independence (in any dimension), and testing histograms. It should be noted that proving sample complexity lower bounds by bounding the mutual information  is a classical approach in information theory. Perhaps surprisingly, prior to our work,  this method had not been used in distribution testing.  Previously used techniques were either based on the birthday paradox or on moment-matching~\\cite{RRSS09, PV11sicomp}, and were thus restricted to testing symmetric properties. Our direct information-theoretic technique circumvents the use of the moment-matching approach, and is not restricted to symmetric properties.}}\n\n\n\\subsection{Notation}\n\nWe write $[n]$ to denote the set $\\{1, \\ldots, n\\}$. We consider discrete distributions over $[n]$, which are functions\n$p: [n] \\rightarrow [0,1]$ such that $\\sum_{i=1}^n p_i =1.$ We will use the notation $p_i$ to denote the probability of element\n$i$ in distribution $p$. The $\\ell_1$ (resp. $\\ell_2$) norm of a distribution is identified with the $\\ell_1$ (resp. $\\ell_2$) norm of the corresponding $n$-vector, i.e.,\n$\\|p\\|_1 = \\sum_{i=1}^n |p_i|$ and $\\|p\\|_2 = \\sqrt{\\sum_{i=1}^n p^2_i}$. The $\\ell_1$ (resp. $\\ell_2$) distance between distributions $p$ and $q$\nis defined as the  the $\\ell_1$ (resp. $\\ell_2$) norm of the vector of their difference, i.e., $\\|p-q\\|_1 = \\sum_{i=1}^n |p_i -q_i|$ and\n$\\|p-q\\|_2 = \\sqrt{\\sum_{i=1}^n (p_i-q_i)^2}$.   For $\\lambda \\ge 0$, we denote by ${\\mathop{\\textnormal{Poi}}\\nolimits}(\\lambda)$ the Poisson distribution with parameter\n$\\lambda.$ \n\n\n\\subsection{Our Contributions} \\label{ssec:results}\n\nThe main contribution of this paper is a reduction--based framework to obtain optimal testing algorithms,\nand a direct approach to prove tight lower bounds. \nWe remark that we do not aim to exhaustively cover all possible applications of our approach, but rather\nto give some selected results that are indicative of the generality and power of our methods.\n\nMore specifically, we obtain the following results:\n\\begin{enumerate}\n\\item We give an alternative $\\ell_1$ identity tester against a fixed distribution with sample complexity $O(\\sqrt{n}/{\\epsilon}^2).$\nThis sample bound is known to be optimal, and matches the recently obtained tight bound of~\\cite{VV14, DKN:15}. \n{{The main advantage of our tester is its simplicity: Our reduction and its analysis are remarkably  short and simple in this case. Moreover, our tester straightforwardly implies  the ``$\\chi^2$ versus $\\ell_1$'' performance guarantee that was recently used as the main statistical test in~\\cite{ADK15}.}}\n\nAs an additional illustration of our approach, we give a simple algorithm and proof\nof a nearly instance-optimal $\\ell_1$-identity tester against an explicit distribution $q$ -- \nspecifically, one with sample complexity $O(\\|q\\|_{2/3}\\mathrm{polylog}(n/{\\epsilon})/{\\epsilon}^2)$ --\nmatching the recent optimal bound of~\\cite{VV14} up to poly-logarithmic factors for all $q$ with $\\|q\\|_\\infty < 1/2.$\n\n\\item We design an optimal tester for the problem $\\ell_1$-closeness testing between two unknown distributions. For the standard \ncase of equal sample size (i.e., the case that we draw the same number of samples from each of the two distributions), \nwe recover the sample complexity of $O(\\max(n^{2/3}/{\\epsilon}^{4/3},n^{1/2}/{\\epsilon}^2)),$ matching the tight bound of~\\cite{CDVV14}. \nAs we explain below, our tester straightforwardly extends to the case of unequal sized samples, giving the first optimal\ntester in this more general setting.\n\nA natural generalization of the closeness testing problem is to consider unequal sized samples from the\ntwo distributions. This testing problem was considered in~\\cite{AcharyaJOS14c} who gave the first algorithm and lower bound\nwith a polynomial gap between the two.\nUsing our framework, we obtain the first sample-optimal tester for this problem: Our tester uses\n$m_1 {{= \\Omega(\\max(n^{2/3}/{\\epsilon}^{4/3},n^{1/2}/{\\epsilon}^2))}}$ samples from one distribution\nand $m_2=O(\\max(nm_1^{-1/2}/{\\epsilon}^2,\\sqrt{n}/{\\epsilon}^2))$ from the other.\nWe note that this tradeoff is optimal for all settings of the parameters, and improves on the recent work~\\cite{BV15} \nthat obtains the same tradeoff under the additional assumption that ${\\epsilon} > n^{-1/12}.$ In sharp contrast to~\\cite{BV15}, \nour algorithm is extremely simple and its analysis fits in a few lines.\n\n\\item We obtain the first sample-optimal algorithm and a matching lower bound for the problem of testing independence \nover $\\times_{i=1}^d [n_i].$ Prior to our work, the sample complexity of this problem remained open, even for the two-dimensional case.\n\nSpecifically, we prove that the optimal sample complexity of independence testing (upper and lower bound) is\n\n", "index": 1, "text": "$$\\Theta\\left(\\max\\left(({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/2}/\\epsilon^2,n_j^{1/3}({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/3}/\\epsilon^{4/3}\\right)\\right).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\Theta\\left(\\max\\left(({\\mathop{\\textstyle\\prod}}_{i=1}^{d}n_{i})^{1/2}/%&#10;\\epsilon^{2},n_{j}^{1/3}({\\mathop{\\textstyle\\prod}}_{i=1}^{d}n_{i})^{1/3}/%&#10;\\epsilon^{4/3}\\right)\\right).\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"false\"><msubsup><mo>\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mstyle><msub><mi>n</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo>,</mo><mrow><mrow><msubsup><mi>n</mi><mi>j</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"false\"><msubsup><mo>\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mstyle><msub><mi>n</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup></mrow><mo>/</mo><msup><mi>\u03f5</mi><mrow><mn>4</mn><mo>/</mo><mn>3</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis proves the first claim.\n\nFor the second claim, we note that the expected squared $\\ell_2$ norm of $p_S$ is\n$\n\\sum_{i=1}^n p_i^2 {\\mathbb{E}}[a_i^{-1}].\n$\nWe note that $a_i$ is distributed as $1+X$ where $X$ is a $\\mathrm{Poi}(mp_i)$ random variable. Recall that\nif $X$ is a random variable distributed as $\\mathrm{Poi}(\\lambda)$, then\n$\n{\\mathbb{E}}[z^X] = e^{\\lambda(z-1)}.\n$\nTaking an integral we find that\n\n", "itemtype": "equation", "pos": 24376, "prevtext": "\nOur upper bound is obtained by an application of our reduction-based approach, while our lower bound is proved directly\nusing information-theoretic arguments.\n\nWe note that previous testers for independence were suboptimal up to polynomial factors in both $n$ and $1/{\\epsilon}$, \neven for the case of $d=2.$ Specifically, Batu {\\em et al.}~\\cite{BFFKRW:01} gave an independence tester over $[n] \\times [m]$\nwith sample complexity $\\widetilde{O}(n^{2/3} m^{1/3}) \\cdot {\\mathrm{poly}}(1/{\\epsilon})$, for $n \\ge m.$ \nOn the lower bound side, Levi, Ron, and Rubinfeld \\cite{LRR11} showed a sample complexity lower bound of \n$\\Omega(\\sqrt{n m})$ (for all $n \\ge m$), and $\\Omega(n^{2/3} m^{1/3})$ (for $n = \\Omega(m \\log m))$. \nMore recently, Acharya {\\em et al.}~\\cite{ADK15} gave an upper bound of $O((({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/2}+{\\mathop{\\textstyle \\sum}}_{i=1}^d n_i)/{\\epsilon}^2),$ \nwhich is optimal up to constant factors for the very special case that all the $n_i$'s are the same. In summary, we\nresolve the sample complexity of this problem in any dimension $d$, up to a constant factor, as a function of all relevant parameters.\n\n\n\\item We obtain the first sample-optimal algorithms for testing equivalence for collections of distributions~\\cite{LRR11} \nin both the sampling and the oracle model. Our results improve upon the bounds from~\\cite{LRR11} by polynomial factors.\n\nIn the sampling model, we observe that the problem is equivalent to (a variant of) two-dimensional independence testing.\nIn fact, in the unknown-weights case, the problem is identical.\nIn the known-weights case, the problem is equivalent to two-dimensional independence testing,\nwhere the algorithm is given explicit access to one of the marginals (say, the marginal distribution on $[m]$).\nFor this setting, we give a sample-optimal tester with sample complexity \n$O(\\max(\\sqrt{nm}/{\\epsilon}^2,n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$\\footnote{It should be noted that, \nwhile this is the same form as the sample complexity for independence testing in two dimensions, \nthere is a crucial difference. In this setting, the parameter $m$ represents the support size of the marginal that is explicitly given to us,\nrather than the marginal with smaller support size.}.\n\nIn the query model, we give a sample-optimal closeness tester for $m$ distributions \nover $[n]$ with sample complexity $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/{\\epsilon}^{4/3}))$.\nNote that this bound is independent of $m$ and matches the bound for testing closeness \nbetween two unknown distributions.\n\n\\item As a final application of our techniques, we study the problem of \ntesting whether a distribution belongs in a given ``structured family''~\\cite{ADK15, CDGR15}.\nMore specifically, we focus on the property of being a $k$-histogram over $[n],$ \ni.e., that the probability mass function is piecewise constant with at most $k$ known interval pieces.\nThis is a natural hypothesis testing problem of particular interest in the context of model selection.\nFor $k=1,$ the problem is tantamount to uniformity testing, while for $k = \\Omega(n)$ it is easily seen to be \nequivalent to testing closeness between two unknown distributions over a domain of size $\\Omega(n).$\nUsing our framework, we design a tester for the property of being a $k$-histogram (with respect to a given set of intervals) with sample complexity \n$O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples. We also prove that this bound is \ninformation-theoretically optimal, up to constant factors.\n\\end{enumerate}\n\n\n\\subsection{Prior Techniques and Overview of our Approach} \\label{ssec:overview}\nThe work of Batu {\\em et al.}~\\cite{BFR+:00, Batu13} designs and analyzes a natural $\\ell_2$-closeness tester,\nand then appropriately reduces $\\ell_1$-closeness testing to $\\ell_2$-closeness testing.\nThe main problem with the $\\ell_2 \\rightarrow \\ell_1$ testing approach of ~\\cite{BFR+:00, Batu13}\narises when  the underlying distributions\nhave ``heavy'' bins (domain elements) (hence, large $\\ell_2$-norm), which their $\\ell_1$-tester handles separately.\nRoughly speaking,~\\cite{BFR+:00, Batu13} first learn the subdistribution on the heavy elements, and then \napply an $\\ell_2$-tester on the subdistribution over the light elements.\nAs pointed out in~\\cite{CDVV14}, this approach (of explicitly learning the heavy elements) \ninherently leads to a suboptimal sample complexity.\n\nWe remark that the standard $\\ell_2$-closeness testers are sample-efficient for the corresponding \n$\\ell_1$-testing problem, as long as {\\em at least one} of the distributions in question has small $\\ell_2$-norm.\nTo circumvent the aforementioned inefficiency of the~\\cite{BFR+:00, Batu13} reduction, roughly speaking,\nwe proceed as follows: We show that it is generally possible to (randomly) transform the initial distributions under consideration \n(i.e., the distributions we are given sample access to) to new distributions (over a potentially larger support) \nsuch that (at least) one of the new distributions has appropriately small $\\ell_2$-norm.\nThe transformation we define preserves the $\\ell_1$-norm, and is such that we can easily simulate samples from the new distributions.\nThe main idea is to artificially subdivide the heavy bins of one distribution in question\n(which, if needed, can be detected by drawing samples) into multiple bins\nin order to bring the $\\ell_2$-norm to a more manageable size. This procedure decreases the $\\ell_2$-norm while increasing\nthe domain size. By balancing these two quantities, we obtain sample-optimal testers for a wide variety of properties.\n\n\nOur lower bounds proceed by constructing explicit distributions $\\mathcal{D}$ and $\\mathcal{D'}$\nover (sets of) distributions, so that a random distribution $p$ drawn from $\\mathcal{D}$ satisfies the property,\na random distribution $p$ from $\\mathcal{D'}$ is far from satisfying the property (with high probability), and\nit is hard to distinguish between the two cases given a small number of samples.\nOur analysis is based on classical information-theoretic notions \nand is significantly different from previous approaches in this context.\nInstead of using techniques involving matching moments~\\cite{PValiant:08},\nwe are able to directly prove that the mutual information between the set of samples \ntaken and the distribution that $p$ was drawn from is small. Appropriately bounding the mutual information \nis perhaps somewhat technical, but quite manageable only requiring elementary calculations.\nWe believe that this technique is more flexible than the techniques of~\\cite{PValiant:08} \n(for example, it is not restricted to symmetric properties), \nand may prove useful in future problems.\n\n\n\n\n\n\\subsection{Organization}\n\nThe structure of this paper is as follows: In Section~\\ref{sec:upper}, we describe our $\\ell_2 \\rightarrow \\ell_1$ reduction and exploit\nit to obtain our optimal testers for a variety of problems. In Section~\\ref{sec:lb}, we illustrate our lower bound approach by proving\ntight lower bounds for independence testing and related problems.\n\n\n\\section{Our Reduction and its Algorithmic Applications} \\label{sec:upper}\n\nIn Section~\\ref{ssec:red}, we describe our basic reduction from $\\ell_1$ to $\\ell_2$ testing.\nIn Section~\\ref{ssec:apps}, we apply our reduction to a variety of concrete distribution testing problems.\n\n\n\\subsection{Reduction of $\\ell_1$-testing to $\\ell_2$-testing} \\label{ssec:red}\nThe starting point of our reduction-based approach is a ``basic tester'' for the identity between two unknown distributions \nwith respect to the $\\ell_2$-norm.\nWe emphasize that a simple and natural tester turns out to be optimal in this setting.\nMore specifically, we will use the following simple lemma (that follows, e.g., from Proposition 3.1 in~\\cite{CDVV14}):\n\n\\begin{lemma}\\label{L2TesterLem}\nLet $p$ and $q$ be two unknown distributions on $[n]$.\nThere exists an algorithm that {{on input $n$,  ${\\epsilon}>0,$ and $b \\geq \\max\\{\\|p\\|_2, \\|q\\|_2 \\}$}}\ndraws $O(bn/{\\epsilon}^2)$ samples from each of $p$ and $q$,\nand with probability at least $2/3$ distinguishes between the cases that $p=q$ and $\\|p-q\\|_1 > {\\epsilon}.$\n\\end{lemma}\n\n\\begin{remark} \\label{rem:l2} \n{\\em {{We remark that Proposition 3.1 of~\\cite{CDVV14} provides a somewhat stronger guarantee than the one of Lemma~\\ref{L2TesterLem}. Specifically, it yields a {\\em robust} $\\ell_2$-closeness tester with the following performance guarantee: Given $O(bn/{\\epsilon}^2)$ samples from distributions $p, q$ over $[n]$, where $b \\geq \\max\\{\\|p\\|_2, \\|q\\|_2 \\}$, the algorithm distinguishes (with probability at least $2/3$) between the cases that $\\|p-q\\|_2 \\leq {\\epsilon}/(2\\sqrt{n})$  and $\\|p-q\\|_2 \\geq {\\epsilon}/\\sqrt{n}.$ The soundness guarantee of Lemma~\\ref{L2TesterLem} follows from the Cauchy-Schwarz inequality.}}\n}\n\\end{remark}\n\nObserve that if $\\|p\\|_2$ and $\\|q\\|_2$ are both small, the algorithm of Lemma~\\ref{L2TesterLem} is in fact sample-efficient.\nFor example, if both are $O(1/\\sqrt{n}),$\nits sample complexity is an optimal $O(\\sqrt{n}/\\epsilon^2).$\nOn the other hand, the performance of this algorithm degrades as $\\|p\\|_2$ or $\\|q\\|_2$ increases.\nFortunately, there are some convenient reductions that simplify matters.\nTo begin with, we note that it suffices that only one of $\\|p\\|_2$ and $\\|q\\|_2$ is small.\nThis is essentially because if there is a large difference between the two, this is easy to detect.\n\n\\begin{lemma}\\label{L2TesterImprovedLem}\nLet $p$ and $q$ be two unknown distributions on $[n]$.\nThere exists an algorithm that \n {{on input $n$,  ${\\epsilon}>0,$ and $b \\geq \\min \\{\\|p\\|_2, \\|q\\|_2 \\}$}}\ndraws $O(bn/{\\epsilon}^2)$ samples\nfrom each of $p$ and $q$ and, with probability at least $2/3$,\ndistinguishes between the cases that $p=q$ and $\\|p-q\\|_1 > {\\epsilon}.$\n\\end{lemma}\n\\begin{proof}\nThe basic idea is to first test if $\\|p\\|_2 = \\Theta(\\|q\\|_2),$ and if so to run the tester of Lemma~\\ref{L2TesterLem}.\nTo test whether $\\|p\\|_2 = \\Theta(\\|q\\|_2)$, we estimate $\\|p\\|_2$ and $\\|q\\|_2$ up to a multiplicative constant factor.\nIt is known~\\cite{GR00, BFFKRW:01} that this can be done with\n $O(\\sqrt{n})=O(\\min (\\|p\\|_2, \\|q\\|_2) n)$ samples. If $\\|p\\|_2$ and $\\|q\\|_2$ do not agree to within a constant factor,\n we can conclude that $p\\neq q.$ Otherwise, we use the tester from Lemma \\ref{L2TesterLem}, and note\n that the number of required samples is $O(\\|p\\|_2n/{\\epsilon}^2).$\n\\end{proof}\n\n\\noindent In our applications of Lemma~\\ref{L2TesterImprovedLem}, \nwe take the parameter $b$ to be equal  to our upper bound on $\\min \\{\\|p\\|_2, \\|q\\|_2 \\}.$\nIn all our algorithms in Section~\\ref{ssec:apps} this upper bound will be clear from the context.\n\n\nIf both our initial distributions have large $\\ell_2$ norm, we describe a new way to reduce them\nby splitting the large weight bins (domain elements) into pieces. \nThe following key definition is the basis for our reduction:\n\n\\begin{definition}\nGiven a probability distribution $p$ on $[n]$ and a multiset $S$ of elements of $[n]$, define the \\emph{split distribution} $p_S$ on $[n+|S|]$ as follows:\nFor $1\\leq i\\leq n$, let $a_i$ denote $1$ plus the number of elements of $S$ that are equal to $i$.\nThus, $\\sum_{i=1}^n a_i = n+|S|.$ We can therefore associate the elements of $[n+|S|]$ to elements of the set\n$B=\\{(i,j):i\\in [n], 1\\leq j \\leq a_i\\}$.\nWe now define a distribution $p_S$ with support $B$, by letting a random sample from $p_S$ be given by $(i,j)$,\nwhere $i$ is drawn randomly from $p$ and $j$ is drawn randomly from $[a_i]$.\n\\end{definition}\n\nWe now show two basic facts about split distributions:\n\\begin{fact}\\label{splitDistributionFactsLem}\nLet $p$ and $q$ be probability distributions on $[n]$, and $S$ a given multiset of $[n]$. Then\n\\begin{itemize}\n\\item We can simulate a sample from $p_S$ or $q_S$ by taking a single sample from $p$ or $q$, respectively.\n\\item $\\|p_S-q_S\\|_1 = \\|p-q\\|_1$.\n\\end{itemize}\n\\end{fact}\nFact~\\ref{splitDistributionFactsLem} implies that it suffices to be able to test\nthe closeness of $p_S$ and $q_S$, for some $S$.\nIn particular, we want to find an $S$ so that $\\|p_S\\|_2$ and $\\|q_S\\|_2$ are small.\nWe show that this can be achieved as follows:\n\\begin{lemma}\\label{splitL2Lem}\nLet $p$ be a distribution on $[n]$.\n\\begin{itemize}\n\\item For any multisets $S\\subseteq S'$ of $[n]$, $\\|p_{S'}\\|_2 \\leq \\|p_S\\|_2$.\n\\item If $S$ is obtained by taking $\\mathrm{Poi}(m)$ samples from $p$, then ${\\mathbb{E}}[\\|p_S\\|_2^2] \\leq 1/m$.\n\\end{itemize}\n\\end{lemma}\n\\begin{proof}\n\nLet $a_i$ equal one plus the number of copies of $i$ in $S$,\nand $a_i'$ equal one plus the number of copies of $i$ in $S'$.\nWe note that $p_S=(i,j)$ with probability $p_i/a_i$. Therefore, we have that\n\n", "index": 3, "text": "$$\n\\|p_S\\|_2^2 = \\sum_{i=1}^n \\sum_{j=1}^{a_i} \\left(\\frac{p_i}{a_i} \\right)^2 = \\sum_{i=1}^n p_i^2/a_i \\geq \\sum_{i=1}^n p_i^2/a_i' = \\|p_{S'}\\|_2^2.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\|p_{S}\\|_{2}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{a_{i}}\\left(\\frac{p_{i}}{a_{i}}%&#10;\\right)^{2}=\\sum_{i=1}^{n}p_{i}^{2}/a_{i}\\geq\\sum_{i=1}^{n}p_{i}^{2}/a_{i}^{%&#10;\\prime}=\\|p_{S^{\\prime}}\\|_{2}^{2}.\" display=\"block\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><msub><mi>p</mi><mi>S</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>a</mi><mi>i</mi></msub></munderover><msup><mrow><mo>(</mo><mfrac><msub><mi>p</mi><mi>i</mi></msub><msub><mi>a</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup><mo>/</mo><msub><mi>a</mi><mi>i</mi></msub></mrow></mrow><mo>\u2265</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup><mo>/</mo><msubsup><mi>a</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow></mrow><mo>=</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>p</mi><msup><mi>S</mi><mo>\u2032</mo></msup></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nTherefore, we have that\n\n", "itemtype": "equation", "pos": 24944, "prevtext": "\nThis proves the first claim.\n\nFor the second claim, we note that the expected squared $\\ell_2$ norm of $p_S$ is\n$\n\\sum_{i=1}^n p_i^2 {\\mathbb{E}}[a_i^{-1}].\n$\nWe note that $a_i$ is distributed as $1+X$ where $X$ is a $\\mathrm{Poi}(mp_i)$ random variable. Recall that\nif $X$ is a random variable distributed as $\\mathrm{Poi}(\\lambda)$, then\n$\n{\\mathbb{E}}[z^X] = e^{\\lambda(z-1)}.\n$\nTaking an integral we find that\n\n", "index": 5, "text": "\\begin{align*}\n{\\mathbb{E}}\\left[ \\frac{1}{1+X} \\right] & = {\\mathbb{E}}\\left[\\int_0^1 z^X dz\\right]\n = \\int_0^1 {\\mathbb{E}}[z^X] dz\n = \\int_0^1 e^{\\lambda(z-1)} dz\n = \\frac{1-e^{-\\lambda}}{\\lambda}\n \\leq \\frac{1}{\\lambda}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbb{E}}\\left[\\frac{1}{1+X}\\right]\" display=\"inline\"><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>X</mi></mrow></mfrac></mstyle><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathbb{E}}\\left[\\int_{0}^{1}z^{X}dz\\right]=\\int_{0}^{1}{%&#10;\\mathbb{E}}[z^{X}]dz=\\int_{0}^{1}e^{\\lambda(z-1)}dz=\\frac{1-e^{-\\lambda}}{%&#10;\\lambda}\\leq\\frac{1}{\\lambda}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup></mstyle><mrow><msup><mi>z</mi><mi>X</mi></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>z</mi></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup></mstyle><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msup><mi>z</mi><mi>X</mi></msup><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>z</mi></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mn>1</mn></msubsup></mstyle><mrow><msup><mi>e</mi><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>z</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>z</mi></mrow></mrow></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mi>e</mi><mrow><mo>-</mo><mi>\u03bb</mi></mrow></msup></mrow><mi>\u03bb</mi></mfrac></mstyle><mo>\u2264</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac></mstyle></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis completes the proof.\n\\end{proof}\n\n\\subsection{Algorithmic Applications} \\label{ssec:apps}\n\n\n\\subsubsection{Testing Identity to a Known Distribution}\n\nIn this section, we use our framework to give simple alternate optimal testers\nfor the problems of testing identity to a fixed distribution in the minimax and the instance optimal setting.\n\nAs a first application, we consider the problem of identity testing against a known distribution.\nIn this case our algorithm is extremely easy, and provides a much simpler proof\nof the minimax optimal bound~\\cite{VV14, DKN:15}:\n\n\\begin{proposition} \\label{prop:identity-fixed}\nThere exists an algorithm that given an explicit distribution $q$ supported on $[n]$\nand $O(\\sqrt{n}/{\\epsilon}^2)$ independent samples from a distribution $p$ over $[n]$\ndistinguishes with probability at least $2/3$ between the cases where $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}.$\n\\end{proposition}\n\\begin{proof}\nLet $S$ be the multiset where $S$ contains $\\lfloor n q_i \\rfloor$ of $q_i.$\nNote that $|S| \\leq \\sum_{i=1}^n nq_i  = n.$ Note also that $q_S$ assigns\nprobability mass at most $1/n$ to each bin.\nTherefore, we have that $\\|q_S\\|_2=O(1/\\sqrt{n}).$\nIt now suffices to distinguish between the cases that $p_S=q_S$\nand the case that $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\nUsing the basic tester from Lemma~\\ref{L2TesterImprovedLem} for $b  =  O(1/\\sqrt{n}),$ we can do this\nusing $O(2n b /{\\epsilon}^2) = O(\\sqrt{n}/{\\epsilon}^2)$ samples from $p_S.$\nThis can be simulated using $O(\\sqrt{n}/{\\epsilon}^2)$ samples from $p,$ which completes the proof.\n\\end{proof}\n\n{{ \\begin{remark} {\\em  We observe here that the identity tester of Proposition~\\ref{prop:identity-fixed} satisfies a stronger completeness guarantee: More specifically, it distinguishes between the cases that $\\chi^2(p, q) : = \\sum_{i=1}^n (p_i - q_i)^2/q_i \\leq {\\epsilon}^2/10$ versus $\\|p-q\\|_1 \\geq {\\epsilon}.$ Hence, it immediately implies Theorem~1 of~\\cite{ADK15}. This can be seen as follows: As explained in Remark~\\ref{rem:l2}, the basic tester of Lemma~\\ref{L2TesterLem} from~\\cite{CDVV14}  is a robust tester with respect to the $\\ell_2$-norm. Thus, the tester of Proposition~\\ref{prop:identity-fixed} distinguishes  between the cases that $\\|p_S-q_S\\|_2 \\leq {\\epsilon}/(2\\sqrt{n})$ and $\\|p_S-q_S\\|_2 \\geq {\\epsilon}/\\sqrt{n}.$ The desired soundness follows from the fact $\\|p-q\\|_1 = \\|p_S-q_S\\|_1$ and the Cauchy-Schwarz inequality.  The desired ``chi-squared'' completeness property follows from the easily verifiable (in)equalities $\\chi^2(p, q) = \\chi^2(p_S, q_S)$ and $\\chi^2(p_S, q_S)  \\geq n \\cdot \\|p_S-q_S\\|^2_2.$ } \\end{remark} }}\n\nThe tester of Proposition~\\ref{prop:identity-fixed} is sample-optimal for a worst-case choice of distribution $q.$\nAs first shown in~\\cite{VV14}, for many choices of $q,$ one can actually do substantially better.\nIn the following proposition, we give a simple proof of this fact:\n\n\n\\begin{proposition} \nThere exists an algorithm that on input an explicit distribution $q$ over $[n]$, a parameter ${\\epsilon}>0,$\nand $O(\\|q\\|_{2/3}\\mathrm{polylog}(n/{\\epsilon})/{\\epsilon}^2)$ \nsamples from a distribution $p$ over $[n]$ distinguishes with probability at least $2/3$\nbetween the cases where $p=q$ and $\\|p-q\\|_1 \\geq \\epsilon.$\n\\end{proposition}\n\n\\begin{proof}\nFor $i=0,\\ldots,k$ with $k=\\lceil 2\\log_2(10n/{\\epsilon}) \\rceil,$ \nlet $S_i\\subset [n]$ be the set of coordinates $j$ so that $q_j\\in (2^{-i-1},2^{-i}].$ \nLet $S_\\infty$ be the set of coordinates so that $q_j < 2^{-k} \\leq  {\\epsilon}/(10 n).$ \nLet $p|_S$ or $q|_S$ denote the vector of probabilities of $p$ or $q$ restricted to the set $S.$ \nWe note that if $\\|p-q\\|_1 > {\\epsilon},$ then $\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ for some $j.$ \nIt therefore suffices to find a tester that distinguishes between $p|_{S_j}=q|_{S_j}$ and \n$\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ with probability at least $2/3.$ \nRepeating this tester $O(\\log\\log(n/{\\epsilon}))$ times will then amplify the probability of success \nto $1-1/\\log^2(n/{\\epsilon}),$ and by a union bound over $j$ \nwe find that all such testers are correct with probability at least $2/3.$ \nIf all return that $p|_{S_j}=q|_{S_j},$ then we must have $p=q.$ \nOtherwise, we will have that $\\|p-q\\|_1 > {\\epsilon}.$\n\nIn order to distinguish between $\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ \nand $p|_{S_j}=q|_{S_j},$ we check two things. \nWe first use $O(\\log^2(n/{\\epsilon})/{\\epsilon}^2)$ samples to approximate $\\|p|_{S_j}\\|_1$ \nto within error ${\\epsilon}/(10\\log(n/{\\epsilon})).$ \nIf $\\|q|_{S_j}\\|_1$ is not within the range of possible values, \nwe determine that $p|_{S_j}\\neq q|_{S_j}$. Otherwise, let $p[S_j]$ and $q[S_j]$ \nbe the distributions obtained from $p$ and $q$ by conditioning on the entries lying in $S_j.$ \nIt now suffices to distinguish between the cases that $p[S_j]=q[S_j]$ and \n$\\|p[S_j]-q[S_j]\\|_1 \\gg {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1).$ \nNote that we can assume that $\\|q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$, \notherwise there is nothing to prove. We note that this necessarily fails to happen if $j=\\infty.$\n\nLet $m_j=|S_j|$. We have that $\\|q_{S_j}\\|_1 =\\Theta(m_j 2^{-j})$ and $\\|q[S_j]\\|_2 = \\Theta(m_j^{-1/2})$.\nTherefore, using the algorithm from Lemma~\\ref{L2TesterImprovedLem},\nwe can distinguish between $p[S_j]=q[S_j]$ and $\\|p[S_j]-q[S_j]\\|_1 \\gg {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1)$ using\n$\nO(m_j \\|q[S_j]\\|_2 {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1)^{-2}) = O(m_j^{5/2}4^{-j}\\log^2(n/{\\epsilon})/{\\epsilon}^2)\n$\nsamples from $p[S_j]$. The probability that a sample from $p$ lies in $S_j$ is $\\|p|_{S_j}\\|_1 \\gg \\|q_{S_j}\\|_1 \\gg m_j 2^{-j}$. \nUsing rejection sampling, we can get a sample from $p[S_j]$ using $O(2^j/m_j)$ samples from $p$. \nTherefore, the number of samples from $p$ needed to make the above determination is \n$O(m_j^{3/2}2^{-j} \\log^2(n/{\\epsilon})/{\\epsilon}^2)$.\n\nIn conclusion, we have described an algorithm that distinguishes between $p=q$ and $\\|p-q\\|_1>{\\epsilon}$ with sample complexity\n$\\textrm{polylog}(n/{\\epsilon}) \\cdot O((1+ \\max_j(m_j^{3/2} 2^{-j}))/{\\epsilon}^2).\n$\nWe note that\n\n", "itemtype": "equation", "pos": 25206, "prevtext": "\nTherefore, we have that\n\n", "index": 7, "text": "$$\n{\\mathbb{E}}[\\|p_S\\|_2^2] \\leq \\sum_{i=1}^n \\frac{p_i^2}{mp_i} = \\frac{1}{m} \\sum_{i=1}^n p_i = \\frac{1}{m}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}[\\|p_{S}\\|_{2}^{2}]\\leq\\sum_{i=1}^{n}\\frac{p_{i}^{2}}{mp_{i}}=%&#10;\\frac{1}{m}\\sum_{i=1}^{n}p_{i}=\\frac{1}{m}.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mrow><mo>\u2225</mo><msub><mi>p</mi><mi>S</mi></msub><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2264</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><msubsup><mi>p</mi><mi>i</mi><mn>2</mn></msubsup><mrow><mi>m</mi><mo>\u2062</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mfrac></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nTherefore, the number of samples required by the above algorithm is at most $O(\\|q\\|_{2/3}\\textrm{polylog}(n/\\epsilon)/\\epsilon^2)$.\n\\end{proof}\n\n\n\\subsubsection{Testing Closeness between two Unknown Distributions}\n\nIn this section, we consider the problem of testing closeness between two unknown distributions $p, q$.\nThe difficulty of this case lies in the fact that,\nnot knowing $q,$ we cannot subdivide into bins in such\na way as to guarantee that $\\|q_S\\|_2=O(1/\\sqrt{n}).$\nHowever, we can do nearly as well by first drawing an appropriate number of samples from $q,$ \nand then using them to provide our subdivisions.\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$ between the cases\n$p=q$ and $\\|p-q\\|_1>{\\epsilon}$ using $O(\\max(n^{2/3}/{\\epsilon}^{4/3},\\sqrt{n}/{\\epsilon}^2))$ samples from each of $p$ and $q$.\n\\end{proposition}\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}{\\epsilon}^{-4/3})$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\n\nTo show correctness, we first note that with high probability we have $|S|=O(n)$. Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$ norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nThus, the total number of samples taken by the algorithm is $O(k+nk^{-1/2}/{\\epsilon}^2) = O(\\max(n^{2/3}{\\epsilon}^{-4/3},\\sqrt{n}/{\\epsilon}^2)).$\n\\end{proof}\n\n\nA generalization of this problem considers the case that we have access to different size samples from the two distributions\n$p$ and $q$. There is an easy way to adapt our previous algorithm to this case:\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions,\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$\nbetween the cases $p=q$ and $\\|p-q\\|_1>{\\epsilon}$\ngiven $m_1$ samples from $q$ and an additional $m_2=O(\\max(nm_1^{-1/2}/{\\epsilon}^2,\\sqrt{n}/{\\epsilon}^2))$\nsamples from each of $p$ and $q.$\n\\end{proposition}\n\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness-Unequal\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\\item Let $k=\\min(n,m_1)$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\nTo show correctness, we first note that with high probability we have $|S|=O(n).$ Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$-norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nIn addition to the $m_1$ samples from $q$, we had to take $O(nk^{-1/2}/\\epsilon^2)=O(m_2)$ samples from each of $p$ and $q$.\n\\end{proof}\n\n\n\n\\subsubsection{Independence Testing} \\label{sec:ind}\n\nIn this subsection we provide our new sample-optimal independence tester.\nOur algorithm for testing independence in two dimensions is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Independence-2D\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to a distribution $p$ on $[n]\\times [m]$ with $n \\geq m$ and ${\\epsilon}>0.$\n\n\\textbf{Output:}``YES'' with probability at least $2/3$ if the coordinates of $p$ are independent,\n``NO'' with probability at least $2/3$ if $p$ is ${\\epsilon}$-far from any product distribution on $[n]\\times[m]$.\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}m^{1/3}{\\epsilon}^{-4/3}).$\n\n\\vspace{-0.2cm}\n\n\\item Let $S_1$ be a multiset in $[n]$ obtained by taking $\\mathrm{Poi}(k)$ samples from $p_1=\\pi_1(p)$.\nLet $S_2$ be a multiset in $[m]$ obtained by taking $\\mathrm{Poi}(m)$ samples from $p_2=\\pi_2(p)$.\nLet $S$ be the multiset of elements of $[n]\\times[m]$ so that\n\\vspace{-0.2cm}\n    \n", "itemtype": "equation", "pos": 31534, "prevtext": "\nThis completes the proof.\n\\end{proof}\n\n\\subsection{Algorithmic Applications} \\label{ssec:apps}\n\n\n\\subsubsection{Testing Identity to a Known Distribution}\n\nIn this section, we use our framework to give simple alternate optimal testers\nfor the problems of testing identity to a fixed distribution in the minimax and the instance optimal setting.\n\nAs a first application, we consider the problem of identity testing against a known distribution.\nIn this case our algorithm is extremely easy, and provides a much simpler proof\nof the minimax optimal bound~\\cite{VV14, DKN:15}:\n\n\\begin{proposition} \\label{prop:identity-fixed}\nThere exists an algorithm that given an explicit distribution $q$ supported on $[n]$\nand $O(\\sqrt{n}/{\\epsilon}^2)$ independent samples from a distribution $p$ over $[n]$\ndistinguishes with probability at least $2/3$ between the cases where $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}.$\n\\end{proposition}\n\\begin{proof}\nLet $S$ be the multiset where $S$ contains $\\lfloor n q_i \\rfloor$ of $q_i.$\nNote that $|S| \\leq \\sum_{i=1}^n nq_i  = n.$ Note also that $q_S$ assigns\nprobability mass at most $1/n$ to each bin.\nTherefore, we have that $\\|q_S\\|_2=O(1/\\sqrt{n}).$\nIt now suffices to distinguish between the cases that $p_S=q_S$\nand the case that $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\nUsing the basic tester from Lemma~\\ref{L2TesterImprovedLem} for $b  =  O(1/\\sqrt{n}),$ we can do this\nusing $O(2n b /{\\epsilon}^2) = O(\\sqrt{n}/{\\epsilon}^2)$ samples from $p_S.$\nThis can be simulated using $O(\\sqrt{n}/{\\epsilon}^2)$ samples from $p,$ which completes the proof.\n\\end{proof}\n\n{{ \\begin{remark} {\\em  We observe here that the identity tester of Proposition~\\ref{prop:identity-fixed} satisfies a stronger completeness guarantee: More specifically, it distinguishes between the cases that $\\chi^2(p, q) : = \\sum_{i=1}^n (p_i - q_i)^2/q_i \\leq {\\epsilon}^2/10$ versus $\\|p-q\\|_1 \\geq {\\epsilon}.$ Hence, it immediately implies Theorem~1 of~\\cite{ADK15}. This can be seen as follows: As explained in Remark~\\ref{rem:l2}, the basic tester of Lemma~\\ref{L2TesterLem} from~\\cite{CDVV14}  is a robust tester with respect to the $\\ell_2$-norm. Thus, the tester of Proposition~\\ref{prop:identity-fixed} distinguishes  between the cases that $\\|p_S-q_S\\|_2 \\leq {\\epsilon}/(2\\sqrt{n})$ and $\\|p_S-q_S\\|_2 \\geq {\\epsilon}/\\sqrt{n}.$ The desired soundness follows from the fact $\\|p-q\\|_1 = \\|p_S-q_S\\|_1$ and the Cauchy-Schwarz inequality.  The desired ``chi-squared'' completeness property follows from the easily verifiable (in)equalities $\\chi^2(p, q) = \\chi^2(p_S, q_S)$ and $\\chi^2(p_S, q_S)  \\geq n \\cdot \\|p_S-q_S\\|^2_2.$ } \\end{remark} }}\n\nThe tester of Proposition~\\ref{prop:identity-fixed} is sample-optimal for a worst-case choice of distribution $q.$\nAs first shown in~\\cite{VV14}, for many choices of $q,$ one can actually do substantially better.\nIn the following proposition, we give a simple proof of this fact:\n\n\n\\begin{proposition} \nThere exists an algorithm that on input an explicit distribution $q$ over $[n]$, a parameter ${\\epsilon}>0,$\nand $O(\\|q\\|_{2/3}\\mathrm{polylog}(n/{\\epsilon})/{\\epsilon}^2)$ \nsamples from a distribution $p$ over $[n]$ distinguishes with probability at least $2/3$\nbetween the cases where $p=q$ and $\\|p-q\\|_1 \\geq \\epsilon.$\n\\end{proposition}\n\n\\begin{proof}\nFor $i=0,\\ldots,k$ with $k=\\lceil 2\\log_2(10n/{\\epsilon}) \\rceil,$ \nlet $S_i\\subset [n]$ be the set of coordinates $j$ so that $q_j\\in (2^{-i-1},2^{-i}].$ \nLet $S_\\infty$ be the set of coordinates so that $q_j < 2^{-k} \\leq  {\\epsilon}/(10 n).$ \nLet $p|_S$ or $q|_S$ denote the vector of probabilities of $p$ or $q$ restricted to the set $S.$ \nWe note that if $\\|p-q\\|_1 > {\\epsilon},$ then $\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ for some $j.$ \nIt therefore suffices to find a tester that distinguishes between $p|_{S_j}=q|_{S_j}$ and \n$\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ with probability at least $2/3.$ \nRepeating this tester $O(\\log\\log(n/{\\epsilon}))$ times will then amplify the probability of success \nto $1-1/\\log^2(n/{\\epsilon}),$ and by a union bound over $j$ \nwe find that all such testers are correct with probability at least $2/3.$ \nIf all return that $p|_{S_j}=q|_{S_j},$ then we must have $p=q.$ \nOtherwise, we will have that $\\|p-q\\|_1 > {\\epsilon}.$\n\nIn order to distinguish between $\\|p|_{S_j} - q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$ \nand $p|_{S_j}=q|_{S_j},$ we check two things. \nWe first use $O(\\log^2(n/{\\epsilon})/{\\epsilon}^2)$ samples to approximate $\\|p|_{S_j}\\|_1$ \nto within error ${\\epsilon}/(10\\log(n/{\\epsilon})).$ \nIf $\\|q|_{S_j}\\|_1$ is not within the range of possible values, \nwe determine that $p|_{S_j}\\neq q|_{S_j}$. Otherwise, let $p[S_j]$ and $q[S_j]$ \nbe the distributions obtained from $p$ and $q$ by conditioning on the entries lying in $S_j.$ \nIt now suffices to distinguish between the cases that $p[S_j]=q[S_j]$ and \n$\\|p[S_j]-q[S_j]\\|_1 \\gg {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1).$ \nNote that we can assume that $\\|q|_{S_j}\\|_1 \\gg {\\epsilon}/\\log(n/{\\epsilon})$, \notherwise there is nothing to prove. We note that this necessarily fails to happen if $j=\\infty.$\n\nLet $m_j=|S_j|$. We have that $\\|q_{S_j}\\|_1 =\\Theta(m_j 2^{-j})$ and $\\|q[S_j]\\|_2 = \\Theta(m_j^{-1/2})$.\nTherefore, using the algorithm from Lemma~\\ref{L2TesterImprovedLem},\nwe can distinguish between $p[S_j]=q[S_j]$ and $\\|p[S_j]-q[S_j]\\|_1 \\gg {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1)$ using\n$\nO(m_j \\|q[S_j]\\|_2 {\\epsilon}/(\\log(n/{\\epsilon})\\|q|_{S_j}\\|_1)^{-2}) = O(m_j^{5/2}4^{-j}\\log^2(n/{\\epsilon})/{\\epsilon}^2)\n$\nsamples from $p[S_j]$. The probability that a sample from $p$ lies in $S_j$ is $\\|p|_{S_j}\\|_1 \\gg \\|q_{S_j}\\|_1 \\gg m_j 2^{-j}$. \nUsing rejection sampling, we can get a sample from $p[S_j]$ using $O(2^j/m_j)$ samples from $p$. \nTherefore, the number of samples from $p$ needed to make the above determination is \n$O(m_j^{3/2}2^{-j} \\log^2(n/{\\epsilon})/{\\epsilon}^2)$.\n\nIn conclusion, we have described an algorithm that distinguishes between $p=q$ and $\\|p-q\\|_1>{\\epsilon}$ with sample complexity\n$\\textrm{polylog}(n/{\\epsilon}) \\cdot O((1+ \\max_j(m_j^{3/2} 2^{-j}))/{\\epsilon}^2).\n$\nWe note that\n\n", "index": 9, "text": "$$\n\\|q\\|_{2/3} \\geq \\max_j ({\\mathop{\\textstyle \\sum}}_{i\\in S_j} q_i^{2/3} )^{3/2} \\geq \\max_j(m_j 2^{-2j/3})^{3/2} = \\max_j (m_j^{3/2} 2^{-j}).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\|q\\|_{2/3}\\geq\\max_{j}({\\mathop{\\textstyle\\sum}}_{i\\in S_{j}}q_{i}^{2/3})^{3/%&#10;2}\\geq\\max_{j}(m_{j}2^{-2j/3})^{3/2}=\\max_{j}(m_{j}^{3/2}2^{-j}).\" display=\"block\"><mrow><mo>\u2225</mo><mi>q</mi><msub><mo>\u2225</mo><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow></msub><mo>\u2265</mo><munder><mi>max</mi><mi>j</mi></munder><msup><mrow><mo stretchy=\"false\">(</mo><mstyle displaystyle=\"false\"><msub><mo>\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>S</mi><mi>j</mi></msub></mrow></msub></mstyle><msubsup><mi>q</mi><mi>i</mi><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>\u2265</mo><munder><mi>max</mi><mi>j</mi></munder><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mi>j</mi></msub><msup><mn>2</mn><mrow><mo>-</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>j</mi></mrow><mo>/</mo><mn>3</mn></mrow></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>=</mo><munder><mi>max</mi><mi>j</mi></munder><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>m</mi><mi>j</mi><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow></msubsup><msup><mn>2</mn><mrow><mo>-</mo><mi>j</mi></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\n\n\\vspace{-0.3cm}\n\n\\item Let $q$ be the distribution on $[n]\\times [m]$ obtained by taking $(x_1,y_1),(x_2,y_2)$ independent samples from $p$ and returning $(x_1,y_2)$.\nRun the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between the cases\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{0.2cm}\n\n\nFor correctness, we note that by Lemma \\ref{splitL2Lem},\nwith probability at least $9/10$ over our samples from $S_1$ and $S_2$, all of the above hold:\n(i) $|S_1|=O(n)$ and $|S_2|=O(m),$ and (ii) $\\|(p_1)_{S_1}\\|_2^2 = O(1/k)$, $\\|(p_2)_{S_2}\\|_2^2 = O(1/m).$\nWe henceforth condition on this event. We note that the distribution $q$ is exactly $p_1\\times p_2$. Therefore, if the coordinates of $p$ are independent, then\n$p=q$. On the other hand, since $q$ has independent coordinates, if $p$ is ${\\epsilon}$-far from any product distribution, $\\|p-q\\|_1\\geq {\\epsilon}$.\nTherefore, it suffices to distinguish between $p=q$ and $\\|p-q\\|_1\\geq {\\epsilon}$. By Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq \\epsilon.$ This completes correctness.\n\nWe now analyze the sample complexity.\nWe first draw samples when picking $S_1$ and $S_2$. With high probability, the corresponding number of samples is\n$O(m+k)=O(\\max(n^{2/3}m^{1/3}{\\epsilon}^{-4/3},\\sqrt{nm}/{\\epsilon}^2)).$ \nNext, we note that $q_S = (p_1)_{S_1}\\times (p_2)_{S_2}.$ \nTherefore, by Lemma~\\ref{L2TesterImprovedLem}, the number of samples drawn in the last step of the algorithm is at most\n\n", "itemtype": "equation", "pos": 37009, "prevtext": "\nTherefore, the number of samples required by the above algorithm is at most $O(\\|q\\|_{2/3}\\textrm{polylog}(n/\\epsilon)/\\epsilon^2)$.\n\\end{proof}\n\n\n\\subsubsection{Testing Closeness between two Unknown Distributions}\n\nIn this section, we consider the problem of testing closeness between two unknown distributions $p, q$.\nThe difficulty of this case lies in the fact that,\nnot knowing $q,$ we cannot subdivide into bins in such\na way as to guarantee that $\\|q_S\\|_2=O(1/\\sqrt{n}).$\nHowever, we can do nearly as well by first drawing an appropriate number of samples from $q,$ \nand then using them to provide our subdivisions.\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$ between the cases\n$p=q$ and $\\|p-q\\|_1>{\\epsilon}$ using $O(\\max(n^{2/3}/{\\epsilon}^{4/3},\\sqrt{n}/{\\epsilon}^2))$ samples from each of $p$ and $q$.\n\\end{proposition}\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}{\\epsilon}^{-4/3})$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\n\nTo show correctness, we first note that with high probability we have $|S|=O(n)$. Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$ norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nThus, the total number of samples taken by the algorithm is $O(k+nk^{-1/2}/{\\epsilon}^2) = O(\\max(n^{2/3}{\\epsilon}^{-4/3},\\sqrt{n}/{\\epsilon}^2)).$\n\\end{proof}\n\n\nA generalization of this problem considers the case that we have access to different size samples from the two distributions\n$p$ and $q$. There is an easy way to adapt our previous algorithm to this case:\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions,\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$\nbetween the cases $p=q$ and $\\|p-q\\|_1>{\\epsilon}$\ngiven $m_1$ samples from $q$ and an additional $m_2=O(\\max(nm_1^{-1/2}/{\\epsilon}^2,\\sqrt{n}/{\\epsilon}^2))$\nsamples from each of $p$ and $q.$\n\\end{proposition}\n\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness-Unequal\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\\item Let $k=\\min(n,m_1)$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\nTo show correctness, we first note that with high probability we have $|S|=O(n).$ Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$-norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nIn addition to the $m_1$ samples from $q$, we had to take $O(nk^{-1/2}/\\epsilon^2)=O(m_2)$ samples from each of $p$ and $q$.\n\\end{proof}\n\n\n\n\\subsubsection{Independence Testing} \\label{sec:ind}\n\nIn this subsection we provide our new sample-optimal independence tester.\nOur algorithm for testing independence in two dimensions is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Independence-2D\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to a distribution $p$ on $[n]\\times [m]$ with $n \\geq m$ and ${\\epsilon}>0.$\n\n\\textbf{Output:}``YES'' with probability at least $2/3$ if the coordinates of $p$ are independent,\n``NO'' with probability at least $2/3$ if $p$ is ${\\epsilon}$-far from any product distribution on $[n]\\times[m]$.\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}m^{1/3}{\\epsilon}^{-4/3}).$\n\n\\vspace{-0.2cm}\n\n\\item Let $S_1$ be a multiset in $[n]$ obtained by taking $\\mathrm{Poi}(k)$ samples from $p_1=\\pi_1(p)$.\nLet $S_2$ be a multiset in $[m]$ obtained by taking $\\mathrm{Poi}(m)$ samples from $p_2=\\pi_2(p)$.\nLet $S$ be the multiset of elements of $[n]\\times[m]$ so that\n\\vspace{-0.2cm}\n    \n", "index": 11, "text": "\\begin{align*}\n    & 1+\\{\\textrm{Number of copies of }(a,b)\\textrm{ in }S \\} = \\\\ & (1+\\{\\textrm{Number of copies of }a\\textrm{ in }S_1 \\})(1+\\{\\textrm{Number of copies of }b\\textrm{ in }S_2 \\}).\n    \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 1+\\{\\textrm{Number of copies of }(a,b)\\textrm{ in }S\\}=\" display=\"inline\"><mrow><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1+\\{\\textrm{Number of copies of }a\\textrm{ in }S_{1}\\})(1+\\{%&#10;\\textrm{Number of copies of }b\\textrm{ in }S_{2}\\}).\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><msub><mi>S</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><msub><mi>S</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nDrawing a sample from $q$ requires taking only two samples from $p,$ which completes the analysis.\n\n\\medskip\n\nIn the following proposition, we generalize the two dimensional algorithm to optimally test independence in any number of dimensions.\n\n\\begin{proposition}\nLet $q$ be a distribution taking values in $\\times_{i=1}^d [n_i].$ \nThere is an algorithm that draws \n\n", "itemtype": "equation", "pos": 38798, "prevtext": "\n\n\\vspace{-0.3cm}\n\n\\item Let $q$ be the distribution on $[n]\\times [m]$ obtained by taking $(x_1,y_1),(x_2,y_2)$ independent samples from $p$ and returning $(x_1,y_2)$.\nRun the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between the cases\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{0.2cm}\n\n\nFor correctness, we note that by Lemma \\ref{splitL2Lem},\nwith probability at least $9/10$ over our samples from $S_1$ and $S_2$, all of the above hold:\n(i) $|S_1|=O(n)$ and $|S_2|=O(m),$ and (ii) $\\|(p_1)_{S_1}\\|_2^2 = O(1/k)$, $\\|(p_2)_{S_2}\\|_2^2 = O(1/m).$\nWe henceforth condition on this event. We note that the distribution $q$ is exactly $p_1\\times p_2$. Therefore, if the coordinates of $p$ are independent, then\n$p=q$. On the other hand, since $q$ has independent coordinates, if $p$ is ${\\epsilon}$-far from any product distribution, $\\|p-q\\|_1\\geq {\\epsilon}$.\nTherefore, it suffices to distinguish between $p=q$ and $\\|p-q\\|_1\\geq {\\epsilon}$. By Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq \\epsilon.$ This completes correctness.\n\nWe now analyze the sample complexity.\nWe first draw samples when picking $S_1$ and $S_2$. With high probability, the corresponding number of samples is\n$O(m+k)=O(\\max(n^{2/3}m^{1/3}{\\epsilon}^{-4/3},\\sqrt{nm}/{\\epsilon}^2)).$ \nNext, we note that $q_S = (p_1)_{S_1}\\times (p_2)_{S_2}.$ \nTherefore, by Lemma~\\ref{L2TesterImprovedLem}, the number of samples drawn in the last step of the algorithm is at most\n\n", "index": 13, "text": "\\begin{align*}\nO(nm\\|q_{S}\\|_2 /{\\epsilon}^2) & = O(nm\\|(p_1)_{S_1}\\times(p_2)_{S_2}\\|_2/{\\epsilon}^2) = O(nm\\|(p_1)_{S_1}\\|_2\\|(p_2)_{S_2}\\|_2/{\\epsilon}^2)\\\\\n&= O(nm k^{-1/2}m^{-1/2}/{\\epsilon}^2)  = O(\\max(n^{2/3}m^{1/3}{\\epsilon}^{-4/3},\\sqrt{nm}/{\\epsilon}^2)).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle O(nm\\|q_{S}\\|_{2}/{\\epsilon}^{2})\" display=\"inline\"><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi>q</mi><mi>S</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=O(nm\\|(p_{1})_{S_{1}}\\times(p_{2})_{S_{2}}\\|_{2}/{\\epsilon}^{2})%&#10;=O(nm\\|(p_{1})_{S_{1}}\\|_{2}\\|(p_{2})_{S_{2}}\\|_{2}/{\\epsilon}^{2})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>S</mi><mn>1</mn></msub></msub><mo>\u00d7</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>S</mi><mn>2</mn></msub></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>S</mi><mn>1</mn></msub></msub><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>S</mi><mn>2</mn></msub></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=O(nmk^{-1/2}m^{-1/2}/{\\epsilon}^{2})=O(\\max(n^{2/3}m^{1/3}{%&#10;\\epsilon}^{-4/3},\\sqrt{nm}/{\\epsilon}^{2})).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><msup><mi>k</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>n</mi><mrow><mn>2</mn><mo>/</mo><mn>3</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup><mo>\u2062</mo><msup><mi>\u03f5</mi><mrow><mo>-</mo><mrow><mn>4</mn><mo>/</mo><mn>3</mn></mrow></mrow></msup></mrow><mo>,</mo><mrow><msqrt><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi></mrow></msqrt><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nsamples form $q$ and with probability at least $2/3$ \ndistinguishes between the coordinates of $q$ being independent and $q$ being ${\\epsilon}$-far from any such distribution.\n\\end{proposition}\n\\begin{proof}\nWe can assume that all $n_i\\geq 2$, for otherwise removing that term does not affect the problem.\nWe first note that the obvious generalization of {{Test-Independence-2D (draw $\\min(n_i, \\max_j n_j^{1/3}({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/3}/{\\epsilon}^{4/3})$  samples from the $i$-th marginal and use them to subdivide the domain in that dimension;  then run the basic $\\ell_2$-closeness tester between $q_S$ and the product of the marginals)}}\nallows us for any constant $d$ to distinguish between $q$ having independent coordinates\nand $\\|q-q^{\\ast}\\|_1>{\\epsilon},$ where $q^{\\ast}$ is the product of the marginals of $q,$ with \narbitrarily small constant probability of failure. This generalization incurs an additional $2^{O(d)}$\nfactor in the sample complexity, hence is not optimal for super-constant $d.$ To obtain the optimal sample complexity,\nwe will use the aforementioned algorithm for $d=2, 3$ along with a careful recursion to reduce the dimension.\n\nOur sample-optimal independence tester in $d$ dimensions is as follows: \nFirst, let us assume for simplicity that the maximum in the sample complexity is attained by the second term with $j=1.$\nThen, we use the algorithm {{Test-Independence-2D}} to distinguish between the cases that the first coordinate\nis independent of the others from the case that $q$ is at least ${\\epsilon}/2$-far from the product of the distributions\non the first coordinate and the distribution on the remaining coordinates. If it is not, we return ``NO''.\nOtherwise, we recursively test whether or not the coordinates $(q_2,\\ldots,q_d)$ are independent\nversus at least ${\\epsilon}/2$-far from the product of their marginals, and return the result.\nWe note that if $(q_2,\\ldots,q_d)$ is ${\\epsilon}/2$-close to the product distribution on $q_2,\\ldots,q_d,$\nand if $q$ is ${\\epsilon}/2$-close to the product distribution on its first coordinate with the remaining coordinates,\nthen $q$ is ${\\epsilon}$-close to the product of its marginals.\n\nWe next deal with the remaining case. We let $N= \\prod_{i=1}^d n_i.$\nWe first partition $[N]$ into sets $S_i$ for $1\\leq i\\leq 3$ so that $\\prod_{j\\in S_i} n_j \\leq \\sqrt{N}.$\nWe do this by greedily adding elements to a single set $S_1$ until the product is more than $\\sqrt{N}$.\nWe then remove the most recently added element, place it in $S_2$, and place all remaining elements in $S_3.$\nThis clearly satisfies the desired property. We let $q_{S_i}$ be the distribution of $q$ ignoring all but the coordinates in $S_i.$\nWe use the obvious independence tester {{in three dimensions}} to distinguish whether the $q_{S_i}$ are independent versus $q$ differing\nfrom the product by at least ${\\epsilon}/4.$ In the latter case, we return ``NO''.\nIn the former, we recursively distinguish between $q_{S_i}$ having independent coordinates\nversus being ${\\epsilon}/4$-far from the product of its marginals for each $i$ and return ``NO'' unless all three pass.\n\nIn order to analyze the sample complexity, \nwe note that our $d$-dimensional independence tester uses \n$O(\\max(({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/2}/{\\epsilon}^2,n_j^{1/3}({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/3}/{\\epsilon}^{4/3}))$ \nsamples on the highest level call to the $2$ or $3$-dimensional version of the tester. \nIt then needs to make $O(1)$ recursive calls to the high-dimensional version of the algorithm on distributions \nwith support of size at most $(\\prod_{i=1}^d n_i)^{1/2}$ and error at most ${\\epsilon}/4.$ \nThese recursive calls take a total of at most $O((\\prod_{i=1}^d n_i)^{1/3}/{\\epsilon}^2)$ samples, \nwhich is well within our desired bounds.\n\\end{proof}\n\n\n\\subsubsection{Testing Properties of Collections of Distributions}\nIn this subsection, we consider the model of testing properties of \ncollections of distributions~\\cite{LRR11} in both the sampling and query models.\n\nWe begin by considering the sampling model, as this is closely related to independence testing.\nIn fact, in the unknown-weights case, the problem is identical.\nIn the known-weights case, the problem is equivalent to independence testing,\nwhere the algorithm is given explicit access to one of the marginals (say, the distribution on $[m]$).\nFor this setting, we give a tester with sample complexity $O(\\max(\\sqrt{nm}/{\\epsilon}^2,n^{2/3}m^{1/3}/{\\epsilon}^{4/3})).$\nWe also note that this bound can be shown the be optimal. Formally, we prove the following:\n\n\\begin{proposition}\nThere is an algorithm that given sample access to a distribution $p$ on $[n] \\times [m]$\nand an explicit description of the marginal of $p$ on $[m]$\ndistinguishes between the cases that the  coordinates of $p$ are independent\nand the case where $p$ is ${\\epsilon}$-far from any product distribution on $[n]\\times[m]$\nwith probability at least $2/3$ using $O(\\max(\\sqrt{nm}/{\\epsilon}^2,n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$ samples.\n\\end{proposition}\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Collection-Sample-Model\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to a distribution $p$ on $[n]\\times [m]$ with ${\\epsilon}>0,$ and an explicit description of the marginal of $p$ on $[m]$.\n\n\\textbf{Output:}``YES'' with probability at least $2/3$ if the coordinates of $p$ are independent,\n``NO'' with probability at least $2/3$ if $p$ is ${\\epsilon}$-far from any product distribution on $[n]\\times[m]$.\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}m^{1/3}\\epsilon^{-4/3})$.\n\n\\item Let $S_1$ be a multiset in $[n]$ obtained by taking $\\mathrm{Poi}(k)$ samples from $p_1=\\pi_1(p).$\nLet $S_2$ be a multiset in $[m]$ obtained by taking $\\lfloor m(p_2)_i \\rfloor$ copies of $i$. \nLet $S$ be the multiset of elements of $[n]\\times[m]$ so that\n    \n", "itemtype": "equation", "pos": 39445, "prevtext": "\nDrawing a sample from $q$ requires taking only two samples from $p,$ which completes the analysis.\n\n\\medskip\n\nIn the following proposition, we generalize the two dimensional algorithm to optimally test independence in any number of dimensions.\n\n\\begin{proposition}\nLet $q$ be a distribution taking values in $\\times_{i=1}^d [n_i].$ \nThere is an algorithm that draws \n\n", "index": 15, "text": "$$O\\left(\\max\\left(({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/2}/{\\epsilon}^2,n_j^{1/3}({\\mathop{\\textstyle \\prod}}_{i=1}^d n_i)^{1/3}/{\\epsilon}^{4/3}\\right)\\right)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"O\\left(\\max\\left(({\\mathop{\\textstyle\\prod}}_{i=1}^{d}n_{i})^{1/2}/{\\epsilon}^%&#10;{2},n_{j}^{1/3}({\\mathop{\\textstyle\\prod}}_{i=1}^{d}n_{i})^{1/3}/{\\epsilon}^{4%&#10;/3}\\right)\\right)\" display=\"block\"><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"false\"><msubsup><mo>\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mstyle><msub><mi>n</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo>/</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo>,</mo><mrow><mrow><msubsup><mi>n</mi><mi>j</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mstyle displaystyle=\"false\"><msubsup><mo>\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mstyle><msub><mi>n</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup></mrow><mo>/</mo><msup><mi>\u03f5</mi><mrow><mn>4</mn><mo>/</mo><mn>3</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\n\n\\vspace{-0.3cm}\n\n\\item Let $q$ be the distribution on $[n]\\times [m]$ obtained by taking $(x_1,y_1),(x_2,y_2)$ independent samples from $p$ and returning $(x_1,y_2)$.\nRun the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between the cases\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{0.2cm}\n\n\nFor correctness, we note that by Lemma \\ref{splitL2Lem},\nwith probability at least $9/10$ over our samples from $S_1$ and $S_2$, all of the above hold:\n(i) $|S_1|=O(n)$ and $|S_2|=O(m),$ and (ii) $\\|(p_1)_{S_1}\\|_2^2 = O(1/k)$, $\\|(p_2)_{S_2}\\|_2^2 = O(1/m).$\nWe henceforth condition on this event. We note that the distribution $q$ is exactly $p_1\\times p_2$. Therefore, if the coordinates of $p$ are independent, then\n$p=q$. On the other hand, since $q$ has independent coordinates, if $p$ is ${\\epsilon}$-far from any product distribution, $\\|p-q\\|_1\\geq {\\epsilon}$.\nTherefore, it suffices to distinguish between $p=q$ and $\\|p-q\\|_1\\geq {\\epsilon}$. By Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq \\epsilon.$ This completes correctness.\n\nWe now analyze the sample complexity.\nWe first draw samples when picking $S_1$ and $S_2$. With high probability, the corresponding number of samples is\n$O(m+k)=O(\\max(n^{2/3}m^{1/3}{\\epsilon}^{-4/3},\\sqrt{nm}/{\\epsilon}^2)).$ \nNext, we note that $q_S = (p_1)_{S_1}\\times (p_2)_{S_2}.$ \nTherefore, by Lemma~\\ref{L2TesterImprovedLem}, the number of samples drawn in the last step of the algorithm is at most\n\n", "itemtype": "equation", "pos": 37009, "prevtext": "\nTherefore, the number of samples required by the above algorithm is at most $O(\\|q\\|_{2/3}\\textrm{polylog}(n/\\epsilon)/\\epsilon^2)$.\n\\end{proof}\n\n\n\\subsubsection{Testing Closeness between two Unknown Distributions}\n\nIn this section, we consider the problem of testing closeness between two unknown distributions $p, q$.\nThe difficulty of this case lies in the fact that,\nnot knowing $q,$ we cannot subdivide into bins in such\na way as to guarantee that $\\|q_S\\|_2=O(1/\\sqrt{n}).$\nHowever, we can do nearly as well by first drawing an appropriate number of samples from $q,$ \nand then using them to provide our subdivisions.\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$ between the cases\n$p=q$ and $\\|p-q\\|_1>{\\epsilon}$ using $O(\\max(n^{2/3}/{\\epsilon}^{4/3},\\sqrt{n}/{\\epsilon}^2))$ samples from each of $p$ and $q$.\n\\end{proposition}\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}{\\epsilon}^{-4/3})$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\n\nTo show correctness, we first note that with high probability we have $|S|=O(n)$. Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$ norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nThus, the total number of samples taken by the algorithm is $O(k+nk^{-1/2}/{\\epsilon}^2) = O(\\max(n^{2/3}{\\epsilon}^{-4/3},\\sqrt{n}/{\\epsilon}^2)).$\n\\end{proof}\n\n\nA generalization of this problem considers the case that we have access to different size samples from the two distributions\n$p$ and $q$. There is an easy way to adapt our previous algorithm to this case:\n\n\\begin{proposition}\nThere exists an algorithm that given sample access to two distributions,\n$p$ and $q$ over $[n]$ distinguishes with probability $2/3$\nbetween the cases $p=q$ and $\\|p-q\\|_1>{\\epsilon}$\ngiven $m_1$ samples from $q$ and an additional $m_2=O(\\max(nm_1^{-1/2}/{\\epsilon}^2,\\sqrt{n}/{\\epsilon}^2))$\nsamples from each of $p$ and $q.$\n\\end{proposition}\n\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Closeness-Unequal\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to distributions $p$ and $q$ supported on $[n]$ and ${\\epsilon}>0.$\n\n\\textbf{Output:} ``YES'' with probability at least $2/3$ if $p=q$, ``NO'' with probability at least $2/3$ if $\\|p-q\\|_1\\geq {\\epsilon}.$\n\n\\begin{enumerate}\n\\item Let $k=\\min(n,m_1)$.\n\n\\item Define a multiset $S$ by taking $\\mathrm{Poi}(k)$ samples from $q$.\n\n\\item Run the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}.$\n\\end{enumerate}\n}}\n\n\\vspace{.2cm}\n\nTo show correctness, we first note that with high probability we have $|S|=O(n).$ Furthermore, by Lemma \\ref{splitL2Lem}\nit follows that the expected squared $\\ell_2$-norm of $q_S$ is at most $1/k.$ Therefore,\nwith probability at least $9/10$, we have that $|S|=O(n)$ and $\\|q_S\\|_2 = O(1/\\sqrt{k}).$\n\nThe tester from Lemma \\ref{L2TesterImprovedLem} distinguishes between $p_S=q_S$ and $\\|p_S-q_S\\|_1 \\geq {\\epsilon}$\n with $O(nk^{-1/2}/{\\epsilon}^2)$ samples.\nBy Fact \\ref{splitDistributionFactsLem}, this is equivalent to distinguishing between $p=q$ and $\\|p-q\\|_1 \\geq {\\epsilon}$.\nIn addition to the $m_1$ samples from $q$, we had to take $O(nk^{-1/2}/\\epsilon^2)=O(m_2)$ samples from each of $p$ and $q$.\n\\end{proof}\n\n\n\n\\subsubsection{Independence Testing} \\label{sec:ind}\n\nIn this subsection we provide our new sample-optimal independence tester.\nOur algorithm for testing independence in two dimensions is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Independence-2D\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to a distribution $p$ on $[n]\\times [m]$ with $n \\geq m$ and ${\\epsilon}>0.$\n\n\\textbf{Output:}``YES'' with probability at least $2/3$ if the coordinates of $p$ are independent,\n``NO'' with probability at least $2/3$ if $p$ is ${\\epsilon}$-far from any product distribution on $[n]\\times[m]$.\n\n\\begin{enumerate}\n\n\\item Let $k=\\min(n,n^{2/3}m^{1/3}{\\epsilon}^{-4/3}).$\n\n\\vspace{-0.2cm}\n\n\\item Let $S_1$ be a multiset in $[n]$ obtained by taking $\\mathrm{Poi}(k)$ samples from $p_1=\\pi_1(p)$.\nLet $S_2$ be a multiset in $[m]$ obtained by taking $\\mathrm{Poi}(m)$ samples from $p_2=\\pi_2(p)$.\nLet $S$ be the multiset of elements of $[n]\\times[m]$ so that\n\\vspace{-0.2cm}\n    \n", "index": 11, "text": "\\begin{align*}\n    & 1+\\{\\textrm{Number of copies of }(a,b)\\textrm{ in }S \\} = \\\\ & (1+\\{\\textrm{Number of copies of }a\\textrm{ in }S_1 \\})(1+\\{\\textrm{Number of copies of }b\\textrm{ in }S_2 \\}).\n    \\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 1+\\{\\textrm{Number of copies of }(a,b)\\textrm{ in }S\\}=\" display=\"inline\"><mrow><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><mi>S</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(1+\\{\\textrm{Number of copies of }a\\textrm{ in }S_{1}\\})(1+\\{%&#10;\\textrm{Number of copies of }b\\textrm{ in }S_{2}\\}).\" display=\"inline\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><msub><mi>S</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>Number of copies of\u00a0</mtext><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mtext>\u00a0in\u00a0</mtext><mo>\u2062</mo><msub><mi>S</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nTherefore, since $\\frac{1}{m}\\sum_{i=1}^m \\|q_{\\ast}-q_i\\|_1 >{\\epsilon}$, \nwe have that for some $k$ it holds \n$|\\{i:\\|q_{\\ast}-q_i\\|\\geq 2^{k-1}{\\epsilon}\\}| = \\Omega(m2^{-5k/4}).$ \nFor this value of $k$, there is at least a $9/10$ probability that some $i$ with this property \nwas selected as one of our $C2^{5k/4}$ that were used, \nand then assuming that the appropriate tester returned correctly, our algorithm will output ``NO''. \nThis establishes correctness.\nThe total sample complexity of this algorithm is easily seen to be\n$\\sum_k 2^{5k/4}k \\cdot O(\\sqrt{n}/{\\epsilon}^2 4^{-k} + n^{2/3}/{\\epsilon}^{4/3} 2^{-4k/3}) = O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/{\\epsilon}^{4/3})).$\n\\end{proof}\n\n\n\\subsubsection{Testing $k$-Histograms}\n\nFinally, in this subsection we use our framework to design a sample-optimal\nalgorithm for the property of being a $k$-histogram with known intervals.\n\nLet ${\\cal I}$ be a partition of $[n]$ into $k$ intervals.\nWe wish to be able to distinguish between the cases\nwhere a distribution $p$ has constant density on each interval\nversus the case where it is ${\\epsilon}$-far from any such distribution.\nWe show the following:\n\n\n\\begin{proposition}\nLet ${\\cal I}$ be a partition of $[n]$ into $k$ intervals.\nLet $p$ be a distribution on $[n]$.\nThere exists an algorithm which draws $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$\nindependent samples from $p$ and \ndistinguishes between the cases where $p$ is uniform on each of the intervals in $\\cal I$\nfrom the case where $p$ is ${\\epsilon}$-far from any such distribution with probability at least $2/3.$\n\\end{proposition}\n\\begin{proof}\nFirst, we wish to guarantee that each of the intervals has reasonably large support.\nWe can achieve this as follows:\nFor each interval $I\\in\\cal I$ we divide each bin within $I$ into $\\lceil n/(k|I|) \\rceil$ bins.\nNote that this increases the number of bins in $I$ by at most $n/k,$\nhence doing this to each interval in $\\cal I$ at most doubles the total size of the domain.\nTherefore, after applying this operation we get a distribution over {{a domain of size $O(n)$, and each of the $k$ intervals in $\\cal I$}} is of length $\\Omega(n/k).$\n\nNext, in order to use an $\\ell_2$-closeness tester, we want to further subdivide bins\n{{using our randomized transformation}}.\nTo this end, we let $m=\\min({{k}},n^{1/3}k^{1/3}/\\epsilon^{4/3})$ and take $\\mathrm{Poi}(m)$ samples from $p.$\nThen, for each interval $I_i\\in {\\cal I}$, we divide each bin in $I_i$ into $\\lfloor n a_i/(k|I_i|)\\rfloor+1$ new bins,\nwhere $a_i$ is the number of samples that were drawn from $I_i.$\n{{Let $I'_i$ denote the new interval obtained from $I_i.$}}\nNote that after this procedure the total number of bins is still $O(n)$ and that the number of bins in {{$I'_i$}} is now $\\Omega((n/k)(a_i+1)).$\n{{Let $p'$ be the distribution obtained from $p$ under this transformation.}}\n\nLet $q'$ be the distribution obtained by sampling from $p'$ and then returning a uniform random bin from the same interval $I'_i$ as the sample.\nWe claim that the $\\ell_2$-norm of $q'$ is small. In particular the squared $\\ell_2$-norm will be the sum over intervals $I'$ in our new partition\n(that is, after the subdivisions described above) of $O(p(I')^2/((n/k)(a_i+1)))$. Recall that $1/(a_i+1)$ has expectation at most $1/(mp(I')).$\nThis implies that the expected squared $\\ell_2$-norm of $q'$ is at most $\\sum_{I'} O(p(I')/(nm/k)) = O(k/(nm)).$\nTherefore, with large constant probability, we have that $\\|q'\\|_2^2 = O(k/(nm)).$\n\nWe can now apply the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish\nbetween the cases where $p'=q'$ and $\\|p'-q'\\|_1 > {\\epsilon}$ with\n$O(n^{1/2} k^{1/2} m^{-1/2}/{\\epsilon}^2) = O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples.\nWe have that $p'=q'$ if and only if $p$ is flat on each of the intervals in $\\cal I$,\nand $\\|p'-q'\\|_1>{\\epsilon}$ if $p$ is ${\\epsilon}$-far from any distribution which is flat on $\\cal I.$\nThis final test is sufficient to make our determination.\n\\end{proof}\n\n\n\n\\section{Sample Complexity Lower Bounds} \\label{sec:lb}\nIn this section, {{we illustrate our lower bound approach by proving a tight information-theoretic lower bound for the problem of testing independence in two dimensions. Using our construction, we also give tight lower bounds for related problems, in particular  the independence testing problem in any dimension, and the problem of testing histograms. }}\n\n\\subsection{Lower Bound for Two-Dimensional Independence Testing}\n\nThe main result of this subsection is as follows:\n\\begin{theorem}\nLet $n\\geq m \\geq 2$ be integers and ${\\epsilon}>0$ a sufficiently small universal constant.\nThen, any algorithm that draws samples from a distribution $p$ on $[n]\\times [m]$\nand, with probability at least $2/3$, distinguishes between the case\nthat the coordinates of $p$ are independent\nand the case where $p$ is ${\\epsilon}$-far from any product distribution\nmust use $\\Omega(\\max(\\sqrt{nm}/{\\epsilon}^{2},n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$ samples.\n\\end{theorem}\n\nWe split our argument into two parts proving each of the above lower bounds separately.\n\n\\subsubsection{The $\\Omega(\\sqrt{nm}\\epsilon^{-2})$ Lower Bound}\nWe start by proving the easier of the two bounds. \nIt should be noted that this part of the lower bound can essentially be obtained using known results.\nWe give a proof using our technique, in part as a guide to the somewhat\nmore complicated proof in the next section, which will be along similar lines.\n\nFirst, we note that it suffices to consider the case where $n$ and $m$\nare each sufficiently large since $\\Omega({\\epsilon}^{-2})$\nsamples are required to distinguish the uniform distribution on $[2]\\times [2]$\nfrom the distribution which takes value $(i,j)$ with probability $(1+(2\\delta_{i,j}-1){\\epsilon})/2.$\n\nOur goal is to exhibit distributions $\\mathcal{D}$ and $\\mathcal{D'}$\nover distributions on $[n]\\times [m]$ so that all distributions in $\\mathcal{D}$\nhave independent coordinates, and all distributions in $\\mathcal{D'}$\nare ${\\epsilon}$-far from product distributions,\nso that for any $k=o(\\sqrt{nm}/{\\epsilon}^2)$,\nno algorithm given $k$ independent samples \nfrom a random element of either $\\mathcal{D}$ or $\\mathcal{D'}$ \ncan determine which family the distribution came from with greater than $90\\%$ probability.\n\nAlthough the above will be our overall approach, we will actually analyze the following generalization \nin order to simplify the argument. First, we use the standard Poissonization trick. \nIn particular, instead of drawing $k$ samples from the appropriate distribution, \nwe will draw $\\mathrm{Poi}(k)$ samples. This is acceptable because with $99\\%$ probability, \nthis is at least $\\Omega(k)$ samples. Next, we relax the condition that elements of $\\mathcal{D'}$ \nbe ${\\epsilon}$-far from product distributions, and simply require that they are $\\Omega({\\epsilon})$-far from product distributions with $99\\%$ probability.\nThis is clearly equivalent upon accepting an additional $1\\%$ probability of failure, and altering ${\\epsilon}$ by a constant factor.\n\nFinally, we will relax the constraint that elements of $\\mathcal{D}$ and $\\mathcal{D'}$ are probability distributions.\nInstead, we will merely require that they are positive measures on $[n]\\times[m]$,\nso that elements of $\\mathcal{D}$ are product measures and elements of $\\mathcal{D'}$ \nare $\\Omega({\\epsilon})$-far from being product measures with probability at least $99\\%$.\nWe will require that the selected measures have total mass $\\Theta(1)$ with probability at least $99\\%$, \nand instead of taking samples from these measures (as this is no longer as sensible concept), \nwe will use the points obtained from a Poisson process of parameter $k$ \n(so the number of samples in a given bin is a Poisson random variable with parameter $k$ times the mass of the bin). \nThis is sufficient, because the output of such a Poisson process for a measure $\\mu$ \nis identical to the outcome of drawing $\\mathrm{Poi}(\\|\\mu\\|_1 k)$ samples from the distribution $\\mu/\\|\\mu\\|_1$. \nMoreover, the distance from $\\mu$ to the nearest product distribution is $\\|\\mu\\|_1$ times the distance \nfrom $\\mu/\\|\\mu\\|_1$ to the nearest product distribution.\n\n\\smallskip\n\nWe are now prepared to describe $\\mathcal{D}$ and $\\mathcal{D'}$ explicitly:\n\\begin{itemize}\n\\item We define $\\mathcal{D}$ to deterministically return the uniform distribution $\\mu$ with $\\mu(i,j)=\\frac{1}{nm}$ for all $(i,j) \\in [n] \\times [m].$\n\\item We define $\\mathcal{D'}$ to return the positive measure $\\nu$ so that for each $(i,j) \\in [n] \\times [m]$ the value $\\nu(i,j)$ \nis either $\\frac{1+\\epsilon}{nm}$ or $\\frac{1-\\epsilon}{nm}$ each with probability $1/2$ and independently over different pairs $(i,j)$.\n\\end{itemize}\n\nIt is clear that $\\|\\mu\\|_1,\\|\\nu\\|_1 = \\Theta(1)$ deterministically.\nWe need to show that the relevant Poisson processes return similar distributions. \nTo do this, we consider the following procedure: \nLet $X$ be a uniformly random bit. Let $p$ be a measure on $[n]\\times [m]$ drawn from either $\\mathcal{D}$ \nif $X=0$ or from $\\mathcal{D'}$ if $X=1$. \nWe run a Poisson process with parameter $k$ on $p,$ and let $a_{i,j}$ be the number of samples \ndrawn from bin $(i,j).$ We wish to show that, given access to all $a_{i,j}$'s, one is not able to determine the value of $X$ \nwith probability more than $51\\%$. To prove this, it suffices to bound from above the mutual information between $X$ and the set of samples \n$(a_{i,j})_{(i,j)\\in[n]\\times [m]}$. In particular, this holds true because of the following simple fact:\n\\begin{lemma}\\label{informationTheoryLem}\nIf $X$ is a uniform random bit and $A$ is a correlated random variable, \nthen if $f$ is any function so that $f(A)=X$ with at least $51\\%$ probability, \nthen $I(X:A)\\geq 2\\cdot 10^{-4}$.\n\\end{lemma}\n\\begin{proof}\nThis is a standard result in information theory, and the simple proof is included here for the sake of completeness.\nWe begin by showing that $I(X:f(A)) \\geq   2\\cdot 10^{-4}.$\nThis is because the conditional entropy, $H(X|f(A)),$ is the expectation \nover $f(A)$ of $h(q)=-q\\log(q)-(1-q)\\log(1-q)$, where $q$ is the probability that $X=F(A)$ \nconditional on that value of $f(A)$. Since ${\\mathbb{E}}[q]\\geq 51\\%$ and since $h$ is concave, \nwe have that $H(X|f(A)) \\leq h(0.51) < \\log(2)-2\\cdot 10^{-4}.$ Therefore, we have that\n\n", "itemtype": "equation", "pos": 49388, "prevtext": "\n\n\\item Let $q$ be the distribution on $[n]\\times [m]$ obtained by taking $(x_1,y_1),(x_2,y_2)$ independent samples from $p$ and returning $(x_1,y_2)$.\nRun the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish between the cases\n$p_{S}=q_{S}$ and $\\|p_{S}-q_{S}\\|_1 \\geq {\\epsilon}.$\n\n\n\\end{enumerate}\n}}\n\n\\vspace{0.2cm}\n\nFor the analysis, we note that $\\|(p_2)_{S_2}\\|_2 = O(1/\\sqrt{m})$\nand with probability at least $9/10$, it holds $\\|(p_1)_{S_1}\\|_2 = O(1/\\sqrt{k}).$\nTherefore, we have that $\\|(p_1\\times p_2)_S\\|_2 = O(1/\\sqrt{km})$.\nThus, the $\\ell_2$-tester of Lemma~\\ref{L2TesterImprovedLem} draws \n$O(nm^{1/2}k^{-1/2}/{\\epsilon}^2) =  O(\\max(\\sqrt{nm}/{\\epsilon}^2,n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$ \nsamples and the sample complexity is bounded as desired. \n\\end{proof}\n\n\n\n\n\nNext, we consider the query model. In this model, we are essentially guaranteed that the distribution on $[m]$ is uniform, \nbut are allowed to extract samples conditioned on a particular value of the second coordinate. \nEquivalently, there are $m$ distributions $q_1,\\ldots, q_m$ on $[n].$. \nWe wish to distinguish between the cases that the $q_i$'s are identical and the case \nwhere there is no distribution $q$ so that $\\frac{1}{m}\\sum_{i=1}^m \\|q-q_i\\|_1 \\leq {\\epsilon}.$ \nWe show that we can solve this problem with $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/\\epsilon^{4/3}))$ samples for any $m.$ \nThis is optimal for all $m\\geq 2,$ even if we are guaranteed that \n$q_1=q_2= \\ldots = q_{\\lfloor m/2\\rfloor}$ and $q_{\\lfloor m/2 +1 \\rfloor}=\\ldots = q_m.$\n\n\\begin{proposition}\nThere is an algorithm that given sample access to distributions $q_1,\\ldots,q_m$ on $[n]$\ndistinguishes between the cases that the $q_i$'s are identical\nand the case where there is no distribution $q$ so that $\\frac{1}{m}\\sum_{i=1}^m \\|q-q_i\\|_1 \\leq {\\epsilon}$\nwith probability at least $2/3$ using $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/{\\epsilon}^{4/3}))$ samples.\n\\end{proposition}\n\\begin{proof}\nThe algorithm is as follows:\n\n\\vspace{0.2cm}\n\n\n\\fbox{\\parbox{6in}{\n{\\bf Algorithm} Test-Collection-Query-Model\\\\\n\n\\vspace{-0.2cm}\n\n\\textbf{Input:} Sample access to a distribution $q_1,\\ldots, q_m$ on $[n]$ with ${\\epsilon}>0.$\n\n\\textbf{Output:}``YES'' with probability at least $2/3$ if the $q_i$ are identical,\n``NO'' with probability at least $2/3$ if there is no distribution $q$ so that $\\frac{1}{m}\\sum_{i=1}^m \\|q-q_i\\|_1 \\leq \\epsilon$.\n\n\\begin{enumerate}\n\n\\item Let $C$ be a sufficiently large constant.\n\n\\vspace{-0.2cm}\n\n\n\\item Let $q_{\\ast}$ denote the distribution obtained by sampling from a uniformly random $q_i.$\n\n\\vspace{-0.1cm}\n\n\\item For $k$ from $0$ to $\\lceil \\log_2(m) \\rceil$:\n\n\\vspace{-0.2cm}\n\n\\begin{enumerate}\n\n\\item Select $2^{5k/4} C$ uniformly random elements $i\\in [m].$\n\n\\vspace{-0.1cm}\n\n\\item For each selected $i$, use the $\\ell_1$-closeness tester to distinguish between \n$q_{\\ast}=q_i$ and $\\|q_{\\ast}-q_i\\|_1 > 2^{k-1} {\\epsilon}$ with failure probability at most $C^{-2}6^{-k}.$\n\n\\vspace{-0.1cm}\n\n\\item If any of these testers returned ``NO'', return ``NO''.\n\n\\end{enumerate}\n\n\\item Return ``YES''.\n\n\\end{enumerate}\n}}\n\n\\vspace{0.2cm}\n\nTo analyze this algorithm, we note that with probability $9/10$ all the testers we call whose hypotheses are satisfied output correctly.\nTherefore, if all $q_i$ are equal, they are equal to $q_{\\ast}$, and thus our algorithm returns ``YES'' with appropriately large probability.\nOn the other hand, if for any $q$ we have that $\\frac{1}{m}\\sum_{i=1}^m \\|q-q_i\\|_1 > {\\epsilon},$ \nthen in particular $\\frac{1}{m}\\sum_{i=1}^m \\|q_{\\ast}-q_i\\|_1 > {\\epsilon}.$\nNote that\n\n", "index": 19, "text": "$$\n\\frac{1}{m}\\sum_{i=1}^m \\|q_{\\ast}-q_i\\|_1 \\leq {\\epsilon}/2+ O\\left(\\sum_k \\frac{|\\{i:\\|q_*-q_i\\|\\geq 2^{k-1}{\\epsilon}\\}|2^k{\\epsilon}}{m}  \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{m}\\sum_{i=1}^{m}\\|q_{\\ast}-q_{i}\\|_{1}\\leq{\\epsilon}/2+O\\left(\\sum_{k%&#10;}\\frac{|\\{i:\\|q_{*}-q_{i}\\|\\geq 2^{k-1}{\\epsilon}\\}|2^{k}{\\epsilon}}{m}\\right).\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mrow><mo>\u2225</mo><mrow><msub><mi>q</mi><mo>\u2217</mo></msub><mo>-</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2264</mo><mrow><mrow><mi>\u03f5</mi><mo>/</mo><mn>2</mn></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo>:</mo><mrow><mrow><mo>\u2225</mo><mrow><msub><mi>q</mi><mo>*</mo></msub><mo>-</mo><msub><mi>q</mi><mi>i</mi></msub></mrow><mo>\u2225</mo></mrow><mo>\u2265</mo><mrow><msup><mn>2</mn><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msup><mn>2</mn><mi>k</mi></msup><mo>\u2062</mo><mi>\u03f5</mi></mrow><mi>m</mi></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThe lemma now follows from the data processing inequality, i.e., the fact that $I(X:A) \\geq I(X:f(A)).$\n\\end{proof}\n\n\n\nIn order to bound $I(X:\\{a_{i,j}\\})$ from above, we note that the $a_{i,j}$'s are independent conditional on $X,$ \nand therefore that\n\n", "itemtype": "equation", "pos": 59948, "prevtext": "\nTherefore, since $\\frac{1}{m}\\sum_{i=1}^m \\|q_{\\ast}-q_i\\|_1 >{\\epsilon}$, \nwe have that for some $k$ it holds \n$|\\{i:\\|q_{\\ast}-q_i\\|\\geq 2^{k-1}{\\epsilon}\\}| = \\Omega(m2^{-5k/4}).$ \nFor this value of $k$, there is at least a $9/10$ probability that some $i$ with this property \nwas selected as one of our $C2^{5k/4}$ that were used, \nand then assuming that the appropriate tester returned correctly, our algorithm will output ``NO''. \nThis establishes correctness.\nThe total sample complexity of this algorithm is easily seen to be\n$\\sum_k 2^{5k/4}k \\cdot O(\\sqrt{n}/{\\epsilon}^2 4^{-k} + n^{2/3}/{\\epsilon}^{4/3} 2^{-4k/3}) = O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{2/3}/{\\epsilon}^{4/3})).$\n\\end{proof}\n\n\n\\subsubsection{Testing $k$-Histograms}\n\nFinally, in this subsection we use our framework to design a sample-optimal\nalgorithm for the property of being a $k$-histogram with known intervals.\n\nLet ${\\cal I}$ be a partition of $[n]$ into $k$ intervals.\nWe wish to be able to distinguish between the cases\nwhere a distribution $p$ has constant density on each interval\nversus the case where it is ${\\epsilon}$-far from any such distribution.\nWe show the following:\n\n\n\\begin{proposition}\nLet ${\\cal I}$ be a partition of $[n]$ into $k$ intervals.\nLet $p$ be a distribution on $[n]$.\nThere exists an algorithm which draws $O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$\nindependent samples from $p$ and \ndistinguishes between the cases where $p$ is uniform on each of the intervals in $\\cal I$\nfrom the case where $p$ is ${\\epsilon}$-far from any such distribution with probability at least $2/3.$\n\\end{proposition}\n\\begin{proof}\nFirst, we wish to guarantee that each of the intervals has reasonably large support.\nWe can achieve this as follows:\nFor each interval $I\\in\\cal I$ we divide each bin within $I$ into $\\lceil n/(k|I|) \\rceil$ bins.\nNote that this increases the number of bins in $I$ by at most $n/k,$\nhence doing this to each interval in $\\cal I$ at most doubles the total size of the domain.\nTherefore, after applying this operation we get a distribution over {{a domain of size $O(n)$, and each of the $k$ intervals in $\\cal I$}} is of length $\\Omega(n/k).$\n\nNext, in order to use an $\\ell_2$-closeness tester, we want to further subdivide bins\n{{using our randomized transformation}}.\nTo this end, we let $m=\\min({{k}},n^{1/3}k^{1/3}/\\epsilon^{4/3})$ and take $\\mathrm{Poi}(m)$ samples from $p.$\nThen, for each interval $I_i\\in {\\cal I}$, we divide each bin in $I_i$ into $\\lfloor n a_i/(k|I_i|)\\rfloor+1$ new bins,\nwhere $a_i$ is the number of samples that were drawn from $I_i.$\n{{Let $I'_i$ denote the new interval obtained from $I_i.$}}\nNote that after this procedure the total number of bins is still $O(n)$ and that the number of bins in {{$I'_i$}} is now $\\Omega((n/k)(a_i+1)).$\n{{Let $p'$ be the distribution obtained from $p$ under this transformation.}}\n\nLet $q'$ be the distribution obtained by sampling from $p'$ and then returning a uniform random bin from the same interval $I'_i$ as the sample.\nWe claim that the $\\ell_2$-norm of $q'$ is small. In particular the squared $\\ell_2$-norm will be the sum over intervals $I'$ in our new partition\n(that is, after the subdivisions described above) of $O(p(I')^2/((n/k)(a_i+1)))$. Recall that $1/(a_i+1)$ has expectation at most $1/(mp(I')).$\nThis implies that the expected squared $\\ell_2$-norm of $q'$ is at most $\\sum_{I'} O(p(I')/(nm/k)) = O(k/(nm)).$\nTherefore, with large constant probability, we have that $\\|q'\\|_2^2 = O(k/(nm)).$\n\nWe can now apply the tester from Lemma \\ref{L2TesterImprovedLem} to distinguish\nbetween the cases where $p'=q'$ and $\\|p'-q'\\|_1 > {\\epsilon}$ with\n$O(n^{1/2} k^{1/2} m^{-1/2}/{\\epsilon}^2) = O(\\max(\\sqrt{n}/{\\epsilon}^2,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples.\nWe have that $p'=q'$ if and only if $p$ is flat on each of the intervals in $\\cal I$,\nand $\\|p'-q'\\|_1>{\\epsilon}$ if $p$ is ${\\epsilon}$-far from any distribution which is flat on $\\cal I.$\nThis final test is sufficient to make our determination.\n\\end{proof}\n\n\n\n\\section{Sample Complexity Lower Bounds} \\label{sec:lb}\nIn this section, {{we illustrate our lower bound approach by proving a tight information-theoretic lower bound for the problem of testing independence in two dimensions. Using our construction, we also give tight lower bounds for related problems, in particular  the independence testing problem in any dimension, and the problem of testing histograms. }}\n\n\\subsection{Lower Bound for Two-Dimensional Independence Testing}\n\nThe main result of this subsection is as follows:\n\\begin{theorem}\nLet $n\\geq m \\geq 2$ be integers and ${\\epsilon}>0$ a sufficiently small universal constant.\nThen, any algorithm that draws samples from a distribution $p$ on $[n]\\times [m]$\nand, with probability at least $2/3$, distinguishes between the case\nthat the coordinates of $p$ are independent\nand the case where $p$ is ${\\epsilon}$-far from any product distribution\nmust use $\\Omega(\\max(\\sqrt{nm}/{\\epsilon}^{2},n^{2/3}m^{1/3}/{\\epsilon}^{4/3}))$ samples.\n\\end{theorem}\n\nWe split our argument into two parts proving each of the above lower bounds separately.\n\n\\subsubsection{The $\\Omega(\\sqrt{nm}\\epsilon^{-2})$ Lower Bound}\nWe start by proving the easier of the two bounds. \nIt should be noted that this part of the lower bound can essentially be obtained using known results.\nWe give a proof using our technique, in part as a guide to the somewhat\nmore complicated proof in the next section, which will be along similar lines.\n\nFirst, we note that it suffices to consider the case where $n$ and $m$\nare each sufficiently large since $\\Omega({\\epsilon}^{-2})$\nsamples are required to distinguish the uniform distribution on $[2]\\times [2]$\nfrom the distribution which takes value $(i,j)$ with probability $(1+(2\\delta_{i,j}-1){\\epsilon})/2.$\n\nOur goal is to exhibit distributions $\\mathcal{D}$ and $\\mathcal{D'}$\nover distributions on $[n]\\times [m]$ so that all distributions in $\\mathcal{D}$\nhave independent coordinates, and all distributions in $\\mathcal{D'}$\nare ${\\epsilon}$-far from product distributions,\nso that for any $k=o(\\sqrt{nm}/{\\epsilon}^2)$,\nno algorithm given $k$ independent samples \nfrom a random element of either $\\mathcal{D}$ or $\\mathcal{D'}$ \ncan determine which family the distribution came from with greater than $90\\%$ probability.\n\nAlthough the above will be our overall approach, we will actually analyze the following generalization \nin order to simplify the argument. First, we use the standard Poissonization trick. \nIn particular, instead of drawing $k$ samples from the appropriate distribution, \nwe will draw $\\mathrm{Poi}(k)$ samples. This is acceptable because with $99\\%$ probability, \nthis is at least $\\Omega(k)$ samples. Next, we relax the condition that elements of $\\mathcal{D'}$ \nbe ${\\epsilon}$-far from product distributions, and simply require that they are $\\Omega({\\epsilon})$-far from product distributions with $99\\%$ probability.\nThis is clearly equivalent upon accepting an additional $1\\%$ probability of failure, and altering ${\\epsilon}$ by a constant factor.\n\nFinally, we will relax the constraint that elements of $\\mathcal{D}$ and $\\mathcal{D'}$ are probability distributions.\nInstead, we will merely require that they are positive measures on $[n]\\times[m]$,\nso that elements of $\\mathcal{D}$ are product measures and elements of $\\mathcal{D'}$ \nare $\\Omega({\\epsilon})$-far from being product measures with probability at least $99\\%$.\nWe will require that the selected measures have total mass $\\Theta(1)$ with probability at least $99\\%$, \nand instead of taking samples from these measures (as this is no longer as sensible concept), \nwe will use the points obtained from a Poisson process of parameter $k$ \n(so the number of samples in a given bin is a Poisson random variable with parameter $k$ times the mass of the bin). \nThis is sufficient, because the output of such a Poisson process for a measure $\\mu$ \nis identical to the outcome of drawing $\\mathrm{Poi}(\\|\\mu\\|_1 k)$ samples from the distribution $\\mu/\\|\\mu\\|_1$. \nMoreover, the distance from $\\mu$ to the nearest product distribution is $\\|\\mu\\|_1$ times the distance \nfrom $\\mu/\\|\\mu\\|_1$ to the nearest product distribution.\n\n\\smallskip\n\nWe are now prepared to describe $\\mathcal{D}$ and $\\mathcal{D'}$ explicitly:\n\\begin{itemize}\n\\item We define $\\mathcal{D}$ to deterministically return the uniform distribution $\\mu$ with $\\mu(i,j)=\\frac{1}{nm}$ for all $(i,j) \\in [n] \\times [m].$\n\\item We define $\\mathcal{D'}$ to return the positive measure $\\nu$ so that for each $(i,j) \\in [n] \\times [m]$ the value $\\nu(i,j)$ \nis either $\\frac{1+\\epsilon}{nm}$ or $\\frac{1-\\epsilon}{nm}$ each with probability $1/2$ and independently over different pairs $(i,j)$.\n\\end{itemize}\n\nIt is clear that $\\|\\mu\\|_1,\\|\\nu\\|_1 = \\Theta(1)$ deterministically.\nWe need to show that the relevant Poisson processes return similar distributions. \nTo do this, we consider the following procedure: \nLet $X$ be a uniformly random bit. Let $p$ be a measure on $[n]\\times [m]$ drawn from either $\\mathcal{D}$ \nif $X=0$ or from $\\mathcal{D'}$ if $X=1$. \nWe run a Poisson process with parameter $k$ on $p,$ and let $a_{i,j}$ be the number of samples \ndrawn from bin $(i,j).$ We wish to show that, given access to all $a_{i,j}$'s, one is not able to determine the value of $X$ \nwith probability more than $51\\%$. To prove this, it suffices to bound from above the mutual information between $X$ and the set of samples \n$(a_{i,j})_{(i,j)\\in[n]\\times [m]}$. In particular, this holds true because of the following simple fact:\n\\begin{lemma}\\label{informationTheoryLem}\nIf $X$ is a uniform random bit and $A$ is a correlated random variable, \nthen if $f$ is any function so that $f(A)=X$ with at least $51\\%$ probability, \nthen $I(X:A)\\geq 2\\cdot 10^{-4}$.\n\\end{lemma}\n\\begin{proof}\nThis is a standard result in information theory, and the simple proof is included here for the sake of completeness.\nWe begin by showing that $I(X:f(A)) \\geq   2\\cdot 10^{-4}.$\nThis is because the conditional entropy, $H(X|f(A)),$ is the expectation \nover $f(A)$ of $h(q)=-q\\log(q)-(1-q)\\log(1-q)$, where $q$ is the probability that $X=F(A)$ \nconditional on that value of $f(A)$. Since ${\\mathbb{E}}[q]\\geq 51\\%$ and since $h$ is concave, \nwe have that $H(X|f(A)) \\leq h(0.51) < \\log(2)-2\\cdot 10^{-4}.$ Therefore, we have that\n\n", "index": 21, "text": "$$\nI(X:f(A)) = H(X)-H(X|f(A)) \\geq \\log(2)-(\\log(2)-2\\cdot 10^{-4}) = 2\\cdot 10^{-4}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"I(X:f(A))=H(X)-H(X|f(A))\\geq\\log(2)-(\\log(2)-2\\cdot 10^{-4})=2\\cdot 10^{-4}.\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>H</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">|</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>log</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mi>log</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mn>2</mn><mo>\u22c5</mo><msup><mn>10</mn><mrow><mo>-</mo><mn>4</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mn>2</mn><mo>\u22c5</mo><msup><mn>10</mn><mrow><mo>-</mo><mn>4</mn></mrow></msup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nBy symmetry, it is clear that all of the $a_{i,j}$'s are the same, \nso it suffices to consider $I(X:a)$ for $a$ being one of the $a_{i,j}.$\nWe prove the following technical lemma:\n\\begin{lemma}\\label{infBoundLem}\nFor all $(i, j) \\in [n] \\times [m],$ it holds\n$I(X:a_{i, j}) = O(k^2 \\epsilon^4 / (m^2n^2)).$\n\\end{lemma}\nThe proof of this lemma is technical and is deferred to Appendix \\ref{lbAppend}.\nThe essential idea is that we condition on whether or not $\\lambda: = k/(nm)\\geq 1.$\nIf $\\lambda < 1$, then the probabilities of seeing $0$ or $1$ samples\nare approximately the same, and most of the information\ncomes from how often one sees exactly $2$ samples.\nFor $\\lambda \\geq 1$, we are comparing a Poisson distribution to a mixture\nof Poisson distributions with the same average mean,\nand we can deal with the information theory by making a Gaussian approximation.\n\nBy Lemma \\ref{infBoundLem}, (\\ref{eqn:info-ub}) yields that\n\n", "itemtype": "equation", "pos": 60290, "prevtext": "\nThe lemma now follows from the data processing inequality, i.e., the fact that $I(X:A) \\geq I(X:f(A)).$\n\\end{proof}\n\n\n\nIn order to bound $I(X:\\{a_{i,j}\\})$ from above, we note that the $a_{i,j}$'s are independent conditional on $X,$ \nand therefore that\n\n", "index": 23, "text": "\\begin{equation} \\label{eqn:info-ub}\nI(X:(a_{i,j})_{(i,j)\\in[n]\\times [m]}) \\leq \\sum_{(i,j)\\in[n]\\times [m]} I(X:a_{i,j}).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"I(X:(a_{i,j})_{(i,j)\\in[n]\\times[m]})\\leq\\sum_{(i,j)\\in[n]\\times[m]}I(X:a_{i,j%&#10;}).\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">[</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">[</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></munder><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nIn conjunction with Lemma \\ref{informationTheoryLem}, \nthis implies that $o(\\sqrt{mn}/{\\epsilon}^2)$ samples are insufficient to reliably distinguish \nan element of $\\mathcal{D}$ from an element of $\\mathcal{D'}.$\n\nTo complete the proof, it remains to show that elements of $\\mathcal{D}$ are all product distributions,\nand that most elements of $\\mathcal{D'}$ are far from product distributions. \nThe former follows trivially, and the latter is not difficult. We show:\n\\begin{lemma}\\label{notProductLem}\nWith $99\\%$ probability a sample from $\\mathcal{D'}$ is $\\Omega({\\epsilon})$-far from being a product distribution.\n\\end{lemma}\n\\begin{proof}\nFor this, we require the following simple claim:\n\\begin{claim}\\label{notProdLem}\nLet $\\mu$ be a measure on $[n]\\times [m]$ with marginals $\\mu_1$ and $\\mu_2.$\nIf $\\|\\mu-\\mu_1\\times \\mu_2/\\|\\mu\\|_1\\|_1>{\\epsilon}\\|\\mu\\|_1$, then $\\mu$ is \nat least ${\\epsilon} \\|\\mu\\|_1/4$-far from any product measure.\n\\end{claim}\n\\begin{proof}\nBy normalizing, we may assume that $\\|\\mu\\|_1=1$.\nSuppose for the sake of contradiction that for some measures\n$\\nu_1,\\nu_2$ it holds $\\|\\mu-\\nu_1\\times \\nu_2\\|_1 \\leq {\\epsilon}/4.$\nThen, we must have that $\\|\\mu_i-\\nu_i\\|_1 \\leq {\\epsilon}/4.$ This means that\n\n", "itemtype": "equation", "pos": 61360, "prevtext": "\nBy symmetry, it is clear that all of the $a_{i,j}$'s are the same, \nso it suffices to consider $I(X:a)$ for $a$ being one of the $a_{i,j}.$\nWe prove the following technical lemma:\n\\begin{lemma}\\label{infBoundLem}\nFor all $(i, j) \\in [n] \\times [m],$ it holds\n$I(X:a_{i, j}) = O(k^2 \\epsilon^4 / (m^2n^2)).$\n\\end{lemma}\nThe proof of this lemma is technical and is deferred to Appendix \\ref{lbAppend}.\nThe essential idea is that we condition on whether or not $\\lambda: = k/(nm)\\geq 1.$\nIf $\\lambda < 1$, then the probabilities of seeing $0$ or $1$ samples\nare approximately the same, and most of the information\ncomes from how often one sees exactly $2$ samples.\nFor $\\lambda \\geq 1$, we are comparing a Poisson distribution to a mixture\nof Poisson distributions with the same average mean,\nand we can deal with the information theory by making a Gaussian approximation.\n\nBy Lemma \\ref{infBoundLem}, (\\ref{eqn:info-ub}) yields that\n\n", "index": 25, "text": "$$\nI(X:(a_{i,j})_{(i,j)\\in[n]\\times [m]}) = O(k^2 {\\epsilon}^4/mn) = o(1).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"I(X:(a_{i,j})_{(i,j)\\in[n]\\times[m]})=O(k^{2}{\\epsilon}^{4}/mn)=o(1).\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">[</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>O</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>k</mi><mn>2</mn></msup><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>/</mo><mi>m</mi><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>o</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nwhich yields the desired contradiction.\n\\end{proof}\n\nIn light of the above claim, it suffices to show that with $99\\%$ probability\nover the choice of $\\nu$ from $\\mathcal{D'}$ we have\n$\\|\\nu-\\nu_1\\times \\nu_2/\\|\\nu\\|\\|_1 = \\Omega({\\epsilon}).$\nFor this, we note that when $n$ and $m$ are sufficiently large constants,\nwith $99\\%$ probability we have that:  (i) $|\\|\\nu\\|_1-1| \\leq {\\epsilon}/10,$\n(ii) $\\nu_1$ has mass in the range $[(1-{\\epsilon}/10)/n,(1+{\\epsilon}/10)/n]$ for at least half\nof its points, and (iii) $\\nu_2$ has mass in the range $[(1-{\\epsilon}/10)/m,(1+{\\epsilon}/10)/m]$ for at least half of its points.\nIf all of these conditions hold, then for at least a quarter of all points \nthe mass assigned by $\\nu_1\\times \\nu_2/\\|\\nu\\|_1$\nis between $(1-{\\epsilon}/2)/(nm)$ and $(1+{\\epsilon}/2)/(nm).$ \nIn such points, the difference between this quantity\nand the mass assigned by $\\nu$ is at least ${\\epsilon}/(2mn).$\nTherefore, under these conditions, we have that\n\n", "itemtype": "equation", "pos": 62674, "prevtext": "\nIn conjunction with Lemma \\ref{informationTheoryLem}, \nthis implies that $o(\\sqrt{mn}/{\\epsilon}^2)$ samples are insufficient to reliably distinguish \nan element of $\\mathcal{D}$ from an element of $\\mathcal{D'}.$\n\nTo complete the proof, it remains to show that elements of $\\mathcal{D}$ are all product distributions,\nand that most elements of $\\mathcal{D'}$ are far from product distributions. \nThe former follows trivially, and the latter is not difficult. We show:\n\\begin{lemma}\\label{notProductLem}\nWith $99\\%$ probability a sample from $\\mathcal{D'}$ is $\\Omega({\\epsilon})$-far from being a product distribution.\n\\end{lemma}\n\\begin{proof}\nFor this, we require the following simple claim:\n\\begin{claim}\\label{notProdLem}\nLet $\\mu$ be a measure on $[n]\\times [m]$ with marginals $\\mu_1$ and $\\mu_2.$\nIf $\\|\\mu-\\mu_1\\times \\mu_2/\\|\\mu\\|_1\\|_1>{\\epsilon}\\|\\mu\\|_1$, then $\\mu$ is \nat least ${\\epsilon} \\|\\mu\\|_1/4$-far from any product measure.\n\\end{claim}\n\\begin{proof}\nBy normalizing, we may assume that $\\|\\mu\\|_1=1$.\nSuppose for the sake of contradiction that for some measures\n$\\nu_1,\\nu_2$ it holds $\\|\\mu-\\nu_1\\times \\nu_2\\|_1 \\leq {\\epsilon}/4.$\nThen, we must have that $\\|\\mu_i-\\nu_i\\|_1 \\leq {\\epsilon}/4.$ This means that\n\n", "index": 27, "text": "\\begin{align*}\n\\|\\mu-\\mu_1\\times\\mu_2\\|_1 & \\leq \\|\\mu-\\nu_1\\times \\nu_2\\|_1 + \\|\\nu_1\\times \\nu_2 - \\mu_1\\times \\nu_2\\|_1 + \\|\\mu_1\\times \\nu_2 - \\mu_1\\times \\mu_2\\|_1\\\\\n& \\leq {\\epsilon} /4 + \\|\\nu_2\\|_1\\|\\mu_1-\\nu_1\\|_1 + \\|\\mu_1\\|_1\\|\\mu_2-\\nu_2\\|_1 \\\\\n& \\leq {{\\epsilon}/4(3+{\\epsilon}/4)} \\leq {\\epsilon} \\;,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|\\mu-\\mu_{1}\\times\\mu_{2}\\|_{1}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><mi>\u03bc</mi><mo>-</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bc</mi><mn>2</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\|\\mu-\\nu_{1}\\times\\nu_{2}\\|_{1}+\\|\\nu_{1}\\times\\nu_{2}-\\mu_{%&#10;1}\\times\\nu_{2}\\|_{1}+\\|\\mu_{1}\\times\\nu_{2}-\\mu_{1}\\times\\mu_{2}\\|_{1}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>\u03bc</mi><mo>-</mo><mrow><msub><mi>\u03bd</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>\u03bd</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow><mo>-</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow><mo>-</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bc</mi><mn>2</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\epsilon}/4+\\|\\nu_{2}\\|_{1}\\|\\mu_{1}-\\nu_{1}\\|_{1}+\\|\\mu_{1}%&#10;\\|_{1}\\|\\mu_{2}-\\nu_{2}\\|_{1}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mi>\u03f5</mi><mo>/</mo><mn>4</mn></mrow><mo>+</mo><mrow><msub><mrow><mo>\u2225</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>-</mo><msub><mi>\u03bd</mi><mn>1</mn></msub></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mrow><mo>\u2225</mo><msub><mi>\u03bc</mi><mn>1</mn></msub><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msub><mi>\u03bc</mi><mn>2</mn></msub><mo>-</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{{\\epsilon}/4(3+{\\epsilon}/4)}\\leq{\\epsilon}\\;,\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mrow><mi>\u03f5</mi><mo>/</mo><mn>4</mn></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>3</mn><mo>+</mo><mrow><mi>\u03f5</mi><mo>/</mo><mn>4</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mpadded width=\"+2.8pt\"><mi>\u03f5</mi></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis completes the proof.\n\\end{proof}\n\n\n\\subsubsection{The $\\Omega(n^{2/3}m^{1/3}\\epsilon^{-4/3})$ Lower Bound}\nIn this subsection, we prove the other half of the lower bound.\nAs in the proof of the previous subsection,\nit suffices to exhibit a pair of distributions $\\mathcal{D},\\mathcal{D'}$ over measures on $[n]\\times [m]$,\nso that with $99\\%$ probability each of these measures has total mass $\\Theta(1),$\nthe measures from $\\mathcal{D}$ are product measures\nand those from $\\mathcal{D'}$ are $\\Omega({\\epsilon})$-far from being product measures,\nand so that if a Poisson process with parameter $k=o(n^{2/3}m^{1/3}/{\\epsilon}^{-4/3})$\nis used to draw samples from $[n]\\times [m]$ by way of a uniformly random measure \nfrom either $\\mathcal{D}$ or $\\mathcal{D'}$, it is impossible to reliably determine \nwhich distribution the measure came from.\n\nWe start by noting that it suffices to consider only the case where $k\\leq n/2,$ \nsince otherwise the bound follows from the previous subsection.\nWe define the distributions over measures as follows:\n\\begin{itemize}\n\\item When generating an element from either $\\mathcal{D}$ or $\\mathcal{D'},$\nwe generate a sequence $c_1,\\ldots,c_n,$ where $c_i$ is $1/k$ with probability $k/n$ and $1/n$ otherwise.\nFurthermore, we assume that the $c_i$'s are selected independently of each other.\n\n\\item Then $\\mathcal{D}$ returns the measure $\\mu$ where $\\mu(i,j)=c_i/m.$\n\n\\item The distribution $\\mathcal{D'}$ generates the measure $\\nu,$\nwhere $\\nu(i,j)=1/(km)$ if $c_i=1/k$ and otherwise $\\nu(i,j)$\nis randomly either $(1+{\\epsilon})/(nm)$ or $(1-{\\epsilon})/(nm).$\n\\end{itemize}\nIt is easy to verify that with $99\\%$ probability that $\\|\\mu\\|_1,\\|\\nu\\|_1=\\Theta(1).$\nIt is also easy to see that $\\mathcal{D}$ only generates product measures.\nWe can show that $\\mathcal{D'}$ typically generates measures far from product measures: \n\\begin{lemma}\\label{notProduct2Lem}\nWith $99\\%$ probability a sample from $\\mathcal{D'}$ is $\\Omega({\\epsilon})$-far from being a product distribution.\n\\end{lemma}\n\\begin{proof}\nIf $\\nu$ is a random draw from $\\mathcal{D'}$ and $\\nu_2$ is second marginal distribution,\nit is easy to see that with high probability it holds \n$\\nu_2(j)/\\|\\nu\\|_1 \\in [(1-{\\epsilon}/3)/m,(1+{\\epsilon}/3)/m]$ for at least half of the $j\\in [m].$ \nAlso, with high probability, for at least half of the $i\\in [n]$ we have that \n$\\nu_1(i)\\in [(1-{\\epsilon}/3)/n,(1+{\\epsilon}/3)/n].$ \nFor such pairs $(i,j)$, we have that $|\\nu(i,j)-\\nu_1(i)\\nu_2(j)/\\|\\nu\\|_1| \\geq {\\epsilon}/(4nm),$ \nand thus\n\n", "itemtype": "equation", "pos": 63984, "prevtext": "\nwhich yields the desired contradiction.\n\\end{proof}\n\nIn light of the above claim, it suffices to show that with $99\\%$ probability\nover the choice of $\\nu$ from $\\mathcal{D'}$ we have\n$\\|\\nu-\\nu_1\\times \\nu_2/\\|\\nu\\|\\|_1 = \\Omega({\\epsilon}).$\nFor this, we note that when $n$ and $m$ are sufficiently large constants,\nwith $99\\%$ probability we have that:  (i) $|\\|\\nu\\|_1-1| \\leq {\\epsilon}/10,$\n(ii) $\\nu_1$ has mass in the range $[(1-{\\epsilon}/10)/n,(1+{\\epsilon}/10)/n]$ for at least half\nof its points, and (iii) $\\nu_2$ has mass in the range $[(1-{\\epsilon}/10)/m,(1+{\\epsilon}/10)/m]$ for at least half of its points.\nIf all of these conditions hold, then for at least a quarter of all points \nthe mass assigned by $\\nu_1\\times \\nu_2/\\|\\nu\\|_1$\nis between $(1-{\\epsilon}/2)/(nm)$ and $(1+{\\epsilon}/2)/(nm).$ \nIn such points, the difference between this quantity\nand the mass assigned by $\\nu$ is at least ${\\epsilon}/(2mn).$\nTherefore, under these conditions, we have that\n\n", "index": 29, "text": "$$\\|\\nu-\\nu_1\\times \\nu_2/\\|\\nu\\|_1\\|_1 \\geq (nm/4)({\\epsilon}/(2mn)) = \\epsilon/8 = \\Omega({\\epsilon}). $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\|\\nu-\\nu_{1}\\times\\nu_{2}/\\|\\nu\\|_{1}\\|_{1}\\geq(nm/4)({\\epsilon}/(2mn))=%&#10;\\epsilon/8=\\Omega({\\epsilon}).\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>\u03bd</mi><mo>-</mo><mrow><mrow><msub><mi>\u03bd</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow><mo>/</mo><msub><mrow><mo>\u2225</mo><mi>\u03bd</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi></mrow><mo>/</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03f5</mi><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03f5</mi><mo>/</mo><mn>8</mn></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThe lemma follows by Claim~\\ref{notProdLem}.\n\\end{proof}\n\n\nIt remains to show that the Poisson process in question is insufficient to distinguish\nwhich distribution the measure came from with non-trivial probability.\nAs before, we let $X$ be a uniformly random bit, and let\n$\\mu$ be a measure drawn from either $\\mathcal{D}$ if $X=0$ or $\\mathcal{D'}$ if $X=1.$\nWe run the Poisson process and let $a_{i,j}$ be the number of elements drawn from bin $(i,j)$.\nWe let $A_i$ be the vector $(a_{i,1},a_{i,2},\\ldots,a_{i,m}).$\nIt suffices to show that the mutual information $I(X:A_1,A_2,\\ldots,A_n)$ is small.\n\nNote that the $A_i$'s are conditionally independent on $X$\n(though that $a_{i,j}$'s are not, because $a_{i,1}$ and $a_{i,2}$ are correlated due to their relation to $c_i$).\nTherefore, we have that\n\n", "itemtype": "equation", "pos": 66636, "prevtext": "\nThis completes the proof.\n\\end{proof}\n\n\n\\subsubsection{The $\\Omega(n^{2/3}m^{1/3}\\epsilon^{-4/3})$ Lower Bound}\nIn this subsection, we prove the other half of the lower bound.\nAs in the proof of the previous subsection,\nit suffices to exhibit a pair of distributions $\\mathcal{D},\\mathcal{D'}$ over measures on $[n]\\times [m]$,\nso that with $99\\%$ probability each of these measures has total mass $\\Theta(1),$\nthe measures from $\\mathcal{D}$ are product measures\nand those from $\\mathcal{D'}$ are $\\Omega({\\epsilon})$-far from being product measures,\nand so that if a Poisson process with parameter $k=o(n^{2/3}m^{1/3}/{\\epsilon}^{-4/3})$\nis used to draw samples from $[n]\\times [m]$ by way of a uniformly random measure \nfrom either $\\mathcal{D}$ or $\\mathcal{D'}$, it is impossible to reliably determine \nwhich distribution the measure came from.\n\nWe start by noting that it suffices to consider only the case where $k\\leq n/2,$ \nsince otherwise the bound follows from the previous subsection.\nWe define the distributions over measures as follows:\n\\begin{itemize}\n\\item When generating an element from either $\\mathcal{D}$ or $\\mathcal{D'},$\nwe generate a sequence $c_1,\\ldots,c_n,$ where $c_i$ is $1/k$ with probability $k/n$ and $1/n$ otherwise.\nFurthermore, we assume that the $c_i$'s are selected independently of each other.\n\n\\item Then $\\mathcal{D}$ returns the measure $\\mu$ where $\\mu(i,j)=c_i/m.$\n\n\\item The distribution $\\mathcal{D'}$ generates the measure $\\nu,$\nwhere $\\nu(i,j)=1/(km)$ if $c_i=1/k$ and otherwise $\\nu(i,j)$\nis randomly either $(1+{\\epsilon})/(nm)$ or $(1-{\\epsilon})/(nm).$\n\\end{itemize}\nIt is easy to verify that with $99\\%$ probability that $\\|\\mu\\|_1,\\|\\nu\\|_1=\\Theta(1).$\nIt is also easy to see that $\\mathcal{D}$ only generates product measures.\nWe can show that $\\mathcal{D'}$ typically generates measures far from product measures: \n\\begin{lemma}\\label{notProduct2Lem}\nWith $99\\%$ probability a sample from $\\mathcal{D'}$ is $\\Omega({\\epsilon})$-far from being a product distribution.\n\\end{lemma}\n\\begin{proof}\nIf $\\nu$ is a random draw from $\\mathcal{D'}$ and $\\nu_2$ is second marginal distribution,\nit is easy to see that with high probability it holds \n$\\nu_2(j)/\\|\\nu\\|_1 \\in [(1-{\\epsilon}/3)/m,(1+{\\epsilon}/3)/m]$ for at least half of the $j\\in [m].$ \nAlso, with high probability, for at least half of the $i\\in [n]$ we have that \n$\\nu_1(i)\\in [(1-{\\epsilon}/3)/n,(1+{\\epsilon}/3)/n].$ \nFor such pairs $(i,j)$, we have that $|\\nu(i,j)-\\nu_1(i)\\nu_2(j)/\\|\\nu\\|_1| \\geq {\\epsilon}/(4nm),$ \nand thus\n\n", "index": 31, "text": "$$\n\\|\\nu-\\nu_1\\times \\nu_2/\\|\\nu\\|_1\\|_1 \\geq (nm/4)({\\epsilon}/4nm) \\geq {\\epsilon}/16 = \\Omega({\\epsilon}).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\|\\nu-\\nu_{1}\\times\\nu_{2}/\\|\\nu\\|_{1}\\|_{1}\\geq(nm/4)({\\epsilon}/4nm)\\geq{%&#10;\\epsilon}/16=\\Omega({\\epsilon}).\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>\u03bd</mi><mo>-</mo><mrow><mrow><msub><mi>\u03bd</mi><mn>1</mn></msub><mo>\u00d7</mo><msub><mi>\u03bd</mi><mn>2</mn></msub></mrow><mo>/</mo><msub><mrow><mo>\u2225</mo><mi>\u03bd</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><mi>m</mi></mrow><mo>/</mo><mn>4</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03f5</mi><mo>/</mo><mn>4</mn></mrow><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>\u03f5</mi><mo>/</mo><mn>16</mn></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03f5</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nby symmetry where $A=A_i.$\nTo complete the proof, we need the following technical lemma:\n\\begin{lemma}\\label{SILem} We have that\n$ I(X:A) = O(k^3{\\epsilon}^4/(n^3m)).$\n\\end{lemma}\nMorally speaking, this lemma holds because if all the $c$'s were $1/n,$\nwe would get an expectation of roughly $O(k^2{\\epsilon}^4/(n^2m)),$\nby techniques from the last section.\nHowever, the possibility that $c=1/k$ adds sufficient amount of ``noise''\nto somewhat decrease the amount of available information.\nThe formal proof is deferred to Appendix \\ref{lbAppend}.\n\nCombining (\\ref{eqn:info-ub2}) and the above lemma, we obtain that\n\n", "itemtype": "equation", "pos": 67551, "prevtext": "\nThe lemma follows by Claim~\\ref{notProdLem}.\n\\end{proof}\n\n\nIt remains to show that the Poisson process in question is insufficient to distinguish\nwhich distribution the measure came from with non-trivial probability.\nAs before, we let $X$ be a uniformly random bit, and let\n$\\mu$ be a measure drawn from either $\\mathcal{D}$ if $X=0$ or $\\mathcal{D'}$ if $X=1.$\nWe run the Poisson process and let $a_{i,j}$ be the number of elements drawn from bin $(i,j)$.\nWe let $A_i$ be the vector $(a_{i,1},a_{i,2},\\ldots,a_{i,m}).$\nIt suffices to show that the mutual information $I(X:A_1,A_2,\\ldots,A_n)$ is small.\n\nNote that the $A_i$'s are conditionally independent on $X$\n(though that $a_{i,j}$'s are not, because $a_{i,1}$ and $a_{i,2}$ are correlated due to their relation to $c_i$).\nTherefore, we have that\n\n", "index": 33, "text": "\\begin{equation} \\label{eqn:info-ub2}\nI(X:A_1,A_2,\\ldots,A_n) \\leq \\sum_{i=1}^n I(X:A_i) = n I(X:A),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"I(X:A_{1},A_{2},\\ldots,A_{n})\\leq\\sum_{i=1}^{n}I(X:A_{i})=nI(X:A),\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>n</mi><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis completes the proof.\n\n\\subsection{Lower Bound for High Dimensional Independence Testing}\n\nWe need to show two lower bounds, \nnamely $\\sqrt{\\prod_{i=1}^d n_i}/\\epsilon^2$ and $n_i^{1/3}\\left(\\prod_{j=1}^d n_j\\right)^{1/3}/{\\epsilon}^{4/3}$. \nWe can obtain both of these from the lower bound constructions from the $2$-variable case. \nIn particular, for the first bound, we have shown that it takes this many samples to distinguish \nbetween the uniform distribution on $N=\\prod_{i=1}^d n_i$ inputs (which is a product distribution), \nfrom a distribution that assigns probability $(1\\pm {\\epsilon})/N$ randomly to each input \n(which once renormalized is probably $\\Omega({\\epsilon})$-far from being a product distribution). \nFor the latter bound, we think of $[n_1]\\times\\cdots\\times[n_d]$ as \n$[n_i]\\times([n_1]\\times\\cdots[n_{i-1}]\\times[n_{i+1}]\\times\\cdots\\times[n_d])$, \nand consider the lower bound construction for $2$-variable independence testing. \nIt then takes at least $n_i^{1/3}N^{1/3}/{\\epsilon}^{4/3}$ samples to reliably distinguish \na ``YES'' instance from a ``NO'' instance. Note that in a ``YES'' instance the first \nand second coordinates are independent and the distribution on the second coordinate is uniform. \nTherefore, in a ``YES'' instance we have a $d$-dimensional product distribution. \nOn the other hand, a ``NO'' instance is likely $\\Omega({\\epsilon})$-far from any distribution \nthat is a product distribution over just this partition of the coordinates, \nand therefore $\\Omega({\\epsilon})$ far from any $d$-dimensional product distribution. \nThis completes the proof.\n\n\\subsection{Lower Bound for $k$-Histograms}\n\nWe can use the above construction to show that our upper bound for $k$-histograms is in fact tight. \nIn particular, if we rewrite $[n]$ as $[k]\\times [n/k]$ \nand let the intervals be given by the subsets $[n/k]\\times \\{i\\}$ for $1\\leq i\\leq k$, \nwe need to show that $\\Omega(\\max(\\sqrt{n}/{\\epsilon}^2 ,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples \nare required from a distribution $p$ on $[k]\\times [n/k]$ \nto distinguish between the coordinates of $p$ being independent \nwith the second coordinate having the uniform distribution, \nand $p$ being ${\\epsilon}$-far from any such distribution. \nWe note that in the lower bound distributions given for each part of our lower bound constructions \nfor the independence tester, the ``YES'' distributions  all had uniform marginal over the second coordinate. \nTherefore, the same hard distributions give a lower bound for testing $k$-histograms \nof $\\Omega(\\max(\\sqrt{n}/{\\epsilon}^2,k^{2/3}(n/k)^{1/3}/{\\epsilon}^{4/3}))= \\Omega(\\max(\\sqrt{n}/{\\epsilon}^2 ,n^{1/3}k^{1/3}/{\\epsilon}^{4/3})).$\nThis completes the proof.\n\n\n\n\n\n\n\n\\bibliographystyle{alpha}\n\n\\nocite{}\n\n\\bibliography{allrefs}\n\n\\appendix\n\n\n\n\\section{Omitted Proofs from Section~\\ref{sec:lb}}\\label{lbAppend}\n\n\\subsection{Proof of Lemma~\\ref{infBoundLem}}\n\nWe note that\n\n", "itemtype": "equation", "pos": 68281, "prevtext": "\nby symmetry where $A=A_i.$\nTo complete the proof, we need the following technical lemma:\n\\begin{lemma}\\label{SILem} We have that\n$ I(X:A) = O(k^3{\\epsilon}^4/(n^3m)).$\n\\end{lemma}\nMorally speaking, this lemma holds because if all the $c$'s were $1/n,$\nwe would get an expectation of roughly $O(k^2{\\epsilon}^4/(n^2m)),$\nby techniques from the last section.\nHowever, the possibility that $c=1/k$ adds sufficient amount of ``noise''\nto somewhat decrease the amount of available information.\nThe formal proof is deferred to Appendix \\ref{lbAppend}.\n\nCombining (\\ref{eqn:info-ub2}) and the above lemma, we obtain that\n\n", "index": 35, "text": "$$I(X:A_1,\\ldots,A_n)=    O(k^3{\\epsilon}^4/(n^2m) = o(1). $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"I(X:A_{1},\\ldots,A_{n})=O(k^{3}{\\epsilon}^{4}/(n^{2}m)=o(1).\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>O</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>k</mi><mn>3</mn></msup><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>o</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nA simple computation yields that\n\n", "itemtype": "equation", "pos": 71264, "prevtext": "\nThis completes the proof.\n\n\\subsection{Lower Bound for High Dimensional Independence Testing}\n\nWe need to show two lower bounds, \nnamely $\\sqrt{\\prod_{i=1}^d n_i}/\\epsilon^2$ and $n_i^{1/3}\\left(\\prod_{j=1}^d n_j\\right)^{1/3}/{\\epsilon}^{4/3}$. \nWe can obtain both of these from the lower bound constructions from the $2$-variable case. \nIn particular, for the first bound, we have shown that it takes this many samples to distinguish \nbetween the uniform distribution on $N=\\prod_{i=1}^d n_i$ inputs (which is a product distribution), \nfrom a distribution that assigns probability $(1\\pm {\\epsilon})/N$ randomly to each input \n(which once renormalized is probably $\\Omega({\\epsilon})$-far from being a product distribution). \nFor the latter bound, we think of $[n_1]\\times\\cdots\\times[n_d]$ as \n$[n_i]\\times([n_1]\\times\\cdots[n_{i-1}]\\times[n_{i+1}]\\times\\cdots\\times[n_d])$, \nand consider the lower bound construction for $2$-variable independence testing. \nIt then takes at least $n_i^{1/3}N^{1/3}/{\\epsilon}^{4/3}$ samples to reliably distinguish \na ``YES'' instance from a ``NO'' instance. Note that in a ``YES'' instance the first \nand second coordinates are independent and the distribution on the second coordinate is uniform. \nTherefore, in a ``YES'' instance we have a $d$-dimensional product distribution. \nOn the other hand, a ``NO'' instance is likely $\\Omega({\\epsilon})$-far from any distribution \nthat is a product distribution over just this partition of the coordinates, \nand therefore $\\Omega({\\epsilon})$ far from any $d$-dimensional product distribution. \nThis completes the proof.\n\n\\subsection{Lower Bound for $k$-Histograms}\n\nWe can use the above construction to show that our upper bound for $k$-histograms is in fact tight. \nIn particular, if we rewrite $[n]$ as $[k]\\times [n/k]$ \nand let the intervals be given by the subsets $[n/k]\\times \\{i\\}$ for $1\\leq i\\leq k$, \nwe need to show that $\\Omega(\\max(\\sqrt{n}/{\\epsilon}^2 ,n^{1/3}k^{1/3}/{\\epsilon}^{4/3}))$ samples \nare required from a distribution $p$ on $[k]\\times [n/k]$ \nto distinguish between the coordinates of $p$ being independent \nwith the second coordinate having the uniform distribution, \nand $p$ being ${\\epsilon}$-far from any such distribution. \nWe note that in the lower bound distributions given for each part of our lower bound constructions \nfor the independence tester, the ``YES'' distributions  all had uniform marginal over the second coordinate. \nTherefore, the same hard distributions give a lower bound for testing $k$-histograms \nof $\\Omega(\\max(\\sqrt{n}/{\\epsilon}^2,k^{2/3}(n/k)^{1/3}/{\\epsilon}^{4/3}))= \\Omega(\\max(\\sqrt{n}/{\\epsilon}^2 ,n^{1/3}k^{1/3}/{\\epsilon}^{4/3})).$\nThis completes the proof.\n\n\n\n\n\n\n\n\\bibliographystyle{alpha}\n\n\\nocite{}\n\n\\bibliography{allrefs}\n\n\\appendix\n\n\n\n\\section{Omitted Proofs from Section~\\ref{sec:lb}}\\label{lbAppend}\n\n\\subsection{Proof of Lemma~\\ref{infBoundLem}}\n\nWe note that\n\n", "index": 37, "text": "$$\nI(X:a) = \\sum_\\ell O\\left(\\Pr(a=\\ell)\\left(1-\\frac{\\Pr(a=\\ell|X=0)}{\\Pr(a=\\ell|X=1)}\\right)^2 \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"I(X:a)=\\sum_{\\ell}O\\left(\\Pr(a=\\ell)\\left(1-\\frac{\\Pr(a=\\ell|X=0)}{\\Pr(a=\\ell|%&#10;X=1)}\\right)^{2}\\right).\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi mathvariant=\"normal\">\u2113</mi></munder><mi>O</mi><mrow><mo>(</mo><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow><msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 71406, "prevtext": "\nA simple computation yields that\n\n", "index": 39, "text": "$$\n\\Pr(a=\\ell|X=0) = e^{-k/mn}\\frac{(k/mn)^\\ell}{\\ell!},$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\Pr(a=\\ell|X=0)=e^{-k/mn}\\frac{(k/mn)^{\\ell}}{\\ell!},\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup><mrow><mi mathvariant=\"normal\">\u2113</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nWe condition based on the size of $k/mn$. First, we analyze the case that $k/mn \\leq 1$.\n\nExpanding the above out as a Taylor series in $\\epsilon$ we note that the odd degree terms cancel. Therefore, we can see that if $\\ell\\leq 2$\n\n", "itemtype": "equation", "pos": 71465, "prevtext": "\n\n", "index": 41, "text": "$$\\Pr(a=\\ell|X=1) = \\left(e^{-k/mn}\\frac{(k/mn)^\\ell}{\\ell!}\\right) \\left(\\frac{e^{-k\\epsilon/mn}(1+\\epsilon)^\\ell+e^{k\\epsilon/mn}(1-\\epsilon)^\\ell}{2} \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\Pr(a=\\ell|X=1)=\\left(e^{-k/mn}\\frac{(k/mn)^{\\ell}}{\\ell!}\\right)\\left(\\frac{e%&#10;^{-k\\epsilon/mn}(1+\\epsilon)^{\\ell}+e^{k\\epsilon/mn}(1-\\epsilon)^{\\ell}}{2}%&#10;\\right).\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><mfrac><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup><mrow><mi mathvariant=\"normal\">\u2113</mi><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow><mo>+</mo><mrow><msup><mi>e</mi><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow></mrow><mn>2</mn></mfrac><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nand for $2\\epsilon^{-1} \\geq \\ell \\geq 2$,\n\n", "itemtype": "equation", "pos": 71862, "prevtext": "\nWe condition based on the size of $k/mn$. First, we analyze the case that $k/mn \\leq 1$.\n\nExpanding the above out as a Taylor series in $\\epsilon$ we note that the odd degree terms cancel. Therefore, we can see that if $\\ell\\leq 2$\n\n", "index": 43, "text": "\\begin{equation}\\label{smallwtEqn}\n\\left(\\frac{e^{-k\\epsilon/mn}(1+\\epsilon)^\\ell+e^{k\\epsilon/mn}(1-\\epsilon)^\\ell}{2} \\right) = 1+ O\\left(\\epsilon^2 (k/mn)^{2-\\ell} \\right),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\left(\\frac{e^{-k\\epsilon/mn}(1+\\epsilon)^{\\ell}+e^{k\\epsilon/mn}(1-\\epsilon)^%&#10;{\\ell}}{2}\\right)=1+O\\left(\\epsilon^{2}(k/mn)^{2-\\ell}\\right),\" display=\"block\"><mrow><mrow><mrow><mo>(</mo><mfrac><mrow><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow><mo>+</mo><mrow><msup><mi>e</mi><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow></mrow><mn>2</mn></mfrac><mo>)</mo></mrow><mo>=</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>2</mn><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nHence, we have that\n\n", "itemtype": "equation", "pos": 72096, "prevtext": "\nand for $2\\epsilon^{-1} \\geq \\ell \\geq 2$,\n\n", "index": 45, "text": "$$\n\\left(\\frac{e^{-k\\epsilon/mn}(1+\\epsilon)^\\ell+e^{k\\epsilon/mn}(1-\\epsilon)^\\ell}{2} \\right) = 1+ O\\left(\\epsilon^2 \\ell^2 \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\left(\\frac{e^{-k\\epsilon/mn}(1+\\epsilon)^{\\ell}+e^{k\\epsilon/mn}(1-\\epsilon)^%&#10;{\\ell}}{2}\\right)=1+O\\left(\\epsilon^{2}\\ell^{2}\\right).\" display=\"block\"><mrow><mrow><mrow><mo>(</mo><mfrac><mrow><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow><mo>+</mo><mrow><msup><mi>e</mi><mrow><mrow><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow></mrow><mn>2</mn></mfrac><mo>)</mo></mrow><mo>=</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u2113</mi><mn>2</mn></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nwhere in the last step we use that ${\\mathbb{E}}[a(a-1)+a(a-1)(a-2)(a-3)]=(k/mn)^2+(k/mn)^4$,\nand the last term is analyzed by case analysis based on whether or not $\\epsilon > (mn)^{-1/8}$.\n\nFor $\\lambda = k/mn \\geq 1$, we note that the probability that $|a-\\lambda| > \\sqrt{\\lambda}\\log(mn)$ is $o(1/(mn))$.\nSo, it suffices to consider only $\\ell$ at least this close to $\\lambda$. We note that for $\\ell$ in this range,\n\n", "itemtype": "equation", "pos": 72254, "prevtext": "\nHence, we have that\n\n", "index": 47, "text": "\\begin{align*}\nI(X:a) & \\leq O(\\epsilon^4 (k/mn)^2) + \\sum_{\\ell=2}^{2\\epsilon^{-1}} \\Pr(a=\\ell)O(\\epsilon^4 \\ell^4) + \\Pr(a>2\\epsilon^{-1})\\\\\n& = O(\\epsilon^4(k/mn)^2) + O(\\epsilon^4{\\mathbb{E}}[a(a-1)+a(a-1)(a-2)(a-3)]) + (\\epsilon k/(mn))^{1/\\epsilon}\\\\\n& = O(\\epsilon^4(k/mn)^2),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle I(X:a)\" display=\"inline\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq O(\\epsilon^{4}(k/mn)^{2})+\\sum_{\\ell=2}^{2\\epsilon^{-1}}\\Pr(%&#10;a=\\ell)O(\\epsilon^{4}\\ell^{4})+\\Pr(a&gt;2\\epsilon^{-1})\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mn>2</mn></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03f5</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></munderover></mstyle><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u2113</mi><mn>4</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>&gt;</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03f5</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=O(\\epsilon^{4}(k/mn)^{2})+O(\\epsilon^{4}{\\mathbb{E}}[a(a-1)+a(a-%&#10;1)(a-2)(a-3)])+(\\epsilon k/(mn))^{1/\\epsilon}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>-</mo><mn>3</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03f5</mi><mo>\u2062</mo><mi>k</mi></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mi>\u03f5</mi></mrow></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=O(\\epsilon^{4}(k/mn)^{2}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>m</mi></mrow><mo>\u2062</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis implies that\n\n", "itemtype": "equation", "pos": 72974, "prevtext": "\nwhere in the last step we use that ${\\mathbb{E}}[a(a-1)+a(a-1)(a-2)(a-3)]=(k/mn)^2+(k/mn)^4$,\nand the last term is analyzed by case analysis based on whether or not $\\epsilon > (mn)^{-1/8}$.\n\nFor $\\lambda = k/mn \\geq 1$, we note that the probability that $|a-\\lambda| > \\sqrt{\\lambda}\\log(mn)$ is $o(1/(mn))$.\nSo, it suffices to consider only $\\ell$ at least this close to $\\lambda$. We note that for $\\ell$ in this range,\n\n", "index": 49, "text": "$$\ne^{\\pm \\lambda \\epsilon}(1 \\mp \\epsilon)^\\ell = \\exp(\\pm\\epsilon(\\lambda - \\ell) + O(\\lambda \\epsilon^2)) = 1 \\pm \\epsilon(\\lambda -\\ell) + O(\\lambda \\epsilon^2).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"e^{\\pm\\lambda\\epsilon}(1\\mp\\epsilon)^{\\ell}=\\exp(\\pm\\epsilon(\\lambda-\\ell)+O(%&#10;\\lambda\\epsilon^{2}))=1\\pm\\epsilon(\\lambda-\\ell)+O(\\lambda\\epsilon^{2}).\" display=\"block\"><mrow><mrow><mrow><msup><mi>e</mi><mrow><mo>\u00b1</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2213</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">\u2113</mi></msup></mrow><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo>\u00b1</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mn>1</mn><mo>\u00b1</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nwhich completes the proof.\n\n\n\n\\subsection{Proof of Lemma \\ref{SILem}}\nAs before,\n\n", "itemtype": "equation", "pos": 73161, "prevtext": "\nThis implies that\n\n", "index": 51, "text": "\\begin{equation}\\label{poissonerrEqn}\n\\left(1-\\frac{\\Pr(a=\\ell|X=0)}{\\Pr(a=\\ell|X=1)}\\right)^2 = O(\\lambda^2 \\epsilon^4),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\left(1-\\frac{\\Pr(a=\\ell|X=0)}{\\Pr(a=\\ell|X=1)}\\right)^{2}=O(\\lambda^{2}%&#10;\\epsilon^{4}),\" display=\"block\"><mrow><mrow><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03bb</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>4</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nWe break this sum up into pieces based on whether or not $|v|_1 \\geq 2$.\n\nIf $|v|_1 < 2$, note that $\\Pr(A=v|c_i=1/k,X=0) = \\Pr(A=v|c_i=1/k,X=1).$ Therefore,\n\n", "itemtype": "equation", "pos": 73379, "prevtext": "\nwhich completes the proof.\n\n\n\n\\subsection{Proof of Lemma \\ref{SILem}}\nAs before,\n\n", "index": 53, "text": "$$\nI(X:A) = \\sum_v O\\left(\\Pr(A=v)\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)^2 \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"I(X:A)=\\sum_{v}O\\left(\\Pr(A=v)\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)%&#10;^{2}\\right).\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>v</mi></munder><mi>O</mi><mrow><mo>(</mo><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>=</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nHence, the contribution coming from all such $v$ is at most\n\n", "itemtype": "equation", "pos": 73634, "prevtext": "\nWe break this sum up into pieces based on whether or not $|v|_1 \\geq 2$.\n\nIf $|v|_1 < 2$, note that $\\Pr(A=v|c_i=1/k,X=0) = \\Pr(A=v|c_i=1/k,X=1).$ Therefore,\n\n", "index": 55, "text": "\\begin{align*}\n\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)^2 \\leq \\left(1-\\frac{\\Pr(A=v|X=0,c_i=1/n)}{\\Pr(A=v|X=1,c_i=1/n)}\\right)^2.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)^{2}\\leq\\left(1-%&#10;\\frac{\\Pr(A=v|X=0,c_{i}=1/n)}{\\Pr(A=v|X=1,c_{i}=1/n)}\\right)^{2}.\" display=\"inline\"><mrow><mrow><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2264</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nNote that the $a_{i,j}$ are actually independent of each other conditionally on both $X$ and $c_i$. We have by Equation (\\ref{smallwtEqn}) that\n\n", "itemtype": "equation", "pos": 73846, "prevtext": "\nHence, the contribution coming from all such $v$ is at most\n\n", "index": 57, "text": "$$\n\\Pr(|A|_1=1|c_i=1/n))\\max_{|v|_1=1} O\\left(1-\\frac{\\Pr(A=v|X=0,c_i=1/n)}{\\Pr(A=v|X=1,c_i=1/n)}\\right)^2+O\\left(1-\\frac{\\Pr(A=0|X=0,c_i=1/n)}{\\Pr(A=0|X=1,c_i=1/n)}\\right)^2.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\Pr(|A|_{1}=1|c_{i}=1/n))\\max_{|v|_{1}=1}O\\left(1-\\frac{\\Pr(A=v|X=0,c_{i}=1/n)%&#10;}{\\Pr(A=v|X=1,c_{i}=1/n)}\\right)^{2}+O\\left(1-\\frac{\\Pr(A=0|X=0,c_{i}=1/n)}{%&#10;\\Pr(A=0|X=1,c_{i}=1/n)}\\right)^{2}.\" display=\"block\"><mrow><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">|</mo><mi>A</mi><msub><mo stretchy=\"false\">|</mo><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo><mi>max</mi><msub><mi/><mrow><msub><mrow><mo stretchy=\"false\">|</mo><mi>v</mi><mo stretchy=\"false\">|</mo></mrow><mn>1</mn></msub><mo>=</mo><mn>1</mn></mrow></msub><mi>O</mi><mo>(</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo><msup><mi/><mn>2</mn></msup><mo>+</mo><mi>O</mi><mo>(</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo><msup><mi/><mn>2</mn></msup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nif $|v|_1=1,$ and\n\n", "itemtype": "equation", "pos": 74169, "prevtext": "\nNote that the $a_{i,j}$ are actually independent of each other conditionally on both $X$ and $c_i$. We have by Equation (\\ref{smallwtEqn}) that\n\n", "index": 59, "text": "$$\n\\frac{\\Pr(A=v|X=0,c_i=1/n)}{\\Pr(A=v|X=1,c_i=1/n)} = \\exp\\left(O(\\epsilon^2k^2 n^{-2} m^{-1}+\\epsilon^2k n^{-1}m^{-1}) \\right),\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Pr(A=v|X=0,c_{i}=1/n)}{\\Pr(A=v|X=1,c_{i}=1/n)}=\\exp\\left(O(\\epsilon^{2}%&#10;k^{2}n^{-2}m^{-1}+\\epsilon^{2}kn^{-1}m^{-1})\\right),\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>k</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>+</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nWe have that $\\Pr(|A|_1=1|c_i=1/n)$ is the probability that a Poisson statistic with parameter $k/n(1+O(\\epsilon))$ gives $1$, which is $O(k/n)$.\nTherefore, the contribution to $I(X:A)$ coming from these terms is\n\n", "itemtype": "equation", "pos": 74320, "prevtext": "\nif $|v|_1=1,$ and\n\n", "index": 61, "text": "$$\n\\frac{\\Pr(A=0|X=0,c_i=1/n)}{\\Pr(A=0|X=1,c_i=1/n)} = \\exp\\left(O(\\epsilon^2k^2 n^{-2} m^{-1}) \\right).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\Pr(A=0|X=0,c_{i}=1/n)}{\\Pr(A=0|X=1,c_{i}=1/n)}=\\exp\\left(O(\\epsilon^{2}%&#10;k^{2}n^{-2}m^{-1})\\right).\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\u03f5</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>k</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nNext, we consider the contribution coming from terms with $|v|_1\\geq 2.$ \nWe note that \n", "itemtype": "equation", "pos": 74641, "prevtext": "\nWe have that $\\Pr(|A|_1=1|c_i=1/n)$ is the probability that a Poisson statistic with parameter $k/n(1+O(\\epsilon))$ gives $1$, which is $O(k/n)$.\nTherefore, the contribution to $I(X:A)$ coming from these terms is\n\n", "index": 63, "text": "$$\nO(\\epsilon^4 k^4 n^{-4} m^{-2}+\\epsilon^4 k^3 n^{-3} m^{-2}) = o(k n^{-2}) + o(n^{-1}) = o(n^{-1}).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"O(\\epsilon^{4}k^{4}n^{-4}m^{-2}+\\epsilon^{4}k^{3}n^{-3}m^{-2})=o(kn^{-2})+o(n^%&#10;{-1})=o(n^{-1}).\" display=\"block\"><mrow><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mi>k</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>4</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup></mrow><mo>+</mo><mrow><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>\u2062</mo><msup><mi>k</mi><mn>3</mn></msup><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>3</mn></mrow></msup><mo>\u2062</mo><msup><mi>m</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>\u2062</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>o</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": " is (for either $x=0$ or $x=1$)\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nNext, we consider the contribution coming from terms with $|v|_1\\geq 2.$ \nWe note that \n", "index": 65, "text": "$$\\Pr(A=v,c_i=1/k|X=x)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"\\Pr(A=v,c_{i}=1/k|X=x)\" display=\"block\"><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nHowever, $\\Pr(A=v,c_i=1/n|X=x)$ is at most\n\n", "itemtype": "equation", "pos": 74890, "prevtext": " is (for either $x=0$ or $x=1$)\n\n", "index": 67, "text": "$$\n\\frac{ke^{-1}}{n}(m)^{-|v|_1}\\prod_{i=1}^m \\frac{1}{v_i!}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex37.m1\" class=\"ltx_Math\" alttext=\"\\frac{ke^{-1}}{n}(m)^{-|v|_{1}}\\prod_{i=1}^{m}\\frac{1}{v_{i}!}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mi>k</mi><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mi>n</mi></mfrac><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><msub><mrow><mo stretchy=\"false\">|</mo><mi>v</mi><mo stretchy=\"false\">|</mo></mrow><mn>1</mn></msub></mrow></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nThis is at most $2k/n$ times $\\Pr(A=v,c_i=1/k|X=x)$. Therefore, we have that\n\n", "itemtype": "equation", "pos": 74998, "prevtext": "\nHowever, $\\Pr(A=v,c_i=1/n|X=x)$ is at most\n\n", "index": 69, "text": "$$\n((1+\\epsilon)k/nm)^{|v|_1}\\prod_{i=1}^m \\frac{1}{v_i!}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex38.m1\" class=\"ltx_Math\" alttext=\"((1+\\epsilon)k/nm)^{|v|_{1}}\\prod_{i=1}^{m}\\frac{1}{v_{i}!}.\" display=\"block\"><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>k</mi></mrow><mo>/</mo><mi>n</mi></mrow><mo>\u2062</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mrow><mo stretchy=\"false\">|</mo><mi>v</mi><mo stretchy=\"false\">|</mo></mrow><mn>1</mn></msub></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nNote that\n\n", "itemtype": "equation", "pos": 75137, "prevtext": "\nThis is at most $2k/n$ times $\\Pr(A=v,c_i=1/k|X=x)$. Therefore, we have that\n\n", "index": 71, "text": "$$\n\\Pr(A=v)\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)^2  = O(k/n)\\Pr(A=v|c_i=1/n)\\left(1-\\frac{\\Pr(A=v|X=0,c_i=1/n)}{\\Pr(A=v|X=1,c_i=1/n)}\\right)^2.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex39.m1\" class=\"ltx_Math\" alttext=\"\\Pr(A=v)\\left(1-\\frac{\\Pr(A=v|X=0)}{\\Pr(A=v|X=1)}\\right)^{2}=O(k/n)\\Pr(A=v|c_{%&#10;i}=1/n)\\left(1-\\frac{\\Pr(A=v|X=0,c_{i}=1/n)}{\\Pr(A=v|X=1,c_{i}=1/n)}\\right)^{2}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>/</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 75305, "prevtext": "\nNote that\n\n", "index": 73, "text": "$$\n\\Pr(A=v|X=0,c_i=1/n) = e^{-k/n}(k/nm)^{|v|_1}\\prod_{i=1}^m \\frac{1}{v_i!}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex40.m1\" class=\"ltx_Math\" alttext=\"\\Pr(A=v|X=0,c_{i}=1/n)=e^{-k/n}(k/nm)^{|v|_{1}}\\prod_{i=1}^{m}\\frac{1}{v_{i}!}\" display=\"block\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>k</mi><mo>/</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>n</mi></mrow><mo>\u2062</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mrow><mo stretchy=\"false\">|</mo><mi>v</mi><mo stretchy=\"false\">|</mo></mrow><mn>1</mn></msub></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nTherefore,\n\n", "itemtype": "equation", "pos": 75389, "prevtext": "\nand\n\n", "index": 75, "text": "$$\n\\Pr(A=v|X=1,c_i=1/n) \\geq e^{-k(1+\\epsilon)/n}(k/nm)^{|v|_1}\\prod_{i=1}^m \\frac{1}{v_i!} \\gg \\Pr(A=v|X=0,c_i=1/n).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex41.m1\" class=\"ltx_Math\" alttext=\"\\Pr(A=v|X=1,c_{i}=1/n)\\geq e^{-k(1+\\epsilon)/n}(k/nm)^{|v|_{1}}\\prod_{i=1}^{m}%&#10;\\frac{1}{v_{i}!}\\gg\\Pr(A=v|X=0,c_{i}=1/n).\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>n</mi></mrow></mrow></msup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>k</mi><mo>/</mo><mi>n</mi></mrow><mo>\u2062</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow><msub><mrow><mo stretchy=\"false\">|</mo><mi>v</mi><mo stretchy=\"false\">|</mo></mrow><mn>1</mn></msub></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace=\"0pt\" rspace=\"3.5pt\">!</mo></mrow></mfrac></mrow></mrow><mo>\u226b</mo><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nFinally, we have that\n\n", "itemtype": "equation", "pos": 75521, "prevtext": "\nTherefore,\n\n", "index": 77, "text": "$$\n\\sum_v \\Pr(A=v|c_i=1/n)\\left(1-\\frac{\\Pr(A=v|X=0,c_i=1/n)}{\\Pr(A=v|X=1,c_i=1/n)}\\right)^2 = \\Theta(I(A:X|c_i=1/n)).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex42.m1\" class=\"ltx_Math\" alttext=\"\\sum_{v}\\Pr(A=v|c_{i}=1/n)\\left(1-\\frac{\\Pr(A=v|X=0,c_{i}=1/n)}{\\Pr(A=v|X=1,c_%&#10;{i}=1/n)}\\right)^{2}=\\Theta(I(A:X|c_{i}=1/n)).\" display=\"block\"><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>v</mi></munder><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>=</mo><mi>v</mi><mo stretchy=\"false\">|</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>A</mi><mo>=</mo><mi>v</mi></mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>)</mo></mrow><mn>2</mn></msup><mo>=</mo><mi mathvariant=\"normal\">\u0398</mi><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>:</mo><mi>X</mi><mo stretchy=\"false\">|</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nby Equation (\\ref{poissonerrEqn}). This means that the contribution from these terms to $I(X:A)$ is at most\n\n", "itemtype": "equation", "pos": 75665, "prevtext": "\nFinally, we have that\n\n", "index": 79, "text": "$$\nI(X:A|c_i=1/n)\\leq \\sum_{j=1}^m I(X:a_{i,j}|c_i=1/n) = O(k^2{\\epsilon}^4/(n^2 m)) \\;,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex43.m1\" class=\"ltx_Math\" alttext=\"I(X:A|c_{i}=1/n)\\leq\\sum_{j=1}^{m}I(X:a_{i,j}|c_{i}=1/n)=O(k^{2}{\\epsilon}^{4}%&#10;/(n^{2}m))\\;,\" display=\"block\"><mrow><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><mi>A</mi><mo stretchy=\"false\">|</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2264</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>I</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>:</mo><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>O</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>k</mi><mn>2</mn></msup><msup><mi>\u03f5</mi><mn>4</mn></msup><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05557.tex", "nexttext": "\nand the proof is complete.\n\n\n\n\n", "itemtype": "equation", "pos": 75865, "prevtext": "\nby Equation (\\ref{poissonerrEqn}). This means that the contribution from these terms to $I(X:A)$ is at most\n\n", "index": 81, "text": "$$\nO(k^3{\\epsilon}^4/(n^3 m)) \\;,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex44.m1\" class=\"ltx_Math\" alttext=\"O(k^{3}{\\epsilon}^{4}/(n^{3}m))\\;,\" display=\"block\"><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>k</mi><mn>3</mn></msup><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>4</mn></msup></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>n</mi><mn>3</mn></msup><mo>\u2062</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]