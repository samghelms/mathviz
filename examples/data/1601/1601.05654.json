[{"file": "1601.05654.tex", "nexttext": "\n\n\nwhere ${\\mbox{\\boldmath ${x}$}}(t)\\in{\\mathbb{R}^{{D\\times 1}}}$ are  the hidden activations of the reservoir at time $t$, and $h(\\cdot)$ is a nonlinear function commonly chosen as the $\\tanh(\\cdot)$ function. Bias terms have been omitted in the formulation for the sake of clarity in notation.\n\nAccording to standard ESN methodology \\cite{Lukosevicius2009}, parameters  {\\mbox{\\boldmath ${v}$}} and {\\mbox{\\boldmath ${U}$}} in Eqs.~\\eqref{eq:esn_recursive_one}, \\eqref{eq:esn_recursive_two} are randomly\ngenerated\\footnote{The spectral radius of the reservoir's\nweight matrix {\\mbox{\\boldmath ${U}$}} is made $< 1$ to encourage {\\it the Echo State Property}.} and fixed. The only trainable parameters are the readout weights {\\mbox{\\boldmath ${w}$}}. \nTraining involves feeding at each time step $t$ an observation $y(t)$ and recording\nthe resulting activations ${\\mbox{\\boldmath ${x}$}}(t)$ row-wise into a matrix ${\\mbox{\\boldmath ${X}$}}\\in{\\mathbb{R}^{{T \\times D}}}$. Usually, some initial\nobservations are dismissed in order to ``washout\" \\cite{Lukosevicius2009} dependence on the initial arbitrary reservoir state (e.g. ${\\mbox{\\boldmath ${x}$}}(1)={\\mbox{\\boldmath ${0}$}}$). Given matrix ${\\mbox{\\boldmath ${X}$}}$, the following objective function is minimised:\n\n\n\n", "itemtype": "equation", "pos": 8086, "prevtext": "\n\n\\maketitle\n\n\n\n\\begin{abstract}\nWe present an approach for the visualisation of a set of time series that combines an echo state network with an autoencoder. For each time series in the dataset we train an echo state network, using a common and fixed reservoir of hidden neurons, and use the optimised readout weights as the new representation. Dimensionality reduction is then performed via an autoencoder on the readout weight representations. The crux of the work is to equip the autoencoder with a loss function that correctly interprets the reconstructed readout weights by associating them with a reconstruction error measured in the data space of sequences. This essentially amounts  to measuring the predictive performance that the reconstructed readout weights exhibit on their corresponding sequences when plugged back into the echo state network with the same fixed reservoir.\nWe demonstrate that the proposed visualisation framework can deal both with real valued sequences as well as\nbinary sequences. We derive magnification factors in order to analyse distance preservations and distortions in the visualisation space.\nThe versatility and advantages of the proposed method are demonstrated on datasets of time series that originate from  diverse domains.\n\\end{abstract}\n\n\n\n\\section{Introduction}\n\\label{sec:introduction}\n\n\n\nTime series\\footnote{We  interchangeably use the terms time series and sequence.} are sequences of observations that exhibit  short or long term\n dependencies between them in time. These dependencies can be thought of as  manifestations of a latent regime (e.g.~natural law) governing the behaviour of the time series. Machine learning approaches  designed to deal with data of a vectorial nature have no knowledge of such temporal dependencies. By introducing a model that accounts for  temporal behaviour,  algorithms  can become ``aware'' of the sequential nature of the data and make the most of the available information.\n\n\nEcho state networks (ESNs) \\cite{Lukosevicius2009} are discrete time recurrent neural networks that have emerged as a popular method to capture the latent regime underlying a time series.\nESNs have the great advantage that the hidden part, the reservoir of neurons,\nis fixed and only the output weights need to be trained. The ESN is thus essentially a linear model and so the output weights, also known as readout weights, can thus be easily optimised via least squares.\nThe processing of structured data has been a topic of research for a long time \\cite{Jaakkola1998, Jebara2004}. Regarding time series, recent attempts \\cite{Chatzis2011, Chen2013, Chen2014} have exploited the predictive capabilities of ESNs in regression and classification tasks. \nIn the unsupervised setting, the work in \\cite{Sperduti2013} suggests compressing a  linear state space model through a linear autoencoder in order to extract vectorial representations of structured data. The work in \\cite{Wang2012} considers the visualisation of  individual observations belonging to a {\\itshape single} sequence by temporally linking them using an ESN.\n\n\nIn this work, we  employ the ESN in the formulation of a dimensionality reduction algorithm for visualising a dataset of time series (we extend previous work presented in \\cite{Gianniotis2015}).\nGiven a fixed reservoir, the only free parameters in the ESN are in the readout weight vector which maps the state space to the sequence space.\nThus, an optimised (i.e.~trained) readout weight vector  uniquely addresses an instance of the ESN (always for the same fixed reservoir) that best predicts on  a given sequence. \nWe can also reason backwards: given an observed sequence, we can train the ESN (Section \\ref{sec:echo_state_networks}) and identify the readout weight vector that best predicts the given sequence.\nHence, each sequence in the dataset can be mapped to the readout weight vector that exhibits the best predictive performance on it.  These readout weight vectors in conjunction with the common and fixed reservoir, capture temporal features of their respective sequences. Representing sequences as weight vectors, constitutes the first part of our proposed approach (Section \\ref{sec:encoding}).\n\n\\begin{figure*}\n\\centering\n\\includegraphics[trim=0 0 0 0.2cm, width=0.90\\textwidth]{newfigures/overview.eps}\n\\caption{Sketch of proposed method. In a first stage, time series {\\mbox{\\boldmath ${y}$}} are cast to readout weights  ${\\mbox{\\boldmath ${w}$}}$ in the weight space (see Section \\ref{sec:encoding}). In a second stage, the autoencoder projects readout weights  ${\\mbox{\\boldmath ${w}$}}$ onto coordinates ${\\mbox{\\boldmath ${z}$}}$ residing in a two-dimensional space,\nand reconstructs them again as $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ (see Section \\ref{sec:esn_coupled}).\nBy multiplying with the state space, given by {\\mbox{\\boldmath ${X}$}}, we map the reconstructed readout weights $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ to the sequence space\nwhere reconstruction error is measured (see Eq.~\\eqref{eq:new_objective}).\n}\n\\label{fig:overview}\n\\end{figure*}\n\nThe second stage of our approach involves training an autoencoder  \\cite{Kramer1991} on the obtained readout weight vectors in order to induce a two-dimensional representation, the visualisation, at the bottleneck. At the heart of the autoencoder lies the  reconstruction error function  \nwhich drives the visualisation induced at the bottleneck.\nDuring training, the autoencoder receives readout weights as inputs, compresses them at the bottleneck, and outputs an approximate version of the inputs, the reconstructed readout weights.\nTypically, one would take as the reconstruction error function the $L_2$ norm between the original readout weights and reconstructed readout weights. In the proposed work, we equip the autoencoder with a different reconstruction function\nthat assesses how well the reconstructed readout weights still predict on the sequence that it represents.\nIf it predicts well, we deem it a good reconstruction; if it predicts poorly, we deem it a poor reconstruction\n(Section \\ref{sec:esn_coupled}). An overview of the proposed method is displayed in Fig.~\\ref{fig:overview}.\n\n\n\n\n\n\nIn Section \\ref{sec:experiments}, we show that the autoencoder with the proposed reconstruction error function is capable of interpreting similarities between time series better than other dimensionality reduction algorithms. In Section \\ref{sec:discussion}, we discuss \nthe possibility of alternative formulations of the proposed approach\nbefore concluding with some remarks on future work in Section \\ref{sec:conclusion}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Preliminary}\n\\label{sec:Preliminary}\n\n\n\nThis section introduces some notation and terminology while briefly reviewing ESNs and the autoencoder. \n\n\n\n\n\n\n\\subsection{Echo State Networks}\n\\label{sec:echo_state_networks}\n\n\n\nAn ESN is a discrete-time recurrent neural network with fading memory. It processes time series\ncomposed by a sequence of observations $y(t)\\in {\\mathbb{R}^{{}}}$  over time $t$ that we  denote here by ${\\mbox{\\boldmath ${y}$}}=(y(1),\\dots,y(T))$, where $T$ is the length\\footnote{In general, each sequence can have its own length $T_n$. For ease of exposition, here all sequences have the same $T$.} of the sequences. Hence ${\\mbox{\\boldmath ${y}$}} \\in {\\mathbb{R}^{{T \\times 1}}}$.\nGiven an input $y(t)$, the task of the ESN is to make a prediction $\\hat{y}(t+1)$ for the observation $y(t+1)$ in the next time step.\nSimilarly to a feedforward neural network, the ESN comprises an input layer with weights ${\\mbox{\\boldmath ${v}$}}\\in{\\mathbb{R}^{{D\\times 1}}}$, a hidden layer with weights ${\\mbox{\\boldmath ${U}$}}\\in{\\mathbb{R}^{{D\\times D}}}$ (hence $D$ is the size of the reservoir) and an output layer with weights ${\\mbox{\\boldmath ${w}$}}\\in{\\mathbb{R}^{{D\\times 1}}}$, the latter weights ${\\mbox{\\boldmath ${w}$}}$ also known as readout weights. \nHowever, in contrast to feedforward networks, ESNs equip the hidden neurons with feedback connections. The operation of an ESN is specified by the equations:\n\n\n\n", "index": 1, "text": "\\begin{align}\n{\\mbox{\\boldmath ${x}$}}(t+1) &=h({\\mbox{\\boldmath ${U}$}}{\\mbox{\\boldmath ${x}$}}(t) + {\\mbox{\\boldmath ${v}$}} y(t)) \\ , \\label{eq:esn_recursive_one} \\\\\n\\hat{y}(t+1) &=  {\\mbox{\\boldmath ${w}$}}^T {\\mbox{\\boldmath ${x}$}}(t+1) \\label{eq:esn_recursive_two} \\ ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mbox{\\boldmath${x}$}}(t+1)\" display=\"inline\"><mrow><mi>\ud835\udc99</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h({\\mbox{\\boldmath${U}$}}{\\mbox{\\boldmath${x}$}}(t)+{\\mbox{%&#10;\\boldmath${v}$}}y(t))\\ ,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mi>\ud835\udc7c</mi><mi>\ud835\udc99</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udc97</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{y}(t+1)\" display=\"inline\"><mrow><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mbox{\\boldmath${w}$}}^{T}{\\mbox{\\boldmath${x}$}}(t+1)\\ ,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mi>\ud835\udc98</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc99</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\n\nThe above objective\n\ncan be supplemented by a regularisation term and so the combined objective is $\\ell({\\mbox{\\boldmath ${w}$}}) + \\mu^2 \\|{\\mbox{\\boldmath ${w}$}}\\|^2 $. \n\nThe combined objective can be exactly minimised by solving the pertaining least squares problem and obtaining\n${\\mbox{\\boldmath ${w}$}} = ({\\mbox{\\boldmath ${X}$}}^T {\\mbox{\\boldmath ${X}$}}+ \\mu^2 {\\mbox{\\boldmath ${I}$}}_D)^{-1} {\\mbox{\\boldmath ${X}$}}^T  {\\mbox{\\boldmath ${y}$}} $ as the solution,\nwhere ${\\mbox{\\boldmath ${I}$}}_D$ is the $D\\times D$ identity matrix. \nGiven this result, we introduce function $g({\\mbox{\\boldmath ${y}$}})$ that maps a given time series to the optimal readout weights:\n\n\n\n", "itemtype": "equation", "pos": 9651, "prevtext": "\n\n\nwhere ${\\mbox{\\boldmath ${x}$}}(t)\\in{\\mathbb{R}^{{D\\times 1}}}$ are  the hidden activations of the reservoir at time $t$, and $h(\\cdot)$ is a nonlinear function commonly chosen as the $\\tanh(\\cdot)$ function. Bias terms have been omitted in the formulation for the sake of clarity in notation.\n\nAccording to standard ESN methodology \\cite{Lukosevicius2009}, parameters  {\\mbox{\\boldmath ${v}$}} and {\\mbox{\\boldmath ${U}$}} in Eqs.~\\eqref{eq:esn_recursive_one}, \\eqref{eq:esn_recursive_two} are randomly\ngenerated\\footnote{The spectral radius of the reservoir's\nweight matrix {\\mbox{\\boldmath ${U}$}} is made $< 1$ to encourage {\\it the Echo State Property}.} and fixed. The only trainable parameters are the readout weights {\\mbox{\\boldmath ${w}$}}. \nTraining involves feeding at each time step $t$ an observation $y(t)$ and recording\nthe resulting activations ${\\mbox{\\boldmath ${x}$}}(t)$ row-wise into a matrix ${\\mbox{\\boldmath ${X}$}}\\in{\\mathbb{R}^{{T \\times D}}}$. Usually, some initial\nobservations are dismissed in order to ``washout\" \\cite{Lukosevicius2009} dependence on the initial arbitrary reservoir state (e.g. ${\\mbox{\\boldmath ${x}$}}(1)={\\mbox{\\boldmath ${0}$}}$). Given matrix ${\\mbox{\\boldmath ${X}$}}$, the following objective function is minimised:\n\n\n\n", "index": 3, "text": "\\begin{equation}\n\\ell({\\mbox{\\boldmath ${w}$}}) =\\|{\\mbox{\\boldmath ${X}$}}{\\mbox{\\boldmath ${w}$}} - {\\mbox{\\boldmath ${y}$}}\\|^2 \\ .\n\\label{eq:ESN_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\ell({\\mbox{\\boldmath${w}$}})=\\|{\\mbox{\\boldmath${X}$}}{\\mbox{\\boldmath${w}$}}%&#10;-{\\mbox{\\boldmath${y}$}}\\|^{2}\\ .\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+5pt\"><msup><mrow><mo>\u2225</mo><mrow><mrow><mi>\ud835\udc7f</mi><mi>\ud835\udc98</mi></mrow><mo>-</mo><mi>\ud835\udc9a</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\n\\subsection{Deterministically Constructed Echo State Networks}\n\n\\label{sec:deterministic}\n\n\n\nIn the original formulation of the ESN \\cite{Lukosevicius2009} the weights in {\\mbox{\\boldmath ${v}$}} and {\\mbox{\\boldmath ${U}$}} are generated stochastically and so are the connections between the hidden neurons in the reservoir. This makes the training and use of the ESN dependent on random initialisations. In order to avoid this source of randomness, we make use of a class of ESNs that are constructed in a deterministic fashion \\cite{Rodan2011}.\n\n\nDeterministic ESNs make several simplifications over standard ESNs. All entries in  ${\\mbox{\\boldmath ${v}$}}$ have the same absolute value of a single scalar parameter $v>0$.\nThe signs of the entries in ${\\mbox{\\boldmath ${v}$}}$ are deterministically generated by an aperiodic sequence: e.g. a pseudorandom binary sequence (coin flips), with  outcomes $0$ and $1$ corresponding to $-$ and $+$ respectively.\nSimilarly, the entries in ${\\mbox{\\boldmath ${U}$}}$ are  parametrised by a single scalar $u>0$. As opposed to random connectivity, deterministic ESNs impose a fixed regular topology on the hidden neurons in the reservoir. Amongst possible choices, one can arrange the neurons in a cycle. A cyclic arrangement imposes the following structure on  ${\\mbox{\\boldmath ${U}$}}$: the only nonzero entries in ${\\mbox{\\boldmath ${U}$}}$ are on the lower sub-diagonal\n${\\mbox{\\boldmath ${U}$}}_{i+1,i} = u$, and at the upper-right corner ${\\mbox{\\boldmath ${U}$}}_{1,D} = u$.\nAn illustration of a cyclic deterministic ESN is shown in Fig.~\\ref{fig:cyclic_esn}.\n\n\nIn this work we employ deterministic ESNs with a cyclic connectivity. Deterministic ESNs have three degrees of freedom: the reservoir size $D$, the input weight $v$ and  reservoir weight $u$. Hence, the triple $(D,v,u)$ completely specifies an ESN.  \nIt has been shown empirically and theoretically (memory capacity) \\cite{Rodan2011}   that deterministic ESNs perform up to par with their stochastic counterparts.\nTraining of a deterministic ESN is performed in exactly  the same fashion as in  stochastically constructed ESNs  using the objective $\\ell({\\mbox{\\boldmath ${w}$}})$ in Eq.~\\eqref{eq:ESN_objective}.\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.22\\textwidth]{newfigures/cyclic_esn.eps}\n\\caption{Deterministic ESN with cyclic architecture, see Section \\ref{sec:deterministic}. Circles denote  neurons and arrows connections between neurons.\nAll input neurons connect to the hidden neurons, and all hidden neurons connect to the output neurons. Hidden neurons are connected in a cyclic fashion to each other. All input weights have the same absolute value $v$, and the sign is determined by a deterministic aperiodic sequence. The hidden reservoir weights are fixed to the same scalar $u$. The readout weights ${\\mbox{\\boldmath ${w}$}}$ are the only adaptable part of the ESN. }\n\\label{fig:cyclic_esn}\n\\end{figure}\n\n\n\n\n\\subsection{Autoencoder}\n\\label{sec:autoencoder}\n\n\n\nThe autoencoder \\cite{Kramer1991} is a feedforward neural network that defines\n a three hidden layer architecture\\footnote{To be perfectly precise, we use what is widely considered the standard autoencoder specified  in  \\cite[Sec. 12.4.2]{Bishop}).} with the middle layer, the  ``bottleneck'', being smaller than the others in terms of the number of neurons denoted by $Q$. \nThe autoencoder learns  an identity mapping by training on targets identical to the inputs.\nLearning is hampered by the bottleneck that forces the autoencoder\nto reduce the dimensionality of the inputs, and hence the output is only an\napproximate reconstruction of the input.  \n\n\nGiven general vectors ${\\mbox{\\boldmath ${s}$}}$, we want to reduce them to a $Q$-dimensional representation. \nThe autoencoder is the composition of an encoding $f_{enc}$ and a decoding $f_{dec}$\nfunction. Encoding maps inputs ${\\mbox{\\boldmath ${s}$}}$ to low-dimensional compressed versions, $f_{enc}({\\mbox{\\boldmath ${s}$}}) = {\\mbox{\\boldmath ${z}$}} \\in{\\mathbb{R}^{{Q}}}$, while decoding  maps approximately back to the inputs, $f_{dec}({\\mbox{\\boldmath ${z}$}}) = \\tilde{{\\mbox{\\boldmath ${s}$}}}$. The complete autoencoder is the function $f({\\mbox{\\boldmath ${s}$}};{\\mbox{\\boldmath ${\\theta}$}}) = f_{dec}(f_{enc}({\\mbox{\\boldmath ${s}$}})) = \\tilde{{\\mbox{\\boldmath ${s}$}}}$, where {\\mbox{\\boldmath ${\\theta}$}} are the weights of the autoencoder. Training the autoencoder involves minimising the $L_2$ norm between  $N$ given vectors ${\\mbox{\\boldmath ${s}$}}$  and their reconstructions:\n\n\n", "itemtype": "equation", "pos": 10513, "prevtext": "\n\n\nThe above objective\n\ncan be supplemented by a regularisation term and so the combined objective is $\\ell({\\mbox{\\boldmath ${w}$}}) + \\mu^2 \\|{\\mbox{\\boldmath ${w}$}}\\|^2 $. \n\nThe combined objective can be exactly minimised by solving the pertaining least squares problem and obtaining\n${\\mbox{\\boldmath ${w}$}} = ({\\mbox{\\boldmath ${X}$}}^T {\\mbox{\\boldmath ${X}$}}+ \\mu^2 {\\mbox{\\boldmath ${I}$}}_D)^{-1} {\\mbox{\\boldmath ${X}$}}^T  {\\mbox{\\boldmath ${y}$}} $ as the solution,\nwhere ${\\mbox{\\boldmath ${I}$}}_D$ is the $D\\times D$ identity matrix. \nGiven this result, we introduce function $g({\\mbox{\\boldmath ${y}$}})$ that maps a given time series to the optimal readout weights:\n\n\n\n", "index": 5, "text": "\\begin{equation}\ng({\\mbox{\\boldmath ${y}$}}) = ({\\mbox{\\boldmath ${X}$}}^T {\\mbox{\\boldmath ${X}$}} + \\mu^2 {\\mbox{\\boldmath ${I}$}}_D)^{-1} {\\mbox{\\boldmath ${X}$}}^T  {\\mbox{\\boldmath ${y}$}} = {\\mbox{\\boldmath ${w}$}} \\ .\n\\label{eq:esn_project}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"g({\\mbox{\\boldmath${y}$}})=({\\mbox{\\boldmath${X}$}}^{T}{\\mbox{\\boldmath${X}$}}%&#10;+\\mu^{2}{\\mbox{\\boldmath${I}$}}_{D})^{-1}{\\mbox{\\boldmath${X}$}}^{T}{\\mbox{%&#10;\\boldmath${y}$}}={\\mbox{\\boldmath${w}$}}\\ .\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc7f</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc7f</mi></mrow><mo>+</mo><mrow><msup><mi>\u03bc</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>\ud835\udc70</mi><mi>D</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc7f</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc9a</mi></mrow><mo>=</mo><mpadded width=\"+5pt\"><mi>\ud835\udc98</mi></mpadded></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Model Formulation}\n\\label{model_formulation}\n\n\nThe proposed approach consists of two stages. In Section \\ref{sec:encoding}, we\ndiscuss how time series ${\\mbox{\\boldmath ${y}$}}$ are embedded in the space of readout weight vectors ${\\mbox{\\boldmath ${w}$}}$.\nSection \\ref{sec:esn_coupled} discusses how an autoencoder with a modified reconstruction function\nis applied on the readout weight vectors in a meaningful manner.\n\n\n\n\n\n\n\n\n\n\n\\subsection{Embedding time series in the space of readout weights}\n\\label{sec:encoding}\n\n\n\nGiven a deterministically constructed and fixed reservoir $(D,v,u)$, we cast a  dataset $\\{{\\mbox{\\boldmath ${y}$}}_1,\\dots,{\\mbox{\\boldmath ${y}$}}_N\\}$ \nvia $g({\\mbox{\\boldmath ${y}$}}_n) = {\\mbox{\\boldmath ${w}$}}_n$ to a  new dataset of readout weights $\\{{\\mbox{\\boldmath ${w}$}}_1,\\dots,{\\mbox{\\boldmath ${w}$}}_N\\}$.\n\\emph{We emphasise that all time series are embedded in the space of readout weights with respect to the same fixed dynamic\nreservoir $(D,v,u)$}. After this embedding,  visualisation proceeds by performing dimensionality reduction on the new representations ${\\mbox{\\boldmath ${w}$}}_n$.\nWe take the view that the readout  weight ${\\mbox{\\boldmath ${w}$}}_n$ is a good representation for a sequence ${\\mbox{\\boldmath ${y}$}}_n$ with respect to the fixed reservoir $(D,v,u)$. The readout weight ${\\mbox{\\boldmath ${w}$}}_n$ captures important information about ${\\mbox{\\boldmath ${y}$}}_n$ in the sense that it exhibits good predictive power on it. Moreover, the readout weight vector ${\\mbox{\\boldmath ${w}$}}_n$ features  time-shift invariance, and can  accommodate sequences of variable length.\n\n\n\n\nA prerequisite for a successful embedding is a common, fixed reservoir that  enables good predictive  performance on the data. To find this reservoir, we opt for a simple strategy. \nFor both $v$ and $u$ we take a regular grid of e.g.~$10$ candidate values $[10^{-2},\\dots,1.0]$. \nFor each combination  $(u,v) \\in [10^{-2},\\dots,1.0] \\times [10^{-2},\\dots,1.0]$, \nwe perform the following:\n\\begin{enumerate}\n\\item Split each sequence {\\mbox{\\boldmath ${y}$}} in two halves  ${\\mbox{\\boldmath ${y}$}}_n^{(train)}$ and  ${\\mbox{\\boldmath ${y}$}}_n^{(test)}$.\n\\item According to Eq.~\\eqref{eq:ESN_objective}, train  ESN on ${\\mbox{\\boldmath ${y}$}}_n^{(train)}$ by minimising $\\ell^{(train)}({\\mbox{\\boldmath ${w}$}}) =\\|{\\mbox{\\boldmath ${X}$}}_n^{(train)}{\\mbox{\\boldmath ${w}$}} - {\\mbox{\\boldmath ${y}$}}_n^{(train)}\\|^2$ which yields ${\\mbox{\\boldmath ${w}$}}_n$.\n\\item Measure test error  via $\\ell^{(test)}({\\mbox{\\boldmath ${w}$}}_n) =\\|{\\mbox{\\boldmath ${X}$}}_n^{(test)}{\\mbox{\\boldmath ${w}$}}_n - {\\mbox{\\boldmath ${y}$}}_n^{(test)}\\|^2$.\n\\end{enumerate}\nMatrices ${\\mbox{\\boldmath ${X}$}}_n^{(train)}$ and ${\\mbox{\\boldmath ${X}$}}_n^{(test)}$ respectively record row-wise the activations $y_n^{(train)}(t)$ and $y_n^{(test)}(t)$  as specified in Section \\ref{sec:echo_state_networks}.\nThe combination $(u,v)$ with the lowest test error over all sequences $\\sum_{n=1}^N \\ell^{(test)}({\\mbox{\\boldmath ${w}$}}_n)$, determines the ESN that will cast all time series in the dataset to readout weights.\nParameters $D$ and $\\mu$ may also be included in this simple validation  scheme.\n\n\n\n\n\n\n\\subsection{ESN-coupled Autoencoder} \n\\label{sec:esn_coupled}\n\n\n\nWe want to reduce the dimensionality of the new representations  $\\{{\\mbox{\\boldmath ${w}$}}_1,\\dots,{\\mbox{\\boldmath ${w}$}}_N\\}$\n using an autoencoder. One possibility is to  directly apply the autoencoder taking as input the readout weights and returning \n\n\ntheir reconstructed versions, $f: {\\mathbb{R}^{{D\\times 1}}} \\rightarrow {\\mathbb{R}^{{D\\times 1}}}$.\nWe could then minimise the following objective function with respect to the autoencoder weights ${\\mbox{\\boldmath ${\\theta}$}}$:\n\n\n", "itemtype": "equation", "pos": 15350, "prevtext": "\n\n\n\n\n\n\n\n\n\n\\subsection{Deterministically Constructed Echo State Networks}\n\n\\label{sec:deterministic}\n\n\n\nIn the original formulation of the ESN \\cite{Lukosevicius2009} the weights in {\\mbox{\\boldmath ${v}$}} and {\\mbox{\\boldmath ${U}$}} are generated stochastically and so are the connections between the hidden neurons in the reservoir. This makes the training and use of the ESN dependent on random initialisations. In order to avoid this source of randomness, we make use of a class of ESNs that are constructed in a deterministic fashion \\cite{Rodan2011}.\n\n\nDeterministic ESNs make several simplifications over standard ESNs. All entries in  ${\\mbox{\\boldmath ${v}$}}$ have the same absolute value of a single scalar parameter $v>0$.\nThe signs of the entries in ${\\mbox{\\boldmath ${v}$}}$ are deterministically generated by an aperiodic sequence: e.g. a pseudorandom binary sequence (coin flips), with  outcomes $0$ and $1$ corresponding to $-$ and $+$ respectively.\nSimilarly, the entries in ${\\mbox{\\boldmath ${U}$}}$ are  parametrised by a single scalar $u>0$. As opposed to random connectivity, deterministic ESNs impose a fixed regular topology on the hidden neurons in the reservoir. Amongst possible choices, one can arrange the neurons in a cycle. A cyclic arrangement imposes the following structure on  ${\\mbox{\\boldmath ${U}$}}$: the only nonzero entries in ${\\mbox{\\boldmath ${U}$}}$ are on the lower sub-diagonal\n${\\mbox{\\boldmath ${U}$}}_{i+1,i} = u$, and at the upper-right corner ${\\mbox{\\boldmath ${U}$}}_{1,D} = u$.\nAn illustration of a cyclic deterministic ESN is shown in Fig.~\\ref{fig:cyclic_esn}.\n\n\nIn this work we employ deterministic ESNs with a cyclic connectivity. Deterministic ESNs have three degrees of freedom: the reservoir size $D$, the input weight $v$ and  reservoir weight $u$. Hence, the triple $(D,v,u)$ completely specifies an ESN.  \nIt has been shown empirically and theoretically (memory capacity) \\cite{Rodan2011}   that deterministic ESNs perform up to par with their stochastic counterparts.\nTraining of a deterministic ESN is performed in exactly  the same fashion as in  stochastically constructed ESNs  using the objective $\\ell({\\mbox{\\boldmath ${w}$}})$ in Eq.~\\eqref{eq:ESN_objective}.\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.22\\textwidth]{newfigures/cyclic_esn.eps}\n\\caption{Deterministic ESN with cyclic architecture, see Section \\ref{sec:deterministic}. Circles denote  neurons and arrows connections between neurons.\nAll input neurons connect to the hidden neurons, and all hidden neurons connect to the output neurons. Hidden neurons are connected in a cyclic fashion to each other. All input weights have the same absolute value $v$, and the sign is determined by a deterministic aperiodic sequence. The hidden reservoir weights are fixed to the same scalar $u$. The readout weights ${\\mbox{\\boldmath ${w}$}}$ are the only adaptable part of the ESN. }\n\\label{fig:cyclic_esn}\n\\end{figure}\n\n\n\n\n\\subsection{Autoencoder}\n\\label{sec:autoencoder}\n\n\n\nThe autoencoder \\cite{Kramer1991} is a feedforward neural network that defines\n a three hidden layer architecture\\footnote{To be perfectly precise, we use what is widely considered the standard autoencoder specified  in  \\cite[Sec. 12.4.2]{Bishop}).} with the middle layer, the  ``bottleneck'', being smaller than the others in terms of the number of neurons denoted by $Q$. \nThe autoencoder learns  an identity mapping by training on targets identical to the inputs.\nLearning is hampered by the bottleneck that forces the autoencoder\nto reduce the dimensionality of the inputs, and hence the output is only an\napproximate reconstruction of the input.  \n\n\nGiven general vectors ${\\mbox{\\boldmath ${s}$}}$, we want to reduce them to a $Q$-dimensional representation. \nThe autoencoder is the composition of an encoding $f_{enc}$ and a decoding $f_{dec}$\nfunction. Encoding maps inputs ${\\mbox{\\boldmath ${s}$}}$ to low-dimensional compressed versions, $f_{enc}({\\mbox{\\boldmath ${s}$}}) = {\\mbox{\\boldmath ${z}$}} \\in{\\mathbb{R}^{{Q}}}$, while decoding  maps approximately back to the inputs, $f_{dec}({\\mbox{\\boldmath ${z}$}}) = \\tilde{{\\mbox{\\boldmath ${s}$}}}$. The complete autoencoder is the function $f({\\mbox{\\boldmath ${s}$}};{\\mbox{\\boldmath ${\\theta}$}}) = f_{dec}(f_{enc}({\\mbox{\\boldmath ${s}$}})) = \\tilde{{\\mbox{\\boldmath ${s}$}}}$, where {\\mbox{\\boldmath ${\\theta}$}} are the weights of the autoencoder. Training the autoencoder involves minimising the $L_2$ norm between  $N$ given vectors ${\\mbox{\\boldmath ${s}$}}$  and their reconstructions:\n\n\n", "index": 7, "text": "\\begin{equation}\n\\sum_{n=1}^{N} \\| \\tilde{{\\mbox{\\boldmath ${s}$}}}_n - {\\mbox{\\boldmath ${s}$}}_n\\|^2  = \\sum_{n=1}^{N} \\| f({\\mbox{\\boldmath ${s}$}}_n;{\\mbox{\\boldmath ${\\theta}$}}) - {\\mbox{\\boldmath ${s}$}}_n\\|^2  .\n\\label{eq:L2_AE_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\sum_{n=1}^{N}\\|\\tilde{{\\mbox{\\boldmath${s}$}}}_{n}-{\\mbox{\\boldmath${s}$}}_{n%&#10;}\\|^{2}=\\sum_{n=1}^{N}\\|f({\\mbox{\\boldmath${s}$}}_{n};{\\mbox{\\boldmath${\\theta%&#10;}$}})-{\\mbox{\\boldmath${s}$}}_{n}\\|^{2}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc94</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc94</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>\u2225</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc94</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>\ud835\udc94</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\nA limitation of the above objective function is that it merely measures how well the reconstructions $f({\\mbox{\\boldmath ${w}$}};{\\mbox{\\boldmath ${\\theta}$}}) = \\tilde{{\\mbox{\\boldmath ${w}$}}}$ approximate the original inputs ${\\mbox{\\boldmath ${w}$}}$ in the $L_2$ sense. \n\nA better objective would measure reconstruction error in the sequence space as opposed to the space of readout weights.\nTo that purpose, we  map the reconstructed readout weights $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ to the sequence space by multiplying\nwith the respective state matrix,  ${\\mbox{\\boldmath ${X}$}}\\tilde{{\\mbox{\\boldmath ${w}$}}} = \\tilde{{\\mbox{\\boldmath ${y}$}}}$.\nIn actual fact,  function $\\ell(\\cdot)$ in Eq.~\\eqref{eq:ESN_objective} is cut out for this task:\n\nif $\\ell(\\tilde{{\\mbox{\\boldmath ${w}$}}})$ returns high likelihood, then $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ is a good reconstruction; if $\\ell(\\tilde{{\\mbox{\\boldmath ${w}$}}})$ returns low likelihood, then $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ is a poor reconstruction. The new objective function reads:\n\n\n\n", "itemtype": "equation", "pos": 19434, "prevtext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Model Formulation}\n\\label{model_formulation}\n\n\nThe proposed approach consists of two stages. In Section \\ref{sec:encoding}, we\ndiscuss how time series ${\\mbox{\\boldmath ${y}$}}$ are embedded in the space of readout weight vectors ${\\mbox{\\boldmath ${w}$}}$.\nSection \\ref{sec:esn_coupled} discusses how an autoencoder with a modified reconstruction function\nis applied on the readout weight vectors in a meaningful manner.\n\n\n\n\n\n\n\n\n\n\n\\subsection{Embedding time series in the space of readout weights}\n\\label{sec:encoding}\n\n\n\nGiven a deterministically constructed and fixed reservoir $(D,v,u)$, we cast a  dataset $\\{{\\mbox{\\boldmath ${y}$}}_1,\\dots,{\\mbox{\\boldmath ${y}$}}_N\\}$ \nvia $g({\\mbox{\\boldmath ${y}$}}_n) = {\\mbox{\\boldmath ${w}$}}_n$ to a  new dataset of readout weights $\\{{\\mbox{\\boldmath ${w}$}}_1,\\dots,{\\mbox{\\boldmath ${w}$}}_N\\}$.\n\\emph{We emphasise that all time series are embedded in the space of readout weights with respect to the same fixed dynamic\nreservoir $(D,v,u)$}. After this embedding,  visualisation proceeds by performing dimensionality reduction on the new representations ${\\mbox{\\boldmath ${w}$}}_n$.\nWe take the view that the readout  weight ${\\mbox{\\boldmath ${w}$}}_n$ is a good representation for a sequence ${\\mbox{\\boldmath ${y}$}}_n$ with respect to the fixed reservoir $(D,v,u)$. The readout weight ${\\mbox{\\boldmath ${w}$}}_n$ captures important information about ${\\mbox{\\boldmath ${y}$}}_n$ in the sense that it exhibits good predictive power on it. Moreover, the readout weight vector ${\\mbox{\\boldmath ${w}$}}_n$ features  time-shift invariance, and can  accommodate sequences of variable length.\n\n\n\n\nA prerequisite for a successful embedding is a common, fixed reservoir that  enables good predictive  performance on the data. To find this reservoir, we opt for a simple strategy. \nFor both $v$ and $u$ we take a regular grid of e.g.~$10$ candidate values $[10^{-2},\\dots,1.0]$. \nFor each combination  $(u,v) \\in [10^{-2},\\dots,1.0] \\times [10^{-2},\\dots,1.0]$, \nwe perform the following:\n\\begin{enumerate}\n\\item Split each sequence {\\mbox{\\boldmath ${y}$}} in two halves  ${\\mbox{\\boldmath ${y}$}}_n^{(train)}$ and  ${\\mbox{\\boldmath ${y}$}}_n^{(test)}$.\n\\item According to Eq.~\\eqref{eq:ESN_objective}, train  ESN on ${\\mbox{\\boldmath ${y}$}}_n^{(train)}$ by minimising $\\ell^{(train)}({\\mbox{\\boldmath ${w}$}}) =\\|{\\mbox{\\boldmath ${X}$}}_n^{(train)}{\\mbox{\\boldmath ${w}$}} - {\\mbox{\\boldmath ${y}$}}_n^{(train)}\\|^2$ which yields ${\\mbox{\\boldmath ${w}$}}_n$.\n\\item Measure test error  via $\\ell^{(test)}({\\mbox{\\boldmath ${w}$}}_n) =\\|{\\mbox{\\boldmath ${X}$}}_n^{(test)}{\\mbox{\\boldmath ${w}$}}_n - {\\mbox{\\boldmath ${y}$}}_n^{(test)}\\|^2$.\n\\end{enumerate}\nMatrices ${\\mbox{\\boldmath ${X}$}}_n^{(train)}$ and ${\\mbox{\\boldmath ${X}$}}_n^{(test)}$ respectively record row-wise the activations $y_n^{(train)}(t)$ and $y_n^{(test)}(t)$  as specified in Section \\ref{sec:echo_state_networks}.\nThe combination $(u,v)$ with the lowest test error over all sequences $\\sum_{n=1}^N \\ell^{(test)}({\\mbox{\\boldmath ${w}$}}_n)$, determines the ESN that will cast all time series in the dataset to readout weights.\nParameters $D$ and $\\mu$ may also be included in this simple validation  scheme.\n\n\n\n\n\n\n\\subsection{ESN-coupled Autoencoder} \n\\label{sec:esn_coupled}\n\n\n\nWe want to reduce the dimensionality of the new representations  $\\{{\\mbox{\\boldmath ${w}$}}_1,\\dots,{\\mbox{\\boldmath ${w}$}}_N\\}$\n using an autoencoder. One possibility is to  directly apply the autoencoder taking as input the readout weights and returning \n\n\ntheir reconstructed versions, $f: {\\mathbb{R}^{{D\\times 1}}} \\rightarrow {\\mathbb{R}^{{D\\times 1}}}$.\nWe could then minimise the following objective function with respect to the autoencoder weights ${\\mbox{\\boldmath ${\\theta}$}}$:\n\n\n", "index": 9, "text": "\\begin{equation}\n \\sum_{n=1}^{N} \\| f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}}) - {\\mbox{\\boldmath ${w}$}}_n\\|^2 =  \\sum_{n=1}^{N} \\| \\tilde{{\\mbox{\\boldmath ${w}$}}}_n - {\\mbox{\\boldmath ${w}$}}_n\\|^2 .\n\\label{eq:direct_L2_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\sum_{n=1}^{N}\\|f({\\mbox{\\boldmath${w}$}}_{n};{\\mbox{\\boldmath${\\theta}$}})-{%&#10;\\mbox{\\boldmath${w}$}}_{n}\\|^{2}=\\sum_{n=1}^{N}\\|\\tilde{{\\mbox{\\boldmath${w}$}%&#10;}}_{n}-{\\mbox{\\boldmath${w}$}}_{n}\\|^{2}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>\u2225</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc98</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\nwhere $\\ell_n$  and ${\\mbox{\\boldmath ${X}$}}_n$ are respectively the objective function and state space pertaining to sequence ${\\mbox{\\boldmath ${y}$}}_n$, see Eq.~\\eqref{eq:ESN_objective}.\n\nThe gradient of the new objective function in Eq. \\eqref{eq:new_objective} with respect to the weights {\\mbox{\\boldmath ${\\theta}$}}, is calculated by backpropagation \\cite{Bishop}. We use L-BFGS as the optimisation routine\nfor training the weights {\\mbox{\\boldmath ${\\theta}$}}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Data Projection}\n\n\n\nHaving trained the autoencoder $f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}})$, we would like to project a time series ${\\mbox{\\boldmath ${y}$}}^*$ to a coordinate ${\\mbox{\\boldmath ${z}$}}^* \\in {\\mathbb{R}^{{Q}}}$.\nTo that end, we first use the fixed ESN reservoir to cast the time series to  $g({\\mbox{\\boldmath ${y}$}}^*)={\\mbox{\\boldmath ${w}$}}^*$.\nThen, the readout weight ${\\mbox{\\boldmath ${w}$}}^*$ is projected using the encoding part of the autoencoder\nto obtain  $f_{enc}({\\mbox{\\boldmath ${w}$}}^*)= {\\mbox{\\boldmath ${z}$}}^*$.\n\n\n\n\n\n\n\n\\section{Binary Sequences}\n\\label{sec:symbolic}\n\n\nThe time series considered so far are sequences of reals $y(t)\\in {\\mathbb{R}^{{}}}$. However, it is possible to extend the proposed approach to the processing of symbolic sequences. In particular, we consider binary sequences composed of observations $y(t)\\in \\{ 0,1\\}$. \nFor an ESN to process binary sequences, we pass its outputs  through the logistic function $\\sigma(\\cdot) = (1+\\exp(\\cdot))^{-1}$  (link function of the Bernoulli distribution).\nHence, the  equations specifying the operation of the ESN now read\\footnote{ In Eq.~\\eqref{eq:logistic_esn_recursive_one} we subtract 0.5 from $y(t)$, since the symmetric $\\tanh(\\cdot)$ transfer function $h$ is used in the dynamic reservoir. }:\n\n\n\n", "itemtype": "equation", "pos": 20763, "prevtext": "\n\nA limitation of the above objective function is that it merely measures how well the reconstructions $f({\\mbox{\\boldmath ${w}$}};{\\mbox{\\boldmath ${\\theta}$}}) = \\tilde{{\\mbox{\\boldmath ${w}$}}}$ approximate the original inputs ${\\mbox{\\boldmath ${w}$}}$ in the $L_2$ sense. \n\nA better objective would measure reconstruction error in the sequence space as opposed to the space of readout weights.\nTo that purpose, we  map the reconstructed readout weights $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ to the sequence space by multiplying\nwith the respective state matrix,  ${\\mbox{\\boldmath ${X}$}}\\tilde{{\\mbox{\\boldmath ${w}$}}} = \\tilde{{\\mbox{\\boldmath ${y}$}}}$.\nIn actual fact,  function $\\ell(\\cdot)$ in Eq.~\\eqref{eq:ESN_objective} is cut out for this task:\n\nif $\\ell(\\tilde{{\\mbox{\\boldmath ${w}$}}})$ returns high likelihood, then $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ is a good reconstruction; if $\\ell(\\tilde{{\\mbox{\\boldmath ${w}$}}})$ returns low likelihood, then $\\tilde{{\\mbox{\\boldmath ${w}$}}}$ is a poor reconstruction. The new objective function reads:\n\n\n\n", "index": 11, "text": "\\begin{equation}\n\\sum_{n=1}^{N} \\ell_n( f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}})) = \n\\sum_{n=1}^{N} \\|{\\mbox{\\boldmath ${X}$}}_n f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}}) - {\\mbox{\\boldmath ${y}$}}_n\\|^2 = \n\\sum_{n=1}^{N} \\|\\tilde{{\\mbox{\\boldmath ${y}$}}}_n - {\\mbox{\\boldmath ${y}$}}_n\\|^2 \\ ,  \n\\label{eq:new_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\sum_{n=1}^{N}\\ell_{n}(f({\\mbox{\\boldmath${w}$}}_{n};{\\mbox{\\boldmath${\\theta}%&#10;$}}))=\\sum_{n=1}^{N}\\|{\\mbox{\\boldmath${X}$}}_{n}f({\\mbox{\\boldmath${w}$}}_{n}%&#10;;{\\mbox{\\boldmath${\\theta}$}})-{\\mbox{\\boldmath${y}$}}_{n}\\|^{2}=\\sum_{n=1}^{N%&#10;}\\|\\tilde{{\\mbox{\\boldmath${y}$}}}_{n}-{\\mbox{\\boldmath${y}$}}_{n}\\|^{2}\\ ,\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi mathvariant=\"normal\">\u2113</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>\ud835\udc7f</mi><mi>n</mi></msub><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>\ud835\udc9a</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mpadded width=\"+5pt\"><msup><mrow><mo>\u2225</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc9a</mi><mo stretchy=\"false\">~</mo></mover><mi>n</mi></msub><mo>-</mo><msub><mi>\ud835\udc9a</mi><mi>n</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mpadded></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\n\n\nHere the output $\\hat{y}(t+1)\\in[0\\dots 1]$ of the ESN is interpreted as the probability of the next observation\n$y(t+1)$ being equal to $1$, i.e. $\\hat{y}(t+1)=p(y(t+1)=1)$.\nAccordingly, the objective function $\\ell({\\mbox{\\boldmath ${w}$}})$ in Eq.~\\eqref{eq:ESN_objective}\nneeds to be redefined. Instead of solving a least squares problem, we minimise the cross-entropy:\n\n\n\n", "itemtype": "equation", "pos": 22977, "prevtext": "\n\nwhere $\\ell_n$  and ${\\mbox{\\boldmath ${X}$}}_n$ are respectively the objective function and state space pertaining to sequence ${\\mbox{\\boldmath ${y}$}}_n$, see Eq.~\\eqref{eq:ESN_objective}.\n\nThe gradient of the new objective function in Eq. \\eqref{eq:new_objective} with respect to the weights {\\mbox{\\boldmath ${\\theta}$}}, is calculated by backpropagation \\cite{Bishop}. We use L-BFGS as the optimisation routine\nfor training the weights {\\mbox{\\boldmath ${\\theta}$}}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Data Projection}\n\n\n\nHaving trained the autoencoder $f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}})$, we would like to project a time series ${\\mbox{\\boldmath ${y}$}}^*$ to a coordinate ${\\mbox{\\boldmath ${z}$}}^* \\in {\\mathbb{R}^{{Q}}}$.\nTo that end, we first use the fixed ESN reservoir to cast the time series to  $g({\\mbox{\\boldmath ${y}$}}^*)={\\mbox{\\boldmath ${w}$}}^*$.\nThen, the readout weight ${\\mbox{\\boldmath ${w}$}}^*$ is projected using the encoding part of the autoencoder\nto obtain  $f_{enc}({\\mbox{\\boldmath ${w}$}}^*)= {\\mbox{\\boldmath ${z}$}}^*$.\n\n\n\n\n\n\n\n\\section{Binary Sequences}\n\\label{sec:symbolic}\n\n\nThe time series considered so far are sequences of reals $y(t)\\in {\\mathbb{R}^{{}}}$. However, it is possible to extend the proposed approach to the processing of symbolic sequences. In particular, we consider binary sequences composed of observations $y(t)\\in \\{ 0,1\\}$. \nFor an ESN to process binary sequences, we pass its outputs  through the logistic function $\\sigma(\\cdot) = (1+\\exp(\\cdot))^{-1}$  (link function of the Bernoulli distribution).\nHence, the  equations specifying the operation of the ESN now read\\footnote{ In Eq.~\\eqref{eq:logistic_esn_recursive_one} we subtract 0.5 from $y(t)$, since the symmetric $\\tanh(\\cdot)$ transfer function $h$ is used in the dynamic reservoir. }:\n\n\n\n", "index": 13, "text": "\\begin{align}\n{\\mbox{\\boldmath ${x}$}}(t+1) &=h({\\mbox{\\boldmath ${U}$}}{\\mbox{\\boldmath ${x}$}}(t) + {\\mbox{\\boldmath ${v}$}}(y(t)-0.5)) \\ , \\label{eq:logistic_esn_recursive_one} \\\\\n\\hat{y}(t+1) &=\\sigma(  {\\mbox{\\boldmath ${w}$}}^T  {\\mbox{\\boldmath ${x}$}}(t+1)  )\\label{eq:logistic_esn_recursive_two} \\ .\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mbox{\\boldmath${x}$}}(t+1)\" display=\"inline\"><mrow><mi>\ud835\udc99</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=h({\\mbox{\\boldmath${U}$}}{\\mbox{\\boldmath${x}$}}(t)+{\\mbox{%&#10;\\boldmath${v}$}}(y(t)-0.5))\\ ,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mi>\ud835\udc7c</mi><mi>\ud835\udc99</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\ud835\udc97</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mn>0.5</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{y}(t+1)\" display=\"inline\"><mrow><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\mbox{\\boldmath${w}$}}^{T}{\\mbox{\\boldmath${x}$}}(t+1))\\ .\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc98</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc99</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\n\n\n\nTraining of the ESN is carried out by iterative gradient minimisation of Eq.~\\eqref{eq:logistic_ESN_objective}\npreceded by a period of washout. \n\n\nThe above modifications  to the ESN, call for a modification also in the autoencoder. While in Eq.~\\eqref{eq:new_objective}\nreconstruction  is measured via the least-squares based function $\\ell({\\mbox{\\boldmath ${w}$}})$, we now use\nthe cross-entropy based function $\\ell^{ce}({\\mbox{\\boldmath ${w}$}})$.  In order for the autoencoder to  process correctly the weights coming from binary sequences,\nits objective function needs to be changed to:\n\n\n", "itemtype": "equation", "pos": 23677, "prevtext": "\n\n\n\nHere the output $\\hat{y}(t+1)\\in[0\\dots 1]$ of the ESN is interpreted as the probability of the next observation\n$y(t+1)$ being equal to $1$, i.e. $\\hat{y}(t+1)=p(y(t+1)=1)$.\nAccordingly, the objective function $\\ell({\\mbox{\\boldmath ${w}$}})$ in Eq.~\\eqref{eq:ESN_objective}\nneeds to be redefined. Instead of solving a least squares problem, we minimise the cross-entropy:\n\n\n\n", "index": 15, "text": "\\begin{equation}\n\\ell^{ce}({\\mbox{\\boldmath ${w}$}}) =  - \\sum_{t=1}^T   y(t) \\log \\hat{y}(t) \\ .\n\n\\label{eq:logistic_ESN_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\ell^{ce}({\\mbox{\\boldmath${w}$}})=-\\sum_{t=1}^{T}y(t)\\log\\hat{y}(t)\\ .\\par&#10;\" display=\"block\"><mrow><mrow><mrow><msup><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>e</mi></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc98</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\nIn the case of binary sequences, the outputs of the autoencoder $f({\\mbox{\\boldmath ${w}$}};{\\mbox{\\boldmath ${\\theta}$}})$ are put though the function $\\ell^{ce}(\\cdot)$. \n\n\nBy adopting a 1-of-K coding scheme for the symbols, and the softmax function in the place of the logistic function, an extension to $K$ number of symbols is possible.  The resulting objective for training the ESN is again a cross-entropy function.\n\n\n\n\n\n\n\n\n\n\n\\section{Magnification Factors}\n\\label{sec:magnification}\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{newfigures/magn_factors.eps}\n\\caption{Stylised sketch: mapping $f_{dec}({\\mbox{\\boldmath ${z}$}})$ embeds the visualisation space $\\mathcal{V}$  as a manifold $\\mathcal{M}$\nin the  space of readout weights.\nEach point ${\\mbox{\\boldmath ${z}$}}$ addresses a probabilistic ESN\nwith readout weights ${\\mbox{\\boldmath ${w}$}}$.}\n\\label{fig:magn_factors}\n\\end{figure}\n\nIn Fig.~\\ref{fig:magn_factors}, the  smooth nonlinear function $f_{dec}({\\mbox{\\boldmath ${z}$}})$ embeds the low-dimensional visualisation space ${\\cal V}$\nas a  $Q$-dimensional manifold $\\mathcal{M}$ in the space of readout weights ${\\mbox{\\boldmath ${w}$}}$.\nEach point ${\\mbox{\\boldmath ${z}$}}\\in{\\cal V}$ addresses an ESN model\\footnote{We always have the same fixed reservoir.} with readout weights ${\\mbox{\\boldmath ${w}$}}\\in \\mathcal{M}$.\nThe ESN model may be viewed as a probabilistic model, if we assume that observations $y(t)$ are corrupted by  i.i.d. Gaussian noise of zero mean and variance $\\epsilon^2$:\n\n\n", "itemtype": "equation", "pos": 24424, "prevtext": "\n\n\n\n\nTraining of the ESN is carried out by iterative gradient minimisation of Eq.~\\eqref{eq:logistic_ESN_objective}\npreceded by a period of washout. \n\n\nThe above modifications  to the ESN, call for a modification also in the autoencoder. While in Eq.~\\eqref{eq:new_objective}\nreconstruction  is measured via the least-squares based function $\\ell({\\mbox{\\boldmath ${w}$}})$, we now use\nthe cross-entropy based function $\\ell^{ce}({\\mbox{\\boldmath ${w}$}})$.  In order for the autoencoder to  process correctly the weights coming from binary sequences,\nits objective function needs to be changed to:\n\n\n", "index": 17, "text": "\\begin{equation}\n\\sum_{n=1}^{N} \\ell_n^{ce} ( f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}})) = \n- \\sum_{n=1}^{N} \\sum_{t=1}^T   y(t) \\log \\sigma(  f({\\mbox{\\boldmath ${w}$}}_n;{\\mbox{\\boldmath ${\\theta}$}})^T {\\mbox{\\boldmath ${x}$}}_n(t+1)  ) \\  .\n\n\\label{eq:logistic_new_objective}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\sum_{n=1}^{N}\\ell_{n}^{ce}(f({\\mbox{\\boldmath${w}$}}_{n};{\\mbox{\\boldmath${%&#10;\\theta}$}}))=-\\sum_{n=1}^{N}\\sum_{t=1}^{T}y(t)\\log\\sigma(f({\\mbox{\\boldmath${w%&#10;}$}}_{n};{\\mbox{\\boldmath${\\theta}$}})^{T}{\\mbox{\\boldmath${x}$}}_{n}(t+1))\\ .\\par&#10;\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi mathvariant=\"normal\">\u2113</mi><mi>n</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc98</mi><mi>n</mi></msub><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc99</mi><mi>n</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\nThus,  each  point ${\\mbox{\\boldmath ${z}$}}$ addresses a probabilistic  model $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}}))$, and  $\\cal M$ is  a  manifold of probabilistic models $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}})) $.\n\nManifold $\\cal M$ is endowed with a natural metric for measuring distances between probabilistic models $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}})) $. Specifically, the metric tensor on a statistical manifold at a given point ${\\mbox{\\boldmath ${z}$}}$ is the $Q \\times Q$ Fisher information  matrix (FIM)  \\cite{Kullback1959}. Here, we approximate it through the {\\it observed FIM} over the given dataset of sequences:\n\n\n", "itemtype": "equation", "pos": 26282, "prevtext": "\n\nIn the case of binary sequences, the outputs of the autoencoder $f({\\mbox{\\boldmath ${w}$}};{\\mbox{\\boldmath ${\\theta}$}})$ are put though the function $\\ell^{ce}(\\cdot)$. \n\n\nBy adopting a 1-of-K coding scheme for the symbols, and the softmax function in the place of the logistic function, an extension to $K$ number of symbols is possible.  The resulting objective for training the ESN is again a cross-entropy function.\n\n\n\n\n\n\n\n\n\n\n\\section{Magnification Factors}\n\\label{sec:magnification}\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{newfigures/magn_factors.eps}\n\\caption{Stylised sketch: mapping $f_{dec}({\\mbox{\\boldmath ${z}$}})$ embeds the visualisation space $\\mathcal{V}$  as a manifold $\\mathcal{M}$\nin the  space of readout weights.\nEach point ${\\mbox{\\boldmath ${z}$}}$ addresses a probabilistic ESN\nwith readout weights ${\\mbox{\\boldmath ${w}$}}$.}\n\\label{fig:magn_factors}\n\\end{figure}\n\nIn Fig.~\\ref{fig:magn_factors}, the  smooth nonlinear function $f_{dec}({\\mbox{\\boldmath ${z}$}})$ embeds the low-dimensional visualisation space ${\\cal V}$\nas a  $Q$-dimensional manifold $\\mathcal{M}$ in the space of readout weights ${\\mbox{\\boldmath ${w}$}}$.\nEach point ${\\mbox{\\boldmath ${z}$}}\\in{\\cal V}$ addresses an ESN model\\footnote{We always have the same fixed reservoir.} with readout weights ${\\mbox{\\boldmath ${w}$}}\\in \\mathcal{M}$.\nThe ESN model may be viewed as a probabilistic model, if we assume that observations $y(t)$ are corrupted by  i.i.d. Gaussian noise of zero mean and variance $\\epsilon^2$:\n\n\n", "index": 19, "text": "\\begin{align}\n {\\mbox{\\boldmath ${y}$}} =& {\\mbox{\\boldmath ${X}$}}{\\mbox{\\boldmath ${w}$}} + \\epsilon \\ , \\\\\n p({\\mbox{\\boldmath ${y}$}};{\\mbox{\\boldmath ${w}$}}) =& \\mathcal{N}({\\mbox{\\boldmath ${y}$}}| {\\mbox{\\boldmath ${X}$}}{\\mbox{\\boldmath ${w}$}} ,\\epsilon{\\mbox{\\boldmath ${I}$}}_{T}) \\ ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mbox{\\boldmath${y}$}}=\" display=\"inline\"><mrow><mi>\ud835\udc9a</mi><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mbox{\\boldmath${X}$}}{\\mbox{\\boldmath${w}$}}+\\epsilon\\ ,\" display=\"inline\"><mrow><mrow><mrow><mi>\ud835\udc7f</mi><mi>\ud835\udc98</mi></mrow><mo>+</mo><mpadded width=\"+5pt\"><mi>\u03f5</mi></mpadded></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\mbox{\\boldmath${y}$}};{\\mbox{\\boldmath${w}$}})=\" display=\"inline\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9a</mi><mo>;</mo><mi>\ud835\udc98</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{N}({\\mbox{\\boldmath${y}$}}|{\\mbox{\\boldmath${X}$}}{\\mbox%&#10;{\\boldmath${w}$}},\\epsilon{\\mbox{\\boldmath${I}$}}_{T})\\ ,\" display=\"inline\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9a</mi><mo stretchy=\"false\">|</mo><mrow><mi>\ud835\udc7f</mi><mi>\ud835\udc98</mi></mrow><mo>,</mo><mi>\u03f5</mi><msub><mi>\ud835\udc70</mi><mi>T</mi></msub><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": "\n\nWe note that the visualisation space $\\cal V$ does not necessarily reflect  distances between models on $\\cal M$.\nIn Fig.~\\ref{fig:magn_factors}, we see how neighbourhoods of {\\mbox{\\boldmath ${z}$}}, depicted as dotted ellipses, transform on $\\cal M$.  \nThus, in order to interpret  distances  in $\\cal V$, it is important to push-forward the natural notion of distances on $\\cal M$ onto the visualization space $\\cal V$. In the topographic mapping literature the induced metric in the visualization space from the data space is usually represented through magnification factors \\cite{Bishop1997}. In the following we show how magnification factors can be computed in the ESN-AE setting.\n\n\n\nGiven the FIM, one can push forward local distances $\\Delta{\\mbox{\\boldmath ${z}$}}$ from $\\cal M$ onto $\\cal V$  via $\\Delta{\\mbox{\\boldmath ${z}$}}^T {\\mbox{\\boldmath ${F}$}}({\\mbox{\\boldmath ${z}$}}) \\Delta{\\mbox{\\boldmath ${z}$}}$. In particular, at a given point $\\Delta{\\mbox{\\boldmath ${z}$}}$ it is possible to estimate in which direction $d{\\mbox{\\boldmath ${z}$}}$ the distance changes the most. This can be easily calculated by solving the following constrained problem:\n\n", "itemtype": "equation", "pos": 27303, "prevtext": "\n\nThus,  each  point ${\\mbox{\\boldmath ${z}$}}$ addresses a probabilistic  model $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}}))$, and  $\\cal M$ is  a  manifold of probabilistic models $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}})) $.\n\nManifold $\\cal M$ is endowed with a natural metric for measuring distances between probabilistic models $p({\\mbox{\\boldmath ${y}$}} ; f_{dec}({\\mbox{\\boldmath ${z}$}})) $. Specifically, the metric tensor on a statistical manifold at a given point ${\\mbox{\\boldmath ${z}$}}$ is the $Q \\times Q$ Fisher information  matrix (FIM)  \\cite{Kullback1959}. Here, we approximate it through the {\\it observed FIM} over the given dataset of sequences:\n\n\n", "index": 21, "text": "\\begin{equation}\n{\\mbox{\\boldmath ${F}$}}({\\mbox{\\boldmath ${z}$}})_{i,j}\\!= \\! -\\sum_{n=1}^N \\left[ \\left(\\frac{\\partial}{\\partial z_i} \\log p({\\mbox{\\boldmath ${y}$}}_n ; f_{dec}({\\mbox{\\boldmath ${z}$}}))\\right) \\left(\\frac{\\partial}{\\partial z_j} \\log p({\\mbox{\\boldmath ${y}$}}_n ; f_{dec}({\\mbox{\\boldmath ${z}$}}))\\right) \\right] \\\\ .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"{\\mbox{\\boldmath${F}$}}({\\mbox{\\boldmath${z}$}})_{i,j}\\!=\\!-\\sum_{n=1}^{N}%&#10;\\left[\\left(\\frac{\\partial}{\\partial z_{i}}\\log p({\\mbox{\\boldmath${y}$}}_{n};%&#10;f_{dec}({\\mbox{\\boldmath${z}$}}))\\right)\\left(\\frac{\\partial}{\\partial z_{j}}%&#10;\\log p({\\mbox{\\boldmath${y}$}}_{n};f_{dec}({\\mbox{\\boldmath${z}$}}))\\right)%&#10;\\right]\\\\&#10;.\" display=\"block\"><mrow><mrow><mrow><mi>\ud835\udc6d</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9b</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mpadded></mrow><mo rspace=\"0.8pt\">=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc9a</mi><mi>n</mi></msub><mo>;</mo><mrow><msub><mi>f</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>z</mi><mi>j</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>p</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc9a</mi><mi>n</mi></msub><mo>;</mo><mrow><msub><mi>f</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc9b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05654.tex", "nexttext": " \nThe solution to this problem is given by setting $\\Delta{\\mbox{\\boldmath ${z}$}}^{*}$ to the eigenvector corresponding to the largest eigenvalue $\\lambda^{*}$.\nEigenvalue $\\lambda^{*}$ informs us of the maximum local distortion in distance and can be taken as a measure for the local magnification factor.\n\n\n\n\n\n\n\n\n\n\\section{Numerical Experiments}\n\\label{sec:experiments}\n\n\nIn the following we compare the proposed method to other visualisation algorithms  and discuss the results.\n\n\n\n\\subsection{Datasets}\n\n\n\n\n\nIn order to judge whether a visualisation truly captures similarities, we need to know a priori \nwhich time series are similar to which. We therefore\n employ the following particular datasets whose data items fall under known classes and are labelled. For these datasets, there is a very strong a priori expectation that the classes {\\it are governed by  qualitatively distinct dynamical regimes}.\nThus, time series of the same class are expected to appear similar (close together)\nin the visualisation, while time series belonging to different classes are expected to appear dissimilar (separate) in the visualisation.\n\n\n\n\n\t\n\n\n\n\\paragraph{\\bf NARMA} \n\nWe generate $100$ sequences of length $1000$ from the three qualitatively different NARMA classes \\cite{Rodan2011} of orders $10, 20, 30$. The NARMA time series is an interesting benchmark problem  due to the presence of long-term dependencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\paragraph{\\bf Cauchy}\n\nWe sample sequences from a stationary Gaussian process\nwith correlation function given by $c(x_t,x_{t+h})=(1+ |h|^a)^{-\\frac{a}{b}}$  \\cite{Gneiting2004}.\nWe generated $4$ classes by permuting \nparameters $a \\in \\{0.65,1.95\\}$ and $b \\in \\{0.1,0.95\\}$. \nWe generated from each class $100$ time series of length $1,000$.\nParameters $a$ and $b$  are respectively related\nto the fractal dimension (measuring self-similarity) and the Hurst\ncoefficient (measuring long-memory dependence) of the series.\nBy construction, the four classes have  distinct characteristics.\n\n\n\n\\paragraph{\\bf X-ray}\n\n\nThe binary system GRS1915+105 is composed of an extremely heavy stellar black hole and a low-mass star. \nMaterial is transferred from the star towards the black hole through the \nRoche lobe. While falling into the gravitational potential of the black hole, energy is released \nby radiating X-ray and radio (jet) emission which is typical for the class of microquasars. \nA thorough investigation\ncarried out in \\cite{2000AA}, detected the presence of\nclasses of distinct dynamical patterns.\nDue to the lack of multiple time sequences per state, we split the observations\ninto equal-length parts, resulting in $161$ sequences. Here we visualise classes $delta, kappa, phi, rho$ and $chi$.\n\n\n\n\n\n\\paragraph{\\bf Wind}\n\n\nWe visualise wind data\\footnote{Kindly provided by the \nDeutscher Wetterdienst, \\href{ftp://ftp-cdc.dwd.de/}{ftp://ftp-cdc.dwd.de/ \\ .}} taken from the vicinity of  Hamburg, Frankfurt and Munich.\nAround each city,  we select the 10 closest stations with a completeness of more than 99\\% of hourly measured wind \nspeed data between 13/01/2014 - 31/12/2014 ($8,471$ measurements per station). \nMissing data are interpolated using a spline function of the $3rd$ degree. In order to increase the number\nof visualised entities, the time series of each station are cut into two non-overlapping parts of $4,000$ data points each. \nIn these data there is a strong a priori expectation that time series\nassociated with the coastal city of Hamburg are different to the other data.\n\n\n\n\n\n\n\n\n\n\\paragraph{{\\bf Textual data (symbolic)}}\n\n\n\n\n\nWe visualise the first chapter of \nJ. K. \nRowling's ``Harry Potter and the Philosopher's Stone'' in three languages  German, English and \nSpanish. A full symbolic representation of the alphabet makes the optimisation of the ESN \ndifficult and it would be a trivial task to separate the languages as they could be identified \nby single words. Here, we choose a binary representation where the states $0$ and $1$\nrepresent vowels and consonants. \n\n\n\nPunctuation and whitespaces are ignored. E.g. \na German sentence \nis converted as follows:\n\\begin{center}\n$\\gg$Die Potters, das stimmt, das hab ich geh\\\"ort -$\\ll$ \\\\\n\\_011\\_0100100\\_\\_010\\_001000\\_\\_010\\_010\\_100\\_010100\\_\\_\\_\n\\end{center}\n\n\n\n\n\nDiscarded symbols are marked by an underscore. This representation returns sequences of different length \nfor each language, but all with at least $24,000$ symbols. To increase the number of sequences per \nlanguage, we split the binary vectors into sequences of length $2,000$ with neighbouring sequences overlapping by $50\\%$. \nIt is interesting to see whether texts originating from different languages still retain their distinguishing  dynamics after subjected to this drastic ``binarisation\". \n\n\n\n\\subsection{Dimensionality Reduction Algorithms}\n\n\n\n\n\n\\begin{figure*}[!t]\n\\centering\n\\subfloat[PCA on NARMA.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/pca_narma.eps}\n\\label{fig:pca:narma}\n}\n\\subfloat[t-SNE on NARMA, perpl.=40.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/tsne40_narma.eps}\n\\label{fig:tsne:narma}\n}\n\\subfloat[Standard-AE on NARMA.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/ae_narma_mf.eps}\n\\label{fig:ae:narma}\n}\n\\subfloat[ESN-AE on NARMA.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/esn_narma.eps}\n\\label{fig:esn:narma}\n}\n\n\n\\\\\n\n\n\\subfloat[PCA on Cauchy.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/pca_cauchy.eps}\n\\label{fig:pca:cauchy}\n}\n\\subfloat[t-SNE on Cauchy, perpl.=30.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/tsne30_cauchy.eps}\n\\label{fig:tsne:cauchy}\n}\n\\subfloat[Standard-AE on Cauchy.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/ae_cauchy_mf.eps}\n\\label{fig:ae:cauchy}\n}\n\\subfloat[ESN-AE on Cauchy.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/esn_cauchy.eps}\n\\label{fig:esn:cauchy}\n}\n\n\n\\\\\n\n\n\\subfloat[PCA on X-ray.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/pca_xray.eps}\n\\label{fig:pca:xray}\n}\n\\subfloat[t-SNE on X-ray, perpl.=10.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/tsne10_xray.eps}\n\\label{fig:tsne:xray}\n}\n\\subfloat[Standard-AE on X-ray.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/ae_xray.eps}\n\\label{fig:ae:xray}\n}\n\\subfloat[ESN-AE on X-ray.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/esn_xray.eps}\n\\label{fig:esn:xray}\n}\n\\caption{Visualisations on NARMA (top), Cauchy (middle) and X-ray (bottom) data. High/low magnifications correspond to bright/dark regions. Legends specify which markers correspond to which classes.}\n\\label{fig:visualisations_A}\n\\end{figure*}\n\n\n\n\nThe following dimensionality reduction algorithms are compared in the numerical experiments. \nAll  algorithms operate on the readout weights ${\\mbox{\\boldmath ${w}$}}$. Sequences are represented as readout weights using a deterministic cyclic ESN whose\nparameters are selected using the validation procedure  in Section \\ref{sec:encoding}.\nAdditionally, in this validation scheme we include the regularisation parameter $\\mu \\in \\{10^{-2},10^{-3},10^{-4}\\}$.\nIn all experiments the size of the reservoir is fixed to $D=50$ and we set a washout period of $50$ time steps. We set $Q=2$ for constructing  2D visualisations.\n\n\n\n\n\\paragraph{\\bf PCA} \n\nWe include PCA as it helps us gauge how difficult it is to project a dataset to low dimensions: if PCA delivers a good result, this hints that a complex, non-linear projection is superfluous.  \n\n\n\n\n\\paragraph{\\bf t-SNE} \n\nWe include t-SNE \\cite{Maaten2008} as one of the most popular and well performing algorithms designed for vectorial data. We train t-SNE with perplexities in $[5,10,20,30,40,50]$, and display the visualisation that shows the best class separation. The chosen perplexity is quoted in the figures.\n\n\n\n\\paragraph{\\bf Standard autoencoder (standard-AE)} \n\nWe employ the standard autoencoder  operating directly on the readout weights.\nThe hidden layers of the encoding and decoding part have the same number $H$ of neurons. We also add a regulariser on the weights of the autoencoder $\\nu^2 \\|{\\mbox{\\boldmath ${\\theta}$}}\\|^2$ to control complexity.  In all experiments, we set $H=10$,  $\\nu=1$.  \n\n\n\n\n\\paragraph{\\bf Proposed approach (ESN-AE)} \n\nThe proposed ESN-AE has the same hyperparameters as the standard-AE. We again  fix \nthe hyperparameters to $H=10$,  $\\nu=1$. \n\n\n\n\n\n\n\n\\subsection{Results}\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[!t]\n\\centering\n\n\\subfloat[PCA on Wind.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/pca_wind.eps}\n\\label{fig:pca:wind}\n}\n\\subfloat[t-SNE on Wind, perpl.=5.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/tsne5_wind.eps}\n\\label{fig:tsne:wind}\n}\n\\subfloat[Standard-AE on Wind.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/ae_wind.eps}\n\\label{fig:ae:wind}\n}\n\\subfloat[ESN-AE on Wind.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/esn_wind.eps}\n\\label{fig:esn:wind}\n}\n\n\n\\\\\n\n\n\\subfloat[PCA on Textual.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/pca_harry.eps}\n\\label{fig:pca:text}\n}\n\\subfloat[t-SNE on Textual, perpl.=10.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/tsne10_harry.eps}\n\\label{fig:tsne:text}\n}\n\\subfloat[Standard-AE on Textual.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/ae_harry.eps}\n\\label{fig:ae:text}\n}\n\\subfloat[ESN-AE on Textual.]{\n\\includegraphics[width=0.25\\textwidth]{newfigures/esn_harry.eps}\n\\label{fig:esn:text}\n}\n\\caption{Visualisations on Wind (top), and Textual (bottom) data. High/low magnifications correspond to bright/dark regions. Legends specify which markers correspond to which classes.}\n\\label{fig:visualisations_B}\n\\end{figure*}\n\n\nWe present the visualisations in Figs.~\\ref{fig:visualisations_A} and \\ref{fig:visualisations_B}. Each column of plots corresponds to one\nof the aforementioned dimensionality reduction algorithms,\nand each row to a dataset. The projections in the plots\nappear as coloured markers of different shapes indicating class origin.\nThe legend in each plot shows which marker corresponds to which class.\nFollowing Section \\ref{sec:magnification}, we display local magnification factors, for the autoencoders,\nas the maximum eigenvalue $\\lambda^{*}$ of matrix ${\\mbox{\\boldmath ${F}$}}({\\mbox{\\boldmath ${z}$}})$ on a regular grid of points\n{\\mbox{\\boldmath ${z}$}} on the visualisation space.\nDark and bright values signify low and high eigenvalues/magnification factors respectively.\nThere are no magnification factors for PCA, as the linear mapping connecting the visualisation space\nto the high-dimensional space is  length/distance preserving. Also, we do not present magnification factors for t-SNE, as it does not define an explicit mapping between the visualisation and high-dimensional space. It thus requires a different framework than the one used here in order to study magnifications.\n\n\n\\begin{table}[]\n\\centering\n\\caption{Mean squared errors between NARMA classes, the smaller the more similar. }\n\\label{tbl:narma_cross_loglikel}\n\\begin{tabular}{lrrr}\n\\hline\n         & \\multicolumn{1}{l}{\\small Order 10} & \\multicolumn{1}{l}{\\small Order 20} & \\multicolumn{1}{l}{\\small  Order 30} \\\\ \\hline\n\\small Order 10 &\\small  5.331                        &\\small  2185.935                     &\\small  37.161                       \\\\\n\\small Order 20 &\\small  2213.019                     &\\small  0.052                        & \\small 2030.409                     \\\\\n\\small Order 30 &\\small  30.478                       &\\small  1983.585                     &\\small  6.031                        \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\t\n\t\n\n\n\\paragraph{{\\bf NARMA}, top row in Fig.~\\ref{fig:visualisations_A}}\n\n\n\n\t\nWe note  that all visualisations separate the three classes, and  show that the three classes are equidistant. The magnifications in \\ref{fig:ae:narma} show that the standard-AE views the three classes indeed as distinctly separable clusters. However, in the case of the ESN-AE in \\ref{fig:esn:narma}, the magnification factors suggest the presence of  distortions in  distances close to class ``Order 20''. This means that in actual fact  class ``Order 20''  is separated by significant distance from the other two classes, and that classes ``Order 10'' and ``Order 30'' are closer and more similar to each other.  We investigate this hypothesis with a simple experiment. We generate from each class additional $200$ sequences. For each pair of classes (classes also pair with themselves), we train on sequences from one class and measure the mean squared error on the unseen sequences of the other classes. These errors are reported in Table \\ref{tbl:narma_cross_loglikel}, and support this hypothesis put forward by the magnification factors in the ESN-AE visualisation.\n\n\t\n\t\n\t\n\n\\paragraph{{\\bf Cauchy}, middle row in Fig.~\\ref{fig:visualisations_A}}\n\nPCA in \\ref{fig:pca:cauchy} and t-SNE in \\ref{fig:tsne:cauchy} manage to organise the classes coherently to some degree, while the standard-AE in \\ref{fig:ae:cauchy}\nfails to produce a convincing result. ESN-AE in \\ref{fig:esn:cauchy} displays a clear separation between all four classes.\nIn particular the presence of magnification factors close to the two classes located in the upper right corner, shows that these two classes are potentially separated by a larger distance to the other two.\n\n\n\n\t\n\n\\paragraph{{\\bf X-ray}, bottom row in Fig.~\\ref{fig:visualisations_A}}\n\n\nAll visualisations clearly separate the $rho$ and $chi$ classes.\nFor standard-AE in \\ref{fig:ae:xray}, the strong magnification suggest that the $chi$ class is quite different to the others. t-SNE in \\ref{fig:tsne:xray} and ESN-AE distinguish in \\ref{fig:esn:xray} the classes in a clearer fashion. ESN-AE exhibits less overlapping projections, but does not put enough distance between  classes $delta$ and $phi$.   The presence of magnifications close to the $chi$ class is a hint that this class is quite different to the other ones.  Still even in the absence of labels (i.e. colour markers), the classes are identifiable in the visualisation produced by ESN-AE.\n\n\\begin{table}[!t]\n\\centering\n\\caption{Mean reconstruction and standard deviation, averaged over 10 runs.}\n\\label{tbl:reconstruction}\n\\begin{tabular}{lrrr}\n\\hline\n\n& \\multicolumn{1}{c}{\\scriptsize \\hspace{-0.25cm} PCA}             &  \\multicolumn{1}{c}{\\hspace{-0.3cm} \\scriptsize standard AE} & \\multicolumn{1}{c}{\\hspace{-0.2cm}  \\scriptsize ESN-AE} \\\\ \\hline\n\\scriptsize \\hspace{-0.25cm}NARMA              & \\hspace{-0.35cm} \\scriptsize 151451.130 $\\pm$ 14984.801 &\\scriptsize 116.041 $\\pm$ 46.606     &\\hspace{-0.1cm}  \\scriptsize 44.126 $\\pm$ 11.962  \\\\ \\hline\n\\scriptsize \\hspace{-0.25cm} Cauchy             & \\hspace{-0.35cm} \\scriptsize 121.110 $\\pm$ 4.022                             &\\hspace{-0.2cm}  \\scriptsize 102.115 $\\pm$ 2.522                     & \\scriptsize 95.176 $\\pm$ 2.675                            \\\\ \\hline\n\\scriptsize \\hspace{-0.25cm} Xray               & \\hspace{-0.35cm} \\scriptsize 25.376 $\\pm$ 2.274                              &\\hspace{-0.2cm}  \\scriptsize 42.727 $\\pm$ 17.423                   &\\scriptsize  21.798 $\\pm$ 1.617                            \\\\ \\hline\n\\scriptsize \\hspace{-0.25cm} Wind              & \\hspace{-0.35cm} \\scriptsize 5.0498 $\\pm$ 0.253                              &\\hspace{-0.2cm}  \\scriptsize 5.192 $\\pm$ 0.260                         & \\scriptsize 5.079 $\\pm$ 0.231                            \\\\ \\hline\n\\scriptsize \\hspace{-0.25cm} Textual &\\hspace{-0.35cm} \\scriptsize  0.691 $\\pm$ 0.007   &\\scriptsize 0.700 $\\pm$ 0.010                         &\\hspace{-0.2cm}  \\scriptsize 0.694 $\\pm$ 0.017                             \\\\ \\hline\n\n\\end{tabular}\n\\end{table}\n\n\n\n\\paragraph{{\\bf Wind}, top row in Fig.~\\ref{fig:visualisations_B}}\n\n\nNone of the visualisations separates the Munich from the Frankfurt stations.\nMatching our  prior expectation, \nESN-AE in \\ref{fig:esn:wind} organises the stations around Hamburg in a single region,\nin contrast to the  other visualisations which show overlap.\nStandard-AE fails to produce a clear result and its magnifications do not\nhelp in its interpretation any further.\n\n\n\n\n\n\n\n\n\n\\paragraph{{\\bf Textual data (symbolic)}, bottom row in Fig.~\\ref{fig:visualisations_B}}\n\n\nThe binary representation of the text data in three different languages shows the true power behind the \nESN-AE equipped here with the logistic function. While other visualisations do not exhibit  adequate separation,  ESN-AE in \\ref{fig:esn:text} exhibits some clear organisation.   Additionally, magnifications suggest some separation between the German and English sequences. The bright magnifications that appear in the unpopulated corners are simply artefacts as the model has not seen any data in these areas.\n\n\n\n\n\n\n\\paragraph{{\\bf Reconstruction}}\n\n\nIn order to give a quantitative impression of the quality of the visualisations,\nwe report reconstruction errors in Table \\ref{tbl:reconstruction}. Each dataset is randomly\npartitioned $10$ times into  equally sized training and test sets.\nFor each partitioning, we train the dimensionality algorithms and measure the\nerror on the test data using Eq. \\eqref{eq:ESN_objective}. \nFor the binary textual data, the error is measured as the fraction of predictions coinciding\nwith the binary test sequences. We exclude t-SNE as it does not offer a way of reconstructing\nweights from the projections.\n\n\n\n\\section{Discussion}\n\\label{sec:discussion}\n\n\n\n\n\nThough the conversion of time-series into fixed-length representations is not new (e.g. \\cite{Grabocka2015}), we \nbelieve that  converting the time series via a non-parametric state space\nmodel with fixed dynamic part (i.e~ESN) in conjunction \nwith  an appropriately defined\nreconstruction function, does provide a new way of performing dimensionality reduction on time series.\nThe results show that the proposed visualisation is better at understanding\nwhat makes sequences (dis)similar as it \nmanages to separate classes that are governed by qualitatively distinct dynamical regimes. Indeed, the produced visualisations\nreflect our prior expectations as to which sequences should be similar.\n\nOf course, combining the ESN with the autoencoder is just one possible scheme, and certainly other dimensionality reduction\nschemes can be devised along this line.\nOne can exchange the ESN with other models such as\nautoregressive-moving-average models (ARMA), and use them to cast the time-series to fixed parameter vectors.\nE.g.~for slow changing signals, models based on Fourier series\nmight be more suitable than the ESN.\nChoosing the ESN to model the temporal features of the sequences, is indeed a subjective choice. However, this does not mean that it is a bad choice: in the relevant literature, a wealth of applications\ndemonstrate that ESNs are good models for \na large variety of real-world time series.\n\n\nBesides the autoencoder, other dimensionality reduction methods that rely on optimising reconstruction error (e.g. GPLVM \\cite{Lawrence2005}) can be adapted to the visualisation of time-series; one has to modify their objective to measure reconstruction in the sequence space, just as the loss function of ESN-AE does.\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\n\n\nWe have presented a method for the visualisation of time series that couples an ESN to an autoencoder. Time series are represented as readout weights of an ESN and are  subsequently compressed to a low dimensional representation by an autoencoder. The autoencoder attempts reconstruction of the readout weights in the context of the state space pertaining to the sequences thanks to the modified loss function. \nIn future research, we plan to work on irregularly sampled time series that originate from eclipsing binary stars. The ESN will be replaced by a physical model that will cast the time series to vectors of physical parameters.\n\n\n\n\\section*{Acknowledgement}\nThe authors from HITS gratefully acknowledge the support of the Klaus Tschira Foundation.\nWe thank Ranjeev Misra for providing the X-ray dataset.\nWe would also like to thank the anonymous reviewers for helping us improve the presented work.\n\n\n\n\n\n\n\n\n\n\n  \\bibliographystyle{plain} \n  \\bibliography{main}\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nWe note that the visualisation space $\\cal V$ does not necessarily reflect  distances between models on $\\cal M$.\nIn Fig.~\\ref{fig:magn_factors}, we see how neighbourhoods of {\\mbox{\\boldmath ${z}$}}, depicted as dotted ellipses, transform on $\\cal M$.  \nThus, in order to interpret  distances  in $\\cal V$, it is important to push-forward the natural notion of distances on $\\cal M$ onto the visualization space $\\cal V$. In the topographic mapping literature the induced metric in the visualization space from the data space is usually represented through magnification factors \\cite{Bishop1997}. In the following we show how magnification factors can be computed in the ESN-AE setting.\n\n\n\nGiven the FIM, one can push forward local distances $\\Delta{\\mbox{\\boldmath ${z}$}}$ from $\\cal M$ onto $\\cal V$  via $\\Delta{\\mbox{\\boldmath ${z}$}}^T {\\mbox{\\boldmath ${F}$}}({\\mbox{\\boldmath ${z}$}}) \\Delta{\\mbox{\\boldmath ${z}$}}$. In particular, at a given point $\\Delta{\\mbox{\\boldmath ${z}$}}$ it is possible to estimate in which direction $d{\\mbox{\\boldmath ${z}$}}$ the distance changes the most. This can be easily calculated by solving the following constrained problem:\n\n", "index": 23, "text": "\\begin{equation}\n\\mbox{maximise} \\ \\Delta{\\mbox{\\boldmath ${z}$}}^T {\\mbox{\\boldmath ${F}$}} \\Delta{\\mbox{\\boldmath ${z}$}} \\ \\ \\  \\mbox{over} \\ \\Delta{\\mbox{\\boldmath ${z}$}}, \\ \\   \\mbox{subject to} \\  \\|\\Delta{\\mbox{\\boldmath ${z}$}}\\|^2=1. \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\mbox{maximise}\\ \\Delta{\\mbox{\\boldmath${z}$}}^{T}{\\mbox{\\boldmath${F}$}}%&#10;\\Delta{\\mbox{\\boldmath${z}$}}\\ \\ \\ \\mbox{over}\\ \\Delta{\\mbox{\\boldmath${z}$}},%&#10;\\ \\ \\mbox{subject to}\\ \\|\\Delta{\\mbox{\\boldmath${z}$}}\\|^{2}=1.\" display=\"block\"><mrow><mrow><mrow><mrow><mpadded width=\"+5pt\"><mtext>maximise</mtext></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>\ud835\udc9b</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc6d</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc9b</mi></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003\u2006</mo><mrow><mpadded width=\"+5pt\"><mtext>over</mtext></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc9b</mi></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mpadded width=\"+5pt\"><mtext>subject to</mtext></mpadded><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\ud835\udc9b</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}]