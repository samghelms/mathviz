[{"file": "1601.05470.tex", "nexttext": "\nand where $x_i$ represents the $i$-th order pseudospectral coefficients. To evaluate the integral in~\\eqref{equation_int}, we choose an $n$-point numerical quadrature rule with points $\\gamma_i \\in \\mathcal{R}$ and associated weights $w_i \\in \\mathcal{R}_{+}$ with $i = 0, \\ldots,n-1$. We remark here that the integral in~\\eqref{equation_int} can be approximated via Monte Carlo sampling---where the quadrature nodes are random points---or via other quadrature methods such as Gauss, Clenshaw-Curtis or Newton-Cotes. The arguments presented in this section are independent of the chosen quadrature rule. The number of points $n$ in the integration rule determines the computational cost; that is, at each quadrature point we will evaluate our physical model to compute the polynomial coefficients. The coefficients obtained from the integral in \\eqref{equation_int} can be expressed as a discrete Fourier transform\n\n", "itemtype": "equation", "pos": 11514, "prevtext": "\n\n\\title{\\textsc{Optimal Quadrature Subsampling for \\\\ Least Squares Polynomial Approximations} }\n\n\\author{Pranay Seshadri\\thanks{Postdoctoral Researcher, Civil and Environmental Engineering, Vanderbilt University, Nashville TN 37235, \\texttt{ps583@cam.ac.uk}}, Akil Narayan\\thanks{Research Computer Scientist, Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT 84112, \\texttt{akil@sci.utah.edu}}, Sankaran Mahadevan\\thanks{John R. Murray Sr. Professor, Civil and Environmental Engineering, Vanderbilt University, Nashville TN 37235, \\texttt{sankaran.mahadevan@vanderbilt.edu}} }\n\n\\maketitle{}\n\\begin{abstract}\nThis paper proposes a new sampling strategy for constructing polynomial chaos approximations for expensive physics simulation models. The proposed approach, \\emph{optimal quadrature subsampling} involves sparsely subsampling an existing tensor grid such that well-conditioned least squares estimates can be computed. By implementing QR column pivoting on our weighted orthogonal design matrix along with a simple total order basis selection strategy, we demonstrate the efficacy of our approach compared with regular tensor grid quadratures and randomized subsampling. We include numerical experiments on an analytical problem and an aerodynamic computational fluid dynamics model. Essentially for the same computational cost as randomized quadrature, the proposed method is able to offer more stable and accurate polynomial coefficient estimates. \n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\\noindent Polynomial chaos is a powerful tool for uncertainty quantification. It approximates a model's response with respect to uncertainty in the input parameters using orthogonal polynomials. The coefficients of these polynomials in turn have very revealing properties. When approximating smooth and continuous model responses, these coefficients can provide vital clues into a model's behavior. They can be used to approximate statistical moments such as the mean and variance, sensitivity parameters such as Sobol indices (see \\cite{Sudret}) and skewness metrics (such as \\cite{Owen} using polynomial expressions given in \\cite{Geraci}) ---all this at a fraction of the cost of other sampling techniques such as Monte Carlo and Latin hypercube sampling. Thus, accurately estimating these coefficients is of value to designers, engineers and analysts. In fact polynomial chaos has been widely used in various engineering disciplines ranging from aeronautics \\cite{Seshadri_LEAK,  Aeroelas} and dynamics and mechanics \\cite{mechanical} to chemical engineering \\cite{chemical}. \n\nBeneath the broad umbrella of polynomial chaos methods there are principally three polynomial approximation techniques: stochastic Galerkin, stochastic collocation and discrete projections (also known as the pseudospectral approximation, nonintrusive polynomial chaos (PC) or nonintrusive spectral projection (NISP) method). Throughout this paper our focus will be on the latter, the pseudospectral approximation method. This method is advantageous for black-box type models because it is nonintrusive and requires only model evaluations at a specific points in the parameter space. It is closely related to stochastic collocation---where Lagrange polynomials are used instead of orthogonal polynomials---which also requires model evaluations at specific points. One of the key assumptions of the pseudospectral method is the that all the input parameters are mutually independent. In cases where this is not the case, one may choose to apply Rosenblatt or Nataf transformations to generate uncorrelated parameter sets \\cite{Ralf}, and then generate pseudospectral approximations. \n\nThe main challenge in computing pseudospectral approximations for a simulation or model, as discussed earlier, is the determination of the pseudospectral coefficients. To obtain these coefficients, one typically computes an integral over the space of the model's response to the uncertainties. This integral may be approximated using a quadrature rule, which requires evaluating the model over a range of design-of-experiment values determined by its quadrature points. In a multidimensional setting, this design-of-experiment set is formed by tensor products of one-dimensional  quadrature rules. These tensor grids are however notoriously expensive, growing exponentially with dimension. Prior endeavors to tackle this curse of dimensionality have been rooted in sparse grids \\cite{Sparse1, Sparse2, SPAM}, which are linear combinations of select tensor product operators. Part of the appeal of sparse grids is that they have favorable nesting properties for certain quadrature families such as Gauss-Patterson and Gauss-Kronrod \\cite{SPAM, Kronrod, Patterson}, which reduce the number of function evaluations required with successive grid refinement\\footnote{We use the term grid refinement loosely in the context of nesting. More concretely stated, nested sets reuse quadrature point evaluations when increasing the \\emph{order} of a tensor grid or the \\emph{level} of a sparse grid.}. To this end, while sparse grids may offer some reduction, they are by no means a viable solution for high dimensional data sets. Smith \\cite{Ralf} provides a relative comparison for the number of points in various levels of sparse and tensor grids (see Table~\\ref{table_compare}), which reflects the cost associated with both sparse and tensor grids. \nA possible path to tackle the \\emph{curse of dimensionality} in pseudospectral approximation techniques may be rooted in least squares. \n\n\n\n\\begin{table}\n \\begin{center}\n   \\caption{Number of sparse and tensor grid quadrature points using Clenshaw-Curtis quadrature with 9 points in each direction, for increasing dimensionality (adapted from Smith \\cite{Ralf}).}\n   \\vspace{3mm}\n   \\label{table_compare}\n   \\begin{tabular}{lcc}\n\n    Dimensions & Sparse grid points & Tensor grid points \\\\\n    \\hline\n2 &  29 & 81\\\\  \n5 &  241 & 59,049\\\\\n10 & 1581 & $>3 \\times 10^{9}$\\\\\n50 & 171,901 & $>5 \\times 10^{47}$\\\\\n\\hline\n\n   \\end{tabular}\n \\end{center}\n\\end{table}\n\n\nWhile the idea of employing least squares techniques in a pseudospectral approximation context is by no means a new idea, new theoretical results have given way to the idea of randomized or subsampled quadratures. In a nutshell, the idea is to evaluate the simulation at only a handful (or subsample) of quadrature points of a full tensor grid, and then construct an approximation using either compressed sensing analogues \\cite{Doos_1, Hampton, Peng, Tang} or least squares \\cite{Zhou}. While compressed sensing (CS) ---which is an $l_1$ norm minimization problem---has seen some use in high-dimensional polynomial surrogate construction, it is best suited for models where a majority of the polynomial coefficients are zero or where the coefficients are rapidly decaying. For a general purpose black-box model---for which we may have little-to-no information about---we may not always be able to make such an assumption. For further details on CS we refer the interested reader to Foucart and Rauhut \\cite{CS_book}, and for applications with polynomials to Doostan and Owhadi \\cite{Doos_1}. Least squares approximation on the other hand---which is an $l_2$ norm minimization problem---makes no such assertion on the coefficients, making it a very attractive option for subsampled pseudospectral approximations.\n\nWhile relatively easy to implement---e.g., a few lines in MATLAB---least squares has known stability issues. For instance, when using Newton-Cotes (equidistant point sets) quadrature it is highly unstable even for an infinitely smooth noiseless function \\cite{Cohen}. Recent theoretical work by \\cite{Migliorati_1,Migliorati_2, Zhou, Cohen} has been centered around determining the stability conditions for least squares, when using \\emph{iid} or random sampling. In Cohen et al. \\cite{Cohen} the authors analyze univariate polynomials and observe for approximating an m-$th$ order polynomial approximately $cm^2$ points are required when sampling randomly from a uniform distribution on the inputs, or approximately $cm$ points when sampling from a Chebyshev distribution, for some constant $c$. Theoretical extensions to multivariate polynomial spaces can be found in Chkifa et al. \\cite{Chkifa}, and with applications to orthogonal polynomials in tensor and total order indices in \\cite{Zhou}. We defer a more mathematical treatment of the literature review to sections~\\ref{sec:poly} once we have introduced the relevant notation. \n\nWhile the focus of prior research in least squares pseudospectral approximation has been driven by the \\emph{curse of dimensionality}, the reduction in the number of samples has been largely restricted to total order or hyperbolic index sets \\cite{Zhou}. This is still not sufficient for generating polynomial responses when individual simulations may take several hours on multi-core machines. Motivated by this, in this paper, we introduce a new approach for deterministically subsampling quadratures in the context of pseduospectral approximations. We term this \\emph{optimal quadratures}. The central idea is to select just as many basis functions as available subsamples and use a QR factorization column pivoting strategy to determine which subsamples to pick. Details of this approach are in Section~\\ref{sec:optimal}. We provide analytical examples in the former Section and a CFD case study in Section~\\ref{sec:numex}. In the spirit of reproducible research all our results and codes can be found at \\url{https://bitbucket.org/psesh/optimal_quadrature}. Our codes use modules from Paul Constantine and David Gleich's \\texttt{pmpack} package, available at \\url{http://www.mathworks.com/matlabcentral/fileexchange/29228-pmpack-parameterized-matrix-package}. \n\n\n\\section{Weighted least squares pseudospectral approximations}\n\\label{sec:poly}\nIn this section we begin by providing a cursory overview of pseudospectral approximations. This is followed by the weighted least squares technique for computing the pseudospectral coefficients. For a more thorough treatment of pseudospectral approximations, we refer the reader to Chapters 10 and 11 of \\cite{Ralf} and \\cite{SPAM}. We remark that our notation closely follows that of Constantine et al. in \\cite{SPAM}, where matrices are used instead of nested summations. \n\n\\subsection{Pseudospectral approximations}\nLet $\\xi$ be a random variable with probability density $\\rho$ defined on the interval $\\mathcal{R}$. We define $P_0$ as the subspace of constant functions on $\\mathcal{R}$.  Let $P_n$ be the space of degree $n$  polynomials on $\\mathcal{R}$ that are orthogonal to the space $P_{n-1}$.  Orthogonality is defined in terms of the weighted $L^2$ norm induced by $\\rho$. In this space, let $\\psi_n(\\xi)$ represent the set of polynomials of order $n$ that are orthogonal to the weight function $\\rho(\\xi)$. By the definition of orthogonality we have $\\left\\langle \\psi_{i}\\left(\\xi\\right)\\psi_{j}\\left(\\xi\\right)\\right\\rangle_\\rho =\\delta_{ij}$, where $\\left\\langle \\cdot,\\cdot\\right\\rangle _{\\rho}$ denotes the $L^2$ inner product over the interval $\\mathcal{R}$ with the weight $\\rho(\\xi)$, and where $\\delta_{ij}$ is the Kronecker delta.\n\nConsider a function $g=g(\\xi)$ that represents the response of a physical model with random variables that characterize the uncertainty in the physical system. We can approximate $g$ through a pseudospectral approximation ($f$) of order $m$:\n\n", "index": 1, "text": "\\begin{equation}\ng\\left(\\xi\\right)\\approx f\\left(\\xi\\right)=\\sum_{i=0}^{m-1}x_{i}\\psi_{i}\\left(\\xi\\right), \\; \\; \\textrm{where} \\; \\; x_{i}=\\int_{\\mathcal{R}}g\\left(\\xi\\right)\\psi_{i}\\left(\\xi\\right)\\rho\\left(\\xi\\right)d\\xi, \n\\label{equation_int}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"g\\left(\\xi\\right)\\approx f\\left(\\xi\\right)=\\sum_{i=0}^{m-1}x_{i}\\psi_{i}\\left(%&#10;\\xi\\right),\\;\\;\\textrm{where}\\;\\;x_{i}=\\int_{\\mathcal{R}}g\\left(\\xi\\right)\\psi%&#10;_{i}\\left(\\xi\\right)\\rho\\left(\\xi\\right)d\\xi,\" display=\"block\"><mrow><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow></mrow><mo>\u2248</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow></mrow></mrow></mrow><mo rspace=\"8.1pt\">,</mo><mrow><mrow><mpadded width=\"+5.6pt\"><mtext>where</mtext></mpadded><mo>\u2062</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\u211b</mi></msub><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo>(</mo><mi>\u03be</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>\u03be</mi></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 12691, "prevtext": "\nand where $x_i$ represents the $i$-th order pseudospectral coefficients. To evaluate the integral in~\\eqref{equation_int}, we choose an $n$-point numerical quadrature rule with points $\\gamma_i \\in \\mathcal{R}$ and associated weights $w_i \\in \\mathcal{R}_{+}$ with $i = 0, \\ldots,n-1$. We remark here that the integral in~\\eqref{equation_int} can be approximated via Monte Carlo sampling---where the quadrature nodes are random points---or via other quadrature methods such as Gauss, Clenshaw-Curtis or Newton-Cotes. The arguments presented in this section are independent of the chosen quadrature rule. The number of points $n$ in the integration rule determines the computational cost; that is, at each quadrature point we will evaluate our physical model to compute the polynomial coefficients. The coefficients obtained from the integral in \\eqref{equation_int} can be expressed as a discrete Fourier transform\n\n", "index": 3, "text": "\\begin{equation}\n\\vx = \\Psi \\mW^2 \\vg,\n\\label{DFT}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\vx=\\Psi\\mW^{2}\\vg,\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo>=</mo><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mn>2</mn></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nHere $\\Psi(i,j) = \\psi_{i}(\\gamma_{j})$ with dimensions $\\Psi \\in \\mathbb{R}^{m \\times n}$ is a matrix of the first $m-$th order orthogonal polynomials evaluated at an $n-$th point quadrature rule. In the case where $m=n$, $\\Psi$ and $\\mW$ are square matrices. To compute the approximation to the model $g(\\xi)$, the inverse DFT can be used\n\n", "itemtype": "equation", "pos": 12763, "prevtext": "\nwhere\n\n", "index": 5, "text": "\\begin{equation}\n\\vx=\\left[\\begin{array}{c}\nx_{0}\\\\\n\\vdots\\\\\nx_{m-1}\n\\end{array}\\right],\\; \n\\mW=\\left[\\begin{array}{ccc}\n\\sqrt{w_{0}}\\\\\n & \\ddots\\\\\n &  & \\sqrt{w_{n-1}}\n\\end{array}\\right], \\; \n\\vg=\\left[\\begin{array}{c}\ng\\left(\\gamma_{0}\\right)\\\\\n\\vdots\\\\\ng\\left(\\gamma_{n-1}\\right)\n\\end{array}\\right].\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\vx=\\left[\\begin{array}[]{c}x_{0}\\\\&#10;\\vdots\\\\&#10;x_{m-1}\\end{array}\\right],\\;\\mW=\\left[\\begin{array}[]{ccc}\\sqrt{w_{0}}\\\\&#10;&amp;\\ddots\\\\&#10;&amp;&amp;\\sqrt{w_{n-1}}\\end{array}\\right],\\;\\vg=\\left[\\begin{array}[]{c}g\\left(\\gamma%&#10;_{0}\\right)\\\\&#10;\\vdots\\\\&#10;g\\left(\\gamma_{n-1}\\right)\\end{array}\\right].\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>x</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><msub><mi>x</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo rspace=\"5.3pt\">,</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msqrt><msub><mi>w</mi><mn>0</mn></msub></msqrt></mtd><mtd/><mtd/></mtr><mtr><mtd/><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22f1</mi></mtd><mtd/></mtr><mtr><mtd/><mtd/><mtd columnalign=\"center\"><msqrt><msub><mi>w</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub></msqrt></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo rspace=\"5.3pt\">,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03b3</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03b3</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 13422, "prevtext": "\nHere $\\Psi(i,j) = \\psi_{i}(\\gamma_{j})$ with dimensions $\\Psi \\in \\mathbb{R}^{m \\times n}$ is a matrix of the first $m-$th order orthogonal polynomials evaluated at an $n-$th point quadrature rule. In the case where $m=n$, $\\Psi$ and $\\mW$ are square matrices. To compute the approximation to the model $g(\\xi)$, the inverse DFT can be used\n\n", "index": 7, "text": "\\begin{equation}\n\\vf=\\Psi^{T}\\vx,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\vf=\\Psi^{T}\\vx,\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vf</mtext></merror><mo>=</mo><mrow><msup><mi mathvariant=\"normal\">\u03a8</mi><mi>T</mi></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nHere $\\Psi \\in \\mathbb{R}^{m \\times k}$, where $k$ represents the number of points $[\\xi_0, \\ldots, \\xi_k]$ within the support of $\\xi$. If we substitute the $n$ quadrature points in~\\eqref{equ_iDFT}, then we obtain the polynomial approximation to the model at the $n$ quadrature points given by $\\vg$.\n\n\\subsection{Index sets}\nOur discussion so far has been limited to the model output $g$ being univariate with $d=1$. For multivariate cases ($d>1$), the above notions extend via tensor product constructions. For a given multi-index, $\\vm=\\left(m_{1},\\ldots,m_{d}\\right)\\in\\mathbb{M}^{d}$, we define the set of multi-indices, $\\mathcal{I}_{\\vm}=\\left\\{ \\vi:\\vi\\in\\mathbb{M}^{d},1\\leq i_{k}\\leq m_{k},k=1,\\ldots,d\\right\\}$. This index set can be used to reference components for tensor product extensions.\n\nThere are three well known index sets, suited for a range of polynomial spaces: tensor order indices (analogous to tensor products), total order indices and hyperbolic cross \\cite{Sudret_hyperbolic} index sets. Tensor product spaces are governed by the rule $max_j \\; \\left|i_j\\right| \\leq m$ and have a cardinality (number of elements) of $(m+1)^d$. Total order index sets are given by $\\sum_{j}=\\left|i_{j}\\right| \\leq m$. Total order indices disregard the higher order interactions in tensor grids and have a cardinality of $\\left(\\begin{array}{c}\nm+d\\\\\nm\n\\end{array}\\right)$. Sparse grids under a linear growth rule are used for computing pseudospectral coefficients (using the DFT) associated with total order index sets. Finally, hyperbolic cross spaces are governed by the rule $\\prod_{j}\\left(i_{j}+1\\right)\\leq m+1$. While the cardinality of this space is difficult to specify precisely, an approximation is given by $(m+1)(1+log(m+1))^{d-1}$ \\cite{Zhou}. In this paper, we will be using total index sets and constraining the number of elements depending on the number of subsample evaluations permitted. This will be explained in detail in the following section. Before closing, a few more points on multi-dimensional pseudospectral approximations are in order. The multivariate discrete Fourier transform can be written as per~\\eqref{DFT}, where \n\n", "itemtype": "equation", "pos": 13477, "prevtext": "\nwhere\n\n", "index": 9, "text": "\\begin{equation}\nf=\\left[\\begin{array}{c}\nf\\left(\\xi_{0}\\right)\\\\\n\\vdots\\\\\nf\\left(\\xi_{k}\\right)\n\\end{array}\\right],\\Psi=\\left[\\begin{array}{ccc}\n\\psi_{0}\\left(\\xi_{0}\\right) & \\ldots & \\psi_{0}\\left(\\xi_{k}\\right)\\\\\n\\vdots & \\ddots\\\\\n\\psi_{m-1}\\left(\\xi_{0}\\right) &  & \\psi_{m-1}\\left(\\xi_{k}\\right)\n\\end{array}\\right].\n\\label{equ_iDFT}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"f=\\left[\\begin{array}[]{c}f\\left(\\xi_{0}\\right)\\\\&#10;\\vdots\\\\&#10;f\\left(\\xi_{k}\\right)\\end{array}\\right],\\Psi=\\left[\\begin{array}[]{ccc}\\psi_{0%&#10;}\\left(\\xi_{0}\\right)&amp;\\ldots&amp;\\psi_{0}\\left(\\xi_{k}\\right)\\\\&#10;\\vdots&amp;\\ddots\\\\&#10;\\psi_{m-1}\\left(\\xi_{0}\\right)&amp;&amp;\\psi_{m-1}\\left(\\xi_{k}\\right)\\end{array}%&#10;\\right].\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>=</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03c8</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u2026</mi></mtd><mtd columnalign=\"center\"><mrow><msub><mi>\u03c8</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22f1</mi></mtd><mtd/></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>\u03c8</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow></mtd><mtd/><mtd columnalign=\"center\"><mrow><msub><mi>\u03c8</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>\u03be</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nHere the quadrature points and weights are given as tensor products of the 1-D quadrature points and weights. More concretely, the quadrature points are given by the set $\\Gamma=\\left\\{ \\gamma_{\\vi}=\\left(\\gamma_{i_{1}},\\ldots,\\gamma_{i_{d}}\\right):\\vi\\in\\mathcal{I}_{\\vm}\\right\\}$, and the weights by \n$\\Xi=\\left\\{ \\sqrt{\\omega}_{\\vi}=\\sqrt{\\omega_{i_{1}}}\\ldots\\sqrt{\\omega_{i_{d}}}:\\vi\\in\\mathcal{I}_{\\vm}\\right\\}$ \\cite{SPAM}. Thus, $\\vg$ as per equation \\eqref{DFT} is now the vector of function evaluations at the tensor grid of quadrature points given by $\\Gamma$. \n\n\\subsection{Weighted least squares}\nIn this section we outline the weighted least squares approximation for computing the pseudospectral coefficients. \n\\begin{definition}(Design matrix)\n\\label{design_matrix}\nLet us define our design matrix as $\\mA \\in \\mathbb{R}^{n \\times m}$, where $m=n$. This imples that the number of basis terms is equivalent to the number of quadrature points. Also, let the weighted function evaluations $\\vb \\in \\mathbb{R}^{n}$, be given by  \n\n", "itemtype": "equation", "pos": 15997, "prevtext": "\nHere $\\Psi \\in \\mathbb{R}^{m \\times k}$, where $k$ represents the number of points $[\\xi_0, \\ldots, \\xi_k]$ within the support of $\\xi$. If we substitute the $n$ quadrature points in~\\eqref{equ_iDFT}, then we obtain the polynomial approximation to the model at the $n$ quadrature points given by $\\vg$.\n\n\\subsection{Index sets}\nOur discussion so far has been limited to the model output $g$ being univariate with $d=1$. For multivariate cases ($d>1$), the above notions extend via tensor product constructions. For a given multi-index, $\\vm=\\left(m_{1},\\ldots,m_{d}\\right)\\in\\mathbb{M}^{d}$, we define the set of multi-indices, $\\mathcal{I}_{\\vm}=\\left\\{ \\vi:\\vi\\in\\mathbb{M}^{d},1\\leq i_{k}\\leq m_{k},k=1,\\ldots,d\\right\\}$. This index set can be used to reference components for tensor product extensions.\n\nThere are three well known index sets, suited for a range of polynomial spaces: tensor order indices (analogous to tensor products), total order indices and hyperbolic cross \\cite{Sudret_hyperbolic} index sets. Tensor product spaces are governed by the rule $max_j \\; \\left|i_j\\right| \\leq m$ and have a cardinality (number of elements) of $(m+1)^d$. Total order index sets are given by $\\sum_{j}=\\left|i_{j}\\right| \\leq m$. Total order indices disregard the higher order interactions in tensor grids and have a cardinality of $\\left(\\begin{array}{c}\nm+d\\\\\nm\n\\end{array}\\right)$. Sparse grids under a linear growth rule are used for computing pseudospectral coefficients (using the DFT) associated with total order index sets. Finally, hyperbolic cross spaces are governed by the rule $\\prod_{j}\\left(i_{j}+1\\right)\\leq m+1$. While the cardinality of this space is difficult to specify precisely, an approximation is given by $(m+1)(1+log(m+1))^{d-1}$ \\cite{Zhou}. In this paper, we will be using total index sets and constraining the number of elements depending on the number of subsample evaluations permitted. This will be explained in detail in the following section. Before closing, a few more points on multi-dimensional pseudospectral approximations are in order. The multivariate discrete Fourier transform can be written as per~\\eqref{DFT}, where \n\n", "index": 11, "text": "\\begin{equation}\n\\Psi=\\Psi_{m_{1}}\\otimes\\ldots\\otimes\\Psi_{m_{d}},\\; \\; \\; \\; \\mW=\\mW_{m_{1}}\\otimes\\ldots\\otimes \\mW_{m_{d}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\Psi=\\Psi_{m_{1}}\\otimes\\ldots\\otimes\\Psi_{m_{d}},\\;\\;\\;\\;\\mW=\\mW_{m_{1}}%&#10;\\otimes\\ldots\\otimes\\mW_{m_{d}},\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><msub><mi>m</mi><mn>1</mn></msub></msub><mo>\u2297</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2297</mo><msub><mi mathvariant=\"normal\">\u03a8</mi><msub><mi>m</mi><mi>d</mi></msub></msub></mrow></mrow><mo rspace=\"13.7pt\">,</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mo>=</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><msub><mi>m</mi><mn>1</mn></msub></msub><mo>\u2297</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2297</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><msub><mi>m</mi><mi>d</mi></msub></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nwhere $\\mW$, $\\Psi$ and $\\vg$ are as previously defined. For $d>1$, note that the construction of $\\mA$ is simply given by the following tensor products\n\n", "itemtype": "equation", "pos": 17182, "prevtext": "\nHere the quadrature points and weights are given as tensor products of the 1-D quadrature points and weights. More concretely, the quadrature points are given by the set $\\Gamma=\\left\\{ \\gamma_{\\vi}=\\left(\\gamma_{i_{1}},\\ldots,\\gamma_{i_{d}}\\right):\\vi\\in\\mathcal{I}_{\\vm}\\right\\}$, and the weights by \n$\\Xi=\\left\\{ \\sqrt{\\omega}_{\\vi}=\\sqrt{\\omega_{i_{1}}}\\ldots\\sqrt{\\omega_{i_{d}}}:\\vi\\in\\mathcal{I}_{\\vm}\\right\\}$ \\cite{SPAM}. Thus, $\\vg$ as per equation \\eqref{DFT} is now the vector of function evaluations at the tensor grid of quadrature points given by $\\Gamma$. \n\n\\subsection{Weighted least squares}\nIn this section we outline the weighted least squares approximation for computing the pseudospectral coefficients. \n\\begin{definition}(Design matrix)\n\\label{design_matrix}\nLet us define our design matrix as $\\mA \\in \\mathbb{R}^{n \\times m}$, where $m=n$. This imples that the number of basis terms is equivalent to the number of quadrature points. Also, let the weighted function evaluations $\\vb \\in \\mathbb{R}^{n}$, be given by  \n\n", "index": 13, "text": "\\begin{equation}\n\\mA=\\mW^{T} \\Psi^T \\; \\; \\textrm{and} \\; \\; \\vb = \\mW^T \\vg,\n\\label{design}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mA=\\mW^{T}\\Psi^{T}\\;\\;\\textrm{and}\\;\\;\\vb=\\mW^{T}\\vg,\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>=</mo><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><mpadded width=\"+5.6pt\"><msup><mi mathvariant=\"normal\">\u03a8</mi><mi>T</mi></msup></mpadded><mo>\u2062</mo><mpadded width=\"+5.6pt\"><mtext>and</mtext></mpadded><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vb</mtext></merror></mrow><mo>=</mo><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\n\\end{definition}\nWe remark here that the columns of $\\mA$ are orthonormal if and only if the quadrature rule chosen integrates polynomials of a sufficiently high accuracy. If $m_i$ is the number of basis functions in dimension $i$, then for $\\mA^T \\mA = \\mI$, the quadrature rule should integrate all polynomials of order $2m_i - 1$ for dimensions $i=1,\\ldots,d$. Under this restriction, using the pseudospectral approximation via the DFT in~\\eqref{DFT} we have:\n\n", "itemtype": "equation", "pos": 17443, "prevtext": "\nwhere $\\mW$, $\\Psi$ and $\\vg$ are as previously defined. For $d>1$, note that the construction of $\\mA$ is simply given by the following tensor products\n\n", "index": 15, "text": "\\begin{equation}\n\\mA=\\left(\\mW_{m_{1}}^{T}\\Psi_{m_{1}}^{T}\\right)\\otimes\\ldots\\otimes\\left(\\mW_{m_{d}}^{T}\\Psi_{m_{d}}^{T}\\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\mA=\\left(\\mW_{m_{1}}^{T}\\Psi_{m_{1}}^{T}\\right)\\otimes\\ldots\\otimes\\left(\\mW_%&#10;{m_{d}}^{T}\\Psi_{m_{d}}^{T}\\right).\" display=\"block\"><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>=</mo><mrow><mrow><mo>(</mo><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><msub><mi>m</mi><mn>1</mn></msub><mi>T</mi></msubsup><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a8</mi><msub><mi>m</mi><mn>1</mn></msub><mi>T</mi></msubsup></mrow><mo>)</mo></mrow><mo>\u2297</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2297</mo><mrow><mo>(</mo><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><msub><mi>m</mi><mi>d</mi></msub><mi>T</mi></msubsup><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u03a8</mi><msub><mi>m</mi><mi>d</mi></msub><mi>T</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nUsing the definition of $\\mA$ from~\\eqref{design}, this yields $\\vb=\\mW^T \\left(\\mA^T \\mA \\right)\\vg = \\mW^T \\vg$. This shows that the relation $\\mA \\vx = \\vb$ holds. It should be noted that the solution to the least squares problem given by\n\n", "itemtype": "equation", "pos": 18052, "prevtext": "\n\\end{definition}\nWe remark here that the columns of $\\mA$ are orthonormal if and only if the quadrature rule chosen integrates polynomials of a sufficiently high accuracy. If $m_i$ is the number of basis functions in dimension $i$, then for $\\mA^T \\mA = \\mI$, the quadrature rule should integrate all polynomials of order $2m_i - 1$ for dimensions $i=1,\\ldots,d$. Under this restriction, using the pseudospectral approximation via the DFT in~\\eqref{DFT} we have:\n\n", "index": 17, "text": "\\begin{equation}\n\\mA \\vx=\\mA\\left(\\Psi \\mW^{2}\\vg\\right)=\\mW^T \\Psi^T \\Psi \\mW^{2}\\vg = \\mW^T \\left(\\mW^T\\Psi^{T}\\Psi \\mW\\right)\\vg.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\mA\\vx=\\mA\\left(\\Psi\\mW^{2}\\vg\\right)=\\mW^{T}\\Psi^{T}\\Psi\\mW^{2}\\vg=\\mW^{T}%&#10;\\left(\\mW^{T}\\Psi^{T}\\Psi\\mW\\right)\\vg.\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror></mrow><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mn>2</mn></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mi>T</mi></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mn>2</mn></msup><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror></mrow><mo>=</mo><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><mrow><mo>(</mo><mrow><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror><mi>T</mi></msup><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mi>T</mi></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mW</mtext></merror></mrow><mo>)</mo></mrow><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vg</mtext></merror></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nis identical to the solution obtained via the DFT in~\\eqref{DFT} provided that the quadrature grid exactly integrates polynomials of a certain accuracy. Thus computing all the coefficients associated with the full tensor grid of Gaussian quadrature points through least squares is no different than computing the coefficients via the DFT. Our objective here however is not to compute all the coefficients, but just a few. This introduces the notion of subsampling. As mentioned earlier, instead of evaluating the simulation or model at each quadrature point in the parameter space, we will only use a small subsample.  \n\\begin{definition} (Subsampled design matrix) In the context of our definitions, the authors in \\cite{Zhou} create a \\emph{subsampled design matrix} $\\mA_{*} \\in \\mathbb{R}^{l \\times m}$ which comprises of $m < n$ columns and $l < n$ rows of $\\mA$. This results in the system $\\mA_{*} \\tilde{\\vx} = \\vb_{\\ast}$, where $\\tilde{\\vx} \\in \\mathbb{R}^{m}$ and $\\vb_{\\ast} \\in \\mathbb{R}^{l}$. The $(\\cdot)_{\\ast}$ symbol is used to indicate a subvector of the argument. \n\\end{definition}\n\nTo solve the least squares problem Zhou et al. \\cite{Zhou} require that $l >> m$. While the rows are selected randomly---corresponding to randomly subsampling $l$ points from the the full tensor grid of points $n$---the columns are selected either based on a total order index or a hyperbolic cross space. Once $\\mA_{*}$ and $\\vb_{\\ast}$ have been evaluated---using either basis---$\\tilde{\\vx}$ can be obtained by solving the normal equations or via QR factorization. As discussed in \\cite{Moler} the QR factorization method is preferred for numerical stability.\n\n\\subsection{Stability and condition numbers}\nWe end this section with a brief note on the stability of the weighted least squares pseudospectral approximation. Perhaps the simplest way to characterize the stability of a least squares solution is to examine how small perturbations in the \\emph{subsampled design matrix} $\\mA_{\\ast} \\in \\mathbb{R}^{l \\times m}$ affect the solution $\\tilde{\\vx} \\in \\mathbb{R}^{m}$. This can be done by taking the singular value decomposition of $\\mA_{*}$:\n\n", "itemtype": "equation", "pos": 18442, "prevtext": "\nUsing the definition of $\\mA$ from~\\eqref{design}, this yields $\\vb=\\mW^T \\left(\\mA^T \\mA \\right)\\vg = \\mW^T \\vg$. This shows that the relation $\\mA \\vx = \\vb$ holds. It should be noted that the solution to the least squares problem given by\n\n", "index": 19, "text": "\\begin{equation}\n\\hat{\\vx}=\\textrm{argmin} \\; \\; \\left\\Vert \\mA \\vx - \\vb \\right\\Vert_{2} \n\\label{equ_lsqr}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\vx}=\\textrm{argmin}\\;\\;\\left\\|\\mA\\vx-\\vb\\right\\|_{2}\" display=\"block\"><mrow><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mpadded width=\"+5.6pt\"><mtext>argmin</mtext></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror></mrow><mo>-</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vb</mtext></merror></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nSubstituting in $\\tilde{\\vx} = \\mA^{\\dagger}_{\\ast} \\vb_{\\ast}$ (where $\\mA^{\\dagger}_{\\ast}$ is the pseudoinverse of $\\mA_{\\ast}$) yields\n\n", "itemtype": "equation", "pos": 20722, "prevtext": "\nis identical to the solution obtained via the DFT in~\\eqref{DFT} provided that the quadrature grid exactly integrates polynomials of a certain accuracy. Thus computing all the coefficients associated with the full tensor grid of Gaussian quadrature points through least squares is no different than computing the coefficients via the DFT. Our objective here however is not to compute all the coefficients, but just a few. This introduces the notion of subsampling. As mentioned earlier, instead of evaluating the simulation or model at each quadrature point in the parameter space, we will only use a small subsample.  \n\\begin{definition} (Subsampled design matrix) In the context of our definitions, the authors in \\cite{Zhou} create a \\emph{subsampled design matrix} $\\mA_{*} \\in \\mathbb{R}^{l \\times m}$ which comprises of $m < n$ columns and $l < n$ rows of $\\mA$. This results in the system $\\mA_{*} \\tilde{\\vx} = \\vb_{\\ast}$, where $\\tilde{\\vx} \\in \\mathbb{R}^{m}$ and $\\vb_{\\ast} \\in \\mathbb{R}^{l}$. The $(\\cdot)_{\\ast}$ symbol is used to indicate a subvector of the argument. \n\\end{definition}\n\nTo solve the least squares problem Zhou et al. \\cite{Zhou} require that $l >> m$. While the rows are selected randomly---corresponding to randomly subsampling $l$ points from the the full tensor grid of points $n$---the columns are selected either based on a total order index or a hyperbolic cross space. Once $\\mA_{*}$ and $\\vb_{\\ast}$ have been evaluated---using either basis---$\\tilde{\\vx}$ can be obtained by solving the normal equations or via QR factorization. As discussed in \\cite{Moler} the QR factorization method is preferred for numerical stability.\n\n\\subsection{Stability and condition numbers}\nWe end this section with a brief note on the stability of the weighted least squares pseudospectral approximation. Perhaps the simplest way to characterize the stability of a least squares solution is to examine how small perturbations in the \\emph{subsampled design matrix} $\\mA_{\\ast} \\in \\mathbb{R}^{l \\times m}$ affect the solution $\\tilde{\\vx} \\in \\mathbb{R}^{m}$. This can be done by taking the singular value decomposition of $\\mA_{*}$:\n\n", "index": 21, "text": "\\begin{equation}\n\\mA_{\\ast} = \\mU \\Sigma \\mV^{T}=\\sum_{i=1}^{n}\\sigma_{i} \\vu_{i} \\vv_{i}^{T}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mA_{\\ast}=\\mU\\Sigma\\mV^{T}=\\sum_{i=1}^{n}\\sigma_{i}\\vu_{i}\\vv_{i}^{T}.\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2217</mo></msub><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mU</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a3</mi><mo>\u2062</mo><msup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mV</mtext></merror><mi>T</mi></msup></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vu</mtext></merror><mi>i</mi></msub><mo>\u2062</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vv</mtext></merror><mi>i</mi><mi>T</mi></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nFrom~\\eqref{stability_equ} it is lucid that for small eigenvalues $\\sigma_{i}$, small changes in $\\mA_{\\ast} $ can result in large changes in $\\tilde{\\vx}$ \\cite{Golub_book}, making the least squares solution unstable. This relationship between the eigenvalues of $\\mA_{\\ast} $ and stability can be more precisely quantified with the definition of the condition number. Formally, it is defined as the ratio of the largest and smallest eigenvalues of $\\mA_{*} $:\n\n", "itemtype": "equation", "pos": 20971, "prevtext": "\nSubstituting in $\\tilde{\\vx} = \\mA^{\\dagger}_{\\ast} \\vb_{\\ast}$ (where $\\mA^{\\dagger}_{\\ast}$ is the pseudoinverse of $\\mA_{\\ast}$) yields\n\n", "index": 23, "text": "\\begin{equation}\n\\tilde{\\vx}=\\sum_{i=1}^{n}\\frac{\\vu_{i}^{T}\\vb_{\\ast}}{\\sigma_{i}}\\vv_{i}.\n\\label{stability_equ}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\vx}=\\sum_{i=1}^{n}\\frac{\\vu_{i}^{T}\\vb_{\\ast}}{\\sigma_{i}}\\vv_{i}.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vu</mtext></merror><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vb</mtext></merror><mo>\u2217</mo></msub></mrow><msub><mi>\u03c3</mi><mi>i</mi></msub></mfrac><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vv</mtext></merror><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nIn general, larger $\\kappa$ values lead to more unstable and inaccurate solutions.  \n\n\n\\section{Proposed approach}\n\\label{sec:optimal}\nWe now discuss the main contribution of this paper. Instead of using randomized samples on total order or hyperbolic cross spaces, we present a different sampling strategy that is inherently deterministic and optimal. We term our sampling strategy \\emph{optimal quadrature} as it seeks to obtain the best least squares estimate---by reducing the condition number of the linear system---for a given set of basis functions. We also describe our strategy for basis selection. \n\n\\subsection{Preliminaries}\nPrior to detailing the proposed approach, we provide a few key definitions that will set the stage for our \\emph{optimal quadrature} technique---which utilizes QR column pivoting. \n\\begin{definition} (Permutation matrix)\nA permutation matrix is an orthogonal matrix with permuted rows of an identity matrix. Products of permutation matrices are also permutations \\cite{Hansen}. Consider a integer valued vector $\\textsf{v}$ in $\\mathbb{R}^{n}$ with each entry $\\leq n$ and with no repetitions. The integers can be arranged in any permutation. We call this vector, denoted by $\\textsf{v}$, a permutation vector. We define its corresponding permutation matrix $\\Pi \\in \\mathbb{R}^{n \\times n}$ as an identity matrix where the rows are reordered based on the values in $\\textsf{v}$. Thus, if the first component of $\\textsf{v}$ is 2, the first row of the permutation matrix is $[0, 1, 0, 0, \\ldots]$ and so on. \n\\end{definition}\n\\begin{definition}(Matrix rank and range)\nThe rank of a matrix $\\mD \\in \\mathbb{R}^{n \\times m}$ is the maximum number of linearly independent rows or columns and is given by $\\textsf{rank}(\\mD)$. If the columns of $\\mD$ can be partitioned into a form $\\mD=\\left[\\vd_{1}\\left|\\ldots\\right|\\vd_{m}\\right]$, then the range can be written as $\\textsf{ran} \\left(D\\right)=\\textsf{span}\\left\\{ \\vd_{1},\\ldots,\\vd_{m}\\right\\}$. \n\\end{definition}\n\nThe objective of QR column pivoting is to find a permutation matrix $\\Pi$, such that the first $r=\\textsf{rank}(\\mD)$ columns of $\\mD \\Pi$, with $\\mD \\in \\mathbb{R}^{n \\times m}$ are linearly independent and span $\\textsf{ran}(\\mD)$. This gives rise to the notion of \\emph{rank revealing QR factorization}.\n\\begin{definition} (Rank-revealing QR factorization (page 95 in \\cite{Hansen}) ). Let $\\mD \\in \\mathbb{R}^{n \\times m}$ have singular values $\\sigma_{1}\\geq\\sigma_{2}\\geq\\ldots \\geq \\sigma_{r} \\geq \\sigma_{r+1} \\ldots \\geq\\sigma_{n}\\geq0$ where there is a gap between $\\sigma_{r}$ and $\\sigma_{r + 1}$. Here $r$ is the numerical rank of $\\mD$. Then, the rank revealing QR factorization is given by\n\n", "itemtype": "equation", "pos": 21562, "prevtext": "\nFrom~\\eqref{stability_equ} it is lucid that for small eigenvalues $\\sigma_{i}$, small changes in $\\mA_{\\ast} $ can result in large changes in $\\tilde{\\vx}$ \\cite{Golub_book}, making the least squares solution unstable. This relationship between the eigenvalues of $\\mA_{\\ast} $ and stability can be more precisely quantified with the definition of the condition number. Formally, it is defined as the ratio of the largest and smallest eigenvalues of $\\mA_{*} $:\n\n", "index": 25, "text": "\\begin{equation}\n\\kappa=\\frac{\\sigma_{max}\\left(\\mA_{*} \\right)}{\\sigma_{min}\\left(\\mA_{\\ast} \\right)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\kappa=\\frac{\\sigma_{max}\\left(\\mA_{*}\\right)}{\\sigma_{min}\\left(\\mA_{\\ast}%&#10;\\right)}.\" display=\"block\"><mrow><mrow><mi>\u03ba</mi><mo>=</mo><mfrac><mrow><msub><mi>\u03c3</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>x</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>*</mo></msub><mo>)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mrow><mi>m</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>n</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2217</mo></msub><mo>)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nif $\\kappa(\\mD) \\simeq \\sigma_{1}/\\sigma_{r}$ and $\\left\\Vert \\mR_{22}\\right\\Vert \\simeq\\sigma_{r+1}$, where $\\mR_{11} \\in \\mathbb{R}^{r \\times r}$ is upper triangular and $\\mR_{22}$ is a submatrix of a small norm. \n\\label{defn_RRQR}\n\\end{definition}\n\nQR factorizations with a permutation matrix can be computed using Householder reflections with column perturbations iteratively. As Hansen et al. \\cite{Hansen} note, there are two key computational aspects to the stepwise procedure for obtaining a permutation matrix---which always exists (see Hong and Pan \\cite{Hong}). The first is determining the termination criterion. Definition~\\ref{defn_RRQR} addresses this---when $\\mR_{11}$ embeds the linearly independent information of $\\mD$ and has numerical rank $r$. The second is determining how to select the most linearly independent columns of $\\mD$. One can think of the latter as solving a dual-objective optimization problem with the aim of minimizing the norm of $\\mR_{22}$ and maximizing $\\sigma_r$ \\cite{Hansen}. This is covered by Businger and Golub's (1965) \\emph{Householder QR with column pivoting} algorithm (see page 278 in \\cite{Golub_book}). We refer the interested reader to the aforementioned texts for further details.\n\n\\subsection{Basis selection}\nOur basis selection strategy is particularly straightforward. Seeing as most engineering quantities of interest within design optimization and uncertainty quantification modules are integrated across the geometry or flow-field; they tend to be captured well by first and second order terms. Thus, we arrange all our basis functions in increasing order of their total index and use the first $m$ terms only. \n\n\\subsection{Subsampling with QR column pivoting}\nOur subsampling strategy attempts to reduce the condition number of our \\emph{subsampled design matrix}, $\\mA_{\\ast}$. To aid this effort, we will employ a QR column pivoting strategy. This strategy is also known as constructing \\emph{approximate Fekete points} \\cite{AFP} and has been used in the context of finding near-optimal interpolation points in multidimensional space. Let $m$ be the number of basis functions we wish to approximate with and let $l$ be the number of allowed function evaluations, assuming $l > m$. Given the square $n \\times n$ matrix $\\mA$ (e.g., tensor-product Gauss quadrature rule on a tensor-product polynomial space), we only want to use the first $m < n$ basis functions columns from this full matrix. Now, in general for least squares, we require $l > m$ points, implying that we want to subselect $l$ points from the $n-$point tensor-product grid. Thus, we let $\\mA_{\\coprod}$ be the $n \\times l$ matrix formed by choosing the first $l$ columns of $\\mA$. We then perform a QR column pivoting on $\\mA_{\\coprod}^T$, which selects $l$ points. Then we define the $l \\times m$ subsampled design matrix $\\mA_{\\ast}$, to be the submatrix of $\\mA$ formed by choosing the $l$ points (rows) from the QR pivoting, and the first $m$ columns. Thus, given $\\mA$ and a set of tensor-grid quadrature points and weights, the least squares polynomial approximation is solved by\n\n", "itemtype": "equation", "pos": 24390, "prevtext": "\nIn general, larger $\\kappa$ values lead to more unstable and inaccurate solutions.  \n\n\n\\section{Proposed approach}\n\\label{sec:optimal}\nWe now discuss the main contribution of this paper. Instead of using randomized samples on total order or hyperbolic cross spaces, we present a different sampling strategy that is inherently deterministic and optimal. We term our sampling strategy \\emph{optimal quadrature} as it seeks to obtain the best least squares estimate---by reducing the condition number of the linear system---for a given set of basis functions. We also describe our strategy for basis selection. \n\n\\subsection{Preliminaries}\nPrior to detailing the proposed approach, we provide a few key definitions that will set the stage for our \\emph{optimal quadrature} technique---which utilizes QR column pivoting. \n\\begin{definition} (Permutation matrix)\nA permutation matrix is an orthogonal matrix with permuted rows of an identity matrix. Products of permutation matrices are also permutations \\cite{Hansen}. Consider a integer valued vector $\\textsf{v}$ in $\\mathbb{R}^{n}$ with each entry $\\leq n$ and with no repetitions. The integers can be arranged in any permutation. We call this vector, denoted by $\\textsf{v}$, a permutation vector. We define its corresponding permutation matrix $\\Pi \\in \\mathbb{R}^{n \\times n}$ as an identity matrix where the rows are reordered based on the values in $\\textsf{v}$. Thus, if the first component of $\\textsf{v}$ is 2, the first row of the permutation matrix is $[0, 1, 0, 0, \\ldots]$ and so on. \n\\end{definition}\n\\begin{definition}(Matrix rank and range)\nThe rank of a matrix $\\mD \\in \\mathbb{R}^{n \\times m}$ is the maximum number of linearly independent rows or columns and is given by $\\textsf{rank}(\\mD)$. If the columns of $\\mD$ can be partitioned into a form $\\mD=\\left[\\vd_{1}\\left|\\ldots\\right|\\vd_{m}\\right]$, then the range can be written as $\\textsf{ran} \\left(D\\right)=\\textsf{span}\\left\\{ \\vd_{1},\\ldots,\\vd_{m}\\right\\}$. \n\\end{definition}\n\nThe objective of QR column pivoting is to find a permutation matrix $\\Pi$, such that the first $r=\\textsf{rank}(\\mD)$ columns of $\\mD \\Pi$, with $\\mD \\in \\mathbb{R}^{n \\times m}$ are linearly independent and span $\\textsf{ran}(\\mD)$. This gives rise to the notion of \\emph{rank revealing QR factorization}.\n\\begin{definition} (Rank-revealing QR factorization (page 95 in \\cite{Hansen}) ). Let $\\mD \\in \\mathbb{R}^{n \\times m}$ have singular values $\\sigma_{1}\\geq\\sigma_{2}\\geq\\ldots \\geq \\sigma_{r} \\geq \\sigma_{r+1} \\ldots \\geq\\sigma_{n}\\geq0$ where there is a gap between $\\sigma_{r}$ and $\\sigma_{r + 1}$. Here $r$ is the numerical rank of $\\mD$. Then, the rank revealing QR factorization is given by\n\n", "index": 27, "text": "\\begin{equation}\n\\mD \\Pi= \\mQ \\underset{\\begin{array}{cc}\nr & \\left(m-r\\right)\\end{array}}{\\left(\\begin{array}{cc}\n\\mR_{11} & \\mR_{12}\\\\\n0 & \\mR_{22}\n\\end{array}\\right)}\\begin{array}{c}\nr\\\\\n\\left(n-r\\right)\n\\end{array},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\mD\\Pi=\\mQ\\underset{\\begin{array}[]{cc}r&amp;\\left(m-r\\right)\\end{array}}{\\left(%&#10;\\begin{array}[]{cc}\\mR_{11}&amp;\\mR_{12}\\\\&#10;0&amp;\\mR_{22}\\end{array}\\right)}\\begin{array}[]{c}r\\\\&#10;\\left(n-r\\right)\\end{array},\" display=\"block\"><mrow><mrow><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mD</mtext></merror><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a0</mi></mrow><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mQ</mtext></merror><mo>\u2062</mo><munder accentunder=\"true\"><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mn>11</mn></msub></mtd><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mn>12</mn></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mn>0</mn></mtd><mtd columnalign=\"center\"><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mR</mtext></merror><mn>22</mn></msub></mtd></mtr></mtable><mo>)</mo></mrow><mtable columnspacing=\"5pt\" displaystyle=\"true\"><mtr><mtd columnalign=\"center\"><mi>r</mi></mtd><mtd columnalign=\"center\"><mrow><mo>(</mo><mrow><mi>m</mi><mo>-</mo><mi>r</mi></mrow><mo>)</mo></mrow></mtd></mtr></mtable></munder><mo>\u2062</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>r</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mo>(</mo><mrow><mi>n</mi><mo>-</mo><mi>r</mi></mrow><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nwhich can be computed succinctly in MATLAB as follows:\n\\begin{lstlisting} \n[Q,R,Pi] = qr(A(:,1:l)', 'vector'); \nPi = Pi(1:l); \nW = diag(sqrt(quadrature_weights(Pi,:) )); \ng = feval(quadrature_pts(Pi,:)); \nb_ast = W' * g; \nA_ast = A(Pi,1:m) \nx = A_ast \\ b_ast \n\\end{lstlisting} \nEquation~\\eqref{equ_lsqr_2}, which solves the subsampled least squares problem is (as will be shown in the forthcoming examples section) the best approximation to equation~\\eqref{equ_lsqr}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn alternative way to think about the strategy above is that for a given design matrix $\\mA$, and a maximum number of permitted model evaluations $l$, we are extracting the coefficients of at most $m=l$ basis functions. The QR column pivoting algorithm in this context offers a deterministic recipe for subsampling a tensor grid. In addition, the QR column pivoting algorithm can produce accurate low-rank approximations to the matrix $\\mA_{\\coprod}$ \\cite{harbrecht_2012}. To provide some context, Figure~\\ref{figure0} plots from a 50-point Gauss-Legendre quadrature rule along with subsamples obtained from QR column pivoting. To be abundantly clear, these subsamples are obtained by inspecting the first 5, 10 and 20 integer values of $\\textsf{v}$ and using the quadrature points corresponding to those integers. \n\n\n\n\\begin{figure}[h]\n\\center\n\\includegraphics[natwidth=962, natheight=218, width=15cm]{subsamples.pdf}\n\\caption{Various QR column pivoted subsamples from a 50 point Gauss-Legendre quadrature rule.}\n\\label{figure0}\n\\end{figure}\n\n\n\nWe note that we have described this QR selection strategy as an attempt to construct a well-conditioned design matrix. Remarkably, the limiting behavior of the points selected via this algorithm is known. Consider the $d=1$ case as shown in Figure 1 on an interval, but suppose that we take $n \\rightarrow \\infty$. That is, the number of candidate points from which our QR method selects points is an infinite, dense grid on an interval. In this case, it is known that the QR selection strategy chooses points that distribute according to the Chebyshev (arcsine) measure \\cite{bos_2011}. In addition, this property holds in the multidimensional setting on a hypercube (the set formed from the Cartesian product of univariate bounded intervals). If we use the QR strategy to select points from a dense grid on a hypercube, these points distribute according to the product Chebyshev measure on the hypercube. \n\n\\section{An Analytical Example}\nIn this example we explore the stability of the least squares solution and the effect of QR column pivoting with a univariate function $f(x) = exp(x)$. We remark here that we use a univariate function for simplicity. The concepts illustrated here---and discussed in the prior sections---extend to multivariate functions as well. \n\nWe generate a $100 \\times 100$ $\\mA$ matrix using Legendre orthonormal polynomials evaluated at Gauss-Legendre quadrature points. We compute $f(\\gamma_i) = exp(\\gamma_i)$ where $\\gamma_i$ represents the $i-$th quadrature point with $i=0,\\ldots 99$. We weight these function evaluations and compute the pseudospectral coefficients $\\vx$ using the DFT in~\\eqref{DFT}. We refer to the resulting coefficients as $x^{true}$. Our objective now is to solve a least squares problem with different dimensions of $l$ and $m$ in $\\mA_{\\ast} \\in \\mathbb{R}^{l \\times m}$ with $l \\geq m$. In other words, for a particular number of quadrature sub-samples $(l)$, we reduce the number of basis functions ($m$), starting with $l=m$; generate $\\mA_{\\ast}$ and solve the resulting least squares problem. We want to know the effect of the dimensions of $\\mA_{\\ast}$ and thus the number of quadrature subsamples on the coefficient accuracy. If $\\hat{\\vx} \\in \\mathbb{R}^{m}$ represents the coefficients obtained from subsampling, we define the error as\n \n", "itemtype": "equation", "pos": 27747, "prevtext": "\nif $\\kappa(\\mD) \\simeq \\sigma_{1}/\\sigma_{r}$ and $\\left\\Vert \\mR_{22}\\right\\Vert \\simeq\\sigma_{r+1}$, where $\\mR_{11} \\in \\mathbb{R}^{r \\times r}$ is upper triangular and $\\mR_{22}$ is a submatrix of a small norm. \n\\label{defn_RRQR}\n\\end{definition}\n\nQR factorizations with a permutation matrix can be computed using Householder reflections with column perturbations iteratively. As Hansen et al. \\cite{Hansen} note, there are two key computational aspects to the stepwise procedure for obtaining a permutation matrix---which always exists (see Hong and Pan \\cite{Hong}). The first is determining the termination criterion. Definition~\\ref{defn_RRQR} addresses this---when $\\mR_{11}$ embeds the linearly independent information of $\\mD$ and has numerical rank $r$. The second is determining how to select the most linearly independent columns of $\\mD$. One can think of the latter as solving a dual-objective optimization problem with the aim of minimizing the norm of $\\mR_{22}$ and maximizing $\\sigma_r$ \\cite{Hansen}. This is covered by Businger and Golub's (1965) \\emph{Householder QR with column pivoting} algorithm (see page 278 in \\cite{Golub_book}). We refer the interested reader to the aforementioned texts for further details.\n\n\\subsection{Basis selection}\nOur basis selection strategy is particularly straightforward. Seeing as most engineering quantities of interest within design optimization and uncertainty quantification modules are integrated across the geometry or flow-field; they tend to be captured well by first and second order terms. Thus, we arrange all our basis functions in increasing order of their total index and use the first $m$ terms only. \n\n\\subsection{Subsampling with QR column pivoting}\nOur subsampling strategy attempts to reduce the condition number of our \\emph{subsampled design matrix}, $\\mA_{\\ast}$. To aid this effort, we will employ a QR column pivoting strategy. This strategy is also known as constructing \\emph{approximate Fekete points} \\cite{AFP} and has been used in the context of finding near-optimal interpolation points in multidimensional space. Let $m$ be the number of basis functions we wish to approximate with and let $l$ be the number of allowed function evaluations, assuming $l > m$. Given the square $n \\times n$ matrix $\\mA$ (e.g., tensor-product Gauss quadrature rule on a tensor-product polynomial space), we only want to use the first $m < n$ basis functions columns from this full matrix. Now, in general for least squares, we require $l > m$ points, implying that we want to subselect $l$ points from the $n-$point tensor-product grid. Thus, we let $\\mA_{\\coprod}$ be the $n \\times l$ matrix formed by choosing the first $l$ columns of $\\mA$. We then perform a QR column pivoting on $\\mA_{\\coprod}^T$, which selects $l$ points. Then we define the $l \\times m$ subsampled design matrix $\\mA_{\\ast}$, to be the submatrix of $\\mA$ formed by choosing the $l$ points (rows) from the QR pivoting, and the first $m$ columns. Thus, given $\\mA$ and a set of tensor-grid quadrature points and weights, the least squares polynomial approximation is solved by\n\n", "index": 29, "text": "\\begin{equation}\n\\hat{\\tilde{\\vx}}=\\textrm{argmin} \\; \\; \\left\\Vert \\mA_{\\ast} \\tilde{\\vx} -\\vb_{\\ast} \\right\\Vert_{2}, \n\\label{equ_lsqr_2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\tilde{\\vx}}=\\textrm{argmin}\\;\\;\\left\\|\\mA_{\\ast}\\tilde{\\vx}-\\vb_{\\ast}%&#10;\\right\\|_{2},\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mpadded width=\"+5.6pt\"><mtext>argmin</mtext></mpadded><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\mA</mtext></merror><mo>\u2217</mo></msub><mo>\u2062</mo><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo stretchy=\"false\">~</mo></mover></mrow><mo>-</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vb</mtext></merror><mo>\u2217</mo></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05470.tex", "nexttext": "\nwhere $\\vx_{1:m}^{true}$ represents the first $m$ coefficients of $\\vx^{true}$. We first explore the case where the subsamples are selected randomly---as per \\emph{randomized quadrature} \\cite{Zhou}---which involves randomly subselecting $l$ out $n$ quadrature points.\n\n\\subsection{Randomized sampling for $l \\geq m$}\nFigure~\\ref{figure1}(a) plots $\\epsilon$ for the univariate function. The x-axis and y-axis refer to the $l$ and $m$ sizes for the matrix $\\mA_{\\ast}$. Each colored `pixel' in the graph shows the coefficient error on a base-10 logarithmic scale. Thus for regions that are shaded dark red, the coefficient error is large, while regions that are dark blue are accurate to machine precision. We restrict our attention to $m$ and $l$ values of 70 as they are sufficient to state our observations.\n\nFrom (a) we can see that adding more than 10 basis terms does not change the solution drastically. However, the number of samples does seem to make some difference. In Figure (b) we plot the condition numbers. It is clear that when $m$ is close to $l$ the condition numbers drastically increase and lead to an increase in $\\epsilon$. \n \n\\begin{figure}\n\\begin{subfigmatrix}{2}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{exp_coeff_GL.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{cond_GL.pdf}}\n\\end{subfigmatrix}\n\\caption{Polynomial coefficient errors in (a) and corresponding condition numbers in (b); on a base-10 logarithmic scale. The design matrix, $\\mA_{*}$ was formed with orthonormal Legendre polynomials evaluated at randomly subsampled Gauss-Legendre quadrature points. Results are shown for an example function $y=exp(x_1)$.}\n\\label{figure1}\n\\end{figure}\n\n\n\\subsection{The case of $l=m$}\nFigure~\\ref{figure2} plots the results only for the diagonal case where $l=m$, where the number of basis terms $m$ is equivalent to the number of samples $l$. As the coefficient estimate using the randomized quadratures vary depending on the subsample choice, we plot the minimum and maximum values along with the mean of a 100 random draws of different values of $m$. These plots are compared with the result of the proposed optimal quadrature method which obtains a far lower condition number---and thus error---in the coefficient estimates. \n\n\\begin{figure}\n\\begin{subfigmatrix}{2}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{baseline_a.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{baseline_b.pdf}}\n\\end{subfigmatrix}\n\\caption{Polynomial coefficient errors in (a) and condition numbers in (b) for $l=m$ using least squares and column pivoted QR least squares on $f=exp(x)$. These solutions correspond to marching along the $l=m$ diagonal in Figure~\\ref{figure1}}\n\\label{figure2}\n\\end{figure}\n\n\n\\subsection{The case of $l=1.2m$}\nWe now investigate the case where $l=1.2m$, in other words where there are fewer basis terms than quadrature subsamples. While the mean and variance in the error of the randomized quadratures are reduced, they are still greater than the errors from our \\emph{optimal quadrature} technique. With further reductions in the number of basis functions, reduced errors were observed in the randomized quadrature technique. However, in all numerical experiments conducted, the optimal quadrature technique always produced a lower coefficient error.  \n\n\\begin{figure}\n\\begin{subfigmatrix}{2}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{next_a.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{next_b.pdf}}\n\\end{subfigmatrix}\n\\caption{Polynomial coefficient errors in (a) and condition numbers in (b) for $l=1.2m$ using least squares and column pivoted QR least squares on $f=exp(x)$.}\n\\label{figure3}\n\\end{figure}\n\n\\subsection{Implementation steps}\n\\label{sec:approach}\nFor completeness, we now detail our \\emph{optimal quadrature} approach step-by-step. \n\\begin{enumerate}\n\\item For a $d$ dimensional problem, determine the maximum number of subsamples that can be evaluated given the computational run-time of the model. Let the number of subsamples be $m$.\n\\item Select a suitably sized tensor grid with at least 2-5 points in each direction. Let the total number of quadrature points in this grid be $n \\gg m$.\n\\item Generate the $n \\times n$ design matrix $\\mA$ \\footnote{We remark here that it is not necessary to generate the full $\\mA$. For large $n$, it may be more efficient to simply compute the pertinent rows and columns}.\n\\item Create the matrix $\\mA_{\\coprod}$ that only has the first $l$ columns of $\\mA$. If $l$ is sufficiently large, its indices can be determined by a hyperbolic cross set or a sparse grid. Also, if more information about the simulation is known a priori, the indices can be weighted in a particular direction. There is some flexibility in this step. The main criterion we wish to enforce is that at most only $l$ columns should be chosen.\n\\item Compute the permutation matrix, $\\Pi$ in the QR column pivoting of $\\mA_{\\coprod}^T$. Select the first $l$ entries of the vector associated with $\\Pi$. These represent the subsamples indices. We recommend setting $l=m$ initially, and then pruning down to $m \\leq l$ columns. \n\\item Using QR factorization, solve the weighted least squares problem on $\\mA_{\\ast} \\in \\mathbb{R}^{l \\times m}$.\n\\end{enumerate}\n\n\\section{A NACA0012 Case Study}\n\\label{sec:numex}\nAs mentioned earlier, there is a strong need for developing computationally feasible high dimensional surrogates for uncertainty quantification and sensitivity analysis in aerodynamics. While traditional projection-based polynomial methods have seen widespread application, their use is somewhat restricted to low-dimensional problems by the curse of dimensionality associated with sparse and tensor grids. In this section, we demonstrate the efficacy of the proposed \\emph{optimal quadrature} approach in Section~\\ref{sec:optimal} on a NACA0012 design problem with CFD analysis.\n\n\\subsection{Problem setup}\nWe assume that owing to some manufacturing variation, there is uncertainty in the local airfoil curvature of a NACA0012 at specific chord locations. We also make the assumption that these local curvature variations---and thus their associated parameterization---are mutually independent. The NACA0012 airfoil is parameterized with 10 Hicks-Henne bump functions: 5 on the upper surface and 5 on the lower surface. Our uncertainty is the height of each bump; a particular vector of these bumps in the uncertainty space produces a perturbation from the NACA0012 profile. The height ranges and locations for the bumps are shown in Table~\\ref{Hicks}. We assume all ten uncertain variables are uniformly distributed. \n\n\\begin{table}\n \\begin{center}\n   \\caption{Hicks-Henne bump function heights and locations as a proportion of chord. The heights and locations are shown for the upper surface; the lower surface has the same parameterization. For the baseline NACA0012 profile, all bump amplitudes are set to 0.0}\n\t\\vspace{3mm}   \n   \\label{Hicks}\n   \\begin{tabular}{lcc}\n\n    Location & Bump amplitude range \\\\\n    \\hline\n0.05 & $-0.0007$ to $0.0007$\\\\  \n0.15 & $-0.007$ to $0.009$\\\\  \n0.45 & $-0.007$ to $0.009$\\\\  \n0.65 & $-0.007$ to $0.009$\\\\  \n0.85 & $-0.006$ to $0.006$\\\\  \n\n   \\end{tabular}\n \\end{center}\n\\end{table}\n\nFor a change in these bump amplitudes, the airfoil geometry and mesh are deformed using a torsional spring analogy. The flow solver runs on the new mesh producing a lift-to-drag $(L/D)$ ratio, which is our response of interest. What we seek to determine is the effect of these uncertainties on the mean and variance in $L/D$. Furthermore, we also wish to determine which of the 10 parameters exhibits the greatest sensitivity.\n\nTo aid this endeavor, the Stanford University Unstructured (SU2) \\cite{SU2} suite of codes are used on the computational mesh shown in Figure~\\ref{nacamesh}. We solve the steady compressible Euler equations at a Mach number of 0.66 and an angle of attack of 5 degrees. The Euler equations are discretized through a standard edge-based approach on an unstructured mesh using a finite volume method. A standard dual grid, constructed using a medial-dual vertex scheme, is used to evaluate the convective fluxes---integrated using the Jameson-Turkel-Schmidt scheme \\cite{Chung}---at the midpoint of an edge. This scheme employs higher-order artificial dissipation, which is computed using the differences in the higher-order Laplacians of connecting nodes and the difference in the lower-order conserved variables on connecting nodes. A pressure switch is used to blend these two levels of dissipation and for triggering lower-order dissipation next to shock waves. We remark here that the use of the NACA0012 example in SU2 has been previously used by the first author in the context of design optimization with univariate least squares in \\cite{density_matching}.\n\n\\begin{figure}\n\\centering\n\\includegraphics[natwidth=738, natheight=354, width=10cm]{naca_file.pdf}\n\\caption{NACA0012 geometry and Euler mesh (no boundary layer), taken from the SU2 testcase database \\cite{SU2_online} }\n\\label{nacamesh}\n\\end{figure}\n\n\\subsection{Full tensor grid evaluation}\nWe create an anisotropic Gauss-Legendre tensor grid with orders (2,2,1,1,1,2,1,2,1,2), yielding a grid with 7776 quadrature points in 10 dimensional space. We solve the Euler equations for the 7776 perturbed geometries and save the corresponding L/D values. Using the formulation previously outlined we compute the pseudospectral coefficients using the DFT in~\\eqref{DFT}. From these coefficient values the mean, variance and the first order Sobol indices are computed. The latter are evaluated using Sudret's formulations (see~\\cite{Sudret} for formulation). We will use these values as a basis of comparison with the optimal quadrature.  \n\n\\subsection{Optimal quadrature results}\nWe employ the steps in~\\ref{sec:approach} and test it for four different values of $m$=[10,30,100,1000]. This corresponds to subsampling ratios of approximately 0.013, 0.04, 0.013 and 0.13. All four $m$'s are lower than the number of samples required to generate a quadratic polynomial in 10 dimensions---which requires $3^{10}=59,049$ evaluations---or even a linear model in 10 dimensions---which requires $2^{10}=1024$ model evaluations.  \n\nFollowing the approach in~\\ref{sec:approach}, our $\\mA$ matrix has dimensions $7776 \\times 7776$. We create the $\\mA_{\\coprod}$ submatrix, which only has the first $m$ columns of $\\mA$. In the case where $m=10$, this yields $\\mA_{\\coprod} \\in \\mathbb{R}^{7776 \\times 10}$. As eluded to earlier, we assume that the columns of $\\mA$ are arranged according to the total order of the polynomial. Now, to ensure that we solve the weighted least squares problem on a square matrix, we need to determine which 10 of the 7776 rows to use. We compute the permutation matrix on $\\mA_{\\coprod}^T$ via QR factorization and use the first 10 indices of $\\textsf{v}$ to create $\\mA_{\\ast}$. \n\nFigure~\\ref{final_result}(a) compares the coefficient estimates obtained from the full tensor grid with our \\emph{optimal quadrature} subsampling approach. Also shown are the results of weighted least squares on randomized quadratures. Depending on which $m$ random points were chosen, the weighted least squares results gave rather varied coefficient estimates. This is expected given the condition number associated with random square matrices, as was discussed in~\\ref{sec:optimal}.\n\n\\begin{figure}\n\\begin{subfigmatrix}{2}\n\\subfigure[]{\\includegraphics[natwidth=748, natheight=562]{samples_10.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{samples_30.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{samples_100.pdf}}\n\\subfigure[]{\\includegraphics[natwidth=560, natheight=420]{samples_1000.pdf}}\n\\end{subfigmatrix}\n\\caption{Polynomial coefficients for NACA0012 problem with different values of m: (a) 10; (b) 30; (c) 100; (d) 1000. In each figure the red crosses represent the tensor grid polynomial coefficients (obtained from 7776 quadrature points), the light blue squares are the coefficient estimates from 100 different trials of least squares with randomized quadratures with $m$ points. Lastly, the dark blue circles---termed optimal quadrature---are the results of our approach.}\n\\label{final_result}\n\\end{figure}\n\nWe remark here that when using weighted least squares on randomized quadratures, it is best to have more quadrature points than the number of basis functions. This drastically reduces both the condition numbers of the matrices and the scatter in the data. To illustrate this point, in Figure~\\ref{final_result2}, we reduce the number of basis functions for randomized quadratures by 30. This results in $\\mA_{*} \\in \\mathbb{R}^{1000 \\times 970}$. While these results show significantly less scatter than those in Figure~\\ref{final_result}(d), they are still not as accurate when compared with the optimal quadrature results.\n\n\\begin{figure}\n\\centering\n\\includegraphics[natwidth=748, natheight=562, width=9cm]{samples_1000b.pdf}\n\\caption{Polynomial coefficients for NACA0012 problem with $m=1000$. Here the randomized quadratures are carried out with on 970 basis terms only.}\n\\label{final_result2}\n\\end{figure}\n\n\nStatistics obtained from the experiments are shown in Table~\\ref{l2_NACA_stat}. Symbols $\\tilde{\\mu}$ and $\\tilde{\\sigma}^2$ are the mean and variance estimates from weighted least squares with optimal quadrature. These are compared with the pseudospectral results on the full tensor grid. As can be expected, while the mean is fairly well captured even in 10 samples, the variance requires approximately a 1000 samples before its value is comparable to the 7776 point pseudospectral result. It is our observation (not shown here) that beyond a 1000 samples, the errors in the mean and variance are relatively small. Also shown in Table~\\ref{l2_NACA_stat} are the first order Sobol indices for the 10 different Hicks-Henne amplitudes. Here, the correct trends in the Sobol indices are correctly captured at 100 samples and above. \n\n\\begin{table}\n \\begin{center}\n   \\caption{Statistics and first order Sobol indices for weighted least squares with optimal quadratures for 10, 100, 300 and 1000 random samples comapred with the pseudospectral coefficients from the full tensor grid.}\n   \\label{l2_NACA_stat}\n   \\begin{tabular}{lcc}\n\n    No. & $\\tilde{\\mu}$ & $\\tilde{\\sigma}^2$ \\\\\n    \\hline\n    10 &  35.8392  & 144.8995 \\\\\n\t30 &  35.5669  & 55.2057 \\\\\n\t100 & 35.9972 & 138.4484 \\\\\n\t1000 & 36.0079 & 155.771 \\\\\n\t\\hline\n\tTensor (7776) & 36.0303 & 154.6305 \\\\\n\\hline\n\\vspace{2 mm}\n   \\end{tabular}\n   \n      \\begin{tabular}{lcccccccccc}\n\n    No.& $S_1$ & $S_2$ &  $S_3$ & $S_4$ &  $S_5$ & $S_6$ &  $S_7$ & $S_8$ &  $S_9$ & $S_{10}$ \\\\ \n    \\hline\n    30 &    0.0767  &  0.0627    & 0.1321 &   0.5167  &  0.1300  &  0.0020  &  0.0100  &  0.0270   & 0.0015 &   0.0016 \\\\\n    100 &  0.0113 &   0.4125  &  0.0633  &  0.2100  &  0.0411  &  0.0008  &   0.0023  &  0.0094   & 0.0019 &   0.0005\\\\\n\t1000 &  0.0060  &  0.3884  &  0.1139  &  0.2414  &  0.0411  &  0.0000  &  0.0023 &   0.0112   & 0.0021  &  0.0010 \\\\\n\\hline\n    Tensor (7776) &  0.0058  & 0.3915 & 0.1130 & 0.2443 & 0.0417 & 0.00 & 0.0022 & 0.0118 & 0.0021 & 0.0009\\\\\n    \\hline\n   \\end{tabular}\n   \n \\end{center}\n\\end{table}\n\nAccording to these results the second, third and fourth parameters---which are located on the suction surface at 15, 45 and 65 $\\%$ chord---have the greatest effect on $L/D$. \n\n\\section{Conclusion}\nThis paper proposed a new sampling strategy for generating polynomial approximations, namely \\emph{optimal quadrature}. Optimal quadrature involves subsampling an existing tensor grid such that well-conditioned least squares estimates can be computed. By implementing a QR column pivoting routine on our \\emph{design matrix} along with a simple total order basis selection strategy, we demonstrate the efficacy of our approach compared with regular tensor grid quadratures and randomized subsampling. The former is too expensive and suffers from the \\emph{curse of dimensionality}, while the latter requires far fewer basis terms for stable and accurate solutions. Future work may investigate the incorporation of gradients in optimal quadrature. \n\n\\section{Acknowledgements}\nThis research was supported by the Air Force Office of Scientific Research under Grant No. FA9550-15-1-0018 (Dr. David Stargel technical monitor). The first author would like to thank Prof. Paul Constantine for valuable discussions on polynomial least squares and Prof. Alireza Doostan for insightful exchanges on compressed sensing strategies with pseudospectral approximations.   \n\n\n\\begin{thebibliography}{99}\n\\bibliographystyle{unsrt}\n\\bibitem{Sudret_hyperbolic}{\\sc G. Blatman and B. Sudret}, {\\em Adaptive sparse polynomial chaos expansion based on least angle regression}, Journal of Computational Physics, 230 (2011), pp. 2345-2367.\n\n\\bibitem{bos_2011}{\\sc L. Bos, J.-P. Calvi, N. Levenberg, A. Sommariva, and M. Vianello}, {\\em Geometric weakly admissible meshes, discrete least squares approximations and approximate Fekete points}, Mathematics of Computation, 80 (2011), pp. 1623-1638.\n\n\\bibitem{AFP}{\\sc L. Bos, S. De Marchi, A. Sommariva, and M. Vianello}, {\\em Computing Multivariate Fekete and Leja Points by Numerical Linear Algebra}, SIAM Journal on Numerical Analysis, 48 (2010), p. 1984.\n\n\\bibitem{Aeroelas}{\\sc L. Bruno, C. Canuto, and D. Fransos}, {\\em Stochastic aerodynamics and aeroelasticity of a flat plate via generalised polynomial chaos}, Journal of Fluids and Structures, 25 (2009), pp. 1158-1176.\n\n\\bibitem{Sparse2}{\\sc H. J. Bungartz and M. Griebel}, {\\em Sparse grids}, Acta numerica, 13 (2004), pp. 147-269.\n\n\\bibitem{Chkifa}{\\sc A. Chkifa, A. Cohen, G. Migliorati, F. Nobile, and R. Tempone}, {\\em Discrete least squares polynomial approximation with random evaluations application to parametric and stochastic elliptic pdes}, ESAIM: Mathematical Modelling and Numerical Analysis, (2015).\n\n\\bibitem{Chung}{\\sc T. J. Chung}, {\\em Computational Fluid Dynamics}, Cambridge University Press, 2010.\n\n\\bibitem{Cohen}{\\sc A. Cohen, M. A. Davenport, and D. Leviatan}, {\\em On the stability and accuracy of least squares approximations}, Foundations of computational mathematics, 13 (2013), pp. 819-834.\n\n\\bibitem{SPAM} {\\sc P. G. Constantine, M. S. Eldred, and E. T. Phipps}, {\\em Sparse pseudospectral approximation method}, Computer Methods in Applied Mechanics and Engineering, 229 (2012), pp. 1-12.\n\n\\bibitem{Doos_1} {\\sc A.Doostan and H.Owhadi},{\\em A non adapted sparse approximation of pdes with stochastic inputs},Journal of Computational Physics, 230 (2011), pp. 3015-3034.\n\n\\bibitem{CS_book} {\\sc S. Foucart and H. Rauhut}, {\\em A mathematical introduction to compressive sensing}, Springer, 2013.\n\n\\bibitem{Kronrod} {\\sc W. Gautschi and S. E. Notaris}, {\\em Gauss-Kronrod quadrature formulae for weight functions of Bernstein-Szego type}, Journal of Computational and Applied Mathematics, 25 (1989), pp. 199-224.\n\n\\bibitem{Geraci} {\\sc G. Geraci, P.M. Congedo, R. Abgrall, and G. Iaccarino}, {\\em High-order statistics in global sensitivity analysis: Decomposition and model reduction}, Computer Methods in Applied Mechanics and\nEngineering, 301 (2016), pp. 80-115.\n\n\\bibitem{Sparse1}{\\sc T. Gerstner and M. Griebel}, {\\em Numerical integration using sparse grids}, Numerical algorithms, 18 (1998), pp. 209-232.\n   \n\\bibitem{Golub_book}{\\sc G. H. Golub and C. F. Van Loan}, {\\em Matrix computations}, vol. 3, JHU Press, 2012.\n\n\\bibitem{Hampton} {\\sc J. Hampton and A. Doostan}, {\\em Compressive sampling of polynomial chaos expansions: convergence analysis and sampling strategies}, Journal of Computational Physics, 280 (2015), pp. 363-386.\n\n\\bibitem{Hansen} {\\sc P. C. Hansen, V. Pereyra, and G. Scherer}, {\\em Least squares data fitting with applications}, JHU Press, 2012.\n\n\\bibitem{harbrecht_2012} {\\sc H. Harbrecht, M. Peters, and R. Schneider}, {\\em On the low-rank approximation by the pivoted Cholesky decomposition}, Applied Numerical Mathematics, 62 (2012), pp. 428-440.\n\n\\bibitem{Hong} {\\sc Y. P. Hong and C.T. Pan}, {\\em Rank-revealing qr factorizations and the singular value decomposition}, Mathematics of Computation, 58 (1992), pp. 213-232.\n\n\\bibitem{Migliorati_2} {\\sc G. Migliorati, F. Nobile, E. von Schwerin, and R. Tempone}, {\\em Approximation of quantities of interest in stochastic pdes by the random discrete l2 projection on polynomial spaces}, SIAM Journal\non Scientific Computing, 35 (2013), pp. A1440-A1460.\n\n\\bibitem{Migliorati_1} {\\sc G. Migliorati, F. Nobile, E. von Schwerin, and R. Tempone}, {\\em Analysis of discrete l2 projection on polynomial spaces with random evaluations}, Foundations of Computational Mathematics, 14 (2014), pp. 419-456.\n\n\\bibitem{Moler} {\\sc C. Moler}, {\\em Numerical Computing with MATLAB}, 2004, ch. 5, pp. 139-163.\n\n\\bibitem{Owen} {\\sc A. B. Owen, J. Dick, and S. Chen}, {\\em Higher order Sobol' indices}, Information and Inference, 3 (2014), pp. 59-81.\n\n\\bibitem{SU2} {\\sc F. Palacios, M. R. Colonno, A. C. Aranake, A. Campos, S. R. Copeland, T. D. Economon,\nA. K. Lonkar, T. W. Lukaczyk, T. W. R. Taylor, and J. J. Alonso}, {\\em Stanford university Unstructured (SU2): An open-source integrated computational environment for multi-physics simulation and design}, AIAA Paper, 287 (2013)\n\n\\bibitem{Patterson} {\\sc T. Patterson}, {\\em The optimum addition of points to quadrature formulae}, Mathematics of Computation, 22 (1968), pp. 847-856.\n\n\\bibitem{Peng} {\\sc J. Peng, J. Hampton, and A. Doostan}, {A weighted l1-minimization approach for sparse polynomial chaos expansions}, Journal of Computational Physics, 267 (2014), pp. 92-111.\n\n\\bibitem{mechanical} {\\sc A. Sandu, C. Sandu, and M. Ahmadian}, {\\em Modeling multibody systems with uncertainties. part i: Theoretical and computational aspects}, Multibody System Dynamics, 15 (2006), pp. 369-391.\n\n\\bibitem{density_matching} {\\sc P. Seshadri, P. G. Constantine, G. Iaccarino, and G. T. Parks}, {\\em A density-matching approach for optimization under uncertainty}, arXiv preprint arXiv:1409.7089v2, (2015).\n\n\\bibitem{Seshadri_LEAK} {\\sc P. Seshadri, G. T. Parks, and S. Shahpar}, {\\em Leakage uncertainties in compressors: The case of rotor 37}, Journal of Propulsion and Power, 31 (2014), pp. 456-466.\n\n\\bibitem{Ralf} {\\sc R. C. Smith}, {\\em Uncertainty Quantification: Theory, Implementation, and Applications}, vol. 12, SIAM, 2013.\n\n\\bibitem{SU2_online}{\\sc Stanford University Unstructured (SU2)}, {\\em Test Cases, https://github.com/su2code/testcases}, 2016.\n\n\\bibitem{Sudret} {\\sc B. Sudret}, {\\em Global sensitivity analysis using polynomial chaos expansions}, Reliability Engineering and System Safety, 93 (2008), pp. 964-979. Bayesian Networks in Dependability.\n\n\\bibitem{Tang} {\\sc G. Tang and G. Iaccarino}, {\\em Subsampled gauss quadrature nodes for estimating polynomial chaos expansions}, SIAM/ASA Journal on Uncertainty Quantification, 2 (2014), pp. 423-443.\n\n\\bibitem{chemical}{\\sc Y. Zhang and N. V. Sahinidis}, {\\em Uncertainty quantification in CO2 sequestration using surrogate models from polynomial chaos expansion}, Industrial and Engineering Chemistry Research, 52 (2012), pp. 3121-3132.\n\n\\bibitem{Zhou} {\\sc T. Zhou, A. Narayan, and D. Xiu}, {\\em Weighted discrete least-squares polynomial approximation using randomized quadratures}, Journal of Computational Physics, 298 (2015), pp. 787-800.\n\\end{thebibliography}\n\n\n\n\n", "itemtype": "equation", "pos": 31739, "prevtext": "\nwhich can be computed succinctly in MATLAB as follows:\n\\begin{lstlisting} \n[Q,R,Pi] = qr(A(:,1:l)', 'vector'); \nPi = Pi(1:l); \nW = diag(sqrt(quadrature_weights(Pi,:) )); \ng = feval(quadrature_pts(Pi,:)); \nb_ast = W' * g; \nA_ast = A(Pi,1:m) \nx = A_ast \\ b_ast \n\\end{lstlisting} \nEquation~\\eqref{equ_lsqr_2}, which solves the subsampled least squares problem is (as will be shown in the forthcoming examples section) the best approximation to equation~\\eqref{equ_lsqr}. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn alternative way to think about the strategy above is that for a given design matrix $\\mA$, and a maximum number of permitted model evaluations $l$, we are extracting the coefficients of at most $m=l$ basis functions. The QR column pivoting algorithm in this context offers a deterministic recipe for subsampling a tensor grid. In addition, the QR column pivoting algorithm can produce accurate low-rank approximations to the matrix $\\mA_{\\coprod}$ \\cite{harbrecht_2012}. To provide some context, Figure~\\ref{figure0} plots from a 50-point Gauss-Legendre quadrature rule along with subsamples obtained from QR column pivoting. To be abundantly clear, these subsamples are obtained by inspecting the first 5, 10 and 20 integer values of $\\textsf{v}$ and using the quadrature points corresponding to those integers. \n\n\n\n\\begin{figure}[h]\n\\center\n\\includegraphics[natwidth=962, natheight=218, width=15cm]{subsamples.pdf}\n\\caption{Various QR column pivoted subsamples from a 50 point Gauss-Legendre quadrature rule.}\n\\label{figure0}\n\\end{figure}\n\n\n\nWe note that we have described this QR selection strategy as an attempt to construct a well-conditioned design matrix. Remarkably, the limiting behavior of the points selected via this algorithm is known. Consider the $d=1$ case as shown in Figure 1 on an interval, but suppose that we take $n \\rightarrow \\infty$. That is, the number of candidate points from which our QR method selects points is an infinite, dense grid on an interval. In this case, it is known that the QR selection strategy chooses points that distribute according to the Chebyshev (arcsine) measure \\cite{bos_2011}. In addition, this property holds in the multidimensional setting on a hypercube (the set formed from the Cartesian product of univariate bounded intervals). If we use the QR strategy to select points from a dense grid on a hypercube, these points distribute according to the product Chebyshev measure on the hypercube. \n\n\\section{An Analytical Example}\nIn this example we explore the stability of the least squares solution and the effect of QR column pivoting with a univariate function $f(x) = exp(x)$. We remark here that we use a univariate function for simplicity. The concepts illustrated here---and discussed in the prior sections---extend to multivariate functions as well. \n\nWe generate a $100 \\times 100$ $\\mA$ matrix using Legendre orthonormal polynomials evaluated at Gauss-Legendre quadrature points. We compute $f(\\gamma_i) = exp(\\gamma_i)$ where $\\gamma_i$ represents the $i-$th quadrature point with $i=0,\\ldots 99$. We weight these function evaluations and compute the pseudospectral coefficients $\\vx$ using the DFT in~\\eqref{DFT}. We refer to the resulting coefficients as $x^{true}$. Our objective now is to solve a least squares problem with different dimensions of $l$ and $m$ in $\\mA_{\\ast} \\in \\mathbb{R}^{l \\times m}$ with $l \\geq m$. In other words, for a particular number of quadrature sub-samples $(l)$, we reduce the number of basis functions ($m$), starting with $l=m$; generate $\\mA_{\\ast}$ and solve the resulting least squares problem. We want to know the effect of the dimensions of $\\mA_{\\ast}$ and thus the number of quadrature subsamples on the coefficient accuracy. If $\\hat{\\vx} \\in \\mathbb{R}^{m}$ represents the coefficients obtained from subsampling, we define the error as\n \n", "index": 31, "text": "\\begin{equation}\n\\epsilon=\\left\\Vert \\tilde{\\vx}-\\vx_{1:m}^{true}\\right\\Vert _{2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\epsilon=\\left\\|\\tilde{\\vx}-\\vx_{1:m}^{true}\\right\\|_{2}\" display=\"block\"><mrow><mi>\u03f5</mi><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mover accent=\"true\"><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mo stretchy=\"false\">~</mo></mover><mo>-</mo><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\vx</mtext></merror><mrow><mn>1</mn><mo>:</mo><mi>m</mi></mrow><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></math>", "type": "latex"}]