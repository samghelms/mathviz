[{"file": "1601.01490.tex", "nexttext": "\nBecause the survival function (i.e., probability that a random variable is larger than a certain value) of $\\phi(\\Delta t)$ is given by $\\int_{\\Delta t}^{\\infty} \\phi(t^{\\prime}){\\rm d}t^{\\prime} = e^{-N\\langle\\lambda\\rangle\\Delta t}$, we obtain\n$\\Delta t = -\\log u/N\\langle\\lambda\\rangle$, where $u$ is a realisation of the random variable drawn from the uniform density on the interval $[0, 1]$. Second, we determine the process $i$ that has produced the event with probability\n\n", "itemtype": "equation", "pos": 9183, "prevtext": "\n\\setlength{\\baselineskip}{24pt}\n\n\\maketitle\n\n\n\n\n\n\n\n\n\n\n\n\n\\section*{Abstract}\n\nThe Gillespie algorithm provides statistically exact methods to simulate stochastic dynamics modelled as interacting sequences of discrete events including systems of biochemical reactions or earthquakes, networks of queuing processes or spiking neurons, and epidemic and opinion formation processes on social networks. Empirically, inter-event times of various human activities, in particular human communication, and some natural phenomena are often distributed according to long-tailed distributions. The Gillespie algorithm and its extant variants either assume the Poisson process, which produces exponentially distributed inter-event times, not long-tailed distributions, assume particular functional forms for time courses of the event rate, or works for non-Poissonian renewal processes including the case of long-tailed distributions of inter-event times but at a high computational cost. In the present study, we propose an innovative Gillespie algorithm for renewal processes on the basis of the Laplace transform. It uses the fact that a class of point processes is represented as a mixture of Poisson processes with different event rates. The method allows renewal processes whose survival function of inter-event times is completely monotone functions and works faster than a recently proposed Gillespie algorithm for general renewal processes. We also propose a method to generate sequences of event times with a given distribution of inter-event times and a tunable amount of positive correlation between inter-event times. We demonstrate our algorithm with exact simulations of epidemic processes on networks. We find that positive correlation in inter-event times modulates dynamics but in a quantitatively minor way with the amount of positive correlation comparable with empirical data.\n\n\n\n\n\\section{Introduction}\n\nSocial, biological, chemical, neural, seismological, and financial dynamics, among others, are often driven by time-stamped discrete events. For example, an individual human or animal transmits an infectious disease to another only when a contact event between the two individuals happens. The state transition in such a system, e.g., whether an individual is infected or not, can be modelled as being event-driven. Another example is chemical substances in which a chemical reaction event changes the number of reagents in a discrete manner in both time and state. Stochastic point processes are central tools for emulating these phenomena \\cite{Ogata1999PureApplGeophys,Vankampen2007book,Gabbiani2010book} and also find applications in operations research domains such as queuing systems and reliability analysis \\cite{Jacobsen2006book}. The most central point process model is the Poisson process, which assumes that events independently occur at a constant rate throughout time.\n\nConsider an event-driven system in which events are generated by Poisson processes running in parallel.\nIn chemical reaction systems, each Poisson process possibly with a different rate is attached to one reaction. In epidemic processes taking place on human or animal contact networks, each Poisson process is assigned to an individual or a link, which potentially transmits the infection. The event rate of some of the Poisson processes may change upon the occurrence of a reaction or infection in the entire system. The simplest simulation method is to discretize time and judge whether an event occurs or not in each time window for individual processes. This widely used method, called the rejection method, is sub-optimal because the size of the time window must be sufficiently small for high accuracy, which is computationally costly \\cite{Vestergaard2015PlosComputBiol}.\n\nThe Gillespie algorithm is an efficient and statistically exact algorithm for such an interacting population of Poisson processes \\cite{Kendall1950JRStatSocSerB,Gillespie1976JComputPhys,Gillespie1977JPhysChem}. The Gillespie algorithm, or particularly the direct method of Gillespie \\cite{Gillespie1976JComputPhys,Gillespie1977JPhysChem}, exploits the fact that superposition of independent Poisson processes is a single Poisson process whose event rate is the summation of those of the constituent Poisson processes. Using this mathematical property, only a single Poisson process needs to be emulated in the Gillespie algorithm (section~\\ref{sec:Gillespie algorithm}).\n\nEmpirical data obtained from various domains suggest that real-life event sequences are far from those generated by the Poisson process. In particular, several empirical distributions of inter-event times obey long-tailed distributions \\cite{Eckmann2004PNAS,Barabasi2005Nature,VazquezA2006PhysRevE-burst,GohBarabasi2008EPL,HolmeSaramaki2012PhysRep}, whereas the Poisson process generates the exponential distribution of inter-event times. This is a prominent example of non-Markovian event sequences, that is, one needs the history of events to know the statistics of the time of the next event. This is not the case for the Poisson process, which is memoryless. When inter-event times are independently generated from a given distribution, the point process is called the renewal process \\cite{Cox1962book}.\n\nEfficient and accurate simulations of interacting renewal processes contribute to the understanding of effects of long-tailed behaviour of inter-event times on various dynamics in well-mixed and networked populations \\cite{HolmeSaramaki2012PhysRep}. One may draw the next event time for all processes from the predetermined distributions, select the process that has generated the minimum waiting time to the next event, execute the event, draw the next event times for the affected processes, and repeat. This is so-called the Gillespie's first reaction method \\cite{Gillespie1976JComputPhys}, whose improved versions are the next reaction method \\cite{GibsonBruck2000JPhysChemA} and the modified next reaction method \\cite{Anderson2007JChemPhys}. In fact, for non-Markovian renewal processes, it is generally difficult to numerically solve equations for determining the next event time although these methods call for generation of fewer random numbers than the Gillespie algorithm \\cite{Vestergaard2015PlosComputBiol}.\n\n\n\nIn the following, we restrict ourselves to the Gillespie algorithm and its variants.\n \nMotivated by situations of chemical reactions, many extensions of the Gillespie's algorithm to the case of non-Markovian processes assume that the dynamical change in the event rate is exogenously driven in particular functional forms \\cite{LuVolfson2004SystBiol,Carletti2012ComputMathMethodsMed}. They are not applicable to non-Markovian renewal processes characterised by long-tailed distributions of inter-event times because such an exogeneous drive would be different across the renewal processes running in parallel and would not have a desirable functional form. Bogu\\~{n}\\'{a} and colleagues extended the Gillespie algorithm to be applicable to general renewal processes\n\\cite{BogunaLafuerza2014PhysRevE} (section~\\ref{sec:nMGA}; also see \\cite{Vestergaard2015PlosComputBiol} for further developments). However, the algorithm has practical limitations.\nFirst, it is not accurate when the number of ongoing renewal processes is small \\cite{BogunaLafuerza2014PhysRevE}, thus affecting the beginning or ending of the dynamics of epidemic and opinion formation models, in which only a small number of processes is active even in large well-mixed or networked populations \\cite{Vestergaard2015PlosComputBiol}.\nSecond, it is necessary to recalculate the instantaneous event rate of each process upon every event in the entire population, a procedure that can be computationally costly.\n\nIn the present study, we propose an innovative Gillespie algorithm, the Laplace Gillespie algorithm, applicable to general renewal processes. It exploits the mathematical properties of the Laplace transform, is accurate for an arbitrary number of ongoing renewal processes, and runs faster than the previous algorithm \\cite{BogunaLafuerza2014PhysRevE}. To demostrate the generalizability of the algorithm, we introduce a method to generate event sequences with positive correlation in temporally close inter-event times, as typically observed in human behaviour and natural phenomena \\cite{GohBarabasi2008EPL}, for a given distribution of inter-event times. We also demostrate our methods by performing exact simulations of an epidemic process in which inter-event times follow a power-law distribution.\n\n\\section{Gillespie algorithm\\label{sec:Gillespie algorithm}}\n\nThe original Gillespie algorithm \\cite{Kendall1950JRStatSocSerB,Gillespie1976JComputPhys,Gillespie1977JPhysChem} assumes $N$ independent Poisson processes with rate $\\lambda_i$ ($1\\le i\\le N$) running in parallel. Because of the independence of different Poisson processes, superposition of the $N$ processes is also a Poisson process with rate $N\\langle\\lambda\\rangle \\equiv \\sum_{i=1}^N \\lambda_i$. Therefore, we first draw $\\Delta t$, an increment in time to the next event of the superposed Poisson process, from the exponential distribution given by\n\n", "index": 1, "text": "\\begin{equation}\n\\phi(\\Delta t) = N\\langle\\lambda\\rangle e^{-N\\langle\\lambda\\rangle\\Delta t}.\n\\label{eq:phi(Delta t) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\phi(\\Delta t)=N\\langle\\lambda\\rangle e^{-N\\langle\\lambda\\rangle\\Delta t}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03bb</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03bb</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nThird, we advance the time by $\\Delta t$ and repeat this procedure.\nUpon an event, any $\\lambda_i$ is allowed to change.\n\n\n\n\\section{Non-Markovian Gillespie algorithm\\label{sec:nMGA}} \n\nNow consider $N$ renewal processes running in parallel and denote by $\\psi_i(\\tau)$ the probability density function of inter-event time for the $i$th process ($1\\le i\\le N$). If the process is Poisson, we obtain $\\psi_i(\\tau) = \\lambda_i e^{-\\lambda_i \\tau}$. For such a population of general renewal processes, Bogu\\~{n}\\'{a} and colleagues proposed an extension of the Gillespie algorithm, which they called the non-Markovian Gillespie algorithm (nMPA) \\cite{BogunaLafuerza2014PhysRevE}. The nMGA works as follows.\n\nTo determine the time of the next event, we track the time since the last event for each process, denoted by $t_i$. If the $i$th process were running in isolation, the waiting time $\\tau$ until the next event would be distributed according to\n\n", "itemtype": "equation", "pos": 9806, "prevtext": "\nBecause the survival function (i.e., probability that a random variable is larger than a certain value) of $\\phi(\\Delta t)$ is given by $\\int_{\\Delta t}^{\\infty} \\phi(t^{\\prime}){\\rm d}t^{\\prime} = e^{-N\\langle\\lambda\\rangle\\Delta t}$, we obtain\n$\\Delta t = -\\log u/N\\langle\\lambda\\rangle$, where $u$ is a realisation of the random variable drawn from the uniform density on the interval $[0, 1]$. Second, we determine the process $i$ that has produced the event with probability\n\n", "index": 3, "text": "\\begin{equation}\n\\Pi_i = \\frac{\\lambda_i}{N\\langle\\lambda\\rangle}.\n\\label{eq:Pi(i) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\Pi_{i}=\\frac{\\lambda_{i}}{N\\langle\\lambda\\rangle}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>\u03bb</mi><mi>i</mi></msub><mrow><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03bb</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 10863, "prevtext": "\nThird, we advance the time by $\\Delta t$ and repeat this procedure.\nUpon an event, any $\\lambda_i$ is allowed to change.\n\n\n\n\\section{Non-Markovian Gillespie algorithm\\label{sec:nMGA}} \n\nNow consider $N$ renewal processes running in parallel and denote by $\\psi_i(\\tau)$ the probability density function of inter-event time for the $i$th process ($1\\le i\\le N$). If the process is Poisson, we obtain $\\psi_i(\\tau) = \\lambda_i e^{-\\lambda_i \\tau}$. For such a population of general renewal processes, Bogu\\~{n}\\'{a} and colleagues proposed an extension of the Gillespie algorithm, which they called the non-Markovian Gillespie algorithm (nMPA) \\cite{BogunaLafuerza2014PhysRevE}. The nMGA works as follows.\n\nTo determine the time of the next event, we track the time since the last event for each process, denoted by $t_i$. If the $i$th process were running in isolation, the waiting time $\\tau$ until the next event would be distributed according to\n\n", "index": 5, "text": "\\begin{equation}\n\\psi_i^{\\rm w}(\\tau | t_i) = \\frac{\\psi_i(t_i+\\tau)}{\\Psi_i(t_i)},\n\\label{eq:psi(tau | t_i) generalized Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\psi_{i}^{\\rm w}(\\tau|t_{i})=\\frac{\\psi_{i}(t_{i}+\\tau)}{\\Psi_{i}(t_{i})},\" display=\"block\"><mrow><msubsup><mi>\u03c8</mi><mi>i</mi><mi mathvariant=\"normal\">w</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">|</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nis the survival function, i.e., probability that the inter-event time is larger than $t_i$.\n\nThe $i$th process coexists with the other $N-1$ processes. We denote by $\\phi(\\Delta t, i | \\{t_j\\})$ the probability density with which the $i$th process, but not the other $N-1$ processes, generates the next event in the set of $N$ processes after time $\\Delta t$ given the time since the last event for each process, $\\{t_j\\}$, i.e., $t_1, \\ldots, t_N$. We obtain\n\n", "itemtype": "equation", "pos": 11016, "prevtext": "\nwhere\n\n", "index": 7, "text": "\\begin{equation}\n\\Psi_i(t_i) = \\int_{t_i}^\\infty \\psi_i(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}\n\\label{eq:Psi_i(t_i)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\Psi_{i}(t_{i})=\\int_{t_{i}}^{\\infty}\\psi_{i}(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><msub><mi>t</mi><mi>i</mi></msub><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 11604, "prevtext": "\nis the survival function, i.e., probability that the inter-event time is larger than $t_i$.\n\nThe $i$th process coexists with the other $N-1$ processes. We denote by $\\phi(\\Delta t, i | \\{t_j\\})$ the probability density with which the $i$th process, but not the other $N-1$ processes, generates the next event in the set of $N$ processes after time $\\Delta t$ given the time since the last event for each process, $\\{t_j\\}$, i.e., $t_1, \\ldots, t_N$. We obtain\n\n", "index": 9, "text": "\\begin{equation}\n\\phi(\\Delta t, i | \\{t_j\\}) = \\psi_i^{\\rm w}(\\Delta t | t_i) \\prod_{j=1; j\\neq i}^N \\Psi_j(\\Delta t | t_j),\n\\label{eq:phi(Delta t, i | set of t_j) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\phi(\\Delta t,i|\\{t_{j}\\})=\\psi_{i}^{\\rm w}(\\Delta t|t_{i})\\prod_{j=1;j\\neq i}%&#10;^{N}\\Psi_{j}(\\Delta t|t_{j}),\" display=\"block\"><mrow><mi>\u03d5</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msubsup><mi>\u03c8</mi><mi>i</mi><mi mathvariant=\"normal\">w</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mo>;</mo><mrow><mi>j</mi><mo>\u2260</mo><mi>i</mi></mrow></mrow><mi>N</mi></munderover><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nis the probability that the time to the next event for the hypothetically isolated $j$th process is larger than $\\tau$ conditioned that the last event occurred at time $t_j$ before. Using Eqs.~\\eqref{eq:psi(tau | t_i) generalized Gillespie} and \\eqref{eq:Psi_j(Delta t | t_j) Gillespie}, we rewrite Eq.~\\eqref{eq:phi(Delta t, i | set of t_j) Gillespie} as\n\n", "itemtype": "equation", "pos": 11800, "prevtext": "\nwhere\n\n", "index": 11, "text": "\\begin{equation}\n\\Psi_j(\\Delta t | t_j) = \\int_{\\Delta t}^{\\infty} \\psi_j^{\\rm w}(\\tau^{\\prime} | t_j) {\\rm d}\\tau^{\\prime} =\n\\frac{\\Psi_j(t_j+\\Delta t)}{\\Psi_j(t_j)}\n\\label{eq:Psi_j(Delta t | t_j) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\Psi_{j}(\\Delta t|t_{j})=\\int_{\\Delta t}^{\\infty}\\psi_{j}^{\\rm w}(\\tau^{\\prime%&#10;}|t_{j}){\\rm d}\\tau^{\\prime}=\\frac{\\Psi_{j}(t_{j}+\\Delta t)}{\\Psi_{j}(t_{j})}\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mi mathvariant=\"normal\">\u221e</mi></msubsup><msubsup><mi>\u03c8</mi><mi>j</mi><mi mathvariant=\"normal\">w</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">d</mi><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo>=</mo><mfrac><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 12380, "prevtext": "\nis the probability that the time to the next event for the hypothetically isolated $j$th process is larger than $\\tau$ conditioned that the last event occurred at time $t_j$ before. Using Eqs.~\\eqref{eq:psi(tau | t_i) generalized Gillespie} and \\eqref{eq:Psi_j(Delta t | t_j) Gillespie}, we rewrite Eq.~\\eqref{eq:phi(Delta t, i | set of t_j) Gillespie} as\n\n", "index": 13, "text": "\\begin{equation}\n\\phi(\\Delta t, i | \\{t_j\\}) = \\frac{\\psi_i(t_i+\\Delta t)}{\\Psi_i(t_i+\\Delta t)} \\Phi(\\Delta t | \\{t_j\\}),\n\\label{eq:phi(Delta t, i | set of t_j) Gillespie 2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\phi(\\Delta t,i|\\{t_{j}\\})=\\frac{\\psi_{i}(t_{i}+\\Delta t)}{\\Psi_{i}(t_{i}+%&#10;\\Delta t)}\\Phi(\\Delta t|\\{t_{j}\\}),\" display=\"block\"><mrow><mi>\u03d5</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03c8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\nEquation~\\eqref{eq:Phi(Delta t | set of t_j) Gillespie} represents the probability that no process generates an event for another time $\\Delta t$. By equating this quantity to $u$, a random variate on the unit interval, we can determine $\\Delta t$, i.e., the time to the next event in the entire population of the $N$ renewal processes. Equation~\\eqref{eq:phi(Delta t, i | set of t_j) Gillespie 2} implies that, once $\\Delta t$ is determined, $\\lambda_i(t_i+\\Delta t) \\equiv \\psi_i(t_i+\\Delta t) / \\Psi_i(t_i+\\Delta t)$ is the instantaneous rate of the $i$th process and proportional to the probability that the $i$th process generates this event. \nTherefore, the exact Gillespie algorithm for general renewal processes is given as follows:\n\\begin{enumerate}\n\n\\item Initialise $t_j$ ($1\\le j\\le N$) for all $j$ (for example, $t_j=0$).\n\n\\item Draw the time to the next event, $\\Delta t$, by solving $\\Phi(\\Delta t | \\{t_j\\}) = u$, where $u$ is a random variate uniformly distributed on $[0, 1]$.\n\n\\item Select the process $i$ that has generated the event with probability \n\n", "itemtype": "equation", "pos": 12576, "prevtext": "\nwhere\n\n", "index": 15, "text": "\\begin{equation}\n\\Phi(\\Delta t | \\{t_j\\}) = \\prod_{j=1}^N \\frac{\\Psi_j(t_j+\\Delta t)}{\\Psi_j(t_j)}.\n\\label{eq:Phi(Delta t | set of t_j) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\Phi(\\Delta t|\\{t_{j}\\})=\\prod_{j=1}^{N}\\frac{\\Psi_{j}(t_{j}+\\Delta t)}{\\Psi_{%&#10;j}(t_{j})}.\" display=\"block\"><mrow><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\n\\item Update the time since the last event, $t_j$, to $t_j + \\Delta t$ ($j\\neq i$) and $t_i=0$.\n\n\\item Repeat steps 2--4.\n\n\\end{enumerate}\n\nAlthough this algorithm is statistically exact, step 2 is time-consuming \\cite{BogunaLafuerza2014PhysRevE,Vestergaard2015PlosComputBiol}. \nTo improve performance, Bogu\\~{n}\\'{a} and colleagues introduced the nMGA. The nMGA is an approximation to the aforementioned algorithm and exact in the limit of $N\\to\\infty$.\nWhen $\\Delta t$ is small, which would be the case when $N$ is large, Eq.~\\eqref{eq:Phi(Delta t | set of t_j) Gillespie} is approximated as\n\n", "itemtype": "equation", "pos": 13811, "prevtext": "\n\nEquation~\\eqref{eq:Phi(Delta t | set of t_j) Gillespie} represents the probability that no process generates an event for another time $\\Delta t$. By equating this quantity to $u$, a random variate on the unit interval, we can determine $\\Delta t$, i.e., the time to the next event in the entire population of the $N$ renewal processes. Equation~\\eqref{eq:phi(Delta t, i | set of t_j) Gillespie 2} implies that, once $\\Delta t$ is determined, $\\lambda_i(t_i+\\Delta t) \\equiv \\psi_i(t_i+\\Delta t) / \\Psi_i(t_i+\\Delta t)$ is the instantaneous rate of the $i$th process and proportional to the probability that the $i$th process generates this event. \nTherefore, the exact Gillespie algorithm for general renewal processes is given as follows:\n\\begin{enumerate}\n\n\\item Initialise $t_j$ ($1\\le j\\le N$) for all $j$ (for example, $t_j=0$).\n\n\\item Draw the time to the next event, $\\Delta t$, by solving $\\Phi(\\Delta t | \\{t_j\\}) = u$, where $u$ is a random variate uniformly distributed on $[0, 1]$.\n\n\\item Select the process $i$ that has generated the event with probability \n\n", "index": 17, "text": "\\begin{equation}\n\\Pi_i \\equiv \\frac{\\lambda_i(t_i+\\Delta t)}{\\sum_{j=1}^N \\lambda_j(t_j+\\Delta t)}.\n\\label{eq:Pi(i | Delta t, set of t_j) Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\Pi_{i}\\equiv\\frac{\\lambda_{i}(t_{i}+\\Delta t)}{\\sum_{j=1}^{N}\\lambda_{j}(t_{j%&#10;}+\\Delta t)}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a0</mi><mi>i</mi></msub><mo>\u2261</mo><mfrac><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 14570, "prevtext": "\n\n\\item Update the time since the last event, $t_j$, to $t_j + \\Delta t$ ($j\\neq i$) and $t_i=0$.\n\n\\item Repeat steps 2--4.\n\n\\end{enumerate}\n\nAlthough this algorithm is statistically exact, step 2 is time-consuming \\cite{BogunaLafuerza2014PhysRevE,Vestergaard2015PlosComputBiol}. \nTo improve performance, Bogu\\~{n}\\'{a} and colleagues introduced the nMGA. The nMGA is an approximation to the aforementioned algorithm and exact in the limit of $N\\to\\infty$.\nWhen $\\Delta t$ is small, which would be the case when $N$ is large, Eq.~\\eqref{eq:Phi(Delta t | set of t_j) Gillespie} is approximated as\n\n", "index": 19, "text": "\\begin{align}\n\\Phi(\\Delta t | \\{t_j\\}) =& \\exp \\left[ -\\sum_{j=1}^N \\ln\\frac{\\Psi_j(t_j)}{\\Psi_j(t_j+\\Delta t)}\\right]\\notag\\\\\n\n=& \\exp \\left[ -\\sum_{j=1}^N \\ln\\frac{\\Psi_j(t_j)}{\\Psi_j(t_j) - \\psi_j(t_j)\\Delta t + O(\\Delta t^2)}\\right]\\notag\\\\\n\n\\approx& \\exp \\left[-\\Delta t N \\overline{\\lambda}(\\{t_j\\})\\right],\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Phi(\\Delta t|\\{t_{j}\\})=\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u03a6</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0394</mi><mi>t</mi><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>=</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\exp\\left[-\\sum_{j=1}^{N}\\ln\\frac{\\Psi_{j}(t_{j})}{\\Psi_{j}(t_{j}%&#10;+\\Delta t)}\\right]\" display=\"inline\"><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>ln</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\exp\\left[-\\sum_{j=1}^{N}\\ln\\frac{\\Psi_{j}(t_{j})}{\\Psi_{j}(t_{j}%&#10;)-\\psi_{j}(t_{j})\\Delta t+O(\\Delta t^{2})}\\right]\" display=\"inline\"><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>ln</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow><mo>+</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msup><mi>t</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;\\approx\" display=\"inline\"><mo>\u2248</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\exp\\left[-\\Delta tN\\overline{\\lambda}(\\{t_{j}\\})\\right],\" display=\"inline\"><mrow><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03bb</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nWith this approximation, the time to the next event is determined from \n$\\Phi(\\Delta t | \\{t_j\\}) \\approx \\exp \\left[-\\Delta t N \\overline{\\lambda}(\\{t_j\\})\\right] = u$, i.e.,\n$\\Delta t = -\\ln u/N\\overline{\\lambda}(\\{t_j\\})$.\nThe process that generates the event is determined by setting $\\Delta t=0$ in Eq.~\\eqref{eq:Pi(i | Delta t, set of t_j) Gillespie}.\nFor the Poisson process, we set $\\lambda_i(t_i) = \\lambda_i$ to recover the original Gillespie algorithm (Eqs.~\\eqref{eq:phi(Delta t) Gillespie} and \\eqref{eq:Pi(i) Gillespie}).\n\n\\section{Laplace Gillespie algorithm}\n\n\\subsection{Algorithm}\n\nIn the nMGA, we update the instantaneous event rates for allthe processes and its summation\n$\\overline{\\lambda}(\\{t_j\\})$ in Eq.~\\eqref{eq:average instantaneous rate for generalized Gillespie} upon the occurrence of each event. This is because $t_j$ ($1\\le j\\le N$) is updated upon an event. This procedure is time consuming when $N$ is large; we have to update individual instantaneous rates even if the probability density of the inter-event time for a process is not perturbed by an event that has occurred elsewhere.\n\nTo construct an efficient Gillespie algorithm for non-Markovian point processes, we start by considering the following renewal process, which we call the event-modulated Poisson process.\nWhen an event occurs, we first draw the rate of the Poisson process, denoted by $\\lambda$, according to a fixed probability density function $p(\\lambda)$. Then, we draw the time of the next event according to the Poisson process with rate $\\lambda$. Upon the next event, we renew the rate $\\lambda$ by redrawing it from $p(\\lambda)$. We repeat these procedures.\n\nThis model, the event-modulated Poisson process, is a mixture of Poisson processes with different rates. It is a non-Poissonian renewal process in general. It is slightly different from the mixed Poisson process, in which a single rate is drawn from a random ensemble in the beginning and used throughout a realisation \\cite{Karr1991book,Grandell1997book}. It is also different from doubly stochastic Poisson process (also called Cox process), in which the rate of the Poisson process is a stochastic process \\cite{Kingman1964MathprocCambPhilSoc,Karr1991book,Grandell1997book}, or its subclass called the Markov-modulated Poisson process, in which the event rate switches in time according to a Markov process \\cite{Fischer1992PerfEval}. In these processes, the dynamics of the event rate are independent of the occurrence of events. In contrast, the event rate changes upon the occurrence of events in the event-modulated Poisson process.\n\nThe event-modulated Poisson process is a Poisson process when conditioned on the current value of $\\lambda$. Therefore, when we simulate $N$ event-modulated Poisson processes, they are independent of each other and of the past event sequences if we are given the instantaneous rate of the $i$th process, denoted by $\\lambda_i$, for all $i$ ($1\\le i\\le N$). This property enables us to construct a Gillespie algorithm, similar to the original one. By engineering $p(\\lambda)$, we can emulate a range of renewal processes with different distributions of inter-event times. The new Gillespie algorithm, which we term the Laplace Gillespie algorithm (the reason for Laplace will be clear in section~\\ref{sub:theory}; it has a theoretical basis in the Laplace transform), is defined as the Gillespie algorithm for event-modulated Poisson processes. We denote the density of the event rate for the $i$th process by $p_i(\\lambda_i)$. The Laplace Gillespie algorithm proceeds as follows:\n\n\\begin{enumerate}\n\n\\item Initialise each of the $N$ processes by drawing the rate $\\lambda_i$ ($1\\le i\\le N$) according to the respective density function $p_i(\\lambda_i)$.\n\n\\item Draw the time to the next event $\\Delta t = -\\ln u/\\sum_{j=1}^N \\lambda_j$, where $u$ is the random variate uniformly distributed on $[0, 1]$.\n\n\\item Select the process $i$ that has generated the event with probability $\\lambda_i/\\sum_{j=1}^N \\lambda_j$.\n\n\\item Draw a new rate $\\lambda_i$ according to $p_i(\\lambda_i)$. If there are processes $j$ ($1\\le j\\le N$) for which the statistics of inter-event times have changed upon the event generated in steps 2 and 3 (e.g., a decrease in the rate of being infected owing to the recovery of an infected neighbour), modify $p_j(\\lambda_j)$ accordingly and draw a new rate $\\lambda_j$ from the modified $p_j(\\lambda_j)$. The rate of the remaining processes is unchanged.\n\n\\item Repeat steps 2--4.\n\n\\end{enumerate}\n\n\\subsection{Theory\\label{sub:theory}}\n\nThe event-modulated Poisson process is a renewal process. The renewal process is fully characterised by the probability density of inter-event times, $\\psi(\\tau)$. For the event-modulated Poisson process with the probability density of the event rate $p(\\lambda)$, we obtain\n\n", "itemtype": "equation", "pos": 14902, "prevtext": "\nwhere\n\n", "index": 21, "text": "\\begin{equation}\n\\overline{\\lambda}(\\{t_j\\}) = \\frac{\\sum_{j=1}^N \\lambda_j(t_j)}{N} = \\frac{1}{N}\\sum_{j=1}^N \\frac{\\psi_j(t_j)}{\\Psi_j(t_j)}.\n\\label{eq:average instantaneous rate for generalized Gillespie}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\overline{\\lambda}(\\{t_{j}\\})=\\frac{\\sum_{j=1}^{N}\\lambda_{j}(t_{j})}{N}=\\frac%&#10;{1}{N}\\sum_{j=1}^{N}\\frac{\\psi_{j}(t_{j})}{\\Psi_{j}(t_{j})}.\" display=\"block\"><mrow><mrow><mrow><mover accent=\"true\"><mi>\u03bb</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mi>N</mi></mfrac><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mrow><msub><mi>\u03c8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi mathvariant=\"normal\">\u03a8</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nIntegration of both sides of Eq.~\\eqref{eq:psi(tau) mix} gives the survival function of the inter-event time as follows:\n\n", "itemtype": "equation", "pos": 19973, "prevtext": "\nWith this approximation, the time to the next event is determined from \n$\\Phi(\\Delta t | \\{t_j\\}) \\approx \\exp \\left[-\\Delta t N \\overline{\\lambda}(\\{t_j\\})\\right] = u$, i.e.,\n$\\Delta t = -\\ln u/N\\overline{\\lambda}(\\{t_j\\})$.\nThe process that generates the event is determined by setting $\\Delta t=0$ in Eq.~\\eqref{eq:Pi(i | Delta t, set of t_j) Gillespie}.\nFor the Poisson process, we set $\\lambda_i(t_i) = \\lambda_i$ to recover the original Gillespie algorithm (Eqs.~\\eqref{eq:phi(Delta t) Gillespie} and \\eqref{eq:Pi(i) Gillespie}).\n\n\\section{Laplace Gillespie algorithm}\n\n\\subsection{Algorithm}\n\nIn the nMGA, we update the instantaneous event rates for allthe processes and its summation\n$\\overline{\\lambda}(\\{t_j\\})$ in Eq.~\\eqref{eq:average instantaneous rate for generalized Gillespie} upon the occurrence of each event. This is because $t_j$ ($1\\le j\\le N$) is updated upon an event. This procedure is time consuming when $N$ is large; we have to update individual instantaneous rates even if the probability density of the inter-event time for a process is not perturbed by an event that has occurred elsewhere.\n\nTo construct an efficient Gillespie algorithm for non-Markovian point processes, we start by considering the following renewal process, which we call the event-modulated Poisson process.\nWhen an event occurs, we first draw the rate of the Poisson process, denoted by $\\lambda$, according to a fixed probability density function $p(\\lambda)$. Then, we draw the time of the next event according to the Poisson process with rate $\\lambda$. Upon the next event, we renew the rate $\\lambda$ by redrawing it from $p(\\lambda)$. We repeat these procedures.\n\nThis model, the event-modulated Poisson process, is a mixture of Poisson processes with different rates. It is a non-Poissonian renewal process in general. It is slightly different from the mixed Poisson process, in which a single rate is drawn from a random ensemble in the beginning and used throughout a realisation \\cite{Karr1991book,Grandell1997book}. It is also different from doubly stochastic Poisson process (also called Cox process), in which the rate of the Poisson process is a stochastic process \\cite{Kingman1964MathprocCambPhilSoc,Karr1991book,Grandell1997book}, or its subclass called the Markov-modulated Poisson process, in which the event rate switches in time according to a Markov process \\cite{Fischer1992PerfEval}. In these processes, the dynamics of the event rate are independent of the occurrence of events. In contrast, the event rate changes upon the occurrence of events in the event-modulated Poisson process.\n\nThe event-modulated Poisson process is a Poisson process when conditioned on the current value of $\\lambda$. Therefore, when we simulate $N$ event-modulated Poisson processes, they are independent of each other and of the past event sequences if we are given the instantaneous rate of the $i$th process, denoted by $\\lambda_i$, for all $i$ ($1\\le i\\le N$). This property enables us to construct a Gillespie algorithm, similar to the original one. By engineering $p(\\lambda)$, we can emulate a range of renewal processes with different distributions of inter-event times. The new Gillespie algorithm, which we term the Laplace Gillespie algorithm (the reason for Laplace will be clear in section~\\ref{sub:theory}; it has a theoretical basis in the Laplace transform), is defined as the Gillespie algorithm for event-modulated Poisson processes. We denote the density of the event rate for the $i$th process by $p_i(\\lambda_i)$. The Laplace Gillespie algorithm proceeds as follows:\n\n\\begin{enumerate}\n\n\\item Initialise each of the $N$ processes by drawing the rate $\\lambda_i$ ($1\\le i\\le N$) according to the respective density function $p_i(\\lambda_i)$.\n\n\\item Draw the time to the next event $\\Delta t = -\\ln u/\\sum_{j=1}^N \\lambda_j$, where $u$ is the random variate uniformly distributed on $[0, 1]$.\n\n\\item Select the process $i$ that has generated the event with probability $\\lambda_i/\\sum_{j=1}^N \\lambda_j$.\n\n\\item Draw a new rate $\\lambda_i$ according to $p_i(\\lambda_i)$. If there are processes $j$ ($1\\le j\\le N$) for which the statistics of inter-event times have changed upon the event generated in steps 2 and 3 (e.g., a decrease in the rate of being infected owing to the recovery of an infected neighbour), modify $p_j(\\lambda_j)$ accordingly and draw a new rate $\\lambda_j$ from the modified $p_j(\\lambda_j)$. The rate of the remaining processes is unchanged.\n\n\\item Repeat steps 2--4.\n\n\\end{enumerate}\n\n\\subsection{Theory\\label{sub:theory}}\n\nThe event-modulated Poisson process is a renewal process. The renewal process is fully characterised by the probability density of inter-event times, $\\psi(\\tau)$. For the event-modulated Poisson process with the probability density of the event rate $p(\\lambda)$, we obtain\n\n", "index": 23, "text": "\\begin{equation}\n\\psi(\\tau) = \\int_0^{\\infty} p(\\lambda) \\lambda e^{-\\lambda \\tau} {\\rm d}\\lambda.\n\\label{eq:psi(tau) mix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\int_{0}^{\\infty}p(\\lambda)\\lambda e^{-\\lambda\\tau}{\\rm d}\\lambda.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>\u03bb</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nEquation~\\eqref{eq:Psi(tau) mix} indicates that $\\Psi(\\tau)$ is the Laplace transform of $p(\\lambda)$. Therefore, the necessary and sufficient condition for a renewal process to be simulated by the Laplace Gillespie algorithm is that $\\Psi(\\tau)$ is the Laplace transform of a probability density function of a random variable taking non-negative values. Although this statement can be made more rigorous if we replace $p(\\lambda){\\rm d}\\lambda$ by the probability distribution function, we use the probability density representation for simplicity.\n\nA necessary and sufficient condition for the existence of $p(\\lambda)$ is that $\\Psi(\\tau)$ is a completely monotone and $\\Psi(0)=1$ \\cite{\n\nFeller1971book2}. The complete monotonicity is defined by\n\n", "itemtype": "equation", "pos": 20232, "prevtext": "\nIntegration of both sides of Eq.~\\eqref{eq:psi(tau) mix} gives the survival function of the inter-event time as follows:\n\n", "index": 25, "text": "\\begin{equation}\n\\Psi(\\tau) = \\int_{\\tau}^{\\infty} \\psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime} = \\int_0^{\\infty} p(\\lambda) e^{-\\lambda \\tau} {\\rm d}\\lambda.\n\\label{eq:Psi(tau) mix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)=\\int_{\\tau}^{\\infty}\\psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}=\\int_{0%&#10;}^{\\infty}p(\\lambda)e^{-\\lambda\\tau}{\\rm d}\\lambda.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>\u03c4</mi><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup></mrow></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>\u03bb</mi></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nCondition $\\Psi(0)=1$ is satisfied by any survival function.\nWith $n=0$, Eq.~\\eqref{eq:completely monotone} reads $\\Psi(\\tau)\\ge 0$, which all survival functions satisfy.\nWith $n=1$, Eq.~\\eqref{eq:completely monotone} reads $\\psi(\\tau)\\ge 0$, which is also always satisfied. Equation~\\eqref{eq:completely monotone} with $n\\ge 2$ imposes nontrivial conditions.\n\n\\subsection{Examples\\label{sub:examples}}\n\nIn this section we show examples of distribution of inter-event times $\\psi(\\tau)$ for which the Laplace Gillespie algorithm can be used. These examples are summarised in Table~\\ref{tab:examples}.\n\n\\begin{itemize}\n\n\\item Exponential distribution\n\nThe Poisson process with rate $\\lambda_0$, i.e., $\\psi(\\tau) = \\lambda_0 e^{-\\lambda_0 \\tau}$, is trivially produced by $p(\\lambda)=\\delta(\\lambda-\\lambda_0)$, where $\\delta$ is the delta function.\n\n\\item Power-law distribution\n\nConsider the case in which $p(\\lambda)$ is the gamma distribution given by\n\n", "itemtype": "equation", "pos": 21175, "prevtext": "\nEquation~\\eqref{eq:Psi(tau) mix} indicates that $\\Psi(\\tau)$ is the Laplace transform of $p(\\lambda)$. Therefore, the necessary and sufficient condition for a renewal process to be simulated by the Laplace Gillespie algorithm is that $\\Psi(\\tau)$ is the Laplace transform of a probability density function of a random variable taking non-negative values. Although this statement can be made more rigorous if we replace $p(\\lambda){\\rm d}\\lambda$ by the probability distribution function, we use the probability density representation for simplicity.\n\nA necessary and sufficient condition for the existence of $p(\\lambda)$ is that $\\Psi(\\tau)$ is a completely monotone and $\\Psi(0)=1$ \\cite{\n\nFeller1971book2}. The complete monotonicity is defined by\n\n", "index": 27, "text": "\\begin{equation}\n(-1)^n \\frac{{\\rm d}\\Psi(\\tau)}{{\\rm d}\\tau} \\ge 0\\quad (\\lambda\\ge 0, n= 0, 1, \\ldots).\n\\label{eq:completely monotone}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"(-1)^{n}\\frac{{\\rm d}\\Psi(\\tau)}{{\\rm d}\\tau}\\geq 0\\quad(\\lambda\\geq 0,n=0,1,%&#10;\\ldots).\" display=\"block\"><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mfrac><mo>\u2265</mo><mn>0</mn><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo>\u2265</mo><mn>0</mn><mo>,</mo><mi>n</mi><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere $\\Gamma(\\alpha)$ is the gamma function, $\\alpha$ is the shape parameter of the gamma distribution, and $\\kappa$ is the scale parameter of the gamma distribution. With $\\alpha=1$, $p(\\lambda)$ is reduced to the exponential distribution.\nBy combining Eqs.~\\eqref{eq:Psi(tau) mix} and \\eqref{eq:p(lambda) gamma}, we obtain\n\n", "itemtype": "equation", "pos": 22282, "prevtext": "\nCondition $\\Psi(0)=1$ is satisfied by any survival function.\nWith $n=0$, Eq.~\\eqref{eq:completely monotone} reads $\\Psi(\\tau)\\ge 0$, which all survival functions satisfy.\nWith $n=1$, Eq.~\\eqref{eq:completely monotone} reads $\\psi(\\tau)\\ge 0$, which is also always satisfied. Equation~\\eqref{eq:completely monotone} with $n\\ge 2$ imposes nontrivial conditions.\n\n\\subsection{Examples\\label{sub:examples}}\n\nIn this section we show examples of distribution of inter-event times $\\psi(\\tau)$ for which the Laplace Gillespie algorithm can be used. These examples are summarised in Table~\\ref{tab:examples}.\n\n\\begin{itemize}\n\n\\item Exponential distribution\n\nThe Poisson process with rate $\\lambda_0$, i.e., $\\psi(\\tau) = \\lambda_0 e^{-\\lambda_0 \\tau}$, is trivially produced by $p(\\lambda)=\\delta(\\lambda-\\lambda_0)$, where $\\delta$ is the delta function.\n\n\\item Power-law distribution\n\nConsider the case in which $p(\\lambda)$ is the gamma distribution given by\n\n", "index": 29, "text": "\\begin{equation}\np(\\lambda) = \\frac{\\lambda^{\\alpha-1}e^{-\\lambda/\\kappa}}{\\Gamma(\\alpha)\\kappa^{\\alpha}},\n\\label{eq:p(lambda) gamma}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"p(\\lambda)=\\frac{\\lambda^{\\alpha-1}e^{-\\lambda/\\kappa}}{\\Gamma(\\alpha)\\kappa^{%&#10;\\alpha}},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>\u03bb</mi><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>/</mo><mi>\u03ba</mi></mrow></mrow></msup></mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\u03ba</mi><mi>\u03b1</mi></msup></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nThe probability density of inter-event time is given by the following power-law distribution:\n\n", "itemtype": "equation", "pos": 22757, "prevtext": "\nwhere $\\Gamma(\\alpha)$ is the gamma function, $\\alpha$ is the shape parameter of the gamma distribution, and $\\kappa$ is the scale parameter of the gamma distribution. With $\\alpha=1$, $p(\\lambda)$ is reduced to the exponential distribution.\nBy combining Eqs.~\\eqref{eq:Psi(tau) mix} and \\eqref{eq:p(lambda) gamma}, we obtain\n\n", "index": 31, "text": "\\begin{equation}\n\\Psi(\\tau) = \\frac{1}{(1+\\kappa\\tau)^{\\alpha}}.\n\\label{eq:Psi(tau) power law}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)=\\frac{1}{(1+\\kappa\\tau)^{\\alpha}}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mn>1</mn><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\n\n\nThe same mathematical procedure has been used in a reinforcement learning model for generating discount rates that decay as a power law in time \\cite{Kurthnelson2009PlosOne}.\n\n\\item Power-law distribution with an exponential tail\n\nConsider a shifted gamma distribution \\cite{Grandell1997book} given by\n\n", "itemtype": "equation", "pos": 22961, "prevtext": "\nThe probability density of inter-event time is given by the following power-law distribution:\n\n", "index": 33, "text": "\\begin{equation}\n\\psi(\\tau) = \\frac{\\kappa}{(1+\\kappa\\tau)^{\\alpha+1}}.\n\\label{eq:psi(tau) power law}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\frac{\\kappa}{(1+\\kappa\\tau)^{\\alpha+1}}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mi>\u03ba</mi><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>\u03b1</mi><mo>+</mo><mn>1</mn></mrow></msup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere $\\lambda_0$ is a constant.\nBy combining Eqs.~\\eqref{eq:Psi(tau) mix} and \\eqref{eq:p(lambda) shifted gamma}, we obtain\n\n", "itemtype": "equation", "pos": 23383, "prevtext": "\n\n\n\nThe same mathematical procedure has been used in a reinforcement learning model for generating discount rates that decay as a power law in time \\cite{Kurthnelson2009PlosOne}.\n\n\\item Power-law distribution with an exponential tail\n\nConsider a shifted gamma distribution \\cite{Grandell1997book} given by\n\n", "index": 35, "text": "\\begin{equation}\np(\\lambda) = \\begin{cases}\n\\frac{(\\lambda-\\lambda_0)^{\\alpha-1}e^{-(\\lambda-\\lambda_0)/\\kappa}}{\\Gamma(\\alpha)} & (\\lambda\\ge\\lambda_0),\\\\\n\n0 & (0 < \\lambda < \\lambda_0),\n\\end{cases}\n\\label{eq:p(lambda) shifted gamma}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"p(\\lambda)=\\begin{cases}\\frac{(\\lambda-\\lambda_{0})^{\\alpha-1}e^{-(\\lambda-%&#10;\\lambda_{0})/\\kappa}}{\\Gamma(\\alpha)}&amp;(\\lambda\\geq\\lambda_{0}),\\\\&#10;\\par&#10; 0&amp;(0&lt;\\lambda&lt;\\lambda_{0}),\\end{cases}\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"false\"><mfrac><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>-</mo><msub><mi>\u03bb</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>-</mo><msub><mi>\u03bb</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mi>\u03ba</mi></mrow></mrow></msup></mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>\u2265</mo><msub><mi>\u03bb</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>0</mn><mo>&lt;</mo><mi>\u03bb</mi><mo>&lt;</mo><msub><mi>\u03bb</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nBy differentiating Eq.~\\eqref{eq:Psi(tau) from shifted gamma}, we obtain a power-law distribution with an exponential tail given by\n\n", "itemtype": "equation", "pos": 23758, "prevtext": "\nwhere $\\lambda_0$ is a constant.\nBy combining Eqs.~\\eqref{eq:Psi(tau) mix} and \\eqref{eq:p(lambda) shifted gamma}, we obtain\n\n", "index": 37, "text": "\\begin{equation}\n\\Psi(\\tau) = \\frac{e^{-\\lambda_0 \\tau}}{(1+\\kappa\\tau)^{\\alpha}}.\n\\label{eq:Psi(tau) from shifted gamma}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)=\\frac{e^{-\\lambda_{0}\\tau}}{(1+\\kappa\\tau)^{\\alpha}}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\n\\item Weibull distribution\n\nThe Weibull distribution is defined by\n\n", "itemtype": "equation", "pos": 24027, "prevtext": "\nBy differentiating Eq.~\\eqref{eq:Psi(tau) from shifted gamma}, we obtain a power-law distribution with an exponential tail given by\n\n", "index": 39, "text": "\\begin{equation}\n\\psi(\\tau) = \\frac{e^{-\\lambda_0 \\tau}}{(1+\\kappa\\tau)^{\\alpha}}\\left(\\lambda_0+\\frac{\\kappa\\alpha}{1+\\kappa\\tau}\\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\frac{e^{-\\lambda_{0}\\tau}}{(1+\\kappa\\tau)^{\\alpha}}\\left(\\lambda_{%&#10;0}+\\frac{\\kappa\\alpha}{1+\\kappa\\tau}\\right).\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><msup><mi>e</mi><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>+</mo><mfrac><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhich yields\n\n", "itemtype": "equation", "pos": 24249, "prevtext": "\n\n\\item Weibull distribution\n\nThe Weibull distribution is defined by\n\n", "index": 41, "text": "\\begin{equation}\n\\Psi(\\tau) = e^{-(\\mu\\tau)^{\\alpha}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)=e^{-(\\mu\\tau)^{\\alpha}},\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nThe Weibull distribution with $\\alpha=1$ is the exponential distribution. The Weibull distribution has a longer and shorter tail than the exponential distribution when $\\alpha < 1$ and $\\alpha > 1$, respectively.\nThe Weibull distribution is expressed as the Laplace transform of a $p(\\lambda)$ if and only if\n$0<\\alpha\\le 1$ \\cite{Jewell1982AnnStat,Yannaros1994AnnInstStatMath}. The distribution when $\\alpha=1/2$ is the so-called stable distribution of order $1/2$, for which we obtain \\cite{Feller1971book2,Jewell1982AnnStat,Grandell1997book}\n\n", "itemtype": "equation", "pos": 24332, "prevtext": "\nwhich yields\n\n", "index": 43, "text": "\\begin{equation}\n\\psi(\\tau) = \\alpha\\mu^{\\alpha}\\tau^{\\alpha-1} e^{-(\\mu\\tau)^{\\alpha}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\alpha\\mu^{\\alpha}\\tau^{\\alpha-1}e^{-(\\mu\\tau)^{\\alpha}}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mi>\u03bc</mi><mi>\u03b1</mi></msup><mo>\u2062</mo><msup><mi>\u03c4</mi><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bc</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\nFor other values of $\\alpha$ ($0<\\alpha<1/2$, $1/2<\\alpha < 1$), the explicit form of $p(\\lambda)$ is complicated \\cite{Jewell1982AnnStat} such that it is impractical to use the Laplace Gillespie algorithm. \nFor these $\\alpha$ values, a mixture of a small number of exponential distributions may resemble the Weibull distribution \\cite{JinGonigunta2010JStatComputSimul} such that we may be able to use $p(\\lambda)$ with point masses at some discrete values of $\\lambda$ to approximate the Weibull distribution of inter-event times.\n\n\\item Gamma distribution\n\nWhen the inter-event time obeys the gamma distribution, i.e., \n\n", "itemtype": "equation", "pos": 24981, "prevtext": "\nThe Weibull distribution with $\\alpha=1$ is the exponential distribution. The Weibull distribution has a longer and shorter tail than the exponential distribution when $\\alpha < 1$ and $\\alpha > 1$, respectively.\nThe Weibull distribution is expressed as the Laplace transform of a $p(\\lambda)$ if and only if\n$0<\\alpha\\le 1$ \\cite{Jewell1982AnnStat,Yannaros1994AnnInstStatMath}. The distribution when $\\alpha=1/2$ is the so-called stable distribution of order $1/2$, for which we obtain \\cite{Feller1971book2,Jewell1982AnnStat,Grandell1997book}\n\n", "index": 45, "text": "\\begin{equation}\np(\\lambda) = \\frac{m^{\\frac{1}{2}}e^{-\\frac{m}{4\\lambda}}} {2\\sqrt{\\pi}\\lambda^{\\frac{3}{2}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"p(\\lambda)=\\frac{m^{\\frac{1}{2}}e^{-\\frac{m}{4\\lambda}}}{2\\sqrt{\\pi}\\lambda^{%&#10;\\frac{3}{2}}}.\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>m</mi><mfrac><mn>1</mn><mn>2</mn></mfrac></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mi>m</mi><mrow><mn>4</mn><mo>\u2062</mo><mi>\u03bb</mi></mrow></mfrac></mrow></msup></mrow><mrow><mn>2</mn><mo>\u2062</mo><msqrt><mi>\u03c0</mi></msqrt><mo>\u2062</mo><msup><mi>\u03bb</mi><mfrac><mn>3</mn><mn>2</mn></mfrac></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n$\\Psi(\\tau)$ is expressed as the Laplace transform of a probability density function $p(\\lambda)$ if and only if $0<\\alpha\\le 1$ \\cite{Yannaros1988JApplProb,Gleser1989AmStat}. We obtain \\cite{Gleser1989AmStat}\n\n", "itemtype": "equation", "pos": 25731, "prevtext": "\n\nFor other values of $\\alpha$ ($0<\\alpha<1/2$, $1/2<\\alpha < 1$), the explicit form of $p(\\lambda)$ is complicated \\cite{Jewell1982AnnStat} such that it is impractical to use the Laplace Gillespie algorithm. \nFor these $\\alpha$ values, a mixture of a small number of exponential distributions may resemble the Weibull distribution \\cite{JinGonigunta2010JStatComputSimul} such that we may be able to use $p(\\lambda)$ with point masses at some discrete values of $\\lambda$ to approximate the Weibull distribution of inter-event times.\n\n\\item Gamma distribution\n\nWhen the inter-event time obeys the gamma distribution, i.e., \n\n", "index": 47, "text": "\\begin{equation}\n\\psi(\\tau) = \\frac{\\tau^{\\alpha-1}e^{-\\tau/\\kappa}}{\\Gamma(\\alpha)\\kappa^{\\alpha}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\frac{\\tau^{\\alpha-1}e^{-\\tau/\\kappa}}{\\Gamma(\\alpha)\\kappa^{\\alpha%&#10;}},\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>\u03c4</mi><mrow><mi>\u03b1</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03c4</mi><mo>/</mo><mi>\u03ba</mi></mrow></mrow></msup></mrow><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>\u03ba</mi><mi>\u03b1</mi></msup></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\n\\item Mittag-Leffler distribution\n\nConsider the distribution of inter-event times defined in terms of the survival function given by\n\n", "itemtype": "equation", "pos": 26057, "prevtext": "\n$\\Psi(\\tau)$ is expressed as the Laplace transform of a probability density function $p(\\lambda)$ if and only if $0<\\alpha\\le 1$ \\cite{Yannaros1988JApplProb,Gleser1989AmStat}. We obtain \\cite{Gleser1989AmStat}\n\n", "index": 49, "text": "\\begin{equation}\np(\\lambda) = \\begin{cases}\n0 & (0 < \\lambda < \\kappa^{-1}),\\\\\n\n\\frac{1}{\\Gamma(\\alpha)\\Gamma(1-\\alpha)\\lambda(\\kappa\\lambda-1)^{\\alpha}} & (\\lambda\\ge \\kappa^{-1}).\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"p(\\lambda)=\\begin{cases}0&amp;(0&lt;\\lambda&lt;\\kappa^{-1}),\\\\&#10;\\par&#10;\\frac{1}{\\Gamma(\\alpha)\\Gamma(1-\\alpha)\\lambda(\\kappa\\lambda-1)^{\\alpha}}%&#10;&amp;(\\lambda\\geq\\kappa^{-1}).\\end{cases}\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>0</mn><mo>&lt;</mo><mi>\u03bb</mi><mo>&lt;</mo><msup><mi>\u03ba</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b1</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03bb</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03ba</mi><mo>\u2062</mo><mi>\u03bb</mi></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mi>\u03b1</mi></msup></mrow></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03bb</mi><mo>\u2265</mo><msup><mi>\u03ba</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": " \nwhere\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\item Mittag-Leffler distribution\n\nConsider the distribution of inter-event times defined in terms of the survival function given by\n\n", "index": 51, "text": "\\begin{equation}\n\\Psi(\\tau) = E_{\\beta}(-\\tau^{\\beta}),\n\\label{eq:Psi(tau) Mittag}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)=E_{\\beta}(-\\tau^{\\beta}),\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>E</mi><mi>\u03b2</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msup><mi>\u03c4</mi><mi>\u03b2</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nis the so-called Mittag-Leffler function. When $0<\\beta<1$, $\\Psi(\\tau)$ is completely monotone, and we obtain\n\\cite{Gorenflo1997chapter}\n\n", "itemtype": "equation", "pos": 26505, "prevtext": " \nwhere\n\n", "index": 53, "text": "\\begin{equation}\nE_{\\beta}(z) = \\sum_{n=0}^{\\infty} \\frac{z^n}{\\Gamma(1+\\beta n)}\n\\label{eq:Mittag-Leffler function}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"E_{\\beta}(z)=\\sum_{n=0}^{\\infty}\\frac{z^{n}}{\\Gamma(1+\\beta n)}\" display=\"block\"><mrow><mrow><msub><mi>E</mi><mi>\u03b2</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">\u221e</mi></munderover><mfrac><msup><mi>z</mi><mi>n</mi></msup><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>n</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nWhen $\\beta=1$, Eqs.~\\eqref{eq:Psi(tau) Mittag} and \\eqref{eq:Mittag-Leffler function} imply $\\Psi(\\tau)=e^{-\\lambda \\tau}$, yielding the Poisson process. When $0<\\beta<1$, $\\Psi(\\tau)$ is long-tailed with the following asymptotics \\cite{Gorenflo1997chapter,Georgiou2015PhysRevE}:\n\n", "itemtype": "equation", "pos": 26775, "prevtext": "\nis the so-called Mittag-Leffler function. When $0<\\beta<1$, $\\Psi(\\tau)$ is completely monotone, and we obtain\n\\cite{Gorenflo1997chapter}\n\n", "index": 55, "text": "\\begin{equation}\np(\\lambda) = \\frac{1}{\\pi} \\frac{\\lambda^{\\beta-1}\\sin(\\beta\\pi)}{\\lambda^{2\\beta} + 2\\lambda^{\\beta} \\cos(\\beta\\pi) + 1}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"p(\\lambda)=\\frac{1}{\\pi}\\frac{\\lambda^{\\beta-1}\\sin(\\beta\\pi)}{\\lambda^{2\\beta%&#10;}+2\\lambda^{\\beta}\\cos(\\beta\\pi)+1}.\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>\u03c0</mi></mfrac><mo>\u2062</mo><mfrac><mrow><msup><mi>\u03bb</mi><mrow><mi>\u03b2</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c0</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msup><mi>\u03bb</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03b2</mi></mrow></msup><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03bb</mi><mi>\u03b2</mi></msup><mo>\u2062</mo><mrow><mi>cos</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c0</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nor equivalently,\n\n", "itemtype": "equation", "pos": 27211, "prevtext": "\nWhen $\\beta=1$, Eqs.~\\eqref{eq:Psi(tau) Mittag} and \\eqref{eq:Mittag-Leffler function} imply $\\Psi(\\tau)=e^{-\\lambda \\tau}$, yielding the Poisson process. When $0<\\beta<1$, $\\Psi(\\tau)$ is long-tailed with the following asymptotics \\cite{Gorenflo1997chapter,Georgiou2015PhysRevE}:\n\n", "index": 57, "text": "\\begin{equation}\n\\Psi(\\tau) \\approx \\frac{\\sin(\\beta\\pi)\\Gamma(\\beta)}{\\pi \\tau^{\\beta}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"\\Psi(\\tau)\\approx\\frac{\\sin(\\beta\\pi)\\Gamma(\\beta)}{\\pi\\tau^{\\beta}},\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mfrac><mrow><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c0</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><msup><mi>\u03c4</mi><mi>\u03b2</mi></msup></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nTherefore, this class of $\\psi(\\tau)$ produces long-tailed distributions of inter-event times with a power-law exponent between one and two. A special case is given with $\\beta=1/2$, in which case\n$\\Psi(\\tau) = e^{-\\tau^{2\\beta}}\\left[1-\\text{erf} (\\tau^{\\beta})\\right]$, where $\\text{erf}(z)\\equiv (2/\\sqrt{\\pi})\\int_0^z e^{-z^{\\prime 2}} {\\rm d}z^{\\prime}$ is the error function.\n\n\\item Integral of a valid survival function\n\nFor a survival function $\\Psi(\\tau)$ corresponding to the probability density of inter-event times $\\psi(\\tau)$, consider \n\n", "itemtype": "equation", "pos": 27333, "prevtext": "\nor equivalently,\n\n", "index": 59, "text": "\\begin{equation}\n\\psi(\\tau) \\approx \\frac{\\beta\\sin(\\beta\\pi)\\Gamma(\\beta)}{\\pi \\tau^{\\beta+1}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)\\approx\\frac{\\beta\\sin(\\beta\\pi)\\Gamma(\\beta)}{\\pi\\tau^{\\beta+1}}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mfrac><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mi>sin</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>\u03c0</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><msup><mi>\u03c4</mi><mrow><mi>\u03b2</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nFunction $\\Psi^{\\rm w}(\\tau)$ is well-defined if and only if $\\langle \\tau\\rangle_{\\psi}$, i.e., the mean inter-event time with respect to density $\\psi(\\tau)$, is finite. Assume that the renewal process generated by $\\psi(\\tau)$ permits the use of the Laplace Gillespie algorithm. Because $\\Psi^{\\rm w}(\\tau)\\ge 0$ ($\\tau\\ge 0$), ${\\rm d}^n\\Psi^{\\rm w}(\\tau)/{\\rm d}\\tau^n = - \\left[{\\rm d}^{n-1}\\Psi(\\tau)/{\\rm d}\\tau^{n-1}\\right]/\\int_0^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}$ ($n=1, 2, \\ldots$), and $\\Psi(\\tau)$ is completely monotone, $\\Psi^{\\rm w}(\\tau)$ is completely monotone. In addition, Eq.~\\eqref{eq:Psi^w(tau)} implies $\\Psi^{\\rm w}(0)=1$. Therefore, the renewal process with survival function $\\Psi^{\\rm w}(\\tau)$ can be also simulated by the Laplace Gillespie algorithm.\n\nThe corresponding probability density of the inter-event time is given by\n\n", "itemtype": "equation", "pos": 27996, "prevtext": "\nTherefore, this class of $\\psi(\\tau)$ produces long-tailed distributions of inter-event times with a power-law exponent between one and two. A special case is given with $\\beta=1/2$, in which case\n$\\Psi(\\tau) = e^{-\\tau^{2\\beta}}\\left[1-\\text{erf} (\\tau^{\\beta})\\right]$, where $\\text{erf}(z)\\equiv (2/\\sqrt{\\pi})\\int_0^z e^{-z^{\\prime 2}} {\\rm d}z^{\\prime}$ is the error function.\n\n\\item Integral of a valid survival function\n\nFor a survival function $\\Psi(\\tau)$ corresponding to the probability density of inter-event times $\\psi(\\tau)$, consider \n\n", "index": 61, "text": "\\begin{equation}\n\\Psi^{\\rm w}(\\tau) \\equiv \\frac{\\int_{\\tau}^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}}\n{\\int_0^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}}\n\n= \\frac{\\int_{\\tau}^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}}\n{\\langle \\tau\\rangle_{\\psi}}.\n\\label{eq:Psi^w(tau)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"\\Psi^{\\rm w}(\\tau)\\equiv\\frac{\\int_{\\tau}^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}%&#10;\\tau^{\\prime}}{\\int_{0}^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}}\\par&#10;=%&#10;\\frac{\\int_{\\tau}^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}}{\\langle\\tau%&#10;\\rangle_{\\psi}}.\" display=\"block\"><mrow><mrow><mrow><msup><mi mathvariant=\"normal\">\u03a8</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2261</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>\u03c4</mi><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi>\u03c4</mi><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03c4</mi><mo>\u2032</mo></msup></mrow></mrow></mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03c4</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mi>\u03c8</mi></msub></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nIn terms of $p(\\lambda)$, we obtain\n\n", "itemtype": "equation", "pos": 29171, "prevtext": "\nFunction $\\Psi^{\\rm w}(\\tau)$ is well-defined if and only if $\\langle \\tau\\rangle_{\\psi}$, i.e., the mean inter-event time with respect to density $\\psi(\\tau)$, is finite. Assume that the renewal process generated by $\\psi(\\tau)$ permits the use of the Laplace Gillespie algorithm. Because $\\Psi^{\\rm w}(\\tau)\\ge 0$ ($\\tau\\ge 0$), ${\\rm d}^n\\Psi^{\\rm w}(\\tau)/{\\rm d}\\tau^n = - \\left[{\\rm d}^{n-1}\\Psi(\\tau)/{\\rm d}\\tau^{n-1}\\right]/\\int_0^{\\infty}\\Psi(\\tau^{\\prime}){\\rm d}\\tau^{\\prime}$ ($n=1, 2, \\ldots$), and $\\Psi(\\tau)$ is completely monotone, $\\Psi^{\\rm w}(\\tau)$ is completely monotone. In addition, Eq.~\\eqref{eq:Psi^w(tau)} implies $\\Psi^{\\rm w}(0)=1$. Therefore, the renewal process with survival function $\\Psi^{\\rm w}(\\tau)$ can be also simulated by the Laplace Gillespie algorithm.\n\nThe corresponding probability density of the inter-event time is given by\n\n", "index": 63, "text": "\\begin{equation}\n\\psi^{\\rm w}(\\tau) = -\\frac{{\\rm d}\\Psi^{\\rm w}(\\tau)}{{\\rm d}\\tau} =\n\\frac{\\Psi(\\tau)}\n{\\langle \\tau\\rangle_{\\psi}}, \n\\label{eq:psi^w(tau)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\psi^{\\rm w}(\\tau)=-\\frac{{\\rm d}\\Psi^{\\rm w}(\\tau)}{{\\rm d}\\tau}=\\frac{\\Psi(%&#10;\\tau)}{\\langle\\tau\\rangle_{\\psi}},\" display=\"block\"><mrow><mrow><mrow><msup><mi>\u03c8</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mfrac><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u03a8</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi mathvariant=\"normal\">d</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mfrac></mrow><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">\u03a8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\u03c4</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mi>\u03c8</mi></msub></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 29380, "prevtext": "\nIn terms of $p(\\lambda)$, we obtain\n\n", "index": 65, "text": "\\begin{equation}\n\\psi^{\\rm w}(\\tau) = \\frac{\\int_0^{\\infty} p(\\lambda) e^{-\\lambda\\tau}{\\rm d}\\lambda}\n{\\int_0^{\\infty}\\frac{p(\\lambda^{\\prime})}{\\lambda^{\\prime}}{\\rm d}\\lambda^{\\prime}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E33.m1\" class=\"ltx_Math\" alttext=\"\\psi^{\\rm w}(\\tau)=\\frac{\\int_{0}^{\\infty}p(\\lambda)e^{-\\lambda\\tau}{\\rm d}%&#10;\\lambda}{\\int_{0}^{\\infty}\\frac{p(\\lambda^{\\prime})}{\\lambda^{\\prime}}{\\rm d}%&#10;\\lambda^{\\prime}}\" display=\"block\"><mrow><mrow><msup><mi>\u03c8</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>\u03bb</mi></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mfrac><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nTherefore, in each update in the Laplace Gillespie algorithm with the density of inter-event times $\\psi^{\\rm w}(\\tau)$, the rate of the Poisson process $\\lambda$ should be sampled according to density $p^{\\rm w}(\\lambda)$, where\n\n", "itemtype": "equation", "pos": 29587, "prevtext": "\nand\n\n", "index": 67, "text": "\\begin{equation}\n\\Psi^{\\rm w}(\\tau) = \\frac{\\int_0^{\\infty}\\frac{p(\\lambda)}{\\lambda}e^{-\\lambda\\tau}{\\rm d}\\lambda}\n{\\int_0^{\\infty}\\frac{p(\\lambda^{\\prime})}{\\lambda^{\\prime}}{\\rm d}\\lambda^{\\prime}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E34.m1\" class=\"ltx_Math\" alttext=\"\\Psi^{\\rm w}(\\tau)=\\frac{\\int_{0}^{\\infty}\\frac{p(\\lambda)}{\\lambda}e^{-%&#10;\\lambda\\tau}{\\rm d}\\lambda}{\\int_{0}^{\\infty}\\frac{p(\\lambda^{\\prime})}{%&#10;\\lambda^{\\prime}}{\\rm d}\\lambda^{\\prime}}.\" display=\"block\"><mrow><mrow><mrow><msup><mi mathvariant=\"normal\">\u03a8</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>\u03bb</mi></mfrac><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msup><mo>\u2062</mo><mrow><mo>d</mo><mi>\u03bb</mi></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mfrac><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\n\nFor example, if $\\psi(\\tau)$ is the exponential distribution, $\\psi^{\\rm w}(\\tau)$ is the exponential distribution with the same rate. If $\\psi(\\tau)$ is the power-law distribution given by Eq.~\\eqref{eq:psi(tau) power law},\n$\\psi^{\\rm w}(\\tau)$ is the same form of the power-law distribution with $\\alpha$ replaced by $\\alpha-1$.\n\n\\item Product of valid survival functions\n\nThe product of two completely monotone functions $\\Psi_1(\\tau)$ and $\\Psi_2(\\tau)$ are completely monotone \\cite{Feller1971book2}. In addition, $\\Psi_1(0)\\Psi_2(0)=1$ if $\\Psi_1(0) = \\Psi_2(0)=1$. Therefore, survival function $\\Psi(\\tau)\\equiv \\Psi_1(\\tau)\\Psi_2(\\tau)$ admits the Laplace Gillespie algorithm if $\\Psi_1(\\tau)$ and $\\Psi_2(\\tau)$ do. The probability density of the rate will be the convolution of $p_1(\\lambda)$ and $p_2(\\lambda)$, where $\\Psi_i(\\tau)=\\int_0^{\\infty} p_i(\\lambda) e^{-\\lambda \\tau}{\\rm d}\\lambda$ ($i=1, 2$).\n\n\\end{itemize}\n\n\\subsection{Limitations\\label{sub:not allowed}} \n\nThe Laplace Gillespie algorithm cannot be used if and only if the survival function of the inter-event time is not completely monotone. In this section, we present some convenient conditions and examples that fit in this class of survival functions.\n\n\\begin{itemize}\n\n\\item Non-monotone\n\nBy setting $n=2$ in Eq.~\\eqref{eq:completely monotone}, we obtain ${\\rm d\\psi(\\tau)}/{\\rm d}\\tau \\le 0$. Therefore, $\\psi(\\tau)$ must monotonically decrease with $\\tau$ for the Laplace Gillespie algorithm to be applicable. This condition excludes the gamma and Weibull distributions with shape parameter $\\alpha>1$, any log-normal distribution, and any Pareto distribution, i.e.,\n\n", "itemtype": "equation", "pos": 30035, "prevtext": "\nTherefore, in each update in the Laplace Gillespie algorithm with the density of inter-event times $\\psi^{\\rm w}(\\tau)$, the rate of the Poisson process $\\lambda$ should be sampled according to density $p^{\\rm w}(\\lambda)$, where\n\n", "index": 69, "text": "\\begin{equation}\np^{\\rm w}(\\lambda) = \\frac{\\frac{p(\\lambda)}{\\lambda}}\n{\\int_0^{\\infty}\\frac{p(\\lambda^{\\prime})}{\\lambda^{\\prime}}{\\rm d}\\lambda^{\\prime}}.\n\\label{eq:p^w(lambda)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E35.m1\" class=\"ltx_Math\" alttext=\"p^{\\rm w}(\\lambda)=\\frac{\\frac{p(\\lambda)}{\\lambda}}{\\int_{0}^{\\infty}\\frac{p(%&#10;\\lambda^{\\prime})}{\\lambda^{\\prime}}{\\rm d}\\lambda^{\\prime}}.\" display=\"block\"><mrow><mrow><mrow><msup><mi>p</mi><mi mathvariant=\"normal\">w</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mi>\u03bb</mi></mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup><mrow><mfrac><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mfrac><mo>\u2062</mo><mrow><mo>d</mo><msup><mi>\u03bb</mi><mo>\u2032</mo></msup></mrow></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere $\\alpha>0$ and $\\tau_0>0$.\n\n\\item Coefficient of variation smaller than unity\n\nThe complete monotonicity implies that the coefficient of variation (CV), i.e., standard deviation divided by the mean, of $\\tau$ is larger than or equal to unity \\cite{Yannaros1994AnnInstStatMath}. This condition excludes the gamma and Weibull distributions with $\\alpha>1$. In practice, a CV value smaller than unity indicates that events occur more regularly than in the case of the Poisson process, which yields $\\text{CV} = 1$. Therefore, renewal processes producing relatively periodic event sequences are also excluded.\n\n\\item Higher-order conditions\n\nEven if ${\\rm d}\\psi(\\tau)/{\\rm d}\\tau \\le 0$ and the CV is large, the survival function of a common distribution may not be completely monotone. For example, the one-sided Cauchy distribution defined by $\\psi(\\tau) = 1/\\left[\\pi(\\tau^2 + 1)^2\\right]$ yields ${\\rm d}^2\\psi(\\tau)/{\\rm d}\\tau^2 = 4(3\\tau^2-1)/(\\tau^2+1)^3$, whose sign depends on the value of $\\tau$.\n\nEmpirical evidence of online correspondences of humans suggests bimodal $\\psi(\\tau)$ in the sense that, excluding very small $\\tau$, it obeys a power-law distribution for small $\\tau$ and an exponential distribution for large $\\tau$ \\cite{WuZhou2010PNAS}. Such a $\\psi(\\tau)$ monotonically decreases with $\\tau$, verifying ${\\rm d}\\psi(\\tau)/{\\rm d}\\tau \\le 0$. However, the sign of ${\\rm d}^2\\psi(\\tau)/{\\rm d}\\tau^2$ depends on the $\\tau$ value such that the corresponding survival function is not completely monotone.\n \n\\end{itemize}\n\n\\subsection{Empirical distributions of inter-event times}\n\nWe are often interested in informing population dynamics described as interacting stochastic point processes, such as epidemic processes, by empirical data of event sequences. The standard numerical method emulates the dynamics on top of empirical event sequences, i.e., using a given list of events with the event type and time stamp \\cite{HolmeSaramaki2012PhysRep,Masuda2013F1000,Holme2015EurPhysJB}. Another approach would be to estimate $\\psi(\\tau)$ from empirical data and use a variant of the Gillespie algorithm to simulate stochastic processes.\n\nThe nMGA \\cite{BogunaLafuerza2014PhysRevE} and the modified next reaction method \\cite{Anderson2007JChemPhys} allow arbitrary $\\psi(\\tau)$ including the empirical distribution. In contrast, the Laplace Gillespie algorithm requires the survival function, $\\Psi(\\tau)$, to be completely monotone. Under this condition, we may be able to compute the inverse Laplace transform to obtain $p(\\lambda)$ at a reasonable cost \\cite{Abate1992QueueSyst,Abate1995OrsaJComput}. Because an empirical distribution is likely not completely monotone, we propose two alternative methods for empirical data. The first method is to fit a completely monotone survival function of inter-event times, such as Eq.~\\eqref{eq:Psi(tau) power law}, to given data. The second method is to estimate a mixture of exponential distributions with different rates to approximate the empirical $\\psi(\\tau)$ or $\\Psi(\\tau)$. Likelihood or other cost-function methods are available for the estimation \\cite{Jewell1982AnnStat,Heckman1990JAmStatAssoc,Kleinberg2003DataMinKnowlDisc,Politi2007PhysicaA,JinGonigunta2010JStatComputSimul}. The approximation error is guaranteed to decay inversely proportionally to the number of constituent distributions \\cite{LiBarron2000NIPS}.\n\n\\subsection{Initial conditions}\n\nWhen we start running $N$ processes in parallel, we may initially draw the inter-event time for each process from $\\psi(\\tau)$. This initial condition defines the so-called ordinary renewal process \\cite{Cox1962book}. An alternative model, called the equilibrium renewal process, assumes that the process has started at time $t=-\\infty$ such that the first inter-event time for each process, drawn at $t=0$, uses the distribution of waiting times to the next event in the equilibrium, not $\\psi(\\tau)$ (i.e., distribution of inter-event times) \\cite{Cox1962book}. The probability density of the waiting time to the next event is given by Eq.~\\eqref{eq:psi^w(tau)}. To simulate the equilibrium renewal process, we start by drawing the rate of the Poisson process according to $p^{\\rm w}(\\lambda)$ given by Eq.~\\eqref{eq:p^w(lambda)}. Afterwards, we draw the rate according to $p(\\lambda)$.\n\n\\section{Numerical performance}\n\nIn this section, we compare the performance of the nMGA \\cite{BogunaLafuerza2014PhysRevE} and the Laplace Gillespie algorithm.\nWe use the power-law distribution of inter-event times given by Eq.~\\eqref{eq:psi(tau) power law}.\nBecause $\\kappa$ only controls the scale of the inter-event time, we set $\\kappa=1$ without loss of generality.\nTo generate gamma-distributed random variates, we use a popular algorithm based on the rejection method  \\cite{Marsaglia2000ACMTransMathSoft}. In fact, we adapt an open source code \\cite{Marsaglia-code} to our purposes. We generate $N/3$ processes by Eq.~\\eqref{eq:psi(tau) power law} with $\\alpha=1$, another $N/3$ processes with $\\alpha=1.5$, and the remaining processes with $\\alpha=2$. We continue the simulation until any of the $N$ processes generates $10^6$ inter-event times for the first time. We assume the ordinary renewal process such that the initial inter-event time for each process is drawn from $\\psi(\\tau)$.\n\nThe survival function for one process with $\\alpha=1$, one with $\\alpha=1.5$, and another with $\\alpha=2$\nis shown by the solid curves for the nMGA and the Laplace Gillespie algorithm in Figs.~\\ref{fig:survival}(a) and \\ref{fig:survival}(b), respectively, with $N=10$. The theoretical survival function, Eq.~\\eqref{eq:Psi(tau) power law}, is plotted by the dashed curves. The results obtained from the Laplace Gillespie algorithm (Fig.~\\ref{fig:survival}(b)) are more accurate than those obtained from the nMGA. This is because the nMGA is exact in the limit of $N\\to\\infty$, whereas the Laplace Gillespie algorithm is exact for any $N$. When $N=10^3$, the nMGA is sufficiently accurate (Fig.~\\ref{fig:survival}(c)), as is the Laplace Gillespie algorithm (Fig.~\\ref{fig:survival}(d)). The results shown in Figs.~\\ref{fig:survival}(a) and \\ref{fig:survival}(c) are consistent with the numerical results obtained in Ref.~\\cite{BogunaLafuerza2014PhysRevE}.\n\nThe nMGA may require a lot of time in calculating the instantaneous event rate \n($\\lambda_j(0)$ in Eq.~\\eqref{eq:average instantaneous rate for generalized Gillespie}) for all processes every time an event occurs in one of the $N$ processes. The Laplace Gillespie algorithm avoid the rate recalculation whereas it may be costly to calculate the gamma variate each time an event occurs. We numerically compare the computation time for the two algorithms by varying $N$. The other parameters are the same as those used in Fig.~\\ref{fig:survival}. We do not optimise the method to select the $i$ value with probability $\\Pi_i$ at the occurrence of each event; we use the simple linear search for both the nMGA and Laplace Gillespie algorithm. In the Laplace Gillespie algorithm, we use a binary tree data structure to store and update $\\lambda_i$ ($1\\le i\\le N$) to accelerate the algorithm. This structure is useful when only a small number of $\\lambda_i$ is changed upon each event. This is not the case for the nMGA.\nWe use codes written in C++, compiled with a standard g++ compiler without an optimisation option on a Mac Book Air with 1.7 GHz Intel Core i7 and 8Gz 1600 MHz DDR3. The computation time in secs plotted against $N$ in Fig.~\\ref{fig:survival}(e) indicates that the Laplace Gillespie algorithm is substantially faster than the nMGA as $N$ increases.\n\nThe Laplace Gillespie algorithm outperforms the nMGA in that the Laplace Gillespie algorithm is exact for any $N$ and generally runs faster than the nMGA. The price paid is that the form of $\\psi(\\tau)$ is limited, whereas the nMGA allows any $\\psi(\\tau)$.\n\n\\section{Applications}\n\n\\subsection{Positively correlated sequences of inter-event times\\label{sub:positive correlation}}\n\nWe have considered the renewal processes, i.e., stationary point processes without correlation between different inter-event times. However, inter-event times are positively correlated, albeit weakly, in a majority of data recorded from human activity \\cite{GohBarabasi2008EPL,Masuda2013hawkes}. This and other types of correlation in sequences of event times is known to affect upshots of epidemic processes \\cite{Karsai2011PhysRevE,Miritello2011PhysRevE,Rocha2011PlosComputBiol,Masuda2013F1000}.\n\nThe Laplace Gillespie algorithm provides a succinct method for generating point processes with positive correlation without changing $\\psi(\\tau)$. To generate such event sequences, we redraw a new event rate for the Poisson process, $\\lambda_i$, with probability $1-q$ ($0\\le q< 1$), when the $i$th process has generated an event. With probability $q$, we continue to use the same value of $\\lambda_i$ until the $i$th process generates the next event. The standard Laplace Gillespie algorithm is recovered with $q=0$. The correlation between inter-event times is expected to grow as $q$ increases. Although the same $\\lambda_i$ value may be used for generating consecutive inter-event times, the corresponding inter-event times are different because they are generated from the Poisson process. The computation time decreases as $q$ increases because the number of times one has to redraw $\\lambda_i$ is proportional to $1-q$.\n\nIn the continuous-time Markov process with a state-dependent hopping rate, the inter-event time defined as the time between two consecutive hops regardless of the state is generally correlated across inter-event times \\cite{Schwalger2010EurPhysJSpecTopics}. The present algorithm can be interpreted as a special case of this general framework such that the state is continuous, the process hops to the current state with probability $1-q$, and it hops to any other state with the probability proportional to $q\\times p(\\lambda)$. The correlated Laplace Gillespie algorithm can be alternatively built on top of a finite-state \\cite{Schwalger2010EurPhysJSpecTopics} or infinite-state \\cite{Kleinberg2003DataMinKnowlDisc} Markov process with a general transition probability between states. This variant of the model will be similar to a two-state cascading Poisson process that assumes transitions between a normal and excited states, with the excited state having a higher event rate than the normal state \\cite{Malmgren2008PNAS}.\n\nTwo remarks are in order. First, $\\psi(\\tau)$ is independent of the $p$ value. This is because the stationary density of the corresponding continuous-time Markov process in the $\\lambda$-space is equal to $p(\\lambda)$ irrespectively of the $q$ value. Second, this algorithm cannot be used to generate correlated event sequences when $\\psi(\\tau)$ is the exponential distribution. In this case, the event rate $\\lambda$ must be kept constant throughout the time and therefore cannot be modulated in a temporally correlated manner.\n\nWe measure the so-called memory coefficient \\cite{GohBarabasi2008EPL} to quantify the amount of correlation in a sequence of inter-event times. The memory coefficient for a sequence of inter-event times, $\\{\\tau_1, \\tau_2, \\ldots, \\tau_n\\}$, where $n$ is the number of inter-event times, is defined by\n\n", "itemtype": "equation", "pos": 31883, "prevtext": "\n\nFor example, if $\\psi(\\tau)$ is the exponential distribution, $\\psi^{\\rm w}(\\tau)$ is the exponential distribution with the same rate. If $\\psi(\\tau)$ is the power-law distribution given by Eq.~\\eqref{eq:psi(tau) power law},\n$\\psi^{\\rm w}(\\tau)$ is the same form of the power-law distribution with $\\alpha$ replaced by $\\alpha-1$.\n\n\\item Product of valid survival functions\n\nThe product of two completely monotone functions $\\Psi_1(\\tau)$ and $\\Psi_2(\\tau)$ are completely monotone \\cite{Feller1971book2}. In addition, $\\Psi_1(0)\\Psi_2(0)=1$ if $\\Psi_1(0) = \\Psi_2(0)=1$. Therefore, survival function $\\Psi(\\tau)\\equiv \\Psi_1(\\tau)\\Psi_2(\\tau)$ admits the Laplace Gillespie algorithm if $\\Psi_1(\\tau)$ and $\\Psi_2(\\tau)$ do. The probability density of the rate will be the convolution of $p_1(\\lambda)$ and $p_2(\\lambda)$, where $\\Psi_i(\\tau)=\\int_0^{\\infty} p_i(\\lambda) e^{-\\lambda \\tau}{\\rm d}\\lambda$ ($i=1, 2$).\n\n\\end{itemize}\n\n\\subsection{Limitations\\label{sub:not allowed}} \n\nThe Laplace Gillespie algorithm cannot be used if and only if the survival function of the inter-event time is not completely monotone. In this section, we present some convenient conditions and examples that fit in this class of survival functions.\n\n\\begin{itemize}\n\n\\item Non-monotone\n\nBy setting $n=2$ in Eq.~\\eqref{eq:completely monotone}, we obtain ${\\rm d\\psi(\\tau)}/{\\rm d}\\tau \\le 0$. Therefore, $\\psi(\\tau)$ must monotonically decrease with $\\tau$ for the Laplace Gillespie algorithm to be applicable. This condition excludes the gamma and Weibull distributions with shape parameter $\\alpha>1$, any log-normal distribution, and any Pareto distribution, i.e.,\n\n", "index": 71, "text": "\\begin{equation}\n\\psi(\\tau)= \n\\begin{cases}\n\\frac{\\alpha}{\\tau_0}\\left(\\frac{\\tau_0}{\\tau}\\right)^{\\alpha+1} & (\\tau\\ge \\tau_0),\\\\\n0 & (\\tau < \\tau_0),\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E36.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\tau)=\\begin{cases}\\frac{\\alpha}{\\tau_{0}}\\left(\\frac{\\tau_{0}}{\\tau}%&#10;\\right)^{\\alpha+1}&amp;(\\tau\\geq\\tau_{0}),\\\\&#10;0&amp;(\\tau&lt;\\tau_{0}),\\end{cases}\" display=\"block\"><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mstyle displaystyle=\"false\"><mfrac><mi>\u03b1</mi><msub><mi>\u03c4</mi><mn>0</mn></msub></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"false\"><mfrac><msub><mi>\u03c4</mi><mn>0</mn></msub><mi>\u03c4</mi></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>\u03b1</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>\u2265</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03c4</mi><mo>&lt;</mo><msub><mi>\u03c4</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01490.tex", "nexttext": "\nwhere $m_1 = \\sum_{i=1}^{n-1} \\tau_i / (n-1)$ and $m_2 = \\sum_{i=2}^n \\tau_i/(n-1)$.\n\nFor the power-law distribution of inter-event times given by Eq.~\\eqref{eq:psi(tau) power law} with $\\kappa=1$, we generate a sequence of $n=10^5$ inter-event times and calculate $M$. The mean and standard deviation of $M$ calculated on the basis of $10^3$ sequences are plotted for $\\alpha=1$ and $\\alpha=2$ in Figs.~\\ref{fig:correlated}(a) and \\ref{fig:correlated}(b), respectively. For both $\\alpha$ values, $M$ monotonically increases with $p$, and a range of $M$ between 0 and $\\approx 0.4$ is produced. In empirical data, $M$ is between 0 and 0.1 for human activities and 0.1 and 0.25 for natural phenomena \\cite{GohBarabasi2008EPL}. These ranges of $M$ are produced with approximately $0\\le q\\le 0.2$ and $0.2 \\le q\\le 0.5$, respectively.\n\n\\subsection{Epidemic processes}\n\nPrevious numerical efforts suggested that dynamics of epidemic processes in well-mixed populations or networks were altered if contact events were generated by renewal processes different from the Poisson process\n\\cite{Rocha2013PlosComputBiol,Vanmieghem2013PhysRevLett,MinGohKim2013EPL,Horvath2014NewJPhys}.\nThe nMGA and Laplace Gillespie algorithm can be used for efficiently implementing such models of epidemic processes. To demonstrate the use of the Laplace Gillespie algorithm, we model node-centric susceptible-infected-recovered (SIR) epidemic spreading, which is similar to previous models \\cite{Rocha2011PlosComputBiol,Rocha2013PlosComputBiol,Perra2012SciRep\n\n}.\n\nConsider a population of $N$ nodes.\nAt any point of time, each node assumes one of the three states, susceptible, infected, or recovered. An infected node is assumed to infect a susceptible node upon a contact. An infected node transits to the recovered state according to the Poisson process with rate $\\mu$. A recovered node neither infects nor is infected by other nodes.\nRegarding infection events, each node $i$ ($1\\le i\\le N$) is driven by an independent and identical point process whose probability density of inter-event times is given by $\\psi(\\tau)$. It should be noted that a similar approach could be link-centric, that is, the activation of each link follws independent and idential point processes. When an event occurs, node $i$ is instantaneously activated and contacts a randomly selected node $j$. In the case of a well-mixed population, each $j$ is selected with probability $1/(N-1)$. In the case of a network, which is fixed over time, $j$ is selected from the neighbours of node $i$ with the equal probability. If either $i$ or $j$ is infected and the other is susceptible, the infection is transmitted such that the susceptible node becomes infected. Then, node $i$ waits for another time $\\tau$ drawn from $\\psi(\\tau)$ before the next activation.\n\nThe mean time to the node activation, which enables infection, is given by $\\langle \\tau\\rangle = \\int_0^{\\infty}\\tau \\psi(\\tau){\\rm d}\\tau$. The mean time for an infected node to recover is equal to $1/\\mu$. We define the effective infection rate by $\\lambda_{\\rm eff} = (1/\\mu) / \\langle \\tau\\rangle$\n\\cite{BogunaLafuerza2014PhysRevE}. We control $\\lambda_{\\rm eff}$ by changing $\\mu$ for a given $\\psi(\\tau)$.\nThis definition is justified because multiplying $\\langle \\tau\\rangle$ and $1/\\mu$ by the same factor only changes the time scale of the dynamics.\n\nWe assume the equilibrium point process, i.e., start the simulation from the equilibrium state. This is equivalent to drawing the first event time for each node from the waiting-time distribution, $\\psi^{\\rm w}(\\tau)$, not from $\\psi(\\tau)$, and subsequent event times from $\\psi(\\tau)$. The population structure is assumed to be either well-mixed or the regular random graph in which all nodes have degree five and all links are randomly formed. In both cases, we set $N=10^4$. Each simulation starts from the same initial condition in which a particular node, which is the same in all simulations, is infected and all the other $N-1$ nodes are susceptible. We measure the number of recovered nodes at the end of the simulation, called the final size, normalised by $N$ and averaged over $10^4$ simulations. We consider four point processes for node activation, i.e., the exponential distribution, corresponding to the Poisson process, and three power-law distributions given by Eq.~\\eqref{eq:psi(tau) power law} with $\\alpha=1.5$, $\\kappa=1$, and $q=0$, 0.2, and 0.9.\n\nThe final size for the well-mixed population and the regular random graph is shown in Figs.~\\ref{fig:epidemic}(a) and \\ref{fig:epidemic}(b), respectively. For both population structures and across the entire range of the effective infection rate, $\\lambda_{\\rm eff}$, the final size is larger when $\\psi(\\tau)$ is the power-law than when $\\psi(\\tau)$ is the exponential distribution. Consistent with this result, the epidemic threshold, i.e., the value of $\\lambda_{\\rm eff}$ at which the final size becomes positive, is smaller for the power-law $\\psi(\\tau)$ than the exponential $\\psi(\\tau)$.\n\n\n\nThe final size is larger with positive correlation of inter-event times ($q=0.9$) than with no correlation ($q=0$). The results for $q=0.2$ are almost the same as those for $q=0$. \nBecause realistic values of the memory coefficient, $M$, are produced by $0\\le q\\le 0.2$ (section~\\ref{sub:positive correlation}), we conclude that a realistic amount of positive correlation in inter-event times does not affect the final size.\n\n\n\n\n\n\\section{Discussion}\n\nWe provided a generalisation of the Gillespie algorithm for non-Poissonian renewal processes, called the Laplacian Gillespie algorithm. Our algorithm is exact for any number of processes running in parallel and faster than the nMGA \\cite{BogunaLafuerza2014PhysRevE}. Although it is only applicable to the renewal processes whose survival function is completely monotone, it covers several renewal processes of interest. We also proposed a method to simulate non-renewal point processes with an arbitrary distribution of inter-event times and positive correlation between inter-event times on the same event sequence.\n\nWe applied the Laplace Gillespie algorithm to an epidemic model in well-mixed and networked populations. The applicability of the Laplace Gillespie algorithm, as well as that of the modified next reaction method \\cite{Anderson2007JChemPhys} and the nMGA, is much wider.\nThese algorithms can simulate systems of spiking neurons, earthquakes, financial time series, and so on. Inter-event times of these systems are often distributed according to distributions whose CV (i.e., coefficient of variation, i.e., standard deviation divided by the mean) is larger than unity \\cite{HolmeSaramaki2012PhysRep}, therefore not excluding the use of the Laplace Gillespie algorithm. It is also straightforward to allow births and deaths of nodes  \\cite{Bansal2010JBiolDyn,Rocha2013PlosComputBiol} and links \\cite{Holme2014SciRep} of contact networks, or rewiring of links \\cite{Volz2007ProcRSocB,GrossBlasius2008JRSocInterface}, if the occurrence of these events obeys renewal processes or our non-renewal processes with positive correlation. In more realistic scenarios, the recovery events do not have to obey the Poisson process, as assumed in our numerical simulations. The Laplace Gillespie algorithm has some limitations. Evidence suggests that empirical recovery times are less dispersed than the exponential distribution, implying the CV value less than unity. Therefore, the gamma distribution with scale parameter $\\alpha>1$ or even the delta distribution are often employed in numerical and theoretical analysis\n\\cite{Lloyd2001ProcRSocLondB\n\n,Wearing2005PlosMed,Heffernan2006TheorPopulBiol}. In contrast, the Laplace Gillespie algorithm is only applicable to distributions of inter-event times whose CV is larger than unity.\n\nPrevious studies aimed at Poissonian explanation of long-tailed distributions of inter-event times. Examples include a non-homogeneous Poisson process with switching between two event rates plus an additional periodic modulation of the rate \\cite{Malmgren2008PNAS} and the self-exciting Hawkes process with an exponential memory kernel \\cite{Masuda2013hawkes}. We showed that the power-law distribution of inter-event times, Eq.~\\eqref{eq:psi(tau) power law}, was produced when the rate of the Poisson process is drawn from the gamma distribution. This observation provides a theoretical underpinning of the fact that modulated Poisson processes \\cite{Malmgren2008PNAS,Masuda2013hawkes} may generate power-law distributed inter-event times. Although we theoretically need an infinite number of Poisson processes to fully produce a power-law distribution, a mixture of a small number of Poisson processes may be practically sufficient. In fact, a mixture of a small number of exponential distributions is sometimes employed to fit empirical distributions of inter-event times \\cite{Jewell1982AnnStat,Heckman1990JAmStatAssoc,Politi2007PhysicaA,JinGonigunta2010JStatComputSimul}.\n\n\n\n\\section*{Acknowledgments}\n\n\nN.M. acknowledges the support provided through JST, CREST, and JST, ERATO, Kawarabayashi Large Graph Project. L.E.C.R. is a Charg\\'{e} de recherche of the Fonds de la Recherche Scientifique - FNRS.\n\n\n\n\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem{Ogata1999PureApplGeophys}\nY~Ogata.\n\\newblock {Seismicity analysis through point-process modeling: A review}.\n\\newblock {\\em Pure Appl. Geophys.}, 155:471--507, 1999.\n\n\\bibitem{Vankampen2007book}\nN.~G. Van~Kampen.\n\\newblock {\\em Stochastic Processes in Physics and Chemistry}.\n\\newblock Elsevier, Netherlands, Amsterdam, 3rd edition, 2007.\n\n\\bibitem{Gabbiani2010book}\nF.~Gabbiani and S.~J. Cox.\n\\newblock {\\em Mathematics for Neuroscientists}.\n\\newblock Academic Press, Amsterdam, 2010.\n\n\\bibitem{Jacobsen2006book}\nM.~Jacobsen.\n\\newblock {\\em Point Processes Theory and Applications (second edition)}.\n\\newblock Birkh\\\"{a}user, Boston, MA, 2006.\n\n\\bibitem{Vestergaard2015PlosComputBiol}\nC.~L. Vestergaard and M.~G\\'{e}nois.\n\\newblock {Temporal Gillespie algorithm: Fast simulation of contagion processes\n  on time-varying networks}.\n\\newblock {\\em PLOS Comput. Biol.}, 11:e1004579, 2015.\n\n\\bibitem{Kendall1950JRStatSocSerB}\nD.~G. Kendall.\n\\newblock {An artificial realization of a simple ``birth-and-death'' process}.\n\\newblock {\\em J. R. Stat. Soc. Ser. B}, 12:116--119, 1950.\n\n\\bibitem{Gillespie1976JComputPhys}\nD.~T. Gillespie.\n\\newblock {A general method for numerically simulating the stochastic time\n  evolution of coupled chemical reactions}.\n\\newblock {\\em J. Comput. Phys.}, 22:403--434, 1976.\n\n\\bibitem{Gillespie1977JPhysChem}\nD.~T. Gillespie.\n\\newblock {Exact stochastic simulation of coupled chemical reactions}.\n\\newblock {\\em J. Phys. Chem.}, 81:2340--2361, 1977.\n\n\\bibitem{Eckmann2004PNAS}\nJ.~P. Eckmann, E.~Moses, and D.~Sergi.\n\\newblock {Entropy of dialogues creates coherent structures in e-mail traffic}.\n\\newblock {\\em Proc. Natl. Acad. Sci. USA}, 101:{14333}--{14337}, 2004.\n\n\\bibitem{Barabasi2005Nature}\nA.~L. Barab\\'{a}si.\n\\newblock {The origin of bursts and heavy tails in human dynamics}.\n\\newblock {\\em Nature}, 435:207--211, 2005.\n\n\\bibitem{VazquezA2006PhysRevE-burst}\nA.~V\\'{a}zquez, J.~G. Oliveira, Z.~Dezs\\\"{o}, K.~I. Goh, I.~Kondor, and A.~L.\n  Barab\\'{a}si.\n\\newblock {Modeling bursts and heavy tails in human dynamics}.\n\\newblock {\\em Phys. Rev. E}, 73:{036127}, 2006.\n\n\\bibitem{GohBarabasi2008EPL}\nK.~I. Goh and A.~L. Barab\\'{a}si.\n\\newblock {Burstiness and memory in complex systems}.\n\\newblock {\\em Europhys. Lett.}, 81:48002, 2008.\n\n\\bibitem{HolmeSaramaki2012PhysRep}\nP.~Holme and J.~Saram\\\"{a}ki.\n\\newblock {Temporal networks}.\n\\newblock {\\em Phys. Rep.}, 519:97--125, 2012.\n\n\\bibitem{Cox1962book}\nD.~R. Cox.\n\\newblock {\\em Renewal Theory}.\n\\newblock Methuen \\& Co. Ltd, Frome, UK, 1962.\n\n\\bibitem{GibsonBruck2000JPhysChemA}\nM.~A. Gibson and J.~Bruck.\n\\newblock {Efficient exact stochastic simulation of chemical systems with many\n  species and many channels}.\n\\newblock {\\em J. Phys. Chem. A}, 104:1876--1889, 2000.\n\n\\bibitem{Anderson2007JChemPhys}\nD.~F. Anderson.\n\\newblock {A modified next reaction method for simulating chemical systems with\n  time dependent propensities and delays}.\n\\newblock {\\em J. Chem. Phys.}, 127:214107, 2007.\n\n\\bibitem{LuVolfson2004SystBiol}\nT.~Lu, D.~Volfson, L.~Tsimring, and J.~Hasty.\n\\newblock {Cellular growth and division in the Gillespie algorithm}.\n\\newblock {\\em Syst. Biol.}, 1:121--128, 2004.\n\n\\bibitem{Carletti2012ComputMathMethodsMed}\nT.~Carletti and A.~Filisetti.\n\\newblock {The stochastic evolution of a protocell: The Gillespie algorithm in\n  a dynamically varying volume}.\n\\newblock {\\em Comput. Math. Methods Med.}, 2012:423627, 2012.\n\n\\bibitem{BogunaLafuerza2014PhysRevE}\nM.~Bogu\\~{n}\\'{a}, L.~F. Lafuerza, R.~Toral, and M.~\\'{A}. Serrano.\n\\newblock {Simulating non-Markovian stochastic processes}.\n\\newblock {\\em Phys. Rev. E}, 90:042108, 2014.\n\n\\bibitem{Karr1991book}\nA.~F. Karr.\n\\newblock {\\em Point Processes and their Statistical Inference (second\n  edition)}.\n\\newblock CRC Press, New York, NY, 1991.\n\n\\bibitem{Grandell1997book}\nJ.~Grandell.\n\\newblock {\\em Mixed Poisson Processes}.\n\\newblock Chapman \\& Hall, London, 1997.\n\n\\bibitem{Kingman1964MathprocCambPhilSoc}\nJ.~F.~C. Kingman.\n\\newblock {On doubly stochastic Poisson processes}.\n\\newblock {\\em Math. Proc. Camb. Phil. Soc.}, 60:923--930, 1964.\n\n\\bibitem{Fischer1992PerfEval}\nW.~Fischer and K.~Meier-Hellstern.\n\\newblock {The Markov-modulated Poisson process (MMPP) cookbook}.\n\\newblock {\\em Perf. Eval.}, 18:149--171, 1992.\n\n\\bibitem{Feller1971book2}\nW.~Feller.\n\\newblock {\\em An Introduction to Probability Theory and its Applications,\n  Volume II, Second Edition}.\n\\newblock John Wiley \\& Sons, 1971.\n\n\\bibitem{Kurthnelson2009PlosOne}\nZ.~Kurth-Nelson and A.~D. Redish.\n\\newblock Temporal-difference reinforcement learning with distributed\n  representations.\n\\newblock {\\em PLOS ONE}, 4:e7362, 2009.\n\n\\bibitem{Jewell1982AnnStat}\nN.~P. Jewell.\n\\newblock {Mixtures of exponential distributions}.\n\\newblock {\\em Ann. Stat.}, 10:479--484, 1982.\n\n\\bibitem{Yannaros1994AnnInstStatMath}\nN.~Yannaros.\n\\newblock {Weibull renewal processes}.\n\\newblock {\\em Ann. Inst. Stat. Math.}, 46:641--648, 1994.\n\n\\bibitem{JinGonigunta2010JStatComputSimul}\nT.~Jin and L.~S. Gonigunta.\n\\newblock {Exponential approximation to Weibull renewal with decreasing failure\n  rate}.\n\\newblock {\\em J. Stat. Comput. Simul.}, 80:273--285, 2010.\n\n\\bibitem{Yannaros1988JApplProb}\nN.~Yannaros.\n\\newblock {On Cox processes and gamma renewal processes}.\n\\newblock {\\em J. Appl. Prob.}, 25:423--427, 1988.\n\n\\bibitem{Gleser1989AmStat}\nL.~J. Gleser.\n\\newblock {The gamma distribution as a mixture of exponential distributions}.\n\\newblock {\\em Am. Stat.}, 43:115--117, 1989.\n\n\\bibitem{Gorenflo1997chapter}\nR.~Gorenflo and F.~Mainardi.\n\\newblock Fractional calculus: Integral and differential equations of\n  fractional order.\n\\newblock In A.~Carpinteri and F.~Mainardi, editors, {\\em Fractals and\n  Fractional Calculus in Continuum Mechanics}, pages 223--276. Springer,\n  Berlin, 1997.\n\n\\bibitem{Georgiou2015PhysRevE}\nN.~Georgiou, I.~Z. Kiss, and E.~Scalas.\n\\newblock Solvable non-markovian dynamic network.\n\\newblock {\\em Phys. Rev. E}, 92:042801, 2015.\n\n\\bibitem{WuZhou2010PNAS}\nY.~Wu, C.~Zhou, J.~Xiao, J.~Kurths, and H.~J. Schellnhuber.\n\\newblock {Evidence for a bimodal distribution in human communication}.\n\\newblock {\\em Proc. Natl. Acad. Sci. USA}, 107:{18803}--{18808}, 2010.\n\n\\bibitem{Masuda2013F1000}\nN.~Masuda and P.~Holme.\n\\newblock {Predicting and controlling infectious disease epidemics using\n  temporal networks}.\n\\newblock {\\em F1000Prime Reports}, 5:6, 2013.\n\n\\bibitem{Holme2015EurPhysJB}\nP.~Holme.\n\\newblock {Modern temporal network theory: A colloquium}.\n\\newblock {\\em Eur. Phys. J. B}, 88:234, 2015.\n\n\\bibitem{Abate1992QueueSyst}\nJ.~Abate and W.~Whitt.\n\\newblock {The Fourier-series method for inverting transforms of probability\n  distributions}.\n\\newblock {\\em Queue. Syst.}, 10:5--87, 1992.\n\n\\bibitem{Abate1995OrsaJComput}\nJ.~Abate and W.~Whitt.\n\\newblock {Numerical inversion of Laplace transforms of probability\n  distributions}.\n\\newblock {\\em ORSA J. Comput.}, 7:36--43, 1995.\n\n\\bibitem{Heckman1990JAmStatAssoc}\nJ.~J. Heckman, R.~Robb, and J.~R. Walker.\n\\newblock {Testing the mixture of exponentials hypothesis and estimating the\n  mixing distribution by the method of moments}.\n\\newblock {\\em J. Am. Stat. Assoc.}, 85:582--589, 1990.\n\n\\bibitem{Kleinberg2003DataMinKnowlDisc}\nJ.~Kleinberg.\n\\newblock {Bursty and hierarchical structure in streams}.\n\\newblock {\\em Data Min. Knowl. Disc.}, 7:373--397, 2003.\n\n\\bibitem{Politi2007PhysicaA}\nM.~Politi and E.~Scalas.\n\\newblock {Activity spectrum from waiting-time distribution}.\n\\newblock {\\em Physica A}, 383:43--48, 2007.\n\n\\bibitem{LiBarron2000NIPS}\nJ.~Q. Li and A.~R. Barron.\n\\newblock {Mixture density estimation}.\n\\newblock In S.~A. Solla, T.~K. Leen, and K.~R M\\\"{u}ller, editors, {\\em\n  Advances in Neural Information Processing Systems}, volume~12, pages\n  279--285, Cambridge, MA, 2000. MIT Press.\n\n\\bibitem{Marsaglia2000ACMTransMathSoft}\nG.~Marsaglia and W.~W. Tsang.\n\\newblock {A simple method for generating gamma variables}.\n\\newblock {\\em ACM Trans. Math. Software}, 26:363--372, 2000.\n\n\\bibitem{Marsaglia-code}\n{\\rm http://www.know-all.net/articles/view/56}.\n\n\\bibitem{Masuda2013hawkes}\nN.~Masuda, T.~Takaguchi, N.~Sato, and K.~Yano.\n\\newblock {Self-exciting point process modeling of conversation event\n  sequences}.\n\\newblock In P.~Holme and J.~Saram\\\"{a}ki, editors, {\\em Temporal Networks},\n  pages 245--264. Springer-Verlag, Berlin, 2013.\n\n\\bibitem{Karsai2011PhysRevE}\nM.~Karsai, M.~Kivel\\\"{a}, R.~K. Pan, K.~Kaski, J.~Kert\\'{e}sz, A.~L.\n  Barab\\'{a}si, and J.~Saram\\\"{a}ki.\n\\newblock {Small but slow world: How network topology and burstiness slow down\n  spreading}.\n\\newblock {\\em Phys. Rev. E}, 83:{025102(R)}, 2011.\n\n\\bibitem{Miritello2011PhysRevE}\nG.~Miritello, E.~Moro, and R.~Lara.\n\\newblock Dynamical strength of social ties in information spreading.\n\\newblock {\\em Phys. Rev. E}, 83:045102(R), 2011.\n\n\\bibitem{Rocha2011PlosComputBiol}\nL.~E.~C. Rocha, F.~Liljeros, and P.~Holme.\n\\newblock {Simulated epidemics in an empirical spatiotemporal network of 50,185\n  sexual contacts.}\n\\newblock {\\em PLOS Comput. Biol.}, 7:e1001109, 2011.\n\n\\bibitem{Schwalger2010EurPhysJSpecTopics}\nT.~Schwalger and B.~Lindner.\n\\newblock {Theory for serial correlations of interevent intervals}.\n\\newblock {\\em Eur. Phys. J. Spec. Topics}, 187:211--221, 2010.\n\n\\bibitem{Malmgren2008PNAS}\nR.~D. Malmgren, D.~B. Stouffer, A.~E. Motter, and L.~A.~N. Amaral.\n\\newblock {A Poissonian explanation for heavy tails in e-mail communication}.\n\\newblock {\\em Proc. Natl. Acad. Sci. USA}, 105:{18153}--{18158}, 2008.\n\n\\bibitem{Rocha2013PlosComputBiol}\nL.~E.~C. Rocha and V.~D. Blondel.\n\\newblock {Bursts of vertex activation and epidemics in evolving networks}.\n\\newblock {\\em PLOS Comput. Biol.}, 9:e1002974, 2013.\n\n\\bibitem{Vanmieghem2013PhysRevLett}\nP.~{Van Mieghem} and R.~{van de Bovenkamp}.\n\\newblock {Non-Markovian infection spread dramatically alters the\n  susceptible-infected-susceptible epidemic threshold in networks}.\n\\newblock {\\em Phys. Rev. Lett.}, 110:108701, 2013.\n\n\\bibitem{MinGohKim2013EPL}\nB.~Min, K.~I. Goh, and I.~M. Kim.\n\\newblock {Suppression of epidemic outbreaks with heavy-tailed contact\n  dynamics}.\n\\newblock {\\em EPL}, 103:50002, 2013.\n\n\\bibitem{Horvath2014NewJPhys}\nD.~X. Horv{\\'a}th and J.~Kert\\'{e}sz.\n\\newblock {Spreading dynamics on networks: The role of burstiness, topology and\n  non-stationarity}.\n\\newblock {\\em New J. Phys.}, 16:073037, 2014.\n\n\\bibitem{Perra2012SciRep}\nN.~Perra, B.~Gon\\c{c}alves, R.~Pastor-Satorras, and A.~Vespignani.\n\\newblock {Activity driven modeling of time varying networks.}\n\\newblock {\\em Sci. Rep.}, 2:469, 2012.\n\n\\bibitem{Bansal2010JBiolDyn}\nS.~Bansal, J.~Read, B.~Pourbohloul, and L.~A. Meyers.\n\\newblock {The dynamic nature of contact networks in infectious disease\n  epidemiology}.\n\\newblock {\\em J. Biol. Dyn.}, 4:478--489, 2010.\n\n\\bibitem{Holme2014SciRep}\nP.~Holme and F.~Liljeros.\n\\newblock {Birth and death of links control disease spreading in empirical\n  contact networks.}\n\\newblock {\\em Sci. Rep.}, 4:4999, 2014.\n\n\\bibitem{Volz2007ProcRSocB}\nE.~Volz and L.~A. Meyers.\n\\newblock {Susceptible-infected-recovered epidemics in dynamic contact\n  networks}.\n\\newblock {\\em Proc. R. Soc. B}, 274:2925--2933, 2007.\n\n\\bibitem{GrossBlasius2008JRSocInterface}\nT.~Gross and B.~Blasius.\n\\newblock {Adaptive coevolutionary networks: A review}.\n\\newblock {\\em J. R. Soc. Interface}, 5:259--271, 2008.\n\n\\bibitem{Lloyd2001ProcRSocLondB}\nA.~L. Lloyd.\n\\newblock {Destabilization of epidemic models with the inclusion of realistic\n  distributions of infectious periods}.\n\\newblock {\\em Proc. R. Soc. Lond. B}, 268:985--993, 2001.\n\n\\bibitem{Wearing2005PlosMed}\nH.~J. Wearing, P.~Rohani, and M.~J. Keeling.\n\\newblock {Appropriate models for the management of infectious diseases}.\n\\newblock {\\em PLOS Med.}, 2:e174, 2005.\n\n\\bibitem{Heffernan2006TheorPopulBiol}\nJ.~M. Heffernan and L.~M. Wahl.\n\\newblock {Improving estimates of the basic reproductive ratio: Using both the\n  mean and the dispersal of transition times}.\n\\newblock {\\em Theor. Popul. Biol.}, 70:135--145, 2006.\n\n\\end{thebibliography}\n\n\\newpage\n\\clearpage\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=8cm]{surv-nMGA-N10.eps}\n\\includegraphics[width=8cm]{surv-LGA-N10.eps}\n\\includegraphics[width=8cm]{surv-nMGA-N1000.eps}\n\\includegraphics[width=8cm]{surv-LGA-N1000.eps}\n\\includegraphics[width=8cm]{time-compare-gillespie.eps}\n\\caption{Comparison between the nMGA and the Laplace Gillespie algorithm. The distribution of inter-event times is power law (Eq.~\\eqref{eq:psi(tau) power law}) with $\\kappa=1$. Among the $N$ processes, $N/3$ processes age simulated with $\\alpha=1$, another $N/3$ processes with $\\alpha=1.5$, and the other $N/3$ processes with $\\alpha=2$. (a)--(d) Survival function of inter-event times for one process with $\\alpha=1$, another with $\\alpha=1.5$, and another with $\\alpha=2$, from the top to the bottom. (a) nMGA with $N=10$. (b) Laplace Gillespie algorithm with $N=10$. (c) nMGA with $N=10^3$. (d) Laplace Gillespie algorithm with $N=10^3$. (e) Computation time as a function of the number of processes, $N$.\n}\n\\label{fig:survival}\n\\end{center}\n\\end{figure}\n\n\\newpage\n\\clearpage\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=8cm]{corr-LGA-alpha1.eps}\n\\includegraphics[width=8cm]{corr-LGA-alpha2.eps}\n\\includegraphics[width=8cm]{surv-corr-alpha1.eps}\n\\includegraphics[width=8cm]{surv-corr-alpha2.eps}\n\\caption{Memory coefficient, $M$, as a function of $q$, for the Laplace Gillespie algorithm.\nWe used the power-law distribution of inter-event times given by Eq.~\\eqref{eq:psi(tau) power law} with $\\kappa=1$. (a) $\\alpha=1$. (b) $\\alpha=2$. The error bar represents the mean $\\pm$ standard deviation. (c) Survival function of a single event sequence (i.e., $N=1$) with $10^6$ events with $\\alpha=1$ and $q=0.1$, 0.5, and 0.9. (d) Same for $\\alpha=2$.\n}\n\\label{fig:correlated}\n\\end{center}\n\\end{figure}\n\n\\newpage\n\\clearpage\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=8cm]{sir-node-complete-LGA.eps}\n\\includegraphics[width=8cm]{sir-node-rrg-LGA.eps}\n\\caption{Final outbreak size for the SIR epidemic model. (a) Well-mixed population. (b) Regular random graph with degree of each node equal to five. We set $N=10^4$ nodes. For the power-law density of inter-event times, we use Eq.~\\eqref{eq:psi(tau) power law} with $\\kappa=1$ and $\\alpha=1.5$.}\n\\label{fig:epidemic}\n\\end{center}\n\\end{figure}\n\n\\newpage\n\\clearpage\n\n\\begin{table}\n\\begin{center}\n\\caption{Distributions of Inter-event times, $\\psi(\\tau)$, for which the Laplace Gillespie algorithm can be used. $H$ is the Heaviside function defined by $H(x) = 1$ ($x\\ge 0$) and $H(x)=0$ ($x<0$).}\n\\label{tab:examples}\n\\begin{tabular}{|c|c|c|c|c|}\\hline\ndistribution & $\\psi(\\tau)$ & $\\Psi(\\tau)$ & condition & $p(\\lambda)$ \\\\ \\hline\n\nexponential &  $\\lambda_0 e^{-\\lambda_0 \\tau}$ & $e^{-\\lambda_0 \\tau}$ &  & $\\delta(\\lambda-\\lambda_0)$ \\rule[0mm]{0mm}{8mm}\\\\ \n\npower-law & $\\frac{\\lambda^{\\alpha-1}e^{-\\lambda/\\kappa}}{\\Gamma(\\alpha)\\kappa^{\\alpha}}$ &\n$\\frac{\\kappa}{(1+\\kappa\\tau)^{\\alpha+1}}$ &  & $\\frac{1}{(1+\\kappa\\tau)^{\\alpha}}$ \\rule[0mm]{0mm}{8mm}\\\\ \n\npower-law with & \\multirow{2}{*}{$\\frac{e^{-\\lambda_0 \\tau}}{(1+\\kappa\\tau)^{\\alpha}}\\left(\\lambda_0+\\frac{\\kappa\\alpha}{1+\\kappa\\tau}\\right)$} & \\multirow{2}{*}{$\\frac{e^{-\\lambda_0 \\tau}}{(1+\\kappa\\tau)^{\\alpha}}$} & \\multirow{2}{*}{} & \\multirow{2}{*}{\n$\\frac{(\\lambda-\\lambda_0)^{\\alpha-1}e^{-(\\lambda-\\lambda_0)/\\kappa}H(\\lambda-\\lambda_0)}{\\Gamma(\\alpha)}\n$} \\rule[0mm]{0mm}{4mm}\\\\\n\nexponential tail &&&& \\rule[0mm]{0mm}{4mm}\\\\\n\nWeibull & $\\alpha\\mu^{\\alpha}\\tau^{\\alpha-1} e^{-(\\mu\\tau)^{\\alpha}}$ & $e^{-(\\mu\\tau)^{\\alpha}}$\n& $0<\\alpha\\le 1$ & complicated \\rule[0mm]{0mm}{8mm}\\\\\n\nGamma & $\\frac{\\tau^{\\alpha-1}e^{-\\tau/\\kappa}}{\\Gamma(\\alpha)\\kappa^{\\alpha}}$ & complicated & \n$0<\\alpha\\le 1$ & $\\frac{H(\\lambda-\\kappa^{-1})}{\\Gamma(\\alpha)\\Gamma(1-\\alpha)\\lambda(\\kappa\\lambda-1)^{\\alpha}}$ \\rule[0mm]{0mm}{8mm}\\\\\n\nMittag-Leffler & $\\approx \\frac{\\beta\\sin(\\beta\\pi)\\Gamma(\\beta)}{\\pi \\tau^{\\beta+1}}$ & $E_{\\beta}(-\\tau^{\\beta})$\n& $0<\\beta<1$ & $\\frac{1}{\\pi} \\frac{\\lambda^{\\beta-1}\\sin(\\beta\\pi)}{\\lambda^{2\\beta} + 2\\lambda^{\\beta} \\cos(\\beta\\pi) + 1}$ \\rule[0mm]{0mm}{8mm}\\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\n\n\n", "itemtype": "equation", "pos": 43374, "prevtext": "\nwhere $\\alpha>0$ and $\\tau_0>0$.\n\n\\item Coefficient of variation smaller than unity\n\nThe complete monotonicity implies that the coefficient of variation (CV), i.e., standard deviation divided by the mean, of $\\tau$ is larger than or equal to unity \\cite{Yannaros1994AnnInstStatMath}. This condition excludes the gamma and Weibull distributions with $\\alpha>1$. In practice, a CV value smaller than unity indicates that events occur more regularly than in the case of the Poisson process, which yields $\\text{CV} = 1$. Therefore, renewal processes producing relatively periodic event sequences are also excluded.\n\n\\item Higher-order conditions\n\nEven if ${\\rm d}\\psi(\\tau)/{\\rm d}\\tau \\le 0$ and the CV is large, the survival function of a common distribution may not be completely monotone. For example, the one-sided Cauchy distribution defined by $\\psi(\\tau) = 1/\\left[\\pi(\\tau^2 + 1)^2\\right]$ yields ${\\rm d}^2\\psi(\\tau)/{\\rm d}\\tau^2 = 4(3\\tau^2-1)/(\\tau^2+1)^3$, whose sign depends on the value of $\\tau$.\n\nEmpirical evidence of online correspondences of humans suggests bimodal $\\psi(\\tau)$ in the sense that, excluding very small $\\tau$, it obeys a power-law distribution for small $\\tau$ and an exponential distribution for large $\\tau$ \\cite{WuZhou2010PNAS}. Such a $\\psi(\\tau)$ monotonically decreases with $\\tau$, verifying ${\\rm d}\\psi(\\tau)/{\\rm d}\\tau \\le 0$. However, the sign of ${\\rm d}^2\\psi(\\tau)/{\\rm d}\\tau^2$ depends on the $\\tau$ value such that the corresponding survival function is not completely monotone.\n \n\\end{itemize}\n\n\\subsection{Empirical distributions of inter-event times}\n\nWe are often interested in informing population dynamics described as interacting stochastic point processes, such as epidemic processes, by empirical data of event sequences. The standard numerical method emulates the dynamics on top of empirical event sequences, i.e., using a given list of events with the event type and time stamp \\cite{HolmeSaramaki2012PhysRep,Masuda2013F1000,Holme2015EurPhysJB}. Another approach would be to estimate $\\psi(\\tau)$ from empirical data and use a variant of the Gillespie algorithm to simulate stochastic processes.\n\nThe nMGA \\cite{BogunaLafuerza2014PhysRevE} and the modified next reaction method \\cite{Anderson2007JChemPhys} allow arbitrary $\\psi(\\tau)$ including the empirical distribution. In contrast, the Laplace Gillespie algorithm requires the survival function, $\\Psi(\\tau)$, to be completely monotone. Under this condition, we may be able to compute the inverse Laplace transform to obtain $p(\\lambda)$ at a reasonable cost \\cite{Abate1992QueueSyst,Abate1995OrsaJComput}. Because an empirical distribution is likely not completely monotone, we propose two alternative methods for empirical data. The first method is to fit a completely monotone survival function of inter-event times, such as Eq.~\\eqref{eq:Psi(tau) power law}, to given data. The second method is to estimate a mixture of exponential distributions with different rates to approximate the empirical $\\psi(\\tau)$ or $\\Psi(\\tau)$. Likelihood or other cost-function methods are available for the estimation \\cite{Jewell1982AnnStat,Heckman1990JAmStatAssoc,Kleinberg2003DataMinKnowlDisc,Politi2007PhysicaA,JinGonigunta2010JStatComputSimul}. The approximation error is guaranteed to decay inversely proportionally to the number of constituent distributions \\cite{LiBarron2000NIPS}.\n\n\\subsection{Initial conditions}\n\nWhen we start running $N$ processes in parallel, we may initially draw the inter-event time for each process from $\\psi(\\tau)$. This initial condition defines the so-called ordinary renewal process \\cite{Cox1962book}. An alternative model, called the equilibrium renewal process, assumes that the process has started at time $t=-\\infty$ such that the first inter-event time for each process, drawn at $t=0$, uses the distribution of waiting times to the next event in the equilibrium, not $\\psi(\\tau)$ (i.e., distribution of inter-event times) \\cite{Cox1962book}. The probability density of the waiting time to the next event is given by Eq.~\\eqref{eq:psi^w(tau)}. To simulate the equilibrium renewal process, we start by drawing the rate of the Poisson process according to $p^{\\rm w}(\\lambda)$ given by Eq.~\\eqref{eq:p^w(lambda)}. Afterwards, we draw the rate according to $p(\\lambda)$.\n\n\\section{Numerical performance}\n\nIn this section, we compare the performance of the nMGA \\cite{BogunaLafuerza2014PhysRevE} and the Laplace Gillespie algorithm.\nWe use the power-law distribution of inter-event times given by Eq.~\\eqref{eq:psi(tau) power law}.\nBecause $\\kappa$ only controls the scale of the inter-event time, we set $\\kappa=1$ without loss of generality.\nTo generate gamma-distributed random variates, we use a popular algorithm based on the rejection method  \\cite{Marsaglia2000ACMTransMathSoft}. In fact, we adapt an open source code \\cite{Marsaglia-code} to our purposes. We generate $N/3$ processes by Eq.~\\eqref{eq:psi(tau) power law} with $\\alpha=1$, another $N/3$ processes with $\\alpha=1.5$, and the remaining processes with $\\alpha=2$. We continue the simulation until any of the $N$ processes generates $10^6$ inter-event times for the first time. We assume the ordinary renewal process such that the initial inter-event time for each process is drawn from $\\psi(\\tau)$.\n\nThe survival function for one process with $\\alpha=1$, one with $\\alpha=1.5$, and another with $\\alpha=2$\nis shown by the solid curves for the nMGA and the Laplace Gillespie algorithm in Figs.~\\ref{fig:survival}(a) and \\ref{fig:survival}(b), respectively, with $N=10$. The theoretical survival function, Eq.~\\eqref{eq:Psi(tau) power law}, is plotted by the dashed curves. The results obtained from the Laplace Gillespie algorithm (Fig.~\\ref{fig:survival}(b)) are more accurate than those obtained from the nMGA. This is because the nMGA is exact in the limit of $N\\to\\infty$, whereas the Laplace Gillespie algorithm is exact for any $N$. When $N=10^3$, the nMGA is sufficiently accurate (Fig.~\\ref{fig:survival}(c)), as is the Laplace Gillespie algorithm (Fig.~\\ref{fig:survival}(d)). The results shown in Figs.~\\ref{fig:survival}(a) and \\ref{fig:survival}(c) are consistent with the numerical results obtained in Ref.~\\cite{BogunaLafuerza2014PhysRevE}.\n\nThe nMGA may require a lot of time in calculating the instantaneous event rate \n($\\lambda_j(0)$ in Eq.~\\eqref{eq:average instantaneous rate for generalized Gillespie}) for all processes every time an event occurs in one of the $N$ processes. The Laplace Gillespie algorithm avoid the rate recalculation whereas it may be costly to calculate the gamma variate each time an event occurs. We numerically compare the computation time for the two algorithms by varying $N$. The other parameters are the same as those used in Fig.~\\ref{fig:survival}. We do not optimise the method to select the $i$ value with probability $\\Pi_i$ at the occurrence of each event; we use the simple linear search for both the nMGA and Laplace Gillespie algorithm. In the Laplace Gillespie algorithm, we use a binary tree data structure to store and update $\\lambda_i$ ($1\\le i\\le N$) to accelerate the algorithm. This structure is useful when only a small number of $\\lambda_i$ is changed upon each event. This is not the case for the nMGA.\nWe use codes written in C++, compiled with a standard g++ compiler without an optimisation option on a Mac Book Air with 1.7 GHz Intel Core i7 and 8Gz 1600 MHz DDR3. The computation time in secs plotted against $N$ in Fig.~\\ref{fig:survival}(e) indicates that the Laplace Gillespie algorithm is substantially faster than the nMGA as $N$ increases.\n\nThe Laplace Gillespie algorithm outperforms the nMGA in that the Laplace Gillespie algorithm is exact for any $N$ and generally runs faster than the nMGA. The price paid is that the form of $\\psi(\\tau)$ is limited, whereas the nMGA allows any $\\psi(\\tau)$.\n\n\\section{Applications}\n\n\\subsection{Positively correlated sequences of inter-event times\\label{sub:positive correlation}}\n\nWe have considered the renewal processes, i.e., stationary point processes without correlation between different inter-event times. However, inter-event times are positively correlated, albeit weakly, in a majority of data recorded from human activity \\cite{GohBarabasi2008EPL,Masuda2013hawkes}. This and other types of correlation in sequences of event times is known to affect upshots of epidemic processes \\cite{Karsai2011PhysRevE,Miritello2011PhysRevE,Rocha2011PlosComputBiol,Masuda2013F1000}.\n\nThe Laplace Gillespie algorithm provides a succinct method for generating point processes with positive correlation without changing $\\psi(\\tau)$. To generate such event sequences, we redraw a new event rate for the Poisson process, $\\lambda_i$, with probability $1-q$ ($0\\le q< 1$), when the $i$th process has generated an event. With probability $q$, we continue to use the same value of $\\lambda_i$ until the $i$th process generates the next event. The standard Laplace Gillespie algorithm is recovered with $q=0$. The correlation between inter-event times is expected to grow as $q$ increases. Although the same $\\lambda_i$ value may be used for generating consecutive inter-event times, the corresponding inter-event times are different because they are generated from the Poisson process. The computation time decreases as $q$ increases because the number of times one has to redraw $\\lambda_i$ is proportional to $1-q$.\n\nIn the continuous-time Markov process with a state-dependent hopping rate, the inter-event time defined as the time between two consecutive hops regardless of the state is generally correlated across inter-event times \\cite{Schwalger2010EurPhysJSpecTopics}. The present algorithm can be interpreted as a special case of this general framework such that the state is continuous, the process hops to the current state with probability $1-q$, and it hops to any other state with the probability proportional to $q\\times p(\\lambda)$. The correlated Laplace Gillespie algorithm can be alternatively built on top of a finite-state \\cite{Schwalger2010EurPhysJSpecTopics} or infinite-state \\cite{Kleinberg2003DataMinKnowlDisc} Markov process with a general transition probability between states. This variant of the model will be similar to a two-state cascading Poisson process that assumes transitions between a normal and excited states, with the excited state having a higher event rate than the normal state \\cite{Malmgren2008PNAS}.\n\nTwo remarks are in order. First, $\\psi(\\tau)$ is independent of the $p$ value. This is because the stationary density of the corresponding continuous-time Markov process in the $\\lambda$-space is equal to $p(\\lambda)$ irrespectively of the $q$ value. Second, this algorithm cannot be used to generate correlated event sequences when $\\psi(\\tau)$ is the exponential distribution. In this case, the event rate $\\lambda$ must be kept constant throughout the time and therefore cannot be modulated in a temporally correlated manner.\n\nWe measure the so-called memory coefficient \\cite{GohBarabasi2008EPL} to quantify the amount of correlation in a sequence of inter-event times. The memory coefficient for a sequence of inter-event times, $\\{\\tau_1, \\tau_2, \\ldots, \\tau_n\\}$, where $n$ is the number of inter-event times, is defined by\n\n", "index": 73, "text": "\\begin{equation}\nM = \\frac{\\sum_{i=1}^{n-1} (\\tau_i-m_1)(\\tau_{i+1}-m_2)}\n\n{\\sqrt{\\sum_{i=1}^{n-1} (\\tau_i-m_1)^2 \\sum_{i=2}^n (\\tau_{i+1}-m_2)^2}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E37.m1\" class=\"ltx_Math\" alttext=\"M=\\frac{\\sum_{i=1}^{n-1}(\\tau_{i}-m_{1})(\\tau_{i+1}-m_{2})}{\\par&#10;}{\\sqrt{\\sum_%&#10;{i=1}^{n-1}(\\tau_{i}-m_{1})^{2}\\sum_{i=2}^{n}(\\tau_{i+1}-m_{2})^{2}}},\" display=\"block\"><mrow><mrow><mi>M</mi><mo>=</mo><mrow><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c4</mi><mi>i</mi></msub><mo>-</mo><msub><mi>m</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c4</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi>m</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mi/></mfrac><mo>\u2062</mo><msqrt><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c4</mi><mi>i</mi></msub><mo>-</mo><msub><mi>m</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03c4</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi>m</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></msqrt></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]