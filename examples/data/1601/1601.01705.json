[{"file": "1601.01705.tex", "nexttext": "\n(This assumes that the root module of $z$ produces a distribution over labels $y$.)\nThe set of possible layouts $z$ is restricted by module \\emph{type constraints}:\nsome modules (like {\\texttt{\\smaller {find}}} above) operate directly on the input representation,\nwhile others (like {\\texttt{\\smaller {describe}}} above) also depend on input from specific earlier \nmodules. Two base types are considered in this paper are {\\underline{Attention}\\xspace} (a distribution over pixels \nor entities) and {\\underline{Labels}\\xspace} (a distribution over answers).\n\nParameters are tied across multiple instances of the same module, so different \ninstantiated networks may share some parameters but not others.\nModules have both \\emph{parameter arguments} (shown in square brackets) and ordinary \ninputs (shown in parentheses). Parameter arguments, like the running {\\texttt{\\smaller {bird}}} example\nin \\autoref{sec:programs}, are provided by the layout, and\nare used to specialize module behavior for particular lexical items. Ordinary inputs\nare the result of computation lower in the network. In addition to parameter-specific \nweights, modules have global weights shared across all instances of the module (but not\nshared with other modules). We write $A, a, B, b, \\dots$ for global weights and \n$u^i, v^i$ for weights associated with the parameter argument $i$. $\\oplus$ and $\\odot$ denote\n(possibly broadcasted) elementwise addition and multiplication respectively. The complete set of\nglobal weights and parameter-specific weights constitutes $\\theta_e$.\n\\emph{Every} module has access to the world representation, represented as a \ncollection of vectors $w^1, w^2, \\dots$ (or $W$ expressed as a matrix). The nonlinearity $\\sigma$ denotes a rectified \nlinear unit.\n\nThe modules used in this paper are shown below, with names\nand type constraints in the first row and a description of the module's computation following.\n\\vspace{1mm}\n\n\n\n\n\n\n\n{\\small\n\\setlength{\\belowdisplayskip}{5pt} \\setlength{\\belowdisplayshortskip}{5pt}\n\\setlength{\\abovedisplayskip}{5pt} \\setlength{\\abovedisplayshortskip}{5pt}\n\n\\tabulinesep=2mm\n\\noindent\n\\begin{tabu}{|p{0.95\\columnwidth}|}\n\t\\hline\n    \n    \\textbf{Lookup} \\hfill ($\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {lookup[$i$]}}} produces an attention focused entirely at the index $f(i)$,\n\twhere the relationship $f$ between words and positions in the input map is \n    known ahead of time (e.g. string matches on database fields).\n\t\n", "itemtype": "equation", "pos": 13201, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\n  We describe a question answering model that applies to both images and\n  structured knowledge bases.\n  The model uses natural language strings to\n  automatically assemble neural networks from a collection of\n  composable modules. Parameters for these modules are learned\n  jointly with network-assembly parameters via reinforcement \n  learning, with only (world, question, answer) triples as supervision. Our \n  approach, which we term a \\emph{dynamic neural module network}, \n  achieves state-of-the-art results on benchmark datasets in both \n  visual and structured domains.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nThis paper presents a compositional, attentional model for answering questions\nabout a variety of world representations, including images and structured\nknowledge bases. The model translates from questions to dynamically assembled\nneural networks, then applies these networks to world representations (images or\nknowledge bases) to produce answers. We take advantage of two largely independent\nlines of work: on  one hand, an extensive literature on answering questions by \nmapping from strings to logical representations  of meaning; on the other, \na series of recent successes in deep neural models for image recognition and \ncaptioning. By constructing neural networks instead of logical forms, our\nmodel leverages the best aspects of both linguistic compositionality and continuous \nrepresentations.\n\nOur model has two components, trained jointly: first, a collection of neural\n``modules'' that can be freely composed (\\autoref{fig:teaser}b); second, a network layout predictor\nthat assembles modules into complete deep networks tailored to each question (\\autoref{fig:teaser}a).\nPrevious work has used manually-specified modular structures for visual learning \n\\cite{Andreas15NMN}. Here we:\n\\begin{itemize}\n\t\\item \\emph{learn} a network structure predictor jointly with module \n    \t  parameters themselves \n    \\item \\emph{extend} visual primitives from previous work to reason over\n    \t  structured world representations\n\\end{itemize}\nTraining data consists of (world, question, answer) triples: our approach requires no \nsupervision of network layouts.\nWe achieve state-of-the-art performance on two markedly different question \nanswering tasks: one with questions about natural images, and another with more compositional questions about United States geography.\n\n\\begin{figure}\n  \\includegraphics[\n    width=\\columnwidth,\n    trim=0 5.5cm 5.5cm 0cm, \n    clip\n  ]{fig/schematic2.pdf}\n  \\caption{A learned syntactic analysis (a) is used to assemble a \t\n  collection of neural modules (b) into a deep neural network (c), \n  and applied to a world representation (d) to produce an answer.}\n  \n  \\label{fig:teaser}\n\\end{figure}\n\n\\section{Deep networks as functional programs}\n\\label{sec:programs}\n\n\n\n\n\n\n\n\n\n\n\n\nWe begin with a high-level discussion of the kinds of composed networks we would like to learn. \n\\pagebreak\n\n\\newcite{Andreas15NMN} describe a heuristic approach for\ndecomposing visual question answering tasks into sequence of modular sub-problems. For example,\nthe question \\emph{What color is the bird?}\\ might be answered in two steps: first, ``where \nis the bird?'' (\\autoref{fig:examples}a), second, ``what color is that part of the image?'' \n(\\autoref{fig:examples}c).\n\nThis first step, a generic \\textbf{module} called {\\texttt{\\smaller {find}}}, can be expressed as a fragment of a neural \nnetwork that maps from image features and a lexical item (here \\emph{bird}) to a distribution over pixels. \nThis operation is commonly referred to as the \\emph{attention mechanism}, and is a standard tool for \nmanipulating images \\cite{Xu15SAT} and text representations \\cite{Hermann15AttQA}\n\n\nThe first contribution of this paper is an extension and generalization of this mechanism \nto enable fully-differentiable reasoning \nabout more structured semantic representations. \n\\autoref{fig:examples}b shows how the same module can be used to focus on the entity \\emph{Georgia}\nin a non-visual grounding domain; more generally, by representing every entity in the\nuniverse of discourse as a feature vector, we can obtain a distribution \nover entities that corresponds roughly to a logical set-valued denotation.\n\nHaving obtained such a distribution, existing neural approaches use it to \nimmediately compute a weighted average of image features and project back into a labeling decision---a \n{\\texttt{\\smaller {describe}}} module\n(\\autoref{fig:examples}c). But the logical perspective suggests a number of novel modules that might\noperate on attentions: e.g.\\ combining them (by analogy to conjunction or disjunction) or inspecting \nthem directly without a return to feature space (by analogy to quantification, \n\\autoref{fig:examples}d). These modules are discussed in detail in \\autoref{sec:model}. \nUnlike their formal counterparts, they are differentiable end-to-end, facilitating their \nintegration into learned models. Building on previous work, we learn behavior\nfor a collection of heterogeneous modules from (world, question, answer) triples.\n\n\n\\begin{figure}\n  \\centering\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \\includegraphics[\n  \twidth=.9\\columnwidth,\n    trim=0.5cm 7.3cm 11.5cm 0.5cm,\n    clip\n  ]{fig/examples2}\n  \\caption{Simple neural module networks, corresponding to the questions\n  \\emph{What color is the bird?}\\ and \\emph{Are there any states?} \n  (a)~A neural {\\texttt{\\smaller {find}}} module for\n  computing an attention over pixels. (b) The same operation applied to a\n  knowledge base. (c)~Using an attention produced by a lower module to identify\n  the color of the region of the image attended to. (d) Performing quantification\n  by evaluating an attention directly.}\n  \\label{fig:examples}\n  \\vspace{-3mm}\n\\end{figure}\n\nThe second contribution of this paper is a model for learning to assemble such modules\ncompositionally. Isolated modules are of limited use---to obtain expressive power comparable\nto either formal approaches or monolithic deep networks, they must be composed into larger \nstructures. \\autoref{fig:examples} shows simple examples of composed structures, but for \nrealistic question-answering tasks, even larger networks are required.\nThus our goal is to automatically induce variable-free, tree-structured computation descriptors. \nWe can use a familiar functional notation from formal semantics (e.g. Liang et al., 2011) to \nrepresent these computations.\\footnote{But note that unlike formal semantics, the behavior of the primitive \nfunctions here is itself unknown.} We write the two examples in \\autoref{fig:examples} as\n\n\n\n\n{\\smaller\n\\begin{verbatim}\n    (describe[color] find[bird])\n\\end{verbatim}\n}\n\n\\noindent and\n\n{\\smaller \n\\begin{verbatim}\n    (exists find[state])\n\\end{verbatim}\n}\n\n\n\n\\noindent respectively. These are \\textbf{network layouts}: they specify a structure for arranging modules (and their lexical parameters) into a complete network. \\newcite{Andreas15NMN} use hand-written \nrules to deterministically  transform dependency trees into layouts, and restricted to producing\nsimple structures like the above for non-synthetic data. For full generality, \nwe will need to solve harder problems, like transforming \\emph{What cities are in Georgia?}\n(\\autoref{fig:teaser}) into\n\n{\\smaller \n\\begin{verbatim}\n    (and\n        find[city]\n        (relate[in] lookup[Georgia]))\n\\end{verbatim}\n}\n\\noindent In this paper, we present a model for learning to select such structures from a set of automatically\ngenerated candidates. We call this model a \\textbf{dynamic neural module network}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Related work}\n\\label{sec:related}\n\nThere is an extensive literature on database question answering, in which\nstrings are mapped to logical forms, then evaluated by a black-box execution \nmodel to produce answers. Supervision may be provided either by annotated logical forms\n\\cite{Wong07WASP,Kwiatkowski10UBL,Andreas13SPMT} or from (world,\nquestion, answer) triples alone \\cite{Liang11DCS,Pasupat15Tables}. In general the\nset of primitive functions from which these logical forms can be assembled is\nfixed, but one recent line of work focuses on inducing new predicates functions\nautomatically, either from perceptual features \\cite{Krish2013Grounded} or \nthe underlying schema \\cite{Kwiatkowski13Ontology}. The model we describe in this paper\nhas a unified framework for handling both the perceptual and schema cases, and\ndiffers from existing work primarily in learning a differentiable\nexecution model with continuous evaluation results.\n\nNeural models for question answering are also a subject of current interest.\nThese include approaches that model the task directly as a multiclass classification \nproblem  \\cite{Iyyer14Factoid}, models that attempt to embed questions and answers\nin a shared vector space \\cite{Bordes14GraphEmbedding} and attentional models that select words from\ndocuments sources \\cite{Hermann15AttQA}. Such approaches generally require\nthat answers can be retrieved directly based on surface linguistic features, \nwithout requiring intermediate computation. A more structured approach\ndescribed by \\newcite{Yin15NeuralTable} learns a query execution model\nfor database tables without any natural language component. Previous efforts\ntoward unifying formal logic and representation learning include those of \n\\newcite{Grefenstette13Logic} and \\newcite{Krishnamurthy13CompVector}.\n\nThe visually-grounded component of this work relies on recent advances\nin convolutional networks for computer vision\n\\cite{Simonyan14VGG}, and in particular the fact that late\nconvolutional layers in networks trained for image recognition contain rich features \nuseful for other downstream vision tasks, while preserving spatial information.\nThese features have been used for both image captioning \\cite{Xu15SAT} and visual question\nanswering \\cite{Yang15AttVQA}.\n\n\nMost previous approaches to visual question answering either apply a recurrent\nmodel to deep representations of both the image and the question\n\\cite{Ren15VQA,Malinowski15VQA}, or use the question to compute an attention\nover the input image, and then answer based on both the question and the image\nfeatures attended to \\cite{Yang15AttVQA,Xu15AttVQA}.  Other approaches include\nthe simple classification model described by \\newcite{Zhou15ClassVQA} and the\ndynamic parameter prediction network described by \\newcite{Noh15DPPVQA}.  All of\nthese models assume that a fixed computation can be performed on the image and\nquestion to compute the answer, rather than adapting the structure of the\ncomputation to the question. \n\n\n\n\nAs noted,\n\\newcite{Andreas15NMN} previously considered a simple generalization of these\nattentional approaches in  which small variations in the network structure \nper-question were permitted, with the structure chosen by (deterministic)\nsyntactic processing of questions. Other approaches in this \ngeneral family include the ``universal parser'' sketched by \\newcite{Bottou14Reasoning}, \nand the recursive neural networks of \\newcite{Socher13CVG}, which use a fixed tree structure \nto perform further linguistic analysis without any external world representation.\nWe are unaware of previous work that succeeds in simultaneously learning both the \nparameters for and structures of instance-specific neural networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Model}\n\\label{sec:model}\n\nRecall that our goal is to map from questions and world representations to answers. \nThis process involves the following variables:\n\\begin{enumerate}\n  \\item $w$ a world representation\n  \\item $x$ a question\n  \\item $y$ an answer\n  \\item $z$ a network layout\n  \\item $\\theta$ a collection of model parameters\n\\end{enumerate}\nOur model is built around two distributions:\na \\textbf{layout model} $p(z|x;{\\theta_\\ell})$ which chooses a layout for a sentence,\nand a \\textbf{execution model} $p_z(y|w;\\theta_e)$ which applies the network \nspecified by $z$ to $w$.\n\n\n\n\n\n\nFor ease of\npresentation, we introduce these models in reverse order. We first imagine that $z$ is always observed, and in \\autoref{sec:model:modules} describe how to \nevaluate and learn modules parameterized by $\\theta_e$ within fixed structures. In \\autoref{sec:model:assemblingNetworks}, we move to the real scenario, where $z$ is unknown. We describe how to predict layouts \nfrom questions and learn $\\theta_e$ and ${\\theta_\\ell}$ jointly without layout supervision.\n\n\n\n\\subsection{Evaluating modules}\n\\label{sec:model:modules}\n\n\nGiven a layout $z$, we assemble the corresponding modules into a full neural network\n(\\autoref{fig:teaser}c), and apply it to the knowledge representation. \nIntermediate results flow between modules until an answer is\nproduced at the root. We denote the output of the network with layout $z$ on\ninput world $w$ as ${\\llbracket {z} \\rrbracket}_w$; when explicitly referencing the substructure\nof $z$, we can alternatively write ${\\llbracket {m(h^1, h^2)} \\rrbracket}$ for a top-level module $m$\nwith submodule outputs $h^1$ and $h^2$.\nWe then define the execution model:\n\n", "index": 1, "text": "\\begin{equation}\n  p_z(y|w) = ({\\llbracket {z} \\rrbracket}_w)_y\n  \\label{eq:simple-execution}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"p_{z}(y|w)=({\\llbracket{z}\\rrbracket}_{w})_{y}\" display=\"block\"><mrow><msub><mi>p</mi><mi>z</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mrow><mo stretchy=\"false\">(</mo><msub><mrow><mo fence=\"true\">\u27e6</mo><mi>z</mi><mo fence=\"true\">\u27e7</mo></mrow><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>y</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\n\twhere $e_i$ is the basis vector that is $1$ in the $i$th position and 0 elsewhere. \\\\\n    \n    \\hline\n    \n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\\textbf{Find} \\hfill ($\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {find[$i$]}}} computes a distribution over indices by concatenating the parameter \n\targument with each  position of the input feature map, and passing the concatenated \n\tvector through a MLP:\n\t\n", "itemtype": "equation", "pos": 15799, "prevtext": "\n(This assumes that the root module of $z$ produces a distribution over labels $y$.)\nThe set of possible layouts $z$ is restricted by module \\emph{type constraints}:\nsome modules (like {\\texttt{\\smaller {find}}} above) operate directly on the input representation,\nwhile others (like {\\texttt{\\smaller {describe}}} above) also depend on input from specific earlier \nmodules. Two base types are considered in this paper are {\\underline{Attention}\\xspace} (a distribution over pixels \nor entities) and {\\underline{Labels}\\xspace} (a distribution over answers).\n\nParameters are tied across multiple instances of the same module, so different \ninstantiated networks may share some parameters but not others.\nModules have both \\emph{parameter arguments} (shown in square brackets) and ordinary \ninputs (shown in parentheses). Parameter arguments, like the running {\\texttt{\\smaller {bird}}} example\nin \\autoref{sec:programs}, are provided by the layout, and\nare used to specialize module behavior for particular lexical items. Ordinary inputs\nare the result of computation lower in the network. In addition to parameter-specific \nweights, modules have global weights shared across all instances of the module (but not\nshared with other modules). We write $A, a, B, b, \\dots$ for global weights and \n$u^i, v^i$ for weights associated with the parameter argument $i$. $\\oplus$ and $\\odot$ denote\n(possibly broadcasted) elementwise addition and multiplication respectively. The complete set of\nglobal weights and parameter-specific weights constitutes $\\theta_e$.\n\\emph{Every} module has access to the world representation, represented as a \ncollection of vectors $w^1, w^2, \\dots$ (or $W$ expressed as a matrix). The nonlinearity $\\sigma$ denotes a rectified \nlinear unit.\n\nThe modules used in this paper are shown below, with names\nand type constraints in the first row and a description of the module's computation following.\n\\vspace{1mm}\n\n\n\n\n\n\n\n{\\small\n\\setlength{\\belowdisplayskip}{5pt} \\setlength{\\belowdisplayshortskip}{5pt}\n\\setlength{\\abovedisplayskip}{5pt} \\setlength{\\abovedisplayshortskip}{5pt}\n\n\\tabulinesep=2mm\n\\noindent\n\\begin{tabu}{|p{0.95\\columnwidth}|}\n\t\\hline\n    \n    \\textbf{Lookup} \\hfill ($\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {lookup[$i$]}}} produces an attention focused entirely at the index $f(i)$,\n\twhere the relationship $f$ between words and positions in the input map is \n    known ahead of time (e.g. string matches on database fields).\n\t\n", "index": 3, "text": "\\begin{equation}\n\t\t{\\llbracket {{\\texttt{\\smaller {lookup[$i$]}}}} \\rrbracket} = e_{f(i)}\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\llbracket{{\\texttt{{lookup[$i$]}}}}\\rrbracket}=e_{f(i)}\" display=\"block\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mrow><mtext mathsize=\"83%\" mathvariant=\"monospace\">lookup[</mtext><mi mathsize=\"83%\">i</mi><mtext mathsize=\"83%\" mathvariant=\"monospace\">]</mtext></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><msub><mi>e</mi><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": " \\\\\n    \n    \\hline\n    \n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\\textbf{Relate} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {relate}}} directs focus from one region of the input to another. It behaves much\n    like the {\\texttt{\\smaller {find}}} module, but also conditions its behavior on the current region\n    of attention $h$. Let \n         $\\bar{w}(h) = \\sum_k h_k w^k$, where $h_k$ is the $k^{th}$ element of $h$. Then,\n         {\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\twhere $e_i$ is the basis vector that is $1$ in the $i$th position and 0 elsewhere. \\\\\n    \n    \\hline\n    \n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\\textbf{Find} \\hfill ($\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {find[$i$]}}} computes a distribution over indices by concatenating the parameter \n\targument with each  position of the input feature map, and passing the concatenated \n\tvector through a MLP:\n\t\n", "index": 5, "text": "\\begin{equation}\n\t\t{\\llbracket {{\\texttt{\\smaller {find[$i$]}}}} \\rrbracket} = \\textrm{softmax}(a \\odot \\sigma(B v^i \\oplus C W \\oplus d))\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\llbracket{{\\texttt{{find[$i$]}}}}\\rrbracket}=\\textrm{softmax}(a\\odot\\sigma(%&#10;Bv^{i}\\oplus CW\\oplus d))\" display=\"block\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mrow><mtext mathsize=\"83%\" mathvariant=\"monospace\">find[</mtext><mi mathsize=\"83%\">i</mi><mtext mathsize=\"83%\" mathvariant=\"monospace\">]</mtext></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>\u2299</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><msup><mi>v</mi><mi>i</mi></msup><mo>\u2295</mo><mi>C</mi><mi>W</mi><mo>\u2295</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "}\n         \\\\\n    \\hline\n    \n    \n\t\\textbf{And} \\hfill ({\\underline{Attention}\\xspace}{}* $\\to$ {\\underline{Attention}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{0pt}\n    {\\texttt{\\smaller {and}}} performs an operation analogous to set intersection\n    for attentions. The analogy to probabilistic logic suggests multiplying probabilities:\n    \n", "itemtype": "equation", "pos": -1, "prevtext": " \\\\\n    \n    \\hline\n    \n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\\textbf{Relate} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Attention}\\xspace}) \\newline\n\t{\\texttt{\\smaller {relate}}} directs focus from one region of the input to another. It behaves much\n    like the {\\texttt{\\smaller {find}}} module, but also conditions its behavior on the current region\n    of attention $h$. Let \n         $\\bar{w}(h) = \\sum_k h_k w^k$, where $h_k$ is the $k^{th}$ element of $h$. Then,\n         {\n", "index": 7, "text": "\\begin{align}\n            & {\\llbracket {{\\texttt{\\smaller {relate[$i$]}}}(h)} \\rrbracket} = \\textrm{softmax}(a \\ \\odot \\nonumber \\\\ \n            &\\qquad\\sigma(B v^i \\oplus C W \\oplus D\\bar{w}(h) \\oplus e))\n         \\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\llbracket{{\\texttt{{relate[$i$]}}}(h)}\\rrbracket}=\\textrm{%&#10;softmax}(a\\ \\odot\" display=\"inline\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mrow><mtext mathsize=\"83%\" mathvariant=\"monospace\">relate[</mtext><mi mathsize=\"83%\">i</mi><mtext mathsize=\"83%\" mathvariant=\"monospace\">]</mtext></mrow><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+5pt\"><mi>a</mi></mpadded><mo>\u2299</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad\\sigma(Bv^{i}\\oplus CW\\oplus D\\bar{w}(h)\\oplus e))\" display=\"inline\"><mrow><mi>\u2003\u2003</mi><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><msup><mi>v</mi><mi>i</mi></msup><mo>\u2295</mo><mi>C</mi><mi>W</mi><mo>\u2295</mo><mi>D</mi><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2295</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\\\\\n    \n    \\hline\n    \n\t\\textbf{Describe} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Labels}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{-5pt}\n\t{\\texttt{\\smaller {describe[$i$]}}} computes a weighted average of $w$ under the input attention. This average is then used to predict an answer representation. With $\\bar{w}$ as above,\n\t\n", "itemtype": "equation", "pos": -1, "prevtext": "}\n         \\\\\n    \\hline\n    \n    \n\t\\textbf{And} \\hfill ({\\underline{Attention}\\xspace}{}* $\\to$ {\\underline{Attention}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{0pt}\n    {\\texttt{\\smaller {and}}} performs an operation analogous to set intersection\n    for attentions. The analogy to probabilistic logic suggests multiplying probabilities:\n    \n", "index": 9, "text": "\\begin{equation}\n    {\\llbracket {{\\texttt{\\smaller {and}}}(h^1, h^2, \\ldots)} \\rrbracket} = h^1 \\odot h^2 \\odot \\cdots\n    \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\llbracket{{\\texttt{{and}}}(h^{1},h^{2},\\ldots)}\\rrbracket}=h^{1}\\odot h^{2}\\odot\\cdots\" display=\"block\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mtext mathsize=\"83%\">\ud835\ude8a\ud835\ude97\ud835\ude8d</mtext><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mn>1</mn></msup><mo>,</mo><msup><mi>h</mi><mn>2</mn></msup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo stretchy=\"false\">)</mo></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><msup><mi>h</mi><mn>1</mn></msup><mo>\u2299</mo><msup><mi>h</mi><mn>2</mn></msup><mo>\u2299</mo><mi mathvariant=\"normal\">\u22ef</mi></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": " \\\\\n    \n    \\hline\n    \n\t\\textbf{Exists} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Labels}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\n    {\\texttt{\\smaller {exists}}} is the existential quantifiers, and inspects the incoming\n\tattention directly to produce a label, rather than producing an intermediate feature\n\tvector like {\\texttt{\\smaller {describe}}}:\n\t\n", "itemtype": "equation", "pos": -1, "prevtext": "\\\\\n    \n    \\hline\n    \n\t\\textbf{Describe} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Labels}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{-5pt}\n\t{\\texttt{\\smaller {describe[$i$]}}} computes a weighted average of $w$ under the input attention. This average is then used to predict an answer representation. With $\\bar{w}$ as above,\n\t\n", "index": 11, "text": "\\begin{equation}\n\t    {\\llbracket {{\\texttt{\\smaller {describe[$i$]}}}(h)} \\rrbracket} = \\textrm{softmax}(A \\sigma(B \\bar{w}(h) + v^i))\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\llbracket{{\\texttt{{describe[$i$]}}}(h)}\\rrbracket}=\\textrm{softmax}(A\\sigma%&#10;(B\\bar{w}(h)+v^{i}))\" display=\"block\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mrow><mtext mathsize=\"83%\" mathvariant=\"monospace\">describe[</mtext><mi mathsize=\"83%\">i</mi><mtext mathsize=\"83%\" mathvariant=\"monospace\">]</mtext></mrow><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><mover accent=\"true\"><mi>w</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><msup><mi>v</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": " \\\\\n    \\hline\n\\end{tabu}\n}\n\n\n\n\n\n\nWith $z$ observed, the model we have described so far corresponds largely\nto that of \\newcite{Andreas15NMN}, though the module inventory is different---in\nparticular, our new {\\texttt{\\smaller {exists}}} and {\\texttt{\\smaller {relate}}} modules do not depend on the\ntwo-dimensional spatial structure of the input. This enables generalization to non-visual \nworld representations.\n\n\n\n\nLearning in this simplified setting is straightforward. Assuming the top-level \nmodule in each layout is a {\\texttt{\\smaller {describe}}} or {\\texttt{\\smaller {exists}}} module, the fully-\ninstantiated network corresponds to a distribution over labels conditioned on\nlayouts. To train, we maximize\n\n    \n    \n$\n    \\sum_{(w,y,z)} \\log p_z(y|w;\\theta_e)\n$\n\ndirectly.\nThis can  be understood as a \nparameter-tying scheme, where the decisions about which \nparameters to tie are governed by the observed layouts $z$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Assembling networks}\n\\label{sec:model:assemblingNetworks}\n\\begin{figure}\n  \\centering\n  \\vspace{-2mm}\n  \\includegraphics[\n    width=0.8\\columnwidth,\n    trim=0.5cm 0.5cm 11.5cm 0.5cm,\n    clip\n  ]{fig/layouts.pdf}\n  \\caption{Generation of layout candidates. The input sentence (a) is represented\n  as a dependency parse (b). Fragments of this dependency parse are then \t\n  associated with appropriate modules (c), and these fragments are assembled \n  into full layouts (d).}\n  \\label{fig:layout}\n  \\vspace{-3mm}\n\\end{figure}\n\nNext we describe the layout model $p(z|x;{\\theta_\\ell})$.\nWe first use a fixed syntactic parse to generate a small set of candidate\nlayouts, analogously to the way a semantic grammar \ngenerates candidate semantic parses in previous work \\cite{Berant14Paraphrasing}. \n\nA semantic parse differs from a syntactic parse in two primary ways.\nFirst, lexical items must be mapped onto a (possibly smaller) set of \nsemantic primitives. Second, these semantic primitives must be combined into a \nstructure that closely, but not exactly, parallels the structure provided by syntax.\nFor example, \\emph{state} and \\emph{province} might need to be identified with the\nsame field in a database schema, while \\emph{all states have a capital} might need\nto be identified with the correct (\\emph{in situ}) quantifier scope. \n\n\n\n\n\nWhile we cannot avoid the structure selection problem, continuous representations\nsimplify the lexical selection problem. For modules that accept a vector parameter,\nwe associate these parameters with \\emph{words} rather than semantic tokens,\n and thus turn the combinatorial optimization problem associated with lexicon induction \ninto a continuous one. Now, in order to learn that \\emph{province} and \\emph{state} \nhave the same denotation, it is sufficient to learn that their associated parameters \nare close in some embedding space---a task amenable to gradient descent.\n(Note that this is easy only in an optimizability sense, and not an information-theoretic\none---we must still learn to associate each independent lexical item with the correct\nvector.) The remaining combinatorial problem is to arrange the provided lexical items \ninto the right computational structure. In this respect, layout prediction is \nmore like syntactic parsing  than ordinary semantic parsing, and we can rely on an \noff-the-shelf syntactic parser to get most of the way there. In this work, syntactic\nstructure is provided by the Stanford dependency parser \\cite{DeMarneffe08Deps}.\n\nThe construction of layout candidates is depicted in \\autoref{fig:layout}. We assume queries are \nconjunctive at the top level, and collect the set of attributes and prepositional \nrelations that depend on the wh-word or copula in the question. The parser is free to \nconsider subsets of this \n\nconjunction, and optionally to insert an \nexistential quantifier. These are strong simplifying \nassumptions, but appear sufficient to cover most of the examples that \nappear in both of our tasks. As our approach includes both categories, relations\nand simple quantification, the range of phenomena considered is generally broader than\nprevious perceptually-grounded QA work \\cite{Krish2013Grounded,Matuszek12Grounded}.\n\nHaving generated a set of candidate parses, we need to score them. This is a\nreranking problem; as in the rest of our approach, we solve it using standard\nneural machinery. In particular, we produce an LSTM representation of the question, a \nfeature-based representation of the query (with indicators on the type and number of \nmodules used), and pass both representations through a multilayer perceptron (MLP). \nWhile one can easily imagine a more sophisticated parse-scoring model, this simple approach works well for our tasks.\n\nFormally, for a question $x$, let $h_q(x)$ be an LSTM encoding of the question \n(i.e. the last hidden layer of an LSTM applied word-by-word to the input question). \nLet $\\{z_1, z_2, \\ldots\\}$ be the proposed layouts for $x$, and let\n$f(z_i)$ be a feature vector representing the $i$th layout. Then the score\n$s(z_i|x)$ for the layout $z_i$ is\n\n", "itemtype": "equation", "pos": -1, "prevtext": " \\\\\n    \n    \\hline\n    \n\t\\textbf{Exists} \\hfill ({\\underline{Attention}\\xspace} $\\to$ {\\underline{Labels}\\xspace}) \\newline\n    \\setlength{\\belowdisplayskip}{-5pt}\n\t\n    {\\texttt{\\smaller {exists}}} is the existential quantifiers, and inspects the incoming\n\tattention directly to produce a label, rather than producing an intermediate feature\n\tvector like {\\texttt{\\smaller {describe}}}:\n\t\n", "index": 13, "text": "\\begin{equation}\n\t\t{\\llbracket {{\\texttt{\\smaller {exists]}}}(h)} \\rrbracket} = \\textrm{softmax}\\Big(\\big(\\max_k h_k\\big)a  + b\\Big)\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\llbracket{{\\texttt{{exists]}}}(h)}\\rrbracket}=\\textrm{softmax}\\Big{(}\\big{(}%&#10;\\max_{k}h_{k}\\big{)}a+b\\Big{)}\" display=\"block\"><mrow><mrow><mo fence=\"true\">\u27e6</mo><mtext mathsize=\"83%\" mathvariant=\"monospace\">exists]</mtext><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><mo fence=\"true\">\u27e7</mo></mrow><mo>=</mo><mtext>softmax</mtext><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><munder><mi>max</mi><mi>k</mi></munder><msub><mi>h</mi><mi>k</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\ni.e.\\ a the output of an MLP with inputs $h_q(x)$ and $f(z_i)$, and parameters\n${\\theta_\\ell} = \\{a, B, C, d\\}$. Finally, we normalize these scores to obtain a\ndistribution:\n\n", "itemtype": "equation", "pos": 23858, "prevtext": " \\\\\n    \\hline\n\\end{tabu}\n}\n\n\n\n\n\n\nWith $z$ observed, the model we have described so far corresponds largely\nto that of \\newcite{Andreas15NMN}, though the module inventory is different---in\nparticular, our new {\\texttt{\\smaller {exists}}} and {\\texttt{\\smaller {relate}}} modules do not depend on the\ntwo-dimensional spatial structure of the input. This enables generalization to non-visual \nworld representations.\n\n\n\n\nLearning in this simplified setting is straightforward. Assuming the top-level \nmodule in each layout is a {\\texttt{\\smaller {describe}}} or {\\texttt{\\smaller {exists}}} module, the fully-\ninstantiated network corresponds to a distribution over labels conditioned on\nlayouts. To train, we maximize\n\n    \n    \n$\n    \\sum_{(w,y,z)} \\log p_z(y|w;\\theta_e)\n$\n\ndirectly.\nThis can  be understood as a \nparameter-tying scheme, where the decisions about which \nparameters to tie are governed by the observed layouts $z$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Assembling networks}\n\\label{sec:model:assemblingNetworks}\n\\begin{figure}\n  \\centering\n  \\vspace{-2mm}\n  \\includegraphics[\n    width=0.8\\columnwidth,\n    trim=0.5cm 0.5cm 11.5cm 0.5cm,\n    clip\n  ]{fig/layouts.pdf}\n  \\caption{Generation of layout candidates. The input sentence (a) is represented\n  as a dependency parse (b). Fragments of this dependency parse are then \t\n  associated with appropriate modules (c), and these fragments are assembled \n  into full layouts (d).}\n  \\label{fig:layout}\n  \\vspace{-3mm}\n\\end{figure}\n\nNext we describe the layout model $p(z|x;{\\theta_\\ell})$.\nWe first use a fixed syntactic parse to generate a small set of candidate\nlayouts, analogously to the way a semantic grammar \ngenerates candidate semantic parses in previous work \\cite{Berant14Paraphrasing}. \n\nA semantic parse differs from a syntactic parse in two primary ways.\nFirst, lexical items must be mapped onto a (possibly smaller) set of \nsemantic primitives. Second, these semantic primitives must be combined into a \nstructure that closely, but not exactly, parallels the structure provided by syntax.\nFor example, \\emph{state} and \\emph{province} might need to be identified with the\nsame field in a database schema, while \\emph{all states have a capital} might need\nto be identified with the correct (\\emph{in situ}) quantifier scope. \n\n\n\n\n\nWhile we cannot avoid the structure selection problem, continuous representations\nsimplify the lexical selection problem. For modules that accept a vector parameter,\nwe associate these parameters with \\emph{words} rather than semantic tokens,\n and thus turn the combinatorial optimization problem associated with lexicon induction \ninto a continuous one. Now, in order to learn that \\emph{province} and \\emph{state} \nhave the same denotation, it is sufficient to learn that their associated parameters \nare close in some embedding space---a task amenable to gradient descent.\n(Note that this is easy only in an optimizability sense, and not an information-theoretic\none---we must still learn to associate each independent lexical item with the correct\nvector.) The remaining combinatorial problem is to arrange the provided lexical items \ninto the right computational structure. In this respect, layout prediction is \nmore like syntactic parsing  than ordinary semantic parsing, and we can rely on an \noff-the-shelf syntactic parser to get most of the way there. In this work, syntactic\nstructure is provided by the Stanford dependency parser \\cite{DeMarneffe08Deps}.\n\nThe construction of layout candidates is depicted in \\autoref{fig:layout}. We assume queries are \nconjunctive at the top level, and collect the set of attributes and prepositional \nrelations that depend on the wh-word or copula in the question. The parser is free to \nconsider subsets of this \n\nconjunction, and optionally to insert an \nexistential quantifier. These are strong simplifying \nassumptions, but appear sufficient to cover most of the examples that \nappear in both of our tasks. As our approach includes both categories, relations\nand simple quantification, the range of phenomena considered is generally broader than\nprevious perceptually-grounded QA work \\cite{Krish2013Grounded,Matuszek12Grounded}.\n\nHaving generated a set of candidate parses, we need to score them. This is a\nreranking problem; as in the rest of our approach, we solve it using standard\nneural machinery. In particular, we produce an LSTM representation of the question, a \nfeature-based representation of the query (with indicators on the type and number of \nmodules used), and pass both representations through a multilayer perceptron (MLP). \nWhile one can easily imagine a more sophisticated parse-scoring model, this simple approach works well for our tasks.\n\nFormally, for a question $x$, let $h_q(x)$ be an LSTM encoding of the question \n(i.e. the last hidden layer of an LSTM applied word-by-word to the input question). \nLet $\\{z_1, z_2, \\ldots\\}$ be the proposed layouts for $x$, and let\n$f(z_i)$ be a feature vector representing the $i$th layout. Then the score\n$s(z_i|x)$ for the layout $z_i$ is\n\n", "index": 15, "text": "\\begin{equation}\n\ts(z_i|x) = a^\\top \\sigma(B h_q(x) + C f(z_i) + d)\n    \\label{eq:layout-score}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"s(z_{i}|x)=a^{\\top}\\sigma(Bh_{q}(x)+Cf(z_{i})+d)\" display=\"block\"><mrow><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mi>a</mi><mo>\u22a4</mo></msup><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>B</mi><msub><mi>h</mi><mi>q</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>C</mi><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\n\nHaving defined a layout selection module $p(z|x;{\\theta_\\ell})$ and a network execution\nmodel $p_z(y|w;\\theta_e)$, we are ready to define a model for predicting answers\ngiven only (world, question) pairs. The key constraint is that we want to minimize\nevaluations of $p_z(y|w;\\theta_e)$ (which involves expensive application of a deep network\nto a large input representation), but can tractably evaluate $p(z|x;{\\theta_\\ell})$ for all\n$z$ (which involves application of a shallow network to a relatively small set of\ncandidates). This is the opposite of the situation usually encountered semantic\nparsing, where calls to the query execution model are fast but the set of candidate\nparses is too large to score exhaustively. \n\n\n\nIn fact, the problem more closely resembles\nthe scenario faced by agents in the reinforcement learning setting (where it is cheap \nto score actions, but potentially expensive to execute them and obtain rewards). \nWe adopt a common approach from that literature, and\nexpress our model as a stochastic policy. Under this policy, we first \\emph{sample} a \nlayout $z$  from a distribution $p(z|x;{\\theta_\\ell})$, and then apply $z$ to the knowledge \nsource and obtain a distribution over answers $p(y|z,w;\\theta_e)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter $z$ is chosen, we can train the execution model directly by maximizing $\\log\np(y|z,w;\\theta_e)$ with respect to $\\theta_e$ as before (this is ordinary\nbackpropagation).  Because the hard selection of $z$ is non-differentiable, we\noptimize $p(z|x;{\\theta_\\ell})$ using a policy gradient method.  \\newcite{Williams92Reinforce} showed that the gradient of the reward surface $J$ with \nrespect to the  parameters of the policy is\n\n", "itemtype": "equation", "pos": 24143, "prevtext": "\ni.e.\\ a the output of an MLP with inputs $h_q(x)$ and $f(z_i)$, and parameters\n${\\theta_\\ell} = \\{a, B, C, d\\}$. Finally, we normalize these scores to obtain a\ndistribution:\n\n", "index": 17, "text": "\\begin{equation}\n\n\tp(z|x;{\\theta_\\ell}) = e^{s(z_i|x)} / \\sum_{j=1}^n e^{s(z_j|x)}\n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\par&#10; p(z|x;{\\theta_{\\ell}})=e^{s(z_{i}|x)}/\\sum_{j=1}^{n}e^{s(z_{j}|x)}\\par&#10;\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo>;</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mi>e</mi><mrow><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup><mo>/</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><mrow><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\n(this is the {\\sc reinforce} rule).  Here the expectation is taken with\nrespect to rollouts of the policy, and $r$ is the reward. Because our goal is to\nselect the network that makes the most accurate predictions, we take the\nreward to be identically the negative log-probability from the execution phase,\ni.e.\n\n", "itemtype": "equation", "pos": 25987, "prevtext": "\n\nHaving defined a layout selection module $p(z|x;{\\theta_\\ell})$ and a network execution\nmodel $p_z(y|w;\\theta_e)$, we are ready to define a model for predicting answers\ngiven only (world, question) pairs. The key constraint is that we want to minimize\nevaluations of $p_z(y|w;\\theta_e)$ (which involves expensive application of a deep network\nto a large input representation), but can tractably evaluate $p(z|x;{\\theta_\\ell})$ for all\n$z$ (which involves application of a shallow network to a relatively small set of\ncandidates). This is the opposite of the situation usually encountered semantic\nparsing, where calls to the query execution model are fast but the set of candidate\nparses is too large to score exhaustively. \n\n\n\nIn fact, the problem more closely resembles\nthe scenario faced by agents in the reinforcement learning setting (where it is cheap \nto score actions, but potentially expensive to execute them and obtain rewards). \nWe adopt a common approach from that literature, and\nexpress our model as a stochastic policy. Under this policy, we first \\emph{sample} a \nlayout $z$  from a distribution $p(z|x;{\\theta_\\ell})$, and then apply $z$ to the knowledge \nsource and obtain a distribution over answers $p(y|z,w;\\theta_e)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter $z$ is chosen, we can train the execution model directly by maximizing $\\log\np(y|z,w;\\theta_e)$ with respect to $\\theta_e$ as before (this is ordinary\nbackpropagation).  Because the hard selection of $z$ is non-differentiable, we\noptimize $p(z|x;{\\theta_\\ell})$ using a policy gradient method.  \\newcite{Williams92Reinforce} showed that the gradient of the reward surface $J$ with \nrespect to the  parameters of the policy is\n\n", "index": 19, "text": "\\begin{equation}\n  {\\nabla} J({\\theta_\\ell}) = {\\mathbb{E}}[ {\\nabla} \\log p(z|x;{\\theta_\\ell}) \\cdot r ] \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\nabla}J({\\theta_{\\ell}})={\\mathbb{E}}[{\\nabla}\\log p(z|x;{\\theta_{\\ell}})%&#10;\\cdot r]\" display=\"block\"><mrow><mo>\u2207</mo><mi>J</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\ud835\udd3c</mi><mrow><mo stretchy=\"false\">[</mo><mo>\u2207</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo>;</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mi>r</mi><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\nThus the update to the layout-scoring model at each timestep is simply the\ngradient of the log-probability of the chosen layout, scaled by the accuracy of\nthat layout's predictions.\n\nAt training time, we approximate the expectation with a single rollout, so at\neach step we update ${\\theta_\\ell}$ in the direction\n\n$\n  ({\\nabla} \\log p(z|x;{\\theta_\\ell})) \\cdot \\log p(y|z,w;\\theta_e)\n$\n\nfor a single $z \\sim p(z|x;{\\theta_\\ell})$. $\\theta_e$ and ${\\theta_\\ell}$ are optimized using \n{\\sc adadelta} \\cite{Zeiler12Adadelta}\nwith $\\rho=0.95,$ $\\varepsilon=1e-6$ and gradient clipping at a norm of $10$.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nThe framework described in this paper is general, and we are interested in how well it\nperforms on datasets of varying domain, size and linguistic complexity. To that end, we\nevaluate our model on tasks at opposite extremes of both these criteria: a large \nvisual question answering dataset, and a small collection of more structured geography \nquestions.\n\n\\subsection{Questions about images}\n\nOur first task is the recently-introduced Visual Question Answering challenge (VQA) \\cite{Antol15VQA}.\nThe VQA dataset consists of more than 200,000 images paired with human-annotated questions\nand answers, as in \\autoref{fig:vqa:qualitative-results}.\n\nWe use the VQA 1.0 release, employing the development set for\nmodel selection and hyperparameter tuning, and reporting final results from the\nevaluation server on the test-standard set. For the experiments described in this\nsection, the input feature representations $w_i$ are computed by the the fifth\nconvolutional layer of a 16-layer VGG\\-Net after pooling \\cite{Simonyan14VGG}. \nInput images are scaled to 448$\\times$448 before computing their representations.\nWe found that performance on this task was best if the candidate layouts were\nrelatively simple: only {\\texttt{\\smaller {describe}}}, {\\texttt{\\smaller {and}}} and {\\texttt{\\smaller {find}}} modules\nare used, and layouts contain at most two conjuncts.\n\nOne weakness of this basic framework is a difficulty modeling prior knowledge about answers\n(of the form \\emph{bears are brown}). This kinds of linguistic ``prior'' is essential for the\nVQA task, and easily incorporated. We simply introduce an extra hidden layer for recombining the\nfinal module network output with the input sentence representation $h_q(x)$ \n(see \\autoref{eq:layout-score}), replacing \\autoref{eq:simple-execution} with:\n\n", "itemtype": "equation", "pos": 26420, "prevtext": "\n(this is the {\\sc reinforce} rule).  Here the expectation is taken with\nrespect to rollouts of the policy, and $r$ is the reward. Because our goal is to\nselect the network that makes the most accurate predictions, we take the\nreward to be identically the negative log-probability from the execution phase,\ni.e.\n\n", "index": 21, "text": "\\begin{equation}\n  {\\mathbb{E}} [({\\nabla} \\log p(z|x;{\\theta_\\ell})) \\cdot \\log p(y|z,w;\\theta_e)]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}[({\\nabla}\\log p(z|x;{\\theta_{\\ell}}))\\cdot\\log p(y|z,w;\\theta_{e})]\" display=\"block\"><mrow><mi>\ud835\udd3c</mi><mrow><mo stretchy=\"false\">[</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u2207</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">|</mo><mi>x</mi><mo>;</mo><msub><mi>\u03b8</mi><mi mathvariant=\"normal\">\u2113</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u22c5</mo><mi>log</mi><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>z</mi><mo>,</mo><mi>w</mi><mo>;</mo><msub><mi>\u03b8</mi><mi>e</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01705.tex", "nexttext": "\n(Now modules with output type {\\underline{Labels}\\xspace} should be understood as producing an answer\nembedding rather than a distribution over answers.) This allows the question to influence\nthe answer directly.\n\n\\begin{table}\n  \\footnotesize\n  \\centering\n  \\sisetup{\n    table-figures-decimal = 1\n  }\n  \n  \\begin{tabular}{l@{\\ }ccccc}\n    \\toprule\n    & \\multicolumn{4}{c}{test-dev} & test-std \\\\\n    \\cmidrule(lr){2-5} \\cmidrule(lr){6-6}\n    & Yes/No & Number & Other & All & All \\\\\n    \\midrule\n    Zhou (2015) & 76.6 & 35.0 & 42.6 & 55.7 & 55.9 \\\\\n    Noh (2015)  & 80.7 & 37.2 & 41.7 & 57.2 & 57.4 \\\\\n    NMN \n    \n                & 77.7 & 37.2 & 39.3 & 54.8 & 55.1 \\\\\n    NMN*        & 79.7 & 37.1 & 42.8 & 57.3 & --   \\\\\n    \n    D-NMN       & 80.5 & 37.4 & 43.1 & 57.9 & \\bf 58.0 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{.5mm}\n  \\caption{Results on the VQA test server. NMN is the parameter-tying\n  model from Andreas et al.\\ (2015), while NMN* is a reimplementation using the\n  same image processing pipeline as D-NMN. The model with dynamic network \n  structure prediction achieves the best published results on this task.}\n    \\label{tbl:vqa:quantitative-results}\n    \\vspace{-3mm}\n\\end{table}\n\n\nResults are shown in \\autoref{tbl:vqa:quantitative-results}. \nThe use of dynamic networks provides a small gain, most noticeably on yes/no questions.\nWe achieve state-of-the-art results on this task,\noutperforming a highly effective visual bag-of-words model \\cite{Zhou15ClassVQA},\na model with dynamic network parameter prediction (but fixed network\nstructure) \\cite{Noh15DPPVQA}, and a previous approach using neural module\nnetworks with no structure prediction \\cite{Andreas15NMN}. For this last model,\nwe report both the numbers from the original paper, and a reimplementation of the\nmodel that uses the same image preprocessing as the dynamic module network experiments\nin this paper. A more conventional attentional model has also been applied to this \ntask \\cite{Yang15AttVQA}; while we also outperform their reported performance, the \nevaluation uses different  train/test split, so results are not directly comparable.  \n\nSome examples are shown in \\autoref{fig:vqa:qualitative-results}. In general, \nthe model learns to focus on the correct region of the image, and tends to consider \na broad window around the region. This facilitates answering questions like \n\\emph{Where is the cat?}, which requires knowledge of the surroundings as well as the \nobject in question.\n\n\\begin{figure}\n  \\centering\n  \\scalebox{0.78}{\n    \\smaller\n    \\tabulinesep=3mm\n    \\begin{tabu}{|p{1.5in}|p{1.0in}|p{0.93in}|}\n    \t\\hline\n        \\includegraphics[height=1in]{fig/examples/sheep.jpg} &\n        \\centering\n        \\includegraphics[height=1in]{fig/examples/woman.jpg} &\n        \\centering\n        \\includegraphics[height=1in]{fig/examples/beach.jpg}\n        \\[-4mm]\n        \\includegraphics[height=1in]{fig/examples/sheep_att.png} &\n        \\centering\n        \\includegraphics[height=1in]{fig/examples/woman_att.png} &\n        \\centering\n        \\includegraphics[height=1in]{fig/examples/beach_att.png}\n        \\[-3mm]\n        \\emph{What is in the sheep's ear?} &\n        \\raggedright\n        \\emph{What color is she wearing?} &\n        \\raggedright\n        \\emph{What is the man dragging?}\n        \\[-3mm]\n        {\\texttt{\\smaller {(describe[what] \\newline         \\strut~~~~(and find[sheep] \\newline         \\strut~~~~~~~~~find[ear]))}}} &\n        {\\texttt{\\smaller {(describe[color] \\newline         \\strut~~~~find[wear])}}} &\n        {\\texttt{\\smaller {(describe[what] \\newline         \\strut~~~~find[man])}}}\n        \\[-3mm]\n        {\\textcolor{answergreen}{\\textbf{{tag}}}} &\n        {\\textcolor{answergreen}{\\textbf{{white}}}} &\n        {\\textcolor{answerred}{\\textbf{{boat}}}} (board) \\\\\n        \\hline\n    \\end{tabu}\n  }\n  \\vspace{.5mm}\n  \\caption{Sample outputs for the visual question answering task. The second\n  row shows the final attention provided as input to the top-level {\\texttt{\\smaller {describe}}} \n  module. For the first two examples, the model produces reasonable parses,\n  attends to the correct region of the images (the ear and the woman's \n  clothing), and generates the correct answer. In the third image, the\n  verb is discarded and a wrong answer is produced.}\n  \\label{fig:vqa:qualitative-results}\n  \\vspace{-5mm}\n\\end{figure}\n\n\\subsection{Questions about geography}\n\nThe next set of experiments we consider focuses on GeoQA, a geographical\nquestion-answering task first introduced by \\newcite{Krish2013Grounded}.\nThis task was originally paired with a visual question answering task\nmuch simpler than the one just discussed, and is appealing for a\nnumber of reasons. In contrast to the VQA dataset, GeoQA is quite small,\ncontaining only 263 examples. Two baselines are available: one using a \nclassical semantic parser backed by a database, and another which \ninduces logical predicates using linear \nclassifiers over both spatial and distributional features. This allows\nus to evaluate the quality of our model relative to other perceptually\ngrounded logical semantics, as well as strictly logical approaches.\n\nThe GeoQA domain consists of a set of entities (e.g.\\ states, cities, \nparks) which participate in various relations (e.g.\\ north-of, \ncapital-of). Here we take the world representation to consist of two pieces:\na set of category features (used by the {\\texttt{\\smaller {find}}} module) and a different\nset of relational features (used by the {\\texttt{\\smaller {relate}}} module). For our experiments,\nwe use a subset of the features originally used by Krishnamurthy et al.\n\n\n\nThe original dataset includes no quantifiers, and treats the questions\n\\emph{What cities are in Texas?}\\ and \\emph{Are there any cities in Texas?}\\\nidentically. Because we're interested in testing the parser's ability to\npredict a variety of different structures, we introduce a new version\nof the dataset, GeoQA+Q, which distinguishes these two cases, and expects a\nBoolean answer to questions of the second kind.\n\n\\begin{table}\n  \\footnotesize\n  \\centering\n  \\sisetup{\n    table-figures-decimal = 1\n  }\n  \\begin{tabular}{lcc}\n    \\toprule\n    & \\multicolumn{2}{c}{Accuracy} \\\\\n    \\cmidrule(){2-3}\n    Model & GeoQA & GeoQA+Q \\\\\n    \\midrule\n    LSP-F & 48\\phantom{.0} & -- \\\\\n    LSP-W & 51\\phantom{.0} & -- \\\\\n    NMN   & 51.7 & 35.7\\\\\n    D-NMN   & \\bf 54.3 & \\bf 42.9 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{.5em}\n  \\caption{Results on the GeoQA dataset, and the GeoQA dataset\n  with quantification. Our approach outperforms both a purely\n  logical model (LSP-F) and a model with learned perceptual\n  predicates (LSP-W) on the original dataset, and a fixed-structure \n  NMN under both evaluation conditions.}\n  \\label{tbl:geo:quantitative}\n  \\vspace{-4mm}\n\\end{table}\n\nResults are shown in \\autoref{tbl:geo:quantitative}. As in the original work, we report the results\nof leave-one-environment-out cross-validation on the set of 10 environments.\nOur dynamic model (D-NMN) outperforms both the logical (LSP-F) and perceptual models (LSP-W) described by\n\\cite{Krish2013Grounded}, as well as a fixed-structure neural module net (NMN). \nThis improvement is particularly notable on the dataset with  quantifiers, where dynamic \nstructure prediction produces a 20\\% relative improvement  over the fixed baseline. \nA variety of predicted layouts are shown in \\autoref{fig:geo:qualitative}.\n\n\\begin{figure}\n\\vspace{-1mm}\n\\centering\n\\footnotesize\n\\tabulinesep=2mm\n\\begin{tabu}{|p{0.92\\columnwidth}|}\n\\hline\nIs Key Largo an island? \\smallskip\\newline\n\n\n{\\texttt{\\smaller {(exists (and lookup[key-largo] find[island]))}}} \\smallskip\\newline\n{\\textcolor{answergreen}{\\textbf{{yes}}}}: correct \\\\\n\\hline\nWhat national parks are in Florida? \\smallskip\\newline\n\n\n{\\texttt{\\smaller {(and find[park] (relate[in] lookup[florida]))}}} \\smallskip\\newline\n{\\textcolor{answergreen}{\\textbf{{everglades}}}}: correct \\\\\n\\hline\nWhat are some beaches in Florida? \\smallskip\\newline\n{\\texttt{\\smaller {(exists (and lookup[beach] \\newline \\strut~~~~~~~~~~~~~(relate[in] lookup[florida])))}}} \\smallskip\\newline\n{\\textcolor{answerred}{\\textbf{{yes}}}} (daytona-beach): wrong parse \\\\\n\\hline\nWhat beach city is there in Florida? \\smallskip\\newline\n\n\n\n{\\texttt{\\smaller {(and lookup[beach] lookup[city] \\newline \\strut~~~~~(relate[in] lookup[florida]))}}} \\smallskip\\newline\n{\\textcolor{answerred}{\\textbf{{[none]}}}} (daytona-beach): wrong module behavior \\\\\n\\hline\n\\end{tabu}\n\\vspace{.5mm}\n\\caption{Example layouts and answers selected by the model on the GeoQA \ndataset. For incorrect predictions, the correct answer is shown in parentheses.}\n\\label{fig:geo:qualitative}\n\\vspace{-4mm}\n\\end{figure}\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe have introduced a new model, the \\emph{dynamic neural module network},\nfor answering queries about both structured and unstructured sources of\ninformation. Given only (question, world, answer) triples as training data, \nthe model learns to assemble neural networks on the fly from an inventory\nof neural models, and simultaneously learns weights for these modules so\nthat they can be composed into novel structures. Our approach achieves\nstate-of-the-art results on two tasks.\n\nWe believe that the success of this work derives from two factors:\n\n\\emph{Continuous representations improve the expressiveness and learnability of\nsemantic parsers}: by replacing discrete predicates with differentiable neural\nnetwork fragments, we bypass the challenging combinatorial optimization problem\nassociated with induction of a semantic lexicon. In structured\nworld representations, neural predicate representations allow the model to\ninvent reusable attributes and relations not expressed in\nthe schema. Perhaps more importantly, we can extend\ncompositional question-answering machinery to complex, continuous world\nrepresentations like images.\n\n\n\n\\emph{Semantic structure prediction improves generalization in deep networks}:\nby replacing a fixed network topology with a dynamic one, we can tailor the\ncomputation performed to each problem instance, using deeper networks for more\ncomplex questions and representing combinatorially many queries with\ncomparatively few parameters.  In practice, this results in considerable gains\nin speed and sample efficiency, even with very little training data.\n\nThese observations are not limited to the question answering domain, and\nwe expect that they can be applied similarly to tasks like\ninstruction following, game playing, and language generation.\n\n\\bibliography{jacob}\n\\bibliographystyle{naaclhlt2016}\n\n\n", "itemtype": "equation", "pos": 28988, "prevtext": "\nThus the update to the layout-scoring model at each timestep is simply the\ngradient of the log-probability of the chosen layout, scaled by the accuracy of\nthat layout's predictions.\n\nAt training time, we approximate the expectation with a single rollout, so at\neach step we update ${\\theta_\\ell}$ in the direction\n\n$\n  ({\\nabla} \\log p(z|x;{\\theta_\\ell})) \\cdot \\log p(y|z,w;\\theta_e)\n$\n\nfor a single $z \\sim p(z|x;{\\theta_\\ell})$. $\\theta_e$ and ${\\theta_\\ell}$ are optimized using \n{\\sc adadelta} \\cite{Zeiler12Adadelta}\nwith $\\rho=0.95,$ $\\varepsilon=1e-6$ and gradient clipping at a norm of $10$.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nThe framework described in this paper is general, and we are interested in how well it\nperforms on datasets of varying domain, size and linguistic complexity. To that end, we\nevaluate our model on tasks at opposite extremes of both these criteria: a large \nvisual question answering dataset, and a small collection of more structured geography \nquestions.\n\n\\subsection{Questions about images}\n\nOur first task is the recently-introduced Visual Question Answering challenge (VQA) \\cite{Antol15VQA}.\nThe VQA dataset consists of more than 200,000 images paired with human-annotated questions\nand answers, as in \\autoref{fig:vqa:qualitative-results}.\n\nWe use the VQA 1.0 release, employing the development set for\nmodel selection and hyperparameter tuning, and reporting final results from the\nevaluation server on the test-standard set. For the experiments described in this\nsection, the input feature representations $w_i$ are computed by the the fifth\nconvolutional layer of a 16-layer VGG\\-Net after pooling \\cite{Simonyan14VGG}. \nInput images are scaled to 448$\\times$448 before computing their representations.\nWe found that performance on this task was best if the candidate layouts were\nrelatively simple: only {\\texttt{\\smaller {describe}}}, {\\texttt{\\smaller {and}}} and {\\texttt{\\smaller {find}}} modules\nare used, and layouts contain at most two conjuncts.\n\nOne weakness of this basic framework is a difficulty modeling prior knowledge about answers\n(of the form \\emph{bears are brown}). This kinds of linguistic ``prior'' is essential for the\nVQA task, and easily incorporated. We simply introduce an extra hidden layer for recombining the\nfinal module network output with the input sentence representation $h_q(x)$ \n(see \\autoref{eq:layout-score}), replacing \\autoref{eq:simple-execution} with:\n\n", "index": 23, "text": "\\begin{equation}\n  \\log p_z(y|w,x) = (A h_q(x) + B {\\llbracket {z} \\rrbracket}_w)_y\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\log p_{z}(y|w,x)=(Ah_{q}(x)+B{\\llbracket{z}\\rrbracket}_{w})_{y}\" display=\"block\"><mrow><mi>log</mi><msub><mi>p</mi><mi>z</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">|</mo><mi>w</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><msub><mi>h</mi><mi>q</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>B</mi><msub><mrow><mo fence=\"true\">\u27e6</mo><mi>z</mi><mo fence=\"true\">\u27e7</mo></mrow><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>y</mi></msub></mrow></math>", "type": "latex"}]