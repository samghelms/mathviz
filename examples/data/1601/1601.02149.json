[{"file": "1601.02149.tex", "nexttext": "\nwhere $E_\\pi (\\cdot)$ represents the expected value under the distribution $\\pi$.\nThat is, the upper semiparametric bound of the function $f$ is calculated by finding the supremum of $E_\\pi\\left(f(X)\\right)$ across all possible probability distributions $\\pi$, with support {\\color{black} {on the set}} ${\\mathcal{D}}$, that satisfy the $2m$ expected value constrains. \n\nThe parameters $\\sigma_j^-,\\sigma_j^+$, $j=1,\\dots,m$ allow for \nconfidence interval estimates for the expected value of $E_\\pi(g_j(X))$,\nthat are not typically considered in the \nanalytical solution of special instances of {(\\ref{{eq:ub}})} \n(i.e., typically $\\sigma_j^- = \\sigma_j^+$ in analytical solutions). \n\nThe {\\em lower semiparametric bound} of the function $f$ is formulated as the corresponding minimization problem, that is, by changing the $\\sup$ to $\\inf$ in the objective of \\eqref{eq:ub}. \nWe will provide details about the solution of the upper semiparametric bound problem {(\\ref{{eq:ub}})} that apply in analogous fashion to the corresponding lower semiparametric bound problem.\nAlso, for ease of presentation we will at times refer to both the upper and lower bound semiparametric problems using {(\\ref{{eq:ub}})}.\n\nWhile specific instances of \\eqref{eq:ub} have been solved analytically {\\color{black} {\\citep[see, e.g.,][]{lo, cox, schepper, zhang11}}}, semidefinite programming (SDP) is currently the main approach\nused in the related literature to numerically solve the general problem being considered here\n{\\color{black} {\\citep[c.f.,][]{BoylL97,bert02,popescu}}} whenever the functions $f(\\cdot)$ and $g_j(\\cdot)$ are \npiecewise polynomials. \nHowever, the SDP approach has important limitations in terms of the capacity of practitioners to use it. First, there are no commercially available SDP solvers. Second, the formulation of the SDP that needs to be solved for a given problem is not ``simple'' ({\\citet{{coxzul10}}}) and must be re-derived for different support sets ${\\mathcal{D}}$ of the distribution of $X$ \\citep[see, e.g.,][Proposition 1]{bert02}. \n\n\n\n\n\\citet{BirgD91} proposed an alternative numerical method to solve the semiparametric bound\nproblem {(\\ref{{eq:ub}})} by using a CG\napproach (see, e.g., \\citet[Chp. 22]{Dantzig}, \\citet{colgen}) that has received little attention in the financial and\nactuarial science literature. Here, we show that the CG solution approach\naddresses the limitations of the SDP solution approach discussed above.\nAdditional \nadvantages of the CG solution approach in contrast to SDP techniques will be discussed at the end of Section~\\ref{sec:method}.\n\n{\\color{black} {It is worth to mention that although in the next section we present the proposed algorithm in pseudo-algorithmic form (e.g., see Algorithm~\\ref{fig:alg}), our implementation of the algorithm is available upon request to the authors.}}\n\n\n\n\n\n\n\n\n\n{\\section[{Solution via column generation}]{\\centering {Solution via column generation}}} \\label{sec:method}\n\nIn this section we present the CG solution approach proposed\nby \\citet[Sec. 3]{BirgD91} to solve the semiparametric bound problem {(\\ref{{eq:ub}})}.\nFor the sake of simplifying the exposition throughout we will assume that \\eqref{eq:ub} has a feasible solution, and\nthat the functions $f(\\cdot), g_j(\\cdot)$, $j=1,\\dots,m$ are {\\color{black} {Borel measurable in  ${\\mathcal{D}} \\subseteq {\\mathbb{R}}$ \\citep[cf.,][]{zulu05}}}. \nNow let $J\\subseteq {\\mathcal{D}}$ be a set of given {\\em atoms}, and construct the following linear program (LP) related to {(\\ref{{eq:ub}})} by associating a\nprobability decision-variable $p_x$ for every $x\\in J$:\n\n", "itemtype": "equation", "pos": 5351, "prevtext": "\n\\doublespacing\n\n\n\n\\maketitle\n\n\\begin{abstract}\n\\begin{singlespace}\n\\noindent It has been recently shown that numerical semiparametric bounds on the expected \npayoff of financial or actuarial instruments\ncan be computed using semidefinite programming. However, this approach has practical limitations. \nHere we use column generation, a classical optimization technique, to \naddress these limitations.\nFrom column generation, it follows that  practical univariate semiparametric bounds can be found\nby solving a series of linear programs. In addition to moment information, the column generation approach allows the inclusion of extra information about the random variable; for instance, unimodality and continuity, as well as the construction of corresponding worst/best-case distributions in a simple way.\n\\end{singlespace}\n\n\n\\end{abstract}\n\n\n\n\n{\\section[{Introduction}]{\\centering {Introduction}}}\nMany financial and insurance instruments protect against underlying losses for which it is difficult to make exact distributional assumptions. Under these circumstances, it is difficult to provide a good estimate of the loss distribution, which in turn makes it difficult to estimate payments on the corresponding insured loss. Computing {\\em semiparametric bounds} on the expected payments is an approach that has been successfully used to deal with this problem. This involves finding the minimum and maximum expected payments on the insurance instrument, when only partial information (e.g., moments) of the underlying loss distribution is known. For example, consider the work of {\\citet{{cox, jansen,vill}}}. This approach has also been used to address the estimation of bounds on extreme loss probabilities ({\\citet{{coxzul10}}}), and the prices of insurance instruments, and financial options ({\\citet{{brock,lo,schepper}}}). These semiparametric bounds are useful when the structure of the product is too complex to develop analytical or simulation based valuation methods, or when it is difficult to make strong distributional assumptions on the underlying risk factors. Furthermore, even when distributional assumptions can be made, and analytical valuation formulas or simulation based prices can be derived, these bounds are useful to check the consistency of such assumptions.\n\nThe semiparametric bound approach is also referred as {\\em distributionaly-robust} \\citep[see, e.g.,][]{Ye10} or {\\em ambiguity-averse} \\citep[see, e.g.,][]{Nata11}. Also, it has been shown that this approach partially reflects the manner in which persons naturally make decisions \\citep[cf.,][]{Nata11}\n\nIn the actuarial science and financial literature, there are two main approaches used to compute semiparametric bounds: analytically, by deriving closed-form formulas for special instances of the problem \\citep[see, e.g.,][]{zhang11,cox,schepper}; and numerically, by using {\\em semidedefinite programming} techniques \\citep[cf.,][]{todd01} to solve general instances of the problem \\citep[see, e.g.,][]{bert02, BoylL97,coxzul11,coxzul10}. \nAn alternative numerical approach to solve semiparametric bounds proposed in the {\\em stochastic\nprogramming} literature by \\citet{BirgD91}, based on the classical {\\em column generation} (CG)\napproach for mathematical optimization problems (see, e.g., \\citet[Chp. 22]{Dantzig}, \\citet{colgen}), has received little attention in the financial and\nactuarial science literature.\n\n\nHere, we \nconsider the use of CG to obtain semiparametric bounds\nin the context of financial and actuarial science applications.\nIn particular, we show that for all practical purposes, univariate semiparametric bounds can be found\nby solving a sequence of linear programs associated to the CG {\\em master} problem (cf., Section~\\ref{sec:method}).\nWe also show that the CG approach allows the inclusion of extra information about the random variable such as unimodality and continuity, as well as the construction of the corresponding worst/best-case distributions in a simple way.\nAlso, the CG methodology achieves accurate results at a very small computational cost, it is straightforward to implement,\nand the core of its implementation remains the same for very general, and practical instances of semiparametric bound\nproblems. \n\nTo illustrate the potential of the CG approach,  \nin Section~\\ref{sec:ex1m2}, semiparametric lower and upper bounds are computed for the loss elimination ratio of a right censored deductible insurance policy, when the underlying risk distribution\nis assumed to be unimodal, and have known first and second-order moments.\nFurthermore, in Section~\\ref{sec:worstcase}, we illustrate how continuous representations of\nthe worst/best-case distributions associated with the semiparametric bounds can be readily constructed and analyzed.\n\n\n\n\n\n{\\section[{Problem Description}]{\\centering {Problem Description}}}\nConsider a random variable $X$ with an unknown underlying distribution $\\pi$,\nbut known support ${\\mathcal{D}} \\subseteq {\\mathbb{R}}$ (not necessarily finite), and interval estimates $[\\sigma_j^-,\\sigma_j^+]$, $j = 1, \\ldots, m$ for the expected value\nof functions $g_j:\\mathbb{R} \\rightarrow \\mathbb{R}$ for $j=1,\\dots,m$ (e.g., typically, $g_j(x) = x^j$). \nThe {\\em upper semiparametric bound} on the expected value of the (target) function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ is defined as:\n\n", "index": 1, "text": "\\begin{equation} \\label{eq:ub}\n\t\\begin{aligned}\n\t\tB^*:=~ & \\underset{\\pi}{\\text{sup}} & & E_\\pi \\left(f(X)\\right) \\\\\n\t\t& \\text{s.t.} & & \\sigma_j^- \\leq E_\\pi \\left(g_j(X)\\right) \\leq \\sigma_j^+ \\quad \\quad j = 1, \\ldots, m \\\\\n\t\t\n\t\t\n\t\t& & & \\pi \\text{ a probability distribution on } {\\mathcal{D}},\n\t\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle B^{*}:=\" display=\"inline\"><mrow><msup><mi>B</mi><mo>*</mo></msup><mo>:=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{\\pi}{\\text{sup}}\" display=\"inline\"><munder accentunder=\"true\"><mtext>sup</mtext><mo>\ud835\udf0b</mo></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1X.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle E_{\\pi}\\left(f(X)\\right)\" display=\"inline\"><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1Xa.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sigma_{j}^{-}\\leq E_{\\pi}\\left(g_{j}(X)\\right)\\leq\\sigma_{j}^{+}%&#10;\\quad\\quad j=1,\\ldots,m\" display=\"inline\"><mrow><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2264</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>\u2264</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1Xb.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\pi\\text{ a probability distribution on }{\\mathcal{D}},\" display=\"inline\"><mrow><mrow><mi>\u03c0</mi><mo>\u2062</mo><mtext>\u00a0a probability distribution on\u00a0</mtext><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\n\nFurthermore, we assume that the set $J \\subseteq {\\mathcal{D}}$ is {\\em feasible}; that is, the corresponding LP {(\\ref{{eq:mj}})} is feasible. The existence of such $J \\subseteq {\\mathcal{D}}$ follows from the classical result by \\citet[Theorem 1]{kemp}, and can be found by solving algorithmically a {\\em Phase I} version \\citep[cf.,][]{bertlp} of the CG Algorithm~\\ref{fig:alg}. \nFollowing CG terminology, given a set $J \\subseteq {\\mathcal{D}}$ we will refer to {(\\ref{{eq:mj}})}  as the {\\em master} problem.\n\n{\\color{black} {Notice that any feasible solution of problem \\eqref{eq:mj} will be a feasible (atomic distribution) for problem~{(\\ref{{eq:ub}})}. Also, the objectives of the two problems are the expected value of the function $f(x)$ over the corresponding decision variable distribution. Thus, $M_J^*$ is a lower bound for the optimal value of the upper semiparametric bound problem~{(\\ref{{eq:ub}})}.}}\nFurthermore, it is possible to iteratively improve this lower bound by updating the set $J \\subseteq {\\mathcal{D}}$ using the optimal {\\em{dual values}} \\citep[cf.,][]{bertlp} of the constrains after the solution of the master problem~{(\\ref{{eq:mj}})}. \nNamely, let $\\rho_j^-,\\rho_j^+$, $j = 1, \\ldots, m$ and $\\tau$ be the dual variables of the upper/lower moment \n\n(i.e., first set of constraints in eq.~\\eqref{eq:mj}) and total probability \n\n(i.e, $\\sum_{x \\in J} p_x =1$)\nconstrains\n respectively. Given a feasible set $J \\subseteq {\\mathcal{D}}$, the dual variables can be used to select a new point $x \\in {\\mathcal{D}}$, to add to $J \\subseteq {\\mathcal{D}}$, that will make the corresponding LP {(\\ref{{eq:mj}})} a tighter approximation of \\eqref{eq:ub}. In particular, given $\\rho_j^-,\\rho_j^+$, $j=1,\\dots,m$ and $\\tau$, consider the following {\\em subproblem} to find $x$.\n\n", "itemtype": "equation", "pos": 9295, "prevtext": "\nwhere $E_\\pi (\\cdot)$ represents the expected value under the distribution $\\pi$.\nThat is, the upper semiparametric bound of the function $f$ is calculated by finding the supremum of $E_\\pi\\left(f(X)\\right)$ across all possible probability distributions $\\pi$, with support {\\color{black} {on the set}} ${\\mathcal{D}}$, that satisfy the $2m$ expected value constrains. \n\nThe parameters $\\sigma_j^-,\\sigma_j^+$, $j=1,\\dots,m$ allow for \nconfidence interval estimates for the expected value of $E_\\pi(g_j(X))$,\nthat are not typically considered in the \nanalytical solution of special instances of {(\\ref{{eq:ub}})} \n(i.e., typically $\\sigma_j^- = \\sigma_j^+$ in analytical solutions). \n\nThe {\\em lower semiparametric bound} of the function $f$ is formulated as the corresponding minimization problem, that is, by changing the $\\sup$ to $\\inf$ in the objective of \\eqref{eq:ub}. \nWe will provide details about the solution of the upper semiparametric bound problem {(\\ref{{eq:ub}})} that apply in analogous fashion to the corresponding lower semiparametric bound problem.\nAlso, for ease of presentation we will at times refer to both the upper and lower bound semiparametric problems using {(\\ref{{eq:ub}})}.\n\nWhile specific instances of \\eqref{eq:ub} have been solved analytically {\\color{black} {\\citep[see, e.g.,][]{lo, cox, schepper, zhang11}}}, semidefinite programming (SDP) is currently the main approach\nused in the related literature to numerically solve the general problem being considered here\n{\\color{black} {\\citep[c.f.,][]{BoylL97,bert02,popescu}}} whenever the functions $f(\\cdot)$ and $g_j(\\cdot)$ are \npiecewise polynomials. \nHowever, the SDP approach has important limitations in terms of the capacity of practitioners to use it. First, there are no commercially available SDP solvers. Second, the formulation of the SDP that needs to be solved for a given problem is not ``simple'' ({\\citet{{coxzul10}}}) and must be re-derived for different support sets ${\\mathcal{D}}$ of the distribution of $X$ \\citep[see, e.g.,][Proposition 1]{bert02}. \n\n\n\n\n\\citet{BirgD91} proposed an alternative numerical method to solve the semiparametric bound\nproblem {(\\ref{{eq:ub}})} by using a CG\napproach (see, e.g., \\citet[Chp. 22]{Dantzig}, \\citet{colgen}) that has received little attention in the financial and\nactuarial science literature. Here, we show that the CG solution approach\naddresses the limitations of the SDP solution approach discussed above.\nAdditional \nadvantages of the CG solution approach in contrast to SDP techniques will be discussed at the end of Section~\\ref{sec:method}.\n\n{\\color{black} {It is worth to mention that although in the next section we present the proposed algorithm in pseudo-algorithmic form (e.g., see Algorithm~\\ref{fig:alg}), our implementation of the algorithm is available upon request to the authors.}}\n\n\n\n\n\n\n\n\n\n{\\section[{Solution via column generation}]{\\centering {Solution via column generation}}} \\label{sec:method}\n\nIn this section we present the CG solution approach proposed\nby \\citet[Sec. 3]{BirgD91} to solve the semiparametric bound problem {(\\ref{{eq:ub}})}.\nFor the sake of simplifying the exposition throughout we will assume that \\eqref{eq:ub} has a feasible solution, and\nthat the functions $f(\\cdot), g_j(\\cdot)$, $j=1,\\dots,m$ are {\\color{black} {Borel measurable in  ${\\mathcal{D}} \\subseteq {\\mathbb{R}}$ \\citep[cf.,][]{zulu05}}}. \nNow let $J\\subseteq {\\mathcal{D}}$ be a set of given {\\em atoms}, and construct the following linear program (LP) related to {(\\ref{{eq:ub}})} by associating a\nprobability decision-variable $p_x$ for every $x\\in J$:\n\n", "index": 3, "text": "\\begin{equation} \\label{eq:mj}\n\n\n\\begin{array}{lllllllllllll}\n\tM_J^* :=~& \\max_{p_x} & & {\\displaystyle \\sum}_{x \\in J} p_x f(x) \\\\%\\label{eq:mjini}\\\\\n\t& \\text{s.t.} & & \\sigma_j^- \\leq {\\displaystyle \\sum}_{x \\in J} p_x g_j(x) \\leq \\sigma_j^+ & & j = 1, \\ldots, m,\\\\% \\label{eq:mja}\\\\% & & (\\rho_j^+) \\\\\n\n\t& & & {\\displaystyle \\sum}_{x \\in J} p_x =1,  & &\\\\% \\label{eq:mjb}\\\\%& & (\\tau) \\\\\n\t& & & p_x \\geq 0 & & \\text{for all $x \\in J$} \\\\%\\label{eq:mjend}.\n\n\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\par&#10;\\begin{array}[]{lllllllllllll}M_{J}^{*}:=&amp;\\max_{p_{x}}&amp;&amp;{%&#10;\\displaystyle\\sum}_{x\\in J}p_{x}f(x)\\\\&#10;&amp;\\text{s.t.}&amp;&amp;\\sigma_{j}^{-}\\leq{\\displaystyle\\sum}_{x\\in J}p_{x}g_{j}(x)\\leq%&#10;\\sigma_{j}^{+}&amp;&amp;j=1,\\ldots,m,\\\\&#10;\\par&#10;&amp;&amp;&amp;{\\displaystyle\\sum}_{x\\in J}p_{x}=1,&amp;&amp;\\\\&#10;&amp;&amp;&amp;p_{x}\\geq 0&amp;&amp;\\text{for all $x\\in J$}\\\\&#10;\\par&#10;\\par&#10;\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msubsup><mi>M</mi><mi>J</mi><mo>*</mo></msubsup><mo>:=</mo><mi/></mrow></mtd><mtd columnalign=\"left\"><munder><mi>max</mi><msub><mi>p</mi><mi>x</mi></msub></munder></mtd><mtd/><mtd columnalign=\"left\"><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>J</mi></mrow></munder><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd/><mtd columnalign=\"left\"><mtext>s.t.</mtext></mtd><mtd/><mtd columnalign=\"left\"><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2264</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>J</mi></mrow></munder><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>\u2062</mo><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup></mrow></mtd><mtd/><mtd columnalign=\"left\"><mrow><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd><mi/></mtd><mtd/><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>J</mi></mrow></munder><msub><mi>p</mi><mi>x</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr><mtr><mtd/><mtd/><mtd/><mtd columnalign=\"left\"><mrow><msub><mi>p</mi><mi>x</mi></msub><mo>\u2265</mo><mn>0</mn></mrow></mtd><mtd/><mtd columnalign=\"left\"><mrow><mtext>for all\u00a0</mtext><mrow><mi>x</mi><mo>\u2208</mo><mi>J</mi></mrow></mrow></mtd><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/><mtd/></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\n\nThe objective value of \\eqref{eq:srt} represents the {\\em reduced cost} of adding the new point $x$ to $J$; that is, the marginal amount by which the objective in {(\\ref{{eq:mj}})} can be improved with the addition of $x$ in the master problem~{(\\ref{{eq:mj}})}. Using the master problem \\eqref{eq:mj} and  subproblem~\\eqref{eq:srt} admits an iterative algorithm that (under suitable conditions) converges to the optimal value of \\eqref{eq:ub}. More specifically, at each iteration, the master problem {(\\ref{{eq:mj}})} is solved and \nits corresponding dual variables are used in the subproblem~\\eqref{eq:srt} to select a new point $x$ to be added to \nthe set $J \\subseteq {\\mathcal{D}}$. This is called a CG algorithm \nsince at each iteration, a new variable $p_x$, corresponding to the new given point $x$,  is added to the master problem~{(\\ref{{eq:mj}})}.\n\n\nIf the functions $f(\\cdot)$, $g_j(\\cdot), j=1,\\dots,m$ are continuous, and the support ${\\mathcal{D}}$ of the \nunderlying risk distribution is known to be compact, then the asymptotic convergence of the column generation algorithm follows from \\citet[Thm. 5, Chp. 24]{Dantzig}. However, for the numerical solution of the \npractical instances of~{(\\ref{{eq:ub}})} considered here, it suffices to have a ``stopping criteria'' for\nthe CG algorithm.\n\n\n\\begin{theorem} \\label{thm:conv}\n{\\color{black} { Let $J \\subseteq {\\mathcal{D}}$ be given, and $B^*$, $M^*_{J}$, $S^*_{\\rho, \\tau}$ be the optimal objective values of \\eqref{eq:ub}, \\eqref{eq:mj}, and~\\eqref{eq:srt} respectively.  Then $0 \\leq B^* - M^*_{J} \\leq S^*_{\\rho, \\tau}$. }}\n\n\\end{theorem}\nTheorem~\\ref{thm:conv} \nfollows from \\citet[Thm. 3, Chp. 24]{Dantzig}, and \nstates that the LP approximation from below in \\eqref{eq:mj} will be within \n$\\epsilon$ of the optimal objective of \\eqref{eq:ub} if the objective of subproblem \\eqref{eq:srt} is less\nthan $\\epsilon$. {\\color{black} {It is worth mentioning that under additional assumptions about the feasible set of~\\eqref{eq:ub}, one has that in the long-run $S^*_{\\rho, \\tau} \\to 0$; that is, the CG algorithm will converge to the optimal solution of  the semiparametric bound problem \\eqref{eq:ub}. This follows as a consequence of \\citet[Thm. 5, Chp. 24]{Dantzig}.}}\nIn practice, Theorem~\\ref{thm:conv} provides a stopping criteria for the implementation of the CG algorithm under\nonly the assumption of the original\nproblem~\\eqref{eq:ub} being feasible. Specifically, the CG Algorithm~\\ref{fig:alg} can be used to find the optimal upper bound $B^*$ up to $\\epsilon$-accuracy. As mentioned before, a {\\em Phase I} version \\citep[cf.,][]{bertlp} of the CG Algorithm~\\ref{fig:alg},\ncan be used to construct an initial feasible set $J_0 \\in {\\mathcal{D}}$.\n\n\n\n\n\n\n\\begin{algorithm}[!htb]\n\\caption{Semiparametric bounds via column generation}\n\\begin{algorithmic}[1]\n\\Procedure{GC}{feasible $J_0$, $\\epsilon > 0$}\n\\State $J \\gets J_0$, $S_{\\rho, \\tau}^* = \\infty$\n\\While{$S_{\\rho, \\tau}^* > \\epsilon$}\n\\State {\\bf compute} $M_{J}^*$, $p^* := \\{p^*_x\\}_{x\\in J}$, the optimal objective and solution of master problem~\\eqref{eq:mj}\n\n\\State  {\\bf compute} $S_{\\rho, \\tau}^*$, $x^*$ , the optimal objective  and solution of\nsubproblem \\eqref{eq:srt}\n\\State  $J \\gets J \\cup \\{x^*\\}$\n\\EndWhile\n\\State  {\\bf return}  $J^* = J$, $p^*$, and $M_{J}^* \\approx B^*$ (where $\\approx$ stands for $M_{J}^*$ approximates $B^*$ )\n\n\\EndProcedure\n\\end{algorithmic}\n\\label{fig:alg}\n\\end{algorithm}\n\n\n\n\n\n\n\n\nNote that (in principle) problem {(\\ref{{eq:ub}})}  has an infinite number of columns (i.e., variables) and a finite number of constrains;\nthat is, the semiparametric bound problem \\eqref{eq:ub} is a {\\em semi-infinite} program. Thus, \nthe approach outlined above is an application to a semi-infinite program \nof column generation techniques\ninitially introduced by \\citet{DantW61} for LPs, generalized\nlinear programs, and convex programs (cf. \\citet[Chp. 22--24]{Dantzig}).\nFor a survey of column generation methods see {\\citet{{colgen}}}.\n\n\\subsection{Solving the subproblem}\n\\label{sec:sub}\n\nAs observed by \\citet{BirgD91}, the main difficulty in using the CG approach to solve the\nsemiparametric bound problem~{(\\ref{{eq:ub}})} is that the subproblem~{(\\ref{{eq:srt}})} is in general\na {\\em non-convex} optimization problem \\citep[cf.,][]{Nocedal2006NO}. However, in the\npractically relevant instances of the problem considered here, the following assumption holds.\n\n\\begin{assumption}\n\\label{assume}\nThe functions $f(\\cdot)$, and $g_j(\\cdot)$, $j=1,\\dots,m$ in {(\\ref{{eq:ub}})} are piecewise polynomials of degree less than five (5).\n\\end{assumption}\n\nMore specifically, typically no higher than fifth-order moment\ninformation on the risk will be assumed to be known (e.g., $g_j(X) = X^j$ for $j=1,\\dots,m$ and $m \\le 5$). Also, the function $f(\\cdot)$ typically defines: the piecewise linear cost or payoff of an  insurance instrument (e.g., $f(x) = \\max\\{0,x-d\\}$); a ruin event using a (piecewise constant) indicator function (e.g., {\\color{black} {$f(x) = \\mathbb{I}_{[0,r]}(x)$}}) ; or a lower than fifth-order moment of the risk or insurance policy cost/payoff \\citep[see, e.g.][]{brock,cox, lo, schepper}. In such cases, the objective of \\eqref{eq:srt} is an univariate\nfifth-degree (or lower) piecewise polynomial. Thus, \nsubproblem~\\eqref{eq:srt}\ncan be solved ``exactly'' by finding the roots of fourth degree polynomials (using {\\em first-order} optimality conditions \\citep[see, e.g.,][]{Nocedal2006NO}). As a result, we have the\nfollowing remark.\n\n\\begin{remark}\n\\label{rem:LPs}\nUnder Assumption~\\ref{assume} and using the CG Algorithm~\\ref{fig:alg}, the solution to problem {(\\ref{{eq:ub}})} can be found by solving a sequence of LPs {(\\ref{{eq:mj}})} where the column updates {(\\ref{{eq:srt}})} can be found with simple arithmetic operations.\n\\end{remark}\n\n{\\color{black} {Moreover, thanks to current numerical algorithms for finding roots of univariate polynomials, it is not difficult to solve subproblem~\\eqref{eq:srt} numerically to a high precision, even when the polynomials involved in the problem have degree higher than 5. In turn, this means that Algorithm~\\ref{fig:alg} would perform well for instances of the problem with high degree polynomials.}}\n\nGenerating semiparametric bounds using the CG approach outlined above has several key advantages over the semidefinite programming (SDP) solution\napproach introduced by \\citet{bert02, popescu}. First, only a linear programming solver for \\eqref{eq:mj} and the ability to find the roots of polynomials with degree no more than four for \\eqref{eq:srt} is required in most practically relevant situations. This means that the methodology can use any commercial LP solver allowing for rapid and numerically stable solutions. Second, the problem does not need to be reformulated for changes in the support ${\\mathcal{D}}$ of the underlying risk distribution $\\pi$ of $X$. Accounting for alternate support requires only limiting the search space in the subproblem \\eqref{eq:srt}. Finally, problem~\\eqref{eq:mj} is explicitly defined in terms of the distribution used to generate the bound value. So, for any bound computed, the {\\em worst-case} (resp. {\\em best-case} for the lower bound) distribution that yielded that bound can also be analyzed; with the SDP approach no such insight into the resulting distribution is, to the best of our knowledge, readily possible. The ability to analyze the resulting distribution would be of particular use to practitioners in the insurance and risk management industry and will be further discussed in Section~\\ref{sec:worstcase}. Third, the CG approach works analogously for both the upper and lower semiparametric bound problems. In contrast, the SDP approach \ncommonly results in SDP formulations of the problem that are more involved for the lower than for the upper semiparametric bound. Finally, as shown in Section~\\ref{sec:extend}, the CG approach allows the addition of information about the class of distribution to which the underlying risk belongs (e.g., continuous, unimodal) without changing the core of the solution algorithm.\n\n\n\n\n\n\n\n{\\section[{Additional Distribution Information}]{\\centering {Additional Distribution Information}}} \\label{sec:extend}\nAs mentioned earlier, in practical instances of the semiparametric bound problem~{(\\ref{{eq:ub}})} the functions $g_j(\\cdot)$, $j=1,\\dots,m$ are typically set to assume the knowledge of moments of the underlying loss distribution; for example, by setting $g_j(X) = X^j$, $j=1,\\dots,m$ in {(\\ref{{eq:ub}})}.\nThe general semiparametric bound problem {(\\ref{{eq:ub}})} can be extended to include additional distributional information other than moments \\citep[see, e.g.,][]{popescu,schepper}. This is important as the resulting bounds will be tighter and the corresponding worst/best-case\ndistribution will have characteristics consistent with the practitioner's application-specific knowledge about continuity, unimodality, and heavy tails in financial loss contexts. In this section it is shown that \nthe CG solution approach outlined in Section~\\ref{sec:method} for semiparametric bound problems can\nbe extended to constrain the underlying distribution to \nbe unimodal and continuous.\n\n\n\n\\subsection{Mixture Transformation} \\label{sec:transform}\n\nNote that a point  $x\\in J^*$ obtained after running the CG Algorithm~\\ref{fig:alg} can be interpreted\nas the mean of Dirac delta distributions $\\delta_{x}$ parametrized by (centered at) $x$. \nIn turn, the resulting optimal distribution $\\pi^*$ of the random variable $X$ in {(\\ref{{eq:mj}})} is a mixture of Dirac delta distributions;\nthat is, \n$\\pi^* \\sim \\sum_{x \\in J^*} p_{x} \\delta_{x}$.\nAs we show below, the CG Algorithm~\\ref{fig:alg} can be used to obtain optimal worst/best-case distributions\nassociated with the semiparametric bound problem~{(\\ref{{eq:ub}})} when besides the expected value constrains, information \nis known about the class of distribution to which $\\pi$ belongs; for example, unimodal, smooth, asymmetric, etc. Basically\nthis is done by replacing $\\delta_x \\to H_x$ in the mixture composition of $\\pi$, where $H_x$ is an appropriately chosen\ndistribution parametrized by $x$.\n\nMore specifically, assume that besides the moment information used in the definition of the semiparametric\nbound problem~{(\\ref{{eq:ub}})}, it is known that the distribution $\\pi$ is a mixture of\nknown probability distributions $H_x$, parametrized by a single parameter $x \\in {\\mathbb{R}}$.\nFor example, $x$ could be the mean of the distribution $H_x$, or $H_x$ could be a\nuniform distribution between $0$ and $x$. Note that for any $g : \\mathbb{R} \\rightarrow \\mathbb{R}$, \nit follows from the mixture composition of the distribution $\\pi$ that:\n\n", "itemtype": "equation", "pos": 11589, "prevtext": "\n\nFurthermore, we assume that the set $J \\subseteq {\\mathcal{D}}$ is {\\em feasible}; that is, the corresponding LP {(\\ref{{eq:mj}})} is feasible. The existence of such $J \\subseteq {\\mathcal{D}}$ follows from the classical result by \\citet[Theorem 1]{kemp}, and can be found by solving algorithmically a {\\em Phase I} version \\citep[cf.,][]{bertlp} of the CG Algorithm~\\ref{fig:alg}. \nFollowing CG terminology, given a set $J \\subseteq {\\mathcal{D}}$ we will refer to {(\\ref{{eq:mj}})}  as the {\\em master} problem.\n\n{\\color{black} {Notice that any feasible solution of problem \\eqref{eq:mj} will be a feasible (atomic distribution) for problem~{(\\ref{{eq:ub}})}. Also, the objectives of the two problems are the expected value of the function $f(x)$ over the corresponding decision variable distribution. Thus, $M_J^*$ is a lower bound for the optimal value of the upper semiparametric bound problem~{(\\ref{{eq:ub}})}.}}\nFurthermore, it is possible to iteratively improve this lower bound by updating the set $J \\subseteq {\\mathcal{D}}$ using the optimal {\\em{dual values}} \\citep[cf.,][]{bertlp} of the constrains after the solution of the master problem~{(\\ref{{eq:mj}})}. \nNamely, let $\\rho_j^-,\\rho_j^+$, $j = 1, \\ldots, m$ and $\\tau$ be the dual variables of the upper/lower moment \n\n(i.e., first set of constraints in eq.~\\eqref{eq:mj}) and total probability \n\n(i.e, $\\sum_{x \\in J} p_x =1$)\nconstrains\n respectively. Given a feasible set $J \\subseteq {\\mathcal{D}}$, the dual variables can be used to select a new point $x \\in {\\mathcal{D}}$, to add to $J \\subseteq {\\mathcal{D}}$, that will make the corresponding LP {(\\ref{{eq:mj}})} a tighter approximation of \\eqref{eq:ub}. In particular, given $\\rho_j^-,\\rho_j^+$, $j=1,\\dots,m$ and $\\tau$, consider the following {\\em subproblem} to find $x$.\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:srt}\n\\begin{aligned}\n\tS_{\\rho, \\tau}^* :=~& \\underset{x \\in {\\mathcal{D}}}{\\max} & & f(x) - \\tau - \\sum_{j=1}^m (\\rho_j^+ + \\rho_j^-) g_j(x) \\\\\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle S_{\\rho,\\tau}^{*}:=\" display=\"inline\"><mrow><msubsup><mi>S</mi><mrow><mi>\u03c1</mi><mo>,</mo><mi>\u03c4</mi></mrow><mo>*</mo></msubsup><mo>:=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{x\\in{\\mathcal{D}}}{\\max}\" display=\"inline\"><munder accentunder=\"true\"><mi>max</mi><mrow><mi>x</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle f(x)-\\tau-\\sum_{j=1}^{m}(\\rho_{j}^{+}+\\rho_{j}^{-})g_{j}(x)\" display=\"inline\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>\u03c4</mi><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\u03c1</mi><mi>j</mi><mo>+</mo></msubsup><mo>+</mo><msubsup><mi>\u03c1</mi><mi>j</mi><mo>-</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nThis means that with the additional distribution mixture constrain, the associated semiparametric\nbound problem can be solved with the CG Algorithm~\\ref{fig:alg}\nafter replacing \n\n", "itemtype": "equation", "pos": 22534, "prevtext": "\n\nThe objective value of \\eqref{eq:srt} represents the {\\em reduced cost} of adding the new point $x$ to $J$; that is, the marginal amount by which the objective in {(\\ref{{eq:mj}})} can be improved with the addition of $x$ in the master problem~{(\\ref{{eq:mj}})}. Using the master problem \\eqref{eq:mj} and  subproblem~\\eqref{eq:srt} admits an iterative algorithm that (under suitable conditions) converges to the optimal value of \\eqref{eq:ub}. More specifically, at each iteration, the master problem {(\\ref{{eq:mj}})} is solved and \nits corresponding dual variables are used in the subproblem~\\eqref{eq:srt} to select a new point $x$ to be added to \nthe set $J \\subseteq {\\mathcal{D}}$. This is called a CG algorithm \nsince at each iteration, a new variable $p_x$, corresponding to the new given point $x$,  is added to the master problem~{(\\ref{{eq:mj}})}.\n\n\nIf the functions $f(\\cdot)$, $g_j(\\cdot), j=1,\\dots,m$ are continuous, and the support ${\\mathcal{D}}$ of the \nunderlying risk distribution is known to be compact, then the asymptotic convergence of the column generation algorithm follows from \\citet[Thm. 5, Chp. 24]{Dantzig}. However, for the numerical solution of the \npractical instances of~{(\\ref{{eq:ub}})} considered here, it suffices to have a ``stopping criteria'' for\nthe CG algorithm.\n\n\n\\begin{theorem} \\label{thm:conv}\n{\\color{black} { Let $J \\subseteq {\\mathcal{D}}$ be given, and $B^*$, $M^*_{J}$, $S^*_{\\rho, \\tau}$ be the optimal objective values of \\eqref{eq:ub}, \\eqref{eq:mj}, and~\\eqref{eq:srt} respectively.  Then $0 \\leq B^* - M^*_{J} \\leq S^*_{\\rho, \\tau}$. }}\n\n\\end{theorem}\nTheorem~\\ref{thm:conv} \nfollows from \\citet[Thm. 3, Chp. 24]{Dantzig}, and \nstates that the LP approximation from below in \\eqref{eq:mj} will be within \n$\\epsilon$ of the optimal objective of \\eqref{eq:ub} if the objective of subproblem \\eqref{eq:srt} is less\nthan $\\epsilon$. {\\color{black} {It is worth mentioning that under additional assumptions about the feasible set of~\\eqref{eq:ub}, one has that in the long-run $S^*_{\\rho, \\tau} \\to 0$; that is, the CG algorithm will converge to the optimal solution of  the semiparametric bound problem \\eqref{eq:ub}. This follows as a consequence of \\citet[Thm. 5, Chp. 24]{Dantzig}.}}\nIn practice, Theorem~\\ref{thm:conv} provides a stopping criteria for the implementation of the CG algorithm under\nonly the assumption of the original\nproblem~\\eqref{eq:ub} being feasible. Specifically, the CG Algorithm~\\ref{fig:alg} can be used to find the optimal upper bound $B^*$ up to $\\epsilon$-accuracy. As mentioned before, a {\\em Phase I} version \\citep[cf.,][]{bertlp} of the CG Algorithm~\\ref{fig:alg},\ncan be used to construct an initial feasible set $J_0 \\in {\\mathcal{D}}$.\n\n\n\n\n\n\n\\begin{algorithm}[!htb]\n\\caption{Semiparametric bounds via column generation}\n\\begin{algorithmic}[1]\n\\Procedure{GC}{feasible $J_0$, $\\epsilon > 0$}\n\\State $J \\gets J_0$, $S_{\\rho, \\tau}^* = \\infty$\n\\While{$S_{\\rho, \\tau}^* > \\epsilon$}\n\\State {\\bf compute} $M_{J}^*$, $p^* := \\{p^*_x\\}_{x\\in J}$, the optimal objective and solution of master problem~\\eqref{eq:mj}\n\n\\State  {\\bf compute} $S_{\\rho, \\tau}^*$, $x^*$ , the optimal objective  and solution of\nsubproblem \\eqref{eq:srt}\n\\State  $J \\gets J \\cup \\{x^*\\}$\n\\EndWhile\n\\State  {\\bf return}  $J^* = J$, $p^*$, and $M_{J}^* \\approx B^*$ (where $\\approx$ stands for $M_{J}^*$ approximates $B^*$ )\n\n\\EndProcedure\n\\end{algorithmic}\n\\label{fig:alg}\n\\end{algorithm}\n\n\n\n\n\n\n\n\nNote that (in principle) problem {(\\ref{{eq:ub}})}  has an infinite number of columns (i.e., variables) and a finite number of constrains;\nthat is, the semiparametric bound problem \\eqref{eq:ub} is a {\\em semi-infinite} program. Thus, \nthe approach outlined above is an application to a semi-infinite program \nof column generation techniques\ninitially introduced by \\citet{DantW61} for LPs, generalized\nlinear programs, and convex programs (cf. \\citet[Chp. 22--24]{Dantzig}).\nFor a survey of column generation methods see {\\citet{{colgen}}}.\n\n\\subsection{Solving the subproblem}\n\\label{sec:sub}\n\nAs observed by \\citet{BirgD91}, the main difficulty in using the CG approach to solve the\nsemiparametric bound problem~{(\\ref{{eq:ub}})} is that the subproblem~{(\\ref{{eq:srt}})} is in general\na {\\em non-convex} optimization problem \\citep[cf.,][]{Nocedal2006NO}. However, in the\npractically relevant instances of the problem considered here, the following assumption holds.\n\n\\begin{assumption}\n\\label{assume}\nThe functions $f(\\cdot)$, and $g_j(\\cdot)$, $j=1,\\dots,m$ in {(\\ref{{eq:ub}})} are piecewise polynomials of degree less than five (5).\n\\end{assumption}\n\nMore specifically, typically no higher than fifth-order moment\ninformation on the risk will be assumed to be known (e.g., $g_j(X) = X^j$ for $j=1,\\dots,m$ and $m \\le 5$). Also, the function $f(\\cdot)$ typically defines: the piecewise linear cost or payoff of an  insurance instrument (e.g., $f(x) = \\max\\{0,x-d\\}$); a ruin event using a (piecewise constant) indicator function (e.g., {\\color{black} {$f(x) = \\mathbb{I}_{[0,r]}(x)$}}) ; or a lower than fifth-order moment of the risk or insurance policy cost/payoff \\citep[see, e.g.][]{brock,cox, lo, schepper}. In such cases, the objective of \\eqref{eq:srt} is an univariate\nfifth-degree (or lower) piecewise polynomial. Thus, \nsubproblem~\\eqref{eq:srt}\ncan be solved ``exactly'' by finding the roots of fourth degree polynomials (using {\\em first-order} optimality conditions \\citep[see, e.g.,][]{Nocedal2006NO}). As a result, we have the\nfollowing remark.\n\n\\begin{remark}\n\\label{rem:LPs}\nUnder Assumption~\\ref{assume} and using the CG Algorithm~\\ref{fig:alg}, the solution to problem {(\\ref{{eq:ub}})} can be found by solving a sequence of LPs {(\\ref{{eq:mj}})} where the column updates {(\\ref{{eq:srt}})} can be found with simple arithmetic operations.\n\\end{remark}\n\n{\\color{black} {Moreover, thanks to current numerical algorithms for finding roots of univariate polynomials, it is not difficult to solve subproblem~\\eqref{eq:srt} numerically to a high precision, even when the polynomials involved in the problem have degree higher than 5. In turn, this means that Algorithm~\\ref{fig:alg} would perform well for instances of the problem with high degree polynomials.}}\n\nGenerating semiparametric bounds using the CG approach outlined above has several key advantages over the semidefinite programming (SDP) solution\napproach introduced by \\citet{bert02, popescu}. First, only a linear programming solver for \\eqref{eq:mj} and the ability to find the roots of polynomials with degree no more than four for \\eqref{eq:srt} is required in most practically relevant situations. This means that the methodology can use any commercial LP solver allowing for rapid and numerically stable solutions. Second, the problem does not need to be reformulated for changes in the support ${\\mathcal{D}}$ of the underlying risk distribution $\\pi$ of $X$. Accounting for alternate support requires only limiting the search space in the subproblem \\eqref{eq:srt}. Finally, problem~\\eqref{eq:mj} is explicitly defined in terms of the distribution used to generate the bound value. So, for any bound computed, the {\\em worst-case} (resp. {\\em best-case} for the lower bound) distribution that yielded that bound can also be analyzed; with the SDP approach no such insight into the resulting distribution is, to the best of our knowledge, readily possible. The ability to analyze the resulting distribution would be of particular use to practitioners in the insurance and risk management industry and will be further discussed in Section~\\ref{sec:worstcase}. Third, the CG approach works analogously for both the upper and lower semiparametric bound problems. In contrast, the SDP approach \ncommonly results in SDP formulations of the problem that are more involved for the lower than for the upper semiparametric bound. Finally, as shown in Section~\\ref{sec:extend}, the CG approach allows the addition of information about the class of distribution to which the underlying risk belongs (e.g., continuous, unimodal) without changing the core of the solution algorithm.\n\n\n\n\n\n\n\n{\\section[{Additional Distribution Information}]{\\centering {Additional Distribution Information}}} \\label{sec:extend}\nAs mentioned earlier, in practical instances of the semiparametric bound problem~{(\\ref{{eq:ub}})} the functions $g_j(\\cdot)$, $j=1,\\dots,m$ are typically set to assume the knowledge of moments of the underlying loss distribution; for example, by setting $g_j(X) = X^j$, $j=1,\\dots,m$ in {(\\ref{{eq:ub}})}.\nThe general semiparametric bound problem {(\\ref{{eq:ub}})} can be extended to include additional distributional information other than moments \\citep[see, e.g.,][]{popescu,schepper}. This is important as the resulting bounds will be tighter and the corresponding worst/best-case\ndistribution will have characteristics consistent with the practitioner's application-specific knowledge about continuity, unimodality, and heavy tails in financial loss contexts. In this section it is shown that \nthe CG solution approach outlined in Section~\\ref{sec:method} for semiparametric bound problems can\nbe extended to constrain the underlying distribution to \nbe unimodal and continuous.\n\n\n\n\\subsection{Mixture Transformation} \\label{sec:transform}\n\nNote that a point  $x\\in J^*$ obtained after running the CG Algorithm~\\ref{fig:alg} can be interpreted\nas the mean of Dirac delta distributions $\\delta_{x}$ parametrized by (centered at) $x$. \nIn turn, the resulting optimal distribution $\\pi^*$ of the random variable $X$ in {(\\ref{{eq:mj}})} is a mixture of Dirac delta distributions;\nthat is, \n$\\pi^* \\sim \\sum_{x \\in J^*} p_{x} \\delta_{x}$.\nAs we show below, the CG Algorithm~\\ref{fig:alg} can be used to obtain optimal worst/best-case distributions\nassociated with the semiparametric bound problem~{(\\ref{{eq:ub}})} when besides the expected value constrains, information \nis known about the class of distribution to which $\\pi$ belongs; for example, unimodal, smooth, asymmetric, etc. Basically\nthis is done by replacing $\\delta_x \\to H_x$ in the mixture composition of $\\pi$, where $H_x$ is an appropriately chosen\ndistribution parametrized by $x$.\n\nMore specifically, assume that besides the moment information used in the definition of the semiparametric\nbound problem~{(\\ref{{eq:ub}})}, it is known that the distribution $\\pi$ is a mixture of\nknown probability distributions $H_x$, parametrized by a single parameter $x \\in {\\mathbb{R}}$.\nFor example, $x$ could be the mean of the distribution $H_x$, or $H_x$ could be a\nuniform distribution between $0$ and $x$. Note that for any $g : \\mathbb{R} \\rightarrow \\mathbb{R}$, \nit follows from the mixture composition of the distribution $\\pi$ that:\n\n", "index": 7, "text": "\\begin{align}\n\tE_{\\pi}(g) &= \\int_0^\\infty g(u) E_{\\pi(X)}(H_X(u) ) du \\nonumber \\\\\n\t&= E_{\\pi(X)} \\left ( \\int_0^\\infty g(u) H_X(u) du\\right ) \\nonumber \\\\\n\t&= E_{\\pi(X)}\\left ( E_{H_X(U)}(g(U)) \\right )\\label{eq:mixh}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E_{\\pi}(g)\" display=\"inline\"><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\int_{0}^{\\infty}g(u)E_{\\pi(X)}(H_{X}(u))du\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>E</mi><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>H</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=E_{\\pi(X)}\\left(\\int_{0}^{\\infty}g(u)H_{X}(u)du\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mi>E</mi><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>H</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=E_{\\pi(X)}\\left(E_{H_{X}(U)}(g(U))\\right).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>E</mi><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>E</mi><mrow><msub><mi>H</mi><mi>X</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nfor $j=1,\\dots,m$ in {(\\ref{{eq:mj}})}, and {(\\ref{{eq:srt}})}. \n\nNote that in many instances, the expectations\nin {(\\ref{{eq:trans}})} can be computed in closed-form as a function of the mixture\ndistribution parameter $x$. Moreover, the expectations in~{(\\ref{{eq:trans}})} are commonly\npiecewise polynomials in $x$ (e.g., if $H_x$ is a uniform distribution between $0$ and $x$),\nor can be written as polynomials after an appropriate change in variable\n(e.g., if $H_x$ is a lognormal distribution with chosen volatility parameter $\\sigma \\in {\\mathbb{R}}^+$, and mean $e^{x+\\frac{1}{2}\\sigma^2}$). In such cases, after applying the transformation {(\\ref{{eq:trans}})} the \nobjective of subproblem~{(\\ref{{eq:srt}})} will be a piecewise polynomial on $x$. As discussed\nin Section~\\ref{sec:sub}, the subproblem can then be solved ``exactly'', \nand Remark~\\ref{rem:LPs} will still hold as long as Assumption~\\ref{assume} is valid after\nthe transformation~{(\\ref{{eq:trans}})}.\nThis is illustrated in Example~\\ref{ex} below. Also, as we show with numerical experiments in Section~\\ref{sec:num}, this is the case in most\npractical applications.\n\n\n{\\color{black} { \\begin{example} \\label{ex} Consider a simple insurance policy with  no deductilble  on a loss $X$ for which the non-central  moments up to $m$-order are assumed to be known. Specifically,  let $f(x) = \\max\\{0,x\\}$, and $g_j(x) = x^j$, $j=1,\\dots,m$. Also, assume that the distribution of the loss $X$ is known to be a mixture of uniform distributions $H_x$ of the form $H_x \\sim \\operatorname{Uniform}(0,x)$ in {(\\ref{{eq:mixh}})}.  That is, $H_x(u) = \\frac{1}{x}\\mathbb{I}_{[0,x]}(u)$. From~{(\\ref{{eq:trans}})}, it follows that $E_{H_x}(f) = \\frac{x}{2}$, $E_{H_x}(g_j) = \\frac{1}{j+1}x^{j}$ for $j=1,\\dots,m$, and Remark~\\ref{rem:LPs} will hold for any $m \\le 4$. \\end{example} }}\n\nIn other cases; that is, when Assumption~\\ref{assume} does not hold after the transformation~{(\\ref{{eq:trans}})}, one can sharply approximate the expectations\nin {(\\ref{{eq:trans}})} using up to fifth-degree piecewise polynomials on~$x$ to take advantage of\nRemark~\\ref{rem:LPs}. Alternatively, given that the subproblem~{(\\ref{{eq:srt}})} is an univariate optimization problem,\nglobal optimization solvers such as {\\tt BARON} (cf., {\\citet{{ts:05}}}) can be used to effectively solve it.\n\nIn Section~\\ref{sec:unimod} we will discuss how the mixture transformation {(\\ref{{eq:trans}})}\ncan be used to substantially strengthen semiparametric bounds by using reasonable\nassumptions about the \nunderlying risk distribution regarding unimodality, and or continuity by using\na mixture of appropriate distributions. Moreover, in Section~\\ref{sec:worstcase}, we use this transformation\nto construct reasonable worst/best-case distributions associated to a given semiparametric\nbound problem.\n\n\n\n\\subsection{Unimodality}\n\n\\label{sec:unimod}\n\n{\\color{black} {In many instances of the semiparametric bound problem, it might be reasonable to assume that the  unknown distribution $\\pi$ of $X$ in {(\\ref{{eq:ub}})} is unimodal with known mode {\\mbox{$M$} }. This is particularly the case  when  the underlying random variable  represents a  financial asset or a portfolio of financial assets which are typically modelled by a lognormal distribution when using parametric techniques \\citep[see, e.g.][]{schepper}.  In this section, we discuss how the unimodality information can be used in a straightforward fashion within the CG algorithm solution approach to obtain tighter semiparametric bounds.  Before discussing this below, it is worth mentioning that there are problems in the context to Actuarial Science where it is not appropriate to assume unimodality. For example, as shown in \\citet{LeeL10}, this is the  case when the underlying random variable is associated with Property/Casualty Losses which often exhibit a multimodal behaviour due to the combination sources compounding the loss (e.g., fire, wind, storms, hurricanes).}}\n\n\nIt has been shown by {\\citet{{popescu}}} that semiparametric bound problems with the additional constrain of the underlying distribution being unimodal can also be reformulated as a SDP by calling upon the classical probability result \nby {\\citet{{khintchine}}} regarding the representation of unimodal distributions. \nSpecifically, {\\citet{{khintchine}}} proved that any unimodal distribution can be represented by a mixture of uniform distributions each of which have {\\mbox{$M$} } as an endpoint (either the right or left endpoint). This same result can be embedded in the framework of Section~\\ref{sec:method} by leveraging the variable transformation of Section~\\ref{sec:transform}.\n\n\nRecall that the CG algorithm can be defined in terms of mixing distributions $H_x$,\nwhere $x$ represents a parameter of the distribution. In particular, for given (mode) {\\mbox{$M$} }$\\in {\\mathbb{R}}$, let\n\n", "itemtype": "equation", "pos": 22946, "prevtext": "\nThis means that with the additional distribution mixture constrain, the associated semiparametric\nbound problem can be solved with the CG Algorithm~\\ref{fig:alg}\nafter replacing \n\n", "index": 9, "text": "\\begin{align}\nf(x) &\\to \\int_0^{\\infty} f(u) H_x(u)du = E_{H_x}(f),\\nonumber \\\\\ng_j(x) &\\to  \\int_0^{\\infty} g_j(u) H_x(u)du =  E_{H_x}(g_j)\\label{eq:trans},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f(x)\" display=\"inline\"><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\to\\int_{0}^{\\infty}f(u)H_{x}(u)du=E_{H_{x}}(f),\" display=\"inline\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>H</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>E</mi><msub><mi>H</mi><mi>x</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g_{j}(x)\" display=\"inline\"><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\to\\int_{0}^{\\infty}g_{j}(u)H_{x}(u)du=E_{H_{x}}(g_{j}),\" display=\"inline\"><mrow><mrow><mi/><mo>\u2192</mo><mrow><mstyle displaystyle=\"true\"><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi mathvariant=\"normal\">\u221e</mi></msubsup></mstyle><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>H</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>u</mi></mrow></mrow></mrow><mo>=</mo><mrow><msub><mi>E</mi><msub><mi>H</mi><mi>x</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>g</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nUsing $H_x$ above in {(\\ref{{eq:trans}})} to transform the semiparametric bound problem~{(\\ref{{eq:ub}})} will lead to a bound\nover distributions that are unimodal with mode ${\\mbox{$M$} }$. \n\n\nThe simple transformation {(\\ref{{eq:trans}})} using the mixture of uniforms {(\\ref{{eq:unifh}})} allows the CG approach to leverage the results of {\\citet{{khintchine}}} while avoiding a complex reformulation as in the case of the SDP methodology of {\\citet{{popescu}}}. Enforcing unimodality is a straightforward special case that highlights the flexibility of the methodology discussed in Section~\\ref{sec:method}.\n\n\n\\subsection{Smoothness and Unimodality}\n\\label{sec:cont}\nThe base method of Section~\\ref{sec:method} computes the desired semiparametric bounds, and provides a discrete (atomic) worst/best-case distribution $(x,p_x)$ for all $x \\in J$ associated with the bound.\nIn practice it is more desirable and intuitive to work with a continuous probability density function. If one is considering a problem measuring financial loss, then having discrete loss values may not provide the insight that a continuous probability density function would, given that a discrete collection of outcomes is highly unrealistic.\nUsing the uniform mixture defined in \\eqref{eq:unifh} is guaranteed to yield a unimodal distribution in the computation of the semiparametric bounds {(\\ref{{eq:ub}})}. However, the resulting density will contain multiple discontinuities including at the mode itself. Furthermore, the density will only be nonzero over the interval $[\\min\\{x: x \\in J^*\\},\\max\\{x: x \\in J^*\\}]$; that is, it has finite support. \nIt would be desirable to obtain worst/best-case distributions associated with the semiparametric bounds that are {\\em smooth}; that is, both continuous and differentiable.\n\nBelow we show that by appropriately choosing\nthe distribution $H_x$ (and its parameters) in the mixture, it is possible to \nobtain worst/best-case distributions that are both smooth and unimodal, and that closely replicate the corresponding upper (best) and lower (worst) semiparametric bounds.\nThis can be readily done using the CG approach by reformulating the \nsemiparametric bound problem~{(\\ref{{eq:ub}})} using the transformation~{(\\ref{{eq:trans}})}, and choosing \n\n", "itemtype": "equation", "pos": 27988, "prevtext": "\nfor $j=1,\\dots,m$ in {(\\ref{{eq:mj}})}, and {(\\ref{{eq:srt}})}. \n\nNote that in many instances, the expectations\nin {(\\ref{{eq:trans}})} can be computed in closed-form as a function of the mixture\ndistribution parameter $x$. Moreover, the expectations in~{(\\ref{{eq:trans}})} are commonly\npiecewise polynomials in $x$ (e.g., if $H_x$ is a uniform distribution between $0$ and $x$),\nor can be written as polynomials after an appropriate change in variable\n(e.g., if $H_x$ is a lognormal distribution with chosen volatility parameter $\\sigma \\in {\\mathbb{R}}^+$, and mean $e^{x+\\frac{1}{2}\\sigma^2}$). In such cases, after applying the transformation {(\\ref{{eq:trans}})} the \nobjective of subproblem~{(\\ref{{eq:srt}})} will be a piecewise polynomial on $x$. As discussed\nin Section~\\ref{sec:sub}, the subproblem can then be solved ``exactly'', \nand Remark~\\ref{rem:LPs} will still hold as long as Assumption~\\ref{assume} is valid after\nthe transformation~{(\\ref{{eq:trans}})}.\nThis is illustrated in Example~\\ref{ex} below. Also, as we show with numerical experiments in Section~\\ref{sec:num}, this is the case in most\npractical applications.\n\n\n{\\color{black} { \\begin{example} \\label{ex} Consider a simple insurance policy with  no deductilble  on a loss $X$ for which the non-central  moments up to $m$-order are assumed to be known. Specifically,  let $f(x) = \\max\\{0,x\\}$, and $g_j(x) = x^j$, $j=1,\\dots,m$. Also, assume that the distribution of the loss $X$ is known to be a mixture of uniform distributions $H_x$ of the form $H_x \\sim \\operatorname{Uniform}(0,x)$ in {(\\ref{{eq:mixh}})}.  That is, $H_x(u) = \\frac{1}{x}\\mathbb{I}_{[0,x]}(u)$. From~{(\\ref{{eq:trans}})}, it follows that $E_{H_x}(f) = \\frac{x}{2}$, $E_{H_x}(g_j) = \\frac{1}{j+1}x^{j}$ for $j=1,\\dots,m$, and Remark~\\ref{rem:LPs} will hold for any $m \\le 4$. \\end{example} }}\n\nIn other cases; that is, when Assumption~\\ref{assume} does not hold after the transformation~{(\\ref{{eq:trans}})}, one can sharply approximate the expectations\nin {(\\ref{{eq:trans}})} using up to fifth-degree piecewise polynomials on~$x$ to take advantage of\nRemark~\\ref{rem:LPs}. Alternatively, given that the subproblem~{(\\ref{{eq:srt}})} is an univariate optimization problem,\nglobal optimization solvers such as {\\tt BARON} (cf., {\\citet{{ts:05}}}) can be used to effectively solve it.\n\nIn Section~\\ref{sec:unimod} we will discuss how the mixture transformation {(\\ref{{eq:trans}})}\ncan be used to substantially strengthen semiparametric bounds by using reasonable\nassumptions about the \nunderlying risk distribution regarding unimodality, and or continuity by using\na mixture of appropriate distributions. Moreover, in Section~\\ref{sec:worstcase}, we use this transformation\nto construct reasonable worst/best-case distributions associated to a given semiparametric\nbound problem.\n\n\n\n\\subsection{Unimodality}\n\n\\label{sec:unimod}\n\n{\\color{black} {In many instances of the semiparametric bound problem, it might be reasonable to assume that the  unknown distribution $\\pi$ of $X$ in {(\\ref{{eq:ub}})} is unimodal with known mode {\\mbox{$M$} }. This is particularly the case  when  the underlying random variable  represents a  financial asset or a portfolio of financial assets which are typically modelled by a lognormal distribution when using parametric techniques \\citep[see, e.g.][]{schepper}.  In this section, we discuss how the unimodality information can be used in a straightforward fashion within the CG algorithm solution approach to obtain tighter semiparametric bounds.  Before discussing this below, it is worth mentioning that there are problems in the context to Actuarial Science where it is not appropriate to assume unimodality. For example, as shown in \\citet{LeeL10}, this is the  case when the underlying random variable is associated with Property/Casualty Losses which often exhibit a multimodal behaviour due to the combination sources compounding the loss (e.g., fire, wind, storms, hurricanes).}}\n\n\nIt has been shown by {\\citet{{popescu}}} that semiparametric bound problems with the additional constrain of the underlying distribution being unimodal can also be reformulated as a SDP by calling upon the classical probability result \nby {\\citet{{khintchine}}} regarding the representation of unimodal distributions. \nSpecifically, {\\citet{{khintchine}}} proved that any unimodal distribution can be represented by a mixture of uniform distributions each of which have {\\mbox{$M$} } as an endpoint (either the right or left endpoint). This same result can be embedded in the framework of Section~\\ref{sec:method} by leveraging the variable transformation of Section~\\ref{sec:transform}.\n\n\nRecall that the CG algorithm can be defined in terms of mixing distributions $H_x$,\nwhere $x$ represents a parameter of the distribution. In particular, for given (mode) {\\mbox{$M$} }$\\in {\\mathbb{R}}$, let\n\n", "index": 11, "text": "\\begin{equation} \\label{eq:unifh} H_x \\sim {\\operatorname{\\operatorname{Uniform}}}(\\min\\{x,{\\mbox{$M$} }\\}, \\max\\{x,{\\mbox{$M$} }\\}). \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"H_{x}\\sim{\\operatorname{\\operatorname{Uniform}}}(\\min\\{x,{\\mbox{$M$}}\\},\\max\\{%&#10;x,{\\mbox{$M$}}\\}).\" display=\"block\"><mrow><mrow><msub><mi>H</mi><mi>x</mi></msub><mo>\u223c</mo><mrow><mo>Uniform</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">}</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nwhere $\\mu_x, \\sigma_x$ are given in terms of $x$ by the equations\n\n", "itemtype": "equation", "pos": 30413, "prevtext": "\nUsing $H_x$ above in {(\\ref{{eq:trans}})} to transform the semiparametric bound problem~{(\\ref{{eq:ub}})} will lead to a bound\nover distributions that are unimodal with mode ${\\mbox{$M$} }$. \n\n\nThe simple transformation {(\\ref{{eq:trans}})} using the mixture of uniforms {(\\ref{{eq:unifh}})} allows the CG approach to leverage the results of {\\citet{{khintchine}}} while avoiding a complex reformulation as in the case of the SDP methodology of {\\citet{{popescu}}}. Enforcing unimodality is a straightforward special case that highlights the flexibility of the methodology discussed in Section~\\ref{sec:method}.\n\n\n\\subsection{Smoothness and Unimodality}\n\\label{sec:cont}\nThe base method of Section~\\ref{sec:method} computes the desired semiparametric bounds, and provides a discrete (atomic) worst/best-case distribution $(x,p_x)$ for all $x \\in J$ associated with the bound.\nIn practice it is more desirable and intuitive to work with a continuous probability density function. If one is considering a problem measuring financial loss, then having discrete loss values may not provide the insight that a continuous probability density function would, given that a discrete collection of outcomes is highly unrealistic.\nUsing the uniform mixture defined in \\eqref{eq:unifh} is guaranteed to yield a unimodal distribution in the computation of the semiparametric bounds {(\\ref{{eq:ub}})}. However, the resulting density will contain multiple discontinuities including at the mode itself. Furthermore, the density will only be nonzero over the interval $[\\min\\{x: x \\in J^*\\},\\max\\{x: x \\in J^*\\}]$; that is, it has finite support. \nIt would be desirable to obtain worst/best-case distributions associated with the semiparametric bounds that are {\\em smooth}; that is, both continuous and differentiable.\n\nBelow we show that by appropriately choosing\nthe distribution $H_x$ (and its parameters) in the mixture, it is possible to \nobtain worst/best-case distributions that are both smooth and unimodal, and that closely replicate the corresponding upper (best) and lower (worst) semiparametric bounds.\nThis can be readily done using the CG approach by reformulating the \nsemiparametric bound problem~{(\\ref{{eq:ub}})} using the transformation~{(\\ref{{eq:trans}})}, and choosing \n\n", "index": 13, "text": "\\begin{equation}\n\\label{eq:logmix}\nH_x \\sim \\operatorname{lognormal}(\\mu_x, \\sigma_x),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"H_{x}\\sim\\operatorname{lognormal}(\\mu_{x},\\sigma_{x}),\" display=\"block\"><mrow><mrow><msub><mi>H</mi><mi>x</mi></msub><mo>\u223c</mo><mrow><mo>lognormal</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03bc</mi><mi>x</mi></msub><mo>,</mo><msub><mi>\u03c3</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nfor a given $\\alpha \\in {\\mathbb{R}}^+$. That is, the lognormal distribution $H_x$ is set to have a mean of $x$, and variance~$\\alpha^2$.\nNote that besides the mean parameter $x$, which will be used to construct the mixture using the CG Algorithm~\\ref{fig:alg}, one needs to set a second parameter $\\alpha$ in {(\\ref{{eq:lnparam}})} to properly define the \nlognormal distribution $H_x$ in {(\\ref{{eq:logmix}})}.\n\nThe  lognormal mixture approach (i.e., {(\\ref{{eq:logmix}})}, and {(\\ref{{eq:trans}})}) can be used to obtain solutions to the semiparametric bound problem {(\\ref{{eq:ub}})} where the underlying worst/best-case distribution is both unimodal, and smooth,\nand replicates as close as possible the semiparametic bound obtained when the distribution is assumed to be unimodal (and not necessarily smooth). This is in part thanks to the additional degree of freedom given by the choice of the parameter $\\alpha$ in {(\\ref{{eq:lnparam}})}.\n\n\n\n\n\n\n\n\n\n\n{\\color{black} {To see this, let us refer to  \n", "itemtype": "equation", "pos": 30582, "prevtext": "\nwhere $\\mu_x, \\sigma_x$ are given in terms of $x$ by the equations\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eq:lnparam}\n\\begin{array}{rl}\ne^{\\mu_x+ \\frac{1}{2}\\sigma_x^2} & = x,\\\\\n(e^{\\sigma_x^2}-1)e^{2\\mu_x +\\sigma_x^2} & = \\alpha^2.\\\\\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{rl}e^{\\mu_{x}+\\frac{1}{2}\\sigma_{x}^{2}}&amp;=x,\\\\&#10;(e^{\\sigma_{x}^{2}}-1)e^{2\\mu_{x}+\\sigma_{x}^{2}}&amp;=\\alpha^{2}.\\\\&#10;\\end{array}\" display=\"block\"><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msup><mi>e</mi><mrow><msub><mi>\u03bc</mi><mi>x</mi></msub><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>x</mi><mn>2</mn></msubsup></mrow></mrow></msup></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mi>x</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>e</mi><msubsup><mi>\u03c3</mi><mi>x</mi><mn>2</mn></msubsup></msup><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>x</mi></msub></mrow><mo>+</mo><msubsup><mi>\u03c3</mi><mi>x</mi><mn>2</mn></msubsup></mrow></msup></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><msup><mi>\u03b1</mi><mn>2</mn></msup></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": " and assume that $\\tilde{\\mu}^+, \\tilde{\\mu}^-, \\tilde{\\sigma}^2$ are bounded (i.e., this  will be the case if $g_j(X) = X^j$, $j=1,\\dots,m$, with $m \\ge 2$ in {(\\ref{{eq:ub}})}), and that ${\\mathcal{D}} \\subseteq {\\mathbb{R}}^+$ (as in practice).  Clearly, for the lognormal distribution mixture~{(\\ref{{eq:logmix}})} to be feasible for the semiparametric bound problem~{(\\ref{{eq:ub}})}, $\\alpha$ in {(\\ref{{eq:lnparam}})} should be chosen such that $\\alpha^2 \\le \\tilde{\\sigma}^2$ to ensure that the variance of the lognormal distribution used for the mixture is less than the maximum possible variance of the probability distributions $\\pi$ associated with the expected value constrains in {(\\ref{{eq:ub}})}.   Moreover, as $\\alpha \\to \\tilde{\\sigma}$. That is, the only feasible solution of the semiparametric bound problem with lognormal mixture would be a (single) lognormal with variance $\\alpha^2$, and mean $x$ satisfying $\\tilde{\\mu}^- \\le x \\le \\tilde{\\mu}^+$, which is unimodal. Thus, there exists an $\\alpha \\in [0,\\tilde{\\sigma}]$, such that the lognormal mixture obtained with the CG Algorithm~\\ref{fig:alg} will be unimodal. To find the value of $\\alpha$ such that the lognormal mixture obtained with the CG approach is both unimodal and as close as possible to replicate the semiparametric bound obtained by assuming that the probability distribution $\\pi$ in {(\\ref{{eq:ub}})} is unimodal (and not necessarily smooth), one can use the {\\em bisection} Algorithm~\\ref{fig:unimodbis}.}}\n\n\\begin{algorithm}[!htb]\n\\caption{Smooth and unimodal worst/best-case distribution}\n\\begin{algorithmic}[1]\n\\Procedure{Bisection}{$0 < \\alpha_{lo} < \\alpha_{hi} < \\tilde{\\sigma}$, $\\epsilon >0$}\n\\While{$|\\alpha_{hi} - \\alpha_{lo}| > \\epsilon$}\n\\State $\\alpha_{k} \\gets \\frac{1}{2}(\\alpha_{lo}+\\alpha_{hi})$\n\\State {\\bf compute} $J^*$, $p^* := \\{p^*_x\\}_{x\\in J}$, using CG Algorithm~\\ref{fig:alg} and $H_x$ in {(\\ref{{eq:logmix}})}\n\n\\If{$\\pi \\sim \\sum_{x \\in J^*} p^*_x H_x$ is unimodal} \\State{$\\alpha_{hi} \\gets \\alpha_k$}\n\\Else  \\State{$\\alpha_{lo} \\gets \\alpha_k$} \\EndIf\n\\EndWhile\n\\State  {\\bf return} $J^*$, $p^*$, $\\alpha = \\alpha_k$, and $M_{J}^*$\n\n\\EndProcedure\n\\end{algorithmic}\n\\label{fig:unimodbis}\n\\end{algorithm}\n\n\n\nNote that in the discussion above, the choice of the lognormal distribution is not key. Instead, the same would apply as long as the mixture distribution in {(\\ref{{eq:logmix}})} is smooth, unimodal, and has at least two appropriate degrees of freedom in the choice of parameters (e.g., in case of random variables with support on the whole real line, the normal distribution could be used to form the mixture).\nIn Section~\\ref{sec:worstcase}, we illustrate with a numerical example how the {\\em bisection} Algorithm~\\ref{fig:unimodbis} can\nbe used to obtain a smooth and unimodal worst-case distribution that closely replicates the behaviour of the worst-case unimodal distribution.\n\n\nWhen using a smooth distribution to define the mixture component $H_x$ in {(\\ref{{eq:trans}})},\nit is important to understand the impact of the selection of mixture components $H_x$. Ideally, computing the bound with a mixture of smooth distributions $H_x$ would yield the optimal value across all possible smooth distributions in the semiparametric bound problem~{(\\ref{{eq:ub}})}. Instead, it is the semiparametric bound across all mixtures with components $H_x$. However, in Theorem~\\ref{thm:unifcv} below, we show that the optimal semiparametric bounds across all smooth unimodal distributions is the \nsame as the one across unimodal distributions. Loosely speaking, this\nfollows from the fact that the density function of a uniform distribution can be \narbitrarily approximated by an appropriate smooth density function.\n\n\n\n\n\n\\begin{theorem}\n\t\\label{thm:unifcv}\n\tThe semiparametric bound problem~{(\\ref{{eq:ub}})}, with the additional constrain of the underlying distribution being smooth and unimodal  is equivalent to problem~{(\\ref{{eq:ub}})}, \nwith the additional constrain\t of the underlying distribution being unimodal.\n\\eqref{eq:unifh}.\n\\end{theorem}\n\\begin{proof}\nLet $B^*_u$ be the bound corresponding to the semiparametric bound problem~{(\\ref{{eq:ub}})}, \nwith the additional constrain\t that the underlying distribution $\\pi$ is unimodal.\nNote that there exists a distribution $\\pi^*$ such\nthat  $B^*_u := E_{\\pi^*}(f(X))$ and $\\pi^*$ is a mixture of uniform distributions (cf., Section~\\ref{sec:unimod}); that is, with $H_x\\sim {\\operatorname{\\operatorname{Uniform}}}(\\min\\{x,{\\mbox{$M$} }\\}, \\max\\{x,{\\mbox{$M$} }\\})$ in {(\\ref{{eq:trans}})}. Now, for $\\eta >0$, let $\\pi^\\eta$ be the mixture obtained after replacing $H_x$ by\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nfor a given $\\alpha \\in {\\mathbb{R}}^+$. That is, the lognormal distribution $H_x$ is set to have a mean of $x$, and variance~$\\alpha^2$.\nNote that besides the mean parameter $x$, which will be used to construct the mixture using the CG Algorithm~\\ref{fig:alg}, one needs to set a second parameter $\\alpha$ in {(\\ref{{eq:lnparam}})} to properly define the \nlognormal distribution $H_x$ in {(\\ref{{eq:logmix}})}.\n\nThe  lognormal mixture approach (i.e., {(\\ref{{eq:logmix}})}, and {(\\ref{{eq:trans}})}) can be used to obtain solutions to the semiparametric bound problem {(\\ref{{eq:ub}})} where the underlying worst/best-case distribution is both unimodal, and smooth,\nand replicates as close as possible the semiparametic bound obtained when the distribution is assumed to be unimodal (and not necessarily smooth). This is in part thanks to the additional degree of freedom given by the choice of the parameter $\\alpha$ in {(\\ref{{eq:lnparam}})}.\n\n\n\n\n\n\n\n\n\n\n{\\color{black} {To see this, let us refer to  \n", "index": 17, "text": "\\begin{equation} \\label{eq:moms} \\begin{array}{l} \\tilde{\\mu}^+ := \\displaystyle \\sup_{\\pi \\text{ a p.d. on ${\\mathcal{D}}$}} \\{E(X)~|~\\sigma_j^- \\le E_{\\pi}(g_j(X)) \\le \\sigma_j^+, j=1,\\dots,m\\},\\\\ \\tilde{\\mu}^- :=  \\displaystyle \\inf_{\\pi \\text{ a p.d. on ${\\mathcal{D}}$}} \\{E(X)~|~\\sigma_j^- \\le E_{\\pi}(g_j(X)) \\le \\sigma_j^+, j=1,\\dots,m\\},\\\\ \\tilde{\\sigma}^2 :=  \\displaystyle \\sup_{\\pi \\text{ a p.d. on ${\\mathcal{D}}$}} \\{\\operatorname{Var}(X)~|~\\sigma_j^- \\le E_{\\pi}(g_j(X)) \\le \\sigma_j^+, j=1,\\dots,m\\},\\\\ \\end{array} \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\tilde{\\mu}^{+}:=\\displaystyle\\sup_{\\pi\\text{ a p.d. on ${%&#10;\\mathcal{D}}$}}\\{E(X)~{}|~{}\\sigma_{j}^{-}\\leq E_{\\pi}(g_{j}(X))\\leq\\sigma_{j}%&#10;^{+},j=1,\\dots,m\\},\\\\&#10;\\tilde{\\mu}^{-}:=\\displaystyle\\inf_{\\pi\\text{ a p.d. on ${\\mathcal{D}}$}}\\{E(X%&#10;)~{}|~{}\\sigma_{j}^{-}\\leq E_{\\pi}(g_{j}(X))\\leq\\sigma_{j}^{+},j=1,\\dots,m\\},%&#10;\\\\&#10;\\tilde{\\sigma}^{2}:=\\displaystyle\\sup_{\\pi\\text{ a p.d. on ${\\mathcal{D}}$}}\\{%&#10;\\operatorname{Var}(X)~{}|~{}\\sigma_{j}^{-}\\leq E_{\\pi}(g_{j}(X))\\leq\\sigma_{j}%&#10;^{+},j=1,\\dots,m\\},\\\\&#10;\\end{array}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msup><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">~</mo></mover><mo>+</mo></msup><mo>:=</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mtext>\u00a0a p.d. on\u00a0</mtext><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2264</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup></mrow><mo>,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msup><mover accent=\"true\"><mi>\u03bc</mi><mo stretchy=\"false\">~</mo></mover><mo>-</mo></msup><mo>:=</mo><mrow><munder><mo movablelimits=\"false\">inf</mo><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mtext>\u00a0a p.d. on\u00a0</mtext><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2264</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup></mrow><mo>,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msup><mover accent=\"true\"><mi>\u03c3</mi><mo stretchy=\"false\">~</mo></mover><mn>2</mn></msup><mo>:=</mo><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><mi>\u03c0</mi><mo>\u2062</mo><mrow><mtext>\u00a0a p.d. on\u00a0</mtext><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo>Var</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo rspace=\"5.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">|</mo><mrow><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2264</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup></mrow><mo>,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nin the mixture $\\pi^*$, where $a = \\min\\{x, {\\mbox{$M$} }\\}$ and $b = \\max\\{x,{\\mbox{$M$} }\\}$, where ${\\mbox{$M$} }$ is the mode of $\\pi^*$. The statement follows since by letting $\\eta \\to \\infty$, one obtains a smooth distribution $H^\\eta_x$ (see, Lemma~\\ref{totpro} in Appendix~\\ref{app:approx}) that is arbitrarily close to $H_x$ (see, Lemma~\\ref{l:u} in Appendix~\\ref{app:approx}) .\n\\end{proof}\n\nA numerical example to illustrate Theorem~\\ref{thm:unifcv} is provided in Section~\\ref{sec:worstcase}.\n\n\n\n\n\\section{Numerical Illustration}\n\\label{sec:num}\n\nProblem \\eqref{eq:ub} is of particular interest in actuarial science because the target function $f(\\cdot)$ in {(\\ref{{eq:ub}})} can take the form of payoffs for common insurance and risk management products for which the distribution of the underlying random loss is {\\em ambiguous} \\citep[see, e.g.,][for a recent reference]{Ye10, Nata11}; that is, it is not known precisely. Let $X$ represent the loss, and $d$ \nbe the deductible associated with an insurance policy on $X$. For example, \\citet[][Sections 3.1 and 3.2]{schepper}\nprovide upper and lower bounds on the expected cost per policy payment $\\max(X-d,0)$ when only\nup to third-order moment information on the loss distribution is assumed to be known. This is done by solving {(\\ref{{eq:ub}})}  analytically with \n\n", "itemtype": "equation", "pos": 36972, "prevtext": " and assume that $\\tilde{\\mu}^+, \\tilde{\\mu}^-, \\tilde{\\sigma}^2$ are bounded (i.e., this  will be the case if $g_j(X) = X^j$, $j=1,\\dots,m$, with $m \\ge 2$ in {(\\ref{{eq:ub}})}), and that ${\\mathcal{D}} \\subseteq {\\mathbb{R}}^+$ (as in practice).  Clearly, for the lognormal distribution mixture~{(\\ref{{eq:logmix}})} to be feasible for the semiparametric bound problem~{(\\ref{{eq:ub}})}, $\\alpha$ in {(\\ref{{eq:lnparam}})} should be chosen such that $\\alpha^2 \\le \\tilde{\\sigma}^2$ to ensure that the variance of the lognormal distribution used for the mixture is less than the maximum possible variance of the probability distributions $\\pi$ associated with the expected value constrains in {(\\ref{{eq:ub}})}.   Moreover, as $\\alpha \\to \\tilde{\\sigma}$. That is, the only feasible solution of the semiparametric bound problem with lognormal mixture would be a (single) lognormal with variance $\\alpha^2$, and mean $x$ satisfying $\\tilde{\\mu}^- \\le x \\le \\tilde{\\mu}^+$, which is unimodal. Thus, there exists an $\\alpha \\in [0,\\tilde{\\sigma}]$, such that the lognormal mixture obtained with the CG Algorithm~\\ref{fig:alg} will be unimodal. To find the value of $\\alpha$ such that the lognormal mixture obtained with the CG approach is both unimodal and as close as possible to replicate the semiparametric bound obtained by assuming that the probability distribution $\\pi$ in {(\\ref{{eq:ub}})} is unimodal (and not necessarily smooth), one can use the {\\em bisection} Algorithm~\\ref{fig:unimodbis}.}}\n\n\\begin{algorithm}[!htb]\n\\caption{Smooth and unimodal worst/best-case distribution}\n\\begin{algorithmic}[1]\n\\Procedure{Bisection}{$0 < \\alpha_{lo} < \\alpha_{hi} < \\tilde{\\sigma}$, $\\epsilon >0$}\n\\While{$|\\alpha_{hi} - \\alpha_{lo}| > \\epsilon$}\n\\State $\\alpha_{k} \\gets \\frac{1}{2}(\\alpha_{lo}+\\alpha_{hi})$\n\\State {\\bf compute} $J^*$, $p^* := \\{p^*_x\\}_{x\\in J}$, using CG Algorithm~\\ref{fig:alg} and $H_x$ in {(\\ref{{eq:logmix}})}\n\n\\If{$\\pi \\sim \\sum_{x \\in J^*} p^*_x H_x$ is unimodal} \\State{$\\alpha_{hi} \\gets \\alpha_k$}\n\\Else  \\State{$\\alpha_{lo} \\gets \\alpha_k$} \\EndIf\n\\EndWhile\n\\State  {\\bf return} $J^*$, $p^*$, $\\alpha = \\alpha_k$, and $M_{J}^*$\n\n\\EndProcedure\n\\end{algorithmic}\n\\label{fig:unimodbis}\n\\end{algorithm}\n\n\n\nNote that in the discussion above, the choice of the lognormal distribution is not key. Instead, the same would apply as long as the mixture distribution in {(\\ref{{eq:logmix}})} is smooth, unimodal, and has at least two appropriate degrees of freedom in the choice of parameters (e.g., in case of random variables with support on the whole real line, the normal distribution could be used to form the mixture).\nIn Section~\\ref{sec:worstcase}, we illustrate with a numerical example how the {\\em bisection} Algorithm~\\ref{fig:unimodbis} can\nbe used to obtain a smooth and unimodal worst-case distribution that closely replicates the behaviour of the worst-case unimodal distribution.\n\n\nWhen using a smooth distribution to define the mixture component $H_x$ in {(\\ref{{eq:trans}})},\nit is important to understand the impact of the selection of mixture components $H_x$. Ideally, computing the bound with a mixture of smooth distributions $H_x$ would yield the optimal value across all possible smooth distributions in the semiparametric bound problem~{(\\ref{{eq:ub}})}. Instead, it is the semiparametric bound across all mixtures with components $H_x$. However, in Theorem~\\ref{thm:unifcv} below, we show that the optimal semiparametric bounds across all smooth unimodal distributions is the \nsame as the one across unimodal distributions. Loosely speaking, this\nfollows from the fact that the density function of a uniform distribution can be \narbitrarily approximated by an appropriate smooth density function.\n\n\n\n\n\n\\begin{theorem}\n\t\\label{thm:unifcv}\n\tThe semiparametric bound problem~{(\\ref{{eq:ub}})}, with the additional constrain of the underlying distribution being smooth and unimodal  is equivalent to problem~{(\\ref{{eq:ub}})}, \nwith the additional constrain\t of the underlying distribution being unimodal.\n\\eqref{eq:unifh}.\n\\end{theorem}\n\\begin{proof}\nLet $B^*_u$ be the bound corresponding to the semiparametric bound problem~{(\\ref{{eq:ub}})}, \nwith the additional constrain\t that the underlying distribution $\\pi$ is unimodal.\nNote that there exists a distribution $\\pi^*$ such\nthat  $B^*_u := E_{\\pi^*}(f(X))$ and $\\pi^*$ is a mixture of uniform distributions (cf., Section~\\ref{sec:unimod}); that is, with $H_x\\sim {\\operatorname{\\operatorname{Uniform}}}(\\min\\{x,{\\mbox{$M$} }\\}, \\max\\{x,{\\mbox{$M$} }\\})$ in {(\\ref{{eq:trans}})}. Now, for $\\eta >0$, let $\\pi^\\eta$ be the mixture obtained after replacing $H_x$ by\n\n", "index": 19, "text": "\\begin{equation} \\label{eq:log} H^\\eta_x(u) = \\frac{1}{b(x)-a(x)} \\left[\\frac{1}{1+e^{-\\eta (u-a(x))}} - \\frac{1}{1+e^{-\\eta (u-b(x))}}\\right], \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"H^{\\eta}_{x}(u)=\\frac{1}{b(x)-a(x)}\\left[\\frac{1}{1+e^{-\\eta(u-a(x))}}-\\frac{1%&#10;}{1+e^{-\\eta(u-b(x))}}\\right],\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>H</mi><mi>x</mi><mi>\u03b7</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mo>[</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>-</mo><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nIn practice, losses do not exceed certain maximum, say $b$. Taking this into account, \\citet[][Proposition 3.2]{cox} provides upper and lower bounds on the expected cost per policy $\\max(X-d,0)$ when only the maximum potential loss $b$ and up to second-order moment information on the loss distribution is assumed to be known. Accordingly, this is done \nby solving {(\\ref{{eq:ub}})}  analytically with \n\n", "itemtype": "equation", "pos": 38464, "prevtext": "\nin the mixture $\\pi^*$, where $a = \\min\\{x, {\\mbox{$M$} }\\}$ and $b = \\max\\{x,{\\mbox{$M$} }\\}$, where ${\\mbox{$M$} }$ is the mode of $\\pi^*$. The statement follows since by letting $\\eta \\to \\infty$, one obtains a smooth distribution $H^\\eta_x$ (see, Lemma~\\ref{totpro} in Appendix~\\ref{app:approx}) that is arbitrarily close to $H_x$ (see, Lemma~\\ref{l:u} in Appendix~\\ref{app:approx}) .\n\\end{proof}\n\nA numerical example to illustrate Theorem~\\ref{thm:unifcv} is provided in Section~\\ref{sec:worstcase}.\n\n\n\n\n\\section{Numerical Illustration}\n\\label{sec:num}\n\nProblem \\eqref{eq:ub} is of particular interest in actuarial science because the target function $f(\\cdot)$ in {(\\ref{{eq:ub}})} can take the form of payoffs for common insurance and risk management products for which the distribution of the underlying random loss is {\\em ambiguous} \\citep[see, e.g.,][for a recent reference]{Ye10, Nata11}; that is, it is not known precisely. Let $X$ represent the loss, and $d$ \nbe the deductible associated with an insurance policy on $X$. For example, \\citet[][Sections 3.1 and 3.2]{schepper}\nprovide upper and lower bounds on the expected cost per policy payment $\\max(X-d,0)$ when only\nup to third-order moment information on the loss distribution is assumed to be known. This is done by solving {(\\ref{{eq:ub}})}  analytically with \n\n", "index": 21, "text": "\\begin{equation}\n\\label{eq:ex2}\nf(X) =\\max(X-d,0), g_j(X) = X^j, \\sigma_j^+ = \\sigma_j^- \\text{ for } j = 1,\\dots,m,  {\\mathcal{D}} = {\\mathbb{R}}^+, \\text{ and } m=2,3.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"f(X)=\\max(X-d,0),g_{j}(X)=X^{j},\\sigma_{j}^{+}=\\sigma_{j}^{-}\\text{ for }j=1,%&#10;\\dots,m,{\\mathcal{D}}={\\mathbb{R}}^{+},\\text{ and }m=2,3.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>-</mo><mi>d</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>X</mi><mi>j</mi></msup></mrow><mo>,</mo><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup><mo>=</mo><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo>=</mo><msup><mi>\u211d</mi><mo>+</mo></msup></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>m</mi></mrow><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\n\n\n\n\n\\subsection{Second-order LER Bounds with Unimodality} \\label{sec:ex1m2}\n\nLet us reconsider the semiparametric bound on the expected cost per policy defined in~{(\\ref{{eq:ex1}})}. Note that from semiparametric bounds on the expected cost per policy, one can readily obtain bounds on the expected {\\em loss elimination ratio} (LER) of the policy. Specifically, note that the expected LER associated with a policy with payoff $\\max\\{0, X-d\\}$ is \\citep[cf.,][]{cox}\n\n", "itemtype": "equation", "pos": 39052, "prevtext": "\nIn practice, losses do not exceed certain maximum, say $b$. Taking this into account, \\citet[][Proposition 3.2]{cox} provides upper and lower bounds on the expected cost per policy $\\max(X-d,0)$ when only the maximum potential loss $b$ and up to second-order moment information on the loss distribution is assumed to be known. Accordingly, this is done \nby solving {(\\ref{{eq:ub}})}  analytically with \n\n", "index": 23, "text": "\\begin{equation}\n\\label{eq:ex1}\nf(X) =\\max(X-d,0), g_j(X) = X^j,\\sigma_j^+ = \\sigma_j^- \\text{ for } j = 1,\\dots,m,  {\\mathcal{D}} = [0,b] \\subset {\\mathbb{R}}^+, \\text{ and } m=2.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"f(X)=\\max(X-d,0),g_{j}(X)=X^{j},\\sigma_{j}^{+}=\\sigma_{j}^{-}\\text{ for }j=1,%&#10;\\dots,m,{\\mathcal{D}}=[0,b]\\subset{\\mathbb{R}}^{+},\\text{ and }m=2.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>-</mo><mi>d</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>X</mi><mi>j</mi></msup></mrow><mo>,</mo><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>+</mo></msubsup><mo>=</mo><mrow><msubsup><mi>\u03c3</mi><mi>j</mi><mo>-</mo></msubsup><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>b</mi><mo stretchy=\"false\">]</mo></mrow><mo>\u2282</mo><msup><mi>\u211d</mi><mo>+</mo></msup></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>m</mi></mrow><mo>=</mo><mn>2</mn></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nBeing able to compute bounds on the expected LER would be beneficial for an insurer attempting to set a deductible in cases where the actual loss distribution is ambiguous. For example, \nin \\citet[][Section 3]{cox}, the relationship {(\\ref{{eq:lerdef}})} is used to obtain upper and lower semiparametric bounds on the \nexpected LER of an insurance policy with deductible, assuming only the knowledge of the mean and variance of the loss, and that the loss cannot exceed a known maximum. \n\nAnother sensible premise is to assume that the loss distribution is unimodal. To illustrate the potential of the CG approach, in Figure~\\ref{fig:ler}, we use the mixture transformation of Section~\\ref{sec:unimod} to compute upper and lower semiparametric bounds on the insurance policy with payoff $\\max\\{0, X-d\\}$ when the mean $\\mu$ and variance $\\sigma^2$ of the loss are assumed to be known, and the loss cannot exceed the value of $b$, where $d$ is the policy's deductible, and the loss distribution is also assumed to be unimodal with mode {\\mbox{$M$} }. The results are compared with the analytical formula of \\citet[][Section3]{cox} to illustrate the tightening of the bounds obtained by adding the unimodality assumption. Specifically, following \\citet[Section 4]{cox}, in Figure~\\ref{fig:ler} we set $\\mu = 50$, $\\sigma = 15$, $b=100$, and ${\\mbox{$M$} } = \\{45, 50\\}$.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.475\\textwidth]{Figure1.eps} \\includegraphics[width = 0.475\\textwidth]{Figure2.eps} \n\n\t\\caption{Expected LER bounds (left) and gaps (right) for different values of the deductible $d$, when the mean $\\mu = 50$, and variance \n\t$\\sigma^2 = 225$ of the underlying loss, as well as its potential maximum value $b=100$, are assumed to be known. Gaps indicates the difference between upper and lower bounds. Results are presented for bounds without the unimodality constrain, and with unimodality constrain with mode ${\\mbox{$M$} } = \\{45, 50\\}$.}\n\t\\label{fig:ler}\n\\end{figure}\n\nObserve in Figure~\\ref{fig:ler} that the expected LER {\\em gap}; that is, the difference between the upper and lower semiparametric expected LER's bounds, is significantly tighter when unimodality is included. When ${\\mbox{$M$} } = 50$ the gap is symmetrical with a small spike at the mean/mode. The case in which ${\\mbox{$M$} } = 45$ yields a corresponding peak in the gap around the mode. \nFor either very high or very low\ndeductible values, the choice of the mode has little impact on the size of the gap. \nRegardless of the mode's value,\nthe gaps are of similar magnitude and narrower than in the case where the underlying loss distribution is not assumed to be unimodal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Examining worst-case distributions} \\label{sec:worstcase}\n\n\n\nSuppose we wish to compute the semiparametric upper bound defined by {(\\ref{{eq:ex2}})} with $m=2$. \nSpecifically, let the expected value (moment) constrains in {(\\ref{{eq:ub}})} be given by\n\n", "itemtype": "equation", "pos": 39715, "prevtext": "\n\n\n\n\n\\subsection{Second-order LER Bounds with Unimodality} \\label{sec:ex1m2}\n\nLet us reconsider the semiparametric bound on the expected cost per policy defined in~{(\\ref{{eq:ex1}})}. Note that from semiparametric bounds on the expected cost per policy, one can readily obtain bounds on the expected {\\em loss elimination ratio} (LER) of the policy. Specifically, note that the expected LER associated with a policy with payoff $\\max\\{0, X-d\\}$ is \\citep[cf.,][]{cox}\n\n", "index": 25, "text": "\\begin{equation} \\label{eq:lerdef} E({\\rm LER}(X)) = \\frac{E(\\min(X,d))}{E(X)} =  \\frac{E(X) - E(\\max(X-d,0))}{E(X)}.\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"E({\\rm LER}(X))=\\frac{E(\\min(X,d))}{E(X)}=\\frac{E(X)-E(\\max(X-d,0))}{E(X)}.\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>LER</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>-</mo><mi>d</mi></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nfor given $\\mu, \\sigma \\in {\\mathbb{R}}^+$. That is, both the mean and the variance of the underlying loss distribution\nare assumed to be known. In this case, equation {(\\ref{{eq:moms}})} reduces to\n$\\mu_\\pi^+ = \\mu_\\pi^- = \\mu$, and $\\sigma_\\pi^2 = \\sigma^2$.\n\nA closed-form solution for the corresponding semiparametric upper bound problem was derived by  {\\citet{{lo}}}, where he considers $f$ in {(\\ref{{eq:ex2}})} as the payoff of an European call option with strike $d$. If furthermore, it is assumed that the underlying distribution $\\pi$ of the loss (or asset price) is unimodal, the corresponding semiparametric upper bound can be computed using: \nthe analytical formula provided by \\citet[][Section 3.3]{schepper}, the SDP techniques provided by \\citet{popescu}, \nor the uniform mixture  approach of Section~\\ref{sec:unimod} with components \\eqref{eq:unifh}. The CG uniform mixture method readily provides a worst-case distribution. This distribution, however, is not smooth, has finite support, and is unrealistic as a model for the uncertainty of losses. For this reason we compute upper bounds using smooth mixture compontents in {(\\ref{{eq:trans}})} and inspect the resulting worst-case probability and cumulative density functions. The resulting smooth distribution is then compared to the optimal unimodal uniform mixture distribution. Specifically, we use the lognormal mixture {(\\ref{{eq:logmix}})}. \n\nIn particular, let us sample the values of $\\mu, \\sigma$ in {(\\ref{{eq:const}})} from a lognormal asset price dynamics model which is also commonly used to model (a non-ambiguous) loss distribution \\citep[see, e.g.][]{Cox2004, Jaimungal2006}. Namely, let\n$\\mu = X_0e^{rT}$, and $\\sigma= X_0e^{rT}(e^{\\nu^2T}-1)^{\\frac12}$ for values of $X_0 = 49.50$, $r = 1\\%$, $\\nu = 20\\%$, and $T = 1$.\nAlso let $d = X_0$ in {(\\ref{{eq:ex2}})}; that is, consider a policy where the expected value of the loss is equal to the deductible.\n\nThe semiparametric upper bound was computed \nusing the lognormal mixture {(\\ref{{eq:logmix}})} for different values of $\\alpha \\in [1, 1.5, \\ldots, 20]$ and the percent above the parametric value of the policy based on Black-Scholes formula was plotted in Figure~\\ref{fig:byalpha}. The corresponding semiparametric bound without the unimodality assumption (given by {\\citet{{lo}}}), and unimodal bounds with an uniform mixture from Section~\\ref{sec:unimod}   are also plotted for reference.\nThe bold point in Figure~\\ref{fig:byalpha}\n \n represents the smooth, unimodal bound obtained with the bisection Algorithm~\\ref{fig:unimodbis} and a mixture of lognormal distributions. Also, the plot illustrates the point ($\\alpha = 11.34$) at which the the bounds obtained by the bisection Algorithm~\\ref{fig:unimodbis} and the CG Algorithm~\\ref{fig:alg} with a mixture of uniform distributions are equal.\n\nIn Figure~\\ref{fig:byalpha} one can observe that for extremely low values of $\\alpha$, the component distributions of the mixture are very narrow, approaching the pessimistic discrete distribution case associated with closed-form bound of {\\citet{{lo}}}. We also see that as $\\alpha \\rightarrow \\sigma = 20.4$ the resulting bound distribution converges to the lognormal specified by the Black and Scholes asset pricing framework. This convergence is seen in Figure~\\ref{fig:byalpha} since the error goes to zero and the upper bound price equals the analytical Black-Scholes price.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.5\\textwidth]{Figure3.eps}\n\t\\caption{Percentage above the parametric Black and Scholes price of the \\citet{lo} upper bound (Lo's Bound) and the lognormal mixtures obtained from Algorithm~\\ref{fig:unimodbis} (lognormal Mixture Bounds).  The bold point denotes the value of $\\alpha = 13.75$ in which the lognormal mixture bound obtained from Algorithm~\\ref{fig:unimodbis} produces a unimodal distribution.} \n\t\n\t\\label{fig:byalpha}\n\\end{figure}\n\nFigure~\\ref{fig:byalpha} also highlights the result discussed in Theorem~\\ref{thm:unifcv}. The bound computed using uniform mixture components is greater than the unimodal bound from the lognormal mixture with the gap size under 4\\%. Note that the unimodal upper bound using lognormal mixture components occurred at $\\alpha = 13.75$. As mentioned before, the $\\alpha$ that yields the same bound value as that from the uniform mixture is $\\alpha = 11.34$. The smaller the $\\alpha$ in the lognormal mixture {(\\ref{{eq:logmix}})} the higher the conservatism associated with the semiparametric bound. Figure~\\ref{fig:df} shows the probability distribution function (PDF) and cumulative distribution function (CDF) of the lognormal mixtures at $\\alpha = \\{11.34, 13.75\\}$ as well as the associated {\\em true} lognormal distribution with mean and variance equal to the moments used to compute the semiparametric bounds. To highlight the advantage of using the lognormal mixtures instead of the uniform mixtures, Figure~\\ref{fig:pcdfpop} shows the optimal PDF and CDF of the latter along with the associated true lognormal distribution.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\n\t\\includegraphics[width = 0.475\\textwidth]{Figure4.eps} \\includegraphics[width = 0.475\\textwidth]{Figure5.eps}\n\t\\caption{PDF and CDF that yields the optimal unimodal bound via uniform mixtures compared with an associated lognormal distribution.}\n\t\\label{fig:pcdfpop}\n\\end{figure}\n\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.475\\textwidth]{Figure6.eps} \\includegraphics[width = 0.475\\textwidth]{Figure7.eps}\n\t\\caption{PDFs and CDFs that yield the optimal bounds  via lognormal mixtures (cf., Algorithm~\\ref{fig:unimodbis}) for $\\alpha = \\{11.34, 13.75\\}$, \n\t compared with an associated lognormal distribution.}\n\t\\label{fig:df}\n\\end{figure}\n\n\n\nObserve in Figure~\\ref{fig:df} that the unimodal lognormal mixture at $\\alpha = 13.75$ is relatively close to the shape of the associated true lognormal distribution. Contrast this with the PDF of the unimodal mixture of Figure~\\ref{fig:pcdfpop} which bares little similarity to the associated true lognormal probability density. The primary difference to note is that the lognormal mixture approach yields an unimodal distribution, but the mode is not specified.\nUsing the uniform mixture approach of Section~\\ref{sec:unimod} will produce a density with a specified mode, but at the cost of an unrealistic distribution. The lognormal mixture at $\\sigma = 11.34$ is bimodal and does not resemble the true density. In each case the cumulative densities are fairly close to the true CDF. This example highlights the ability of the lognormal mixture approach to construct realistic unimodal distributions while still being close to the optimal unimodal bound; here the gap was shown to be under 4\\%.\n\n\\subsection{Illustration of Theorem~\\ref{thm:unifcv}}\nWe finish this section by providing numerical results to illustrate Theorem~\\ref{thm:unifcv}. Reconsider the semiparametric bound problem defined in {(\\ref{{eq:ex2}})} with $m=2$ (i.e., with up to second-order moment information),  and the additional constrain that the underlying loss distribution is unimodal. \n\nSuppose we compute the semiparametric upper bound of the at-the-money policy described in Section~\\ref{sec:worstcase} enforcing the first two known moments and continuity. To illustrate Theorem~\\ref{thm:unifcv}, the upper bound is also computed for the option using \\eqref{eq:log} and various levels of $\\eta$. The percentage difference between the former and latter are plotted against $\\eta$ in Figure~\\ref{fig:unifconv}.\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width = 0.5\\textwidth]{Figure8.eps}\n\t\\caption{Illustration of how the upper bound with mixture components \\eqref{eq:log} converges to the \n\tunimodality bound~\\eqref{eq:unifh} (without smoothness requirements)\n\t as $\\eta \\rightarrow \\infty$. The plot shows the difference in percentage between these bounds \n\t as a function of $\\eta$.}\n\t\\label{fig:unifconv}\n\\end{figure}\n\nFrom Figure~\\ref{fig:unifconv} we see that by implementing the algorithm with \\eqref{eq:log} and increasing $\\eta$ the upper bound converges to that computed with mixture component \\eqref{eq:unifh}. Bounds computed using smoothness \nand unimodality can yield values arbitrarily close to, but not greater than those obtained when only unimodality is enforced. The implication of Theorem~\\ref{thm:unifcv} is that any tightening of the upper bound from a smooth mixture is a byproduct of the choice of the mixture distribution $H_x$, not from the inclusion of smoothness. In practice it can be confirmed that the change in bound from choice of $H_x$ is generally fairly small.\n\n\n{\\section[{Extensions}]{\\centering {Extensions}}}\nBesides the common features of insurance policies considered in Section~\\ref{sec:num} such as the policy's deductible $d$, and the fact that losses typically do not exceed a maximum loss $b$; a maximum payment, and coinsurance are also common features in insurance policies. If a policy will only cover up to a maximum loss of $u \\in R^+$, and coinsurance stipulates that only some proportion $\\gamma \\in [0,1]$ of the losses will be covered, then the policy's payoff can be written as $f(X) = \\gamma[\\min(X,u) - \\min(X,d)]$. \n\nAll of these policy modifications can be readily incorporated into the CG solution approach framework.\nIn particular, the CG methodology can be applied to compute bounds on the expected policy loss, for a wide variety of standard functions of loss random variables used in industry.\n\nIn the numerical examples in Section~\\ref{sec:num}, the target function $f$ in {(\\ref{{eq:ub}})} was used to model piecewise linear insurance policy payoffs, and the functions $g_j$, $j=1,\\dots,m$ to use the knowledge of up to m-order non-central moments of the loss distribution.\nHowever,  \nthe methodology discussed here applies similarly to functional forms of $f$ that are not piecewise linear policy payoffs, and the \n functions $g_j$, $j=1,\\dots,m$ can be used to represent the knowledge of other than non-central moment's\n information. As an example, let $c_j$ be the European call prices on some stock $X$ for various strike prices $K_j$, $j=1,\\dots,m$. Recall that the payoff for this type of option is the same as that of the $d$-deductible policy described in \\eqref{eq:ex2}. The constrain set for \\eqref{eq:ub} can be defined to enforce market prices of options by setting\n", "itemtype": "equation", "pos": 42801, "prevtext": "\nBeing able to compute bounds on the expected LER would be beneficial for an insurer attempting to set a deductible in cases where the actual loss distribution is ambiguous. For example, \nin \\citet[][Section 3]{cox}, the relationship {(\\ref{{eq:lerdef}})} is used to obtain upper and lower semiparametric bounds on the \nexpected LER of an insurance policy with deductible, assuming only the knowledge of the mean and variance of the loss, and that the loss cannot exceed a known maximum. \n\nAnother sensible premise is to assume that the loss distribution is unimodal. To illustrate the potential of the CG approach, in Figure~\\ref{fig:ler}, we use the mixture transformation of Section~\\ref{sec:unimod} to compute upper and lower semiparametric bounds on the insurance policy with payoff $\\max\\{0, X-d\\}$ when the mean $\\mu$ and variance $\\sigma^2$ of the loss are assumed to be known, and the loss cannot exceed the value of $b$, where $d$ is the policy's deductible, and the loss distribution is also assumed to be unimodal with mode {\\mbox{$M$} }. The results are compared with the analytical formula of \\citet[][Section3]{cox} to illustrate the tightening of the bounds obtained by adding the unimodality assumption. Specifically, following \\citet[Section 4]{cox}, in Figure~\\ref{fig:ler} we set $\\mu = 50$, $\\sigma = 15$, $b=100$, and ${\\mbox{$M$} } = \\{45, 50\\}$.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.475\\textwidth]{Figure1.eps} \\includegraphics[width = 0.475\\textwidth]{Figure2.eps} \n\n\t\\caption{Expected LER bounds (left) and gaps (right) for different values of the deductible $d$, when the mean $\\mu = 50$, and variance \n\t$\\sigma^2 = 225$ of the underlying loss, as well as its potential maximum value $b=100$, are assumed to be known. Gaps indicates the difference between upper and lower bounds. Results are presented for bounds without the unimodality constrain, and with unimodality constrain with mode ${\\mbox{$M$} } = \\{45, 50\\}$.}\n\t\\label{fig:ler}\n\\end{figure}\n\nObserve in Figure~\\ref{fig:ler} that the expected LER {\\em gap}; that is, the difference between the upper and lower semiparametric expected LER's bounds, is significantly tighter when unimodality is included. When ${\\mbox{$M$} } = 50$ the gap is symmetrical with a small spike at the mean/mode. The case in which ${\\mbox{$M$} } = 45$ yields a corresponding peak in the gap around the mode. \nFor either very high or very low\ndeductible values, the choice of the mode has little impact on the size of the gap. \nRegardless of the mode's value,\nthe gaps are of similar magnitude and narrower than in the case where the underlying loss distribution is not assumed to be unimodal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Examining worst-case distributions} \\label{sec:worstcase}\n\n\n\nSuppose we wish to compute the semiparametric upper bound defined by {(\\ref{{eq:ex2}})} with $m=2$. \nSpecifically, let the expected value (moment) constrains in {(\\ref{{eq:ub}})} be given by\n\n", "index": 27, "text": "\\begin{equation}\n\\label{eq:const}\nE_\\pi(X) = \\mu, E_\\pi(X^2) = \\sigma^2 + \\mu^2,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"E_{\\pi}(X)=\\mu,E_{\\pi}(X^{2})=\\sigma^{2}+\\mu^{2},\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>\u03bc</mi></mrow><mo>,</mo><mrow><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>\u03c3</mi><mn>2</mn></msup><mo>+</mo><msup><mi>\u03bc</mi><mn>2</mn></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nThe market price constrains {(\\ref{{eq:price}})} can then be used to compute semiparametric bounds on the variance of the underlying asset. This is accomplished by definining the target function $f(X)$ in {(\\ref{{eq:ub}})} as\n\n", "itemtype": "equation", "pos": 53278, "prevtext": "\nfor given $\\mu, \\sigma \\in {\\mathbb{R}}^+$. That is, both the mean and the variance of the underlying loss distribution\nare assumed to be known. In this case, equation {(\\ref{{eq:moms}})} reduces to\n$\\mu_\\pi^+ = \\mu_\\pi^- = \\mu$, and $\\sigma_\\pi^2 = \\sigma^2$.\n\nA closed-form solution for the corresponding semiparametric upper bound problem was derived by  {\\citet{{lo}}}, where he considers $f$ in {(\\ref{{eq:ex2}})} as the payoff of an European call option with strike $d$. If furthermore, it is assumed that the underlying distribution $\\pi$ of the loss (or asset price) is unimodal, the corresponding semiparametric upper bound can be computed using: \nthe analytical formula provided by \\citet[][Section 3.3]{schepper}, the SDP techniques provided by \\citet{popescu}, \nor the uniform mixture  approach of Section~\\ref{sec:unimod} with components \\eqref{eq:unifh}. The CG uniform mixture method readily provides a worst-case distribution. This distribution, however, is not smooth, has finite support, and is unrealistic as a model for the uncertainty of losses. For this reason we compute upper bounds using smooth mixture compontents in {(\\ref{{eq:trans}})} and inspect the resulting worst-case probability and cumulative density functions. The resulting smooth distribution is then compared to the optimal unimodal uniform mixture distribution. Specifically, we use the lognormal mixture {(\\ref{{eq:logmix}})}. \n\nIn particular, let us sample the values of $\\mu, \\sigma$ in {(\\ref{{eq:const}})} from a lognormal asset price dynamics model which is also commonly used to model (a non-ambiguous) loss distribution \\citep[see, e.g.][]{Cox2004, Jaimungal2006}. Namely, let\n$\\mu = X_0e^{rT}$, and $\\sigma= X_0e^{rT}(e^{\\nu^2T}-1)^{\\frac12}$ for values of $X_0 = 49.50$, $r = 1\\%$, $\\nu = 20\\%$, and $T = 1$.\nAlso let $d = X_0$ in {(\\ref{{eq:ex2}})}; that is, consider a policy where the expected value of the loss is equal to the deductible.\n\nThe semiparametric upper bound was computed \nusing the lognormal mixture {(\\ref{{eq:logmix}})} for different values of $\\alpha \\in [1, 1.5, \\ldots, 20]$ and the percent above the parametric value of the policy based on Black-Scholes formula was plotted in Figure~\\ref{fig:byalpha}. The corresponding semiparametric bound without the unimodality assumption (given by {\\citet{{lo}}}), and unimodal bounds with an uniform mixture from Section~\\ref{sec:unimod}   are also plotted for reference.\nThe bold point in Figure~\\ref{fig:byalpha}\n \n represents the smooth, unimodal bound obtained with the bisection Algorithm~\\ref{fig:unimodbis} and a mixture of lognormal distributions. Also, the plot illustrates the point ($\\alpha = 11.34$) at which the the bounds obtained by the bisection Algorithm~\\ref{fig:unimodbis} and the CG Algorithm~\\ref{fig:alg} with a mixture of uniform distributions are equal.\n\nIn Figure~\\ref{fig:byalpha} one can observe that for extremely low values of $\\alpha$, the component distributions of the mixture are very narrow, approaching the pessimistic discrete distribution case associated with closed-form bound of {\\citet{{lo}}}. We also see that as $\\alpha \\rightarrow \\sigma = 20.4$ the resulting bound distribution converges to the lognormal specified by the Black and Scholes asset pricing framework. This convergence is seen in Figure~\\ref{fig:byalpha} since the error goes to zero and the upper bound price equals the analytical Black-Scholes price.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.5\\textwidth]{Figure3.eps}\n\t\\caption{Percentage above the parametric Black and Scholes price of the \\citet{lo} upper bound (Lo's Bound) and the lognormal mixtures obtained from Algorithm~\\ref{fig:unimodbis} (lognormal Mixture Bounds).  The bold point denotes the value of $\\alpha = 13.75$ in which the lognormal mixture bound obtained from Algorithm~\\ref{fig:unimodbis} produces a unimodal distribution.} \n\t\n\t\\label{fig:byalpha}\n\\end{figure}\n\nFigure~\\ref{fig:byalpha} also highlights the result discussed in Theorem~\\ref{thm:unifcv}. The bound computed using uniform mixture components is greater than the unimodal bound from the lognormal mixture with the gap size under 4\\%. Note that the unimodal upper bound using lognormal mixture components occurred at $\\alpha = 13.75$. As mentioned before, the $\\alpha$ that yields the same bound value as that from the uniform mixture is $\\alpha = 11.34$. The smaller the $\\alpha$ in the lognormal mixture {(\\ref{{eq:logmix}})} the higher the conservatism associated with the semiparametric bound. Figure~\\ref{fig:df} shows the probability distribution function (PDF) and cumulative distribution function (CDF) of the lognormal mixtures at $\\alpha = \\{11.34, 13.75\\}$ as well as the associated {\\em true} lognormal distribution with mean and variance equal to the moments used to compute the semiparametric bounds. To highlight the advantage of using the lognormal mixtures instead of the uniform mixtures, Figure~\\ref{fig:pcdfpop} shows the optimal PDF and CDF of the latter along with the associated true lognormal distribution.\n\n\\begin{figure}[ht]\n\t\\centering\n\t\n\t\\includegraphics[width = 0.475\\textwidth]{Figure4.eps} \\includegraphics[width = 0.475\\textwidth]{Figure5.eps}\n\t\\caption{PDF and CDF that yields the optimal unimodal bound via uniform mixtures compared with an associated lognormal distribution.}\n\t\\label{fig:pcdfpop}\n\\end{figure}\n\n\n\\begin{figure}[ht]\n\t\\centering\n\t\\includegraphics[width = 0.475\\textwidth]{Figure6.eps} \\includegraphics[width = 0.475\\textwidth]{Figure7.eps}\n\t\\caption{PDFs and CDFs that yield the optimal bounds  via lognormal mixtures (cf., Algorithm~\\ref{fig:unimodbis}) for $\\alpha = \\{11.34, 13.75\\}$, \n\t compared with an associated lognormal distribution.}\n\t\\label{fig:df}\n\\end{figure}\n\n\n\nObserve in Figure~\\ref{fig:df} that the unimodal lognormal mixture at $\\alpha = 13.75$ is relatively close to the shape of the associated true lognormal distribution. Contrast this with the PDF of the unimodal mixture of Figure~\\ref{fig:pcdfpop} which bares little similarity to the associated true lognormal probability density. The primary difference to note is that the lognormal mixture approach yields an unimodal distribution, but the mode is not specified.\nUsing the uniform mixture approach of Section~\\ref{sec:unimod} will produce a density with a specified mode, but at the cost of an unrealistic distribution. The lognormal mixture at $\\sigma = 11.34$ is bimodal and does not resemble the true density. In each case the cumulative densities are fairly close to the true CDF. This example highlights the ability of the lognormal mixture approach to construct realistic unimodal distributions while still being close to the optimal unimodal bound; here the gap was shown to be under 4\\%.\n\n\\subsection{Illustration of Theorem~\\ref{thm:unifcv}}\nWe finish this section by providing numerical results to illustrate Theorem~\\ref{thm:unifcv}. Reconsider the semiparametric bound problem defined in {(\\ref{{eq:ex2}})} with $m=2$ (i.e., with up to second-order moment information),  and the additional constrain that the underlying loss distribution is unimodal. \n\nSuppose we compute the semiparametric upper bound of the at-the-money policy described in Section~\\ref{sec:worstcase} enforcing the first two known moments and continuity. To illustrate Theorem~\\ref{thm:unifcv}, the upper bound is also computed for the option using \\eqref{eq:log} and various levels of $\\eta$. The percentage difference between the former and latter are plotted against $\\eta$ in Figure~\\ref{fig:unifconv}.\n\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width = 0.5\\textwidth]{Figure8.eps}\n\t\\caption{Illustration of how the upper bound with mixture components \\eqref{eq:log} converges to the \n\tunimodality bound~\\eqref{eq:unifh} (without smoothness requirements)\n\t as $\\eta \\rightarrow \\infty$. The plot shows the difference in percentage between these bounds \n\t as a function of $\\eta$.}\n\t\\label{fig:unifconv}\n\\end{figure}\n\nFrom Figure~\\ref{fig:unifconv} we see that by implementing the algorithm with \\eqref{eq:log} and increasing $\\eta$ the upper bound converges to that computed with mixture component \\eqref{eq:unifh}. Bounds computed using smoothness \nand unimodality can yield values arbitrarily close to, but not greater than those obtained when only unimodality is enforced. The implication of Theorem~\\ref{thm:unifcv} is that any tightening of the upper bound from a smooth mixture is a byproduct of the choice of the mixture distribution $H_x$, not from the inclusion of smoothness. In practice it can be confirmed that the change in bound from choice of $H_x$ is generally fairly small.\n\n\n{\\section[{Extensions}]{\\centering {Extensions}}}\nBesides the common features of insurance policies considered in Section~\\ref{sec:num} such as the policy's deductible $d$, and the fact that losses typically do not exceed a maximum loss $b$; a maximum payment, and coinsurance are also common features in insurance policies. If a policy will only cover up to a maximum loss of $u \\in R^+$, and coinsurance stipulates that only some proportion $\\gamma \\in [0,1]$ of the losses will be covered, then the policy's payoff can be written as $f(X) = \\gamma[\\min(X,u) - \\min(X,d)]$. \n\nAll of these policy modifications can be readily incorporated into the CG solution approach framework.\nIn particular, the CG methodology can be applied to compute bounds on the expected policy loss, for a wide variety of standard functions of loss random variables used in industry.\n\nIn the numerical examples in Section~\\ref{sec:num}, the target function $f$ in {(\\ref{{eq:ub}})} was used to model piecewise linear insurance policy payoffs, and the functions $g_j$, $j=1,\\dots,m$ to use the knowledge of up to m-order non-central moments of the loss distribution.\nHowever,  \nthe methodology discussed here applies similarly to functional forms of $f$ that are not piecewise linear policy payoffs, and the \n functions $g_j$, $j=1,\\dots,m$ can be used to represent the knowledge of other than non-central moment's\n information. As an example, let $c_j$ be the European call prices on some stock $X$ for various strike prices $K_j$, $j=1,\\dots,m$. Recall that the payoff for this type of option is the same as that of the $d$-deductible policy described in \\eqref{eq:ex2}. The constrain set for \\eqref{eq:ub} can be defined to enforce market prices of options by setting\n", "index": 29, "text": "\\begin{equation} \\label{eq:price} E_\\pi(g_j(X)) := E_\\pi(\\max(X-K_j,0)) = c_j, j=1,\\dots,m. \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"E_{\\pi}(g_{j}(X)):=E_{\\pi}(\\max(X-K_{j},0))=c_{j},j=1,\\dots,m.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>-</mo><msub><mi>K</mi><mi>j</mi></msub></mrow><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>c</mi><mi>j</mi></msub></mrow><mo>,</mo><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>m</mi></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nwhere $\\mu$ is the known first moment of $X$. In {\\citet{{bert02}}} it was shown \nthat computing semiparametric bounds on {(\\ref{{eq:risk}})} using knowledge on the prices {(\\ref{{eq:price}})}\nis a useful alternative to the standard methods of computing implied volatilities. For risk management purposes, semiparametric bounds can also be used to compute bounds on one sided deviations of the underlying risk, that is, its semivariance. Each of these common applications readily fit into the framework of the CG methodology presented in Section~\\ref{sec:method}, demonstrating the variety of contexts in which the CG approach can be used to compute semiparametric bounds. \n\n\n\n\n\n\n\n\n{\\section[{Summary}]{\\centering {Summary}}}\nThe CG methodology presented here provides a practical optimization-based algorithm for computing semiparametric bounds on the expected payments of general insurance instruments and financial assets. Section~\\ref{sec:method} described how the general problem described in \\eqref{eq:ub} can be solved, in most practical instances, by solving a sequence of linear programs that are updated using simple arithmetic operations. The CG approach also readily provides a representation of the worst/best-case distributions associated with a semiparametric bound problem.\n\n\nTo illustrate the potential of the of the CG algorithm semiparametric bounds on the payoff of common insurance policies were computed.\nIt was also shown that additional distributional information such as continuity and unimodality can be incorporated in the formulation in a straightfoward fashion. The ability to include these constrains provides tighter bounds on the quantity of interest as well as  distributions that fit the practitioner's problem specific knowledge. \n{\\color{black} { Note that from the recent work of \\citet{LeeL10}, it follows that for some Property/Casualty insurance problems it will be suitable to consider that the underlying random variable follows a distribution that is a mixture of Erlang distributions. The advantage of using mixtures of Erlang distributions is the existence of extremely efficient expectation--maximization (EM) algorithms for parameter estimation from raw data. This interesting line of work will be the subject of future research work.}}\n\nThe CG methodology offers a powerful and compelling alternative for computing semiparametric bounds \nin comparison with the main approaches used in the literature to compute them; namely, \nderiving analytical solutions to special cases of the problem or solving it numerically  using semidefinite programming.\nThis is due to the speed, generality, and ease of implementation of the CG algorithm.\nThe CG algorithm achieves accurate results at a very small computational cost. The straightforward implementation used for the test problems shown here generated solutions in at most 1-2 seconds. Furthermore, although the examples considered here focused on obtaining semiparametric bounds for insurance policies with piecewise linear payoff given moment information about the underlying loss, \nthe CG approach presented here allows for a very general class of univariate semiparametric bounds to be computed using the same \ncore the solution algorithm.\n\n\\section*{\\centering Acknowledgements}\nThis work has been carried out thanks to funding from the Casualty Actuarial Society, Actuarial Foundation\nof Canada, and Society of Actuaries.\n\n\n\\setcounter{equation}{0}\n\\numberwithin{equation}{section}\n\n\\section*{\\centering Appendices}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendix \n\n\n\n\n\n\n\n\n\\section{Smooth Approximations of the Uniform Distribution} \\label{app:approx}\nLet $X$ be a random variable that is uniformly distributed on the interval $[a,b]$. The probability density function (PDF) of $X$ is $f(x) = \\frac{1}{b-a}$. It is possible to construct a smooth function that approximates $f(x)$ and is asymptotically equal to the true PDF. We define the following $\\eta$ parameterized function.\n\n", "itemtype": "equation", "pos": 53611, "prevtext": "\nThe market price constrains {(\\ref{{eq:price}})} can then be used to compute semiparametric bounds on the variance of the underlying asset. This is accomplished by definining the target function $f(X)$ in {(\\ref{{eq:ub}})} as\n\n", "index": 31, "text": "\\begin{equation} \\label{eq:risk} \\quad E_\\pi(f(X)) := E_\\pi((X-\\mu)^2) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\quad E_{\\pi}(f(X)):=E_{\\pi}((X-\\mu)^{2})\" display=\"block\"><mrow><mrow><msub><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>E</mi></mpadded><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msub><mi>E</mi><mi>\u03c0</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>-</mo><mi>\u03bc</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\nThe probability function $f_\\eta(x)$ is the difference in two shifted logistic functions. \\begin{lemma}\n\\label{totpro}\nFor any $\\eta > 0$, and $a,b \\in {\\mathbb{R}}$ such that $b \\ge a$, the cumulative probability distribution $F_\\eta(x)$ associated with the probability distribution $f_\\eta(x)$ is $F_\\eta(x) = 1 - \\frac{1}{\\eta(b-a)} \\ln \\left ( \\frac{1 + e^{-\\eta(x-b)}}{1 + e^{-\\eta(x-a)}} \\right )$. In particular, $\\lim_{x \\to \\infty} F_\\eta(x) = 1$, $\\lim_{x \\to -\\infty} F_\\eta(x) = 0$, and {\\color{black} {$\\frac{d F_\\eta(x)}{d x} \\ge 0$ for all $x \\in {\\mathbb{R}}$}}.\n\\end{lemma}\n\n\nIn Lemma~\\ref{l:u} we show that as $\\eta \\rightarrow \\infty$ \\eqref{eq:uneta} will converge to the PDF of $X$.\n\n\n\n\\begin{lemma}\n\t\\label{l:u}\n\tAs $\\eta \\rightarrow \\infty$ the function $f_\\eta(x)$ in \\eqref{eq:uneta} converges to a uniform distribution on $[a,b]$.\n\\end{lemma}\n\\begin{proof}\n\tTo show that $f_\\eta(x) \\rightarrow {\\operatorname{\\operatorname{Uniform}}}(a,b)$ as $\\eta \\rightarrow \\infty$ consider three different cases corresponding to three intervals of $x$. First consider the case in which $x < a$. For $x < a$ each of the exponent terms are positive, i.e. $0 < -\\eta(x-b) < -\\eta(x-a)$, for all $\\eta > 0$. So, for $x < a$ we see that $f_\\eta(x) \\rightarrow 0$. Next we look at $x > b$. In this case each of the exponent terms are negative, which again yields a limit of $0$. Finally, consider $a < x < b$. On this interval the exponent terms satisfy $-\\eta(x-a) < 0 < -\\eta(x-b)$ for all $\\eta > 0$. So, for $a < x < b$ we have $f_\\eta(x) \\rightarrow \\frac{1}{b-a} (1 - 0) = \\frac{1}{b-a}$. We can conclude that the limit of $f_\\eta(x)$ is as follows\n\n", "itemtype": "equation", "pos": 57700, "prevtext": "\nwhere $\\mu$ is the known first moment of $X$. In {\\citet{{bert02}}} it was shown \nthat computing semiparametric bounds on {(\\ref{{eq:risk}})} using knowledge on the prices {(\\ref{{eq:price}})}\nis a useful alternative to the standard methods of computing implied volatilities. For risk management purposes, semiparametric bounds can also be used to compute bounds on one sided deviations of the underlying risk, that is, its semivariance. Each of these common applications readily fit into the framework of the CG methodology presented in Section~\\ref{sec:method}, demonstrating the variety of contexts in which the CG approach can be used to compute semiparametric bounds. \n\n\n\n\n\n\n\n\n{\\section[{Summary}]{\\centering {Summary}}}\nThe CG methodology presented here provides a practical optimization-based algorithm for computing semiparametric bounds on the expected payments of general insurance instruments and financial assets. Section~\\ref{sec:method} described how the general problem described in \\eqref{eq:ub} can be solved, in most practical instances, by solving a sequence of linear programs that are updated using simple arithmetic operations. The CG approach also readily provides a representation of the worst/best-case distributions associated with a semiparametric bound problem.\n\n\nTo illustrate the potential of the of the CG algorithm semiparametric bounds on the payoff of common insurance policies were computed.\nIt was also shown that additional distributional information such as continuity and unimodality can be incorporated in the formulation in a straightfoward fashion. The ability to include these constrains provides tighter bounds on the quantity of interest as well as  distributions that fit the practitioner's problem specific knowledge. \n{\\color{black} { Note that from the recent work of \\citet{LeeL10}, it follows that for some Property/Casualty insurance problems it will be suitable to consider that the underlying random variable follows a distribution that is a mixture of Erlang distributions. The advantage of using mixtures of Erlang distributions is the existence of extremely efficient expectation--maximization (EM) algorithms for parameter estimation from raw data. This interesting line of work will be the subject of future research work.}}\n\nThe CG methodology offers a powerful and compelling alternative for computing semiparametric bounds \nin comparison with the main approaches used in the literature to compute them; namely, \nderiving analytical solutions to special cases of the problem or solving it numerically  using semidefinite programming.\nThis is due to the speed, generality, and ease of implementation of the CG algorithm.\nThe CG algorithm achieves accurate results at a very small computational cost. The straightforward implementation used for the test problems shown here generated solutions in at most 1-2 seconds. Furthermore, although the examples considered here focused on obtaining semiparametric bounds for insurance policies with piecewise linear payoff given moment information about the underlying loss, \nthe CG approach presented here allows for a very general class of univariate semiparametric bounds to be computed using the same \ncore the solution algorithm.\n\n\\section*{\\centering Acknowledgements}\nThis work has been carried out thanks to funding from the Casualty Actuarial Society, Actuarial Foundation\nof Canada, and Society of Actuaries.\n\n\n\\setcounter{equation}{0}\n\\numberwithin{equation}{section}\n\n\\section*{\\centering Appendices}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\appendix \n\n\n\n\n\n\n\n\n\\section{Smooth Approximations of the Uniform Distribution} \\label{app:approx}\nLet $X$ be a random variable that is uniformly distributed on the interval $[a,b]$. The probability density function (PDF) of $X$ is $f(x) = \\frac{1}{b-a}$. It is possible to construct a smooth function that approximates $f(x)$ and is asymptotically equal to the true PDF. We define the following $\\eta$ parameterized function.\n\n", "index": 33, "text": "\\begin{equation} f_\\eta(x) = \\frac{1}{b-a} \\left[\\frac{1}{1+e^{-\\eta(x-a)}} - \\frac{1}{1+e^{-\\eta(x-b)}}\\right] \\label{eq:uneta} \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"f_{\\eta}(x)=\\frac{1}{b-a}\\left[\\frac{1}{1+e^{-\\eta(x-a)}}-\\frac{1}{1+e^{-\\eta(%&#10;x-b)}}\\right]\" display=\"block\"><mrow><mrow><msub><mi>f</mi><mi>\u03b7</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>b</mi><mo>-</mo><mi>a</mi></mrow></mfrac><mo>\u2062</mo><mrow><mo>[</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>-</mo><mi>b</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msup></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02149.tex", "nexttext": "\n\\end{proof}\n\nTo demonstrate Lemma~\\ref{l:u}, consider $X \\sim {\\operatorname{\\operatorname{Uniform}}}(20,30)$. The PDF of $X$ can be approximated using \\eqref{eq:uneta} with $a = 20$ and $b = 30$. In Figure~\\ref{fig:byeta} we plot the PDF of X as well as the approximation curve for different values of $\\eta$.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width = 0.5\\textwidth]{Figure9.eps}\n\t\\caption{PDF of ${\\operatorname{\\operatorname{Uniform}}}(20,30)$ distribution along with the approximating density \\eqref{eq:uneta} for different values of $\\eta$.}\n\t\\label{fig:byeta}\n\\end{figure}\nFrom Figure~\\ref{fig:byeta}, we see that as the parameter $\\eta$ increases the curve of \\eqref{eq:uneta} approaches the PDF of~$X$.\n\n\n\n\n\n\\begin{thebibliography}{}\n\n\\bibitem[Bertsimas and Popescu, 2002]{bert02}\nBertsimas, D. and Popescu, I. (2002).\n\\newblock On the relation between option and stock prices.\n\\newblock {\\em Operations Research}, 50(2):358--74.\n\n\\bibitem[Bertsimas and Tsitsiklis, 1997]{bertlp}\nBertsimas, D. and Tsitsiklis, J.~N. (1997).\n\\newblock {\\em Introduction to Linear Optimization}.\n\\newblock Athena Scientific, Belmont, Massachusetts.\n\n\\bibitem[Birge and Dul{\\'a}, 1991]{BirgD91}\nBirge, J.~R. and Dul{\\'a}, J.~H. (1991).\n\\newblock Bounding separable recourse functions with limited distribution\n  information.\n\\newblock {\\em Annals of Operations Research}, 30:277--298.\n\n\\bibitem[Boyle and Lin, 1997]{BoylL97}\nBoyle, P. and Lin, X. (1997).\n\\newblock Bounds on contingent claims based on several assets.\n\\newblock {\\em J. Fin. Econ.}, 46(3):383--400.\n\n\\bibitem[Brockett et~al., 1996]{brock}\nBrockett, P., Cox, S., and Smith, J. (1996).\n\\newblock Bounds on the price of catastrophe insurance options on futures\n  contracts.\n\\newblock {\\em Securitization of Insurance Risks. Society of Actuaries\n  Monograph Series}.\n\n\\bibitem[Chen et~al., 2011]{zhang11}\nChen, L., He, S., and Zhang, S. (2011).\n\\newblock Tight bounds for some risk measures, with applications to robust\n  portfolio selection.\n\\newblock {\\em Operations Research}, 59(4):847--865.\n\n\\bibitem[Cox, 1991]{cox}\nCox, S. (1991).\n\\newblock Bounds on expected values of insurance payments and option prices.\n\\newblock {\\em Transactions of the Society of Actuaries}, XLIII:231--60.\n\n\\bibitem[Cox et~al., 2010]{coxzul10}\nCox, S., Lin, Y., Tian, R., and Zuluaga, L. (2010).\n\\newblock Bounds for probabilities of extreme events defined by two random\n  variables.\n\\newblock {\\em Variance}, 4(1):47--65.\n\n\\bibitem[Cox et~al., 2011]{coxzul11}\nCox, S., Lin, Y., Tian, R., and Zuluaga, L.~F. (2011).\n\\newblock Mortality portfolio risk management.\n\\newblock {\\em Journal of Risk and Insurance}.\n\n\\bibitem[Cox et~al., 2004]{Cox2004}\nCox, S.~H., Fairchild, J.~R., and Pedersen, H.~W. (2004).\n\\newblock Valuation of structured risk management products.\n\\newblock {\\em Insurance: Mathematics and Economics}, 34:259--272.\n\n\\bibitem[Dantzig, 1963]{Dantzig}\nDantzig, G.~B. (1963).\n\\newblock {\\em Linear Programming and Extensions}.\n\\newblock Princeton University Press.\n\\newblock \\url{http://www.rand.org/pubs/reports/R366.html}.\n\n\\bibitem[Dantzig and Wolfe, 1961]{DantW61}\nDantzig, G.~B. and Wolfe, P. (1961).\n\\newblock The decomposition algorithm for linear programming.\n\\newblock {\\em Econometrica}, 29(4).\n\n\\bibitem[Delage and Ye, 2010]{Ye10}\nDelage, E. and Ye, Y. (2010).\n\\newblock Distributionally robust optimization under moment uncertainty with\n  application to data-driven problems.\n\\newblock {\\em Operations Research}, 58(3):595--612.\n\n\\bibitem[Jaimungal and Wang, 2006]{Jaimungal2006}\nJaimungal, S. and Wang, T. (2006).\n\\newblock Catastrophe options with stochastic interest rates and compound\n  {P}oisson losses.\n\\newblock {\\em Insurance: Mathematics and Economics}, 38:469--483.\n\n\\bibitem[Jansen et~al., 1986]{jansen}\nJansen, K., Haezendonc, J., and Goovaerts, M. (1986).\n\\newblock Upper bounds on stop-loss premiums in case of known moments up to the\n  fourth order.\n\\newblock {\\em Insurance: Mathematics and Economics}, 5:315--34.\n\n\\bibitem[Kemperman, 1968]{kemp}\nKemperman, J. H.~B. (1968).\n\\newblock The general moment problem, a geometric approach.\n\\newblock {\\em The Annals of Mathematical Statistics}, 39(1):93--122.\n\n\\bibitem[Khintchine, 1938]{khintchine}\nKhintchine, A.~Y. (1938).\n\\newblock On unimodal distributions.\n\\newblock {\\em Izv. Naucho-Isled. Inst. Mat. Mech. Tomsk. Gos. Univ.}, 2:1--7.\n\n\\bibitem[Lee and Lin, 2010]{LeeL10}\nLee, S. and Lin, S. (2010).\n\\newblock Modeling and evaluating insurance losses via mixtures of erlang\n  distributions.\n\\newblock {\\em North American Actuarial Journal}, 14(1).\n\n\\bibitem[Lo, 1987]{lo}\nLo, A. (1987).\n\\newblock Semi-parametric upper bounds for option prices and expected payoffs.\n\\newblock {\\em Journal of Financial Economics}, 19(2):373--388.\n\n\\bibitem[Lubbecke and Desrosiers, 2005]{colgen}\nLubbecke, M.~E. and Desrosiers, J. (2005).\n\\newblock Selected topics in column generation.\n\\newblock {\\em Operations Research}, 53(6):1007--023.\n\n\\bibitem[Natarajan et~al., 2011]{Nata11}\nNatarajan, K., Sim, M., and Chung-Piaw, T. (2011).\n\\newblock Beyond risk: Ambiguity in supply chains.\n\\newblock In {\\em Handbook of Integrated Risk Management in Global Supply\n  Chains}, pages 103--124. John Wiley \\& Sons Inc.\n\n\\bibitem[Nocedal and Wright, 2006]{Nocedal2006NO}\nNocedal, J. and Wright, S.~J. (2006).\n\\newblock {\\em Numerical Optimization}.\n\\newblock Springer, New York, 2nd edition.\n\n\\bibitem[Popescu, 2005]{popescu}\nPopescu, I. (2005).\n\\newblock A semidefinite programming approach to optimal moment bounds for\n  convex classes of distributions.\n\\newblock {\\em Mathematics of Operations Research}, 30(3):632--57.\n\n\\bibitem[Schepper and Heijnen, 2007]{schepper}\nSchepper, D. and Heijnen, B. (2007).\n\\newblock Distribution-free option pricing.\n\\newblock {\\em Insurance: Mathematics and Economics}, 40:179--99.\n\n\\bibitem[Tawarmalani and Sahinidis, 2005]{ts:05}\nTawarmalani, M. and Sahinidis, N.~V. (2005).\n\\newblock {A polyhedral branch-and-cut approach to global optimization}.\n\\newblock {\\em Mathematical Programming}, 103:225--249.\n\n\\bibitem[Todd, 2001]{todd01}\nTodd, M. (2001).\n\\newblock Semidefinite optimization.\n\\newblock {\\em Acta Numerica}, 10.\n\n\\bibitem[Villegas et~al., 2010]{vill}\nVillegas, M., Medaglia, A.~L., and Zuluaga, L. (2010).\n\\newblock Computing bounds on the expected payoff of alternative risk transfer\n  products.\n\\newblock {\\em Mathematics: Insurance and Economics (forthcoming)}.\n\n\\bibitem[Zuluaga and Pe{\\~n}a, 2005]{zulu05}\nZuluaga, L. and Pe{\\~n}a, J. (2005).\n\\newblock A conic programming approach to generalized tchebycheff inequalities.\n\\newblock {\\em Mathematics of Operations Research}, 30(2):369--88.\n\n\\end{thebibliography}\n\n\t\n\n", "itemtype": "equation", "pos": 59507, "prevtext": "\nThe probability function $f_\\eta(x)$ is the difference in two shifted logistic functions. \\begin{lemma}\n\\label{totpro}\nFor any $\\eta > 0$, and $a,b \\in {\\mathbb{R}}$ such that $b \\ge a$, the cumulative probability distribution $F_\\eta(x)$ associated with the probability distribution $f_\\eta(x)$ is $F_\\eta(x) = 1 - \\frac{1}{\\eta(b-a)} \\ln \\left ( \\frac{1 + e^{-\\eta(x-b)}}{1 + e^{-\\eta(x-a)}} \\right )$. In particular, $\\lim_{x \\to \\infty} F_\\eta(x) = 1$, $\\lim_{x \\to -\\infty} F_\\eta(x) = 0$, and {\\color{black} {$\\frac{d F_\\eta(x)}{d x} \\ge 0$ for all $x \\in {\\mathbb{R}}$}}.\n\\end{lemma}\n\n\nIn Lemma~\\ref{l:u} we show that as $\\eta \\rightarrow \\infty$ \\eqref{eq:uneta} will converge to the PDF of $X$.\n\n\n\n\\begin{lemma}\n\t\\label{l:u}\n\tAs $\\eta \\rightarrow \\infty$ the function $f_\\eta(x)$ in \\eqref{eq:uneta} converges to a uniform distribution on $[a,b]$.\n\\end{lemma}\n\\begin{proof}\n\tTo show that $f_\\eta(x) \\rightarrow {\\operatorname{\\operatorname{Uniform}}}(a,b)$ as $\\eta \\rightarrow \\infty$ consider three different cases corresponding to three intervals of $x$. First consider the case in which $x < a$. For $x < a$ each of the exponent terms are positive, i.e. $0 < -\\eta(x-b) < -\\eta(x-a)$, for all $\\eta > 0$. So, for $x < a$ we see that $f_\\eta(x) \\rightarrow 0$. Next we look at $x > b$. In this case each of the exponent terms are negative, which again yields a limit of $0$. Finally, consider $a < x < b$. On this interval the exponent terms satisfy $-\\eta(x-a) < 0 < -\\eta(x-b)$ for all $\\eta > 0$. So, for $a < x < b$ we have $f_\\eta(x) \\rightarrow \\frac{1}{b-a} (1 - 0) = \\frac{1}{b-a}$. We can conclude that the limit of $f_\\eta(x)$ is as follows\n\n", "index": 35, "text": "\\begin{align*}\n\t\\underset{\\eta \\rightarrow \\infty}{\\lim} f_\\eta(x) &= \\left\\{\n\t\t\\begin{array}{l l}\n\t\t\t0 & x < a \\\\\n\t\t\t\\frac{1}{b-a} & a < x < b \\\\\n\t\t\t0 & x > b\n\t\t\\end{array} \\right. \\\\\n\t\t&= {\\operatorname{\\operatorname{Uniform}}}(a,b)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\underset{\\eta\\rightarrow\\infty}{\\lim}f_{\\eta}(x)\" display=\"inline\"><mrow><munder accentunder=\"true\"><mo movablelimits=\"false\">lim</mo><mrow><mi>\u03b7</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2062</mo><msub><mi>f</mi><mi>\u03b7</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left\\{\\begin{array}[]{l l}0&amp;x&lt;a\\\\&#10;\\frac{1}{b-a}&amp;a&lt;x&lt;b\\\\&#10;0&amp;x&gt;b\\end{array}\\right.\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>&lt;</mo><mi>a</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>b</mi><mo>-</mo><mi>a</mi></mrow></mfrac></mstyle></mtd><mtd columnalign=\"left\"><mrow><mi>a</mi><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mi>b</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mi>x</mi><mo>&gt;</mo><mi>b</mi></mrow></mtd></mtr></mtable><mi/></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\operatorname{Uniform}}}(a,b)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>Uniform</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]