[{"file": "1601.04746.tex", "nexttext": "\nHere $vol(S)$ denotes the total weight incident to the vertex set $S$, and $cut_G(S,\\bar{S})$ denotes the total weight crossing from $S$ to $\\bar{S}$ in $G$.\n\nThe data graph $G_D$ is actually an implicit encoding of soft ML constraints. Indeed, pairwise affinities between nodes can be viewed as `soft declarations' that such nodes should be connected rather than disconnected in a clustering. Let now $d_i$ denote the total incident weight of vertex $i$ in $G_D$. Consider the {\\bf demand graph}~$K$ of implicit soft CL constraints, defined by the adjacency $K_{ij}=d_id_j/vol(V)$. It is easy to verify that $vol(S)vol(\\bar{S})/vol(V)=cut_K(S,\\bar{S})$. We have\n\n", "itemtype": "equation", "pos": 6545, "prevtext": "\n\n\\title{Scalable Constrained Clustering: \\\\\nA Generalized Spectral Method \\thanks{Significant part of this work\nwas carried out while M. Cucuringu, I. Koutis and G. Miller were visiting the\nSimons Institute for the Theory of Computing at UC Berkeley in Fall 2014. I. Koutis is supported\nby NSF CAREER award CCF-1149048.}}\n\n\n\\author{\nMihai Cucuringu  \\\\U. of California, Los Angeles \\\\ mihai@math.ucla.edu\n\\and Ioannis Koutis \\\\ U. of Puerto Rico - Rio Piedras\\\\ ioannis.koutis@upr.edu\n\\and Sanjay Chawla \\\\  Qatar C  , HBKU\\footnote{and University of Sydney.} \\\\ chawla@it.usyd.edu.au\n\\and Gary Miller \\\\ Carnegie Mellon University \\\\ glmiller@cs.cmu.edu\n\\and Richard Peng \\\\ Georgia Institute of Technology \\\\ rpeng@cc.gatech.edu\n}\n\n\n\\maketitle\n\n\n\\begin{abstract}\nWe present a simple spectral approach to the well-studied constrained\nclustering problem. It captures constrained clustering as a generalized eigenvalue problem with graph Laplacians. The algorithm works in nearly-linear time and provides\nconcrete guarantees for the quality of the clusters, at least for the\ncase of 2-way partitioning. In practice this translates\nto a very fast implementation that consistently outperforms\nexisting spectral approaches both in speed and quality.\n\\end{abstract}\n\n\\section{Introduction}\\label{section:intro}\nClustering with constraints is a problem of central importance\nin machine learning and data mining. It captures the case when information about an\napplication task comes in the form of both data and domain knowledge.\nWe study the standard problem where domain knowledge is\nspecified as a set of {\\em soft} must-link (ML) and cannot-link (CL) constraints\n\\cite{basuDavidson}.\n\n\nThe extensive literature reports a plethora of methods, including\nspectral algorithms that explore various modifications and extensions of the basic\nspectral algorithm by Shi and Malik~\\cite{ShiM00} and its\nvariant by Ng et al.~\\cite{NgJW01}.\n\n\n\nThe distinctive feature of our algorithm is that it constitutes a natural \\textbf{generalization}, rather\nthan an extension of the basic spectral method. The generalization is based on a critical look at how existing methods handle constraints, in section~\\ref{sec:rethinking}.\nThe solution is derived from a geometric embedding obtained via a spectral relaxation of an\noptimization problem, exactly in the spirit of~\\cite{NgJW01,ShiM00}. This is depicted\nin the workflow in Figure~1.  Data\nand ML constraints are represented by a Laplacian matrix $L$ and CL constraints by another\nLaplacian matrix $H$. The embedding is realized by computing a few eigenvectors\nof the generalized eigenvalue problem $Lx = \\lambda Hx$. The generalization of~\\cite{NgJW01,ShiM00}\nlies essentially in $H$ being a Laplacian matrix rather than the diagonal $D$ of $L$. In\nfact, as we will discuss later, $D$ itself is equivalent to a specific Laplacian matrix;\nthus our method encompasses the basic spectral method as a special case of constrained clustering.\n\n\n\\begin{figure}[h]\n\\includegraphics[width=1\\columnwidth]{flow_gen}\n\\caption{A schematic overview of our approach.}\n\\label{fig:flow_gen}\n\\end{figure}\n\nOur approach is characterized by its conceptual simplicity that enables\na straightforward mathematical derivation of the algorithm, possibly the simplest among all\ncompeting spectral methods. Reducing the problem to a relatively simple generalized\neigensystem enables us to derive directly from recent significant progress  due to Lee~et~al.~\\cite{Lee12} in the\ntheoretical understanding of the standard spectral clustering method,\noffering its first practical realization. In addition, the algorithm comes with\ntwo features that are not simultaneously shared by {\\em any} of the prior methods:\n(i) it is provably fast by design as it leverages\nfast linear system solvers for Laplacian systems~\\cite{Koutis:2012}\n(ii) it provides a concrete theoretical\nguarantee for the quality of 2-way constrained partitioning, with\nrespect to the underlying discrete optimization problem,\nvia a generalized Cheeger inequality (section~\\ref{sec:cheeger}).\n\nIn practice, our method is at least 10x faster than\ncompeting methods on large data sets. It solves data sets with millions of\npoints in less than 2 minutes, on very modest hardware. Furthermore\nthe quality of the computed segmentations is often dramatically better.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Problem definition}\n\\label{sec:pdef}\n\nThe constrained clustering problem is specified by three weighted graphs:\n\n\\smallskip\n\\noindent \\textbf{1.} The {\\em data graph} $G_D$ which contains a given number of $k$ clusters that we seek to find. Formally, the graph is a triple $G_D=(V,E_D,w_D)$, with the edge weights $w_D$ being positive real numbers indicating the level of `affinity' of their endpoints.\n\n\\smallskip\n\n\\noindent \\textbf{2.} The {\\em knowledge graphs} $G_{ML}$ and $G_{CL}$.\nThe two graphs are formally triples $G_{ML}=(V,E_{ML},w_{ML})$\nand $G_{CL}=(V,E_{CL},w_{CL})$.\nEach edge in $G_{ML}$ indicates that its two\nendpoints should be in the same cluster, and each\nedge in $G_{CL}$ indicates that its two endpoints should\nbe in different clusters. The weight of an edge indicates the level of belief placed in the corresponding constraint.\n\n\\smallskip\n\nWe emphasize that prior knowledge does not have to be exact\nor even self-consistent, and thus the constraints should not\nbe viewed as `hard' ones. However, to conform with prior\nliterature, we will use the existing terminology of `must\nlink' (ML) and `cannot link' (CL) constraints to which\n $G_{ML}$ and $G_{CL}$ owe their notation respectively.\n\nIn the constrained clustering problem the general goal is to find $k$ {disjoint}  clusters in the data graph. Intuitively, the clusters should result from cutting a small number of edges in the data graph, while simultaneously respecting as much as possible the constraints in the knowledge graphs.\n\n\n\n\n\n\\section{Re-thinking constraints}\n\\label{sec:rethinking}\n\nMany approaches have been pursued within the constrained spectral clustering framework. They are quite distinct but do share a common point of view: constraints are viewed as entities structurally extraneous to the basic spectral formulation, necessitating its modification or extension with additional mathematical features. However, a key fact is overlooked:\n\n\\begin{center}\n{\\em Standard clustering is a special case of constrained clustering with implicit soft ML and CL constraints}.\n\\end{center}\n\nTo see why, let us briefly recall the optimization problem in the standard method (\\textsc{Ncut}).\n\n", "index": 1, "text": "$$\n     \\phi =  \\min_{S\\subseteq V} \\frac{cut_{G_D}(S,\\bar{S}) }{vol(S)vol(\\bar{S})/vol(V)}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\phi=\\min_{S\\subseteq V}\\frac{cut_{G_{D}}(S,\\bar{S})}{vol(S)vol(\\bar{S})/vol(V%&#10;)}.\" display=\"block\"><mrow><mrow><mi>\u03d5</mi><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><msub><mi>G</mi><mi>D</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>v</mi></mrow><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nIn other words, the \\textsc{Ncut} objective can be viewed as:\n\n", "itemtype": "equation", "pos": 7304, "prevtext": "\nHere $vol(S)$ denotes the total weight incident to the vertex set $S$, and $cut_G(S,\\bar{S})$ denotes the total weight crossing from $S$ to $\\bar{S}$ in $G$.\n\nThe data graph $G_D$ is actually an implicit encoding of soft ML constraints. Indeed, pairwise affinities between nodes can be viewed as `soft declarations' that such nodes should be connected rather than disconnected in a clustering. Let now $d_i$ denote the total incident weight of vertex $i$ in $G_D$. Consider the {\\bf demand graph}~$K$ of implicit soft CL constraints, defined by the adjacency $K_{ij}=d_id_j/vol(V)$. It is easy to verify that $vol(S)vol(\\bar{S})/vol(V)=cut_K(S,\\bar{S})$. We have\n\n", "index": 3, "text": "$$\n  \\min_{S\\subseteq V} \\frac{cut_{G_D}(S,\\bar{S}) }{vol(S)vol(\\bar{S})/vol(V)} =  \\min_{S\\subseteq V}\\frac{cut_{G_D}(S,\\bar{S}) }{cut_{K}(S,\\bar{S}) }.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\min_{S\\subseteq V}\\frac{cut_{G_{D}}(S,\\bar{S})}{vol(S)vol(\\bar{S})/vol(V)}=%&#10;\\min_{S\\subseteq V}\\frac{cut_{G_{D}}(S,\\bar{S})}{cut_{K}(S,\\bar{S})}.\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><msub><mi>G</mi><mi>D</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>v</mi></mrow><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><msub><mi>G</mi><mi>D</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><mi>K</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nWith this realization, it becomes evident that incorporating the knowledge graphs ($G_{ML},G_{CL}$) is mainly a degree-of-belief issue, between implicit and {\\em explicit constraints}.  Yet all existing methods insist on handling the explicit constraints separately. For example, \\cite{RangapuramH12} modify the \\textsc{Ncut} optimization function by adding in the numerator the number of violated explicit constraints (independently of them being ML or CL), times a parameter $\\gamma$. In another example, \\cite{WangQD14} solve the spectral relaxation of \\textsc{Ncut}, but under the constraint that the number of satisfied ML constraints minus the number of violated CL constraints is lower bounded by a parameter $\\alpha$. Despite the separate handling of the explicit constraints, degree-of-belief decisions  (reflected by parameters $\\alpha$ and $\\gamma$) are not avoided. The actual handling also appears to be somewhat arbitrary. For instance, most methods take the constraints unweighted, as usually provided by a user, and handle them uniformly; but it is unclear why one constraint in a densely connected part of the graph should be treated equally to another constraint in a less well-connected part. Moreover, most prior methods enforce the use of the balance implicit constraints in $K$, without questioning their role, which may be actually adverserial in some cases.\nIn general, the mechanisms for including the explicit constraints are {\\em oblivious} of the input, or even of the underlying algebra.\n\n\\noindent \\textbf{\\large Our approach.} We choose to temporarily drop the distinction of the constraints into explicit and implicit. We instead assume that we are given one set of ML constraints, and one set of CL constraints, in the form of weighted graphs $G$ and $H$. We then design a generalized spectral clustering method that retains the $k$-way version of the objective shown in equation~\\ref{eq:rethink}. We apply this generalized method to our original problem, after a {\\em merging step} of the explicit and implicit CL/ML constraints into one set of CL/ML constraints.\n\nThe merging step can be left entirely up to the user, who may be able to exploit problem-specific information and provide their choice of weights for $G$ and $H$. Of course, we expect that in most cases explicit CL and ML constraints will be provided in the form of simple unweighted graphs $G_{ML}$ and $G_{CL}$. For this case we provide a simple method that resolves the degree-of-belief issue and constructs $G$ and $H$ {\\em automatically}. The method is heuristic, but not oblivious to the data graph, as they adjust to it. \n\n\n\n\n\n\n\n\n\\section{Related Work}\n\\label{section:related}\n\nThe literature on constrained clustering is quite extensive, as\nthe problem has been pursued under various guises from different communities.\nHere we present a short and unavoidably partial review.   \n\nA number of methods incorporate the constraints via only\nmodifying the data matrix in the standard method. In certain \ncases some or all of the CL constraints are dropped in order to prevent\nthe matrix from turning negative~\\cite{KamvarKM03,DBLP:conf/cvpr/LuC08}.\nThe formulation of~\\cite{RangapuramH12} incorporates all constraints \ninto the data matrix, essentially by adding a {\\em signed Laplacian},\nwhich is a generalization of the Laplacian for graphs with negative weights;\nnotably, their algorithm does not solve a spectral relaxation of the problem but\nattempts to solve the (hard) optimization problem exactly, via a continuous\noptimization approach. \n \nA different approach is proposed in~\\cite{LiLT09}: constraints are used \nin order to improve the embedding obtained through the standard problem, \nbefore applying the partitioning step. In principle this embedding-processing\nstep is orthogonal to methods that compute some embedding (including ours), \nand it can be used to potentially improve them.\n\nA number of other works\nuse the ML and CL constraints to super-impose algebraic constraints onto the spectral  relaxation\nof the standard problem.\nThese additional algebraic constraints usually yield\nmuch harder constrained optimization problems~\\cite{ErikssonOK11,BoleyK13,XuLS09,WangQD14}.\n\nBesides our work, there exists a number of other approaches that reduce constrained\nclustering into generalized eigenvalue problems $Ax =\\lambda B x$ that \ndeviate substantially from than the standard formulation. \nThese methods can be implemented to run fast, as long as:\n(i) linear systems in $A$ can be solved efficiently,\n(ii) $A$ and $B$ are positive semi-definite.  \nSpecifically, \n\\cite{DBLP:conf/cvpr/YuS01, DBLP:journals/pami/YuS04} use\na generalized eigenvalue problem in which $B$ is \na diagonal, but $A$ is not generally amenable to existing efficient\nlinear system solvers.\nIn~\\cite{WangQD14} matrix $A$ is set to be \n the normalized Laplacian of the data graph\n(implicitly attempting to impose the standard balance constraints),\nand $B$ has both positive and negative off-diagonal\nentries representing ML and CL constraints respectively. In the general case $B$ is not\npositive, forcing the computation of full eigenvalue\ndecompositions. However the method can be modified to use a (positive)\nsigned Laplacian as the matrix $B$, as partially observed in~\\cite{Wang:2012}.\nThis modification has a fast implementation. The formulation in~\\cite{RangapuramH12} \nalso leads to a fast implementation of its spectral relaxation.\n\n\n\n\\section{Algorithm and its derivation} \\label{sec:algorithms}\n\n\\subsection{Graph Laplacians} \\label{sec:lap}\nLet $G=(V,E,w)$ be a graph with positive weights. The {\\em Laplacian} $L_G$ of $G$\nis defined by $L_G(i,j) = -w_{ij}$ and\n$L_G(i,i) = \\sum_{j\\neq i} w_{ij}$.\n\nThe graph Laplacian satisfies the following basic identity for all\nvectors $x$:\n\\begin{eqnarray} \\label{eq:quadratic}\n    x^T L_G x & =& \\sum_{i,j} w_{ij}(x_i-x_j)^2.\n\\end{eqnarray}\nGiven a cluster $C\\subseteq V$ we define a cluster indicator vector by\n$ x_C(i) = 1$ if $i \\in C$ and $x_C(i)=0$ otherwise. We have:\n\\begin{eqnarray} \\label{eq:LaplacianCut}\n   x_C^T L_G x_C = cut_G(C,\\bar{C})\n\\end{eqnarray}\nwhere $cut_G(C,\\bar{C})$ denotes the total weight crossing from $C$ to $\\bar{C}$ in $G$.\n\n\n\n\\subsection{The optimization problem}\n\nAs we discussed in section \\ref{sec:rethinking}, we assume that the input consists of two\nweighted graphs, the must-link constraints $G$, and the cannot-link\nconstraints $H$.\n\nOur objective is to partition the node set $V$ into k disjoint clusters $C_{i}$.\nWe define an individual measure of {\\em badness}\nfor each cluster $C_i$:\n\n", "itemtype": "equation", "pos": 7523, "prevtext": "\nIn other words, the \\textsc{Ncut} objective can be viewed as:\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:rethink}\n   \\min_{S\\subseteq V} \\frac{\\textnormal{weight of cut (violated) implicit ML constraints}}{\\textnormal{weight of cut (satisfied) implicit CL constraints}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\min_{S\\subseteq V}\\frac{\\textnormal{weight of cut (violated) implicit ML %&#10;constraints}}{\\textnormal{weight of cut (satisfied) implicit CL constraints}}.\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow></munder><mo>\u2061</mo><mfrac><mtext>weight of cut (violated) implicit ML constraints</mtext><mtext>weight of cut (satisfied) implicit CL constraints</mtext></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nThe numerator is equal to the total weight of the violated ML constraints, because cutting\none such constraint violates it. The denominator is equal to the total weight of the satisfied\nCL constraints, because cutting one such constraint satisfies it. Thus the minimization\nof the individual badness is a sensible objective.\n\nWe would like then to find\nclusters $C_1,\\ldots,C_k$ that minimize the maximum badness, i.e. solve the following problem:\n\n", "itemtype": "equation", "pos": 14301, "prevtext": "\n\nWith this realization, it becomes evident that incorporating the knowledge graphs ($G_{ML},G_{CL}$) is mainly a degree-of-belief issue, between implicit and {\\em explicit constraints}.  Yet all existing methods insist on handling the explicit constraints separately. For example, \\cite{RangapuramH12} modify the \\textsc{Ncut} optimization function by adding in the numerator the number of violated explicit constraints (independently of them being ML or CL), times a parameter $\\gamma$. In another example, \\cite{WangQD14} solve the spectral relaxation of \\textsc{Ncut}, but under the constraint that the number of satisfied ML constraints minus the number of violated CL constraints is lower bounded by a parameter $\\alpha$. Despite the separate handling of the explicit constraints, degree-of-belief decisions  (reflected by parameters $\\alpha$ and $\\gamma$) are not avoided. The actual handling also appears to be somewhat arbitrary. For instance, most methods take the constraints unweighted, as usually provided by a user, and handle them uniformly; but it is unclear why one constraint in a densely connected part of the graph should be treated equally to another constraint in a less well-connected part. Moreover, most prior methods enforce the use of the balance implicit constraints in $K$, without questioning their role, which may be actually adverserial in some cases.\nIn general, the mechanisms for including the explicit constraints are {\\em oblivious} of the input, or even of the underlying algebra.\n\n\\noindent \\textbf{\\large Our approach.} We choose to temporarily drop the distinction of the constraints into explicit and implicit. We instead assume that we are given one set of ML constraints, and one set of CL constraints, in the form of weighted graphs $G$ and $H$. We then design a generalized spectral clustering method that retains the $k$-way version of the objective shown in equation~\\ref{eq:rethink}. We apply this generalized method to our original problem, after a {\\em merging step} of the explicit and implicit CL/ML constraints into one set of CL/ML constraints.\n\nThe merging step can be left entirely up to the user, who may be able to exploit problem-specific information and provide their choice of weights for $G$ and $H$. Of course, we expect that in most cases explicit CL and ML constraints will be provided in the form of simple unweighted graphs $G_{ML}$ and $G_{CL}$. For this case we provide a simple method that resolves the degree-of-belief issue and constructs $G$ and $H$ {\\em automatically}. The method is heuristic, but not oblivious to the data graph, as they adjust to it. \n\n\n\n\n\n\n\n\n\\section{Related Work}\n\\label{section:related}\n\nThe literature on constrained clustering is quite extensive, as\nthe problem has been pursued under various guises from different communities.\nHere we present a short and unavoidably partial review.   \n\nA number of methods incorporate the constraints via only\nmodifying the data matrix in the standard method. In certain \ncases some or all of the CL constraints are dropped in order to prevent\nthe matrix from turning negative~\\cite{KamvarKM03,DBLP:conf/cvpr/LuC08}.\nThe formulation of~\\cite{RangapuramH12} incorporates all constraints \ninto the data matrix, essentially by adding a {\\em signed Laplacian},\nwhich is a generalization of the Laplacian for graphs with negative weights;\nnotably, their algorithm does not solve a spectral relaxation of the problem but\nattempts to solve the (hard) optimization problem exactly, via a continuous\noptimization approach. \n \nA different approach is proposed in~\\cite{LiLT09}: constraints are used \nin order to improve the embedding obtained through the standard problem, \nbefore applying the partitioning step. In principle this embedding-processing\nstep is orthogonal to methods that compute some embedding (including ours), \nand it can be used to potentially improve them.\n\nA number of other works\nuse the ML and CL constraints to super-impose algebraic constraints onto the spectral  relaxation\nof the standard problem.\nThese additional algebraic constraints usually yield\nmuch harder constrained optimization problems~\\cite{ErikssonOK11,BoleyK13,XuLS09,WangQD14}.\n\nBesides our work, there exists a number of other approaches that reduce constrained\nclustering into generalized eigenvalue problems $Ax =\\lambda B x$ that \ndeviate substantially from than the standard formulation. \nThese methods can be implemented to run fast, as long as:\n(i) linear systems in $A$ can be solved efficiently,\n(ii) $A$ and $B$ are positive semi-definite.  \nSpecifically, \n\\cite{DBLP:conf/cvpr/YuS01, DBLP:journals/pami/YuS04} use\na generalized eigenvalue problem in which $B$ is \na diagonal, but $A$ is not generally amenable to existing efficient\nlinear system solvers.\nIn~\\cite{WangQD14} matrix $A$ is set to be \n the normalized Laplacian of the data graph\n(implicitly attempting to impose the standard balance constraints),\nand $B$ has both positive and negative off-diagonal\nentries representing ML and CL constraints respectively. In the general case $B$ is not\npositive, forcing the computation of full eigenvalue\ndecompositions. However the method can be modified to use a (positive)\nsigned Laplacian as the matrix $B$, as partially observed in~\\cite{Wang:2012}.\nThis modification has a fast implementation. The formulation in~\\cite{RangapuramH12} \nalso leads to a fast implementation of its spectral relaxation.\n\n\n\n\\section{Algorithm and its derivation} \\label{sec:algorithms}\n\n\\subsection{Graph Laplacians} \\label{sec:lap}\nLet $G=(V,E,w)$ be a graph with positive weights. The {\\em Laplacian} $L_G$ of $G$\nis defined by $L_G(i,j) = -w_{ij}$ and\n$L_G(i,i) = \\sum_{j\\neq i} w_{ij}$.\n\nThe graph Laplacian satisfies the following basic identity for all\nvectors $x$:\n\\begin{eqnarray} \\label{eq:quadratic}\n    x^T L_G x & =& \\sum_{i,j} w_{ij}(x_i-x_j)^2.\n\\end{eqnarray}\nGiven a cluster $C\\subseteq V$ we define a cluster indicator vector by\n$ x_C(i) = 1$ if $i \\in C$ and $x_C(i)=0$ otherwise. We have:\n\\begin{eqnarray} \\label{eq:LaplacianCut}\n   x_C^T L_G x_C = cut_G(C,\\bar{C})\n\\end{eqnarray}\nwhere $cut_G(C,\\bar{C})$ denotes the total weight crossing from $C$ to $\\bar{C}$ in $G$.\n\n\n\n\\subsection{The optimization problem}\n\nAs we discussed in section \\ref{sec:rethinking}, we assume that the input consists of two\nweighted graphs, the must-link constraints $G$, and the cannot-link\nconstraints $H$.\n\nOur objective is to partition the node set $V$ into k disjoint clusters $C_{i}$.\nWe define an individual measure of {\\em badness}\nfor each cluster $C_i$:\n\n", "index": 7, "text": "\\begin{equation}\n\\phi_i(G,H) = \\frac{cut_{G}(C_i,\\bar{C_i})}{cut_{H}(C_i,\\bar{C_i})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\phi_{i}(G,H)=\\frac{cut_{G}(C_{i},\\bar{C_{i}})}{cut_{H}(C_{i},\\bar{C_{i}})}\" display=\"block\"><mrow><mrow><msub><mi>\u03d5</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi>i</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><msub><mi>t</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mi>i</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>C</mi><mi>i</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nUsing equation \\ref{eq:LaplacianCut}, the above  can be captured in terms of Laplacians: letting $x_{C_i}$\ndenote the indicator vector for cluster $i$, we have\n\n", "itemtype": "equation", "pos": 14849, "prevtext": "\nThe numerator is equal to the total weight of the violated ML constraints, because cutting\none such constraint violates it. The denominator is equal to the total weight of the satisfied\nCL constraints, because cutting one such constraint satisfies it. Thus the minimization\nof the individual badness is a sensible objective.\n\nWe would like then to find\nclusters $C_1,\\ldots,C_k$ that minimize the maximum badness, i.e. solve the following problem:\n\n", "index": 9, "text": "\\begin{equation} \\label{eq:clustergoal}\n{\\Phi}_k = \\textnormal{{\\bf min} $\\max_{i} {\\phi_i}$}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\Phi}_{k}=\\textnormal{{\\bf min} $\\max_{i}{\\phi_{i}}$}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>k</mi></msub><mo>=</mo><mrow><mtext>\ud835\udc26\ud835\udc22\ud835\udc27</mtext><mtext>\u00a0</mtext><mrow><msub><mi>max</mi><mi>i</mi></msub><mo>\u2061</mo><msub><mi>\u03d5</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nTherefore, solving the minimization problem posed in equation \\ref{eq:clustergoal}\namounts to finding $k$ vectors in $\\{0,1\\}^n$ with disjoint support.\n\nNotice that the optimization problem may not be well-defined in the event that there are very few\nCL constraints in $H$. This can be detected easily and the user can be notified. The merging phase\nalso takes automatically care of this case. Thus we assume that the problem is well-defined.\n\n\n\\subsection{Spectral Relaxation}\n\nTo relax the problem we instead look for $k$ vectors in $y_1,\n\\ldots, y_k \\in {\\mathbb R}^n$, such that for all $i\\neq j$, we have\n$y_i L_H y_j = 0$. These $L_H$-{orthogonality} constraints can be viewed\nas a relaxation of the disjointness requirement. Of course their\nparticular form is motivated by the fact that they directly give rise\nto a generalized eigenvalue problem. Concretely, the $k$ vectors $y_i$\nthat minimize the maximum among the $k$\nRayleigh quotients $(y_i^T L_G y_i)/(y_i^T L_H y_i)$ are precisely\nthe generalized eigenvectors corresponding to the $k$ smallest\neigenvalues of the problem:\n$\n     L_G x = \\lambda L_H x.\n$\\footnote{When $H$ is the demand graph $K$ discussed in section~\\ref{sec:pdef},\nthe problem is identical to the standard problem $L_Gx = \\lambda D x$, where $D$ is the diagonal\nof $L_G$. This is because $L_K = D - dd^T/(d^T {\\bf 1}$), and the eigenvectors\nof $L_Gx = \\lambda D x$ are $d$-orthogonal, where $d$ is vector of degrees in $G$.}\nThis fact is well understood and follows from a generalization\nof the min-max characterization of the eigenvalues for symmetric\nmatrices; details can be found for instance in~\\cite{Stewart.Sun}.\n\nNotice that $H$ does not have to be connected. Since we are looking\nfor a minimum, the optimization function\navoids vectors that are in the null space of $L_H$. That means that no restriction\nneeds to be placed on $x$ so that the eigenvalue problem is well defined, other than\nit can't be the constant vector (which is in the null space of both $L_G$ and $L_H$),\nassuming without loss of generality that $G$ is connected.\n\n\n\n\n\n\n\n\n\n\n\\subsection{The embedding}\n\n\n\n\n\nLet $X$ be the $n\\times k$ matrix of the first $k$ generalized eigenvectors\nfor $L_G x= \\lambda L_H x$. The embedding is shown in Figure~\\ref{EmbeddingSpace}.\n\n\n\nWe discuss the intuition behind the embedding.\nWithout step~4 and with $L_H$ replaced with the diagonal $D$,\nthe embedding is exactly the one recently proposed and analyzed in~\\cite{Lee12}.\nIt is a combination of the embeddings\nconsidered in~\\cite{ShiM00,NgJW01,Verma03acomparison}, but\nthe first known to produce clusters with approximation\nguarantees.\n\nThe generalized eigenvalue problem $Lx = \\lambda D x$ can be\nviewed as a simple eigenvalue problem over a space\nendowed with the $D$-inner product: $\\left<x,y\\right>_D = x^T D y$.\nStep~5 normalizes the eigenvectors\nto a unit $D$-norm, i.e. $x^T D x=1$. Given this normalization,\nit is shown in~\\cite{Lee12} that the rows of $U$ at step~7 (vectors\nin $k$-dimensional space) are expected to concentrate in $k$ different {\\em directions}.\nThis justifies steps~8-10 that normalize these row vectors onto the $k$-dimensional\nsphere, in order to concentrate them in a {\\em spatial} sense. Then a geometric\npartitioning algorithm can be applied.\n\n\n\n\\begin{figure} [h]\n\\begin{algorithmic}[1]\n\\REQUIRE\n $X,L_H,d$ \\\\\n\n\n\n\\ENSURE embedding $U \\in {\\mathbb R}^{n\\times k}$, $l \\in {\\mathbb R}^{n\\times 1}$\n\\STATE $u \\leftarrow  1^n$\n\\FOR {$i=1:k$}\n\\STATE $x = X_{:,i}$\n\\STATE $x = x - (x^Td/u^Td)u$\n\\STATE $x = x/\\sqrt{x^T L_H x}$\n\\STATE $U_{:,i} = x$\n\\ENDFOR\n\\FOR {$j=1:n$}\n\\STATE $l_j = ||U_{j,:}||_2$\n\\STATE $U_{j,:} = U_{j,:}/l_j$\n\\ENDFOR\n\\end{algorithmic}\n\\caption{Embedding Computation (based on~\\cite{Lee12}).}\n\\label{EmbeddingSpace}\n\\end{figure}\n\n\n\n\n\nFrom a technical point of view, working with $L_H$ instead of $D$\nmakes almost no difference. $L_H$ is a positive definite matrix. It\ncan be rank-deficient, but the eigenvectors avoid the null space\nof $L_H$, by definition. Thus the geometric intuition about $U$ remains\nthe same if we syntactically replace $D$ by $L_H$.\nHowever, there is a subtlety: $L_G$ and $L_H$ share the constant\nvector in their null spaces. This means that if $x$ is an eigenvector,\nthen for all $c$ the vector $x+ c{\\bf 1}^n$ is also an eigenvector\nwith the same eigenvalue. Among all such possible eigenvectors we pick\none representative: in Step~4 we pick $c$ such that $x+ c{\\bf 1}^n$ is orthogonal to~$d$.\nThe intuition for this is derived from the proof of the Cheeger inequality\nclaimed in section~\\ref{sec:cheeger}; this choice is what\nmakes possible the analysis of a theoretical guarantee for a 2-way cut.\n\n\n\n\n\n\n\n\n\n\\subsection{Computing Eigenvectors}\n\nIt is understood that spectral algorithms based on eigenvector embeddings\ndo not require the exact eigenvectors, but only approximations of them,\nin the sense that the quotients $x^T L x/x^T H x$ are close to\ntheir exact values, i.e. close to the eigenvalues~\\cite{chung1,Lee12}.\nThe computation of such approximate  generalized eigenvectors for $L_G x = \\lambda L_H x$\nis the most time-consuming part of the entire process. The asymptotically fastest known\nalgorithm for the problem runs in\n$O(km \\log^2 m)$ time. It combines a fast Laplacian linear system solver~\\cite{KoutisMP11}\nand a standard power method~\\cite{GoVa96}.\n\nIn practice we use the combinatorial multigrid solver~\\cite{KoutisMT11} which empirically\nruns in $O(m)$ time. The solver provides an approximate inverse for $L_G$\nwhich in turn is used with the preconditioned eigenvalue solver \\textsc{LOBPCG}~\\cite{Knyazev01towardthe}.\n\n\\subsection{Partitioning}\n\nFor the special case when $k=2$, we can compute the second\neigenvector, sort it, and then select the sparsest cut among\nthe $n-1$ possible cuts into\n$\\{v_1,\\ldots,v_i\\}$ and $\\{v_{i+1}\\ldots v_n\\}$, for $i\\in [1,n]$,\nwhere $v_j$ is the vertex that corresponds to coordinate $j$ after\nthe sorting. This `Cheeger sweep' method is associated with the proof of the\nCheeger inequality~\\cite{chung1}, and is also used in the proof\nof the inequality we claim in section~\\ref{sec:cheeger}.\n\n \n\n\n\n\n\n\n\n\n\nIn the general case, given the embedding matrix embedding $U$, the clustering algorithm\ninvokes \\texttt{kmeans}$(U)$ (with a random start), which returns a $k$-partitioning.\n\n\nThe partitioning can be  refined optionally into a $k$-clustering\nby performing a Cheeger sweep among the nodes of each component,\nindependently for each component:\nthe nodes are sorted  according to the values of the corresponding coordinates in the vector $l$\nreturned by the embedding algorithm given in~\\ref{EmbeddingSpace}. We will not\nuse this refinement option in our experiments.\n\n\\subsection{Merging Constraints}\n\nAs we discussed in section~\\ref{sec:pdef}, it is frequently\nthe case that a user provides unweighted constraints\n$G_{ML}$ and $G_{CL}$. Merging these unweighted constraints\nwith the data into one pair of graphs $G$ and $H$\nis an interesting problem.\n\n\nHere we propose a simple heuristic. We construct\ntwo weighted graphs $\\hat{G}_{ML}$ and $\\hat{G}_{CL}$,\nas follows: if edge $(i,j)$ is a constraint,\nwe take its weight in the corresponding\ngraph to be $d_id_j/(d_{\\min}d_{\\max})$, where $d_i$\ndenotes the total incident weight of vertex $i$,\nand $d_{\\min},d_{\\max}$ the minimum and maximum\namong the $d_i$'s.\nWe then let $G= G_D + \\hat{G}_{ML}$ and $H = K/n+ \\hat{G}_{CL}$,\nwhere $K$ is the demand graph\nand $n$ is the size of the data graph, whose edges are\nnormalized to have minimum weight. We include this small\ncopy of $K$ in $H$ in order to render the problem well-defined in all cases\nof user input.\n\nThe intuition behind this choice of weights is better understood in the\ncontext of a sparse unweighted graph. A constraint\non two high-degree vertices is more significant relative to\na constraint on two lower-degree vertices, as it has the potential to\ndrastically change the clustering, if enforced.\nIn addition,\nassuming that noisy/inaccurate constraints are uniformly random,\nthere is a lower probability\nthat a high-degree constraint is inaccurate, simply because its two\nendpoints are relatively rare, due to their high degree.\nFrom an algebraic point of view, it also makes sense\nhaving a higher weight on this edge, in order to be\ncomparable with the neighborhood of $i$ and $j$ and\nhave an effect in the value of the objective function. Notice\nalso that when no constraints are available the method reverts\nto standard spectral clustering.\n\n\n\\iffalse\n\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{A generalized Cheeger inequality} \\label{sec:cheeger}\n\nThe success of the standard spectral clustering method is often\nattributed to the existence of non-trivial approximation guarantees,\nwhich in the 2-way case is given by the Cheeger inequality and the\nassociated method~\\cite{chung1}.\nHere we present a generalization of the Cheeger inequality.\nWe believe that it provides supporting mathematical evidence for the advantages of\nexpressing the constrained clustering problem as a generalized\neigenvalue problem with Laplacians.\n\n\\begin{theorem}\n\\label{thm:generalizedcheeger}\n\nLet $G$ and $H$ be any two weighted graphs and $d$ be\nthe vector containing the degrees of the vertices in $G$.\nFor any vector $x$ such that $x^Td =0$, we have\n\n", "itemtype": "equation", "pos": 15120, "prevtext": "\n\nUsing equation \\ref{eq:LaplacianCut}, the above  can be captured in terms of Laplacians: letting $x_{C_i}$\ndenote the indicator vector for cluster $i$, we have\n\n", "index": 11, "text": "$$\n   {\\phi}_i(G,H) = \\frac{x_{C_i}^T L_{G} x_{C_i}}{x_{C_i}^TL_{H}x_{C_i}}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"{\\phi}_{i}(G,H)=\\frac{x_{C_{i}}^{T}L_{G}x_{C_{i}}}{x_{C_{i}}^{T}L_{H}x_{C_{i}}}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>\u03d5</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mi>x</mi><msub><mi>C</mi><mi>i</mi></msub><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>L</mi><mi>G</mi></msub><mo>\u2062</mo><msub><mi>x</mi><msub><mi>C</mi><mi>i</mi></msub></msub></mrow><mrow><msubsup><mi>x</mi><msub><mi>C</mi><mi>i</mi></msub><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><msub><mi>x</mi><msub><mi>C</mi><mi>i</mi></msub></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nwhere $K$ is the demand graph.\nA cut meeting the guarantee of the inequality can\nbe obtained via a Cheeger sweep on $x$.\n\\end{theorem}\n\nDue to its length, the proof is given separately in section~\\ref{sec:proof}.\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nIn this section, we sample some of our experimental results. We compare our algorithm \\textbf{Fast-GE} against two other methods, \\textbf{CSP}~\\cite{WangQD14} and \\textbf{COSC}~\\cite{RangapuramH12}.\n\n\\textbf{COSC} is an iterative algorithm that attempts to solve exactly an NP-hard discrete optimization problem that captures 2-way constrained clustering; $k$-way partitions are computed via recursive calls to the 2-way partitioner. The  method actually comes in two variants, an exact version which is very slow in all but very small problems, and an approximate `fast' version which has no convergence guarantees. The size of the data in our experiments forces us to use the fast version, \\textbf{COSf}.\n\n\\textbf{CSP} reduces constrained clustering to a generalized eigenvalue problem. However, the problem is indefinite and the method requires the computation of a full eigenvalue decomposition.\n\nWe focus on these two methods because of their readily available implementations but mostly because the corresponding papers provide sufficient evidence that they outperform other competing methods. We also selected them because they can be both modified or extended into methods that have fast implementations.\n\n\\subsection{Some negative findings.} \\textbf{COSC} has a natural spectral relaxation into a generalized eigenvalue problem $Ax = \\lambda B x$ where $A$ is a signed Laplacian and $B$ is a diagonal. \\textbf{CSP} can also be modified by replacing the indefinite matrix $Q$ of its generalized eigenvalue problem with a signed Laplacian that counts the number of satisfied constraints. In this way both methods become scalable. We did a number of experiments based on these observations. The results were disappointing, especially when $k>2$. The output quality was comparable or worse to that obtained by \\textbf{COSf} and \\textbf{CSP} in the reported experiments. We attribute this the less-clean mathematical properties of the signed Laplacian.\n\nWe also experimented with the automated merging phase of \\textbf{Fast-GE}. Specifically we tried adding more significance to the standard implicit balance constraints, by increasing the coefficient of the demand graph $K$ in graph $H$. The output deteriorates (often significantly) for the more challenging problems we tried. This supports our decision to not enforce the use of balance constraints in our generalized formulation, unlike all prior methods.\n\n\n\\subsection{Synthetic Data Sets.} We begin with a number of small synthetic experiments. The purpose is to test the output quality, especially under the presence of noise.\n\n\nWe generically apply the following construction: we chose uniformly at random a set of nodes for which we assume cluster-membership information is provided. The cluster-membership information gives unweighted ML and CL constraints in the obvious way. We also add random noise in the data.\n\nMore concretely, we say that a graph $G$ is generated from the ensemble \\textit{NoisyKnn}($n,k_g,l_g$)  with parameters $n$, $k_g$ and $l_g$ if $G$ of size $n$ is the union of two (non-necessarily disjoint) graphs $H_1$ and $H_2$ each on the same set of $n$ vertices\n$ G = H_1 \\cup H_2,$\nwhere $H_1$ is a k-nearest-neighbor (knn) graph with each node connected to its $k_g$ nearest neighbors, and $H_2$ is an Erd\\H{o}s-R\\'{e}nyi graph where each edge appears independently with probability ${l_g}/{n}$. One may interpret the  parameter  $l_g$ as the noise level in the data, since the larger $l_g$ the more random edges are wired across the different clusters, thus rendering the problem more difficult to solve. In other words, the \\textit{planted} clusters are harder to detect when there is a large amount of noise in the data, obscuring the separation of the clusters.\n\nSince in these synthetic data sets, the ground truth partition is available, we measure the accuracy of the methods by the popular Rand Index~\\cite{rand1971}. The Rand Index indicates how well the resulting partition matches the ground truth partition; a value closer to 1 indicates an almost perfect recovery, while a value closer to 0 indicates an almost random assignment of the nodes into clusters.\n\n\\noindent \\textbf{Four Moons.}  Our first synthetic example is the `Four-Moons' data set, where the underlying graph $G$ is generated from the ensemble\n\n\\textit{NoisyKnn}($n=1500, k_g=30,l_g=15$). \n\nThe plots in Figure~\\ref{fig:FourMoons1500_curves} show the accuracy and running times of all three methods on this example, while Figure~\\ref{fig:FourMoons1500_output} shows a random instance of the clustering returned by each of the methods, with 75 constraints. The accuracy of \\textbf{FAST-GE} and \\textbf{COSf} is very similar,\nwith \\textbf{FAST-GE} being somewhat better with more constraints, as shown in Figure~\\ref{fig:FourMoons1500_curves}. However \\textbf{FAST-GE} is already at least 4x faster than \\textbf{COSf}, for this size.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Segmentation for a random instance of the Four-Moons data set with $75$ labels produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE} (right).}\n\\label{fig:FourMoons1500_output}\n\\end{figure}\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.48\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\end{center}\n\\caption{Accuracy and running times for the Four-Moons data set, where the underlying graph given by the model NoisyKnn($n=1500, k=30,l=15$), for varying  number of constraints.\nTime is in logarithmic scale. The bars indicate the variance in the output over random trials\nusing the same number of constraints.}\n\\label{fig:FourMoons1500_curves}\n\\end{figure}\n\n\n\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Double-Moons data set, with $n=500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=3$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:DoubleMoons500}\n\\end{figure}\n\n\n\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Double-Moons data set, with $n=1500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=15$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:DoubleMoons1500}\n\\end{figure}\n\\fi\n\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Four-Moons data set, with $n=500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=3$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE}(right).\n}\n\\label{fig:FourMoons500}\n\\end{figure}\n\\fi\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Four-Moons data set, with $n=1500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=15$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:FourMoons1500}\n\\end{figure}\n\\fi\n\n\n\n\n\n\\noindent \\textbf{PACM.} Our second synthetic example is the somewhat more irregular \\textit{PACM} graph, formed by a cloud of $n=426$ points in the shape of letters $\\{ P,A,C,M\\}$, whose topology renders the segmentation particularly challenging. The details about this data set are given in the section~\\ref{sec:additional}. Here we only present a visualization of the obtained segmentations.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_CSP_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_COSf_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Segmentation for a random instance of the PACM data set with $125$ labels produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE} (right)}\n\\label{fig:PACM_clusterings}\n\\end{figure}\n\n\n\n\n\n\\subsection{Image  Data}\nIn terms of real data, we consider two very different applications. Our first application is to segmentation of real images, where the  underlying grid graph is given by the affinity matrix of the image, computed using the RBF kernel based on the grayscale values.\n\nWe construct the constraints by assigning cluster-membership information to a very small number of the pixels, which are shown colored in the pictures below. The cluster-membership information is then turned into pairwise constraints in the obvious way. Our output is obtained by running $k$-means 20 times and selecting the best segmentation according to the $k$-means objective value.\n\n\\noindent \\textbf{Patras.} Figure \\ref{fig:PatrasLarge} shows the 5-way segmentation of an image with approximately 44K pixels, which our method is able to detect in under \\textbf{3 seconds}. The size of this problem is prohibitive for \\textbf{CSP}. The \\textbf{COSf} algorithm runs in \\textbf{40 seconds} and while it does better on the lower part of the image it erroneously merges two of the clusters (the red and the blue one) into a single region.\n\n\\begin{figure}[h]\n\\begin{center}\n{\\includegraphics[width=0.48\\columnwidth]{figures/rec_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432} }\n{\\includegraphics[width=0.48\\columnwidth]{figures/rec_COSC-FAST_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432} }\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i1_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432}\n\\end{center}\n\\vspace{-3mm}\n\\caption{ \\textit{Patras}: Top-left: Output of \\textbf{FAST-GE}, in 2.8 seconds. Top-right: output of \\textbf{COSf}, in 40.2 seconds. Bottom:  heatmaps for the first two eigenvectors computed by \\textbf{FAST-GE}.\n\n}\n\\label{fig:PatrasLarge}\n\\end{figure}\n\n\n\n\n\\noindent {\\bf Santorini.}\nIn Figure \\ref{fig:Santorini} we test our proposed method on the \\textit{Santorini} image, with approximately $250K$ pixels. Our approach successfully recovers a 4-way partitioning, with few errors, in just \\textbf{15 seconds}. Computing clusterings in data of this size is infeasible for \\textbf{CSP}. The output of the \\textbf{COSf} method, which runs in over \\textbf{260 seconds}, is meaningless.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/rec_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\includegraphics[width=0.48\\columnwidth]{figures/_COSC_fast_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\end{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i1_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\n\\vspace{-3mm}\n\\caption{ \\textit{Santorini}: Left: output of \\textbf{FAST-GE}, in 15.2 seconds. Right: output of \\textbf{COSf}, in 263.6  seconds. Bottom: heatmaps for the first two eigenvectors computed by  \\textbf{FAST-GE}.}\n\\label{fig:Santorini}\n\\end{figure}\n\n\n\n\\noindent \\textbf{Soccer.} In Figure \\ref{fig:Soccer} we consider one last \\textit{Soccer} image, with approximately 1.1 million pixels. We compute a 5-way partitioning using the \\textbf{Fast-GE}  method in just \\textbf{94 seconds}. Note that while k-means clustering hinders some of the details in the image, the individual eigenvectors are able to capture finer details, such as the soccer ball for example, as shown in the two bottom plots of the same Figure \\ref{fig:Soccer}. The output of the \\textbf{COSf} method is obtained in {\\bf 25 minutes} and is again  meaningless.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/soccer_image}\n\\includegraphics[width=0.48\\columnwidth]{figures/_PowerFast_kmeans_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\includegraphics[width=0.48\\columnwidth]{figures/_COSC_fast_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\end{center}\n\\vspace{-3mm}\n\\caption{Top-right: output of \\textbf{FAST-GE}, in under 94 seconds. Bottom-right: output of \\textbf{COSf} in 25 minutes. Bottom-left: heat-maps of eigenvectors. }\n\\label{fig:Soccer}\n\\end{figure}\n\n\n\\subsection{Friendship Networks}\nOur final data sets represent Facebook networks in American colleges. The work in~\\cite{traud2012Facebook} studies the structure of Facebook  networks at one hundred American colleges and universities at a single point in time (2005) and investigate the community structure at each institution, as well as the impact and correlation of various self-identified user characteristics (such as residence, class year, major, and high school) with the identified network communities. While at many institutions, the community structures are organized almost exclusively according to class year, as pointed out in \\cite{TraudSIAMreview},\nother institutions are known to be organized almost exclusively according to its undergraduate \\textit{House} system (dormitory residence), which is very well reflected in the identified communities. It is thus a natural assumption to consider the dormitory affiliation as the ground truth clustering, and aim to recover this underlying structure from the available friendship graph and any available constraints. We add constraints to the clustering problem by sampling uniformly at random nodes in the graph, and the resulting pairwise constraints are generated depending on whether the two nodes belong to the same cluster or not. In order for us to be able to compare to the computationally expensive \\textbf{CSP} method, we consider two small-sized schools,\nSimmons College ($  n=850$, $\\bar{d}=36$, $k=10$) and Haverford College ($n=1025$, $\\bar{d}=72$, $k=15$), where $\\bar{d}$ denotes the average degree in the graph and $k$ the number of clusters. For both examples, \\textbf{FAST-GE} yields more accurate results than both \\textbf{CSP} and \\textbf{COSf}, and does so at a much smaller computational cost.\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Simmons_n_850_none_0_noiseDeg_0_cons_clique_nrExp5_Errors.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Simmons_n_850_none_0_noiseDeg_0_cons_clique_nrExp5_RunningTimes.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Haverford76_n_1025_none_0_noiseDeg_0_cons_clique_nrExp5_Errors.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Haverford76_n_1025_none_0_noiseDeg_0_cons_clique_nrExp5_RunningTimes.eps}\n\\end{center}\n\\vspace{-1mm}\n\\caption{\\textit{Facebook networks}. Top: accuracy and running times for the Simmons College ($  n=850$, $\\bar{d}=36$, $k=10$). Bottom: accuracy and running times for Haverford College ($n=1025$, $\\bar{d}=72$, $k=15$). Time is in logarithmic scale. }\n\\label{fig:Facebook}\n\\end{figure}\n\n\n\n\\section{Final Remarks}\n\nWe presented a spectral method that reduces constrained\nclustering into a generalized eigenvalue problem in which both matrices\nare Laplacians. This  offers\ntwo advantages that are not simultaneously shared by\nany of the previous methods: an efficient implementation\nand an approximation guarantee for the 2-way partitioning problem\nin the form of a generalized Cheeger inequality. In practice\nthis translates to a method that is at least 10x faster than some of\nthe best existing algorithms, while producing output of superior quality.\nIts speed makes our method a good candidate for some type of\niteration, e.g. as in~\\cite{TolliverM06},\nor interactive user feedback, that would further improve its output.\n\nWe view the Cheeger inequality we\npresented in section~\\ref{sec:cheeger}\nas indicative of the rich\nmathematical properties of generalized Laplacian\neigenvalue problems. We expect that tighter\nversions\nare to be discovered, along the lines of~\\cite{KLLGT13}.\nFinding $k$-way generalizations of the Cheeger inequality,\nas in~\\cite{Lee12}, poses an interesting open problem.\n\n\n\n\n{\\small\n\\flushend\n\\bibliographystyle{nalpha}\n\n}\n\n\n\\newpage\n\\onecolumn\n\\section{Proof of the Generalized Cheeger Inequality} \\label{sec:proof}\n\nWe begin with two Lemmas.\n\n\\begin{lemma} \\label{th:splitting}\nFor all $a_i, b_i > 0$ we have\n    \n", "itemtype": "equation", "pos": 24478, "prevtext": "\nTherefore, solving the minimization problem posed in equation \\ref{eq:clustergoal}\namounts to finding $k$ vectors in $\\{0,1\\}^n$ with disjoint support.\n\nNotice that the optimization problem may not be well-defined in the event that there are very few\nCL constraints in $H$. This can be detected easily and the user can be notified. The merging phase\nalso takes automatically care of this case. Thus we assume that the problem is well-defined.\n\n\n\\subsection{Spectral Relaxation}\n\nTo relax the problem we instead look for $k$ vectors in $y_1,\n\\ldots, y_k \\in {\\mathbb R}^n$, such that for all $i\\neq j$, we have\n$y_i L_H y_j = 0$. These $L_H$-{orthogonality} constraints can be viewed\nas a relaxation of the disjointness requirement. Of course their\nparticular form is motivated by the fact that they directly give rise\nto a generalized eigenvalue problem. Concretely, the $k$ vectors $y_i$\nthat minimize the maximum among the $k$\nRayleigh quotients $(y_i^T L_G y_i)/(y_i^T L_H y_i)$ are precisely\nthe generalized eigenvectors corresponding to the $k$ smallest\neigenvalues of the problem:\n$\n     L_G x = \\lambda L_H x.\n$\\footnote{When $H$ is the demand graph $K$ discussed in section~\\ref{sec:pdef},\nthe problem is identical to the standard problem $L_Gx = \\lambda D x$, where $D$ is the diagonal\nof $L_G$. This is because $L_K = D - dd^T/(d^T {\\bf 1}$), and the eigenvectors\nof $L_Gx = \\lambda D x$ are $d$-orthogonal, where $d$ is vector of degrees in $G$.}\nThis fact is well understood and follows from a generalization\nof the min-max characterization of the eigenvalues for symmetric\nmatrices; details can be found for instance in~\\cite{Stewart.Sun}.\n\nNotice that $H$ does not have to be connected. Since we are looking\nfor a minimum, the optimization function\navoids vectors that are in the null space of $L_H$. That means that no restriction\nneeds to be placed on $x$ so that the eigenvalue problem is well defined, other than\nit can't be the constant vector (which is in the null space of both $L_G$ and $L_H$),\nassuming without loss of generality that $G$ is connected.\n\n\n\n\n\n\n\n\n\n\n\\subsection{The embedding}\n\n\n\n\n\nLet $X$ be the $n\\times k$ matrix of the first $k$ generalized eigenvectors\nfor $L_G x= \\lambda L_H x$. The embedding is shown in Figure~\\ref{EmbeddingSpace}.\n\n\n\nWe discuss the intuition behind the embedding.\nWithout step~4 and with $L_H$ replaced with the diagonal $D$,\nthe embedding is exactly the one recently proposed and analyzed in~\\cite{Lee12}.\nIt is a combination of the embeddings\nconsidered in~\\cite{ShiM00,NgJW01,Verma03acomparison}, but\nthe first known to produce clusters with approximation\nguarantees.\n\nThe generalized eigenvalue problem $Lx = \\lambda D x$ can be\nviewed as a simple eigenvalue problem over a space\nendowed with the $D$-inner product: $\\left<x,y\\right>_D = x^T D y$.\nStep~5 normalizes the eigenvectors\nto a unit $D$-norm, i.e. $x^T D x=1$. Given this normalization,\nit is shown in~\\cite{Lee12} that the rows of $U$ at step~7 (vectors\nin $k$-dimensional space) are expected to concentrate in $k$ different {\\em directions}.\nThis justifies steps~8-10 that normalize these row vectors onto the $k$-dimensional\nsphere, in order to concentrate them in a {\\em spatial} sense. Then a geometric\npartitioning algorithm can be applied.\n\n\n\n\\begin{figure} [h]\n\\begin{algorithmic}[1]\n\\REQUIRE\n $X,L_H,d$ \\\\\n\n\n\n\\ENSURE embedding $U \\in {\\mathbb R}^{n\\times k}$, $l \\in {\\mathbb R}^{n\\times 1}$\n\\STATE $u \\leftarrow  1^n$\n\\FOR {$i=1:k$}\n\\STATE $x = X_{:,i}$\n\\STATE $x = x - (x^Td/u^Td)u$\n\\STATE $x = x/\\sqrt{x^T L_H x}$\n\\STATE $U_{:,i} = x$\n\\ENDFOR\n\\FOR {$j=1:n$}\n\\STATE $l_j = ||U_{j,:}||_2$\n\\STATE $U_{j,:} = U_{j,:}/l_j$\n\\ENDFOR\n\\end{algorithmic}\n\\caption{Embedding Computation (based on~\\cite{Lee12}).}\n\\label{EmbeddingSpace}\n\\end{figure}\n\n\n\n\n\nFrom a technical point of view, working with $L_H$ instead of $D$\nmakes almost no difference. $L_H$ is a positive definite matrix. It\ncan be rank-deficient, but the eigenvectors avoid the null space\nof $L_H$, by definition. Thus the geometric intuition about $U$ remains\nthe same if we syntactically replace $D$ by $L_H$.\nHowever, there is a subtlety: $L_G$ and $L_H$ share the constant\nvector in their null spaces. This means that if $x$ is an eigenvector,\nthen for all $c$ the vector $x+ c{\\bf 1}^n$ is also an eigenvector\nwith the same eigenvalue. Among all such possible eigenvectors we pick\none representative: in Step~4 we pick $c$ such that $x+ c{\\bf 1}^n$ is orthogonal to~$d$.\nThe intuition for this is derived from the proof of the Cheeger inequality\nclaimed in section~\\ref{sec:cheeger}; this choice is what\nmakes possible the analysis of a theoretical guarantee for a 2-way cut.\n\n\n\n\n\n\n\n\n\n\\subsection{Computing Eigenvectors}\n\nIt is understood that spectral algorithms based on eigenvector embeddings\ndo not require the exact eigenvectors, but only approximations of them,\nin the sense that the quotients $x^T L x/x^T H x$ are close to\ntheir exact values, i.e. close to the eigenvalues~\\cite{chung1,Lee12}.\nThe computation of such approximate  generalized eigenvectors for $L_G x = \\lambda L_H x$\nis the most time-consuming part of the entire process. The asymptotically fastest known\nalgorithm for the problem runs in\n$O(km \\log^2 m)$ time. It combines a fast Laplacian linear system solver~\\cite{KoutisMP11}\nand a standard power method~\\cite{GoVa96}.\n\nIn practice we use the combinatorial multigrid solver~\\cite{KoutisMT11} which empirically\nruns in $O(m)$ time. The solver provides an approximate inverse for $L_G$\nwhich in turn is used with the preconditioned eigenvalue solver \\textsc{LOBPCG}~\\cite{Knyazev01towardthe}.\n\n\\subsection{Partitioning}\n\nFor the special case when $k=2$, we can compute the second\neigenvector, sort it, and then select the sparsest cut among\nthe $n-1$ possible cuts into\n$\\{v_1,\\ldots,v_i\\}$ and $\\{v_{i+1}\\ldots v_n\\}$, for $i\\in [1,n]$,\nwhere $v_j$ is the vertex that corresponds to coordinate $j$ after\nthe sorting. This `Cheeger sweep' method is associated with the proof of the\nCheeger inequality~\\cite{chung1}, and is also used in the proof\nof the inequality we claim in section~\\ref{sec:cheeger}.\n\n \n\n\n\n\n\n\n\n\n\nIn the general case, given the embedding matrix embedding $U$, the clustering algorithm\ninvokes \\texttt{kmeans}$(U)$ (with a random start), which returns a $k$-partitioning.\n\n\nThe partitioning can be  refined optionally into a $k$-clustering\nby performing a Cheeger sweep among the nodes of each component,\nindependently for each component:\nthe nodes are sorted  according to the values of the corresponding coordinates in the vector $l$\nreturned by the embedding algorithm given in~\\ref{EmbeddingSpace}. We will not\nuse this refinement option in our experiments.\n\n\\subsection{Merging Constraints}\n\nAs we discussed in section~\\ref{sec:pdef}, it is frequently\nthe case that a user provides unweighted constraints\n$G_{ML}$ and $G_{CL}$. Merging these unweighted constraints\nwith the data into one pair of graphs $G$ and $H$\nis an interesting problem.\n\n\nHere we propose a simple heuristic. We construct\ntwo weighted graphs $\\hat{G}_{ML}$ and $\\hat{G}_{CL}$,\nas follows: if edge $(i,j)$ is a constraint,\nwe take its weight in the corresponding\ngraph to be $d_id_j/(d_{\\min}d_{\\max})$, where $d_i$\ndenotes the total incident weight of vertex $i$,\nand $d_{\\min},d_{\\max}$ the minimum and maximum\namong the $d_i$'s.\nWe then let $G= G_D + \\hat{G}_{ML}$ and $H = K/n+ \\hat{G}_{CL}$,\nwhere $K$ is the demand graph\nand $n$ is the size of the data graph, whose edges are\nnormalized to have minimum weight. We include this small\ncopy of $K$ in $H$ in order to render the problem well-defined in all cases\nof user input.\n\nThe intuition behind this choice of weights is better understood in the\ncontext of a sparse unweighted graph. A constraint\non two high-degree vertices is more significant relative to\na constraint on two lower-degree vertices, as it has the potential to\ndrastically change the clustering, if enforced.\nIn addition,\nassuming that noisy/inaccurate constraints are uniformly random,\nthere is a lower probability\nthat a high-degree constraint is inaccurate, simply because its two\nendpoints are relatively rare, due to their high degree.\nFrom an algebraic point of view, it also makes sense\nhaving a higher weight on this edge, in order to be\ncomparable with the neighborhood of $i$ and $j$ and\nhave an effect in the value of the objective function. Notice\nalso that when no constraints are available the method reverts\nto standard spectral clustering.\n\n\n\\iffalse\n\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{A generalized Cheeger inequality} \\label{sec:cheeger}\n\nThe success of the standard spectral clustering method is often\nattributed to the existence of non-trivial approximation guarantees,\nwhich in the 2-way case is given by the Cheeger inequality and the\nassociated method~\\cite{chung1}.\nHere we present a generalization of the Cheeger inequality.\nWe believe that it provides supporting mathematical evidence for the advantages of\nexpressing the constrained clustering problem as a generalized\neigenvalue problem with Laplacians.\n\n\\begin{theorem}\n\\label{thm:generalizedcheeger}\n\nLet $G$ and $H$ be any two weighted graphs and $d$ be\nthe vector containing the degrees of the vertices in $G$.\nFor any vector $x$ such that $x^Td =0$, we have\n\n", "index": 13, "text": "\n\\[\n\\frac{x^TL_Gx}{x^TL_Hx} \\geq \\phi(G,K) \\cdot \\phi(G, H)/4,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\frac{x^{T}L_{G}x}{x^{T}L_{H}x}\\geq\\phi(G,K)\\cdot\\phi(G,H)/4,\" display=\"block\"><mrow><mrow><mfrac><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>G</mi></msub><mo>\u2062</mo><mi>x</mi></mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>\u2265</mo><mrow><mrow><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>\u03d5</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mn>4</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{lemma} \\label{th:DvsDG}\nLet $G$ be a graph, $d$ be the vector containing the degrees of the vertices,\nand $D$ be corresponding diagonal matrix. For\nall vectors $x$ where $x^Td = 0$ we have\n\n", "itemtype": "equation", "pos": 45160, "prevtext": "\nwhere $K$ is the demand graph.\nA cut meeting the guarantee of the inequality can\nbe obtained via a Cheeger sweep on $x$.\n\\end{theorem}\n\nDue to its length, the proof is given separately in section~\\ref{sec:proof}.\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nIn this section, we sample some of our experimental results. We compare our algorithm \\textbf{Fast-GE} against two other methods, \\textbf{CSP}~\\cite{WangQD14} and \\textbf{COSC}~\\cite{RangapuramH12}.\n\n\\textbf{COSC} is an iterative algorithm that attempts to solve exactly an NP-hard discrete optimization problem that captures 2-way constrained clustering; $k$-way partitions are computed via recursive calls to the 2-way partitioner. The  method actually comes in two variants, an exact version which is very slow in all but very small problems, and an approximate `fast' version which has no convergence guarantees. The size of the data in our experiments forces us to use the fast version, \\textbf{COSf}.\n\n\\textbf{CSP} reduces constrained clustering to a generalized eigenvalue problem. However, the problem is indefinite and the method requires the computation of a full eigenvalue decomposition.\n\nWe focus on these two methods because of their readily available implementations but mostly because the corresponding papers provide sufficient evidence that they outperform other competing methods. We also selected them because they can be both modified or extended into methods that have fast implementations.\n\n\\subsection{Some negative findings.} \\textbf{COSC} has a natural spectral relaxation into a generalized eigenvalue problem $Ax = \\lambda B x$ where $A$ is a signed Laplacian and $B$ is a diagonal. \\textbf{CSP} can also be modified by replacing the indefinite matrix $Q$ of its generalized eigenvalue problem with a signed Laplacian that counts the number of satisfied constraints. In this way both methods become scalable. We did a number of experiments based on these observations. The results were disappointing, especially when $k>2$. The output quality was comparable or worse to that obtained by \\textbf{COSf} and \\textbf{CSP} in the reported experiments. We attribute this the less-clean mathematical properties of the signed Laplacian.\n\nWe also experimented with the automated merging phase of \\textbf{Fast-GE}. Specifically we tried adding more significance to the standard implicit balance constraints, by increasing the coefficient of the demand graph $K$ in graph $H$. The output deteriorates (often significantly) for the more challenging problems we tried. This supports our decision to not enforce the use of balance constraints in our generalized formulation, unlike all prior methods.\n\n\n\\subsection{Synthetic Data Sets.} We begin with a number of small synthetic experiments. The purpose is to test the output quality, especially under the presence of noise.\n\n\nWe generically apply the following construction: we chose uniformly at random a set of nodes for which we assume cluster-membership information is provided. The cluster-membership information gives unweighted ML and CL constraints in the obvious way. We also add random noise in the data.\n\nMore concretely, we say that a graph $G$ is generated from the ensemble \\textit{NoisyKnn}($n,k_g,l_g$)  with parameters $n$, $k_g$ and $l_g$ if $G$ of size $n$ is the union of two (non-necessarily disjoint) graphs $H_1$ and $H_2$ each on the same set of $n$ vertices\n$ G = H_1 \\cup H_2,$\nwhere $H_1$ is a k-nearest-neighbor (knn) graph with each node connected to its $k_g$ nearest neighbors, and $H_2$ is an Erd\\H{o}s-R\\'{e}nyi graph where each edge appears independently with probability ${l_g}/{n}$. One may interpret the  parameter  $l_g$ as the noise level in the data, since the larger $l_g$ the more random edges are wired across the different clusters, thus rendering the problem more difficult to solve. In other words, the \\textit{planted} clusters are harder to detect when there is a large amount of noise in the data, obscuring the separation of the clusters.\n\nSince in these synthetic data sets, the ground truth partition is available, we measure the accuracy of the methods by the popular Rand Index~\\cite{rand1971}. The Rand Index indicates how well the resulting partition matches the ground truth partition; a value closer to 1 indicates an almost perfect recovery, while a value closer to 0 indicates an almost random assignment of the nodes into clusters.\n\n\\noindent \\textbf{Four Moons.}  Our first synthetic example is the `Four-Moons' data set, where the underlying graph $G$ is generated from the ensemble\n\n\\textit{NoisyKnn}($n=1500, k_g=30,l_g=15$). \n\nThe plots in Figure~\\ref{fig:FourMoons1500_curves} show the accuracy and running times of all three methods on this example, while Figure~\\ref{fig:FourMoons1500_output} shows a random instance of the clustering returned by each of the methods, with 75 constraints. The accuracy of \\textbf{FAST-GE} and \\textbf{COSf} is very similar,\nwith \\textbf{FAST-GE} being somewhat better with more constraints, as shown in Figure~\\ref{fig:FourMoons1500_curves}. However \\textbf{FAST-GE} is already at least 4x faster than \\textbf{COSf}, for this size.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Segmentation for a random instance of the Four-Moons data set with $75$ labels produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE} (right).}\n\\label{fig:FourMoons1500_output}\n\\end{figure}\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.48\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\end{center}\n\\caption{Accuracy and running times for the Four-Moons data set, where the underlying graph given by the model NoisyKnn($n=1500, k=30,l=15$), for varying  number of constraints.\nTime is in logarithmic scale. The bars indicate the variance in the output over random trials\nusing the same number of constraints.}\n\\label{fig:FourMoons1500_curves}\n\\end{figure}\n\n\n\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Double_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Double-Moons data set, with $n=500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=3$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:DoubleMoons500}\n\\end{figure}\n\n\n\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Double_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Double-Moons data set, with $n=1500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=15$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:DoubleMoons1500}\n\\end{figure}\n\\fi\n\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_500_knn_30_noiseDeg_3_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Four-Moons data set, with $n=500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=3$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE}(right).\n}\n\\label{fig:FourMoons500}\n\\end{figure}\n\\fi\n\n\\iffalse\n\\begin{figure}[p!]\n\\begin{center}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_Errors.eps}\n\\includegraphics[width=0.47\\columnwidth]{figures/Synthetic/AVG_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10_RunningTimes}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_CSP_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_COSf_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\includegraphics[width=0.32\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C75_Four_moons_n_1500_knn_30_noiseDeg_15_cons_clique_nrExp10}\n\\end{center}\n\\caption{Top: Accuracy and running times for the Four-Moons data set, with $n=1500$ nodes, and an underlying graph is given by the model NoisyKnn($k=30, l=15$), as we vary the number of constraints. We average the results over 20 experiments.. Bottom: Segmentation for a random instance produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and FAST-GE (right).\n}\n\\label{fig:FourMoons1500}\n\\end{figure}\n\\fi\n\n\n\n\n\n\\noindent \\textbf{PACM.} Our second synthetic example is the somewhat more irregular \\textit{PACM} graph, formed by a cloud of $n=426$ points in the shape of letters $\\{ P,A,C,M\\}$, whose topology renders the segmentation particularly challenging. The details about this data set are given in the section~\\ref{sec:additional}. Here we only present a visualization of the obtained segmentations.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_CSP_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_COSf_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Segmentation for a random instance of the PACM data set with $125$ labels produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE} (right)}\n\\label{fig:PACM_clusterings}\n\\end{figure}\n\n\n\n\n\n\\subsection{Image  Data}\nIn terms of real data, we consider two very different applications. Our first application is to segmentation of real images, where the  underlying grid graph is given by the affinity matrix of the image, computed using the RBF kernel based on the grayscale values.\n\nWe construct the constraints by assigning cluster-membership information to a very small number of the pixels, which are shown colored in the pictures below. The cluster-membership information is then turned into pairwise constraints in the obvious way. Our output is obtained by running $k$-means 20 times and selecting the best segmentation according to the $k$-means objective value.\n\n\\noindent \\textbf{Patras.} Figure \\ref{fig:PatrasLarge} shows the 5-way segmentation of an image with approximately 44K pixels, which our method is able to detect in under \\textbf{3 seconds}. The size of this problem is prohibitive for \\textbf{CSP}. The \\textbf{COSf} algorithm runs in \\textbf{40 seconds} and while it does better on the lower part of the image it erroneously merges two of the clusters (the red and the blue one) into a single region.\n\n\\begin{figure}[h]\n\\begin{center}\n{\\includegraphics[width=0.48\\columnwidth]{figures/rec_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432} }\n{\\includegraphics[width=0.48\\columnwidth]{figures/rec_COSC-FAST_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432} }\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i1_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Patras_K5_n_44589_givenLabels_0_noiseDeg_0_cons_clique_C432}\n\\end{center}\n\\vspace{-3mm}\n\\caption{ \\textit{Patras}: Top-left: Output of \\textbf{FAST-GE}, in 2.8 seconds. Top-right: output of \\textbf{COSf}, in 40.2 seconds. Bottom:  heatmaps for the first two eigenvectors computed by \\textbf{FAST-GE}.\n\n}\n\\label{fig:PatrasLarge}\n\\end{figure}\n\n\n\n\n\\noindent {\\bf Santorini.}\nIn Figure \\ref{fig:Santorini} we test our proposed method on the \\textit{Santorini} image, with approximately $250K$ pixels. Our approach successfully recovers a 4-way partitioning, with few errors, in just \\textbf{15 seconds}. Computing clusterings in data of this size is infeasible for \\textbf{CSP}. The output of the \\textbf{COSf} method, which runs in over \\textbf{260 seconds}, is meaningless.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/rec_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\includegraphics[width=0.48\\columnwidth]{figures/_COSC_fast_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\end{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i1_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Santorini4_K4_n_263132_givenLabels_0_noiseDeg_0_cons_clique_C288}\n\n\\vspace{-3mm}\n\\caption{ \\textit{Santorini}: Left: output of \\textbf{FAST-GE}, in 15.2 seconds. Right: output of \\textbf{COSf}, in 263.6  seconds. Bottom: heatmaps for the first two eigenvectors computed by  \\textbf{FAST-GE}.}\n\\label{fig:Santorini}\n\\end{figure}\n\n\n\n\\noindent \\textbf{Soccer.} In Figure \\ref{fig:Soccer} we consider one last \\textit{Soccer} image, with approximately 1.1 million pixels. We compute a 5-way partitioning using the \\textbf{Fast-GE}  method in just \\textbf{94 seconds}. Note that while k-means clustering hinders some of the details in the image, the individual eigenvectors are able to capture finer details, such as the soccer ball for example, as shown in the two bottom plots of the same Figure \\ref{fig:Soccer}. The output of the \\textbf{COSf} method is obtained in {\\bf 25 minutes} and is again  meaningless.\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.48\\columnwidth]{figures/soccer_image}\n\\includegraphics[width=0.48\\columnwidth]{figures/_PowerFast_kmeans_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\includegraphics[width=0.48\\columnwidth]{figures/heat/heat_i2_FAST-GE_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\includegraphics[width=0.48\\columnwidth]{figures/_COSC_fast_Soccer4_K5_n_1134092_givenLabels_0_noiseDeg_0_cons_clique_C540}\n\\end{center}\n\\vspace{-3mm}\n\\caption{Top-right: output of \\textbf{FAST-GE}, in under 94 seconds. Bottom-right: output of \\textbf{COSf} in 25 minutes. Bottom-left: heat-maps of eigenvectors. }\n\\label{fig:Soccer}\n\\end{figure}\n\n\n\\subsection{Friendship Networks}\nOur final data sets represent Facebook networks in American colleges. The work in~\\cite{traud2012Facebook} studies the structure of Facebook  networks at one hundred American colleges and universities at a single point in time (2005) and investigate the community structure at each institution, as well as the impact and correlation of various self-identified user characteristics (such as residence, class year, major, and high school) with the identified network communities. While at many institutions, the community structures are organized almost exclusively according to class year, as pointed out in \\cite{TraudSIAMreview},\nother institutions are known to be organized almost exclusively according to its undergraduate \\textit{House} system (dormitory residence), which is very well reflected in the identified communities. It is thus a natural assumption to consider the dormitory affiliation as the ground truth clustering, and aim to recover this underlying structure from the available friendship graph and any available constraints. We add constraints to the clustering problem by sampling uniformly at random nodes in the graph, and the resulting pairwise constraints are generated depending on whether the two nodes belong to the same cluster or not. In order for us to be able to compare to the computationally expensive \\textbf{CSP} method, we consider two small-sized schools,\nSimmons College ($  n=850$, $\\bar{d}=36$, $k=10$) and Haverford College ($n=1025$, $\\bar{d}=72$, $k=15$), where $\\bar{d}$ denotes the average degree in the graph and $k$ the number of clusters. For both examples, \\textbf{FAST-GE} yields more accurate results than both \\textbf{CSP} and \\textbf{COSf}, and does so at a much smaller computational cost.\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Simmons_n_850_none_0_noiseDeg_0_cons_clique_nrExp5_Errors.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Simmons_n_850_none_0_noiseDeg_0_cons_clique_nrExp5_RunningTimes.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Haverford76_n_1025_none_0_noiseDeg_0_cons_clique_nrExp5_Errors.eps}\n\\includegraphics[width=0.42\\columnwidth]{figures/FB/AVG_Haverford76_n_1025_none_0_noiseDeg_0_cons_clique_nrExp5_RunningTimes.eps}\n\\end{center}\n\\vspace{-1mm}\n\\caption{\\textit{Facebook networks}. Top: accuracy and running times for the Simmons College ($  n=850$, $\\bar{d}=36$, $k=10$). Bottom: accuracy and running times for Haverford College ($n=1025$, $\\bar{d}=72$, $k=15$). Time is in logarithmic scale. }\n\\label{fig:Facebook}\n\\end{figure}\n\n\n\n\\section{Final Remarks}\n\nWe presented a spectral method that reduces constrained\nclustering into a generalized eigenvalue problem in which both matrices\nare Laplacians. This  offers\ntwo advantages that are not simultaneously shared by\nany of the previous methods: an efficient implementation\nand an approximation guarantee for the 2-way partitioning problem\nin the form of a generalized Cheeger inequality. In practice\nthis translates to a method that is at least 10x faster than some of\nthe best existing algorithms, while producing output of superior quality.\nIts speed makes our method a good candidate for some type of\niteration, e.g. as in~\\cite{TolliverM06},\nor interactive user feedback, that would further improve its output.\n\nWe view the Cheeger inequality we\npresented in section~\\ref{sec:cheeger}\nas indicative of the rich\nmathematical properties of generalized Laplacian\neigenvalue problems. We expect that tighter\nversions\nare to be discovered, along the lines of~\\cite{KLLGT13}.\nFinding $k$-way generalizations of the Cheeger inequality,\nas in~\\cite{Lee12}, poses an interesting open problem.\n\n\n\n\n{\\small\n\\flushend\n\\bibliographystyle{nalpha}\n\n}\n\n\n\\newpage\n\\onecolumn\n\\section{Proof of the Generalized Cheeger Inequality} \\label{sec:proof}\n\nWe begin with two Lemmas.\n\n\\begin{lemma} \\label{th:splitting}\nFor all $a_i, b_i > 0$ we have\n    \n", "index": 15, "text": "$$\\frac{\\sum_i {a_i}}{\\sum_i b_i} \\geq \\min_i \\left\\{ \\frac{a_i}{b_i}\\right\\}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sum_{i}{a_{i}}}{\\sum_{i}b_{i}}\\geq\\min_{i}\\left\\{\\frac{a_{i}}{b_{i}}%&#10;\\right\\}.\" display=\"block\"><mrow><mrow><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msub><mi>a</mi><mi>i</mi></msub></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><msub><mi>b</mi><mi>i</mi></msub></mrow></mfrac><mo>\u2265</mo><mrow><munder><mi>min</mi><mi>i</mi></munder><mo>\u2061</mo><mrow><mo>{</mo><mfrac><msub><mi>a</mi><mi>i</mi></msub><msub><mi>b</mi><mi>i</mi></msub></mfrac><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nwhere $D_G$ is the demand graph for $G$.\n\\end{lemma}\n\\begin{proof}\nLet $d$ be the vector consisting of the entries\nalong the diagonal of $D$. By definition, we have\n\n", "itemtype": "equation", "pos": 45450, "prevtext": "\n\\end{lemma}\n\n\\begin{lemma} \\label{th:DvsDG}\nLet $G$ be a graph, $d$ be the vector containing the degrees of the vertices,\nand $D$ be corresponding diagonal matrix. For\nall vectors $x$ where $x^Td = 0$ we have\n\n", "index": 17, "text": "$$\n    x^T D x = x^T L_{D_G} x,\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"x^{T}Dx=x^{T}L_{D_{G}}x,\" display=\"block\"><mrow><mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>=</mo><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><msub><mi>D</mi><mi>G</mi></msub></msub><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nThe lemma follows.\n\\end{proof}\n\n\nWe prove the following theorem.\n\\begin{theorem}\n\\label{thm:generalizedcheeger}\n\nLet $G$ and $H$ be any two weighted graphs and $D$ be\nthe vector containing the degrees of the vertices in $G$.\nF any vector $x$ such that $x^Td =0$, we have\n\n", "itemtype": "equation", "pos": 45650, "prevtext": "\nwhere $D_G$ is the demand graph for $G$.\n\\end{lemma}\n\\begin{proof}\nLet $d$ be the vector consisting of the entries\nalong the diagonal of $D$. By definition, we have\n\n", "index": 19, "text": "$$\n   L_{D_G} = D - \\frac{dd^T}{vol(V)}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"L_{D_{G}}=D-\\frac{dd^{T}}{vol(V)}.\" display=\"block\"><mrow><mrow><msub><mi>L</mi><msub><mi>D</mi><mi>G</mi></msub></msub><mo>=</mo><mrow><mi>D</mi><mo>-</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msup><mi>d</mi><mi>T</mi></msup></mrow><mrow><mi>v</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nwhere $D_G$ is the demand graph of $G$.\nA cut meeting the guarantee of the inequality can\nbe obtained via a Cheeger sweep on $x$.\n\\end{theorem}\n\nLet $V^{-}$ denote the set of $u$ such that $x_u \\leq 0$ and\n$V^{+}$ denote the set such that $x_u > 0$.\nThen we can divide $E_G$ into two sets: $E_G^{same}$ consisting\nof edges with both endpoints in $V^{-}$ or $V^{+}$, and\n$E_G^{dif}$ consisting of edges with one endpoint in each.\nIn other words:\n\n", "itemtype": "equation", "pos": 45965, "prevtext": "\nThe lemma follows.\n\\end{proof}\n\n\nWe prove the following theorem.\n\\begin{theorem}\n\\label{thm:generalizedcheeger}\n\nLet $G$ and $H$ be any two weighted graphs and $D$ be\nthe vector containing the degrees of the vertices in $G$.\nF any vector $x$ such that $x^Td =0$, we have\n\n", "index": 21, "text": "\n\\[\n\\frac{x^TL_Gx}{x^TL_Hx} \\geq \\phi(G,D_G) \\cdot \\phi(G, H)/4,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\frac{x^{T}L_{G}x}{x^{T}L_{H}x}\\geq\\phi(G,D_{G})\\cdot\\phi(G,H)/4,\" display=\"block\"><mrow><mrow><mfrac><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>G</mi></msub><mo>\u2062</mo><mi>x</mi></mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>\u2265</mo><mrow><mrow><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><msub><mi>D</mi><mi>G</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>\u03d5</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mn>4</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nWe also define $E_H^{dif}$ and $E_H^{same}$ similarly.\n\nWe first show a lemma which is identical to one used in the proof\nof Cheeger's inequality~\\cite{chung1}:\n\n\\begin{lemma} \\label{lem:abssqr}\nLet $G$ and $H$ be any two weighted graphs on the same vertex set $V$\npartitioned into $V^{-}$ and $V^{+}$. For any vector $x$ we have\n", "itemtype": "equation", "pos": 46478, "prevtext": "\nwhere $D_G$ is the demand graph of $G$.\nA cut meeting the guarantee of the inequality can\nbe obtained via a Cheeger sweep on $x$.\n\\end{theorem}\n\nLet $V^{-}$ denote the set of $u$ such that $x_u \\leq 0$ and\n$V^{+}$ denote the set such that $x_u > 0$.\nThen we can divide $E_G$ into two sets: $E_G^{same}$ consisting\nof edges with both endpoints in $V^{-}$ or $V^{+}$, and\n$E_G^{dif}$ consisting of edges with one endpoint in each.\nIn other words:\n\n", "index": 23, "text": "\\begin{align*}\n& E_G^{dif} = \\delta_G\\left(V^{-}, V^{+}\\right),\\text{ and}\\\\\n& E_G^{same} = E_G \\setminus E_G^{dif}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E_{G}^{dif}=\\delta_{G}\\left(V^{-},V^{+}\\right),\\text{ and}\" display=\"inline\"><mrow><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup><mo>=</mo><mrow><mrow><msub><mi>\u03b4</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msup><mi>V</mi><mo>-</mo></msup><mo>,</mo><msup><mi>V</mi><mo>+</mo></msup><mo>)</mo></mrow></mrow><mo>,</mo><mtext>\u00a0and</mtext></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E_{G}^{same}=E_{G}\\setminus E_{G}^{dif}.\" display=\"inline\"><mrow><mrow><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>E</mi><mi>G</mi></msub><mo>\u2216</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{proof}\n\nWe begin with a few algebraic identities:\n\nNote that $2x_u^2+2x_v^2-(x_u - x_v)^2=(x_u+x_v)^2 \\geq 0$\ngives:\n", "itemtype": "equation", "pos": 46938, "prevtext": "\n\nWe also define $E_H^{dif}$ and $E_H^{same}$ similarly.\n\nWe first show a lemma which is identical to one used in the proof\nof Cheeger's inequality~\\cite{chung1}:\n\n\\begin{lemma} \\label{lem:abssqr}\nLet $G$ and $H$ be any two weighted graphs on the same vertex set $V$\npartitioned into $V^{-}$ and $V^{+}$. For any vector $x$ we have\n", "index": 25, "text": "\n\\[\n\\frac{ \\sum_{uv \\in E_G^{same}}w_G\\left(u,v\\right)\\left|x_u^2-x_v^2\\right| +\n\\sum_{uv \\in E_G^{dif}}w_G(u, v) \\left(x_u^2 + x_v^2\\right) }{x^T L_H x}\n\\geq \\frac{\\phi(G, H)}{2} .\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sum_{uv\\in E_{G}^{same}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}-x_{v}^{2}%&#10;\\right|+\\sum_{uv\\in E_{G}^{dif}}w_{G}(u,v)\\left(x_{u}^{2}+x_{v}^{2}\\right)}{x^%&#10;{T}L_{H}x}\\geq\\frac{\\phi(G,H)}{2}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>\u2265</mo><mfrac><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nAlso, suppose $uv \\in E_H^{same}$ and without loss of generality that $|x_u| \\geq |x_v|$.\nThen letting $y = |x_u| - |x_v|$, we get:\n\\begin{eqnarray*}\n|x_u^2-x_v^2| & = & \\left(\\left|x_v\\right| + y\\right) ^2 - \\left|x_v\\right|^2 \\\\\n& = & y^2 + y |x_v|\\\\\n& \\geq & y^2 = \\left(x_u - x_v\\right)^2.\n\\end{eqnarray*}\nThe last equality follows because $x_u$ and $x_v$ have the same sign.\n\n\nWe then use the above inequalities to decompose the $x^T L_H x$ term.\n\\begin{eqnarray}\n x^T L_H   & = & \\sum_{uv \\in E_H^{same}}w_H(u,v)\\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}}w_H(u, v)\\left(x_u - x_v\\right)^2 \\nonumber \\\\\n& \\leq & \\sum_{uv \\in E_H^{same}} w_H(u, v) \\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(2x_u^2 + 2x_v^2\\right) \\nonumber \\\\\n& \\leq & 2 \\left( \\sum_{uv \\in E_H^{same}}  w_H(u, v) \\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(x_u^2 + x_v^2\\right) \\right) \\nonumber \\\\\n& \\leq & 2\\left( \\sum_{uv \\in E_H^{same}}  w_H(u, v) \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(x_u^2 + x_v^2\\right) \\right).\n\\end{eqnarray}\n\nWe can now decompose the summation further into parts for\n$V^{-}$ and $V^{+}$:\n\n", "itemtype": "equation", "pos": 47259, "prevtext": "\n\\end{lemma}\n\n\\begin{proof}\n\nWe begin with a few algebraic identities:\n\nNote that $2x_u^2+2x_v^2-(x_u - x_v)^2=(x_u+x_v)^2 \\geq 0$\ngives:\n", "index": 27, "text": "\n\\[\n\\left(x_u - x_v\\right)^2 \\leq 2x_u ^2 + 2x_v^2.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\left(x_{u}-x_{v}\\right)^{2}\\leq 2x_{u}^{2}+2x_{v}^{2}.\" display=\"block\"><mrow><mrow><msup><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>u</mi></msub><mo>-</mo><msub><mi>x</mi><mi>v</mi></msub></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>\u2264</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nDoing the same for $ \\sum_{uv \\in E_H^{same}}  w_H(u, v) |x_u^2 - x_v^2|\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) (x_u^2 + x_v^2)$ we get:\n\n", "itemtype": "equation", "pos": 48484, "prevtext": "\n\nAlso, suppose $uv \\in E_H^{same}$ and without loss of generality that $|x_u| \\geq |x_v|$.\nThen letting $y = |x_u| - |x_v|$, we get:\n\\begin{eqnarray*}\n|x_u^2-x_v^2| & = & \\left(\\left|x_v\\right| + y\\right) ^2 - \\left|x_v\\right|^2 \\\\\n& = & y^2 + y |x_v|\\\\\n& \\geq & y^2 = \\left(x_u - x_v\\right)^2.\n\\end{eqnarray*}\nThe last equality follows because $x_u$ and $x_v$ have the same sign.\n\n\nWe then use the above inequalities to decompose the $x^T L_H x$ term.\n\\begin{eqnarray}\n x^T L_H   & = & \\sum_{uv \\in E_H^{same}}w_H(u,v)\\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}}w_H(u, v)\\left(x_u - x_v\\right)^2 \\nonumber \\\\\n& \\leq & \\sum_{uv \\in E_H^{same}} w_H(u, v) \\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(2x_u^2 + 2x_v^2\\right) \\nonumber \\\\\n& \\leq & 2 \\left( \\sum_{uv \\in E_H^{same}}  w_H(u, v) \\left(x_u -x_v\\right)^2\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(x_u^2 + x_v^2\\right) \\right) \\nonumber \\\\\n& \\leq & 2\\left( \\sum_{uv \\in E_H^{same}}  w_H(u, v) \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) \\left(x_u^2 + x_v^2\\right) \\right).\n\\end{eqnarray}\n\nWe can now decompose the summation further into parts for\n$V^{-}$ and $V^{+}$:\n\n", "index": 29, "text": "\\begin{align*}\n& \\sum_{uv \\in E_G^{same}}w_G\\left(u,v\\right)\\left|x_u^2-x_v^2\\right| +\n\\sum_{uv \\in E_G^{dif}}w_G\\left(u, v\\right) \\left(x_u^2 + x_v^2\\right) \\\\\n= &\\sum_{u \\in V^{-}, v \\in V^{-}}w_G\\left(u,v\\right)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G\\left(u,v\\right) x_u^2 \\\\\n& + \\sum_{u \\in V^{+}, v \\in V^{+}}w_G\\left(u,v\\right)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G\\left(u,v\\right) x_u^2.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{uv\\in E_{G}^{same}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}-x_{%&#10;v}^{2}\\right|+\\sum_{uv\\in E_{G}^{dif}}w_{G}\\left(u,v\\right)\\left(x_{u}^{2}+x_{%&#10;v}^{2}\\right)\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}-%&#10;x_{v}^{2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}\\left(u,v\\right)x_{u}^{2}\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum_{u\\in V^{+},v\\in V^{+}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}%&#10;-x_{v}^{2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}\\left(u,v\\right)x_{u}^{2}.\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nThe inequality comes from applying of Lemma \\ref{th:splitting}.\n\nBy symmetry in $V^{-}$ and $V^{+}$, it suffices to show that\n\n", "itemtype": "equation", "pos": 49072, "prevtext": "\n\nDoing the same for $ \\sum_{uv \\in E_H^{same}}  w_H(u, v) |x_u^2 - x_v^2|\n+ \\sum_{uv \\in E_H^{dif}} w_H(u, v) (x_u^2 + x_v^2)$ we get:\n\n", "index": 31, "text": "\\begin{align*}\n& \\frac{ \\sum_{uv \\in E_G^{same}}w_G(u,v)\\left|x_u^2-x_v^2\\right| +\n\\sum_{uv \\in E_G^{dif}}w_G(u, v) \\left(x_u^2 + x_v^2\\right) }{x^T L_H x} \\\\\n\\geq & \\min \\left\\{\n\\frac{\\sum_{u \\in V^{-}, v \\in V^{-}}w_G(u,v)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G(u,v) x_u^2}\n{\\sum_{u \\in V^{-}, v \\in V^{-}}w_H(u,v)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_H(u,v) x_u^2},\\right.\\\\\n&\\qquad \\left.\\frac{\\sum_{u \\in V^{+}, v \\in V^{+}}w_G(u,v)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G(u,v) x_v^2}\n{\\sum_{u \\in V^{+}, v \\in V^{+}}w_H(u,v)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_H(u,v) x_v^2}\n\\right\\}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\sum_{uv\\in E_{G}^{same}}w_{G}(u,v)\\left|x_{u}^{2}-x_{v}^{2%&#10;}\\right|+\\sum_{uv\\in E_{G}^{dif}}w_{G}(u,v)\\left(x_{u}^{2}+x_{v}^{2}\\right)}{x%&#10;^{T}L_{H}x}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><mi>x</mi></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\" display=\"inline\"><mo>\u2265</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\left\\{\\frac{\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}(u,v)\\left|x_{u%&#10;}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}(u,v)x_{u}^{2}}{\\sum_{%&#10;u\\in V^{-},v\\in V^{-}}w_{H}(u,v)\\left|x_{u}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V^%&#10;{-},v\\in V^{+}}w_{H}(u,v)x_{u}^{2}},\\right.\" display=\"inline\"><mrow><mi>min</mi><mrow><mo>{</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mfrac></mstyle><mo>,</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad\\left.\\frac{\\sum_{u\\in V^{+},v\\in V^{+}}w_{G}(u,v)\\left|x_{%&#10;u}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}(u,v)x_{v}^{2}}{\\sum_%&#10;{u\\in V^{+},v\\in V^{+}}w_{H}(u,v)\\left|x_{u}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V%&#10;^{-},v\\in V^{+}}w_{H}(u,v)x_{v}^{2}}\\right\\}.\" display=\"inline\"><mrow><mi>\u2003\u2003</mi><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mfrac></mstyle><mo>}</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nWe sort the $x_u$ in increasing order of $|x_u|$ into\nsuch that $x_{u_1} \\geq \\ldots  \\geq  x_{u_k}$, and let $S_k=\\{x_{u_1},\\ldots,x_{u_k}\\}$. We have\n\n", "itemtype": "equation", "pos": 49890, "prevtext": "\nThe inequality comes from applying of Lemma \\ref{th:splitting}.\n\nBy symmetry in $V^{-}$ and $V^{+}$, it suffices to show that\n\n", "index": 33, "text": "\\begin{equation} \\label{eq:toprove}\n \\frac{\\sum_{u\\in V^{-}, v\\in V^{-}}w_G\\left(u, v\\right)\\left|x_u^2 -x_v^2\\right| +\n \\sum_{u\\in V^{-}, v\\in V^{+} }w_G(u, v) x_u^2}{\n \\sum_{u\\in V^{-}, v\\in V^{-}}w_G\\left(u, v\\right)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G\\left(u, v\\right) x_u^2 }\n\\geq \\phi(G, H).  \\\\\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}-x_{v}^{%&#10;2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}(u,v)x_{u}^{2}}{\\sum_{u\\in V^{-},v%&#10;\\in V^{-}}w_{G}\\left(u,v\\right)\\left|x_{u}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V^{%&#10;-},v\\in V^{+}}w_{G}\\left(u,v\\right)x_{u}^{2}}\\geq\\phi(G,H).\\\\&#10;\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mfrac><mo>\u2265</mo><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nand\n\n", "itemtype": "equation", "pos": 50385, "prevtext": "\n\nWe sort the $x_u$ in increasing order of $|x_u|$ into\nsuch that $x_{u_1} \\geq \\ldots  \\geq  x_{u_k}$, and let $S_k=\\{x_{u_1},\\ldots,x_{u_k}\\}$. We have\n\n", "index": 35, "text": "\\begin{align*}\n& \\sum_{u\\in V^{-}, v\\in V^{-}}w_G(u, v)\\left|x_u^2 -x_v^2\\right| +\n \\sum_{u\\in V^{-}, v\\in V^{+} }w_G(u, v) x_u^2\n=  \\sum_{i=1 \\dots k} \\left(x_{u_i}^2 - x_{u_{i-1}}^2\\right) cap_G\\left(S_k, \\bar{S_k}\\right),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}(u,v)\\left|x_{u}^{2}-x_{v}^{2}%&#10;\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}(u,v)x_{u}^{2}=\\sum_{i=1\\dots k}\\left%&#10;(x_{u_{i}}^{2}-x_{u_{i-1}}^{2}\\right)cap_{G}\\left(S_{k},\\bar{S_{k}}\\right),\" display=\"inline\"><mrow><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>k</mi></mrow></mrow></munder></mstyle><mrow><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><msub><mi>u</mi><mi>i</mi></msub><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><msub><mi>u</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mn>2</mn></msubsup></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>p</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mi>k</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>S</mi><mi>k</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\nApplying Lemma \\ref{th:splitting} we have\n", "itemtype": "equation", "pos": 50627, "prevtext": "\nand\n\n", "index": 37, "text": "\\begin{align*}\n& \\sum_{u\\in V^{-}, v\\in V^{-}}w_H(u, v)\\left|x_u^2 -x_v^2\\right| +\n \\sum_{u\\in V^{-}, v\\in V^{+} }w_H(u, v) x_u^2\n = \\sum_{i=1 \\dots k} \\left(x_{u_i}^2 - x_{u_{i-1}}^2\\right) cap_H\\left(S_k, \\bar{S_k}\\right).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{u\\in V^{-},v\\in V^{-}}w_{H}(u,v)\\left|x_{u}^{2}-x_{v}^{2}%&#10;\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{H}(u,v)x_{u}^{2}=\\sum_{i=1\\dots k}\\left%&#10;(x_{u_{i}}^{2}-x_{u_{i-1}}^{2}\\right)cap_{H}\\left(S_{k},\\bar{S_{k}}\\right).\" display=\"inline\"><mrow><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2062</mo><mi>k</mi></mrow></mrow></munder></mstyle><mrow><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><msub><mi>u</mi><mi>i</mi></msub><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><msub><mi>u</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mn>2</mn></msubsup></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>p</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mi>k</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>S</mi><mi>k</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nwhere the second inequality is by definition of $\\phi(G,H)$. This proves equation \\ref{eq:toprove} and the Lemma follows.\n\\end{proof}\n\n\nWe now proceed with the proof of the main Theorem.\n\n\n\\begin{proof}\n\n\nWe have\n\\begin{eqnarray}\n{x^TL_Gx} & = & \\sum_{uv \\in E_G}w_G(u,v)(x_u-x_v)^2 \\nonumber\\\\\n& = & \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif}}w_G(u,v)(x_u-x_v)^2 \\nonumber \\\\\n& \\geq & \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif} }w_G(u,v) (x_u^2 + x_v^2). \\nonumber \\\\\n\\end{eqnarray}\nThe last inequality follows by $x_ux_v \\leq 0$ as $x_u \\leq 0$ for all\n$u \\in V^{-}$ and $x_v \\geq 0$ for all $v \\in V^{+}$.\n\nWe multiply both sides of the inequality by\n\n", "itemtype": "equation", "pos": 50907, "prevtext": "\n\nApplying Lemma \\ref{th:splitting} we have\n", "index": 39, "text": "\n\\[\n \\frac{\\sum_{u\\in V^{-}, v\\in V^{-}}w_G(u, v)|x_u^2 -x_v^2| +\n \\sum_{u\\in V^{-}, v\\in V^{+} }w_G(u, v) x_u^2}\n { \\sum_{u\\in V^{-}, v\\in V^{-}}w_G\\left(u, v\\right)\\left|x_u^2-x_v^2\\right|\n+ \\sum_{u \\in V^{-}, v \\in V^{+}}w_G\\left(u, v\\right) x_u^2 }\n\\geq \\min_k \\frac{cap_H\\left(S_G, \\bar{S_i}\\right)}{cap_H\\left(S_i, \\bar{S_i}\\right)}\n\\geq \\phi(G, H),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}(u,v)|x_{u}^{2}-x_{v}^{2}|+\\sum_{u\\in V%&#10;^{-},v\\in V^{+}}w_{G}(u,v)x_{u}^{2}}{\\sum_{u\\in V^{-},v\\in V^{-}}w_{G}\\left(u,%&#10;v\\right)\\left|x_{u}^{2}-x_{v}^{2}\\right|+\\sum_{u\\in V^{-},v\\in V^{+}}w_{G}%&#10;\\left(u,v\\right)x_{u}^{2}}\\geq\\min_{k}\\frac{cap_{H}\\left(S_{G},\\bar{S_{i}}%&#10;\\right)}{cap_{H}\\left(S_{i},\\bar{S_{i}}\\right)}\\geq\\phi(G,H),\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>|</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>-</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo>|</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>V</mi><mo>-</mo></msup></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2208</mo><msup><mi>V</mi><mo>+</mo></msup></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mfrac><mo>\u2265</mo><mrow><munder><mi>min</mi><mi>k</mi></munder><mo>\u2061</mo><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>p</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo>)</mo></mrow></mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>p</mi><mi>H</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">\u00af</mo></mover><mo>)</mo></mrow></mrow></mfrac></mrow><mo>\u2265</mo><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\n\nWe have\n\\begin{eqnarray*}\n&\\left( \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif} }w_G(u,v) (x_u^2 + x_v^2)  \\right)\\\\\n& \\cdot \\left( \\sum_{uv \\in E_G^{same}} w_G(u,v) (x_u + x_v)^2\n + \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right) \\\\\n\\geq & \\left(  \\sum_{uv \\in E_G^{same}} |x_u - x_v| |x_u + x_v|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right)^2\\\\\n= & \\left(  \\sum_{uv \\in E_G^{same}} |x_u^2 - x_v^2|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right)^2.\n\\end{eqnarray*}\n\nFurthermore, notice that $(x_u + x_v)^2 \\leq 2x_u ^2 + 2x_v ^2$ since\n$2x_u^2 + 2x_v^2 - (x_u + x_v)^2 = (x_u - x_v)^2 \\geq 0$.\nSo, we have\n\n", "itemtype": "equation", "pos": 51972, "prevtext": "\nwhere the second inequality is by definition of $\\phi(G,H)$. This proves equation \\ref{eq:toprove} and the Lemma follows.\n\\end{proof}\n\n\nWe now proceed with the proof of the main Theorem.\n\n\n\\begin{proof}\n\n\nWe have\n\\begin{eqnarray}\n{x^TL_Gx} & = & \\sum_{uv \\in E_G}w_G(u,v)(x_u-x_v)^2 \\nonumber\\\\\n& = & \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif}}w_G(u,v)(x_u-x_v)^2 \\nonumber \\\\\n& \\geq & \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif} }w_G(u,v) (x_u^2 + x_v^2). \\nonumber \\\\\n\\end{eqnarray}\nThe last inequality follows by $x_ux_v \\leq 0$ as $x_u \\leq 0$ for all\n$u \\in V^{-}$ and $x_v \\geq 0$ for all $v \\in V^{+}$.\n\nWe multiply both sides of the inequality by\n\n", "index": 41, "text": "$$\\sum_{uv \\in E_G^{same}} w_G(u,v) (x_u + x_v)^2\n + \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2).$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\sum_{uv\\in E_{G}^{same}}w_{G}(u,v)(x_{u}+x_{v})^{2}+\\sum_{uv\\in E_{G}^{dif}}w%&#10;_{G}(u,v)(x_{u}^{2}+x_{v}^{2}).\" display=\"block\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></munder><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>u</mi></msub><mo>+</mo><msub><mi>x</mi><mi>v</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></munder><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\nwhere $D$ is the diagonal of $L_G$ and the last inequality\ncomes from Lemma~\\ref{th:DvsDG}. Combining the last two inequalities we get:\n\\begin{eqnarray*}\n\\frac{x^TL_Gx}{x^TL_Hx} \\geq & \\frac{1}{2}\n\\cdot \\left( \\frac{\\sum_{uv \\in E_G^{same}} \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) \\left(x_u^2 + x_v^2\\right)}{x^T L_H x} \\right)\\\\\n& \\cdot \\left( \\frac{\\sum_{uv \\in E_G^{same}} \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) \\left(x_u^2 + x_v^2\\right)}{x^T L_{D_G} x} \\right).\n\\end{eqnarray*}\n\nBy Lemma \\ref{lem:abssqr}, we have that the first factor is bounded by\n$\\frac{1}{2} \\phi(G, H)$  and the second factor bounded by\n$\\frac{1}{2} \\phi(G, D_G)$. Hence we get\n\n", "itemtype": "equation", "pos": 52748, "prevtext": "\n\n\nWe have\n\\begin{eqnarray*}\n&\\left( \\sum_{uv \\in E_G^{same}}w_G(u,v)(x_u-x_v)^2\n+ \\sum_{uv \\in E_G^{dif} }w_G(u,v) (x_u^2 + x_v^2)  \\right)\\\\\n& \\cdot \\left( \\sum_{uv \\in E_G^{same}} w_G(u,v) (x_u + x_v)^2\n + \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right) \\\\\n\\geq & \\left(  \\sum_{uv \\in E_G^{same}} |x_u - x_v| |x_u + x_v|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right)^2\\\\\n= & \\left(  \\sum_{uv \\in E_G^{same}} |x_u^2 - x_v^2|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right)^2.\n\\end{eqnarray*}\n\nFurthermore, notice that $(x_u + x_v)^2 \\leq 2x_u ^2 + 2x_v ^2$ since\n$2x_u^2 + 2x_v^2 - (x_u + x_v)^2 = (x_u - x_v)^2 \\geq 0$.\nSo, we have\n\n", "index": 43, "text": "\\begin{align*}\n& \\sum_{uv \\in E_G^{same}} w_G(u,v) (x_u + x_v)^2\n + \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\\\\n\\leq & 2 \\left( \\sum_{uv \\in E_G^{same}} w_G(u,v) (x_u^2 + x_v^2)\n + \\sum_{uv \\in E_G^{dif}} w_G(u, v) (x_u^2 + x_v^2) \\right)\\\\\n & = 2 x^T Dx \\leq 4 x^T L_{D_G} x,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{uv\\in E_{G}^{same}}w_{G}(u,v)(x_{u}+x_{v})^{2}+\\sum_{uv\\in E%&#10;_{G}^{dif}}w_{G}(u,v)(x_{u}^{2}+x_{v}^{2})\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>u</mi></msub><mo>+</mo><msub><mi>x</mi><mi>v</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\" display=\"inline\"><mo>\u2264</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle 2\\left(\\sum_{uv\\in E_{G}^{same}}w_{G}(u,v)(x_{u}^{2}+x_{v}^{2})+%&#10;\\sum_{uv\\in E_{G}^{dif}}w_{G}(u,v)(x_{u}^{2}+x_{v}^{2})\\right)\" display=\"inline\"><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>e</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2208</mo><msubsup><mi>E</mi><mi>G</mi><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>f</mi></mrow></msubsup></mrow></munder></mstyle><mrow><msub><mi>w</mi><mi>G</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>x</mi><mi>u</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=2x^{T}Dx\\leq 4x^{T}L_{D_{G}}x,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2264</mo><mrow><mn>4</mn><mo>\u2062</mo><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><msub><mi>D</mi><mi>G</mi></msub></msub><mo>\u2062</mo><mi>x</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04746.tex", "nexttext": "\n\\end{proof}\n\n\\section{Additional Experiments} \\label{sec:additional}\n\n\n\n{\\bf PACM graph.} We again consider the (very) noisy ensemble \\textit{NoisyKnn}($n=436, k_g=30,l_g=15$). Figure \\ref{fig:PACM_clusterings} shows a random instance of the clustering returned by each of the methods, with 125 constraints. Figure \\ref{fig:PACM_curves} shows the accuracy and running times of all three methods on this example.\n\nAgain, our approach returns superior results when compared to \\textbf{CSP}, and it is somewhat better than \\textbf{COSf}. In this example, our running time is larger than that of both \\textbf{COSf} and \\textbf{CSP}, which is due to the small size of the problem ($n=426$). For such small problems a full eigenvalue decomposition is faster due to its better utilization of the FPU, as well as some overheads of the iterative method (e.g. preconditioning). In principle we can use the full eigenvalue decomposition to speed-up our algorithm for these smaller problems and at least match the performance of~\\textbf{CSP}. However the running times are already very small.\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_CSP_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_COSf_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\includegraphics[width=0.30\\columnwidth]{figures/Synthetic/recs/rec_FAST-GE_C125_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20}\n\\end{center}\n\\caption{Top: Segmentation for a random instance of the PACM data set with $125$ labels produced by \\textbf{CSP} (left), \\textbf{COSf} (middle) and \\textbf{FAST-GE} (right)}\n\\label{fig:PACM_clusterings}\n\\end{figure}\n\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=.48\\columnwidth]{figures/Synthetic/AVG_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20_Errors}\n\\includegraphics[width=.48\\columnwidth]{figures/Synthetic/AVG_PACM_n_426_knn_30_noiseDeg_15_cons_clique_nrExp20_RunningTimes}\n\\end{center}\n\\caption{Leftmost plots illustrate the accuracy and running times for the Four-Moons data set, where the underlying graph given by the model NoisyKnn($n=1500, k=30,l=15$), for varying  number of constraints. \nThe rightmost two plots show similar statistics for the PACM data set, with the noise model given by NoisyKnn($n=426$, $k=30, l=15$). We average all results over 20 runs.}\n\\label{fig:PACM_curves}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 53749, "prevtext": "\nwhere $D$ is the diagonal of $L_G$ and the last inequality\ncomes from Lemma~\\ref{th:DvsDG}. Combining the last two inequalities we get:\n\\begin{eqnarray*}\n\\frac{x^TL_Gx}{x^TL_Hx} \\geq & \\frac{1}{2}\n\\cdot \\left( \\frac{\\sum_{uv \\in E_G^{same}} \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) \\left(x_u^2 + x_v^2\\right)}{x^T L_H x} \\right)\\\\\n& \\cdot \\left( \\frac{\\sum_{uv \\in E_G^{same}} \\left|x_u^2 - x_v^2\\right|\n+ \\sum_{uv \\in E_G^{dif}} w_G(u, v) \\left(x_u^2 + x_v^2\\right)}{x^T L_{D_G} x} \\right).\n\\end{eqnarray*}\n\nBy Lemma \\ref{lem:abssqr}, we have that the first factor is bounded by\n$\\frac{1}{2} \\phi(G, H)$  and the second factor bounded by\n$\\frac{1}{2} \\phi(G, D_G)$. Hence we get\n\n", "index": 45, "text": "\\begin{align}\n\\frac{x^TL_Gx}{x^TL_Hx}\n& \\geq \\frac{1}{4} \\phi(G, H) \\phi(G,{D_G}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{x^{T}L_{G}x}{x^{T}L_{H}x}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>G</mi></msub><mo>\u2062</mo><mi>x</mi></mrow><mrow><msup><mi>x</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>L</mi><mi>H</mi></msub><mo>\u2062</mo><mi>x</mi></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\frac{1}{4}\\phi(G,H)\\phi(G,{D_{G}}).\" display=\"inline\"><mrow><mrow><mi/><mo>\u2265</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>4</mn></mfrac></mstyle><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo>,</mo><msub><mi>D</mi><mi>G</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]