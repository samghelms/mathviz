[{"file": "1601.06683.tex", "nexttext": "\nwhere $\\mathfrak{S}_{k}$ is the set of permutations of $[k]$. This quantity is monotonously increasing with the number of correctly classified items. In the limit $n\\to \\infty$, it vanishes for a random guess, and equals unity if the recovery is perfect. Finally, we note an important special case for which analytical results can be derived, which is the case of symmetric clusters: $\\forall a,b\\in[k]$\n\n", "itemtype": "equation", "pos": 5537, "prevtext": "\n\n\n\n\\title{Clustering from Sparse Pairwise Measurements}\n\\author{\\IEEEauthorblockN{Alaa Saade}\n\\IEEEauthorblockA{Laboratoire de Physique Statistique\\\\\n\\'Ecole Normale Sup\\'erieure, 24 Rue Lhomond\\\\\nParis 75005}\n\\\\\n\\IEEEauthorblockN{Marc Lelarge}\n\\IEEEauthorblockA{INRIA and \\'Ecole Normale Sup\\'erieure \\\\\nParis, France}\n\n\\and\n\n\\IEEEauthorblockN{Florent\nKrzakala}\n\\IEEEauthorblockA{\nSorbonne Universit\\'es, UPMC Univ. Paris 06\\\\\nLaboratoire de Physique Statistique, CNRS UMR 8550 \\\\\n\\'Ecole Normale Sup\\'erieure, 24 Rue Lhomond, Paris\\\\\n}\n\\\\\n\n\\IEEEauthorblockN{Lenka Zdeborov\\'{a}}\n\\IEEEauthorblockA{Institut de Physique Th\\'{e}orique \\\\ CEA Saclay and\nCNRS, France.}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\\begin{abstract}\n  \n  We consider the problem of grouping items into clusters based\n  on few random pairwise comparisons between the items. We introduce\n  three closely related algorithms for this task: a belief propagation\n  algorithm approximating the Bayes optimal solution, and two spectral\n  algorithms based on the non-backtracking and Bethe Hessian\n  operators. For the case of two symmetric clusters, we show that the\n  spectral approaches are asymptotically optimal in that they detect\n  the clusters as soon as it is information theoretically possible to\n  do so. As a by-product, we prove a conjecture on the detectability\n  transition of the labeled stochastic block model in a special case.\n  \\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\n\\section{Introduction}\n\n\\subsection{Problem and model}\n\\label{sec:problem_and_model}\nSimilarity-based clustering is a standard approach to label items in a dataset based on some measure of their resemblance. In general, given a dataset $\\{x_i\\}_{i\\in[n]}\\in \\mathcal{X}^n$, and a symmetric measurement function $s : \\mathcal{X}^2 \\to \\mathbb{R}$ quantifying the similarity between two items, the aim is to cluster the dataset from the knowledge of the pairwise measurements $s_{ij} := s(x_{i},x_{j})$, for $1\\leq i < j \\leq n$. This information is conveniently encoded in a similarity graph, which vertices represent items in the dataset, and the weighted edges carry the pairwise similarities. Typical choices for this similarity graph are the complete graph and the nearest neighbor graph (see e.g. \\cite{Tutorial} for a discussion in the context of spectral clustering).    \n\nHere however, we will not assume the measurement function $s$ to quantify the \\emph{similarity} between items, but more generally ask that the measurements be \\emph{typically different} depending on the cluster memberships of the items, in a way that will be made quantitative in the following. For instance, $s$ could be a distance in an Euclidean space or could take values in a set of colors (i.e. $s$ does not need to be real-valued). \nAdditionally, we will not assume knowledge of the measurements for all pairs of items in the dataset, but only for $O(n)$ of them chosen uniformly at random. \nSampling is a well-known technique to speed up computations by reducing the number of non-zero entries \\cite{achlioptas2001fast}. The main challenge is to choose the lowest possible sampling rate while still being able to detect the signal of interest. \n\nIn this paper, we compute explicitly this fundamental limit for a simple probabilistic model and present three algorithms allowing partial recovery of the signal above this limit. Below the limit, in the case of two clusters, no algorithm can give an output positively correlated with the true clusters. Our three algorithms are respectively a belief propagation algorithm and two spectral algorithms based on the non-backtracking operator and the Bethe Hessian. Although these three algorithms are intimately related, so far, a rigorous analysis is available only for the spectral properties of the non-backtracking matrix. From a practical perspective however, belief propagation and the Bethe Hessian are much simpler to implement and show even better numerical performance.\n\nTo evaluate the performance of our proposed algorithms, we construct a\nmodel with $n$ items in $k$ predefined clusters of same average size $n/k$, by assigning to each item $i \\in [n]$ a cluster label $c_{i}\\in[k]$ with uniform probability\n$1/k$.\n\nWe assume that the pairwise measurement between an item in cluster $a$ and\nanother item in cluster $b$ is a random variable with density\n$p_{a,b}$.\n\n\nWe choose the observed pairwise measurements uniformly at random, by generating an Erd\\Ho{o}s-R\\'enyi random graph $G=(V=[n],E)\\in {\\cal G}(n,\\alpha/n)$. \nThe average degree $\\alpha$ corresponds to the sampling rate: pairwise measurements are observed only on the edges of $G$, and $\\alpha$ therefore controls the difficulty of the problem. \nFrom\nthe base graph $G$, we build a measurement graph by \nweighting each edge $(ij)\\in E$ with the measurement $s_{ij}$,\ndrawn from the probability density $p_{c_{i},c_{j}}$. The aim is to\nrecover the cluster assignments $c_{i}$ for $i\\in [n]$ from the\nmeasurement graph thus constructed.\n\nWe consider the sparse regime $\\alpha = O(1)$, and the limit $n\\to \\infty$. With high probability, the graph $G$ is disconnected, so that \\emph{exact recovery} of the clusters, as considered e.g. in \\cite{jog2015information,7282873}, is impossible. In this paper, we address instead the question of how many measurements are needed to \\emph{partially recover} the cluster assignments, i.e. to infer cluster assignments $\\hat{c}_{i}$ such that the following quantity, called \\emph{overlap}, is strictly positive:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{overlap}\n\\max_{\\sigma\\in\\mathfrak{S}_k}\\frac{\\frac{1}{n}\\sum_{i} {\\mathbf{1}}(\\sigma(\\hat{c}_{i}) = c_{i}) - \\frac{1}{k} }{1 - \\frac{1}{k}}\\, ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\sigma\\in\\mathfrak{S}_{k}}\\frac{\\frac{1}{n}\\sum_{i}{\\mathbf{1}}(\\sigma(%&#10;\\hat{c}_{i})=c_{i})-\\frac{1}{k}}{1-\\frac{1}{k}}\\,,\" display=\"block\"><mrow><mrow><munder><mi>max</mi><mrow><mi>\u03c3</mi><mo>\u2208</mo><msub><mi>\ud835\udd16</mi><mi>k</mi></msub></mrow></munder><mo>\u2061</mo><mpadded width=\"+1.7pt\"><mfrac><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mi>i</mi></msub><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>c</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mfrac><mn>1</mn><mi>k</mi></mfrac></mrow><mrow><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>k</mi></mfrac></mrow></mfrac></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nFor this particular case, we conjecture that all of the three algorithms we propose achieve partial recovery of the clusters whenever $\\alpha > \\alpha_c$, where\n\n", "itemtype": "equation", "pos": 6124, "prevtext": "\nwhere $\\mathfrak{S}_{k}$ is the set of permutations of $[k]$. This quantity is monotonously increasing with the number of correctly classified items. In the limit $n\\to \\infty$, it vanishes for a random guess, and equals unity if the recovery is perfect. Finally, we note an important special case for which analytical results can be derived, which is the case of symmetric clusters: $\\forall a,b\\in[k]$\n\n", "index": 3, "text": "\\begin{align}\n\\begin{split}\n\np_{a,b}(s) &= {\\mathbf{1}}(a=b)p_{\\rm in}(s) + {\\mathbf{1}}(a\\neq b)p_{\\rm out}(s).\n\\end{split}\n\\label{symmetric_model}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{split}\\displaystyle\\par&#10; p_{a,b}(s)&amp;\\displaystyle={\\mathbf%&#10;{1}}(a=b)p_{\\rm in}(s)+{\\mathbf{1}}(a\\neq b)p_{\\rm out}(s).\\end{split}\" display=\"inline\"><mtable columnspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><msub><mi>p</mi><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>=</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mi>in</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>\u2260</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><msub><mi>p</mi><mi>out</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nwhere $\\mathcal{K}$ is the support of the function $p_{\\rm in} + (k-1)p_{\\rm out}$.\nIn the following, we prove this claim for the case of $k\\!=\\!2$ symmetric clusters, and discrete measurement distributions. \nNote that the model we introduce is a special case of the labeled stochastic block model of \\cite{HLM2012}. In particular, for the case $k\\!=\\!2$, \nit was proven in \\cite{LMX2013} that partial recovery is information theoretically impossible if $\\alpha < \\alpha_c$. In this contribution, we prove that this bound is tight, namely that partial recovery is possible whenever $\\alpha > \\alpha_c$, and that the algorithms we propose are optimal, in that they achieve this threshold. Note also that when the measurements $s_{ij}=\\pm 1$ are binary variables, the symmetric model (\\ref{symmetric_model}) reduces to the censored block model of \\cite{abbe2014decoding}. More precisely, $p_{\\rm in}$ and $p_{\\rm out}$ are (in this particular case) discrete distributions on $\\{\\pm 1\\}$ with $p_{\\rm in}(+1)=p_{\\rm out}(-1)=1-\\epsilon$ so that $\\alpha_c=(1-2\\epsilon)^{-2}$. In this case, the claimed result is known \\cite{saade2015spectral}, and to the best of our knowledge, this is the only case where our result is known.\n\n\n\\subsection{Motivation and related work}\n\n\n\n\n\n\n\n\n\nThe ability to cluster data from as few pairwise comparisons as\npossible is of broad practical interest \\cite{7282873}. First, there are situations where all the pairwise comparisons are simply not available. This is particularly the case if a comparison is the result of a human-based experiment. For instance, in crowdclustering \\cite{gomes2011crowdclustering,yi2012crowdclustering}, people are asked to compare a subset of the items in a dataset, and the aim is to cluster the whole dataset based on these comparisons. Clearly, for a large dataset of size $n$, we can't expect to have all $O(n^{2})$ measurements. Second, even if these comparisons can be automated, the typical cost of computing all pairwise measurements is $O(n^{2}d)$ where $d$ is the dimension of the data. For large datasets with $n$ in the millions or billions, or large dimensional data, like high resolution images, this cost is often prohibitive. Storing all $O(n^{2})$ measurements is also problematic. Our work supports the idea that if the measurements between different classes of items are sufficiently different, a random subsampling of $O(n)$ measurements might be enough to accurately cluster the data. \n\nThis work is inspired by recent progress in the problem of detecting\ncommunities in the sparse stochastic block model (SBM) where partial\nrecovery is possible only when the average degree $\\alpha$ is larger\nthan a threshold value, first conjectured in\n\\cite{decelle2011asymptotic}, and proved in\n\\cite{mossel2012stochastic,massoulie2013community,mossel2013proof}. A\nbelief propagation (BP) algorithm similar to the one presented here is\nintroduced in \\cite{decelle2011asymptotic}, and argued to be optimal\nin the SBM. Spectral algorithms that match the performance of BP were\nlater introduced in \\cite{krzakala2013spectral,saade2014spectral}. The\nspectral algorithms presented here are based on a generalization of\nthe operators that they introduce.\n\n\n\\subsection{Outline and main results}\n\nIn Sec. \\ref{sec:algorithms}, we describe three closely related algorithms to solve the partial recovery problem of Sec. \\ref{sec:problem_and_model}. The first one is a belief propagation (BP) algorithm approximating the Bayes optimal solution. The other two are spectral methods derived from BP. We show numerically that all three methods achieve the threshold (\\ref{transition}). Next in Sec. \\ref{sec:spectral} we prove this claim for the spectral method based on the non-backtracking operator. \n\n\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.48\\textwidth]{overlapVSalpha.pdf}\n\\end{center}\n\n\\vspace{-1.2em}\n\\caption{Performance in clustering model-generated measurement graphs in the symmetric case (\\ref{symmetric_model}). The overlap is averaged over $20$ realizations  of graphs of size $n=10^{5}$, with $k=2,3$ clusters, and Gaussian $p_{\\rm in},p_{\\rm out}$ with mean respectively $1.5$ and $0$, and unit variance. The theoretical transition $(\\ref{transition})$ is at $\\alpha_{c}\\approx 2.63$ for $k=2$, and conjectured to be at $\\alpha_{c}\\approx 5.5$ for $k=3$. All three methods achieve the theoretical transition, although the Bethe Hessian (H) and belief propagation (BP) achieve a higher overlap than the non-backtracking operator (B).}\n\\label{fig:overlapVSalpha}\n\\end{figure}\n\n\n\n\\section{Algorithms}\n\\label{sec:algorithms}\n\n\\subsection{Belief propagation}\n\nWe consider a measurement graph generated from the model of Section \\ref{sec:problem_and_model}. From Bayes' rule, we have:\n\n", "itemtype": "equation", "pos": 6446, "prevtext": "\nFor this particular case, we conjecture that all of the three algorithms we propose achieve partial recovery of the clusters whenever $\\alpha > \\alpha_c$, where\n\n", "index": 5, "text": "\\begin{equation}\n\\label{transition}\n\\frac{1}{\\alpha_c} = \\frac{1}{k}\\int_{\\mathcal{K}} {\\text {d}} s\\ \\frac{\\big(p_{\\rm in}(s) - p_{\\rm out}(s)\\big)^{2}}{p_{\\rm in}(s) + (k-1)p_{\\rm out}(s)}\\, ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{\\alpha_{c}}=\\frac{1}{k}\\int_{\\mathcal{K}}{\\text{d}}s\\ \\frac{\\big{(}p_%&#10;{\\rm in}(s)-p_{\\rm out}(s)\\big{)}^{2}}{p_{\\rm in}(s)+(k-1)p_{\\rm out}(s)}\\,,\" display=\"block\"><mrow><mrow><mfrac><mn>1</mn><msub><mi>\u03b1</mi><mi>c</mi></msub></mfrac><mo>=</mo><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi></msub><mrow><mtext>d</mtext><mo>\u2062</mo><mpadded width=\"+5pt\"><mi>s</mi></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mfrac><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mrow><msub><mi>p</mi><mi>in</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>p</mi><mi>out</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup><mrow><mrow><msub><mi>p</mi><mi>in</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>p</mi><mi>out</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mpadded></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nwhere $Z$ is a normalization. The Bayes optimal assignment, maximizing the overlap (\\ref{overlap}), is $\\hat{c}_{i} = {\\rm argmax}\\ {\\rm P}_{i}$, the mode of the marginal of node $i$. We approximate this marginal using belief propagation (BP):\n\n", "itemtype": "equation", "pos": 11425, "prevtext": "\nwhere $\\mathcal{K}$ is the support of the function $p_{\\rm in} + (k-1)p_{\\rm out}$.\nIn the following, we prove this claim for the case of $k\\!=\\!2$ symmetric clusters, and discrete measurement distributions. \nNote that the model we introduce is a special case of the labeled stochastic block model of \\cite{HLM2012}. In particular, for the case $k\\!=\\!2$, \nit was proven in \\cite{LMX2013} that partial recovery is information theoretically impossible if $\\alpha < \\alpha_c$. In this contribution, we prove that this bound is tight, namely that partial recovery is possible whenever $\\alpha > \\alpha_c$, and that the algorithms we propose are optimal, in that they achieve this threshold. Note also that when the measurements $s_{ij}=\\pm 1$ are binary variables, the symmetric model (\\ref{symmetric_model}) reduces to the censored block model of \\cite{abbe2014decoding}. More precisely, $p_{\\rm in}$ and $p_{\\rm out}$ are (in this particular case) discrete distributions on $\\{\\pm 1\\}$ with $p_{\\rm in}(+1)=p_{\\rm out}(-1)=1-\\epsilon$ so that $\\alpha_c=(1-2\\epsilon)^{-2}$. In this case, the claimed result is known \\cite{saade2015spectral}, and to the best of our knowledge, this is the only case where our result is known.\n\n\n\\subsection{Motivation and related work}\n\n\n\n\n\n\n\n\n\nThe ability to cluster data from as few pairwise comparisons as\npossible is of broad practical interest \\cite{7282873}. First, there are situations where all the pairwise comparisons are simply not available. This is particularly the case if a comparison is the result of a human-based experiment. For instance, in crowdclustering \\cite{gomes2011crowdclustering,yi2012crowdclustering}, people are asked to compare a subset of the items in a dataset, and the aim is to cluster the whole dataset based on these comparisons. Clearly, for a large dataset of size $n$, we can't expect to have all $O(n^{2})$ measurements. Second, even if these comparisons can be automated, the typical cost of computing all pairwise measurements is $O(n^{2}d)$ where $d$ is the dimension of the data. For large datasets with $n$ in the millions or billions, or large dimensional data, like high resolution images, this cost is often prohibitive. Storing all $O(n^{2})$ measurements is also problematic. Our work supports the idea that if the measurements between different classes of items are sufficiently different, a random subsampling of $O(n)$ measurements might be enough to accurately cluster the data. \n\nThis work is inspired by recent progress in the problem of detecting\ncommunities in the sparse stochastic block model (SBM) where partial\nrecovery is possible only when the average degree $\\alpha$ is larger\nthan a threshold value, first conjectured in\n\\cite{decelle2011asymptotic}, and proved in\n\\cite{mossel2012stochastic,massoulie2013community,mossel2013proof}. A\nbelief propagation (BP) algorithm similar to the one presented here is\nintroduced in \\cite{decelle2011asymptotic}, and argued to be optimal\nin the SBM. Spectral algorithms that match the performance of BP were\nlater introduced in \\cite{krzakala2013spectral,saade2014spectral}. The\nspectral algorithms presented here are based on a generalization of\nthe operators that they introduce.\n\n\n\\subsection{Outline and main results}\n\nIn Sec. \\ref{sec:algorithms}, we describe three closely related algorithms to solve the partial recovery problem of Sec. \\ref{sec:problem_and_model}. The first one is a belief propagation (BP) algorithm approximating the Bayes optimal solution. The other two are spectral methods derived from BP. We show numerically that all three methods achieve the threshold (\\ref{transition}). Next in Sec. \\ref{sec:spectral} we prove this claim for the spectral method based on the non-backtracking operator. \n\n\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.48\\textwidth]{overlapVSalpha.pdf}\n\\end{center}\n\n\\vspace{-1.2em}\n\\caption{Performance in clustering model-generated measurement graphs in the symmetric case (\\ref{symmetric_model}). The overlap is averaged over $20$ realizations  of graphs of size $n=10^{5}$, with $k=2,3$ clusters, and Gaussian $p_{\\rm in},p_{\\rm out}$ with mean respectively $1.5$ and $0$, and unit variance. The theoretical transition $(\\ref{transition})$ is at $\\alpha_{c}\\approx 2.63$ for $k=2$, and conjectured to be at $\\alpha_{c}\\approx 5.5$ for $k=3$. All three methods achieve the theoretical transition, although the Bethe Hessian (H) and belief propagation (BP) achieve a higher overlap than the non-backtracking operator (B).}\n\\label{fig:overlapVSalpha}\n\\end{figure}\n\n\n\n\\section{Algorithms}\n\\label{sec:algorithms}\n\n\\subsection{Belief propagation}\n\nWe consider a measurement graph generated from the model of Section \\ref{sec:problem_and_model}. From Bayes' rule, we have:\n\n", "index": 7, "text": "\\begin{align}\n{\\rm P}( \\{c_{i}\\} | \\{s_{ij}\\} ) &= \\frac{1}{Z} \\underset{(ij)\\in{E}}{\\prod} p_{c_{i},c_{j}}(s_{ij})\\, ,\n\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\rm P}(\\{c_{i}\\}|\\{s_{ij}\\})\" display=\"inline\"><mrow><mi mathvariant=\"normal\">P</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{Z}\\underset{(ij)\\in{E}}{\\prod}p_{c_{i},c_{j}}(s_{ij})\\,,\\par&#10;\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>Z</mi></mfrac></mstyle><mo>\u2062</mo><munder accentunder=\"true\"><mstyle displaystyle=\"true\"><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>E</mi></mrow></munder><mo>\u2062</mo><msub><mi>p</mi><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>j</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nwhere $\\partial i$ denotes the neighbors of node $i$ in the measurement graph $G$, $Z_{i}$ is a normalization, and the ${\\rm P}_{i\\to j}(c_{i})$ are the fixed point of the recursion:\n\n", "itemtype": "equation", "pos": 11802, "prevtext": "\nwhere $Z$ is a normalization. The Bayes optimal assignment, maximizing the overlap (\\ref{overlap}), is $\\hat{c}_{i} = {\\rm argmax}\\ {\\rm P}_{i}$, the mode of the marginal of node $i$. We approximate this marginal using belief propagation (BP):\n\n", "index": 9, "text": "\\begin{equation}\n\\label{BPmarg}\n{\\rm P}_{i}(c_{i}) \\approx \\frac{1}{Z_{i}}  \\prod_{l\\in \\partial i} \\sum_{c_{l}=1}^{k} p_{c_{i},c_{l}}(s_{il}) {\\rm P}_{l\\to i}(c_{l})\\, , \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\rm P}_{i}(c_{i})\\approx\\frac{1}{Z_{i}}\\prod_{l\\in\\partial i}\\sum_{c_{l}=1}^{%&#10;k}p_{c_{i},c_{l}}(s_{il}){\\rm P}_{l\\to i}(c_{l})\\,,\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">P</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2248</mo><mrow><mfrac><mn>1</mn><msub><mi>Z</mi><mi>i</mi></msub></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>l</mi><mo>\u2208</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>i</mi></mrow></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>c</mi><mi>l</mi></msub><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>p</mi><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>l</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">P</mi><mrow><mi>l</mi><mo>\u2192</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>l</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nIn practice, starting from a random initial condition, we iterate\n(\\ref{BPrec}) until convergence, and estimate the marginals from\n(\\ref{BPmarg}). On sparse tree-like random graphs generated by our\nmodel, BP is widely believed to give asymptotically accurate results,\nthough a rigorous proof is still lacking.  This algorithm is general\nand applies to any model parameters $p_{ab}$. For now on, however, we\nrestrict our theoretical discussion to the symmetric model\n(\\ref{symmetric_model}).  Eq. (\\ref{BPrec}) can be written in the\ncompact form ${\\rm P} = F({\\rm P})$, where ${\\rm P}\\in\\mathbb{R}^{2mk}$ and $m = |E|$\nis the number of edges in $G$.\n\nThe first step in understanding the behavior of BP is to note that in\nthe case of symmetric clusters (\\ref{symmetric_model}), there exists a\ntrivial fixed point of the recursion (\\ref{BPrec}), namely\n${\\rm P}_{i\\to j}(c_{i}) = 1/k$.  This fixed point is uninformative, in the\nsense that it corresponds to a random guess, with vanishing\noverlap. If this fixed point is stable, then starting from an initial\ncondition close to it will cause BP to fail to recover the\nclusters. We therefore investigate the linearization of (\\ref{BPrec})\naround this fixed point, given by the Jacobian $J_F$.\n\n\\subsection{The non-backtracking operator}\n\\label{sec:non_bactracking}\nA simple computation yields \n\n", "itemtype": "equation", "pos": 12172, "prevtext": "\nwhere $\\partial i$ denotes the neighbors of node $i$ in the measurement graph $G$, $Z_{i}$ is a normalization, and the ${\\rm P}_{i\\to j}(c_{i})$ are the fixed point of the recursion:\n\n", "index": 11, "text": "\\begin{equation}\n\\label{BPrec}\n{\\rm P}_{i\\to j}(c_{i}) = \\frac{1}{Z_{i\\to j}}  \\prod_{l\\in \\partial i\\backslash j} \\sum_{c_{l}=1}^{k} p_{c_{i},c_{l}}(s_{il}) {\\rm P}_{l\\to i}(c_{l})\\, .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\rm P}_{i\\to j}(c_{i})=\\frac{1}{Z_{i\\to j}}\\prod_{l\\in\\partial i\\backslash j}%&#10;\\sum_{c_{l}=1}^{k}p_{c_{i},c_{l}}(s_{il}){\\rm P}_{l\\to i}(c_{l})\\,.\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">P</mi><mrow><mi>i</mi><mo>\u2192</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi>Z</mi><mrow><mi>i</mi><mo>\u2192</mo><mi>j</mi></mrow></msub></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>l</mi><mo>\u2208</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>i</mi><mo>\\</mo><mi>j</mi></mrow></mrow></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>c</mi><mi>l</mi></msub><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>p</mi><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>l</mi></msub></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi mathvariant=\"normal\">P</mi><mrow><mi>l</mi><mo>\u2192</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>l</mi></msub><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nwhere $I_{k}$ is the $k\\times k$ identity matrix, $U_{k}$ is the $k\\times k$ matrix with all its entries equal to $1$, $\\otimes$ denotes the tensor product, and ${\\rm B}$ is a $2m\\times 2m$ matrix called the non-backtracking operator, acting on the directed edges of the graph $G$, with elements for $(ab)$ and $(cd)\\in E$:\n\n", "itemtype": "equation", "pos": 13713, "prevtext": "\nIn practice, starting from a random initial condition, we iterate\n(\\ref{BPrec}) until convergence, and estimate the marginals from\n(\\ref{BPmarg}). On sparse tree-like random graphs generated by our\nmodel, BP is widely believed to give asymptotically accurate results,\nthough a rigorous proof is still lacking.  This algorithm is general\nand applies to any model parameters $p_{ab}$. For now on, however, we\nrestrict our theoretical discussion to the symmetric model\n(\\ref{symmetric_model}).  Eq. (\\ref{BPrec}) can be written in the\ncompact form ${\\rm P} = F({\\rm P})$, where ${\\rm P}\\in\\mathbb{R}^{2mk}$ and $m = |E|$\nis the number of edges in $G$.\n\nThe first step in understanding the behavior of BP is to note that in\nthe case of symmetric clusters (\\ref{symmetric_model}), there exists a\ntrivial fixed point of the recursion (\\ref{BPrec}), namely\n${\\rm P}_{i\\to j}(c_{i}) = 1/k$.  This fixed point is uninformative, in the\nsense that it corresponds to a random guess, with vanishing\noverlap. If this fixed point is stable, then starting from an initial\ncondition close to it will cause BP to fail to recover the\nclusters. We therefore investigate the linearization of (\\ref{BPrec})\naround this fixed point, given by the Jacobian $J_F$.\n\n\\subsection{The non-backtracking operator}\n\\label{sec:non_bactracking}\nA simple computation yields \n\n", "index": 13, "text": "\\begin{equation}\nJ_{F} = {\\rm B}\\otimes\\left({\\rm I}_{k} - \\frac{1}{k}U_{k}\\right)\\, ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"J_{F}={\\rm B}\\otimes\\left({\\rm I}_{k}-\\frac{1}{k}U_{k}\\right)\\,,\" display=\"block\"><mrow><mrow><msub><mi>J</mi><mi>F</mi></msub><mo>=</mo><mrow><mi mathvariant=\"normal\">B</mi><mo>\u2297</mo><mrow><mo>(</mo><mrow><msub><mi mathvariant=\"normal\">I</mi><mi>k</mi></msub><mo>-</mo><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mo>\u2062</mo><msub><mi>U</mi><mi>k</mi></msub></mrow></mrow><mo rspace=\"4.2pt\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nNote however, that in order to keep consistency with the analysis of BP algorithm, we defined the non-backtracking operator in an unconventional manner. We recover the standard non-backtracking operator by taking the transpose of the current definition.\nThis matrix generalizes the non-backtracking operators of \\cite{krzakala2013spectral,saade2015spectral} to arbitrary edge weights.\nMore precisely, for the censored block model \\cite{abbe2014decoding}, we have $s=\\pm 1$ and $w(s)=(1-2\\epsilon)s$ so that ${\\rm B}$ is simply a scaled version of the matrix introduced in \\cite{saade2015spectral}. We also introduce an operator ${\\rm C}\\in\\mathbb{R}^{n\\times 2m}$ defined as \n\n", "itemtype": "equation", "pos": 14139, "prevtext": "\nwhere $I_{k}$ is the $k\\times k$ identity matrix, $U_{k}$ is the $k\\times k$ matrix with all its entries equal to $1$, $\\otimes$ denotes the tensor product, and ${\\rm B}$ is a $2m\\times 2m$ matrix called the non-backtracking operator, acting on the directed edges of the graph $G$, with elements for $(ab)$ and $(cd)\\in E$:\n\n", "index": 15, "text": "\\begin{align}\n\\begin{split}\n\\label{B}\n{\\rm B}_{(a\\to b),(c\\to d)} &= w(s_{cd}){\\mathbf{1}}(a=d){\\mathbf{1}}(b \\neq c)\\, , \\\\\n\\forall s, w(s) &:= \\frac{p_{\\rm in}(s) - p_{\\rm out}(s)}{p_{\\rm in}(s) + (k-1)p_{\\rm out}(s)}\\, .\n\\end{split}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{split}\\displaystyle{\\rm B}_{(a\\to b),(c\\to d)}&amp;%&#10;\\displaystyle=w(s_{cd}){\\mathbf{1}}(a=d){\\mathbf{1}}(b\\neq c)\\,,\\\\&#10;\\displaystyle\\forall s,w(s)&amp;\\displaystyle:=\\frac{p_{\\rm in}(s)-p_{\\rm out}(s)}%&#10;{p_{\\rm in}(s)+(k-1)p_{\\rm out}(s)}\\,.\\end{split}\" display=\"inline\"><mtable columnspacing=\"0pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi mathvariant=\"normal\">B</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>\u2192</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>\u2192</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>w</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>=</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo>\u2260</mo><mi>c</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mo>\u2200</mo><mi>s</mi></mrow><mo>,</mo><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>:=</mo><mpadded width=\"+1.7pt\"><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>p</mi><mi>in</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>p</mi><mi>out</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mrow><msub><mi>p</mi><mi>in</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>p</mi><mi>out</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mpadded></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": "\nThis operator follows from the linearization of eq. (\\ref{BPmarg}) for small ${\\rm P}_{l\\to i}$. \nBased on these operators, we propose the following spectral algorithm. First, compute the real eigenvalues of ${\\rm B}$ with modulus greater than $1$. Let $r$ be their number, and denote by $v_{1},...,v_{r}\\in\\mathbb{R}^{2m}$ the corresponding eigenvectors. If $r=0$, raise an error. Otherwise, form the matrix $Y = [v_{1} \\cdots v_{r}]\\in\\mathbb{R}^{2m\\times r}$ by stacking the eigenvectors in columns, and let $X = {\\rm C} Y\\in\\mathbb{R}^{n\\times r}$. Finally, regarding each item $i$ as a vector in $\\mathbb{R}^{r}$ specified by the $i$-th line of $X$, cluster the items, using e.g. the k-means algorithm. \n\nTheoretical guarantees for the case of $k = 2$ clusters are provided in the next section, stating that this simple algorithm succeeds in partially recovering the true clusters all the way down to the transition (\\ref{transition}). Intuitively, this algorithm can be thought of as a spectral relaxation of belief propagation. Indeed, for the particular case of $k=2$ symmetric clusters, we will show that the spectral radius of ${\\rm B}$ is larger than $1$ if and only if $\\alpha>\\alpha_{c}$. As a simple consequence, whenever $\\alpha < \\alpha_{c}$, the spectral radius of $J_{F}$ is also smaller than $1$, so that the trivial fixed point of BP is stable, and BP fails to recover the clusters. On the other hand, when $\\alpha > \\alpha_c$, a small perturbation of the trivial fixed point grows when iterating BP. Our spectral algorithm approximates the evolution of this perturbation by replacing the non-linear operator $F$ by its Jacobian $J_{F}$. In practice, as shown on figure \\ref{fig:overlapVSalpha}, the non-linearity of the function $F$ allows BP to achieve a better overlap than the spectral method based on ${\\rm B}$, but a rigorous proof that BP is asymptotically optimal is still lacking.   \n\n\\subsection{The Bethe Hessian}\n\\label{sec:bethe_hessian}\n\nThe non-backtracking operator ${\\rm B}$ of the last section is a large, non-symmetric matrix, making the implementation of the previous algorithm numerically challenging. A much smaller, closely related symmetric matrix can be defined that empirically performs as well in recovering the clusters, and in fact slightly better than ${\\rm B}$. For a real parameter $x\\geq 1$, define a matrix ${\\rm H}(x)\\in\\mathbb{R}^{n \\times n}$ with non-zero elements: \n\n", "itemtype": "equation", "pos": 15063, "prevtext": "\nNote however, that in order to keep consistency with the analysis of BP algorithm, we defined the non-backtracking operator in an unconventional manner. We recover the standard non-backtracking operator by taking the transpose of the current definition.\nThis matrix generalizes the non-backtracking operators of \\cite{krzakala2013spectral,saade2015spectral} to arbitrary edge weights.\nMore precisely, for the censored block model \\cite{abbe2014decoding}, we have $s=\\pm 1$ and $w(s)=(1-2\\epsilon)s$ so that ${\\rm B}$ is simply a scaled version of the matrix introduced in \\cite{saade2015spectral}. We also introduce an operator ${\\rm C}\\in\\mathbb{R}^{n\\times 2m}$ defined as \n\n", "index": 17, "text": "\\begin{equation}\n\\label{defC}{\\rm C}_{i,j\\to l} = w(s_{jl}){\\mathbf{1}}(i = l)\\, .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\rm C}_{i,j\\to l}=w(s_{jl}){\\mathbf{1}}(i=l)\\,.\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">C</mi><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>\u2192</mo><mi>l</mi></mrow></msub><mo>=</mo><mi>w</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>\ud835\udfcf</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>=</mo><mi>l</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06683.tex", "nexttext": " \n where $\\partial i$ denotes the set of neighbors of node $i$ in the graph $G$, and $w$ is defined in (\\ref{B}). A simple computation, analogous to \\cite{saade2015spectral}, allows to show that $(\\lambda\\geq 1,v)$ is an eigenpair of ${\\rm B}$, if and only ${\\rm H}(\\lambda)v=0$. This property justifies the following picture \\cite{saade2014spectral}. For $x$ large enough, ${\\rm H}(x)$ is positive definite and has no negative eigenvalue. As we decrease $x$, ${\\rm H}(x)$ gains a new negative eigenvalue whenever $x$ becomes smaller than an eigenvalue of ${\\rm B}$. Finally, at $x = 1$, there is a one to one correspondence between the negative eigenvalues of ${\\rm H}(x)$ and the real eigenvalues of ${\\rm B}$ that are larger than $1$. We call Bethe Hessian the matrix ${\\rm H}(1)$, and propose the following spectral algorithm, by analogy with Sec. \\ref{sec:non_bactracking}. First, compute all the negative eigenvalues of ${\\rm H}(1)$. Let $r$ be their number. If $r=0$, raise an error. Otherwise, denoting $v_{i},...,v_{r}\\in\\mathbb{R}^{n}$ the corresponding eigenvectors, form the matrix $X =[v_{1} \\cdots v_{r}]\\in\\mathbb{R}^{n\\times r}$ by stacking them in columns. Finally, regarding each item $i$ as a vector in $\\mathbb{R}^{r}$ specified by the $i$-th line of $X$, cluster the items, using e.g. the k-means algorithm. \n\n In the case of two symmetric clusters, the results of the next section imply that if $\\alpha > \\alpha_c$, denoting by $\\lambda_1 > 1$ the largest eigenvalue of ${\\rm B}$, the smallest eigenvalue of ${\\rm H}(\\lambda_1)$ is $0$, and the corresponding eigenvector allows partial recovery of the clusters. While the present algorithm replaces the matrix ${\\rm H}(\\lambda_1)$ by the matrix ${\\rm H}(1)$ and is therefore beyond the scope of this theoretical guarantee, we find empirically that the eigenvectors with negative eigenvalues of ${\\rm H}(1)$ are also positively correlated with the hidden clusters, and in fact allow better recovery (see figure \\ref{fig:overlapVSalpha}), without the need to build the non-backtracking operator ${\\rm B}$ and to compute its leading eigenvalue. \n\n This last algorithm also has an intuitive justification. \n It is well known \\cite{yedidia2001bethe} that BP tries to optimize the so-called Bethe free energy. In the same way ${\\rm B}$ can be seen as a spectral relaxation of BP, ${\\rm H}(1)$ can be seen as a spectral relaxation of the direct optimization of the Bethe free energy. In fact, it corresponds to the Hessian of the Bethe free energy around a trivial stationary point (see e.g. \\cite{saade2015matrix,saade2014spectral}).\n\n\n\n\n\n \\subsection{Numerical results}\n\n \\label{sec:numerical}\n\n Figure \\ref{fig:overlapVSalpha} shows the performance of all three algorithms on model-generated problems. We consider the symmetric problem defined by (\\ref{symmetric_model}) with $k=2,3$, fixed $p_{\\rm in}$ and $p_{\\rm out}$, chosen to be Gaussian with a strong overlap, and we vary $\\alpha$. All three algorithms achieve the theoretical threshold.    \n\n While all the algorithms presented in this work assume the knowledge of the parameters of the model, namely the functions $p_{a,b}$ for $a,b\\in[k]$, we argue that the belief propagation algorithm is robust to large imprecisions on the estimation of these parameters. To support this claim, we show on figure \\ref{fig:toy_datasets} the result of the belief propagation algorithm on standard toy datasets where the parameters were estimated on a small fraction of labeled data.  \n\n \\begin{figure}[!t]\n \\includegraphics[width = 0.48\\textwidth]{toy_datasets.jpg}\n \\caption{Clustering of toy datasets using belief propagation. Each dataset is composed of $20000$ points, $200$ of which come labeled and constitute the training set. We used the Euclidean distance as the measurement function $s$, and estimated the probability densities $p_{ab}$ on the training set using kernel density estimation (middle row). Although these estimates are very noisy and overlapping, belief propagation is able to achieve a very high accuracy using a random measurement graph $G$ of small average degree $\\alpha$ (top row). For comparison, we show in the third row the result of spectral clustering with the normalized Laplacian, using a $3$-nearest neighbors similarity graph (see e.g. \\cite{Tutorial}) built from $G$, i.e. using only the available measurements.}\n \\label{fig:toy_datasets}\n \\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Properties of the non-backtracking operator}\n\\label{sec:spectral}\n\nWe now state our results concerning the spectrum of ${{\\rm B}}$. We restrict ourselves to the case where $k=2$ and $p_{\\rm in}$ and $p_{\\rm out}$ are distributions on finite alphabet although we believe our results extend to general $k$ for the symmetric model defined by (\\ref{symmetric_model}).\n\n\n\\begin{theorem}\\label{the:main}\n  Consider an Erd\\Ho{o}s-R\\'enyi random graph on $n$ vertices with average degree $\\alpha$, with variables assigned to vertices $c_i\\in \\{1,2\\}$ uniformly at random independently from the graph and measurements $s_{ij}$ between any two neighboring vertices drawn according to the probability density: $p_{c_i, c_j}(s) = {\\mathbf{1}}(c_i=c_j)p_{\\rm in}(s) + {\\mathbf{1}}(c_i\\neq c_j)p_{\\rm out}(s)$ for two fixed (i.e. independent of $n$) discrete distributions $p_{\\rm in}\\neq p_{\\rm out}$ on $\\mathcal{S}$.\n  Let ${\\rm B}$ be the matrix defined by (\\ref{B}) and denote by $|\\lambda_1|\\geq |\\lambda_2|\\geq \\dots \\geq |\\lambda_{2m}|$ the eigenvalues of ${\\rm B}$ in order of decreasing magnitude, where $m$ is the number of edges in the graph.\n  Recall that $\\alpha_c$ is defined by (\\ref{transition}).\n\n\n\n  Then, with probability tending to $1$ as $n\\to \\infty$:\n  \\begin{itemize}\n  \\item[(i)] If $\\alpha<\\alpha_c$, then $|\\lambda_1|\\leq \\sqrt{\\frac{\\alpha}{\\alpha_c}}+o(1)$.\n  \\item[(ii)] If $\\alpha>\\alpha_c$, then $\\lambda_1=\\frac{\\alpha}{\\alpha_c}+o(1)$ and $|\\lambda_2|\\leq \\sqrt{\\frac{\\alpha}{\\alpha_c}}+o(1)$. Additionally, denoting by $v$ the eigenvector associated with $\\lambda_1$, ${\\rm C} v$ is positively correlated with the planted variables $(c_i)_{i\\in [n]}$, where ${\\rm C}$ is defined in (\\ref{defC}). \n\n\n\n\n\\end{itemize}\n\\end{theorem}\n\nNote that for the censored block model, our theorem implies Theorem 1 in \\cite{saade2015spectral}. The main idea for the proof of Theorem \\ref{the:main} is to introduce a new non-backtracking operator with spectral properties close to those of ${\\rm B}$ and then apply the techniques developed in \\cite{blm15} to it. We try to use notations consistent with \\cite{blm15}:\n\nfor an oriented edge $e=u\\to v=(u,v)$ from node $u$ to node $v$, we set $e_1=u$, $e_2=v$ and\n$e^{-1}=(v,u)$. For a matrice $M$, its transpose is denoted by\n$M^*$. We also define $\\sigma_i=2c_i-3$ for each $i\\in [n]$.\n\n\nWe start by a simple transformation: if $t$ is the vector in\n${\\mathbb{R}}^{\\vec E}$ defined by $t_e = \\sigma_{e_2}$ and $\\odot$ is the\nHadamard product, i.e. $(t\\odot x)_e = \\sigma_{e_2}x_e$, then we have\n\\begin{eqnarray}\n\\label{eq:trans}{\\rm B}^* x = \\lambda x \\Leftrightarrow {\\rm B}^X(t\\odot x)= \\lambda (t\\odot x)\\, ,\n\\end{eqnarray} with ${\\rm B}^X$ defined by\n${\\rm B}^X_{ef}= {\\rm B}_{fe}\\sigma_{f_1}\\sigma_{f_2}$. In particular, ${\\rm B}^X$\nan ${\\rm B}$ have the same spectrum and there is a trivial relation\nbetween their eigenvectors. With $X_f=\\sigma_{f_1}w(s_f)\\sigma_{f_2}$, we have:\n\\begin{eqnarray*}\n{\\rm B}^X_{ef} = X_f{\\mathbf{1}}(e_2=f_1){\\mathbf{1}}(e_1\\neq f_2)\\, .\n\\end{eqnarray*}\nMoreover, note that the random variables $(A_f=\\sigma_{f_1}\\sigma_{f_2})_{f\\in E}$ are i.i.d. with ${\\mathbf{P}}(A_f=1)=1/2$ and hence the random variables $(X_f)_{f\\in E}$ are also i.i.d. with\n\\begin{eqnarray*}\n{\\mathbf{E}} X_f = {\\mathbf{E}} X_f^2 = \\frac{1}{2}\\sum_{s}\\frac{(p_{\\rm in}(s)-p_{\\rm out}(s))^2}{p_{\\rm in}(s)+p_{\\rm out}(s)}=\\frac{1}{\\alpha_c}\\, .\n\\end{eqnarray*}\n\nWe now define another non-backtracking operator ${\\rm B}^Y$. First, letting $\\epsilon(s) = \\frac{p_{\\rm out}(s)}{p_{\\rm in }(s)+p_{\\rm out}(s)}\\in [0,1]$, we define the sequence of independent random variables $\\{\\tilde{Y}_{e}\\}_{e\\in E}$ with ${\\mathbf{P}}(\\tilde{Y}_e=+1|s_e)=1-{\\mathbf{P}}(\\tilde{Y}_e=-1|s_e)=1-\\epsilon(s_e)$, so that ${\\mathbf{E}}[\\tilde{Y}_e|s_e] = w(s_e)$. We define $Y_e=\\tilde{Y}_e\\sigma_{e_1}\\sigma_{e_2}$ and finally\n\\begin{eqnarray*}\n{\\rm B}^Y_{ef}=Y_f{\\mathbf{1}}(e_2=f_1){\\mathbf{1}}(e_1\\neq f_2)\\, , \n\\end{eqnarray*}\nso that ${\\mathbf{E}}[{\\rm B}^Y|G,\\{s_e\\}_{e\\in E}]=B^X$ and the sequence $\\{Y_e\\}_{e\\in E}$ is now a sequence of i.i.d. random signs with mean $\\frac{1}{\\alpha_c}$.\n\nIt turns out that the analysis\n of the matrix ${\\rm B}^Y$ can be done with the techniques developped in \\cite{blm15}. More precisely, we define $P$ the linear mapping on ${\\mathbb{R}}^{\\vec E}$ defined by\n$(Px)_e=Y_ex_{e^{-1}}$ (i.e. the matrix associated to $P$ is $P_{ef} =\nY_e{\\mathbf{1}}(f=e^{-1})$). Note that $P^*=P$ and since $Y_e^2=1$, $P$ is an\ninvolution so that $P$ is an orthogonal matrix. A simple computation\nshows that $(B^Y)^kP = P(B^Y)^{*k}$, hence $(B^Y)^kP$ is a symmetric matrix.\nThis symmetry corresponds to the oriented path symmetry in\n\\cite{blm15} and is crucial to our analysis.\nIf $(\\tau_{j,k}), 1\\leq j\\leq 2m$ are the eigenvalues of $(B^Y)^kP$ and $(x_{j,k})$ is an orthonormal basis of eigenvectors, we deduce that\n\\begin{eqnarray}\n\\label{sing}(B^Y)^k = \\sum_{j=1}^{2m} \\tau_{j,k}x_{j,k}P x_{j,k}\\, .\n\\end{eqnarray}\nSince $P$ is an orthogonal matrix $(Px_{j,k}), 1\\leq j\\leq 2m$ is also an orthonormal basis of ${\\mathbb{R}}^{\\vec E}$. In particular, (\\ref{sing}) gives the singular value decomposition of $(B^Y)^k$. Indeed, if $t_{j,k}=|\\tau_{j,k}|$ and $y_{j,k}={\\rm sign}(\\tau_{j,k})Px_{j,k}$, then we get\n$(B^Y)^k = \\sum_{j=1}^{2m} t_{j,k}x_{j,k}y^*_{j,k}$,\nwhich is precisely the singular value decomposition of $(B^Y)^k$. As shown in \\cite{blm15}, for large $k$, the decomposition (\\ref{sing}) carries structural information on the graph.\n\nA crucial element in the proof of \\cite{blm15} is the result of Kesten and Stigum \\cite{ks1,ks2} and we give now its extension required here which can be seen as a version of Kesten and Stigum's work in a random environment.\nWe write ${\\mathbb{N}}^*=\\{1,2,\\dots\\}$ and $U=\\cup_{n\\geq 0} ({\\mathbb{N}}^*)^n$ the set of finite sequences composed by ${\\mathbb{N}}^*$, where $({\\mathbb{N}}^*)^0$ contains the null sequence $\\emptyset$. For $u,v\\in U$, we note $|u|=n$ for the lenght of $u$ and $uv$ for the sequence obtained by the juxtaposition of $u$ and $v$. Suppose that $\\{(N_u, A_{u1},A_{u2},\\dots)\\}_{u\\in U}$ is a sequence of i.i.d. random variables with value in ${\\mathbb{N}}\\times {\\mathbb{R}}^{{\\mathbb{N}}^*}$ such that $N_u$ is a Poisson random variable with mean $\\alpha$ and the $A_{ui}$ are independent i.i.d. random signs with ${\\mathbf{P}}(A_{u1}=1)={\\mathbf{P}}(A_{u1}=-1)=\\frac{1}{2}$. We then define the following random variables: first $s_u$ such that ${\\mathbf{P}}(s_u|A_u=1)=p_{\\rm in }(s_u)$ and ${\\mathbf{P}}(s_u|A_u=-1)=p_{\\rm out}(s_u)$, then $X_{u}= A_u s_u$ and $Y_u=A_u \\tilde{Y}_u$ where ${\\mathbf{P}}(\\tilde{Y}_u=1|s_u)=1-{\\mathbf{P}}(Y_u=-1|s_u)=1-\\epsilon(s_u)$. We assume that for all $u\\in U$ and $i>N_u$, $A_{ui}=s_{ui}=X_{ui}=Y_{ui}=0$.\n\n$N_u$ will be the number of children of node $u$ and the sequence $(s_{u1},\\dots, s_{uN_u})$ the measurements on edges between $u$ and its children.\nWe set for $u=u_1u_2\\dots u_n\\in U$,\n\\begin{eqnarray*}\nP^X_{\\emptyset} =1,&& P^X_u=X_{u_1}X_{u_1 u_2}\\dots X_{u_1\\dots u_n}\\, ,\\\\\nP^Y_{\\emptyset} =1,&& P^Y_u=Y_{u_1}Y_{u_1 u_2}\\dots Y_{u_1\\dots u_n}\\, .\n\\end{eqnarray*}\nWe define (with the convention $\\frac{0}{0}=0$)\\, ,\n\\begin{eqnarray}\nM_0=1,\\quad M_t= \\sum_{|u|=t} \\frac{P^Y_u}{\\alpha^t P^X_u}\\, .\n\\end{eqnarray}\nThen conditionnaly on the variables $(s_u)_{u\\in U}$, $M_t$ is a martingale converging almost surely and in $L^2$ as soon as $\\alpha>\\alpha_c$. The fact that this martingale is bounded in $L^2$ follows from an argument given in the proof of Theorem 3 in \\cite{HLM2012}.\n\nIn order to apply the technique of \\cite{blm15}, we need to deal with the $\\ell$-th power of the non-backtracking operators. For $\\ell$ not too large, the local struture of the graph (up to depth $\\ell$) can be coupled to a Poisson Galton-Watson branching process, so that the computations done for $M_\\ell$ above provide a good approximation of the $\\ell$-th power of the non-backtracking operator and we can use the algebraic tools about perturbation of eigenvalues and eigenvectors, see the Bauer-Fike theorem in Section 4 in \\cite{blm15}.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\nWe have considered the problem of clustering a dataset from as few measurements as possible. On a reasonable model, we have made a precise prediction on the number of measurements needed to cluster better than chance, and have proved this prediction on an interesting particular case. \nWe have also introduced three efficient and optimal algorithms, based on belief propagation, that are able to cluster model-generated data starting from this transition. \nOur results suggest that clustering can be significantly sped up by using a number of measurements \\emph{linear} in the size of the dataset, instead of quadratic. \nThese algorithms, however, require an estimate of the distribution of the measurements between objects depending on their cluster membership. On toy datasets, we have demonstrated the robustness of the belief propagation algorithm to large imprecisions on these estimates, paving the way for broad applications in real world, large-scale data analysis. A natural avenue for future work is the unsupervised estimation of these distributions, through e.g. an expectation-maximization approach.    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\balance\n\n\\section*{Acknowledgment}\nThis work has been supported by the ERC under the European Union's FP7\nGrant Agreement 307087-SPARCS and by the French Agence Nationale de la\nRecherche under reference ANR-11-JS02-005-01 (GAP project).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliography{mybib}\n\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThis operator follows from the linearization of eq. (\\ref{BPmarg}) for small ${\\rm P}_{l\\to i}$. \nBased on these operators, we propose the following spectral algorithm. First, compute the real eigenvalues of ${\\rm B}$ with modulus greater than $1$. Let $r$ be their number, and denote by $v_{1},...,v_{r}\\in\\mathbb{R}^{2m}$ the corresponding eigenvectors. If $r=0$, raise an error. Otherwise, form the matrix $Y = [v_{1} \\cdots v_{r}]\\in\\mathbb{R}^{2m\\times r}$ by stacking the eigenvectors in columns, and let $X = {\\rm C} Y\\in\\mathbb{R}^{n\\times r}$. Finally, regarding each item $i$ as a vector in $\\mathbb{R}^{r}$ specified by the $i$-th line of $X$, cluster the items, using e.g. the k-means algorithm. \n\nTheoretical guarantees for the case of $k = 2$ clusters are provided in the next section, stating that this simple algorithm succeeds in partially recovering the true clusters all the way down to the transition (\\ref{transition}). Intuitively, this algorithm can be thought of as a spectral relaxation of belief propagation. Indeed, for the particular case of $k=2$ symmetric clusters, we will show that the spectral radius of ${\\rm B}$ is larger than $1$ if and only if $\\alpha>\\alpha_{c}$. As a simple consequence, whenever $\\alpha < \\alpha_{c}$, the spectral radius of $J_{F}$ is also smaller than $1$, so that the trivial fixed point of BP is stable, and BP fails to recover the clusters. On the other hand, when $\\alpha > \\alpha_c$, a small perturbation of the trivial fixed point grows when iterating BP. Our spectral algorithm approximates the evolution of this perturbation by replacing the non-linear operator $F$ by its Jacobian $J_{F}$. In practice, as shown on figure \\ref{fig:overlapVSalpha}, the non-linearity of the function $F$ allows BP to achieve a better overlap than the spectral method based on ${\\rm B}$, but a rigorous proof that BP is asymptotically optimal is still lacking.   \n\n\\subsection{The Bethe Hessian}\n\\label{sec:bethe_hessian}\n\nThe non-backtracking operator ${\\rm B}$ of the last section is a large, non-symmetric matrix, making the implementation of the previous algorithm numerically challenging. A much smaller, closely related symmetric matrix can be defined that empirically performs as well in recovering the clusters, and in fact slightly better than ${\\rm B}$. For a real parameter $x\\geq 1$, define a matrix ${\\rm H}(x)\\in\\mathbb{R}^{n \\times n}$ with non-zero elements: \n\n", "index": 19, "text": "\\begin{equation}\n\\label{Bethe_hessian}\n{\\rm H}_{ij}(x) = \n\\begin{cases}\n1 + \\sum_{l\\in\\partial i} \\frac{w(s_{il})^{2}}{x^{2}  -   w(s_{il})^{2} } \\text{ if } i=j  \\vspace{0.5em}  \\\\ \n-  \\frac{x w(s_{ij})}{x^{2}  -   w(s_{ij})^{2} } \\text{ if } (ij)\\in E \\\\\n\\end{cases} \n\\, ,\n \n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\rm H}_{ij}(x)=\\begin{cases}1+\\sum_{l\\in\\partial i}\\frac{w(s_{il})^{2}}{x^{2}%&#10;-w(s_{il})^{2}}\\text{ if }i=j\\\\&#10;-\\frac{xw(s_{ij})}{x^{2}-w(s_{ij})^{2}}\\text{ if }(ij)\\in E\\\\&#10;\\end{cases}\\,,\\par&#10;\" display=\"block\"><mrow><mrow><mrow><msub><mi mathvariant=\"normal\">H</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mn>1</mn><mo>+</mo><mrow><mstyle displaystyle=\"false\"><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>i</mi></mrow></mrow></msub></mstyle><mrow><mstyle displaystyle=\"false\"><mfrac><mrow><mi>w</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>-</mo><mrow><mi>w</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow></mrow></mrow><mo>=</mo><mi>j</mi></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"false\"><mfrac><mrow><mi>x</mi><mo>\u2062</mo><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>-</mo><mrow><mi>w</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2208</mo><mi>E</mi></mrow></mtd><mtd/></mtr></mtable></mrow></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}]