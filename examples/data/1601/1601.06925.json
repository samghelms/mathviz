[{"file": "1601.06925.tex", "nexttext": "\nIt is a global measure, that is, it is not too sensitive to strong changes in the distribution taking \nplace on a small-sized region of $\\Omega$. \nSuch is not the case with {\\it Fisher's Information Measure\\/} (FIM) $\\mathcal F$\\cite{Fisher1922,Frieden2004}, \nwhich constitutes a measure of the gradient content of the distribution $f$, thus being quite sensitive even \nto tiny localized perturbations. \nIt reads\n\n", "itemtype": "equation", "pos": 17716, "prevtext": "\n\\vspace*{0.35in}\n\n\\begin{flushleft}\n{\\Large\n\\textbf\\newline{Classification and Verification of Online Handwritten Signatures with Time Causal Information Theory Quantifiers}\n}\n\\newline\n\n\\\\\nOsvaldo A.\\ Rosso\\textsuperscript{1,*},\nRaydonal Ospina\\textsuperscript{2},\nAlejandro C.\\ Frery\\textsuperscript{3},\n\\\\\n\\bigskip\n\\bf{1} Instituto de F\\'{\\i}sica, \n          Universidade Federal de Alagoas (UFAL), \n          Av.~Lourival Melo Mota, s/n, 57072-900 Macei\\'o, AL, Brazil; and\\\\\n    Instituto Tecnol\\'ogico de Buenos Aires (ITBA),\n          Av.~Eduardo Madero 399, C1106ACD Ciudad Aut\\'onoma de Buenos Aires, Argentina; and\\\\\n          Complex Systems Group, \n          Facultad de Ingenier\\'{\\i}a y Ciencias Aplicadas,\n          Universidad de los Andes, \n          Av. Mons. \\'Alvaro del Portillo 12.455, Las Condes, Santiago, Chile\n\\\\\n\\bf{2} Centro de Ci\\^encias Exatas e da Natureza, \n          Departamento de Estat\\'istica,\n          Universidade Federal de Pernambuco (UFPE), \n          Cidade Universit\\'aria, 50740-540 Recife, PE, Brasil\n\\\\\n\\bf{3} Instituto de Computa\\c c\\~ao, \n          Universidade Federal de Alagoas (UFAL), \n          Av.~Lourival Melo Mota, s/n, 57072-900, Macei\\'o, AL, Brazil\n\\\\\n\\bigskip\n\n* oarosso@gmail.com\n\n\\end{flushleft}\n\n\\section*{Abstract}\nWe present a new approach for online handwritten signature classification and verification based on \ndescriptors stemming from Information Theory.\n\nThe proposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher Information evaluated over the Bandt and Pompe symbolization of the horizontal and vertical coordinates of signatures.\n\nThese six features are easy and fast to compute, and they are the input to an One-Class Support Vector Machine classifier.\nThe results produced surpass state-of-the-art techniques that employ higher-dimensional feature spaces which often require specialized software and hardware.\n\nWe assess the consistency of our proposal with respect to the size of the training sample, and we also use it to classify the signatures into meaningful groups.\n\n\\section*{Introduction}\n\nThe word {\\it biometrics\\/} is associated to human traits or behaviors which can be  measured \nand used for individual recognition.  \nIn fact, the biometry  recognition, as a personal authentication  signal processing, can \nbe used in applications where users need to be security identified\\cite{OrtegaGarcia2004}.  \nClearly,  these kind of systems can either verify or identify.  \n\nTwo types of biometrics  can be defined according to the personal traits considered:  \nphysical/physiological or behavioral. \nPhysical/physiological biometrics is about catering the biological traits of users, like fingerprints, \niris, face, hand, etc. \nBehavioral biometrics takes into account dynamic traits of users, such as, voice, handwritten and \nsignature expressions. \n\nOne of the main advantages of biometric systems is that users do not have \nto remember passwords or carry access keys. \nAnother important advantage lies in the difficulty to steal, imitate or generate genuine \nbiometric data, leading to enhanced security\\cite{OrtegaGarcia2004}.\n\nAs mentioned,  behavioral biometrics is based on measurements extracted from an activity \nperformed by the user, in conscious or unconscious  way, that are inherent to his/her \nown personality or learned behavior.\nIn this aspect,  behavioral biometrics has interesting pros, like user acceptance and \ncancelability, but it still lacks of some level of the uniqueness physiological biometrics has.  \n\nAmong the pure behavioral biometric traits, the handwritten \nsignature and the way we sign is the one with widest social and legal acceptance\\cite{Plamondon1989,Leclerc1994,Gupta1997,Impedovo2008,Ahmed2013}. \nIdentity verification by signature analysis requires no invasive measurements and people \nare familiar with the use of signatures in their daily life.  \nAlso, it is the modality confronted with the highest level of attacks. \n\nA signature is a handwritten depiction of someone's name or some other mark of \nidentification written on documents and devices as proof of identification. \nThe formation of signature varies from person to person, or even from the same person due \nto the psychophysical state of the signer and the conditions under which the signature \napposition process occurs.\n\nHilton\\cite{Hilton1992} studied how signatures are produced, and found that the signature has at least three attributes: \nform, movement  and variation;  being  movement the most important, because signatures \nare produced by moving a writing device. \nThe study also noted that a person's signature does evolve over time and, with the vast majority \nof users, once the signature style has been established the modifications are usually slight. \nThe movement is produced by muscles of fingers, hand, wrist, and in some writers the arm; \nthese muscles are controlled by nerve impulses.  \nWhen one person is signing these nerve impulses are controlled by the brain without any \nparticular attention to detail. \nThe signing processes  can be described then, at high level, as how the central nervous \nsystem (the brain) recovers information from long term memory in which parameters such as \nsize, shape, timing etc.  are specified.  \nAt the peripheral level, commands are generated for muscles. \nIn consequence, the signing process is believed to be a reflex action \n(ballistic action\\footnote{Ballistic movement can be defined as muscle contractions that exhibit maximum velocities and accelerations over a very short period of time. \nThey exhibit high firing rates, high force production, and very brief contraction times\\cite{ballistic}.}) \nrather than  a deliberate action. \nThen, the production of genuine signatures is associated to a ballistic handwriting, which \nis characterized by  a spurt of activity, without positional feedback, whereas  the \nproduction of forgery signature is associated to a deliberate handwriting which is \ncharacterized by a conscious attempt to produce a visual pattern with the aid of positional \nfeedback\\cite{DerGon1965,Nalwa1997}.\n\nHandwritten signature verification is a problem in which the input signature \n(a test signature) is classified as genuine or forged. \nThis process is usually performed in three main phases:\\cite{Plamondon1989,Leclerc1994,Gupta1997,Impedovo2008,Ahmed2013} \n\\begin{itemize}\n   \\item [$\\bullet$]{\\bf Data acquisition and pre-processing.}\n   Two different categories of systems can be identified, depending on whether there is electronic \n   access to the handwritten process or not. \n   {\\it a)~Online or dynamic recognition\\/}, in which the pen's instantaneous information trajectories, \n   and also information like pressure, speed  or pen-up movements can be captured.\n   {\\it b)~Offline or static recognition\\/}: those that record signatures as images \n   on paper which can be later digitized by means of a scanner, and processed.  \n   In the latter, the pre-processing phase involves filtering, noise reduction and \n   smoothing.\n   Online signature verification offers reliable identity protection, as it employs dynamic information not available on the signature image itself  but in the process of signing. As a consequence, \n   online signature verification systems usually achieve better accuracy than offline systems.\n\n   \\item [$\\bullet$]{\\bf Feature extraction.\\/}  \n   Two types of features can be used. \n   {\\it a)~Function features of the signature\\/}: time \n   functions whose values constitute the feature set. \n   {\\it b)~Parameter features\\/}: the signature is characterized as a vector of \n   elements, each one representative of the value of the feature. \n   Usually, the last one yields better performance, but it is also time-consuming.\n\n   \\item [$\\bullet$]{\\bf Classification.\\/}\n   In the verification process, the authenticity of the test signature is evaluated by matching \n   it against those stored in the knowledge base developed during the enrollment stage. \n   This process produces a single response that attests to the authenticity of the test signature. \n   When template matching techniques are considered, a questioned sample is matched against \n   templates of authentic/forgery signatures. \n   Distance-based classifiers, mostly when parameters are used as features, are usually developed \n   with statistical techniques, e.g. with Mahalanobis and Euclidean distances.\n  The performance of a signature verification system is commonly assessed in terms \n   of the percentage Equal Error Rate.\n\\end{itemize}\n\nOn the one hand, template matching attempts at finding similarities between the input signature and those in a data base.\nMost approaches use Dynamic Time Warping to perform this match\\cite{Impedovo2008,Ahmed2013}.\nOn the other hand, distance-based classifiers rely on the use of features derived from the signatures.\n\nTwo opposite mechanisms describing the signing process can be found in the literature.\nThe nonlinear character and chaotic behavior of several physiological complex processes are well \nestablished\\cite{Goldberger1990,West2013}. \nIn particular, Longstaff and Heath\\cite{Longstaff1999}\nfound evidence of chaotic behavior on the underlying dynamics\nof time series related to velocity profiles of handwritten texts. \nTaking into account the inherent behavioral nature of the online signing process, the input \ninformation could be associated to deterministic (nonlinear low dimensional chaotic) signals, and \nthe handwritten signature variations as a consequence of chaos (sensibility to initial conditions). \nIn opposition, most of the research in the field of  signal verification considers  the input \ninformation as well described by a random  process\\cite{Plamondon1989,Leclerc1994,Gupta1997,Impedovo2008,Ahmed2013}. \nThen, the dynamic input information acquired through a time sampling procedure must be \nconsequently considered as discrete time random sequence. \n\nIn any case, the signature analysis taken as a time-based sequence characterization process is \nstrongly related to the way in which a reference model is established. \nFrom the stochastic point of view, Hidden Markov Models are among the most commonly used in the \nliterature, and the ones with the best performance in signature \nverification\\cite{Plamondon1989,Leclerc1994,Gupta1997,Impedovo2008,Ahmed2013}.\n\nOur proposal relies on the use of time causal quantifiers based on Information Theory for the \ncharacterization  of online handwritten signatures: \nnormalized permutation Shannon entropy, permutation statistical complexity and permutation Fisher \ninformation measure. \nThese quantifiers have proved to be useful in the identification of chaotic and stochastic \ndynamics throughout the associated time series\\cite{Rosso2007,Rosso2015}. \nTheir evaluation is simple and fast, making them apt for the signature verification problem. \nWe apply our proposal to the well know MCYT online signature data base\\cite{MCYT2003}.\n\nNext section describes the database used in this study, followed by a section where we detail \nthe quantifiers employed and by their application to the data.\nIn addition to the usual data flow, we present an exploratory data analysis (EDA) of the features that enhances their appropriateness for this problem.\nThe expressiveness and usefulness of these descriptors for the problem of online signature \nclassification and verification follows in the sequence: we experiment their application to the test-bed.\n\n\\section*{Handwritten signatures database}\n\\label{Sec:HandwrittenData}\n\nThe present study is carried out on the freely available and widely used handwritten signatures \ndatabase MCYT-100 subset of 100 persons\\cite{MCYT2003}.\nThe acquisition of each on-line signature is accomplished dynamically using a graphics tablet. \nThe signatures  are acquired on a\nWACOM$^\\copyright$ graphic tablet, model~INTUOS~A6~USB.\n\nThe tablet resolution is  \\SIunits{2540}~{lines\\per in} (\\SIunits{100}~{lines\\per\\milli\\meter}), \nand the precision is \\SIunits{$\\pm$0.25}~{\\milli\\meter}. \nThe maximum detection height is \\SIunits{10}~{\\milli\\meter} \n(so also pen-up movements are considered), and the capture area is \n\\SIunits{127}~{\\milli\\meter} (width) $\\times$ \\SIunits{97}~{\\milli\\meter} (height).\nThis tablet provides the following discrete-time sequences:\n{\\it a)\\/} position $x_t$ in the $x$-axis,\n{\\it b)\\/} position $y_t$ in the $y$-axis, and \n{\\it c)\\/} \nalso the time series corresponding to the pressure $p_t$ applied by the pen, as well as the\nazimuth  $\\gamma_t$ and altitude $\\varphi_t$ angles of the pen with respect to the tablet, not used in\nthe present work.\nThe sampling frequency is set to \\SIunits{100}~{\\hertz}.\nTaking into account the Nyquist sampling criterion and the fact that the maximum frequencies of the\nrelated  biomechanical sequences are always  under  \\SIunits{20-30}~{\\hertz}\\cite{Baron1989}, \nthis sampling frequency leads to a precise discrete-time signature representation.\n\nThe signature corpus comprises genuine and shape-based highly skilled forgeries with natural \ndynamics\\cite{MCYT2003,Salicetti2009}.\nIn order to obtain the forgeries, each contributor is requested to imitate other signers by writing\nnaturally.\nFor this task, they were given the printed signature to imitate and were asked not only to imitate the\nshape but also to generate the imitation without artifacts such as breaks or slow-downs (see \n\\cite{MCYT2003,Salicetti2009} for more details of the acquisition procedure).\nEach signer contributes with $25$ genuine signatures in five groups of five signatures each, \nand is forged $25$ times by five different imitators.\nFigure~\\ref{fig:MCYT-firmas} presents examples for six different subjects,\nbeing the first two columns genuine and the third column forgery signatures.\n\n\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width = \\linewidth]{3firmas-22-H1A-new}\n\\includegraphics[width = \\linewidth]{3firmas-39-H1B-new}\n\\includegraphics[width = \\linewidth]{3firmas-60-H2A-new}\n\\includegraphics[width = \\linewidth]{3firmas-6-H2B-new}\n\\includegraphics[width = \\linewidth]{3firmas-98-H3A-new}\n\\includegraphics[width = \\linewidth]{3firmas-46-H3B-new}\n\\caption{Six different subjects signatures from the MCYT database.\nTwo genuine signatures (left, blue) and a skilled forgery (right, red). \nThe two first signatures were classified as H1A and H1B,\nthe following two to types H2A and H2B, and the last two to types H3A and H3B; cf.\\ Sec.\\ Signatures classification.}\n\\label{fig:MCYT-firmas}\n\\end{figure}\n\nSince signers are concentrated in a different writing task between genuine signature sets, the \nvariability between client signatures from different acquisition sets is expected to be  higher than \nthe variability of signatures within the same set.\nThe total number of contributors in the MCYT is $330$, and the total number of signatures present in\nthe signature database is $16,500$, half of them genuine signatures and the rest \nforgeries\\cite{MCYT2003,Salicetti2009}.\n\nAs previously mentioned, we used a subset of the database, denominated MCYT-100, \nwhich includes $100$ subjects and for each one, $25$ genuine  and $25$ \nskilled  \n\nforged signatures, and only the \ncorresponding time series corresponding to the $x$- and $y$-coordinates of each signature will be analyzed.\nIn particular, one must note that the time series' lengths are quite variable.\nIn order to facilitate our Information Theory analysis, we pre-processed each time series as follows:\n{\\it a)\\/}~the coordinates were re-scaled into the unit square $[0,1]\\times[0,1]$; \n{\\it b)\\/}~taken as base these scaled values, the original total number of data for \neach time series is expanded to $M = 2000$ points using a cubic Hermite polynomial.\nIn this way, for each subject $k$ ($k = 1, \\dots, 100$) and associated signatures $j$ ($j=1, \\dots, 25$) \nwe will analyze two time series, denoted by\n$ {\\mathbf X}^{(k;\\alpha)}_j = \\{ 0 \\leq {\\tilde{x}}^{(k;\\alpha)}_{j;i} \\leq 1,~i = 1, \\ldots, M \\}$  and \n$ {\\mathbf Y}^{(k;\\alpha)}_j = \\{ 0 \\leq {\\tilde{y}}^{(k;\\alpha)}_{j;i} \\leq 1,~i = 1, \\ldots, M \\}$,\nin which the supra-index $\\alpha = G,~F$ denotes genuine and forgery signature, and \n${\\tilde{x}}$ and ${\\tilde{y}}$  are the interpolated values, respectively.\n\n\n\\section*{Information Theory quantifiers}\n\\label{Sec:Quantifiers}\n\nThe basic elements for the study of a system dynamics, either natural or man-made, are sequences of measurements or observations whose evolution can be tracked through time.\nThen, given\nan observable of such system, a natural question that arises is: how much information is this \nobservable encoding about the dynamics of the underlying system?\nThe information contents of a system are typically evaluated via a probability distribution function (PDF) $P$ \nobtained from such observable. \nWe can define Information Theory quantifiers as measures able to characterize relevant properties of the \nPDF\nassociated with these time series, and in this way we should judiciously \nextract information on the dynamical system under study. \n\n\\subsection*{Shannon entropy, Fisher Information Measure, and Statistical Complexity}\n\\label{Sec:HFC}\n\nEntropy is a basic quantity with multiple field-specific interpretations; for instance, it has been associated \nwith disorder, state-space volume, and lack of information\\cite{Brissaud2005}.\nWhen dealing with information content, the Shannon entropy is often considered the foundational and most \nnatural one\\cite{Shannon1948,Shannon1949}. \n\nGiven a continuous probability distribution function (PDF) $f(x)$ with $x \\in \\Omega \\subset {\\mathbb R}$ and \n$\\int_{\\Omega} f(x)~dx = 1$, its associated {\\it Shannon Entropy\\/} $S$  \\cite{Shannon1948,Shannon1949} is\n\n", "index": 1, "text": "\\begin{equation}\n\\label{shannon}\n{\\mathrm S}[f] = -\\int_{\\Omega} f(x) \\ln f(x)  dx .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{S}}[f]=-\\int_{\\Omega}f(x)\\ln f(x)dx.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>f</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi mathvariant=\"normal\">\u03a9</mi></msub><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>x</mi></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\n\nThe Fisher Information Measure can be variously interpreted as a measure of the ability to estimate a parameter,\nas the amount of information that can be extracted from a set of measurements, and also as a measure of the state\nof disorder of a system or phenomenon\\cite{Frieden2004}, its most important property being the so-called\nCramer-Rao bound.\nIt is important to remark  that the gradient operator significantly influences the contribution of minute local\n$f$-variations to the Fisher information value, accordingly, this quantifier is called ``local\"\\cite{Frieden2004}. \nNote that the Shannon entropy decreases with the distribution skewness, while the Fisher information increases.\n\nLocal sensitivity is useful in scenarios whose description necessitates an appeal to a notion of ``order''.\n\nIn the previous definition of FIM (Eq.~(\\ref{fisher})) the division by $f(x)$ is not convenient  if \n$f(x) \\rightarrow 0$ at certain points of the support $\\Omega$. \nWe avoid this if we work with real probability amplitudes, by means of the alternative expression that employs \n$\\psi(x)$\\cite{Fisher1922,Frieden2004}.\nThis form requires no divisions, and shows that $\\mathcal F$ simply measures the gradient content in \n$\\psi(x)$.\n\nLet now $P=\\{p_i; i=1,\\ldots, N\\}$  be a  discrete probability distribution, with $N$ the number of possible \nstates of the system under study.\nThe Shannon's logarithmic information measure reads\n\n", "itemtype": "equation", "pos": 18229, "prevtext": "\nIt is a global measure, that is, it is not too sensitive to strong changes in the distribution taking \nplace on a small-sized region of $\\Omega$. \nSuch is not the case with {\\it Fisher's Information Measure\\/} (FIM) $\\mathcal F$\\cite{Fisher1922,Frieden2004}, \nwhich constitutes a measure of the gradient content of the distribution $f$, thus being quite sensitive even \nto tiny localized perturbations. \nIt reads\n\n", "index": 3, "text": "\\begin{equation}\n\\label{fisher}\n{\\mathcal F}[f] = \\int_{\\Omega} { \\frac{1}{f(x)} } \\left[ { \\frac{df(x)}{dx} }\\right]^2 dx\n    = 4 \\int_{\\Omega} \\left[ { \\frac{d \\psi(x)} {dx} }\\right]^2 ,\\quad\n    \\text{where } \\psi(x) = \\sqrt{f(x)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{F}}[f]=\\int_{\\Omega}{\\frac{1}{f(x)}}\\left[{\\frac{df(x)}{dx}}\\right]^%&#10;{2}dx=4\\int_{\\Omega}\\left[{\\frac{d\\psi(x)}{dx}}\\right]^{2},\\quad\\text{where }%&#10;\\psi(x)=\\sqrt{f(x)}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>f</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi mathvariant=\"normal\">\u03a9</mi></msub><mrow><mfrac><mn>1</mn><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo>[</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>]</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>x</mi></mrow></mrow></mrow><mo>=</mo><mrow><mn>4</mn><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mi mathvariant=\"normal\">\u03a9</mi></msub><msup><mrow><mo>[</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>]</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mtext>where\u00a0</mtext><mo>\u2062</mo><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nThis can be regarded to as a measure of the uncertainty associated (information) to the physical process described by $P$.\nFor instance, if ${\\mathrm S}[P] = {\\mathrm S}_{\\min} = 0$, we are in position to predict with complete certainty \nwhich of the possible outcomes $i$, whose probabilities are given by $p_i$, will actually take place. \nOur knowledge of the underlying process described by the probability distribution is maximal in this instance. \nIn contrast, our knowledge is minimal for a uniform distribution $P_e = \\{ p_i = 1/N, \\forall i=1, \\ldots , N \\}$\nsince every outcome exhibits the same probability of occurrence, and the uncertainty is maximal, i.e.,\n ${\\mathrm S}[P_e] = {\\mathrm S}_{\\max} = \\ln N$.\nIn the discrete case,  we define a ``normalized\" Shannon entropy, $0 \\leq {\\mathcal H} \\leq 1$, as\n\n", "itemtype": "equation", "pos": 19908, "prevtext": "\n\nThe Fisher Information Measure can be variously interpreted as a measure of the ability to estimate a parameter,\nas the amount of information that can be extracted from a set of measurements, and also as a measure of the state\nof disorder of a system or phenomenon\\cite{Frieden2004}, its most important property being the so-called\nCramer-Rao bound.\nIt is important to remark  that the gradient operator significantly influences the contribution of minute local\n$f$-variations to the Fisher information value, accordingly, this quantifier is called ``local\"\\cite{Frieden2004}. \nNote that the Shannon entropy decreases with the distribution skewness, while the Fisher information increases.\n\nLocal sensitivity is useful in scenarios whose description necessitates an appeal to a notion of ``order''.\n\nIn the previous definition of FIM (Eq.~(\\ref{fisher})) the division by $f(x)$ is not convenient  if \n$f(x) \\rightarrow 0$ at certain points of the support $\\Omega$. \nWe avoid this if we work with real probability amplitudes, by means of the alternative expression that employs \n$\\psi(x)$\\cite{Fisher1922,Frieden2004}.\nThis form requires no divisions, and shows that $\\mathcal F$ simply measures the gradient content in \n$\\psi(x)$.\n\nLet now $P=\\{p_i; i=1,\\ldots, N\\}$  be a  discrete probability distribution, with $N$ the number of possible \nstates of the system under study.\nThe Shannon's logarithmic information measure reads\n\n", "index": 5, "text": "\\begin{equation}\n\\label{shannon-disc}\n{\\mathrm S}[P] = -\\sum_{i=1}^{N} p_i \\ln p_i.  \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathrm{S}}[P]=-\\sum_{i=1}^{N}p_{i}\\ln p_{i}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\n\nThe concomitant problem of loss of information due to the discretization has been thoroughly studied (see, for\ninstance, \\cite{Zografos1986,Pardo1994} and references therein) and, in particular, it entails the\nloss of Fisher's shift-invariance, which is of no importance for our present purposes.\nFor the FIM we take the expression in terms of  real probability amplitudes as starting point, then a discrete \nnormalized FIM,  $0 \\leq {\\mathcal F} \\leq 1$, convenient for our present purposes, is given by\n\n", "itemtype": "equation", "pos": 20828, "prevtext": "\nThis can be regarded to as a measure of the uncertainty associated (information) to the physical process described by $P$.\nFor instance, if ${\\mathrm S}[P] = {\\mathrm S}_{\\min} = 0$, we are in position to predict with complete certainty \nwhich of the possible outcomes $i$, whose probabilities are given by $p_i$, will actually take place. \nOur knowledge of the underlying process described by the probability distribution is maximal in this instance. \nIn contrast, our knowledge is minimal for a uniform distribution $P_e = \\{ p_i = 1/N, \\forall i=1, \\ldots , N \\}$\nsince every outcome exhibits the same probability of occurrence, and the uncertainty is maximal, i.e.,\n ${\\mathrm S}[P_e] = {\\mathrm S}_{\\max} = \\ln N$.\nIn the discrete case,  we define a ``normalized\" Shannon entropy, $0 \\leq {\\mathcal H} \\leq 1$, as\n\n", "index": 7, "text": "\\begin{equation}\n\\label{shannon-disc-normalizada}\n{\\mathcal H} [P] = {\\mathrm S}[P]  / {\\mathrm S}_{\\max} \\ .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{H}}[P]={\\mathrm{S}}[P]/{\\mathrm{S}}_{\\max}\\ .\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>/</mo><mpadded width=\"+5pt\"><msub><mi mathvariant=\"normal\">S</mi><mi>max</mi></msub></mpadded></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nIt has been extensively discussed that this discretization is the best behaved in a  discrete environment\\cite{Dehesa2009}. \nHere the normalization constant $F_0$ reads\n\n", "itemtype": "equation", "pos": 21459, "prevtext": "\n\nThe concomitant problem of loss of information due to the discretization has been thoroughly studied (see, for\ninstance, \\cite{Zografos1986,Pardo1994} and references therein) and, in particular, it entails the\nloss of Fisher's shift-invariance, which is of no importance for our present purposes.\nFor the FIM we take the expression in terms of  real probability amplitudes as starting point, then a discrete \nnormalized FIM,  $0 \\leq {\\mathcal F} \\leq 1$, convenient for our present purposes, is given by\n\n", "index": 9, "text": "\\begin{equation}\n\\label{Fisher-disc}\n{\\mathcal F}[P]=F_0\\sum_{i=1}^{N-1} \\big[\\sqrt{p_{i+1}} - \\sqrt{p_{i}}\\big]^2 .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{F}}[P]=F_{0}\\sum_{i=1}^{N-1}\\big{[}\\sqrt{p_{i+1}}-\\sqrt{p_{i}}\\big{]%&#10;}^{2}.\" display=\"block\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi><mo>-</mo><mn>1</mn></mrow></munderover><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><msqrt><msub><mi>p</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msqrt><mo>-</mo><msqrt><msub><mi>p</mi><mi>i</mi></msub></msqrt></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\n\nThe perfect crystal and the isolated ideal gas are two typical examples of systems with minimum and \nmaximum entropy, respectively. \nHowever, they are also examples of simple models and therefore of systems with zero complexity, as the\nstructure of the perfect crystal is completely described by minimal information (i.e., distances and \nsymmetries that define the elementary cell) and the probability distribution for the accessible states \nis centered around a prevailing state of perfect symmetry. \nOn the other hand, all the accessible states of the ideal gas occur with the same probability and can be\ndescribed by a ``simple\" uniform distribution. \n\nAccording to L\\'opez-Ruiz {\\it et al.}\\cite{LMC1995}, and using an oxymoron, an object, a procedure, \nor system is said to be complex when it does not exhibit patterns regarded as simple. \nIt follows that a suitable complexity measure should vanish both for completely ordered and for completely \nrandom systems and cannot only rely on the concept of information (which is maximal and minimal for the \nabove mentioned systems).\nA suitable measure of complexity can be defined as the product of a measure of information and a measure of\ndisequilibrium, i.e. some kind of distance from the equiprobable distribution of the accessible states of \na system. \nIn this respect, Rosso and coworkers\\cite{Lamberti2004} introduced an effective {\\it Statistical Complexity \nMeasure\\/} (SCM) ${\\mathcal C}$, that is able to detect essential details of the dynamical processes \nunderlying the dataset.\n\nBased on the seminal notion advanced by L\\'opez-Ruiz {\\it et al.}\\cite{LMC1995}, this statistical complexity \nmeasure\\cite{Lamberti2004} is defined through the product\n\n", "itemtype": "equation", "pos": 21760, "prevtext": "\nIt has been extensively discussed that this discretization is the best behaved in a  discrete environment\\cite{Dehesa2009}. \nHere the normalization constant $F_0$ reads\n\n", "index": 11, "text": "\\begin{equation}\n\\label{F0}\nF_0=\\left\\{\n       \\begin{array}{rl}\n                   1,       & \\text{if } p_{i^*} = 1 \\text{ for }\n                            i^* = 1 \\text{ or } i^* = N \\text{ and } p_{i}  = 0, \\forall  i \\neq i^*, \\\\\n                    1/2,     & \\text{otherwise.}\n       \\end{array}\n\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"F_{0}=\\left\\{\\begin{array}[]{rl}1,&amp;\\text{if }p_{i^{*}}=1\\text{ for }i^{*}=1%&#10;\\text{ or }i^{*}=N\\text{ and }p_{i}=0,\\forall i\\neq i^{*},\\\\&#10;1/2,&amp;\\text{otherwise.}\\end{array}\\right.\" display=\"block\"><mrow><msub><mi>F</mi><mn>0</mn></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>p</mi><msup><mi>i</mi><mo>*</mo></msup></msub></mrow><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><msup><mi>i</mi><mo>*</mo></msup></mrow><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mtext>\u00a0or\u00a0</mtext><mo>\u2062</mo><msup><mi>i</mi><mo>*</mo></msup></mrow><mo>=</mo><mrow><mi>N</mi><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msub><mi>p</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo>\u2260</mo><msup><mi>i</mi><mo>*</mo></msup></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise.</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nof the normalized Shannon entropy ${\\mathcal H}$, see Eq.~\\eqref{shannon-disc-normalizada}, and the disequilibrium \n${\\mathcal Q}_{J}$ defined in terms of the Jensen-Shannon divergence ${\\mathcal J}[ P, P_e]$.\nThat is,\n\n", "itemtype": "equation", "pos": 23802, "prevtext": "\n\nThe perfect crystal and the isolated ideal gas are two typical examples of systems with minimum and \nmaximum entropy, respectively. \nHowever, they are also examples of simple models and therefore of systems with zero complexity, as the\nstructure of the perfect crystal is completely described by minimal information (i.e., distances and \nsymmetries that define the elementary cell) and the probability distribution for the accessible states \nis centered around a prevailing state of perfect symmetry. \nOn the other hand, all the accessible states of the ideal gas occur with the same probability and can be\ndescribed by a ``simple\" uniform distribution. \n\nAccording to L\\'opez-Ruiz {\\it et al.}\\cite{LMC1995}, and using an oxymoron, an object, a procedure, \nor system is said to be complex when it does not exhibit patterns regarded as simple. \nIt follows that a suitable complexity measure should vanish both for completely ordered and for completely \nrandom systems and cannot only rely on the concept of information (which is maximal and minimal for the \nabove mentioned systems).\nA suitable measure of complexity can be defined as the product of a measure of information and a measure of\ndisequilibrium, i.e. some kind of distance from the equiprobable distribution of the accessible states of \na system. \nIn this respect, Rosso and coworkers\\cite{Lamberti2004} introduced an effective {\\it Statistical Complexity \nMeasure\\/} (SCM) ${\\mathcal C}$, that is able to detect essential details of the dynamical processes \nunderlying the dataset.\n\nBased on the seminal notion advanced by L\\'opez-Ruiz {\\it et al.}\\cite{LMC1995}, this statistical complexity \nmeasure\\cite{Lamberti2004} is defined through the product\n\n", "index": 13, "text": "\\begin{equation}\n{\\mathcal C}[P] = {\\mathcal Q}_{J}[P,P_e] \\cdot {\\mathcal H}[P]\n\\label{complexity}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{C}}[P]={\\mathcal{Q}}_{J}[P,P_{e}]\\cdot{\\mathcal{H}}[P]\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcac</mi><mi>J</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo>,</mo><msub><mi>P</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u22c5</mo><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nthe above-mentioned Jensen-Shannon divergence and $Q_0$, a normalization constant \nsuch that $0 \\leq {\\mathcal Q}_{J} \\leq 1$:\n\n", "itemtype": "equation", "pos": 24136, "prevtext": "\nof the normalized Shannon entropy ${\\mathcal H}$, see Eq.~\\eqref{shannon-disc-normalizada}, and the disequilibrium \n${\\mathcal Q}_{J}$ defined in terms of the Jensen-Shannon divergence ${\\mathcal J}[ P, P_e]$.\nThat is,\n\n", "index": 15, "text": "\\begin{equation}\n\\label{disequilibrium}\n{\\mathcal Q}_{J} [ P, P_e] = Q_{0} {\\mathcal J}[ P, P_e] = \nQ_{0} \\{ {\\mathrm S}[(P + P_e)/2 ] - {\\mathrm S}[ P ]/2 - {\\mathrm S}[P_e]/2\\},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{Q}}_{J}[P,P_{e}]=Q_{0}{\\mathcal{J}}[P,P_{e}]=Q_{0}\\{{\\mathrm{S}}[(P+%&#10;P_{e})/2]-{\\mathrm{S}}[P]/2-{\\mathrm{S}}[P_{e}]/2\\},\" display=\"block\"><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcac</mi><mi>J</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo>,</mo><msub><mi>P</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>Q</mi><mn>0</mn></msub><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo>,</mo><msub><mi>P</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>Q</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>P</mi><mo>+</mo><msub><mi>P</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mn>2</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>P</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>/</mo><mn>2</mn></mrow><mo>-</mo><mrow><mrow><mi mathvariant=\"normal\">S</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>P</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><mo>/</mo><mn>2</mn></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nare equal to the inverse of the maximum possible value of ${\\mathcal J} [P,P_e]$.\nThis value is obtained when one of the components of $P$, say $p_m$, is equal to one and the remaining $p_j$ \nare zero.\n\nThe Jensen-Shannon divergence, which quantifies the difference between probability distributions, \nis especially useful to compare the symbolic composition between different sequences\\cite{Grosse2002}.\nNote that the above introduced \nSCM\ndepends on two different probability distributions: one associated with the system under analysis, $P$, and the other the uniform distribution, $P_e$.\nFurthermore, it was shown that for a given value of ${\\mathcal H}$, the range of possible ${\\mathcal C}$ \nvalues varies between a minimum ${\\mathcal C}_{min}$ and a maximum ${\\mathcal C}_{max}$, \nrestricting the possible values of the \nSCM\\cite{Martin2006}.\n\nThus, it is clear that important additional information related to the correlational structure between the \ncomponents of the physical system is provided by evaluating the statistical complexity measure. \nIn this way, the information plane ${\\mathcal H} \\times {\\mathcal C}$ constitute a nice tool to visualizate \nand characterize different dynamical systems.\n\nIf our system lies in a very ordered state, which occurs when almost all the $p_{i}$--values are zeros \nexcept for a particular state $k \\neq i$ with $p_{k} \\cong 1$, both\nthe normalized Shannon entropy and statistical complexity are close to zero (${\\mathcal H} \\approx 0$ and \n${\\mathcal C} \\approx 0$), and the normalized Fisher's information measure is close to  one (${\\mathcal F} \\approx 1$).\nOn the other hand, when the system under study is represented by  a very disordered state, that is when all the \n$p_{i}$--values oscillate around the same value, we have ${\\mathcal H} \\approx 1$ while \n${\\mathcal C} \\approx 0$ and ${\\mathcal F} \\approx 0$.\nOne can state that the general FIM--behavior of the present discrete version \n(Eq.~(\\ref{Fisher-disc})),  \nis opposite to that of the Shannon entropy, except for periodic motions.\n\nThe local sensitivity of FIM for discrete--PDFs is reflected in the fact that the specific ``$i-$ordering\" \nof the discrete values $p_{i}$ must be seriously taken into account in evaluating the sum in\n Eq.~(\\ref{Fisher-disc}).\nThis point was extensively discussed by Rosso \nand co-workers\\cite{Olivares2012A,Olivares2012B}.\nThe summands can be regarded to as a kind of ``distance\" between  two contiguous probabilities.\nThus, a different ordering of the pertinent summands would lead to a different FIM-value, hereby its local nature.\nIn the present work, we follow the Lehmer lexicographic order\\cite{Lehmer} in the generation \nof Bandt and Pompe PDF (see next section).\nGiven the local character of FIM, when combined with a global quantifier as the normalized Shannon entropy, \nconforms the Shannon--Fisher plane, ${\\mathcal H} \\times {\\mathcal F}$, introduced by Vignat and Bercher\\cite{Vignat2003}. \nThese authors showed that this plane is able to characterize the non-stationary behavior of a complex signal.\n \n\\subsection*{The Bandt and Pompe approach to the PDF determination}\n\\label{Sec:Bandt-Pompe}\n\nThe evaluation of the Information Theory derived quantifiers, like those previously introduced (Shannon entropy,\nFisher information and statistical complexity), suppose some prior knowledge about the system; specifically, \na probability distribution associated to the time series under analysis should be provided beforehand. \nThe determination of the most adequate PDF is a fundamental problem because the PDF $P$ and the sample space \n$\\Omega$ are inextricably linked. \n\nUsual methodologies assign to each time point of the series ${\\mathcal X}$ a symbol from a  finite alphabet \n$\\mathfrak{A}$, thus creating a {\\it symbolic sequence} that can be regarded to as a {\\it non causal coarse grained} \ndescription of the time series under consideration. \nAs a consequence, order relations and the time scales of the dynamics are lost. \nThe usual histogram technique corresponds to this kind of assignment.\n{\\it Causal information\\/}  may be duly incorporated if information about the past dynamics of the system is \nincluded in the symbolic sequence, i.e., symbols of alphabet $\\mathfrak{A}$ are assigned to a portion of the \nphase-space or trajectory.\n\nMany methods have been proposed for a proper selection of the probability space $(\\Omega, P)$. \n\n\n\n\n\n\n\nBandt and Pompe (BP)\\cite{Bandt2002} introduced a simple and robust symbolic methodology that takes into account \ntime causality of the time series (causal coarse grained methodology) by comparing neighboring values in a \ntime series.\nThe symbolic data are:\n{\\it (i)\\/}~created by ranking the values of the series; and\n{\\it (ii)\\/}~defined by reordering the embedded data in ascending order, which is tantamount to a phase space \nreconstruction with embedding dimension (pattern length) $D$ and time lag $\\tau$.\nIn this way, it is possible to quantify the diversity of the ordering symbols (patterns) derived from a scalar \ntime series.\n\nNote that the appropriate symbol sequence arises naturally from the time series, and no model-based assumptions \nare needed.\nIn fact, the necessary ``partitions'' are devised by comparing the order of neighboring relative values rather \nthan by apportioning amplitudes according to different levels.\nThis technique, as opposed to most of those in current practice, takes into account the temporal structure of the time series generated by the physical process under study.\nAs such, it allows us to uncover important details concerning the ordinal structure of the time \nseries\\cite{Rosso2007,Rosso2012} and can also yield information about temporal correlation\\cite{Rosso2009A,Rosso2009B}.\n\nIt is clear that this type of analysis of a time series entails losing details of the original series' \namplitude information.\nNevertheless, by just referring to the series' intrinsic structure, a meaningful difficulty reduction has \nindeed been achieved by BP with regard to the description of complex systems.\nThe symbolic representation of time series by recourse to a comparison of consecutive ($\\tau = 1$) or \nnonconsecutive ($\\tau > 1$) values allows for an accurate empirical reconstruction of the underlying phase-space, \neven in the presence of weak (observational and dynamic) noise\\cite{Bandt2002}.\nFurthermore, the ordinal patterns associated with the PDF are invariant with respect to nonlinear monotonous \ntransformations.\nAccordingly, nonlinear drifts or scaling artificially introduced by a measurement device will not modify the \nestimation of quantifiers, a nice property if one deals with experimental data (see, e.g.,\\cite{Saco2010}).\nThese advantages make the BP methodology more convenient than conventional methods based on range \npartitioning, i.e., a PDF based on histograms.\n\nTo use the BP methodology\\cite{Bandt2002} for evaluating the PDF, $P$, associated with the time \nseries (dynamical system) under study, one starts by considering partitions of the $D$-dimensional space that \nwill hopefully ``reveal'' relevant details of the ordinal structure of a given one-dimensional time series \n${\\mathcal X}(t) = \\{ x_t; t = 1, \\ldots, M\\}$ with embedding dimension $D > 1$ ($D \\in {\\mathbb N}$) \nand time lag $\\tau$ ($\\tau \\in {\\mathbb N}$).\nWe are interested in ``ordinal patterns'' of order (length) $D$ generated by\n\n", "itemtype": "equation", "pos": 24458, "prevtext": "\nthe above-mentioned Jensen-Shannon divergence and $Q_0$, a normalization constant \nsuch that $0 \\leq {\\mathcal Q}_{J} \\leq 1$:\n\n", "index": 17, "text": "\\begin{equation}\nQ_0 = -2 \\left\\{  {\\frac{N+1}{N}}  \\ln (N+1) - \\ln (2N)  +  \\ln N \\right\\}^{-1} \\ ,\n\\label{q0-jensen-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"Q_{0}=-2\\left\\{{\\frac{N+1}{N}}\\ln(N+1)-\\ln(2N)+\\ln N\\right\\}^{-1}\\ ,\" display=\"block\"><mrow><mrow><msub><mi>Q</mi><mn>0</mn></msub><mo>=</mo><mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mpadded width=\"+5pt\"><msup><mrow><mo>{</mo><mrow><mrow><mrow><mfrac><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mi>N</mi></mfrac><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>ln</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>N</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>ln</mi><mo>\u2061</mo><mi>N</mi></mrow></mrow><mo>}</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mpadded></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nwhich assign to each time $s$ the $D$-dimensional vector of values at times $s, s-\\tau,\\ldots,s-(D-1)\\tau$.\nClearly, the greater $D$, the more information on the past is incorporated into our vectors.\nBy ``ordinal pattern'' related to the time $(s)$, we mean the permutation $\\pi=(r_0,r_1, \\ldots,r_{D-1})$ \nof $[0,1,\\ldots,D-1]$ defined by \n\n", "itemtype": "equation", "pos": 31981, "prevtext": "\nare equal to the inverse of the maximum possible value of ${\\mathcal J} [P,P_e]$.\nThis value is obtained when one of the components of $P$, say $p_m$, is equal to one and the remaining $p_j$ \nare zero.\n\nThe Jensen-Shannon divergence, which quantifies the difference between probability distributions, \nis especially useful to compare the symbolic composition between different sequences\\cite{Grosse2002}.\nNote that the above introduced \nSCM\ndepends on two different probability distributions: one associated with the system under analysis, $P$, and the other the uniform distribution, $P_e$.\nFurthermore, it was shown that for a given value of ${\\mathcal H}$, the range of possible ${\\mathcal C}$ \nvalues varies between a minimum ${\\mathcal C}_{min}$ and a maximum ${\\mathcal C}_{max}$, \nrestricting the possible values of the \nSCM\\cite{Martin2006}.\n\nThus, it is clear that important additional information related to the correlational structure between the \ncomponents of the physical system is provided by evaluating the statistical complexity measure. \nIn this way, the information plane ${\\mathcal H} \\times {\\mathcal C}$ constitute a nice tool to visualizate \nand characterize different dynamical systems.\n\nIf our system lies in a very ordered state, which occurs when almost all the $p_{i}$--values are zeros \nexcept for a particular state $k \\neq i$ with $p_{k} \\cong 1$, both\nthe normalized Shannon entropy and statistical complexity are close to zero (${\\mathcal H} \\approx 0$ and \n${\\mathcal C} \\approx 0$), and the normalized Fisher's information measure is close to  one (${\\mathcal F} \\approx 1$).\nOn the other hand, when the system under study is represented by  a very disordered state, that is when all the \n$p_{i}$--values oscillate around the same value, we have ${\\mathcal H} \\approx 1$ while \n${\\mathcal C} \\approx 0$ and ${\\mathcal F} \\approx 0$.\nOne can state that the general FIM--behavior of the present discrete version \n(Eq.~(\\ref{Fisher-disc})),  \nis opposite to that of the Shannon entropy, except for periodic motions.\n\nThe local sensitivity of FIM for discrete--PDFs is reflected in the fact that the specific ``$i-$ordering\" \nof the discrete values $p_{i}$ must be seriously taken into account in evaluating the sum in\n Eq.~(\\ref{Fisher-disc}).\nThis point was extensively discussed by Rosso \nand co-workers\\cite{Olivares2012A,Olivares2012B}.\nThe summands can be regarded to as a kind of ``distance\" between  two contiguous probabilities.\nThus, a different ordering of the pertinent summands would lead to a different FIM-value, hereby its local nature.\nIn the present work, we follow the Lehmer lexicographic order\\cite{Lehmer} in the generation \nof Bandt and Pompe PDF (see next section).\nGiven the local character of FIM, when combined with a global quantifier as the normalized Shannon entropy, \nconforms the Shannon--Fisher plane, ${\\mathcal H} \\times {\\mathcal F}$, introduced by Vignat and Bercher\\cite{Vignat2003}. \nThese authors showed that this plane is able to characterize the non-stationary behavior of a complex signal.\n \n\\subsection*{The Bandt and Pompe approach to the PDF determination}\n\\label{Sec:Bandt-Pompe}\n\nThe evaluation of the Information Theory derived quantifiers, like those previously introduced (Shannon entropy,\nFisher information and statistical complexity), suppose some prior knowledge about the system; specifically, \na probability distribution associated to the time series under analysis should be provided beforehand. \nThe determination of the most adequate PDF is a fundamental problem because the PDF $P$ and the sample space \n$\\Omega$ are inextricably linked. \n\nUsual methodologies assign to each time point of the series ${\\mathcal X}$ a symbol from a  finite alphabet \n$\\mathfrak{A}$, thus creating a {\\it symbolic sequence} that can be regarded to as a {\\it non causal coarse grained} \ndescription of the time series under consideration. \nAs a consequence, order relations and the time scales of the dynamics are lost. \nThe usual histogram technique corresponds to this kind of assignment.\n{\\it Causal information\\/}  may be duly incorporated if information about the past dynamics of the system is \nincluded in the symbolic sequence, i.e., symbols of alphabet $\\mathfrak{A}$ are assigned to a portion of the \nphase-space or trajectory.\n\nMany methods have been proposed for a proper selection of the probability space $(\\Omega, P)$. \n\n\n\n\n\n\n\nBandt and Pompe (BP)\\cite{Bandt2002} introduced a simple and robust symbolic methodology that takes into account \ntime causality of the time series (causal coarse grained methodology) by comparing neighboring values in a \ntime series.\nThe symbolic data are:\n{\\it (i)\\/}~created by ranking the values of the series; and\n{\\it (ii)\\/}~defined by reordering the embedded data in ascending order, which is tantamount to a phase space \nreconstruction with embedding dimension (pattern length) $D$ and time lag $\\tau$.\nIn this way, it is possible to quantify the diversity of the ordering symbols (patterns) derived from a scalar \ntime series.\n\nNote that the appropriate symbol sequence arises naturally from the time series, and no model-based assumptions \nare needed.\nIn fact, the necessary ``partitions'' are devised by comparing the order of neighboring relative values rather \nthan by apportioning amplitudes according to different levels.\nThis technique, as opposed to most of those in current practice, takes into account the temporal structure of the time series generated by the physical process under study.\nAs such, it allows us to uncover important details concerning the ordinal structure of the time \nseries\\cite{Rosso2007,Rosso2012} and can also yield information about temporal correlation\\cite{Rosso2009A,Rosso2009B}.\n\nIt is clear that this type of analysis of a time series entails losing details of the original series' \namplitude information.\nNevertheless, by just referring to the series' intrinsic structure, a meaningful difficulty reduction has \nindeed been achieved by BP with regard to the description of complex systems.\nThe symbolic representation of time series by recourse to a comparison of consecutive ($\\tau = 1$) or \nnonconsecutive ($\\tau > 1$) values allows for an accurate empirical reconstruction of the underlying phase-space, \neven in the presence of weak (observational and dynamic) noise\\cite{Bandt2002}.\nFurthermore, the ordinal patterns associated with the PDF are invariant with respect to nonlinear monotonous \ntransformations.\nAccordingly, nonlinear drifts or scaling artificially introduced by a measurement device will not modify the \nestimation of quantifiers, a nice property if one deals with experimental data (see, e.g.,\\cite{Saco2010}).\nThese advantages make the BP methodology more convenient than conventional methods based on range \npartitioning, i.e., a PDF based on histograms.\n\nTo use the BP methodology\\cite{Bandt2002} for evaluating the PDF, $P$, associated with the time \nseries (dynamical system) under study, one starts by considering partitions of the $D$-dimensional space that \nwill hopefully ``reveal'' relevant details of the ordinal structure of a given one-dimensional time series \n${\\mathcal X}(t) = \\{ x_t; t = 1, \\ldots, M\\}$ with embedding dimension $D > 1$ ($D \\in {\\mathbb N}$) \nand time lag $\\tau$ ($\\tau \\in {\\mathbb N}$).\nWe are interested in ``ordinal patterns'' of order (length) $D$ generated by\n\n", "index": 19, "text": "\\begin{equation}\n\\label{asignation1}\n(s)\\mapsto \\left(x_{s-(D-1)\\tau},x_{s-(D-2)\\tau},\\ldots, x_{s-\\tau},x_{s}\\right)  ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"(s)\\mapsto\\left(x_{s-(D-1)\\tau},x_{s-(D-2)\\tau},\\ldots,x_{s-\\tau},x_{s}\\right),\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u21a6</mo><mrow><mo>(</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>D</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>D</mi><mo>-</mo><mn>2</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mi>\u03c4</mi></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>s</mi></msub><mo>)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nWe set $r_i < r_{i-1}$ if $x_{s-r_{i}} = x_{s-r_{i-1}}$ for uniqueness, although ties in samples from continuous \ndistributions have null probability.\n\nFor all the $D!$ possible orderings (permutations) $\\pi_i$ when  embedding dimension is $D$, \nand time-lag $\\tau$,\ntheir relative \nfrequencies can be naturally computed according to the number of times this particular order sequence is found \nin the time series,  divided by the total number of sequences,\n\n", "itemtype": "equation", "pos": 32459, "prevtext": "\nwhich assign to each time $s$ the $D$-dimensional vector of values at times $s, s-\\tau,\\ldots,s-(D-1)\\tau$.\nClearly, the greater $D$, the more information on the past is incorporated into our vectors.\nBy ``ordinal pattern'' related to the time $(s)$, we mean the permutation $\\pi=(r_0,r_1, \\ldots,r_{D-1})$ \nof $[0,1,\\ldots,D-1]$ defined by \n\n", "index": 21, "text": "\\begin{equation}\n\\label{asignation2}\nx_{s-r_{D-1}\\tau} \\le~x_{s-r_{D-2}\\tau} \\le \\cdots \\le~x_{s-r_{1}\\tau} \\le x_{s-r_0\\tau}  .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"x_{s-r_{D-1}\\tau}\\leq~{}x_{s-r_{D-2}\\tau}\\leq\\cdots\\leq~{}x_{s-r_{1}\\tau}\\leq x%&#10;_{s-r_{0}\\tau}.\" display=\"block\"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><msub><mi>r</mi><mrow><mi>D</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub><mo rspace=\"5.8pt\">\u2264</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><msub><mi>r</mi><mrow><mi>D</mi><mo>-</mo><mn>2</mn></mrow></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub><mo>\u2264</mo><mi mathvariant=\"normal\">\u22ef</mi><mo rspace=\"5.8pt\">\u2264</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub><mo>\u2264</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mrow><msub><mi>r</mi><mn>0</mn></msub><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nwhere $\\#$ denotes cardinality.\nThus, an ordinal pattern probability distribution $P = \\{ p(\\pi_i), i = 1, \\dots, D! \\}$ is obtained from the \ntime series.\n\nFigure~\\ref{fig:patrones} illustrates the construction principle of the ordinal patterns of length \n$D=2$, $3$ and $4$ with $\\tau = 1$\\cite{Parlitz2012}.\nConsider the sequence of observations $\\{x_0, x_1, x_2, x_3\\}$.\nFor $D=2$, there are only two possible directions from $x_0$ to $x_1$: up and down.\nFor $D=3$, starting from $x_1$ (up) the third part of the pattern can be above $x_1$, below $x_0$, or between \n$x_0$ and $x_1$.\nA similar situation can be found starting from $x_1$ (down). \nFor $D=4$, for each one of the six possible positions for $x_2$, there are four possible localizations for $x_3$, \nyielding $D!=4!=24$ different possible ordinal patterns.\nIn Fig.~\\ref{fig:patrones}, full circles and continuous lines represent the sequence values \n$x_0 < x_1 > x_2 > x_3$, which leads to the pattern  $\\pi=[0321]$.\nA graphical representation of all possible patterns corresponding to $D = 3, 4$ and $5$\ncan be found in Fig.~2 of Parlitz \\textit{et al.}\\cite{Parlitz2012}.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width = \\linewidth]{esquema-permuta-0321}\n\\caption{Illustration of the construction principle for ordinal patterns of length $D$ \\cite{Parlitz2012}. \nIf $D=4$ and $\\tau=1$, full circles and continuous lines represent the sequence of values $x_0 < x_1 > x_2 > x_3$ \nwhich lead to the pattern $\\pi=[0321]$.}\n\\label{fig:patrones}\n\\end{figure}\n\nThe embedding dimension $D$ plays an important role in the evaluation of the appropriate probability \ndistribution, because $D$ determines the number of accessible states $D!$ and also conditions the minimum acceptable \nlength $M \\gg D!$ of the time series that one needs in order to work with reliable statistics\\cite{Kowalski2007}.\nRegarding the selection of the parameters, Bandt and Pompe suggested working with $4 \\leq D \\leq 6$, and specifically \nconsidered a time lag $\\tau = 1$ in their cornerstone paper\\cite{Bandt2002}.\nNevertheless, it is clear that other values of $\\tau$ could provide additional information.\nIt has been recently shown that this parameter is strongly related, if it is relevant, to the intrinsic time scales \nof the system under analysis\\cite{Zunino2010B,Soriano2011,Zunino2012}.\n\nAdditional advantages of the  method reside in\n{\\it i)\\/} its simplicity (it requires few parameters: the pattern length/embedding dimension $D$ and the time lag \n$\\tau$), and\n{\\it ii)\\/} the extremely fast nature of the calculation process. \nThe BP methodology can be applied not only  to time series representative of low dimensional dynamical systems, \nbut also to any type of time series (regular, chaotic, noisy, or reality based).\nIn fact, the existence of an attractor in the $D$-dimensional phase space in not assumed.\nThe only condition for the applicability of the BP method is  a very weak stationary assumption: for $k \\leq D$, \nthe probability for $x_t < x_{t+k}$ should not depend on $t$.\nFor a review of BP's methodology and its applications to physics, biomedical and econophysics signals see \nZanin {\\it et al.\\/}\\cite{Zanin2012}. \nMoreover, \nRosso {\\it et al.\\/}\\cite{Rosso2007} show that the above mentioned quantifiers produce better descriptions \nof the process associated dynamics when the PDF is computed using BP rather than using the usual histogram methodology.\n\nThe BP proposal for associating probability distributions to time series (of an underlying symbolic \nnature) constitutes a significant advance in the study of nonlinear dynamical systems\\cite{Bandt2002}.\nThe method provides univocal prescription for ordinary, global entropic quantifiers of the Shannon-kind.\nHowever, as was shown by Rosso and coworkers\\cite{Olivares2012A,Olivares2012B}, ambiguities arise in applying the \nBP technique with reference to the permutation of ordinal patterns. \nThis happens if one wishes to employ the BP-probability density to construct local entropic quantifiers, \nlike the Fisher information measure, which would characterize time series generated by nonlinear dynamical systems.\n\nThe local sensitivity of the Fisher information measure for discrete PDFs is reflected in the fact that the specific \n``$i$-ordering'' of the discrete values $p_i$ must be seriously taken into account in evaluating Eq.~(\\ref{Fisher-disc}).\nThe numerator can be regarded to as a kind of ``distance'' between two contiguous probabilities.\nThus, a different ordering of the summands will lead, in most cases, to a different Fisher information value.\nIn fact, if we have a discrete PDF given by $P = \\{ p_i, i = 1, \\ldots , N\\}$, we will have $N!$ possibilities \n{for the $i$-ordering.}\n\nThe question is, which is the arrangement that one could regard as the ``proper'' ordering?\nThe answer is straightforward in some cases, the histogram-based PDF constituting a conspicuous example.\nFor such a procedure, one first divides the interval $[a, b]$ (with $a$ and $b$ the minimum and maximum amplitude \nvalues in the time series) into a finite number on non-overlapping sub-intervals (bins).\nThus, the division procedure of the interval $[a, b]$ provides the natural order sequence for the evaluation of \nthe PDF gradient involved in the Fisher information measure.\nIn our current paper, we chose the lexicographic ordering given by the algorithm of Lehmer\\cite{Lehmer}, \namong other possibilities, due to its better distinction of different \ndynamics in the Shannon--Fisher plane, ${\\mathcal H} \\times {\\mathcal F}$ (see\\cite{Olivares2012A,Olivares2012B}).\n\n\n\\section*{Signature features and exploratory data analysis}\n\\label{Sec:Results}\n\nOnline handwritten classification and verification is an interesting and challenging classification problem.\nOn the one hand, intra-personal variation information can be large. \nSome people provide signatures with poor consistency. \nThe speed, pressure and inclination, for example, pertaining to the signatures made by the same person can \ndiffer greatly  on regularity which makes it quite challenging to extract consistent features. \nOn the other hand, we can only obtain few samples from one person and no forgeries in practice. \nThis makes it very difficult to determine the reliability of extracted features. \n\n\n\n\n\nThe main idea is to construct an efficient classification scheme for data acquisition, or the reduction of \noften unmanageable large datasets to a parsimonious form, without mislay important statistical information.\nWe aim at discovering relevant characteristic statistical structures which could be exploited if the key \ninformation can be efficiently condensed into a suitable low-dimensional object. \n\nThe features we employ in this work are the Information Theory quantifiers already presented.\nFor each of the $k$ subjects ($k=1,\\ldots,100$) in the database and its $j$ associated signatures \n($25$ genuine and $25$ \nskilled\nforgery), two associated time series ${\\mathbf X}^{(k;\\alpha)}_j$ and \n${\\mathbf Y}^{(k;\\alpha)}_j$ are extracted and transformed into BP's PDFs with pattern length \n(embedding dimension) $D = 5$ and time lag $\\tau = 1$.\nNote that the condition $M \\gg D!$ its satisfied.\n\nWe denoted these PDFs as:\n\n", "itemtype": "equation", "pos": 33061, "prevtext": "\nWe set $r_i < r_{i-1}$ if $x_{s-r_{i}} = x_{s-r_{i-1}}$ for uniqueness, although ties in samples from continuous \ndistributions have null probability.\n\nFor all the $D!$ possible orderings (permutations) $\\pi_i$ when  embedding dimension is $D$, \nand time-lag $\\tau$,\ntheir relative \nfrequencies can be naturally computed according to the number of times this particular order sequence is found \nin the time series,  divided by the total number of sequences,\n\n", "index": 23, "text": "\\begin{equation}\n\\label{eq:frequ}\np(\\pi_i)= \\frac{\\# \\{s|s\\leq N-(D-1)\\tau ; (s)  \\text{ is of type } \\pi_i \\}}{N-(D-1)\\tau} ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"p(\\pi_{i})=\\frac{\\#\\{s|s\\leq N-(D-1)\\tau;(s)\\text{ is of type }\\pi_{i}\\}}{N-(D%&#10;-1)\\tau},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c0</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>s</mi><mo stretchy=\"false\">|</mo><mrow><mi>s</mi><mo>\u2264</mo><mrow><mrow><mi>N</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>D</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow><mo>;</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0is of type\u00a0</mtext><mo>\u2062</mo><msub><mi>\u03c0</mi><mi>i</mi></msub></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><mi>N</mi><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>D</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\u03c4</mi></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nin which $j=1, \\ldots, 25$, and $\\alpha = G, F$ identify genuine and skilled forgery signatures, respectively.\n\nWe computed the normalized permutation Shannon entropy ${\\mathcal H}$,\nthe permutation statistical complexity ${\\mathcal C}$,\nand the permutation Fisher information measure \n${\\mathcal F}$ from these PDFs, and the obtained values are denoted as:\n\n", "itemtype": "equation", "pos": 40430, "prevtext": "\nwhere $\\#$ denotes cardinality.\nThus, an ordinal pattern probability distribution $P = \\{ p(\\pi_i), i = 1, \\dots, D! \\}$ is obtained from the \ntime series.\n\nFigure~\\ref{fig:patrones} illustrates the construction principle of the ordinal patterns of length \n$D=2$, $3$ and $4$ with $\\tau = 1$\\cite{Parlitz2012}.\nConsider the sequence of observations $\\{x_0, x_1, x_2, x_3\\}$.\nFor $D=2$, there are only two possible directions from $x_0$ to $x_1$: up and down.\nFor $D=3$, starting from $x_1$ (up) the third part of the pattern can be above $x_1$, below $x_0$, or between \n$x_0$ and $x_1$.\nA similar situation can be found starting from $x_1$ (down). \nFor $D=4$, for each one of the six possible positions for $x_2$, there are four possible localizations for $x_3$, \nyielding $D!=4!=24$ different possible ordinal patterns.\nIn Fig.~\\ref{fig:patrones}, full circles and continuous lines represent the sequence values \n$x_0 < x_1 > x_2 > x_3$, which leads to the pattern  $\\pi=[0321]$.\nA graphical representation of all possible patterns corresponding to $D = 3, 4$ and $5$\ncan be found in Fig.~2 of Parlitz \\textit{et al.}\\cite{Parlitz2012}.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width = \\linewidth]{esquema-permuta-0321}\n\\caption{Illustration of the construction principle for ordinal patterns of length $D$ \\cite{Parlitz2012}. \nIf $D=4$ and $\\tau=1$, full circles and continuous lines represent the sequence of values $x_0 < x_1 > x_2 > x_3$ \nwhich lead to the pattern $\\pi=[0321]$.}\n\\label{fig:patrones}\n\\end{figure}\n\nThe embedding dimension $D$ plays an important role in the evaluation of the appropriate probability \ndistribution, because $D$ determines the number of accessible states $D!$ and also conditions the minimum acceptable \nlength $M \\gg D!$ of the time series that one needs in order to work with reliable statistics\\cite{Kowalski2007}.\nRegarding the selection of the parameters, Bandt and Pompe suggested working with $4 \\leq D \\leq 6$, and specifically \nconsidered a time lag $\\tau = 1$ in their cornerstone paper\\cite{Bandt2002}.\nNevertheless, it is clear that other values of $\\tau$ could provide additional information.\nIt has been recently shown that this parameter is strongly related, if it is relevant, to the intrinsic time scales \nof the system under analysis\\cite{Zunino2010B,Soriano2011,Zunino2012}.\n\nAdditional advantages of the  method reside in\n{\\it i)\\/} its simplicity (it requires few parameters: the pattern length/embedding dimension $D$ and the time lag \n$\\tau$), and\n{\\it ii)\\/} the extremely fast nature of the calculation process. \nThe BP methodology can be applied not only  to time series representative of low dimensional dynamical systems, \nbut also to any type of time series (regular, chaotic, noisy, or reality based).\nIn fact, the existence of an attractor in the $D$-dimensional phase space in not assumed.\nThe only condition for the applicability of the BP method is  a very weak stationary assumption: for $k \\leq D$, \nthe probability for $x_t < x_{t+k}$ should not depend on $t$.\nFor a review of BP's methodology and its applications to physics, biomedical and econophysics signals see \nZanin {\\it et al.\\/}\\cite{Zanin2012}. \nMoreover, \nRosso {\\it et al.\\/}\\cite{Rosso2007} show that the above mentioned quantifiers produce better descriptions \nof the process associated dynamics when the PDF is computed using BP rather than using the usual histogram methodology.\n\nThe BP proposal for associating probability distributions to time series (of an underlying symbolic \nnature) constitutes a significant advance in the study of nonlinear dynamical systems\\cite{Bandt2002}.\nThe method provides univocal prescription for ordinary, global entropic quantifiers of the Shannon-kind.\nHowever, as was shown by Rosso and coworkers\\cite{Olivares2012A,Olivares2012B}, ambiguities arise in applying the \nBP technique with reference to the permutation of ordinal patterns. \nThis happens if one wishes to employ the BP-probability density to construct local entropic quantifiers, \nlike the Fisher information measure, which would characterize time series generated by nonlinear dynamical systems.\n\nThe local sensitivity of the Fisher information measure for discrete PDFs is reflected in the fact that the specific \n``$i$-ordering'' of the discrete values $p_i$ must be seriously taken into account in evaluating Eq.~(\\ref{Fisher-disc}).\nThe numerator can be regarded to as a kind of ``distance'' between two contiguous probabilities.\nThus, a different ordering of the summands will lead, in most cases, to a different Fisher information value.\nIn fact, if we have a discrete PDF given by $P = \\{ p_i, i = 1, \\ldots , N\\}$, we will have $N!$ possibilities \n{for the $i$-ordering.}\n\nThe question is, which is the arrangement that one could regard as the ``proper'' ordering?\nThe answer is straightforward in some cases, the histogram-based PDF constituting a conspicuous example.\nFor such a procedure, one first divides the interval $[a, b]$ (with $a$ and $b$ the minimum and maximum amplitude \nvalues in the time series) into a finite number on non-overlapping sub-intervals (bins).\nThus, the division procedure of the interval $[a, b]$ provides the natural order sequence for the evaluation of \nthe PDF gradient involved in the Fisher information measure.\nIn our current paper, we chose the lexicographic ordering given by the algorithm of Lehmer\\cite{Lehmer}, \namong other possibilities, due to its better distinction of different \ndynamics in the Shannon--Fisher plane, ${\\mathcal H} \\times {\\mathcal F}$ (see\\cite{Olivares2012A,Olivares2012B}).\n\n\n\\section*{Signature features and exploratory data analysis}\n\\label{Sec:Results}\n\nOnline handwritten classification and verification is an interesting and challenging classification problem.\nOn the one hand, intra-personal variation information can be large. \nSome people provide signatures with poor consistency. \nThe speed, pressure and inclination, for example, pertaining to the signatures made by the same person can \ndiffer greatly  on regularity which makes it quite challenging to extract consistent features. \nOn the other hand, we can only obtain few samples from one person and no forgeries in practice. \nThis makes it very difficult to determine the reliability of extracted features. \n\n\n\n\n\nThe main idea is to construct an efficient classification scheme for data acquisition, or the reduction of \noften unmanageable large datasets to a parsimonious form, without mislay important statistical information.\nWe aim at discovering relevant characteristic statistical structures which could be exploited if the key \ninformation can be efficiently condensed into a suitable low-dimensional object. \n\nThe features we employ in this work are the Information Theory quantifiers already presented.\nFor each of the $k$ subjects ($k=1,\\ldots,100$) in the database and its $j$ associated signatures \n($25$ genuine and $25$ \nskilled\nforgery), two associated time series ${\\mathbf X}^{(k;\\alpha)}_j$ and \n${\\mathbf Y}^{(k;\\alpha)}_j$ are extracted and transformed into BP's PDFs with pattern length \n(embedding dimension) $D = 5$ and time lag $\\tau = 1$.\nNote that the condition $M \\gg D!$ its satisfied.\n\nWe denoted these PDFs as:\n\n", "index": 25, "text": "\\begin{align*}\nP_{X;j}^{(k;\\alpha)}&= \\text{ Bandt and Pompe's PDF~of } {\\mathbf X}^{(k;\\alpha)}_j |_{D,\\tau}, \\text{ and}\\\\\nP_{Y;j}^{(k;\\alpha)}&= \\text{ Bandt and Pompe's PDF of } {\\mathbf Y}^{(k;\\alpha)}_j |_{D,\\tau},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P_{X;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi>P</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{ Bandt and Pompe's PDF~{}of }{\\mathbf{X}}^{(k;\\alpha)}_{j}%&#10;|_{D,\\tau},\\text{ and}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msub><mrow><mrow><mtext>\u00a0Bandt and Pompe\u2019s PDF\u00a0of\u00a0</mtext><mo>\u2062</mo><msubsup><mi>\ud835\udc17</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mrow><mi>D</mi><mo>,</mo><mi>\u03c4</mi></mrow></msub><mo>,</mo><mtext>\u00a0and</mtext></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle P_{Y;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi>P</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{ Bandt and Pompe's PDF of }{\\mathbf{Y}}^{(k;\\alpha)}_{j}|_%&#10;{D,\\tau},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msub><mrow><mrow><mtext>\u00a0Bandt and Pompe\u2019s PDF of\u00a0</mtext><mo>\u2062</mo><msubsup><mi>\ud835\udc18</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mrow><mi>D</mi><mo>,</mo><mi>\u03c4</mi></mrow></msub></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\n\nWe perform Exploratory Data Analysis (EDA)\non the Information Theory quantifiers \nlooking for simple descriptions of the data. \nApart from simple descriptive univariate measures, we use the Pearson correlation to measure the association between features. \nThis analysis was performed using the {\\tt R} language and \nplatform version~3.2.1 (\\url{http:\\\\www.R-project.org}).  \n\nFigure~\\ref{Fig:HistEntropy} shows a scatterplot of the Entropy for both the genuine and skilled forgery signatures.\nThe $5000$ points correspond to $25$ genuine signatures (in blue) and $25$ forgery signatures \n(in red) for each of the $100$ subjects. \nBoth types of signatures show similar association (Correlation): \n${\\rm Corr}({\\mathcal H}_{X;j}^{(k;G)}, {\\mathcal H}_{Y;j}^{(k;G)}) = 0.9665$ and \n${\\rm Corr}({\\mathcal H}_{X;j}^{(k;F)}, {\\mathcal H}_{Y;j}^{(k;F)}) = 0.9770$.\n\nThe \nentropies of both types of signatures are overlapped and scattered elliptically. \nHowever, the bivariate mean and dispersion values differ. \n\nEntropies are less dispersed in the genuine than in the skilled forgery signatures,  \na signal of the separability between them. \nMarginal density plots show the distribution of entropy for each coordinate of both types of signatures. \nThese plots, however limited due to its marginal nature, reveal several modes, and suggest both wide and narrow \nstructures in the data. \n\n\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{HistEntropy}\n\\caption{Scatter plot with marginal  kernel density estimates of entropy quantifiers in both trajectory coordinates \ntime series \n${\\mathbf X}$ and ${\\mathbf Y}$.\nGenuine (blue) and skilled forgery signatures (red points), 100 subjects. \nMarginal kernel densities depict the distribution of entropy quantifiers along both axes.\n}\\label{Fig:HistEntropy}\n\\end{figure}\n\nFigure~\\ref{Fig:ContourEntropy} shows the contour plots of bivariate kernel density estimates for the \nentropy in genuine and forgery signatures. \nA number of features are immediately noticeable. \nThe dispersion in the former group is much smaller than in the latter (less than $0.4$). \nThe kernel density estimates reveal skewness and a mild multimodality in the joint distribution of the data.\nThere are also quite many points that are far from these curves and cluster centers. \nThese points correspond to abnormal local estimates obtained in heterogeneous blocks, possibly induced by the presence of clusters. \nThe modes in genuine signatures are smaller than in forgery signatures, and this may be used as discriminatory measure. \nSimilar results are obtained for the Complexity and the Fisher information; \nthese are \nreported in the Supplementary Information, see Figs.~S1 to~S4, respectively.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.95\\linewidth]{ContourEntropy}\n\\caption{Contour plot superimposed on the scatterplot of entropy quantifiers for genuine (right panel) \nand skilled forgery signatures (left panel)}\n\\label{Fig:ContourEntropy}\n\\end{figure}\n\n\\section*{Signatures classification}\n\\label{Sec-Classification}\n\n\nAs pointed out by Boul\\'etreal {\\it et al.\\/}\\cite{Bouletreau1998}, a signature  is characterized\nby two aspects: \n{\\it a)\\/}~a conscious one associated  to the pattern signature; and\n{\\it b)\\/}~an unconscious one which leads spontaneous movements constituting the drawing.\nThese two factors produce high variability, being the amount of signature variability \nstrongly writer-dependent.\nIn fact, the signature {\\it variability\\/} or, conversely, the signature {\\it stability\\/} can be considered \nan important indicator for writer characterization\\cite{Houmani2014}.\nHoumani and Garcia-Salicetti\\cite{Houmani2014} argue that signature stability is required in genuine signatures \nin order to characterize a writer: the less stable a signature is, the more likely it is that forgery \nwill be dangerously close to genuine signatures for any classifier.\nAlso, complex enough signatures are required in order to guarantee a certain level of security, in the sense \nthat the more complex a signature is, the more difficult it will be to forge it\\cite{Houmani2014}.\n\nBoul\\'etreal and collaborators\\cite{Bouletreau1998,Vincent2000} propose a signature complexity measure related \nto signature legibility and based on fractal dimension.\nThey classify writer styles into: highly cursive, very legible, separated,  badly formed and small \nwritings, using only genuine signatures.\nUnfortunately, such resulting categories were not confronted to classifiers for performance analysis.\n\nWe classify the genuine signatures based on causal Information Theory quantifiers: \nNormalized Permutation Shannon Entropy, \nPermutation Statistical Complexity and \nPermutation Fisher Information Measure of both $\\mathbf X$ and $\\mathbf Y$ trajectories\non each of the one hundred writers in the MCYT data base, and their $25$ original signatures.\nThe mean and standard deviation values were clustered using the neighbor-joining method and an automatic Hierarchical Clustering  \nwith the Euclidean distance-based dissimilarity matrix.\nEach feature was treated independently, and the results are shown as circular dendrograms.\nFigure~\\ref{Fig:DendroEntropy} shows the results of clustering the Entropy.\nWe distinguish three classes of genuine signatures denoted by H1, H2, and H3. \n\n\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.9\\linewidth, angle=0]{DendogramEntropy}\n\\caption{\nNeighbor-joining, rooted, circular dendrogram clustering of genuine signatures by Entropy: \nH1, H2, and H3, in red, blue, and green, respectively. \n}\\label{Fig:DendroEntropy}\n\\end{figure}\n\nThe H1 group is the first group to form, i.e., the one comprised of the most similar individuals.\nIt is formed below the $25\\%$ level, and it is composed by two subgroups: H1A and H1B. \nThe H1A group is formed exclusively by oversimplified signatures made by simple loops without identifiable letters. \nIt encompasses the following subjects: 1, 16, 17, 22, 23, 27, 29, 37, 83. \nThe same group is formed when the other features are used. \nThe H1B group is comprised of the following subjects: 2, 5, 8, 10, 19, 21, 24, 28, 32, 35, 36, 39, 43, 48, \n49, 51, 55, 58, 59, 64, 69, 70, 74, 77, 89. \nAlthough these are simplified signatures, traces of letters and/or more complex curves appear and differentiate them \nfrom the members of the H1A group.\n\nThe H2 group is formed approximately at the $32\\%$ level, and, again, it is comprised of two distinct groups: \nH2A and H2B. \nThe subjects that make the H2A group are: 4, 7, 12, 15, 18, 20, 30, 31, 34, 38, 40, 41, 42, 52, 57, 60, 62, \n66, 67, 68, 71, 73, 75, 79, 80, 81, 86, 87, 91, 96, 100. \nIt is composed by signatures with traces that resemble letters, but that are not perfectly identifiable, \nand that include circling traces of large or moderate size. \nSignatures in this group are kind of framed by large loops.\nThe H2B group is similar to the previous one, i.e., it is formed by signatures with large and medium size \ncircling traces, but with more identifiable letters than in the previous groups. \nNames and surnames are more readable in this group than in previous ones. \nIt is formed by the following signatures: 6, 9, 13, 25, 33, 45, 50, 63, 65, 76, 78, 82, 84, 85, 88, 92, 94, 95, 97, 99. \n\nThe H3 group is formed at, approximately, the $43\\%$ level by the fusion of two other highly unbalanced subgroups: \none, H3A, with only two subjects (44, 46) and the other, H3B, with thirteen subjects (3, 11, 14, 26, 47, 53, 54, 56, 61, \n72, 90, 93, 98). \nThese two clusters form at approximately the same level. \nThe former is composed of calligraphic signatures where vertical traces predominate over horizontal ones. \nThe latter is composed of highly cursive signatures, where separation between the surname and the family name \npredominates. \n\nThe same results of clustering was obtained with the Manhattan (norm ${\\cal L}_1$) and Maximum distances ($\\mathcal L_\\infty$ norm), showing that Entropy is an expressive and stable quantifier. \nSimilar analyses were carried with the Permutation Statistical Complexity and Permutation Fisher Information \n(presented in figures Figs.~S5 and~S6 in the Supplementary Information).\nComplexity produces the same clusters identified by Entropy, so it adds no new information.\nThe Fisher information measure forms the same H1A group that was identified by the Entropy, but with less \ncohesion, at about $15\\%$. \nIn other words, these nine subjects are more similar locally than globally. \nAs with Entropy, three main groups form at similar levels. \nThe members of these clusters are slight variations of those identified using Entropy, with very similar structure. \n\nTable~\\ref{tab:tab-Measure-subject} presents \nthe mean and standard deviation of the three quantifiers over the $25$ genuine and $25$ skilled forgery signatures \n(${\\mathbf X}$ and ${\\mathbf Y}$ time series) for each of the typical subjects, split in the three aforementioned types H1, H2 and~H3.\nThere are interesting tendencies in these data.\nGenuine signatures present quantifiers values lower than those corresponding to forgery signatures, \nand the latter also exhibit larger standard deviation.\nThis could be explained by the imitative character of these signatures, however it deserves closer  studies.\n\n\\begin{sidewaystable}[hbt] \\centering \n\\centering\n\\begin{tabular}{@{\\extracolsep{5pt}} ccccccc|cc|cc} \n\\\\ \\cmidrule(r){6-11}\n      &             &                    &  &  & \\multicolumn{2}{c|}{Entropy}&\\multicolumn{2}{c|}{Complexity}&\\multicolumn{2}{c}{Fisher Information} \\\\ \\midrule\n\n$Type$ & $Sub-Type$ & Subject & Coordinate & Class & Mean & S.D. & Mean & S.D. & Mean & S.D. \\\\ \n\\midrule \n\\multirow{8}{*}{H1} & \\multirow{4}{*}{H1A} & \\multirow{4}{*}{22} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.1568$ & $0.0052$ & $0.1490$ & $0.0039$ & $0.4688$ & $0.0070$  \\\\ \n\t\t    &  \t\t\t   &\t\t\t &                                &  G & $0.1519$ & $0.0019$ & $0.1457$ & $0.0015$ & $0.4766$ & $0.0035$  \\\\ \\cmidrule(r){4-11}\n\t\t    &                      &  \t\t\t & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.1595$ & $0.0071$ & $0.1511$ & $0.0052$ & $0.4665$ & $0.0097$  \\\\ \n\t\t    &  \t\t\t   &\t\t\t &                                &  G & $0.1512$ & $0.0042$ & $0.1447$ & $0.0037$ & $0.4734$ & $0.0046$  \\\\ \n\\cmidrule(r){3-11} \n& \\multirow{4}{*}{H1B} &\\multirow{4}{*}{39} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2212$ & $0.0384$ & $0.1941$ & $0.0257$ & $0.4286$ & $0.0147$  \\\\ \n&                      &\t\t    &\t\t\t             &  G & $0.1749$ & $0.0037$ & $0.1620$ & $0.0028$ & $0.4497$ & $0.0029$  \\\\ \\cmidrule(r){4-11}\n&\t\t       &\t\t    & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2270$ & $0.0449$ & $0.1980$ & $0.0296$ & $0.4277$ & $0.0153$  \\\\ \n&\t\t       &\t\t    &\t\t\t             &  G & $0.1776$ & $0.0043$ & $0.1644$ & $0.0031$ & $0.4491$ & $0.0035$  \\\\ \n\\midrule \n\\multirow{8}{*}{H2} & \\multirow{4}{*}{H2A} &\\multirow{4}{*}{60} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2482$ & $0.0593$ & $0.2112$ & $0.0365$ & $0.4212$ & $0.0107$  \\\\ \n\t\t   & & &\t\t\t&  G & $0.2010$ & $0.0056$ & $0.1803$ & $0.0040$ & $0.4331$ & $0.0031$  \\\\ \\cmidrule(r){4-11}\n\t\t   & & & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2442$ & $0.0544$ & $0.2090$ & $0.0339$ & $0.4219$ & $0.0134$  \\\\ \n\t\t   & & &\t\t\t&  G & $0.2079$ & $0.0043$ & $0.1861$ & $0.0030$ & $0.4315$ & $0.0024$  \\\\ \n\\cmidrule(r){3-11}\n& \\multirow{4}{*}{H2B}&\\multirow{4}{*}{6} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2621$ & $0.0584$ & $0.2194$ & $0.0334$ & $0.4143$ & $0.0137$ \\\\ \n& &\t\t   &\t\t\t&  G & $0.2337$ & $0.0149$ & $0.2032$ & $0.0095$ & $0.4205$ & $0.0066$ \\\\ \\cmidrule(r){4-11}\n& &\t\t   & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2648$ & $0.0538$ & $0.2218$ & $0.0304$ & $0.4136$ & $0.0134$ \\\\ \n& &\t\t   &\t\t\t&  G & $0.2314$ & $0.0102$ & $0.2018$ & $0.0067$ & $0.4211$ & $0.0050$ \\\\ \n \\midrule\n\\multirow{8}{*}{H3} &  \\multirow{4}{*}{H3A} &\\multirow{4}{*}{98} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.3236$ & $0.0646$ & $0.2529$ & $0.0320$ & $0.3937$ & $0.0208$ \\\\ \n\t\t & &  &\t\t\t&  G & $0.2707$ & $0.0101$ & $0.2268$ & $0.0064$ & $0.4106$ & $0.0032$ \\\\ \\cmidrule(r){4-11}\n\t\t &  & & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.3204$ & $0.0794$ & $0.2497$ & $0.0388$ & $0.3970$ & $0.0208$ \\\\ \n\t\t &  & &\t\t\t&  G & $0.2664$ & $0.0124$ & $0.2243$ & $0.0077$ & $0.4105$ & $0.0034$ \\\\ \n\\cmidrule(r){3-11} \n& \\multirow{4}{*}{H3B}&\\multirow{4}{*}{46} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.3514$ & $0.0641$ & $0.2691$ & $0.0294$ & $0.3940$ & $0.0156$  \\\\ \n& &\t\t   &\t\t\t&  G & $0.3480$ & $0.0282$ & $0.2720$ & $0.0156$ & $0.4019$ & $0.0047$  \\\\ \\cmidrule(r){4-11}\n& &\t\t   & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.3419$ & $0.0681$ & $0.2639$ & $0.0323$ & $0.3940$ & $0.0163$  \\\\ \n& &\t\t   &\t\t\t&  G & $0.3270$ & $0.0263$ & $0.2599$ & $0.0148$ & $0.4008$ & $0.0052$  \\\\ \n\\bottomrule\n\\end{tabular} \n \\caption{Sample mean and standard deviation (S.D.) of the time series quantifiers for the 25 genuine (G) \nand 25 \nskilled\nforged (F) signatures, for each of the typical subjects: H1A, H1B, H2A, H2B, H3A, and H3B (same order as in Fig.~\\ref{fig:MCYT-firmas}).} \n\\label{tab:tab-Measure-subject} \n\\end{sidewaystable} \n\nThe classification into subclasses of genuine signatures was also carried by the parallelepiped \nalgorithm\\cite{Richards1999}, arguably the simplest model-free classification procedure.\nEntropy leads to clusters with nice interpretability. \nFigure~\\ref{Fig:EntropyClasificationBoxesEntropy} shows the regions that define the three classes \nidentified by the dendrogram based on Entropy presented in Fig~\\ref{Fig:DendroEntropy}. \nAll subclasses are well separated by disjoint boxes, with the only exception of H1B and H2A that  \noverlap slightly but without compromising the discrimination. \nThe classes are preserved using this classification superimposed with Complexity and Fisher Information features; see Figs.~S7 and~S8 in the Supplementary Information.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.9\\linewidth, angle=0]{ClassificationWithSignatureTrue}\n\\caption{Classification by the rule of the parallelepiped of genuine signatures using Entropy\n(one signature example from each of the three groups is shown).\nEach subject is identified by its ID.\n}\\label{Fig:EntropyClasificationBoxesEntropy}\n\\end{figure}\n\n\\section*{Online signature verification}\n\\label{Sec-Verification}\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe problem we have at hand consists in identifying suspicious signatures, given that we only have examples from genuine signatures.\nThis is due to the fact that, in practice, it is too expensive, too hard or even impossible to obtain a significant number of good quality forgery signatures for every possible individual in the data base.\nThis, thus, configures a One-Class classification problem.\nAmong the many ways of tackling such problems, Support Vector Machines (SVMs) are suitable for solving machine learning problems even in large dimensional feature spaces\\cite{Campbell2011,Boser1992,Vapnik1995}. \n\nSVMs were introduced by Vapnik and co-workers\\cite{Boser1992,Vapnik1998},\nand extended by a number of other researchers.\nTheir remarkably robust performance with respect to sparse and noisy data makes them the choice in several applications.\nA SVM is primarily a method that performs classification tasks by\nconstructing hyperplanes in a multidimensional space that separates cases of different class labels. \nSVMs perform both regression and classification tasks and can handle multiple continuous and categorical \nvariables.\nTo construct an optimal hyperplane, a SVM employs an iterative training algorithm, which is used\nto minimize an error function.\n\nOne-Class Support Vector Machines (OC-SVMs) are a natural extension of SVMs\\cite{Scholkopf2001, Scholkopf2002}. \nThe solution consists in estimating a distribution that encompasses most of the observations, and then labeling as ``suspicious'' those that lie far from it with respect to a suitable metric.\nAn OC-SVM solution is built estimating a probability distribution function which makes most of the observed data more likely than the rest, and a decision rule that separates these observation by the largest possible margin.\nThe computational complexity of the learning phase is intensive because the training of an OC-SVM involves a quadratic programming problem\\cite{Boser1992}, but once the decision function is determined, it can be used to predict the class label of new test data effortlessly.\n\nIn our case, the observations are six-dimensional vectors: Entropy, Complexity and Fisher Information in each of the two directions, horizontal and vertical, and\nwe train the OC-SVM with genuine signatures.\nLet ${\\cal Z} = \\{ z_1, z_2, \\dots , z_N \\}$ be the six-dimensional training examples of genuine signatures. \nLet $\\Phi \\colon {\\cal Z} \\rightarrow {\\cal G}$ be a kernel map which transforms the\ntraining examples to another space. \nThen, to separate the data set from the origin, one needs to solve the following quadratic programming problem:\n\n", "itemtype": "equation", "pos": 41022, "prevtext": "\nin which $j=1, \\ldots, 25$, and $\\alpha = G, F$ identify genuine and skilled forgery signatures, respectively.\n\nWe computed the normalized permutation Shannon entropy ${\\mathcal H}$,\nthe permutation statistical complexity ${\\mathcal C}$,\nand the permutation Fisher information measure \n${\\mathcal F}$ from these PDFs, and the obtained values are denoted as:\n\n", "index": 27, "text": "\\begin{align*}\n {\\mathcal H}_{X;j}^{(k;\\alpha)} &=  {\\mathcal H}[P_{X;j}^{(k;\\alpha)}],  &{\\mathcal H}_{Y;j}^{(k;\\alpha)} &= {\\mathcal H}[P_{Y;j}^{(k;\\alpha)}];\\\\\n {\\mathcal C}_{X;j}^{(k;\\alpha)} &= \\ {\\mathcal C}[P_{X;j}^{(k;\\alpha)}],  &{\\mathcal C}_{Y;j}^{(k;\\alpha)}  &= {\\mathcal C}[P_{Y;j}^{(k;\\alpha)}];\\\\\n {\\mathcal F}_{X;j}^{(k;\\alpha)} &= {\\mathcal F}[P_{X;j}^{(k;\\alpha)}],  &{\\mathcal F}_{Y;j}^{(k;\\alpha)} &= {\\mathcal F}[P_{Y;j}^{(k;\\alpha)}].\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{H}}_{X;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathcal{H}}[P_{X;j}^{(k;\\alpha)}],\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{H}}_{Y;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathcal{H}}[P_{Y;j}^{(k;\\alpha)}];\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u210b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{C}}_{X;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\ {\\mathcal{C}}[P_{X;j}^{(k;\\alpha)}],\" display=\"inline\"><mrow><mrow><mi/><mo rspace=\"7.5pt\">=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{C}}_{Y;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathcal{C}}[P_{Y;j}^{(k;\\alpha)}];\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{F}}_{X;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathcal{F}}[P_{X;j}^{(k;\\alpha)}],\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>X</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathcal{F}}_{Y;j}^{(k;\\alpha)}\" display=\"inline\"><msubsup><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m4\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\mathcal{F}}[P_{Y;j}^{(k;\\alpha)}].\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>P</mi><mrow><mi>Y</mi><mo>;</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>;</mo><mi>\u03b1</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nsubject to         \n\n", "itemtype": "equation", "pos": 58455, "prevtext": "\n\nWe perform Exploratory Data Analysis (EDA)\non the Information Theory quantifiers \nlooking for simple descriptions of the data. \nApart from simple descriptive univariate measures, we use the Pearson correlation to measure the association between features. \nThis analysis was performed using the {\\tt R} language and \nplatform version~3.2.1 (\\url{http:\\\\www.R-project.org}).  \n\nFigure~\\ref{Fig:HistEntropy} shows a scatterplot of the Entropy for both the genuine and skilled forgery signatures.\nThe $5000$ points correspond to $25$ genuine signatures (in blue) and $25$ forgery signatures \n(in red) for each of the $100$ subjects. \nBoth types of signatures show similar association (Correlation): \n${\\rm Corr}({\\mathcal H}_{X;j}^{(k;G)}, {\\mathcal H}_{Y;j}^{(k;G)}) = 0.9665$ and \n${\\rm Corr}({\\mathcal H}_{X;j}^{(k;F)}, {\\mathcal H}_{Y;j}^{(k;F)}) = 0.9770$.\n\nThe \nentropies of both types of signatures are overlapped and scattered elliptically. \nHowever, the bivariate mean and dispersion values differ. \n\nEntropies are less dispersed in the genuine than in the skilled forgery signatures,  \na signal of the separability between them. \nMarginal density plots show the distribution of entropy for each coordinate of both types of signatures. \nThese plots, however limited due to its marginal nature, reveal several modes, and suggest both wide and narrow \nstructures in the data. \n\n\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{HistEntropy}\n\\caption{Scatter plot with marginal  kernel density estimates of entropy quantifiers in both trajectory coordinates \ntime series \n${\\mathbf X}$ and ${\\mathbf Y}$.\nGenuine (blue) and skilled forgery signatures (red points), 100 subjects. \nMarginal kernel densities depict the distribution of entropy quantifiers along both axes.\n}\\label{Fig:HistEntropy}\n\\end{figure}\n\nFigure~\\ref{Fig:ContourEntropy} shows the contour plots of bivariate kernel density estimates for the \nentropy in genuine and forgery signatures. \nA number of features are immediately noticeable. \nThe dispersion in the former group is much smaller than in the latter (less than $0.4$). \nThe kernel density estimates reveal skewness and a mild multimodality in the joint distribution of the data.\nThere are also quite many points that are far from these curves and cluster centers. \nThese points correspond to abnormal local estimates obtained in heterogeneous blocks, possibly induced by the presence of clusters. \nThe modes in genuine signatures are smaller than in forgery signatures, and this may be used as discriminatory measure. \nSimilar results are obtained for the Complexity and the Fisher information; \nthese are \nreported in the Supplementary Information, see Figs.~S1 to~S4, respectively.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.95\\linewidth]{ContourEntropy}\n\\caption{Contour plot superimposed on the scatterplot of entropy quantifiers for genuine (right panel) \nand skilled forgery signatures (left panel)}\n\\label{Fig:ContourEntropy}\n\\end{figure}\n\n\\section*{Signatures classification}\n\\label{Sec-Classification}\n\n\nAs pointed out by Boul\\'etreal {\\it et al.\\/}\\cite{Bouletreau1998}, a signature  is characterized\nby two aspects: \n{\\it a)\\/}~a conscious one associated  to the pattern signature; and\n{\\it b)\\/}~an unconscious one which leads spontaneous movements constituting the drawing.\nThese two factors produce high variability, being the amount of signature variability \nstrongly writer-dependent.\nIn fact, the signature {\\it variability\\/} or, conversely, the signature {\\it stability\\/} can be considered \nan important indicator for writer characterization\\cite{Houmani2014}.\nHoumani and Garcia-Salicetti\\cite{Houmani2014} argue that signature stability is required in genuine signatures \nin order to characterize a writer: the less stable a signature is, the more likely it is that forgery \nwill be dangerously close to genuine signatures for any classifier.\nAlso, complex enough signatures are required in order to guarantee a certain level of security, in the sense \nthat the more complex a signature is, the more difficult it will be to forge it\\cite{Houmani2014}.\n\nBoul\\'etreal and collaborators\\cite{Bouletreau1998,Vincent2000} propose a signature complexity measure related \nto signature legibility and based on fractal dimension.\nThey classify writer styles into: highly cursive, very legible, separated,  badly formed and small \nwritings, using only genuine signatures.\nUnfortunately, such resulting categories were not confronted to classifiers for performance analysis.\n\nWe classify the genuine signatures based on causal Information Theory quantifiers: \nNormalized Permutation Shannon Entropy, \nPermutation Statistical Complexity and \nPermutation Fisher Information Measure of both $\\mathbf X$ and $\\mathbf Y$ trajectories\non each of the one hundred writers in the MCYT data base, and their $25$ original signatures.\nThe mean and standard deviation values were clustered using the neighbor-joining method and an automatic Hierarchical Clustering  \nwith the Euclidean distance-based dissimilarity matrix.\nEach feature was treated independently, and the results are shown as circular dendrograms.\nFigure~\\ref{Fig:DendroEntropy} shows the results of clustering the Entropy.\nWe distinguish three classes of genuine signatures denoted by H1, H2, and H3. \n\n\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.9\\linewidth, angle=0]{DendogramEntropy}\n\\caption{\nNeighbor-joining, rooted, circular dendrogram clustering of genuine signatures by Entropy: \nH1, H2, and H3, in red, blue, and green, respectively. \n}\\label{Fig:DendroEntropy}\n\\end{figure}\n\nThe H1 group is the first group to form, i.e., the one comprised of the most similar individuals.\nIt is formed below the $25\\%$ level, and it is composed by two subgroups: H1A and H1B. \nThe H1A group is formed exclusively by oversimplified signatures made by simple loops without identifiable letters. \nIt encompasses the following subjects: 1, 16, 17, 22, 23, 27, 29, 37, 83. \nThe same group is formed when the other features are used. \nThe H1B group is comprised of the following subjects: 2, 5, 8, 10, 19, 21, 24, 28, 32, 35, 36, 39, 43, 48, \n49, 51, 55, 58, 59, 64, 69, 70, 74, 77, 89. \nAlthough these are simplified signatures, traces of letters and/or more complex curves appear and differentiate them \nfrom the members of the H1A group.\n\nThe H2 group is formed approximately at the $32\\%$ level, and, again, it is comprised of two distinct groups: \nH2A and H2B. \nThe subjects that make the H2A group are: 4, 7, 12, 15, 18, 20, 30, 31, 34, 38, 40, 41, 42, 52, 57, 60, 62, \n66, 67, 68, 71, 73, 75, 79, 80, 81, 86, 87, 91, 96, 100. \nIt is composed by signatures with traces that resemble letters, but that are not perfectly identifiable, \nand that include circling traces of large or moderate size. \nSignatures in this group are kind of framed by large loops.\nThe H2B group is similar to the previous one, i.e., it is formed by signatures with large and medium size \ncircling traces, but with more identifiable letters than in the previous groups. \nNames and surnames are more readable in this group than in previous ones. \nIt is formed by the following signatures: 6, 9, 13, 25, 33, 45, 50, 63, 65, 76, 78, 82, 84, 85, 88, 92, 94, 95, 97, 99. \n\nThe H3 group is formed at, approximately, the $43\\%$ level by the fusion of two other highly unbalanced subgroups: \none, H3A, with only two subjects (44, 46) and the other, H3B, with thirteen subjects (3, 11, 14, 26, 47, 53, 54, 56, 61, \n72, 90, 93, 98). \nThese two clusters form at approximately the same level. \nThe former is composed of calligraphic signatures where vertical traces predominate over horizontal ones. \nThe latter is composed of highly cursive signatures, where separation between the surname and the family name \npredominates. \n\nThe same results of clustering was obtained with the Manhattan (norm ${\\cal L}_1$) and Maximum distances ($\\mathcal L_\\infty$ norm), showing that Entropy is an expressive and stable quantifier. \nSimilar analyses were carried with the Permutation Statistical Complexity and Permutation Fisher Information \n(presented in figures Figs.~S5 and~S6 in the Supplementary Information).\nComplexity produces the same clusters identified by Entropy, so it adds no new information.\nThe Fisher information measure forms the same H1A group that was identified by the Entropy, but with less \ncohesion, at about $15\\%$. \nIn other words, these nine subjects are more similar locally than globally. \nAs with Entropy, three main groups form at similar levels. \nThe members of these clusters are slight variations of those identified using Entropy, with very similar structure. \n\nTable~\\ref{tab:tab-Measure-subject} presents \nthe mean and standard deviation of the three quantifiers over the $25$ genuine and $25$ skilled forgery signatures \n(${\\mathbf X}$ and ${\\mathbf Y}$ time series) for each of the typical subjects, split in the three aforementioned types H1, H2 and~H3.\nThere are interesting tendencies in these data.\nGenuine signatures present quantifiers values lower than those corresponding to forgery signatures, \nand the latter also exhibit larger standard deviation.\nThis could be explained by the imitative character of these signatures, however it deserves closer  studies.\n\n\\begin{sidewaystable}[hbt] \\centering \n\\centering\n\\begin{tabular}{@{\\extracolsep{5pt}} ccccccc|cc|cc} \n\\\\ \\cmidrule(r){6-11}\n      &             &                    &  &  & \\multicolumn{2}{c|}{Entropy}&\\multicolumn{2}{c|}{Complexity}&\\multicolumn{2}{c}{Fisher Information} \\\\ \\midrule\n\n$Type$ & $Sub-Type$ & Subject & Coordinate & Class & Mean & S.D. & Mean & S.D. & Mean & S.D. \\\\ \n\\midrule \n\\multirow{8}{*}{H1} & \\multirow{4}{*}{H1A} & \\multirow{4}{*}{22} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.1568$ & $0.0052$ & $0.1490$ & $0.0039$ & $0.4688$ & $0.0070$  \\\\ \n\t\t    &  \t\t\t   &\t\t\t &                                &  G & $0.1519$ & $0.0019$ & $0.1457$ & $0.0015$ & $0.4766$ & $0.0035$  \\\\ \\cmidrule(r){4-11}\n\t\t    &                      &  \t\t\t & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.1595$ & $0.0071$ & $0.1511$ & $0.0052$ & $0.4665$ & $0.0097$  \\\\ \n\t\t    &  \t\t\t   &\t\t\t &                                &  G & $0.1512$ & $0.0042$ & $0.1447$ & $0.0037$ & $0.4734$ & $0.0046$  \\\\ \n\\cmidrule(r){3-11} \n& \\multirow{4}{*}{H1B} &\\multirow{4}{*}{39} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2212$ & $0.0384$ & $0.1941$ & $0.0257$ & $0.4286$ & $0.0147$  \\\\ \n&                      &\t\t    &\t\t\t             &  G & $0.1749$ & $0.0037$ & $0.1620$ & $0.0028$ & $0.4497$ & $0.0029$  \\\\ \\cmidrule(r){4-11}\n&\t\t       &\t\t    & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2270$ & $0.0449$ & $0.1980$ & $0.0296$ & $0.4277$ & $0.0153$  \\\\ \n&\t\t       &\t\t    &\t\t\t             &  G & $0.1776$ & $0.0043$ & $0.1644$ & $0.0031$ & $0.4491$ & $0.0035$  \\\\ \n\\midrule \n\\multirow{8}{*}{H2} & \\multirow{4}{*}{H2A} &\\multirow{4}{*}{60} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2482$ & $0.0593$ & $0.2112$ & $0.0365$ & $0.4212$ & $0.0107$  \\\\ \n\t\t   & & &\t\t\t&  G & $0.2010$ & $0.0056$ & $0.1803$ & $0.0040$ & $0.4331$ & $0.0031$  \\\\ \\cmidrule(r){4-11}\n\t\t   & & & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2442$ & $0.0544$ & $0.2090$ & $0.0339$ & $0.4219$ & $0.0134$  \\\\ \n\t\t   & & &\t\t\t&  G & $0.2079$ & $0.0043$ & $0.1861$ & $0.0030$ & $0.4315$ & $0.0024$  \\\\ \n\\cmidrule(r){3-11}\n& \\multirow{4}{*}{H2B}&\\multirow{4}{*}{6} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.2621$ & $0.0584$ & $0.2194$ & $0.0334$ & $0.4143$ & $0.0137$ \\\\ \n& &\t\t   &\t\t\t&  G & $0.2337$ & $0.0149$ & $0.2032$ & $0.0095$ & $0.4205$ & $0.0066$ \\\\ \\cmidrule(r){4-11}\n& &\t\t   & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.2648$ & $0.0538$ & $0.2218$ & $0.0304$ & $0.4136$ & $0.0134$ \\\\ \n& &\t\t   &\t\t\t&  G & $0.2314$ & $0.0102$ & $0.2018$ & $0.0067$ & $0.4211$ & $0.0050$ \\\\ \n \\midrule\n\\multirow{8}{*}{H3} &  \\multirow{4}{*}{H3A} &\\multirow{4}{*}{98} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.3236$ & $0.0646$ & $0.2529$ & $0.0320$ & $0.3937$ & $0.0208$ \\\\ \n\t\t & &  &\t\t\t&  G & $0.2707$ & $0.0101$ & $0.2268$ & $0.0064$ & $0.4106$ & $0.0032$ \\\\ \\cmidrule(r){4-11}\n\t\t &  & & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.3204$ & $0.0794$ & $0.2497$ & $0.0388$ & $0.3970$ & $0.0208$ \\\\ \n\t\t &  & &\t\t\t&  G & $0.2664$ & $0.0124$ & $0.2243$ & $0.0077$ & $0.4105$ & $0.0034$ \\\\ \n\\cmidrule(r){3-11} \n& \\multirow{4}{*}{H3B}&\\multirow{4}{*}{46} & \\multirow{2}{*}{${\\mathbf X}$} &  F & $0.3514$ & $0.0641$ & $0.2691$ & $0.0294$ & $0.3940$ & $0.0156$  \\\\ \n& &\t\t   &\t\t\t&  G & $0.3480$ & $0.0282$ & $0.2720$ & $0.0156$ & $0.4019$ & $0.0047$  \\\\ \\cmidrule(r){4-11}\n& &\t\t   & \\multirow{2}{*}{${\\mathbf Y}$} &  F & $0.3419$ & $0.0681$ & $0.2639$ & $0.0323$ & $0.3940$ & $0.0163$  \\\\ \n& &\t\t   &\t\t\t&  G & $0.3270$ & $0.0263$ & $0.2599$ & $0.0148$ & $0.4008$ & $0.0052$  \\\\ \n\\bottomrule\n\\end{tabular} \n \\caption{Sample mean and standard deviation (S.D.) of the time series quantifiers for the 25 genuine (G) \nand 25 \nskilled\nforged (F) signatures, for each of the typical subjects: H1A, H1B, H2A, H2B, H3A, and H3B (same order as in Fig.~\\ref{fig:MCYT-firmas}).} \n\\label{tab:tab-Measure-subject} \n\\end{sidewaystable} \n\nThe classification into subclasses of genuine signatures was also carried by the parallelepiped \nalgorithm\\cite{Richards1999}, arguably the simplest model-free classification procedure.\nEntropy leads to clusters with nice interpretability. \nFigure~\\ref{Fig:EntropyClasificationBoxesEntropy} shows the regions that define the three classes \nidentified by the dendrogram based on Entropy presented in Fig~\\ref{Fig:DendroEntropy}. \nAll subclasses are well separated by disjoint boxes, with the only exception of H1B and H2A that  \noverlap slightly but without compromising the discrimination. \nThe classes are preserved using this classification superimposed with Complexity and Fisher Information features; see Figs.~S7 and~S8 in the Supplementary Information.\n\n\\begin{figure}[hbt]\n\\centering\n\\includegraphics[width=.9\\linewidth, angle=0]{ClassificationWithSignatureTrue}\n\\caption{Classification by the rule of the parallelepiped of genuine signatures using Entropy\n(one signature example from each of the three groups is shown).\nEach subject is identified by its ID.\n}\\label{Fig:EntropyClasificationBoxesEntropy}\n\\end{figure}\n\n\\section*{Online signature verification}\n\\label{Sec-Verification}\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe problem we have at hand consists in identifying suspicious signatures, given that we only have examples from genuine signatures.\nThis is due to the fact that, in practice, it is too expensive, too hard or even impossible to obtain a significant number of good quality forgery signatures for every possible individual in the data base.\nThis, thus, configures a One-Class classification problem.\nAmong the many ways of tackling such problems, Support Vector Machines (SVMs) are suitable for solving machine learning problems even in large dimensional feature spaces\\cite{Campbell2011,Boser1992,Vapnik1995}. \n\nSVMs were introduced by Vapnik and co-workers\\cite{Boser1992,Vapnik1998},\nand extended by a number of other researchers.\nTheir remarkably robust performance with respect to sparse and noisy data makes them the choice in several applications.\nA SVM is primarily a method that performs classification tasks by\nconstructing hyperplanes in a multidimensional space that separates cases of different class labels. \nSVMs perform both regression and classification tasks and can handle multiple continuous and categorical \nvariables.\nTo construct an optimal hyperplane, a SVM employs an iterative training algorithm, which is used\nto minimize an error function.\n\nOne-Class Support Vector Machines (OC-SVMs) are a natural extension of SVMs\\cite{Scholkopf2001, Scholkopf2002}. \nThe solution consists in estimating a distribution that encompasses most of the observations, and then labeling as ``suspicious'' those that lie far from it with respect to a suitable metric.\nAn OC-SVM solution is built estimating a probability distribution function which makes most of the observed data more likely than the rest, and a decision rule that separates these observation by the largest possible margin.\nThe computational complexity of the learning phase is intensive because the training of an OC-SVM involves a quadratic programming problem\\cite{Boser1992}, but once the decision function is determined, it can be used to predict the class label of new test data effortlessly.\n\nIn our case, the observations are six-dimensional vectors: Entropy, Complexity and Fisher Information in each of the two directions, horizontal and vertical, and\nwe train the OC-SVM with genuine signatures.\nLet ${\\cal Z} = \\{ z_1, z_2, \\dots , z_N \\}$ be the six-dimensional training examples of genuine signatures. \nLet $\\Phi \\colon {\\cal Z} \\rightarrow {\\cal G}$ be a kernel map which transforms the\ntraining examples to another space. \nThen, to separate the data set from the origin, one needs to solve the following quadratic programming problem:\n\n", "index": 29, "text": "\\begin{equation}\n\\label{eq:oc-svm-1}\n         \\min_{{\\mathbf{w}\\in {\\cal G},~\\xi_i , b \\in \\mathbb{R}}} \\qquad \n \\left\\{\\frac{1}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{\\nu N} \\sum_{i=1}^N \\xi_i - b\\right\\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\min_{{\\mathbf{w}\\in{\\cal G},~{}\\xi_{i},b\\in\\mathbb{R}}}\\qquad\\left\\{\\frac{1}{%&#10;2}\\|\\mathbf{w}\\|^{2}+\\frac{1}{\\nu N}\\sum_{i=1}^{N}\\xi_{i}-b\\right\\}\" display=\"block\"><mrow><munder><mi>min</mi><mrow><mrow><mi>\ud835\udc30</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo rspace=\"5.8pt\">,</mo><msub><mi>\u03be</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo><mrow><mi>b</mi><mo>\u2208</mo><mi>\u211d</mi></mrow></mrow></munder><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mo>{</mo><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mi>\ud835\udc30</mi><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mrow><mi>\u03bd</mi><mo>\u2062</mo><mi>N</mi></mrow></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>\u03be</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>-</mo><mi>b</mi></mrow><mo>}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nwhere $\\xi_i$ are nonzero slack variables which allow the procedure to incur in errors.\nThe parameter $\\nu$ characterizes the solution as\n{\\it a)\\/} it sets an upper bound on the fraction of outliers (training examples regarded out-of-class) and,\n{\\it b)\\/} it is a lower bound on the number of training examples used as Support Vectors.\nWe used $\\nu=0.1$ in our proposal.\n\nUsing Lagrange techniques and a kernel function $K(z,z_i) = \\Phi(z)^T \\Phi(z_i)$, for the dot-product\ncalculations, the decision function $f(z)$ becomes:\n\n", "itemtype": "equation", "pos": 58688, "prevtext": "\nsubject to         \n\n", "index": 31, "text": "\\begin{align}\n& \\nu \\in (0,1], \\   \\xi_i \\ge 0, \\  \\forall i=1,\\dots, N \\label{eq:oc-svm-2}, \\text{ and}\\\\\n& (\\mathbf{w} \\cdot \\Phi(z_i) ) \\geq b- \\xi_i, \\  \\forall i=1,\\dots, N   ,     \n\\label{eq:oc-svm-3}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\nu\\in(0,1],\\ \\xi_{i}\\geq 0,\\ \\forall i=1,\\dots,N,\\text{ and}\" display=\"inline\"><mrow><mrow><mi>\u03bd</mi><mo>\u2208</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><msub><mi>\u03be</mi><mi>i</mi></msub><mo>\u2265</mo><mn>0</mn></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi><mo>,</mo><mtext>\u00a0and</mtext></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\mathbf{w}\\cdot\\Phi(z_{i}))\\geq b-\\xi_{i},\\ \\forall i=1,\\dots,N,\" display=\"inline\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc30</mi><mo>\u22c5</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mrow><mi>b</mi><mo>-</mo><msub><mi>\u03be</mi><mi>i</mi></msub></mrow></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>i</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>N</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nThis method thus creates a hyperplane characterized by  $\\mathbf{w}$ and  $b$ which has maximal distance \nfrom the origin in the feature space $\\cal G$ and separates all the data points from the origin.\nHere $\\alpha_i$ are the Lagrange multipliers; every $\\alpha_i >0 $ is {\\it weighted in\\/} the decision function and thus ``supports\" the machine; hence the name Support Vector Machine.\nSince SVMs are considered to be sparse, there will be relatively few Lagrange multipliers with a nonzero value.\n\nOur choice for the kernel is the Gaussian Radial Base function:\n\n", "itemtype": "equation", "pos": 59435, "prevtext": "\nwhere $\\xi_i$ are nonzero slack variables which allow the procedure to incur in errors.\nThe parameter $\\nu$ characterizes the solution as\n{\\it a)\\/} it sets an upper bound on the fraction of outliers (training examples regarded out-of-class) and,\n{\\it b)\\/} it is a lower bound on the number of training examples used as Support Vectors.\nWe used $\\nu=0.1$ in our proposal.\n\nUsing Lagrange techniques and a kernel function $K(z,z_i) = \\Phi(z)^T \\Phi(z_i)$, for the dot-product\ncalculations, the decision function $f(z)$ becomes:\n\n", "index": 33, "text": "\\begin{equation}\n\\label{eq:oc-svm-primal-3}\nf(z) = \\text{sign}\\left\\{(\\mathbf{w}\\cdot \\Phi(z)) - b \\right\\} = \n       \\text{sign}\\left\\{ \\sum_{i=1}^{N}~\\alpha_i ~K(z,z_i) - b \\right\\}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"f(z)=\\text{sign}\\left\\{(\\mathbf{w}\\cdot\\Phi(z))-b\\right\\}=\\text{sign}\\left\\{%&#10;\\sum_{i=1}^{N}~{}\\alpha_{i}~{}K(z,z_{i})-b\\right\\}.\" display=\"block\"><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\ud835\udc30</mi><mo>\u22c5</mo><mi mathvariant=\"normal\">\u03a6</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>b</mi></mrow><mo>}</mo></mrow></mrow><mo>=</mo><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo>{</mo><mrow><mrow><mpadded width=\"+3.3pt\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mpadded><mrow><mpadded width=\"+3.3pt\"><msub><mi>\u03b1</mi><mi>i</mi></msub></mpadded><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mi>b</mi></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06925.tex", "nexttext": "\nwhere $\\sigma \\in {\\mathbb{R}}$ is a kernel parameter  and ${\\|z_i - z_j\\|^2}$ is the\ndissimilarity measure; we used Euclidean distance.\n\nThe parameter $\\sigma^2 = {10}$ was selected by 5-fold-cross validation, that its, the dataset is divided \ninto five disjoint subsets, and the method is repeated five times. \nEach time, one of the subsets is used as the test set and the other four subsets are put together to form the training set. \nThen the average error across all trials is computed. \nEvery observation belongs to a test set exactly once, and belongs to a training set four times.\nAccuracy (ACC), Area Under the ROC Curve (AUC) and Equal Error Rate (EER) are used as performance measures~\\cite{Kohavi1998}.\n\nIn the context of signature verification one-class classification problems, a false positive occurs when a genuine signature is erroneously classified as being atypical. \nThe probability of false positive misclassification is the false positive rate, which is controlled by the parameters $\\nu$ in the aforementioned OC-SVM formulation.\nThe parameter $\\nu$ can be fixed a {\\it priori} and it corresponds to the percentage of observations of the typical data which will be assigned as the Type~I Error. \n\nWe used the LIBSVM (version 2.0) tool, linked with the R software, that supports vector classification and regression, including OC-SVM.\\cite{Chang2001}\nWe used the standard parameters of the algorithm.\n\nIn order to assess the consistency of our procedure, and to promote the comparison with other methods reported in the literature, we evaluate the performance of the proposed verification system for different training samples: \nrandom samples of size $n$ ($n=5, 10, 14, 18, 22$)  of\ngenuine signatures were selected for each user.\nTable~\\ref{tab:ntrain} presents the average value of all performance metrics using $\\sigma^2=10$.\nACC suggests that the larger the training sample, the better the performance is.\nAUC presents a similar tendency, and its average is larger than $0.88$, indicating that our verification system produces an excellent classification. \n\n\n\\begin{table}[hbt] \n\\centering\n\\begin{tabular}{rccc} \n\\toprule\n$n$ & ACC ($\\uparrow$) & AUC ($\\uparrow$) &  EER ($\\downarrow$)(\\%)\\\\ \n\\midrule\n5  & 0.6940  & 0.8816 &  0.1890  \\\\ \n10 & 0.7678  & 0.8940 &  0.1711 \\\\ \n14 & 0.8144  & 0.8975 &  0.1634 \\\\ \n18 & 0.8250  & 0.8866 &  0.1731 \\\\ \n22 & 0.8389  & 0.8909 &  0.1632 \\\\ \n\\bottomrule\n\\end{tabular} \n\\vspace{0.25in}\n\\caption{Performance of the system trained with varying number $n$\nof samples of genuine signatures.\n$\\uparrow$ and $\\downarrow$ denote measures of quality (the higher the better) and of error (the smaller the better), respectively.\n} \n\\label{tab:ntrain}   \n\\end{table} \n\nAs mentioned in the introduction, the two methodologies with best results are those based on Dynamic Time Warping (DTW) and Hidden Markov Models (HMM).\nIn the following we compare our proposal with these two recent state-of-the-art methods using the ERR(\\%) over the same data base:\n\\begin{itemize}\n\\item Fierrez-Aguilar {\\it et al.\\/}\\cite{Fierrez2005},  ERR(\\%) = 2.12 (five training signatures; Global (Parzen WC)\nand local (HMM) experts function);\n\\item Fierrez-Aguilar {\\it et al.\\/}\\cite{Fierrez2007}, ERR(\\%) = 0.74 (ten training signatures; HMM based algorithm);\n\\item Pascual-Gaspar  {\\it et al.\\/}\\cite{Pascual2009}, ERR(\\%) = 1.23 (five training signatures; DTW-bases algorithm,\nresult with scenario-dependent optimal features.\n\\end{itemize}\nThe results of our proposal using five (ten, respectively) training samples, are ERR(\\%) = 0.19 (0.17, respectively).\nClearly, our system provides better performance using similar number of training signatures (see Table~\\ref{tab:ntrain} for more details).\n\nIn the following we analyze the performance of the proposed procedure applied selectively to the pre-classified samples.\nTable~\\ref{tab:class} presents the performance of the system when applied to genuine pre-classified signatures. \nFor all classes we observe that the larger the training sample, also the larger the average ACC is. \nThe best average AUC are observed for the class H2, followed by H1 and H3.\nThis indicate that H2 signatures are easily identifiable. \nNote that the mean values of ERR(\\%) for H2 are smaller than H1 and H3. \n\nThe ERR(\\%) values in H3 indicate that identifying forgeries in this class is hard.\n\n\n\n\n\n\n\\begin{table}[hbt]  \n\\centering\n\\begin{tabular}{crccc} \\toprule\n Class & $n$ & ACC ($\\uparrow$) & AUC ($\\uparrow$) & EER(\\%) ($\\downarrow$)  \\\\ \n \\midrule \n\\multirow{5}{*}{H1} \n & 5 & 0.6758 & 0.8692 &  0.1976  \\\\ \n &10 & 0.7566 & 0.8828 &  0.1812  \\\\ \n &14 & 0.8039 & 0.8857 &  0.1717  \\\\ \n &18 & 0.8217 & 0.8894 &  0.1662  \\\\ \n &22 & 0.8277 & 0.8788 &  0.1631  \\\\ \n\\midrule\n\\multirow{5}{*}{H2} \n & 5 & 0.7059 & 0.8945 & 0.1784  \\\\ \n &10 & 0.7819 & 0.9079 & 0.1548  \\\\ \n &14 & 0.8284 & 0.9096 & 0.1509  \\\\ \n &18 & 0.8327 & 0.8900 & 0.1734  \\\\ \n &22 & 0.8515 & 0.8996 & 0.1608  \\\\ \n  \\midrule\n\\multirow{5}{*}{H3} \n & 5 & 0.6948 & 0.8653 &  0.2053 \\\\ \n &10 & 0.7450 & 0.8720 &  0.2036  \\\\ \n &14 & 0.7907 & 0.8832 &  0.1874 \\\\ \n &18 & 0.8062 & 0.8686 &  0.1874  \\\\ \n &22 & 0.8214 & 0.8889 &  0.1716  \\\\ \n\\bottomrule\n\\end{tabular} \n\\caption{Performance of the classification of pre-classified samples varying the number $n$ of \nsamples of genuine signatures used for training; same coding as in Tab.~\\ref{tab:ntrain}.} \n  \\label{tab:class}\n\\end{table} \n\n\n \n\\section*{Conclusions}\n\\label{Sec:Conclusions}\n\nWe proposed a procedure for identifying skilled forgery online handwritten signatures using time causal Information  Theory quantifiers and One-Class Support Vector Machines.\nThis is a competitive proposal from the computational viewpoint as it uses only the signatures coordinates, and it produces better results than state-of-the-art techniques.\nThe technique also produces meaningful classification of the input data, as it is able to separate different types of signatures.\nTo the best of our knowledge, this is the first time Information Theory quantifiers have been used for this problem.\n\nThe central contribution is the use of the Bandt and Pompe (BP) PDF symbolization  which is invariant to a number of transformations of the input data.\nIn fact, the original time series are pre-processed only to facilitate the signal sampling, and this scaling has no effect on the BP PDFs.\nThis representation, which is sensitive to the time causality, is able to capture essential dynamical characteristics of the signatures that lead to excellent discrimination between skilled forgery and genuine online handwritten signatures, despite the high variability the data possess.\nAdditionally, obtaining the BP PDFs is computationally simple and efficient.\n\nOnly six Information Theory features are required for the classification, three from each horizontal and vertical direction:\nShannon Entropy, Statistical Complexity and Fisher Information.\nThis contrasts many state-of-the-art works that require features in high-dimensional spaces, e.g. forty or even more.\nAs said, our proposal does not require highly specialized hardware able to capture signature speed, pressure, orientation etc.\n\nThe classification was performed by a One-Class Support Vector Machine trained with genuine signatures.\nThe learned rule is consistent with respect to the number of training samples, and with as few as five examples it surpasses the performance of recent successful techniques.\nWe assessed the performance of our proposal using the same data base employed in the current literature, with also the same measures of quality and error.\n\n\\section*{Acknowledgments}\n\nThe authors are grateful to CONICET, CNPq and FACEPE for partial funding of this research.\nThe Biometrics Research Lab (ATVS), Universidad Aut\\'onoma de Madrid, provided the MCYT-100 signature corpus employed in this work.\n\n\\section*{Authors Contributions}\n\nOAR, RO and ACF conceived and designed the research. \nOAR performed the numerical data analysis. \nRO and ACF performed the statistical analysis. \nOAR and RO prepared figures. \nOAR and ACF wrote the manuscript. \nAll authors reviewed and approved the manuscript\n\n\\section*{Competing interests}\n\nThe authors declare no competing financial interests.\n\n\\begin{thebibliography}{10}\n\n\\bibitem{OrtegaGarcia2004}\nOrtega-Garcia J, Bigun J, Reynolds D, Gonzalez-Rodriguez J.\n\\newblock Authentication gets personal with biometrics.\n\\newblock IEEE Signal Processing Magazine. 2004 Mar;21(2):50--62.\n\n\\bibitem{Plamondon1989}\nPlamondon R, Lorette G.\n\\newblock Automatic signature verification and writer identification: the state\n  of the art.\n\\newblock Pattern Recognition. 1989;22(2):107--131.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/0031320389900599}.\n\n\\bibitem{Leclerc1994}\nLeclerc F, Plamondon R.\n\\newblock Automatic signature verification: The state of the art: 1989--1993.\n\\newblock International Journal of Pattern Recognition and Artificial\n  Intelligence. 1994;8(3):643--660.\n\n\\bibitem{Gupta1997}\nl~Gupta G, McCabe A. A review of dynamic handwritten signature verification.\n\\newblock Department of Computer Science, James Cook University, Australia;\n  1997.\n\\newblock Available from:\n  \\url{http://www.cs.jcu.edu.au/~alan/Work/HSV-Lit_rev.html}.\n\n\\bibitem{Impedovo2008}\nImpedovo D, Pirlo G.\n\\newblock Automatic Signature Verification: The State of the Art.\n\\newblock IEEE Transactions on Systems, Man, and Cybernetics, Part C:\n  Applications and Reviews. 2008 Sept;38(5):609--635.\n\n\\bibitem{Ahmed2013}\nEl-Henawy IM, Rashad MZ, Nomir O, Ahmed K.\n\\newblock Online signature verification: state of the art.\n\\newblock International Journal of Computers and Technology. 2013;4:664--678.\n\n\\bibitem{Hilton1992}\nHilton O.\n\\newblock Signatures, review and a new view.\n\\newblock Journal of Forensic Sciences. 1992;37:125--129.\n\n\\bibitem{ballistic}\nWikipedia. Ballistic movement --- Wikipedia{,} The Free Encyclopedia; 2015.\n\\newblock [Online; accessed 18-January-2016].\n\\newblock Available from:\n  \\url{\\url{https://en.wikipedia.org/w/index.php?title=Ballistic_movement&oldid=681747532}}.\n\n\\bibitem{DerGon1965}\nDenier van~der Gon JJ, Thuring JP.\n\\newblock The guiding of human writing movement.\n\\newblock Kybernetik. 1965;2:145--148.\n\n\\bibitem{Nalwa1997}\nNalwa VS.\n\\newblock Automatic on-line signature verification.\n\\newblock Proceedings of the IEEE. 1997 Feb;85(2):215--239.\n\n\\bibitem{Goldberger1990}\nLongstaff M, Heath R.\n\\newblock Chaos and fractals in human physiology.\n\\newblock Scientific American. 1990;262:42--49.\n\n\\bibitem{West2013}\nWest BJ.\n\\newblock Fractal Physiology and Chaos in Medicine.\n\\newblock 2nd ed. No.~16 in Studies of Nonlinear Phenomena in Life Science.\n  Singapore: World Scientific Publishing; 2013.\n\n\\bibitem{Longstaff1999}\nLongstaff M, Heath R.\n\\newblock A nonlinear analysis of temporal characteristic of handwriting.\n\\newblock Human Movement Science. 1999;18:485--524`.\n\n\\bibitem{Rosso2007}\nRosso OA, Larrondo HA, Mart\\'{\\i}n MT, Plastino A, Fuentes MA.\n\\newblock Distinguishing noise from chaos.\n\\newblock Physical Review Letters. 2007;99:154102.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevLett.99.154102}.\n\n\\bibitem{Rosso2015}\nRosso OA, Olivares F, Plastino A.\n\\newblock Noise versus chaos in a causal Fisher-{Shannon} plane.\n\\newblock Papers in Physics. 2015 Apr;7:070006.\n\n\\bibitem{MCYT2003}\nOrtega-Garcia J, Fierrez-Aguilar J, Simon D, Gonzalez J, Faundez-Zanuy M,\n  Espinosa V, et~al.\n\\newblock MCYT baseline corpus: a bimodal biometric database.\n\\newblock IEE Proceedings Vision, Image and Signal Processing. 2003\n  Dec;150(6):395--401.\n\n\\bibitem{Baron1989}\nBaron R, Plamondon R.\n\\newblock Acceleration measurement with an instrumented pen for signature\n  verification and handwriting analysis.\n\\newblock IEEE Transactions on Instrumentation and Measurement. 1989\n  Dec;38(6):1132--1138.\n\n\\bibitem{Salicetti2009}\nGarcia-Salicetti S, Houmani B N abd Ly-Van, Dorizzi B, Alonso-Fernandez F,\n  Fierrez J, Ortega-Garcia J, et~al.\n\\newblock Online handwritten signature verification.\n\\newblock In: Petrovska-Delacr\\'etaz D, Chollet G, Dorizzi B, editors. Guide to\n  Biometric Reference Systems and Performance Evaluation. London:\n  Springer-Verlag; 2009. p. 125--165.\n\n\\bibitem{Brissaud2005}\nBrissaud JB.\n\\newblock The meanings of entropy.\n\\newblock Entropy. 2005;7(1):68--96.\n\\newblock Available from: \\url{http://www.mdpi.com/1099-4300/7/1/68}.\n\n\\bibitem{Shannon1948}\nShannon CE.\n\\newblock A Mathematical Theory of Communication.\n\\newblock The Bell System Technical Journal. 1948;27:379--423.\n\n\\bibitem{Shannon1949}\nShannon C, Weaver W.\n\\newblock The Mathematical Theory of Communication.\n\\newblock University of Illinois Press; 1949.\n\n\\bibitem{Fisher1922}\nFisher RA.\n\\newblock On the mathematical foundations of theoretical statistics.\n\\newblock Philosophical Transactions of the Royal Society of London, A.\n  1922;222:309--368.\n\n\\bibitem{Frieden2004}\nFrieden RB.\n\\newblock Science from {F}isher information: A Unification.\n\\newblock Cambridge, UK: Cambridge University Press; 2004.\n\n\\bibitem{Zografos1986}\nZografos K, Ferentinos K, Papaioannou T.\n\\newblock Discrete approximations to the {C}sisz\\'ar, {R}enyi, and {F}isher\n  measures of information.\n\\newblock Canadian Journal of Statistics. 1986;14(4):355--366.\n\\newblock Available from: \\url{http://dx.doi.org/10.2307/3315194}.\n\n\\bibitem{Pardo1994}\nPardo L, Morales D, Ferentinos K, Zografos K.\n\\newblock Discretization problems on generalized entropies and {R}-divergences.\n\\newblock Kybernetika. 1994;30:445--460`.\n\n\\bibitem{Dehesa2009}\nSanchez-Moreno P, Dehesa JS, Y\\'a\\~nez RJ.\n\\newblock Discrete densities and Fisher Information.\n\\newblock In: Proceedings of the 14th International Conference on Difference\n  Equations and Applications. Difference Equations and Applications. Istanbul,\n  Turkey: U\\u{g}ur-Bah\\c{c}e\\c{s}ehir University Publishing Company; 2009. .\n\n\\bibitem{LMC1995}\nL\\'opez-Ruiz R, Mancini HL, Calbet X.\n\\newblock A statistical measure of complexity.\n\\newblock Physics Letters A. 1995;209(5-6):321--326.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/0375960195008675}.\n\n\\bibitem{Lamberti2004}\nLamberti PW, Mart\\'in MT, Plastino A, Rosso OA.\n\\newblock Intensive entropic non-triviality measure.\n\\newblock Physica A: Statistical Mechanics and its Applications.\n  2004;334(1--2):119--131.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0378437103010963}.\n\n\\bibitem{Grosse2002}\nGrosse I, Bernaola-Galv\\'an P, Carpena P, Rom\\'an-Rold\\'an R, Oliver J, Stanley\n  HE.\n\\newblock Analysis of symbolic sequences using the {J}ensen-{S}hannon\n  divergence.\n\\newblock Phys Rev E. 2002 Mar;65:041905.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevE.65.041905}.\n\n\\bibitem{Martin2006}\nMartin MT, Plastino A, Rosso OA.\n\\newblock Generalized statistical complexity measures: Geometrical and\n  analytical properties.\n\\newblock Physica A. 2006;369:439--462.\n\n\\bibitem{Olivares2012A}\nOlivares F, Plastino A, Rosso OA.\n\\newblock Ambiguities in Bandt-Pompe's methodology for local entropic\n  quantifiers.\n\\newblock Physica A: Statistical Mechanics and its Applications.\n  2012;391(8):2518--2526.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0378437111009691}.\n\n\\bibitem{Olivares2012B}\nOlivares F, Plastino A, Rosso OA.\n\\newblock Contrasting chaos with noise via local versus global information\n  quantifiers.\n\\newblock Physics Letters A. 2012;376(19):1577--1583.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0375960112003441}.\n\n\\bibitem{Lehmer}\nSchwarz K. The Archive of Interesting Code; 2011.\n\\newblock Available from:\n  \\url{http://www.keithschwarz.com/interesting/code/?dir=factoradic-permutation}.\n\n\\bibitem{Vignat2003}\nVignat C, Bercher JF.\n\\newblock Analysis of signals in the {F}isher-{S}hannon information plane.\n\\newblock Physics Letters A. 2003;312(1-2):27--33.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S037596010300570X}.\n\n\\bibitem{Bandt2002}\nBandt C, Pompe B.\n\\newblock Permutation Entropy: A Natural Complexity Measure for Time Series.\n\\newblock Physical Review Letters. 2002 Apr;88:174102--1--174102--4.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevLett.88.174102}.\n\n\\bibitem{Rosso2012}\nRosso OA, Olivares F, Zunino L, {De Micco} L, Aquino ALL, Plastino A, et~al.\n\\newblock Characterization of chaotic maps using the permutation\n  {B}andt-{P}ompe probability distribution.\n\\newblock European Physics Journal B. 2013 Mar;86(4):116--129.\n\\newblock Available from: \\url{http://dx.doi.org/10.1140/epjb/e2013-30764-5}.\n\n\\bibitem{Rosso2009A}\nRosso OA, Masoller C.\n\\newblock Detecting and quantifying stochastic and coherence resonances via\n  information-theory complexity measurements.\n\\newblock Physical Review E. 2009 Apr;79:040106.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevE.79.040106}.\n\n\\bibitem{Rosso2009B}\nRosso OA, Masoller C.\n\\newblock Detecting and quantifying temporal correlations in stochastic\n  resonance via information theory measures.\n\\newblock European Physics Journal B. 2009 Apr;69(1):37--43.\n\\newblock Available from: \\url{http://dx.doi.org/10.1140/epjb/e2009-00146-y}.\n\n\\bibitem{Saco2010}\nSaco PM, Carpi LC, Figliola A, Serrano E, Rosso OA.\n\\newblock Entropy analysis of the dynamics of {E}l {N}i\\~no/Southern\n  Oscillation during the {H}olocene.\n\\newblock Physica A: Statistical Mechanics and its Applications.\n  2010;389(21):5022--5027.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0378437110006151}.\n\n\\bibitem{Parlitz2012}\nParlitz U, Berg S, Luther S, Schirdewan A, Kurths J, Wessel N.\n\\newblock Classifying cardiac biosignals using ordinal pattern statistics and\n  symbolic dynamics.\n\\newblock Computers in Biology and Medicine;42(3):319--327.\n\\newblock Available from:\n  \\url{http://dx.doi.org/10.1016/j.compbiomed.2011.03.017}.\n\n\\bibitem{Kowalski2007}\nKowalski AM, Mart\\'in MT, Plastino A, Rosso OA.\n\\newblock Bandt-{P}ompe approach to the classical-quantum transition.\n\\newblock Physica D: Nonlinear Phenomena. 2007;233(1):21--31.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0167278907001790}.\n\n\\bibitem{Zunino2010B}\nZunino L, Soriano MC, Fischer I, Rosso OA, Mirasso CR.\n\\newblock Permutation-information-theory approach to unveil delay dynamics from\n  time-series analysis.\n\\newblock Physical Review E. 2010 Oct;82:046212.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevE.82.046212}.\n\n\\bibitem{Soriano2011}\nSoriano MC, Zunino L, Rosso OA, Fischer I, Mirasso CR.\n\\newblock Time Scales of a Chaotic Semiconductor Laser With Optical Feedback\n  Under the Lens of a Permutation Information Analysis.\n\\newblock IEEE Journal of Quantum Electronics. 2011 Feb;47(2):252--261.\n\n\\bibitem{Zunino2012}\nZunino L, Soriano MC, Rosso OA.\n\\newblock Distinguishing chaotic and stochastic dynamics from time series by\n  using a multiscale symbolic approach.\n\\newblock Physical Review E. 2012 Oct;86:046210.\n\\newblock Available from:\n  \\url{http://link.aps.org/doi/10.1103/PhysRevE.86.046210}.\n\n\\bibitem{Zanin2012}\nZanin M, Zunino L, Rosso OA, Papo D.\n\\newblock Permutation Entropy and Its Main Biomedical and Econophysics\n  Applications: A Review.\n\\newblock Entropy. 2012;14(8):1553.\n\\newblock Available from: \\url{http://www.mdpi.com/1099-4300/14/8/1553}.\n\n\\bibitem{Bouletreau1998}\nBouletreau V, Vincent N, Sabourin R, Emptoz H.\n\\newblock Handwriting and signature: one or two personality identifiers?\n\\newblock In: Proceedings. Fourteenth International Conference onPattern\n  Recognition. vol.~2; 1998. p. 1758--1760.\n\n\\bibitem{Houmani2014}\nHoumani N, S GS.\n\\newblock Quality measures for online hadwritten signatures.\n\\newblock In: Scharcanski J, ca HP, Du E, editors. Signal and Image Processing\n  for Biometrics. No. 292 in Lecture Notes in Electrical Engineering. Springer;\n  2014. p. 255--283.\n\n\\bibitem{Vincent2000}\nVincent N, Boul\\'etreau V, Empotz H, Sabourin R.\n\\newblock How to use fractal dimensions to qualify writings and writers.\n\\newblock Fractals. 2000;8:85--97.\n\n\\bibitem{Richards1999}\nRichards JA, Jia X.\n\\newblock Remote Sensing Digital Image Analysis.\n\\newblock 4th ed. Berlin: Springer; 2006.\n\n\\bibitem{Campbell2011}\nCampbell C, Ying Y.\n\\newblock Learning with Support Vector Machines.\n\\newblock In: Brachman RJ, Dietterich T, editors. Synthesis Lectures on\n  Artificial Intelligence and Machine Learning. No.~5 in Synthesis Lectures on\n  Artificial Intelligence and Machine Learning. Santa Fe, CA: Morgan and\n  Claypool; 2011. p. 1--95.\n\n\\bibitem{Boser1992}\nBoser BE, Guyon IM, Vapnik VN.\n\\newblock A training algorithm for optimal margin classifiers.\n\\newblock In: Proceedings of the Fifth Annual Workshop on Computational\n  Learning Theory. Pittsburgh: ACM Press; 1992. p. 144--152.\n\n\\bibitem{Vapnik1995}\nVapnik VN.\n\\newblock The Nature of Statistical Learning Theory.\n\\newblock Springer; 1995.\n\n\\bibitem{Vapnik1998}\nVapnik VN.\n\\newblock Statistical Learning Theory.\n\\newblock Willey; 1998.\n\n\\bibitem{Scholkopf2001}\nSch\\\"{o}lkopf B, Platt JC, Shawe-Taylor JC, Smola AJ, Williamson RC.\n\\newblock Estimating the Support of a High-Dimensional Distribution.\n\\newblock Neural Computation. 2001 Jul;13(7):1443--1471.\n\\newblock Available from: \\url{http://dx.doi.org/10.1162/089976601750264965}.\n\n\\bibitem{Scholkopf2002}\nSch\\\"olkopf B, Smola AJ.\n\\newblock Learning with Kernels.\n\\newblock Cambridge: MIT Press; 2002.\n\n\\bibitem{Kohavi1998}\nProvost F, Kohavi R.\n\\newblock On Applied Research in Machine Learning.\n\\newblock Machine Learning. 1998;30(2/3):127--132.\n\\newblock Available from: \\url{http://dx.doi.org/10.1023/A:1007442505281}.\n\n\\bibitem{Chang2001}\nChang CC, Lin CJ. {LIBSVM}: a library for support vector machines; 2001.\n\\newblock Available from: \\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.\n\n\\bibitem{Fierrez2005}\nFierrez-Aguilar J, Nanni L, Lopez-Pe\\~nalba J, Ortega-Garcia J, Maltoni D.\n\\newblock An On-Line Signature Verification System Based on Fusion of Local and\n  Global Information.\n\\newblock In: Kanade T, Jain A, Ratha N, editors. 5th International Conference\n  on Audio- and Video-Based Biometric Person Authentication (AVBPA). vol. 3546\n  of Lecture Notes in Computer Science. Springer Berlin Heidelberg; 2005. p.\n  523--532.\n\\newblock Available from: \\url{http://dx.doi.org/10.1007/11527923_54}.\n\n\\bibitem{Fierrez2007}\nFierrez J, Ortega-Garcia J, Ramos D, Gonzalez-Rodriguez J.\n\\newblock {HMM}-based on-line signature verification: Feature extraction and\n  signature modeling.\n\\newblock Pattern Recognition Letters. 2007;28(16):2325--2334.\n\\newblock Available from:\n  \\url{http://www.sciencedirect.com/science/article/pii/S0167865507002395}.\n\n\\bibitem{Pascual2009}\nPascual-Gaspar JM, Carde\\~noso Payo V, Vivaracho-Pascual CE.\n\\newblock Practical On-Line Signature Verification.\n\\newblock In: Tistarelli M, Nixon MS, editors. Proceedings Third International\n  Conference Advances in Biometrics ICB. vol. 5558 of Lecture Notes in Computer\n  Science. Springer Berlin Heidelberg; 2009. p. 1180--1189.\n\\newblock Available from:\n  \\url{http://dx.doi.org/10.1007/978-3-642-01793-3_119}.\n\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 60200, "prevtext": "\nThis method thus creates a hyperplane characterized by  $\\mathbf{w}$ and  $b$ which has maximal distance \nfrom the origin in the feature space $\\cal G$ and separates all the data points from the origin.\nHere $\\alpha_i$ are the Lagrange multipliers; every $\\alpha_i >0 $ is {\\it weighted in\\/} the decision function and thus ``supports\" the machine; hence the name Support Vector Machine.\nSince SVMs are considered to be sparse, there will be relatively few Lagrange multipliers with a nonzero value.\n\nOur choice for the kernel is the Gaussian Radial Base function:\n\n", "index": 35, "text": "\\begin{equation}\n\\label{eq:oc-svm-primal-4}\nK(z_i,z_j) = \\exp \\Big(-\\frac{1}{2\\sigma^2} \\|z_i - z_j\\|^2\n\\Big) ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"K(z_{i},z_{j})=\\exp\\Big{(}-\\frac{1}{2\\sigma^{2}}\\|z_{i}-z_{j}\\|^{2}\\Big{)},\" display=\"block\"><mrow><mrow><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac><mo>\u2062</mo><msup><mrow><mo>\u2225</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>-</mo><msub><mi>z</mi><mi>j</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]