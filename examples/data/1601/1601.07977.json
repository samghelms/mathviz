[{"file": "1601.07977.tex", "nexttext": "\nwhere $NN(x_{i})=c_{j}$ denotes the set of $x_{i}$ whose nearest neighbor is $c_{j}$. To improve the performance, Gong et al.~\\cite{gong2014multi} calculated multi-scale representations of Eq.~\\ref{Eq1} and concatenated all the scales. Motivated by~\\cite{gong2014multi}, the CFV~\\cite{perronnin2007fisher} can be further used to improve the performance of image representation~\\cite{cimpoi2014deep},~\\cite{yoo2014fisher}.\n\nSuppose we are given the multi-scale activates \\{${X^{(s)}|X^{(s)}\\in \\mathbb{R}^{d\\times N^{(s)}}}$\\}, $X^{(s)}=[x_{1}^{(s)},x_{2}^{(s)},\\cdots,x_{N^{(s)}}^{(s)}]$,~$s = 1,2,\\cdots,S$ from all the training images. Let $u_{\\lambda}=\\sum_{t=1}^{M}\\omega_{i}u_{i}(x)$  denote a Gaussian Mixture Model~(GMM), where $\\lambda=\\{\\lambda_{i}=\\{\\omega_{i},\\mu_{i},\\sigma_{i}\\},i=1,2,\\cdots,M\\}$ represents the parameters of the GMM, and $\\lambda$ can be optimized by the Maximum Likelihood (ML) estimation based on \\{$X^{(s)},s=1,2,\\cdots,S$\\}. Denote the multi-scale activates of the  image $I$ as $\\chi=\\{\\chi_{s}=[x_{1}^{(s)},x_{2}^{(s)},\\cdots,x_{I^{(s)}}^{(s)}],s=1,2,\\cdots,S\\}$. The gradients of the  $u_{\\lambda}(\\chi)$ w.r.t. the $i$-th Gaussian can be represented as follows\n\n", "itemtype": "equation", "pos": 12106, "prevtext": "\n\n\n\n\n\n\n\n\\title{Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain Adaptation}\n\n\n\n\n\n\n\n\n\n\n\n\\author{Guo-Sen Xie,\n\t    Xu-Yao Zhang,\n        Shuicheng Yan,~\\IEEEmembership{Senior Member,~IEEE,}\n        Cheng-Lin Liu,~\\IEEEmembership{~Fellow,~IEEE}\n\\thanks{G.-S. Xie, X.-Y. Zhang and C.-L. Liu are with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, No. 95 Zhongguancun East Road, Beijing 100190, P.R. China. C.-L. Liu is also with the Research Center for Brain-Inspired Intelligence, Institute of Automation, and the CAS Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences. E-mail: \\{guosen.xie, xyz, liucl\\}@nlpr.ia.ac.cn. }\n\n\\thanks{S.~Yan is with the Department of Electrical and Computer Engineering, National University of Singapore, 117583, Singapore. Email: eleyans@nus.edu.sg.}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\markboth{Transaction on circults and systems for video technology}\n{Shell \\MakeLowercase{\\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nConvolutional neural network (CNN) has achieved state-of-the-art performance in many different visual tasks. Learned from a large-scale training dataset, CNN features are much more discriminative and accurate than the hand-crafted features. Moreover, CNN features are also transferable among different domains. On the other hand, traditional dictionary-based features (such as BoW and SPM) contain much more local discriminative and structural information, which is implicitly embedded in the images. To further improve the performance, in this paper, we propose to combine CNN with dictionary-based models for scene recognition and visual domain adaptation. Specifically, based on the well-tuned CNN models~(e.g., AlexNet and VGG Net), two dictionary-based representations are further constructed, namely mid-level local representation (MLR) and convolutional Fisher vector representation (CFV). In MLR, an efficient two-stage clustering method, i.e., weighted spatial and feature space spectral clustering on the parts of a single image followed by clustering all representative parts of all images, is used to generate a class-mixture or a class-specific part dictionary. After that, the part dictionary is used to operate with the multi-scale image inputs for generating mid-level representation. In CFV, a multi-scale and scale-proportional GMM training strategy is utilized to generate Fisher vectors based on the last convolutional layer of CNN. By integrating the complementary information of MLR, CFV and the CNN features of the fully connected layer, the state-of-the-art performance can be achieved on scene recognition and domain adaptation problems. {\\color{black}An interested finding is that our proposed hybrid representation~(from VGG net trained on ImageNet) is also complementary with GoogLeNet and/or VGG-11~(trained on Place205) greatly.} \n\\end{abstract}\n\n\n\\begin{IEEEkeywords}\nConvolutional neural networks, Scene recognition, Domain adaptation, Dictionary, Part learning, Fisher vector.\n\\end{IEEEkeywords}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\\section{Introduction}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEPARstart{W}{ith} the development of deep learning, convolutional neural network~(CNN)~\\cite{lecun1998gradient},~\\cite{krizhevsky2012imagenet} has been successfully applied to various fields, such as object recognition~\\cite{krizhevsky2012imagenet},~\\cite{SimonyanZ14a},~\\cite{szegedy2014going},~\\cite{BatchNormalize},~\\cite{MSRA}, image detection~\\cite{girshick2014rich},~\\cite{sermanet2013pedestrian},~\\cite{ouyang2014deepid}, image segmentation~\\cite{kang2014fully},~\\cite{LongSD14},~\\cite{PinheiroC14},~\\cite{ChenPKMY14}, image retrieval~\\cite{Zhao14}, and so on. State-of-the-art performance achieved by CNN in these fields identify the powerful feature representation ability of CNN for different visual tasks.\n\nThe traditional Bag of Visual Words (BoW) models~\\cite{jurie2005creating},~\\cite{yang2009linear},~\\cite{wang2010locality},~\\cite{perronnin2007fisher},~\\cite{perronnin2010improving},~\\cite{zhou2010image}, which had been quite popular before 2012 in the research community of object recognition, is now being gradually neglected since the CNN model gained the champion in the large-scale competition of ImageNet classification~\\cite{krizhevsky2012imagenet} in 2012~(ILSVRC-12). Due to the strong representation ability of CNN trained on a large dataset, e.g., ImageNet, some works~\\cite{razavian2014cnn},~\\cite{donahue2013decaf} advocate directly using CNN features for classification, and have gained much better performance than traditional methods. There also exist some works~\\cite{zuo2014learning},~\\cite{gong2014multi} that have tried to combine traditional models with the CNN model to get better results. Traditional features and CNN features can complement each other, and better performance can be obtained by combining them.\n\nThe images from the ImageNet database for training CNN are mostly object-oriented, thus it seems that the trained CNN model is more suitable for object recognition than for other tasks, e.g., scene recognition~\\cite{quattoni2009recognizing}. Zhou et al.~\\cite{zhou2014learning}  collected a large-scale place database to train the CNN (PlaceNet), with the same architecture as~\\cite{donahue2013decaf}, and found that based on the features of PlaceNet, better performance can be obtained than the original CNN features~\\cite{donahue2013decaf}. Recently, Mircea et al.~\\cite{cimpoi2014deep} found that stronger CNN architectures can approach and outperform PlaceNet even if trained on ImageNet data. {\\color{black}{Yosinski et al.~\\cite{NIPS2014_5347} verified that CNN features in the lower layer are more ``general\" while the features of higher layers are more ``specific\".}} Therefore, how to explore the underlined representation and discriminative ability of CNN, no matter what kind of database it is trained on, is still an interesting problem.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig1.pdf}\n\t\\caption{The proposed hybrid representation based on the trained CNN. CFV is calculated based on the last convolutional layer~(Convolution-L); FCR1 indicates the global representation based on the first fully connected layer~(Full Connection-1); similarly, FCR2 is calculated based on the second fully connected layer~(Full Connection-2); and our proposed MLR is based on the second fully connected layer too. Operations in the dashed box are multiple convolution, max-pooling and other normalization, determined by the used CNN architecture.}\n\t\\label{fig1}\n\\end{figure}\nIn this paper, we focus on scene recognition~\\cite{quattoni2009recognizing} and visual domain adaptation~\\cite{saenko2010adapting}. Unlike in generic object categorization, scene images have many discriminative part regions, which is beneficial for distinguishing the categories. During the past few years, many works~\\cite{juneja2013blocks},~\\cite{singh2012unsupervised},~\\cite{doersch2013mid},~\\cite{lin2014learning},~\\cite{doersch2012makes},~\\cite{pandey2011scene} have been devoted to discover discriminative parts for scene recognition. The above methods first train a detector or classifier and then detect the part regions, which are not scalable for large databases. To discover discriminative parts more efficiently, there have also been some methods that are based on bottom cues to discover discriminative proposals, e.g.,~\\cite{uijlings2013selective},~\\cite{cheng2014bing},~\\cite{arbelaez2014multiscale},~\\cite{zitnick2014edge}. Visual domain adaptation~(DA) can be classified into two types,  unsupervised and semi-supervised adaptation, according to whether the target domain samples are used or not for training the classifier. Pioneer works of DA include~\\cite{saenko2010adapting},~\\cite{gopalan2011domain},~\\cite{gong2012geodesic},~\\cite{gong2013connecting},~\\cite{fernando2013unsupervised},~\\cite{chopra2013dlid}, which are based on learning with regularization on the manifold. Recent works on DA~\\cite{tzeng2014deep},~\\cite{Menglong2015deep},~\\cite{ghifary2014domain},~\\cite{borgwardt2006integrating} have also adopted the strategy of adding maximum mean discrepancy~(MMD) constraint on the fully connected layer of neural networks during the training process.\n\n{\\color{black}For both} scene recognition and domain adaptation, very few works have  utilized local discriminative information implicitly contained in the images in the context of CNN. Especially for domain adaptation, no one knows whether the local part information can improve the domain transfer performance or not. Herein, we propose to utilize the local part information to improve the discriminative ability of CNN features. Specifically, we propose to combine CNN features with two dictionary-based models. {\\color{black}{The first one is the the mid-level local discriminative representation~(MLR). MLR aims to discover local discriminative information contained in the images, which are beneficial for afterward classification. To construct MLR, we utilize selective search~\\cite{uijlings2013selective} to generate initial parts for each image, followed by our proposed two-stage clustering to filter out redundancy parts and generate part dictionary, finally the locality-constrained linear coding~(LLC)~\\cite{wang2010locality} and spatial pyramid matching (SPM)~\\cite{lazebnik2006beyond} are used for generating the MLR.~(See Fig.~\\ref{fig2} for illustration.)}} \n\nOn the other hand, Fisher vectors of the last convolutional layer of CNN~(CFV) are generated to further boost the performance. {\\color{black}{As we know, Fisher vector~\\cite{perronnin2007fisher} consists of first order and second order differences between the descriptors and the GMM centers, which gives CFV the same representation ability to well distinguish different categories.}} As for GMM training before Fisher vector coding, muti-scale and scale-proportional descriptors are sampled. As for Fisher vector coding, we use the same strategy as~\\cite{yoo2014fisher}, which is denoted as Multi-scale Pyramid Pooling. Another commonly used feature of CNN is the fully connected layer representation~(FCR), which contains the global information of input images. By combining these several representations, i.e., mid-level local representation~(MLR), convolutional Fisher vector representation~(CFV), and the global representions of the last two fully connected layers of CNN~(FCR) together, we can obtain our hybrid representation~(See Fig.~\\ref{fig1} for the whole flowchart). {\\color{black}{The advantages of our hybrid representation lie in~1) having more discriminative ability for classification, and 2) being complementary to each other.}} Experimental results on both scene recognition and domain adaptation validate the strong domain transfer ability from other large database of our hybrid representation. Moreover, we also find that MLR is both complementary with CFV and FCR.\n\nThe remainder of this paper is organized as follows. In Section~II, we give some related works. In Section~III, we illustrate the proposed pipeline elaborately. Section~IV  shows that the hybrid representation can achieve the best results in scene recognition and visual domain adaptation. In Section~V, we conclude this paper and discuss the future work.\n\n\n\n\\section{Related Work}\nIn this section, we introduce the related works to our hybrid representation. Gong et al.~\\cite{gong2014multi} proposed to calculate the VLAD~\\cite{jegou2010aggregating} representation based on the FCR of the trained CNN, of which the performance is better than the original FCR and other traditional methods. Let $X=[x_{1},x_{2},\\cdots,x_{N}]\\in \\mathbb{R}^{d\\times N}$  be the single-scale activates of the image $I$ based on the FCR of the trained CNN, and $C=[c_{1},c_{2},\\cdots,c_{M}]\\in \\mathbb{R}^{d\\times M}$ be the codebook (dictionary) learned by the K-means algorithm based on all the training images. Then, the VLAD coding of the image $I$ can be represented as\n\n", "index": 1, "text": "\\begin{equation}\nv = [\\sum\\limits_{NN(x_{i})=c_{1}} x_{i}-c_{1},\\cdots, \\sum\\limits_{NN(x_{i})=c_{M}} x_{i}-c_{M}],\n\\label{Eq1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"v=[\\sum\\limits_{NN(x_{i})=c_{1}}x_{i}-c_{1},\\cdots,\\sum\\limits_{NN(x_{i})=c_{M%&#10;}}x_{i}-c_{M}],\" display=\"block\"><mrow><mrow><mi>v</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>N</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>c</mi><mn>1</mn></msub></mrow></munder><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>-</mo><msub><mi>c</mi><mn>1</mn></msub></mrow><mo>,</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>,</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>N</mi><mo>\u2062</mo><mi>N</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>c</mi><mi>M</mi></msub></mrow></munder><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>-</mo><msub><mi>c</mi><mi>M</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\nThe Fisher vector of the image $I$ is obtained by concatenating  all the gradients w.r.t. those $M$ Gaussians.\nIn~\\cite{yoo2014fisher},  while calculating gradients w.r.t. each Gaussian, Yoo et al. adopted Multi-scale Pyramid Pooling, where first the GMM parameters are generated based on all the descriptors from different scaled training images, and then scale-specific normalization and max-pooling are implemented.\n\nRecently, Liu et al.~\\cite{Liu2014ACCV} also proposed to learn a part dictionary based on the LC-KSVD method~\\cite{jiang2011learning} directly. In this work, each part element is given a label which is the same as that of the image where it is located on. LC-KSVD is a popular dictionary learning method, but it is very time-consuming and therefore cannot address large-scale problems, e.g., the part dictionary learning of the SUN database~\\cite{xiao2010sun}. Another drawback is that many local parts usually have no explicit semantic~(label) information at all, so unsupervised part dictionary learning may be more reasonable than the supervised counterpart.\n\n\\section{The Proposed Hybrid Method}\nIn this section, we illustrate the whole pipeline of the construction of MLR, CFV and FCR, followed by the details of each part.\n\\subsection{The Whole System}\nTo explore the representation ability of deep CNN features, we propose to combine the three kinds of representations based on CNN features, i.e., our proposed MLR, CFV, and FCR. We illustrate the whole pipeline in Fig.~\\ref{fig1}. In the figure, given the trained CNN models with~(without) fine-tuning, e.g. AlexNet~\\cite{krizhevsky2012imagenet} and VGG Net~\\cite{SimonyanZ14a}, we further extract the multi-scale CFVs, single-scale FCRs and our local discriminative MLRs for each image. Then the concatenated hybrid representations are fed into the linear SVM classifier~\\cite{fan2008liblinear}. As validated by our experiments, the hybrid representations are very powerful features due to their complementary components, i.e., CFV which contains one or two order gradient information, FCR which contains global information of images, and MLR which contains local discriminative (structural) information. {\\color{black}{The whole procedure of our hybrid representation is illustrated in Algorithm~\\ref{Algorithm1}.}}\n\n\\subsection{MLR: Integrating Local Discrimination and CNN Features}\nCNN has become a strong tool to learn invariant features for various visual applications. Nevertheless, during the training process of CNN, the input images are all global ones, and no good strategy has been found to incorporate the local information into the training process of CNN yet. In this paper, we propose a strategy to generate the mid-level local representation~(MLR), which is based on part dictionary clustering and multi-scale mid-level representation generating. MLR is constructed based on the local discriminative part dictionary, which leads to its local discrimination. In this subsection, we illustrate the details of our method for constructing the mid-level representation~(MLR) by utilizing the local discriminative ability of images.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig2.pdf}\n\t\\caption{The process of calculating MLR based on the trained part dictionary. The operations in the red dashed box are the part dictionary generating process based on two-stage clustering, and operations in the below are the MLR generating process which is based on part dictionary and SPM. Best viewed in color.}\n\t\\label{fig2}\n\\end{figure}\n\n\nAt the beginning, we utilize selective search~\\cite{uijlings2013selective} to get the discriminative part proposals for every image, with the constraints of the pixel number of proposals within [60$\\times$60,160$\\times$160], and width/height~(height/width) smaller than 3, which is used to catch the local information of the images.\n\nFurthermore, the spectral clustering in both the spatial and the feature space is conducted with the bounding box information of each image. Specifically, suppose the selected bounding boxes (by selective search) are $B = [B_{1},B_{2},\\cdots,B_{n_{I}}]\\in \\mathbb{R}^{4\\times n_{I}}$ with each $B_{i}$ denoting the coordinates of top-left and bottom-right on the image $I$, and the corresponding last fully connected layer activates of the trained CNN for $B$ are denoted as $F = [f_{1},f_{2},\\cdots,f_{n_{I}}]\\in \\mathbb{R}^{d\\times n_{I}}$~($d=4096$ for current popular networks). Herein, we use the activates after ReLU~\\cite{krizhevsky2012imagenet}.  Under our constraints, while extracting $B$, the number of $n_{I}$ is usually less than 500, so the forward propagation for extracting $F$ will be acceptable. We construct the final similarity graph $G = (V,E)$ based on both $B$ and $F$, and the weights on the edges are as follows:\n\n", "itemtype": "equation", "pos": 13449, "prevtext": "\nwhere $NN(x_{i})=c_{j}$ denotes the set of $x_{i}$ whose nearest neighbor is $c_{j}$. To improve the performance, Gong et al.~\\cite{gong2014multi} calculated multi-scale representations of Eq.~\\ref{Eq1} and concatenated all the scales. Motivated by~\\cite{gong2014multi}, the CFV~\\cite{perronnin2007fisher} can be further used to improve the performance of image representation~\\cite{cimpoi2014deep},~\\cite{yoo2014fisher}.\n\nSuppose we are given the multi-scale activates \\{${X^{(s)}|X^{(s)}\\in \\mathbb{R}^{d\\times N^{(s)}}}$\\}, $X^{(s)}=[x_{1}^{(s)},x_{2}^{(s)},\\cdots,x_{N^{(s)}}^{(s)}]$,~$s = 1,2,\\cdots,S$ from all the training images. Let $u_{\\lambda}=\\sum_{t=1}^{M}\\omega_{i}u_{i}(x)$  denote a Gaussian Mixture Model~(GMM), where $\\lambda=\\{\\lambda_{i}=\\{\\omega_{i},\\mu_{i},\\sigma_{i}\\},i=1,2,\\cdots,M\\}$ represents the parameters of the GMM, and $\\lambda$ can be optimized by the Maximum Likelihood (ML) estimation based on \\{$X^{(s)},s=1,2,\\cdots,S$\\}. Denote the multi-scale activates of the  image $I$ as $\\chi=\\{\\chi_{s}=[x_{1}^{(s)},x_{2}^{(s)},\\cdots,x_{I^{(s)}}^{(s)}],s=1,2,\\cdots,S\\}$. The gradients of the  $u_{\\lambda}(\\chi)$ w.r.t. the $i$-th Gaussian can be represented as follows\n\n", "index": 3, "text": "\\begin{equation}\ng_{i} = \\frac{1}{|\\chi|}\\sum\\limits_{s=1}^{S}\\sum\\limits_{n=1}^{|\\chi_{s}|}\\nabla_{\\lambda_{i}}\\log u_{\\lambda}(x_{n}^{(s)}).\n\\label{Eq2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"g_{i}=\\frac{1}{|\\chi|}\\sum\\limits_{s=1}^{S}\\sum\\limits_{n=1}^{|\\chi_{s}|}%&#10;\\nabla_{\\lambda_{i}}\\log u_{\\lambda}(x_{n}^{(s)}).\" display=\"block\"><mrow><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><mi>\u03c7</mi><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\u03c7</mi><mi>s</mi></msub><mo stretchy=\"false\">|</mo></mrow></munderover><mrow><mrow><mrow><msub><mo>\u2207</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></msub><mo>\u2061</mo><mi>log</mi></mrow><mo>\u2061</mo><msub><mi>u</mi><mi>\u03bb</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\nwhere $\\lambda_{B}$ and $\\lambda_{F}$ are the weighting parameters {\\color{black}{and $\\lambda_{B}+\\lambda_{F}$=1 is used in our paper}} . Specifically, the elements of $W_{B}$ and $W_{F}$ are denoted as\n\n", "itemtype": "equation", "pos": 18439, "prevtext": "\nThe Fisher vector of the image $I$ is obtained by concatenating  all the gradients w.r.t. those $M$ Gaussians.\nIn~\\cite{yoo2014fisher},  while calculating gradients w.r.t. each Gaussian, Yoo et al. adopted Multi-scale Pyramid Pooling, where first the GMM parameters are generated based on all the descriptors from different scaled training images, and then scale-specific normalization and max-pooling are implemented.\n\nRecently, Liu et al.~\\cite{Liu2014ACCV} also proposed to learn a part dictionary based on the LC-KSVD method~\\cite{jiang2011learning} directly. In this work, each part element is given a label which is the same as that of the image where it is located on. LC-KSVD is a popular dictionary learning method, but it is very time-consuming and therefore cannot address large-scale problems, e.g., the part dictionary learning of the SUN database~\\cite{xiao2010sun}. Another drawback is that many local parts usually have no explicit semantic~(label) information at all, so unsupervised part dictionary learning may be more reasonable than the supervised counterpart.\n\n\\section{The Proposed Hybrid Method}\nIn this section, we illustrate the whole pipeline of the construction of MLR, CFV and FCR, followed by the details of each part.\n\\subsection{The Whole System}\nTo explore the representation ability of deep CNN features, we propose to combine the three kinds of representations based on CNN features, i.e., our proposed MLR, CFV, and FCR. We illustrate the whole pipeline in Fig.~\\ref{fig1}. In the figure, given the trained CNN models with~(without) fine-tuning, e.g. AlexNet~\\cite{krizhevsky2012imagenet} and VGG Net~\\cite{SimonyanZ14a}, we further extract the multi-scale CFVs, single-scale FCRs and our local discriminative MLRs for each image. Then the concatenated hybrid representations are fed into the linear SVM classifier~\\cite{fan2008liblinear}. As validated by our experiments, the hybrid representations are very powerful features due to their complementary components, i.e., CFV which contains one or two order gradient information, FCR which contains global information of images, and MLR which contains local discriminative (structural) information. {\\color{black}{The whole procedure of our hybrid representation is illustrated in Algorithm~\\ref{Algorithm1}.}}\n\n\\subsection{MLR: Integrating Local Discrimination and CNN Features}\nCNN has become a strong tool to learn invariant features for various visual applications. Nevertheless, during the training process of CNN, the input images are all global ones, and no good strategy has been found to incorporate the local information into the training process of CNN yet. In this paper, we propose a strategy to generate the mid-level local representation~(MLR), which is based on part dictionary clustering and multi-scale mid-level representation generating. MLR is constructed based on the local discriminative part dictionary, which leads to its local discrimination. In this subsection, we illustrate the details of our method for constructing the mid-level representation~(MLR) by utilizing the local discriminative ability of images.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig2.pdf}\n\t\\caption{The process of calculating MLR based on the trained part dictionary. The operations in the red dashed box are the part dictionary generating process based on two-stage clustering, and operations in the below are the MLR generating process which is based on part dictionary and SPM. Best viewed in color.}\n\t\\label{fig2}\n\\end{figure}\n\n\nAt the beginning, we utilize selective search~\\cite{uijlings2013selective} to get the discriminative part proposals for every image, with the constraints of the pixel number of proposals within [60$\\times$60,160$\\times$160], and width/height~(height/width) smaller than 3, which is used to catch the local information of the images.\n\nFurthermore, the spectral clustering in both the spatial and the feature space is conducted with the bounding box information of each image. Specifically, suppose the selected bounding boxes (by selective search) are $B = [B_{1},B_{2},\\cdots,B_{n_{I}}]\\in \\mathbb{R}^{4\\times n_{I}}$ with each $B_{i}$ denoting the coordinates of top-left and bottom-right on the image $I$, and the corresponding last fully connected layer activates of the trained CNN for $B$ are denoted as $F = [f_{1},f_{2},\\cdots,f_{n_{I}}]\\in \\mathbb{R}^{d\\times n_{I}}$~($d=4096$ for current popular networks). Herein, we use the activates after ReLU~\\cite{krizhevsky2012imagenet}.  Under our constraints, while extracting $B$, the number of $n_{I}$ is usually less than 500, so the forward propagation for extracting $F$ will be acceptable. We construct the final similarity graph $G = (V,E)$ based on both $B$ and $F$, and the weights on the edges are as follows:\n\n", "index": 5, "text": "\\begin{equation}\nW = \\lambda_{B}W_{B} + \\lambda_{F}W_{F},\n\\label{Eq3}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"W=\\lambda_{B}W_{B}+\\lambda_{F}W_{F},\" display=\"block\"><mrow><mrow><mi>W</mi><mo>=</mo><mrow><mrow><msub><mi>\u03bb</mi><mi>B</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>B</mi></msub></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>F</mi></msub><mo>\u2062</mo><msub><mi>W</mi><mi>F</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\n{\\color{black}{where $|\\cdotp|$ indicates calculating the area of the input box.}}\n\n", "itemtype": "equation", "pos": 18728, "prevtext": "\nwhere $\\lambda_{B}$ and $\\lambda_{F}$ are the weighting parameters {\\color{black}{and $\\lambda_{B}+\\lambda_{F}$=1 is used in our paper}} . Specifically, the elements of $W_{B}$ and $W_{F}$ are denoted as\n\n", "index": 7, "text": "\\begin{equation}\nW_{B}(i,j) = \\dfrac{|B_{i}|\\bigcap |B_{j}|}{|B_{i}|\\bigcup |B_{j}|},\n\\label{Eq4}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"W_{B}(i,j)=\\dfrac{|B_{i}|\\bigcap|B_{j}|}{|B_{i}|\\bigcup|B_{j}|},\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mi>B</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo largeop=\"true\" mathsize=\"160%\" stretchy=\"false\" symmetric=\"true\">\u22c2</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo largeop=\"true\" mathsize=\"160%\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\nfor $i,j = 1,2,\\cdots,n_{I}$.\n\n{\\color{black}{After spectral clustering on the graph $G$, we obtain $Q$ clusters $\\mathcal{C}_{1},\\mathcal{C}_{2},\\cdots,\\mathcal{C}_{Q}$. Then we sort the $Q$ clusters in descending order according to the number of bounding boxes and select the top $T$~($T\\leq Q$) clusters.}} We randomly select one bounding box from each $\\mathcal{C}_{i},i=1,2,\\cdots,T$, thus totally $T$ bounding boxes are reserved. We further implement context padding~\\cite{girshick2014rich} on the selected $T$ bounding boxes and do forward propagation (based on CNN) again to get the new feature representation based on the context padded boxes. In this way, we can get $T$ representations $P = [P_{1},P_{2},\\cdots,P_{T}]$ for each image $I$, and we denote them as the prototypes of the corresponding image.\n\nFinally, for all the prototypes of the training images, we do K-means clustering~\\cite{kanungo2002efficient} to get the class-specific part dictionary and the class-mixture part dictionary, denoted as $D_{cs}\\in \\mathbb{R}^{d\\times K}$ and $D_{cm}\\in \\mathbb{R}^{d\\times K}$ respectively. Here the elements of $D_{cs}$ are obtained by first clustering the prototypes from each class and then concatenating them together. While $D_{cm}$ is based on clustering all the prototypes without considering the class information. The part dictionary contains local discriminative and multi-scale information of the training images, due to the selective search algorithm~(SSA) and constraints while running SSA.\n\nWith the part dictionary $D_{cs}$~($D_{cm}$) learned, we can consider it as a group of local discriminative filter banks. Motivated by Object-Bank~\\cite{li2010object}, given an input image at a single scale, we sample square regions at multiple scales densely, i.e., sampled squares with size $128\\times128$, $92\\times92$ and $64\\times64$ and step size 32 pixels for all three scales. Then we calculate the activates of the last fully connected layer of the trained CNN for all these squares under different scales, which generates three scaled activate tensors. This can also be seen as the local feature extraction on square regions. After that, for each scaled activate tensor of each image, we use the $D_{cs}$~($D_{cm}$) to operate with the activates on each location of the tensors, resulting in $K$ new feature maps for each scaled activate tensor, which can be seen as another kind of convolutional operation based on the part dictionary~\\cite{kavukcuoglu2010learning}. For every $K$ feature maps under different scales, we further apply spatial pyramid matching (SPM)~\\cite{lazebnik2006beyond} to divide the map region into spatial cells in three levels, i.e., $1\\times1$, $2\\times2$, $3\\times1$, and do max-pooling on each cell. The final MLR is the concatenation of all the max-pooled features, with the dimension being $3\\times K\\times(1\\times1+2\\times2+3\\times1)$.\n\nMany methods can be adopted as the strategy of operation between the part dictionary $D_{cs}$~($D_{cm}$) and the activates of local square regions, e.g., inner production, sparse coding~\\cite{yang2009linear}, locality-constrained linear coding~(LLC)~\\cite{wang2010locality}, and auto-encoder based coding~\\cite{Xie2014ACCV}. There are detailed coding speed comparisons in~\\cite{Xie2014ACCV}. Here we utilize the LLC for feature coding, due to its relative fast speed and locality-preserve property. \n\nSpecifically, denote the activating tensor of the image $I$ under one scale by $A\\in \\mathbb{R}^{d\\times h \\times h}$, and $A_{(\\cdotp,i,j)}\\in \\mathbb{R}^{d}$, $(i,j = 1,2,\\cdots,h)$ is the activating vector on the location $(i,j)$ of the $h\\times h$ input tensor. To obtain the values $v^{\\ast}\\in \\mathbb{R}^{K}$ on the location $(i,j)$ of $K$ new maps, we only need to solve the following LLC problem:\n\n", "itemtype": "equation", "pos": 18924, "prevtext": "\n{\\color{black}{where $|\\cdotp|$ indicates calculating the area of the input box.}}\n\n", "index": 9, "text": "\\begin{equation}\nW_{F}(i,j) = \\exp(-\\dfrac{\\|f_{i}-f_{j}\\|^{2}_{2}}{2\\sigma^{2}}),\n\\label{Eq5}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"W_{F}(i,j)=\\exp(-\\dfrac{\\|f_{i}-f_{j}\\|^{2}_{2}}{2\\sigma^{2}}),\" display=\"block\"><mrow><mrow><mrow><msub><mi>W</mi><mi>F</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mfrac><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>-</mo><msub><mi>f</mi><mi>j</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\nwhere $D$ can be taken as $D_{cs}$ or $D_{cm}$, $\\odot$ denotes the element-wise multiplication, and $dist = [\\exp(\\|v-d_{1}\\|^{2}/\\tau),\\exp(\\|v-d_{2}\\|_{2}^{2}/\\tau),\\cdots,\\exp(\\|v-d_{K}\\|_{2}^{2}/\\tau)]\\in \\mathbb{R}^{K}$ is the adaptive vector between $v$ and each dictionary element, which preserves the locality between $v$ and the dictionary $D$. In this paper, we utilize the approximated LLC~\\cite{wang2010locality} for the fast calculation of $v^{\\ast}$.\n\nThe flowchart of generating the MLR given the part dictionary $D_{cs}$ or $D_{cm}$ is shown in Fig.~\\ref{fig2}.\n\n\\begin{algorithm}[t!]\n\t\\scriptsize{\n\t\t\\caption{Extracting of Hybrid Representation}\n\t\t\\small\n\t\t\\label{alg2}\n\t\t\\begin{algorithmic}[1]\n\t\t\t\\REQUIRE\n\t\t\t{Training~(source) images: $\\{I_{i}\\}_{i = 1}^{m}$} and test~(target) images: $\\{I_{i}\\}_{i = m+1}^{n}$. A CNN model with or without fine-tuning.\n\t\t\t\\ENSURE{Hybrid Representation $\\{H_{i}\\}_{i = 1}^{n}$ for each training and test images.}\n\t\t\t\\REQUIRE\n\t\t\t\\FOR{$i = 1\\rightarrow n$}\n\t\t\t\\STATE $\\textbf{Selective search}$ on image $I_{i}$, obtain part set $B = [B_{1},B_{2},\\cdots,B_{n_{I}}]\\in \\mathbb{R}^{4\\times n_{I}}$. \n\t\t\t\\STATE $\\textbf{Graph construction}$ based on Eq.~\\ref{Eq3},~\\ref{Eq4},~\\ref{Eq5}, obtain graph $G$ with weight as $W$.\n\t\t\t\\STATE $\\textbf{Spectral clustering}$ on $G$, obtain $Q$ clusters.\n\t\t\t\\STATE $\\textbf{Sort}$ the $Q$ clusters by the number of boxes contained in each cluster.\n\t\t\t\\STATE $\\textbf{Select}$ the top $T$ clusters, and randomly take one box from each of the $T$ cluster.  \n\t\t\t\\STATE $\\textbf{K-means clustering}$ on the selected parts, generate class-specific or class-mixture dictionary. \n\t\t\t\\STATE $\\textbf{LLC coding~(Eq.~\\ref{Eq6}) and SPM}$, generate the $\\textbf{MLR}_{i}$\t\t\n\t\t\t\\STATE $\\textbf{Fisher vector coding}$~(Eq.~\\ref{Eq2}) on the last convolutional layer of CNN, obtain $\\textbf{CFV}_{i}$.\n\t\t\t\n\t\t\t\\STATE $\\textbf{Fully connected representations}$, $\\textbf{FCR1}_{i}$ and $\\textbf{FCR2}_{i}$ \\\\\n\t\t\t\\STATE $H_{i} = [\\textbf{MLR}_{i},\\textbf{CFV}_{i},\\textbf{FCR1}_{i},\\textbf{FCR2}_{i}]$\n\t\t\t\\ENDFOR\n\t\t\\end{algorithmic}\n\t\t\\label{Algorithm1}\n\t}\n\\end{algorithm}\n\\subsection{CFV: Convolutional Fisher Vector}\n In this part, we briefly describe the construction of the Fisher vector~\\cite{perronnin2007fisher} based on the last convolutional layer of CNN. Like~\\cite{perronnin2007fisher},~\\cite{gong2014multi},~\\cite{cimpoi2014deep}, and~\\cite{yoo2014fisher}, we also use multiple scales as input while constructing CFVs.\n\n Moreover, considering the different scale information, we also calculate CFVs for each scale followed by $L2$ normalization and max-pooling. This strategy is known as Multi-scale Pyramid Pooling~\\cite{yoo2014fisher}. The difference between our method and~\\cite{yoo2014fisher} is that for sampling descriptors for GMM~\\cite{reynolds2000speaker} training before Fisher vector coding, we also consider scale information (i.e., the number of sampled descriptors is proportional to that of the total descriptors under different scales in each image). Here the descriptors from the last convolutional layer are without ReLU~\\cite{krizhevsky2012imagenet} throughout our paper.\n\n To improve the performance, power and $L2$ normalization are further applied to the max-pooled representation, which generates the final CFV representation.\n \n\\subsection{FCR: CNN Features from Well-Tuned Networks}\nGiven the well-trained CNN model~(with or without fine-tuning, such as AlexNet and VGG Net), we can use it for other different visual tasks, which is termed as parameter transfer learning. The fully connected representation (FCR) of the resized input images are extracted by forward propagating them until the fully connected layers of the network. We denote FCR1 and FCR2~(Fig.~\\ref{fig1}) as the penultimate and the last fully connected layer representation with ReLU~\\cite{krizhevsky2012imagenet}, respectively. Under this definition, our MLR is constructed based on FCR2 and local discriminative part (spectral clustering) on each image. \n\n{\\color{black}{Before being fed into SVM classifier for training, $L$2 normalization is also applied to the FCRs.}}\n\n\n\\section{Experiments}\nIn this section, we show the classification accuracy of our hybrid representation in two applications, i.e., scene recognition and visual domain adaptation, compared with state-of-the-art models, including traditional and CNN based ones. We first introduce the datasets and experimental settings, then report experimental results on each dataset, after that we give an analysis of the key parameters followed by the analysis of the complementary ability of the proposed hybrid representation with the representations from other Nets, and finally we analysis the time complexity of calculating our representation. \n\n\\subsection{Datasets and Experimental Settings}\nFor scene recognition, we utilize three trained CNN models, i.e., AlexNet~\\cite{krizhevsky2012imagenet}, VGG-19 net~\\cite{SimonyanZ14a}, GoogLeNet~\\cite{szegedy2014going}, and VGG-11~\\cite{WangGHQ15} net based on their caffe implementations~\\cite{jia2013caffe}. Specifically, AlexNet and VGG-19 net are trained on ImageNet database, and these two nets are used to evaluate our hybrid representation system. GoogLeNet and VGG-11 net are trained on Place205 database, which are used to validate the complementary ability of our hybrid representation w.r.t. the representations from other nets. For domain adaptation, we use the AlexNet only, so that we can compare with other CNN based methods fairly. In our experiments, we resize all the images into resolution of $256\\times256$ before the following operations, such as selective search and Fisher vector extraction. After obtaining the hybrid representation, we use linear SVM to train the classifier in all our experiments. The databases and experimental details are as follows:\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.40\\textwidth]{Fig3.pdf}\n\t\\caption{The sample images from different databases. (a) Images from MIT Indoor-67 database. It can be seen that categories ``movie theater\", ``meeting room\" and ``classroom\" are very similar and difficult to distinguish. (b)~Images from SUN-397 database. It includes outdoor scenes, e.g. ``alley\" and ``campus\", and indoor scenes, e.g. ``florist shop\". (c) Images from ``dest-top\" category in Office database.  It contains 3 sub-sets, i.e., Amazon, Dslr, and Webcam. Best viewed in color.}\n\t\\label{fig3}\n\t\n\\end{figure}\n\n\\subsubsection{$\\textbf{MIT Indoor-67 database}$}\nMIT Indoor-67 database~\\cite{quattoni2009recognizing} is a popular indoor scene database, including 15,620 images of 67 indoor scenes. It is difficult to distinguish different classes, because the categories are all indoor scenes, and  inter-class variance between different classes is very little. We follow the same training-test partition as~\\cite{quattoni2009recognizing} by using approximately 80 images from each class for training and 20 for testing. The average class accuracy is reported in our paper. Sample images of Indoor-67 can be found in Fig.~\\ref{fig3}~(a). It can be seen from Fig.~\\ref{fig3}~(a) that the categories of mover theater, meeting room and classroom are very similar.\n\\subsubsection{$\\textbf{SUN-397 database}$}\nSUN-397~\\cite{xiao2010sun} is a large-scale scene recognition database, containing 397 categories and with scenes varying from abbey to zoo, including both indoor and outdoor scenes. At least 100 images are contained in each category. We use the publicly available training-test partitions, and report the average accuracy and standard errors for all the partitions. We use 50 images from each class for training and 50 for testing. Sample images can be found in Fig.~\\ref{fig3}~(b).\n\\subsubsection{$\\textbf{Office database}$}\nOffice dataset~\\cite{saenko2010adapting} contains three sub-datasets from different domains, i.e., Amazon, Webcam,  and Dslr. Each sub-dataset is from a separate domain: images from Amazon are collected from online catalogs of amazon.com, and images in Dslr and Webcam are obtained in the daily office environment by a digital SLR camera and a webcam with high and low resolutions, respectively. There are totally 31 categories which are common for these three domains, and the number of images per category per domain ranges from 8 to 100, and totally 4,652 images are included. Fig.~\\ref{fig3}~(c) shows the domain shift between different domains.\n\\subsubsection{$\\textbf{Details for domain adaptation}$}\nIn domain adaptation, if the training data~(source domain) with labels and the test data~(target domain) without labels are given, it is called unsupervised domain adaptation; if a source domain with labels and a target domain with a small amount of labeled data are given, then the problem is denoted as semi-supervised domain adaptation. We will perform both unsupervised and semi-supervised domain adaptation on all the 31 categories in our experiments. We adopt the standard experimental setup presented in~\\cite{saenko2010adapting}. We use 20 source examples per category when Amazon is taken as  the source domain, and 8 images per category when Webcam or Dslr is taken as the source domain~\\cite{saenko2010adapting}. For the semi-supervised adaptation, three more labeled target examples per category are added into the source domain. We also try another setting presented in~\\cite{gong2013connecting},~\\cite{Menglong2015deep}, where we use all the source domain examples with labels for unsupervised adaptation and three more target domain samples per class for semi-supervised adaptation.\n\nWe evaluate our method across five random training-test partitions for each of the three domain transfer tasks commonly used for evaluation, i.e., Amazon$\\rightarrow$Webcam~($A\\rightarrow W$), Dslr$\\rightarrow$Webcam~($D\\rightarrow W$) and Webcam$\\rightarrow$Dslr~($W\\rightarrow D$), and report average accuracy and standard errors for each setting.\n\n{\\color{black}{Actually, given the CNN model, calculating the hybrid representation based on Algorithm~\\ref{Algorithm1} is already a domain~(parameter) transfer process. As for domain adaptation experiments, we run Algorithm~\\ref{Algorithm1} to generate hybrid representation for both source and target data, which is used for further domain transfer by training classifier on the source representation, and testing on the target one.}} Note that in domain adaptation, after obtaining the prototype boxes based on the first stage spectral clustering, we first carry out box filtering based on variance thresholding of the prototype boxes, followed by the second stage K-means clustering. The purpose of this trick is to filter out the box regions with low variance, which may be the surrounding background regions in the Office database. Specifically, given the prototype box~$I_{b}$, we calculate the variance of its gray scale image as follows:\n\n", "itemtype": "equation", "pos": 22839, "prevtext": "\nfor $i,j = 1,2,\\cdots,n_{I}$.\n\n{\\color{black}{After spectral clustering on the graph $G$, we obtain $Q$ clusters $\\mathcal{C}_{1},\\mathcal{C}_{2},\\cdots,\\mathcal{C}_{Q}$. Then we sort the $Q$ clusters in descending order according to the number of bounding boxes and select the top $T$~($T\\leq Q$) clusters.}} We randomly select one bounding box from each $\\mathcal{C}_{i},i=1,2,\\cdots,T$, thus totally $T$ bounding boxes are reserved. We further implement context padding~\\cite{girshick2014rich} on the selected $T$ bounding boxes and do forward propagation (based on CNN) again to get the new feature representation based on the context padded boxes. In this way, we can get $T$ representations $P = [P_{1},P_{2},\\cdots,P_{T}]$ for each image $I$, and we denote them as the prototypes of the corresponding image.\n\nFinally, for all the prototypes of the training images, we do K-means clustering~\\cite{kanungo2002efficient} to get the class-specific part dictionary and the class-mixture part dictionary, denoted as $D_{cs}\\in \\mathbb{R}^{d\\times K}$ and $D_{cm}\\in \\mathbb{R}^{d\\times K}$ respectively. Here the elements of $D_{cs}$ are obtained by first clustering the prototypes from each class and then concatenating them together. While $D_{cm}$ is based on clustering all the prototypes without considering the class information. The part dictionary contains local discriminative and multi-scale information of the training images, due to the selective search algorithm~(SSA) and constraints while running SSA.\n\nWith the part dictionary $D_{cs}$~($D_{cm}$) learned, we can consider it as a group of local discriminative filter banks. Motivated by Object-Bank~\\cite{li2010object}, given an input image at a single scale, we sample square regions at multiple scales densely, i.e., sampled squares with size $128\\times128$, $92\\times92$ and $64\\times64$ and step size 32 pixels for all three scales. Then we calculate the activates of the last fully connected layer of the trained CNN for all these squares under different scales, which generates three scaled activate tensors. This can also be seen as the local feature extraction on square regions. After that, for each scaled activate tensor of each image, we use the $D_{cs}$~($D_{cm}$) to operate with the activates on each location of the tensors, resulting in $K$ new feature maps for each scaled activate tensor, which can be seen as another kind of convolutional operation based on the part dictionary~\\cite{kavukcuoglu2010learning}. For every $K$ feature maps under different scales, we further apply spatial pyramid matching (SPM)~\\cite{lazebnik2006beyond} to divide the map region into spatial cells in three levels, i.e., $1\\times1$, $2\\times2$, $3\\times1$, and do max-pooling on each cell. The final MLR is the concatenation of all the max-pooled features, with the dimension being $3\\times K\\times(1\\times1+2\\times2+3\\times1)$.\n\nMany methods can be adopted as the strategy of operation between the part dictionary $D_{cs}$~($D_{cm}$) and the activates of local square regions, e.g., inner production, sparse coding~\\cite{yang2009linear}, locality-constrained linear coding~(LLC)~\\cite{wang2010locality}, and auto-encoder based coding~\\cite{Xie2014ACCV}. There are detailed coding speed comparisons in~\\cite{Xie2014ACCV}. Here we utilize the LLC for feature coding, due to its relative fast speed and locality-preserve property. \n\nSpecifically, denote the activating tensor of the image $I$ under one scale by $A\\in \\mathbb{R}^{d\\times h \\times h}$, and $A_{(\\cdotp,i,j)}\\in \\mathbb{R}^{d}$, $(i,j = 1,2,\\cdots,h)$ is the activating vector on the location $(i,j)$ of the $h\\times h$ input tensor. To obtain the values $v^{\\ast}\\in \\mathbb{R}^{K}$ on the location $(i,j)$ of $K$ new maps, we only need to solve the following LLC problem:\n\n", "index": 11, "text": "\\begin{equation}\nv^{\\ast} = \\arg\\min_{v} \\|A_{(\\cdotp,i,j)}-Dv\\|_{2}^{2}+\\lambda\\|dist\\odot v\\|_{2}^{2},\n\\label{Eq6}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"v^{\\ast}=\\arg\\min_{v}\\|A_{(\\cdotp,i,j)}-Dv\\|_{2}^{2}+\\lambda\\|dist\\odot v\\|_{2%&#10;}^{2},\" display=\"block\"><mrow><mrow><msup><mi>v</mi><mo>\u2217</mo></msup><mo>=</mo><mrow><mrow><mrow><mi>arg</mi><mo>\u2061</mo><munder><mi>min</mi><mi>v</mi></munder></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mo>\u22c5</mo><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>-</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>\u2299</mo><mi>v</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "itemtype": "equation", "pos": 33868, "prevtext": "\nwhere $D$ can be taken as $D_{cs}$ or $D_{cm}$, $\\odot$ denotes the element-wise multiplication, and $dist = [\\exp(\\|v-d_{1}\\|^{2}/\\tau),\\exp(\\|v-d_{2}\\|_{2}^{2}/\\tau),\\cdots,\\exp(\\|v-d_{K}\\|_{2}^{2}/\\tau)]\\in \\mathbb{R}^{K}$ is the adaptive vector between $v$ and each dictionary element, which preserves the locality between $v$ and the dictionary $D$. In this paper, we utilize the approximated LLC~\\cite{wang2010locality} for the fast calculation of $v^{\\ast}$.\n\nThe flowchart of generating the MLR given the part dictionary $D_{cs}$ or $D_{cm}$ is shown in Fig.~\\ref{fig2}.\n\n\\begin{algorithm}[t!]\n\t\\scriptsize{\n\t\t\\caption{Extracting of Hybrid Representation}\n\t\t\\small\n\t\t\\label{alg2}\n\t\t\\begin{algorithmic}[1]\n\t\t\t\\REQUIRE\n\t\t\t{Training~(source) images: $\\{I_{i}\\}_{i = 1}^{m}$} and test~(target) images: $\\{I_{i}\\}_{i = m+1}^{n}$. A CNN model with or without fine-tuning.\n\t\t\t\\ENSURE{Hybrid Representation $\\{H_{i}\\}_{i = 1}^{n}$ for each training and test images.}\n\t\t\t\\REQUIRE\n\t\t\t\\FOR{$i = 1\\rightarrow n$}\n\t\t\t\\STATE $\\textbf{Selective search}$ on image $I_{i}$, obtain part set $B = [B_{1},B_{2},\\cdots,B_{n_{I}}]\\in \\mathbb{R}^{4\\times n_{I}}$. \n\t\t\t\\STATE $\\textbf{Graph construction}$ based on Eq.~\\ref{Eq3},~\\ref{Eq4},~\\ref{Eq5}, obtain graph $G$ with weight as $W$.\n\t\t\t\\STATE $\\textbf{Spectral clustering}$ on $G$, obtain $Q$ clusters.\n\t\t\t\\STATE $\\textbf{Sort}$ the $Q$ clusters by the number of boxes contained in each cluster.\n\t\t\t\\STATE $\\textbf{Select}$ the top $T$ clusters, and randomly take one box from each of the $T$ cluster.  \n\t\t\t\\STATE $\\textbf{K-means clustering}$ on the selected parts, generate class-specific or class-mixture dictionary. \n\t\t\t\\STATE $\\textbf{LLC coding~(Eq.~\\ref{Eq6}) and SPM}$, generate the $\\textbf{MLR}_{i}$\t\t\n\t\t\t\\STATE $\\textbf{Fisher vector coding}$~(Eq.~\\ref{Eq2}) on the last convolutional layer of CNN, obtain $\\textbf{CFV}_{i}$.\n\t\t\t\n\t\t\t\\STATE $\\textbf{Fully connected representations}$, $\\textbf{FCR1}_{i}$ and $\\textbf{FCR2}_{i}$ \\\\\n\t\t\t\\STATE $H_{i} = [\\textbf{MLR}_{i},\\textbf{CFV}_{i},\\textbf{FCR1}_{i},\\textbf{FCR2}_{i}]$\n\t\t\t\\ENDFOR\n\t\t\\end{algorithmic}\n\t\t\\label{Algorithm1}\n\t}\n\\end{algorithm}\n\\subsection{CFV: Convolutional Fisher Vector}\n In this part, we briefly describe the construction of the Fisher vector~\\cite{perronnin2007fisher} based on the last convolutional layer of CNN. Like~\\cite{perronnin2007fisher},~\\cite{gong2014multi},~\\cite{cimpoi2014deep}, and~\\cite{yoo2014fisher}, we also use multiple scales as input while constructing CFVs.\n\n Moreover, considering the different scale information, we also calculate CFVs for each scale followed by $L2$ normalization and max-pooling. This strategy is known as Multi-scale Pyramid Pooling~\\cite{yoo2014fisher}. The difference between our method and~\\cite{yoo2014fisher} is that for sampling descriptors for GMM~\\cite{reynolds2000speaker} training before Fisher vector coding, we also consider scale information (i.e., the number of sampled descriptors is proportional to that of the total descriptors under different scales in each image). Here the descriptors from the last convolutional layer are without ReLU~\\cite{krizhevsky2012imagenet} throughout our paper.\n\n To improve the performance, power and $L2$ normalization are further applied to the max-pooled representation, which generates the final CFV representation.\n \n\\subsection{FCR: CNN Features from Well-Tuned Networks}\nGiven the well-trained CNN model~(with or without fine-tuning, such as AlexNet and VGG Net), we can use it for other different visual tasks, which is termed as parameter transfer learning. The fully connected representation (FCR) of the resized input images are extracted by forward propagating them until the fully connected layers of the network. We denote FCR1 and FCR2~(Fig.~\\ref{fig1}) as the penultimate and the last fully connected layer representation with ReLU~\\cite{krizhevsky2012imagenet}, respectively. Under this definition, our MLR is constructed based on FCR2 and local discriminative part (spectral clustering) on each image. \n\n{\\color{black}{Before being fed into SVM classifier for training, $L$2 normalization is also applied to the FCRs.}}\n\n\n\\section{Experiments}\nIn this section, we show the classification accuracy of our hybrid representation in two applications, i.e., scene recognition and visual domain adaptation, compared with state-of-the-art models, including traditional and CNN based ones. We first introduce the datasets and experimental settings, then report experimental results on each dataset, after that we give an analysis of the key parameters followed by the analysis of the complementary ability of the proposed hybrid representation with the representations from other Nets, and finally we analysis the time complexity of calculating our representation. \n\n\\subsection{Datasets and Experimental Settings}\nFor scene recognition, we utilize three trained CNN models, i.e., AlexNet~\\cite{krizhevsky2012imagenet}, VGG-19 net~\\cite{SimonyanZ14a}, GoogLeNet~\\cite{szegedy2014going}, and VGG-11~\\cite{WangGHQ15} net based on their caffe implementations~\\cite{jia2013caffe}. Specifically, AlexNet and VGG-19 net are trained on ImageNet database, and these two nets are used to evaluate our hybrid representation system. GoogLeNet and VGG-11 net are trained on Place205 database, which are used to validate the complementary ability of our hybrid representation w.r.t. the representations from other nets. For domain adaptation, we use the AlexNet only, so that we can compare with other CNN based methods fairly. In our experiments, we resize all the images into resolution of $256\\times256$ before the following operations, such as selective search and Fisher vector extraction. After obtaining the hybrid representation, we use linear SVM to train the classifier in all our experiments. The databases and experimental details are as follows:\n\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.40\\textwidth]{Fig3.pdf}\n\t\\caption{The sample images from different databases. (a) Images from MIT Indoor-67 database. It can be seen that categories ``movie theater\", ``meeting room\" and ``classroom\" are very similar and difficult to distinguish. (b)~Images from SUN-397 database. It includes outdoor scenes, e.g. ``alley\" and ``campus\", and indoor scenes, e.g. ``florist shop\". (c) Images from ``dest-top\" category in Office database.  It contains 3 sub-sets, i.e., Amazon, Dslr, and Webcam. Best viewed in color.}\n\t\\label{fig3}\n\t\n\\end{figure}\n\n\\subsubsection{$\\textbf{MIT Indoor-67 database}$}\nMIT Indoor-67 database~\\cite{quattoni2009recognizing} is a popular indoor scene database, including 15,620 images of 67 indoor scenes. It is difficult to distinguish different classes, because the categories are all indoor scenes, and  inter-class variance between different classes is very little. We follow the same training-test partition as~\\cite{quattoni2009recognizing} by using approximately 80 images from each class for training and 20 for testing. The average class accuracy is reported in our paper. Sample images of Indoor-67 can be found in Fig.~\\ref{fig3}~(a). It can be seen from Fig.~\\ref{fig3}~(a) that the categories of mover theater, meeting room and classroom are very similar.\n\\subsubsection{$\\textbf{SUN-397 database}$}\nSUN-397~\\cite{xiao2010sun} is a large-scale scene recognition database, containing 397 categories and with scenes varying from abbey to zoo, including both indoor and outdoor scenes. At least 100 images are contained in each category. We use the publicly available training-test partitions, and report the average accuracy and standard errors for all the partitions. We use 50 images from each class for training and 50 for testing. Sample images can be found in Fig.~\\ref{fig3}~(b).\n\\subsubsection{$\\textbf{Office database}$}\nOffice dataset~\\cite{saenko2010adapting} contains three sub-datasets from different domains, i.e., Amazon, Webcam,  and Dslr. Each sub-dataset is from a separate domain: images from Amazon are collected from online catalogs of amazon.com, and images in Dslr and Webcam are obtained in the daily office environment by a digital SLR camera and a webcam with high and low resolutions, respectively. There are totally 31 categories which are common for these three domains, and the number of images per category per domain ranges from 8 to 100, and totally 4,652 images are included. Fig.~\\ref{fig3}~(c) shows the domain shift between different domains.\n\\subsubsection{$\\textbf{Details for domain adaptation}$}\nIn domain adaptation, if the training data~(source domain) with labels and the test data~(target domain) without labels are given, it is called unsupervised domain adaptation; if a source domain with labels and a target domain with a small amount of labeled data are given, then the problem is denoted as semi-supervised domain adaptation. We will perform both unsupervised and semi-supervised domain adaptation on all the 31 categories in our experiments. We adopt the standard experimental setup presented in~\\cite{saenko2010adapting}. We use 20 source examples per category when Amazon is taken as  the source domain, and 8 images per category when Webcam or Dslr is taken as the source domain~\\cite{saenko2010adapting}. For the semi-supervised adaptation, three more labeled target examples per category are added into the source domain. We also try another setting presented in~\\cite{gong2013connecting},~\\cite{Menglong2015deep}, where we use all the source domain examples with labels for unsupervised adaptation and three more target domain samples per class for semi-supervised adaptation.\n\nWe evaluate our method across five random training-test partitions for each of the three domain transfer tasks commonly used for evaluation, i.e., Amazon$\\rightarrow$Webcam~($A\\rightarrow W$), Dslr$\\rightarrow$Webcam~($D\\rightarrow W$) and Webcam$\\rightarrow$Dslr~($W\\rightarrow D$), and report average accuracy and standard errors for each setting.\n\n{\\color{black}{Actually, given the CNN model, calculating the hybrid representation based on Algorithm~\\ref{Algorithm1} is already a domain~(parameter) transfer process. As for domain adaptation experiments, we run Algorithm~\\ref{Algorithm1} to generate hybrid representation for both source and target data, which is used for further domain transfer by training classifier on the source representation, and testing on the target one.}} Note that in domain adaptation, after obtaining the prototype boxes based on the first stage spectral clustering, we first carry out box filtering based on variance thresholding of the prototype boxes, followed by the second stage K-means clustering. The purpose of this trick is to filter out the box regions with low variance, which may be the surrounding background regions in the Office database. Specifically, given the prototype box~$I_{b}$, we calculate the variance of its gray scale image as follows:\n\n", "index": 13, "text": "\\begin{equation}\nVAR = var(gray(I_{b})).\n\\label{Eq7}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"VAR=var(gray(I_{b})).\" display=\"block\"><mrow><mrow><mrow><mi>V</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mrow><mi>v</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>I</mi><mi>b</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}, {"file": "1601.07977.tex", "nexttext": "\\textbf{0.24}$ \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w &$\\textbf{57.31}\n", "itemtype": "equation", "pos": -1, "prevtext": "\nThen if $VAR < 125$, we delete the prototype. Fig.~\\ref{fig4} illustrates the deleted prototype boxes based on our strategy.\n\\subsubsection{$\\textbf{Parameter settings}$} \n{\\color{black}In Algorithm~\\ref{Algorithm1}, there are several key parameters, we give a detailed description of these parameters. As for graph constructing, the parameter $(\\lambda_{B},\\lambda_{F})$ in~Eq.~\\ref{Eq3} is set as $(1,0)$ for Indoor-67 database, and $(0.5,0.5)$ for SUN397 and Office databases~\\footnote{ Without considering the feature space information on Indoor-67~($\\lambda_{F}=0$) is based on the observations: the layout in Images of Indoor-67 is almost dense, while the layouts in images of SUN-397 and Office are sparse, i.e., some sub-regions are the same, e.g. the sky in the campus category of SUN-397, and the background of ruler category in Office dataset.}. $\\sigma$ in Eq.~\\ref{Eq5} is set as $1$. The number $G$ of clusters for spectral clustering is set as $10$, and the number of top ranking $T$ clusters is set as $5$, which is equal to the number of boxes selected from each image. As for Fisher vector coding, the Gaussian components of GMM training is fixed as $64$, and we use multiple scales in our paper. Specifically, given the CNN input size of  $L\\times L$ for the image $I$, the used five scales are $L \\times \\sqrt{2}^{[0\\, 1\\, 2\\, 3\\, 4]} = [L\\; \\sqrt{2}L\\; 2L\\; 2\\sqrt{2}L\\; 4L]$~\\footnote{Note that for Indoor-67 experiment under VGG-19 net, we use the 10 scales the same as~\\cite{cimpoi2014deep}, i.e., $L \\times \\sqrt{2}^{[-6\\,-5\\,-4\\,-3\\,-2\\,-1\\,0\\, 1\\, 2\\, 3]}$   to reproduce their results.}. As for FCR1 and FCR2, we extract features with the whole image as input for Indoor-67 and SUN-397, and extract features with the central cropped sub-region as input for Office. The SVM parameter $C$ is fixed as $1$ in all our experiments.}\n\\subsubsection{$\\textbf{Abbreviation in the experiments}$} \nTo make some key definition clear, we list these abbreviation in Table~\\ref{table1}.\n \\begin{table}[!t]\n \t\n \t\n \t\n \t\\caption{Abbreviation in this paper.}\n \t\\label{table_example}\n \t\\centering\n \t\n \t\n \t\\scriptsize{\\begin{tabular}{|c|c|}\n \t\t\\hline\n \t\t\\textbf{Abbreviation}& Description\\\\\n \t\t\\hline\n \t\tFCR1-c & FCR1 based on central crop as input  \\\\\n \t\t\\hline\n \t\tFCR1-w & FCR1 based on whole image as input  \\\\\n \t\t\\hline\n  \t\tFCR2-c & FCR2 based on central crop as input  \\\\\n  \t\t\\hline\n  \t\tFCR2-w & FCR2 based on whole image as input  \\\\\n  \t\t\\hline\n  \t\tCM & Construct MLR based on class-mixture dictionary  \\\\\n  \t\t\\hline\t\t\n  \t\tCS & Construct MLR based on class-specific dictionary  \\\\\n  \t\t\\hline \n  \t\tG\\_P205 & $\\begin{aligned}\n  \t\t\\text{The average pooled representation before loss3} \\\\\n  \t\t\\text{of GoogLeNet~(trained on Place205)}\n  \t\t\\end{aligned}$ \\\\\n  \t\t\\hline\n  \t\tVGG-11\\_P205 & FCR1-w of VGG-11 trained on Place205   \\\\\n  \t\t\\hline \n  \t\tVGG-19\\_Hybrid & FCR1-w+CFV+MLR based on VGG-19 trained on ImageNet  \\\\\n  \t\t\\hline\n \t\\end{tabular}}\n \t\t\\vspace{-0.5cm}\n \t\t\\label{table1}\n \\end{table}\n \n\n\\begin{figure*}[t!]\n\t\\centering\n\t\\includegraphics[width=0.85\\textwidth]{Fig4.pdf}\n\t\\caption{Examples of original images~(first column in both (a) Webcam and (b) Dslr domains) and their corresponding prototype boxes~(2nd-6th columns) based on our proposed first stage spectral clustering. The boxes are resized to fit CNN input. Red dashed boxes are removed ones based on Eq.~\\ref{Eq7}.  Best viewed in color.}\n\t\\label{fig4}\n\t\\vspace{-0.5cm}\n\\end{figure*}\n\n\n\n\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparison of classification rate on MIT Indoor-67 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods} & \\multicolumn{2}{c|}{Accuracy~(\\%)} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tROI~\\cite{quattoni2009recognizing} & \\multicolumn{2}{c|}{26.05}\\\\\n\t\t\t\\hline\n\t\t\tMM-Scene~\\cite{zhu2010large} & \\multicolumn{2}{c|}{28.00}\\\\\n\t\t\t\\hline\n\t\t\tDPM~\\cite{pandey2011scene} &  \\multicolumn{2}{c|}{30.40}\\\\\n\t\t\t\\hline\n\t\t\tCENTRIST~\\cite{wu2011centrist} &  \\multicolumn{2}{c|}{36.90}\\\\\n\t\t\t\\hline\n\t\t\tObject Bank~\\cite{li2010object} &  \\multicolumn{2}{c|}{37.60}\\\\\n\t\t\t\\hline\n\t\t\tRBOW~\\cite{parizi2012reconfigurable} &  \\multicolumn{2}{c|}{37.93}\\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Patches~\\cite{singh2012unsupervised} &  \\multicolumn{2}{c|}{38.10} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{39.80} \\\\\n\t\t\t\\hline\n\t\t\tLPR-LIN~\\cite{sadeghi2012latent} &  \\multicolumn{2}{c|}{44.84}\\\\\n\t\t\t\\hline\n\t\t\tBOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{46.10} \\\\\n\t\t\t\\hline\n\t\t\tMI-SVM~\\cite{li2013harvesting} &  \\multicolumn{2}{c|}{46.40} \\\\\n\t\t\t\\hline\n\t\t\tHybrid parts+GIST-color+SP~\\cite{zheng2012learning} &  \\multicolumn{2}{c|}{47.20} \\\\\n\t\t\t\\hline\n\t\t\tISPR~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{50.10} \\\\\n\t\t\t\\hline\n\t\t\tMMDL~\\cite{wang2013max} &  \\multicolumn{2}{c|}{50.15} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Parts~\\cite{sun2013learning} &  \\multicolumn{2}{c|}{51.40}\\\\\n\t\t\t\\hline\n\t\t\tDSFL~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{52.24} \\\\\n\t\t\t\\hline\n\t\t\tDiscriminative Lie Group~\\cite{ludiscriminative} &  \\multicolumn{2}{c|}{55.58} \\\\\n\t\t\t\\hline\n\t\t\tIFV~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{60.77} \\\\\n\t\t\t\\hline\n\t\t\tIFV+BOP~\\cite{juneja2013blocks} &  \\multicolumn{2}{c|}{63.10} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{64.03} \\\\\n\t\t\t\\hline\n\t\t\tMode-Seeking + IFV~\\cite{doersch2013mid} &  \\multicolumn{2}{c|}{66.87} \\\\\n\t\t\t\\hline\n\t\t\tISPR+IFV~\\cite{lin2014learning} &  \\multicolumn{2}{c|}{68.50} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2~(placeNet)~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{68.24} \\\\\n\t\t\t\\hline\n\t\t\tOrder-less Pooling~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{68.90} \\\\\n\t\t\t\\hline\n\t\t\tCNNaug-SVM~\\cite{razavian2014cnn} &  \\multicolumn{2}{c|}{69.00} \\\\\n\t\t\t\\hline\n\t\t\tFCR2~HybridNet~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{70.80} \\\\\n\t\t\t\\hline\n\t\t\tURDL+CNNaug~\\cite{Liu2014ACCV} &  \\multicolumn{2}{c|}{71.90} \\\\\n\t\t\t\\hline\n\t\t\tMPP-FCR2~(7 scales)~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{75.67} \\\\\n\t\t\t\\hline\n\t\t\tDSFL+CNN~\\cite{zuo2014learning} &  \\multicolumn{2}{c|}{76.23} \\\\\n\t\t\t\\hline\n\t\t\tMPP+DSFL~\\cite{yoo2014fisher} &  \\multicolumn{2}{c|}{80.78} \\\\\n\t\t\t\\hline\n\t\t\tCFV~(VGG-19)~\\cite{cimpoi2014deep} &  \\multicolumn{2}{c|}{81.00} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR1-c & 59.71 & 72.96\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 61.63 & 71.48\\\\\n\t\t\t\\hline\n\t\t\tFCR2-c & 59.48 & 70.23\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w & 61.01 & 68.71\\\\\n\t\t\t\\hline\n\t\t\tFCR2-w~(fine-tuning) & 64.78 & $-$\\\\\n\t\t\t\\hline\n\t\t\tMLR & 69.33 & 77.51\\\\\n\t\t\t\\hline\n\t\t\tCFV & 68.68 & 81.05\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 62.43 & 70.59\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w & 70.20 & 77.94 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 71.27 & 80.75\\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w & 70.66 & 78.81\\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 70.94 & 81.60\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 72.80 & 81.47 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & $\\textbf{74.09}$ & $ \\textbf{82.24}$\\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR2-w & 73.17 & 81.57 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w+FCR2-w & 70.72 & 79.70 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w+FCR2-w & 69.57 & 78.34  \\\\\n\t\t\t\\hline \n\t\t\tMLR+CFV+FCR1-w+FCR2-w & 72.98 & 80.91  \\\\\n\t\t\t\\hline     \n\t\t\\end{tabular}}\n\t\t\\vspace{-0.5cm}\n        \\label{table2}\n\t\\end{table}\n\n\n\\subsection{MIT Indoor-67 Experiments}\n\nIn this Subsection, we report and analyze the experimental results on the MIT Indoor-67 database. We first show the changing tendency of the classification rate under different part dictionary sizes, in the class-mixture and the class-specific manner respectively, without fine-tuning the CNN. The part dictionary size can be seen as the number of representative parts per category multiplying the total category number (see Fig.~\\ref{fig5}-\\ref{fig7} for details).\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig5.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-mixture dictionary $D_{cm}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig5}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig6.pdf}\n\t\\caption{Changing tendency of classification rate with different part element numbers of class-specific dictionary $D_{cs}$ on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig6}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nIn both Fig.~\\ref{fig5} and Fig.~\\ref{fig6}, ``0\" on the x-axis means that representations FCR1-w, FCR2-w, CFV, FCR1-w+FCR2-w, CFV+FCR2-w, CFV+FCR1-w and CFV+FCR1-w+FCR2-w are not generated based on the part dictionary. The detailed meaning of FCR1-w and FCR2-w are listed in Table~\\ref{table1}. From Fig.~\\ref{fig5} and~\\ref{fig6}, it can be seen that our part dictionary is much better than that of Liu et al.~\\cite{Liu2014ACCV} in the classification rate under different part dictionary sizes, and the hybrid representations combined with MLR are much better than their counterpart representations. From Fig.~\\ref{fig7},  a conclusion can be drawn that class-specific dictionaries are always better than class-mixture ones. Moreover, learning a class-specific dictionary is much faster than learning a class-mixture one based on K-means, thus the best choice of the second stage clustering is the class-specific part dictionary.\n\n\nTo better evaluate the representation ability of our proposed MLR, we also compare the classification rate with or without fine-tuning based on AlexNet~\\cite{krizhevsky2012imagenet}. We use the AlexNet with the architecture of Caffe's implementation~\\cite{jia2013caffe}, which contains five convolutional layers, two fully connected layers and one output layer with a node number equal to the number of categories (i.e., the output number of nodes is $67$ for Indoor-67 database). We first fine-tune AlexNet with all the global training images~(resized to $256\\times 256$) of Indoor-67. The initialization of the parameters of the first seven layers is the same as the model trained on ImageNet database, and the parameters of the last layer are randomly initialized with Gaussian distribution. The learning rates of the first seven layers and the last fully-connected layer are initialized as\n0.001 and 0.01 respectively, and reduced to one tenth of the current rates after fixed iterations~(10,000 in our experiments). By setting a larger learning rate for the last layer, due to the random initialization of this layer, we hope the parameters of this layer can be updated to be more suitable for the new task-specific database. For the previous layers of AlexNet, we hope the parameters change as little as possible to preserve the already learned texture and shape information during the learning based on ImageNet database. Then based on our fine-tuned task-specific model, we further fine-tune the AlexNet based on the prototype bounding boxes~(with labels the same as the images where they are located on), which are extracted based on the first stage spatial and feature spectral clustering. \n  \nAn observation is that the classification rate of MLR based on fine-tuned AlexNet is comparable with their counterpart representation calculated based on AlexNet without fine-tuning. See Fig.~\\ref{fig8} for details. Therefore, we do not fine-tune our used CNN models in the later experiments.\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig7.pdf}\n\t\\caption{Comparison of changing tendency of classification rate under different part element numbers and different part dictionaries, i.e., Class-Mixture~(CM) and Class-Specific~(CS) on Indoor-67 database, based on AlexNet. Take MLR(CS) as an example. The accuracies from bottom to top are corresponding to the part element number that varies from $10\\times67$ to $70\\times67$. Best viewed in color.}\n\t\\label{fig7}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\\begin{figure}[t!]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{Fig8.pdf}\n\t\\caption{Comparison of classification rate~(with or without fine-tuning) under different part element numbers and different dictionaries on Indoor-67 database, based on AlexNet. Best viewed in color.}\n\t\\label{fig8}\n\t\t\\vspace{-0.5cm}\n\\end{figure}\n\n\nFinally, we list the classification accuracies on Indoor-67 and compare our hybrid methods with other state-of-the-art methods, both under AlexNet and VGG-19 networks. Table~\\ref{table2} gives the detailed accuracies of the traditional models, the CNN based models, and our hybrid representation based methods. Here the dictionary size for constructing MLR is 67$\\times$40 and 67$\\times$30 for AlexNet and VGG-19, respectively. It can be seen that our hybrid representation achieves the result of 82.24$\\%$ based on VGG-19 network, which has been the best result on Indoor-67 based on the net trained on ImageNet database. This justifies the effectiveness of combining CNN and dictionary-based features in improving the classification accuracy.\n\\begin{table}[!t]\n\t\n\t\n\t\n\t\\caption{Comparisons of classification rate on SUN-397 database.}\n\t\\label{table_example}\n\t\\centering\n\t\n\t\n\t\\scriptsize{\\begin{tabular}{|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Traditional Methods}&  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tS-manifold~\\cite{kwitt2012scene} &  \\multicolumn{2}{c|}{28.90} \\\\\n\t\t\t\\hline\n\t\t\tOTC~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{34.56} \\\\\n\t\t\t\\hline\n\t\t\tcontextBow+semantic~\\cite{su2012improving} &  \\multicolumn{2}{c|}{35.60} \\\\\n\t\t\t\\hline\n\t\t\tXiao et al.~\\cite{xiao2010sun} &  \\multicolumn{2}{c|}{38.00}\\\\\n\t\t\t\\hline\n\t\t\tFV~(SIFT + Local Color Statistic)~\\cite{sanchez2013image} &  \\multicolumn{2}{c|}{47.20}\\\\\n\t\t\t\\hline\n\t\t\tOTC + HOG2x2~\\cite{margolin2014otc} &  \\multicolumn{2}{c|}{49.60} \\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\textbf{CNN based Methods} &  \\multicolumn{2}{c|}{Accuracy~(\\%)}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tDecaf~\\cite{donahue2013decaf} &  \\multicolumn{2}{c|}{40.94} \\\\\n\t\t\t\\hline\n\t\t\tMOP-CNN~\\cite{gong2014multi} &  \\multicolumn{2}{c|}{51.98} \\\\\n\t\t\t\\hline\n\t\t\tHybrid-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{53.86$\\pm$0.21}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{54.32$\\pm$0.14}\\\\\n\t\t\t\\hline\n\t\t\tPlace-CNN ft~\\cite{zhou2014learning} &  \\multicolumn{2}{c|}{56.2}\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Our hybrid Methods~(CS)}} & \\multicolumn{2}{c|}{Accuracy~( \\%)} \\\\ \n\t\t\t\\cline{2-3}\n\t\t\t& AlexNet & VGG-19\\\\\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\tFCR2-w & 46.27$\\pm$0.37 & 52.78$\\pm$0.25\\\\\n\t\t\t\\hline\n\t\t\tFCR1-w & 46.42$\\pm$0.47 & 54.98$\\pm$0.10 \\\\\n\t\t\t\\hline\n\t\t\tMLR & 53.84$\\pm$0.16 & 61.09$\\pm$0.11 \\\\\n\t\t\t\\hline\n\t\t\tCFV & 52.30$\\pm$0.09 & 62.47$\\pm$0.41 \\\\\n\t\t\t\\hline\n\t\t\tFCR1-w+FCR2-w & 47.19$\\pm$0.53 & 54.51$\\pm$0.14  \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR2-w &  55.22$\\pm$0.34 & 62.09$\\pm$0.24 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR2-w & 54.65$\\pm$0.40 & 62.77$\\pm$0.21 \\\\\n\t\t\t\\hline\n\t\t\tMLR+FCR1-w &  55.17$\\pm$0.33 & 62.59$\\pm$0.14 \\\\\n\t\t\t\\hline\n\t\t\tCFV+FCR1-w & 54.34$\\pm$0.22 & 63.16$\\pm$0.29 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV & 56.66$\\pm$0.17 & 64.14$\\pm$0.32 \\\\\n\t\t\t\\hline\n\t\t\tMLR+CFV+FCR1-w & 57.15$\\pm$0.26& $\\textbf{64.53}\n", "index": 15, "text": "$$\\pm$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"block\"><mo>\u00b1</mo></math>", "type": "latex"}]