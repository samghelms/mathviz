[{"file": "1601.03623.tex", "nexttext": "\nwhere \n\n", "itemtype": "equation", "pos": 7236, "prevtext": "\n\\title{Evaluation of the Partitioned Global Address Space (PGAS) model for an inviscid Euler solver} \n\\author[uibk]{Martina Prugger\\corref{cor1}}\n\\ead{martina.prugger@uibk.ac.at}\n\\cortext[cor1]{Corresponding author}\n\\author[uibk]{Lukas Einkemmer} \n\\author[uibk]{Alexander Ostermann} \n \n\n\\address[uibk]{Department of Mathematics, University of Innsbruck, Austria} \n\n\\begin{abstract} In this paper we evaluate the performance of Unified Parallel C (which implements the partitioned global address space programming model) using a numerical method that is widely used in fluid dynamics. In order to evaluate the incremental approach to parallelization (which is possible with UPC) and its performance characteristics, we implement different levels of optimization of the UPC code and compare it with an MPI parallelization on four different clusters of the Austrian HPC infrastructure (LEO3, LEO3E, VSC2, VSC3) and on an Intel Xeon Phi. We find that UPC is significantly easier to develop in compared to MPI and that the performance achieved is comparable to MPI in most situations. The obtained results show worse performance (on VSC2), competitive performance (on LEO3, LEO3E and VSC3), and superior performance (on the Intel Xeon Phi). \\end{abstract} \n\\maketitle\n\n\n\n\n\\section{Introduction}\n\nBoth in industry and in academia, fluid dynamics is an important research\narea. Lots of scientific codes have been developed that predict weather\npatterns, simulate the behavior of flows over aircrafts, or describe\nthe density within interstellar nebulae.\n\nUsually, such codes compute the numerical solution of an underlying\npartial differential equation (PDE) describing the phenomenon in question.\nDescribing the time evolution of a fluid typically leads to nonlinearities\nof the governing PDEs. A classic example of such a system of equations\nis the so-called Euler equations of gas dynamics. They are used to\ndescribe compressible, inviscid fluids by modeling the behavior of\nmass density, velocity, and energy. An important aspect of these equations\nis that due to the nonlinearity so-called shock waves (i.e., rapid\nchanges in the medium) can form. Therefore, special numerical methods\nhave to be used that can cope with these discontinuities without diminishing\nthe performance. A number of software packages has been developed\nthat are used both in an industrial as well as in an academic setting.\nHowever, there is still a lot of progress to be made with respect\nto numerical integrators and their implementation on large scale HPC\nsystems.\n\nA typical supercomputer consists of a number of connected computers\nthat work together as one system. To exploit such a system, parallel\nprogramming techniques are necessary. The most important programming\nmodel to communicate between the different processes within such a\ncluster is message passing, which is usually implemented via the Message\nPassing Interface (MPI) standard. \n\nIn recent years, MPI has become the classical approach for HPC applications.\nThis is due to its portability between different hardware systems\nas well as its scalability on large clusters. However, the standard\nis mainly focused on two-sided communication, i.e., the transfer has\nto be acknowledged by both the sender as well as the receiver of a\nmessage. This is true even if both processes are located on the same\ncomputation node and thus share the same memory. On today's HPC systems,\nthis overhead is not significant, however, experts predict that on\nfuture generations of supercomputers, e.g. exascale systems, this\nmay result in a noticeable loss of performance. There are various\napproaches to combine MPI code for off-node communication with OpenMP\nfor on-node communication into a hybrid model, however, the development\nof such codes becomes more difficult.\n\nSince parallelization with MPI has to be done explicitly by the programmer,\nparallelizing a sequential code is in many situations quite difficult\n(even without considering a hybridization with OpenMP). In recent\nyears Partitioned Global Address Space (PGAS) languages have emerged\nas a viable alternative for cluster programming. PGAS languages like,\ne.g., Unified Parallel C (UPC) or Coarray Fortran try to exploit the\nprinciple of locality on a compute node. The syntax to access data\nis similar to the OpenMP approach, which is usually easier for the\nprogrammer than MPI (see, e.g., \\cite{easier1}). However, in contrast\nto OpenMP, it offers additional locality control (which is important\nfor distributed memory systems). Thus, a PGAS language is able to\nnaturally operate within a modern cluster (a distributed memory system\nthat is build from shared memory nodes).\n\nThe computer code can access data using one-sided communication primitives,\neven if the actual data resides on a different node. The compiler\nis responsible for optimizing data transfers (and thus, if implemented\nwell, can be as effective as MPI). However, usually a naive implementation\ndoes not result in optimal performance. In this case the programmer\nhas the opportunity to optimize the parallel code step by step until\nthe desired level of scaling is achieved. For further information\nabout UPC, see e.g. \\cite{upc_info3,upc_info1,upc_info2}.\n\nSince PGAS systems are an active area of research, there still may\noccur problems with hardware compatibility as well as compiler optimization\nand portability. Such issues usually do no longer affect MPI systems,\ndue to the time in development as well as the popularity of MPI.\n\nNevertheless, the PGAS approach seems to be a viable alternative for\nresearchers to develop parallel scientific codes that scale well even\non large HPC systems. A lot of research has been conducted which compares\nthe performance of MPI with PGAS languages. However, most of the published\npapers only include benchmarks examples (e.g., \\cite{benchmark1,benchmark2})\nor are performed on HPC systems with vendor provided versions of Coarray\nFortran or UPC (e.g., \\cite{involved2,involved1}).\n\nThus, the goal of this paper is to investigate the performance of\na fluid solver that is based on a widely used numerical algorithm\nfor solving the Euler equations of gas dynamics. This algorithm is\nimplemented with UPC as well as with MPI. We are especially interested\nin how the performance of UPC improves as we provide a progressively\nbetter optimized code. Thus, the parallelization is done step by step\nand the scalability of these versions is investigated. This analysis\nis done in the context of four HPC systems of the Austrian research\ninfrastructure (VSC2, VSC3, LEO3, LEO3E) which, however, lack direct\nvendor support for UPC (the Berkeley UPC \\cite{berkeley_compiler}\npackage is used on each of these systems). We believe that these systems\nare typical in terms of the HPC resources that most researchers have\navailable and that many practitioners could profit from the programming\nmodel that UPC (and PGAS languages in general) offer.\n\n\n\nIn the next section, we introduce the basics of the sequential program\nand describe its parallelization. Then, we describe the results we\nobtained from on different systems.\n\n\n\\section{Implementation and Parallelization}\n\nWe consider the Euler equations of gas dynamics in two space dimensions,\ni.e.\n", "index": 1, "text": "\n\\[\nU_{t}+F(U)_{x}+G(U)_{y}=0,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"U_{t}+F(U)_{x}+G(U)_{y}=0,\" display=\"block\"><mrow><mrow><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>F</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mi>x</mi></msub></mrow><mo>+</mo><mrow><mi>G</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mi>y</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\nis the vector of the conserved quantities: density $\\rho$, momentum\nin the $x$-direction $\\rho u$, momentum in the $y$-direction $\\rho v$,\nand energy $E$. The flux is given by\n\n", "itemtype": "equation", "pos": 7277, "prevtext": "\nwhere \n\n", "index": 3, "text": "\n\\[\nU=\\left(\\begin{array}{c}\n\\rho\\\\\n\\rho u\\\\\n\\rho v\\\\\nE\n\\end{array}\\right)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"U=\\left(\\begin{array}[]{c}\\rho\\\\&#10;\\rho u\\\\&#10;\\rho v\\\\&#10;E\\end{array}\\right)\" display=\"block\"><mrow><mi>U</mi><mo>=</mo><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\u03c1</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>u</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>v</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>E</mi></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\n\n\nThe equation is in conserved form. It can also be expressed by the\nphysical variables: density $\\rho$, velocity in the $x$-direction\n$u$, velocity in the $y$-direction $v$, as well as pressure $p$,\ndue to the relation $E=\\rho\\cdot\\left(\\frac{u^{2}+v^{2}}{2}+\\frac{p}{(\\gamma-1)\\rho}\\right)$.\nHere, $\\gamma$ is a physical constant. Usually, numerical codes are\ntested for an ideal gas, where $\\gamma=1.4$. We also use that setup\nin this paper.\n\nThese equations describe the time evolution of the conserved quantities\ninside a closed system. Since there is no source or sink in the system,\nthe integrals of these variables are conserved. However, mass can\nbe transferred within the system in the $x$- and $y$-direction according\nto the fluxes $F(u)$ and $G(u)$, respectively. We note that the\nflux terms include nonlinearities that can lead to the development\nof shock waves, even for smooth initial data. Shocks are important\nphysical phenomena and thus need to be captured accurately by the\nnumerical scheme. \n\nIn this paper, we use the well known first order finite volume Godunov\nmethod in one space dimension. To apply this scheme, we split our\nproblem into two one-dimensional problems. Godunov's method relies\non the fact that the Riemann Problem for a one-dimensional conservation\nlaw \n", "itemtype": "equation", "pos": 7531, "prevtext": "\nis the vector of the conserved quantities: density $\\rho$, momentum\nin the $x$-direction $\\rho u$, momentum in the $y$-direction $\\rho v$,\nand energy $E$. The flux is given by\n\n", "index": 5, "text": "\n\\[\nF(U)=\\left(\\begin{array}{c}\n\\rho u\\\\\n\\rho u^{2}+p\\\\\n\\rho uv\\\\\nu(E+p)\n\\end{array}\\right),\\qquad G(U)=\\left(\\begin{array}{c}\n\\rho v\\\\\n\\rho uv\\\\\n\\rho v^{2}+p\\\\\nv(E+p)\n\\end{array}\\right).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"F(U)=\\left(\\begin{array}[]{c}\\rho u\\\\&#10;\\rho u^{2}+p\\\\&#10;\\rho uv\\\\&#10;u(E+p)\\end{array}\\right),\\qquad G(U)=\\left(\\begin{array}[]{c}\\rho v\\\\&#10;\\rho uv\\\\&#10;\\rho v^{2}+p\\\\&#10;v(E+p)\\end{array}\\right).\" display=\"block\"><mrow><mrow><mrow><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>u</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><msup><mi>u</mi><mn>2</mn></msup></mrow><mo>+</mo><mi>p</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>u</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mi>G</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>v</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>\u03c1</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>v</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><msup><mi>v</mi><mn>2</mn></msup></mrow><mo>+</mo><mi>p</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>E</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\nwith the initial values \n", "itemtype": "equation", "pos": 9015, "prevtext": "\n\n\nThe equation is in conserved form. It can also be expressed by the\nphysical variables: density $\\rho$, velocity in the $x$-direction\n$u$, velocity in the $y$-direction $v$, as well as pressure $p$,\ndue to the relation $E=\\rho\\cdot\\left(\\frac{u^{2}+v^{2}}{2}+\\frac{p}{(\\gamma-1)\\rho}\\right)$.\nHere, $\\gamma$ is a physical constant. Usually, numerical codes are\ntested for an ideal gas, where $\\gamma=1.4$. We also use that setup\nin this paper.\n\nThese equations describe the time evolution of the conserved quantities\ninside a closed system. Since there is no source or sink in the system,\nthe integrals of these variables are conserved. However, mass can\nbe transferred within the system in the $x$- and $y$-direction according\nto the fluxes $F(u)$ and $G(u)$, respectively. We note that the\nflux terms include nonlinearities that can lead to the development\nof shock waves, even for smooth initial data. Shocks are important\nphysical phenomena and thus need to be captured accurately by the\nnumerical scheme. \n\nIn this paper, we use the well known first order finite volume Godunov\nmethod in one space dimension. To apply this scheme, we split our\nproblem into two one-dimensional problems. Godunov's method relies\non the fact that the Riemann Problem for a one-dimensional conservation\nlaw \n", "index": 7, "text": "\n\\[\nU_{t}+F(U)_{x}=0,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"U_{t}+F(U)_{x}=0,\" display=\"block\"><mrow><mrow><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>F</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mi>x</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\nwhere $U_{L}$ and $U_{R}$ are constants, can be solved exactly. \n\nFor the numerical method, we discretize our conserved quantities and\nobtain $U_{i}^{n}$ at time step $n$ and grid point $i$ (see Figure\n1). \n\\begin{figure}\n\\begin{centering}\n\\includegraphics[scale=0.5]{finite_vol}\n\\par\\end{centering}\n\n\\protect\\caption{Finite volume discretization at time step $n$. The conserved quantity\n$U$ is averaged over each cell. At the cell interfaces, Riemann problems\noccur. }\n\n\n\\end{figure}\n The value between the cell interfaces (located at $i-\\frac{1}{2}$\nand $i+\\frac{1}{2}$) is constant. Thus at each cell interface a Riemann\nproblems occurs. We then solve these Riemann problems exactly in order\nto update the value at the next time step. Godunov's method can then\nbe written as \n", "itemtype": "equation", "pos": 9064, "prevtext": "\nwith the initial values \n", "index": 9, "text": "\n\\[\nU(x,0)=\\begin{cases}\nU_{L}, & x<0,\\\\\nU_{R,} & x>0,\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"U(x,0)=\\begin{cases}U_{L},&amp;x&lt;0,\\\\&#10;U_{R,}&amp;x&gt;0,\\end{cases}\" display=\"block\"><mrow><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>U</mi><mi>L</mi></msub><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>x</mi><mo>&lt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msub><mi>U</mi><mrow><mi>R</mi><mo>,</mo></mrow></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\n where $\\Delta t$ and $\\Delta x$ are the time step and the length\nof a cell respectively. The numerical fluxes $F_{i-\\frac{1}{2}}$\nand $F_{i+\\frac{1}{2}}$ are obtained by the Riemann solver at the\ncell interface to the left and to the right of the grid point $i$.\nNote that this method is restricted by a CFL-condition. Since we are\nmostly interested in the method and its parallelization, we choose\na fixed time step that is small enough such that the CFL-condition\nin our simulations is always satisfied. \n\nSince an exact solution can only be calculated for one spatial dimension,\nwe use Lie splitting to separate our two-dimensional problem into\ntwo one-dimensional problems, i.e. we alternatingly solve:\n\n", "itemtype": "equation", "pos": 9912, "prevtext": "\nwhere $U_{L}$ and $U_{R}$ are constants, can be solved exactly. \n\nFor the numerical method, we discretize our conserved quantities and\nobtain $U_{i}^{n}$ at time step $n$ and grid point $i$ (see Figure\n1). \n\\begin{figure}\n\\begin{centering}\n\\includegraphics[scale=0.5]{finite_vol}\n\\par\\end{centering}\n\n\\protect\\caption{Finite volume discretization at time step $n$. The conserved quantity\n$U$ is averaged over each cell. At the cell interfaces, Riemann problems\noccur. }\n\n\n\\end{figure}\n The value between the cell interfaces (located at $i-\\frac{1}{2}$\nand $i+\\frac{1}{2}$) is constant. Thus at each cell interface a Riemann\nproblems occurs. We then solve these Riemann problems exactly in order\nto update the value at the next time step. Godunov's method can then\nbe written as \n", "index": 11, "text": "\n\\[\nU_{i}^{n+1}=U_{i}^{n}+\\frac{\\Delta t}{\\Delta x}\\left(F_{i-\\frac{1}{2}}-F_{i+\\frac{1}{2}}\\right),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"U_{i}^{n+1}=U_{i}^{n}+\\frac{\\Delta t}{\\Delta x}\\left(F_{i-\\frac{1}{2}}-F_{i+%&#10;\\frac{1}{2}}\\right),\" display=\"block\"><mrow><mrow><msubsup><mi>U</mi><mi>i</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mrow><msubsup><mi>U</mi><mi>i</mi><mi>n</mi></msubsup><mo>+</mo><mrow><mfrac><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>x</mi></mrow></mfrac><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>F</mi><mrow><mi>i</mi><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msub><mo>-</mo><msub><mi>F</mi><mrow><mi>i</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\n\n\nand \n", "itemtype": "equation", "pos": 10724, "prevtext": "\n where $\\Delta t$ and $\\Delta x$ are the time step and the length\nof a cell respectively. The numerical fluxes $F_{i-\\frac{1}{2}}$\nand $F_{i+\\frac{1}{2}}$ are obtained by the Riemann solver at the\ncell interface to the left and to the right of the grid point $i$.\nNote that this method is restricted by a CFL-condition. Since we are\nmostly interested in the method and its parallelization, we choose\na fixed time step that is small enough such that the CFL-condition\nin our simulations is always satisfied. \n\nSince an exact solution can only be calculated for one spatial dimension,\nwe use Lie splitting to separate our two-dimensional problem into\ntwo one-dimensional problems, i.e. we alternatingly solve:\n\n", "index": 13, "text": "\n\\[\nx\\mbox{-direction: }\\begin{cases}\nU_{t}+F(U)_{x}=0\\\\\nU(0)=U^{n}\n\\end{cases}\\overset{\\Delta t}{\\Rightarrow}U^{n+\\frac{1}{2}}=U(\\Delta t)\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"x\\mbox{-direction: }\\begin{cases}U_{t}+F(U)_{x}=0\\\\&#10;U(0)=U^{n}\\end{cases}\\overset{\\Delta t}{\\Rightarrow}U^{n+\\frac{1}{2}}=U(\\Delta&#10;t)\" display=\"block\"><mrow><mrow><mi>x</mi><mo>\u2062</mo><mtext>-direction:\u00a0</mtext><mo>\u2062</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>F</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mi>x</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>U</mi><mi>n</mi></msup></mrow></mtd><mtd/></mtr></mtable></mrow><mo>\u2062</mo><mover accent=\"true\"><mo>\u21d2</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mover><mo>\u2062</mo><msup><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03623.tex", "nexttext": "\n We first take a step into the $x$-direction (using Godunov's method)\nand then take a step into the $y$-direction to conclude our time\nstep. \n\nFor a more detailed discussion of the Euler equations and Godunov's\nmethod, see e.g., \\cite{eulergodu2,eulergodu1,toro2009riemann}. \n\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[width=14cm]{sim-crop}\n\\par\\end{centering}\n\n\\protect\\caption{Simulation of a Sedov explosion on the domain $[0,1]\\times[0,1]$.\nThe variables are shown from top to bottom in the following order:\ndensity, pressure, velocity in the $x$- and velocity in the $y$-direction.\nFrom left to the right, snapshots in time are shown at $t=0.005$,\n$t=0.025$, $t=0.075$ and $t=0.15$. The initial condition is chosen\nas follows: the velocity is zero in both directions, the density is\nset to one except for a hill of 100 at the left and a basin of 0.01\nat the right. The pressure is a sharp Gaussian distribution. The density\nforms a shock front which wraps around the density hill and is accelerated\nby the sink.}\n\\end{figure}\n\n\n\n\nFigure 2 shows the behavior of the physical variables for a simple\ntest problem on the domain $[0,1]\\times[0,1]$. The snapshots are\ntaken at time $t=0.005$, $t=0.025$, $t=0.075$, and $t=0.15$. As\ninitial condition, we set a background density to $\\rho=1$ with a\ndensity hill of $\\rho=100$ on the left side and a density basin of\n$\\rho=0.01$ on the right side of the domain. The pressure has a background\nvalue of $p=0.0001$ and a Gaussian with a peak of $1200$ and the\nstandard deviation $\\sigma=20.48$. Both the velocity in the $x$-\nand in the $y$-direction are $0$ at the beginning of the simulation.\n\nUp to the time considered in the first column of Figure 2, the dynamics\nof the system is mainly determined by the variation in pressure (as\nthe initial values for the other variables are constant). For the\ndensity field, a sharp front develops that spreads out symmetrically.\nThis is the propagation of a shock front which we have discussed earlier.\nNote that the density variable varies over three orders of magnitude.\nThus, we use a logarithmic scale in order to better observe the behavior\nof the solution. Such a scaling is not necessary for the other variables.\nThe pressure flattens out very quickly during this time period where\nit causes an acceleration of the particles. To be able to observe\nthe behavior of the pressure variable at the last shown time step,\nwe limited the color range to a maximum of $5$ (the largest value\nis close to $49$; at time $t=0.005$).\n\nIn the second column, we see that the density shock wave hits the\nhill and the basin of the density field. For the hill, we observe\nthat the shock wave is absorbed by the much higher density, while\nat the basin, it is drawn into the region of lower pressure. This\nbehavior can be observed by the pressure variable as well. When we\nintegrate further, the original shock wave wraps around the hill,\nuntil it completely envelopes the hill and vortices begin to form.\nThe velocity variables show that the particles within the right part\nof the domain are further accelerated, while at the hill, there is\nalmost no movement. \n\n\n\\subsection{Structure of the sequential code}\n\nWe start with a sequential code which we will parallelize both with\nMPI and UPC. The basic structure of the code is described in the following\npseudocode:\n\n\\begin{lstlisting}\nset initial physical variables;\ntime loop\n  call_one_d in x-direction;\n  call_one_d in y-direction;\n\\end{lstlisting}\n with the function \n\\begin{lstlisting}\ncall_one_d:\n  change physical to conserved variables;\n  calculate the flux;\n  execute Godunov's method;\n  change conserved to physical variables;\n\\end{lstlisting}\n\n\n\n\\subsection{Parallelization: MPI}\n\nThe MPI version of the code is based on the Single Data Multiple Program\n(SDMP) approach. The two-dimensional arrays for the four physical\nvariables $\\rho$, $u$, $v$ and $p$ are partitioned among the number\nof processes in a row-wise manner (see the illustration in Figure\n3). \n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[scale=0.5]{upc_distribution}\n\\par\\end{centering}\n\n\\protect\\caption{Setup for the physical variables in the \\textit{row}-decomposition.\nIn red, blue, yellow and purple, the slices are colored with respect\nto their thread affinity. In grey, ghost cells are indicated, which\nprotect from reflections in the outflow (which is vital in order to\nimplement the outflow boundary condition). Note that ghost cells can\nextend over more than a single thread. This setup is valid for the\nMPI \\textit{row} implementation as well as the \\textit{naive}, \\textit{pointer},\n\\textit{barrier}, and \\textit{halo} UPC implementation.}\n\\end{figure}\n\n\nIn this simulation, we take a look at two different boundary conditions.\nIn the $x$-direction, we use periodic boundary conditions, i.e.,\nthe left most grid point is identified with the right most grid point.\nIn the $y$-direction, we consider an inflow condition at the bottom\nand an outflow condition at the top. In this case, no MPI-communication\nis necessary. The value at the bottom is a constant value that we\ndefine as a global variable. At the top, we just use the last value\ntwice (which corresponds to homogeneous Neumann boundary conditions).\nSince such a setting will lead to unintended reflections, we introduce\na couple of ghost cells to hide these artifacts. This setting is illustrated\nin Figure 3.\n\nFor comparison, we also introduce the distribution of the data in\npatches, as illustrated in Figure 4. This framework leads to less\ncommunication.\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[scale=0.5]{mpi_distribution}\n\\par\\end{centering}\n\n\\protect\\caption{Setup for the physical variables in the \\textit{patch}-decomposition.\nThe dashes indicate communication with the nearest neighbor. In grey,\nghost cells are indicated, which protect from reflections in the outflow\n(which is vital in order to implement the outflow boundary condition).\nNote that ghost cells can extend over more than a single thread. This\nsetup is valid for the MPI \\textit{patch} implementation as well as\nthe \\textit{patch} UPC implementation.}\n\\end{figure}\n\n\nFor both implementations, the sequential code is extended with an\nadditional function that communicates the boundary values, i.e.:\n\n\\begin{lstlisting}\nset initial physical variables;\ntime loop\n  communicate boundaries in x-direction;\n  call_one_d in x-direction;\n  communicate boundaries in y-direction;\n  call_one_d in y-direction;\n\\end{lstlisting}\nIn each time step, first the boundaries in the $x$-direction are\ncommunicated and then the calculations in this direction are performed\nlocally on each processor. To conclude the time step, the same is\ndone in $y$-direction.\n\n\n\\subsection{Parallelization: UPC}\n\nFor the UPC parallelization, we take advantage of the shared data\nstructures provided by UPC. As discussed earlier, a big advantage\nof UPC is the incremental approach to parallelization. It is therefore\npossible to parallelize the sequential code by just adding a few additional\nlines to the sequential code. However, to obtain good scalability\nwe require a more sophisticated implementation. We therefore introduce\na sequence of optimization steps, whose performance we will analyze\nin the next section on different HPC systems. \n\nFor our first parallelization, which we call the \\textit{naive} approach,\nwe just declare our work arrays as shared such that every processor\ncan access it. Therefore, no visible communication is performed. However,\nthe workload on each thread still has to be defined by the programmer.\nThe cells with affinity to certain threads are distributed in rows\n(see Figure 3). Due to the assumed memory layout of UPC, this simplifies\nthe implementation. Note that this data distribution is also valid\nfor the next three implementations. \n\nSince in the $x$-direction the slices are located on the same thread,\nall calculations can be done locally. However, in the $y$-direction,\nthe cells are distributed among the threads. Therefore, we need additional\nshared slices that store the data for the conserved variables when\ndoing the computation in the $y$-direction.\n\nOur code thus has the following structure:\n\n\\begin{lstlisting}\nset shared initial physical variables;\ntime loop\n  call_one_d in x-direction:\n    do calculation;\n  call_one_d in y-direction:\n    use shared slices for calculation;\n\\end{lstlisting}\n\n\nIn this implementation, communication is the bottleneck. Even though\nthread 0 can access a data point on thread 3, this might be very expensive,\nespecially, if both processes do not reside on the same node. As a\nrule of thumb, shared variables are expensive to access, and thus\naccessing them one by one should be avoided, if possible. In our application,\nmost of the memory accesses is local. \n\nWe call our second approach \\textit{pointer} approach, because in\nthe $y$-direction, we create a pointer on each thread that only manipulates\nlocal data of the shared array slice, i.e.:\n\n\\begin{lstlisting}\nset initial physical variables shared;\ntime loop\n  call_one_d in x-direction:\n    do calculation;\n  call_one_d in y-direction:\n    create local pointers to shared;\n    use local pointers for calc;\n\\end{lstlisting}\n\n\nIn principle, this optimization can be performed by the UPC compiler.\nHowever, it is not clear, how efficient this optimization is in practice. \n\nSimilar to the OpenMP programming paradigm, we need to avoid race\nconditions, i.e., we need to make sure that when a calculation step\ntakes information from a shared object the corresponding data point\nhas already been updated. This is guaranteed by barriers. However,\nat barriers all threads have to wait for each other until they can\ncontinue with their work. This is accompanied with a significant overhead\nand thus, barriers should be used as little as possible in the code. \n\nUsing the performance tool \\textit{upc\\_trace} on our code we found\nthat barriers in our code slowed down the simulation. Our next optimization\nstep is therefore called the \\textit{barrier} approach, because we\ndivided the calculation loop into two loops, so that we can move the\nbarrier outside of the loop. The idea is demonstrated in the following\npseudocode: \n\n\\begin{lstlisting}\ncall_one_d in y direction:\n\ndefine local calculation arrays glob;\n\nstart calculation loop\n  cast local pointers;\n  change physical to conserved variables in global array;\n  calculate the flux in global array;\nend calculation loop;\nbarrier;\nstart calculation loop\n  cast local pointers;\n  execute Godunov's method with global array;\n  change conserved to physical variables;\nend calculation loop;\n\\end{lstlisting}\n In this case, we have to define some arrays that we need for the\ncalculation globally (so that we can use them in the two loops). However,\nwe only have to call the barrier once in each time step, removing\na lot of overhead. Note that this approach only incurs a small overhead\nin the amount of memory used. \n\nUp to now, we only tried to exploit locality of the computation, but\nkept the main working array shared. As the next step, we used an idea\nwhich is usually implemented in MPI code: we do no longer work on\nthe whole shared array, but use shared variables only for communicating\nghost cells. Therefore, similar to MPI, only the boundaries are communicated\nall at once and the remainder of the computation is done locally.\nWe thus name this level of optimization the \\textit{halo} approach:\n\n\\begin{lstlisting}\nset initial physical variables shared;\nget local copies of shared variables;\ntime loop\n  call_one_d in x-direction:\n    do calculation locally;\n  get boundaries from shared array;\n  call_one_d in y-direction:\n    do calculation locally;\n\\end{lstlisting}\n\n\nAll the approaches above are distributed in a row-wise manner. However,\nthis setup may lead to more communication than is necessary. A final\noptimization step is thus to distribute the data points on the threads\nin patches (see Figure 4). Since the data points are contiguous in\nthe $x$-direction the data communication is done by a simple call\nof \\textit{upc\\_memget} (as was done in the previous optimization\nsteps). To communicate the data in $y$-direction, we used the strided\nfunction \\textit{upc\\_memget\\_strided}. Note that this function is\npart of the Berkeley UPC compiler and not yet part of the UPC standard,\nhowever, it greatly facilitates the implementation. Due to the data\ndistribution, we call this implementation the \\textit{patch} approach.\n\nAs we can see, the optimization steps are getting more and more sophisticated\nand the code gets more and more involved. However, changing and debugging\nthe code incrementally is usually much easier than writing an already\nperfectly optimized version in the first place. The scaling behavior\nof the different approaches on different hardware is discussed in\nthe next section. \n\n\n\\section{Results}\n\nIn this section we investigate the scaling behavior of the different\ncodes described in the previous section and compare the results on\ndifferent hardware. For this purpose, we have access to four different\nHPC systems. LEO3 and LEO3E are local clusters at the University of\nInnsbruck. Assembled in 2011 with 162 compute nodes, LEO3 is a medium\nsized but relatively old cluster. In 2015, the computing infrastructure\nin Innsbruck was extended by a smaller but more modern system, LEO3E,\nwith 45 computing nodes. Both systems have approximately equal peak\nperformance. \n\nIn addition, we also use the main high performance facility in Austria:\nthe Vienna Scientific Cluster (VSC). In this study, we use VSC2, which\nranked 56 of the Top 500 systems when it came into operation in 2011\nand consists of 1314 computing nodes. In addition, we consider VSC3,\nwhich occupied rank 85 of the Top 500 systems in its first year of\noperation (2014) and consists of 2020 computing nodes. \n\nWe therefore have the opportunity to compare Tier 2 with Tier 1 systems\nas well as relatively old with relatively new hardware.\n\nAll of the above systems are relatively traditional high performance\ncluster models. We believe that such systems are representative for\nthe HPC resources available to most researchers. \n\nSince the shared memory model used by UPC is a natural fit for the\nIntel Xeon Phi, we also investigate the scaling behavior of our implementation\non that platform. The Xeon Phi implements a many-core architecture\n(similar to graphic processing units) with 60 physical cores (240\ncores are available for hyperthreading). Since the cores of the Xeon\nPhi are based on an x86 architecture, it is relatively straightforward\nto compile a UPC program for it.\n\nFor detailed hardware specifications, see Table 1.\n\n\n\n\\begin{table}\n\\begin{centering}\n\\small\n\\begin{tabular}{lrlrrrl}\nsystem & nodes & CPU on node & cores & memory & Rpeak & nw controller\\tabularnewline\n\\hline \nLEO3 & 162 & 2 x Intel Xeon X5650, 2.7 GHz & 12 & 4 TB & 18 TFlop/s & Mellanox\\tabularnewline\nLEO3E & 45 & 2 x Intel Xeon E5-2650-v3, 2.6 GHz & 10 & 4 TB & 29 TFlop/s & Mellanox\\tabularnewline\nVSC2 & 1314 & 2 x AMD Opteron 6132HE, 2.2GHz & 8 & 42 TB & 185 TFlop/s & Mellanox\\tabularnewline\nVSC3 & 2020 & 2 x Intel Xeon E5-2650v2, 2.6GHz & 8 & 131 TB & 682 TFlop/s & Intel\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\protect\\caption{Hardware specifications for the four clusters used in this paper.}\n\\end{table}\n\n\n\n\nOn each system, we execute both a strong as well as a weak scaling\nanalysis. For the strong scaling analysis, we choose a fixed problem\nsize and run the program on different number of cores. Ideally, the\nrun time would decrease linearly in the number of cores. In this paper,\nwe choose our problem size as a grid of $512\\times1024$ points and\na final time $T=0.005$. \n\nThe weak scaling analysis is performed by increasing the grid according\nto the number of cores. Ideally, the time it takes to finish the larger\nproblem would remain constant. In our case, we choose for one core\na domain with $64\\times128$ grid points and a final integration time\nof $T=0.001$. When quadrupling the number of cores, we quadruple\nthe problem size by choosing a grid of $128\\times256$ for four cores,\netc. \n\nIn our simulations we use a constant time step size $\\Delta t=10^{-5}$\n(which is small enough such that the CFL-condition is always satisfied).\n\n\n\\subsection{Results on LEO3}\n\nOn LEO3 we use up to 256 cores. Even though the network adapter is\nfrom Mellanox and thus in theory would support the network option\n\\textit{\\textcolor{black}{mxm}} (which uses the InfiniBand library\nprovided by Mellanox) for UPC, the driver version present on the system\nis too old to work with UPC. We therefore use \\textit{\\textcolor{black}{ibv}}\n(InfiniBand verbs which is a generic library used to access the InfiniBand\nhardware) as the network type. The results are shown in Table 2. \\textit{ }\n\n\\begin{table}\n\\begin{centering}\n\\small\n\\begin{tabular}{r|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{15}{c}{LEO3, strong scaling}\\tabularnewline\n & \\multicolumn{4}{c|}{MPI} & \\multicolumn{10}{c}{UPC}\\tabularnewline\nthreads & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & 982&8 \\phantom{00}(1.0) & 977&9 \\phantom{00}(1.0) & 921&4 \\phantom{0}(1.0) & 897&0 \\phantom{0}(1.0) & 903&0 \\phantom{0}(1.0) & \\textcolor{black}{930}&\\textcolor{black}{0 }\\phantom{0}\\textcolor{black}{(1.0)} & 943&2 \\phantom{0}(1.0)\\tabularnewline\n4 & 287&9 \\phantom{00}(3.4) & 249&1 \\phantom{00}(3.9) & 253&8 \\phantom{0}(3.6) & 242&6 \\phantom{0}(3.7) & 243&5 \\phantom{0}(3.7) & \\textcolor{black}{253}&\\textcolor{black}{7 }\\phantom{0}\\textcolor{black}{(3.7)} & 234&9 \\phantom{0}(4.0)\\tabularnewline\n16 & 107&8 \\phantom{00}(9.1) & 63&7 \\phantom{0}(15.4) & 83&7 (11.0) & 81&1 (11.1) & 79&6 (11.3) & \\textcolor{black}{76}&\\textcolor{black}{7 (12.1)} & 72&9 (12.9)\\tabularnewline\n64 & 35&4 \\phantom{0}(27.8) & 17&5 \\phantom{0}(55.9) & 36&1 (25.5) & 34&8 (25.8) & 31&6 (28.6) & \\textcolor{black}{30}&\\textcolor{black}{3 (30.7)} & 27&0 (34.9)\\tabularnewline\n256 & 9&2 (106.8) & 6&4 (152.8) & \\textcolor{black}{25}&\\textcolor{black}{7 }(35.9) & 18&1 (49.6) & 14&6 (61.8) & 10&2 (91.2) & 11&7 (80.6)\\tabularnewline\n\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\begin{centering}\n\\small\n\\begin{tabular}{rr|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{16}{c}{LEO3, weak scaling}\\tabularnewline\n &  & \\multicolumn{4}{c|}{MPI} & \\multicolumn{10}{c}{UPC}\\tabularnewline\nthreads & grid & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & $64\\times\\phantom{0}128$  & 3&0 (1.0) & 3&0 (1.0) & 2&8 (1.0) & 2&6 (1.0) & 2&6 (1.0) & 2&5 (1.0) & 2&6 (1.0)\\tabularnewline\n4 & $128\\times\\phantom{0}256$ & 3&1 (1.0) & 3&1 (1.0) & 3&0 (1.0) & 2&6 (1.1) & 2&6 (1.0) & \\textcolor{black}{2}&\\textcolor{black}{6 }(1.0) & 2&6 (1.0)\\tabularnewline\n16 & $256\\times\\phantom{0}512$ & 3&6 (1.2) & 3&2 (1.1) & 3&4 (1.2) & 3&6 (1.4) & 2&9 (1.1) & \\textcolor{black}{2}&\\textcolor{black}{7 }(1.1) & 2&9 (1.1)\\tabularnewline\n64 & $512\\times1024$ & 4&8 (1.6) & 3&2 (1.1) & 4&9 (1.8) & \\textcolor{black}{4}&\\textcolor{black}{6 }(1.8) & 3&7 (1.4) & \\textcolor{black}{3}&\\textcolor{black}{3 }(1.3) & 3&2 (1.2)\\tabularnewline\n256 & $1024\\times2048$ & 6&0 (2.0) & 3&4 (1.1) & 9&5 (3.4) & \\textcolor{black}{9}&\\textcolor{black}{5 }(3.6) & 6&7 (2.6) & \\textcolor{black}{5}&\\textcolor{black}{4 }(2.1) & 3&5 (1.3)\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\protect\\caption{This table shows the strong and weak scaling results on the LEO3 system.\nFor the strong scaling analysis, we used the grid $512\\times1024$.\nIn addition to the runtime, we also include the speedup (for the strong\nscaling analysis) and the increase in runtime normalized to the single\ncore implementation (for the weak scaling). These quantities are shown\nin parenthesis.}\n\\end{table}\n\n\n\\textit{row-wise communication pattern}: On a single node, the strong\nscaling analysis for all of the UPC implementations exceed the results\nof the MPI version. However, as soon as we leave the node, the speedup\ndramatically depends on the optimization level of the implementation.\nThe \\textit{halo} implementation competes with the speedup of the\nMPI program. Similar results are observed for the weak scaling analysis. \n\n\\textit{patched communication pattern}: For this communication pattern,\nthe single node performance is similar in strong as well as weak scaling\nfor the MPI and the UPC version. However, the overhead for the UPC\nversion seems to be higher as soon as we run the simulation on a larger\nnumber of nodes.\n\n\n\\subsection{Results on LEO3E}\n\nSimilar to LEO3, we can not take advantage of the Mellanox network\ndriver for the UPC code. Therefore, \\textit{\\textcolor{black}{ibv}}\nis used as the network type. The results for LEO3E can be found in\nTable 3. \n\n\n\n\n\n\\begin{table}\n\\begin{centering}\n\\small\n\\begin{tabular}{r|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{15}{c}{LEO3E, strong scaling}\\tabularnewline\n & \\multicolumn{4}{c|}{MPI} & \\multicolumn{10}{c}{UPC}\\tabularnewline\nthreads & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & 849&4 \\phantom{0}(1.0) & 843&9 \\phantom{00}(1.0) & 616&7 \\phantom{0}(1.0) & 580&9 \\phantom{0}(1.0) & 599&7 \\phantom{0}(1.0) & \\textcolor{black}{595}&\\textcolor{black}{8 }\\phantom{0}(1.0) & 610&8 \\phantom{0}(1.0)\\tabularnewline\n4 & 242&7 \\phantom{0}(3.5) & 212&5 \\phantom{00}(4.0) & \\textcolor{black}{169}&\\textcolor{black}{7} \\phantom{0}(3.6) & \\textcolor{black}{156}&\\textcolor{black}{9} \\phantom{0}(3.7) & \\textcolor{black}{161}&\\textcolor{black}{0} \\phantom{0}(3.7) & \\textcolor{black}{159}&\\textcolor{black}{4 }\\phantom{0}(3.7) & 148&7 \\phantom{0}(4.1)\\tabularnewline\n16 & 91&4 \\phantom{0}(9.3) & 56&5 \\phantom{0}(14.9) & \\textcolor{black}{54}&\\textcolor{black}{3} (11.4) & 50&8 (11.4) & \\textcolor{black}{50}&\\textcolor{black}{3 }(11.9) & \\textcolor{black}{49}&\\textcolor{black}{4 }(12.1) & 47&9 (12.8)\\tabularnewline\n64 & 27&4 (31.0) & 14&9 \\phantom{0}(56.6) & \\textcolor{black}{23}&2 (26.6) & 23&4 (24.8) & \\textcolor{black}{20}&\\textcolor{black}{8} (28.8) & \\textcolor{black}{19}&\\textcolor{black}{4} (30.7) & 18&2 (33.6)\\tabularnewline\n256 & 9&6 (88.5) & 6&7 (126.0) & \\multicolumn{2}{c}{--} & \\multicolumn{2}{c}{--} & 10&2 (58.8) & 6&5 (91.7) & 7&3 (83.7)\\tabularnewline\n\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\begin{centering}\n\n\\par\\end{centering}\n\n\\begin{centering}\n\\small\n\\begin{tabular}{rr|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{16}{c}{LEO3E, weak scaling}\\tabularnewline\n &  & \\multicolumn{4}{c|}{MPI} & \\multicolumn{10}{c}{UPC}\\tabularnewline\nthreads & grid & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & $64\\times\\phantom{0}128$  & 2&6 (1.0) & 2&5 (1.0) & 1&8 (1.0) & 1&7 (1.0) & 1&7 (1.0) & \\textcolor{black}{1}&\\textcolor{black}{7 }(1.0) & 1&7 (1.0)\\tabularnewline\n4 & $128\\times\\phantom{0}256$ & 2&6 (1.0) & 2&6 (1.0) & 1&8 (1.0) & 1&7 (1.0) & 1&7 (1.0) & \\textcolor{black}{1}&\\textcolor{black}{7 }(1.0) & 1&7 (1.0)\\tabularnewline\n16 & $256\\times\\phantom{0}512$ & 3&0 (1.2) & 2&6 (1.0) & 2&0 (1.1) & 1&9 (1.1) & 1&9 (1.1) & \\textcolor{black}{1}&\\textcolor{black}{7 }(1.0) & 1&9 (1.1)\\tabularnewline\n64 & $512\\times1024$ & 4&1 (1.6) & 2&7 (1.1) & 2&9 (1.6) & 2&8 (1.6) & 2&4 (1.4) & \\textcolor{black}{2}&\\textcolor{black}{0 }(1.2) & 2&1 (1.2)\\tabularnewline\n256 & $1024\\times2048$ & 6&5 (2.5) & 4&1 (1.6) & 6&2 (3.4) & 6&0 (3.5) & 5&1 (3.0) & 3&4 (2.0) & 2&4 (1.4)\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\protect\\caption{This table shows the strong and weak scaling results on the LEO3E\nsystem. For the strong scaling analysis, we used the grid $512\\times1024$.\nIn addition to the runtime, we also include the speedup (for the strong\nscaling analysis) and the increase in runtime normalized to the single\ncore implementation (for the weak scaling). These quantities are shown\nin parenthesis.}\n\\end{table}\n\n\nThe processors on this hardware are newer and thus faster than on\nLEO3. This can be seen in the significantly shorter runtime of the\nsequential code. Due to the bad scaling properties of the \\textit{naive}\nand the \\textit{pointer} implementation, we do not include the strong\nscaling results for these implementations on $256$ cores.\\textit{ }\n\n\\textit{row-wise communication pattern}: On a single node, the different\noptimization stages of the UPC code are similar and compete with the\nMPI version. For a larger number of nodes, the \\textit{halo} version\nscales better compared to the MPI program. This is valid for both\nthe strong as well as the weak scaling analysis.\n\n\\textit{patched communication pattern}: The speedup for the strong\nscaling analysis of the MPI version is better than the UPC program.\nHowever, for the weak scaling analysis, UPC scales better compared\nto the MPI version. \n\n\n\\subsection{Results on VSC2}\n\nVSC2 uses a Mellanox adapter and we are able to make use of the \\textit{mxm}-network\ntype. For this system, we experience a significant performance issue\nbrought about by the synchronization barriers in our code. Due to\nthe significant run time increase for the \\textit{naive}, \\textit{pointer},\nand \\textit{barrier} UPC runs, we only compare the \\textit{halo} implementation\nwith MPI (Table 4). Since the VSC2 system is larger than LEO3 and\nLEO3E, we perform a weak scaling analysis for up to 1024 cores. This\ncan not be done for strong scaling, since due to the data distribution\nof the UPC code, we have not enough data points for the problem size\nconsidered. \n\n\n\n\n\n\\begin{table}\n\\begin{centering}\n\\small\n\\begin{tabular}{r|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{9}{c}{VSC2, strong scaling}\\tabularnewline\n & \\multicolumn{4}{c|}{MPI} & \\multicolumn{4}{c}{UPC}\\tabularnewline\nthreads & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & 1615&6 \\phantom{0}(1.0\\phantom{*}) & 1535&7 \\phantom{0}(1.0\\phantom{*}) & \\textcolor{black}{1649}&\\textcolor{black}{9} (1.0\\phantom{*}) & 1653&2 (1.0\\phantom{*})\\tabularnewline\n4 & 451&4 \\phantom{0}(3.6\\phantom{*}) & 380&6 \\phantom{0}(4.0\\phantom{*}) & \\textcolor{black}{415}&\\textcolor{black}{2 }(4.0\\phantom{*}) & 391&9 (4.2\\phantom{*})\\tabularnewline\n16 & 337&2 \\phantom{0}(4.8\\phantom{*}) & 196&6 \\phantom{0}(7.8\\phantom{*}) & \\textcolor{black}{197}&\\textcolor{black}{5 }(8.4\\phantom{*}) & 217&0 (7.6\\phantom{*})\\tabularnewline\n\\hline \n64 & 102&4 \\phantom{0}(3.3{*}) & 52&9 \\phantom{0}(3.8{*}) & \\textcolor{black}{119}&\\textcolor{black}{9 }(1.6{*}) & 135&6 (1.6{*})\\tabularnewline\n256 & 28&6 (11.8{*}) & 16&3 (12.1{*}) & \\textcolor{black}{65}&\\textcolor{black}{4 }(3.0{*}) & 104&9 (2.1{*})\\tabularnewline\n\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\begin{centering}\n\n\\par\\end{centering}\n\n\\begin{centering}\n\\small\n\\begin{tabular}{rr|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{10}{c}{VSC2, weak scaling}\\tabularnewline\n &  & \\multicolumn{4}{c|}{MPI} & \\multicolumn{4}{c}{UPC}\\tabularnewline\nthreads & grid & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c|}{patch} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & $64\\times\\phantom{0}128$  & 4&5 (1\\inputencoding{latin1}{.0}\\inputencoding{latin9}\\phantom{*}) & 4&5 (1.0\\phantom{*}) & \\textcolor{black}{4}&\\textcolor{black}{4 }(1.0\\phantom{*}) & 4&4 (1.0\\phantom{*})\\tabularnewline\n4 & $128\\times\\phantom{0}256$ & 4&8 (1.1\\phantom{*}) & 4&6 (1.0\\phantom{*}) & \\textcolor{black}{4}&\\textcolor{black}{3 }(1.0\\phantom{*}) & 4&2 (1.0\\phantom{*})\\tabularnewline\n16 & $256\\times\\phantom{0}512$ & 11&0 (2.4\\phantom{*}) & 9&2 (2.0\\phantom{*}) & \\textcolor{black}{10}&\\textcolor{black}{5 }(2.4\\phantom{*}) & 17&0 (3.9\\phantom{*})\\tabularnewline\n\\hline \n64 & $512\\times1024$ & 14&9 (1.4{*}) & 9&6 (1.0{*}) & \\textcolor{black}{14}&\\textcolor{black}{9 }(1.4{*}) & 20&1 (1.2{*})\\tabularnewline\n256 & $1024\\times2048$ & 19&3 (1.8{*}) & 9&8 (1.1{*}) & \\textcolor{black}{23}&\\textcolor{black}{2 }(2.2{*}) & 28&1 (1.7{*})\\tabularnewline\n1024 & $2048\\times4096$ & 27&5 (2.5{*}) & 11&6 (1.3{*}) & 30&9 (2.9{*}) & 37&3 (2.2{*})\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\protect\\caption{This table shows the strong and weak scaling results on the VSC2 system.\nFor the strong scaling analysis, we used the grid $512\\times1024$.\nIn addition to the runtime, we also include the speedup (for the strong\nscaling analysis) and the increase in runtime normalized to the single\ncore implementation (for the weak scaling). These quantities are shown\nin parenthesis. The numbers denoted with {*} are not based on one\ncore but on 16 cores due to the hardware properties described in the\ntext.}\n\\end{table}\n\n\n\n\nDue to the architecture of the AMD CPU, which includes only a single\nfloating point unit for every two cores, we only observe a speedup\nof 8 on a single node. This is an inherent limitation of the CPU and\ncan be observed for both the MPI as well as the UPC implementation.\n\n\\textit{row-wise communication pattern}: We observe that the single\nnode performance of the UPC \\textit{halo} implementation outperforms\nthe MPI version. However, as soon as we use a higher number of nodes,\nthe performance is disappointing for UPC. This is a result of the\nfact that a large portion of the run time is spend in the two calls\n(per time step) to \\textit{upc\\_barrier} (see Table 4).\n\n\\textit{patched communication pattern}: Similar to the row pattern\nimplementations, the single node performance of the MPI and the UPC\nimplementation is comparable, however, the more cores are used, the\nmore time is spend at the \\textit{upc\\_barrier} calls that significantly\nincreases the run time of UPC. However, it should be noted that the\nscaling behavior of the MPI code is also far from ideal on this system.\n\n\n\\subsection{Results on VSC3}\n\nVSC3 has an InfiniBand network adapter from Intel. Since Berkeley\nUPC does not include native support for this hardware, we use\\textit{\nibv} as the network type. VSC3 is the largest system we have at our\ndisposal. We therefore perform the weak scaling analysis for up to\n4096 cores. \n\nHowever, for problems with 1024 or more cores, by default, the UPC\nrun time opens more InfiniBand connections than the hardware supports.\nIn principle, to address this issue, XRC and SRQ were developed. Unfortunately,\nthese technologies are not supported on VSC3. Thus, we have to bundle\nthe network connections manually by reducing the number of processes\non a single node. In UPC, this is accomplished by setting the pthread\noption on the command line. Increasing the number of pthreads also\nleads to an increase of the run time. We therefore only use the smallest\nnumber of pthreads possible. I.e., for $1024$ threads, we use $pthreads=2$.\n\n\n\n\n\n\\begin{table}\n\\begin{centering}\n\n\\par\\end{centering}\n\n\\begin{centering}\n\\small\n\\begin{tabular}{c|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{5}{c}{VSC3, strong scaling, MPI}\\tabularnewline\nthreads & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & 532&2 \\phantom{00}(1.0) & \\textcolor{black}{535}&\\textcolor{black}{8 }\\phantom{00}\\textcolor{black}{(1.0)}\\tabularnewline\n4 & 149&5 \\phantom{00}(3.6) & \\textcolor{black}{132}&\\textcolor{black}{9 }\\phantom{00}\\textcolor{black}{(4.0)}\\tabularnewline\n16 & 53&2 \\phantom{0}(10.0) & \\textcolor{black}{33}&\\textcolor{black}{7 }\\phantom{0}\\textcolor{black}{(15.9)}\\tabularnewline\n64 & 15&7 \\phantom{0}(33.9) & \\textcolor{black}{8}&\\textcolor{black}{7 }\\phantom{0}\\textcolor{black}{(61.6)}\\tabularnewline\n256 & 5&0 (106.4) & \\textcolor{black}{3}&\\textcolor{black}{2 (167.4)}\\tabularnewline\n\\multicolumn{1}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\multicolumn{1}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\\qquad \\small\n\\begin{tabular}{c|cr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{6}{c}{VSC3, weak scaling, MPI}\\tabularnewline\nthreads & grid & \\multicolumn{2}{c}{row} & \\multicolumn{2}{c}{patch}\\tabularnewline\n\\hline \n1 & $64\\times\\phantom{0}128$  & 1&5 (1.0) & \\textcolor{black}{1}&\\textcolor{black}{5 (1.0)}\\tabularnewline\n4 & $128\\times\\phantom{0}256$ & 1&6 (1.1) & 1&6 (1.1)\\tabularnewline\n16 & $256\\times\\phantom{0}512$ & 1&8 (1.2) & \\textcolor{black}{1}&\\textcolor{black}{6 (1.1)}\\tabularnewline\n64 & $512\\times1024$ & 2&4 (1.6) & 1&6 (1.1)\\tabularnewline\n256 & $1024\\times2048$ & 3&2 (2.1) & \\textcolor{black}{1}&\\textcolor{black}{8 (1.2)}\\tabularnewline\n1024 & $2048\\times4096$ & 8&8 (5.7) & \\textcolor{black}{2}&\\textcolor{black}{3 (1.5)}\\tabularnewline\n\\multicolumn{1}{c}{} &  & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\\\\\n\\small\n\\begin{tabular}{c|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{13}{c}{VSC3, strong scaling, UPC}\\tabularnewline\nthreads & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch} & \\multicolumn{2}{c}{patch\\_mod}\\tabularnewline\n\\hline \n1 & 516&5 \\phantom{0}\\textcolor{black}{(1.0)} & 459&1 \\phantom{0}\\textcolor{black}{(1.0)} & 478&1 \\phantom{0}\\textcolor{black}{(1.0)} & \\textcolor{black}{482}&\\textcolor{black}{2 }\\phantom{0}\\textcolor{black}{(1.0)} & 478&3 \\phantom{0}(1.0) & 478&0 \\phantom{0}(1.0)\\tabularnewline\n4 & 141&2 \\phantom{0}\\textcolor{black}{(3.7)} & 123&8 \\phantom{0}\\textcolor{black}{(3.7)} & 128&2 \\phantom{0}\\textcolor{black}{(3.7)} & \\textcolor{black}{125}&\\textcolor{black}{9 }\\phantom{0}\\textcolor{black}{(3.8)} & 116&8 \\phantom{0}(4.1) & 117&9 \\phantom{0}(4.1)\\tabularnewline\n16 & 42&6 \\textcolor{black}{(12.1)} & 37&8 \\textcolor{black}{(12.1)} & 39&8 \\textcolor{black}{(12.0)} & \\textcolor{black}{38}&\\textcolor{black}{0 (12.7)} & 36&3 (13.2) & 36&1 (13.2)\\tabularnewline\n64 & 38&3 \\textcolor{black}{(13.5)} & 37&0 \\textcolor{black}{(12.4)} & 33&3 \\textcolor{black}{(14.4)} & \\textcolor{black}{14}&\\textcolor{black}{5 (33.3)} & 12&8 (37.4) & 12&7 (37.6)\\tabularnewline\n256 & 35&3 \\textcolor{black}{(14.6)} & 34&2 \\textcolor{black}{(13.4)} & 28&0 \\textcolor{black}{(17.1)} & \\textcolor{black}{4}&\\textcolor{black}{9 (98.4)} & 5&6 (85.4) & 5&5 (86.9)\\tabularnewline\n\\multicolumn{1}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}\\\\\n\\small\n\\begin{tabular}{c|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{13}{c}{VSC3, weak scaling, UPC}\\tabularnewline\nthreads & \\multicolumn{2}{c}{naive} & \\multicolumn{2}{c}{pointer} & \\multicolumn{2}{c}{barrier} & \\multicolumn{2}{c}{halo} & \\multicolumn{2}{c}{patch} & \\multicolumn{2}{c}{patch\\_mod}\\tabularnewline\n\\hline \n1 & 1&5 \\textcolor{black}{(1.0)} & 1&3 \\phantom{0}\\textcolor{black}{(1.0)} & 1&3 \\textcolor{black}{(1.0)} & \\textcolor{black}{1}&\\textcolor{black}{3}\\phantom{*}\\textcolor{black}{{} (1.0)} & 1&3\\phantom{*} (1.0) & 1&3\\phantom{*} (1.0)\\tabularnewline\n4 & 1&5 \\textcolor{black}{(1.0)} & 1&3 \\phantom{0}\\textcolor{black}{(1.0)} & 1&3 \\textcolor{black}{(1.0)} & \\textcolor{black}{1}&\\textcolor{black}{3}\\phantom{*}\\textcolor{black}{{} (1.0)} & 1&3\\phantom{*} (1.0) & 1&3\\phantom{*} (1.0)\\tabularnewline\n16 & 1&6 \\textcolor{black}{(1.1)} & 1&4 \\phantom{0}\\textcolor{black}{(1.1)} & 1&3 \\textcolor{black}{(1.0)} & \\textcolor{black}{1}&\\textcolor{black}{3}\\phantom{*}\\textcolor{black}{{} (1.0)} & 1&3\\phantom{*} (1.0) & 1&3\\phantom{*} (1.0)\\tabularnewline\n64 & 6&4 \\textcolor{black}{(4.3)} & 6&1 \\phantom{0}\\textcolor{black}{(4.7)} & 5&6 \\textcolor{black}{(4.2)} & \\textcolor{black}{1}&\\textcolor{black}{6}\\phantom{*}\\textcolor{black}{{} (1.2)} & 1&4\\phantom{*} (1.1) & 1&4\\phantom{*} (1.1)\\tabularnewline\n256 & 14&3 \\textcolor{black}{(9.6)} & 14&6 \\textcolor{black}{(11.2)} & 11&8 \\textcolor{black}{(8.9)} & \\textcolor{black}{2}&\\textcolor{black}{6}\\phantom{*}\\textcolor{black}{{} (2.0)} & 1&7\\phantom{*} (1.3) & 1&6\\phantom{*} (1.2)\\tabularnewline\n1024 & \\multicolumn{2}{c}{--} & \\multicolumn{2}{c}{--} & \\multicolumn{2}{c}{--} & \\textcolor{black}{4}&\\textcolor{black}{5{*} (3.5)} & 3&3{*} (2.5) & 2&7{*} (2.1)\\tabularnewline\n\\end{tabular}\n\\par\\end{centering}\n\n\\protect\\caption{This table shows the strong and weak scaling results on the VSC3 system.\nFor the strong scaling analysis, we used the grid $512\\times1024$.\nIn addition to the runtime, we also include the speedup (for the strong\nscaling analysis) and the increase in runtime normalized to the single\ncore implementation (for the weak scaling). These quantities are shown\nin parenthesis. The numbers marked with a {*} are simulated by using\n2 pthreads. This issue is discussed further within the text.}\n\\end{table}\n\n\n\n\nIn this simulation, we have the newest as well as largest cluster\nwithin this paper at our disposal. This already affects the run time\nfor one core, which is nearly half of the run time on the LEO3 system\nfor all of the runs investigated.\n\n\\textit{row-wise communication pattern}: Similar to the other hardware,\nall UPC optimization stages perform similarly on the same node. However,\nfor a larger number of nodes, only the UPC \\textit{halo} version is\nable to compete with the MPI implementation. The strong scaling analysis\nis comparable for both implementations. For more than a thousand cores,\nthe weak scaling results for the \\textit{halo} UPC implementation\nsignificantly outperforms the MPI implementation.\n\n\\textit{patched communication pattern}: The MPI version shows better\nscaling results than the UPC version. Since on this hardware, a significant\namount of time is spend in the barriers, we also include the results\nfor a modified \\textit{patch} version \\textit{patch\\_mod}: In this\nversion, we communicate the data both in $x$- as well as $y$-direction\nbefore the splitting. This means that in each time step, we only communicate\nonce, but twice as many data. This enables us to get rid of two barriers\nthat are necessary to avoid race conditions in the \\textit{patch}\nimplementation. Note that this is a first order approximation, even\nthough the boundary data in the $y$-direction is not updated with\nthe output from the splitting step in the $x$-direction, but from\nthe data of the previous time step. \n\nSince the runtime results for both patched versions are similar on\nLEO3 and  LEO3E, we only include this additional optimization for\n VSC3. We can see that especially for more than a thousand cores,\nthe overhead of the barriers have a significant impact. However, for\nthis communication pattern, UPC can still not compete with MPI.\n\n\n\\subsection{Results on the Intel Xeon Phi}\n\nNote that the Intel Xeon Phi usually benefits from additional vectorization\noptimizations. However, in this paper we use our most optimized UPC\nrun as well as our MPI run without any change. The Intel Xeon Phi\nhas 60 cores and 4 hyperthreads at each core that we could use. Note,\nhowever, that even with hyperthreading, we do not expect a linear\nincrease after 60 cores. We only use the \\textit{row} implementation\nfor MPI and the UPC \\textit{halo} version in this section. In Table\n6, we show the results for strong scaling for both CPUs on a single\nnode and for the Intel Xeon Phi. \n\n\\begin{table*}\n\\centering\n\n\n\n\\begin{tabular}{r|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{5}{c}{2$\\times$CPU}\\tabularnewline\nthreads & \\multicolumn{2}{c}{MPI: row} & \\multicolumn{2}{c}{UPC: halo}\\tabularnewline\n\\hline \n1 & 3&58 \\phantom{0}(1.0) & 3&08 \\phantom{0}(1.0)\\tabularnewline\n4 & 0&99 \\phantom{0}(3.6) & 0&86 \\phantom{0}(3.6)\\tabularnewline\n8 & 0&55 \\phantom{0}(6.5) & 0&52 \\phantom{0}(5.9)\\tabularnewline\n16 & 0&32 (11.2) & 0&26 (11.8)\\tabularnewline\n\\multicolumn{1}{r}{} & \\multicolumn{2}{c}{} & \\multicolumn{2}{c}{}\\tabularnewline\n\\end{tabular}$\\qquad$\n\\begin{tabular}{r|r@{\\extracolsep{0pt}.}lr@{\\extracolsep{0pt}.}l}\n\\multicolumn{5}{c}{Intel Xeon Phi}\\tabularnewline\nthreads & \\multicolumn{2}{c}{MPI: row} & \\multicolumn{2}{c}{UPC: halo}\\tabularnewline\n\\hline \n1 & 45&83 \\phantom{0}(1.0) & 33&06 \\phantom{0}(1.0)\\tabularnewline\n30 & 1&78 (25.7) & 1&15 (28.7)\\tabularnewline\n60 & 0&98 (46.8) & 0&60 (55.1)\\tabularnewline\n120 & 0&82 (55.9) & 0&43 (76.9)\\tabularnewline\n240 & 0&92 (49.8) & 0&41 (80.6)\\tabularnewline\n\\end{tabular}\n\n\\caption{This table shows the strong scaling results with the grid $512 \\times 1024$ on the Intel Xeon Phi system. The left analysis is performed on the two E5-2630v3 CPUs within that system. The left analysis is performed on the Intel Xeon Phi 7120. We also include the speedup in parenthesis. The grid size in $y$-direction is a multiple of 60. The final integration time is $T=5\\cdot 10^{-5}$.}\n\\end{table*}\n\nSince UPC exploits the shared memory architecture (comparable to OpenMP),\nwe see that it is a good fit for the Intel Xeon Phi. The scaling results\nfor the UPC implementation are better by a factor of $1.6$ compared\nto the MPI implementation. We therefore conclude that UPC is a viable\noption for programming accelerators. \n\n\n\\section{Conclusion }\n\nIn this paper, we investigate the usability of the PGAS language UPC\nfor a scientific code and its competitiveness with MPI. We use a basic\nfluid dynamics code to solve the Euler equations of gas dynamics and\nparallelized it with both MPI and different optimization stages of\nUPC for two different communication patterns. Then, we compare the\nresults on different hardware systems by conducting a strong and a\nweak scaling analysis, respectively.\n\nWe find that in most cases, for the row implementation UPC exceeds\nthe MPI implementation. However, for the patched implementation, MPI\nscales usually better.\n\nWe experience a major drawback at  VSC2, where barriers prove to be\nextremely expensive. This issue is not nearly as significant for the\nother hardware. Furthermore, on more than 1024 cores on  VSC3 the\nefficiency of connection bundling degrades the performance. \n\nDespite these issues, especially the possibility of incremental parallelization\nconvinces us that UPC is a viable option for scientific computing\non these HPC systems.\n\n\n\\section{Acknowledgments}\n\nWe want to thank Paul Hargrove (Lawrence Berkeley National Laboratory)\nfor helping us to set up UPC on  VSC3.\n\nThis paper is based upon work supported by the VSC Research Center\nfunded by the Austrian Federal Ministry of Science, Research and Economy\n(bmwfw). The computational results presented have been achieved in\npart using the Vienna Scientific Cluster (VSC).\n\nThis work was supported by the Austrian Ministry of Science BMWF as\npart of the UniInfrastrukturprogramm of the Focal Point Scientific\nComputing at the University of Innsbruck.\n\n\\bibliographystyle{plain}\n\\bibliography{cites}\n\n\n", "itemtype": "equation", "pos": 10873, "prevtext": "\n\n\nand \n", "index": 15, "text": "\n\\[\ny\\mbox{-direction: }\\begin{cases}\nU_{t}+G(U)_{y}=0\\\\\nU(0)=U^{n+\\frac{1}{2}}\n\\end{cases}\\overset{\\Delta t}{\\Rightarrow}U^{n+1}=U(\\Delta t).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"y\\mbox{-direction: }\\begin{cases}U_{t}+G(U)_{y}=0\\\\&#10;U(0)=U^{n+\\frac{1}{2}}\\end{cases}\\overset{\\Delta t}{\\Rightarrow}U^{n+1}=U(%&#10;\\Delta t).\" display=\"block\"><mrow><mrow><mrow><mi>y</mi><mo>\u2062</mo><mtext>-direction:\u00a0</mtext><mo>\u2062</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>G</mi><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><mi>y</mi></msub></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mtd><mtd/></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></msup></mrow></mtd><mtd/></mtr></mtable></mrow><mo>\u2062</mo><mover accent=\"true\"><mo>\u21d2</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow></mover><mo>\u2062</mo><msup><mi>U</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mo>=</mo><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>t</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]