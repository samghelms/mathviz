[{"file": "1601.05403.tex", "nexttext": "\nand the {\\it signed degree matrix \\/} $\\overline{{D}}$ as\n", "itemtype": "equation", "pos": 9126, "prevtext": "\n\n\\twocolumn[\n\n\\icmltitle{Semantic Word Clusters Using Signed Normalized Graph Cuts}\n\n\\icmlauthor{Jo\\~{a}o Sedoc}{joao@cis.upenn.edu}\n\\icmladdress{Department of Computer and Information Science, University of Pennsylvania\n            Philadelphia, PA 19104 USA}\n\\icmlauthor{Jean Gallier}{jean@cis.upenn.edu}\n\\icmladdress{Department of Computer and Information Science, University of Pennsylvania\n            Philadelphia, PA 19104 USA}\n\\icmlauthor{Lyle Ungar}{ungar@cis.upenn.edu}\n\\icmladdress{Department of Computer and Information Science, University of Pennsylvania\n            Philadelphia, PA 19104 USA}\n\\icmlauthor{Dean Foster}{dean@foster.net}\n\\icmladdress{Amazon, New York, NY USA}\n]\n\n\\frenchspacing\n\n\n\n\n\n\n\n\\begin{abstract}\nVector space representations of words capture many aspects of word similarity, but such methods tend to make vector spaces in which antonyms (as well as synonyms) are close to each other.  We present a new signed spectral normalized graph cut algorithm, {\\em signed clustering}, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights.  Our signed clustering algorithm produces clusters of words which simultaneously capture distributional and synonym relations.  We evaluate these clusters against the SimLex-999 dataset \\cite{hill2014simlex} of human judgments of word pair similarities, and also show the benefit of using our clusters to predict the sentiment of a given text. \n\\end{abstract}\n\\label{intro}\n\\section{Introduction}\n\nWhile vector space models \\citep{turney2010frequency} such as Eigenwords, Glove, or word2vec capture relatedness, they do not adequately encode synonymy and similarity \\citep{mohammad2013computing,scheible2013uncovering}. \nOur goal was to create clusters of synonyms or semantically-equivalent words \nand linguistically-motivated unified constructs. \nWe innovated a novel theory and method that extends multiclass normalized cuts \n(K-cluster) to signed graphs \\citep{gallier2015spectral}, which\n\n\n\n\n\nallows the incorporation of semi-supervised \ninformation. Negative edges serve as repellent or opposite relationships between nodes. \n\n\n\n\nIn distributional vector representations opposite relations\nare not fully captured. Take, for example, words such as ``great'' and ``awful'', which can appear with similar frequency in the same sentence structure: ``today is a great day'' and ``today is an awful day''. \nWord embeddings, which are successful in a wide array of NLP tasks, fail to capture this antonymy because they follow the {\\it distributional hypothesis} that \nsimilar words are used in similar contexts \\citep{harris1954distributional}, thus assigning small cosine or euclidean distances between the\nvector representations of ``great'' and ``awful''.\nOur signed spectral normalized graph cut algorithm (henceforth, signed clustering) builds antonym relations into the vector space, while \nmaintaining distributional similarity. Furthermore, another strength of K-clustering of signed graphs\nis that it can be used collaboratively with other methods for augmenting semantic\nmeaning. Signed clustering leads to improved\nclusters over spectral clustering of word embeddings, and has better coverage than thesaurus look-up.\nThis is because thesauri erroneously give equal weight to rare senses of word, such as ``absurd'' and its rarely used synonym ``rich''. \nAlso, the overlap between thesauri is small, due to their manual creation.\n\\citet{lin1998automatic} found 0.178397 between-synonym set from Roget's Thesaurus and WordNet 1.5.\nWe also found similarly small overlap between all three thesauri tested.\n \n\n\n\n\n\nWe evaluated our clusters by comparing them to different\nvector representations. In addition, we evaluated our clusters against SimLex-999.\nFinally, we tested our method on the sentiment analysis task.\nOverall, signed spectral clustering results are a very clean and \nelegant augmentation to current methods, and may have broad application to many fields.\n\nOur main contributions are the novel method for signed clustering of signed graphs by \\citet{gallier2015spectral},\nthe application of this method to create semantic word clusters which are agnostic to both vector space representations and thesauri, \nand finally, the systematic evaluation and creation of word clusters using thesauri.\n\n \\subsection{Related Work}\n \nSemantic word cluster and distributional thesauri have been well studied \\citep{lin1998automatic,curran2004distributional}.\nRecently there has been a lot of work on incorporating synonyms and antonyms into word embeddings.\nMost recent models either attempt to make richer contexts, in order to find semantic similarity, \\\nor overlay thesaurus information in a supervised or semi-supervised manner.\n\\citet{tang2014learning} created sentiment-specific word embedding (SSWE), which\nwere trained for twitter sentiment. \n\\citet{yih2012polarity} proposed \npolarity induced latent semantic analysis (PILSA) using thesauri,\nwhich was extended by \\citet{chang2013multi} to a multi-relational setting.\nThe Bayesian tensor factorization model (BPTF) was introduced in order to\ncombine multiple sources of information \\citep{zhang2014word}.\n\\citet{faruqui2014retrofitting} used belief propagation to modify existing vector space representations.\nThe word embeddings on Thesauri and Distributional\ninformation (WE-TD) model \\citep{ono2015word} incorporated thesauri by \naltering the objective function for word embedding representations.\nSimilarly, \\citet{marcobaroni2multitask} introduced multitask Lexical Contrast Model\nwhich extended the word2vec Skip-gram method to optimize for both context as well as synonymy/antonym relations.\nOur approach differs from the afore-mentioned methods in that\nwe created word clusters using the antonym relationships as negative links.\nSimilar to \\citet{faruqui2014retrofitting} our signed clustering method uses existing vector representations to create word clusters.\n\nTo our knowledge, \\citet{gallier2015spectral} is the first theoretical foundation of multiclass signed normalized cuts. \n\\citet{hou2005bounds} used positive degrees of nodes in the degree matrix\nof a signed graph with weights (-1, 0, 1), which was advanced by  \n\\citet{kolluri2004spectral,kunegis2010spectral} using absolute values of weights in the degree matrix. \nAlthough must-link and cannot-link soft spectral clustering \\citep{rangapuramconstrained} both share similarities with our method, this similarity only applies to cases where cannot-link edges are present. Our method excludes a weight term of cannot-link, as well as the volume of cannot-link edges within the clusters.\nFurthermore, our optimization method differs from that of must-link / cannot-link algorithms.\nWe developed a novel theory and algorithm that extends the \nclustering of \\citet{shi2000normalized,yu2003multiclass} to the multi-class signed graph case \\citep{gallier2015spectral}.\n\n\n\n\n\n\n\n\\section{Signed Graph Cluster Estimation}\n\n\\subsection{Signed Normalized Cut}\n\nWeighted graphs\nfor which the weight matrix is a symmetric matrix in which negative\nand positive entries are allowed are called {\\it signed graphs\\/}.\nSuch graphs (with weights $(-1, 0, +1)$) were introduced as early as\n1953 by  \\cite{harary1953notion}, to model social relations involving disliking,\nindifference, and liking.  The problem of clustering the nodes\nof a signed graph arises naturally as a generalization of the\nclustering problem for weighted graphs. Figure 1 shows a signed graph of word similarities with a thesaurus overlay.\n\\begin{figure}[ht]\n  \\label{fig:dc2}\n  \\includegraphics[width=\\linewidth]{signed_word_cluster.jpg}\n  \\caption{Signed graph of words using \n  a distance metric from the word embedding. The red edges represent the antonym relation while blue edges represent synonymy relations.}\n\\end{figure}\n\\citet{gallier2015spectral} extends normalized cuts signed graphs in order to incorporate antonym information into word clusters.\n\n\n\n\\theoremstyle{definition}\n\\begin{definition}\n\\label{graph-weighted}\nA {\\it  weighted graph\\/} \nis a pair $G = (V, W)$, where \n$V = \\{v_1,  \\ldots, v_m\\}$ is a set of\n{\\it nodes\\/} or {\\it vertices\\/}, and $W$ is a symmetric matrix\ncalled the {\\it weight matrix\\/}, such that $w_{i\\, j} \\geq 0$\nfor all $i, j \\in \\{1, \\ldots, m\\}$, \nand $w_{i\\, i} = 0$ for $i = 1, \\ldots, m$.\nWe say that a set $\\{v_i, v_j\\}$  is an edge iff\n$w_{i\\, j} > 0$. The corresponding (undirected) graph $(V, E)$\nwith $E = \\{\\{v_i, v_j\\} \\mid w_{i\\, j} > 0\\}$, \nis called the {\\it underlying graph\\/} of $G$.\n\\end{definition}\n\nGiven a signed graph $G = (V, W)$ (where $W$ is a symmetric matrix\nwith zero diagonal entries), the {\\it underlying graph\\/} of $G$ is\nthe graph with node set $V$ and set of (undirected) edges\n$E = \\{\\{v_i, v_j\\} \\mid w_{i j} \\not= 0\\}$.\n\nIf $(V, W)$ is a signed graph, where $W$ is an $m\\times m$ symmetric\nmatrix with zero diagonal entries and with the other entries\n$w_{i j}\\in {\\mathbb{R}}$ arbitrary, for any node $v_i \\in V$, the {\\it signed degree\\/} of $v_i$ is defined as\n", "index": 1, "text": "\n\\[\n\\overline{d}_i = \\overline{d}(v_i) = \\sum_{j = 1}^m |w_{i j}|,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\overline{d}_{i}=\\overline{d}(v_{i})=\\sum_{j=1}^{m}|w_{ij}|,\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>d</mi><mo>\u00af</mo></mover><mi>i</mi></msub><mo>=</mo><mrow><mover accent=\"true\"><mi>d</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nFor any subset $A$ of the set of nodes\n$V$, let\n", "itemtype": "equation", "pos": 9253, "prevtext": "\nand the {\\it signed degree matrix \\/} $\\overline{{D}}$ as\n", "index": 3, "text": "\n\\[\n\\overline{{D}} = \\mathrm{diag}(\\overline{d}(v_1) , \\ldots, \\overline{d}(v_m)).\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\overline{{D}}=\\mathrm{diag}(\\overline{d}(v_{1}),\\ldots,\\overline{d}(v_{m})).\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>=</mo><mrow><mi>diag</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mover accent=\"true\"><mi>d</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mover accent=\"true\"><mi>d</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nFor any two subsets $A$ and $B$ of $V$, \ndefine $\\mathrm{links}^+(A,B)$, \n$\\mathrm{links}^-(A,B)$, and $\\mathrm{cut}(A,\\overline{A})$ by\n\n", "itemtype": "equation", "pos": 9386, "prevtext": "\nFor any subset $A$ of the set of nodes\n$V$, let\n", "index": 5, "text": "\n\\[\n\\mathrm{vol}(A) = \\sum_{v_i\\in A} \\overline{d}_i = \n \\sum_{v_i\\in A} \\sum_{j = 1}^m |w_{i j}|.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{vol}(A)=\\sum_{v_{i}\\in A}\\overline{d}_{i}=\\sum_{v_{i}\\in A}\\sum_{j=1}^%&#10;{m}|w_{ij}|.\" display=\"block\"><mrow><mrow><mrow><mi>vol</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><mi>A</mi></mrow></munder><msub><mover accent=\"true\"><mi>d</mi><mo>\u00af</mo></mover><mi>i</mi></msub></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><mi>A</mi></mrow></munder><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\n\n\n\n\n\nThen, the {\\it signed Laplacian\\/} $\\overline{L}$ is defined by\n", "itemtype": "equation", "pos": 9625, "prevtext": "\nFor any two subsets $A$ and $B$ of $V$, \ndefine $\\mathrm{links}^+(A,B)$, \n$\\mathrm{links}^-(A,B)$, and $\\mathrm{cut}(A,\\overline{A})$ by\n\n", "index": 7, "text": "\\begin{align*}\n\\mathrm{links}^+(A, B) & = \n\\sum_{\\begin{subarray}{c}\nv_i \\in A, v_j\\in B \\\\\nw_{i j} > 0\n\\end{subarray}\n}\nw_{i j}  \\\\\n\\mathrm{links}^-(A,B) & = \n\\sum_{\\begin{subarray}{c}\nv_i \\in A, v_j\\in B \\\\\nw_{i j} < 0\n\\end{subarray}\n}\n- w_{i j} \\\\\n\\mathrm{cut}(A,\\overline{A}) & = \n\\sum_{\\begin{subarray}{c}\nv_i \\in A, v_j\\in \\overline{A} \\\\\nw_{i j} \\not=  0\n\\end{subarray}\n}\n|w_{i j}| .\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{links}^{+}(A,B)\" display=\"inline\"><mrow><msup><mi>links</mi><mo>+</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{\\begin{subarray}{c}v_{i}\\in A,v_{j}\\in B\\\\&#10;w_{ij}&gt;0\\end{subarray}}w_{ij}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><mi>A</mi></mrow><mo>,</mo><mrow><msub><mi>v</mi><mi>j</mi></msub><mo>\u2208</mo><mi>B</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></munder></mstyle><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{links}^{-}(A,B)\" display=\"inline\"><mrow><msup><mi>links</mi><mo>-</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{\\begin{subarray}{c}v_{i}\\in A,v_{j}\\in B\\\\&#10;w_{ij}&lt;0\\end{subarray}}-w_{ij}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><mi>A</mi></mrow><mo>,</mo><mrow><msub><mi>v</mi><mi>j</mi></msub><mo>\u2208</mo><mi>B</mi></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></munder></mstyle><mo>-</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{cut}(A,\\overline{A})\" display=\"inline\"><mrow><mi>cut</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo>,</mo><mover accent=\"true\"><mi>A</mi><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{\\begin{subarray}{c}v_{i}\\in A,v_{j}\\in\\overline{A}\\\\&#10;w_{ij}\\not=0\\end{subarray}}|w_{ij}|.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mtable class=\"ltx_align_c\" rowspacing=\"0.0pt\"><mtr><mtd><mrow><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><mi>A</mi></mrow><mo>,</mo><mrow><msub><mi>v</mi><mi>j</mi></msub><mo>\u2208</mo><mover accent=\"true\"><mi>A</mi><mo>\u00af</mo></mover></mrow></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></mtd></mtr></mtable></munder></mstyle><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nand its normalized version $\\overline{L}_{\\mathrm{sym}}$ by\n", "itemtype": "equation", "pos": 10098, "prevtext": "\n\n\n\n\n\n\nThen, the {\\it signed Laplacian\\/} $\\overline{L}$ is defined by\n", "index": 9, "text": "\n\\[\n\\overline{L} = \\overline{{D}} - W, \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\overline{L}=\\overline{{D}}-W,\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>=</mo><mrow><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>-</mo><mi>W</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nFor a graph without isolated vertices, we have $\\overline{d}(v_i) > 0$\nfor $i = 1, \\ldots, m$, so $\\overline{{D}}^{-1/2}$ is well defined.\n\n\\begin{proposition}\n\\label{Laplace1s}\nFor any  $m\\times m$ symmetric matrix $W = (w_{i j})$, if we let $\\overline{L} = \\overline{{D}} - W$\nwhere $\\overline{{D}}$ is the signed degree matrix associated with $W$,\nthen  we have\n", "itemtype": "equation", "pos": 10200, "prevtext": "\nand its normalized version $\\overline{L}_{\\mathrm{sym}}$ by\n", "index": 11, "text": "\n\\[\n\\overline{L}_{\\mathrm{sym}} =  \\overline{{D}}^{-1/2}\\, \\overline{L}\\,\n\\overline{{D}}^{-1/2}\n= I - \\overline{{D}}^{-1/2} W \\overline{{D}}^{-1/2}.  \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\overline{L}_{\\mathrm{sym}}=\\overline{{D}}^{-1/2}\\,\\overline{L}\\,\\overline{{D}%&#10;}^{-1/2}=I-\\overline{{D}}^{-1/2}W\\overline{{D}}^{-1/2}.\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mi>sym</mi></msub><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mpadded><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover></mpadded><mo>\u2062</mo><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><mo>=</mo><mrow><mi>I</mi><mo>-</mo><mrow><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nConsequently, $\\overline{L}$  is positive semidefinite.\n\\end{proposition}\n\nGiven a partition of $V$ into $K$\nclusters $(A_1, \\ldots, A_K)$, if we represent the $j$th block of\nthis partition by a vector $X^j$ such that\n", "itemtype": "equation", "pos": 10718, "prevtext": "\nFor a graph without isolated vertices, we have $\\overline{d}(v_i) > 0$\nfor $i = 1, \\ldots, m$, so $\\overline{{D}}^{-1/2}$ is well defined.\n\n\\begin{proposition}\n\\label{Laplace1s}\nFor any  $m\\times m$ symmetric matrix $W = (w_{i j})$, if we let $\\overline{L} = \\overline{{D}} - W$\nwhere $\\overline{{D}}$ is the signed degree matrix associated with $W$,\nthen  we have\n", "index": 13, "text": "\n\\[\n{{x}^{\\top}} \\overline{L} x =\n\\frac{1}{2}\\sum_{i, j = 1}^m |w_{i  j}| (x_i - \\mathrm{sgn}(w_{i j}) x_j)^2\n\\quad\\mathrm{for\\ all}\\> x\\in {\\mathbb{R}}^m.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"{{x}^{\\top}}\\overline{L}x=\\frac{1}{2}\\sum_{i,j=1}^{m}|w_{ij}|(x_{i}-\\mathrm{%&#10;sgn}(w_{ij})x_{j})^{2}\\quad\\mathrm{for\\ all}\\&gt;x\\in{\\mathbb{R}}^{m}.\" display=\"block\"><mrow><mrow><mrow><mrow><msup><mi>x</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mi>x</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mrow><mi>sgn</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003</mo><mrow><mrow><mpadded width=\"+5pt\"><mi>for</mi></mpadded><mo>\u2062</mo><mpadded width=\"+2.2pt\"><mi>all</mi></mpadded><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2208</mo><msup><mi>\u211d</mi><mi>m</mi></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nfor some $a_j \\not= 0$.\n\n\\begin{definition}\n\\label{sncut}\nThe {\\it signed normalized cut\\/}\n$\\mathrm{sNcut}(A_1, \\ldots, A_K)$ of the\npartition $(A_1, ..., A_K)$ is defined as\n", "itemtype": "equation", "pos": 11094, "prevtext": "\nConsequently, $\\overline{L}$  is positive semidefinite.\n\\end{proposition}\n\nGiven a partition of $V$ into $K$\nclusters $(A_1, \\ldots, A_K)$, if we represent the $j$th block of\nthis partition by a vector $X^j$ such that\n", "index": 15, "text": "\n\\[\nX^j_i = \n\\begin{cases}\na_j & \\text{if $v_i \\in A_j$} \\\\\n0 &  \\text{if $v_i \\notin A_j$} ,\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"X^{j}_{i}=\\begin{cases}a_{j}&amp;\\text{if $v_{i}\\in A_{j}$}\\\\&#10;0&amp;\\text{if $v_{i}\\notin A_{j}$},\\end{cases}\" display=\"block\"><mrow><msubsup><mi>X</mi><mi>i</mi><mi>j</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msub><mi>a</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2208</mo><msub><mi>A</mi><mi>j</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>\u2209</mo><msub><mi>A</mi><mi>j</mi></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": " \n\\end{definition}\n\nAnother formulation is\n", "itemtype": "equation", "pos": -1, "prevtext": "\nfor some $a_j \\not= 0$.\n\n\\begin{definition}\n\\label{sncut}\nThe {\\it signed normalized cut\\/}\n$\\mathrm{sNcut}(A_1, \\ldots, A_K)$ of the\npartition $(A_1, ..., A_K)$ is defined as\n", "index": 17, "text": "\n\\[\n\\mathrm{sNcut}(A_1, \\ldots, A_K) = \\sum_{j = 1}^K\n\\frac{\\mathrm{cut}(A_j, \\overline{A_j}) + \n2 \\mathrm{links}^-(A_j, A_j)}{\\mathrm{vol}(A_j)}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{sNcut}(A_{1},\\ldots,A_{K})=\\sum_{j=1}^{K}\\frac{\\mathrm{cut}(A_{j},%&#10;\\overline{A_{j}})+2\\mathrm{links}^{-}(A_{j},A_{j})}{\\mathrm{vol}(A_{j})}.\" display=\"block\"><mrow><mrow><mrow><mi>sNcut</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>A</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><mrow><mrow><mi>cut</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>j</mi></msub><mo>,</mo><mover accent=\"true\"><msub><mi>A</mi><mi>j</mi></msub><mo>\u00af</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">l</mi><mo>\u2062</mo><mi mathvariant=\"normal\">i</mi><mo>\u2062</mo><mi mathvariant=\"normal\">n</mi><mo>\u2062</mo><mi mathvariant=\"normal\">k</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">s</mi><mo>-</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>j</mi></msub><mo>,</mo><msub><mi>A</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mi>vol</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nwhere $X$ is the $N\\times K$ matrix whose $j$th column is $X^j$.\n\n\nObserve that minimizing $\\mathrm{sNcut}(A_1, \\ldots, A_K)$ amounts to \nminimizing the number of positive and negative edges between clusters,\nand also minimizing the number of negative edges within clusters.\nThis second minimization captures the intuition that nodes connected\nby a negative edge should not be together  (they do not ``like''\neach other; they should be far from each other).\n\n\\subsection{Optimization Problem}\n\n\nWe have our first formulation of $K$-way clustering\nof a graph using normalized cuts, called problem PNC1 \n(the notation PNCX  is used in  Yu \\cite{yu2003multiclass}, Section 2.1):\n\nIf we let\n\n", "itemtype": "equation", "pos": 11569, "prevtext": " \n\\end{definition}\n\nAnother formulation is\n", "index": 19, "text": "\n\\[\n\\mathrm{sNcut}(A_1, \\ldots, A_K) = \n\\sum_{j = 1}^K \\frac{{{(X^j)}^{\\top}} \\overline{L} X^j} \n  {{{(X^j)}^{\\top}} \\overline{{D}} X^j}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{sNcut}(A_{1},\\ldots,A_{K})=\\sum_{j=1}^{K}\\frac{{{(X^{j})}^{\\top}}%&#10;\\overline{L}X^{j}}{{{(X^{j})}^{\\top}}\\overline{{D}}X^{j}}.\" display=\"block\"><mrow><mrow><mrow><mi>sNcut</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>A</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>j</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mi>X</mi><mi>j</mi></msup></mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>j</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mi>X</mi><mi>j</mi></msup></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nour solution set is\n", "itemtype": "equation", "pos": 12397, "prevtext": "\nwhere $X$ is the $N\\times K$ matrix whose $j$th column is $X^j$.\n\n\nObserve that minimizing $\\mathrm{sNcut}(A_1, \\ldots, A_K)$ amounts to \nminimizing the number of positive and negative edges between clusters,\nand also minimizing the number of negative edges within clusters.\nThis second minimization captures the intuition that nodes connected\nby a negative edge should not be together  (they do not ``like''\neach other; they should be far from each other).\n\n\\subsection{Optimization Problem}\n\n\nWe have our first formulation of $K$-way clustering\nof a graph using normalized cuts, called problem PNC1 \n(the notation PNCX  is used in  Yu \\cite{yu2003multiclass}, Section 2.1):\n\nIf we let\n\n", "index": 21, "text": "\\begin{align*}\n{\\cal {X}}  = \\Big\\{[X^1\\> \\ldots \\> X^K] \\mid\nX^j = a_j(x_1^j, \\ldots, x_N^j) , \\\\\n\\>\nx_i^j \\in \\{1, 0\\},\n a_j\\in {\\mathbb{R}}, \\> X^j \\not= 0\n\\Big\\}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\cal{X}}=\\Big{\\{}[X^{1}\\&gt;\\ldots\\&gt;X^{K}]\\mid X^{j}=a_{j}(x_{1}^{j%&#10;},\\ldots,x_{N}^{j}),\" display=\"inline\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>=</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">{</mo><mrow><mo stretchy=\"false\">[</mo><mpadded width=\"+2.2pt\"><msup><mi>X</mi><mn>1</mn></msup></mpadded><mpadded width=\"+2.2pt\"><mi mathvariant=\"normal\">\u2026</mi></mpadded><msup><mi>X</mi><mi>K</mi></msup><mo stretchy=\"false\">]</mo></mrow><mo>\u2223</mo><msup><mi>X</mi><mi>j</mi></msup><mo>=</mo><msub><mi>a</mi><mi>j</mi></msub><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mn>1</mn><mi>j</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msubsup><mi>x</mi><mi>N</mi><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\&gt;x_{i}^{j}\\in\\{1,0\\},a_{j}\\in{\\mathbb{R}},\\&gt;X^{j}\\not=0\\Big{\\}}\" display=\"inline\"><mrow><msubsup><mi>x</mi><mi>i</mi><mi>j</mi></msubsup><mo>\u2208</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow><mo>,</mo><msub><mi>a</mi><mi>j</mi></msub><mo>\u2208</mo><mi>\u211d</mi><mo rspace=\"4.7pt\">,</mo><msup><mi>X</mi><mi>j</mi></msup><mo>\u2260</mo><mn>0</mn><mo maxsize=\"160%\" minsize=\"160%\">}</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\n\\medskip\\noindent\n{\\bf $K$-way Clustering of a graph using Normalized Cut, Version 1: \\\\\nProblem PNC1}\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 12595, "prevtext": "\nour solution set is\n", "index": 23, "text": "\n\\[\n{\\cal {K}}  = \\big\\{\nX  \\in{\\cal {X}}  \\mid {{X}^{\\top}}  \\overline{{D}}\\mathbf{1} = 0\n\\big\\}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"{\\cal{K}}=\\big{\\{}X\\in{\\cal{X}}\\mid{{X}^{\\top}}\\overline{{D}}\\mathbf{1}=0\\big{%&#10;\\}}.\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca6</mi><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>\u2223</mo><mrow><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mn>\ud835\udfcf</mn></mrow><mo>=</mo><mn>0</mn></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nAn equivalent version of the optimization problem is\n\n\\medskip\\noindent\n\n\n{\\bf Problem PNC2}\n\n\n", "itemtype": "equation", "pos": 12811, "prevtext": "\n\n\\medskip\\noindent\n{\\bf $K$-way Clustering of a graph using Normalized Cut, Version 1: \\\\\nProblem PNC1}\n\n\n\n\n\n\n\n\n\n\n\n", "index": 25, "text": "\\begin{align*}\n& \\mathrm{minimize}     &  \\sum_{j = 1}^K \n\\frac{{{(X^j)}^{\\top}} \\overline{L} X^j}{{{(X^j)}^{\\top}}\\overline{{D}} X^j}& &  &  &\\\\\n& \\mathrm{subject\\ to} & \n {{(X^i)}^{\\top}} \\overline{{D}} X^j = 0,  & &  &  &\\\\\n& & \\quad 1\\leq i, j \\leq K,\\> \ni\\not= j,  & &  X\\in {\\cal {X}}. & &  & \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{minimize}\" display=\"inline\"><mi>minimize</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{j=1}^{K}\\frac{{{(X^{j})}^{\\top}}\\overline{L}X^{j}}{{{(X^{j}%&#10;)}^{\\top}}\\overline{{D}}X^{j}}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>j</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mi>X</mi><mi>j</mi></msup></mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>j</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mi>X</mi><mi>j</mi></msup></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{subject\\ to}\" display=\"inline\"><mrow><mpadded width=\"+5pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mi>to</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{{(X^{i})}^{\\top}}\\overline{{D}}X^{j}=0,\" display=\"inline\"><mrow><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mi>X</mi><mi>j</mi></msup></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\quad 1\\leq i,j\\leq K,\\&gt;i\\not=j,\" display=\"inline\"><mrow><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2003</mi><mo>\u2062</mo><mn>1</mn></mrow><mo>\u2264</mo><mi>i</mi></mrow><mo>,</mo><mrow><mrow><mi>j</mi><mo>\u2264</mo><mi>K</mi></mrow><mo rspace=\"4.7pt\">,</mo><mrow><mi>i</mi><mo>\u2260</mo><mi>j</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle X\\in{\\cal{X}}.\" display=\"inline\"><mrow><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\n\nThe natural relaxation of problem PNC2 is to drop the condition\nthat $X\\in {\\cal {X}}$, and we obtain the \n\n\\medskip\\noindent\n{\\bf Problem $(*_2)$}\n\n\n", "itemtype": "equation", "pos": 13219, "prevtext": "\n\nAn equivalent version of the optimization problem is\n\n\\medskip\\noindent\n\n\n{\\bf Problem PNC2}\n\n\n", "index": 27, "text": "\\begin{align*}\n& \\mathrm{minimize}     &  &  \n\\mathrm{tr}({{X}^{\\top}} \\overline{L} X)& &  &  &\\\\\n& \\mathrm{subject\\ to} &  & \n{{X}^{\\top}} \\overline{{D}} X = I, \n & &  X\\in {\\cal {X}}. & &\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{minimize}\" display=\"inline\"><mi>minimize</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{tr}({{X}^{\\top}}\\overline{L}X)\" display=\"inline\"><mrow><mi>tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{subject\\ to}\" display=\"inline\"><mrow><mpadded width=\"+5pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mi>to</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{{X}^{\\top}}\\overline{{D}}X=I,\" display=\"inline\"><mrow><mrow><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo>=</mo><mi>I</mi></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle X\\in{\\cal{X}}.\" display=\"inline\"><mrow><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nIf $X$ is a solution to the relaxed problem, then $XQ$ is also a solution, where $Q\\in\\mathbf{O}(K)$.\n\nIf we make the change of variable $Y = \\overline{{D}}^{1/2} X$ or equivalently\n$X = \\overline{{D}}^{-1/2} Y$.\n\nHowever, since ${{Y}^{\\top}} Y = I$, we have\n", "itemtype": "equation", "pos": 13573, "prevtext": "\n\n\nThe natural relaxation of problem PNC2 is to drop the condition\nthat $X\\in {\\cal {X}}$, and we obtain the \n\n\\medskip\\noindent\n{\\bf Problem $(*_2)$}\n\n\n", "index": 29, "text": "\\begin{align*}\n& \\mathrm{minimize}     &  &  \n\\mathrm{tr}({{X}^{\\top}} \\overline{L} X)& &  &  &\\\\\n& \\mathrm{subject\\ to} &  & \n{{X}^{\\top}} \\overline{{D}} X = I, \n & &  & & \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{minimize}\" display=\"inline\"><mi>minimize</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{tr}({{X}^{\\top}}\\overline{L}X)\" display=\"inline\"><mrow><mi>tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{subject\\ to}\" display=\"inline\"><mrow><mpadded width=\"+5pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mi>to</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{{X}^{\\top}}\\overline{{D}}X=I,\" display=\"inline\"><mrow><mrow><mrow><msup><mi>X</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mo>\u2062</mo><mi>X</mi></mrow><mo>=</mo><mi>I</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nso we get the equivalent problem\n\n\n\\medskip\\noindent\n{\\bf Problem $(**_2)$}\n\n\n", "itemtype": "equation", "pos": 14019, "prevtext": "\n\nIf $X$ is a solution to the relaxed problem, then $XQ$ is also a solution, where $Q\\in\\mathbf{O}(K)$.\n\nIf we make the change of variable $Y = \\overline{{D}}^{1/2} X$ or equivalently\n$X = \\overline{{D}}^{-1/2} Y$.\n\nHowever, since ${{Y}^{\\top}} Y = I$, we have\n", "index": 31, "text": "\n\\[\nY^+ = {{Y}^{\\top}},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"Y^{+}={{Y}^{\\top}},\" display=\"block\"><mrow><mrow><msup><mi>Y</mi><mo>+</mo></msup><mo>=</mo><msup><mi>Y</mi><mo>\u22a4</mo></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nThe minimum of  problem $(**_2)$\nis achieved by any $K$ unit eigenvectors $(u_1, \\ldots, u_K)$ associated with the smallest\neigenvalues\n", "itemtype": "equation", "pos": 14123, "prevtext": "\nso we get the equivalent problem\n\n\n\\medskip\\noindent\n{\\bf Problem $(**_2)$}\n\n\n", "index": 33, "text": "\\begin{align*}\n& \\mathrm{minimize}     &  &  \n\\mathrm{tr}({{Y}^{\\top}}\\overline{{D}}^{-1/2} \\overline{L} \\overline{{D}}^{-1/2} Y)& &  &  &\\\\\n& \\mathrm{subject\\ to} &  & \n{{Y}^{\\top}} Y = I. \n & &  & &  \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{minimize}\" display=\"inline\"><mi>minimize</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{tr}({{Y}^{\\top}}\\overline{{D}}^{-1/2}\\overline{L}%&#10;\\overline{{D}}^{-1/2}Y)\" display=\"inline\"><mrow><mi>tr</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mover accent=\"true\"><mi>L</mi><mo>\u00af</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>D</mi><mo>\u00af</mo></mover><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathrm{subject\\ to}\" display=\"inline\"><mrow><mpadded width=\"+5pt\"><mi>subject</mi></mpadded><mo>\u2062</mo><mi>to</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{{Y}^{\\top}}Y=I.\" display=\"inline\"><mrow><mrow><mrow><msup><mi>Y</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mi>Y</mi></mrow><mo>=</mo><mi>I</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nof $L_{\\mathrm{sym}}$.\n\n\\subsection{Finding an Approximate Discrete Solution}\n\nGiven a solution $Z$ of problem $(*_2)$, \nwe  look for pairs\n$(X, Q)$ with $X\\in {\\cal {X}}$ and  where $Q$ is a $K\\times K$ matrix with\nnonzero and pairwise orthogonal columns,\nwith $\\left\\|{X}\\right\\|_F = \\left\\|{Z}\\right\\|_F$,\nthat minimize\n", "itemtype": "equation", "pos": 14475, "prevtext": "\n\nThe minimum of  problem $(**_2)$\nis achieved by any $K$ unit eigenvectors $(u_1, \\ldots, u_K)$ associated with the smallest\neigenvalues\n", "index": 35, "text": "\n\\[\n0 = \\nu_1\\leq  \\nu_2 \\leq  \\ldots \\leq  \\nu_K\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"0=\\nu_{1}\\leq\\nu_{2}\\leq\\ldots\\leq\\nu_{K}\" display=\"block\"><mrow><mn>0</mn><mo>=</mo><msub><mi>\u03bd</mi><mn>1</mn></msub><mo>\u2264</mo><msub><mi>\u03bd</mi><mn>2</mn></msub><mo>\u2264</mo><mi mathvariant=\"normal\">\u2026</mi><mo>\u2264</mo><msub><mi>\u03bd</mi><mi>K</mi></msub></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nHere, $\\left\\|{A}\\right\\|_F$ is the Frobenius norm of $A$.\n\nThis is a difficult nonlinear optimization problem \ninvolving two unknown matrices $X$ and $Q$. \nTo simplify the problem,\nwe proceed by alternating steps during which we minimize \n$\\varphi(X, Q) = \\left\\|{X - ZQ}\\right\\|_F$ with respect to $X$ holding $Q$\nfixed, and steps during which we minimize \n$\\varphi(X, Q) = \\left\\|{X - ZQ}\\right\\|_F$ with respect to $Q$ holding $X$\nfixed. \n\nThis second step in which $X$ is held fixed has been studied, but it\nis still a hard problem for which no closed--form solution is known.\nConsequently, we further simplify the problem.\nSince $Q$ is of the form $Q = R\\Lambda$ where\n$R\\in \\mathbf{O}(K)$ and $\\Lambda$ is a diagonal invertible matrix,\nwe minimize $\\left\\|{X - ZR\\Lambda}\\right\\|_F$ in two stages.\n\\begin{enumerate}\n\\item\nWe set $\\Lambda = I$ and find $R\\in \\mathbf{O}(K)$\nthat minimizes  $\\left\\|{X - ZR}\\right\\|_F$.\n\\item\nGiven $X$, $Z$, and $R$,  find a \ndiagonal invertible matrix $\\Lambda$ that\nminimizes  $\\left\\|{X - ZR\\Lambda}\\right\\|_F$.\n\\end{enumerate}\n\nThe matrix $R\\Lambda$ is not a minimizer of\n$\\left\\|{X - ZR\\Lambda}\\right\\|_F$ in general, but it is an improvement\non $R$ alone, and both stages can be solved quite easily.\n\nIn stage 1, the matrix $Q=R$ is orthogonal, so $Q{{Q}^{\\top}} = I$, and\nsince $Z$ and $X$ are given, \nthe problem reduces to minimizing\n$ - 2\\mathrm{tr}({{Q}^{\\top}}{{Z}^{\\top}}X)$; that is,\nmaximizing   $\\mathrm{tr}({{Q}^{\\top}}{{Z}^{\\top}}X)$.\n\n\n\\section{Metrics}\n\nThe evaluation of clusters is non-trivial to generalize. We used both intrinsic and extrinsic methods of evaluation.\nIntrinsic evaluation is two fold where we only examine cluster entropy, purity, number of disconnected components and \nnumber of negative edges. We also compare multiple word embeddings and thesauri to show stability of our method.\nThe second intrinsic measure is using a gold standard. We chose a gold standard \ndesigned for the task of capturing word similarity. Our metric for evaluation is a detailed\naccuracy and recall. \nFor extrinsic evaluation, we use our clusters to\nidentify polarity and apply this to the task. \n\n\\subsection{Similarity Metric and Edge Weight}\nFor clustering there are several choices to make. The first choice being the similarity metric.\nIn this paper we chose the heat kernel based off of Euclidean distance between word vector\nrepresentations. We define the distance between two words $w_i$ and $w_j$ as \n$dist(w_i, w_j) = \\left\\|{w_i - w_j}\\right\\|$.\nIn the paper by \\citet{belkin2003laplacian}, the authors show that the \nheat kernel where\n", "itemtype": "equation", "pos": 14850, "prevtext": "\nof $L_{\\mathrm{sym}}$.\n\n\\subsection{Finding an Approximate Discrete Solution}\n\nGiven a solution $Z$ of problem $(*_2)$, \nwe  look for pairs\n$(X, Q)$ with $X\\in {\\cal {X}}$ and  where $Q$ is a $K\\times K$ matrix with\nnonzero and pairwise orthogonal columns,\nwith $\\left\\|{X}\\right\\|_F = \\left\\|{Z}\\right\\|_F$,\nthat minimize\n", "index": 37, "text": "\n\\[\n\\varphi(X, Q) = \\left\\|{X - ZQ}\\right\\|_F.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"\\varphi(X,Q)=\\left\\|{X-ZQ}\\right\\|_{F}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>X</mi><mo>-</mo><mrow><mi>Z</mi><mo>\u2062</mo><mi>Q</mi></mrow></mrow><mo>\u2225</mo></mrow><mi>F</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\n\n\n\n\n\n\n\nThe next choice of how to combine the word embeddings with the thesauri in order to make a signed \ngraph also has hyperparameters. We can represent the thesaurus as a matrix where \n", "itemtype": "equation", "pos": 17502, "prevtext": "\nHere, $\\left\\|{A}\\right\\|_F$ is the Frobenius norm of $A$.\n\nThis is a difficult nonlinear optimization problem \ninvolving two unknown matrices $X$ and $Q$. \nTo simplify the problem,\nwe proceed by alternating steps during which we minimize \n$\\varphi(X, Q) = \\left\\|{X - ZQ}\\right\\|_F$ with respect to $X$ holding $Q$\nfixed, and steps during which we minimize \n$\\varphi(X, Q) = \\left\\|{X - ZQ}\\right\\|_F$ with respect to $Q$ holding $X$\nfixed. \n\nThis second step in which $X$ is held fixed has been studied, but it\nis still a hard problem for which no closed--form solution is known.\nConsequently, we further simplify the problem.\nSince $Q$ is of the form $Q = R\\Lambda$ where\n$R\\in \\mathbf{O}(K)$ and $\\Lambda$ is a diagonal invertible matrix,\nwe minimize $\\left\\|{X - ZR\\Lambda}\\right\\|_F$ in two stages.\n\\begin{enumerate}\n\\item\nWe set $\\Lambda = I$ and find $R\\in \\mathbf{O}(K)$\nthat minimizes  $\\left\\|{X - ZR}\\right\\|_F$.\n\\item\nGiven $X$, $Z$, and $R$,  find a \ndiagonal invertible matrix $\\Lambda$ that\nminimizes  $\\left\\|{X - ZR\\Lambda}\\right\\|_F$.\n\\end{enumerate}\n\nThe matrix $R\\Lambda$ is not a minimizer of\n$\\left\\|{X - ZR\\Lambda}\\right\\|_F$ in general, but it is an improvement\non $R$ alone, and both stages can be solved quite easily.\n\nIn stage 1, the matrix $Q=R$ is orthogonal, so $Q{{Q}^{\\top}} = I$, and\nsince $Z$ and $X$ are given, \nthe problem reduces to minimizing\n$ - 2\\mathrm{tr}({{Q}^{\\top}}{{Z}^{\\top}}X)$; that is,\nmaximizing   $\\mathrm{tr}({{Q}^{\\top}}{{Z}^{\\top}}X)$.\n\n\n\\section{Metrics}\n\nThe evaluation of clusters is non-trivial to generalize. We used both intrinsic and extrinsic methods of evaluation.\nIntrinsic evaluation is two fold where we only examine cluster entropy, purity, number of disconnected components and \nnumber of negative edges. We also compare multiple word embeddings and thesauri to show stability of our method.\nThe second intrinsic measure is using a gold standard. We chose a gold standard \ndesigned for the task of capturing word similarity. Our metric for evaluation is a detailed\naccuracy and recall. \nFor extrinsic evaluation, we use our clusters to\nidentify polarity and apply this to the task. \n\n\\subsection{Similarity Metric and Edge Weight}\nFor clustering there are several choices to make. The first choice being the similarity metric.\nIn this paper we chose the heat kernel based off of Euclidean distance between word vector\nrepresentations. We define the distance between two words $w_i$ and $w_j$ as \n$dist(w_i, w_j) = \\left\\|{w_i - w_j}\\right\\|$.\nIn the paper by \\citet{belkin2003laplacian}, the authors show that the \nheat kernel where\n", "index": 39, "text": "\n\\[\nW_{ij} = \n\\begin{cases}\n0 & \\text{ if } e^{-\\frac{dist(w_i, w_j)^2}{\\sigma}} < thresh \\\\\ne^{-\\frac{dist(w_i, w_j)^2}{\\sigma}} & \\text{ otherwise}\n\\end{cases}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"W_{ij}=\\begin{cases}0&amp;\\text{ if }e^{-\\frac{dist(w_{i},w_{j})^{2}}{\\sigma}}&lt;%&#10;thresh\\\\&#10;e^{-\\frac{dist(w_{i},w_{j})^{2}}{\\sigma}}&amp;\\text{ otherwise}\\end{cases}.\" display=\"block\"><mrow><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo><msub><mi>w</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mi>\u03c3</mi></mfrac></mrow></msup></mrow><mo>&lt;</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>h</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><msup><mi>e</mi><mrow><mo>-</mo><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo><msub><mi>w</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mi>\u03c3</mi></mfrac></mrow></msup></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\nAnother alternative is to only look at the antonym information, so\n", "itemtype": "equation", "pos": 17856, "prevtext": "\n\n\n\n\n\n\n\n\nThe next choice of how to combine the word embeddings with the thesauri in order to make a signed \ngraph also has hyperparameters. We can represent the thesaurus as a matrix where \n", "index": 41, "text": "\n\\[\nT_{ij} = \n\\begin{cases}\n1 & \\text{ if words $i$ and $j$ are synonyms} \\\\\n-1 & \\text{ if words $i$ and $j$ are antonyms} \\\\\n0 & \\text{ otherwise} \\\\\n\\end{cases}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"T_{ij}=\\begin{cases}1&amp;\\text{ if words $i$ and $j$ are synonyms}\\\\&#10;-1&amp;\\text{ if words $i$ and $j$ are antonyms}\\\\&#10;0&amp;\\text{ otherwise}\\\\&#10;\\end{cases}.\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>\u00a0if words\u00a0</mtext><mi>i</mi><mtext>\u00a0and\u00a0</mtext><mi>j</mi><mtext>\u00a0are synonyms</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>\u00a0if words\u00a0</mtext><mi>i</mi><mtext>\u00a0and\u00a0</mtext><mi>j</mi><mtext>\u00a0are antonyms</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nWe can write the signed graph as $\\hat{W_{ij}} = \\beta T_{ij} W_{ij}$ or in matrix form\n$\\hat{W} =\\beta T \\odot W$  where $\\odot$ computes Hadamard product (element-wise multiplication); however,\n\nthe graph will only contain the overlapping vocabulary. \n\n\n\n\n\n\n\n\nIn order to solve this problem we use $\\hat{W} = \\gamma W + \\beta^{ant} T^{ant} \\odot W  + \\beta T \\odot W$.\n\n\n\\subsection{Evaluation Metrics}\n\n\n\n\n\n\n\n\n\n\n\nIt is important to note that this metric does not require a gold standard.\nObviously we want this number to be as small as possible.\n\n\n\n\n\n\n\nAs we used thesaurus information for two other novel metrics which are the number of\nnegative edges (NNE) in the clusters, and the number of disconnected components (NDC) in the cluster where we only use synonym edges. \n\n", "itemtype": "equation", "pos": 18090, "prevtext": "\nAnother alternative is to only look at the antonym information, so\n", "index": 43, "text": "\n\\[\nT^{ant}_{ij} = \n\\begin{cases}\n-1 & \\text{ if words $i$ and $j$ are antonyms} \\\\\n0 & \\text{ otherwise} \\\\\n\\end{cases}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"T^{ant}_{ij}=\\begin{cases}-1&amp;\\text{ if words $i$ and $j$ are antonyms}\\\\&#10;0&amp;\\text{ otherwise}\\\\&#10;\\end{cases}.\" display=\"block\"><mrow><mrow><msubsup><mi>T</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>t</mi></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign=\"left\"><mrow><mtext>\u00a0if words\u00a0</mtext><mi>i</mi><mtext>\u00a0and\u00a0</mtext><mi>j</mi><mtext>\u00a0are antonyms</mtext></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise</mtext></mtd></mtr></mtable></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nThe NDC has the disadvantage of thesaurus coverage. \n\n\n\nFigure 2 shows a graphical representation of the number of disconnected components and negative edges.\n\n\\begin{figure}[ht]\n  \\label{fig:dc2}\n\n\n\n\n  \n\n\n\n\n\n  \\begin{subfigure}\n  \\par\n    \\label{fig:figure2}\n    \\includegraphics[width=\\linewidth]{cluster_2_dc.jpg}\n    {\\small {\\it Figure 2.1.} Cluster with two disconnected components. \n    All edges represent synonymy relations. The edge colors are only meant to highlight the different components.}\n  \\end{subfigure}\n  \\begin{subfigure}\n  \\par\n    \\label{fig:ant1}\n    \\includegraphics[width=\\linewidth]{cluster_1_ant.jpg}\n    {\\small {\\it Figure 2.1.} Cluster with one antonym relation. \n    The red edge represents the antonym relation. Blue edges represent synonymy relations.}\n  \\end{subfigure}\n  \\caption{Disconnected component and number of antonym evaluations.}\n\\end{figure}\n\n\n\nNext we evaluate our clusters using an external gold standard. \nCluster purity and entropy \\cite{zhao2001criterion} is defined as, \n\n", "itemtype": "equation", "pos": 18992, "prevtext": "\n\nWe can write the signed graph as $\\hat{W_{ij}} = \\beta T_{ij} W_{ij}$ or in matrix form\n$\\hat{W} =\\beta T \\odot W$  where $\\odot$ computes Hadamard product (element-wise multiplication); however,\n\nthe graph will only contain the overlapping vocabulary. \n\n\n\n\n\n\n\n\nIn order to solve this problem we use $\\hat{W} = \\gamma W + \\beta^{ant} T^{ant} \\odot W  + \\beta T \\odot W$.\n\n\n\\subsection{Evaluation Metrics}\n\n\n\n\n\n\n\n\n\n\n\nIt is important to note that this metric does not require a gold standard.\nObviously we want this number to be as small as possible.\n\n\n\n\n\n\n\nAs we used thesaurus information for two other novel metrics which are the number of\nnegative edges (NNE) in the clusters, and the number of disconnected components (NDC) in the cluster where we only use synonym edges. \n\n", "index": 45, "text": "\\begin{align*}\nNDC &= \\sum_{r=1}^{k}{\\sum_{i=1}^{C}{(n_r^i)}}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle NDC\" display=\"inline\"><mrow><mi>N</mi><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mi>C</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{r=1}^{k}{\\sum_{i=1}^{C}{(n_{r}^{i})}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover></mstyle><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>n</mi><mi>r</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05403.tex", "nexttext": "\n\nwhere $q$ is the number of classes, $k$ the number of clusters, $n_r$ is the size of cluster $r$, and $n_r^i$\nnumber of data points in class $i$ clustered in cluster $r$.\nThe purity and entropy measures improve (increased purity, decreased entropy) monotonically with the number of clusters.\n\n\\section{Empirical Results}\n\nIn this section we begin with intrinsic analysis of the resulting clusters. \nWe then compare empirical clusters with \nSimLex-999 as a gold standard for semantic word similarity.\nFinally, we evaluate our metric using the sentiment prediction task.\nOur synonym clusters are well suited for\nthis task, as including antonyms in clusters results in incorrect predictions.\n\n\n\\subsection{Simulated Data}\nIn order to evaluate our signed graph clustering method, we first focused on intrinsic measures of cluster quality.\n Figure 3.2  demonstrates that the number of negative edges within a cluster is minimized using our clustering algorithm on simulated data. However, as the number of clusters becomes large, the number of disconnected components, which includes clusters of size one, increases. For our further empirical analysis, we used both the number of disconnected components as well as the number of antonyms within clusters in order to set the cluster size.\n\n\\begin{figure}[ht]\n  \\label{fig:dcnemetric}\n  \\begin{subfigure}\n  \\par\n    \\label{fig:simgraph}\n    \\includegraphics[width=\\linewidth]{simulated_signed_graph.jpg}\n    {\\small {\\it Figure 3.1.} Simulated signed graph \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\n  \\end{subfigure}\n  \\begin{subfigure}\n  \\par\n    \\label{fig:ant1}\n    \\includegraphics[width=\\linewidth]{DC_NE_vs_NC.jpg}\n    {\\small {\\it Figure 3.2.} This is a plot of the relationship between the number of disconnected components and negative edges within the clusters.}\n  \\end{subfigure}\n  \\caption{Graph of Disconnected Component and Negative Edge Relations}\n\\end{figure}\n\n\\subsection{Data}\n\n\\subsubsection{Word Embeddings}\n\nFor comparison, we used four different word embedding methods: Skip-gram vectors (word2vec) \\citep{mikolov2013efficient}, Global vectors (GloVe) \\citep{pennington2014glove}, Eigenwords \\citep{dhillon2015eigenwords}, and Global Context (GloCon) \\citep{huang2012improving} vector representation. \n\nWe used word2vec 300 dimensional embeddings which were trained using word2vec code on several billion words of English comprising the entirety of Gigaword and the English discussion forum data gathered as part of BOLT. A minimal tokenization was performed based on CMU's twoknenize\\footnote{\\url{https://github.com/brendano/ark-tweet-nlp}}. \nFor GloVe we used pretrained 200 dimensional vector embeddings\\footnote{\\url{http://nlp.stanford.edu/projects/GloVe/}} trained using Wikipedia 2014 + Gigaword 5 (6B tokens). \nEigenwords were trained on English Gigaword with no lowercasing or cleaning.\nFinally, we used 50 dimensional vector representations from \\citet{huang2012improving}, which\nused the April 2010 snapshot of the Wikipedia corpus \\cite{lin1998automatic,shaoul2010westbury},\nwith a total of about 2 million articles and 990 million tokens.\n\n\\subsubsection{Thesauri}\n\nSeveral thesauri were used, in order to test robustness (including Roget's Thesaurus,\nthe Microsoft Word English (MS Word) thesaurus from \\citet{samsonovic2010principal} and WordNet 3.0) \\citep{miller1995wordnet}.\n\\citet{jarmasz2004roget,hale1998comparison} have shown that Roget's thesaurus has better semantic similarity than WordNet.\nThis is consistent with our results using a larger dataset of SimLex-999. \n\n\nWe chose a subset of 5108 words for the training dataset, which had high overlap between various sources. Changes to the training dataset had minimal effects on the optimal parameters. \nWithin the training dataset, each of the thesauri had roughly 3700 antonym pairs, and combined they had 6680. However, the number of distinct connected components varied, with Roget's Thesaurus having the least (629), and MS Word Thesaurus (1162) and WordNet (2449) having the most. These ratios were consistent across the full dataset.\n\n\\subsection{Cluster Evaluation}\n\nOne of our main goals was to go beyond qualitative analysis into quantitative measures of \nsynonym clusters and word similarity. \n\nIn Table \\ref{tab:qualclusters}, we show the 4 most-associated words with ``accept'', ``negative'' and ``unlike''.\n\n\n\\begin{table*}[!tbh]\n  \\centering\n  \\small\n  \\begin{tabular}{|l||l|l|l|l|l|l|l|}\n  \\hline \n{\\bf Ref word} & {\\bf Roget} & {\\bf WordNet} & {\\bf MS Word} & {\\bf W2V} & {\\bf GloDoc} & {\\bf EW} & {\\bf Glove} \\\\ \\hline\n\n\n\n\n\n\n\n\naccept \n& adopt & agree & take & accepts & seek & approve & agree \\\\ \n& accept your fate & get & swallow & reject & consider & declare & reject \\\\\n& be fooled by & fancy & consent & agree & know & endorse & willin \\\\ \n& acquiesce & hold & assume & accepting & ask & reconsider & refuse \\\\\n\n\n\nnegative\n& not advantageous & unfavorable & severe & positive & reverse & unfavorable & positive \\\\\n& pejorative & denial & hard & adverse & obvious & positive & impact \\\\ \n& pessimistic & resisting & wasteful & Negative & calculation & dire & suggesting \\\\ \n& no & pessimistic & charged & negatively & cumulative & worrisome & result \\\\ \n\n\n\n\n\\hline\nunlike \n&  {\\bf no synonyms} & incongruous & different & Unlike & whereas & Unlike & instance \\\\\n& & unequal & dissimilar & Like & true & Like & though \\\\ \n& & separate &  & even & though & Whereas & whereas \\\\ \n& & hostile &  & But & bit & whereas & likewise \\\\ \n\n\n\n\n\\hline\n\n\n\n \\end{tabular}\n   \\caption{Qualitative comparison of clusters.}\n  \\label{tab:qualclusters}\n\\end{table*}\n\n\\subsubsection{Cluster Similarity and Hyperparameter Optimization}\n\nFor a similarity metric between any two words, we use the heat kernel of Euclidean distance, so $sim(w_i, w_j) = e^{-\\frac{\\left\\|{w_i-w_j}\\right\\|^2}{\\sigma}}$.\nThe thesaurus matrix entry $T_{ij}$ has a weight of 1 if words $i$ and $j$ are synonyms, -1  if words $i$ and $j$ are antonyms, and 0 otherwise. Thus the weight matrix entries $W_{ij} = T_{ij}e^{-\\frac{\\left\\|{w_i-w_j}\\right\\|^2}{\\sigma}}$.\n\n\n\\begin{table*}[!tbh]\n  \\centering\n  \\small\n  \\begin{tabular}{|l||p{1cm}|c|c|c|c|c|}\n  \\hline \n\\multicolumn{1}{|c||}{\\bf Method} & \n{\\bf $\\sigma$} & \n{\\bf thresh} & \n{\\bf \\# Clusters} & \n{\\bf Error  $\\downarrow$ } & \n\n{\\bf Purity $\\uparrow$} &\n{\\bf Entropy $\\downarrow$}   \\\\\n                 &       &      &     & $\\frac{(NNE+NDC)}{|V|}$ & & \\\\ \\hline \nWord2Vec         & 0.2   & 0.04 & 750  & 0.716 & 0.88 & 0.14 \\\\ \\hline\n\nWord2Vec + Roget & 0.7   & 0.04 & 750  & 0.033 & 0.94 & 0.09 \\\\ \\hline \nEigenword        & 2.0   & 0.07 & 200  & 0.655 & 0.84 & 0.25 \\\\ \\hline\nEigenword + MSW  & 1.0   & 0.08 & 200  & 0.042 & 0.95 & 0.01 \\\\ \\hline \nGloCon            & 3.0   & 0.09 & 100  & 0.691 & 0.98 & 0.03 \\\\ \\hline\nGloCon + Roget    & 0.9   & 0.06 & 750  & 0.048 & 0.94 & 0.02 \\\\ \\hline \nGlove            & 9.0   & 0.09 & 200  & 0.657 & 0.72 & 0.33 \\\\ \\hline\nGlove + Roget    & 11.0  & 0.01 & 1000 & 0.070 & 0.91 & 0.10 \\\\ \\hline\n\\end{tabular}\n   \\caption{Clustering evaluation after parameter optimization minimizing error using grid search.}\n  \\label{tab:optimalparams}\n\\end{table*}\n\n\n\n\nTable \\ref{tab:optimalparams} shows results from the grid search of hyperparameter optimization.\n\nHere we show that Eigenword + MSW outperforms Eigenword + Roget, which is in contrast \nwith the other word embeddings where the combination with Roget performs better. \n\n\nAs a baseline, we created clusters using K-means where the number of K clusters was set to 750. \n\nAll K-means clusters have a statistically significant difference in the number of antonym pairs relative to\nrandom assignment of labels. When compared with the MS Word thesaurus, Word2Vec, Eigenword, GloCon, and GloVe word embeddings had a total of 286, 235, 235, 220 negative edges, respectively. The results are similar with the other thesauri. This shows that there are a significant number of antonyms pairs in the K-means clusters derived from the word embeddings. By optimizing the hyperparameters using normalized cuts without thesauri information, we found a significant decrease in the number of negative edges, which was indistinguishable from random assignment and corresponded to a roughly ninety percent decrease across clusters. When analyzed using an out of sample thesaurus and 27081 words, the number of antonym clusters decreased to under 5 for all word embeddings, with the addition of antonym relationship information. \n\n\nIf we examined the number of distinct connected components within the different word clusters, we\nobserved that when K-means were used, the number of disconnected components were statistically significant from random labelling. This suggests that the word embeddings capture synonym relationships. By optimizing the hyperparameters we found roughly a 10 percent decrease in distinct connected components using normalized cuts. When we added the signed antonym relationships using our signed clustering algorithm, on average we found a thirty-nine percent decrease over the K-means clusters. Again, this shows that the hyperparameter optimization is highly effective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n    \n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Evaluation Using Gold Standard}\n\nSimLex-999 is a gold standard resource for semantic similarity, not relatedness, based on ratings by human annotators. \nThe differentiation between relatedness and similarity was a problem with previous datasets such as WordSim-353.\n\n\n\n\\citet{hill2014simlex} has a further comparison of SimLex-999 to previous datasets. \n\nTable \\ref{tab:simlex} shows the difference between SimLex-999 and WordSim-353.\n\n\\begin{table*}[!tbh]\n  \\centering\n  \\small\n  \\begin{tabular}{|c||c|c|}\n  \\hline \n{\\bf Pair} & \t{\\bf Simlex-999 rating} & {\\bf\tWordSim-353 rating} \\\\ \\hline\ncoast - shore &\t9.00 &\t9.10 \\\\ \\hline\nclothes - closet & \t1.96 & 8.00 \\\\ \\hline\n\\end{tabular}\n   \\caption{Comparison between SimLex-999 and WordSim-353.\n   This is from \\url{http://www.cl.cam.ac.uk/~fh295/simlex.html}}.\n  \\label{tab:simlex}\n\\end{table*}\n\nSimLex-999 comprises of multiple parts-of-speech with 666 Noun-Noun pairs, 222 Verb-Verb pairs and 111 Adjective-Adjective pairs. \n\n\nIn a perfect setting, all word pairs rated highly similar by human annotators would be in the same cluster, and all words which were rated dissimilar would be in different clusters. \n\nSince our clustering algorithm  produced sets of words, we used this evaluation instead of the more commonly-reported correlations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[!tbh]\n  \\centering\n  \\small\n  \\begin{tabular}{|l||c|c|}\n  \\hline \n\\multicolumn{1}{|c||}{\\bf Method} & {\\bf Accuracy} & {\\bf Coverage} \\\\ \\hline\nMS Thes Lookup & 0.70 & 0.57 \\\\ \\hline\n\nRoget Thes Lookup & 0.63 & 0.99  \\\\ \\hline\nWordNet Thes Lookup & 0.43 & 1.00 \\\\ \\hline\nCombined Thes Lookup & 0.90 & 1.00 \\\\ \\hline\nWord2Vec & 0.36 & 1.00 \\\\ \\hline\n\nWord2Vec+CombThes & 0.67 & 1.00 \\\\ \\hline\nEigenwords & 0.23 & 1.00  \\\\ \\hline\nEigenwords+CombThes & 0.12 & 1.00  \\\\ \\hline\nGloCon & 0.07 & 1.00  \\\\ \\hline\nGloCon+CombThes & 0.05 & 1.00  \\\\ \\hline\nGloVe & 0.33 & 1.00  \\\\ \\hline\nGloVe+CombThes & 0.58 & 1.00 \\\\ \\hline\nThes Lookup+W2V+CombThes & 0.96 & 1.00 \\\\ \\hline\n\\end{tabular}\n   \\caption{Clustering evaluation using SimLex-999 with 120 word pairs having similarity score over 8.}\n  \\label{tab:simlexeval}\n\\end{table}\n\nIn Table \\ref{tab:simlexeval} we show the results of the evaluation with SimLex-999. \nAccuracy increased for all of the clustering methods aside from Eigenwords+CombThes. However, we achieved better results when we exclusively used the MS Word thesaurus. \nCombining thesaurus lookup and word2vec+CombThes clusters yielded an accuracy of 0.96. \n\n\\subsubsection{Sentiment Analysis}\nWe used the \\citet{socher2013recursive}\nsentiment treebank \\footnote{\\url{http://nlp.stanford.edu/sentiment/treebank.html}} with coarse grained labels on phrases and\nsentences from movie review excerpts.\nThe treebank is split into training (6920) , development (872), and test (1821)\ndatasets. \nWe trained an $l_2$-norm regularized logistic regression \\citep{friedman2001elements} using our word clusters \nin order to predict the coarse-grained sentiment at the sentence level. \nWe compared our model against existing models: Naive\nBayes with bag of words (NB), \nsentence word embedding averages (VecAvg),\nretrofitted sentence word embeddings (RVecAvg) \\citep{faruqui2014retrofitting},\nsimple recurrent neural network (RNN),\nrecurrent neural tensor network (RNTN) \\citep{socher2013recursive}, \nand the state-of-the art Convolutional neural network (CNN) \\citep{kim2014convolutional}.\nTable \\ref{tab:sentanalysis} shows that although our model\ndoes not out-perform the state-of-the-art,\nsigned clustering performs better than comparable models, including the recurrent neural network,\nwhich has access to more information. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[!tbh]\n  \\centering\n  \\small\n  \\begin{tabular}{|l||c|c|}\n  \\hline \n\\multicolumn{1}{|c||}{\\bf Model} & {\\bf Accuracy} \\\\ \\hline\nNB \\citep{socher2013recursive} & 0.818 \\\\ \\hline \n\nVecAvg (W2V, GV, GC)  & 0.812, 0.796, 0.678  \\\\ \n\\citep{faruqui2014retrofitting} & \\\\ \\hline\nRVecAvg (W2V, GV, GC)  & 0.821, 0.822, 0.689  \\\\  \n\\citep{faruqui2014retrofitting} & \\\\ \\hline\nRNN, RNTN \\citep{socher2013recursive} & 0.824, 0.854 \\\\ \\hline\nCNN \\citep{le2015compositional} & 0.881 \\\\ \\hline \n\n\\hline\nSC W2V & 0.836 \\\\ \\hline\nSC GV & 0.819 \\\\ \\hline\nSC GC & 0.572 \\\\ \\hline\nSC EW & 0.820 \\\\ \\hline\n\\end{tabular}\n   \\caption{Sentiment analysis accuracy for binary predictions of signed clustering algorithm (SC) versus other models.}\n  \\label{tab:sentanalysis}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\nWe developed a novel theory for signed normalized cuts\nas well as an algorithm for finding the discrete solution.\nWe showed that we can find superior synonym clusters which\ndo not require new word embeddings, but simply overlay thesaurus information.\nThe clusters are general and can be used with several out of the box word embeddings. \nBy accounting for antonym relationships, our algorithm greatly outperforms simple normalized cuts, even with Huang's word embeddings\n\n, which are designed to capture semantic relations.\nFinally, we examined our clustering method on the sentiment analysis task from \\citet{socher2013recursive} sentiment treebank dataset \n\nand showed\nimproved performance versus comparable models.\n\n\n\n\nThis method could be applied to a broad range of NLP tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analysis of clusters which capture positive, negative, and objective emotional content. It could also be used to explore multi-view relationships, such as aligning synonym clusters across multiple languages. Another possibility is to use thesauri and word vector representations together with word sense disambiguation\nto generate synonym clusters for multiple senses of words.\n\nFinally, our signed clustering could be extended to evolutionary signed clustering.\n\n\n\n\\bibliographystyle{icml2015}\n\\bibliography{spectral_clustering,ssc}\n\n\n", "itemtype": "equation", "pos": 20091, "prevtext": "\n\nThe NDC has the disadvantage of thesaurus coverage. \n\n\n\nFigure 2 shows a graphical representation of the number of disconnected components and negative edges.\n\n\\begin{figure}[ht]\n  \\label{fig:dc2}\n\n\n\n\n  \n\n\n\n\n\n  \\begin{subfigure}\n  \\par\n    \\label{fig:figure2}\n    \\includegraphics[width=\\linewidth]{cluster_2_dc.jpg}\n    {\\small {\\it Figure 2.1.} Cluster with two disconnected components. \n    All edges represent synonymy relations. The edge colors are only meant to highlight the different components.}\n  \\end{subfigure}\n  \\begin{subfigure}\n  \\par\n    \\label{fig:ant1}\n    \\includegraphics[width=\\linewidth]{cluster_1_ant.jpg}\n    {\\small {\\it Figure 2.1.} Cluster with one antonym relation. \n    The red edge represents the antonym relation. Blue edges represent synonymy relations.}\n  \\end{subfigure}\n  \\caption{Disconnected component and number of antonym evaluations.}\n\\end{figure}\n\n\n\nNext we evaluate our clusters using an external gold standard. \nCluster purity and entropy \\cite{zhao2001criterion} is defined as, \n\n", "index": 47, "text": "\\begin{align*}\nPurity &= \\sum_{r=1}^{k}{\\frac{1}{n}max_i(n_r^i)} \\\\\nEntropy &= \\sum_{r=1}^{k}{\\frac{n_r}{n}\\left({-\\frac{1}{\\log q}\\sum_{i=1}^{q}{\\frac{n_r^i}{n_r}\\log \\frac{n_r^i}{n_r}}}\\right)}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Purity\" display=\"inline\"><mrow><mi>P</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>y</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{r=1}^{k}{\\frac{1}{n}max_{i}(n_{r}^{i})}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><msub><mi>x</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>n</mi><mi>r</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle Entropy\" display=\"inline\"><mrow><mi>E</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>y</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{r=1}^{k}{\\frac{n_{r}}{n}\\left({-\\frac{1}{\\log q}\\sum_{i=1}%&#10;^{q}{\\frac{n_{r}^{i}}{n_{r}}\\log\\frac{n_{r}^{i}}{n_{r}}}}\\right)}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mi>r</mi></msub><mi>n</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>log</mi><mo>\u2061</mo><mi>q</mi></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>n</mi><mi>r</mi><mi>i</mi></msubsup><msub><mi>n</mi><mi>r</mi></msub></mfrac></mstyle><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>n</mi><mi>r</mi><mi>i</mi></msubsup><msub><mi>n</mi><mi>r</mi></msub></mfrac></mstyle></mrow></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]