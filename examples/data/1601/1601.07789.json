[{"file": "1601.07789.tex", "nexttext": "\n\n\\noindent $v$ is the real number obtained by the evaluation of the formula for\na normalized floating-point value with sign $s$, mantissa $m$ of length\n$M$ bits, and exponent $e$. The value $bias$ is an integer constant which\ndiffers for each format. The formats all use a single bit to represent the\nsign, but different numbers of bits for the exponent and mantissa components.\n\nThe exponent determines a power of two by which the rest of the number is\nmultiplied, while the mantissa represents a fractional quantity in the interval\n$[1,2)$ obtained by summing successively smaller powers of two, starting from\n$2^{-1}$ and continuing up to the length of the mantissa. If the $i$th mantissa\nbit is set, the corresponding power of two is present in the sum. For\nnormalized numbers, the leading 1 is not explicitly stored in the mantissa, but\nis implied.\n\nThe structure of the IEEE-754 binary encoding means that a change in an\nexponent bit strongly influences the resulting value, while a change in a\nmantissa bit has less influence. Furthermore, a change in any bit of the\nmantissa has exponentially greater effect on the resulting value than a change\nin the next-least-significant bit. These observations lead naturally to a\nscheme for representing values at precisions between those specified by the\nIEEE-754 standard: varying the number of low-order mantissa bits. Previous\nproposals based around this concept have made use of customized floating point\nhardware support via FPGA~\\cite{de2009generating} or high-level data\nreorganization in massively parallel systems~\\cite{jenkins2012byte}. In this\npaper, we demonstrate how reduced precision representations can be used on\ngeneral-purpose processors without requiring any special hardware support.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Reduced Precision on GPPs}\n\\label{sec:scheme}\n\nIt is possible to \\textit{emulate} floating point operations in software using\ninteger instructions. However, each floating point operation requires many\ninteger instructions so emulation is slow. In particular, the IEEE-754 standard\nhas several special values and ranges such as not-a-number (NaN), positive and\nnegative zero, infinities, and sub-normal numbers, each of which adds to the\ncomplexity of software emulation \\cite{sidwell2006improving}. Modern processors\nprovide hardware floating point units which dramatically reduce the time and\nenergy required for floating point computations. However, these units normally\nsupport just two floating point types, typically {$\\mathtt{{binary32}}$} ({$\\mathtt{{float}}$}) and\n{$\\mathtt{{binary64}}$} ({$\\mathtt{{double}}$}).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Our Approach}\n\\label{sec:approach}\n\nWe propose a set of non-standard \\textbf{fl}oating point multib\\textbf{yte}\ntypes that we refer to as \\textit{flyte}s. Rather than emulating computation on\n\\textit{flyte}s in software, we convert them to/from the next largest natively\nsupported type. Thus, a 24-bit \\textit{flyte} ({$\\mathtt{{flyte24}}$}) is converted to\n{$\\mathtt{{binary32}}$} before computation, and the {$\\mathtt{{binary32}}$} result is converted\nback to {$\\mathtt{{flyte24}}$} after. We need to solve two problems:\n\n\\begin{itemize}\n\n\\item Efficiently loading and storing non-standard data sizes, such as 24-bit\ndata, where there is no hardware support for such operations.\n\n\\item Converting between built-in floating point types and our non-standard\ntypes.\n\n\\end{itemize}\n\nEfficiently supporting non-standard floating point types using this approach\ncreates two types of problems. The first is supporting reduced precision types\nwith acceptably low overhead. This is the topic of the majority of this\npaper.\n\nThe second type of problem is that performing computation in one floating point\ntype and storing values in a less precise type introduces problems, such as\ndouble-rounding, that complicate numerical analysis. We make every effort to be\nclear about this latter group of problems but in most cases we do not have\nsolutions. Our techniques are aimed squarely at problems where some\napproximation is acceptable and the developer has a good understanding of\nexactly how much precision is required. Our main contribution is to show\n\\textit{how} to implement multibyte floating point formats efficiently; the\nquestion of \\textit{whether} to use them in any particular algorithm depends on\nthe numerical properties of the algorithm.\n\n\\noindent In general, converting between floating point formats has many\nspecial cases. In particular, converting between formats with different-sized\nexponents may cause numbers to overflow to infinity or underflow to/from\nsub-normal numbers. Dealing correctly with these cases in software is\ncomplicated and slow. Our solution to the problem is that in all our\n\\textit{flyte} types, the size of the exponent is equal to the size of the\nexponent of the next largest built-in type. For example, in our {$\\mathtt{{flyte16}}$}\nand {$\\mathtt{{flyte24}}$} types, the exponent has eight bits, just like {$\\mathtt{{binary32}}$}.\nThis dramatically reduces the complexity of conversions.\n\n\\subsection{Simple Scalar Code Approach}\n\\label{sec:scalar-approach}\n\nFigure \\ref{fig:simple-flyte} shows a simple implementation of the {$\\mathtt{{flyte24}}$}\ntype in C++. It relies on the \\emph{bit field} facility in C/C++ to specify\nthat the {$\\mathtt{{num}}$} field contains a 24-bit integer. It also uses the GCC\n\\texttt{packed} attribute to indicate to the compiler that arrays of the type\nshould be packed to exactly 24 bits, rather than padded to 32 bits. Figure\n\\ref{fig:simple-flyte} also shows a routine for converting from {$\\mathtt{{flyte24}}$} to\n{$\\mathtt{{float}}$} (i.e. {$\\mathtt{{binary32}}$}). The 24-bit pattern stored in the {$\\mathtt{{flyte24}}$}\nvariable is scaled to 32 bits and padded with zeros. The resulting 32-bit\npattern is returned as a {$\\mathtt{{float}}$}.\n\n\\begin{figure}[ht]\n\\begin{footnotesize}\n\\begin{verbatim}\nclass flyte24 {\nprivate:\n  unsigned num:24;\npublic:\n  operator float() {\n    u32 temp = num << 8;\n    return(cast_u32_to_f32(temp));\n  };\n  ...\n} __attribute__((packed));\n\\end{verbatim}\n\\end{footnotesize}\n\\vspace{-9pt}\n\\caption{Simple implementation of {$\\mathtt{{flyte24}}$} in C++}\n\\label{fig:simple-flyte}\n\\end{figure}\n\n\\noindent The code that is sketched in Fig. \\ref{fig:simple-flyte} can be used\nto implement programs with arrays of {$\\mathtt{{flyte24}}$} values, but it is very slow.\nFigure \\ref{fig:realworldlib} shows a comparison of the execution time of\nseveral BLAS kernels using {$\\mathtt{{flyte24}}$} and other {$\\mathtt{{flyte}}$} and IEEE-754\ntypes. The order of magnitude difference in execution time is the result of (1)\nthe cost of converting between \\text{flyte24} and {$\\mathtt{{binary32}}$} before and\nafter each computation; and (2) the cost of loading and storing individual\n24-bit values. In particular, storing data to successive elements of packed\n{$\\mathtt{{flyte24}}$} arrays can result in sequences of overlapping unaligned stores.\nLoad/store hardware in GPPs is not designed to deal with such operations, which\nresults in extraordinarily slow execution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable~\\ref{table:formats} summarizes our proposed set of \\emph{multibyte}\nformats for floating-point values which preserve the sign and exponent fields\nof the corresponding IEEE-754 representations.\n\n\n\n\n\\begin{table}[ht]\n  \\caption{\\textit{flyte} storage formats for IEEE-754 types.}\n  \\centering\n  \\begin{tabular}{ccccc}\n    \\hfill & \\hfill & \\multicolumn{3}{c}{\\textit{flyte} layout (bits)} \\\\\n    \\toprule\n    IEEE-754 type & \\textit{flyte} format & Sign & Exp. & Mant. \\\\\n    \\midrule\n    {$\\mathtt{{binary32}}$} & 16-bit  & 1& 8& 7\\\\\n    . & 24-bit  & 1& 8& 15\\\\\n    . & 32-bit  & 1& 8& 23\\\\\n    {$\\mathtt{{binary64}}$} & 40-bit & 1& 11& 28\\\\\n    . & 48-bit & 1& 11& 36\\\\\n    . & 56-bit & 1& 11& 44\\\\\n    . & 64-bit & 1& 11& 52\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\label{table:formats}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\\section{Reading and Writing In Reduced Precision}\n\nReading and writing reduced-precision representations might be expected to\nincur a significant performance penalty due to the overheads outlined in\nSection~\\ref{sec:scalar-approach}. Particular concerns are (1) the overhead of\nconversion between data formats, in addition to (2) the overheads of memory\naccess to arrays of datatypes that may have non--power-of-two byte width, where\nmemory movements may be overlapping and misaligned. Although these overheads\nare encountered both when reading and when writing reduced-precision\nrepresentations, there are important differences between the two cases.\n\n\\subsection{Reading In Reduced Precision}\n\\label{sec:reads}\n\nModern instruction set architectures typically have native support for data\nmovement using types with power-of-two byte widths -- usually 1, 2, 4, and 8\nbytes. Since our \\textit{flyte} types differ in width from standard IEEE-754\ntypes by multiples of 8 bits, this means we can always fetch a \\textit{flyte}\nwith a single read using a wider native type (e.g. a 4-byte read for a 3-byte\n{$\\mathtt{{flyte24}}$}). Since we propose to store \\textit{flyte}s packed\nconsecutively in arrays without padding, the majority of such accesses will be\nmisaligned. Specifically, a consecutive \\textit{flyte} array access will only\nbe aligned to the next largest native type once every $lcm(\n\\mathtt{native}_{bits}, \\mathtt{flyte}_{bits}) / \\mathtt{flyte}_{bits}$ array\nelements.\n\nUnaligned access can cause extra cache misses versus aligned access due to the\npossibility that the accessed item spans a cache line boundary. Since\nreads do not modify memory locations, the strategy of using overlapping\naccesses at the next largest native type does not introduce a correctness\nproblem as it does for writes (Section~\\ref{sec:writes}). Also, the conversion\nprocess when reading \\textit{flytes} is relatively simple, requiring only that\nthe read data be shifted and padded with zero bits.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Writing In Reduced Precision}\n\\label{sec:writes}\n\nWriting data to a reduced-precision format is more complex than reading, both\nin terms of the memory movement (since memory locations are modified), and due\nto the fact that when writing, the number format narrows, and precision is\nlost. Loss of precision is a natural consequence of working with floating point\nnumbers. The precise result of a floating point computation can be a real\nnumber which cannot be exactly encoded in a finite representation (for example,\nit might require infinitely many digits). In these cases, loss of precision is\nnecessary to make the result representable. The standard specifies several\nmethods which are used to \\emph{round} numbers to a nearby representable value.\nOne straightforward way to perform rounding is to simply truncate extra digits\n(i.e. bits, in a binary representation). This is the standard round-to-zero\nrounding mode~\\cite{zuras2008ieee}. Other rounding modes are specified by the\nIEEE-754 standard, including round-to-nearest-even and round-to-infinity\n(positive or negative).\n\n\n\n\n\n\n\n\n\n\n\\subsection{Vectorized Reading and Writing}\n\\label{sec:vector-approach}\n\nWe propose a compiler-based approach which can greatly reduce the overhead of\noperating on \\textit{flyte} arrays using automatic vectorization. Our approach\nuses vector instructions to load, convert, and store \\textit{flytes}. We\ngenerate vectorized code to load a number of \\textit{flyte} values at once, and\nunpack those values to the next largest IEEE type using vector shuffle and\nblend instructions. By restricting the size of a \\textit{flyte} to be a\nmultiple of 8 bits, we ensure that widely available fast byte-level vector\nreorganization instructions can be used. Finally, when computation is complete,\nwe again use vector instructions to convert back to \\textit{flyte} types. This\nmay involve a rounding step when reducing the precision of a standard IEEE-754\nformat to a \\textit{flyte} type, followed by packing the data in vector\nregisters before the results are written to memory.\n\n\n\n\n\n\n\n\n\nVectorized loading and storing of packed data elements that do not correspond\nto any native machine type presents additional challenges over scalar memory\nmovement. Since vector lengths on modern GPPs are usually a power-of-two bytes,\nvectorized access to \\textit{flyte} arrays often leads to the splitting of data\nelements across vector memory operations, where the leading bytes of an element\nare accessed by one operation, and the trailing bytes by the next.\nFigure~\\ref{fig:f24} displays one such scenario: storing in {$\\mathtt{{flyte24}}$} format\ncomputational results produced in {$\\mathtt{{binary32}}$}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[ht]\n\\centering\\includegraphics[scale=1.2]{f24-compact}\n\n\\caption{Layout of data in 128-bit (16-byte) vector registers. (top) before\nformat conversion, (center) after rounding to the desired size, and (bottom)\nthe desired layout in memory. Note that the desired memory layout requires data\nelements to straddle the boundary between vector registers.}\n\n\\label{fig:f24}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n  \n    \n    \n    \n  \n\n\n\n  \n  \n\n\n\n\n\n\\noindent While vectorized reading is not significantly more complicated than\nscalar reading, vectorized writing has additional issues. A straightforward\napproach to vectorized writing of unpadded \\textit{flyte} data could pack as\nmany consecutive \\textit{flyte} elements in a vector register as would fit, and\nperform a store with the trailing unused byte positions predicated off.\nSubsequent stores, if there are more than one, could overlap prior stores in\nthese unused positions, so that the data is consecutive in memory. Due\nto the structure of load/store hardware in GPPs, this approach is likely to be\nextremely inefficient.\n\n\\noindent Our vectorized approach to storing values in reduced precision format\nworks by packing all the rounded data elements to be stored consecutively in a\nset of vector registers, which are mapped to a set of consecutive,\n\\textbf{non-overlapping} vector memory movements (shown in\nFigure~\\ref{fig:f24}). We use a two-phase permute and blend approach. Vector\npermute instructions are initially used to compact the rounded data in each\nregister into memory order, and align the contents of some registers so that\nthe leading data element in each register is located at the position of the\nfirst unused byte in the previous register. Next, the compacted vector\nregisters are combined together using vector blend instructions until a number\nof fully packed vector registers result. The resulting registers can be stored\nwithout overlap, and data elements are correctly split across register\nboundaries. If the total number of bytes written is not a multiple of the\nlength of a vector register, the last store may still need to be performed as a\npredicated store or a read-modify-write for correctness.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n  \n  \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n    \n    \n  \n  \n  \n\n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n    \n    \n  \n  \n  \n\n\n\n\n  \n  \n  \n    \n    \n    \n  \n  \n  \n  \n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n    \n    \n  \n  \n  \n\n\n\n  \n  \n    \n    \n    \n  \n  \n  \n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n    \n    \n  \n  \n  \n\n\n\n  \n  \n    \n    \n    \n  \n  \n  \n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n    \n    \n  \n  \n  \n\n\n\n\n  \n  \n    \n    \n    \n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n    \n    \n  \n  \n  \n    \n    \n  \n  \n  \n\n\n\n\n  \n  \n  \n  \n\n\n\\section{Performance Evaluation}\n\nWe benchmarked the performance of our proposed scalar code implementation from\nSection~\\ref{sec:scalar-approach} and vectorized implementation from\nSection~\\ref{sec:vector-approach}. Experiments were run on 64-bit Linux with a\n4.2 series kernel, using a machine with 16GB of RAM and an Intel Core i5-3450\n(Ivy Bridge) processor. We followed Intel's guidelines for benchmarking short\nprograms on this architecture~\\cite{paoloni2010benchmark}.\nFigure~\\ref{fig:realworld} presents the results of benchmarking.\n\n\\begin{figure}[ht]\n\n\\begin{subfigure}[h]{0.5\\textwidth}\n\\centering\\includegraphics[scale=0.68]{real-world-LIB}\n\\caption{Scalar Approach (from Figure~\\ref{fig:simple-flyte}) }\n\\label{fig:realworldlib}\n\\end{subfigure}\n\n\\begin{subfigure}[h]{0.5\\textwidth}\n\\centering\\includegraphics[scale=0.68]{real-world-SSE}\n\\caption{128-bit SSE Vectorized Approach (from Figure~\\ref{fig:f24}) }\n\\label{fig:realworldsse}\n\\end{subfigure}\n\n\\caption{Variation of performance with precision of memory representation\nacross a number of BLAS kernels. Performance displayed as normalized execution\ntime (cycles per input element) -- lower is better. Graphs share a key, shown\nin 4b only. Overhead due to {$\\mathtt{{flyte}}$} conversion can be read as the difference\nbetween the {$\\mathtt{{flyte}}$} type and the next largest native type.}\n\n\\label{fig:realworld}\n\n\\end{figure}\n\n\\noindent The large overhead of scalar access in Figure~\\ref{fig:realworldlib}\nis due to the design mismatch between our non-standard use-case and the typical\nstructure of a GPP scalar datapath, discussed in detail in\nSection~\\ref{sec:scalar-approach}. In contrast, our compiler-based vectorized\napproach (Figure~\\ref{fig:realworldsse}) marshalls and unmarshalls data from\nmultiple misaligned non-standard accesses into the consecutive, non-overlapping\naccesses which are the best-case for performance using the (SSE) vector\ndatapath. Overheads in the scalar implementation are in the mid to high tens of\ncycles per accessed element, while the vectorized implementation overheads\nversus native IEEE-754 types are on the order of a single cycle per input\nelement. In computationally heavy programs like \\texttt{magnitude} this\noverhead is effectively hidden by instruction-level parallelism\n(Figure~\\ref{fig:realworldsse}). The relatively high overhead of {$\\mathtt{{flyte40}}$}\nand {$\\mathtt{{flyte24}}$} in the \\texttt{scale} benchmark in\nFigure~\\ref{fig:realworldsse} is due to two factors: the alignment of the\naccesses is odd (5 and 3 bytes, respectively), and the benchmark performs an\n\\emph{in-place} update of the data, where reads and writes overlap.\n\nOur results show that reducing memory traffic with non-standard floating point\nmemory representations can increase performance. However, the picture is much\nbetter for our compiler-based vectorized approach, where even in a pathological\ncase such as in-place update of 5-byte values, with associated alignment and\npacking issues, the overhead is less than 3 cycles per input element versus the\nnext largest native IEEE-754 type, {$\\mathtt{{binary64}}$}. Where available memory is a\npressing concern, the tradeoff of a 37.5\\% reduction in memory requirements for\n3 cycles overhead per accessed element may even be attractive.\n\nThe benchmark \\texttt{gemv-unroll2} is the BLAS Level 2 \\texttt{GEMV} kernel\nunrolled twice to increase the amount of data movement per vectorized loop\niteration. In this benchmark, data transfer accounts for a large portion of\ntotal execution time. The benchmark demonstrates that a win-win is possible:\nthe choice of a reduced-precision memory representation can actually\n\\emph{increase} overall performance versus the next largest IEEE-754 type,\nwhile also reducing memory requirements, even on a general purpose processor\nwithout special hardware support for non-standard floating point.\n\n\\section{Conclusions and Related Work}\n\nMuch prior work discusses reduced precision floating\npoint~\\cite{rubio2013precimonious,lam2013automatically,buttari2006exploiting}.\nJenkins et al.~\\cite{jenkins2012byte} evaluate a reduced-precision scheme using\nGPPs in an extreme-scale computing context. They do not utilize SIMD, address\nonly reads, and convert in a pre-pass.\n\nMany approaches use FPGAs or otherwise customized hardware; notably Tong et\nal.~\\cite{tong2000reducing}, who propose customizing ALUs to support\nshort-mantissa representations. More recently, De Dinechin et\nal.~\\cite{de2009generating} also propose custom hardware to support reduced\nprecision. Ou et al. accelerate mixed-precision floating point using a vector\nprocessor with a customized datapath~\\cite{ou2015mixed}. Our approach, since it\ntargets GPPs, is necessarily less flexible than FPGA/custom hardware based\napproaches. However, it is precisely because GPPs are so widely deployed that\nreduced precision support on GPPs is attractive.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this paper, we propose \\textit{flyte}s; a scheme representing floating-point\ndata at precisions along a continuum between IEEE-754 types. We propose a\nmethod for converting between IEEE-754 floating point and \\textit{flyte}s, and\nshow how it can be accelerated using vectorization on general purpose\nprocessors, without requiring special hardware support. Our technique\nhandles both reads and writes, and supports reduced precision floating point with\nvery low overhead.\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{abbrv}\n\n\\bibliography{main}\n\n\n\n", "itemtype": "equation", "pos": 5672, "prevtext": "\n\n\n\n\n\n\n\n\\title{Vectorization of Multibyte Floating Point Data Formats}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Andrew~Anderson\n        and~David~Gregg\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem The authors are with the School\nof Computer Science and Statistics, Trinity College Dublin, Ireland.\n\n\nE-mail: \\{aanderso,~dgregg\\}@scss.tcd.ie\nThis work was supported\nby Science Foundation Ireland grant 12/IA/1381, and also by Science Foundation Ireland grant 10/CE/I1855 to Lero ---\nthe Irish Software Research Centre (www.lero.ie).\n}\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\markboth{PREPRINT}\n{Anderson~and~Gregg: Vectorization of Multibyte Floating Point Data Formats}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEtitleabstractindextext{\n\\begin{abstract}\n\nWe propose a scheme for reduced-precision representation of floating point data\non a continuum between IEEE-754 floating point types. Our scheme enables the\nuse of lower precision formats for a reduction in storage space requirements\nand data transfer volume. We describe how our scheme can be accelerated using\nexisting hardware vector units on a general-purpose processor. Exploiting\nnative vector hardware allows us to support reduced precision floating point\nwith low overhead.\n\n\n\n\n\n\\end{abstract}\n\n\n\\begin{IEEEkeywords}\nFloating Point, Precision, SIMD, Vector Architecture.\n\\end{IEEEkeywords}}\n\n\n\n\\maketitle\n\n\n\n\n\n\n\n\n\n\\IEEEdisplaynontitleabstractindextext\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEraisesectionheading{\\section{Motivation}\\label{sec:motivation}}\n\\IEEEPARstart{I}{t} has long been recognized that different applications and\nalgorithms need different amounts of floating point precision to achieve\naccurate results\n\\cite{tong2000reducing,buttari2006exploiting,rubio2013precimonious}. For\nexample, 64-bit double precision floating point is often needed for scientific\napplications, whereas 32-bit single precision is sufficient for most graphics\nand computer vision applications. Indeed, modern GPUs and embedded processors\nincreasingly support 16-bit half precision floating point for applications that\nare highly tolerant of approximation such as speech recognition\n\\cite{dixon2009fast}.\n\nIn some contexts it is possible to customize the level of precision precisely\nto the application. For example, field-programmable gate arrays (FPGAs) can be\nused to implement customized floating point with bit-level control over the\nsize of the mantissa and exponent \\cite{de2009generating,tong2000reducing}.\nMore recently, Aiken et al. \\cite{schkufza2014stochastic} have shown how\nsuperoptimization can be used to generate iterative floating\npoint algorithms that guarantee that a given level of accuracy.\nFor example, a exponential function may return a 64-bit floating point value in\nwhich at least the first, say, 40 bits of the mantissa are guaranteed to be\naccurate.\n\nA problem with customizing floating point precision on general-purpose\nprocessors (GPPs) is that most support only two standard types: IEEE-754 single\nprecision (or {$\\mathtt{{binary32}}$}) and double precision ({$\\mathtt{{binary64}}$}). If an\napplication needs more precision than {$\\mathtt{{binary32}}$} and less than\n{$\\mathtt{{binary64}}$}, in most cases the developer must use\n{$\\mathtt{{binary64}}$} values everywhere.\n\n\n\n\n\n\nIn this paper we propose a compiler-based mechanism for supporting several\nnon-standard \\emph{multibyte} floating point memory formats, such as 24-, 40-,\nor 48-bit floating point. By \\emph{multibyte} we refer to the fact that these\nformats differ in length from standard IEEE-754 formats by multiples of 8 bits.\nBy using these types, a developer can reduce the precision of floating point\ndata in memory, resulting in a smaller memory footprint for their data, as\ncompared with the next largest standard type.\n\nIn addition, an\nincreasingly important factor in the design of computing systems is energy\nconsumption. In embedded computing systems the energy consumed by data movement\ncan be much greater than the energy used to perform arithmetic\n\\cite{dally2008efficient}. Customizing the size of floating point data in\nmemory to the needs of the application may significantly reduce the amount of\ndata movement.\n\n\\noindent It is relatively straighforward to implement a C/C++ data type\nrepresenting, for example, 40-bit floating point values. However, we have found\nthat the performance of such code can be extraordinarily poor when operating on\narrays of data.\n\n\n\n\nWe\ntherefore propose a technique to generate vectorized code that performs a\nnumber of adjacent reads or writes together, along with the required packing\nor unpacking.\n\n\n\n\n\n\n\\begin{itemize}\n\n\\item We present a practical representation of multibyte floating\n  point values that can easily be converted to and from the next\n  larger natively-supported type.\n\n\\item We propose a compiler vectorization scheme for packing and\n  unpacking our floating point types, and performing computations\n  using vector instructions.\n\n\n  \n  \n\n\n  \n\n\\item We demonstrate experimentally that our techniques\nprovide a low-overhead way to support customizing floating point on\ngeneral-purpose processors.\n\n\\end{itemize}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Customizing Floating Point Types}\n\n\\label{sec:ieee-754-intro}\n\nThe IEEE-754 2008 standard~\\cite{zuras2008ieee} defines a number of finite\nbinary representations of real numbers at different resolutions (16, 32, and 64\nbits, among others). Each format encodes floating-point values using three\nbinary integers: \\emph{sign}, \\emph{exponent}, and \\emph{mantissa}, which\nuniquely specify a point on the real number line following a general formula.\n\n", "index": 1, "text": "\\begin{align*}\nv = (-1)^{s} \\times (1 + \\sum_{i=1}^{M}(m_{i}2^{-i})) \\times 2^{e-bias}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle v=(-1)^{s}\\times(1+\\sum_{i=1}^{M}(m_{i}2^{-i}))\\times 2^{e-bias}\" display=\"inline\"><mrow><mi>v</mi><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mi>s</mi></msup><mo>\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>\u2062</mo><msup><mn>2</mn><mrow><mo>-</mo><mi>i</mi></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><msup><mn>2</mn><mrow><mi>e</mi><mo>-</mo><mrow><mi>b</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></msup></mrow></mrow></math>", "type": "latex"}]