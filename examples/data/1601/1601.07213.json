[{"file": "1601.07213.tex", "nexttext": "\n\n\n\n\n\\\\\n\n\\\\\n\n\n\n\\noindent\nwhere $\\mathbf{d} = (d_1,d_2,...d_k)$ is a data sample, $t$ is its corresponding label/target and $\\Theta= \\{W_1,W_2,...,W_K\\}$ represents the parameters of a $K$ layer neural network. We use  ${\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_i}$ to denote the Jacobian of ${\\operatorname{\\mathcal{L}}}_i$ (the gradient of ${\\operatorname{\\mathcal{L}}}_i$ with respect to $\\mathbf{d}$). $\\lambda_0,\\lambda_1,\\dots,\\lambda_m$ are the weight coefficients of the terms in the DataGrad loss function. {\\textcolor{black}{{Close to our work, \\citep{lyuunify} present a heuristic way to optimize a special case of this objective. By directly provding an algorithm, our analysis can explain what their algorithm optimizes.}}}\n\n\n\n\n\n\n\nWe denote the entire dataset as $\\mathcal{D}=\\{(\\mathbf{d}^{(1)}, t^{(1)}),\\dots, (\\mathbf{d}^{(n)}, t^{(n)})\\}$. Following the framework of empirical risk minimization with stochastic gradient descent, the goal is to minimize the objective function: $\\sum\\limits_{i=1}^n {\\operatorname{\\mathcal{L}}}_{DG}(t^{(i)},\\mathbf{d}^{(i)},\\Theta)$\n\n\n\nby iterating the following parameter updates (here $w^{\\ell}_{ij}$ is the component of $\\Theta$ representing the weight of the incoming edge to node $i$ of layer $\\ell$ from node $j$ of layer $\\ell-1$):\n\n", "itemtype": "equation", "pos": 3940, "prevtext": "\n\\hspace{13.9cm}1\n\n\\ \\vspace{20mm}\\\\\n\n{\\LARGE Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization}\n\n\\ \\\\\n{\\bf \\large Alexander G. Ororbia II$^{\\displaystyle 1}$}\\\\\n{\\bf \\large Daniel Kifer$^{\\displaystyle 1}$}\\\\\n{\\bf \\large C. Lee Giles$^{\\displaystyle 1}$}\\\\\n\n{$^{\\displaystyle 1}$The Pennsylvania State University.}\\\\\n\n\n\n\n{\\bf Keywords:} neural architectures, adversarial examples, data-driven regularization, approximate deep Jacobian\n\n\\thispagestyle{empty}\n\\markboth{}{NC instructions}\n\n\\ \\vspace{-0mm}\\\\\n\n\n\\begin{center} {\\bf Abstract} \\end{center}\nWe present \\emph{DataGrad}, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets -- this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions.  In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples.\n\n\n\n\n\\section{Introduction}\nDeep neural architectures\nare highly effective at a vast array of tasks, both supervised and unsupervised. However, recently, it has been shown that deep architectures are sensitive to certain kinds of pertubations of the input, which can range from being barely perceptible to quite noticeable (even semi-random noise), as in \\cite{nguyen_deep_2014}. Samples containing this type of noise are called ``adversarial examples'' \\citep{szegedy_intriguing_2013} and can cause a trained network to confidently misclassify its input.  While there are a variety of ways to generate adversarial samples, the fastest and most effective approaches in the current literature are based on the idea of using back-propagation to acquire the derivative of the loss with respect to an input image (i.e. the Jacobian) and adding a small multiple of the Jacobian to the image.\n\nEarlier work suggested adding a regularization penalty on the deep Jacobian \\citep{goodfellow_explaining_2014,gu_towards_2014}, but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms. Instead, they utilized approximations such as a shallow layerwise Jacobian penalty \\citep{gu_towards_2014}, also used for regularizing contractive auto-encoders \\citep{gu_towards_2014}. {\\textcolor{black}{{Meanwhile \\citep{lyuunify} also presented a heuristic algorithm for this objective.}}}\n\nHere we provide an efficient, deterministic back-propagation style algorithm for training with a wide variety of Jacobian penalties.  It would seem that the resulting algorithm has the potential for unifying existing approaches for deep adversarial training. In particular, it helps explain some of the newer approaches to adversarial training \\citep{miyato_distributional_2015,huang_learning_2015}. These approaches set up an adversarial objective as a constrained optimization problem and then approximate/simplify it using properties that hold for optimal solutions of \\emph{unconstrained} problems. The algorithms then developed approximate optimization (when compared to our algorithms) and can be viewed as regularizations of the deep Jacobian.\n\n\n\n\n\n\n\\section{The DataGrad Framework}\n\\label{unified_view}\n\nGiven a set of loss functions ${\\operatorname{\\mathcal{L}}}_0,{\\operatorname{\\mathcal{L}}}_1,\\dots,{\\operatorname{\\mathcal{L}}}_m$ and regulariziers ${\\operatorname{\\mathcal{R}}}_1,\\dots,{\\operatorname{\\mathcal{R}}}_m$, consider:\n\n", "index": 1, "text": "\\begin{align*}\n{\\operatorname{\\mathcal{L}}}_{DG}(t,\\mathbf{d},\\Theta) = \\lambda_0{\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta) \n+ \\lambda_1 {\\operatorname{\\mathcal{R}}}_1({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_1}(t,\\mathbf{d},\\Theta))\n+ \\cdots \n+ \\lambda_m {\\operatorname{\\mathcal{R}}}_m({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_m}(t,\\mathbf{d},\\Theta))\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\operatorname{\\mathcal{L}}}_{DG}(t,\\mathbf{d},\\Theta)=\\lambda_{0%&#10;}{\\operatorname{\\mathcal{L}}}_{0}(t,\\mathbf{d},\\Theta)+\\lambda_{1}{%&#10;\\operatorname{\\mathcal{R}}}_{1}({\\operatorname{\\mathcal{J}}}_{{\\operatorname{%&#10;\\mathcal{L}}}_{1}}(t,\\mathbf{d},\\Theta))+\\cdots+\\lambda_{m}{\\operatorname{%&#10;\\mathcal{R}}}_{m}({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_{%&#10;m}}(t,\\mathbf{d},\\Theta))\" display=\"inline\"><mrow><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mrow><mi>D</mi><mo>\u2062</mo><mi>G</mi></mrow></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>\u2062</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mn>0</mn></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mn>1</mn></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mn>1</mn></msub></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>m</mi></msub><mo>\u2062</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>m</mi></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>m</mi></msub></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07213.tex", "nexttext": "\n\\noindent\nwhere $\\eta$ is the step-size coefficient.\n\\subsection{The Derivation}\\label{sec:derivation}\n The first update term of Equation \\ref{eqn:updateeqn}, $\\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)\\right]$, is provided by standard back-propagation. For the remaining terms, since the Jacobian of the loss also depends on the current weights $\\Theta$, we see that\n\n", "itemtype": "equation", "pos": 5656, "prevtext": "\n\n\n\n\n\\\\\n\n\\\\\n\n\n\n\\noindent\nwhere $\\mathbf{d} = (d_1,d_2,...d_k)$ is a data sample, $t$ is its corresponding label/target and $\\Theta= \\{W_1,W_2,...,W_K\\}$ represents the parameters of a $K$ layer neural network. We use  ${\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_i}$ to denote the Jacobian of ${\\operatorname{\\mathcal{L}}}_i$ (the gradient of ${\\operatorname{\\mathcal{L}}}_i$ with respect to $\\mathbf{d}$). $\\lambda_0,\\lambda_1,\\dots,\\lambda_m$ are the weight coefficients of the terms in the DataGrad loss function. {\\textcolor{black}{{Close to our work, \\citep{lyuunify} present a heuristic way to optimize a special case of this objective. By directly provding an algorithm, our analysis can explain what their algorithm optimizes.}}}\n\n\n\n\n\n\n\nWe denote the entire dataset as $\\mathcal{D}=\\{(\\mathbf{d}^{(1)}, t^{(1)}),\\dots, (\\mathbf{d}^{(n)}, t^{(n)})\\}$. Following the framework of empirical risk minimization with stochastic gradient descent, the goal is to minimize the objective function: $\\sum\\limits_{i=1}^n {\\operatorname{\\mathcal{L}}}_{DG}(t^{(i)},\\mathbf{d}^{(i)},\\Theta)$\n\n\n\nby iterating the following parameter updates (here $w^{\\ell}_{ij}$ is the component of $\\Theta$ representing the weight of the incoming edge to node $i$ of layer $\\ell$ from node $j$ of layer $\\ell-1$):\n\n", "index": 3, "text": "\\begin{align}\nw^{\\ell}_{ij} \\leftarrow w^\\ell_{ij} - \\eta \\lambda_0\\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)\\right] - \\eta\\sum\\limits_{r=1}^m \\lambda_r \\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{R}}}_r({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_r}(t,\\mathbf{d},\\Theta)\\right] \\label{eqn:updateeqn}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w^{\\ell}_{ij}\\leftarrow w^{\\ell}_{ij}-\\eta\\lambda_{0}\\frac{%&#10;\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{L}}}_{0}(t,%&#10;\\mathbf{d},\\Theta)\\right]-\\eta\\sum\\limits_{r=1}^{m}\\lambda_{r}\\frac{\\partial}{%&#10;\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{R}}}_{r}({\\operatorname%&#10;{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_{r}}(t,\\mathbf{d},\\Theta)\\right]\" display=\"inline\"><mrow><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup><mo>\u2190</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup><mo>-</mo><mi>\u03b7</mi><msub><mi>\u03bb</mi><mn>0</mn></msub><mstyle displaystyle=\"true\"><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle><mrow><mo>[</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mn>0</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow><mo>-</mo><mi>\u03b7</mi><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><msub><mi>\u03bb</mi><mi>r</mi></msub><mstyle displaystyle=\"true\"><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle><mrow><mo>[</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></msub><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07213.tex", "nexttext": "\nwhere $a_s$ is a variable that takes the current value of $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r}{\\partial d_k}$.\nIt turns out that these mixed partial derivatives (with respect to weights and with respect to data) have structural similarities to the Hessian (since derivatives with respect to the data are computed almost exactly the same way as the derivatives with respect to the lowest layer weights). Since exact computation of the Hessian is slow \\citep{bishophessian}, we would expect that the computation of this matrix of partial derivatives would also be slow. However, it turns out that we do not need to compute the full matrix -- we only need this matrix times a vector, and hence we can use ideas reminiscent of fast Hessian multiplication algorithms \\citep{multiplyhessian}. At points of continuous differentiability, we have:\n\n", "itemtype": "equation", "pos": 6491, "prevtext": "\n\\noindent\nwhere $\\eta$ is the step-size coefficient.\n\\subsection{The Derivation}\\label{sec:derivation}\n The first update term of Equation \\ref{eqn:updateeqn}, $\\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)\\right]$, is provided by standard back-propagation. For the remaining terms, since the Jacobian of the loss also depends on the current weights $\\Theta$, we see that\n\n", "index": 5, "text": "\\begin{align}\n\\frac{\\partial {\\operatorname{\\mathcal{R}}}_r({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_r}(t,\\mathbf{d},\\Theta))}{\\partial w^{(\\ell)}_{ij}}\n\n &= \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r}{\\partial d_1},\\dots,\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r}{\\partial d_k})}{\\partial w^{(\\ell)}_{ij}}\n=\\sum\\limits_{s=1}^k \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_s} \\frac{\\partial^2 {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial  w^{(\\ell)}_{ij}\\partial d_s}\\label{eqn:partialderiva}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial{\\operatorname{\\mathcal{R}}}_{r}({\\operatorname{%&#10;\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_{r}}(t,\\mathbf{d},\\Theta))}{%&#10;\\partial w^{(\\ell)}_{ij}}\\par&#10;\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mo class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></msub><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial{\\operatorname{\\mathcal{R}}}_{r}(\\frac{\\partial{%&#10;\\operatorname{\\mathcal{L}}}_{r}}{\\partial d_{1}},\\dots,\\frac{\\partial{%&#10;\\operatorname{\\mathcal{L}}}_{r}}{\\partial d_{k}})}{\\partial w^{(\\ell)}_{ij}}=%&#10;\\sum\\limits_{s=1}^{k}\\frac{\\partial{\\operatorname{\\mathcal{R}}}_{r}(a_{1},%&#10;\\dots,a_{k})}{\\partial a_{s}}\\frac{\\partial^{2}{\\operatorname{\\mathcal{L}}}_{r%&#10;}(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}\\partial d_{s}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>d</mi><mn>1</mn></msub></mrow></mfrac><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>a</mi><mi>s</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07213.tex", "nexttext": "\nevaluated at the point $\\phi=0$ and direction $\\mathbf{y}=( \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_1}, \\dots,  \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_k})$. The outer directional derivative with respect to the scalar $\\phi$ can be computed using finite differences.\nThus, Equations \\ref{eqn:partialderiva} and \\ref{eqn:partialderivb} mean that we can compute the term $\\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{R}}}_r({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_r}(t,\\mathbf{d},\\Theta)\\right]$ from the stochastic gradient descent update equation (Equation \\ref{eqn:updateeqn}) as follows.\n\n\\begin{enumerate}\n\\item Use standard back-propagation to simultaneously compute the vector derivatives $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\Theta}$ and  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\mathbf{d}}$ (note that the latter corresponds to the vector $(a_1,\\dots,a_s)$ in our derivation).\n\\item Analytically determine the gradient of ${\\operatorname{\\mathcal{R}}}_r$ with respect to its immediate inputs. For example, if ${\\operatorname{\\mathcal{R}}}_r$ is the $L_2$ penalty ${\\operatorname{\\mathcal{R}}}_r(x_1,\\dots,x_s)=|x_1|^2+\\cdots+|x_s|^2$ then the immediate gradient would be $(2x_1, \\dots, 2x_s)$ and if ${\\operatorname{\\mathcal{R}}}_r$ is the $L_1$ penalty, the immediate gradient would be $(\\text{sign}(x_1),\\dots,\\text{sign}(x_s))$.\n\\item Evaluate the immediate gradient of ${\\operatorname{\\mathcal{R}}}_r$ at the vector  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\mathbf{d}}$. This corresponds to the adversarial direction, as is denoted by $\\mathbf{y}$ in our derivation.\n\\item Form the adversarial example $\\widehat{\\mathbf{d}} = \\mathbf{d}+\\phi\\mathbf{y}$, where $\\mathbf{y}$ is the result of the previous step and $\\phi$ is a small constant.\n\\item Use a second back-propagation pass (with $\\widehat{\\mathbf{d}}$ as input) to compute  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial \\Theta}$ and then return the finite difference $\\left( \\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial \\Theta}  -  \\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\Theta}\\right)/\\phi$\n\\end{enumerate}\n\\subsection{The High-level View: Putting it All Together}\\label{sec:together}\nAt a high level, the loss ${\\operatorname{\\mathcal{L}}}_r$ and regularizer ${\\operatorname{\\mathcal{R}}}_r$ together serve to define an adversarial noise vector $\\mathbf{y}$ and adversarial example $\\widehat{\\mathbf{d}}=\\mathbf{d}+\\phi \\mathbf{y}$ (where $\\phi$ is a small constant), as explained in the previous section. Different choices of ${\\operatorname{\\mathcal{L}}}_r$ and ${\\operatorname{\\mathcal{R}}}_r$ result in different types of adversarial examples. For example, setting ${\\operatorname{\\mathcal{R}}}_r$ to be the $L_1$ penalty, the resulting adversarial example is the same as the \\emph{fast gradient sign method} of \\cite{goodfellow_explaining_2014}.\n\nPutting together the components of our finite differences algorithm, the stochastic gradient descent update equation becomes:\n\n", "itemtype": "equation", "pos": 7962, "prevtext": "\nwhere $a_s$ is a variable that takes the current value of $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r}{\\partial d_k}$.\nIt turns out that these mixed partial derivatives (with respect to weights and with respect to data) have structural similarities to the Hessian (since derivatives with respect to the data are computed almost exactly the same way as the derivatives with respect to the lowest layer weights). Since exact computation of the Hessian is slow \\citep{bishophessian}, we would expect that the computation of this matrix of partial derivatives would also be slow. However, it turns out that we do not need to compute the full matrix -- we only need this matrix times a vector, and hence we can use ideas reminiscent of fast Hessian multiplication algorithms \\citep{multiplyhessian}. At points of continuous differentiability, we have:\n\n", "index": 7, "text": "\\begin{align}\n\\sum\\limits_{s=1}^k \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_s} \\frac{\\partial^2 {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial  w^{(\\ell)}_{ij}\\partial d_s}&=\\sum\\limits_{s=1}^k \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_s} \\frac{\\partial^2 {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial d_s\\partial  w^{(\\ell)}_{ij}}\\nonumber\\\\\n&= \\frac{\\partial^2 {\\operatorname{\\mathcal{L}}}(t, \\mathbf{d}+\\phi\\mathbf{y},\\Theta)}{\\partial \\phi~\\partial w^\\ell_{ij}} \\label{eqn:partialderivb}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum\\limits_{s=1}^{k}\\frac{\\partial{\\operatorname{\\mathcal{R}}}_{%&#10;r}(a_{1},\\dots,a_{k})}{\\partial a_{s}}\\frac{\\partial^{2}{\\operatorname{%&#10;\\mathcal{L}}}_{r}(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}\\partial d_{s}}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>a</mi><mi>s</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum\\limits_{s=1}^{k}\\frac{\\partial{\\operatorname{\\mathcal{R}}}_%&#10;{r}(a_{1},\\dots,a_{k})}{\\partial a_{s}}\\frac{\\partial^{2}{\\operatorname{%&#10;\\mathcal{L}}}_{r}(t,\\mathbf{d},\\Theta)}{\\partial d_{s}\\partial w^{(\\ell)}_{ij}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u211b</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>a</mi><mi>s</mi></msub></mrow></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>d</mi><mi>s</mi></msub></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\partial^{2}{\\operatorname{\\mathcal{L}}}(t,\\mathbf{d}+\\phi%&#10;\\mathbf{y},\\Theta)}{\\partial\\phi~{}\\partial w^{\\ell}_{ij}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msup><mo>\u2202</mo><mn>2</mn></msup><mo>\u2061</mo><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mrow><mi>\ud835\udc1d</mi><mo>+</mo><mrow><mi>\u03d5</mi><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mpadded width=\"+3.3pt\"><mi>\u03d5</mi></mpadded></mrow><mo>\u2062</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup></mrow></mrow></mfrac></mstyle></mrow></math>", "type": "latex"}, {"file": "1601.07213.tex", "nexttext": "\nwhere $\\mathbf{x}_r$ is the adversarial example of $\\mathbf{d}$ resulting from regularizer ${\\operatorname{\\mathcal{R}}}_r$ in conjunction with loss ${\\operatorname{\\mathcal{L}}}_r$, and the notation $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{x},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\Big|_{\\mathbf{x}=\\mathbf{x}_r}$ here specifically means to compute the derivative using back-propagation with $\\mathbf{x}_r$ as an input -- in other words, $\\mathbf{x}_r$ is not to be treated as a function of $\\Theta$ (and its components  $w^{(\\ell)}_{ij}$ ) when computing this partial derivative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Related Work}\n\\label{related_work}\n\nSince the recent discovery of adversarial samples \\citep{szegedy_intriguing_2013}, a variety of remedies have been proposed to make neural architectures robust to this problem.  A straightforward solution is to simply add adversarial examples during each training round of stochastic gradient descent  \\citep{szegedy_intriguing_2013}. This is exactly what Equation \\ref{eqn:updateeqn2} specifies, so that post-hoc  solution can be justified as a regularization of the data gradient. Subsequent work \\citep{goodfellow_explaining_2014} introduced the objective function\n $\\sum_d \\alpha{\\operatorname{\\mathcal{L}}}(t,\\mathbf{d},\\Theta) + (1-\\alpha){\\operatorname{\\mathcal{L}}}(t,\\widehat{\\mathbf{d}},\\Theta)$, where $\\widehat{\\mathbf{d}}$ is the adversarial version of input $d$.  A gradient-based method would need to compute the derivative with respect to $w^{(\\ell)}_{ij}$, which is $\\alpha\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,d,\\Theta)}{\\partial w^{(\\ell)}_{ij}} + (1-\\alpha)\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial w^{(\\ell)}_{ij}} + (1-\\alpha)\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,\\widehat{d},\\Theta)}{\\partial \\widehat{\\mathbf{d}}} \\cdot \\frac{d~ \\widehat{\\mathbf{d}}}{d w^{(\\ell)}_{ij}}$, since the construction of $\\widehat{\\mathbf{d}}$ depends on $w^{(\\ell)}_{ij}$.  Their work approximates the optimization by ignoring the third term, as it is difficult to compute.  This approximation then results in an updated equation having the form of Equation \\ref{eqn:updateeqn2}, and hence actually optimizes the DataGrad objective.\n\\cite{nokland_improving_2015} present a variant where the deep network is trained using back-propagation only on adversarial examples (rather than a mix of adversarial and original examples). Equation 4 shows that this method optimizes the DataGrad objective with {\\textcolor{black}{{$r=1$ and $\\lambda_0$ and $\\lambda_1$ chosen so that the $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}$ term is eliminated.}}}\n\nBoth  \\cite{huang_learning_2015} and  \\cite{miyato_distributional_2015} propose to optimize constrained objective functions that can be put in the form $\\min_{\\Theta}\\sum_{\\mathbf{d}} \\max_{g(r)\\leq c} f(t,\\mathbf{d},r,\\Theta)$, where $r$ represents adversarial noise and the constraint $g(r)\\leq c$ puts a bound on the size of the noise. Letting $r^*(\\mathbf{d},\\Theta)$ be the (constrained) optimal value of $r$ for each $\\mathbf{d}$ and setting of $\\Theta$, this is the same as the objective $\\min_{\\Theta}\\sum_{\\mathbf{d}}f(t,\\mathbf{d},r^*(\\mathbf{d},\\Theta),\\Theta)$. The derivative of any term in the summation respect to $w^{(\\ell)}_{ij}$ is then equal to \n\n", "itemtype": "equation", "pos": 11858, "prevtext": "\nevaluated at the point $\\phi=0$ and direction $\\mathbf{y}=( \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_1}, \\dots,  \\frac{\\partial {\\operatorname{\\mathcal{R}}}_r(a_1,\\dots,a_k)}{\\partial a_k})$. The outer directional derivative with respect to the scalar $\\phi$ can be computed using finite differences.\nThus, Equations \\ref{eqn:partialderiva} and \\ref{eqn:partialderivb} mean that we can compute the term $\\frac{\\partial}{\\partial w^{(\\ell)}_{ij}}\\left[{\\operatorname{\\mathcal{R}}}_r({\\operatorname{\\mathcal{J}}}_{{\\operatorname{\\mathcal{L}}}_r}(t,\\mathbf{d},\\Theta)\\right]$ from the stochastic gradient descent update equation (Equation \\ref{eqn:updateeqn}) as follows.\n\n\\begin{enumerate}\n\\item Use standard back-propagation to simultaneously compute the vector derivatives $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\Theta}$ and  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\mathbf{d}}$ (note that the latter corresponds to the vector $(a_1,\\dots,a_s)$ in our derivation).\n\\item Analytically determine the gradient of ${\\operatorname{\\mathcal{R}}}_r$ with respect to its immediate inputs. For example, if ${\\operatorname{\\mathcal{R}}}_r$ is the $L_2$ penalty ${\\operatorname{\\mathcal{R}}}_r(x_1,\\dots,x_s)=|x_1|^2+\\cdots+|x_s|^2$ then the immediate gradient would be $(2x_1, \\dots, 2x_s)$ and if ${\\operatorname{\\mathcal{R}}}_r$ is the $L_1$ penalty, the immediate gradient would be $(\\text{sign}(x_1),\\dots,\\text{sign}(x_s))$.\n\\item Evaluate the immediate gradient of ${\\operatorname{\\mathcal{R}}}_r$ at the vector  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\mathbf{d}}$. This corresponds to the adversarial direction, as is denoted by $\\mathbf{y}$ in our derivation.\n\\item Form the adversarial example $\\widehat{\\mathbf{d}} = \\mathbf{d}+\\phi\\mathbf{y}$, where $\\mathbf{y}$ is the result of the previous step and $\\phi$ is a small constant.\n\\item Use a second back-propagation pass (with $\\widehat{\\mathbf{d}}$ as input) to compute  $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial \\Theta}$ and then return the finite difference $\\left( \\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial \\Theta}  -  \\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial \\Theta}\\right)/\\phi$\n\\end{enumerate}\n\\subsection{The High-level View: Putting it All Together}\\label{sec:together}\nAt a high level, the loss ${\\operatorname{\\mathcal{L}}}_r$ and regularizer ${\\operatorname{\\mathcal{R}}}_r$ together serve to define an adversarial noise vector $\\mathbf{y}$ and adversarial example $\\widehat{\\mathbf{d}}=\\mathbf{d}+\\phi \\mathbf{y}$ (where $\\phi$ is a small constant), as explained in the previous section. Different choices of ${\\operatorname{\\mathcal{L}}}_r$ and ${\\operatorname{\\mathcal{R}}}_r$ result in different types of adversarial examples. For example, setting ${\\operatorname{\\mathcal{R}}}_r$ to be the $L_1$ penalty, the resulting adversarial example is the same as the \\emph{fast gradient sign method} of \\cite{goodfellow_explaining_2014}.\n\nPutting together the components of our finite differences algorithm, the stochastic gradient descent update equation becomes:\n\n", "index": 9, "text": "\\begin{align}\nw^{\\ell}_{ij} &\\leftarrow w^\\ell_{ij} - \\eta \\lambda_0\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}} - \\eta\\sum\\limits_{r=1}^m \\frac{\\lambda_r}{\\phi}\\left(\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{x},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\Big|_{\\mathbf{x}=\\mathbf{x}_r}-\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\right)  \\nonumber\\\\\n&= w^\\ell_{ij} - \\eta \\left(\\lambda_0 - \\sum_r\\frac{\\lambda_r}{\\phi}\\right)\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}} - \\eta\\sum\\limits_{r=1}^m \\frac{\\lambda_r}{\\phi}\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{x},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\Big|_{\\mathbf{x}=\\mathbf{x}_r}\n\\label{eqn:updateeqn2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w^{\\ell}_{ij}\" display=\"inline\"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leftarrow w^{\\ell}_{ij}-\\eta\\lambda_{0}\\frac{\\partial{%&#10;\\operatorname{\\mathcal{L}}}_{0}(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}%&#10;}-\\eta\\sum\\limits_{r=1}^{m}\\frac{\\lambda_{r}}{\\phi}\\left(\\frac{\\partial{%&#10;\\operatorname{\\mathcal{L}}}_{r}(t,\\mathbf{x},\\Theta)}{\\partial w^{(\\ell)}_{ij}%&#10;}\\Big{|}_{\\mathbf{x}=\\mathbf{x}_{r}}-\\frac{\\partial{\\operatorname{\\mathcal{L}}%&#10;}_{r}(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\right)\" display=\"inline\"><mrow><mi/><mo>\u2190</mo><mrow><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mn>0</mn></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03bb</mi><mi>r</mi></msub><mi>\u03d5</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc31</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>\ud835\udc31</mi><mo>=</mo><msub><mi>\ud835\udc31</mi><mi>r</mi></msub></mrow></msub><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=w^{\\ell}_{ij}-\\eta\\left(\\lambda_{0}-\\sum_{r}\\frac{\\lambda_{r}}{%&#10;\\phi}\\right)\\frac{\\partial{\\operatorname{\\mathcal{L}}}_{0}(t,\\mathbf{d},\\Theta%&#10;)}{\\partial w^{(\\ell)}_{ij}}-\\eta\\sum\\limits_{r=1}^{m}\\frac{\\lambda_{r}}{\\phi}%&#10;\\frac{\\partial{\\operatorname{\\mathcal{L}}}_{r}(t,\\mathbf{x},\\Theta)}{\\partial w%&#10;^{(\\ell)}_{ij}}\\Big{|}_{\\mathbf{x}=\\mathbf{x}_{r}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi mathvariant=\"normal\">\u2113</mi></msubsup><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\u03bb</mi><mn>0</mn></msub><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>r</mi></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03bb</mi><mi>r</mi></msub><mi>\u03d5</mi></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mn>0</mn></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></mrow><mo>-</mo><msub><mrow><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03bb</mi><mi>r</mi></msub><mi>\u03d5</mi></mfrac></mstyle><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mo class=\"ltx_font_mathcaligraphic\">\u2112</mo><mi>r</mi></msub></mrow><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc31</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></mrow></mrow></mrow><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>\ud835\udc31</mi><mo>=</mo><msub><mi>\ud835\udc31</mi><mi>r</mi></msub></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07213.tex", "nexttext": "\nNow, if $r^*(\\mathbf{d},\\Theta)$ were an unconstrained maximum value of $r$, then  $\\frac{\\partial f(t,\\mathbf{d},r,\\Theta)}{\\partial r}\\Big|_{r=r^*(\\mathbf{d},\\Theta)}$ would equal $0$ and the second term of Equation \\ref{eqn:constrainedopt} would disappear. However, since $r^*$ is a constrained optimum and the constraint is active, the second term would generally be nonzero. Since the derivative of the constrained optimum is difficult to compute, \\cite{huang_learning_2015} and  \\cite{miyato_distributional_2015} opt to approximate/simplify the derivative making the second term disappear (as it would in the unconstrained case). {\\textcolor{black}{{Comparing the remaining term to Equation \\ref{eqn:updateeqn2} shows that they are optimizing the DataGrad objective with $r=1$ and $\\lambda_0$ and $\\lambda_1$ carefully chosen to eliminate the $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}$ term.}}}\n\n{\\textcolor{black}{{In an approach that  ends up closely related to ours, \\citep{lyuunify} consider the objective $\\min_{\\theta}\\max_{r:||r||_p\\leq\\sigma} {\\operatorname{\\mathcal{L}}}(x+r;\\theta)$ and a linearized inner version $\\max_{r:||r||_p\\leq\\sigma} {\\operatorname{\\mathcal{L}}}(x)+{\\operatorname{\\nabla}}_x{\\operatorname{\\mathcal{L}}}^T r$. They iteratively select $r$ by optimizing the latter and $\\theta$ by backprop on the former (with $r$ fixed). Since the $\\theta$ update is not directly minimizing the linearized objective, \\citep{lyuunify} claimed the procedure was only an approximation of what we cll the DataGrad objective. However, their method devolves to training on adversarial examples, so as before, Equation \\ref{eqn:updateeqn2} shows they are actually optimizing the DataGrad objective but with $r=1$ and $\\lambda_0$ and $\\lambda_1$ carefully chosen to elimiate the $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}$ term.}}}\n\n\n\n\n\nFinally, \\cite{gu_towards_2014}  penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases \\cite{gu_towards_2014}. Thus,  \\cite{gu_towards_2014} applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian. Since \\emph{DataGrad} does regularize the deep Jacobian, the work of \\cite{gu_towards_2014} can also be viewed as an approximation to \\emph{DataGrad}.\n\nThus, DataGrad provides a unifying view of previously proposed optimizations for training deep architectures that are resilient to adversarial noise.\n\n\n\n\n\n\n\n\n\\section{Experimental Results}\n\\label{results}\nBy setting $\\lambda_0=\\lambda_1=1$ and $\\lambda_j=0$ for $j>1$, setting ${\\operatorname{\\mathcal{L}}}_0={\\operatorname{\\mathcal{L}}}_1$ and either choosing ${\\operatorname{\\mathcal{R}}}_1$ to be either the $L_1$ or $L_2$ penalty, DataGrad would become a regularization algorithm on either the $L_1$ or $L_2$ norm of the Jacobian of the loss ${\\operatorname{\\mathcal{L}}}_0$. \nThese two approaches are denoted as \\emph{DG-L1} and \\emph{DG-L2}, respectively. We are interested in evaluating how this type of regularization compares to traditional $L_1$ and $L_2$ regularization of the network parameters ($L1$ and $L2$, respectively). In this setup, DataGrad requires two forward passes and two backward passes to perform a weight update.\n\n\n\n\\begin{figure}\n\\begin{subfigure}{.475\\textwidth}\n  \\centering\n  \\includegraphics[width=\\linewidth]{neural_comp_l1.pdf}\n  \\caption{L1-adversarial generalization results.}\n  \\label{l1_noise}\n\\end{subfigure}\n\\hspace{0.025\\textwidth}\n\\begin{subfigure}{.475\\textwidth}\n  \\centering\n  \\includegraphics[width=\\linewidth]{neural_comp_l2.pdf}\n  \\caption{L2-adversarial generalization results.}\n  \\label{l2_noise}\n\\end{subfigure}\n\\caption{DataGrad-regularized deep rectifier networks compared against classically-regularized variants and one with no regularization.  Note that the Noise Coefficient (L1 or L2 of ${\\operatorname{\\mathcal{R}}}_1$ and ${\\operatorname{\\mathcal{R}}}_2$) corresponds to the factor $\\phi$ under different noise settings.}\n\\label{adv_error_curves}\n\\end{figure}\n\n\n\nWe implemented several deep sparse rectifier architectures \\citep{glorot_deep_2011} (3 layers), under the various schemes described above, on the permutation-invariant MNIST data-set~\\footnote{http://yann.lecun.com/exdb/mnist/.}, comprised of 60,000 training samples and 10,000 testing samples. 3000 images (randomly sampled without replacement from the training split) were selected as a validation set that was used for tuning architecture meta-parameters via a coarse grid-search. Hyper-parameters and ranges searched included  $\\lambda = [0.0001, 0.1]$ and $\\phi = [0.005, 0.1]$ coefficients for controlling DataGrad, and the $L1 = [0.0001, 0.01]$ and $L2 = [0.0001, 0.01]$ penalty coefficients for controlling the classical regularizers. Mini-batches of size 20 were used to calculate each parameter update in a gradient-descent framework. Image features were gray-scale pixel features of which we normalized to the range of $[0,1]$. The learning rate for all models was held fixed at $0.01$ (found after some manual experimentation) and we did not use any additional gradient-descent heuristics (i.e., momentum, drop-out, etc.) for simplicity, since we are interested in investigating the effect that the regularizers have on model robustness to adversarial samples.\\footnote{We note generalization performance could be further improved with additional mechanisms, such as an adaptive learning rate, finer-grained meta-parameter tuning, and usage of momentum and drop-out.} We conducted ten trials for each learning set-up (with a unique seed for random initialization) and selected the best model for each using an averaged validation error score before application to the test-sets. \n\nBeyond evaluating original MNIST test-split generalization error, we generate and evaluate performance on a series of adversarial test-sets each comprised of 10,000 samples. Each candidate adversarial sample is generated via back-propagation using the derivative of the loss with respect to inputs followed by application of the appropriate regularizer function to create the adversarial noise.  In this procedure, adversarial noise coefficient $\\phi$ is varied along the values $\\{0.005, 0.01, 0.05, 0.1\\}$, where values correspond to maximal pixel gains of $\\{\\sim 1, \\sim 3, \\sim 12, \\sim 25\\}$.  For each setting, we generate two adversarial sets corresponding to noise generated under both ${\\operatorname{\\mathcal{R}}}_1$ and ${\\operatorname{\\mathcal{R}}}_2$.\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\begin{subtable}{\\textwidth}\n  \\centering\n  \\resizebox{0.7\\hsize}{!} {\n  \\begin{tabular}{lrrrrr}\n  \n  \\multicolumn{1}{l}{\\textbf{$\\phi$ =}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.0} \\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.005}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.01}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.05}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.1}\\\\\\end{tabular}}\\tabularnewline\n  \\hline\n  \\textit{Rect-MLP} & $4.87 \\%$ & $6.46 \\%$ & $8.69 \\%$ & $55.71 \\%$ & $90.85 \\%$\\tabularnewline\n  \\hline\n  \\end{tabular}\n  }\n  \n  \\label{adv_norm_error}\n\\end{subtable}\n\\newline\n\\newline\n\\begin{subtable}{\\textwidth}\n  \\centering\n  \\begin{tabular}{ccccc}\n  \\includegraphics[scale=0.95]{img_sample_495_original.jpg}&\\includegraphics[scale=0.95]{img_sample_495_t_0_005.jpg}&\\includegraphics[scale=0.95]{img_sample_495_t_0_01.jpg}&\\includegraphics[scale=0.95]{img_sample_495_t_0_05.jpg}&\\includegraphics[scale=0.95]{img_sample_495_t_0_1.jpg}\\\\\n  \\end{tabular}\n  \n  \\label{adv_norm_samps}\n\\end{subtable}\n\n  \\begin{subtable}{\\textwidth}\n  \\centering\n  \\resizebox{.7\\hsize}{!} {\n  \\begin{tabular}{lrrrrr}\n  \n  \\multicolumn{1}{l}{\\textbf{$\\phi$ =}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.0} \\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.005}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.01}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.05}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.1}\\\\\\end{tabular}}\\tabularnewline\n  \\hline\n  \\textit{Rect-L2} & $4.44 \\%$ & $5.94 \\%$ & $7.96 \\%$ & $52.85 \\%$ & $89.73 \\%$\\tabularnewline\n  \\hline\n  \\end{tabular}\n  }\n  \n  \\label{adv_l2_error}\n\\end{subtable}\n\\newline\n\\newline\n\\begin{subtable}{\\textwidth}\n  \\centering\n  \\begin{tabular}{ccccc}\n  \\includegraphics[scale=0.95]{img_sample_1192_original.jpg}&\\includegraphics[scale=0.95]{img_sample_1192_t_0_005.jpg}&\\includegraphics[scale=0.95]{img_sample_1192_t_0_01.jpg}&\\includegraphics[scale=0.95]{img_sample_1192_t_0_05.jpg}&\\includegraphics[scale=0.95]{img_sample_1192_t_0_1.jpg}\\\\\n  \\end{tabular}\n  \n  \\label{adv_l2_samps}\n\\end{subtable}\n\n  \\begin{subtable}{\\textwidth}\n  \\centering\n  \\resizebox{.7\\hsize}{!} {\n  \\begin{tabular}{lrrrrr}\n  \n  \\multicolumn{1}{l}{\\textbf{$\\phi$ =}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.0} \\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.005}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.01}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.05}\\\\\\end{tabular}}&\\multicolumn{1}{c}{\\begin{tabular}[x]{@{}c@{}}\\textbf{0.1}\\\\\\end{tabular}}\\tabularnewline\n  \\hline\n  \\textit{Rect-DG} & $3.72 \\%$ & $4.26 \\%$ & $4.92 \\%$ & $14.80 \\%$ & $43.30 \\%$\\tabularnewline\n  \\hline\n  \\end{tabular}\n  }\n  \n  \\label{adv_datagrad_error}\n\\end{subtable}\n\\newline\n\\newline\n\\begin{subtable}{\\textwidth}\n  \\centering\n  \\begin{tabular}{ccccc}\n  \\includegraphics[scale=0.95]{img_sample_1112_original.jpg}&\\includegraphics[scale=0.95]{img_sample_1112_t_0_005.jpg}&\\includegraphics[scale=0.95]{img_sample_1112_t_0_01.jpg}&\\includegraphics[scale=0.95]{img_sample_1112_t_0_05.jpg}&\\includegraphics[scale=0.95]{img_sample_1112_t_0_1.jpg}\\\\\n  \\end{tabular}\n  \n  \\label{adv_datagrad_samps}\n\\end{subtable}\n\n\\caption{Adversarial test-set error ($\\phi = 0$ corresponds to original test-split) and samples generated from a deep sparse rectifier network in the case of (1) or no regularization, (2) or L2-regularization, and (3) or L1 DataGrad-regularized.}\n\\label{adv_viz_results}\n\\end{figure}\n\n\n\n\n\nWe observe in Figures \\ref{adv_error_curves} and \\ref{adv_viz_results} that a DataGrad-regularized architecture outperforms a non-regularized and classically-regularized one.  However, it is more striking that as the $\\phi$ factor is raised, the non-regularized model's performance quickly and dramatically degrades.  Classical $L_1$ and $L_2$ regularizers appear to to mitigate some of the damage, but seemingly only afford minimal robustness to adversarial perturbation.  In contrast, the proposed \\emph{DG-L1} and \\emph{DG-L2} regularizers appear to yield a significant reduction in error on the various adversarial test-sets, the improvement clearer as the $\\phi$ is increased.  The visualization of some adversarial samples (Figure \\ref{adv_viz_results}) demonstrates that though more noise is applied to generate adversarials, the sampls themselves are still quite recognizable to the human eye.  However, a neural architecture, such as a deep rectifier network, is sensitive to adversarial noise and incorrectly classifies these images. Interestingly enough, we also observe improved classification error on the original test-set when using \\emph{DataGrad}, the \\emph{DG-L1} variant offering the lowest error of all.\n\n\\section*{Conclusion}\nTo create deep neural architecture robust to adversarial examples, we proposed the \\emph{DataGrad} learning procedure. Our general formulation unifies previous proposals for adversarial training of neural architectures. Empirically, we found that this regularized form of learning not only significantly reduces error in classifying adversarial samples but improves generalization.  We postulate a reason for this is that adversarial samples generated during the \\emph{DataGrad} learning phase potentially cover more of the underlying data manifold (yielding benefits similar to synthetic-data-based training).  \n\nSince \\emph{DataGrad} is effectively a ``deep'' data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised \\cite{bengio_greedy_2007}, or hybrid \\cite{ororbia_deep_hybrid_2015a}). Future work entails further improving the efficiency of the proposed \\emph{DataGrad} back-propagation procedure and investigating our procedure in wider variety of settings.\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{apa}\n\\bibliography{ref}\n\n", "itemtype": "equation", "pos": 16099, "prevtext": "\nwhere $\\mathbf{x}_r$ is the adversarial example of $\\mathbf{d}$ resulting from regularizer ${\\operatorname{\\mathcal{R}}}_r$ in conjunction with loss ${\\operatorname{\\mathcal{L}}}_r$, and the notation $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_r(t,\\mathbf{x},\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\Big|_{\\mathbf{x}=\\mathbf{x}_r}$ here specifically means to compute the derivative using back-propagation with $\\mathbf{x}_r$ as an input -- in other words, $\\mathbf{x}_r$ is not to be treated as a function of $\\Theta$ (and its components  $w^{(\\ell)}_{ij}$ ) when computing this partial derivative.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Related Work}\n\\label{related_work}\n\nSince the recent discovery of adversarial samples \\citep{szegedy_intriguing_2013}, a variety of remedies have been proposed to make neural architectures robust to this problem.  A straightforward solution is to simply add adversarial examples during each training round of stochastic gradient descent  \\citep{szegedy_intriguing_2013}. This is exactly what Equation \\ref{eqn:updateeqn2} specifies, so that post-hoc  solution can be justified as a regularization of the data gradient. Subsequent work \\citep{goodfellow_explaining_2014} introduced the objective function\n $\\sum_d \\alpha{\\operatorname{\\mathcal{L}}}(t,\\mathbf{d},\\Theta) + (1-\\alpha){\\operatorname{\\mathcal{L}}}(t,\\widehat{\\mathbf{d}},\\Theta)$, where $\\widehat{\\mathbf{d}}$ is the adversarial version of input $d$.  A gradient-based method would need to compute the derivative with respect to $w^{(\\ell)}_{ij}$, which is $\\alpha\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,d,\\Theta)}{\\partial w^{(\\ell)}_{ij}} + (1-\\alpha)\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,\\widehat{\\mathbf{d}},\\Theta)}{\\partial w^{(\\ell)}_{ij}} + (1-\\alpha)\\frac{\\partial {\\operatorname{\\mathcal{L}}}(t,\\widehat{d},\\Theta)}{\\partial \\widehat{\\mathbf{d}}} \\cdot \\frac{d~ \\widehat{\\mathbf{d}}}{d w^{(\\ell)}_{ij}}$, since the construction of $\\widehat{\\mathbf{d}}$ depends on $w^{(\\ell)}_{ij}$.  Their work approximates the optimization by ignoring the third term, as it is difficult to compute.  This approximation then results in an updated equation having the form of Equation \\ref{eqn:updateeqn2}, and hence actually optimizes the DataGrad objective.\n\\cite{nokland_improving_2015} present a variant where the deep network is trained using back-propagation only on adversarial examples (rather than a mix of adversarial and original examples). Equation 4 shows that this method optimizes the DataGrad objective with {\\textcolor{black}{{$r=1$ and $\\lambda_0$ and $\\lambda_1$ chosen so that the $\\frac{\\partial {\\operatorname{\\mathcal{L}}}_0(t,\\mathbf{d},\\Theta)}{\\partial w^{(\\ell)}_{ij}}$ term is eliminated.}}}\n\nBoth  \\cite{huang_learning_2015} and  \\cite{miyato_distributional_2015} propose to optimize constrained objective functions that can be put in the form $\\min_{\\Theta}\\sum_{\\mathbf{d}} \\max_{g(r)\\leq c} f(t,\\mathbf{d},r,\\Theta)$, where $r$ represents adversarial noise and the constraint $g(r)\\leq c$ puts a bound on the size of the noise. Letting $r^*(\\mathbf{d},\\Theta)$ be the (constrained) optimal value of $r$ for each $\\mathbf{d}$ and setting of $\\Theta$, this is the same as the objective $\\min_{\\Theta}\\sum_{\\mathbf{d}}f(t,\\mathbf{d},r^*(\\mathbf{d},\\Theta),\\Theta)$. The derivative of any term in the summation respect to $w^{(\\ell)}_{ij}$ is then equal to \n\n", "index": 11, "text": "\\begin{align}\n\\frac{\\partial f(t,\\mathbf{d},r,\\Theta)}{\\partial w^{(\\ell)}_{ij}}\\Big|_{r=r^*(\\mathbf{d},\\Theta)} + \\frac{\\partial f(t,\\mathbf{d},r,\\Theta)}{\\partial r}\\Big|_{r=r^*(\\mathbf{d},\\Theta)}\\cdot \\frac{\\partial r^*(\\mathbf{d,\\Theta})}{\\partial w^{(\\ell)}_{ij}}\\label{eqn:constrainedopt}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial f(t,\\mathbf{d},r,\\Theta)}{\\partial w^{(\\ell)}_{ij}%&#10;}\\Big{|}_{r=r^{*}(\\mathbf{d},\\Theta)}+\\frac{\\partial f(t,\\mathbf{d},r,\\Theta)}%&#10;{\\partial r}\\Big{|}_{r=r^{*}(\\mathbf{d},\\Theta)}\\cdot\\frac{\\partial r^{*}(%&#10;\\mathbf{d,\\Theta})}{\\partial w^{(\\ell)}_{ij}}\" display=\"inline\"><mrow><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>r</mi><mo>=</mo><mrow><msup><mi>r</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>+</mo><mrow><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>r</mi></mrow></mfrac></mstyle><mo fence=\"true\" maxsize=\"160%\" minsize=\"160%\">|</mo></mrow><mrow><mi>r</mi><mo>=</mo><mrow><msup><mi>r</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>\u22c5</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msup><mi>r</mi><mo>*</mo></msup></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc1d</mi><mo>,</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u2113</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mfrac></mstyle></mrow></mrow></math>", "type": "latex"}]