[{"file": "1601.06060.tex", "nexttext": "\nand $d(P') > d^*$. Hence, $S$ must be chosen so that $\\sum_{e \\in S} b(e)=x$. Allocation $r^*$ is optimal and $\\mathcal{S}(I^k)=\\mathcal{S}^*$.\n\n\\flushright\\vspace{-3mm}\\qedhere\n\\end{proof}\n\n\nThe proof of Theorem~\\ref{thm:NPhardGeneral} shows NP-hardness with a streaming graph that is not quite an SPD graph due to the edges ending at nodes $\\nu_i$.\nMoreover, it achieves the reduction from the subset sum problem using a construction that relies on variable transfer and task weights.\nSince many hard problems can be solved efficiently on series-parallel graphs~\\cite{takamizawa82} an interesting question is whether optimal task allocation is efficiently solvable on SPD graphs. The following theorem answers this question in the negative, even for computationally constrained problems and even if all transfer weights and all task weights are constant. The claim follows from a reduction from the decision version of the  NP-complete  \\emph{partition problem}. \n\n\n\\begin{theorem} \\label{thm:NPharduniform}\nThe task allocation problem on SPD graphs in concise format is NP-hard, even if transfer and task weights are uniform.\n\\end{theorem}\n\\begin{proof}\nThe claim follows from a reduction from the decision version of the  NP-complete  \\emph{partition problem}, which, given a multiset $\\mathcal{S} := \\{s_1,\\ldots,s_n\\}$ of positive integers, asks for a partitioning of $\\mathcal{S}$ into two subsets $\\mathcal{S}_1$ and $\\mathcal{S}_2$ such that the sum of the numbers in $\\mathcal{S}_1$ is equal to the sum of the numbers in $\\mathcal{S}_2$. Given an instance $\\mathcal{S}=\\{s_1,\\ldots,s_k\\}$ of the partitioning problem, we construct a task allocation problem instance as follows: for each $s_i$, we construct a streaming graph component $G_i$ consisting of a serial composition of two sets of $s_i$ parallel nodes, i.e., $G_i :=G_{i,l} {\\ensuremath{\\circ_s}} G_{i,r}$ where $G_{i,l} =G_{i,r}=(V,E), |V|=s_i$ and $E=\\emptyset$. Edge weights and node weights have positive constant values, i.e., $w(v)=\\alpha$, $b(e)=\\beta$ for all $v\\in V$, $e\\in E$. Let $\\alpha=\\beta:=1$ for simplicity. \nLet $d$ be the output of the algorithm that computes the streaming cost of the graph $G:=G_1 {\\ensuremath{\\circ_p}} G_2 {\\ensuremath{\\circ_p}} \\ldots {\\ensuremath{\\circ_p}} G_k$ given that $\\mathcal{R}=\\{R_1,R_2\\}$. If $d=n:=2\\cdot \\sum_{i=1}^k s_i$, we answer the partitioning problem positively and negatively otherwise.\nIt remains to show that $d(G)=n$ if and only if there exists a perfect partitioning of $\\mathcal{S}$. If a perfect partitioning exists then the following mapping provides an optimal task allocation:\n\\begin{displaymath}\n r(v) := \\begin{cases} R_1 &\\mbox{if } v\\in G_i \\mbox{ and } \\sigma(s_i)=S_1 \\\\ \nR_2 & \\mbox{otherwise,}\n\\end{cases}\n\\end{displaymath}\nwhere $\\sigma$ is an optimal partitioning of $\\mathcal{S}$ into two sets $S_1,S_2$. Since $\\sigma$ is perfect it holds that $|R_1|=|R_2|=n/2$. Each path in $G$ entails processing cost of $2\\cdot n/2$ and zero transfer cost. For any other mapping $r'$ with $n(R_1)=n(R_2)=n/2$ there is at least one path $(u,v)$ with $r'(u)\\neq r'(v)$ yielding streaming cost $d(G)>n$. For any mapping $r'$ with $n(R_1)\\neq n(R_2)$, let  $n(R_1)> n(R_2)$ w.l.o.g., then there exists a component $G_i$ with more nodes mapped to $R_1$ than $R_2$. Hence, there exists a path $(u,v)\\in G_i$ with $r'(u)=r'(v)=R_1$ yielding streaming cost $d(G)=2\\cdot |R_1|>n$, which contradicts optimality.\n If no perfect partitioning exists then any mapping $r$ with $n(R_1)=n(R_2)=n/2$ implies that there is a component $G_i$ with nodes mapped to $R_1$ and $R_2$. Hence, there exists a path in $G_i$ with cost $2\\cdot n/2 + 1$ and $d(G)>n$. If the optimal mapping $r$ chooses $n(R_1)\\neq n(R_2)$ then the same argument holds as in the case where a perfect partitioning exists: if  $n(R_1)> n(R_2)$, then there is a $P=\\{(u,v)\\}$ with $r(u)=r(v)=R_1$, which entails that $d(G)>n$.\n\\end{proof}\n\nNote that the reduction of the proof of Theorem~\\ref{thm:NPharduniform} uses a streaming graph where the number of nodes is proportional to the sum of the numbers $s_i$ of the partitioning problem. Since partition problem instances are only NP-hard if they contain $s_i \\in \\mathcal{S}$ that are exponentially large in $|\\mathcal{S}|$~\\cite{mertens06}, the used streaming graph contains a number of nodes that is exponential in the bit representation of the partitioning instance. \nTherefore, Theorem~\\ref{thm:NPharduniform} proves NP-hardness only for concise representations of task allocation problem instances. For example, each $G_i$ component of the used graph can be described in polynomial space similarly as in the proof. While we leave the question of hardness for non-concise SPD graph representations open, it is possible to adapt the approximation algorithm presented in Section~\\ref{sec:algorithm} so as to handle concise instances of the graphs used in the proof of Theorem~\\ref{thm:NPharduniform}.\n\nNote that the edge and task weight constants, $\\alpha$ and $\\beta$, in the proof of Theorem~\\ref{thm:NPharduniform} can be set independently to any positive value. As such, the proof  holds for computationally constrained graphs as well as non-constrained graphs.\n\nWhile the task allocation problem is NP-hard, the simple algorithm that assigns all tasks to one resource, i.e., $r(u):=R_1$ for all $u \\in V$, achieves an $n$-approximation: Obviously, we have that $\\ell(e) = 0$ for all $e\\in E$, which implies that $d(P) = \\sum_{v \\in P} d(v) = n\\sum_{v \\in P} w(v)$ for all $P \\in \\mathcal{P}$. Therefore, we get that $d(G) = n \\sum_{v \\in P'} w(v)$ where $P'$ is the path with the largest sum of node weights.\nSince for \\emph{any} allocation, $d(G) \\ge d(P')$ and the minimum cost of path $P'$ is\n\\begin{displaymath}\nd(P') =  \\sum_{v \\in P'} d(v) + \\sum_{e \\in P'} \\ell(e) \\ge \\sum_{v \\in P'} d(v) \\ge \\sum_{v \\in P'} w(v),\n\\end{displaymath}\nthe claimed bound follows.\nThis straightforward solution is optimal if $c=1$ or if edge weights are exceedingly large; in particular if $b(e) \\ge k n D w_{max}$ for each edge $e\\in E$, where $D$ is the graph diameter, $w_{max} := \\max_{v \\in V} w(v)$, and $k$ is a constant $\\ge 1$. Collocating all nodes on one resource is optimal since the streaming cost of any path is upper bounded by $nDw_{max}$.\n\nThe case of large edge weights can be considered the opposite of computationally constrained graphs, since the transfer costs dominate the solution rather than the processing costs. Imposing a specific \\emph{upper bound} on the edge weights, on the other hand, results in a computationally constrained graph:\n\n\\begin{lemma}\\label{lma:compConstrained}\nIf $b(e) \\le k w_{min}\\lceil D/c\\rceil$ for each $e\\in E$, where $w_{min} := \\min_{v \\in V} w(v)$, $D$ is the diameter of $G$,  and $k$ is a positive constant, then $G$ is computationally constrained.\n\\end{lemma}\n\\begin{proof}\n Due to the bound on $b(e)$, the maximum transfer cost along any path is $\\max_{P \\in \\mathcal{P}(G)} \\sum_{e \\in P} b(e) \\le D k w_{min}\\lceil D/c\\rceil$.\nWe will now show that\n$\\hat{d}(G) \\in \\Omega\\left(w_{min}\\left(D+D^2/c\\right)\\right)$\nfor any $G$, $c$, and mapping $r$, which implies that $d(G) \\in O(\\hat{d}(G))$.\nThe lower bound of $\\Omega(w_{min}D)$ is trivial since $d(v) \\ge w_{min}$ for all $v \\in V$, resulting in a total processing cost of at least $w_{min}D$ on each path of length $D$.\nAssume for the sake of argument that there is a path $P$ of length $D$ where $w(v) = w_{min}$ for each $v\\in P$. The streaming cost of $P$ is at least $\\lceil w_{min}D^2/c\\rceil$ even when excluding transfer cost and \\emph{all} $c$ resources are used up exclusively for $P$. Naturally, the streaming cost can only increase when there are nodes with larger weights, the resources are shared with other nodes, or the nodes on $P$ are mapped to fewer than $c$ resources. Thus, $\\Omega(w_{min}D^2/c)$ is a lower bound on $d(G)$ as claimed.\n\\end{proof}\n\n\nThe lemma shows that the edge weights may be considerably larger than the node weights, in the order of $D/c$, without affecting the asymptotic streaming cost. More generally, if $\\sum_{e \\in P} b(e) \\in O(\\hat{d}(G))$ for all $P \\in \\mathcal{P}(G)$ for any mapping $r: V \\rightarrow \\mathcal{R}$, then $G$ is computationally constrained as well.\n\n\n\\section{Algorithm}\n\\label{sec:algorithm}\nBefore describing the main algorithm, we start with a straightforward algorithm to illustrate the difficulty of the task allocation problem.\nThe algorithm $\\mathcal{A}^{avg}$ strives to distribute the work equally among the resources. Specifically, it partitions the nodes into $c$ groups such that the sum of node weights is as balanced as possible. More formally, it minimizes\n\\begin{displaymath}\n\\max_{R\\in \\mathcal{R}} \\sum_{v \\in r^{-1}(R)} w(v) -\\min_{R\\in \\mathcal{R}} \\sum_{v \\in r^{-1}(R)} w(v),\n\\end{displaymath}\nwhere $r^{-1}: \\mathcal{R} \\rightarrow 2^V$ determines the set of nodes mapped to a particular resource $R \\in \\mathcal{R}$.\\footnote{Note that finding such a partitioning is NP-hard by itself.}  \nWhile this approach seems reasonable, there are instances where $\\mathcal{A}^{avg}$ fails to achieve a better approximation ratio than the trivial algorithm that only uses one resource. We can take the graph consisting of $n$ parallel nodes, where $w(v_1) := n/3$ and $w(v_i) := 1$ for all $i\\in \\{2,\\ldots,n\\}$, as an example and set $c:=2$. As the sum of weights is $\\frac{4}{3}n-1$, algorithm $\\mathcal{A}^{avg}$ attempts to assign $\\frac{2}{3}n-\\frac{1}{2}$ work to each resource. Without loss of generality, let $r(v_1) = R_1$. Since the weight of all other nodes is $1$, we get that $n(R_1) \\ge \\frac{1}{3}n-1$. Hence it follows that $d(v_1) \\ge \\frac{n^2}{9}-\\frac{n}{3} \\in \\Omega(n^2)$, implying that $d(G) \\in \\Omega(n^2)$. The optimal solution, however, dedicates one resource completely to $v_1$, which entails that $d(v_1) = n/3$ and $d(v_i) = n-1$ for all $i\\in \\{2,\\ldots,n\\}$. The streaming cost is therefore only $d(G) \\in {\\mathcal{O}}(n)$.\n\nInstead of tackling the task allocation problem directly, we will now take a detour and present an algorithm for a continuous version of the problem, which our main algorithm will leverage.\n\n\\subsection{Continuous Algorithm}\n\nThe continuous version of Problem~\\ref{def:general_problem} is defined as follows. There is a \\emph{capacity} $c > 0$ that must be assigned to the tasks, i.e., each task $v$ gets a \\emph{share} $x(v) \\in \\mathbb{R}^+$ of the capacity subject to $\\sum_{v \\in V} x(v) \\le c$. Given a task's weight and share, the \\emph{continuous processing cost} $\\delta(v)$ is defined as\n\n$\\delta(v) := w(v) / x(v)$. \n\nThere are no transfer costs in this model, and hence the streaming cost of a path $P$ is $\\delta(P) := \\sum_{v\\in P} \\delta(v)$. As in the discrete model, the streaming cost of $G$ in the continuous model is the maximum streaming cost over all paths, i.e.,\n\n$\\delta(G) := \\max_{P\\in \\mathcal{P}(G)} \\delta(P)$.\n\n\nThe goal is to assign shares in a way that minimizes the streaming cost.\n\n\n\\begin{problem}\\label{def:continuous_problem} Given a weighted directed acyclic graph $G$ and a capacity $c>0$, find a mapping $x: V \\rightarrow \\mathbb{R}^+$ that minimizes $\\delta(G)$.\n\\end{problem}\n\nObviously, it must hold that $\\sum_{v \\in V} x(v) = c$ in an optimal allocation, i.e., the entire capacity is assigned. The motivation for studying this problem is that the optimal solution of Problem~\\ref{def:continuous_problem} is a lower bound on the streaming cost in the discrete model.\n\n\\begin{theorem} For all graphs $G$ and $c \\in \\mathbb{N}$ it holds that $\\delta(G) \\le d(G)$, where $c$ is the capacity in the continuous case and the number of resources in the discrete case.\n\\end{theorem}\n\n\\begin{proof}\nConsider the optimal solution $r$ of Problem~\\ref{def:general_problem}. Set $x(v) := 1/n(r(v))$ for all $v\\in V$. It holds that\n\n", "itemtype": "equation", "pos": 20916, "prevtext": "\n\n\\sloppy\n\n\n\\title{Task Allocation for Distributed Stream Processing\\\\(Extended Version)}\n\\date{}\n\n\\author{Raphael Eidenbenz \\\\ABB Corporate Research \n\\\\ raphael.eidenbenz@ch.abb.com \n\\and\nThomas Locher \\\\ABB Corporate Research \n\\\\thomas.locher@ch.abb.com\n}\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\n\nThere is a growing demand for live, on-the-fly processing of increasingly large amounts of data.\nIn order to ensure the timely and reliable processing of streaming data, a variety of distributed stream processing architectures and platforms have been developed, which handle the fundamental tasks of (dynamically) assigning processing tasks to the currently available physical resources and routing streaming data between these resources.\nHowever, while there are plenty of platforms offering such functionality, the theory behind it is not well understood. In particular, it is unclear how to best allocate the processing tasks to the given resources.\n\nIn this paper, we establish a theoretical foundation by formally defining a task allocation problem for distributed stream processing, which we prove to be NP-hard. Furthermore, we propose an approximation algorithm for the class of\nseries-parallel decomposable graphs,\nwhich captures a broad range of common stream processing applications. The algorithm achieves a constant-factor approximation under the assumptions that the number of resources scales at least logarithmically with the number of computational tasks and the computational cost of the tasks dominates the cost of communication.\n\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec:introduction}\n\n\nThe stream processing paradigm, where data streams are processed by applying a series of functions to the elements in the data streams, is gaining in importance due to the large variety of supported applications.\nAn early adopter of stream processing, and the related complex event processing paradigm, was the financial service industry, where it is used, e.g., \nto rapidly detect relevant events in high-frequency stock trading. Stream processing is also applied to digital control systems in order to continuously supervise and record the system state, and in network security to monitor network traffic.\nAnother use case is customer experience management, where, for example, click stream data is analyzed on-the-fly to measure customer behavior.\n\nAs the amount of data to process grows and the steady, uninterrupted processing of data becomes more and more critical,\nscalability and fault-tolerance are becoming key requirements. Distributed stream processing platforms address these issues by spreading the workload across an extensible network of machines and by dynamically redistributing tasks in the event of machine or network failures.\nStream processing jobs typically exhibit the properties that individual data items can enter the various stages of processing independently and simultaneously and that the number of arithmetic operations per data transfer is high, i.e., the computational complexity dominates the communication cost.\n\nThe rising popularity of distributed stream processing has led to the development of many systems (e.g., \\cite{akidau13,arasu03,chandrasekaran03,gedik08,neumeyer10}) that offer simple programming interfaces while abstracting away the underlying complexity of distributing tasks, routing streams, and handling failures, akin to the MapReduce framework~\\cite{dean04} for batch processing. \nThe basic principle behind these platforms is the notion of a processing element (PE) that consumes one or more data streams, processes the received data, and continuously outputs processing results again in the form of data streams. \nThese PEs are interconnected, forming a \\emph{streaming topology} where PEs without incoming streams receive external data streams for processing, and the set of PEs without outgoing streams constitutes the final stage of processing after which the results are typically stored or displayed.\nWhile a PE technically encapsulates a specific computational task, we treat the terms \\emph{PE} and \\emph{task} as synonymous.\nAn essential function of a streaming platform is to allocate the PEs to the available physical resources. \nIn the following, we will refer to this function as \\emph{task allocation}. In most proposed systems, simple schemes, such as round-robin, and heuristics are used to allocate tasks. The lack of a formal specification of the problem negates the possibility to optimize the distribution of PEs for an optimal utilization of the resources, which would result in a minimized processing delay.\n\nIn this paper, we focus on this task allocation problem for distributed stream processing and propose a basis for a formal treatment, enabling the rigorous analysis of allocation strategies.\nFurthemore, we introduce a novel class of series-parallel-type graphs that we use to model streaming topologies: This model is based on the premise that many processing jobs either consist of a series of linearly dependent tasks, tasks that can be executed independently in parallel, or a combination thereof. We argue that this model is of particular interest as it adequately captures a large subset of practical stream processing applications.\nThe task allocation problem is shown to be NP-hard, even when the streaming topology is restricted to the aforementioned class of series-parallel graphs. The main contribution of the paper is an approximation algorithm that deterministically achieves an approximation ratio of ${\\mathcal{O}}(n^{{\\mathcal{O}}(1/c)})$, where $n$ denotes the number of tasks and $c$ is the number of physical resources---subject to the constraint that the communication cost is upper bounded asymptotically by the computational cost, which, as stated before, is often the case in practice.\nHence, the algorithm guarantees a constant-factor approximation to the cost of the best possible allocation provided that $c \\in \\Omega(\\log n)$.\n\nThe paper is structured as follows. In the next section, the model and a few general complexity results are presented. The approximation algorithm is described and analyzed in Section~\\ref{sec:algorithm}. \n\nRelated work on task allocation and distributed stream processing is summarized in Section~\\ref{sec:relatedwork}. Finally, Section~\\ref{sec:conclusion} concludes the paper.\n\n\n\n\\section{Model}\n\\label{sec:model}\n\nIn this section, we introduce the task allocation problem and the class of {series-parallel-decomposable} graphs, and present some general complexity bounds.\n\n\\subsection{Task Allocation}\nWe model the streaming topology as a directed acyclic \\emph{streaming graph} $G = (V,E)$, where $|V| =: n$.\nLet $V_s \\subseteq V$ denote \\emph{source nodes}, i.e., the set of nodes without incoming edges, and let $V_t \\subseteq V$ be the \\emph{sink nodes}, i.e., the set of nodes without outgoing edges.\nGraph $G$ represents the stream processing topology in that each node $v\\in V$ corresponds to a processing task, where a \\emph{task} is an atomic unit of computation to be executed on a single machine.\nA weight function $w: V \\rightarrow \\mathbb{R}^+$ determines the computional complexity of the tasks, e.g., reflecting the time it takes to process an item in the data stream.\nThis definition assumes that the task complexity is independent of the particular data item to process, i.e., the computational effort is equal for all items in the data stream. While this is a simplification, the variance is small for a wide range of tasks in practice, e.g., processing a steady stream of measurements over a fixed size window, and can often be neglected.\n\nThe tasks must be allocated to $c$ processing resources $\\mathcal{R} := \\{R_1,\\ldots,R_c\\}$, which is formally captured by a function $r:V \\rightarrow \\mathcal{R}$ that defines the allocation of tasks to resources.\nThe number of tasks mapped to resource $R$ is defined as $n(R) := |\\{v \\in V \\,|\\, r(v) = R\\}|$. This definition implies that $\\sum_{i=1}^c n(R_i) = n$.\nThe more tasks are executed on the same resource $R$, the more it must divide its processing capacity. We define the \\emph{processing cost} of a task $v$ as\n\\begin{displaymath}\nd(v) := w(v)\\cdot n(r(v)),\n\\end{displaymath}\ni.e., the cost grows linearly with the weight and the number of tasks mapped to the same resource. \nSuch a linear cost model reflects the concurrency model where each allocated task gets an equal share of the resource, which entails that the individual processing times grow linearly with the number of collocated tasks. Note that we refrain from introducing a scaling parameter to the cost of collocating tasks but use the number $n(r(v))$ of collocated tasks directly to compute the processing cost of a task, i.e., we assume that weights are scaled appropriately.\nThe advantage of collocating tasks on the same resource is that data streams between those tasks need not be routed over the network, an operation which incurs a certain cost.\nThe cost of transferring data along an edge $e = (u,v) \\in E$ is given by its edge weight $b(e) \\ge 0$, which must be paid only if $u$ and $v$ are not collocated, i.e., communication on the same resource is assumed to be free of cost. More formally, the \\emph{transfer cost} $\\ell(e)$ of $e=(u,v)$ is\n\\begin{displaymath}\n \\ell(e) := \\begin{cases} b(e) &\\mbox{if } r(u) \\neq r(v) \\\\ \n0 & \\mbox{otherwise.}\n\\end{cases}\n\\end{displaymath}\nTransfer costs can be used to model data rates, latencies, or a combination thereof.\nThe \\emph{streaming cost} on a (directed) $v_s$-$v_t$-path $P = (v_s,v_1,\\ldots,v_\\ell,v_t)$, where $v_s \\in V_s$ and $v_t \\in V_t$, is defined as the sum of the processing cost of each node on the path, plus the transfer cost of each edge on the path.\n\\begin{displaymath}\nd(P) := \\sum_{v \\in P} d(v) + \\sum_{e\\in P} \\ell(e).\n\\end{displaymath}\n\nLet $\\mathcal{P}(G)$ denote the set of all paths in $G$ starting at a source node and terminating at a sink node.\nThe primary objective is to minimize the streaming cost of the entire graph $G$, which is defined as the streaming cost on the worst-case path, i.e.,\n\\begin{displaymath}\nd(G) := \\max_{P\\in \\mathcal{P}(G)} d(P).\n\\end{displaymath}\n\n\\begin{problem}[Task Allocation]\\label{def:general_problem} \nGiven a weighted directed acyclic graph $G$ and a set $\\mathcal{R}$ of $c$ resources, find the mapping $r: V \\rightarrow \\mathcal{R}$ that minimizes $d(G)$.\n\\end{problem}\n\nThe relation between node and edge weights, and the resulting processing and transfer costs, has an immediate impact on the streaming cost.\nAs mentioned before, it is often reasonable to assume that processing costs dominate the transfer costs, \nwhich reflects scenarios where the resources are in physical proximity and connected by means of high-bandwidth, low-latency links. We formally define such computationally constrained problems in Definition~\\ref{def:compConstrained}. Let $\\hat{d}(G)$ be the streaming cost of $G$ when setting $\\ell(e) := 0$ for all edges.\n\n\\begin{definition}\\label{def:compConstrained}\nA streaming graph is \\emph{computationally constrained} if $d(G) \\in {\\mathcal{O}}(\\hat{d}(G))$ for any  mapping $r: V \\rightarrow \\mathcal{R}$.\n\\end{definition}\n\nIn this paper, we primarily study such problem instances and discuss implications for the general case along the way.\n\n\n\\subsection{Series-Parallel-Decomposable Graphs}\n\nRegarding streaming topologies, we focus our attention on directed \\emph{series-parallel-decomposable (SPD) graphs}, which are graphs that can be constructed by a combination of serial and parallel composition steps. \n\nA \\emph{serial composition} $G = G_1 {\\ensuremath{\\circ_s}} G_2$ of two SPD graphs $G_1$ and $G_2$ is defined as follows: The resulting graph $G$ consists of both graphs $G_1$ and $G_2$ where each sink node of $G_1$ is connected to each source node of $G_2$. More formally, $G=(V,E)$ is given by\n\n$V := V_1 \\cup V_2$ and \n $E :=  E_1 \\cup E_2 \\cup \\{(u,v) ~|~ u\\in V_{t,1} \\text{~and~} v\\in V_{s,2}\\}$,\n\nwhere $V_{s,i}$ and $V_{t,i}$ are the source and sink nodes of $G_i$, respectively.\nA \\emph{parallel composition} $G = G_1 {\\ensuremath{\\circ_p}} G_2$ of two SPD graphs $G_1$ and $G_2$ is a mere union of the two graphs without adding any edges, i.e.,\n$V:= V_1 \\cup V_2$ and $ E :=  E_1 \\cup E_2$.\nNote that all source nodes remain sources, and all sink nodes remain sinks.\nFormally, SPD graphs are defined as follows.\n\\begin{definition}[Series-Parallel-Decomposable]\\label{def:spdgraphs} \nA directed graph $G=(V,E)$ is series-parallel-decomposable (SPD) if $|V| = 1$ or there are SPD graphs $G_1$ and $G_2$ such that\n$G = G_1 {\\ensuremath{\\circ_s}} G_2$ or $G = G_1 {\\ensuremath{\\circ_p}} G_2$.\n\\end{definition}\n\n\n\nSince SPD graphs are defined recursively, they can be represented by a rooted series-parallel decomposition tree (\\emph{SPD tree}): The leaves of the SPD tree correspond to the nodes of the SPD graph. Each internal node of the tree represents a serial or parallel composition, i.e., a subtree $T$ with root $r$ corresponds to the graph that results from the composition of the graphs corresponding to the subtrees rooted at $r$'s child nodes. All internal nodes are labeled with $s$ or $p$ to indicate the type of the composition, i.e., serial or parallel. Given an SPD tree, the corresponding SPD graph can be constructed in linear time by traversing the tree in post-order, i.e., from the leaves to the root. \nAn example SPD graph is shown in Figure~\\ref{fig:series-parallel}, and its SPD tree is depicted in Figure~\\ref{fig:series-parallel-recursive}.\n\n\n\\begin{figure}[t]\n\\center\n \\includegraphics[width=.9\\columnwidth]{spd-example}\n \\caption{An example SPD graph consisting of 10 nodes.}\n \\label{fig:series-parallel}\n\\end{figure}\n\n\nIn the remainder, we will often describe streaming graphs by its compositional structure and use the SPD tree representation for recursive algorithms.\nIt is convenient to identify a component $G_i$ of an SPD graph by the corresponding node in the SPD tree representation, i.e., the root of the sub-tree $T(G_i)$ corresponding to $G_i$. For ease of notation, we describe an internal node $z$ of an SPD tree by the type of composition and its child nodes, i.e., $z=(op,\\{z_1,z_2\\})$ where  $op \\in \\{s,p\\}$ and $z_1, z_2$ are the child nodes. Moreover, we extend the SPD tree representation in that we allow nodes to have more than two children, which enables the concise representation of concatenations of serial or parallel compositions: a graph $G = G_1 {\\ensuremath{\\circ_s}} G_2 {\\ensuremath{\\circ_s}} \\ldots {\\ensuremath{\\circ_s}} G_k$ can be represented by an SPD tree $T(G)$ with a root node $z=(s,\\{z_1, \\ldots, z_k\\})$ that has $k$ children $\\{z_1,\\ldots, z_k\\}$, each of which is the root node of a subtree $T(G_i)$. Accordingly, $z=(p,\\{z_1, \\ldots, z_k\\})$ represents $k$ parallel components. Finally, let $\\mathcal{C}(z)$ denote the set of $z$'s  children. If $z$ is a leaf node, then $\\mathcal{C}(z) = \\emptyset$.\n\n\nNote that the class of SPD graphs does not coincide with the class of series-parallel graphs as defined by Takamizawa et al.~\\cite{takamizawa82}. While many graphs are both series-parallel and SPD, there are graphs that are only in one of the two classes.\nFor example, the graph with node set $V=\\{v_1,v_2,v_3\\}$ and edges $E=\\{ (v_1,v_2), (v_2,v_3), (v_1,v_3)\\}$ is series-parallel but not SPD. There are many SPD graphs that are non-planar, e.g., the class of SPD graphs contains all complete bipartite graphs $K_{m,n}$, which are non-planar if $m\\ge n \\ge 3$. In contrast, series-parallel graphs are planar by design.\n\n\\begin{figure}[t]\n\\center\n\\includegraphics[width=.9\\columnwidth]{spd-tree}\n \\caption{Tree representation $T(G)$ of the SPD graph~$G$ in Figure~\\ref{fig:series-parallel}.}\n \\label{fig:series-parallel-recursive}\n\\end{figure}\n\n\\subsection{General Bounds}\\label{sec:general_bounds}\n\nAllocating the tasks of a streaming graph to a set of resources in an optimal fashion is a hard problem in general.\n\n\\begin{theorem} The task allocation problem is NP-hard. \\label{thm:NPhardGeneral}\n\\end{theorem}\n\\begin{proof}\nThis result follows from a polynomial reduction from the NP-complete  {\\tt SUBSETSUM} problem, which, given a multiset $\\mathcal{S} := \\{s_1,\\ldots,s_n\\}$ of positive integers and an integer $x$, asks for a subset $\\mathcal{S}^*$ of $\\mathcal{S}$ such that the sum of the numbers in $\\mathcal{S}^*$ equals $x$. \n\nWe assume w.l.o.g.\\ that $x\\ge s_i$ for all $i$. Note that if $\\mathcal{S}$ contained any $s_i > x$, it could be immediately ruled out from the solution.\n\nGiven an instance $(\\mathcal{S},x)$ of {\\tt SUBSETSUM}, for any $k \\in \\{1,\\ldots n\\}$ we construct a task allocation problem instance $I^k=(G^k,w^k,b^k,\\mathcal{R}^k)$. The optimal mapping of $G^k$ to $n+k$ resources yields a subset $\\mathcal{S}(I^k) \\subseteq \\mathcal{S}$ as described below. The claim is that if $(\\mathcal{S},x)$ has a solution consisting of $k$ elements, then $\\mathcal{S}(I^k)$ is such a solution. Hence, solving  all $I^k$ for $k \\in \\{1,\\ldots n\\}$ either reveals a solution---a candidate solution can be checked in polynomial time---or answers the subset sum problem in the negative. \n\n\\begin{figure*}[t]\n\\center\n \\includegraphics[width=.8\\textwidth]{NP-hard_proof}\n \\caption{The task allocation instance constructed in the reduction from  {\\tt SUBSETSUM}. Here, $b=12nw+x$.}\n \\label{fig:np-proof}\n\\end{figure*}\n\n\nAn instance $I^k$ is constructed as follows (see Figure~\\ref{fig:np-proof}):\n$G^k$ consists of two paths $P:=u_1,v_1,u_2,v_2, \\ldots, u_n, v_n$ and $P' :=u'_1,v'_1,u'_2,v'_2, \\ldots, u'_n, v'_n$, and $2n$ additional nodes $\\nu_1,\\nu'_1, \\ldots,\\nu_n, \\nu'_n$. Each node $\\nu_i$ has incoming edges from $u_i$ and $u'_i$, and each node $\\nu'_i$ has incoming edges from $v_i$ and $v'_i$. \nEdge weights $b^k$ are given by $b^k(u_i,v_i) := s_i$, $b^k(u'_i,v'_i) := (2x/k) - s_i$, $b^k(v_i,u_{i+1})=b^k(v'_i,u'_{i+1}) := 0$, and $b^k(e):=12nw+x$ for all remaining edges $e$.\nNode weights $w^k$ are $0$ for all nodes $\\nu_i$, $\\nu'_i$, and $w:= \\sum_{i=1}^n s_i$ for all other nodes.\nFinally, $\\mathcal{R}^k$ consists of $n+k$ resources.\nNote that only $b^k$ and $\\mathcal{R}^k$ depend on $k$, whereas $G^k$ and $w^k$ remain the same.\n\nGiven a solution $r$ of $I^k$, the {\\tt SUBSETSUM}  solution candidate $\\mathcal{S}(I^k)$ is defined as the set of all $s_i=b(u_i,v_i)$  for which $r(u_i)\\neq r(v_i)$. \n\nIt remains to show that if there is a {\\tt SUBSETSUM} solution $\\mathcal{S}^*$ with $k$ elements, then the optimal allocation $r^*$ for $I^k$ yields a correct solution, i.e.,  $\\mathcal{S}(I^k)=\\mathcal{S}^*$.\n\nLet $F_i := \\{u_i,u'_i,\\nu_i\\}$ and $F'_i := \\{v_i,v'_i,\\nu'_i\\}$. Note that $V$ consists of $2n$ such node triplets, which we will refer to as \\emph{forks}.\nAssuming there is a solution with $k$ elements, then the claimed optimal allocation $r^*$ maps  forks $F_i$ and $F'_i$ to a separate resource each if $s_i \\in \\mathcal{S}^*$ and collocates all nodes in $F_i \\cup F'_i$ otherwise; thus, $2k$ resources host one fork, $n-k$ resources host two forks.  The resulting streaming cost $d^*$ is given by the maximum over the two paths along $P$ and $P'$, both ending in $\\nu'_n$, and amounts to\n\\begin{eqnarray*}\nd^*(P) & = & 2k\\cdot 3w + 2(n-k)\\cdot 6w + \\sum_{s_i \\in \\mathcal{S}^*} s_i , \\\\\nd^*(P') & = & 2k\\cdot 3w + 2(n-k)\\cdot 6w + \\sum_{s_i \\in \\mathcal{S}^*} \\left( \\frac{2x}{k} - s_i \\right)  , \\\\\nd^*(G^k) & = & \\max \\{d^*(P), d^*(P')\\} = (12n-6k) w + x ,\n\\end{eqnarray*}\n\n\n\n\n\nIn the following, we show that $r^*$ is indeed optimal.\n\n\n\nIn an optimal allocation, all nodes of a fork must be collocated, otherwise the cost $(12nw+x)$ of an edge to $\\nu_i$ or $\\nu'_i$ has to be paid on one path ending either in $\\nu_i$ or $\\nu'_i$. The cost on that path exceeds $d^*$, contradicting optimality.\n\n\nFor a large enough $w$, a most even distribution of the forks onto resources is optimal since the processing cost of a set of collocated forks grows quadratically with the size of the set: the processing cost on path $P$ and $P'$ for $\\kappa$ collocated forks is $\\kappa^2\\cdot 3w$. Hence, $2k$ resources must contain exactly one fork, and $n-k$ resources must contain exactly two forks in an optimal allocation. The chosen $w= \\sum_{i=1}^n s_i$ is large enough so that any deviating allocation costs more than $r^*$.\n\nConsequently, at most $n-k$ edges $(u_i,v_i)$ with weights $s_i$ can be \\emph{covered}, i.e., $r(u_i)=r(v_i)$, and $2k$ edges $(u_i,v_i)$ on $P$ must be left \\emph{uncovered} ($r(u_i) \\neq r(v_i)$). Note that if edge $(u_i,v_i)$ is covered on $P$, then $(u'_i,v'_i)$ is covered on $P'$ as well. It is optimal to leave all $k$ edges with weight $0$ and a set $S$ of $k$ edges with $s_i$ weights uncovered on $P$, and a corresponding set $S'$ on $P'$.\n\nIf $\\sum_{e \\in S} b(e) > x$, then the streaming cost of $P$ exceeds $d^*$. \nOn the other hand, if $\\sum_{e \\in S} b(e) < x$, then \n\n", "index": 1, "text": "$$\\sum_{e \\in S'} b(e) = \\sum_{e \\in S'} \\frac{2x}{k} - \\sum_{e \\in S} b(e)  = 2x - \\sum_{e \\in S} b(e) > x,$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\sum_{e\\in S^{\\prime}}b(e)=\\sum_{e\\in S^{\\prime}}\\frac{2x}{k}-\\sum_{e\\in S}b(e%&#10;)=2x-\\sum_{e\\in S}b(e)&gt;x,\" display=\"block\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><msup><mi>S</mi><mo>\u2032</mo></msup></mrow></munder><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><msup><mi>S</mi><mo>\u2032</mo></msup></mrow></munder><mfrac><mrow><mn>2</mn><mo>\u2062</mo><mi>x</mi></mrow><mi>k</mi></mfrac></mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>x</mi></mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>&gt;</mo><mi>x</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": "\ndue to the fact that there are $n(r(v))=n(R)$ tasks assigned to resource $R$ by definition. As the sum of shares is $c$, it is a valid solution for Problem~\\ref{def:continuous_problem}. Thus, the optimal continuous cost $\\delta(G)$ can only be lower or equal.\n\\end{proof}\n\n\nThe two problems are indeed strongly related. If a resource is shared among $k$ tasks, the processing cost in the discrete model is $d(v) = k\\cdot w(v)$ for each such task $v$. In other words, each task gets a \\emph{share} of $1/k$ of the resource, which in the continuous model corresponds to a processing cost of $\\delta(v) = w(v) /x(v) = k\\cdot w(v)$, i.e., a share $x(v)$ can be interpreted as the fraction of a resource dedicated to $v$.\nOf course, the continuous model does not truly have a notion of a ``resource'' as the capacity $c$ can be split up arbitrarily. Moreover, it is admissible to assign a share greater than 1 to a task in the continuous model, which would mean that a task is assigned to more than one resource.\nNevertheless, the relation between the problems can be exploited.\nFirst, we formulate and analyze an algorithm, $\\mathcal{A}^{cont}$, which solves Problem~\\ref{def:continuous_problem} for SPD graphs, then we present  an algorithm $\\mathcal{A}^{disc}$ for the discrete case, which uses $\\mathcal{A}^{cont}$ as a subroutine.\n\nAlgorithm  $\\mathcal{A}^{cont}$ takes $z_0$, i.e., the root of the SPD tree corresponding to graph $G$, and $c$ as input parameters and computes the optimal mapping $x: V \\rightarrow \\mathbb{R}^+$. To this end, it first calls procedure \\textit{computeWeights} with the parameter $z_0$, which computes weights for each node in the tree. The weight of a node $z$ corresponds to the optimal streaming cost of the subtree rooted at $z$.\nNext, procedure \\textit{computeMapping} is called with parameters $z_0$ and $c$, which derives the optimal mapping $x$ based on the weights computed in the previous step.\n\nProcedure \\textit{computeWeights} (see Algorithm~\\ref{algo:computeWeights}) recursively computes the weights of all children of a node $z$. The weight $w(z)$ of a leaf $z$ equals the weight $w(v)$ of the corresponding graph node $v\\in V$.\n\nThe weight of an internal node $z$ is computed from the weights of its children:   $w(z)$ is set to $(\\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)})^2$ if $op = s$ and $\\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$ if $op = p$.\n\n\\begin{algorithm}[t]\n\\small\n   \\caption{computeWeights($z$)}\n   \t\\label{algo:computeWeights}\n\t\\begin{algorithmic}[1]\n\t\n\t  \\FOR{$z_i \\in \\mathcal{C}(z)$}\n            \\STATE computeWeights($z_i$)\n            \\IF{$z = (s,\\cdot)$}\n\t      \\STATE $w(z) := \\left(\\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)}\\right)^2$\n            \\ELSE\n              \\STATE $w(z) := \\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$\n            \\ENDIF\n          \\ENDFOR\n        \n     \\end{algorithmic}\n  \\end{algorithm}\n\n \nProcedure \\textit{computeMapping} (see Algorithm~\\ref{algo:computeMapping}) traverses the SPD tree in a top-down fashion and maps the capacity to nodes. \nIt recursively computes the partitioning of the given capacity, which is $c$ at the root $z_0$, among all children. As in procedure \\textit{computeMapping}, the partitioning is different for serial and parallel compositions. Each child gets a share relative to its contribution to the sum of weights for parallel compositions, whereas the relative contribution with respect to the square roots of the weights is used for serial compositions.  When the recursion arrives at a leaf $z$ with the call  \\textit{computeMapping}$(z,x)$ it receives the share $x$, which implies that $x(v) = x$ for the corresponding graph node.\n\nIn order to prove that the computed mapping is optimal, we must show that the mapping rules lead to an optimal solution, under the assumption that the computed weight of each child is the optimal streaming cost of the corresponding subtree.\n\n\\begin{lemma}\\label{lemma:continuous}\nLet $z = (op,\\{z_1,\\ldots,z_k\\})$ and $w(z_i)$ be the optimal streaming cost of $z_i$. For all $i \\in \\{1,\\ldots,k\\}$, the capacity $c$ is partitioned optimally by setting\n\\begin{numcases}{ x(z_i) :=}\n\\frac{c\\cdot\\sqrt{w(z_i)}}{\\sum_{j=1}^k \\sqrt{w(z_j)}} & if $op=s$\\label{eq:serial}\\\\\n\\frac{c\\cdot w(z_i)}{\\sum_{j=1}^k w(z_j)}  & if $op=p$.\\label{eq:parallel}\n\\end{numcases}\n\\end{lemma}\n\\begin{proof}\n Let $x_i := x(z_i)$. The multivariate function that describes the streaming cost of $z$ is\n\\begin{displaymath}\nw_s(x_1,\\ldots,x_{k-1}) := \\left(\\sum_{i=1}^{k-1} \\frac{w(z_i)}{x_i}\\right) + \\frac{w(z_k)}{c-\\sum_{j=1}^{k-1} x_j}.\n\\end{displaymath}\nThe minimum of this function is attained when $\\frac{\\partial w_s}{\\partial x_i} = 0$ for all $i = 1,\\ldots,k-1$, which implies that\n\\begin{displaymath}\n\\forall i \\in \\{1,\\ldots,k-1\\}: \\frac{w(z_i)}{x_i^2} = \\frac{w(z_k)}{\\left(c-\\sum_{j=1}^{k-1} x_j\\right)^2}.\n\\end{displaymath}\nSolving this equation for $x_i$ yields $x_i = \\frac{c\\cdot\\sqrt{w(z_i)}}{\\sum_{j=1}^k \\sqrt{w(z_j)}}$ for all $i\\in \\{1,\\ldots,k\\}$ as claimed.\n\nThe same pattern can be used to derive the optimal partitioning for parallel compositions.\nAs before, let $x_i := x(z_i)$, and the multivariate function for the streaming cost of $z$ is\n\\begin{displaymath}\nw_p(x_1,\\ldots,x_k) := \\max\\left(\\frac{w(z_1)}{x_1},\\ldots,\\frac{w(z_k)}{c - \\sum_{i=1}^{k-1} x_i}\\right).\n\\end{displaymath}\nThis function is minimized if each term is equal, which is the case if $c \\cdot w(z_i)/x_i = \\sum_{j=1}^k w(z_j)$ as claimed.\n\\end{proof}\n\n\n\n \\begin{algorithm}[t]\n\\small\n\\caption{computeMapping($z,c$)}\n   \\label{algo:computeMapping}\n   \\begin{center}\n    \\begin{algorithmic}[1]\n\t\\IF{$z$ is leaf}\n            \\STATE $x(z) := c$\n        \\ELSE\n\t \\FOR{$z_i \\in \\mathcal{C}(z)$}\n            \\IF{$z = (s,\\cdot)$}\n              \\STATE $W := \\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)}$\n\t      \\STATE computeMapping($z_i,c \\cdot \\sqrt{w(z_i)}/W$)\n            \\ELSE\n              \\STATE $W := \\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$\n              \\STATE computeMapping($z_i,c \\cdot w(z_i)/W$)\n            \\ENDIF\n          \\ENDFOR\n        \\ENDIF\n\n\\vspace{-5pt}\n     \\end{algorithmic}\n   \\end{center}\n \\end{algorithm}\n\n\nAn important observation is that the optimal partitioning scales linearly with $c$ in both cases, i.e., the relative distribution among the constituent parts is independent of $c$.\n\n\\begin{fact}\\label{fact:linear}\nThe optimal partitioning for both serial and parallel compositions scales linearly with the capacity $c$.\n\\end{fact}\n\nThis fact is important as it implies that the optimal solution can be built recursively as long as the optimal costs of all subtrees are known, which is exactly what Algorithm $\\mathcal{A}^{cont}$ does. The following theorem states the main result.\n\n\\begin{theorem}\nAlgorithm $\\mathcal{A}^{cont}$ computes an optimal mapping \\mbox{$x: V \\rightarrow \\mathbb{R}^+$} for any SPD graph $G$ and capacity $c > 0$.\n\\end{theorem}\n\\begin{proof}\nLemma~\\ref{lemma:continuous} and Fact~\\ref{fact:linear} show that procedure \\textit{computeMapping} optimally partitions the capacity in a recursive manner under the assumption that all weights correspond to the minimal streaming cost of the respective subtree. It remains to prove that procedure \\textit{computeWeights} indeed computes the optimal weights.\n\nWe can use an inductive argument to prove this. As the capacity $c$ merely scales the optimal solution linearly, we can ignore it when computing the weights by setting $c := 1$.\nConsider the base case of a serial or parallel composition of leaves, i.e., real nodes in $V$. Let $z$ denote the root of the SPD tree of this subgraph. If it is a serial composition, Lemma~\\ref{lemma:continuous} reveals that the optimal streaming cost is \n\n", "itemtype": "equation", "pos": 32883, "prevtext": "\nand $d(P') > d^*$. Hence, $S$ must be chosen so that $\\sum_{e \\in S} b(e)=x$. Allocation $r^*$ is optimal and $\\mathcal{S}(I^k)=\\mathcal{S}^*$.\n\n\\flushright\\vspace{-3mm}\\qedhere\n\\end{proof}\n\n\nThe proof of Theorem~\\ref{thm:NPhardGeneral} shows NP-hardness with a streaming graph that is not quite an SPD graph due to the edges ending at nodes $\\nu_i$.\nMoreover, it achieves the reduction from the subset sum problem using a construction that relies on variable transfer and task weights.\nSince many hard problems can be solved efficiently on series-parallel graphs~\\cite{takamizawa82} an interesting question is whether optimal task allocation is efficiently solvable on SPD graphs. The following theorem answers this question in the negative, even for computationally constrained problems and even if all transfer weights and all task weights are constant. The claim follows from a reduction from the decision version of the  NP-complete  \\emph{partition problem}. \n\n\n\\begin{theorem} \\label{thm:NPharduniform}\nThe task allocation problem on SPD graphs in concise format is NP-hard, even if transfer and task weights are uniform.\n\\end{theorem}\n\\begin{proof}\nThe claim follows from a reduction from the decision version of the  NP-complete  \\emph{partition problem}, which, given a multiset $\\mathcal{S} := \\{s_1,\\ldots,s_n\\}$ of positive integers, asks for a partitioning of $\\mathcal{S}$ into two subsets $\\mathcal{S}_1$ and $\\mathcal{S}_2$ such that the sum of the numbers in $\\mathcal{S}_1$ is equal to the sum of the numbers in $\\mathcal{S}_2$. Given an instance $\\mathcal{S}=\\{s_1,\\ldots,s_k\\}$ of the partitioning problem, we construct a task allocation problem instance as follows: for each $s_i$, we construct a streaming graph component $G_i$ consisting of a serial composition of two sets of $s_i$ parallel nodes, i.e., $G_i :=G_{i,l} {\\ensuremath{\\circ_s}} G_{i,r}$ where $G_{i,l} =G_{i,r}=(V,E), |V|=s_i$ and $E=\\emptyset$. Edge weights and node weights have positive constant values, i.e., $w(v)=\\alpha$, $b(e)=\\beta$ for all $v\\in V$, $e\\in E$. Let $\\alpha=\\beta:=1$ for simplicity. \nLet $d$ be the output of the algorithm that computes the streaming cost of the graph $G:=G_1 {\\ensuremath{\\circ_p}} G_2 {\\ensuremath{\\circ_p}} \\ldots {\\ensuremath{\\circ_p}} G_k$ given that $\\mathcal{R}=\\{R_1,R_2\\}$. If $d=n:=2\\cdot \\sum_{i=1}^k s_i$, we answer the partitioning problem positively and negatively otherwise.\nIt remains to show that $d(G)=n$ if and only if there exists a perfect partitioning of $\\mathcal{S}$. If a perfect partitioning exists then the following mapping provides an optimal task allocation:\n\\begin{displaymath}\n r(v) := \\begin{cases} R_1 &\\mbox{if } v\\in G_i \\mbox{ and } \\sigma(s_i)=S_1 \\\\ \nR_2 & \\mbox{otherwise,}\n\\end{cases}\n\\end{displaymath}\nwhere $\\sigma$ is an optimal partitioning of $\\mathcal{S}$ into two sets $S_1,S_2$. Since $\\sigma$ is perfect it holds that $|R_1|=|R_2|=n/2$. Each path in $G$ entails processing cost of $2\\cdot n/2$ and zero transfer cost. For any other mapping $r'$ with $n(R_1)=n(R_2)=n/2$ there is at least one path $(u,v)$ with $r'(u)\\neq r'(v)$ yielding streaming cost $d(G)>n$. For any mapping $r'$ with $n(R_1)\\neq n(R_2)$, let  $n(R_1)> n(R_2)$ w.l.o.g., then there exists a component $G_i$ with more nodes mapped to $R_1$ than $R_2$. Hence, there exists a path $(u,v)\\in G_i$ with $r'(u)=r'(v)=R_1$ yielding streaming cost $d(G)=2\\cdot |R_1|>n$, which contradicts optimality.\n If no perfect partitioning exists then any mapping $r$ with $n(R_1)=n(R_2)=n/2$ implies that there is a component $G_i$ with nodes mapped to $R_1$ and $R_2$. Hence, there exists a path in $G_i$ with cost $2\\cdot n/2 + 1$ and $d(G)>n$. If the optimal mapping $r$ chooses $n(R_1)\\neq n(R_2)$ then the same argument holds as in the case where a perfect partitioning exists: if  $n(R_1)> n(R_2)$, then there is a $P=\\{(u,v)\\}$ with $r(u)=r(v)=R_1$, which entails that $d(G)>n$.\n\\end{proof}\n\nNote that the reduction of the proof of Theorem~\\ref{thm:NPharduniform} uses a streaming graph where the number of nodes is proportional to the sum of the numbers $s_i$ of the partitioning problem. Since partition problem instances are only NP-hard if they contain $s_i \\in \\mathcal{S}$ that are exponentially large in $|\\mathcal{S}|$~\\cite{mertens06}, the used streaming graph contains a number of nodes that is exponential in the bit representation of the partitioning instance. \nTherefore, Theorem~\\ref{thm:NPharduniform} proves NP-hardness only for concise representations of task allocation problem instances. For example, each $G_i$ component of the used graph can be described in polynomial space similarly as in the proof. While we leave the question of hardness for non-concise SPD graph representations open, it is possible to adapt the approximation algorithm presented in Section~\\ref{sec:algorithm} so as to handle concise instances of the graphs used in the proof of Theorem~\\ref{thm:NPharduniform}.\n\nNote that the edge and task weight constants, $\\alpha$ and $\\beta$, in the proof of Theorem~\\ref{thm:NPharduniform} can be set independently to any positive value. As such, the proof  holds for computationally constrained graphs as well as non-constrained graphs.\n\nWhile the task allocation problem is NP-hard, the simple algorithm that assigns all tasks to one resource, i.e., $r(u):=R_1$ for all $u \\in V$, achieves an $n$-approximation: Obviously, we have that $\\ell(e) = 0$ for all $e\\in E$, which implies that $d(P) = \\sum_{v \\in P} d(v) = n\\sum_{v \\in P} w(v)$ for all $P \\in \\mathcal{P}$. Therefore, we get that $d(G) = n \\sum_{v \\in P'} w(v)$ where $P'$ is the path with the largest sum of node weights.\nSince for \\emph{any} allocation, $d(G) \\ge d(P')$ and the minimum cost of path $P'$ is\n\\begin{displaymath}\nd(P') =  \\sum_{v \\in P'} d(v) + \\sum_{e \\in P'} \\ell(e) \\ge \\sum_{v \\in P'} d(v) \\ge \\sum_{v \\in P'} w(v),\n\\end{displaymath}\nthe claimed bound follows.\nThis straightforward solution is optimal if $c=1$ or if edge weights are exceedingly large; in particular if $b(e) \\ge k n D w_{max}$ for each edge $e\\in E$, where $D$ is the graph diameter, $w_{max} := \\max_{v \\in V} w(v)$, and $k$ is a constant $\\ge 1$. Collocating all nodes on one resource is optimal since the streaming cost of any path is upper bounded by $nDw_{max}$.\n\nThe case of large edge weights can be considered the opposite of computationally constrained graphs, since the transfer costs dominate the solution rather than the processing costs. Imposing a specific \\emph{upper bound} on the edge weights, on the other hand, results in a computationally constrained graph:\n\n\\begin{lemma}\\label{lma:compConstrained}\nIf $b(e) \\le k w_{min}\\lceil D/c\\rceil$ for each $e\\in E$, where $w_{min} := \\min_{v \\in V} w(v)$, $D$ is the diameter of $G$,  and $k$ is a positive constant, then $G$ is computationally constrained.\n\\end{lemma}\n\\begin{proof}\n Due to the bound on $b(e)$, the maximum transfer cost along any path is $\\max_{P \\in \\mathcal{P}(G)} \\sum_{e \\in P} b(e) \\le D k w_{min}\\lceil D/c\\rceil$.\nWe will now show that\n$\\hat{d}(G) \\in \\Omega\\left(w_{min}\\left(D+D^2/c\\right)\\right)$\nfor any $G$, $c$, and mapping $r$, which implies that $d(G) \\in O(\\hat{d}(G))$.\nThe lower bound of $\\Omega(w_{min}D)$ is trivial since $d(v) \\ge w_{min}$ for all $v \\in V$, resulting in a total processing cost of at least $w_{min}D$ on each path of length $D$.\nAssume for the sake of argument that there is a path $P$ of length $D$ where $w(v) = w_{min}$ for each $v\\in P$. The streaming cost of $P$ is at least $\\lceil w_{min}D^2/c\\rceil$ even when excluding transfer cost and \\emph{all} $c$ resources are used up exclusively for $P$. Naturally, the streaming cost can only increase when there are nodes with larger weights, the resources are shared with other nodes, or the nodes on $P$ are mapped to fewer than $c$ resources. Thus, $\\Omega(w_{min}D^2/c)$ is a lower bound on $d(G)$ as claimed.\n\\end{proof}\n\n\nThe lemma shows that the edge weights may be considerably larger than the node weights, in the order of $D/c$, without affecting the asymptotic streaming cost. More generally, if $\\sum_{e \\in P} b(e) \\in O(\\hat{d}(G))$ for all $P \\in \\mathcal{P}(G)$ for any mapping $r: V \\rightarrow \\mathcal{R}$, then $G$ is computationally constrained as well.\n\n\n\\section{Algorithm}\n\\label{sec:algorithm}\nBefore describing the main algorithm, we start with a straightforward algorithm to illustrate the difficulty of the task allocation problem.\nThe algorithm $\\mathcal{A}^{avg}$ strives to distribute the work equally among the resources. Specifically, it partitions the nodes into $c$ groups such that the sum of node weights is as balanced as possible. More formally, it minimizes\n\\begin{displaymath}\n\\max_{R\\in \\mathcal{R}} \\sum_{v \\in r^{-1}(R)} w(v) -\\min_{R\\in \\mathcal{R}} \\sum_{v \\in r^{-1}(R)} w(v),\n\\end{displaymath}\nwhere $r^{-1}: \\mathcal{R} \\rightarrow 2^V$ determines the set of nodes mapped to a particular resource $R \\in \\mathcal{R}$.\\footnote{Note that finding such a partitioning is NP-hard by itself.}  \nWhile this approach seems reasonable, there are instances where $\\mathcal{A}^{avg}$ fails to achieve a better approximation ratio than the trivial algorithm that only uses one resource. We can take the graph consisting of $n$ parallel nodes, where $w(v_1) := n/3$ and $w(v_i) := 1$ for all $i\\in \\{2,\\ldots,n\\}$, as an example and set $c:=2$. As the sum of weights is $\\frac{4}{3}n-1$, algorithm $\\mathcal{A}^{avg}$ attempts to assign $\\frac{2}{3}n-\\frac{1}{2}$ work to each resource. Without loss of generality, let $r(v_1) = R_1$. Since the weight of all other nodes is $1$, we get that $n(R_1) \\ge \\frac{1}{3}n-1$. Hence it follows that $d(v_1) \\ge \\frac{n^2}{9}-\\frac{n}{3} \\in \\Omega(n^2)$, implying that $d(G) \\in \\Omega(n^2)$. The optimal solution, however, dedicates one resource completely to $v_1$, which entails that $d(v_1) = n/3$ and $d(v_i) = n-1$ for all $i\\in \\{2,\\ldots,n\\}$. The streaming cost is therefore only $d(G) \\in {\\mathcal{O}}(n)$.\n\nInstead of tackling the task allocation problem directly, we will now take a detour and present an algorithm for a continuous version of the problem, which our main algorithm will leverage.\n\n\\subsection{Continuous Algorithm}\n\nThe continuous version of Problem~\\ref{def:general_problem} is defined as follows. There is a \\emph{capacity} $c > 0$ that must be assigned to the tasks, i.e., each task $v$ gets a \\emph{share} $x(v) \\in \\mathbb{R}^+$ of the capacity subject to $\\sum_{v \\in V} x(v) \\le c$. Given a task's weight and share, the \\emph{continuous processing cost} $\\delta(v)$ is defined as\n\n$\\delta(v) := w(v) / x(v)$. \n\nThere are no transfer costs in this model, and hence the streaming cost of a path $P$ is $\\delta(P) := \\sum_{v\\in P} \\delta(v)$. As in the discrete model, the streaming cost of $G$ in the continuous model is the maximum streaming cost over all paths, i.e.,\n\n$\\delta(G) := \\max_{P\\in \\mathcal{P}(G)} \\delta(P)$.\n\n\nThe goal is to assign shares in a way that minimizes the streaming cost.\n\n\n\\begin{problem}\\label{def:continuous_problem} Given a weighted directed acyclic graph $G$ and a capacity $c>0$, find a mapping $x: V \\rightarrow \\mathbb{R}^+$ that minimizes $\\delta(G)$.\n\\end{problem}\n\nObviously, it must hold that $\\sum_{v \\in V} x(v) = c$ in an optimal allocation, i.e., the entire capacity is assigned. The motivation for studying this problem is that the optimal solution of Problem~\\ref{def:continuous_problem} is a lower bound on the streaming cost in the discrete model.\n\n\\begin{theorem} For all graphs $G$ and $c \\in \\mathbb{N}$ it holds that $\\delta(G) \\le d(G)$, where $c$ is the capacity in the continuous case and the number of resources in the discrete case.\n\\end{theorem}\n\n\\begin{proof}\nConsider the optimal solution $r$ of Problem~\\ref{def:general_problem}. Set $x(v) := 1/n(r(v))$ for all $v\\in V$. It holds that\n\n", "index": 3, "text": "$$\\sum_{v\\in V} \\frac{1}{n(r(v))} = \\sum_{R \\in \\mathcal{R}}\\hspace{2pt} \\sum_{v\\in V: r(v)=R} \\frac{1}{n(r(v))}= \\sum_{R \\in \\mathcal{R}} \\frac{n(R)}{n(R)}= c$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\sum_{v\\in V}\\frac{1}{n(r(v))}=\\sum_{R\\in\\mathcal{R}}\\hskip 2.0pt\\sum_{v\\in V:%&#10;r(v)=R}\\frac{1}{n(r(v))}=\\sum_{R\\in\\mathcal{R}}\\frac{n(R)}{n(R)}=c\" display=\"block\"><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mi>V</mi></mrow></munder><mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mrow><mpadded width=\"+2pt\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>R</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u211b</mi></mrow></munder></mpadded><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>v</mi><mo>\u2208</mo><mi>V</mi></mrow><mo>:</mo><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>R</mi></mrow></mrow></munder><mfrac><mn>1</mn><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>R</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u211b</mi></mrow></munder><mfrac><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>n</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mi>c</mi></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": "\n\nSimilarly, we can use Lemma~\\ref{lemma:continuous} to get the optimal streaming cost for a parallel composition, which is \n\n", "itemtype": "equation", "pos": 40758, "prevtext": "\ndue to the fact that there are $n(r(v))=n(R)$ tasks assigned to resource $R$ by definition. As the sum of shares is $c$, it is a valid solution for Problem~\\ref{def:continuous_problem}. Thus, the optimal continuous cost $\\delta(G)$ can only be lower or equal.\n\\end{proof}\n\n\nThe two problems are indeed strongly related. If a resource is shared among $k$ tasks, the processing cost in the discrete model is $d(v) = k\\cdot w(v)$ for each such task $v$. In other words, each task gets a \\emph{share} of $1/k$ of the resource, which in the continuous model corresponds to a processing cost of $\\delta(v) = w(v) /x(v) = k\\cdot w(v)$, i.e., a share $x(v)$ can be interpreted as the fraction of a resource dedicated to $v$.\nOf course, the continuous model does not truly have a notion of a ``resource'' as the capacity $c$ can be split up arbitrarily. Moreover, it is admissible to assign a share greater than 1 to a task in the continuous model, which would mean that a task is assigned to more than one resource.\nNevertheless, the relation between the problems can be exploited.\nFirst, we formulate and analyze an algorithm, $\\mathcal{A}^{cont}$, which solves Problem~\\ref{def:continuous_problem} for SPD graphs, then we present  an algorithm $\\mathcal{A}^{disc}$ for the discrete case, which uses $\\mathcal{A}^{cont}$ as a subroutine.\n\nAlgorithm  $\\mathcal{A}^{cont}$ takes $z_0$, i.e., the root of the SPD tree corresponding to graph $G$, and $c$ as input parameters and computes the optimal mapping $x: V \\rightarrow \\mathbb{R}^+$. To this end, it first calls procedure \\textit{computeWeights} with the parameter $z_0$, which computes weights for each node in the tree. The weight of a node $z$ corresponds to the optimal streaming cost of the subtree rooted at $z$.\nNext, procedure \\textit{computeMapping} is called with parameters $z_0$ and $c$, which derives the optimal mapping $x$ based on the weights computed in the previous step.\n\nProcedure \\textit{computeWeights} (see Algorithm~\\ref{algo:computeWeights}) recursively computes the weights of all children of a node $z$. The weight $w(z)$ of a leaf $z$ equals the weight $w(v)$ of the corresponding graph node $v\\in V$.\n\nThe weight of an internal node $z$ is computed from the weights of its children:   $w(z)$ is set to $(\\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)})^2$ if $op = s$ and $\\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$ if $op = p$.\n\n\\begin{algorithm}[t]\n\\small\n   \\caption{computeWeights($z$)}\n   \t\\label{algo:computeWeights}\n\t\\begin{algorithmic}[1]\n\t\n\t  \\FOR{$z_i \\in \\mathcal{C}(z)$}\n            \\STATE computeWeights($z_i$)\n            \\IF{$z = (s,\\cdot)$}\n\t      \\STATE $w(z) := \\left(\\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)}\\right)^2$\n            \\ELSE\n              \\STATE $w(z) := \\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$\n            \\ENDIF\n          \\ENDFOR\n        \n     \\end{algorithmic}\n  \\end{algorithm}\n\n \nProcedure \\textit{computeMapping} (see Algorithm~\\ref{algo:computeMapping}) traverses the SPD tree in a top-down fashion and maps the capacity to nodes. \nIt recursively computes the partitioning of the given capacity, which is $c$ at the root $z_0$, among all children. As in procedure \\textit{computeMapping}, the partitioning is different for serial and parallel compositions. Each child gets a share relative to its contribution to the sum of weights for parallel compositions, whereas the relative contribution with respect to the square roots of the weights is used for serial compositions.  When the recursion arrives at a leaf $z$ with the call  \\textit{computeMapping}$(z,x)$ it receives the share $x$, which implies that $x(v) = x$ for the corresponding graph node.\n\nIn order to prove that the computed mapping is optimal, we must show that the mapping rules lead to an optimal solution, under the assumption that the computed weight of each child is the optimal streaming cost of the corresponding subtree.\n\n\\begin{lemma}\\label{lemma:continuous}\nLet $z = (op,\\{z_1,\\ldots,z_k\\})$ and $w(z_i)$ be the optimal streaming cost of $z_i$. For all $i \\in \\{1,\\ldots,k\\}$, the capacity $c$ is partitioned optimally by setting\n\\begin{numcases}{ x(z_i) :=}\n\\frac{c\\cdot\\sqrt{w(z_i)}}{\\sum_{j=1}^k \\sqrt{w(z_j)}} & if $op=s$\\label{eq:serial}\\\\\n\\frac{c\\cdot w(z_i)}{\\sum_{j=1}^k w(z_j)}  & if $op=p$.\\label{eq:parallel}\n\\end{numcases}\n\\end{lemma}\n\\begin{proof}\n Let $x_i := x(z_i)$. The multivariate function that describes the streaming cost of $z$ is\n\\begin{displaymath}\nw_s(x_1,\\ldots,x_{k-1}) := \\left(\\sum_{i=1}^{k-1} \\frac{w(z_i)}{x_i}\\right) + \\frac{w(z_k)}{c-\\sum_{j=1}^{k-1} x_j}.\n\\end{displaymath}\nThe minimum of this function is attained when $\\frac{\\partial w_s}{\\partial x_i} = 0$ for all $i = 1,\\ldots,k-1$, which implies that\n\\begin{displaymath}\n\\forall i \\in \\{1,\\ldots,k-1\\}: \\frac{w(z_i)}{x_i^2} = \\frac{w(z_k)}{\\left(c-\\sum_{j=1}^{k-1} x_j\\right)^2}.\n\\end{displaymath}\nSolving this equation for $x_i$ yields $x_i = \\frac{c\\cdot\\sqrt{w(z_i)}}{\\sum_{j=1}^k \\sqrt{w(z_j)}}$ for all $i\\in \\{1,\\ldots,k\\}$ as claimed.\n\nThe same pattern can be used to derive the optimal partitioning for parallel compositions.\nAs before, let $x_i := x(z_i)$, and the multivariate function for the streaming cost of $z$ is\n\\begin{displaymath}\nw_p(x_1,\\ldots,x_k) := \\max\\left(\\frac{w(z_1)}{x_1},\\ldots,\\frac{w(z_k)}{c - \\sum_{i=1}^{k-1} x_i}\\right).\n\\end{displaymath}\nThis function is minimized if each term is equal, which is the case if $c \\cdot w(z_i)/x_i = \\sum_{j=1}^k w(z_j)$ as claimed.\n\\end{proof}\n\n\n\n \\begin{algorithm}[t]\n\\small\n\\caption{computeMapping($z,c$)}\n   \\label{algo:computeMapping}\n   \\begin{center}\n    \\begin{algorithmic}[1]\n\t\\IF{$z$ is leaf}\n            \\STATE $x(z) := c$\n        \\ELSE\n\t \\FOR{$z_i \\in \\mathcal{C}(z)$}\n            \\IF{$z = (s,\\cdot)$}\n              \\STATE $W := \\sum_{z_i \\in \\mathcal{C}(z)} \\sqrt{w(z_i)}$\n\t      \\STATE computeMapping($z_i,c \\cdot \\sqrt{w(z_i)}/W$)\n            \\ELSE\n              \\STATE $W := \\sum_{z_i \\in \\mathcal{C}(z)} w(z_i)$\n              \\STATE computeMapping($z_i,c \\cdot w(z_i)/W$)\n            \\ENDIF\n          \\ENDFOR\n        \\ENDIF\n\n\\vspace{-5pt}\n     \\end{algorithmic}\n   \\end{center}\n \\end{algorithm}\n\n\nAn important observation is that the optimal partitioning scales linearly with $c$ in both cases, i.e., the relative distribution among the constituent parts is independent of $c$.\n\n\\begin{fact}\\label{fact:linear}\nThe optimal partitioning for both serial and parallel compositions scales linearly with the capacity $c$.\n\\end{fact}\n\nThis fact is important as it implies that the optimal solution can be built recursively as long as the optimal costs of all subtrees are known, which is exactly what Algorithm $\\mathcal{A}^{cont}$ does. The following theorem states the main result.\n\n\\begin{theorem}\nAlgorithm $\\mathcal{A}^{cont}$ computes an optimal mapping \\mbox{$x: V \\rightarrow \\mathbb{R}^+$} for any SPD graph $G$ and capacity $c > 0$.\n\\end{theorem}\n\\begin{proof}\nLemma~\\ref{lemma:continuous} and Fact~\\ref{fact:linear} show that procedure \\textit{computeMapping} optimally partitions the capacity in a recursive manner under the assumption that all weights correspond to the minimal streaming cost of the respective subtree. It remains to prove that procedure \\textit{computeWeights} indeed computes the optimal weights.\n\nWe can use an inductive argument to prove this. As the capacity $c$ merely scales the optimal solution linearly, we can ignore it when computing the weights by setting $c := 1$.\nConsider the base case of a serial or parallel composition of leaves, i.e., real nodes in $V$. Let $z$ denote the root of the SPD tree of this subgraph. If it is a serial composition, Lemma~\\ref{lemma:continuous} reveals that the optimal streaming cost is \n\n", "index": 5, "text": "\\begin{align*}\nw(z) &= \\sum_{z_i\\in \\mathcal{C}(z)} \\frac{w(z_i)}{x(z_i)}    \\stackrel{\\eqref{eq:serial}}{=} \\sum_{z_i\\in \\mathcal{C}(z)} \\frac{w(z_i)}{\\sqrt{w(z_i)}}\\left(\\sum_{z_j\\in \\mathcal{C}(z)} \\sqrt{w(z_j)}\\right) \\\\\n &= \\left(\\sum_{z_i\\in \\mathcal{C}(z)} \\sqrt{w(z_i)}\\right)^2.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w(z)\" display=\"inline\"><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{z_{i}\\in\\mathcal{C}(z)}\\frac{w(z_{i})}{x(z_{i})}\\stackrel{%&#10;\\eqref{eq:serial}}{=}\\sum_{z_{i}\\in\\mathcal{C}(z)}\\frac{w(z_{i})}{\\sqrt{w(z_{i%&#10;})}}\\left(\\sum_{z_{j}\\in\\mathcal{C}(z)}\\sqrt{w(z_{j})}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mover><mo movablelimits=\"false\">=</mo><mrow><mi mathsize=\"142%\">(</mi><mo>\u2062</mo><mtext class=\"ltx_missing_label\" mathsize=\"142%\" mathvariant=\"italic\"><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_missing_label ltx_font_italic ltx_ref_self\">LABEL:eq:serial</span></mtext><mo>\u2062</mo><mi mathsize=\"142%\">)</mi></mrow></mover><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><msqrt><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><msqrt><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\sum_{z_{i}\\in\\mathcal{C}(z)}\\sqrt{w(z_{i})}\\right)^{2}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><msqrt><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": "\n\nThus, procedure \\textit{computeWeights} computes the optimal cost, i.e., weight, in both cases and by induction, all weights are computed optimally for the entire graph.\n\\end{proof}\n\n\\subsection{Discrete Algorithm}\nWe use the optimal continuous algorithm $\\mathcal{A}^{cont}$ to devise an algorithm, $\\mathcal{A}^{disc}$, for (the discrete) Problem~\\ref{def:general_problem}.\n\nThe main idea is to use the optimal continuous shares as an indicator for the number of tasks that should be mapped to individual resources. Algorithm $\\mathcal{A}^{disc}$ is a greedy algorithm that allocates tasks to resources starting with the tasks with the largest shares, i.e., the tasks mapped to resources that must not be shared with many other tasks. The algorithm must overcome two issues: First, it is not possible to allocate tasks greedily in such a way that $n(r(v))$ is proportional to $1/x(v)$ for all $v \\in V$.\nThe second issue is that some shares may be larger than 1. An illustrative pathological example is the case where one task $v$ has an exorbitantly large weight, resulting in a share of $x(v) \\lesssim c$. If $d(G) \\in {\\mathcal{O}}(d(v)) = {\\mathcal{O}}(w(v)/c)$, the best possible discrete solution is at least a factor of $c$ worse as $d(v) \\ge w(v)$ for all $v \\in V$ and all allocations.\n\nWe will now present Algorithm $\\mathcal{A}^{disc}$, which is given in Algorithm~\\ref{algo:discrete}, and show how it overcomes the aforementioned issues.\nAfter computing the optimal continuous shares, the largest share $x(v)$ for some task $v \\in V$ is rounded down and fixed to $1$ if it exceeds this threshold. It is fixed in the sense that task $v$ is removed from the optimization problem and replaced with the constant $d(v)$. Subsequently, algorithm $\\mathcal{A}^{cont}$ is executed again with this added constraint. If the largest share still exceeds $1$, the same steps are carried out until all shares are upper bounded by $1$ (Lines~1-9).\n\nThese modified shares are then used to allocate the tasks to the $c$ resources as follows. The shares are first sorted in decreasing order, resulting in shares $\\bar{x}_1 \\ge \\ldots \\ge \\bar{x}_n$. The algorithm then performs a single pass over the sorted shares, starting at the largest value. The tasks with the $\\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\rceil$ largest shares are assigned to resource $R_1$. The share $\\bar{x}_i$ at index $i = \\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\rceil+1$ determines how many resources are assigned to $R_2$, i.e., $\\lceil\\frac{2n^{2/c}}{\\bar{x}_i}\\rceil$ many. This process is repeated until all nodes are assigned to resources (Lines~9-15).\n\n\\begin{algorithm}[t]\n\\small\n\\caption{Algorithm $\\mathcal{A}^{disc}$ takes $z_0$ and $c$ as parameters and computes a mapping $r:V\\rightarrow \\mathcal{R}$}\n   \\label{algo:discrete}\n\n   \\begin{center}\n    \\begin{algorithmic}[1]\n\t\\REPEAT\n\t  \\STATE \\textbf{execute} Algorithm $\\mathcal{A}^{cont}$\n\t  \\STATE $\\mathcal{S} := \\{v \\in V \\;|\\; x(v) > 1\\}$\n\t  \\IF{$\\mathcal{S} \\neq \\emptyset$}\n\t    \\STATE $v_{max} := \\arg\\max_{v \\in \\mathcal{S}} x(v)$\n\t    \\STATE $x(v_{max}) := 1$\n\t    \\STATE \\textbf{remove} $v_{max}$\n\t  \\ENDIF\n\t\\UNTIL{$\\mathcal{S} = \\emptyset$}\n        \\STATE $\\{\\bar{x}_1,\\ldots,\\bar{x}_n\\}$ $:=$ sort-decreasing($\\{x(v_1),\\ldots,x(v_n)\\}$)\n        \\STATE $index := 1$; $k := 1$\n        \\WHILE{$index \\le n$}\n\t  \\STATE $size := \\left\\lceil\\frac{2n^{2/c}}{\\bar{x}_{index}}\\right\\rceil$\n\t  \\FOR{$i=index,\\ldots,\\min\\{index+size-1,n\\}$}\n\t    \\STATE $r(v) := R_k$ \\textbf{where} $x(v) = \\bar{x}_i$\n\t  \\ENDFOR\n\t  \\STATE $index := index+size$; $k := k+1$\n\t\\ENDWHILE\n\\vspace{-5pt}\n     \\end{algorithmic}\n   \\end{center}\n\n \\end{algorithm}\n\n\n\nLemma~\\ref{lma:optConstrained} shows that Algorithm~$\\mathcal{A}^{disc}$ modifies shares $x$ in a way that preserves optimality for the case when shares cannot exceed the capacity of resources. Subsequently, we state the main result in Theorem~\\ref{thm:approximation}.\n\n\\begin{lemma}\\label{lma:optConstrained}\nLines~1-8 in Algorithm~\\ref{algo:discrete} compute optimal shares for any SPD graph G and capacity $c > 0$ subject to the constraint that shares must not exceed $1$.\n\\end{lemma}\n\\begin{proof}\nConsider task $v$ with the largest share $x(v) > 1$. Assume for the sake of contradiction that the optimal constrained share should be $x(v) < 1$. \nLet $z$ be the parent node of $z_i=v$ in the SPD tree.\n\nIf the capacity is not distributed according to Equation~\\eqref{eq:serial} (Equation~\\eqref{eq:parallel}) in a serial (parallel) composition, the distribution can be changed locally, i.e., among $z$ and $\\mathcal{C}(c)$, to the optimal distribution, which reduces $w(z)$ and, inductively, also $w(z_0) = \\delta(G)$, contradicting optimality of the shares. \nOtherwise, the share distribution among $z$ and $\\mathcal{C}(c)$ is optimal, but $x(z)$ is smaller than the $x(z)$ calculated by $\\mathcal{A}^{disc}$. Tracing the cause of the lower share towards the root, we find that either the capacity was not distributed optimally or, again, a share that is too small was assigned at this level. If we arrive at $z_0$, and the capacity is distributed optimally, it must be the case that less than the full capacity $c$ was assigned, which cannot be optimal.  \n\nThis argument establishes that $x(v)$ must equal $1$. As the optimal shares are recomputed under this constraint, the same argument can be used inductively for the next largest share exceeding $1$, which proves the claim.\n\\end{proof}\n\n\n\n\\begin{theorem}\\label{thm:approximation}\nAlgorithm $\\mathcal{A}^{disc}$ computes an ${\\mathcal{O}}(n^{{\\mathcal{O}}(1/c)})$-approximation for any computationally constrained SPD graph $G$. \n\\end{theorem}\n\\begin{proof}\nThe allocation strategy of $\\mathcal{A}^{disc}$ ensures that $n(r(v)) \\le \\lceil\\frac{2n^{2/c}}{x(v)}\\rceil \\le \\frac{2n^{2/c}}{x(v)}+1$ for all $v$ that are allocated \\emph{first} to a resource $r(v)$. Since $x(v) \\ge x(w)$ for any other task $w$ for which $r(v) = r(w)$, the inequality generally holds for all tasks. \nTherefore, we have for all $v \\in V$ that $d(v) \\in {\\mathcal{O}}(n^{{\\mathcal{O}}(1/c)}\\delta(v))$, which implies that\n\n$d(G) \\in {\\mathcal{O}}(\\hat{d}(G)) \\in {\\mathcal{O}}\\left(n^{{\\mathcal{O}}(1/c)}\\delta(G)\\right)$\n\nfor any computationally constrained SPD graph $G$.\n\nIt remains to show that all tasks are allocated to resources. Let\n\n$\\rho(R_i) := \\sum_{v \\in V : r(v) = R_i} x(v).$\n\nIt suffices to show that $\\sum_{i=1}^c \\rho(R_i) \\ge c$.\nWe define $\\bar{x}^{(i)}$ as the largest capacity assigned to resource $R_i$, i.e., $\\bar{x}^{(1)} := \\bar{x}_1$ and  $\\bar{x}^{(2)} := \\bar{x}_j$, where $j = \\left\\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\right\\rceil+1$ and so on. \nFurther let $\\bar{x}^{(c+1)}:= \\bar{x}_n$. \n\nSince the shares are ordered, we know that for all $i \\in \\{1,\\ldots,c\\}$, it holds that\n\\begin{IEEEeqnarray}{lCr}\n\\rho(R_i) &\\ge & \\bar{x}^{(i)} + \\left(\\left\\lceil\\frac{2 n^{2/c}}{\\bar{x}^{(i)}}\\right\\rceil-1\\right)\\bar{x}^{(i+1)} \\nonumber \\\\\n&\\ge& \\bar{x}^{(i)} + \\left(\\frac{2 n^{2/c}}{\\bar{x}^{(i)}}-1\\right)\\bar{x}^{(i+1)}.\\label{eq:rho_i}\n\\end{IEEEeqnarray}\nFor the sum of all $\\rho(R_i)$ we get that\n\n\n", "itemtype": "equation", "pos": 41183, "prevtext": "\n\nSimilarly, we can use Lemma~\\ref{lemma:continuous} to get the optimal streaming cost for a parallel composition, which is \n\n", "index": 7, "text": "\\begin{align*}\nw(z) & = \\max_{z_i \\in \\mathcal{C}(z)} \\frac{w(z_i)}{x(z_i)} \n \\stackrel{\\eqref{eq:parallel}}{=} \\max_{z_i \\in \\mathcal{C}(z)} \\frac{w(z_i)}{w(z_i)}\\sum_{z_i \\in \\mathcal{C}(z)} w(z_i) \\\\\n &= \\sum_{z_j\\in \\mathcal{C}(z)} w(z_i).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle w(z)\" display=\"inline\"><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max_{z_{i}\\in\\mathcal{C}(z)}\\frac{w(z_{i})}{x(z_{i})}\\stackrel{%&#10;\\eqref{eq:parallel}}{=}\\max_{z_{i}\\in\\mathcal{C}(z)}\\frac{w(z_{i})}{w(z_{i})}%&#10;\\sum_{z_{i}\\in\\mathcal{C}(z)}w(z_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><munder><mi>max</mi><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>x</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mover><mo movablelimits=\"false\">=</mo><mrow><mi mathsize=\"142%\">(</mi><mo>\u2062</mo><mtext class=\"ltx_missing_label\" mathsize=\"142%\" mathvariant=\"italic\"><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_missing_label ltx_font_italic ltx_ref_self\">LABEL:eq:parallel</span></mtext><mo>\u2062</mo><mi mathsize=\"142%\">)</mi></mrow></mover><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{z_{j}\\in\\mathcal{C}(z)}w(z_{i}).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><mrow><mi>w</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": "\n\nIf at least half of the terms is at least $2$, then the total sum is at least $c$, which means that all tasks can be allocated to the resources. For the sake of contradiction, assume that more than half of the terms are smaller than $2$. For each such term $2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}} < 2$, it holds that\n\n", "itemtype": "equation", "pos": 48566, "prevtext": "\n\nThus, procedure \\textit{computeWeights} computes the optimal cost, i.e., weight, in both cases and by induction, all weights are computed optimally for the entire graph.\n\\end{proof}\n\n\\subsection{Discrete Algorithm}\nWe use the optimal continuous algorithm $\\mathcal{A}^{cont}$ to devise an algorithm, $\\mathcal{A}^{disc}$, for (the discrete) Problem~\\ref{def:general_problem}.\n\nThe main idea is to use the optimal continuous shares as an indicator for the number of tasks that should be mapped to individual resources. Algorithm $\\mathcal{A}^{disc}$ is a greedy algorithm that allocates tasks to resources starting with the tasks with the largest shares, i.e., the tasks mapped to resources that must not be shared with many other tasks. The algorithm must overcome two issues: First, it is not possible to allocate tasks greedily in such a way that $n(r(v))$ is proportional to $1/x(v)$ for all $v \\in V$.\nThe second issue is that some shares may be larger than 1. An illustrative pathological example is the case where one task $v$ has an exorbitantly large weight, resulting in a share of $x(v) \\lesssim c$. If $d(G) \\in {\\mathcal{O}}(d(v)) = {\\mathcal{O}}(w(v)/c)$, the best possible discrete solution is at least a factor of $c$ worse as $d(v) \\ge w(v)$ for all $v \\in V$ and all allocations.\n\nWe will now present Algorithm $\\mathcal{A}^{disc}$, which is given in Algorithm~\\ref{algo:discrete}, and show how it overcomes the aforementioned issues.\nAfter computing the optimal continuous shares, the largest share $x(v)$ for some task $v \\in V$ is rounded down and fixed to $1$ if it exceeds this threshold. It is fixed in the sense that task $v$ is removed from the optimization problem and replaced with the constant $d(v)$. Subsequently, algorithm $\\mathcal{A}^{cont}$ is executed again with this added constraint. If the largest share still exceeds $1$, the same steps are carried out until all shares are upper bounded by $1$ (Lines~1-9).\n\nThese modified shares are then used to allocate the tasks to the $c$ resources as follows. The shares are first sorted in decreasing order, resulting in shares $\\bar{x}_1 \\ge \\ldots \\ge \\bar{x}_n$. The algorithm then performs a single pass over the sorted shares, starting at the largest value. The tasks with the $\\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\rceil$ largest shares are assigned to resource $R_1$. The share $\\bar{x}_i$ at index $i = \\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\rceil+1$ determines how many resources are assigned to $R_2$, i.e., $\\lceil\\frac{2n^{2/c}}{\\bar{x}_i}\\rceil$ many. This process is repeated until all nodes are assigned to resources (Lines~9-15).\n\n\\begin{algorithm}[t]\n\\small\n\\caption{Algorithm $\\mathcal{A}^{disc}$ takes $z_0$ and $c$ as parameters and computes a mapping $r:V\\rightarrow \\mathcal{R}$}\n   \\label{algo:discrete}\n\n   \\begin{center}\n    \\begin{algorithmic}[1]\n\t\\REPEAT\n\t  \\STATE \\textbf{execute} Algorithm $\\mathcal{A}^{cont}$\n\t  \\STATE $\\mathcal{S} := \\{v \\in V \\;|\\; x(v) > 1\\}$\n\t  \\IF{$\\mathcal{S} \\neq \\emptyset$}\n\t    \\STATE $v_{max} := \\arg\\max_{v \\in \\mathcal{S}} x(v)$\n\t    \\STATE $x(v_{max}) := 1$\n\t    \\STATE \\textbf{remove} $v_{max}$\n\t  \\ENDIF\n\t\\UNTIL{$\\mathcal{S} = \\emptyset$}\n        \\STATE $\\{\\bar{x}_1,\\ldots,\\bar{x}_n\\}$ $:=$ sort-decreasing($\\{x(v_1),\\ldots,x(v_n)\\}$)\n        \\STATE $index := 1$; $k := 1$\n        \\WHILE{$index \\le n$}\n\t  \\STATE $size := \\left\\lceil\\frac{2n^{2/c}}{\\bar{x}_{index}}\\right\\rceil$\n\t  \\FOR{$i=index,\\ldots,\\min\\{index+size-1,n\\}$}\n\t    \\STATE $r(v) := R_k$ \\textbf{where} $x(v) = \\bar{x}_i$\n\t  \\ENDFOR\n\t  \\STATE $index := index+size$; $k := k+1$\n\t\\ENDWHILE\n\\vspace{-5pt}\n     \\end{algorithmic}\n   \\end{center}\n\n \\end{algorithm}\n\n\n\nLemma~\\ref{lma:optConstrained} shows that Algorithm~$\\mathcal{A}^{disc}$ modifies shares $x$ in a way that preserves optimality for the case when shares cannot exceed the capacity of resources. Subsequently, we state the main result in Theorem~\\ref{thm:approximation}.\n\n\\begin{lemma}\\label{lma:optConstrained}\nLines~1-8 in Algorithm~\\ref{algo:discrete} compute optimal shares for any SPD graph G and capacity $c > 0$ subject to the constraint that shares must not exceed $1$.\n\\end{lemma}\n\\begin{proof}\nConsider task $v$ with the largest share $x(v) > 1$. Assume for the sake of contradiction that the optimal constrained share should be $x(v) < 1$. \nLet $z$ be the parent node of $z_i=v$ in the SPD tree.\n\nIf the capacity is not distributed according to Equation~\\eqref{eq:serial} (Equation~\\eqref{eq:parallel}) in a serial (parallel) composition, the distribution can be changed locally, i.e., among $z$ and $\\mathcal{C}(c)$, to the optimal distribution, which reduces $w(z)$ and, inductively, also $w(z_0) = \\delta(G)$, contradicting optimality of the shares. \nOtherwise, the share distribution among $z$ and $\\mathcal{C}(c)$ is optimal, but $x(z)$ is smaller than the $x(z)$ calculated by $\\mathcal{A}^{disc}$. Tracing the cause of the lower share towards the root, we find that either the capacity was not distributed optimally or, again, a share that is too small was assigned at this level. If we arrive at $z_0$, and the capacity is distributed optimally, it must be the case that less than the full capacity $c$ was assigned, which cannot be optimal.  \n\nThis argument establishes that $x(v)$ must equal $1$. As the optimal shares are recomputed under this constraint, the same argument can be used inductively for the next largest share exceeding $1$, which proves the claim.\n\\end{proof}\n\n\n\n\\begin{theorem}\\label{thm:approximation}\nAlgorithm $\\mathcal{A}^{disc}$ computes an ${\\mathcal{O}}(n^{{\\mathcal{O}}(1/c)})$-approximation for any computationally constrained SPD graph $G$. \n\\end{theorem}\n\\begin{proof}\nThe allocation strategy of $\\mathcal{A}^{disc}$ ensures that $n(r(v)) \\le \\lceil\\frac{2n^{2/c}}{x(v)}\\rceil \\le \\frac{2n^{2/c}}{x(v)}+1$ for all $v$ that are allocated \\emph{first} to a resource $r(v)$. Since $x(v) \\ge x(w)$ for any other task $w$ for which $r(v) = r(w)$, the inequality generally holds for all tasks. \nTherefore, we have for all $v \\in V$ that $d(v) \\in {\\mathcal{O}}(n^{{\\mathcal{O}}(1/c)}\\delta(v))$, which implies that\n\n$d(G) \\in {\\mathcal{O}}(\\hat{d}(G)) \\in {\\mathcal{O}}\\left(n^{{\\mathcal{O}}(1/c)}\\delta(G)\\right)$\n\nfor any computationally constrained SPD graph $G$.\n\nIt remains to show that all tasks are allocated to resources. Let\n\n$\\rho(R_i) := \\sum_{v \\in V : r(v) = R_i} x(v).$\n\nIt suffices to show that $\\sum_{i=1}^c \\rho(R_i) \\ge c$.\nWe define $\\bar{x}^{(i)}$ as the largest capacity assigned to resource $R_i$, i.e., $\\bar{x}^{(1)} := \\bar{x}_1$ and  $\\bar{x}^{(2)} := \\bar{x}_j$, where $j = \\left\\lceil\\frac{2n^{2/c}}{\\bar{x}_1}\\right\\rceil+1$ and so on. \nFurther let $\\bar{x}^{(c+1)}:= \\bar{x}_n$. \n\nSince the shares are ordered, we know that for all $i \\in \\{1,\\ldots,c\\}$, it holds that\n\\begin{IEEEeqnarray}{lCr}\n\\rho(R_i) &\\ge & \\bar{x}^{(i)} + \\left(\\left\\lceil\\frac{2 n^{2/c}}{\\bar{x}^{(i)}}\\right\\rceil-1\\right)\\bar{x}^{(i+1)} \\nonumber \\\\\n&\\ge& \\bar{x}^{(i)} + \\left(\\frac{2 n^{2/c}}{\\bar{x}^{(i)}}-1\\right)\\bar{x}^{(i+1)}.\\label{eq:rho_i}\n\\end{IEEEeqnarray}\nFor the sum of all $\\rho(R_i)$ we get that\n\n\n", "index": 9, "text": "$$\\sum_{i=1}^c \\rho(R_i) \\stackrel{\\eqref{eq:rho_i}}{\\ge} \\bar{x}^{(1)} - \\bar{x}_n +\\sum_{i=1}^c 2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}}\n\\ge \\sum_{i=1}^c 2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{c}\\rho(R_{i})\\stackrel{\\eqref{eq:rho_i}}{\\geq}\\bar{x}^{(1)}-\\bar{x%&#10;}_{n}+\\sum_{i=1}^{c}2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}}\\geq\\sum_{i=1%&#10;}^{c}2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}}.\" display=\"block\"><mrow><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mover><mo movablelimits=\"false\">\u2265</mo><mrow><mi mathsize=\"142%\">(</mi><mo>\u2062</mo><mtext class=\"ltx_missing_label\" mathsize=\"142%\" mathvariant=\"italic\"><span xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_ref ltx_missing_label ltx_font_italic ltx_ref_self\">LABEL:eq:rho_i</span></mtext><mo>\u2062</mo><mi mathsize=\"142%\">)</mi></mrow></mover><mrow><mrow><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>-</mo><msub><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>n</mi></msub></mrow><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>n</mi><mrow><mn>2</mn><mo>/</mo><mi>c</mi></mrow></msup><mo>\u2062</mo><mfrac><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mfrac></mrow></mrow></mrow><mo>\u2265</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>n</mi><mrow><mn>2</mn><mo>/</mo><mi>c</mi></mrow></msup><mo>\u2062</mo><mfrac><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mfrac></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": "\nAfter the first $c/2$ cases where Inequality~\\eqref{eq:decrease} holds, we get for the corresponding index $j$ that\n\\begin{displaymath}\n\\bar{x}^{(j)} \\stackrel{\\eqref{eq:decrease}}{<} \\frac{\\bar{x}^{(1)}}{\\left(n^{2/c}\\right)^{c/2}} \\le \\frac{1}{n}.\n\\end{displaymath}\nThus, all subsequent shares are so small that the respective tasks can be allocated to a single resource, and the processing cost for those tasks is not larger than in the continuous case, implying that more than half of the terms cannot satisfy Inequality~\\eqref{eq:decrease}, which concludes the proof.\n\\end{proof}\n\nNote that we used the constant $2$ twice in Line~12 of Algorithm~\\ref{algo:discrete} for ease of exposition. It is straightforward to compute optimal constants for a given $n$ and $c$.\n\n\n\n\\subsection{Non-Computationally Constrained Problems} \\label{app:greedy}\nAs shown, \nthe streaming cost for task allocation remains bounded with algorithm $\\mathcal{A}^{disc}$ if the streaming graph is computationally constrained. If streaming graphs are not computationally constrained, the question arises how to deal with (arbitrarily) large transfer costs.\nSpecifically, we discuss whether we can build upon the techniques used by $\\mathcal{A}^{disc}$: As we took a greedy approach to derive a discrete solution from the continuous solution for computationally constrained streaming graphs, we look into the difficulty of applying greedy strategies to handle large transfer costs.\n\nThe blueprint of our greedy strategies is the following: After computing the optimal continuous solution,\nthe algorithm traverses all edges from heaviest to lightest. For an edge $e = (u,v)$, it adds a constraint that enforces tasks $u$ and $v$ to be collocated, i.e., $r(u) = r(v)$; then, Algorithm $\\mathcal{A}^{disc}$ is executed with the newly added constraint to discover a new allocation $r$. Depending on the quality of $r$, the constraint is retained or dismissed. After the traversal, the allocation computed by $\\mathcal{A}^{disc}$ under the retained constraints is the solution.\nThe considered greedy strategies differ in the retention policy for constraints.\nWe now show that multiple intuitive strategies fail to achieve a better approximation ratio than the trivial ${\\mathcal{O}}(n)$ bound.\n\nArguably the most straightforward strategy is to retain a constraint if adding it results in a reduced streaming cost $d(G)$.\nThis strategy may already fail if there are two paths $P_1$ and $P_2$ for which $d(P_1) = d(P_2) = d(G)$ and there is an edge with an arbitrarily large transfer cost on each path: if both large edges are not covered, i.e., adjacent tasks are not collocated, then each individual constraint may reduce the cost of the respective path, but not $d(G)$; applying both constraints together, however, would result in a reduction of $d(G)$. As the reduction is not bounded, neither is the approximation ratio.\n\nThe deficiency of the strategy above can be overcome by slightly changing the rule to always retain a constraint \\emph{unless} it increases $d(G)$. However, also this strategy $\\mathcal{S}$ fails in that the approximation ratio may grow linearly with the number of tasks, even for a large number of resources.\n\n\\begin{figure}[t]\n\\center\n \\includegraphics[width=.85\\columnwidth]{greedy-worst-case}\n \\caption{SPD graph where the approximation ratio of the greedy strategy $\\mathcal{S}$ is in $\\Omega(n)$.}\n \\label{fig:greedy-worst-case}\n\\end{figure}\n\n\\begin{theorem}\nThe approximation ratio of the streaming cost of strategy $\\mathcal{S}$ is in $\\Omega(n)$ even for $c \\in \\Theta(n)$.\n\\end{theorem}\n\\begin{proof}\nConsider the streaming graph depicted in Figure~\\ref{fig:greedy-worst-case}. Let the number of resources be $c := \\frac{n}{4}+2$.\nNode and edge weights are as stated in the figure. The dashed boxes illustrate the optimal task allocation if transfer costs are disregarded: It holds that $\\hat{d}(\\{v_1,v_2,\\ldots,v_{n/2}\\}) = 2n$ and $\\hat{d}(\\{v_1,x,u_1,u_i\\}) = 2n-1$ for all $i = 2,\\ldots,\\frac{n}{2}-1$, and therefore $\\hat{d}(G) = 2n$.\n\nIf we consider transfer costs as well, strategy $\\mathcal{S}$ will ensure that all tasks $u_i$ are collocated.\nAt this stage, it holds for all $ i = 2,\\ldots,\\frac{n}{2}-1$ that\n\\begin{IEEEeqnarray*}{rCl}\nd(\\{v_1,v_2,\\ldots,v_{n/2}\\}) = 2n + 2n\\cdot(\\frac{n}{4}-1) &=& \\frac{n^2}{2} \\text{,~~~~and} \\\\ \nd(\\{v_1,x,u_1,u_i\\}) = 2n - 1 + (2n+2) &=& 4n+1.\n\\end{IEEEeqnarray*}\nThus, it holds that $d(G) = \\frac{n^2}{2}$.\nStrategy $\\mathcal{S}$ will retain the constraint for the next heaviest edge $(x,u_1)$ because the new cost on path $P = \\{v_1,x,u_1,u_i\\}$ is then\n\\begin{displaymath}\n d(P) = 5 + \\frac{n^2}{2} - \\frac{n}{2} \\le \\frac{n^2}{2}\n\\end{displaymath}\nfor all $i = 2,\\ldots,\\frac{n}{2}-1$ and $n\\ge 10$. As $\\mathcal{S}$ will keep these constraints, it holds that $d(G) \\in \\Omega(n^2)$ also at termination.\nHowever, the optimal solution does not collocate tasks $x$ and $u_1$, instead it reassigns all pairs on the path $\\{v_1,v_2,\\ldots,v_{n/2}\\}$ so that the endpoints of edges with transfer costs of $2n$ are collocated, resulting in a streaming cost of \n", "itemtype": "equation", "pos": 49101, "prevtext": "\n\nIf at least half of the terms is at least $2$, then the total sum is at least $c$, which means that all tasks can be allocated to the resources. For the sake of contradiction, assume that more than half of the terms are smaller than $2$. For each such term $2n^{2/c}\\frac{\\bar{x}^{(i+1)}}{\\bar{x}^{(i)}} < 2$, it holds that\n\n", "index": 11, "text": "\\begin{equation}\n\\bar{x}^{(i+1)} < \\frac{\\bar{x}^{(i)}}{n^{2/c}}.\\label{eq:decrease}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\bar{x}^{(i+1)}&lt;\\frac{\\bar{x}^{(i)}}{n^{2/c}}.\" display=\"block\"><mrow><mrow><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>&lt;</mo><mfrac><msup><mover accent=\"true\"><mi>x</mi><mo stretchy=\"false\">\u00af</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><msup><mi>n</mi><mrow><mn>2</mn><mo>/</mo><mi>c</mi></mrow></msup></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06060.tex", "nexttext": " which proves the claimed bound.\n\\end{proof}\n\n\nVarious other strategies, such as \nkeeping the constraint for each edge $e$ as long as the streaming cost of any path including $e$ does not increase, also fail to achieve a better bound.\nWhile these results do not preclude the existence of a simple strategy that guarantees a good approximation ratio, they suggest that a different approach may be required to cope with large edge weights in the general case.\n\n\n\n\\section{Related Work}\n\\label{sec:relatedwork}\n\nThere is a large body of work on (distributed) stream processing, covering a broad variety of topics.\nThe architectural challenges concerning \\emph{scalability}, \\emph{load management}, \\emph{high availability}, and \\emph{federated operation} have been addressed (see, e.g., \\cite{cherniack03}), as well as the requirements and algorithmic challenges in stream processing in general (e.g., \\cite{babcock02}).\nAs a result, several general-purpose stream processing platforms have been proposed~\\cite{arasu03,chandrasekaran03,abadi05,abadi03,carney02},  enabling the processing of continuous streams from many sources by providing primitive operators, which are building blocks in the form of functions, to build up complex stream processing topologies.\n\nApart from scalability and availability, \\emph{adaptive control} is a key requirement to absorb variable and bursty workloads~\\cite{amini06}.\nXing et al.\\ propose a correlation-based algorithm that strives to minimize both average load and load variance on the resources to protect against bursty inputs~\\cite{xing05}. The basic idea is to measure the correlation coefficient over time and to collocate tasks with small coefficients. Unlike our definition of processing cost, \\emph{load} is defined as the sum of the costs of tasks, where the cost of a task is the arrival rate multiplied by the processing time. While the load distribution problem is NP-hard, it is shown that a greedy algorithm yields a fairly good distribution in practice. It is further illustrated that the streaming model is fundamentally different from other parallel processing models. \n\\emph{Load shedding} is another approach to coping with excessive load. Tatbul et al.\\ model the distributed load shedding problem as a linear optimization problem subject to preserving low-latency processing and minimizing quality degradation~\\cite{tatbul07}.\nBy contrast, our work considers only static allocations at invariable input rates. However, since our algorithm is efficient, it can cope with changing environments through periodic re-execution. Similarly, Chatzistergiou et al.~\\cite{chatzistergiou14} suggest to use greedy task allocation for fast reallocation in dynamic environments. They experimentally evaluate their algorithm, which is tailored to problems consisting of groups of similar-weight tasks.   \nAlthough their problem definition includes arbitrary DAGs, the studied streaming networks are typically SPD graphs with restricted weights. Their model is different in that processing cost of a task is independent of the allocation.\n\nAnother key goal is to make stream processing \\emph{easily accessible}. To this end, platforms have been built that support efficient development of applications for processing continuous unbounded streams of data by exposing a set of simple programming interfaces. Examples of such platforms are S4~\\cite{neumeyer10} and Storm\\footnote{See http://storm.apache.org/.}. Both enable a programmatic specification of a level of parallelism for a processing element (PE) prototype, which determines the number of parallel instances to be executed---a feature that directly corresponds to our definition of parallel composition.\nSystem S takes this approach one step further in that it provides its own language (SPADE) for the composition of parallel data-flow graphs~\\cite{gedik08}. In the context of System S, it has also been studied how to coalesce basic operators into PEs and how to distribute them onto available hosts~\\cite{khandekar09}. Their work differs from ours as the PEs are given in our model and we consider different cost functions.\nAdditionally, the automatic composition of System S workflows, i.e., streaming topologies, has also been investigated~\\cite{riabov06}.\n\nWhile there is no theoretical work on a task allocation model similar to ours, the problem of allocating resources and placing operators in stream processing has received much attention. Wolf et al.\\ propose a scheduler that shifts the allocation dynamically in the face of changes in resource availability and job arrival and departures in order to optimize the weighted average of the allocation quality~\\cite{wolf08}. A key difference to our model is that \\emph{fractional assignments} of tasks to resources are possible in their model.\nXing et al.\\ have introduced the notion of a \\emph{resilient operator (i.e., task) placement plan}, which is resilient in the sense that it can withstand the largest set of input rate combinations~\\cite{xing06}. An algorithm for their model---where load functions can be expressed as linear constraint sets---is shown to improve resilience experimentally. Another approach to operator placement makes use of a layer between the stream processing system and the physical network that determines the placement in a virtual cost space and then maps the cost-space coordinates to physical resources~\\cite{pietzuch06}.\nFinally, Mattheis et al.\\ adapt \\emph{work stealing} strategies for stream processing and give bounds on the maximum latency for certain stealing strategies~\\cite{mattheis12}.\n\nTask allocation for stream processing is also related to \\emph{precedence-constrained scheduling}, \nwhere a set of jobs with precedence constraints has to be scheduled on $m$ processors so as to minimize the makespan or the average job completion time. These problems, which are generally NP-hard, have been extensively studied already in the 1970s (see~\\cite{hartmann10} and references therein).\nThe key difference to stream processing is that each job is executed only once; thus, as soon as a job is completed, no more resources need to be invested in this job. In stream processing, an allocated task is continuously executed on this resource (in parallel to collocated tasks).\n\nThere has been considerable interest in \\emph{series-parallel graphs} for many years due to their versatility and the fact that many NP-hard problems are solvable in polynomial time on these graphs. Examples for such problems are the minimum vertex cover problem, the maximum matching problem, and the maximum disjoint triangle problem~\\cite{takamizawa82}. \nMost related to our work is the topic of scheduling jobs subject to precedence constraints in the form of a series-parallel graph. It has been shown how to minimize the makespan for deteriorating jobs, for which the processing time increases with the start delay, in polynomial time~\\cite{wang08}.\nThe work most similar to ours studies scheduling of task graphs on two identical processors, where tasks have unit execution times and unit communication delays, and communication is free between collocated tasks. While this problem is NP-hard for general graphs~\\cite{rayward87}, Finta et al.\\ present an algorithm that computes an optimal schedule for a class of series-parallel graphs in quadratic time~\\cite{finta96}.\nDespite the similarities, their model is quite different in that there are no precedence relations between the tasks in our model as all allocated tasks must be executed in parallel, which necessitates a completely different approach.\n\n \n\n \n \n \n \n\n\n\n \n\n \n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe have introduced a theoretical model and a task allocation problem where tasks of a streaming topology must be allocated to a fixed set of physical resources for continuous processing.\nAs the problem is NP-hard, we have focused on approximation algorithms and presented an algorithm whose cost is only a small factor larger than in the optimal case under certain assumptions. While the algorithm solves the problem for an important case, there is much left to explore. In particular, the resources may not have uniform capacities and bandwidths, which makes it harder to find an optimal allocation. An interesting open question is also what guarantees on the approximation quality can be given in polynomial time for arbitrary directed acyclic streaming topologies.\n\n\n\n\n\n\\balance\n\\bibliographystyle{IEEEtran}\n\\bibliography{distributedStreamProcessing}\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nAfter the first $c/2$ cases where Inequality~\\eqref{eq:decrease} holds, we get for the corresponding index $j$ that\n\\begin{displaymath}\n\\bar{x}^{(j)} \\stackrel{\\eqref{eq:decrease}}{<} \\frac{\\bar{x}^{(1)}}{\\left(n^{2/c}\\right)^{c/2}} \\le \\frac{1}{n}.\n\\end{displaymath}\nThus, all subsequent shares are so small that the respective tasks can be allocated to a single resource, and the processing cost for those tasks is not larger than in the continuous case, implying that more than half of the terms cannot satisfy Inequality~\\eqref{eq:decrease}, which concludes the proof.\n\\end{proof}\n\nNote that we used the constant $2$ twice in Line~12 of Algorithm~\\ref{algo:discrete} for ease of exposition. It is straightforward to compute optimal constants for a given $n$ and $c$.\n\n\n\n\\subsection{Non-Computationally Constrained Problems} \\label{app:greedy}\nAs shown, \nthe streaming cost for task allocation remains bounded with algorithm $\\mathcal{A}^{disc}$ if the streaming graph is computationally constrained. If streaming graphs are not computationally constrained, the question arises how to deal with (arbitrarily) large transfer costs.\nSpecifically, we discuss whether we can build upon the techniques used by $\\mathcal{A}^{disc}$: As we took a greedy approach to derive a discrete solution from the continuous solution for computationally constrained streaming graphs, we look into the difficulty of applying greedy strategies to handle large transfer costs.\n\nThe blueprint of our greedy strategies is the following: After computing the optimal continuous solution,\nthe algorithm traverses all edges from heaviest to lightest. For an edge $e = (u,v)$, it adds a constraint that enforces tasks $u$ and $v$ to be collocated, i.e., $r(u) = r(v)$; then, Algorithm $\\mathcal{A}^{disc}$ is executed with the newly added constraint to discover a new allocation $r$. Depending on the quality of $r$, the constraint is retained or dismissed. After the traversal, the allocation computed by $\\mathcal{A}^{disc}$ under the retained constraints is the solution.\nThe considered greedy strategies differ in the retention policy for constraints.\nWe now show that multiple intuitive strategies fail to achieve a better approximation ratio than the trivial ${\\mathcal{O}}(n)$ bound.\n\nArguably the most straightforward strategy is to retain a constraint if adding it results in a reduced streaming cost $d(G)$.\nThis strategy may already fail if there are two paths $P_1$ and $P_2$ for which $d(P_1) = d(P_2) = d(G)$ and there is an edge with an arbitrarily large transfer cost on each path: if both large edges are not covered, i.e., adjacent tasks are not collocated, then each individual constraint may reduce the cost of the respective path, but not $d(G)$; applying both constraints together, however, would result in a reduction of $d(G)$. As the reduction is not bounded, neither is the approximation ratio.\n\nThe deficiency of the strategy above can be overcome by slightly changing the rule to always retain a constraint \\emph{unless} it increases $d(G)$. However, also this strategy $\\mathcal{S}$ fails in that the approximation ratio may grow linearly with the number of tasks, even for a large number of resources.\n\n\\begin{figure}[t]\n\\center\n \\includegraphics[width=.85\\columnwidth]{greedy-worst-case}\n \\caption{SPD graph where the approximation ratio of the greedy strategy $\\mathcal{S}$ is in $\\Omega(n)$.}\n \\label{fig:greedy-worst-case}\n\\end{figure}\n\n\\begin{theorem}\nThe approximation ratio of the streaming cost of strategy $\\mathcal{S}$ is in $\\Omega(n)$ even for $c \\in \\Theta(n)$.\n\\end{theorem}\n\\begin{proof}\nConsider the streaming graph depicted in Figure~\\ref{fig:greedy-worst-case}. Let the number of resources be $c := \\frac{n}{4}+2$.\nNode and edge weights are as stated in the figure. The dashed boxes illustrate the optimal task allocation if transfer costs are disregarded: It holds that $\\hat{d}(\\{v_1,v_2,\\ldots,v_{n/2}\\}) = 2n$ and $\\hat{d}(\\{v_1,x,u_1,u_i\\}) = 2n-1$ for all $i = 2,\\ldots,\\frac{n}{2}-1$, and therefore $\\hat{d}(G) = 2n$.\n\nIf we consider transfer costs as well, strategy $\\mathcal{S}$ will ensure that all tasks $u_i$ are collocated.\nAt this stage, it holds for all $ i = 2,\\ldots,\\frac{n}{2}-1$ that\n\\begin{IEEEeqnarray*}{rCl}\nd(\\{v_1,v_2,\\ldots,v_{n/2}\\}) = 2n + 2n\\cdot(\\frac{n}{4}-1) &=& \\frac{n^2}{2} \\text{,~~~~and} \\\\ \nd(\\{v_1,x,u_1,u_i\\}) = 2n - 1 + (2n+2) &=& 4n+1.\n\\end{IEEEeqnarray*}\nThus, it holds that $d(G) = \\frac{n^2}{2}$.\nStrategy $\\mathcal{S}$ will retain the constraint for the next heaviest edge $(x,u_1)$ because the new cost on path $P = \\{v_1,x,u_1,u_i\\}$ is then\n\\begin{displaymath}\n d(P) = 5 + \\frac{n^2}{2} - \\frac{n}{2} \\le \\frac{n^2}{2}\n\\end{displaymath}\nfor all $i = 2,\\ldots,\\frac{n}{2}-1$ and $n\\ge 10$. As $\\mathcal{S}$ will keep these constraints, it holds that $d(G) \\in \\Omega(n^2)$ also at termination.\nHowever, the optimal solution does not collocate tasks $x$ and $u_1$, instead it reassigns all pairs on the path $\\{v_1,v_2,\\ldots,v_{n/2}\\}$ so that the endpoints of edges with transfer costs of $2n$ are collocated, resulting in a streaming cost of \n", "index": 13, "text": "$$d(\\{v_1,v_2,\\ldots,v_{n/2}\\}) = 2n + \\frac{n}{4} \\in {\\mathcal{O}}(n),$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"d(\\{v_{1},v_{2},\\ldots,v_{n/2}\\})=2n+\\frac{n}{4}\\in{\\mathcal{O}}(n),\" display=\"block\"><mrow><mrow><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>v</mi><mrow><mi>n</mi><mo>/</mo><mn>2</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>n</mi></mrow><mo>+</mo><mfrac><mi>n</mi><mn>4</mn></mfrac></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaa</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]