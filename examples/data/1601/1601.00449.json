[{"file": "1601.00449.tex", "nexttext": "\nthat is, the convex hull of the set of vectors of cardinality at most $k$ and $\\ell_2$-norm no greater than one \\cite{Argyriou2012}. \nWe readily see that for $k=1$ and $k=d$ we recover the unit ball of the $\\ell_1$ and $\\ell_2$-norms respectively.\n\nThe $k$-support norm of a vector $w\\in {\\mathbb R}^d$ can be expressed as an infimal convolution \\cite[p.~34]{Rockafellar1970},\n\n", "itemtype": "equation", "pos": 7376, "prevtext": "\n\n\\title{\\LARGE Fitting Spectral Decay with the $k$-Support Norm}\n\n\n\n\\author{Andrew M. McDonald$^1$ \\and Massimiliano Pontil$^{1,2}$ \\and Dimitris Stamos$^{2}$ \\\\ \\\\\n {(1)} Department of Computer Science \\\\ \n University College London \\\\\n {\\em email:~\\{a.mcdonald,d.stamos.12\\}@ucl.ac.uk} \\\\\n Gower Street, London WC1E 6BT, UK \\\\ \\\\\n {(2)} Istituto Italiano di Tecnologia \\\\  \n Via Morego 30, 16163 Genova, Italy\n}\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nThe spectral $k$-support norm enjoys good estimation properties in low rank matrix learning problems, empirically outperforming the trace norm. Its unit ball is the convex hull of rank $k$ matrices with unit Frobenius norm.\nIn this paper we generalize the norm to the spectral $(k,p)$-support norm, whose additional parameter $p$ can be used to tailor the norm to the decay of the spectrum of the underlying model.\nWe characterize the unit ball and we explicitly compute the norm.\nWe further provide a conditional gradient method to solve regularization problems with the norm, and we derive an efficient algorithm to compute the Euclidean projection on the unit ball in the case $p=\\infty$. \nIn numerical experiments, we show that allowing $p$ to vary significantly improves performance over the spectral $k$-support norm on various matrix completion benchmarks, and better captures the spectral decay of the underlying model. \n\\end{abstract}\n\n\n{\\bfseries Keywords.} $k$-support norm, orthogonally invariant norms, matrix completion, multitask learning, proximal point algorithms.\n\n\\section{Introduction}\n\\label{sec:intro}\nThe problem of learning a sparse vector or a low rank matrix has generated much interest in recent years.\nA popular approach is to use convex regularizers which encourage sparsity, and a number of these have been studied with applications including image denoising, collaborative filtering and multitask learning, see for example,\n\n\\cite{Buehlmann2011,Wainwright2014} and references therein.\n\n\nRecently, the \\emph{$k$-support norm} was proposed by \\cite{Argyriou2012}, motivated as a tight relaxation of the set of $k$-sparse vectors of unit Euclidean norm. \n\n\n\nThe authors argue that as a regularizer for sparse vector estimation, the norm empirically outperforms the Lasso \\cite{Tibshirani1996} and Elastic Net \\cite{Zou2005} penalties. \nStatistical bounds on the Gaussian width of the $k$-support norm  have been provided by \\cite{Chatterjee2014}.\n\nThe $k$-support norm has also been extended to the matrix setting. \nBy applying the norm to the vector of singular values of a matrix, \\cite{McDonald2014a} obtain the orthogonally invariant \\emph{spectral $k$-support norm}, reporting state of the art performance on matrix completion benchmarks.  \n\nMotivated by the performance of the $k$-support norm in sparse vector and matrix learning problems, \nin this paper we study a natural generalization by considering the $\\ell_p$-norms (for $p \\in [1,\\infty]$) in place of the Euclidean norm.  \nThese allow a further degree of freedom when fitting a model to the underlying data. We denote the ensuing norm the \\emph{$(k,p)$-support norm}.\nAs we demonstrate in numerical experiments, \nusing $p=2$ is not necessarily the best choice in all instances. \nBy tuning the value of $p$ the model can incorporate prior information regarding the singular values. \nWhen prior knowledge is lacking, the parameter can be chosen by validation, hence the model can adapt to a variety of decay patterns of the singular values.  \n\nAn interesting property of the norm is that it interpolates between the $\\ell_1$ norm (for $k=1$) and the $\\ell_p$-norm (for $k=d$). \nIt follows that varying both $k$ and $p$ the norm allows one to learn sparse vectors which exhibit different patterns of decay in the non-zero elements. \nIn particular, when $p=\\infty$ the norm prefers vectors which are constant.\n\n\nA main goal of the paper is to study the proposed norm in matrix learning problems. \nThe $(k,p)$-support norm is a symmetric gauge function hence it induces the orthogonally invariant \\emph{spectral $(k,p)$-support norm}.   \nThis interpolates between the trace norm (for $k=1$) and the Schatten $p$-norms (for $k=d$) and its unit ball has a simple geometric interpretation as the convex hull of matrices of rank no greater than $k$ and Schatten $p$-norm no greater than one. \nThis suggests that the new norm favors low rank structure and the effect of varying $p$ allows different patterns of decay in the spectrum.\nIn the special case of $p=\\infty$, the $(k,p)$-support norm is the dual of the Ky-Fan $k$-norm \\cite{Bhatia1997} and it encourages a flat spectrum when used as a regularizer.  \n\nThe main contributions of the paper are: i) we propose the $(k,p)$-support norm as an extension of the $k$-support norm and we characterize in particular the unit ball of the induced orthogonally invariant matrix norm (Section \\ref{sec:kp-sup}); ii) we show that the norm can be computed efficiently and we discuss the role of the parameter $p$ (Section \\ref{sec:computing-the-norm}); iii) we outline a conditional gradient method to solve the associated regularization problem for both vector and matrix problems (Section \\ref{sec:optimization});\nand in the special case $p=\\infty$ we provide an $\\mathcal{O}(d \\log d)$ computation of the projection operator (Section \\ref{sec:prox}); finally, iv) we present numerical experiments on matrix completion benchmarks which demonstrate that the proposed norm offers significant improvement over previous methods, and we discuss the effect of the parameter $p$ (Section \\ref{sec:experiments}).\nThe appendix contains derivations of results which are sketched in or are omitted from the main body of the paper.\n\n\n\n\n{\\bf Notation.}\nWe use ${\\mathbb{N}_{{n}}}$ for the set of integers from $1$ up to and including $n$. \nWe let ${\\mathbb R}^d$ be the $d$-dimensional real vector space, whose elements are denoted by lower case letters. \n\n\nFor any vector $w\\in {\\mathbb R}^d$, its {\\em support} is defined as ${\\operatorname{supp}}(w) = \\{i \\in {\\mathbb{N}_{{d}}}: w_i \\neq 0\\} $, and its \\emph{cardinality} is defined as ${\\ensuremath{\\text{\\rm card}}}(w) = \\vert {\\operatorname{supp}}(w) \\vert$. \n\n\n\nWe let ${\\mathbb R}^{d \\times m}$ be the space of $d \\times m$ real matrices.\n\n\n\n\n\n\nWe denote the rank of a matrix as $\\textrm{rank}(W)$. \nWe let $\\sigma(W) \\in {\\mathbb R}^r$ be the vector formed by the singular values of $W$, where $r=\\min(d,m)$, and where we assume that the singular values are ordered nonincreasing, that is \n$\\sigma_1(W) {\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} \\sigma_r(W) {\\ensuremath{\\geqslant}} 0$.\n\n\n\nFor $p\\in[1,\\infty)$ the $\\ell_p$-norm of a vector $w \\in {\\mathbb R}^d$ is defined as $\\|w\\|_p = ( \\sum_{i=1}^d |w_i|^p)^{1/p}$ and $\\|w\\|_\\infty = \\max_{i=1}^d |w_i|$.\nGiven a norm $\\Vert \\cdot \\Vert$ on ${\\mathbb R}^d$ or ${\\mathbb R}^{d \\times m}$, $\\Vert \\cdot \\Vert_*$ denotes the corresponding dual norm, defined by $\\Vert u \\Vert_* = {\\operatorname{\\vphantom{p}sup}} \\{ \\langle u,w \\rangle : \\Vert w \\Vert {\\ensuremath{\\leqslant}} 1  \\}$. \n\nThe convex hull of a subset $S$ of a vector space is denoted $\\textrm{co}(S)$.\n\n\n\n\n\n\n\\section{Background and Previous Work}\n\\label{sec:background}\n\\vspace{-.1truecm}\nFor every $k \\in {\\mathbb N}_d$, the $k$-support norm $\\Vert \\cdot \\Vert_{(k)}$ is defined as the norm whose unit ball is given by \n\n", "index": 1, "text": "\\begin{align}\n\\text{co}\\left\\{ w \\in {\\mathbb R}^d: {\\ensuremath{\\text{\\rm card}}}(w) {\\ensuremath{\\leqslant}} k, \\Vert w \\Vert_2 {\\ensuremath{\\leqslant}} 1 \\right\\},\n\\label{eqn:ksup-unit-ball}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{co}\\left\\{w\\in{\\mathbb{R}}^{d}:{\\text{\\rm card}}(w){%&#10;\\leqslant}k,\\|w\\|_{2}{\\leqslant}1\\right\\},\" display=\"inline\"><mrow><mrow><mtext>co</mtext><mo>\u2062</mo><mrow><mo>{</mo><mrow><mi>w</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow><mo>:</mo><mrow><mrow><mrow><mtext>card</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow><mo>,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2a7d</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere ${\\mathcal{G}}_k$ is the collection of all subsets of ${\\mathbb{N}_{{d}}}$ containing at most $k$ elements and the infimum is over all vectors $v_g\\in {\\mathbb R}^d$ such that $\n{\\operatorname{supp}}(v_g) \\subseteq g$, for $g \\in {\\mathcal{G}}_k$. \nEquation \\eqref{eqn:GLO} highlights that the $k$-support norm is a special case of the group lasso with overlap \\cite{Jacob2009-GL}, where the cardinality of the support sets is at most $k$.  \nThis expression suggests that when used as a regularizer, the norm encourages vectors $w$ to be a sum of a limited number of vectors with small support.  \n\nDue to the variational form of \\eqref{eqn:GLO} computing the norm is not straightforward, however \\cite{Argyriou2012} note that the dual norm has a simple form, namely it is the $\\ell_2$-norm of the $k$ largest components,\n\n", "itemtype": "equation", "pos": 7959, "prevtext": "\nthat is, the convex hull of the set of vectors of cardinality at most $k$ and $\\ell_2$-norm no greater than one \\cite{Argyriou2012}. \nWe readily see that for $k=1$ and $k=d$ we recover the unit ball of the $\\ell_1$ and $\\ell_2$-norms respectively.\n\nThe $k$-support norm of a vector $w\\in {\\mathbb R}^d$ can be expressed as an infimal convolution \\cite[p.~34]{Rockafellar1970},\n\n", "index": 3, "text": "\\begin{align}\n\n\\|w\\|_{(k)}={\\operatorname{\\vphantom{p}inf}}_{(v_g)}  \\Bigg\\{ \\sum_{g \\in {\\mathcal{G}}_k} \\Vert v_g \\Vert_2 :  \\sum_{g \\in {\\mathcal{G}}_k} v_g = w \\Bigg\\},\n\\label{eqn:GLO}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;\\|w\\|_{(k)}={\\operatorname{\\vphantom{p}inf}}_{(v_{g})}\\Bigg{%&#10;\\{}\\sum_{g\\in{\\mathcal{G}}_{k}}\\|v_{g}\\|_{2}:\\sum_{g\\in{\\mathcal{G}}_{k}}v_{g}%&#10;=w\\Bigg{\\}},\" display=\"inline\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mrow><msub><mo>inf</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2061</mo><mrow><mo maxsize=\"260%\" minsize=\"260%\">{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>g</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mi>k</mi></msub></mrow></munder></mstyle><msub><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>g</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>g</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mi>k</mi></msub></mrow></munder></mstyle><msub><mi>v</mi><mi>g</mi></msub></mrow><mo>=</mo><mi>w</mi></mrow></mrow><mo maxsize=\"260%\" minsize=\"260%\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": " \nwhere $|u|^{\\downarrow}$ is the vector obtained from $u$ by reordering its components so that they are nonincreasing in absolute value. \nNote also from equation \\eqref{eqn:ksup-dualeq} that for $k=1$ and $k=d$, the dual norm is equal to the $\\ell_{\\infty}$-norm and $\\ell_2$-norm, respectively, which agrees with our earlier observation regarding the primal norm. \n\n\nA related problem which has been studied in recent years is learning a matrix from a set of linear measurements, in which the underlying matrix is assumed to have sparse spectrum (low rank). \nThe trace norm, the $\\ell_1$-norm of the singular values of a matrix, has been shown to perform well in this setting, see e.g.  \\cite{Argyriou2008,Jaggi2010}.\n\n\n\nRecall that a norm $\\Vert\\cdot \\Vert$ on $\\mathbb{R}^{d \\times m}$ is called orthogonally invariant if \n\n\n\n$\\Vert W \\Vert= \\Vert U W V \\Vert$,\nfor any orthogonal matrices $U \\in \\mathbb{R}^{d \\times d}$ and $V \\in \\mathbb{R}^{m \\times m}$. \nA classical result by von Neumann establishes that a norm is orthogonally invariant if and only if it is of the form $\\Vert W \\Vert = g(\\sigma(W))$, where $\\sigma(W)$ is the vector formed by the singular values of $W$ in nonincreasing order, and $g$ is a symmetric gauge function \\cite{VonNeumann1937}.\nIn other words, $g$ is a norm which is invariant under permutations and sign changes of the vector components, that is \n\n\n\n$g(w) = g(P w)= g(J w)$,\nwhere $P$ is any permutation matrix and $J$ is diagonal with entries equal to $\\pm 1$ \\cite[p. 438]{Horn1991}. \n\nExamples of symmetric gauge functions are the $\\ell_p$ norms for $p \\in [1,\\infty]$ and the corresponding orthogonally invariant norms are called the Schatten $p$-norms \\cite[p. 441]{Horn1991}. \nIn particular, those include the trace norm and Frobenius norm for $p=1$ and $p=2$ respectively.\nRegularization with Schatten $p$-norms has been previously studied by \\cite{Argyriou2007} and a statistical analysis has been performed by \\cite{Rohde2011}.\nAs the set ${\\mathcal{G}}_k$ includes all subsets of size $k$, expression \\eqref{eqn:GLO} for the $k$-support norm reveals that is a symmetric gauge function.  \n\\cite{McDonald2014a} use this fact to introduce the spectral $k$-support norm for matrices, by defining $\\Vert W \\Vert_{(k)} = \\Vert \\sigma(W) \\Vert_{(k)}$, for $W \\in {\\mathbb R}^{d \\times m}$ and report state of the art performance on matrix completion benchmarks. \n\n\n\n\n\n\n\n\n\n\n\n\\section{The $(k,p)$-Support Norm}\n\\label{sec:kp-sup}\n\\vspace{-.1truecm}\nIn this section we introduce the $(k,p)$-support norm as a natural extension of the $k$-support norm. \nThis follows by applying the $\\ell_p$-norm, rather than the Euclidean norm, in the infimum convolution definition of the norm.  \n\n\\begin{definition}\n\\label{def:k-sup-p-norm}\nLet $k \\in {\\mathbb N}_d$ and $p \\in [1,\\infty]$. The $(k,p)$-support norm of a vector $w \\in {\\mathbb R}^d$ is defined as \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere ${\\mathcal{G}}_k$ is the collection of all subsets of ${\\mathbb{N}_{{d}}}$ containing at most $k$ elements and the infimum is over all vectors $v_g\\in {\\mathbb R}^d$ such that $\n{\\operatorname{supp}}(v_g) \\subseteq g$, for $g \\in {\\mathcal{G}}_k$. \nEquation \\eqref{eqn:GLO} highlights that the $k$-support norm is a special case of the group lasso with overlap \\cite{Jacob2009-GL}, where the cardinality of the support sets is at most $k$.  \nThis expression suggests that when used as a regularizer, the norm encourages vectors $w$ to be a sum of a limited number of vectors with small support.  \n\nDue to the variational form of \\eqref{eqn:GLO} computing the norm is not straightforward, however \\cite{Argyriou2012} note that the dual norm has a simple form, namely it is the $\\ell_2$-norm of the $k$ largest components,\n\n", "index": 5, "text": "\\begin{align}\n\\Vert u \\Vert_{(k),*} &= \\sqrt{\\sum_{i=1}^k (\\vert u \\vert^{\\downarrow}_i)^2},~~~u \\in {\\mathbb R}^d\\label{eqn:ksup-dualeq},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|u\\|_{(k),*}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sqrt{\\sum_{i=1}^{k}(|u|^{\\downarrow}_{i})^{2}},~{}~{}~{}u\\in{%&#10;\\mathbb{R}}^{d},\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><msqrt><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mrow><mo stretchy=\"false\">|</mo><mi>u</mi><mo stretchy=\"false\">|</mo></mrow><mi>i</mi><mo>\u2193</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><mo rspace=\"12.4pt\">,</mo><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere the infimum is over all vectors $v_g\\in {\\mathbb R}^d$ such that $ {\\operatorname{supp}}(v_g) \\subseteq g$, for $g \\in {\\mathcal{G}}_k$.\n\\end{definition}\n\nLet us note that the norm is well defined. Indeed, positivity, homogeneity and non degeneracy are immediate. \nTo prove the triangle inequality, let $w,w' \\in {\\mathbb R}^d$.  \nFor any $\\epsilon >0$ there exist $\\{ v_g \\}$ and $\\{ v'_{g}\\}$ such that $w=\\sum_{g} v_g$, $w'=\\sum_{g} v'_{g}$, $\\sum_{g} \\Vert v_g \\Vert_p  {\\ensuremath{\\leqslant}} \\Vert w \\Vert_{(k,p)} + \\epsilon/2$, and $\\sum_{g} \\Vert v'_g \\Vert_p  {\\ensuremath{\\leqslant}} \\Vert w' \\Vert_{(k,p)} + \\epsilon/2$.\nAs $\\sum_{g} v_g + \\sum_{g} v'_{g} = w+w'$, we have\n\n", "itemtype": "equation", "pos": 12028, "prevtext": " \nwhere $|u|^{\\downarrow}$ is the vector obtained from $u$ by reordering its components so that they are nonincreasing in absolute value. \nNote also from equation \\eqref{eqn:ksup-dualeq} that for $k=1$ and $k=d$, the dual norm is equal to the $\\ell_{\\infty}$-norm and $\\ell_2$-norm, respectively, which agrees with our earlier observation regarding the primal norm. \n\n\nA related problem which has been studied in recent years is learning a matrix from a set of linear measurements, in which the underlying matrix is assumed to have sparse spectrum (low rank). \nThe trace norm, the $\\ell_1$-norm of the singular values of a matrix, has been shown to perform well in this setting, see e.g.  \\cite{Argyriou2008,Jaggi2010}.\n\n\n\nRecall that a norm $\\Vert\\cdot \\Vert$ on $\\mathbb{R}^{d \\times m}$ is called orthogonally invariant if \n\n\n\n$\\Vert W \\Vert= \\Vert U W V \\Vert$,\nfor any orthogonal matrices $U \\in \\mathbb{R}^{d \\times d}$ and $V \\in \\mathbb{R}^{m \\times m}$. \nA classical result by von Neumann establishes that a norm is orthogonally invariant if and only if it is of the form $\\Vert W \\Vert = g(\\sigma(W))$, where $\\sigma(W)$ is the vector formed by the singular values of $W$ in nonincreasing order, and $g$ is a symmetric gauge function \\cite{VonNeumann1937}.\nIn other words, $g$ is a norm which is invariant under permutations and sign changes of the vector components, that is \n\n\n\n$g(w) = g(P w)= g(J w)$,\nwhere $P$ is any permutation matrix and $J$ is diagonal with entries equal to $\\pm 1$ \\cite[p. 438]{Horn1991}. \n\nExamples of symmetric gauge functions are the $\\ell_p$ norms for $p \\in [1,\\infty]$ and the corresponding orthogonally invariant norms are called the Schatten $p$-norms \\cite[p. 441]{Horn1991}. \nIn particular, those include the trace norm and Frobenius norm for $p=1$ and $p=2$ respectively.\nRegularization with Schatten $p$-norms has been previously studied by \\cite{Argyriou2007} and a statistical analysis has been performed by \\cite{Rohde2011}.\nAs the set ${\\mathcal{G}}_k$ includes all subsets of size $k$, expression \\eqref{eqn:GLO} for the $k$-support norm reveals that is a symmetric gauge function.  \n\\cite{McDonald2014a} use this fact to introduce the spectral $k$-support norm for matrices, by defining $\\Vert W \\Vert_{(k)} = \\Vert \\sigma(W) \\Vert_{(k)}$, for $W \\in {\\mathbb R}^{d \\times m}$ and report state of the art performance on matrix completion benchmarks. \n\n\n\n\n\n\n\n\n\n\n\n\\section{The $(k,p)$-Support Norm}\n\\label{sec:kp-sup}\n\\vspace{-.1truecm}\nIn this section we introduce the $(k,p)$-support norm as a natural extension of the $k$-support norm. \nThis follows by applying the $\\ell_p$-norm, rather than the Euclidean norm, in the infimum convolution definition of the norm.  \n\n\\begin{definition}\n\\label{def:k-sup-p-norm}\nLet $k \\in {\\mathbb N}_d$ and $p \\in [1,\\infty]$. The $(k,p)$-support norm of a vector $w \\in {\\mathbb R}^d$ is defined as \n\n", "index": 7, "text": "\\begin{align}\n\\Vert w \\Vert_{(k,p)} &= {\\operatorname{\\vphantom{p}inf}}_{(v_g)} \\left\\{ \\sum_{g \\in {\\mathcal{G}}_k} \\Vert v_g \\Vert_p :  \\sum_{g \\in {\\mathcal{G}}_k} v_g = w \\right\\}. \n\\label{eqn:kp-sup-infimum-convolution}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|w\\|_{(k,p)}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}_{(v_{g})}\\left\\{\\sum_{g\\in{%&#10;\\mathcal{G}}_{k}}\\|v_{g}\\|_{p}:\\sum_{g\\in{\\mathcal{G}}_{k}}v_{g}=w\\right\\}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mo>inf</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>g</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mi>k</mi></msub></mrow></munder></mstyle><msub><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>g</mi></msub><mo>\u2225</mo></mrow><mi>p</mi></msub></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>g</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mi>k</mi></msub></mrow></munder></mstyle><msub><mi>v</mi><mi>g</mi></msub></mrow><mo>=</mo><mi>w</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nand the result follows by letting $\\epsilon$ tend to zero. \n\nNote that, since a convex set is equivalent to the convex hull of its extreme points, Definition \\ref{def:k-sup-p-norm} implies that the unit ball of the $(k,p)$-support norm, denoted by $C_k^p$, is given by the convex hull of the set of vectors with cardinality no greater than $k$ and $\\ell_p$-norm no greater than 1, that is \n\n", "itemtype": "equation", "pos": 12956, "prevtext": "\nwhere the infimum is over all vectors $v_g\\in {\\mathbb R}^d$ such that $ {\\operatorname{supp}}(v_g) \\subseteq g$, for $g \\in {\\mathcal{G}}_k$.\n\\end{definition}\n\nLet us note that the norm is well defined. Indeed, positivity, homogeneity and non degeneracy are immediate. \nTo prove the triangle inequality, let $w,w' \\in {\\mathbb R}^d$.  \nFor any $\\epsilon >0$ there exist $\\{ v_g \\}$ and $\\{ v'_{g}\\}$ such that $w=\\sum_{g} v_g$, $w'=\\sum_{g} v'_{g}$, $\\sum_{g} \\Vert v_g \\Vert_p  {\\ensuremath{\\leqslant}} \\Vert w \\Vert_{(k,p)} + \\epsilon/2$, and $\\sum_{g} \\Vert v'_g \\Vert_p  {\\ensuremath{\\leqslant}} \\Vert w' \\Vert_{(k,p)} + \\epsilon/2$.\nAs $\\sum_{g} v_g + \\sum_{g} v'_{g} = w+w'$, we have\n\n", "index": 9, "text": "\\begin{align*}\n\\Vert w+w' \\Vert_{(k,p)}\n&{\\ensuremath{\\leqslant}} \\sum_{g} \\Vert v_g \\Vert_p + \\sum_{g} \\Vert v'_{g} \\Vert_p  \\\\\n&{\\ensuremath{\\leqslant}} \\Vert w \\Vert_{(k,p)} + \\Vert w'\\Vert_{(k,p)} + \\epsilon,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|w+w^{\\prime}\\|_{(k,p)}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><mi>w</mi><mo>+</mo><msup><mi>w</mi><mo>\u2032</mo></msup></mrow><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\leqslant}\\sum_{g}\\|v_{g}\\|_{p}+\\sum_{g}\\|v^{\\prime}_{g}\\|_{p}\" display=\"inline\"><mrow><mi/><mo>\u2a7d</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>g</mi></munder></mstyle><msub><mrow><mo>\u2225</mo><msub><mi>v</mi><mi>g</mi></msub><mo>\u2225</mo></mrow><mi>p</mi></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>g</mi></munder></mstyle><msub><mrow><mo>\u2225</mo><msubsup><mi>v</mi><mi>g</mi><mo>\u2032</mo></msubsup><mo>\u2225</mo></mrow><mi>p</mi></msub></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\leqslant}\\|w\\|_{(k,p)}+\\|w^{\\prime}\\|_{(k,p)}+\\epsilon,\" display=\"inline\"><mrow><mrow><mi/><mo>\u2a7d</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>+</mo><msub><mrow><mo>\u2225</mo><msup><mi>w</mi><mo>\u2032</mo></msup><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>+</mo><mi>\u03f5</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\nDefinition \\ref{def:k-sup-p-norm} gives the norm as the solution of a variational problem. \nIts explicit computation is not straightforward in the general case, however for $p=1$ the unit ball \\eqref{eq:unitball} does not depend on $k$ and is always equal to the $\\ell_1$ unit ball. \nThus, the $(k,1)$-support norm is always equal to the $\\ell_1$-norm, and we do not consider further this case in this section. \n\nSimilarly, for $k=1$ we recover the $\\ell_1$-norm for all values of $p$. \nFor $p=\\infty$, from the definition of the dual norm it is not difficult to show that $\\Vert \\cdot \\Vert_{(k,p)} = \\max \\{\\Vert \\cdot \\Vert_{\\infty}, \\Vert \\cdot \\Vert_1 / k \\}$.  \nWe return to this in Section \\ref{sec:computing-the-norm} when we describe how to compute the norm for all values of $p$.    \n\n\n\nNote further that in Equation \\eqref{eqn:kp-sup-infimum-convolution}, as $p$ tends to $\\infty$, the $\\ell_p$-norm of each $v_g$ is increasingly dominated by the largest component of $v_g$. \nAs the variational formulation tries to identify vectors $v_g$ with small aggregate $\\ell_p$-norm, this suggests that higher values of $p$ encourage each $v_g$ to tend to a vector whose $k$ entries are equal.  \n\nIn this manner varying $p$ allows us adjust the degree to which the components of vector $w$ can be clustered into (possibly overlapping) groups of size $k$. \n\n\nAs in the case of the $k$-support norm, the dual $(k,p)$-support norm has a simple expression.  \n\nRecall that the dual norm of a vector $u \\in {\\mathbb R}^d$ is defined by the optimization problem\n\n", "itemtype": "equation", "pos": 13572, "prevtext": "\nand the result follows by letting $\\epsilon$ tend to zero. \n\nNote that, since a convex set is equivalent to the convex hull of its extreme points, Definition \\ref{def:k-sup-p-norm} implies that the unit ball of the $(k,p)$-support norm, denoted by $C_k^p$, is given by the convex hull of the set of vectors with cardinality no greater than $k$ and $\\ell_p$-norm no greater than 1, that is \n\n", "index": 11, "text": "\\begin{align}\nC_k^p=\\textrm{co}\\left\\{w\\in {\\mathbb R}^d:  {\\ensuremath{\\text{\\rm card}}}(w) {\\ensuremath{\\leqslant}} k, \\Vert w \\Vert_p {\\ensuremath{\\leqslant}} 1 \\right\\}. \n\\label{eq:unitball}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle C_{k}^{p}=\\textrm{co}\\left\\{w\\in{\\mathbb{R}}^{d}:{\\text{\\rm card%&#10;}}(w){\\leqslant}k,\\|w\\|_{p}{\\leqslant}1\\right\\}.\" display=\"inline\"><mrow><mrow><msubsup><mi>C</mi><mi>k</mi><mi>p</mi></msubsup><mo>=</mo><mrow><mtext>co</mtext><mo>\u2062</mo><mrow><mo>{</mo><mrow><mi>w</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow><mo>:</mo><mrow><mrow><mrow><mtext>card</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow><mo>,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mi>p</mi></msub><mo>\u2a7d</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\n\\begin{proposition}\\label{prop:k-sup-p-norm-and-dual}\n\n\n\n\n\n\nIf $p \\in (1,\\infty]$ then the dual $(k,p)$-support norm is given by \n\n", "itemtype": "equation", "pos": 15337, "prevtext": "\n\nDefinition \\ref{def:k-sup-p-norm} gives the norm as the solution of a variational problem. \nIts explicit computation is not straightforward in the general case, however for $p=1$ the unit ball \\eqref{eq:unitball} does not depend on $k$ and is always equal to the $\\ell_1$ unit ball. \nThus, the $(k,1)$-support norm is always equal to the $\\ell_1$-norm, and we do not consider further this case in this section. \n\nSimilarly, for $k=1$ we recover the $\\ell_1$-norm for all values of $p$. \nFor $p=\\infty$, from the definition of the dual norm it is not difficult to show that $\\Vert \\cdot \\Vert_{(k,p)} = \\max \\{\\Vert \\cdot \\Vert_{\\infty}, \\Vert \\cdot \\Vert_1 / k \\}$.  \nWe return to this in Section \\ref{sec:computing-the-norm} when we describe how to compute the norm for all values of $p$.    \n\n\n\nNote further that in Equation \\eqref{eqn:kp-sup-infimum-convolution}, as $p$ tends to $\\infty$, the $\\ell_p$-norm of each $v_g$ is increasingly dominated by the largest component of $v_g$. \nAs the variational formulation tries to identify vectors $v_g$ with small aggregate $\\ell_p$-norm, this suggests that higher values of $p$ encourage each $v_g$ to tend to a vector whose $k$ entries are equal.  \n\nIn this manner varying $p$ allows us adjust the degree to which the components of vector $w$ can be clustered into (possibly overlapping) groups of size $k$. \n\n\nAs in the case of the $k$-support norm, the dual $(k,p)$-support norm has a simple expression.  \n\nRecall that the dual norm of a vector $u \\in {\\mathbb R}^d$ is defined by the optimization problem\n\n", "index": 13, "text": "\\begin{align}\n\\label{eq:dual}\n\\|u\\|_{(k,p),*} = \\max\\left\\{{\\langle} u,w{\\rangle} : \\|w\\|_{(k,p)} = 1 \\right\\}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|u\\|_{(k,p),*}=\\max\\left\\{{\\langle}u,w{\\rangle}:\\|w\\|_{(k,p)}=1%&#10;\\right\\}.\" display=\"inline\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mo stretchy=\"false\">\u27e8</mo><mi>u</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $q=p/(p-1)$ and ${I_k} \\subset {\\mathbb N}_d$ is the set of indices of the $k$ largest components of $u$ in absolute value. \nFurthermore, if $p \\in (1,\\infty)$ and $u \\in {\\mathbb R}^d \\backslash\\{0\\}$ then the maximum in \n\\eqref{eq:dual} is attained for\n\n", "itemtype": "equation", "pos": 15592, "prevtext": "\n\n\\begin{proposition}\\label{prop:k-sup-p-norm-and-dual}\n\n\n\n\n\n\nIf $p \\in (1,\\infty]$ then the dual $(k,p)$-support norm is given by \n\n", "index": 15, "text": "\\begin{align*}\n\\Vert u \\Vert_{(k,p),*} = \\left(\\sum_{i \\in {I_k}} |u_i|^q\\right)^\\frac{1}{q},~~~ u \\in {\\mathbb R}^d,\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|u\\|_{(k,p),*}=\\left(\\sum_{i\\in{I_{k}}}|u_{i}|^{q}\\right)^{\\frac%&#10;{1}{q}},~{}~{}~{}u\\in{\\mathbb{R}}^{d},\" display=\"inline\"><mrow><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></munder></mstyle><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mi>q</mi></msup></mrow><mo>)</mo></mrow><mfrac><mn>1</mn><mi>q</mi></mfrac></msup></mrow><mo rspace=\"12.4pt\">,</mo><mrow><mi>u</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nIf $p =\\infty$ the maximum is attained for\n\n", "itemtype": "equation", "pos": 15984, "prevtext": "\nwhere $q=p/(p-1)$ and ${I_k} \\subset {\\mathbb N}_d$ is the set of indices of the $k$ largest components of $u$ in absolute value. \nFurthermore, if $p \\in (1,\\infty)$ and $u \\in {\\mathbb R}^d \\backslash\\{0\\}$ then the maximum in \n\\eqref{eq:dual} is attained for\n\n", "index": 17, "text": "\\begin{equation}\nw_i =\n\\begin{cases}\n{\\rm sign}(u_i)  \\left(\\frac{|u_i|}{\\|u\\|_{(k,p),*}}\\right)^{\\frac{1}{p-1}} & \\text{if } i \\in {I_k},\\\\\n0 & \\text{otherwise}.\n\\end{cases}\\label{eqn:dual-component}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"w_{i}=\\begin{cases}{\\rm sign}(u_{i})\\left(\\frac{|u_{i}|}{\\|u\\|_{(k,p),*}}%&#10;\\right)^{\\frac{1}{p-1}}&amp;\\text{if }i\\in{I_{k}},\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"false\"><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub></mfrac></mstyle><mo>)</mo></mrow><mfrac><mn>1</mn><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></mfrac></msup></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\\iffalse\nIf $p=1$ equality holds if and only if\n\n", "itemtype": "equation", "pos": 16243, "prevtext": "\nIf $p =\\infty$ the maximum is attained for\n\n", "index": 19, "text": "\\begin{equation*}\nw_i =\n\\begin{cases}\n{\\rm sign}(u_i) & \\text{if } i \\in {I_k}, u_i \\neq 0,\\\\\n\\lambda_i \\in [-1,1]  & \\text{if } i \\in {I_k}, u_i =0 ,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"w_{i}=\\begin{cases}{\\rm sign}(u_{i})&amp;\\text{if }i\\in{I_{k}},u_{i}\\neq 0,\\\\&#10;\\lambda_{i}\\in[-1,1]&amp;\\text{if }i\\in{I_{k}},u_{i}=0,\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><mo>,</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2260</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2208</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><mo>,</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $M = \\{i: |u_i| =  \\max_j |u_j|\\}$ and $\\lambda_i {\\ensuremath{\\geqslant}} 0$, $\\sum_{i \\in M} \\lambda_i = 1$. \nfor any $\\lambda \\in [-1,1]$.\n\\fi\n\\end{proposition}\n\nNote that \n\n\nfor $p=2$ we recover the dual of the $k$-support norm in \\eqref{eqn:ksup-dualeq}. \n\n\n\\subsection{The Spectral $(k,p)$-Support Norm}\nFrom Definition \\ref{def:k-sup-p-norm} it is clear that the $(k,p)$-support norm is a symmetric gauge function. \nThis follows since ${\\mathcal{G}}_k$ contains all groups of cardinality $k$ and the $\\ell_p$-norms only involve absolute values of the components.\nHence we can define the spectral $(k,p)$-support norm as \n\n", "itemtype": "equation", "pos": 16494, "prevtext": "\n\\iffalse\nIf $p=1$ equality holds if and only if\n\n", "index": 21, "text": "\\begin{equation*}\nw_i =\n\\begin{cases}\n{\\rm sign}(u_i) \\lambda_i & \\text{if } i \\in M\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"w_{i}=\\begin{cases}{\\rm sign}(u_{i})\\lambda_{i}&amp;\\text{if }i\\in M\\\\&#10;0&amp;\\text{otherwise}\\end{cases}\" display=\"block\"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><mi>M</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\n\n\nSince the dual of any orthogonally invariant norm is given by $\\|\\cdot\\|_* = \\Vert \\sigma(\\cdot) \\Vert_*$, see e.g. \\cite{Lewis1995}, we conclude that the dual spectral $(k,p)$-support norm is given by \n\n", "itemtype": "equation", "pos": 17264, "prevtext": "\nwhere $M = \\{i: |u_i| =  \\max_j |u_j|\\}$ and $\\lambda_i {\\ensuremath{\\geqslant}} 0$, $\\sum_{i \\in M} \\lambda_i = 1$. \nfor any $\\lambda \\in [-1,1]$.\n\\fi\n\\end{proposition}\n\nNote that \n\n\nfor $p=2$ we recover the dual of the $k$-support norm in \\eqref{eqn:ksup-dualeq}. \n\n\n\\subsection{The Spectral $(k,p)$-Support Norm}\nFrom Definition \\ref{def:k-sup-p-norm} it is clear that the $(k,p)$-support norm is a symmetric gauge function. \nThis follows since ${\\mathcal{G}}_k$ contains all groups of cardinality $k$ and the $\\ell_p$-norms only involve absolute values of the components.\nHence we can define the spectral $(k,p)$-support norm as \n\n", "index": 23, "text": "\\begin{align*}\n\\Vert W \\Vert_{(k,p)} = \\Vert \\sigma(W) \\Vert_{(k,p)}, ~~~ W \\in {\\mathbb R}^{d \\times m}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|W\\|_{(k,p)}=\\|\\sigma(W)\\|_{(k,p)},~{}~{}~{}W\\in{\\mathbb{R}}^{d%&#10;\\times m}.\" display=\"inline\"><mrow><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>W</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><mo rspace=\"12.4pt\">,</mo><mrow><mi>W</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\nThe next result characterizes the unit ball of the spectral $(k,p)$-support norm.  \nDue to the relationship between an orthogonally invariant norm and its corresponding symmetric gauge function, we see that the cardinality constraint for vectors generalizes in a natural manner to the rank operator for matrices.\n\n\\begin{proposition}\n\\label{prop:unit-ball-of-spectral}\nThe unit ball of the spectral $(k,p)$-support norm is the convex hull of the set of matrices of rank at most $k$ and Schatten $p$-norm no greater than one. \n\\end{proposition}\n\n\nIn particular, if $p=\\infty$, the dual vector norm is given by $u \\in {\\mathbb R}^d$, by $\\Vert u \\Vert_{(k,\\infty),*} = \\sum_{i=1}^k \\vert u \\vert^{\\downarrow}_i$. Hence, for any $Z\\in {\\mathbb R}^{d \\times m}$, the dual spectral norm is given by\n$\\Vert Z \\Vert_{(k,\\infty),*} = \\sum_{i=1}^k  \\sigma_i(Z)$, that is the sum of the $k$ largest singular values, which is also known as the Ky-Fan $k$-norm, see e.g. \\cite{Bhatia1997}.\n\n\n\n\n\\section{Computing the Norm}\n\\label{sec:computing-the-norm}\n\\vspace{-.1truecm}\nIn this section we compute the norm, illustrating how it interpolates between the $\\ell_1$ and $\\ell_p$-norms. \n\n\\begin{theorem}\\label{thm:kp-norm}\nLet $p \\in (1,\\infty)$. For every $w \\in \\mathbb{R}^d$, and $k {\\ensuremath{\\leqslant}} d$, it holds that\n\n", "itemtype": "equation", "pos": 17589, "prevtext": "\n\n\n\nSince the dual of any orthogonally invariant norm is given by $\\|\\cdot\\|_* = \\Vert \\sigma(\\cdot) \\Vert_*$, see e.g. \\cite{Lewis1995}, we conclude that the dual spectral $(k,p)$-support norm is given by \n\n", "index": 25, "text": "\\begin{align*}\n\\Vert Z \\Vert_{(k,p),*} = \\Vert \\sigma(Z) \\Vert_{(k,p),*}, ~~~ Z \\in {\\mathbb R}^{d \\times m}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|Z\\|_{(k,p),*}=\\|\\sigma(Z)\\|_{(k,p),*},~{}~{}~{}Z\\in{\\mathbb{R}}%&#10;^{d\\times m}.\" display=\"inline\"><mrow><mrow><mrow><msub><mrow><mo>\u2225</mo><mi>Z</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub></mrow><mo rspace=\"12.4pt\">,</mo><mrow><mi>Z</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere \n$\\frac{1}{p}+\\frac{1}{q}=1$, \nand for $k=d$, we set $\\ell=d$, otherwise $\\ell$ is the largest integer in $\\{0, \\ldots, k-1\\}$ satisfying\n\n", "itemtype": "equation", "pos": 19028, "prevtext": "\n\nThe next result characterizes the unit ball of the spectral $(k,p)$-support norm.  \nDue to the relationship between an orthogonally invariant norm and its corresponding symmetric gauge function, we see that the cardinality constraint for vectors generalizes in a natural manner to the rank operator for matrices.\n\n\\begin{proposition}\n\\label{prop:unit-ball-of-spectral}\nThe unit ball of the spectral $(k,p)$-support norm is the convex hull of the set of matrices of rank at most $k$ and Schatten $p$-norm no greater than one. \n\\end{proposition}\n\n\nIn particular, if $p=\\infty$, the dual vector norm is given by $u \\in {\\mathbb R}^d$, by $\\Vert u \\Vert_{(k,\\infty),*} = \\sum_{i=1}^k \\vert u \\vert^{\\downarrow}_i$. Hence, for any $Z\\in {\\mathbb R}^{d \\times m}$, the dual spectral norm is given by\n$\\Vert Z \\Vert_{(k,\\infty),*} = \\sum_{i=1}^k  \\sigma_i(Z)$, that is the sum of the $k$ largest singular values, which is also known as the Ky-Fan $k$-norm, see e.g. \\cite{Bhatia1997}.\n\n\n\n\n\\section{Computing the Norm}\n\\label{sec:computing-the-norm}\n\\vspace{-.1truecm}\nIn this section we compute the norm, illustrating how it interpolates between the $\\ell_1$ and $\\ell_p$-norms. \n\n\\begin{theorem}\\label{thm:kp-norm}\nLet $p \\in (1,\\infty)$. For every $w \\in \\mathbb{R}^d$, and $k {\\ensuremath{\\leqslant}} d$, it holds that\n\n", "index": 27, "text": "\\begin{align}\n\\Vert w \\Vert_{(k,p)} =  \\Bigg[ \\sum_{i=1}^\\ell  (\\vert w\\vert^{\\downarrow}_i )^p  + \n \\left( \\frac{ \\sum_{i=\\ell+1}^d \\vert w\\vert^{\\downarrow}_i }{\\sqrt[q]{k-\\ell}}\\right)^p \\Bigg]^{\\frac{1}{p}}\n\n\\label{eq:111kp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|w\\|_{(k,p)}=\\Bigg{[}\\sum_{i=1}^{\\ell}(|w|^{\\downarrow}_{i})^{p}%&#10;+\\left(\\frac{\\sum_{i=\\ell+1}^{d}|w|^{\\downarrow}_{i}}{\\sqrt[q]{k-\\ell}}\\right)%&#10;^{p}\\Bigg{]}^{\\frac{1}{p}}\\par&#10;\" display=\"inline\"><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><msup><mrow><mo maxsize=\"260%\" minsize=\"260%\">[</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant=\"normal\">\u2113</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mi>i</mi><mo>\u2193</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>p</mi></msup></mrow><mo>+</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><msubsup><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mi>i</mi><mo>\u2193</mo></msubsup></mrow><mroot><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mi>q</mi></mroot></mfrac></mstyle><mo>)</mo></mrow><mi>p</mi></msup></mrow><mo maxsize=\"260%\" minsize=\"260%\">]</mo></mrow><mfrac><mn>1</mn><mi>p</mi></mfrac></msup></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nFurthermore, the norm can be computed in $\\mathcal{O}(d \\log d)$ time. \n\\end{theorem}\n\\begin{proof}\nNote first that in \\eqref{eq:111kp} when $\\ell=0$ we understand the first term in the right hand side to be zero, and when $\\ell=d$ we understand the second term to be zero. \n\nWe need to compute\n\n", "itemtype": "equation", "pos": 19413, "prevtext": "\nwhere \n$\\frac{1}{p}+\\frac{1}{q}=1$, \nand for $k=d$, we set $\\ell=d$, otherwise $\\ell$ is the largest integer in $\\{0, \\ldots, k-1\\}$ satisfying\n\n", "index": 29, "text": "\\begin{align}\n(k-\\ell)\\vert w\\vert^{\\downarrow}_{\\ell} {\\ensuremath{\\geqslant}} \\sum_{i=\\ell+1}^d \\vert w\\vert^{\\downarrow}_{i}.\n\\label{eq:222kp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(k-\\ell)|w|^{\\downarrow}_{\\ell}{\\geqslant}\\sum_{i=\\ell+1}^{d}|w|^%&#10;{\\downarrow}_{i}.\" display=\"inline\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mi mathvariant=\"normal\">\u2113</mi><mo>\u2193</mo></msubsup></mrow><mo>\u2a7e</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></munderover></mstyle><msubsup><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mi>i</mi><mo>\u2193</mo></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere the dual norm $\\|\\cdot\\|_{(k,p),*}$ is described in Proposition \\ref{prop:k-sup-p-norm-and-dual}. \nLet $z_i = \\vert w\\vert^{\\downarrow}_i$. The problem is then equivalent to \n\n", "itemtype": "equation", "pos": 19866, "prevtext": "\nFurthermore, the norm can be computed in $\\mathcal{O}(d \\log d)$ time. \n\\end{theorem}\n\\begin{proof}\nNote first that in \\eqref{eq:111kp} when $\\ell=0$ we understand the first term in the right hand side to be zero, and when $\\ell=d$ we understand the second term to be zero. \n\nWe need to compute\n\n", "index": 31, "text": "$$\n\\|w\\|_{(k,p)}=\\max \\left\\{\\sum_{i=1}^d u_i w_i : \\|u\\|_{(k,p),*}{\\ensuremath{\\leqslant}} 1\\right\\}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\|w\\|_{(k,p)}=\\max\\left\\{\\sum_{i=1}^{d}u_{i}w_{i}:\\|u\\|_{(k,p),*}{\\leqslant}1\\right\\}\" display=\"block\"><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mrow><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub><mo>\u2a7d</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nThis further simplifies to the $k$-dimensional problem\n\n", "itemtype": "equation", "pos": 20152, "prevtext": "\nwhere the dual norm $\\|\\cdot\\|_{(k,p),*}$ is described in Proposition \\ref{prop:k-sup-p-norm-and-dual}. \nLet $z_i = \\vert w\\vert^{\\downarrow}_i$. The problem is then equivalent to \n\n", "index": 33, "text": "\\begin{align}\n\\max \\left\\{\\sum_{i=1}^d z_i u_i : \\sum_{i=1}^k u_i^q {\\ensuremath{\\leqslant}} 1, u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_d \\right\\}.\n\\label{eq:333kp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max\\left\\{\\sum_{i=1}^{d}z_{i}u_{i}:\\sum_{i=1}^{k}u_{i}^{q}{%&#10;\\leqslant}1,u_{1}{\\geqslant}\\cdots{\\geqslant}u_{d}\\right\\}.\" display=\"inline\"><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>u</mi><mi>i</mi></msub></mrow></mrow><mo>:</mo><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><msubsup><mi>u</mi><mi>i</mi><mi>q</mi></msubsup></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>\u2a7e</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2a7e</mo><msub><mi>u</mi><mi>d</mi></msub></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nNote that when $k=d$, the solution is given by the dual of the $\\ell_q$-norm, that is the $\\ell_p$-norm.  \nFor the remainder of the proof we assume that $k<d$.\nWe can now attempt to use Holder's inequality, which states that for all vectors $x$ such that $\\|x\\|_q=1$,\n${\\langle} x,y{\\rangle} {\\ensuremath{\\leqslant}} \\|y\\|_p$, and the inequality is tight if and only if\n\n", "itemtype": "equation", "pos": 20406, "prevtext": "\nThis further simplifies to the $k$-dimensional problem\n\n", "index": 35, "text": "\\begin{align*}\n\\max \\left\\{\\sum_{i=1}^{k-1} u_i z_i + u_k \\sum_{i=k}^{d} z_i: \\sum_{i=1}^k u_i^q {\\ensuremath{\\leqslant}} 1, u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_k \\right\\}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max\\left\\{\\sum_{i=1}^{k-1}u_{i}z_{i}+u_{k}\\sum_{i=k}^{d}z_{i}:%&#10;\\sum_{i=1}^{k}u_{i}^{q}{\\leqslant}1,u_{1}{\\geqslant}\\cdots{\\geqslant}u_{k}%&#10;\\right\\}.\" display=\"inline\"><mrow><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mrow><mo>+</mo><mrow><msub><mi>u</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi></mrow><mi>d</mi></munderover></mstyle><msub><mi>z</mi><mi>i</mi></msub></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><msubsup><mi>u</mi><mi>i</mi><mi>q</mi></msubsup></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>u</mi><mn>1</mn></msub><mo>\u2a7e</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2a7e</mo><msub><mi>u</mi><mi>k</mi></msub></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nWe use it for the vector $y = (z_1,\\dots,z_{k-1},\\sum_{i=k}^{d} z_i)$. \nThe components of the maximizer $u$ satisfy $u_i = \\left(\\frac{z_i}{M_{k-1}}\\right)^{p-1}$ if $i{\\ensuremath{\\leqslant}} k-1$, and \n\n", "itemtype": "equation", "pos": 20988, "prevtext": "\nNote that when $k=d$, the solution is given by the dual of the $\\ell_q$-norm, that is the $\\ell_p$-norm.  \nFor the remainder of the proof we assume that $k<d$.\nWe can now attempt to use Holder's inequality, which states that for all vectors $x$ such that $\\|x\\|_q=1$,\n${\\langle} x,y{\\rangle} {\\ensuremath{\\leqslant}} \\|y\\|_p$, and the inequality is tight if and only if\n\n", "index": 37, "text": "\\begin{align*}\nx_i = \\left(\\frac{|y_i|}{\\|y\\|_p}\\right)^{p-1}{\\rm sign}(y_i). \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{i}=\\left(\\frac{|y_{i}|}{\\|y\\|_{p}}\\right)^{p-1}{\\rm sign}(y_{%&#10;i}).\" display=\"inline\"><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><msub><mrow><mo>\u2225</mo><mi>y</mi><mo>\u2225</mo></mrow><mi>p</mi></msub></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere for every $\\ell \\in \\{0,\\dots,k-1\\}$, $M_\\ell$ denotes the r.h.s. in equation \\eqref{eq:111kp}.\nWe then need to verify that the ordering constraints are satisfied. \nThis requires that\n\n", "itemtype": "equation", "pos": 21284, "prevtext": "\nWe use it for the vector $y = (z_1,\\dots,z_{k-1},\\sum_{i=k}^{d} z_i)$. \nThe components of the maximizer $u$ satisfy $u_i = \\left(\\frac{z_i}{M_{k-1}}\\right)^{p-1}$ if $i{\\ensuremath{\\leqslant}} k-1$, and \n\n", "index": 39, "text": "\\begin{align*}\nu_{k} =  \\left(\\frac{\\sum_{i=\\ell+1}^d z_i}{M_{k-1}}\\right)^{p-1}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle u_{k}=\\left(\\frac{\\sum_{i=\\ell+1}^{d}z_{i}}{M_{k-1}}\\right)^{p-1}.\" display=\"inline\"><mrow><mrow><msub><mi>u</mi><mi>k</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi>M</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhich is equivalent to inequality \\eqref{eq:222kp} for $\\ell=k-1$. \nIf this inequality is true we are done, otherwise we set $u_{k} = u_{k-1}$ and solve the smaller problem\n\\begin{eqnarray}\n\\nonumber\n\\max \\bigg\\{\\sum_{i=1}^{k-2} u_i z_i + u_{k-1}\\hspace{-.1truecm} \\sum_{i=k-1}^{d} \\hspace{-.1truecm} z_i~:~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\n~~~~~~~~~~\\sum_{i=1}^{k-2} u_i^q + 2 u_{k-1}^q {\\ensuremath{\\leqslant}} 1,~~~ u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_{k-1} \\bigg\\}.\n\\nonumber\n\\end{eqnarray}\nWe use again H\\\"older's inequality and keep the result if the ordering constraints are fulfilled. \nContinuing in this way, the generic problem we need to solve is\n\\begin{eqnarray}\n\\nonumber\n\\max \\bigg\\{\\sum_{i=1}^{\\ell} u_i z_i + u_{\\ell+1} \\sum_{i=\\ell+1}^{d} z_i~:~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\ \n~~~~~~~~~ \\sum_{i=1}^{\\ell} u_i^q + (k-\\ell) u_{\\ell+1}^q {\\ensuremath{\\leqslant}} 1,~~~ u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_{\\ell+1} \\bigg\\}\n\\nonumber\n\\end{eqnarray}\nwhere $\\ell \\in \\{0,\\dots,k-1\\}$.\nWithout the ordering constraints the maximum, $M_\\ell$, \n\n\nis obtained by the change of variable \n\n$u_{\\ell+1} \\mapsto (k-\\ell)^{\\frac{1}{q}} u_{\\ell}$\n\nfollowed by applying H\\\"older's inequality. \nA direct computation provides that the maximizer is $u_i = \\left(\\frac{z_i}{M_\\ell}\\right)^{p-1}$ if $i {\\ensuremath{\\leqslant}} \\ell$, and \n\n", "itemtype": "equation", "pos": 21569, "prevtext": "\nwhere for every $\\ell \\in \\{0,\\dots,k-1\\}$, $M_\\ell$ denotes the r.h.s. in equation \\eqref{eq:111kp}.\nWe then need to verify that the ordering constraints are satisfied. \nThis requires that\n\n", "index": 41, "text": "\\begin{align*}\n(z_{k-1})^{p-1} {\\ensuremath{\\geqslant}} \\left(\\sum_{i=k}^{d} z_i\\right)^{p-1}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(z_{k-1})^{p-1}{\\geqslant}\\left(\\sum_{i=k}^{d}z_{i}\\right)^{p-1}\" display=\"inline\"><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2a7e</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi></mrow><mi>d</mi></munderover></mstyle><msub><mi>z</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nUsing the relationship $\\frac{1}{p}+\\frac{1}{q}=1$, we can rewrite this as\n\n", "itemtype": "equation", "pos": 23053, "prevtext": "\nwhich is equivalent to inequality \\eqref{eq:222kp} for $\\ell=k-1$. \nIf this inequality is true we are done, otherwise we set $u_{k} = u_{k-1}$ and solve the smaller problem\n\\begin{eqnarray}\n\\nonumber\n\\max \\bigg\\{\\sum_{i=1}^{k-2} u_i z_i + u_{k-1}\\hspace{-.1truecm} \\sum_{i=k-1}^{d} \\hspace{-.1truecm} z_i~:~~~~~~~~~~~~~~~~~~~~~~~~ \\\\\n~~~~~~~~~~\\sum_{i=1}^{k-2} u_i^q + 2 u_{k-1}^q {\\ensuremath{\\leqslant}} 1,~~~ u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_{k-1} \\bigg\\}.\n\\nonumber\n\\end{eqnarray}\nWe use again H\\\"older's inequality and keep the result if the ordering constraints are fulfilled. \nContinuing in this way, the generic problem we need to solve is\n\\begin{eqnarray}\n\\nonumber\n\\max \\bigg\\{\\sum_{i=1}^{\\ell} u_i z_i + u_{\\ell+1} \\sum_{i=\\ell+1}^{d} z_i~:~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\ \n~~~~~~~~~ \\sum_{i=1}^{\\ell} u_i^q + (k-\\ell) u_{\\ell+1}^q {\\ensuremath{\\leqslant}} 1,~~~ u_1{\\ensuremath{\\geqslant}} \\cdots {\\ensuremath{\\geqslant}} u_{\\ell+1} \\bigg\\}\n\\nonumber\n\\end{eqnarray}\nwhere $\\ell \\in \\{0,\\dots,k-1\\}$.\nWithout the ordering constraints the maximum, $M_\\ell$, \n\n\nis obtained by the change of variable \n\n$u_{\\ell+1} \\mapsto (k-\\ell)^{\\frac{1}{q}} u_{\\ell}$\n\nfollowed by applying H\\\"older's inequality. \nA direct computation provides that the maximizer is $u_i = \\left(\\frac{z_i}{M_\\ell}\\right)^{p-1}$ if $i {\\ensuremath{\\leqslant}} \\ell$, and \n\n", "index": 43, "text": "\\begin{align*}\n(k-\\ell)^\\frac{1}{q}u_{\\ell+1} =  \\left(\\frac{\\sum_{i=\\ell+1}^d z_i}{(k-\\ell)^\\frac{1}{q} M_\\ell^p}\\right)^{p-1}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(k-\\ell)^{\\frac{1}{q}}u_{\\ell+1}=\\left(\\frac{\\sum_{i=\\ell+1}^{d}z%&#10;_{i}}{(k-\\ell)^{\\frac{1}{q}}M_{\\ell}^{p}}\\right)^{p-1}.\" display=\"inline\"><mrow><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mn>1</mn><mi>q</mi></mfrac></msup><mo>\u2062</mo><msub><mi>u</mi><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><msub><mi>z</mi><mi>i</mi></msub></mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mfrac><mn>1</mn><mi>q</mi></mfrac></msup><mo>\u2062</mo><msubsup><mi>M</mi><mi mathvariant=\"normal\">\u2113</mi><mi>p</mi></msubsup></mrow></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nHence, the ordering constraints are satisfied if \n\n", "itemtype": "equation", "pos": 23270, "prevtext": "\nUsing the relationship $\\frac{1}{p}+\\frac{1}{q}=1$, we can rewrite this as\n\n", "index": 45, "text": "\\begin{align*}\nu_{\\ell+1} = \\left(\\frac{\\sum_{i=\\ell+1}^d z_i}{(k-\\ell) M_\\ell^p}\\right)^{p-1}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle u_{\\ell+1}=\\left(\\frac{\\sum_{i=\\ell+1}^{d}z_{i}}{(k-\\ell)M_{\\ell%&#10;}^{p}}\\right)^{p-1}.\" display=\"inline\"><mrow><mrow><msub><mi>u</mi><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><msub><mi>z</mi><mi>i</mi></msub></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>M</mi><mi mathvariant=\"normal\">\u2113</mi><mi>p</mi></msubsup></mrow></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": " \nwhich is equivalent to \\eqref{eq:222kp}.\nFinally note that \n\n$M_\\ell$ is a nondecreasing function of $\\ell$. \nThis is because the problem with a smaller value of $\\ell$ is more constrained, namely, it solves \\eqref{eq:333kp} with the additional constraints $u_{\\ell+1} = \\cdots = u_d$. \nMoreover, if the constraint \\eqref{eq:222kp} holds for some value $\\ell \\in \\{0,\\dots,k-1\\}$ then it also holds for a smaller value of $\\ell$, hence we maximize the objective by choosing the largest $\\ell$. \n\nThe computational complexity stems from using the monotonicity of $M_{\\ell}$ with respect to $\\ell$, which allows us to identify the critical value of $\\ell$ using binary search. \n\\end{proof}\n\n\n\n\n\nNote that for $k=d$ we recover the $\\ell_p$-norm and for $p =2$ we recover the result in \\cite{Argyriou2012,McDonald2014a}, however our proof technique is different from theirs.\n\n\n\n\n\\begin{remark}[Computation of the norm for $p \\in \\{1,\\infty\\}$] \nSince the norm $\\|\\cdot\\|_{(k,p)}$ computed above for $p \\in (1,\\infty)$ is continuous in $p$, the special cases $p=1$ and $p=\\infty$ can be derived by a limiting argument. We readily see that for $p=1$ the norm does not depend on $k$ and it is always equal to the $\\ell_1$-norm, in agreement with our observation in the previous section. \nFor $p = \\infty$ we obtain that\n\n$\\Vert w \\Vert_{(k,\\infty)} = \\max \\left(\\|w\\|_\\infty, {\\|w\\|_1}/{k}\\right)$.\n\n\\end{remark}\n\n\n\n\n\n\\section{Optimization}\n\\label{sec:optimization}\n\\vspace{-.1truecm}\nIn this section, we describe how to solve regularization problems using the vector and matrix $(k,p)$-support norms. We consider the constrained optimization problem \n\n", "itemtype": "equation", "pos": -1, "prevtext": "\nHence, the ordering constraints are satisfied if \n\n", "index": 47, "text": "\\begin{align*}\nz_\\ell^{p-1} {\\ensuremath{\\geqslant}} \\left(\\frac{\\sum_{i=\\ell+1}^d z_i}{(k-\\ell)}\\right)^{p-1},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle z_{\\ell}^{p-1}{\\geqslant}\\left(\\frac{\\sum_{i=\\ell+1}^{d}z_{i}}{(%&#10;k-\\ell)}\\right)^{p-1},\" display=\"inline\"><mrow><mrow><msubsup><mi>z</mi><mi mathvariant=\"normal\">\u2113</mi><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2a7e</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><msub><mi>z</mi><mi>i</mi></msub></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>-</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mfrac></mstyle><mo>)</mo></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $w$ is in ${\\mathbb R}^d$ or ${\\mathbb R}^{d \\times m}$, $\\alpha >0$ is a regularization parameter and the error function $f$ is assumed to be convex and continuously differentiable. \nFor example, in linear regression a valid choice is the square error, $f(w) = \\|Xw-y\\|_2^2$, where $X$ is matrix of observations and $y$ a vector of response variables. \nConstrained problems of form \\eqref{eqn:ivanov} are also referred to as Ivanov regularization in the inverse problems literature \\cite{Ivanov1978}. \n\nA convenient tool to solve problem \\eqref{eqn:ivanov} is provided by the \\emph{Frank-Wolfe} method \\cite{Frank1956}, see also \\cite{Jaggi2013} for a recent account. \n\\begin{algorithm}[t]\n\\caption{Frank-Wolfe. \\label{alg:FW}}\n\\begin{algorithmic} \n\n\\STATE Choose $w^{(0)}$ such that $\\Vert w^{(0)} \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha$\n\\FOR{$t=0, \\ldots, T$ } \\STATE{\nCompute $g:= \\nabla f(w^{(t)})$ \\\\\nCompute $s:= {\\ensuremath{\\text{\\rm argmin}\\,}} \\left\\{ \\langle s, g)\\rangle: \\Vert s \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\right\\}$  \\\\ \nUpdate $w^{(t+1)} := (1-\\gamma) w^{(t)} + \\gamma s$, for $\\gamma := \\frac{2}{t+2}$}\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\nThe method is outlined in Algorithm \\ref{alg:FW}, and it has worst case convergence rate $\\mathcal{O}(1/T)$.\n\nThe key step of the algorithm is to solve the subproblem\n\n", "itemtype": "equation", "pos": 25200, "prevtext": " \nwhich is equivalent to \\eqref{eq:222kp}.\nFinally note that \n\n$M_\\ell$ is a nondecreasing function of $\\ell$. \nThis is because the problem with a smaller value of $\\ell$ is more constrained, namely, it solves \\eqref{eq:333kp} with the additional constraints $u_{\\ell+1} = \\cdots = u_d$. \nMoreover, if the constraint \\eqref{eq:222kp} holds for some value $\\ell \\in \\{0,\\dots,k-1\\}$ then it also holds for a smaller value of $\\ell$, hence we maximize the objective by choosing the largest $\\ell$. \n\nThe computational complexity stems from using the monotonicity of $M_{\\ell}$ with respect to $\\ell$, which allows us to identify the critical value of $\\ell$ using binary search. \n\\end{proof}\n\n\n\n\n\nNote that for $k=d$ we recover the $\\ell_p$-norm and for $p =2$ we recover the result in \\cite{Argyriou2012,McDonald2014a}, however our proof technique is different from theirs.\n\n\n\n\n\\begin{remark}[Computation of the norm for $p \\in \\{1,\\infty\\}$] \nSince the norm $\\|\\cdot\\|_{(k,p)}$ computed above for $p \\in (1,\\infty)$ is continuous in $p$, the special cases $p=1$ and $p=\\infty$ can be derived by a limiting argument. We readily see that for $p=1$ the norm does not depend on $k$ and it is always equal to the $\\ell_1$-norm, in agreement with our observation in the previous section. \nFor $p = \\infty$ we obtain that\n\n$\\Vert w \\Vert_{(k,\\infty)} = \\max \\left(\\|w\\|_\\infty, {\\|w\\|_1}/{k}\\right)$.\n\n\\end{remark}\n\n\n\n\n\n\\section{Optimization}\n\\label{sec:optimization}\n\\vspace{-.1truecm}\nIn this section, we describe how to solve regularization problems using the vector and matrix $(k,p)$-support norms. We consider the constrained optimization problem \n\n", "index": 49, "text": "\\begin{align}\n\\min \\left\\{ f(w) : \\Vert w \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\right\\} \\label{eqn:ivanov},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\left\\{f(w):\\|w\\|_{(k,p)}{\\leqslant}\\alpha\\right\\},\" display=\"inline\"><mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2a7d</mo><mi>\u03b1</mi></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $g=\\nabla f(w^{(t)})$, that is the gradient of the objective function at the $t$-th iteration. This problem involves computing a subgradient of the dual norm at $g$. \nIt can be solved exactly and efficiently as a consequence of Proposition \\ref{prop:k-sup-p-norm-and-dual}. \nWe discuss here the vector case and postpone the discussion of the matrix case to Section \\ref{sec:MP}. \nBy symmetry of the $\\ell_p$-norm, problem \\eqref{eq:keystep} can be solved in the same manner as the maximum in Proposition \\ref{prop:k-sup-p-norm-and-dual}, and the solution is given by $s_i = -\\alpha w_i$, where $w_i$ is given by \\eqref{eqn:dual-component}. \nSpecifically, letting ${I_k} \\subset {\\mathbb{N}_{{d}}}$ be the set of indices of the $k$ largest components of $g$ in absolute value, for $p \\in (1,\\infty)$ we have \n\n", "itemtype": "equation", "pos": 26689, "prevtext": "\nwhere $w$ is in ${\\mathbb R}^d$ or ${\\mathbb R}^{d \\times m}$, $\\alpha >0$ is a regularization parameter and the error function $f$ is assumed to be convex and continuously differentiable. \nFor example, in linear regression a valid choice is the square error, $f(w) = \\|Xw-y\\|_2^2$, where $X$ is matrix of observations and $y$ a vector of response variables. \nConstrained problems of form \\eqref{eqn:ivanov} are also referred to as Ivanov regularization in the inverse problems literature \\cite{Ivanov1978}. \n\nA convenient tool to solve problem \\eqref{eqn:ivanov} is provided by the \\emph{Frank-Wolfe} method \\cite{Frank1956}, see also \\cite{Jaggi2013} for a recent account. \n\\begin{algorithm}[t]\n\\caption{Frank-Wolfe. \\label{alg:FW}}\n\\begin{algorithmic} \n\n\\STATE Choose $w^{(0)}$ such that $\\Vert w^{(0)} \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha$\n\\FOR{$t=0, \\ldots, T$ } \\STATE{\nCompute $g:= \\nabla f(w^{(t)})$ \\\\\nCompute $s:= {\\ensuremath{\\text{\\rm argmin}\\,}} \\left\\{ \\langle s, g)\\rangle: \\Vert s \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\right\\}$  \\\\ \nUpdate $w^{(t+1)} := (1-\\gamma) w^{(t)} + \\gamma s$, for $\\gamma := \\frac{2}{t+2}$}\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\nThe method is outlined in Algorithm \\ref{alg:FW}, and it has worst case convergence rate $\\mathcal{O}(1/T)$.\n\nThe key step of the algorithm is to solve the subproblem\n\n", "index": 51, "text": "\\begin{equation}\n{\\ensuremath{\\text{\\rm argmin}\\,}} \\left\\{ \\langle s, g\\rangle:\\Vert s \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\right\\},\n\\label{eq:keystep}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"{\\text{\\rm argmin}\\,}\\left\\{\\langle s,g\\rangle:\\|s\\|_{(k,p)}{\\leqslant}\\alpha%&#10;\\right\\},\" display=\"block\"><mrow><mrow><mpadded width=\"+1.7pt\"><mtext>argmin</mtext></mpadded><mo>\u2062</mo><mrow><mo>{</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>s</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>s</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2a7d</mo><mi>\u03b1</mi></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nand, for $p = \\infty$ we choose the subgradient\n\n", "itemtype": "equation", "pos": 27681, "prevtext": "\nwhere $g=\\nabla f(w^{(t)})$, that is the gradient of the objective function at the $t$-th iteration. This problem involves computing a subgradient of the dual norm at $g$. \nIt can be solved exactly and efficiently as a consequence of Proposition \\ref{prop:k-sup-p-norm-and-dual}. \nWe discuss here the vector case and postpone the discussion of the matrix case to Section \\ref{sec:MP}. \nBy symmetry of the $\\ell_p$-norm, problem \\eqref{eq:keystep} can be solved in the same manner as the maximum in Proposition \\ref{prop:k-sup-p-norm-and-dual}, and the solution is given by $s_i = -\\alpha w_i$, where $w_i$ is given by \\eqref{eqn:dual-component}. \nSpecifically, letting ${I_k} \\subset {\\mathbb{N}_{{d}}}$ be the set of indices of the $k$ largest components of $g$ in absolute value, for $p \\in (1,\\infty)$ we have \n\n", "index": 53, "text": "\\begin{equation}\n\\label{eq:FWp}\ns_i = \n\\begin{cases}\n- \\alpha \\, {\\rm sign}(g_i) \\left(\\frac{g_i}{\\|g\\|_{(k,p),*}}\\right)^{\\frac{1}{p-1}}, \\quad & \\text{if } i \\in {I_k} \\\\\n0, & \\text{if } i \\notin {I_k}\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"s_{i}=\\begin{cases}-\\alpha\\,{\\rm sign}(g_{i})\\left(\\frac{g_{i}}{\\|g\\|_{(k,p),*%&#10;}}\\right)^{\\frac{1}{p-1}},&amp;\\text{if }i\\in{I_{k}}\\\\&#10;0,&amp;\\text{if }i\\notin{I_{k}}\\end{cases}\" display=\"block\"><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo>-</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo>(</mo><mstyle displaystyle=\"false\"><mfrac><msub><mi>g</mi><mi>i</mi></msub><msub><mrow><mo>\u2225</mo><mi>g</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub></mfrac></mstyle><mo>)</mo></mrow><mfrac><mn>1</mn><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></mfrac></msup></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2209</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\n\n\n\n\\subsection{Projection Operator}\n\\label{sec:prox}\n\nAn alternative method to solve \\eqref{eqn:ivanov} in the vector case is to consider the equivalent problem\n\n", "itemtype": "equation", "pos": 27960, "prevtext": "\nand, for $p = \\infty$ we choose the subgradient\n\n", "index": 55, "text": "\\begin{equation}\n\\label{eq:FWinf}\ns_i =\n\\begin{cases}\n- \\alpha \\, {\\rm sign}(g_i) & \\text{if } i \\in {I_k},~g_i \\neq 0,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"s_{i}=\\begin{cases}-\\alpha\\,{\\rm sign}(g_{i})&amp;\\text{if }i\\in{I_{k}},~{}g_{i}%&#10;\\neq 0,\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mo>-</mo><mrow><mpadded width=\"+1.7pt\"><mi>\u03b1</mi></mpadded><mo>\u2062</mo><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><mo rspace=\"5.8pt\">,</mo><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>\u2260</mo><mn>0</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $\\delta_C(\\cdot)$ is the indicator function of convex set $C$. \n\nProximal gradient methods can be used to solve optimization problems of the form $\\min \\left\\{ f(w) + \\lambda g(w): w \\in {\\mathbb R}^d\\right\\}$, where $f$ is a convex loss function with Lipschitz continuous gradient, $\\lambda >0$ is a regularization parameter, and $g$ is a convex function for which the proximity operator can be computed efficiently see \\cite{Beck2009,Nesterov2007} and references therein. \n\nThe proximity operator of $g$ with parameter $\\rho>0$ is defined as ${\\ensuremath{\\text{\\rm prox}}}_{\\rho g} (w) = {\\ensuremath{\\text{\\rm argmin}\\,}} \\{\\frac{1}{2} \\Vert x-w\\Vert^2 + \\rho g(x) : x \\in {\\mathbb R}^d \\}$. The proximity operator for the squared $k$-support norm was computed by \\cite{Argyriou2012} and \\cite{McDonald2014a}, and for the $k$-support norm by \\cite{Chatterjee2014}. \n\n\nIn the special case that $g(w) = \\delta_C(w)$, where $C$ is a convex set, the proximity operator reduces to the projection operator onto $C$.  \nFor the $(k,p)$-support norm, for the case $p=\\infty$ we can compute the projection onto its unit ball using the following result.\n\n\\begin{proposition}\n\\label{prop:projection-to-k-infinity-unit-ball}\nFor every $w\\in {\\mathbb R}^d$, the projection $x$ of $w$ onto the unit ball of the $(k,\\infty$)-norm is given by \n\n", "itemtype": "equation", "pos": 28292, "prevtext": "\n\n\n\n\\subsection{Projection Operator}\n\\label{sec:prox}\n\nAn alternative method to solve \\eqref{eqn:ivanov} in the vector case is to consider the equivalent problem\n\n", "index": 57, "text": "\\begin{align}\n\\min \\left\\{ f(w) +\\delta_{ \\{ \\Vert \\cdot \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\} } (w): w \\in {\\mathbb R}^d \\right\\} \\label{eqn:tikhonov},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min\\left\\{f(w)+\\delta_{\\{\\|\\cdot\\|_{(k,p)}{\\leqslant}\\alpha\\}}(w%&#10;):w\\in{\\mathbb{R}}^{d}\\right\\},\" display=\"inline\"><mrow><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b4</mi><mrow><mo stretchy=\"false\">{</mo><mo>\u2225</mo><mo>\u22c5</mo><msub><mo>\u2225</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2a7d</mo><mi>\u03b1</mi><mo stretchy=\"false\">}</mo></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mi>w</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $\\beta=0$ if $\\Vert w \\Vert_1 {\\ensuremath{\\leqslant}} k$, otherwise $\\beta \\in (0,\\infty)$ is chosen such that $\\sum_{i=1}^d \\vert x_i \\vert = k$.\nFurthermore, the projection can be computed in $\\mathcal{O}(d \\log d)$ time.\n\\end{proposition}\n\n\n\\begin{proof}(Sketch)\nWe solve the optimization problem\n\n", "itemtype": "equation", "pos": 29805, "prevtext": "\nwhere $\\delta_C(\\cdot)$ is the indicator function of convex set $C$. \n\nProximal gradient methods can be used to solve optimization problems of the form $\\min \\left\\{ f(w) + \\lambda g(w): w \\in {\\mathbb R}^d\\right\\}$, where $f$ is a convex loss function with Lipschitz continuous gradient, $\\lambda >0$ is a regularization parameter, and $g$ is a convex function for which the proximity operator can be computed efficiently see \\cite{Beck2009,Nesterov2007} and references therein. \n\nThe proximity operator of $g$ with parameter $\\rho>0$ is defined as ${\\ensuremath{\\text{\\rm prox}}}_{\\rho g} (w) = {\\ensuremath{\\text{\\rm argmin}\\,}} \\{\\frac{1}{2} \\Vert x-w\\Vert^2 + \\rho g(x) : x \\in {\\mathbb R}^d \\}$. The proximity operator for the squared $k$-support norm was computed by \\cite{Argyriou2012} and \\cite{McDonald2014a}, and for the $k$-support norm by \\cite{Chatterjee2014}. \n\n\nIn the special case that $g(w) = \\delta_C(w)$, where $C$ is a convex set, the proximity operator reduces to the projection operator onto $C$.  \nFor the $(k,p)$-support norm, for the case $p=\\infty$ we can compute the projection onto its unit ball using the following result.\n\n\\begin{proposition}\n\\label{prop:projection-to-k-infinity-unit-ball}\nFor every $w\\in {\\mathbb R}^d$, the projection $x$ of $w$ onto the unit ball of the $(k,\\infty$)-norm is given by \n\n", "index": 59, "text": "\\begin{align}\nx_i = \n\\begin{cases}\n{\\rm sign}(w_i) (\\vert w_i \\vert -\\beta),\t &\\text{if } \\vert  \\vert  w_i \\vert -\\beta \\vert {\\ensuremath{\\leqslant}} 1,\\\\\n{\\rm sign}(w_i), \t\t &\\text{if } \\vert  \\vert  w_i \\vert - \\beta \\vert  > 1,\n\\end{cases}\\label{eqn:pinf-prox}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{i}=\\begin{cases}{\\rm sign}(w_{i})(|w_{i}|-\\beta),&amp;\\text{if }|%&#10;|w_{i}|-\\beta|{\\leqslant}1,\\\\&#10;{\\rm sign}(w_{i}),&amp;\\text{if }||w_{i}|-\\beta|&gt;1,\\end{cases}\" display=\"inline\"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>sign</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&gt;</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nWe consider two cases.  If $\\sum_{i=1}^d \\vert w_i \\vert {\\ensuremath{\\leqslant}} k$, then the problem decouples and we solve it componentwise.  \nIf $\\sum_{i=1}^d \\vert w_i \\vert > k$, we solve problem \\eqref{eqn:projection-problem-body} by minimizing the Lagrangian function $\\mathcal{L}(x,\\beta) = \\sum_{i=1}^d (x_i-w_i)^2 +2 \\beta ( \\sum_{i=1}^d \\vert x_i \\vert - k)$ with nonnegative multiplier $\\beta$. \nThis can be done componentwise, and at the optimum the constraint $\\sum_{i=1}^d  \\vert x_i \\vert {\\ensuremath{\\leqslant}}  k$ will be tight. \nFinally, both cases can be combined into the form of \\eqref{eqn:pinf-prox}.\nThe complexity follows by taking advantage of the monotonicity of $x_i(\\beta)$. \n\\end{proof}\nWe can use Proposition \\ref{prop:projection-to-k-infinity-unit-ball} to project onto the unit ball of radius $\\alpha>0$ by a rescaling argument (see the appendix for details). \n\n\n\\subsection{Matrix Problem}\n\\label{sec:MP}\nGiven data matrix $X \\in {\\mathbb R}^{d \\times m}$ for which we observe a subset of entries, \n\nwe consider the constrained optimization problem\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 30390, "prevtext": "\nwhere $\\beta=0$ if $\\Vert w \\Vert_1 {\\ensuremath{\\leqslant}} k$, otherwise $\\beta \\in (0,\\infty)$ is chosen such that $\\sum_{i=1}^d \\vert x_i \\vert = k$.\nFurthermore, the projection can be computed in $\\mathcal{O}(d \\log d)$ time.\n\\end{proposition}\n\n\n\\begin{proof}(Sketch)\nWe solve the optimization problem\n\n", "index": 61, "text": "\\begin{align}\n\\min_{x \\in {\\mathbb R}^d}  \\left\\{   \\sum_{i=1}^d (x_i-w_i)^2: \\vert x_i \\vert {\\ensuremath{\\leqslant}} 1, \\sum_{i=1}^d \\vert x_i \\vert {\\ensuremath{\\leqslant}} k     \\right\\}.\\label{eqn:projection-problem-body}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{x\\in{\\mathbb{R}}^{d}}\\left\\{\\sum_{i=1}^{d}(x_{i}-w_{i})^{2}%&#10;:|x_{i}|{\\leqslant}1,\\sum_{i=1}^{d}|x_{i}|{\\leqslant}k\\right\\}.\" display=\"inline\"><mrow><mrow><munder><mi>min</mi><mrow><mi>x</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></munder><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>:</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere the operator $\\Omega$ applied to a matrix sets unobserved values to zero.\nAs in the vector case, the Frank-Wolfe method can be applied to the matrix problems. \nAlgorithm \\ref{alg:FW} is particularly convenient in this case as we only need to compute the largest $k$ singular values, which can result in a computationally efficient algorithm. \nThe result is a direct consequence of Proposition \\ref{prop:k-sup-p-norm-and-dual} and von Neumann's trace inequality, see e.g. \\cite[Ch. 9 Sec. H.1.h]{Marshall1979}. \nWe obtain that the solution of the inner minimization step is\n$\nU_k {\\rm diag}(s) V_k{^{\\scriptscriptstyle \\top}}\n$\nwhere $U_k$ and $V_k$ are the top $k$ left and right singular vectors of the gradient $G$ of the objective function in \\eqref{eqn:matrix-optimization} evaluated at the current solution, whose singular values we denote by $g$, and $s$ is obtained from $g$ as per equations \\eqref{eq:FWp} and \\eqref{eq:FWinf}, for $p\\in (1,\\infty)$ and $p=\\infty$, respectively.\n\nNote also that the proximity operator of the norm and the Euclidean projection on the associated unit ball both require the full singular value decomposition to be performed. Indeed, the proximity operator of an orthogonally invariant norm $\\Vert \\cdot \\Vert = g( \\sigma(\\cdot))$ at $W \\in \\mathbb{R}^{d \\times m}$ is given by $\n{\\ensuremath{\\text{\\rm prox}}}_{\\Vert \\cdot \\Vert} (W)= U \\text{diag}({\\ensuremath{\\text{\\rm prox}}}_{g}(\\sigma(W))) V{^{\\scriptscriptstyle \\top}}$, where $U$ and $V$ are the matrices formed by the left and right singular vectors of $W$, see e.g. \\cite[Prop. 3.1]{Argyriou2011}, and this requires the full decomposition. \n\n\n\\begin{table}[t!]\n\\caption{Matrix completion on rank 5 matrix with flat spectrum. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is considerable (statistically significant at a level $<0.001$). }\n\\vskip 0.1in\n\\label{table:synthetic-flat-p-infinity}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[t]{0.98\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                           & $k$ & $p$  \\\\\n\\midrule\nrank 5       & trace  & 0.8415 (0.03)                         & -   &-  \\\\ \n$\\rho$=10\\%  & k-supp  & 0.8343 (0.03)                          & 3.3 & - \\\\ \n             & kp-supp  & 0.8108 (0.05)                          & 5.0 & $\\infty$ \\\\ \n\\midrule\nrank 5       & trace  & 0.6161 (0.03)                          &  -  & - \\\\ \n$\\rho$=15\\%  & k-supp  & 0.6129 (0.03)                          & 3.3 & - \\\\ \n             & kp-supp  & 0.4262 (0.04)                          & 5.0 & $\\infty$ \\\\ \n\\midrule\nrank 5       & trace  & 0.4453 (0.03)             \t            &-  & -   \\\\ \n$\\rho$=20\\%  & k-supp  & 0.4436 (0.02)                          & 3.5 & - \\\\ \n             & kp-supp  & 0.2425 (0.02)            \t            & 5.0 & $\\infty$  \\\\ \n\\midrule\nrank 5       & trace  & 0.1968 (0.01)             \t            &-  & -   \\\\ \n$\\rho$=30\\%  & k-supp  & 0.1838 (0.01)                          & 5.0 & - \\\\ \n             & kp-supp  & 0.0856 (0.01)            \t            & 5.0 & $\\infty$  \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\n\n\n\n\n\\section{Numerical Experiments}\n\\label{sec:experiments}\n\\vspace{-.1truecm}\n\nIn this section we apply the spectral $(k,p)$-support norm to matrix completion (collaborative filtering),   \nin which we want to recover a low rank, or approximately low rank, matrix from a small sample of its entries, see e.g. \\cite{Jaggi2010}.\n\nOne prominent method of solving this problem is trace norm regularization: we look for a matrix which closely fits the observed entries and has a small trace norm (sum of singular values) \\cite{Jaggi2010,Mazumder2010,Toh2011}. \n\nWe apply the $(k,p)$-support norm to this framework and we investigate the impact of varying $p$.  \nNext we compare the spectral $(k,p)$-support norm to the trace norm and the spectral $k$-support norm ($p=2$) in both synthetic and real datasets.\nIn each case we solve the optimization problem \\eqref{eqn:matrix-optimization} using the Frank-Wolfe method as outlined in Section \\ref{sec:optimization}.\n\nWe determine the values of $k$ and $p {\\ensuremath{\\geqslant}} 1$ by validation, averaged over a number of trials. Specifically, we choose the optimal $p$, $k$, as well as the regularization parameter $\\alpha$ by validation over a grid.\nWe let alpha vary in $10^0$ to $10^5$ with step $10^{0.25}$, we let $p$ vary over $20$ values from $1$ to $50,000$, plus $p=\\infty$, and vary $k$ from $1$ to $20$. Our code is available from {\\em http://www0.cs.ucl.ac.uk/staff/M.Pontil/software.html}.\n\n\\begin{figure}[t]\n\\vskip 0.2in\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{pValueVSdecay.pdf}\n}\n\\caption{Optimal $p$ vs. decay $a$.}\n\\label{fig:p-vs-decay-synthetic}\n\\end{center}\n\\vskip -0.2in\n\\end{figure}\n\n\n\\begin{figure}[t]\n\\vskip 0.2in\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{spectrumVSp-fontStyle_bold-fontSize_22-noLabel-withLegend-withLines.pdf}\n}\n\\caption{Optimal $p$ fitted to Matrix spectra with various decays.}\n\\label{fig:p-vs-decay-real}\n\\end{center}\n\\vskip -0.2in\n\\end{figure}\n\n\n\\noindent {\\bf Impact of $p$.}\nA key motivation for the additional parameter $p$ is that it allows us to tune the norm to the decay of the singular values of the underlying matrix. \nIn particular the variational formulation of \\eqref{eqn:kp-sup-infimum-convolution} suggests that as the spectrum of the true low rank matrix flattens out, larger values of $p$ should be preferred. \n  \nWe ran the method on a set of $100 \\times 100$ matrices of rank 12, with decay of the non zero singular values $\\sigma_\\ell$ proportional to $\\exp(-\\ell a)$, \nfor 26 values of $a$ between $10^{-6}$ and $0.18$, and we determined the corresponding optimal value of $p$.\nFigure \\ref{fig:p-vs-decay-synthetic} illustrates the optimum value of $p$ as a function of $a$. \nWe clearly observe the negative slope, that is the steeper the slope the smaller the optimal value of $p$.  \nFigure \\ref{fig:p-vs-decay-real} shows the spectrum and the optimal $p$ for several decay values. \n \nNote that $k$ is never equal to 1, which is a special case in which the norm is independent of $p$, and is equal to the trace norm. \nIn each case the improvement of the spectral $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. \n\nFigure \\ref{fig:error-vs-p-synthetic} illustrates the impact of the curvature $p$ on the test error on synthetic and real datasets.  \nWe observe that the error levels off as $p$ tends to infinity, so for these specific datasets the major gain is to be had for small values of $p$.  \nThe optimum value of $p$ for both the real and synthetic datasets is statistically different from $p=2$ ($k$-support norm), and $p=1$ (trace norm).\n\n \n\n\n\\begin{table}[t!]\n\\caption{\nMatrix completion on real datasets. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. }\n\\vskip 0.1in\n\\label{table:real-data}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[th]{0.65\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                         & $k$ & $p$  \\\\\n\\midrule\nMovieLens       & trace  & 0.2017                       & -   & -  \\\\ \n100k  \t\t& k-supp  & 0.1990                          & 1.9  & - \\\\ \n             & kp-supp  & 0.1921                           & 2.0 & $\\infty$ \\\\ \n\\midrule\nJester 1      & trace  & 0.1752             \t            &-  & -   \\\\ \n  \t\t\t& k-supp  & 0.1739             \t            & 6.4 & -   \\\\ \n\t\t\t& kp-supp & 0.1744 & 2.0 & $\\infty$  \\\\\n             & kp-supp  & 0.1731             \t            & 2.0 & 6.5  \\\\ \n\\midrule\nJester 3      & trace  & 0.1959              \t            &  -  & - \\\\ \n  \t\t\t& k-supp  & 0.1942                          &  2.1 & - \\\\ \n             & kp-supp  & 0.1841                          & 2.0 & $\\infty$ \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\\noindent {\\bf Simulated Data.}\nNext we compared the performance of the $(k,p)$-support norm to that of the $k$-support norm and the trace norm for a matrix with flat spectrum.  As outlined above, as the spectrum of the true low rank matrix flattens out, larger values of $p$ should be preferred. \nEach $100 \\times 100$ matrix is generated as $W = A S B{^{\\scriptscriptstyle \\top}} + E$, where $U$ and $V$ are the singular vectors of the matrix $U V{^{\\scriptscriptstyle \\top}}$, where $U,V \\in \\mathbb{R}^{100\\times 5}$, the entries of $U$, $V$ and $E$ are i.i.d. standard Gaussian, and $S$ is diagonal with 5 non zero constant entries. \nTable \\ref{table:synthetic-flat-p-infinity} illustrates the performance of the norms on a synthetic dataset of rank 5, with identical singular values, that is a flat spectrum.  \nIn each regime the case $p=\\infty$ outperforms the other norms by a substantial margin, with statistical significance at a level $<0.001$. \nWe followed the framework of \\cite{McDonald2014a}  and use $\\rho$ to denote the percentage of the data to use in the training set. \nWe further replicated the setting of \\cite{McDonald2014a} for synthetic matrix completion, and found that the $(k,p)$-support norm outperformed the standard $k$-support norm, as well as the trace norm, at a statistically significant level (see Table \\ref{table:synthetic-mc} in the appendix for details). \n\n\nWe note that although Frank-Wolfe method for the $(k,p)$-support norm does not generally converge as quickly as proximal methods (which are available in the case of $k$-support norm \\cite{McDonald2014,McDonald2014a,Chatterjee2014}), the computational cost can be mitigated using the continuation method.  Specifically given an ordered sequence of parameter values for $p$ we can proceed sequentially, initializing its value based on the previously computed value.  Empirically we tried this approach for a series of 30 values of $p$ and found that the total computation time increased only moderately. \n\n\n\n\n\n\n\n\\noindent {\\bf Real Data.} Finally, we applied the norms to real collaborative filtering datasets.\nWe observe a subset of the (user, rating) entries of a matrix and predict the unobserved ratings, with the assumption that the true matrix is likely to have low rank. \nWe report on the MovieLens 100k dataset ({\\em http://grouplens.org/datasets/movielens/}), which consists\nof ratings of movies, and\n\n\nthe Jester 1 and 3 datasets ({\\em http://goldberg.berkeley.edu/jester-data/}), which consist of \nratings of jokes. \n\nWe followed the experimental protocol in \\cite{McDonald2014a,Toh2011},\n\n\n\nusing normalized mean absolute error \\cite{Toh2011}, and we implemented a final thresholding step as in \\cite{McDonald2014a} (see the appendix for further details).\n\n\n\n\nThe results are outlined in Table \\ref{table:real-data}.  \nThe spectral $(k,p)$-support outperformed the trace norm and the spectral $k$-support norm, and the improvement is statistically significant at a level $<0.001$ (the standard deviations, not shown here, are of the order of $10^{-5}$). \nIn summary the experiments suggest that the additional flexibility of the $p$ parameter does allow the model to better fit both the sparsity and the decay of the true spectrum. \n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{testVsPvalue3.pdf}\n}\n\\caption{Test error vs curvature ($p$).  Left axis: synthetic data (blue crosses); right axis: Jester 1 dataset (red circles).}\n\\label{fig:error-vs-p-synthetic}\n\\end{center}\n\\end{figure} \n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\\vspace{-.1truecm}\nWe presented a generalization of the $k$-support norm, the $(k,p)$-support norm, where the additional parameter $p$ is used to better fit the decay of the components of the underlying model. We determined the dual norm, characterized the unit ball and computed an explicit expression for the norm.  \nAs the norm is a symmetric gauge function, we further described the induced spectral $(k,p)$-support norm. \nWe adapted the Frank-Wolfe method to solve regularization problems with the norm, and in the particular case $p=\\infty$ we provided a fast computation for the projection operator.\nIn numerical experiments we considered synthetic and real matrix completion problems and we showed that varying $p$ leads to significant  performance improvements.\nFuture work could include deriving statistical bounds for the performance of the norms, and situating the norms in the framework of other structured sparsity norms which have recently been studied.  \n\n\n\\begin{thebibliography}{28}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Argyriou et~al. 2007]{Argyriou2007}\nA.~Argyriou, C.~Micchelli, M.~Pontil, and Y.~Ying.\n\\newblock A spectral regularization framework for multi-task structure\n  learning.\n\\newblock \\emph{NIPS}, 2007.\n\n\\bibitem[Argyriou et~al. 2008]{Argyriou2008}\nA.~Argyriou, T.~Evgeniou, and M.~Pontil.\n\\newblock Convex multi-task feature learning.\n\\newblock \\emph{Machine Learning}, 73(3):\\penalty0 243--272, 2008.\n\n\\bibitem[Argyriou et~al. 2011]{Argyriou2011}\nA.~Argyriou, C.~A. Micchelli, M.~Pontil, L.~Shen, and Y.~Xu.\n\\newblock Efficient first order methods for linear composite regularizers.\n\\newblock \\emph{CoRR}, abs/1104.1436, 2011.\n\n\\bibitem[Argyriou et~al. 2012]{Argyriou2012}\nA.~Argyriou, R.~Foygel, and N.~Srebro.\n\\newblock Sparse prediction with the k-support norm.\n\\newblock \\emph{Advances in Neural Information Processing Systems 25}, pages\n  1466--1474, 2012.\n\n\\bibitem[Beck and Teboulle 2009]{Beck2009}\nA.~Beck and M.~Teboulle.\n\\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse\n  problems.\n\\newblock \\emph{SIAM J. Imaging Sciences}, 2(1):\\penalty0 183--202, 2009.\n\n\\bibitem[Bhatia 1997]{Bhatia1997}\nR.~Bhatia.\n\\newblock \\emph{Matrix Analysis}.\n\\newblock Springer, 1997.\n\n\\bibitem[Buehlmann and van~der Geer 2011]{Buehlmann2011}\nP.~Buehlmann and S.~A. van~der Geer.\n\\newblock \\emph{Statistics for High-Dimensional Data}.\n\\newblock Springer, 2011.\n\n\\bibitem[Chatterjee et~al. 2014]{Chatterjee2014}\nS.~Chatterjee, S.~Chen, and A.~Banerjee.\n\\newblock Generalized {D}antzig selector: application to the k-support norm.\n\\newblock In \\emph{Advances in Neural Information Processing Systems 28}, 2014.\n\n\\bibitem[Frank and Wolfe 1956]{Frank1956}\nM.~Frank and P.~Wolfe.\n\\newblock An algorithm for quadratic programming.\n\\newblock \\emph{Naval Research Logistics Quarterly}, 3 (1-2):\\penalty0 95--110,\n  1956.\n\n\\bibitem[Horn and Johnson 1991]{Horn1991}\nR.~A. Horn and C.~R. Johnson.\n\\newblock \\emph{Topics in Matrix Analysis}.\n\\newblock Cambridge University Press, 1991.\n\n\\bibitem[Ivanov et~al. 1978]{Ivanov1978}\nV.K. Ivanov, V.~V. Vasin, and V.P. Tanana.\n\\newblock \\emph{Theory of Linear Ill-Posed Problems and its Applications}.\n\\newblock De Gruyter, 1978.\n\n\\bibitem[Jacob et~al. 2009]{Jacob2009-GL}\nL.~Jacob, G.~Obozinski, and J.-P. Vert.\n\\newblock Group lasso with overlap and graph lasso.\n\\newblock \\emph{Proceedings of the 26th International Conference on Machine\n  Learning}, 2009.\n\n\\bibitem[Jaggi 2013]{Jaggi2013}\nM.~Jaggi.\n\\newblock Revisiting {F}rank-{W}olfe: Projection-free sparse convex\n  optimization.\n\\newblock \\emph{Proceedings of the 30th International Conference on Machine\n  Learning}, 2013.\n\n\\bibitem[Jaggi and Sulovsky 2010]{Jaggi2010}\nM~Jaggi and M.~Sulovsky.\n\\newblock A simple algorithm for nuclear norm regularized problems.\n\\newblock \\emph{Proceedings of the 27th International Conference on Machine\n  Learning}, 2010.\n\n\\bibitem[Lewis 1995]{Lewis1995}\nA.~S. Lewis.\n\\newblock The convex analysis of unitarily invariant matrix functions.\n\\newblock \\emph{Journal of Convex Analysis}, 2:\\penalty0 173--183, 1995.\n\n\\bibitem[Marshall and Olkin 1979]{Marshall1979}\nA.~W. Marshall and I.~Olkin.\n\\newblock \\emph{Inequalities: Theory of Majorization and its Applications}.\n\\newblock Academic Press, 1979.\n\n\\bibitem[Mazumder et~al. 2010]{Mazumder2010}\nR.~Mazumder, T.~Hastie, and R.~Tibshirani.\n\\newblock Spectral regularization algorithms for learning large incomplete\n  matrices.\n\\newblock \\emph{Journal of Machine Learning Research}, 11:\\penalty0 2287--2322,\n  2010.\n\n\\bibitem[McDonald et~al. 2016]{McDonald2014}\nA.~M. McDonald, M.~Pontil, and D.~Stamos.\n\\newblock New perspectives on k-support and cluster norms.\n\\newblock \\emph{Journal of Machine Learning Research}, 2016{\\natexlab{a}}.\n\n\\bibitem[McDonald et~al. 2014]{McDonald2014a}\nA.~M. McDonald, M.~Pontil, and D.~Stamos.\n\\newblock Spectral k-support regularization.\n\\newblock In \\emph{Advances in Neural Information Processing Systems 28},\n  2014{\\natexlab{b}}.\n\n\\bibitem[Nesterov 2007]{Nesterov2007}\nY.~Nesterov.\n\\newblock Gradient methods for minimizing composite objective function.\n\\newblock \\emph{Center for Operations Research and Econometrics}, 76, 2007.\n\n\\bibitem[Rockafellar 1970]{Rockafellar1970}\nR.~T. Rockafellar.\n\\newblock \\emph{Convex Analysis}.\n\\newblock Princeton University Press, 1970.\n\n\\bibitem[Rohde and Tsybakov 2011]{Rohde2011}\nA.~Rohde and A.B. Tsybakov.\n\\newblock Estimation of high-dimensional low rank matrices.\n\\newblock \\emph{Annals of Statistics}, 39:\\penalty0 887--930, 2011.\n\n\\bibitem[Rudin 1991]{Rudin1991}\nW.~Rudin.\n\\newblock \\emph{Functional Analysis}.\n\\newblock McGraw Hill, 1991.\n\n\\bibitem[Tibshirani 1996]{Tibshirani1996}\nR.~Tibshirani.\n\\newblock Regression shrinkage and selection via the lasso.\n\\newblock \\emph{Journal of the Royal Statistical Society}, 58:\\penalty0\n  267--288, 1996.\n\n\\bibitem[Toh and Yun 2011]{Toh2011}\nK.-C. Toh and S.~Yun.\n\\newblock An accelerated proximal gradient algorithm for nuclear norm\n  regularized least squares problems.\n\\newblock \\emph{SIAM J. on Img. Sci.}, 4:\\penalty0 573--596, 2011.\n\n\\bibitem[Von~Neumann 1937]{VonNeumann1937}\nJ.~Von~Neumann.\n\\newblock \\emph{Some matrix-inequalities and metrization of matric-space}.\n\\newblock Tomsk. Univ. Rev. Vol {I}, 1937.\n\n\\bibitem[Wainwright 2014]{Wainwright2014}\nM.~Wainwright.\n\\newblock Structured regularizers for high-dimensional problems.\n\\newblock \\emph{Annual Review of Statistics and Its Application}, 1:\\penalty0\n  233--253, 2014.\n\n\\bibitem[Zou and Hastie 2005]{Zou2005}\nH.~Zou and T.~Hastie.\n\\newblock Regularization and variable selection via the elastic net.\n\\newblock \\emph{Journal of the Royal Statistical Society, Series B},\n  67(2):\\penalty0 301--320, 2005.\n\n\\end{thebibliography}\n\n\\appendix\n\\section{Appendix}\n\nIn this appendix, we provide proofs of the results stated in the main body of the paper, and we include experimental details and results that were not included in the paper for space reasons.\n\n\\subsection{Proof of Proposition \\ref{prop:k-sup-p-norm-and-dual}}\nFor every $u \\in {\\mathbb R}^d$ we have\n\n", "itemtype": "equation", "pos": 31720, "prevtext": "\nWe consider two cases.  If $\\sum_{i=1}^d \\vert w_i \\vert {\\ensuremath{\\leqslant}} k$, then the problem decouples and we solve it componentwise.  \nIf $\\sum_{i=1}^d \\vert w_i \\vert > k$, we solve problem \\eqref{eqn:projection-problem-body} by minimizing the Lagrangian function $\\mathcal{L}(x,\\beta) = \\sum_{i=1}^d (x_i-w_i)^2 +2 \\beta ( \\sum_{i=1}^d \\vert x_i \\vert - k)$ with nonnegative multiplier $\\beta$. \nThis can be done componentwise, and at the optimum the constraint $\\sum_{i=1}^d  \\vert x_i \\vert {\\ensuremath{\\leqslant}}  k$ will be tight. \nFinally, both cases can be combined into the form of \\eqref{eqn:pinf-prox}.\nThe complexity follows by taking advantage of the monotonicity of $x_i(\\beta)$. \n\\end{proof}\nWe can use Proposition \\ref{prop:projection-to-k-infinity-unit-ball} to project onto the unit ball of radius $\\alpha>0$ by a rescaling argument (see the appendix for details). \n\n\n\\subsection{Matrix Problem}\n\\label{sec:MP}\nGiven data matrix $X \\in {\\mathbb R}^{d \\times m}$ for which we observe a subset of entries, \n\nwe consider the constrained optimization problem\n\n\n\n\n\n\n", "index": 63, "text": "\\begin{align}\n\\min_{W \\in {\\mathbb R}^{d \\times m}} &\\,  \n\\left\\{ \\Vert \\Omega(X) - \\Omega(W) \\Vert_{\\rm F}: \\Vert W \\Vert_{(k,p)} {\\ensuremath{\\leqslant}} \\alpha \\right\\}\n\\label{eqn:matrix-optimization}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{W\\in{\\mathbb{R}}^{d\\times m}}\" display=\"inline\"><munder><mi>min</mi><mrow><mi>W</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>m</mi></mrow></msup></mrow></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\,\\left\\{\\|\\Omega(X)-\\Omega(W)\\|_{\\rm F}:\\|W\\|_{(k,p)}{\\leqslant}%&#10;\\alpha\\right\\}\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2009</mi><mo>\u2062</mo><mrow><mo>{</mo><msub><mrow><mo>\u2225</mo><mrow><mrow><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">F</mi></msub><mo>:</mo><mrow><msub><mrow><mo>\u2225</mo><mi>W</mi><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2a7d</mo><mi>\u03b1</mi></mrow><mo>}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere the first equality uses the definition of the unit ball \\eqref{eq:unitball} and the second equality is true because the maximum of a linear functional on a compact set is attained at an extreme point of the set. The third equality follows by using the cardinality constraint, that is we set $w_i = 0$ if $i \\notin {I_k}$. Finally, the last equality follows by H\\\"older's inequality in ${\\mathbb R}^k$ \n\n\\cite[Ch. 16 Sec. D.1]{Marshall1979}. \n\nThe second claim is a direct consequence of the cardinality constraint and H\\\"older's inequality in ${\\mathbb R}^k$.\n\\qed\n\n\n\n\n\n\nTo prove Proposition \\ref{prop:unit-ball-of-spectral} we require the following auxiliary result.  Let $X$ be a finite dimensional vector space. Recall that a subset $C$ of $X$ is called {\\em balanced} if $\\alpha C \\subseteq C$ whenever $\\vert \\alpha \\vert {\\ensuremath{\\leqslant}} 1$. \nFurthermore, $C$ is called {\\em absorbing} if for any $x \\in X$, $x \\in \\lambda C$ for some $\\lambda >0$.  \n\n\\begin{lemma}\\label{lem:minkowski-bounded}\nLet $C\\subseteq X$ be a bounded, convex, balanced, and absorbing set.  \nThe Minkowski functional $\\mu_C$ of $C$, defined, for every $w \\in X$, as\n\n", "itemtype": "equation", "pos": 50901, "prevtext": "\nwhere the operator $\\Omega$ applied to a matrix sets unobserved values to zero.\nAs in the vector case, the Frank-Wolfe method can be applied to the matrix problems. \nAlgorithm \\ref{alg:FW} is particularly convenient in this case as we only need to compute the largest $k$ singular values, which can result in a computationally efficient algorithm. \nThe result is a direct consequence of Proposition \\ref{prop:k-sup-p-norm-and-dual} and von Neumann's trace inequality, see e.g. \\cite[Ch. 9 Sec. H.1.h]{Marshall1979}. \nWe obtain that the solution of the inner minimization step is\n$\nU_k {\\rm diag}(s) V_k{^{\\scriptscriptstyle \\top}}\n$\nwhere $U_k$ and $V_k$ are the top $k$ left and right singular vectors of the gradient $G$ of the objective function in \\eqref{eqn:matrix-optimization} evaluated at the current solution, whose singular values we denote by $g$, and $s$ is obtained from $g$ as per equations \\eqref{eq:FWp} and \\eqref{eq:FWinf}, for $p\\in (1,\\infty)$ and $p=\\infty$, respectively.\n\nNote also that the proximity operator of the norm and the Euclidean projection on the associated unit ball both require the full singular value decomposition to be performed. Indeed, the proximity operator of an orthogonally invariant norm $\\Vert \\cdot \\Vert = g( \\sigma(\\cdot))$ at $W \\in \\mathbb{R}^{d \\times m}$ is given by $\n{\\ensuremath{\\text{\\rm prox}}}_{\\Vert \\cdot \\Vert} (W)= U \\text{diag}({\\ensuremath{\\text{\\rm prox}}}_{g}(\\sigma(W))) V{^{\\scriptscriptstyle \\top}}$, where $U$ and $V$ are the matrices formed by the left and right singular vectors of $W$, see e.g. \\cite[Prop. 3.1]{Argyriou2011}, and this requires the full decomposition. \n\n\n\\begin{table}[t!]\n\\caption{Matrix completion on rank 5 matrix with flat spectrum. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is considerable (statistically significant at a level $<0.001$). }\n\\vskip 0.1in\n\\label{table:synthetic-flat-p-infinity}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[t]{0.98\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                           & $k$ & $p$  \\\\\n\\midrule\nrank 5       & trace  & 0.8415 (0.03)                         & -   &-  \\\\ \n$\\rho$=10\\%  & k-supp  & 0.8343 (0.03)                          & 3.3 & - \\\\ \n             & kp-supp  & 0.8108 (0.05)                          & 5.0 & $\\infty$ \\\\ \n\\midrule\nrank 5       & trace  & 0.6161 (0.03)                          &  -  & - \\\\ \n$\\rho$=15\\%  & k-supp  & 0.6129 (0.03)                          & 3.3 & - \\\\ \n             & kp-supp  & 0.4262 (0.04)                          & 5.0 & $\\infty$ \\\\ \n\\midrule\nrank 5       & trace  & 0.4453 (0.03)             \t            &-  & -   \\\\ \n$\\rho$=20\\%  & k-supp  & 0.4436 (0.02)                          & 3.5 & - \\\\ \n             & kp-supp  & 0.2425 (0.02)            \t            & 5.0 & $\\infty$  \\\\ \n\\midrule\nrank 5       & trace  & 0.1968 (0.01)             \t            &-  & -   \\\\ \n$\\rho$=30\\%  & k-supp  & 0.1838 (0.01)                          & 5.0 & - \\\\ \n             & kp-supp  & 0.0856 (0.01)            \t            & 5.0 & $\\infty$  \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\n\n\n\n\n\\section{Numerical Experiments}\n\\label{sec:experiments}\n\\vspace{-.1truecm}\n\nIn this section we apply the spectral $(k,p)$-support norm to matrix completion (collaborative filtering),   \nin which we want to recover a low rank, or approximately low rank, matrix from a small sample of its entries, see e.g. \\cite{Jaggi2010}.\n\nOne prominent method of solving this problem is trace norm regularization: we look for a matrix which closely fits the observed entries and has a small trace norm (sum of singular values) \\cite{Jaggi2010,Mazumder2010,Toh2011}. \n\nWe apply the $(k,p)$-support norm to this framework and we investigate the impact of varying $p$.  \nNext we compare the spectral $(k,p)$-support norm to the trace norm and the spectral $k$-support norm ($p=2$) in both synthetic and real datasets.\nIn each case we solve the optimization problem \\eqref{eqn:matrix-optimization} using the Frank-Wolfe method as outlined in Section \\ref{sec:optimization}.\n\nWe determine the values of $k$ and $p {\\ensuremath{\\geqslant}} 1$ by validation, averaged over a number of trials. Specifically, we choose the optimal $p$, $k$, as well as the regularization parameter $\\alpha$ by validation over a grid.\nWe let alpha vary in $10^0$ to $10^5$ with step $10^{0.25}$, we let $p$ vary over $20$ values from $1$ to $50,000$, plus $p=\\infty$, and vary $k$ from $1$ to $20$. Our code is available from {\\em http://www0.cs.ucl.ac.uk/staff/M.Pontil/software.html}.\n\n\\begin{figure}[t]\n\\vskip 0.2in\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{pValueVSdecay.pdf}\n}\n\\caption{Optimal $p$ vs. decay $a$.}\n\\label{fig:p-vs-decay-synthetic}\n\\end{center}\n\\vskip -0.2in\n\\end{figure}\n\n\n\\begin{figure}[t]\n\\vskip 0.2in\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{spectrumVSp-fontStyle_bold-fontSize_22-noLabel-withLegend-withLines.pdf}\n}\n\\caption{Optimal $p$ fitted to Matrix spectra with various decays.}\n\\label{fig:p-vs-decay-real}\n\\end{center}\n\\vskip -0.2in\n\\end{figure}\n\n\n\\noindent {\\bf Impact of $p$.}\nA key motivation for the additional parameter $p$ is that it allows us to tune the norm to the decay of the singular values of the underlying matrix. \nIn particular the variational formulation of \\eqref{eqn:kp-sup-infimum-convolution} suggests that as the spectrum of the true low rank matrix flattens out, larger values of $p$ should be preferred. \n  \nWe ran the method on a set of $100 \\times 100$ matrices of rank 12, with decay of the non zero singular values $\\sigma_\\ell$ proportional to $\\exp(-\\ell a)$, \nfor 26 values of $a$ between $10^{-6}$ and $0.18$, and we determined the corresponding optimal value of $p$.\nFigure \\ref{fig:p-vs-decay-synthetic} illustrates the optimum value of $p$ as a function of $a$. \nWe clearly observe the negative slope, that is the steeper the slope the smaller the optimal value of $p$.  \nFigure \\ref{fig:p-vs-decay-real} shows the spectrum and the optimal $p$ for several decay values. \n \nNote that $k$ is never equal to 1, which is a special case in which the norm is independent of $p$, and is equal to the trace norm. \nIn each case the improvement of the spectral $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. \n\nFigure \\ref{fig:error-vs-p-synthetic} illustrates the impact of the curvature $p$ on the test error on synthetic and real datasets.  \nWe observe that the error levels off as $p$ tends to infinity, so for these specific datasets the major gain is to be had for small values of $p$.  \nThe optimum value of $p$ for both the real and synthetic datasets is statistically different from $p=2$ ($k$-support norm), and $p=1$ (trace norm).\n\n \n\n\n\\begin{table}[t!]\n\\caption{\nMatrix completion on real datasets. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. }\n\\vskip 0.1in\n\\label{table:real-data}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[th]{0.65\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                         & $k$ & $p$  \\\\\n\\midrule\nMovieLens       & trace  & 0.2017                       & -   & -  \\\\ \n100k  \t\t& k-supp  & 0.1990                          & 1.9  & - \\\\ \n             & kp-supp  & 0.1921                           & 2.0 & $\\infty$ \\\\ \n\\midrule\nJester 1      & trace  & 0.1752             \t            &-  & -   \\\\ \n  \t\t\t& k-supp  & 0.1739             \t            & 6.4 & -   \\\\ \n\t\t\t& kp-supp & 0.1744 & 2.0 & $\\infty$  \\\\\n             & kp-supp  & 0.1731             \t            & 2.0 & 6.5  \\\\ \n\\midrule\nJester 3      & trace  & 0.1959              \t            &  -  & - \\\\ \n  \t\t\t& k-supp  & 0.1942                          &  2.1 & - \\\\ \n             & kp-supp  & 0.1841                          & 2.0 & $\\infty$ \\\\ \n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\\noindent {\\bf Simulated Data.}\nNext we compared the performance of the $(k,p)$-support norm to that of the $k$-support norm and the trace norm for a matrix with flat spectrum.  As outlined above, as the spectrum of the true low rank matrix flattens out, larger values of $p$ should be preferred. \nEach $100 \\times 100$ matrix is generated as $W = A S B{^{\\scriptscriptstyle \\top}} + E$, where $U$ and $V$ are the singular vectors of the matrix $U V{^{\\scriptscriptstyle \\top}}$, where $U,V \\in \\mathbb{R}^{100\\times 5}$, the entries of $U$, $V$ and $E$ are i.i.d. standard Gaussian, and $S$ is diagonal with 5 non zero constant entries. \nTable \\ref{table:synthetic-flat-p-infinity} illustrates the performance of the norms on a synthetic dataset of rank 5, with identical singular values, that is a flat spectrum.  \nIn each regime the case $p=\\infty$ outperforms the other norms by a substantial margin, with statistical significance at a level $<0.001$. \nWe followed the framework of \\cite{McDonald2014a}  and use $\\rho$ to denote the percentage of the data to use in the training set. \nWe further replicated the setting of \\cite{McDonald2014a} for synthetic matrix completion, and found that the $(k,p)$-support norm outperformed the standard $k$-support norm, as well as the trace norm, at a statistically significant level (see Table \\ref{table:synthetic-mc} in the appendix for details). \n\n\nWe note that although Frank-Wolfe method for the $(k,p)$-support norm does not generally converge as quickly as proximal methods (which are available in the case of $k$-support norm \\cite{McDonald2014,McDonald2014a,Chatterjee2014}), the computational cost can be mitigated using the continuation method.  Specifically given an ordered sequence of parameter values for $p$ we can proceed sequentially, initializing its value based on the previously computed value.  Empirically we tried this approach for a series of 30 values of $p$ and found that the total computation time increased only moderately. \n\n\n\n\n\n\n\n\\noindent {\\bf Real Data.} Finally, we applied the norms to real collaborative filtering datasets.\nWe observe a subset of the (user, rating) entries of a matrix and predict the unobserved ratings, with the assumption that the true matrix is likely to have low rank. \nWe report on the MovieLens 100k dataset ({\\em http://grouplens.org/datasets/movielens/}), which consists\nof ratings of movies, and\n\n\nthe Jester 1 and 3 datasets ({\\em http://goldberg.berkeley.edu/jester-data/}), which consist of \nratings of jokes. \n\nWe followed the experimental protocol in \\cite{McDonald2014a,Toh2011},\n\n\n\nusing normalized mean absolute error \\cite{Toh2011}, and we implemented a final thresholding step as in \\cite{McDonald2014a} (see the appendix for further details).\n\n\n\n\nThe results are outlined in Table \\ref{table:real-data}.  \nThe spectral $(k,p)$-support outperformed the trace norm and the spectral $k$-support norm, and the improvement is statistically significant at a level $<0.001$ (the standard deviations, not shown here, are of the order of $10^{-5}$). \nIn summary the experiments suggest that the additional flexibility of the $p$ parameter does allow the model to better fit both the sparsity and the decay of the true spectrum. \n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\n\\includegraphics[width=0.65\\columnwidth]{testVsPvalue3.pdf}\n}\n\\caption{Test error vs curvature ($p$).  Left axis: synthetic data (blue crosses); right axis: Jester 1 dataset (red circles).}\n\\label{fig:error-vs-p-synthetic}\n\\end{center}\n\\end{figure} \n\n\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\\vspace{-.1truecm}\nWe presented a generalization of the $k$-support norm, the $(k,p)$-support norm, where the additional parameter $p$ is used to better fit the decay of the components of the underlying model. We determined the dual norm, characterized the unit ball and computed an explicit expression for the norm.  \nAs the norm is a symmetric gauge function, we further described the induced spectral $(k,p)$-support norm. \nWe adapted the Frank-Wolfe method to solve regularization problems with the norm, and in the particular case $p=\\infty$ we provided a fast computation for the projection operator.\nIn numerical experiments we considered synthetic and real matrix completion problems and we showed that varying $p$ leads to significant  performance improvements.\nFuture work could include deriving statistical bounds for the performance of the norms, and situating the norms in the framework of other structured sparsity norms which have recently been studied.  \n\n\n\\begin{thebibliography}{28}\n\\providecommand{\\natexlab}[1]{#1}\n\\providecommand{\\url}[1]{\\texttt{#1}}\n\\expandafter\\ifx\\csname urlstyle\\endcsname\\relax\n  \\providecommand{\\doi}[1]{doi: #1}\\else\n  \\providecommand{\\doi}{doi: \\begingroup \\urlstyle{rm}\\Url}\\fi\n\n\\bibitem[Argyriou et~al. 2007]{Argyriou2007}\nA.~Argyriou, C.~Micchelli, M.~Pontil, and Y.~Ying.\n\\newblock A spectral regularization framework for multi-task structure\n  learning.\n\\newblock \\emph{NIPS}, 2007.\n\n\\bibitem[Argyriou et~al. 2008]{Argyriou2008}\nA.~Argyriou, T.~Evgeniou, and M.~Pontil.\n\\newblock Convex multi-task feature learning.\n\\newblock \\emph{Machine Learning}, 73(3):\\penalty0 243--272, 2008.\n\n\\bibitem[Argyriou et~al. 2011]{Argyriou2011}\nA.~Argyriou, C.~A. Micchelli, M.~Pontil, L.~Shen, and Y.~Xu.\n\\newblock Efficient first order methods for linear composite regularizers.\n\\newblock \\emph{CoRR}, abs/1104.1436, 2011.\n\n\\bibitem[Argyriou et~al. 2012]{Argyriou2012}\nA.~Argyriou, R.~Foygel, and N.~Srebro.\n\\newblock Sparse prediction with the k-support norm.\n\\newblock \\emph{Advances in Neural Information Processing Systems 25}, pages\n  1466--1474, 2012.\n\n\\bibitem[Beck and Teboulle 2009]{Beck2009}\nA.~Beck and M.~Teboulle.\n\\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse\n  problems.\n\\newblock \\emph{SIAM J. Imaging Sciences}, 2(1):\\penalty0 183--202, 2009.\n\n\\bibitem[Bhatia 1997]{Bhatia1997}\nR.~Bhatia.\n\\newblock \\emph{Matrix Analysis}.\n\\newblock Springer, 1997.\n\n\\bibitem[Buehlmann and van~der Geer 2011]{Buehlmann2011}\nP.~Buehlmann and S.~A. van~der Geer.\n\\newblock \\emph{Statistics for High-Dimensional Data}.\n\\newblock Springer, 2011.\n\n\\bibitem[Chatterjee et~al. 2014]{Chatterjee2014}\nS.~Chatterjee, S.~Chen, and A.~Banerjee.\n\\newblock Generalized {D}antzig selector: application to the k-support norm.\n\\newblock In \\emph{Advances in Neural Information Processing Systems 28}, 2014.\n\n\\bibitem[Frank and Wolfe 1956]{Frank1956}\nM.~Frank and P.~Wolfe.\n\\newblock An algorithm for quadratic programming.\n\\newblock \\emph{Naval Research Logistics Quarterly}, 3 (1-2):\\penalty0 95--110,\n  1956.\n\n\\bibitem[Horn and Johnson 1991]{Horn1991}\nR.~A. Horn and C.~R. Johnson.\n\\newblock \\emph{Topics in Matrix Analysis}.\n\\newblock Cambridge University Press, 1991.\n\n\\bibitem[Ivanov et~al. 1978]{Ivanov1978}\nV.K. Ivanov, V.~V. Vasin, and V.P. Tanana.\n\\newblock \\emph{Theory of Linear Ill-Posed Problems and its Applications}.\n\\newblock De Gruyter, 1978.\n\n\\bibitem[Jacob et~al. 2009]{Jacob2009-GL}\nL.~Jacob, G.~Obozinski, and J.-P. Vert.\n\\newblock Group lasso with overlap and graph lasso.\n\\newblock \\emph{Proceedings of the 26th International Conference on Machine\n  Learning}, 2009.\n\n\\bibitem[Jaggi 2013]{Jaggi2013}\nM.~Jaggi.\n\\newblock Revisiting {F}rank-{W}olfe: Projection-free sparse convex\n  optimization.\n\\newblock \\emph{Proceedings of the 30th International Conference on Machine\n  Learning}, 2013.\n\n\\bibitem[Jaggi and Sulovsky 2010]{Jaggi2010}\nM~Jaggi and M.~Sulovsky.\n\\newblock A simple algorithm for nuclear norm regularized problems.\n\\newblock \\emph{Proceedings of the 27th International Conference on Machine\n  Learning}, 2010.\n\n\\bibitem[Lewis 1995]{Lewis1995}\nA.~S. Lewis.\n\\newblock The convex analysis of unitarily invariant matrix functions.\n\\newblock \\emph{Journal of Convex Analysis}, 2:\\penalty0 173--183, 1995.\n\n\\bibitem[Marshall and Olkin 1979]{Marshall1979}\nA.~W. Marshall and I.~Olkin.\n\\newblock \\emph{Inequalities: Theory of Majorization and its Applications}.\n\\newblock Academic Press, 1979.\n\n\\bibitem[Mazumder et~al. 2010]{Mazumder2010}\nR.~Mazumder, T.~Hastie, and R.~Tibshirani.\n\\newblock Spectral regularization algorithms for learning large incomplete\n  matrices.\n\\newblock \\emph{Journal of Machine Learning Research}, 11:\\penalty0 2287--2322,\n  2010.\n\n\\bibitem[McDonald et~al. 2016]{McDonald2014}\nA.~M. McDonald, M.~Pontil, and D.~Stamos.\n\\newblock New perspectives on k-support and cluster norms.\n\\newblock \\emph{Journal of Machine Learning Research}, 2016{\\natexlab{a}}.\n\n\\bibitem[McDonald et~al. 2014]{McDonald2014a}\nA.~M. McDonald, M.~Pontil, and D.~Stamos.\n\\newblock Spectral k-support regularization.\n\\newblock In \\emph{Advances in Neural Information Processing Systems 28},\n  2014{\\natexlab{b}}.\n\n\\bibitem[Nesterov 2007]{Nesterov2007}\nY.~Nesterov.\n\\newblock Gradient methods for minimizing composite objective function.\n\\newblock \\emph{Center for Operations Research and Econometrics}, 76, 2007.\n\n\\bibitem[Rockafellar 1970]{Rockafellar1970}\nR.~T. Rockafellar.\n\\newblock \\emph{Convex Analysis}.\n\\newblock Princeton University Press, 1970.\n\n\\bibitem[Rohde and Tsybakov 2011]{Rohde2011}\nA.~Rohde and A.B. Tsybakov.\n\\newblock Estimation of high-dimensional low rank matrices.\n\\newblock \\emph{Annals of Statistics}, 39:\\penalty0 887--930, 2011.\n\n\\bibitem[Rudin 1991]{Rudin1991}\nW.~Rudin.\n\\newblock \\emph{Functional Analysis}.\n\\newblock McGraw Hill, 1991.\n\n\\bibitem[Tibshirani 1996]{Tibshirani1996}\nR.~Tibshirani.\n\\newblock Regression shrinkage and selection via the lasso.\n\\newblock \\emph{Journal of the Royal Statistical Society}, 58:\\penalty0\n  267--288, 1996.\n\n\\bibitem[Toh and Yun 2011]{Toh2011}\nK.-C. Toh and S.~Yun.\n\\newblock An accelerated proximal gradient algorithm for nuclear norm\n  regularized least squares problems.\n\\newblock \\emph{SIAM J. on Img. Sci.}, 4:\\penalty0 573--596, 2011.\n\n\\bibitem[Von~Neumann 1937]{VonNeumann1937}\nJ.~Von~Neumann.\n\\newblock \\emph{Some matrix-inequalities and metrization of matric-space}.\n\\newblock Tomsk. Univ. Rev. Vol {I}, 1937.\n\n\\bibitem[Wainwright 2014]{Wainwright2014}\nM.~Wainwright.\n\\newblock Structured regularizers for high-dimensional problems.\n\\newblock \\emph{Annual Review of Statistics and Its Application}, 1:\\penalty0\n  233--253, 2014.\n\n\\bibitem[Zou and Hastie 2005]{Zou2005}\nH.~Zou and T.~Hastie.\n\\newblock Regularization and variable selection via the elastic net.\n\\newblock \\emph{Journal of the Royal Statistical Society, Series B},\n  67(2):\\penalty0 301--320, 2005.\n\n\\end{thebibliography}\n\n\\appendix\n\\section{Appendix}\n\nIn this appendix, we provide proofs of the results stated in the main body of the paper, and we include experimental details and results that were not included in the paper for space reasons.\n\n\\subsection{Proof of Proposition \\ref{prop:k-sup-p-norm-and-dual}}\nFor every $u \\in {\\mathbb R}^d$ we have\n\n", "index": 65, "text": "\\begin{align}\n\\Vert u \\Vert_{(k,p),*} \n&= \\max \\left\\{ \\sum_{i=1}^d u_i w_i  : w \\in C_k^p \\right\\} \\nonumber \\\\\n& = \\max \\left\\{ \\sum_{i=1}^d u_i w_i :  {\\ensuremath{\\text{\\rm card}}}(w){\\ensuremath{\\leqslant}} k, \\Vert w \\Vert_{p} {\\ensuremath{\\leqslant}} 1\\right\\} \\nonumber \\\\\n& = \\max \\left\\{ \\sum_{i\\in {I_k}} u_i w_i : \\sum_{i \\in {I_k}} \\vert w_i\\vert^{p}  {\\ensuremath{\\leqslant}} 1 \\right\\}\\nonumber  \\nonumber \\\\\n& = \\left( \\sum_{i \\in {I_k}} \\vert u_i\\vert^{q} \\right)^{\\frac{1}{q}},\n\\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|u\\|_{(k,p),*}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mo>*</mo></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max\\left\\{\\sum_{i=1}^{d}u_{i}w_{i}:w\\in C_{k}^{p}\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mrow><mo>:</mo><mrow><mi>w</mi><mo>\u2208</mo><msubsup><mi>C</mi><mi>k</mi><mi>p</mi></msubsup></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max\\left\\{\\sum_{i=1}^{d}u_{i}w_{i}:{\\text{\\rm card}}(w){%&#10;\\leqslant}k,\\|w\\|_{p}{\\leqslant}1\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mrow><mo>:</mo><mrow><mrow><mrow><mtext>card</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow><mo>,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>w</mi><mo>\u2225</mo></mrow><mi>p</mi></msub><mo>\u2a7d</mo><mn>1</mn></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max\\left\\{\\sum_{i\\in{I_{k}}}u_{i}w_{i}:\\sum_{i\\in{I_{k}}}|w_{i}%&#10;|^{p}{\\leqslant}1\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></munder></mstyle><mrow><msub><mi>u</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></munder></mstyle><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup></mrow><mo>\u2a7d</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\sum_{i\\in{I_{k}}}|u_{i}|^{q}\\right)^{\\frac{1}{q}},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><msub><mi>I</mi><mi>k</mi></msub></mrow></munder></mstyle><msup><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mi>q</mi></msup></mrow><mo>)</mo></mrow><mfrac><mn>1</mn><mi>q</mi></mfrac></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nis a norm on $X$. \n\\end{lemma}\n\n\\begin{proof}\nWe give a direct proof that $\\mu_C$ satisfies the properties of a norm.  \nSee also e.g. \\cite[\\S 1.35]{Rudin1991} for further details.\nClearly $\\mu_C(w) {\\ensuremath{\\geqslant}} 0$ for all $w$, and $\\mu_C(0)=0$. Moreover, as $C$ is bounded,\n$\\mu_C(w) > 0$ whenever $w\\ne 0$. \n\nNext we show that $\\mu_C$ is one-homogeneous. For every $\\alpha \\in {\\mathbb R}$, $\\alpha \\ne 0$, let $\\sigma = {\\rm sign}(\\alpha)$ and note that\n\n", "itemtype": "equation", "pos": 52580, "prevtext": "\nwhere the first equality uses the definition of the unit ball \\eqref{eq:unitball} and the second equality is true because the maximum of a linear functional on a compact set is attained at an extreme point of the set. The third equality follows by using the cardinality constraint, that is we set $w_i = 0$ if $i \\notin {I_k}$. Finally, the last equality follows by H\\\"older's inequality in ${\\mathbb R}^k$ \n\n\\cite[Ch. 16 Sec. D.1]{Marshall1979}. \n\nThe second claim is a direct consequence of the cardinality constraint and H\\\"older's inequality in ${\\mathbb R}^k$.\n\\qed\n\n\n\n\n\n\nTo prove Proposition \\ref{prop:unit-ball-of-spectral} we require the following auxiliary result.  Let $X$ be a finite dimensional vector space. Recall that a subset $C$ of $X$ is called {\\em balanced} if $\\alpha C \\subseteq C$ whenever $\\vert \\alpha \\vert {\\ensuremath{\\leqslant}} 1$. \nFurthermore, $C$ is called {\\em absorbing} if for any $x \\in X$, $x \\in \\lambda C$ for some $\\lambda >0$.  \n\n\\begin{lemma}\\label{lem:minkowski-bounded}\nLet $C\\subseteq X$ be a bounded, convex, balanced, and absorbing set.  \nThe Minkowski functional $\\mu_C$ of $C$, defined, for every $w \\in X$, as\n\n", "index": 67, "text": "\\begin{align*}\n\\mu_C(w) = {\\operatorname{\\vphantom{p}inf}} \\left\\{\\lambda : \\lambda >0, ~\\frac{1}{\\lambda} w \\in C\\right\\}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu_{C}(w)={\\operatorname{\\vphantom{p}inf}}\\left\\{\\lambda:\\lambda%&#10;&gt;0,~{}\\frac{1}{\\lambda}w\\in C\\right\\}\" display=\"inline\"><mrow><mrow><msub><mi>\u03bc</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo>{</mo><mrow><mi>\u03bb</mi><mo>:</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac></mstyle><mo>\u2062</mo><mi>w</mi></mrow><mo>\u2208</mo><mi>C</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere we have made a change of variable and used the fact that $\\sigma C = C$.\n\nFinally, we prove the triangle inequality. For every $v,w \\in X$, if $v/{\\lambda} \\in C$ and $w/\\mu \\in C$ then setting $\\gamma =  {\\lambda}/({\\lambda}+\\mu)$, we have\n\n", "itemtype": "equation", "pos": 53185, "prevtext": "\nis a norm on $X$. \n\\end{lemma}\n\n\\begin{proof}\nWe give a direct proof that $\\mu_C$ satisfies the properties of a norm.  \nSee also e.g. \\cite[\\S 1.35]{Rudin1991} for further details.\nClearly $\\mu_C(w) {\\ensuremath{\\geqslant}} 0$ for all $w$, and $\\mu_C(0)=0$. Moreover, as $C$ is bounded,\n$\\mu_C(w) > 0$ whenever $w\\ne 0$. \n\nNext we show that $\\mu_C$ is one-homogeneous. For every $\\alpha \\in {\\mathbb R}$, $\\alpha \\ne 0$, let $\\sigma = {\\rm sign}(\\alpha)$ and note that\n\n", "index": 69, "text": "\\begin{align*}\n\\mu_C(\\alpha w) &= {\\operatorname{\\vphantom{p}inf}} \\left\\{\\lambda>0 : \\frac{1}{\\lambda} \\alpha w \\in C \\right\\}\\\\\n&= {\\operatorname{\\vphantom{p}inf}} \\left\\{\\lambda>0 : \\frac{|\\alpha|}{\\lambda}  \\sigma w \\in C \\right\\}\\\\\n&= \\vert \\alpha \\vert {\\operatorname{\\vphantom{p}inf}} \\left\\{\\lambda>0 : \\frac{1}{\\lambda} \tw \\in \\sigma C \\right\\}\\\\\n&= \\vert \\alpha \\vert {\\operatorname{\\vphantom{p}inf}} \\left\\{\\lambda>0 : \\frac{1}{\\lambda} \tw \\in  C \\right\\}\\\\\n&= \\vert \\alpha \\vert \\mu_C(w),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mu_{C}(\\alpha w)\" display=\"inline\"><mrow><msub><mi>\u03bc</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mi>w</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}\\left\\{\\lambda&gt;0:\\frac{1}{%&#10;\\lambda}\\alpha w\\in C\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac></mstyle><mo>\u2062</mo><mi>\u03b1</mi><mo>\u2062</mo><mi>w</mi></mrow><mo>\u2208</mo><mi>C</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}\\left\\{\\lambda&gt;0:\\frac{|\\alpha|}%&#10;{\\lambda}\\sigma w\\in C\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">|</mo></mrow><mi>\u03bb</mi></mfrac></mstyle><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><mi>w</mi></mrow><mo>\u2208</mo><mi>C</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=|\\alpha|{\\operatorname{\\vphantom{p}inf}}\\left\\{\\lambda&gt;0:\\frac{1%&#10;}{\\lambda}w\\in\\sigma C\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac></mstyle><mo>\u2062</mo><mi>w</mi></mrow><mo>\u2208</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mi>C</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=|\\alpha|{\\operatorname{\\vphantom{p}inf}}\\left\\{\\lambda&gt;0:\\frac{1%&#10;}{\\lambda}w\\in C\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>\u03bb</mi></mfrac></mstyle><mo>\u2062</mo><mi>w</mi></mrow><mo>\u2208</mo><mi>C</mi></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=|\\alpha|\\mu_{C}(w),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>\u03b1</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msub><mi>\u03bc</mi><mi>C</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nand since $C$ is convex, then $\\frac {v+w}{{\\lambda} +\\mu} \\in C$. We conclude that $\\mu_C(v+w) {\\ensuremath{\\leqslant}} \\mu_C(v) + \\mu_C(w)$. The proof is completed. \n\\end{proof}\nNote that for such set $C$, the unit ball of the induced norm $\\mu_C$ is $C$. \nFurthermore, if $\\|\\cdot\\|$ is a norm then its unit ball satisfies the hypotheses of Lemma \\ref{lem:minkowski-bounded}. \n\n\n\n\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:unit-ball-of-spectral}}\n\n\n\n\n\n\nDefine the set\n\n", "itemtype": "equation", "pos": 53946, "prevtext": "\nwhere we have made a change of variable and used the fact that $\\sigma C = C$.\n\nFinally, we prove the triangle inequality. For every $v,w \\in X$, if $v/{\\lambda} \\in C$ and $w/\\mu \\in C$ then setting $\\gamma =  {\\lambda}/({\\lambda}+\\mu)$, we have\n\n", "index": 71, "text": "$$\n\\frac {v+w}{{\\lambda} +\\mu} =\\gamma \\frac{v}{{\\lambda}} + (1-\\gamma)  \\frac{w}{\\mu}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\frac{v+w}{{\\lambda}+\\mu}=\\gamma\\frac{v}{{\\lambda}}+(1-\\gamma)\\frac{w}{\\mu}\" display=\"block\"><mrow><mfrac><mrow><mi>v</mi><mo>+</mo><mi>w</mi></mrow><mrow><mi>\u03bb</mi><mo>+</mo><mi>\u03bc</mi></mrow></mfrac><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mfrac><mi>v</mi><mi>\u03bb</mi></mfrac></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b3</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mi>w</mi><mi>\u03bc</mi></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nand its convex hull $A_k^p = \\textrm{co}(T_k^p)$, and consider the Minkowski functional \n\n", "itemtype": "equation", "pos": 54512, "prevtext": "\nand since $C$ is convex, then $\\frac {v+w}{{\\lambda} +\\mu} \\in C$. We conclude that $\\mu_C(v+w) {\\ensuremath{\\leqslant}} \\mu_C(v) + \\mu_C(w)$. The proof is completed. \n\\end{proof}\nNote that for such set $C$, the unit ball of the induced norm $\\mu_C$ is $C$. \nFurthermore, if $\\|\\cdot\\|$ is a norm then its unit ball satisfies the hypotheses of Lemma \\ref{lem:minkowski-bounded}. \n\n\n\n\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:unit-ball-of-spectral}}\n\n\n\n\n\n\nDefine the set\n\n", "index": 73, "text": "\\begin{align*}\nT_k^p &= \\{W \\in \\mathbb{R}^{d\\times m} : \\textrm{rank}(W) {\\ensuremath{\\leqslant}} k, \\Vert \\sigma(W) \\Vert_p {\\ensuremath{\\leqslant}} 1 \\} , \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle T_{k}^{p}\" display=\"inline\"><msubsup><mi>T</mi><mi>k</mi><mi>p</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\{W\\in\\mathbb{R}^{d\\times m}:\\textrm{rank}(W){\\leqslant}k,\\|%&#10;\\sigma(W)\\|_{p}{\\leqslant}1\\},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>W</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>m</mi></mrow></msup></mrow><mo>:</mo><mrow><mrow><mrow><mtext>rank</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow><mo>,</mo><mrow><msub><mrow><mo>\u2225</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi>p</mi></msub><mo>\u2a7d</mo><mn>1</mn></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nWe show that $A_k^p$ is absorbing, bounded, convex and symmetric, and it follows by Lemma \\ref{lem:minkowski-bounded} that $\\lambda$ defines a norm on ${\\mathbb R}^{d \\times m}$ with unit ball equal to $A_k^p$.\nThe set $A_k^p$ is clearly bounded, convex and symmetric.  \nTo see that it is absorbing, let $W$ in $\\mathbb{R}^{d \\times m}$ have singular value decomposition $U \\Sigma V{^{\\scriptscriptstyle \\top}}$, and let $r=\\min(d,m)$. \nIf $W$ is zero then clearly $W\\in A_k^p$, so assume it is non zero. \n\n{For $i \\in {\\mathbb N}_r$ let $S_i\\in{\\mathbb R}^{d\\times m}$ have entry $(i,i)$ equal to $1$, and all remaining entries zero.\nWe then have\n\n", "itemtype": "equation", "pos": 54773, "prevtext": "\nand its convex hull $A_k^p = \\textrm{co}(T_k^p)$, and consider the Minkowski functional \n\n", "index": 75, "text": "\\begin{align}\n\\lambda(W) \n&= {\\operatorname{\\vphantom{p}inf}} \\{ \\lambda>0: W \\in \\lambda A_k^p \\}, ~~~ W\\in \\mathbb{R}^{d \\times m}   \\label{eqn:k-sup-matrix-def-2}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lambda(W)\" display=\"inline\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}\\{\\lambda&gt;0:W\\in\\lambda A_{k}^{p%&#10;}\\},~{}~{}~{}W\\in\\mathbb{R}^{d\\times m}.\" display=\"inline\"><mrow><mrow><mrow><mi/><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mi>W</mi><mo>\u2208</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mi>A</mi><mi>k</mi><mi>p</mi></msubsup></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"12.4pt\">,</mo><mrow><mi>W</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>d</mi><mo>\u00d7</mo><mi>m</mi></mrow></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nNow for each $i$, $\\Vert\\sigma( Z_i) \\Vert_p = \\Vert \\sigma(S_i) \\Vert_p = 1$, and $\\textrm{rank}(Z_i) = \\textrm{rank}(S_i) = 1$, so $Z_i \\in T_k^p$ for any $k {\\ensuremath{\\geqslant}} 1$. \nFurthermore $\\lambda_i \\in [0,1]$ and $\\sum_{i=1}^r \\lambda_i=1$, that is $(\\lambda_1, \\ldots, \\lambda_r)$ lies in the unit simplex in ${\\mathbb R}^d$,\n\nso $\\frac{1}{\\lambda}W$ is a convex combination of elements of $Z_i$, in other words $W \\in \\lambda A_k^p$, and we have shown that $A_k^p$ is absorbing.\nIt follows that $A_k^p$ satisfies the hypotheses of Lemma \\ref{lem:minkowski-bounded}, and $\\lambda$ defines a norm on ${\\mathbb R}^{d \\times m}$ with unit ball equal to $A_k^p$.  }\n\n\nSince the constraints in $T_k^p$ involve spectral functions, the sets $T_k^p$ and $A_k^p$ are invariant to left and right multiplication by orthogonal matrices. \nIt follows that $\\lambda$ is a spectral function, that is $\\lambda(W)$ is defined in terms of the singular values of $W$.\nBy von Neumann's Theorem \\cite{VonNeumann1937} the norm it defines is orthogonally invariant and we have\n\n", "itemtype": "equation", "pos": 55600, "prevtext": "\nWe show that $A_k^p$ is absorbing, bounded, convex and symmetric, and it follows by Lemma \\ref{lem:minkowski-bounded} that $\\lambda$ defines a norm on ${\\mathbb R}^{d \\times m}$ with unit ball equal to $A_k^p$.\nThe set $A_k^p$ is clearly bounded, convex and symmetric.  \nTo see that it is absorbing, let $W$ in $\\mathbb{R}^{d \\times m}$ have singular value decomposition $U \\Sigma V{^{\\scriptscriptstyle \\top}}$, and let $r=\\min(d,m)$. \nIf $W$ is zero then clearly $W\\in A_k^p$, so assume it is non zero. \n\n{For $i \\in {\\mathbb N}_r$ let $S_i\\in{\\mathbb R}^{d\\times m}$ have entry $(i,i)$ equal to $1$, and all remaining entries zero.\nWe then have\n\n", "index": 77, "text": "\\begin{align*}\nW&=  U \\Sigma V{^{\\scriptscriptstyle \\top}} \\\\\n&= U \\left(\\sum_{i=1}^r \\sigma_i S_i  \\right) V{^{\\scriptscriptstyle \\top}} \\\\\n&= \\left(\\sum_{i=1}^d \\sigma_i \\right)   \\sum_{i=1}^r \\frac{\\sigma_i}{\\sum_{j=1}^r \\sigma_j}(U S_i V {^{\\scriptscriptstyle \\top}}) \\\\\n& =: \\lambda \\sum_{i=1}^r \\lambda_i Z_i.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle W\" display=\"inline\"><mi>W</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=U\\Sigma V{{}^{\\scriptscriptstyle\\top}}\" display=\"inline\"><mrow><mo>=</mo><mi>U</mi><mi mathvariant=\"normal\">\u03a3</mi><mi>V</mi><msup><mi/><mo mathsize=\"71%\" stretchy=\"false\">\u22a4</mo></msup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=U\\left(\\sum_{i=1}^{r}\\sigma_{i}S_{i}\\right)V{{}^{%&#10;\\scriptscriptstyle\\top}}\" display=\"inline\"><mrow><mo>=</mo><mi>U</mi><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover></mstyle><msub><mi>\u03c3</mi><mi>i</mi></msub><msub><mi>S</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>V</mi><msup><mi/><mo mathsize=\"71%\" stretchy=\"false\">\u22a4</mo></msup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\left(\\sum_{i=1}^{d}\\sigma_{i}\\right)\\sum_{i=1}^{r}\\frac{\\sigma_%&#10;{i}}{\\sum_{j=1}^{r}\\sigma_{j}}(US_{i}V{{}^{\\scriptscriptstyle\\top}})\" display=\"inline\"><mrow><mo>=</mo><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msub><mi>\u03c3</mi><mi>i</mi></msub><mo>)</mo></mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover></mstyle><mstyle displaystyle=\"true\"><mfrac><msub><mi>\u03c3</mi><mi>i</mi></msub><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></msubsup><msub><mi>\u03c3</mi><mi>j</mi></msub></mrow></mfrac></mstyle><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><msub><mi>S</mi><mi>i</mi></msub><mi>V</mi><mmultiscripts><mo stretchy=\"false\">)</mo><mprescripts/><none/><mo mathsize=\"71%\" stretchy=\"false\">\u22a4</mo></mmultiscripts></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=:\\lambda\\sum_{i=1}^{r}\\lambda_{i}Z_{i}.\" display=\"inline\"><mrow><mo>=</mo><mo>:</mo><mi>\u03bb</mi><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover></mstyle><msub><mi>\u03bb</mi><mi>i</mi></msub><msub><mi>Z</mi><mi>i</mi></msub><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere we have used Equation \\eqref{eq:unitball}, which states that $C_k^p$ is the unit ball of the $(k,p)$-support norm. \n\nIt follows that the norm defined by \\eqref{eqn:k-sup-matrix-def-2} is the spectral $(k,p)$-support norm with unit ball given by $A_k^p$.\n\n\n\n\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:projection-to-k-infinity-unit-ball}}\n\n\nWe solve the optimization problem\n\n", "itemtype": "equation", "pos": 56998, "prevtext": "\nNow for each $i$, $\\Vert\\sigma( Z_i) \\Vert_p = \\Vert \\sigma(S_i) \\Vert_p = 1$, and $\\textrm{rank}(Z_i) = \\textrm{rank}(S_i) = 1$, so $Z_i \\in T_k^p$ for any $k {\\ensuremath{\\geqslant}} 1$. \nFurthermore $\\lambda_i \\in [0,1]$ and $\\sum_{i=1}^r \\lambda_i=1$, that is $(\\lambda_1, \\ldots, \\lambda_r)$ lies in the unit simplex in ${\\mathbb R}^d$,\n\nso $\\frac{1}{\\lambda}W$ is a convex combination of elements of $Z_i$, in other words $W \\in \\lambda A_k^p$, and we have shown that $A_k^p$ is absorbing.\nIt follows that $A_k^p$ satisfies the hypotheses of Lemma \\ref{lem:minkowski-bounded}, and $\\lambda$ defines a norm on ${\\mathbb R}^{d \\times m}$ with unit ball equal to $A_k^p$.  }\n\n\nSince the constraints in $T_k^p$ involve spectral functions, the sets $T_k^p$ and $A_k^p$ are invariant to left and right multiplication by orthogonal matrices. \nIt follows that $\\lambda$ is a spectral function, that is $\\lambda(W)$ is defined in terms of the singular values of $W$.\nBy von Neumann's Theorem \\cite{VonNeumann1937} the norm it defines is orthogonally invariant and we have\n\n", "index": 79, "text": "\\begin{align}\n\\lambda(W) &= {\\operatorname{\\vphantom{p}inf}} \\{ \\lambda>0: W \\in \\lambda A_k^p \\} \\notag \\\\\n&= {\\operatorname{\\vphantom{p}inf}} \\{ \\lambda >0: \\sigma(W) \\in \\lambda C_k^p \\}  \\notag \\\\\n&= \\Vert \\sigma(W) \\Vert_{(k)} \\notag\n\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\lambda(W)\" display=\"inline\"><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}\\{\\lambda&gt;0:W\\in\\lambda A_{k}^{p}\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mi>W</mi><mo>\u2208</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mi>A</mi><mi>k</mi><mi>p</mi></msubsup></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{\\vphantom{p}inf}}\\{\\lambda&gt;0:\\sigma(W)\\in\\lambda C%&#10;_{k}^{p}\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><mi>\u03bb</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>:</mo><mrow><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msubsup><mi>C</mi><mi>k</mi><mi>p</mi></msubsup></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\|\\sigma(W)\\|_{(k)}\\par&#10;\" display=\"inline\"><mrow><mi/><mo>=</mo><msub><mrow><mo>\u2225</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nWe consider two cases.  If $\\sum_{i=1}^d \\vert w_i \\vert {\\ensuremath{\\leqslant}} k$, then the problem decouples and we solve it componentwise.  \nSpecifically we minimize $(x_i-w_i)^2$ subject to $ \\vert x_i \\vert {\\ensuremath{\\leqslant}} 1$, and the solution is immediately given by\n\n", "itemtype": "equation", "pos": 57649, "prevtext": "\nwhere we have used Equation \\eqref{eq:unitball}, which states that $C_k^p$ is the unit ball of the $(k,p)$-support norm. \n\nIt follows that the norm defined by \\eqref{eqn:k-sup-matrix-def-2} is the spectral $(k,p)$-support norm with unit ball given by $A_k^p$.\n\n\n\n\n\n\\qed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Proof of Proposition \\ref{prop:projection-to-k-infinity-unit-ball}}\n\n\nWe solve the optimization problem\n\n", "index": 81, "text": "\\begin{align}\n{\\ensuremath{\\text{\\rm argmin}\\,}}_{x \\in {\\mathbb R}^d}  \\left\\{   \\sum_{i=1}^d (x_i-w_i)^2: \\vert x_i \\vert {\\ensuremath{\\leqslant}} 1, \\sum_{i=1}^d \\vert x_i \\vert {\\ensuremath{\\leqslant}} k     \\right\\}.\\label{eqn:projection-problem}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\text{\\rm argmin}\\,}_{x\\in{\\mathbb{R}}^{d}}\\left\\{\\sum_{i=1}^{d}%&#10;(x_{i}-w_{i})^{2}:|x_{i}|{\\leqslant}1,\\sum_{i=1}^{d}|x_{i}|{\\leqslant}k\\right\\}.\" display=\"inline\"><mrow><mrow><msub><mpadded width=\"+1.7pt\"><mtext>argmin</mtext></mpadded><mrow><mi>x</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mi>d</mi></msup></mrow></msub><mo>\u2062</mo><mrow><mo>{</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>:</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><mrow><mo stretchy=\"false\">|</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2a7d</mo><mi>k</mi></mrow></mrow><mo>}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nWe now assume that  $\\sum_{i=1}^d \\vert w_i \\vert > k$.  \nConsider the Lagrangian function $\\mathcal{L}(x,\\beta) = \\sum_{i=1}^d (x_i-w_i)^2 +2 \\beta \\left( \\sum_{i=1}^d \\vert x_i \\vert - k\\right)$ with nonnegative multiplier $\\beta$.  \nWe solve problem \\eqref{eqn:projection-problem} by minimizing the Lagrangian with respect to $x$, which can be done componentwise due to the coupling effect of the Lagrangian.  \nFurthermore, at the optimum the constraint $\\sum_{i=1}^d  \\vert x_i \\vert {\\ensuremath{\\leqslant}}  k$ will be tight. \n\nThe derivative with respect to $x_i$ is zero when $x_i = w_i - \\beta \\, \\text{sign}(x_i)$. \n\n\nIncorporating the constraint $\\vert x_i \\vert {\\ensuremath{\\leqslant}} 1$ we get the following solution\n\n", "itemtype": "equation", "pos": 58197, "prevtext": "\nWe consider two cases.  If $\\sum_{i=1}^d \\vert w_i \\vert {\\ensuremath{\\leqslant}} k$, then the problem decouples and we solve it componentwise.  \nSpecifically we minimize $(x_i-w_i)^2$ subject to $ \\vert x_i \\vert {\\ensuremath{\\leqslant}} 1$, and the solution is immediately given by\n\n", "index": 83, "text": "\\begin{align}\nx_i = \n\\begin{cases}\n-1, \\quad &\\text{if } w_i  < -1,\\\\\nw_i, \\quad &\\text{if } -1 {\\ensuremath{\\leqslant}} w_i {\\ensuremath{\\leqslant}} 1,\\\\\n1, \\quad &\\text{if }   w_i  >1.\n\\end{cases}\\label{eqn:pinf-1}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{i}=\\begin{cases}-1,&amp;\\text{if }w_{i}&lt;-1,\\\\&#10;w_{i},&amp;\\text{if }-1{\\leqslant}w_{i}{\\leqslant}1,\\\\&#10;1,&amp;\\text{if }w_{i}&gt;1.\\end{cases}\" display=\"inline\"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>&lt;</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>-</mo><mn>1</mn></mrow><mo>\u2a7d</mo><msub><mi>w</mi><mi>i</mi></msub><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>&gt;</mo><mn>1</mn></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $\\beta {\\ensuremath{\\geqslant}} 0$ is chosen such that $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert = k$.\nNote that for $\\beta=0$, which corresponds to $\\Vert w\\Vert_1 {\\ensuremath{\\leqslant}} k$, \\eqref{eqn:pinf-2} reduces to \\eqref{eqn:pinf-1}, hence we obtain the compact notation \\eqref{eqn:pinf-prox}.\n\\iffalse\n\n", "itemtype": "equation", "pos": 59158, "prevtext": "\nWe now assume that  $\\sum_{i=1}^d \\vert w_i \\vert > k$.  \nConsider the Lagrangian function $\\mathcal{L}(x,\\beta) = \\sum_{i=1}^d (x_i-w_i)^2 +2 \\beta \\left( \\sum_{i=1}^d \\vert x_i \\vert - k\\right)$ with nonnegative multiplier $\\beta$.  \nWe solve problem \\eqref{eqn:projection-problem} by minimizing the Lagrangian with respect to $x$, which can be done componentwise due to the coupling effect of the Lagrangian.  \nFurthermore, at the optimum the constraint $\\sum_{i=1}^d  \\vert x_i \\vert {\\ensuremath{\\leqslant}}  k$ will be tight. \n\nThe derivative with respect to $x_i$ is zero when $x_i = w_i - \\beta \\, \\text{sign}(x_i)$. \n\n\nIncorporating the constraint $\\vert x_i \\vert {\\ensuremath{\\leqslant}} 1$ we get the following solution\n\n", "index": 85, "text": "\\begin{align}\nx_i = \n\\begin{cases}\n-1, \t\t\t\\quad &\\text{if } w_i + \\beta <  -1,\\\\\nw_i +\\beta,\t\\quad &\\text{if } -1 {\\ensuremath{\\leqslant}} w_i + \\beta {\\ensuremath{\\leqslant}} 0,\\\\\nw_i -\\beta,\t\\quad &\\text{if }  0 {\\ensuremath{\\leqslant}} w_i - \\beta  {\\ensuremath{\\leqslant}} 1,\\\\\n1, \t\t\t\\quad &\\text{if }  w_i - \\beta  > 1,\n\\end{cases}\\label{eqn:pinf-2}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{i}=\\begin{cases}-1,&amp;\\text{if }w_{i}+\\beta&lt;-1,\\\\&#10;w_{i}+\\beta,&amp;\\text{if }-1{\\leqslant}w_{i}+\\beta{\\leqslant}0,\\\\&#10;w_{i}-\\beta,&amp;\\text{if }0{\\leqslant}w_{i}-\\beta{\\leqslant}1,\\\\&#10;1,&amp;\\text{if }w_{i}-\\beta&gt;1,\\end{cases}\" display=\"inline\"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mo>-</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>+</mo><mi>\u03b2</mi></mrow><mo>&lt;</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>\u03b2</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>-</mo><mn>1</mn></mrow><mo>\u2a7d</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>\u03b2</mi></mrow><mo>\u2a7d</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>-</mo><mi>\u03b2</mi></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mn>0</mn></mrow><mo>\u2a7d</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>-</mo><mi>\u03b2</mi></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo>&gt;</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $\\beta=0$ if $\\Vert w \\Vert_1 {\\ensuremath{\\leqslant}} k$, otherwise $\\beta$ is chosen such that $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert = k$.\n\\fi\nFinally, note that the expression $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert$ decreases monotonically as $\\beta$ increases.  \nIn the case that $\\Vert w \\Vert_1 > k$, $\\beta \\in (0, \\vert w_j \\vert -1) $, where $\\vert w_j \\vert = {\\ensuremath{\\text{\\rm argmin}\\,}} \\vert w_i \\vert$, hence we can determine $\\beta$ by binary search in $\\mathcal{O}(d \\log d)$ time. \n\n\\qed\n\n\n\n\nIn order to project onto the unit ball of radius $\\alpha>0$, we solve the optimization problem \n$\\min \\{   \\sum_{i=1}^d (x_i-w_i)^2~:~x \\in {\\mathbb R}^d,~\\vert  x_i \\vert  {\\ensuremath{\\leqslant}} \\alpha, ~\\sum_{i=1}^d \\vert x_i \\vert {\\ensuremath{\\leqslant}} \\alpha k  \\}$. \nTo do so, we make the change of variables $x_i' = x_i/\\alpha$ and note that the problem reduces to computing the projection $x'$ of $w'$ onto the unit ball of the norm, where $w_i' = w_i/\\alpha$, which is the problem \n\nthat was solved in \\eqref{eqn:projection-problem-body}.\nOnce this is done, our solution is given by $x_i=\\alpha x_i'(\\beta)$, where $x'(\\beta)$ is determined in accordance with Proposition \\ref{prop:projection-to-k-infinity-unit-ball}.\n\n\n\n\\subsection{Numerical Experiments}\nIn this section we report further experimental details and results not included in the main body of the paper for space reasons.\n\n\n\n{\\bf Simulated Datasets.}\nWe replicated the setting of \\cite{McDonald2014a} in order to verify that the additional parameter can improve performance. \nEach $100 \\times 100$ matrix is generated as $W=U V{^{\\scriptscriptstyle \\top}} +E$, where $U,V \\in \\mathbb{R}^{100\\times r}$, $r \\ll 100$, and the entries of $U$, $V$ and $E$ are i.i.d. standard Gaussian. \nTable \\ref{table:synthetic-mc} illustrates the results. \nThe error is measured as $\\Vert \\textrm{true} -  \\textrm{predicted}\\Vert^2 / \\Vert \\textrm{true} \\Vert^2$, standard deviations are shown in brackets and the mean values of $k$ and $p$ are selected by validation.  \nWe note that the spectral $(k,p)$-support norm outperforms the standard spectral $k$-support norm in each regime, and the improvement is statistically significant at a level $<0.01$. \n\n\n\n\n\n\\begin{table}[t]\n\\caption{Matrix completion on synthetic datasets generated with decaying spectrum. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. }\n\\vskip 0.1in\n\\label{table:synthetic-mc}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[th]{0.98\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                           & $k$ & $p$  \\\\\n\\midrule\nrank 5       & trace & 0.8184 (0.03)                         & -   &-  \\\\ \n$\\rho$=10\\%  & k-supp  & 0.8036 (0.03)                         &  3.6  & - \\\\ \n             &  kp-supp  & 0.7831 (0.03)                          & 1.8 & 7.3 \\\\ \n\\midrule\nrank 5       & trace  & 0.4085 (0.03)                         &  -  & - \\\\ \n$\\rho$=20\\%  & k-supp  & 0.4031 (0.03)                          &  3.1  & - \\\\ \n             & kp-supp  & 0.3996 (0.03)                          & 2.0 & 4.7 \\\\ \n\\midrule\nrank 10      & trace  & 0.6356 (0.03)             \t            &-  & -   \\\\ \n$\\rho$=20\\%  & k-supp  & 0.6284 (0.03)            \t            & 4.4 & -   \\\\ \n             & kp-supp  & 0.6270 (0.03)            \t            & 2.0 & 4.4  \\\\ \n\n\n\n\n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\n\n\n\n\n{\\bf Real Datasets.} \nThe MovieLens 100k dataset ({\\em http://grouplens.org/datasets/movielens/}) consists of 943 user ratings of 1,682 movies, the ratings are integers from 1 to 5, and all users have rated a minimum number of 20 films. \nThe Jester 1 dataset ({\\em http://goldberg.berkeley.edu/jester-data/}) consists of ratings of 24,983 users of 100 jokes, and the Jester 3 dataset consists of ratings of 34,938 users of 100 jokes, and the ratings are real values from $-10$ to $10$.\n\n\n\n\n\n\nFollowing \\cite{McDonald2014a,Toh2011}, for MovieLens for each user we uniformly sampled $\\rho=50\\%$ of available entries for training, and for Jester 1 and Jester 3 we sampled 20, respectively 8 ratings per user, using 10\\% for validation.\nWe used normalized mean absolute error, \n\n", "itemtype": "equation", "pos": 59839, "prevtext": "\nwhere $\\beta {\\ensuremath{\\geqslant}} 0$ is chosen such that $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert = k$.\nNote that for $\\beta=0$, which corresponds to $\\Vert w\\Vert_1 {\\ensuremath{\\leqslant}} k$, \\eqref{eqn:pinf-2} reduces to \\eqref{eqn:pinf-1}, hence we obtain the compact notation \\eqref{eqn:pinf-prox}.\n\\iffalse\n\n", "index": 87, "text": "\\begin{align}\nx_i = \n\\begin{cases}\n\\textrm{sign}(w_i) (\\vert w_i \\vert -\\beta),\t\\quad &\\text{if }  \\vert ( \\vert w_i \\vert -\\beta) \\vert {\\ensuremath{\\leqslant}} 1,\\\\\n\\textrm{sign}(w_i), \t\t\t\\quad &\\text{if }  \\vert (\\vert w_i \\vert - \\beta)\\vert  > 1,\n\\end{cases}\\notag\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle x_{i}=\\begin{cases}\\textrm{sign}(w_{i})(|w_{i}|-\\beta),&amp;\\text{if%&#10; }|(|w_{i}|-\\beta)|{\\leqslant}1,\\\\&#10;\\textrm{sign}(w_{i}),&amp;\\text{if }|(|w_{i}|-\\beta)|&gt;1,\\end{cases}\" display=\"inline\"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2a7d</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&gt;</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00449.tex", "nexttext": "\nwhere $r_{\\min}$ and $r_{\\max}$ are lower and upper bounds for the ratings \\cite{Toh2011}, \nand we implemented a final thresholding step as in \\cite{McDonald2014a}.\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 64425, "prevtext": "\nwhere $\\beta=0$ if $\\Vert w \\Vert_1 {\\ensuremath{\\leqslant}} k$, otherwise $\\beta$ is chosen such that $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert = k$.\n\\fi\nFinally, note that the expression $\\sum_{i=1}^d \\vert x_i(\\beta) \\vert$ decreases monotonically as $\\beta$ increases.  \nIn the case that $\\Vert w \\Vert_1 > k$, $\\beta \\in (0, \\vert w_j \\vert -1) $, where $\\vert w_j \\vert = {\\ensuremath{\\text{\\rm argmin}\\,}} \\vert w_i \\vert$, hence we can determine $\\beta$ by binary search in $\\mathcal{O}(d \\log d)$ time. \n\n\\qed\n\n\n\n\nIn order to project onto the unit ball of radius $\\alpha>0$, we solve the optimization problem \n$\\min \\{   \\sum_{i=1}^d (x_i-w_i)^2~:~x \\in {\\mathbb R}^d,~\\vert  x_i \\vert  {\\ensuremath{\\leqslant}} \\alpha, ~\\sum_{i=1}^d \\vert x_i \\vert {\\ensuremath{\\leqslant}} \\alpha k  \\}$. \nTo do so, we make the change of variables $x_i' = x_i/\\alpha$ and note that the problem reduces to computing the projection $x'$ of $w'$ onto the unit ball of the norm, where $w_i' = w_i/\\alpha$, which is the problem \n\nthat was solved in \\eqref{eqn:projection-problem-body}.\nOnce this is done, our solution is given by $x_i=\\alpha x_i'(\\beta)$, where $x'(\\beta)$ is determined in accordance with Proposition \\ref{prop:projection-to-k-infinity-unit-ball}.\n\n\n\n\\subsection{Numerical Experiments}\nIn this section we report further experimental details and results not included in the main body of the paper for space reasons.\n\n\n\n{\\bf Simulated Datasets.}\nWe replicated the setting of \\cite{McDonald2014a} in order to verify that the additional parameter can improve performance. \nEach $100 \\times 100$ matrix is generated as $W=U V{^{\\scriptscriptstyle \\top}} +E$, where $U,V \\in \\mathbb{R}^{100\\times r}$, $r \\ll 100$, and the entries of $U$, $V$ and $E$ are i.i.d. standard Gaussian. \nTable \\ref{table:synthetic-mc} illustrates the results. \nThe error is measured as $\\Vert \\textrm{true} -  \\textrm{predicted}\\Vert^2 / \\Vert \\textrm{true} \\Vert^2$, standard deviations are shown in brackets and the mean values of $k$ and $p$ are selected by validation.  \nWe note that the spectral $(k,p)$-support norm outperforms the standard spectral $k$-support norm in each regime, and the improvement is statistically significant at a level $<0.01$. \n\n\n\n\n\n\\begin{table}[t]\n\\caption{Matrix completion on synthetic datasets generated with decaying spectrum. The improvement of the $(k,p)$-support norm over the $k$-support and trace norms is statistically significant at a level $<0.001$. }\n\\vskip 0.1in\n\\label{table:synthetic-mc}\n\\centering\n\\setlength\\tabcolsep{4pt}\n\\begin{minipage}[th]{0.98\\linewidth}\n\\centering\n\\begin{small}\n\\begin{tabular}{llccc}\n\\toprule\n   dataset & norm  & test error                           & $k$ & $p$  \\\\\n\\midrule\nrank 5       & trace & 0.8184 (0.03)                         & -   &-  \\\\ \n$\\rho$=10\\%  & k-supp  & 0.8036 (0.03)                         &  3.6  & - \\\\ \n             &  kp-supp  & 0.7831 (0.03)                          & 1.8 & 7.3 \\\\ \n\\midrule\nrank 5       & trace  & 0.4085 (0.03)                         &  -  & - \\\\ \n$\\rho$=20\\%  & k-supp  & 0.4031 (0.03)                          &  3.1  & - \\\\ \n             & kp-supp  & 0.3996 (0.03)                          & 2.0 & 4.7 \\\\ \n\\midrule\nrank 10      & trace  & 0.6356 (0.03)             \t            &-  & -   \\\\ \n$\\rho$=20\\%  & k-supp  & 0.6284 (0.03)            \t            & 4.4 & -   \\\\ \n             & kp-supp  & 0.6270 (0.03)            \t            & 2.0 & 4.4  \\\\ \n\n\n\n\n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\end{minipage}\n\\end{table}\n\n\n\n\n\n\n\n\n{\\bf Real Datasets.} \nThe MovieLens 100k dataset ({\\em http://grouplens.org/datasets/movielens/}) consists of 943 user ratings of 1,682 movies, the ratings are integers from 1 to 5, and all users have rated a minimum number of 20 films. \nThe Jester 1 dataset ({\\em http://goldberg.berkeley.edu/jester-data/}) consists of ratings of 24,983 users of 100 jokes, and the Jester 3 dataset consists of ratings of 34,938 users of 100 jokes, and the ratings are real values from $-10$ to $10$.\n\n\n\n\n\n\nFollowing \\cite{McDonald2014a,Toh2011}, for MovieLens for each user we uniformly sampled $\\rho=50\\%$ of available entries for training, and for Jester 1 and Jester 3 we sampled 20, respectively 8 ratings per user, using 10\\% for validation.\nWe used normalized mean absolute error, \n\n", "index": 89, "text": "$$\n\\textrm{NMAE}=\\frac{\\Vert \\text{true}-\\text{predicted}\\Vert^2}{\\#\\text{obs.}/(r_{\\max} -r_{\\min})},\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"\\textrm{NMAE}=\\frac{\\|\\text{true}-\\text{predicted}\\|^{2}}{\\#\\text{obs.}/(r_{%&#10;\\max}-r_{\\min})},\" display=\"block\"><mrow><mrow><mtext>NMAE</mtext><mo>=</mo><mfrac><msup><mrow><mo>\u2225</mo><mrow><mtext>true</mtext><mo>-</mo><mtext>predicted</mtext></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup><mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mtext>obs.</mtext></mrow><mo>/</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>r</mi><mi>max</mi></msub><mo>-</mo><msub><mi>r</mi><mi>min</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}]