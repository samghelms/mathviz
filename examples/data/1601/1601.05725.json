[{"file": "1601.05725.tex", "nexttext": "\n}\n\nThis form is common in irregular unsymmetric systems, such as those from circuit simulation~\\cite{klu}.\nIn this form, only submatrices on the diagonal ($A_{ii}$) need to be factored resulting in far less work, reduced memory usage, and a great deal of parallelism.\nIn addition to fill reduction, permuting the matrix to limit pivoting by placing nonzeros on the diagonal is common before computation~\\cite{superludist}.\nFinding such a permutation is done through finding a maximum cardinality matching of a bipartite graph representation of the coefficient matrix~\\cite{duffmatching}. \nHowever, nonzeros on the diagonal is only one half of the issue; a variant that also tries to maximize the values on the diagonal is often used.\nWe will call this variant {{maximum weight-cardinality matching ordering}\\xspace} ({{MWCM}\\xspace})~\\cite{duffmatching}.\n\n\\noindent\\textbf{Sparse LU.}\nWe consider three popular solver packages, namely {{SuperLU-Dist}\\xspace}~\\cite{superludist}, {{Pardiso}\\xspace}~\\cite{pardiso1}, and {{KLU}\\xspace}~\\cite{klu}, to compare their design choices to {\\emph{Basker}\\xspace}.\n\n{{SuperLU-Dist}\\xspace} is a distributed memory unsymmetric direct solver ~\\cite{superludist}\nthat uses a two-dimensional data layout and avoids pivoting by using {{MWCM}\\xspace} that maximizes the sum of the diagonal element (MC64)~\\cite{duffmatching}.\nIn each block matrix, {{SuperLU-Dist}\\xspace} performs a supernodal based $LU$ factorization.\nHowever, supernodal methods have limitations such as\na pivot can only be chosen from inside a single supernode, fill-in must be known before hand, and scaling is limited by the size of supernodes~\\cite{superlumt}.\nA shared-memory version {{SuperLU-MT}\\xspace}~\\cite{superlumt} that uses a one-dimensional data layout exists.\n\n{{Pardiso}\\xspace}~\\cite{pardiso1} is a shared-memory, supernodal, sparse $LU$ solver that uses a number of techniques to achieve high performance.\nThese techniques include using a left-right looking strategy to reduce synchronization and provide three levels of parallelism, namely from the etree, hybrid (left-right) at top levels, and pipelining parallelism. \nWe compare against Intel MKL version of {{Pardiso}\\xspace} and {{SuperLU-MT}\\xspace} in Section~\\ref{sec:results}.\n\n{{KLU}\\xspace}~\\cite{klu} is a serial direct solver, based on the {{Gilbert-Peierls algorithm}\\xspace}, and the closest\nto our effort in algorithmic terms.\nIt achieves good performance by permuting the matrices first into {{BTF}\\xspace}.\nIt then uses the {{Gilbert-Peierls algorithm}\\xspace} to discover the nonzero pattern due to fill-in during numeric factorization in time proportional to arithmetic operations (Algorithm~\\ref{alg:gp}~\\cite{gilbertlu}).\nHowever, {{KLU}\\xspace} has no method to factor any part in parallel.\n{\\emph{Basker}\\xspace} was designed to replace {{KLU}\\xspace} for circuit simulation problems by adding parallel execution both between blocks and within blocks of the {{BTF}\\xspace}.\nIt is part of Trilinos library through both Amesos2~\\cite{amesos2} and ShyLU~\\cite{shylu} packages.\n\n\\input{gp}\n\nThe primary features of {\\emph{Basker}\\xspace} are:\n(1) It is a nonsupernodal factorization;\n(2) It uses a hierarchical data layouts;\n(3) It uses both {{MWCM}\\xspace} and pivoting;\n(4) It is a templated C++ solver using a manycore portable package supporting multiple backends such as OpenMP and PThreads.\n\n\n\\section{Basker Algorithm}\n\\label{sec:alg}\n\nThis section introduces the parallel symbolic (pattern only) and numeric\n(pattern and values) factorization algorithms in {\\emph{Basker}\\xspace}.\nThe nonzero pattern of the coefficient matrix and the data-layout,i.e., how matrix \nentries are stored, determines not only the work but\nalso the available parallelism to a sparse factorization. \nSerial/multithreaded $LU$ \nfactorization codes traditionally utilize a flat one-dimensional (1D) layout of blocks where blocks contain nonzeros in rows/columns stored contiguously.\nThese blocks are derived from some ordering of the matrix (e.g., See Figure~\\ref{fig:onedlayout}).\nHowever, using only 1D layouts limit the algorithms from exploiting sparsity\npatterns within and between block structures.  \nFor instance, a 1D multithreaded supernodal factorization's speedup will be\n limited by the threaded BLAS on a set of columns (rows) called separators\n(e.g, the block column $7$ in Figure~\\ref{fig:onedlayout}). \nWhen these columns are not\ndense, like in circuit/powergrid problems the use of BLAS is limited leading\nto a serial bottleneck in the separators.\nDue to this observation, {\\emph{Basker}\\xspace} uses a variety of reordering\nmethods, such as {{BTF}\\xspace} and ND, to derive a hierarchy of two-dimensional sparse\nblocks.  This reordering allows {\\emph{Basker}\\xspace} to fit the irregular nonzero pattern\ninto a hierarchy of blocks that fit the memory structure of modern nodes\nand allow an algorithm that can utilize the 2D layouts (called 2D algorithm).\n2D algorithms break columns into multiple submatrices \n(e.g., See Figures~\\ref{fig:btf_both},\\ref{fig:nd})\nallowing\nfor multiple threads to work on a column that would have been serial in a\nnonsupernodal method or efficiently use multiple calls of serial\nBLAS.\n\nIn this work, we will focus on two levels of structures, i.e., \n{{BTF}\\xspace} and ND.  We leave the third level (supernodes) within the 2D blocks\nfor future extensions.\n{{BTF}\\xspace} provides both the coarse structure for the whole matrix, \\emph{and} the fine structure for a collection of submatrices.\n{{ND}\\xspace} provides the fine structure for very large submatrices from BTF.\nThe fine structure of ND is used to arrive at a parallel 2D {{Gilbert-Peierls algorithm}\\xspace}.\n\nThe notation used in this section is as follows.\nA submatrix is given as $A_{ij}$, where $i$ and $j$ are the indices in the row and column in the two-dimensional block structure.\nThe nonzero pattern of a column ($c$) in a submatrix $A_{ij}$ is given as ${\\ensuremath{\\mathcal{{A}}}\\xspace}_{ij}(c)$. We use C++ notation for comments in the algorithms.\n\n\n\\begin{figure}[htbp]\n  \\centering\n  \\subfigure[]\n     {\n       \\includegraphics[width=0.22\\textwidth]{onedlayout.png}\n       \\label{fig:onedlayout}\n     }\n  \\subfigure[]\n  {\n\t\t\\includegraphics[width=0.23\\textwidth,height=0.16\\textwidth]{tree3.png}\n    \\label{fig:onedtree}\n  }\n  \\caption{(a) One-dimensional layout of an ND-structure/binary $etree$ structure. The block $[A_{17} A_{77}]$ limits performance. The coloring provides one assignment of threads to computation. (b) Dependency tree of one-dimensional layout.  Note the large top level nodes that must be factored by one thread.}\n  \\label{fig:oned}\n\\end{figure}\n\n\\subsection{Coarse Block Triangular Structure}\n{\\emph{Basker}\\xspace} uses {{block triangular form}\\xspace} (BTF) on the input matrix to compute a coarse structure. \nIt permutes the matrix based on an\nordering found from {{MWCM}\\xspace} ($P_{m1}$) to ensure a non-zero diagonal with\nlarge entries.\nA strongly connected components algorithm is used next to reorder the matrix ($P_{c}$) such that each component corresponds to a \nblock diagonal. \nThe reordered matrix, i.e., $P_{c}P_{m1}AP_{c}$, produces a structure similar to that in Figure~\\ref{fig:btf}.\nThis form is common to matrices from several domains, and is well studied~\\cite{pothenfan}.\nAny of the large diagonal blocks may or may not exist for a particular matrix.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{btf.png}\n\t\t\\label{fig:btf}\n\t}\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{lower.png}\n\t\t\\label{fig:lower}\n\t}\n\t\\caption{(a) Coarse structure, BTF ($P_{c}P_{m}AP_{c}^{T}$). The first level allows {\\emph{Basker}\\xspace} to reduce factorization work by only factoring the diagonal blocks. (b) Representation of fine BTF structure, i.e., $D_{1}$ and $D_{3}$. Coloring of the blocks suggest one possible mapping of thread and blocks. } \n    \\label{fig:btf_both}\n\\end{figure} \n\nIn Figure~\\ref{fig:btf}, a two-dimensional structure with three diagonal blocks is shown.\nAs the multiple tiny subblocks in $D_{1}$ and $D_{3}$ provide enough natural\nparallelism (for factoring each block), {\\emph{Basker}\\xspace} uses this ordering derived from BTF as their second level structure as well.\nThe submatrices from this second level structure are handled using a\n\\emph{Fine Block Triangular Structure} based method.\nIn contrast, $D_{2}$ is very large without an opportunity to expose parallelism.\nWe will use ND to reorder $D_{2}$ further and use \\emph{Fine Nested-Dissection Structure} based method.\n\n\\subsection{Fine Block Triangular Structure}\nA typical representation of fine BTF structure, such as $D_{1}$ and $D_{3}$, is given in Figure~\\ref{fig:lower}.\nThe substructure is easily dealt with as the subblocks are independent of each other.\nTherefore, the sparsity pattern and factorization of each subblock ($A_{ii}$) can be computed concurrently.\nA two-dimensional sparse block structure is used here.  \nThe off-diagonal blocks are ``partitioned'' in a manner  to help the sparse matrix-vector multiplication when solving for a given right-hand side vector.\nThey could further be split, however\nthey tend to be very sparse as they retain the original nonzero pattern.\n\n\\noindent\\textbf{Parallel Symbolic Factorization.}\nThe symbolic factorization algorithm for the fine BTF block is shown in Algorithm~\\ref{alg:sfactsmall}. It is embarrassingly parallel over the blocks.\nWe reorder each diagonal submatrix using {{AMD}\\xspace} (Line 2) for fill-reduction.\nNext, we find the number of nonzeros of each column and estimate the number of\nfloating-point operations required to factor (Line 3).\nUsing the number of floating-point operations, {\\emph{Basker}\\xspace} assigns the submatrices among the threads and memory for $LU$ factors can be allocated.\nThe colors in Figure~\\ref{fig:lower} provides one such assignment for four threads.\n\n\\input{sfactsmall_word}\n\n\\noindent\\textbf{Parallel Numeric Factorization.}\nAfter symbolic factorization, the numeric factorization uses the same thread mapping to submatrices to call sparse $LU$ factorization using {{Gilbert-Peierls algorithm}\\xspace}.\nThe algorithm is not shown as it is a simple parallel-for loop over the diagonal submatrices.\n\n\\subsection{Fine Nested-Dissection Structure}\nA subblock, such as $D_{2}$ in Figure~\\ref{fig:btf} could be too large\nto be factored in serial as in the above {{BTF}\\xspace} fine structure method.\nThis block could easily dominate the factorization time, but there is no simple\nway to factor this block with multiple threads with natural ordering.\nThis block constitutes an average of $68.4\\%$ of the total matrix size in our problem test suite (see Section~\\ref{sec:results}). As observed before,\nusing a 1D layout (Figure~\\ref{fig:onedlayout}) does not provide enough parallelism. Instead we reorder\nthis block even further into finer 2D blocks.\nUsing this structure,we design the first parallel {{Gilbert-Peierls algorithm}\\xspace} so multiple threads can work on a single column.\n\nThe {{nested-dissection ordering}\\xspace} is used in order to discover smaller independent subblocks to factor in parallel.\n{\\emph{Basker}\\xspace} first permutes $D_{2}$ using a {{MWCM}\\xspace} ($P_{m2}$) to find the locally best matching and reduce the need to pivot.\nNext, {\\emph{Basker}\\xspace} computes the {{ND}\\xspace} ordering on the graph of $D_{2}$+$D_{2}^{T}$ with a {{ND}\\xspace} tree.\n{\\emph{Basker}\\xspace} currently limits the number of leafs in the {{ND}\\xspace} tree to the number of threads available ($p$).\nWe note that increasing the number of leafs in the {{ND}\\xspace} tree may provide smaller cache friendly submatrices, but would limit the amount of pivoting allowed.\nThis trade-off is not explored in this paper. \nAdditionally, current implementations of {{ND}\\xspace} provide only a binary tree, and therefore, {\\emph{Basker}\\xspace} is limited to using a power of two threads.\nThe {{ND}\\xspace} ordering ($P_{nd}$) results in $P_{nd}P_{m2}D_{2}P_{nd}^{T}$, and the reordered matrix is given in Figure~\\ref{fig:nd} for four threads.\nThis two-dimensional structure of sparse matrices is used to store both the reordered matrix and factorization ($LU$).\nThe colors suggest one possible layout where blocks of a particular color are shared by a thread.  \n\n\\noindent\\textbf{Dependency Tree.} \n{\\emph{Basker}\\xspace} requires a method to map the {{ND}\\xspace} structure to threads.\nOne option is to use a task-dependency graph, and use a tasking runtime.\nHowever, {\\emph{Basker}\\xspace} is currently limited to using data-parallel methods\n(parallel-for) due to dependence on Kokkos and integration requirements with\nTrilinos and Xyce.  {\\emph{Basker}\\xspace} does this by\ntransforming a task-dependency graph into a dependency tree that represents\nlevel sets that can be executed in parallel.\n\nFigure~\\ref{fig:ndtree} provides a general dependency tree used by both symbolic and numeric factorization for the two-dimensional matrix in Figure~\\ref{fig:nd}, and is read from the bottom-up.\nThis tree represents two levels of dependency.\nThe first level dependencies are between matrices within a node. \nWithin each node, matrices listed in a particular row depend on matrices listed in rows below in the same node.\nFor example, $L_{31}$ depends on having $LU_{11}$.\nThe second level dependencies are between nodes and are represented with arrows.\nThe levels in the dependency tree is denoted as $treelevel$, and $treelevel$ will always be used for only the dependency tree (not the $etree$ or {{ND}\\xspace} tree).\nNodes are colored to match the thread mapping in Figure~\\ref{fig:nd}. \nNote that this tree is different from a {{ND}\\xspace} tree, and\nexpresses the concurrency in the hierarchical layout so {\\emph{Basker}\\xspace} can use level scheduling.\nOne can easily see the difference with Figure \\ref{fig:onedtree} where the root node\nrepresents the entire $LU_{77}$ block column, whereas in the new dependence tree\n$LU_{17} \\ldots LU_{67}$ are distributed to multiple threads and the bottleneck\nin the root node is much smaller.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.35\\textwidth]{nd.png}\n\t\t\\label{fig:nd}\n\t}\n\\subfigure[]\n{\n\t\t\\includegraphics[width=0.35\\textwidth]{tree2.png}\n\t\t\\label{fig:ndtree}\n}\n\\caption{(a) Matrix in nested-dissection ordering of $D_{2}$. Each submatrix is stored by {\\emph{Basker}\\xspace} as a sparse matrix. One possible thread layout indicated by color. Note, $LU$ will be stored in the same two-dimensional structure. (b) Dependency tree based off {{ND}\\xspace} structure.  The dependency is read from the bottom up, both within and between nodes.  The colors represent a static mapping of threads similar to those in (a).}   \n\\end{figure}\n\n\n\\noindent\\textbf{Parallel Symbolic Factorization.}\n{\\emph{Basker}\\xspace} now needs an accurate estimate of the nonzero count for the two-dimensional $LU$ factors found in parallel (Algorithm~\\ref{alg:sfactlarge}).\nA parallel symbolic factorization is crucial in a multithreaded environment as\nrepeated reallocation for $LU$ factors would require a system call,\nwhich is a performance bottleneck when done in a parallel region. \nWe do not form the $etree$ of the whole matrix and instead build\nthe appropriate portions of the $etree$ in different threads.\n\n{\\emph{Basker}\\xspace} first processes the bottom two levels in the dependency tree (Line 2-9) to obtain an accurate nonzero count.\nThe bottom most level of the dependency tree, i.e., $treelevel$ -1, has submatrices corresponding to $A_{11}, A_{22}, A_{33}, A_{44}$. First,\nwe find both the nonzero count per column and the $etree_{i}$~\\cite{davisbook} of either $etree(A_{ii}$+$A_{ii}^{T})$ or $etree(A_{ii}A_{ii}^{T}$) (depending on symmetry and pivoting options) in parallel (Line 5).\nSecond, the nonzero counts for remaining $L_{ik}$ in the node at $treelevel$ -1 is found (Line 6). \nWe note that \n", "itemtype": "equation", "pos": 7977, "prevtext": "\n\n\n\\title{Basker: A Threaded Sparse LU Factorization Utilizing  Hierarchical Parallelism and Data Layouts}\n\n\\author{\\IEEEauthorblockN{Joshua Dennis Booth}\n\\IEEEauthorblockA{Sandia National Laboratories\\\\\nAlbuquerque, New Mexico\\\\\njdbooth@sandia.gov}\n\\and\n\\IEEEauthorblockN{Sivasankaran Rajamanickam}\n\\IEEEauthorblockA{Sandia National Laboratories\\\\\nAlbuquerque, New Mexico\\\\\nsrajama@sandia.gov}\n\\and\n\\IEEEauthorblockN{Heidi K. Thornquist}\n\\IEEEauthorblockA{Sandia National Laboratories \\\\\nAlbuquerque, New Mexico,\\\\\nhkthorn@sandia.gov}\n}\n\n\\maketitle\n\n\n\\begin{abstract}\nScalable sparse $LU$ factorization is critical for efficient numerical simulation\nof circuits and electrical power grids.  \nIn this work, we present a new scalable sparse direct solver called \\emph{Basker}.\n\\emph{Basker} introduces a new algorithm to parallelize the {{Gilbert-Peierls algorithm}\\xspace} for sparse $LU$ factorization.\nAs architectures evolve, there exists a need for algorithms that are\nhierarchical in nature to match the\nhierarchy in thread teams, individual threads, and vector level parallelism.\n{\\emph{Basker}\\xspace} is designed to map well to this hierarchy in architectures.\nThere is also a need for data layouts to match multiple\nlevels of hierarchy in memory. {\\emph{Basker}\\xspace}\nuses a two-dimensional hierarchical structure of sparse matrices that maps to\nthe hierarchy in the memory architectures and to the hierarchy in parallelism.\nWe present performance evaluations of {\\emph{Basker}\\xspace} on the Intel SandyBridge and Xeon Phi\nplatforms using circuit and power grid matrices taken from the University of Florida\nsparse matrix collection and from Xyce circuit simulations.\n{\\emph{Basker}\\xspace} achieves a geometric mean speedup of\n$5.91\\times$ on CPU (16 cores) and $7.4\\times$ on Xeon Phi (32 cores) relative to KLU.\n{\\emph{Basker}\\xspace} outperforms Intel MKL Pardiso (PMKL) by as much as $53\\times$ on CPU\n(16 cores) and $13.3\\times$ on Xeon Phi (32 cores) for low fill-in circuit matrices.\nFurthermore, {\\emph{Basker}\\xspace} provides $5.4\\times$ speedup on a challenging matrix sequence taken \nfrom an actual Xyce simulation.\n\\end{abstract}\n\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\\label{sec:intro}\n\nScalable sparse direct linear solvers play a pivotal role in the efficiency of simulation codes on many-core systems.\nCurrent approaches process multiple columns with similar nonzero structure (supernodes) with threaded Basic Linear Algebra Subprograms (BLAS)~\\cite{superlu}.\nThe approach of using BLAS with one-dimensional data layouts of these matrices may not be able to extract enough parallelism when the matrix has low fill-in or an irregular nonzero pattern, such as matrices generated by Simulation Program with Integrated Circuit Emphasis (SPICE).\nTherefore, a new type of solver is needed that uses a hierarchical  structures to leverage fine-grain parallelism within the irregular nonzero pattern.\nIn this work, we present a new shared-memory sparse direct $LU$ solver, {\\emph{Basker}\\xspace}, designed to use hierarchical data layouts that exposes fine-grain parallelism and naturally fit the hierarchical memory structure of most many-core systems.\n{\\emph{Basker}\\xspace} is targeted towards parallelizing the state-of-the-art {{Gilbert-Peierls algorithm}\\xspace}~\\cite{gilbertlu} for low fill-in problems and thereby becoming the first parallel shared-memory solver to do so.\n \nSparse factorization of unsymmetric indefinite systems is difficult due to the need for numerical pivoting for stability and dynamic nonzero structure generated by such pivoting.\nScaling sparse $LU$ therefore depends on efficiently finding concurrent work inside this dynamic nonzero structure while providing enough numerical stability.\nAs a result, speedups achievable for sparse factorization is far from ideal~\\cite{mumps, pardiso1}.\nCoefficient matrices with low fill-in are particularly difficult, since the existence of supernodes is limited.\nHowever, a hierarchical  structure can often be found in these matrices that can expose multiple levels of parallelism.\n     \n\n{\\emph{Basker}\\xspace} uses a hierarchy of two-dimensional sparse blocks designed to exploit the nonzero structure that can be found in a matrix from circuit/powergrid problems.\nThese blocks can be found using traditional ordering techniques, such as {{block triangular form}\\xspace}~\\cite{klu} and  {{nested-dissection ordering}\\xspace}~\\cite{scotch}.\nThis hierarchy of two-dimensional sparse blocks design allows {\\emph{Basker}\\xspace} to accomplish two goals:\n(1) exploiting any fine-grained parallelism found within or between blocks and\n(2) designing a hierarchical data structure that fits the multiple levels of memory hierarchy and divide data among threads appropriately.\nAs a result, {\\emph{Basker}\\xspace} enables parallelization of {{Gilbert-Peierls algorithm}\\xspace} by allowing multiple threads work simultaneously on a single matrix column. \n\n\nIn this work, we present the algorithm and data layouts used by {\\emph{Basker}\\xspace} to achieve hierarchical parallelism.\n{\\emph{Basker}\\xspace} is a implemented in templated C++11 with Kokkos~\\cite{kokkos}. Kokkos is a package providing portability across multiple {{manycore processors}\\xspace} and device backends. \nThe main contributions of this work are:\n\\begin{itemize}\n\\item Parallelization of the Gilbert-Peierls algorithm;\n\\item A method to expose hierarchical parallelism in sparse matrices using two dimensional data-layouts;\n\\item A new threaded sparse direct $LU$ solver that outperforms Intel MKL's {{Pardiso}\\xspace}~\\cite{pardiso1} and {{KLU}\\xspace}~\\cite{klu} while reducing memory usage on matrices with low fill-in;\n\\item Empirical evaluation of {\\emph{Basker}\\xspace}, KLU, and Pardiso on the Intel Sandy Bridge and Xeon Phi architectures.\n\\item Performance evaluation with $1000$ matrices from a transient simulation performed by the Xyce circuit simulator\n\\end{itemize}\n\nThe remainder of this paper is organized as follows.\nSection~\\ref{sec:background} presents an overview of previous solver work.\nWe then introduce the hierarchically structured algorithm to extract parallelism from sparse matrices in Section~\\ref{sec:alg}.\nImplementation choices are outlined in Section~\\ref{sec:impl}.\nSection~\\ref{sec:results} provides performance results and comparisons with\nother solvers.  Finally, possible future improvements and a summary of our\nfindings are described in Section~\\ref{sec:conclusion}.\n\n\n\\section{Background and Related Work}\n\\label{sec:background}\n\nThis section provides a brief overview of background and related work to the solution of the sparse linear system $Ax=b$, where $A$ is a large sparse coefficient matrix, $x$ is the solution vector, $b$ is the given right-hand side vector.\n\n\\noindent\\textbf{Orderings.}\nAll sparse direct solvers use structural information to improve performance and scalability.\nCoefficient matrices are often reordered to limit fill-in, i.e., zeros becoming nonzero during factorization, or cluster nonzeros into patterns that reveal dependencies in computation.\nMinimum degree orderings, such as {{approximate minimum degree ordering}\\xspace} (AMD), are a type of ordering that is very efficient in reducing fill-in~\\cite{amd}. \nNested-dissection (ND)~\\cite{scotch} is another ordering based on the graph($G$) corresponding to a matrix, using $G(A)$ when $A$ is symmetric and $G(A$+$A^{T})$ when $A$ is unsymmetric. It is commonly used to provide a tree-structure  that can be used in parallel factorizations while reducing fill-in.\n\nIf an unsymmetric matrix does not have the strong Hall property, i.e., if every set of $k$ columns has nonzeros in at least $k$+$1$ rows, then the matrix can be permuted into a {{block triangular form}\\xspace} ({{BTF}\\xspace}) where block submatrices in the lower triangular part are all zeros.\nA coefficient matrix $A$ permuted by matrices $P$ and $Q$ into {{BTF}\\xspace} has the form:\n\n{{\\fontsize{8}{9}\\selectfont}\n", "index": 1, "text": "\n\\[\nPAQ = \n\\begin{bmatrix}\nA_{11} & A_{12} & \\cdots & A_{1k} \\\\\n       & A_{22} &        & \\vdots \\\\\n\t\t\t &        & \\ddots & \\vdots \\\\\n\t\t\t &        &        & A_{kk}\n\\end{bmatrix} .\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"PAQ=\\begin{bmatrix}A_{11}&amp;A_{12}&amp;\\cdots&amp;A_{1k}\\\\&#10;&amp;A_{22}&amp;&amp;\\vdots\\\\&#10;&amp;&amp;\\ddots&amp;\\vdots\\\\&#10;&amp;&amp;&amp;A_{kk}\\end{bmatrix}.\" display=\"block\"><mrow><mrow><mrow><mi>P</mi><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>Q</mi></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>A</mi><mn>11</mn></msub></mtd><mtd columnalign=\"center\"><msub><mi>A</mi><mn>12</mn></msub></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ef</mi></mtd><mtd columnalign=\"center\"><msub><mi>A</mi><mrow><mn>1</mn><mo>\u2062</mo><mi>k</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><msub><mi>A</mi><mn>22</mn></msub></mtd><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22f1</mi></mtd><mtd columnalign=\"center\"><mi mathvariant=\"normal\">\u22ee</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><mi/></mtd><mtd columnalign=\"center\"><msub><mi>A</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05725.tex", "nexttext": "\nAlso, pivoting while factorizing $A_{ii}$ will not affect ${\\ensuremath{\\mathcal{{ L}}}\\xspace}_{ik}(c)$ as $k > i$ by the fill-path theorem~\\cite{rosetwo}. \nTherefore, {\\emph{Basker}\\xspace} can use the above expression to find the nonzeros counts of the lower-diagonal submatrices.\nMoreover, we find a data structure $lest$ with the maximum and minimum row index for each column $c$ that will be used for estimating nonzero counts in higher $treelevel$. \nAt $treelevel$ 0, nonzero counts for the upper-diagonal submatrices, i.e., $U_{ki}$, can be found (Line 8). \nAs ${\\ensuremath{\\mathcal{{U}}}\\xspace}_{ki}(c)$ may depend on the pivoting on $A_{ii}$ the $etree_{i}$ must be used. \nFor each column ($c$), the method counts the nodes encountered starting from each nonzero in the column of ${\\ensuremath{\\mathcal{{A}}}\\xspace}_{ki}(c)$ to the least common ancestor of any nonzero already explored, where the least common ancestor of two nodes is the least numbered node that is the ancestor of both.\nA data structure $uest$ is returned with the maximum and minimum row index for each row.\n\n\\input{sfactlarge_word}\n\nThe estimated nonzero counts for submatrices in the higher levels of the dependency tree are found using the estimates $lest$ and $uest$ by looping over the remaining $treelevels$ (Line 11).\nAt each $treelevel$, all the nodes on the level are handled by finding the nonzero count of the diagonal subblock, e.g., $LU_{33}$ (Line 14).\nNow, \n\n", "itemtype": "equation", "pos": 24107, "prevtext": "\n}\n\nThis form is common in irregular unsymmetric systems, such as those from circuit simulation~\\cite{klu}.\nIn this form, only submatrices on the diagonal ($A_{ii}$) need to be factored resulting in far less work, reduced memory usage, and a great deal of parallelism.\nIn addition to fill reduction, permuting the matrix to limit pivoting by placing nonzeros on the diagonal is common before computation~\\cite{superludist}.\nFinding such a permutation is done through finding a maximum cardinality matching of a bipartite graph representation of the coefficient matrix~\\cite{duffmatching}. \nHowever, nonzeros on the diagonal is only one half of the issue; a variant that also tries to maximize the values on the diagonal is often used.\nWe will call this variant {{maximum weight-cardinality matching ordering}\\xspace} ({{MWCM}\\xspace})~\\cite{duffmatching}.\n\n\\noindent\\textbf{Sparse LU.}\nWe consider three popular solver packages, namely {{SuperLU-Dist}\\xspace}~\\cite{superludist}, {{Pardiso}\\xspace}~\\cite{pardiso1}, and {{KLU}\\xspace}~\\cite{klu}, to compare their design choices to {\\emph{Basker}\\xspace}.\n\n{{SuperLU-Dist}\\xspace} is a distributed memory unsymmetric direct solver ~\\cite{superludist}\nthat uses a two-dimensional data layout and avoids pivoting by using {{MWCM}\\xspace} that maximizes the sum of the diagonal element (MC64)~\\cite{duffmatching}.\nIn each block matrix, {{SuperLU-Dist}\\xspace} performs a supernodal based $LU$ factorization.\nHowever, supernodal methods have limitations such as\na pivot can only be chosen from inside a single supernode, fill-in must be known before hand, and scaling is limited by the size of supernodes~\\cite{superlumt}.\nA shared-memory version {{SuperLU-MT}\\xspace}~\\cite{superlumt} that uses a one-dimensional data layout exists.\n\n{{Pardiso}\\xspace}~\\cite{pardiso1} is a shared-memory, supernodal, sparse $LU$ solver that uses a number of techniques to achieve high performance.\nThese techniques include using a left-right looking strategy to reduce synchronization and provide three levels of parallelism, namely from the etree, hybrid (left-right) at top levels, and pipelining parallelism. \nWe compare against Intel MKL version of {{Pardiso}\\xspace} and {{SuperLU-MT}\\xspace} in Section~\\ref{sec:results}.\n\n{{KLU}\\xspace}~\\cite{klu} is a serial direct solver, based on the {{Gilbert-Peierls algorithm}\\xspace}, and the closest\nto our effort in algorithmic terms.\nIt achieves good performance by permuting the matrices first into {{BTF}\\xspace}.\nIt then uses the {{Gilbert-Peierls algorithm}\\xspace} to discover the nonzero pattern due to fill-in during numeric factorization in time proportional to arithmetic operations (Algorithm~\\ref{alg:gp}~\\cite{gilbertlu}).\nHowever, {{KLU}\\xspace} has no method to factor any part in parallel.\n{\\emph{Basker}\\xspace} was designed to replace {{KLU}\\xspace} for circuit simulation problems by adding parallel execution both between blocks and within blocks of the {{BTF}\\xspace}.\nIt is part of Trilinos library through both Amesos2~\\cite{amesos2} and ShyLU~\\cite{shylu} packages.\n\n\\input{gp}\n\nThe primary features of {\\emph{Basker}\\xspace} are:\n(1) It is a nonsupernodal factorization;\n(2) It uses a hierarchical data layouts;\n(3) It uses both {{MWCM}\\xspace} and pivoting;\n(4) It is a templated C++ solver using a manycore portable package supporting multiple backends such as OpenMP and PThreads.\n\n\n\\section{Basker Algorithm}\n\\label{sec:alg}\n\nThis section introduces the parallel symbolic (pattern only) and numeric\n(pattern and values) factorization algorithms in {\\emph{Basker}\\xspace}.\nThe nonzero pattern of the coefficient matrix and the data-layout,i.e., how matrix \nentries are stored, determines not only the work but\nalso the available parallelism to a sparse factorization. \nSerial/multithreaded $LU$ \nfactorization codes traditionally utilize a flat one-dimensional (1D) layout of blocks where blocks contain nonzeros in rows/columns stored contiguously.\nThese blocks are derived from some ordering of the matrix (e.g., See Figure~\\ref{fig:onedlayout}).\nHowever, using only 1D layouts limit the algorithms from exploiting sparsity\npatterns within and between block structures.  \nFor instance, a 1D multithreaded supernodal factorization's speedup will be\n limited by the threaded BLAS on a set of columns (rows) called separators\n(e.g, the block column $7$ in Figure~\\ref{fig:onedlayout}). \nWhen these columns are not\ndense, like in circuit/powergrid problems the use of BLAS is limited leading\nto a serial bottleneck in the separators.\nDue to this observation, {\\emph{Basker}\\xspace} uses a variety of reordering\nmethods, such as {{BTF}\\xspace} and ND, to derive a hierarchy of two-dimensional sparse\nblocks.  This reordering allows {\\emph{Basker}\\xspace} to fit the irregular nonzero pattern\ninto a hierarchy of blocks that fit the memory structure of modern nodes\nand allow an algorithm that can utilize the 2D layouts (called 2D algorithm).\n2D algorithms break columns into multiple submatrices \n(e.g., See Figures~\\ref{fig:btf_both},\\ref{fig:nd})\nallowing\nfor multiple threads to work on a column that would have been serial in a\nnonsupernodal method or efficiently use multiple calls of serial\nBLAS.\n\nIn this work, we will focus on two levels of structures, i.e., \n{{BTF}\\xspace} and ND.  We leave the third level (supernodes) within the 2D blocks\nfor future extensions.\n{{BTF}\\xspace} provides both the coarse structure for the whole matrix, \\emph{and} the fine structure for a collection of submatrices.\n{{ND}\\xspace} provides the fine structure for very large submatrices from BTF.\nThe fine structure of ND is used to arrive at a parallel 2D {{Gilbert-Peierls algorithm}\\xspace}.\n\nThe notation used in this section is as follows.\nA submatrix is given as $A_{ij}$, where $i$ and $j$ are the indices in the row and column in the two-dimensional block structure.\nThe nonzero pattern of a column ($c$) in a submatrix $A_{ij}$ is given as ${\\ensuremath{\\mathcal{{A}}}\\xspace}_{ij}(c)$. We use C++ notation for comments in the algorithms.\n\n\n\\begin{figure}[htbp]\n  \\centering\n  \\subfigure[]\n     {\n       \\includegraphics[width=0.22\\textwidth]{onedlayout.png}\n       \\label{fig:onedlayout}\n     }\n  \\subfigure[]\n  {\n\t\t\\includegraphics[width=0.23\\textwidth,height=0.16\\textwidth]{tree3.png}\n    \\label{fig:onedtree}\n  }\n  \\caption{(a) One-dimensional layout of an ND-structure/binary $etree$ structure. The block $[A_{17} A_{77}]$ limits performance. The coloring provides one assignment of threads to computation. (b) Dependency tree of one-dimensional layout.  Note the large top level nodes that must be factored by one thread.}\n  \\label{fig:oned}\n\\end{figure}\n\n\\subsection{Coarse Block Triangular Structure}\n{\\emph{Basker}\\xspace} uses {{block triangular form}\\xspace} (BTF) on the input matrix to compute a coarse structure. \nIt permutes the matrix based on an\nordering found from {{MWCM}\\xspace} ($P_{m1}$) to ensure a non-zero diagonal with\nlarge entries.\nA strongly connected components algorithm is used next to reorder the matrix ($P_{c}$) such that each component corresponds to a \nblock diagonal. \nThe reordered matrix, i.e., $P_{c}P_{m1}AP_{c}$, produces a structure similar to that in Figure~\\ref{fig:btf}.\nThis form is common to matrices from several domains, and is well studied~\\cite{pothenfan}.\nAny of the large diagonal blocks may or may not exist for a particular matrix.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{btf.png}\n\t\t\\label{fig:btf}\n\t}\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{lower.png}\n\t\t\\label{fig:lower}\n\t}\n\t\\caption{(a) Coarse structure, BTF ($P_{c}P_{m}AP_{c}^{T}$). The first level allows {\\emph{Basker}\\xspace} to reduce factorization work by only factoring the diagonal blocks. (b) Representation of fine BTF structure, i.e., $D_{1}$ and $D_{3}$. Coloring of the blocks suggest one possible mapping of thread and blocks. } \n    \\label{fig:btf_both}\n\\end{figure} \n\nIn Figure~\\ref{fig:btf}, a two-dimensional structure with three diagonal blocks is shown.\nAs the multiple tiny subblocks in $D_{1}$ and $D_{3}$ provide enough natural\nparallelism (for factoring each block), {\\emph{Basker}\\xspace} uses this ordering derived from BTF as their second level structure as well.\nThe submatrices from this second level structure are handled using a\n\\emph{Fine Block Triangular Structure} based method.\nIn contrast, $D_{2}$ is very large without an opportunity to expose parallelism.\nWe will use ND to reorder $D_{2}$ further and use \\emph{Fine Nested-Dissection Structure} based method.\n\n\\subsection{Fine Block Triangular Structure}\nA typical representation of fine BTF structure, such as $D_{1}$ and $D_{3}$, is given in Figure~\\ref{fig:lower}.\nThe substructure is easily dealt with as the subblocks are independent of each other.\nTherefore, the sparsity pattern and factorization of each subblock ($A_{ii}$) can be computed concurrently.\nA two-dimensional sparse block structure is used here.  \nThe off-diagonal blocks are ``partitioned'' in a manner  to help the sparse matrix-vector multiplication when solving for a given right-hand side vector.\nThey could further be split, however\nthey tend to be very sparse as they retain the original nonzero pattern.\n\n\\noindent\\textbf{Parallel Symbolic Factorization.}\nThe symbolic factorization algorithm for the fine BTF block is shown in Algorithm~\\ref{alg:sfactsmall}. It is embarrassingly parallel over the blocks.\nWe reorder each diagonal submatrix using {{AMD}\\xspace} (Line 2) for fill-reduction.\nNext, we find the number of nonzeros of each column and estimate the number of\nfloating-point operations required to factor (Line 3).\nUsing the number of floating-point operations, {\\emph{Basker}\\xspace} assigns the submatrices among the threads and memory for $LU$ factors can be allocated.\nThe colors in Figure~\\ref{fig:lower} provides one such assignment for four threads.\n\n\\input{sfactsmall_word}\n\n\\noindent\\textbf{Parallel Numeric Factorization.}\nAfter symbolic factorization, the numeric factorization uses the same thread mapping to submatrices to call sparse $LU$ factorization using {{Gilbert-Peierls algorithm}\\xspace}.\nThe algorithm is not shown as it is a simple parallel-for loop over the diagonal submatrices.\n\n\\subsection{Fine Nested-Dissection Structure}\nA subblock, such as $D_{2}$ in Figure~\\ref{fig:btf} could be too large\nto be factored in serial as in the above {{BTF}\\xspace} fine structure method.\nThis block could easily dominate the factorization time, but there is no simple\nway to factor this block with multiple threads with natural ordering.\nThis block constitutes an average of $68.4\\%$ of the total matrix size in our problem test suite (see Section~\\ref{sec:results}). As observed before,\nusing a 1D layout (Figure~\\ref{fig:onedlayout}) does not provide enough parallelism. Instead we reorder\nthis block even further into finer 2D blocks.\nUsing this structure,we design the first parallel {{Gilbert-Peierls algorithm}\\xspace} so multiple threads can work on a single column.\n\nThe {{nested-dissection ordering}\\xspace} is used in order to discover smaller independent subblocks to factor in parallel.\n{\\emph{Basker}\\xspace} first permutes $D_{2}$ using a {{MWCM}\\xspace} ($P_{m2}$) to find the locally best matching and reduce the need to pivot.\nNext, {\\emph{Basker}\\xspace} computes the {{ND}\\xspace} ordering on the graph of $D_{2}$+$D_{2}^{T}$ with a {{ND}\\xspace} tree.\n{\\emph{Basker}\\xspace} currently limits the number of leafs in the {{ND}\\xspace} tree to the number of threads available ($p$).\nWe note that increasing the number of leafs in the {{ND}\\xspace} tree may provide smaller cache friendly submatrices, but would limit the amount of pivoting allowed.\nThis trade-off is not explored in this paper. \nAdditionally, current implementations of {{ND}\\xspace} provide only a binary tree, and therefore, {\\emph{Basker}\\xspace} is limited to using a power of two threads.\nThe {{ND}\\xspace} ordering ($P_{nd}$) results in $P_{nd}P_{m2}D_{2}P_{nd}^{T}$, and the reordered matrix is given in Figure~\\ref{fig:nd} for four threads.\nThis two-dimensional structure of sparse matrices is used to store both the reordered matrix and factorization ($LU$).\nThe colors suggest one possible layout where blocks of a particular color are shared by a thread.  \n\n\\noindent\\textbf{Dependency Tree.} \n{\\emph{Basker}\\xspace} requires a method to map the {{ND}\\xspace} structure to threads.\nOne option is to use a task-dependency graph, and use a tasking runtime.\nHowever, {\\emph{Basker}\\xspace} is currently limited to using data-parallel methods\n(parallel-for) due to dependence on Kokkos and integration requirements with\nTrilinos and Xyce.  {\\emph{Basker}\\xspace} does this by\ntransforming a task-dependency graph into a dependency tree that represents\nlevel sets that can be executed in parallel.\n\nFigure~\\ref{fig:ndtree} provides a general dependency tree used by both symbolic and numeric factorization for the two-dimensional matrix in Figure~\\ref{fig:nd}, and is read from the bottom-up.\nThis tree represents two levels of dependency.\nThe first level dependencies are between matrices within a node. \nWithin each node, matrices listed in a particular row depend on matrices listed in rows below in the same node.\nFor example, $L_{31}$ depends on having $LU_{11}$.\nThe second level dependencies are between nodes and are represented with arrows.\nThe levels in the dependency tree is denoted as $treelevel$, and $treelevel$ will always be used for only the dependency tree (not the $etree$ or {{ND}\\xspace} tree).\nNodes are colored to match the thread mapping in Figure~\\ref{fig:nd}. \nNote that this tree is different from a {{ND}\\xspace} tree, and\nexpresses the concurrency in the hierarchical layout so {\\emph{Basker}\\xspace} can use level scheduling.\nOne can easily see the difference with Figure \\ref{fig:onedtree} where the root node\nrepresents the entire $LU_{77}$ block column, whereas in the new dependence tree\n$LU_{17} \\ldots LU_{67}$ are distributed to multiple threads and the bottleneck\nin the root node is much smaller.\n\n\\begin{figure}[htbp]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.35\\textwidth]{nd.png}\n\t\t\\label{fig:nd}\n\t}\n\\subfigure[]\n{\n\t\t\\includegraphics[width=0.35\\textwidth]{tree2.png}\n\t\t\\label{fig:ndtree}\n}\n\\caption{(a) Matrix in nested-dissection ordering of $D_{2}$. Each submatrix is stored by {\\emph{Basker}\\xspace} as a sparse matrix. One possible thread layout indicated by color. Note, $LU$ will be stored in the same two-dimensional structure. (b) Dependency tree based off {{ND}\\xspace} structure.  The dependency is read from the bottom up, both within and between nodes.  The colors represent a static mapping of threads similar to those in (a).}   \n\\end{figure}\n\n\n\\noindent\\textbf{Parallel Symbolic Factorization.}\n{\\emph{Basker}\\xspace} now needs an accurate estimate of the nonzero count for the two-dimensional $LU$ factors found in parallel (Algorithm~\\ref{alg:sfactlarge}).\nA parallel symbolic factorization is crucial in a multithreaded environment as\nrepeated reallocation for $LU$ factors would require a system call,\nwhich is a performance bottleneck when done in a parallel region. \nWe do not form the $etree$ of the whole matrix and instead build\nthe appropriate portions of the $etree$ in different threads.\n\n{\\emph{Basker}\\xspace} first processes the bottom two levels in the dependency tree (Line 2-9) to obtain an accurate nonzero count.\nThe bottom most level of the dependency tree, i.e., $treelevel$ -1, has submatrices corresponding to $A_{11}, A_{22}, A_{33}, A_{44}$. First,\nwe find both the nonzero count per column and the $etree_{i}$~\\cite{davisbook} of either $etree(A_{ii}$+$A_{ii}^{T})$ or $etree(A_{ii}A_{ii}^{T}$) (depending on symmetry and pivoting options) in parallel (Line 5).\nSecond, the nonzero counts for remaining $L_{ik}$ in the node at $treelevel$ -1 is found (Line 6). \nWe note that \n", "index": 3, "text": "$${\\ensuremath{\\mathcal{{L}}}\\xspace}_{ik}(c) = {\\ensuremath{\\mathcal{{A}}}\\xspace}_{ik}(c)  \\bigcup\\limits_{t=1}^{c-1} \\{  {\\ensuremath{\\mathcal{{L}}}\\xspace}_{ik}(t) | t \\in {\\ensuremath{\\mathcal{{U}}}\\xspace}_{ii}(c) \\}~\\cite{rose}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{{L}}}_{ik}(c)={\\mathcal{{A}}}_{ik}(c)\\bigcup\\limits_{t=1}^{c-1}\\{{%&#10;\\mathcal{{L}}}_{ik}(t)|t\\in{\\mathcal{{U}}}_{ii}(c)\\}~{}\\@@cite[cite]{[%&#10;\\@@bibref{Refnum}{rose}{}{}]}.\" display=\"block\"><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"5.8pt\" stretchy=\"false\">}</mo></mrow><mo>\u2062</mo><mtext class=\"ltx_citemacro_cite\"><cite xmlns=\"http://www.w3.org/1999/xhtml\" class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rose</span>]</cite></mtext></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05725.tex", "nexttext": "\nfor these blocks, where ${\\ensuremath{\\mathcal{{L}}}\\xspace}_{jk}{\\ensuremath{\\mathcal{{U}}}\\xspace}_{kj}(c)$ is the pattern after the multiplication of $L_{jk}U_{kj}(c)$.\n{\\emph{Basker}\\xspace} estimates an upper bound of ${\\ensuremath{\\mathcal{{L}}}\\xspace}_{jk}{\\ensuremath{\\mathcal{{U}}}\\xspace}_{kj}(c)$ using the $lest$ and $uest$ by assuming the column is dense between the minimum and maximum if $lest$ and $uest$ overlap for the column.\nWe find that this is a reasonable upper bound and cheaper than storing the whole nonzero pattern.\nFinally, the column count of any off-diagonal submatrices, such as $L_{73}$ and $U_{37}$, can be computed (Line 15 and 16). \nThe column count for these submatrices use the upper bound as well (i.e., fill-in estimated with $lest$ and $uest$).\n\n\\begin{figure*}[!t]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{sone.png}\n\t\t\\label{fig:sone}\n\t}\n\t\\subfigure[$slevel$ 1]\n\t{\n\t\t\\includegraphics[width=0.08\\textwidth]{stwonew.png}\n\t\t\\label{fig:stwo}\n\t}\n\t\\subfigure[Last separator column, $slevel$ 2]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{sthree.png}\n\t\t\\label{fig:sthree}\n\t}\n\t\\subfigure[]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{sfour.png}\n\t\t\\label{fig:sfour}\n\t}\n\t\\subfigure[]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{sfive.png}\n\t\t\\label{fig:sfive}\n\t}\n\t\\subfigure[]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{ssix.png}\n\t\t\\label{fig:ssix}\n\t}\n\t\\subfigure[]\n\t{\n\t\n\t\t\\includegraphics[width=0.24\\textwidth]{sseven.png}\n\t\t\\label{fig:sseven}\n\t}\n\\caption{Workflow of Algorithm~\\ref{alg:nfactlarge} (Numeric Factorization for Fine ND structure). The subblock forming the lower triangle are subblocks of $L$ and the red line indicates column of $A$ being factored. Note, the only serial bottleneck, i.e., a single colored block, is the bottom right most block in (g).}\n\t\t\\label{fig:nfactorfig}\n\\end{figure*}\n\n\n\n\\noindent\\textbf{Parallel Numeric Factorization.}\nThis subsection describes the parallel left-looking {{Gilbert-Peierls algorithm}\\xspace} (Algorithm \\ref{alg:nfactlarge}).\nTo facilitate understanding, we explain the algorithm using a series of block diagrams of the execution in Figure~\\ref{fig:nfactorfig}.\nBlocks that are not colored gray represent submatrices that are active/used at a stage, and the colors correspond to the thread mapping as in Figure~\\ref{fig:ndtree}.  \n\n\\input{nfactlarge_word}\n\nSubmatrices are factored based on the dependency tree in Figure~\\ref{fig:ndtree} in a \\emph{column-by-column} manner.\nFigure~\\ref{fig:sone} starts with the submatrices in $treelevel$ -1. \n{\\emph{Basker}\\xspace} factors the submatrices on the diagonal that have no dependencies, i.e., computing $LU_{ii}(c)$ (Line 4).\nThis factorization uses the {{Gilbert-Peierls algorithm}\\xspace} similar to Algorithm~\\ref{alg:gp} in parallel on each\nsubmatrix.\nNext, the just computed column $U_{ii}(c)$ is used to compute column $c$ in the lower off-diagonal submatrices in the node at $treelevel$ -1, e.g., $L_{31}(c)$ and $L_{71}(c)$ (Line 5).\nThis is done by discovering the nonzero pattern as a result of parallel sparse matrix-vector multiplication.\nAt $treelevel$ -1, a level synchronization between all threads is needed \nbefore moving to next $treelevel$.\nNote that {\\emph{Basker}\\xspace} need not necessarily sync all threads if done in a task parallel manner. \n\nThe nodes in the dependency tree starting at $treelevel = 0$ has a subtle\nbut important distinction.\nAll submatrices in a tree node are not computed before moving to next node\nas in the symbolic factorization.\nIn contrast, only those submatrices in a tree node corresponding to a particular column $slevel$ are computed (Line 9). The $slevel$ indicates multiple\npasses over the dependency tree (bottom up until $treelevel$).\nFigures~\\ref{fig:sthree}- \\ref{fig:sseven} show the block diagram of $slevel=2$ with $treelevel=0,1, \\mbox{ and } 2$, where the red line indicates the column being factored.\nSubmatrices at this $treelevel=0$ (Figure~\\ref{fig:sthree}), e.g., $U_{17}$, are factorized in parallel using a method similar to Algorithm~\\ref{alg:gp} except that $L_{ii}$ is used for the backsolve (Line 14).\n\n{\\emph{Basker}\\xspace} continues up the dependency tree with a loop over $treelevel$ (Line 15).\nAt each new level, {\\emph{Basker}\\xspace} must synchronize specific threads in order to combine their results (Line 18).\nFigure~\\ref{fig:sfour} shows the blocks used in the reduction.\nThe reduction has two phases. \nThe first phase is multiple parallel sparse matrix-vector multiplication of the matrices colored in $L$ and the column of $U(c)$ just found (the red line in the colored blocks).\nThe second phase is subtracting each threads' matrix-vector product from the corresponding blocks in $A$ (where gray blocks in the column are $A_{37}(c)$, $A_{67}(c)$ and $A_{77}(c)$).\nFor example, one thread computes the reduction results in $\\hat{A}_{37}(c) = A_{37}(c) - L_{31}U_{17}(c) - L_{32}U_{27}(c)$. $\\hat{A}_{67}(c)$ and $\\hat{A}_{77}(c)$  are computed in\nparallel as well.\nOnce the reduction is complete, the newly updated submatrix at $treelevel$ can be factored similar to other upper off-diagonal matrices (Line 20).\nFigure~\\ref{fig:sfive} provides a visual representation of this step.\nAt the last step, when $treelevel = slevel = 2$, at the root, there is one\nreduction needed to the already computed $\\hat{A}_{77}(c)$ (Line 24, \nFigure~\\ref{fig:ssix}) and then a simple factorization in the diagonal block\ncan be computed\n(Line 26, \nFigure~\\ref{fig:sseven}). This last factorization is the \\emph{only} serial\nbottleneck in the algorithm.\n\nIn the more general case, when $treelevel$ = $slevel$ (Line 22) and we are not\nat the root node (not shown in the figures), \nthere is no farther bottom-up traversal of the dependency tree. This would have been true for the $treelevel = slevel = 1$ for block column three in our example.\nIn matrix terms, this means that $U(c)$ for a column has been computed\nand only the block diagonal and $L$ remain to be computed (e.g, $L33(c)$,\n$U_{33}(c)$, and $L_{73}(c)$). This requires a reduction (Line 24)\nand factoring the diagonal submatrix (Line 26) as before, but\nany lower off-diagonal submatrices of $L$ that remain, such as $L_{73}(c)$,\n need to be factored as well (Line 28).\n\n\n\\section{Basker Implementation}\n\\label{sec:impl}\n\\noindent\\textbf{Data Layout.}\n{\\emph{Basker}\\xspace} uses a hierarchy of two-dimensional sparse matrix blocks  to store both the original matrix and $LU$ factors.\nThe 2D structure is composed of multiple compressed sparse column (CSC) format matrices.  \nParallelism must be extracted from between blocks in the {{BTF}\\xspace} structure and within large blocks in order to achieve speedup on low fill-in matrices.\nIn particular, a hierarchical structure needs to be exploited to reveal more parallelism.\nAdditionally, this also breaks the problem into fine-grain data structures that better fit the structure of memory in modern many-core nodes.\n{\\emph{Basker}\\xspace} implements this by building this structure of C++ classes during the symbolic factorization after applying the aforementioned orderings.\n\n\\noindent\\textbf{Synchronization.}\nLight weight synchronizations are needed to allow multiple threads to work on a single column in {\\emph{Basker}\\xspace}.\nThere are multiple places where these synchronizations need to happen in {\\emph{Basker}\\xspace}, and they are marked in Algorithm~\\ref{alg:nfactlarge}.\nThe number of threads that need to synchronize depends on location and iteration in the algorithm.\nFor instance, all threads need to synchronize moving from factoring leaf nodes and parent nodes, but only two threads need to sync in separator columns.\n \nA traditional data-parallel approach launches parallel-for over a set of threads, and these threads rejoin the master only after the end of the loop.\nHowever, if synchronization takes place between all threads at every level, the overhead would be too high.\nIn particular, the total time spent for synchronization in this manner for matrix G2 Circuit with 8 cores is 11\\% of total time. \nTherefore, {\\emph{Basker}\\xspace} uses a different mechanism to synchronize between threads.\nThis mechanism is a point-to-point synchronization that utilizes writing to a volatile variable where synchronization only happens between two threads that have a dependency.\nPoint-to-point synchronization's importance in the speedup of sparse triangular solve has been shown before~\\cite{pointcom}.\nUsing this method, {\\emph{Basker}\\xspace} is able to reduce synchronization overhead to 2.3\\% ($\\sim 79\\%$ improvement) of total runtime for G2 Circuit.\n\n\n\\section{Empirical Evaluation}\n\\label{sec:results}\nWe evaluate {\\emph{Basker}\\xspace} against Pardiso MKL 11.2.2 (PMKL), SuperLU-MT 3.0 (SLU-MT), and KLU 1.3.2 on a set of sparse matrices from circuit and powergrid simulations in terms of memory and runtime. \nOur MWCM implementation is similar to MC64 bottle-neck ordering~\\cite{duffmatching}, unlike SuperLU-Dist's product/sum based MC64 ordering. \nScotch~\\cite{scotch} 6.0 is used to obtain the {{ND}\\xspace} ordering. \nFurthermore, we compare {\\emph{Basker}\\xspace}'s performance on a sequence of $1000$ matrices from circuit simulation of interest.\n\n\\subsection{Experimental Setup}\n\n\\input{test_suite}\n\n\\noindent\\textbf{System Setup.}\nWe use two test beds for our experiments.  The first system has two eight-core\nXeon E5-2670 running at 2.6GHz (SandyBridge).  The two processors are\ninterconnected using Intel's QuickPath Interconnect (QPI), and share 24GB of\nDRAM.  The second system has an Intel Xeon Phi coprocessor with 61 cores running\nat 1.238GHz and 16GB of memory.\nSince {\\emph{Basker}\\xspace} requires a power of two threads, we only test up to 32 cores as 64 threads would oversubscribe the device. \nAll codes are compiled using Intel 15.2 with\n-03 optimization.\n\n\n\\noindent\\textbf{Test Suite.}\n{\\emph{Basker}\\xspace} is evaluated over a test suite of circuit and powergrid matrices taken from Xyce and the University of Florida Sparse Matrix Collection~\\cite{mat}.\nThese matrices vary in size, sparsity pattern, and number of BTF blocks.\nAdditionally, these matrices vary in fill-in density, i.e., $\\frac{|L+U|}{|A|}$ where $|A|$ is the number of nonzeros in $A$.\nWe note that fill-in can be $< 1$ when using {{BTF}\\xspace}, since only the diagonal subblocks of $A$ are factored to $LU$.\nIn Davis and Natarajan~\\cite{klu}, coefficient matrices coming from circuit simulation generally have lower fill-in density than those coming from PDE simulations, i.e., $\\frac{|L+U|}{|A|} < 4.0$.\nMatrices with lower fill-in tend to perform better using a {{Gilbert-Peierls algorithm}\\xspace} than a supernodal approach.\nFor fairness, we include seven matrices with fill-in density larger than $4.0$.\nTable I lists all matrices sorted by increasing fill-in density measured using KLU.\nThe percent of matrix rows in small independent diagonal submatrices (Fine BTF Structure) is shown as BTF\\%. The total number of BTF blocks is also shown.\nA double line divides matrices with fill-in density higher than $4.0$. \nThe test suite is\na mix of matrices with very different properties to exercise different portions of {\\emph{Basker}\\xspace}.\n\n\n\\subsection{Memory Usage}\nWe now compare memory requirements in terms of $|L$+$U|$.\nTable I lists the number of nonzeros in $L$+$U$ for KLU, PMKL, and {\\emph{Basker}\\xspace}.\nWe do not report results for SLU-MT due performance considerations (shown below).\nThe nonzeros reported for PMKL and {\\emph{Basker}\\xspace} are from a run using 8 cores on SandyBridge. We note that this number varies slightly for {\\emph{Basker}\\xspace} depending on number of cores.\nThe best result between PMKL and {\\emph{Basker}\\xspace} is in bold. \nWe observe that {\\emph{Basker}\\xspace} provides factors with less nonzero entries for most matrices with fill-in density $<$ 4.\nThis reduction can be as high as an \\emph{order of magnitude} for the matrix RS\\_b678c2+. This is the result of using the BTF structure and using fill reducing ordering on the subblocks.\nHowever, PMKL uses slightly less memory on matrix with fill-in density $>$ 4.\nThe additional memory used by {\\emph{Basker}\\xspace} on these matrices is far less than the additional memory used by PMKL on the first group of matrices.\n\n\\subsection{Performance}\nWe first compare the general performance of the chosen sparse solver packages.\nOnly the numeric time is compared, since the symbolic factorization of both {\\emph{Basker}\\xspace} and PMKL is limited by finding {{ND}\\xspace} ordering.\nFigure~\\ref{fig:superlumt} gives the raw time on Intel SandyBridge for a selection of six matrices.\nThese six matrices are selected due to their varying fill-in density, and ordered increasing from a density of $1.3$ to $9.2$. \nWe first observe that PMKL is as good or better than SuperLU-MT. Similar results have been reported in the past~\\cite{vecparshylu} in comparing against SuperLU-Dist for circuit problems.\nAdditionally, {\\emph{Basker}\\xspace} performs better than other solvers in 5/6 matrices.\nFor this reason, we only perform additional comparison to PMKL.\n\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.49\\textwidth]{superlumt.pdf}\n  \\caption{Comparison of {\\emph{Basker}\\xspace}, PMKL, and SLU-MT raw time (seconds) on SandyBridge. SLU-MT only does better on Power0 and fails on rajat21. }\n  \\label{fig:superlumt}\n\\end{figure}\n\n\\subsection{Scalability}\nIn this section, we focus on the scalability of the numeric factorization phase of {\\emph{Basker}\\xspace} and PMKL on the two architectures.\nWe use the relative speedup to KLU as that is the state-of-the-art sequential\nsolver, i.e., $Speedup(matrix,solver,p) =\n\\frac{Time(matrix,KLU,1)}{Time(matrix,solver,p)}$, where $Time$ is the time of\nthe numeric factorization phase, $matrix$ is the input matrix, $solver$ is\neither {\\emph{Basker}\\xspace} or PMKL, and $p$ is the number of cores.\n\n\n\\begin{figure}[ht!]\n  \\centering\n  \\subfigure[]\n  {\n  \t\\includegraphics[width=0.49\\textwidth]{ipdps_cpu_speedups.pdf}\n  \t\\label{fig:speedupcpu}  \n  }\n  \n  \\vspace{-10pt}\n  \\subfigure[]\n  {\n    \n    \\includegraphics[width=0.49\\textwidth]{ipdps_phi_speedups.pdf}\n    \\label{fig:speedupphi}\n   }\n  \\caption{Speedup of {\\emph{Basker}\\xspace} and PMKL relative to KLU on Intel SandyBridge. KLU time is given in the title of each figure (a) and Xeon Phi (b) on six matrices that vary in fill-in density from low to high (left to right). Both Freescale1 and Xyce3 are considered to have high fill-in for {\\emph{Basker}\\xspace}.  }\n\\end{figure}\n\n\nFigure~\\ref{fig:speedupcpu} shows the speedup achieved for these six matrices on SandyBridge platform.\nWe provide $Time(matrix,KLU,1)$ in the title of each figure. \nWe observe that {\\emph{Basker}\\xspace} can achieve up to $11.15\\times$ speedup (hvdc2) and outperform PMKL in all but one case (Xyce3) that has a high fill-density of 9.2.\nMoreover, we observe that PMKL has a speedup less than 1 in serial for four problems demonstrating the inefficiency of a supernodal algorithm to a {{Gilbert-Peierls algorithm}\\xspace} for matrices with low fill-in density.\nBy adding more cores, PMKL is not able to recover from this inefficiency and reaches a max speedup of $2.34\\times$ on the first four problems.\nThe reason for this is due to semi-dense columns that {\\emph{Basker}\\xspace} is able to avoid factoring. \nPMKL does factor Xyce3 faster with its high fill-in density, but {\\emph{Basker}\\xspace} scales\nin a similar way.\n\nThe relative speedup of the same six matrices on the Intel Xeon Phi are shown in Figure~\\ref{fig:speedupphi}.\nAgain, KLU time is provided in each figure's title.\nOn Intel Xeon Phi, {\\emph{Basker}\\xspace} is able to out perform PMKL on four out of the six matrices. \n{\\emph{Basker}\\xspace} achieves a $10.76\\times$ maximum speedup (Power0) on these six matrices and PMKL achieves $63\\times$ maximum speedup (Xyce3).\nWe observe that any overhead from using a {{Gilbert-Peierls algorithm}\\xspace} on a matrix with high fill-in density is magnified by the Intel Phi.\nThis is exposed and seen in both Freescale1 and Xyce3.\nOne possible reason for this is that the submatrices in the lowest level of the hierarchical structure are too large to fit into a core's L2 cache ($512KB$).\n{\\emph{Basker}\\xspace} currently makes the submatrices as large as possible to allow for better pivoting. \nHowever, {\\emph{Basker}\\xspace} still achieves speedups higher than PMKL on the four matrices with low fill-in density. \n\n\n\\begin{figure*}[!th]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=.315\\textwidth,height=.23\\textwidth]{ppcpu1new.png}\n\t\t\\label{fig:pp1cpu}\n\t}\n\t\\vspace{-5pt}\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=.315\\textwidth,height=.23\\textwidth]{ppcpu16new.png}\n\t\t\\label{fig:pp16cpu}\n\t}\n\t\\vspace{-5pt}\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=.315\\textwidth,height=.23\\textwidth]{ppmic32new.png}\n\t\t\\label{fig:pp32phi}\n\t}\t\n\t\\caption{Performance profiles of {\\emph{Basker}\\xspace} and PMKL on Intel SandyBridge and Xeon Phi.  A point (x,y) represents the fraction y of test problems within x$\\times$ of the best solver. (a) 1 SandyBridge Core. {\\emph{Basker}\\xspace} is the best solver for almost 70\\% of the matrices and PMKL is the best solver for about $30\\%$. (b) 16 SandyBridge Cores. {\\emph{Basker}\\xspace} is the best solver for almost $80\\%$ of the matrices, while PMKL is the best solver for slightly more than $20\\%$. (c)32 Phi Cores. {\\emph{Basker}\\xspace} is the best solver for over $70\\%$ of the matrices, while PMKL is the best solver for $40\\%$ of the matrices.}\n\\end{figure*}\n\n\nAs a next step, we compare the performance on the whole test suite.\nOn SandyBridge, the geometric mean of speedup for all the matrices with {\\emph{Basker}\\xspace} is  $5.91\\times$ and with PMKL is it $1.5\\times$ using 16 cores.\nOn 16 cores, {\\emph{Basker}\\xspace} is faster than PMKL on 17/22 matrices. The five matrices PMKL is faster on have a high fill-in density.\nOn the Xeon Phi, the geometric mean speedup with {\\emph{Basker}\\xspace} is $7.4\\times$ and with PMKL it is $5.78\\times$ using 32 cores.\nOn 32 cores, {\\emph{Basker}\\xspace} is faster than PMKL on 16/22 matrices. \nThis includes the same matrices as on the SandyBridge except Freescale1.\nThe reason for such a high speedup for PMKL on Xeon Phi is again its higher performance on high fill-in density matrices.\n\n\n While the geometric mean gives some idea on relative performance, we use a\nperformance profile to gain an understanding of the overall performance over the\ntest suite.  The performance profile measures the relative time of a solver on\na given matrix to the best solver.  The values are plotted for all matrices in\na graph with an x-axis of time relative to best time and a y-axis as fraction\nof matrices.  The result is a figure where a point(x,y) is plotted if a solver\ntakes no more than x times the runtime of of the fastest solver for y problems.\n\n\nFigure~\\ref{fig:pp1cpu} shows the performance profile of {\\emph{Basker}\\xspace}, PMKL, and KLU \\emph{in serial} on SandyBridge.\nThis shows a baseline of how well each method does in serial.\nWe observe that {\\emph{Basker}\\xspace} is better on $\\sim 77\\%$ of the problems, while the supernodal method of PMKL is within $5\\times$  of the the best solver for $77\\%$ of \nthe problems.\nHowever, PMKL is the better solver for $\\sim 34\\%$ of the problems.\nDespite having very similar algorithms, {\\emph{Basker}\\xspace} is able to slightly beat KLU.\nThis slight difference is because of the difference in orderings and the use of Kokkos memory padding.\n\n\nThe performance profile of the parallel solvers on SandyBridge (16 cores) is\nshown in Figure~\\ref{fig:pp16cpu}.  Serial KLU is not included in this figure.\n{\\emph{Basker}\\xspace} is the best solver for $\\sim 75\\%$ of the matrices, and PMKL is within\n$\\sim5\\times$  of {\\emph{Basker}\\xspace} on $\\sim 50\\%$ of the matrices.  PMKL is the best\nsolver for $\\sim 30\\%$ of the matrices, which correspond to matrices with high\nfill-in density.  This demonstrates {\\emph{Basker}\\xspace} scales well on SandyBridge for low\nfill-in density matrices.\nOn Intel Xeon Phi with 32 cores, the performance profile is slightly different (Figure~\\ref{fig:pp32phi}).\n{\\emph{Basker}\\xspace} now is the best solver for $70\\%$ matrices, and PMKL is within $6\\times$ of {\\emph{Basker}\\xspace} for $70\\%$ of matrices.   \nPMKL is the best (or very close to the best) for $\\sim40\\%$ of the matrices.\nOne can observe {\\emph{Basker}\\xspace} now does poorly on high fill-in density matrices.\nA reason for that is the missing large shared $L3$ to share data needed during the {\\emph{Basker}\\xspace}'s reductions.\n\n\n\\subsection{Comparison on Ideal Matrices}\nNext, we analyze how well {\\emph{Basker}\\xspace} scales on low fill-in density matrices, compared to how well the supernodal solver PMKL scales on 2/3D mesh problems.\nThis comparison allows us to better understand if {\\emph{Basker}\\xspace} achieves speedup for its ideal input similar to PMKL on its ideal input. The other reason is to see\nhow well we can parallelize {{Gilbert-Peierls algorithm}\\xspace} for its ideal problems.\nWe use a second test suite of matrices for PMKL that come from 2/3D mesh problems in Table II. \n Performance of PMKL on these matrices will be compared to the performance of {\\emph{Basker}\\xspace} on the six matrices of our primary test suite with the lowest fill-in density.\n\n\n\n\\begin{table}[htbp]\n  \\centering\n\t{{\\fontsize{8}{9}\\selectfont}\n\t\\tabcolsep=0.08cm\n  \\caption{2/3D mesh problems to test PMKL's best performance.}\n    \\begin{tabular}{c|c|c|c|c}\n\t\t\\hline\n    Matrix         & $n$    & $|A|$ & $|L$+$U|$ & Description  \\\\\n\t\t\\hline\n    pwtk           & 2.2E5  & 1.2E7 & 9.7E7 & Wind tunnel stiffness matrix \\\\\n    ecology        & 1.0E6  & 5.0E6 & 7.1E7 & 5 pt stencil model movement \\\\\n    apache2        & 7.2E5  & 4.8E6 & 2.8E8 & Finite difference 3D \\\\\n    bmwcra1        & 1.5E5  & 1.1E7 & 1.4E8 & Stiffness matrix \\\\\n    parabolic\\_fem & 5.3E5  & 3.7E6 & 5.2E7 & Parabolic finite element \\\\\n    helm2d03       & 3.9E5  & 2.7E6 & 3.7E7 & Helmholtz on square \\\\\n\t\t\\hline\n    \\end{tabular}\n\t\t}\n  \\label{tab:pideal}\n\\end{table}\n\n\\begin{figure}[h]\n\t\\centering\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{ideal_cpu.png}\n\t\t\\label{fig:ideal_cpu}\n\t}\n\t\\subfigure[]\n\t{\n\t\t\\includegraphics[width=0.22\\textwidth]{ideal_mic.png}\n\t\t\\label{fig:ideal_phi}\n\t}\n\t\\caption{{\\emph{Basker}\\xspace} and PMKL with on 6 ideal input matrices. (a) SandyBridge, {\\emph{Basker}\\xspace} is able to achieve a similar speedup curve as PMKL on 2/3D mesh problems. (b) Intel Xeon Phi, {\\emph{Basker}\\xspace} has a similar plot up to 16 cores as PMKL.  Fine-grain access causes imbalance at 32 cores. }\n\\end{figure}\n\nFigure~\\ref{fig:ideal_cpu} provides a scatter plot of the speedup for each solver relative to itself over its ideal six matrices.\nA linear trend line is shown for each set of solver speedups.\nBoth solvers achieve  similar speedup trend on SandyBridge for\ntheir ideal inputs.\nThis demonstrates that on systems with a large cache hierarchy {\\emph{Basker}\\xspace} is able to achieve so called state-of-the-art performance on low fill-in density matrices.\nIn Figure~\\ref{fig:ideal_phi}, a similar plot is given for our Xeon Phi platform.\nThis time {\\emph{Basker}\\xspace} has a slightly lower trend line starting at 16 cores.\nWe suspect this is due to both the size of the submatrices not fitting into cache and time for the reduction.\nWe plan to address both these issues in future versions of {\\emph{Basker}\\xspace}.\n\n\n\\subsection{Xyce}\n\\input{xyce_result}\n\n\n\n\\section{Conclusions and Future Work}\n\\label{sec:conclusion}\n\nWe introduced a new multithreaded sparse $LU$ factorization, {\\emph{Basker}\\xspace}, that uses hierarchical parallelism and data layouts.\n{\\emph{Basker}\\xspace} provides a nice alternative to traditional solvers that use one-dimensional layout with BLAS.\nIn particular, it is useful for coefficient matrices with hierarchical structure such as circuit problems.\nWe also introduced the first parallel implementation of {{Gilbert-Peierls algorithm}\\xspace}.\nPerformance results show that {\\emph{Basker}\\xspace} scales well for matrices with low fill-in density resulting in a speedup of $5.91\\times$ (geometric mean) over the test suite on 16 SandyBridge cores and $7.5\\times$ over the test suite on 32 Intel Xeon Phi cores relative to KLU.\nParticularly, {\\emph{Basker}\\xspace} can have speedups on these matrices similar to PMKL on 2/3D mesh problems and reduce the time for a sequence of circuit problems from Xyce by $5\\times$.\n{\\emph{Basker}\\xspace} shows that in order to speedup sparse factorization on many-core node, solvers must leverage all available parallelism and may do so by using a hierarchical structure.\n\nWe plan to continue support of {\\emph{Basker}\\xspace} in the ShyLU package of Trilinos for Xyce.\nFuture scheduled improvements include adding supernodes to the hierarchy structure to improve performance on high fill-in matrices, and using asynchronous tasking to reduce synchronization costs.\n\n   \n\n\n\n\n\\section*{Acknowledgment}\nWe would like to thank Erik Boman, Andrew Bradley, Kyungjoo Kim, H.C. Edwards, Christian Trott, and Simon Hammond for insights and discussions. \nSandia is a multiprogram laboratory operated by Sandia Corporation, a\nLockheed Martin Company, for the U.S. Department of Energy under\ncontract DE-AC04-94-AL85000.\n\n\\bibliographystyle{IEEEtran}\n\\bibliography{Ref}\n\n\n", "itemtype": "equation", "pos": 25801, "prevtext": "\nAlso, pivoting while factorizing $A_{ii}$ will not affect ${\\ensuremath{\\mathcal{{ L}}}\\xspace}_{ik}(c)$ as $k > i$ by the fill-path theorem~\\cite{rosetwo}. \nTherefore, {\\emph{Basker}\\xspace} can use the above expression to find the nonzeros counts of the lower-diagonal submatrices.\nMoreover, we find a data structure $lest$ with the maximum and minimum row index for each column $c$ that will be used for estimating nonzero counts in higher $treelevel$. \nAt $treelevel$ 0, nonzero counts for the upper-diagonal submatrices, i.e., $U_{ki}$, can be found (Line 8). \nAs ${\\ensuremath{\\mathcal{{U}}}\\xspace}_{ki}(c)$ may depend on the pivoting on $A_{ii}$ the $etree_{i}$ must be used. \nFor each column ($c$), the method counts the nodes encountered starting from each nonzero in the column of ${\\ensuremath{\\mathcal{{A}}}\\xspace}_{ki}(c)$ to the least common ancestor of any nonzero already explored, where the least common ancestor of two nodes is the least numbered node that is the ancestor of both.\nA data structure $uest$ is returned with the maximum and minimum row index for each row.\n\n\\input{sfactlarge_word}\n\nThe estimated nonzero counts for submatrices in the higher levels of the dependency tree are found using the estimates $lest$ and $uest$ by looping over the remaining $treelevels$ (Line 11).\nAt each $treelevel$, all the nodes on the level are handled by finding the nonzero count of the diagonal subblock, e.g., $LU_{33}$ (Line 14).\nNow, \n\n", "index": 5, "text": "$${\\ensuremath{\\mathcal{{L}}}\\xspace}_{jj}(c) = {\\ensuremath{\\mathcal{{A}}}\\xspace}_{jj}(c) \\bigcup\\limits_{t=1}^{c-1} \\{{\\ensuremath{\\mathcal{{L}}}\\xspace}_{jj}(t) | t \\in {\\ensuremath{\\mathcal{{U}}}\\xspace}_{jj}(c) \\} \\bigcup\\limits_{k=1}^{j} {\\ensuremath{\\mathcal{{L}}}\\xspace}_{jk}{\\ensuremath{\\mathcal{{U}}}\\xspace}_{kj}(c) $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{{L}}}_{jj}(c)={\\mathcal{{A}}}_{jj}(c)\\bigcup\\limits_{t=1}^{c-1}\\{{%&#10;\\mathcal{{L}}}_{jj}(t)|t\\in{\\mathcal{{U}}}_{jj}(c)\\}\\bigcup\\limits_{k=1}^{j}{%&#10;\\mathcal{{L}}}_{jk}{\\mathcal{{U}}}_{kj}(c)\" display=\"block\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>c</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>j</mi></munderover><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb0</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}]