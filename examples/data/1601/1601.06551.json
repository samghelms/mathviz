[{"file": "1601.06551.tex", "nexttext": "\nwhere $\\Pr_{\\theta}[L]$ is the probability of yielding live-edge graph $L$ under vector $\\theta$.\nFrom \\cite{kempe2003maximizing}, we know that the influence spread function is non-negative ($\\forall S \\subseteq V$, $\\sigma_{\\theta}(S) \\geq 0$), monotone ($\\forall S \\subseteq T \\subseteq V$, $\\sigma_{\\theta}(S) \\leq \\sigma_{\\theta}(T)$), and\nsubmodular ($\\forall S \\subseteq T \\subseteq V$, $\\forall v \\in V$ $\\sigma_{\\theta}(S \\cup \\{ v \\}) - \\sigma_{\\theta}(S) \\geq \\sigma_{\\theta}(T \\cup \\{ v \\}) - \\sigma_{\\theta}(T)$).\n\nThe well-known problem  of \\emph{Influence Maximization}\nraised in \\cite{kempe2003maximizing} is stated in the following.\n\\begin{problem}[Influence Maximization \\cite{kempe2003maximizing}]\n\tGiven a graph $G=(V,E)$, parameter vector $\\theta=(p_e)_{e\\in E}$ and a fixed budget $k$,\n\twe are required to find a seed set $S \\subseteq V$ of $k$ vertices, such that the influence spread function $\\sigma_{\\theta}(S)$ is maximized, that is,\n\t\n", "itemtype": "equation", "pos": 14502, "prevtext": "\n\n\n\n\\setcopyright{acmcopyright}\n\n\n\n\n\n\n\n\n\\doi{10.475/123_4}\n\n\n\\isbn{123-4567-24-567/08/06}\n\n\n\n\\conferenceinfo{KDD '16}{August 13--17, San Francisco, CA, USA}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Robust Influence Maximization}\n\t\t\n\t\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\numberofauthors{5} \n\n\n\n\\author{\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\alignauthor Wei Chen \\\\\n\\affaddr{Microsoft Research}\\\\\n\\email{weic@microsoft.com}\n\\and\n\\alignauthor Tian Lin \\\\\n\\affaddr{Tsinghua University}\\\\\n\\email{lint10@mails.tsinghua.edu.cn}\n\\and\n\\alignauthor Zihan Tan \\\\\n\\affaddr{IIIS, Tsinghua University}\\\\\n\\email{zihantan1993@gmail.com}\n\\and\n\\alignauthor Mingfei Zhao \\\\\n\\affaddr{IIIS, Tsinghua University}\\\\\n\\email{mingfeizhao@hotmail.com}\n\\and\n\\alignauthor Xuren Zhou \\\\\n\\affaddr{The Hong Kong University of Science and Technology}\\\\\n\\email{xzhouap@cse.ust.hk}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\\sloppy\n\n\n\\begin{abstract}\n\tIn this paper, we address the important issue of uncertainty in the edge influence\n\tprobability estimates for the well studied influence maximization problem\n\t--- the task of finding $k$ seed nodes in a social network to maximize\n\tthe influence spread.\n\tWe propose the problem of robust influence maximization, which maximizes\n\tthe worse-case ratio between the influence spread of the chosen seed set and\n\tthe optimal seed set, given the uncertainty of the parameter input.\n\tWe design an algorithm that solves this problem with a solution-dependent bound.\n\tWe further study uniform sampling and adaptive sampling methods to\n\teffectively reduce the\n\tuncertainty on parameters and improve the robustness of\n\tthe influence maximization task.\n\tOur empirical results show that parameter uncertainty may greatly affect influence\n\tmaximization performance and prior studies that learned influence probabilities\n\tcould lead to poor performance in robust influence maximization due to\n\trelatively large uncertainty in parameter estimates,\n\tand information cascade based adaptive sampling\n\tmethod may be an effective way to improve the robustness of influence maximization.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{CCSXML}\n\t<ccs2012>\n\t<concept>\n\t<concept_id>10002951.10003260.10003272.10003276</concept_id>\n\t<concept_desc>Information systems~Social advertising</concept_desc>\n\t<concept_significance>500</concept_significance>\n\t</concept>\n\t<concept>\n\t<concept_id>10003120.10003130.10003131.10003292</concept_id>\n\t<concept_desc>Human-centered computing~Social networks</concept_desc>\n\t<concept_significance>500</concept_significance>\n\t</concept>\n\t<concept>\n\t<concept_id>10003752.10003809.10003636.10003814</concept_id>\n\t<concept_desc>Theory of computation~Stochastic approximation</concept_desc>\n\t<concept_significance>300</concept_significance>\n\t</concept>\n\t</ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Information systems~Social advertising}\n\\ccsdesc[500]{Human-centered computing~Social networks}\n\\ccsdesc[300]{Theory of computation~Stochastic approximation}\n\n\n\n\n\n\n\n\n\\printccsdesc\n\n\n\n\n\n\\keywords{social networks, influence maximization, robust optimization, information diffusion}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction} \\label{sect:intro}\nIn social and economic networks, {\\em Influence Maximization} problem has been extensively studied\nover the past decade, due to its wide applications to viral marketing \\cite{domingos2001mining,kempe2003maximizing}, outbreak detection \\cite{leskovec2007cost},\nrumor monitoring \\cite{budak2011limiting}, etc.\nFor example, a company may conduct a promotion campaign in social networks\nby sending free samples to the initial users (termed as seeds),\nand via the word-of-mouth (WoM) effect, more and more users are influenced by social links to join the campaign\nand propagate messages of the promotion.\nThis problem is first introduced by Kempe et al.~\\cite{kempe2003maximizing} under an algorithmic framework to find the most influential seeds,\nand they propose the {\\em independent cascade} model and {\\em linear threshold} model,\nwhich consider the social-psychological factors of information diffusion to simulate such a random process of adoptions.\n\nSince Kempe et al.'s seminal work, extensive researches have been done on influence\nmaximization, especially on improving the efficiency of influence maximization\nin the independent cascade model~\\cite{chen2009efficient, chen2010scalable, goyal2011celf,borgs14,tang14}, all of which\nassume that the ground-truth influence probabilities on edges are exactly known.\nSeparately, a number of studies \\cite{saito2008prediction,tang2009social,goyal2010learning,gomez2011uncover,Netrapalli12} propose learning methods to extract edge influence probabilities.\nDue to inherent data limitation, no learning method could recover the\nexact values of the edge probabilities, and what can be achieved is the estimates\non the true edge probabilities, with confidence intervals indicating that\nthe true values are within the confidence intervals with high probability.\nThe uncertainty in edge probability estimates, however, may adversely affect\nthe performance of the influence maximization task, but this topic has\nleft mostly unexplored.\nThe only attempt addressing this question is a recent study in~\\cite{he2015stability},\nbut due to a technical issue as explained in~\\cite{he2015stability},\nthe results achieved by the study is rather limited.\n\nIn this paper, we utilize the concept of robust optimization~\\cite{ben2002robust}\nin operation research to address the issue of influence maximization\nwith uncertainty.\nIn particular, we consider that the input to the influence maximization task is\nno longer edge influence probability on every edge of a social graph, but instead\nan interval in which the true probability may lie within.\nThus the input is actually a parameter space $\\Theta$, which is the product of\nall intervals on all edges.\nFor any seed set $S$, let $\\sigma_\\theta(S)$ denote the {\\em influence spread} of $S$\nunder parameter setting $\\theta\\in \\Theta$.\nThen we define {\\em robust ratio} of $S$ as\n$g(\\Theta,S) = \\min_{\\theta\\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})} $,\nwhere $S^{*}_{\\theta}$ is the optimal seed set achieving the maximum influence\nspread under parameter $\\theta$.\nIntuitively, robust ratio of $S$ indicates the (multiplicative) gap between\nits influence spread and the optimal influence spread under the worse-case\nparameter $\\theta \\in \\Theta$, since we are unsure which $\\theta \\in \\Theta$\nis the true probability setting.\nThen our optimization task is to find a seed set of size $k$ that\nmaximize the robust ratio under the known parameter space $\\Theta$\n--- we call this task {\\em Robust Influence Maximization (RIM)}.\n\nIt is clear that when there is no uncertainty on edge probabilities, which means\n$\\Theta$ collapses to the single true parameter $\\theta$,\nRIM degenerates to the classical influence maximization problem.\nHowever, when uncertainty exists, solving RIM may be a more difficult task.\nIn this paper, we first propose an algorithm {{\\sf LUGreedy}} that\nsolves the RIM task with a solution-dependent bound on\nits performance, which means\nthat one can verify its performance after it selects the seed\nset (Section~\\ref{sect:rim}).\nWe then show that if the input parameter space $\\Theta$ is only given and cannot\nbe improved, it is possible that even the best robust ratio\nin certain graph instances could be very small (e.g. $O(\\log n / \\sqrt{n})$\nwith $n$ being the number of nodes in the graph).\nThis motivates us to study sampling methods to further tighten parameter\nspace $\\Theta$, and thus improving the robustness of our algorithm\n(Section~\\ref{sect:sample}).\nIn particular, we study both uniform sampling and adaptive sampling for\nimproving RIM performance.\nFor uniform sampling, we provide theoretical results on the sample complexity\nfor achieving a given robust ratio of the output seed set.\nFor adaptive sampling, we propose an information cascade based sampling heuristic\nto adaptively bias our sampling effort to important edges often traversed\nby information cascades.\nThrough extensive empirical evaluations (Section~\\ref{sect:experiments}), we show that\n(a) robust ratio is sensitive to the width of the confidence interval, and\nit decreases rapidly when the width of the confidence interval increases; as\na result prior studies that learned edge probabilities may result in poor robust\nratio due to relative large confidence intervals (and thus high uncertainty);\n(b) information cascade based adaptive sampling method performs better than\nuniform sampling and other baseline sampling methods and can significantly\nimprove the robustness of the influence maximization task.\n\nIn summary, the contribution of our paper includes: (a) proposing the problem\nof robust influence maximization to address the important issue of\nuncertainty in parameter estimates adversely impacting\nthe influence maximization task;\n(b) providing the {{\\sf LUGreedy}} algorithm that guarantees a solution-dependent\nbound; and\n(c) studying uniform and adaptive sampling methods to\nimprove robust influence maximization.\n\n{\n \\ifthenelse{\\equal{{kdd}}{arxiv}}{{Note that proofs of some technical results can be found in the appendix.}}{} }\n\n{\n \\ifthenelse{\\equal{{kdd}}{kdd}}{{Due to space constraint, the proofs of some technical results are omitted. The complete proofs of all results can be found in the full paper~\\cite{ChenLTZZ16}.}}{} }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Additional Related Work} \\label{sect:related}\n\nInfluence maximization has been extensively studied and we already point out\na number of closely related studies to our work in the introduction.\nFor a comprehensive survey, one can refer to the monograph~\\cite{chen2013information}.\nWe discuss a few most relevant work in more detail here.\n\nTo the best of our knowledge, the study by He and Kempe~\\cite{he2015stability}\nis the only attempt prior to our work that also tries to address the\nissue of uncertainty\nof parameter estimates impacting the influence maximization tasks.\nHowever, besides the similarity in motivation, the technical treatments are quite\ndifferent.\nFirst, their central problem, called influence difference maximization, is to\nfind a seed set of size $k$ that maximizes the additive difference between\nthe two influence spreads of the {\\em same} seed set using different parameter values.\nTheir purpose is to see how large the influence gap could be due to the\nuncertainty in parameter space.\nHowever, our goal is still to find the best possible seed set for influence\nmaximization purpose, while considering the adverse effect of the uncertainty,\nand thus we utilize the robust optimization concept and use the worse-case\nmultiplicative ratio between the influence spread of the chosen seed set and\nthe optimal seed set as our objective function.\nSecond, their influence difference maximization turns out to be hard to approximate\nto any reasonable ratio, while we provide an actual algorithm for robust\ninfluence maximization that has both a theoretical solution-dependent bound and\nperforms reasonably well in experiments.\nThird, we further consider using sampling methods to improve RIM, which is not\ndiscussed in~\\cite{he2015stability} .\n\nIn the context of robust optimization, Krause et al.'s work on robust\nsubmodular optimization~\\cite{krause2008robust} is possibly the closest to ours.\nOur RIM problem can be viewed as a specific instance of robust submodular\noptimization studied in~\\cite{krause2008robust}.\nHowever, due to the generality of problem scope studied in~\\cite{krause2008robust},\nthey show strong hardness results and then they have to resolve to\nbi-criteria solutions.\nInstead, we are working on a particular instance of robust submodular optimization,\nand their bicriteria solution may greatly enlarge the selected seed set size,\nwhich may not be allowed in our case.\nFurthermore, they work on finite set of submodular functions, but in our case\nour objective function is parametrized with $\\theta$ from a continuous\nparameter space $\\Theta$, and it is unclear how their results work for\nthe continuous case.\n\nAdaptive sampling for improving RIM bears some resemblance to pure exploration\nbandit research~\\cite{bubeck2011pure}, especially to combinatorial pure exploration\n\\cite{chen2014combinatorial} recently studied.\nBoth use adaptive sampling and achieve some optimization objective in the end.\nHowever, the optimization problem modeled in combinatorial pure exploration\n\\cite{chen2014combinatorial} does not have a robustness objective.\nStudying robust optimization together with combinatorial pure exploration\ncould be a potentially interesting topic for future research.\n\n\n\\section{Model and Problem Definition} \\label{sect:model}\n\nAs in \\cite{kempe2003maximizing}, the {\\em independent cascade (IC)} model can be\nequivalently modeled as a stochastic diffusion process from\nseed nodes or as reachability from seed nodes\nin random live-edge graphs.\nFor brevity, we provide the live-edge graph description below.\nConsider a graph $G=(V,E)$ comprising a set $V$ of nodes and a set $E$ of directed edges,\nwhere every edge $e$ is associated with probability $p_e \\in [0,1]$, and let $n = |V|$ and $m = |E|$.\nTo generate a random live-edge graph, we declare each edge $e$\nas {\\em live}\nif flipping a biased random coin with probability $p_e$ returns success,\ndeclare $e$ as {\\em blocked} otherwise (with probability $1-p_e$).\nThe randomness on all edges are mutually independent.\nWe define the subgraph $L$ consisting of $V$ and the set of\nlive edges as the (random) {\\em live-edge graph}.\nGiven any set $S \\subseteq V$ (referred as {\\em seeds}), let $R_L(S) \\subseteq V$ denote the {\\em reachable set} of nodes from $S$ in live-edge graph $L$, i.e.,\n(1) $S\\subseteq R_L(S)$, and (2) for a node $v\\notin S$, $v \\in R_L(S)$ iff there is a path in $L$ directing from some node in $S$ to $v$.\n\n\n\n\nFor convenience, we use {\\em parameter vector} $\\theta=(p_e)_{e\\in E}$ to denote the probabilities on all edges.\nThe {\\em influence spread} function $\\sigma_{\\theta}(S)$ is defined as the expected size of the reachable set from $S$, that is\n", "index": 1, "text": "\n\\[\n\\sigma_{\\theta}(S) := \\sum_{L}\\Pr_{\\theta}[L]\\cdot |R_L(S)| \\mbox{,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{\\theta}(S):=\\sum_{L}\\Pr_{\\theta}[L]\\cdot|R_{L}(S)|\\mbox{,}\" display=\"block\"><mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>L</mi></munder><mrow><mrow><mrow><munder><mi>Pr</mi><mi>\u03b8</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\\end{problem}\nIt has been shown that Influence Maximization problem is NP-hard \\cite{kempe2003maximizing}.\nSince the objective function $\\sigma_{\\theta}(S)$ is submodular,\nwe have a $(1-\\frac{1}{e})$ approximation using standard greedy policy ${\\sf Greedy}(G,k,\\theta)$ in Algorithm~\\ref{alg:greedy} (assuming a value oracle on function $\\sigma_\\theta(\\cdot)$).\nLet $S^{g}_{\\theta}$ be the solution of ${\\sf Greedy}(G,k,\\theta)$.\nAs a convention, we assume that both optimal seed set $S^*_{\\theta}$ and greedy seed set $S^{g}_{\\theta}$\nin this paper are of fixed size $k$ implicitly.\n\n\n\nOn the other hand, it is proved by Feige~\\cite{feige1998threshold} that such an approximation ratio\ncould not be improved for $k$-max cover problem, which is a special case of the\ninfluence maximization problem under the IC model.\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G$, budget $k$, parameter vector $\\theta$\n\t\t\\STATE $S_0 \\gets \\emptyset$\n\t\t\\FOR {$i = 1,2,\\ldots,k$}\n\t\t\\STATE $v \\gets {\\operatorname{arg\\,max}}_{v \\notin S_i} \\left\\{ \\sigma_{\\theta}(S_{i-1}\\cup \\{v\\})-\\sigma_{\\theta}(S_{i-1}) \\right\\}$\n\t\t\\STATE $S_i \\gets S_{i-1} \\cup \\{v\\}$\n\t\t\\ENDFOR\n\t\t\\RETURN $S_k$\n\t\\end{algorithmic}\n\t\\caption{{\\sf Greedy}($G,k,\\theta$)}\n\t\\label{alg:greedy}\n\\end{algorithm}\n\nHowever, the knowledge of the probability on edges is usually acquired by learning from the real-world\ndata \\cite{saito2008prediction,tang2009social,goyal2010learning,gomez2011uncover,Netrapalli12},\nand the obtained estimates always have some inaccuracy comparing to the\ntrue value.\n\nTherefore, it is natural to assume that, from observations of edge $e$,\nwe can obtain the statistically significant neighborhood $[l_e,r_e]$,\ni.e., the {\\em confidence interval} where the true probability $p_e$\nlies in with high probability.\nThis confidence interval prescribes the uncertainty on the true probability\n$p_e$ of the edge $e$, and such uncertainty on edges may adversely\nimpact the influence maximization task.\nMotivated by this, we study the problem of {\\em robust influence maximization}\nas specified below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose for every edge $e$, we are given an interval $[l_e,r_e]$ ($0\\le l_e\\le r_e\\le 1$)\nindicating the range of the probability, and the ground-truth probability $p_e \\in [l_e,r_e]$ of this edge is unknown.\nDenote $\\Theta=\\times_{e\\in E}[l_e,r_e]$ as the \\emph{parameter space} of network $G$, and\n$\\theta=(p_e)_{e \\in E}$ as the latent parameter vector.\nSpecifically, let $\\theta^{-}(\\Theta)=(l_e)_{e\\in E}$ and $\\theta^{+}(\\Theta)=(r_e)_{e\\in E}$ as the minimum and maximum parameter vectors, respectively, and when the context is clear, we would only use $\\theta^{-}$ and $\\theta^+$.\nFor a seed set $S \\subseteq V$ and $|S| = k$, define its \\emph{robust ratio} under\nparameter space $\\Theta$ as\n\n", "itemtype": "equation", "pos": 15539, "prevtext": "\nwhere $\\Pr_{\\theta}[L]$ is the probability of yielding live-edge graph $L$ under vector $\\theta$.\nFrom \\cite{kempe2003maximizing}, we know that the influence spread function is non-negative ($\\forall S \\subseteq V$, $\\sigma_{\\theta}(S) \\geq 0$), monotone ($\\forall S \\subseteq T \\subseteq V$, $\\sigma_{\\theta}(S) \\leq \\sigma_{\\theta}(T)$), and\nsubmodular ($\\forall S \\subseteq T \\subseteq V$, $\\forall v \\in V$ $\\sigma_{\\theta}(S \\cup \\{ v \\}) - \\sigma_{\\theta}(S) \\geq \\sigma_{\\theta}(T \\cup \\{ v \\}) - \\sigma_{\\theta}(T)$).\n\nThe well-known problem  of \\emph{Influence Maximization}\nraised in \\cite{kempe2003maximizing} is stated in the following.\n\\begin{problem}[Influence Maximization \\cite{kempe2003maximizing}]\n\tGiven a graph $G=(V,E)$, parameter vector $\\theta=(p_e)_{e\\in E}$ and a fixed budget $k$,\n\twe are required to find a seed set $S \\subseteq V$ of $k$ vertices, such that the influence spread function $\\sigma_{\\theta}(S)$ is maximized, that is,\n\t\n", "index": 3, "text": "\\begin{align*}\n\tS^*_{\\theta} := {\\operatorname{arg\\,max}}_{S \\subseteq V, |S| = k} \\sigma_{\\theta}(S) \\mbox{.}\n\t\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S^{*}_{\\theta}:={\\operatorname{arg\\,max}}_{S\\subseteq V,|S|=k}%&#10;\\sigma_{\\theta}(S)\\mbox{.}\" display=\"inline\"><mrow><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo>:=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></mrow></msub><mo>\u2061</mo><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhere $S^{*}_{\\theta}$ is the optimal solution of size $k$ when the probability on every edge is given by $\\theta$.\n\n\nGiven $\\Theta$ and solution $S$, the robust ratio $g(\\Theta,S)$\ncharacterizes the {\\em worst-case}\nratio of influence spread of $S$ and the underlying optimal one,\nwhen the true probability vector $\\theta$ is unknown (except knowing that\n$\\theta \\in \\Theta$).\nThen, the {\\em Robust Influence Maximization} (RIM) problem is defined as follows.\n\\begin{problem}[Robust Influence Maximization]\n\tGiven a graph $G=(V,E)$, parameter space $\\Theta=\\times_{e\\in E}[l_e,r_e]$ and a fixed budget $k$, we are required to find a set $S\\subseteq V$ of $k$ vertices,\n\tsuch that robust ratio $g(\\Theta,S)$ is maximized, i.e.,\n\t\n", "itemtype": "equation", "pos": 18466, "prevtext": "\n\\end{problem}\nIt has been shown that Influence Maximization problem is NP-hard \\cite{kempe2003maximizing}.\nSince the objective function $\\sigma_{\\theta}(S)$ is submodular,\nwe have a $(1-\\frac{1}{e})$ approximation using standard greedy policy ${\\sf Greedy}(G,k,\\theta)$ in Algorithm~\\ref{alg:greedy} (assuming a value oracle on function $\\sigma_\\theta(\\cdot)$).\nLet $S^{g}_{\\theta}$ be the solution of ${\\sf Greedy}(G,k,\\theta)$.\nAs a convention, we assume that both optimal seed set $S^*_{\\theta}$ and greedy seed set $S^{g}_{\\theta}$\nin this paper are of fixed size $k$ implicitly.\n\n\n\nOn the other hand, it is proved by Feige~\\cite{feige1998threshold} that such an approximation ratio\ncould not be improved for $k$-max cover problem, which is a special case of the\ninfluence maximization problem under the IC model.\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G$, budget $k$, parameter vector $\\theta$\n\t\t\\STATE $S_0 \\gets \\emptyset$\n\t\t\\FOR {$i = 1,2,\\ldots,k$}\n\t\t\\STATE $v \\gets {\\operatorname{arg\\,max}}_{v \\notin S_i} \\left\\{ \\sigma_{\\theta}(S_{i-1}\\cup \\{v\\})-\\sigma_{\\theta}(S_{i-1}) \\right\\}$\n\t\t\\STATE $S_i \\gets S_{i-1} \\cup \\{v\\}$\n\t\t\\ENDFOR\n\t\t\\RETURN $S_k$\n\t\\end{algorithmic}\n\t\\caption{{\\sf Greedy}($G,k,\\theta$)}\n\t\\label{alg:greedy}\n\\end{algorithm}\n\nHowever, the knowledge of the probability on edges is usually acquired by learning from the real-world\ndata \\cite{saito2008prediction,tang2009social,goyal2010learning,gomez2011uncover,Netrapalli12},\nand the obtained estimates always have some inaccuracy comparing to the\ntrue value.\n\nTherefore, it is natural to assume that, from observations of edge $e$,\nwe can obtain the statistically significant neighborhood $[l_e,r_e]$,\ni.e., the {\\em confidence interval} where the true probability $p_e$\nlies in with high probability.\nThis confidence interval prescribes the uncertainty on the true probability\n$p_e$ of the edge $e$, and such uncertainty on edges may adversely\nimpact the influence maximization task.\nMotivated by this, we study the problem of {\\em robust influence maximization}\nas specified below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose for every edge $e$, we are given an interval $[l_e,r_e]$ ($0\\le l_e\\le r_e\\le 1$)\nindicating the range of the probability, and the ground-truth probability $p_e \\in [l_e,r_e]$ of this edge is unknown.\nDenote $\\Theta=\\times_{e\\in E}[l_e,r_e]$ as the \\emph{parameter space} of network $G$, and\n$\\theta=(p_e)_{e \\in E}$ as the latent parameter vector.\nSpecifically, let $\\theta^{-}(\\Theta)=(l_e)_{e\\in E}$ and $\\theta^{+}(\\Theta)=(r_e)_{e\\in E}$ as the minimum and maximum parameter vectors, respectively, and when the context is clear, we would only use $\\theta^{-}$ and $\\theta^+$.\nFor a seed set $S \\subseteq V$ and $|S| = k$, define its \\emph{robust ratio} under\nparameter space $\\Theta$ as\n\n", "index": 5, "text": "\\begin{equation}\\label{robustratio}\ng(\\Theta,S) := \\min_{\\theta \\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})} \\mbox{,}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"g(\\Theta,S):=\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S%&#10;^{*}_{\\theta})}\\mbox{,}\" display=\"block\"><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mrow><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\\end{problem}\n\n\n\n\n\n\n\n\nThe objective of this problem is to find a seed set $S^*_{\\Theta}$ that has the largest robust ratio, that is, $S^*_{\\Theta}$ should maximize the\nworst-case ratio between its influence spread and the optimal influence spread,\nwhen the true probability vector $\\theta$ is unknown.\nWhen there is no uncertainty, which means $\\Theta$ collapses to the true\nprobability $\\theta$, we can see that the RIM problem is reduced back\nto the original influence maximization problem.\n\nIn RIM, the knowledge of the confidence interval is assumed to be the input.\nAnother interpretation is that, it can be viewed as given an estimate of probability vector\n$\\hat{\\theta} = (\\hat{p}_e)_{e \\in E}$ with a perturbation level $\\delta_e$ on each edge $e$,\nsuch that the true probability $p_e \\in [\\hat{p}_e - \\delta_e, \\hat{p}_e + \\delta_e] = [l_e, r_e]$,\nwhich constitutes parameter space $\\Theta = \\times_{e \\in E}[l_e,r_e]$.\nNotice that, in reality, this probability could be obtained via edge samplings,\ni.e., we make samples on edges and compute the fraction of times that the edge is live.\nOn the other hand, we can also observe information cascades on each edge when collecting\nthe trace of diffusion in the real world,\nso that the corresponding probability can be learned.\n\nHowever, when the amount of observed information cascade is small, the best robust ratio $\\max_{S}g(\\Theta,S)$ for the given $\\Theta$ can be low so that the output for a RIM algorithm does not have a good enough guarantee of the performance in the worst case. Then a natural question is, given $\\Theta$, how to further make samples on edges (e.g., activating source node $u$ of an edge $(u,v)$ and see if the sink node $v$ is activated through edge $e$) so that $\\max_{S}g(\\Theta,S)$ can be efficiently improved? To be specific, how to make samples on edges and output $\\Theta'$ and $S'$ according to the outcome so that (a) with high probability the true value $\\theta$ lies in the output parameter space $\\Theta'$, where the randomness is taken according to $\\theta$,\nand (b) $g(\\Theta',S')$ is large.\nThis sub-problem is called \\emph{Sampling for Improving Robust Influence Maximization}, and will be addressed in Section~\\ref{sect:sample}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Algorithm and Analysis for RIM} \\label{sect:rim}\n\n\n\n\nConsider the problem of RIM, parameter space $\\Theta = \\times_{e \\in E}[l_e,r_e]$ is given, and we do not know the true probability $\\theta \\in \\Theta$.\nLet $\\theta^{-}=(l_e)_{e\\in E}$ and $\\theta^{+}=(r_e)_{e\\in E}$.\n\nOur first observation is that, when $\\Theta$ is a single vector ($l_e = r_e$, $\\forall e \\in E$),\nit is trivially reduced to the classical Influence Maximization problem.\nTherefore, we still have the following hardness result on RIM \\cite{kempe2003maximizing,feige1998threshold}:\n\n\\begin{theorem} \\label{pro:RIM-nphard}\n\tRIM problem is {\\bf NP}-hard, and for any $\\varepsilon > 0$, it is {\\bf NP}-hard\n\tto find a seed set $S$ with robust ratio at least\n\t$1-\\frac{1}{e} + \\varepsilon$.\n\\end{theorem}\n\n\n\n\n\nTo circumvent the above hardness result, we develop algorithms that achieves reasonably\nlarge robust ratio.\nWhen we are not allowed to make new samples on the edges to improve the input interval, it is natural to\n\nutilize the greedy algorithm of submodular maximization in \\cite{kempe2003maximizing} (i.e., Algorithm~\\ref{alg:greedy})\nas the subroutine to calculate the solution.\nIn light of this, we first propose Lower-Upper Greedy Algorithm and the solution-dependent bound for $g(\\Theta,S)$,\nand then discuss $g(\\Theta,S)$ in the worst-case scenario.\n\n\n\n\\subsection{Lower-Upper Greedy Algorithm}\n\n\n\n\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G = (V,E)$, budget $k$, parameter space $\\Theta = \\times_{e \\in E}[l_e, r_e]$\n\t\t\n\t\t\\STATE $S_{\\theta^{-}}^g \\gets {\\sf Greedy}(G,k,\\theta^-)$ \n\t\t\\STATE $S_{\\theta^{+}}^g \\gets {\\sf Greedy}(G,k,\\theta^+)$\n\t\t\\RETURN ${\\operatorname{arg\\,max}}_{S \\in \\left\\{ S_{\\theta^{-}}^g, S_{\\theta^{+}}^g \\right\\}} \\left\\{ \\sigma_{\\theta^-}(S) \\right\\}$\n\t\\end{algorithmic}\n\t\\caption{{{\\sf LUGreedy}}($G,k,\\Theta$)}\n\t\\label{alg:lugreedy}\n\\end{algorithm}\n\nGiven parameter space $\\Theta=\\times_{e\\in E}[l_e,r_e]$ with the minimum and maximum parameter vectors $\\theta^{-}=(l_e)_{e\\in E}$ and $\\theta^{+}=(r_e)_{e\\in E}$,\nour \\emph{Lower-Upper Greedy algorithm} (${\\sf LUGreedy}(G, k, \\Theta)$) is described in Algorithm~\\ref{alg:lugreedy}\nwhich outputs the best seed set $S^{\\mathsf{LU}}_\\Theta$ for the minimum parameter vector $\\theta^-$ such that\n\n", "itemtype": "equation", "pos": 19354, "prevtext": "\nwhere $S^{*}_{\\theta}$ is the optimal solution of size $k$ when the probability on every edge is given by $\\theta$.\n\n\nGiven $\\Theta$ and solution $S$, the robust ratio $g(\\Theta,S)$\ncharacterizes the {\\em worst-case}\nratio of influence spread of $S$ and the underlying optimal one,\nwhen the true probability vector $\\theta$ is unknown (except knowing that\n$\\theta \\in \\Theta$).\nThen, the {\\em Robust Influence Maximization} (RIM) problem is defined as follows.\n\\begin{problem}[Robust Influence Maximization]\n\tGiven a graph $G=(V,E)$, parameter space $\\Theta=\\times_{e\\in E}[l_e,r_e]$ and a fixed budget $k$, we are required to find a set $S\\subseteq V$ of $k$ vertices,\n\tsuch that robust ratio $g(\\Theta,S)$ is maximized, i.e.,\n\t\n", "index": 7, "text": "\\begin{align*}\n\tS^*_{\\Theta}\n\t:= {\\operatorname{arg\\,max}}_{S \\subseteq V, |S| = k} g(\\Theta,S)\n\t=  {\\operatorname{arg\\,max}}_{S \\subseteq V, |S| = k} \\min_{\\theta\\in \\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})} \\mbox{.}\n\t\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S^{*}_{\\Theta}:={\\operatorname{arg\\,max}}_{S\\subseteq V,|S|=k}g(%&#10;\\Theta,S)={\\operatorname{arg\\,max}}_{S\\subseteq V,|S|=k}\\min_{\\theta\\in\\Theta}%&#10;\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}\\mbox{.}\" display=\"inline\"><mrow><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mo>*</mo></msubsup><mo>:=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></mrow></msub><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mrow><mi>S</mi><mo>\u2286</mo><mi>V</mi></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></mrow></msub><mo>\u2061</mo><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder></mrow><mo>\u2061</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nTo evaluate the performance of this output, we first define the \\emph{gap ratio} $\\alpha(\\Theta) \\in [0,1]$ of the input parameter space to be\n\n", "itemtype": "equation", "pos": 24172, "prevtext": "\n\\end{problem}\n\n\n\n\n\n\n\n\nThe objective of this problem is to find a seed set $S^*_{\\Theta}$ that has the largest robust ratio, that is, $S^*_{\\Theta}$ should maximize the\nworst-case ratio between its influence spread and the optimal influence spread,\nwhen the true probability vector $\\theta$ is unknown.\nWhen there is no uncertainty, which means $\\Theta$ collapses to the true\nprobability $\\theta$, we can see that the RIM problem is reduced back\nto the original influence maximization problem.\n\nIn RIM, the knowledge of the confidence interval is assumed to be the input.\nAnother interpretation is that, it can be viewed as given an estimate of probability vector\n$\\hat{\\theta} = (\\hat{p}_e)_{e \\in E}$ with a perturbation level $\\delta_e$ on each edge $e$,\nsuch that the true probability $p_e \\in [\\hat{p}_e - \\delta_e, \\hat{p}_e + \\delta_e] = [l_e, r_e]$,\nwhich constitutes parameter space $\\Theta = \\times_{e \\in E}[l_e,r_e]$.\nNotice that, in reality, this probability could be obtained via edge samplings,\ni.e., we make samples on edges and compute the fraction of times that the edge is live.\nOn the other hand, we can also observe information cascades on each edge when collecting\nthe trace of diffusion in the real world,\nso that the corresponding probability can be learned.\n\nHowever, when the amount of observed information cascade is small, the best robust ratio $\\max_{S}g(\\Theta,S)$ for the given $\\Theta$ can be low so that the output for a RIM algorithm does not have a good enough guarantee of the performance in the worst case. Then a natural question is, given $\\Theta$, how to further make samples on edges (e.g., activating source node $u$ of an edge $(u,v)$ and see if the sink node $v$ is activated through edge $e$) so that $\\max_{S}g(\\Theta,S)$ can be efficiently improved? To be specific, how to make samples on edges and output $\\Theta'$ and $S'$ according to the outcome so that (a) with high probability the true value $\\theta$ lies in the output parameter space $\\Theta'$, where the randomness is taken according to $\\theta$,\nand (b) $g(\\Theta',S')$ is large.\nThis sub-problem is called \\emph{Sampling for Improving Robust Influence Maximization}, and will be addressed in Section~\\ref{sect:sample}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Algorithm and Analysis for RIM} \\label{sect:rim}\n\n\n\n\nConsider the problem of RIM, parameter space $\\Theta = \\times_{e \\in E}[l_e,r_e]$ is given, and we do not know the true probability $\\theta \\in \\Theta$.\nLet $\\theta^{-}=(l_e)_{e\\in E}$ and $\\theta^{+}=(r_e)_{e\\in E}$.\n\nOur first observation is that, when $\\Theta$ is a single vector ($l_e = r_e$, $\\forall e \\in E$),\nit is trivially reduced to the classical Influence Maximization problem.\nTherefore, we still have the following hardness result on RIM \\cite{kempe2003maximizing,feige1998threshold}:\n\n\\begin{theorem} \\label{pro:RIM-nphard}\n\tRIM problem is {\\bf NP}-hard, and for any $\\varepsilon > 0$, it is {\\bf NP}-hard\n\tto find a seed set $S$ with robust ratio at least\n\t$1-\\frac{1}{e} + \\varepsilon$.\n\\end{theorem}\n\n\n\n\n\nTo circumvent the above hardness result, we develop algorithms that achieves reasonably\nlarge robust ratio.\nWhen we are not allowed to make new samples on the edges to improve the input interval, it is natural to\n\nutilize the greedy algorithm of submodular maximization in \\cite{kempe2003maximizing} (i.e., Algorithm~\\ref{alg:greedy})\nas the subroutine to calculate the solution.\nIn light of this, we first propose Lower-Upper Greedy Algorithm and the solution-dependent bound for $g(\\Theta,S)$,\nand then discuss $g(\\Theta,S)$ in the worst-case scenario.\n\n\n\n\\subsection{Lower-Upper Greedy Algorithm}\n\n\n\n\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G = (V,E)$, budget $k$, parameter space $\\Theta = \\times_{e \\in E}[l_e, r_e]$\n\t\t\n\t\t\\STATE $S_{\\theta^{-}}^g \\gets {\\sf Greedy}(G,k,\\theta^-)$ \n\t\t\\STATE $S_{\\theta^{+}}^g \\gets {\\sf Greedy}(G,k,\\theta^+)$\n\t\t\\RETURN ${\\operatorname{arg\\,max}}_{S \\in \\left\\{ S_{\\theta^{-}}^g, S_{\\theta^{+}}^g \\right\\}} \\left\\{ \\sigma_{\\theta^-}(S) \\right\\}$\n\t\\end{algorithmic}\n\t\\caption{{{\\sf LUGreedy}}($G,k,\\Theta$)}\n\t\\label{alg:lugreedy}\n\\end{algorithm}\n\nGiven parameter space $\\Theta=\\times_{e\\in E}[l_e,r_e]$ with the minimum and maximum parameter vectors $\\theta^{-}=(l_e)_{e\\in E}$ and $\\theta^{+}=(r_e)_{e\\in E}$,\nour \\emph{Lower-Upper Greedy algorithm} (${\\sf LUGreedy}(G, k, \\Theta)$) is described in Algorithm~\\ref{alg:lugreedy}\nwhich outputs the best seed set $S^{\\mathsf{LU}}_\\Theta$ for the minimum parameter vector $\\theta^-$ such that\n\n", "index": 9, "text": "\\begin{align}\\label{def:lu-greedy-solution}\nS^{\\mathsf{LU}}_\\Theta := {\\operatorname{arg\\,max}}_{S \\in \\left\\{ S_{\\theta^{-}}^g, S_{\\theta^{+}}^g \\right\\}} \n\\left\\{ \\sigma_{\\theta^-}(S) \\right\\}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle S^{\\mathsf{LU}}_{\\Theta}:={\\operatorname{arg\\,max}}_{S\\in\\left\\{%&#10;S_{\\theta^{-}}^{g},S_{\\theta^{+}}^{g}\\right\\}}\\left\\{\\sigma_{\\theta^{-}}(S)%&#10;\\right\\}.\" display=\"inline\"><mrow><mrow><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo>:=</mo><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mrow><mi>S</mi><mo>\u2208</mo><mrow><mo>{</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><mi>g</mi></msubsup><mo>,</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo>}</mo></mrow></mrow></msub><mo>\u2061</mo><mrow><mo>{</mo><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nThen, {{\\sf LUGreedy}} achieves the following result:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}[solution-dependent bound] \\label{thm:main}\n\tGiven a graph $G$, parameter space $\\Theta$ and budget limit $k$, {{\\sf LUGreedy}} outputs a seed set\n\t$S^{\\mathsf{LU}}_\\Theta$ of size $k$ such that\n\t\\begin{displaymath}\n\tg(\\Theta, S^{\\mathsf{LU}}_\\Theta) \\ge \\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right) \\mbox{,}\n\t\\end{displaymath}\n\twhere $\\alpha(\\Theta):=\n\t\\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta)}\n\t{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)}$.\n\\end{theorem}\n\n\\begin{proof}\n\tFor any seed set $S$,\n\t$g(\\Theta,S) = \\min_{\\theta\\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}$ \n\tby definition.\n\tObviously, it is a fact that $\\sigma_\\theta(S)$ is monotone on $\\theta$ for any fixed $S$.\n\tFrom the definition of optimal solutions and the greedy algorithm, we can get\n\t$\n\t\\sigma_{\\theta}(S^{*}_{\\theta}) \\le \\sigma_{\\theta^+}(S^{*}_{\\theta}) \\le \\sigma_{\\theta^{+}}(S^{*}_{\\theta^{+}})  \\le \\frac{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) }{1 - 1/e }.\n\t$\n\t\n\tMoreover, it can be implied that\n\t\n", "itemtype": "equation", "pos": 24524, "prevtext": "\n\nTo evaluate the performance of this output, we first define the \\emph{gap ratio} $\\alpha(\\Theta) \\in [0,1]$ of the input parameter space to be\n\n", "index": 11, "text": "\\begin{equation} \\label{eq:def-alpha}\n\n\n\n\\alpha(\\Theta):=\n\\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta)}\n{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)}  \\mbox{.}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\par&#10;\\par&#10;\\alpha(\\Theta):=\\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_{%&#10;\\Theta})}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{g})}\\mbox{.}\" display=\"block\"><mrow><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\tUse seed set $S^{{\\mathsf{LU}}}_{\\Theta}$ from {{\\sf LUGreedy}}, and it follows immediately that\n\t$\n\tg(\\Theta, S^{{\\mathsf{LU}}}_{\\Theta}) \n\t\\ge \\frac{ \\sigma_{\\theta^{-}}(S^{{\\mathsf{LU}}}_{\\Theta}) }{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) } \\left(1-  \\frac{1}{e} \\right)\n\t= \\alpha(\\Theta) \\left(1 - \\frac{1}{e} \\right).\n\t$\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe refer $\\alpha(\\Theta) (1- \\frac{1}{e})$ as the {\\em solution-dependent bound} of $g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$  \nthat {{\\sf LUGreedy}} \nachieves, because it depends on the solution $S^{\\mathsf{LU}}_{\\Theta}$.\nThe good thing is that it can be evaluated once we have the solution, and then\nwe know the robust ratio must be at least this lower bound.\nNote that the bound is good if $\\alpha(\\Theta)$ is not too small, and thus it in turn indicates that\nthe influence spread $\\sigma_{\\theta}(S^{\\mathsf{LU}}_{\\Theta})$\nwe find has a good performance under any probability vector $\\theta \\in \\Theta$.\n\nIt is worth remarking that the choice of using \n$\\alpha(\\Theta) = \\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta) / \\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)$ \n\nas a measurement is for the following reasons:\n(a) Intuitively, $S_{\\theta^{-}}^g$ is expected to be the best possible seed set we can find that maximizes $\\sigma_{\\theta^{-}}(\\cdot)$;\n(b) Meanwhile, we consider $S_{\\theta^{+}}^g$ as a potential seed set for\nthe later theoretical analysis (in the proof of \\Cref{thm:uniform}), \nwhich requires the alignment of the same seed set for the numerator and denominator.\nThus, $\\alpha(\\Theta) \\geq \\max\\{ \\sigma_{\\theta^{-}}(S_{\\theta^{-}}^g), \\sigma_{\\theta^{-}}(S_{\\theta^{+}}^g) \\} / \\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)$.\nIn particular, when $\\theta^+$ and $\\theta^-$ tend to the same value $\\theta$, \nRIM is tending towards the classical Influence Maximization,\nand thus the influence spread $\\sigma_{\\theta}(S^{\\mathsf{LU}}_\\Theta)$\ncan be close to the best possible result $\\sigma_{\\theta}(S_{\\theta}^g)$. \nThe approach adopted by {{\\sf LUGreedy}} is similar to the sandwich approximation used \nin~\\cite{lu2015competition}.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following example shows that for certain problem instances,\nthe gap ratio $\\alpha(\\Theta)$ of {{\\sf LUGreedy}} could match the robust ratio\n$g(\\Theta,S^{\\mathsf{LU}}_\\Theta)$, which also matches\nthe best possible robust ratio $\\max_{|S|=k}g(\\Theta,S)$.\n\\begin{example} \\label{exp:tight}\n\tConsider a graph $G=(V,E)$ where the set of nodes are equally partitioned into $2k$ subsets $V=\\cup_{i=1}^{2k}V_i$ such that every $V_i$ contains $t+1$ nodes.\n\tLet $V_i=\\{v_i^{j}\\mid 1\\le j\\le t+1\\}$ and set $E=\\cup_{i=1}^{2k}E_i$\n\twhere $E_i=\\{(v_i^{1},v_i^{j})\\mid 2\\le j\\le t+1\\}$.\n\tThat is, every $(V_i,E_i)$ forms a star with $v_i^1$ being the node at the center,\n\tall stars are disconnected from one another.\n\tFor the parameter space we set the interval on every edge to be $[l,r]$.\n\tWhen {{\\sf LUGreedy}} select $k$ nodes, since all $v^1_i$'s have the same (marginal)\n\tinfluence spread,  w.l.o.g., suppose that {{\\sf LUGreedy}} selects\n\t$\\{v^1_1, v^1_2,\\ldots, v^1_k \\}$.\n\tThen if we set the true probability vector  $\\theta\\in \\Theta$ such that $p_e=l$ for every $e \\in \\cup_{i=1}^{k} E_i$, and $p_e=r$\n\tfor every $e\\in \\cup_{i=k+1}^{2k} E_i$, it is easy to check that\n\n", "itemtype": "equation", "pos": 25812, "prevtext": "\nThen, {{\\sf LUGreedy}} achieves the following result:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{theorem}[solution-dependent bound] \\label{thm:main}\n\tGiven a graph $G$, parameter space $\\Theta$ and budget limit $k$, {{\\sf LUGreedy}} outputs a seed set\n\t$S^{\\mathsf{LU}}_\\Theta$ of size $k$ such that\n\t\\begin{displaymath}\n\tg(\\Theta, S^{\\mathsf{LU}}_\\Theta) \\ge \\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right) \\mbox{,}\n\t\\end{displaymath}\n\twhere $\\alpha(\\Theta):=\n\t\\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta)}\n\t{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)}$.\n\\end{theorem}\n\n\\begin{proof}\n\tFor any seed set $S$,\n\t$g(\\Theta,S) = \\min_{\\theta\\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}$ \n\tby definition.\n\tObviously, it is a fact that $\\sigma_\\theta(S)$ is monotone on $\\theta$ for any fixed $S$.\n\tFrom the definition of optimal solutions and the greedy algorithm, we can get\n\t$\n\t\\sigma_{\\theta}(S^{*}_{\\theta}) \\le \\sigma_{\\theta^+}(S^{*}_{\\theta}) \\le \\sigma_{\\theta^{+}}(S^{*}_{\\theta^{+}})  \\le \\frac{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) }{1 - 1/e }.\n\t$\n\t\n\tMoreover, it can be implied that\n\t\n", "index": 13, "text": "\\begin{align*}\n\tg(\\Theta,S) \\ge \\min_{\\theta\\in \\Theta} \\frac{ \\sigma_{\\theta}(S) }{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) } \\left(1-  \\frac{1}{e} \\right)\n\t= \\frac{ \\sigma_{\\theta^{-}}(S) }{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) } \\left(1-  \\frac{1}{e} \\right).\n\t\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g(\\Theta,S)\\geq\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{%&#10;\\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}})}\\left(1-\\frac{1}{e}\\right)=\\frac{%&#10;\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}})}\\left(1-\\frac{%&#10;1}{e}\\right).\" display=\"inline\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\\end{example}\nThe intuition from the above example is that, when there are many alternative choices\nfor the best seed set, and these alternative seed sets do not have much overlap\nin their influence coverage, the gap ratio $\\alpha(\\Theta)$ is a good indicator\nof the best possible robust ratio one can achieve.\n\n\n\n\n\n\n\n\n\nIn the next subsection, we will show that the best robust ratio could be very bad\nfor the worst possible graph $G$ and parameter space $\\Theta$,\nwhich motivates us\nto do further sampling to improve $\\Theta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Discussion on the robust ratio} \\label{sect:analysis-sub:hardness}\n\n\n\n\n\n\nFor the theoretical perspective, we show in this part that if we make no assumption or only add loose constraints to the input parameter space $\\Theta$, then no algorithm will guarantee good performance for some worst possible graph $G$.\n\n\n\n\n\n\\begin{theorem}\n\t\\label{thm:3-ratios}\n\tFor RIM,\n\t\\begin{enumerate}\n\t\t\\item \\label{pro:3-ratios-1}\n\t\tThere exists a graph $G=(V,E)$ and parameter space $\\Theta = \\times_{e\\in E} [l_{e}, r_{e}]$, such that\n\t\n", "itemtype": "equation", "pos": 29368, "prevtext": "\n\tUse seed set $S^{{\\mathsf{LU}}}_{\\Theta}$ from {{\\sf LUGreedy}}, and it follows immediately that\n\t$\n\tg(\\Theta, S^{{\\mathsf{LU}}}_{\\Theta}) \n\t\\ge \\frac{ \\sigma_{\\theta^{-}}(S^{{\\mathsf{LU}}}_{\\Theta}) }{ \\sigma_{\\theta^{+}}(S^{g}_{\\theta^{+}}) } \\left(1-  \\frac{1}{e} \\right)\n\t= \\alpha(\\Theta) \\left(1 - \\frac{1}{e} \\right).\n\t$\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe refer $\\alpha(\\Theta) (1- \\frac{1}{e})$ as the {\\em solution-dependent bound} of $g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$  \nthat {{\\sf LUGreedy}} \nachieves, because it depends on the solution $S^{\\mathsf{LU}}_{\\Theta}$.\nThe good thing is that it can be evaluated once we have the solution, and then\nwe know the robust ratio must be at least this lower bound.\nNote that the bound is good if $\\alpha(\\Theta)$ is not too small, and thus it in turn indicates that\nthe influence spread $\\sigma_{\\theta}(S^{\\mathsf{LU}}_{\\Theta})$\nwe find has a good performance under any probability vector $\\theta \\in \\Theta$.\n\nIt is worth remarking that the choice of using \n$\\alpha(\\Theta) = \\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta) / \\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)$ \n\nas a measurement is for the following reasons:\n(a) Intuitively, $S_{\\theta^{-}}^g$ is expected to be the best possible seed set we can find that maximizes $\\sigma_{\\theta^{-}}(\\cdot)$;\n(b) Meanwhile, we consider $S_{\\theta^{+}}^g$ as a potential seed set for\nthe later theoretical analysis (in the proof of \\Cref{thm:uniform}), \nwhich requires the alignment of the same seed set for the numerator and denominator.\nThus, $\\alpha(\\Theta) \\geq \\max\\{ \\sigma_{\\theta^{-}}(S_{\\theta^{-}}^g), \\sigma_{\\theta^{-}}(S_{\\theta^{+}}^g) \\} / \\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)$.\nIn particular, when $\\theta^+$ and $\\theta^-$ tend to the same value $\\theta$, \nRIM is tending towards the classical Influence Maximization,\nand thus the influence spread $\\sigma_{\\theta}(S^{\\mathsf{LU}}_\\Theta)$\ncan be close to the best possible result $\\sigma_{\\theta}(S_{\\theta}^g)$. \nThe approach adopted by {{\\sf LUGreedy}} is similar to the sandwich approximation used \nin~\\cite{lu2015competition}.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following example shows that for certain problem instances,\nthe gap ratio $\\alpha(\\Theta)$ of {{\\sf LUGreedy}} could match the robust ratio\n$g(\\Theta,S^{\\mathsf{LU}}_\\Theta)$, which also matches\nthe best possible robust ratio $\\max_{|S|=k}g(\\Theta,S)$.\n\\begin{example} \\label{exp:tight}\n\tConsider a graph $G=(V,E)$ where the set of nodes are equally partitioned into $2k$ subsets $V=\\cup_{i=1}^{2k}V_i$ such that every $V_i$ contains $t+1$ nodes.\n\tLet $V_i=\\{v_i^{j}\\mid 1\\le j\\le t+1\\}$ and set $E=\\cup_{i=1}^{2k}E_i$\n\twhere $E_i=\\{(v_i^{1},v_i^{j})\\mid 2\\le j\\le t+1\\}$.\n\tThat is, every $(V_i,E_i)$ forms a star with $v_i^1$ being the node at the center,\n\tall stars are disconnected from one another.\n\tFor the parameter space we set the interval on every edge to be $[l,r]$.\n\tWhen {{\\sf LUGreedy}} select $k$ nodes, since all $v^1_i$'s have the same (marginal)\n\tinfluence spread,  w.l.o.g., suppose that {{\\sf LUGreedy}} selects\n\t$\\{v^1_1, v^1_2,\\ldots, v^1_k \\}$.\n\tThen if we set the true probability vector  $\\theta\\in \\Theta$ such that $p_e=l$ for every $e \\in \\cup_{i=1}^{k} E_i$, and $p_e=r$\n\tfor every $e\\in \\cup_{i=k+1}^{2k} E_i$, it is easy to check that\n\n", "index": 15, "text": "\\[\\max_{|S|=k}g(\\Theta,S)=g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})=\\alpha(\\Theta)=\\frac{1+tl}{1+tr}.\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})=\\alpha(\\Theta)=%&#10;\\frac{1+tl}{1+tr}.\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>l</mi></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\t\n\t\t\\item \\label{pro:3-ratios-2}\n\t\tThere exists a graph $G=(V,E)$, constant $\\delta = \\varTheta\\left(\\frac{1}{n}\\right)$ and\n\t\tparameter space $\\Theta = \\times_{e \\in E}[l_{e}, r_{e}]$ where\n\t\t$r_{e} - l_{e} \\leq \\delta$ for every $e \\in E$,\n\t\tsuch that\n\t\n", "itemtype": "equation", "pos": 30561, "prevtext": "\n\\end{example}\nThe intuition from the above example is that, when there are many alternative choices\nfor the best seed set, and these alternative seed sets do not have much overlap\nin their influence coverage, the gap ratio $\\alpha(\\Theta)$ is a good indicator\nof the best possible robust ratio one can achieve.\n\n\n\n\n\n\n\n\n\nIn the next subsection, we will show that the best robust ratio could be very bad\nfor the worst possible graph $G$ and parameter space $\\Theta$,\nwhich motivates us\nto do further sampling to improve $\\Theta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Discussion on the robust ratio} \\label{sect:analysis-sub:hardness}\n\n\n\n\n\n\nFor the theoretical perspective, we show in this part that if we make no assumption or only add loose constraints to the input parameter space $\\Theta$, then no algorithm will guarantee good performance for some worst possible graph $G$.\n\n\n\n\n\n\\begin{theorem}\n\t\\label{thm:3-ratios}\n\tFor RIM,\n\t\\begin{enumerate}\n\t\t\\item \\label{pro:3-ratios-1}\n\t\tThere exists a graph $G=(V,E)$ and parameter space $\\Theta = \\times_{e\\in E} [l_{e}, r_{e}]$, such that\n\t\n", "index": 17, "text": "\\[\n\t\t\\max_{|S|=k}g(\\Theta,S)=\\max_{|S|=k}\\min_{\\theta\\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}=O\\left(\\frac{k}{n}\\right) \\mbox{.}\n\t\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=\\max_{|S|=k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta%&#10;}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}=O\\left(\\frac{k}{n}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mi>k</mi><mi>n</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\t\n\t\t\\item \\label{pro:3-ratios-3}\n\t\tConsider random seeds set $\\tilde{S}$ of size $k$.\n\t\tThere exists a graph $G=(V,E)$, constant $\\delta = \\varTheta\\left(\\frac{1}{\\sqrt{n}}\\right)$ and\n\t\tparameter space $\\Theta = \\times_{e \\in E}[l_{e}, r_{e}]$ where\n\t\t$r_{e} - l_{e} \\leq \\delta$ for every $e \\in E$,\n\t\twe have\n\t\n", "itemtype": "equation", "pos": 30982, "prevtext": "\n\t\t\n\t\t\\item \\label{pro:3-ratios-2}\n\t\tThere exists a graph $G=(V,E)$, constant $\\delta = \\varTheta\\left(\\frac{1}{n}\\right)$ and\n\t\tparameter space $\\Theta = \\times_{e \\in E}[l_{e}, r_{e}]$ where\n\t\t$r_{e} - l_{e} \\leq \\delta$ for every $e \\in E$,\n\t\tsuch that\n\t\n", "index": 19, "text": "\\[\n\t\t\\max_{|S|=k}g(\\Theta,S)=O\\left(\\frac{\\log n}{n}\\right) \\mbox{.}\n\t\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=O\\left(\\frac{\\log n}{n}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>log</mi><mo>\u2061</mo><mi>n</mi></mrow><mi>n</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\twhere $\\Omega$ is any probability distribution over seed sets of size $k$, and\n\t\t${\\mathbb{E}}_{\\tilde{S}\\in \\Omega}[\\cdot]$ is the expectation of random set\n\t\t$\\tilde{S}$ taken from the distribution $\\Omega$.\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t \t\t\n\t\\end{enumerate}\n\\end{theorem}\n\n\n\n\n\nIn the first case, we allow the input $\\Theta$ to be an arbitrary parameter space.\nIt is possible that $\\Theta=\\times_{e\\in E}[0,1]$ for some graph $G$, which means\nthere is no knowledge at all for edge probabilities.\nThen any seed set may achieve $O\\left(\\frac{k}{n}\\right)$-approximation\nof the optimal solution in the worst case.\nIntuitively, a selected seed set $S$ may rarely activate other nodes (i.e., $O(k)$), while optimal solution (to the latent $\\theta$) may cover almost the whole graph (i.e., $\\Omega(n)$).\n\nIn the second case, an additional constraint is assumed on the parameter space\n$\\left\\| {\\theta^+ - \\theta^-} \\right\\|_{\\infty} \\leq \\delta$, i.e.,\nfor every $e\\in E$, $r_e-l_e\\le \\delta$, to see if we could obtain a better performance when $\\delta$ is small.\nHowever, even though $\\delta$ is in the order of $O(1/{n})$, the robust ratio can be as small as $O(\\log{n}/{n})$.\nThe proof is related to the phase transition in the Erd\\H{o}s-R\\'{e}nyi graph\nfor the emergence of giant component.\nIn particular, if we have a graph $G$ consisting of two disconnected, equal-sized\nErd\\H{o}s-R\\'{e}nyi random graphs with edge probabilities close to the\ncritical value of generating a giant connected component, then whenever\nwe select a seed in one component, that component could be just below\nthe threshold resulting in $O(\\log n)$ influence spread while the other\ncomponent is just above the threshold leading to $\\theta(n)$ influence spread.\nThus, the worst-case ratio for any one-node seed set is always\n$O(\\log{n}/{n})$.\nA similar discussion can be found in \\cite{he2015stability}.\n\nIn the third case, we allow the algorithm to be randomized,\nnamely the output seed set $\\tilde{S}$ is a random set of size $k$.\nEven in this case, the robust ratio could be as worse as\n$O(\\log n/\\sqrt{n})$.\n\n\n\n\\if 0\n\n\\begin{proposition}\n\t\\label{pro:no}\n\tIf there is no constraint on input parameter space $\\Theta$, then there exists a graph $G$ such that\n\n", "itemtype": "equation", "pos": 31371, "prevtext": "\n\t\t\n\t\t\\item \\label{pro:3-ratios-3}\n\t\tConsider random seeds set $\\tilde{S}$ of size $k$.\n\t\tThere exists a graph $G=(V,E)$, constant $\\delta = \\varTheta\\left(\\frac{1}{\\sqrt{n}}\\right)$ and\n\t\tparameter space $\\Theta = \\times_{e \\in E}[l_{e}, r_{e}]$ where\n\t\t$r_{e} - l_{e} \\leq \\delta$ for every $e \\in E$,\n\t\twe have\n\t\n", "index": 21, "text": "\\[\n\t\t\\max_{\\Omega} \\min_{\\theta\\in\\Theta}\n\t\t{\\mathbb{E}}_{\\tilde{S}\\in \\Omega}\\left[\n\t\t\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}(S_{\\theta}^*)} \\right]=O\\left(\\frac{\\log n}{\\sqrt{n}}\\right) \\mbox{,}\n\t\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\Omega}\\min_{\\theta\\in\\Theta}{\\mathbb{E}}_{\\tilde{S}\\in\\Omega}\\left[%&#10;\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}(S_{\\theta}^{*})}\\right]=O%&#10;\\left(\\frac{\\log n}{\\sqrt{n}}\\right)\\mbox{,}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mi mathvariant=\"normal\">\u03a9</mi></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><msub><mi>\ud835\udd3c</mi><mrow><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow></msub></mrow></mrow><mo>\u2062</mo><mrow><mo>[</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>log</mi><mo>\u2061</mo><mi>n</mi></mrow><msqrt><mi>n</mi></msqrt></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\\end{proposition}\n\n{\\color{blue}  [\\text{Wei:} {The proof of the above proposition refers to an algorithm, which is wrong. This proposition \tshould not be related to any algorithm. The proof is simply fixing any set $S$, there is always another \t$\\theta$ that make the ratio very bad.}]}\n\n{\\color{blue}  [\\text{Wei:} {However, from the above comment and the proof, I feel that something is strange. \tWhy can we fix $S$ first, and then change $\\theta$ accordingly to get the worst case ratio? \tShouldn't that we should fix $\\theta$ first, that is, it is the true value, and should not change \tif we change a set $S$? \tThinking about playing a game with the adversary, what is the procedure? The adversary \tshould first give a graph, and give the $\\theta$, the graph is known to us, the player, but \tthe $\\theta$ is not known to us, only to the referee. \tThen we choose a set, and the adversary review the $\\theta$ and show us the bound. \tSo this is not as our defined ratio. Let's think about it more.}]}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince $\\Theta$ can be regarded as our knowledge about the transmitting probability on each edge, it is possible that the only knowledge we have is $\\Theta=\\times_{e\\in E}[0,1]$.\nThis proposition shows that any seed set is possibly an $O\\left(\\frac{k}{n}\\right)$-approximation of the optimal solution in the worst case of $\\theta\\in \\Theta$.\nIntuitively, any solution $S$ may only cover $O(k)$ nodes (in the order of the budget), while optimal solution (to the respective $\\theta$)\nmay cover $\\Theta(n)$ (in the order of the whole nodes), for some graph $G$.\n\nNow we may add additive constraints on the input parameter space $\\left\\| {\\theta^+ - \\theta^-} \\right\\|_{\\infty} \\leq \\delta$, i.e.,\nfor every $e\\in E$, $r_e-l_e\\le \\delta$, to see if we could obtain a better performance when $\\delta$ becomes smaller.\nThe following two propositions on the performance are hardness results when imposing certain additive constraints on $\\Theta$.\n\n\\begin{proposition}\n\t\\label{pro:1/n}\n\tWhen $\\delta=\\Omega(\\frac{1}{n})$, then there exists $\\Theta$ and a graph $G$ such that\n\n", "itemtype": "equation", "pos": 33815, "prevtext": "\n\t\twhere $\\Omega$ is any probability distribution over seed sets of size $k$, and\n\t\t${\\mathbb{E}}_{\\tilde{S}\\in \\Omega}[\\cdot]$ is the expectation of random set\n\t\t$\\tilde{S}$ taken from the distribution $\\Omega$.\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t \t\t\n\t\\end{enumerate}\n\\end{theorem}\n\n\n\n\n\nIn the first case, we allow the input $\\Theta$ to be an arbitrary parameter space.\nIt is possible that $\\Theta=\\times_{e\\in E}[0,1]$ for some graph $G$, which means\nthere is no knowledge at all for edge probabilities.\nThen any seed set may achieve $O\\left(\\frac{k}{n}\\right)$-approximation\nof the optimal solution in the worst case.\nIntuitively, a selected seed set $S$ may rarely activate other nodes (i.e., $O(k)$), while optimal solution (to the latent $\\theta$) may cover almost the whole graph (i.e., $\\Omega(n)$).\n\nIn the second case, an additional constraint is assumed on the parameter space\n$\\left\\| {\\theta^+ - \\theta^-} \\right\\|_{\\infty} \\leq \\delta$, i.e.,\nfor every $e\\in E$, $r_e-l_e\\le \\delta$, to see if we could obtain a better performance when $\\delta$ is small.\nHowever, even though $\\delta$ is in the order of $O(1/{n})$, the robust ratio can be as small as $O(\\log{n}/{n})$.\nThe proof is related to the phase transition in the Erd\\H{o}s-R\\'{e}nyi graph\nfor the emergence of giant component.\nIn particular, if we have a graph $G$ consisting of two disconnected, equal-sized\nErd\\H{o}s-R\\'{e}nyi random graphs with edge probabilities close to the\ncritical value of generating a giant connected component, then whenever\nwe select a seed in one component, that component could be just below\nthe threshold resulting in $O(\\log n)$ influence spread while the other\ncomponent is just above the threshold leading to $\\theta(n)$ influence spread.\nThus, the worst-case ratio for any one-node seed set is always\n$O(\\log{n}/{n})$.\nA similar discussion can be found in \\cite{he2015stability}.\n\nIn the third case, we allow the algorithm to be randomized,\nnamely the output seed set $\\tilde{S}$ is a random set of size $k$.\nEven in this case, the robust ratio could be as worse as\n$O(\\log n/\\sqrt{n})$.\n\n\n\n\\if 0\n\n\\begin{proposition}\n\t\\label{pro:no}\n\tIf there is no constraint on input parameter space $\\Theta$, then there exists a graph $G$ such that\n\n", "index": 23, "text": "\\[\n\t\\max_{|S|=k}g(\\Theta,S)=\\max_{|S|=k}\\min_{\\theta\\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}=O\\left(\\frac{k}{n}\\right) \\mbox{.}\n\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=\\max_{|S|=k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta%&#10;}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}=O\\left(\\frac{k}{n}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mi>k</mi><mi>n</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\n\\end{proposition}\n\n\n\nThe proof is via a construction related to Erd\\H{o}s-R\\'{e}nyi graph. Similar discussion can be found in the introduction section of \\cite{he2015stability}.\n\n\nIf we allow the algorithm to be randomized, namely the output seed set $\\tilde{S}$ is a random variable whose possible outcome is any set of nodes with cardinality equal to $k$, the definition of optimal performance would be:\n", "itemtype": "equation", "pos": 36063, "prevtext": "\n\\end{proposition}\n\n{\\color{blue}  [\\text{Wei:} {The proof of the above proposition refers to an algorithm, which is wrong. This proposition \tshould not be related to any algorithm. The proof is simply fixing any set $S$, there is always another \t$\\theta$ that make the ratio very bad.}]}\n\n{\\color{blue}  [\\text{Wei:} {However, from the above comment and the proof, I feel that something is strange. \tWhy can we fix $S$ first, and then change $\\theta$ accordingly to get the worst case ratio? \tShouldn't that we should fix $\\theta$ first, that is, it is the true value, and should not change \tif we change a set $S$? \tThinking about playing a game with the adversary, what is the procedure? The adversary \tshould first give a graph, and give the $\\theta$, the graph is known to us, the player, but \tthe $\\theta$ is not known to us, only to the referee. \tThen we choose a set, and the adversary review the $\\theta$ and show us the bound. \tSo this is not as our defined ratio. Let's think about it more.}]}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince $\\Theta$ can be regarded as our knowledge about the transmitting probability on each edge, it is possible that the only knowledge we have is $\\Theta=\\times_{e\\in E}[0,1]$.\nThis proposition shows that any seed set is possibly an $O\\left(\\frac{k}{n}\\right)$-approximation of the optimal solution in the worst case of $\\theta\\in \\Theta$.\nIntuitively, any solution $S$ may only cover $O(k)$ nodes (in the order of the budget), while optimal solution (to the respective $\\theta$)\nmay cover $\\Theta(n)$ (in the order of the whole nodes), for some graph $G$.\n\nNow we may add additive constraints on the input parameter space $\\left\\| {\\theta^+ - \\theta^-} \\right\\|_{\\infty} \\leq \\delta$, i.e.,\nfor every $e\\in E$, $r_e-l_e\\le \\delta$, to see if we could obtain a better performance when $\\delta$ becomes smaller.\nThe following two propositions on the performance are hardness results when imposing certain additive constraints on $\\Theta$.\n\n\\begin{proposition}\n\t\\label{pro:1/n}\n\tWhen $\\delta=\\Omega(\\frac{1}{n})$, then there exists $\\Theta$ and a graph $G$ such that\n\n", "index": 25, "text": "\\[\n\t\\max_{|S|=k}g(\\Theta,S)=O\\left(\\frac{\\log n}{n}\\right) \\mbox{.}\n\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=O\\left(\\frac{\\log n}{n}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>log</mi><mo>\u2061</mo><mi>n</mi></mrow><mi>n</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhere ${\\textnormal{supp}}(\\tilde{S})$ is the support of distribution $\\tilde{S}$.\n\n\\begin{proposition}\n\t\\label{pro:1/nr}\n\tWhen $\\delta=\\Omega(\\frac{1}{n})$, for any distribution $\\tilde{S}$, then there exists $\\Theta$ and a graph $G$ such that\n\n", "itemtype": "equation", "pos": 36542, "prevtext": "\n\t\n\\end{proposition}\n\n\n\nThe proof is via a construction related to Erd\\H{o}s-R\\'{e}nyi graph. Similar discussion can be found in the introduction section of \\cite{he2015stability}.\n\n\nIf we allow the algorithm to be randomized, namely the output seed set $\\tilde{S}$ is a random variable whose possible outcome is any set of nodes with cardinality equal to $k$, the definition of optimal performance would be:\n", "index": 27, "text": "\n\\[\nr=\\max_{\\tilde{S}:|{\\textnormal{supp}}(\\tilde{S})|= k}\\min_{\\theta\\in\\Theta} {\\mathbb{E}}_{\\tilde{S}}\\left[\n\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}(S_{\\theta}^*)} \\right] \\mbox{,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"r=\\max_{\\tilde{S}:|{\\textnormal{supp}}(\\tilde{S})|=k}\\min_{\\theta\\in\\Theta}{%&#10;\\mathbb{E}}_{\\tilde{S}}\\left[\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}%&#10;(S_{\\theta}^{*})}\\right]\\mbox{,}\" display=\"block\"><mrow><mi>r</mi><mo>=</mo><mrow><mrow><munder><mi>max</mi><mrow><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo>:</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mtext>supp</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><msub><mi>\ud835\udd3c</mi><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover></msub></mrow></mrow><mo>\u2062</mo><mrow><mo>[</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>]</mo></mrow><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\n\\end{proposition}\n\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Sampling for Improving RIM}\\label{sect:sample}\n\nFrom the previous section, we propose {{\\sf LUGreedy}} algorithm to check the solution-dependent bound of the robust ratio,\nand point out the worse-case bound could be small if $\\Theta$ is not assumed to be tight enough.\n\nTheorem~\\ref{thm:3-ratios} in the previous subsection\npoints out that the best possible robust ratio $\\max_{S}g(\\Theta,S)$ can be\ntoo low so that the output for RIM could not provide us with a satisfying seed set in the worst case.\nThen a natural question is: given the input $\\Theta$, can we make efficient samples\non edges\nso that $\\Theta$ is narrowed into $\\Theta'$ (this means the true $\\theta\\in \\Theta'$ with high probability) and then output a seed set $S'$ that makes $g(\\Theta',S')$ large?\nThis problem is called \\emph{Sampling for Improving RIM}.\n\nIn this section we study both uniform sampling and adaptive sampling\nfor improving RIM.\nAccording to the Chernoff's bound, the more samples we make on an edge, the narrower\nthe confidence interval we get that guarantees the true probability\nto be located within the confidence interval with a desired probability of\nconfidence.\nAfter sampling to get a narrower parameter space, we could use \n{{\\sf LUGreedy}} algorithm \t\nto get the seed set.\n\n\n\\subsection{Uniform Sampling} \\label{sect:analysis-sub:Uniform}\n\n\n\n\n\n\nIn Sampling for improving RIM, the goal is to design a sampling and maximization algorithm ${\\mathcal{A}}$ that outputs $\\Theta'$ and $S'$ such that with high probability the robust ratio of $S'$ in $\\Theta'$ is large.\nAfter sampling edges, we can use Chernoff's bound to compute the confidence interval,\nand the confidence interval can be further narrowed down with more samples.\nHowever, the key issue is to connect the width of confidence interval with\nthe stability of influence spread.\nWe propose two ideas\nexploiting properties of additive and multiplicative confidence interval respectively\nto this issue, and incorporate into Uniform Sampling algorithm (in Algorithm~\\ref{alg:uniform-sampling})\nwith theoretical justification (in Theorem~\\ref{thm:uniform}).\n\n\n\n\n\n\n\n\nOur first idea is inspired by the following lemma from \\cite{ChenWY14a} to build the connection in the additive form.\n\n\n\\begin{lemma}[Lemma~7 in \\cite{ChenWY14a}]\n\t\\label{lem:add}\n\tGiven graph $G$ and parameter space $\\Theta$ such that $\\forall \\theta_1,\\theta_2\\in \\Theta$,  $\\left\\| {\\theta_1-\\theta_2} \\right\\|_{\\infty}\\leq \\delta$, then, $\\forall S\\subseteq V$,\n\n", "itemtype": "equation", "pos": 36985, "prevtext": "\nwhere ${\\textnormal{supp}}(\\tilde{S})$ is the support of distribution $\\tilde{S}$.\n\n\\begin{proposition}\n\t\\label{pro:1/nr}\n\tWhen $\\delta=\\Omega(\\frac{1}{n})$, for any distribution $\\tilde{S}$, then there exists $\\Theta$ and a graph $G$ such that\n\n", "index": 29, "text": "\\[\n\t\\max_{\\tilde{S}:|{\\textnormal{supp}}(\\tilde{S})|= k}\\min_{\\theta\\in\\Theta} {\\mathbb{E}}_{\\tilde{S}}\\left[\n\t\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}(S_{\\theta}^*)} \\right]=O\\left(\\frac{\\log n}{\\sqrt{n}}\\right) \\mbox{.}\n\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\max_{\\tilde{S}:|{\\textnormal{supp}}(\\tilde{S})|=k}\\min_{\\theta\\in\\Theta}{%&#10;\\mathbb{E}}_{\\tilde{S}}\\left[\\frac{\\sigma_{\\theta}(\\tilde{S})}{\\sigma_{\\theta}%&#10;(S_{\\theta}^{*})}\\right]=O\\left(\\frac{\\log n}{\\sqrt{n}}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo>:</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mtext>supp</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><msub><mi>\ud835\udd3c</mi><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover></msub></mrow></mrow><mo>\u2062</mo><mrow><mo>[</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mrow><mi>log</mi><mo>\u2061</mo><mi>n</mi></mrow><msqrt><mi>n</mi></msqrt></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\t\n\\end{lemma}\n\nWe use a tight example (in the order of $|V|$ and $|E|$) to illustrate the connection and give an insight of this lemma as follows.\n\n\n\nConsider graph $G = (V,E)$ with $|V|=n$ and $|E| = m$ ($m \\gg n$).\nLet $G$ be two disjoint cycles, each containing exactly $\\frac{n}{2}$ nodes and $\\frac{n}{2}$ edges.\nWe arbitrarily assign the rest $m-n$ edges between two cycles.\nThen, for every edge $e$ in the cycle, the interval is $l_e = r_e = 1$,\nand $l_e = 0$, $r_e = \\delta$ for those between two cycles,\nwhich constitutes $\\Theta = \\times_{e \\in E} [l_e, r_e]$.\nSuppose $\\delta > 0$ is sufficiently small, and let budget $k=1$. For any single-node set $S$,\nit is easy to check that for $\\theta^- = (l_e)_{e \\in E}$, $\\sigma_{\\theta^-}(S) = \\frac{n}{2}$,\nand for $\\theta^+ = (r_e)_{e \\in E}$,\n$\\sigma_{\\theta^+}(S) \\approx \\frac{n}{2} + \\frac{n}{2} (m - n) \\delta$,\nthus $\\left| {\\sigma_{\\theta^+}(S)-\\sigma_{\\theta^-}(S)} \\right| \\approx \\frac{1}{2} n(m-n)\\delta$ in this case.\nAs a comparison, from Lemma~\\ref{lem:add}, we know that $\\left| {\\sigma_{\\theta^+}(S)-\\sigma_{\\theta^-}(S)} \\right| \\leq m n \\delta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, the above lemma establishes the guidance that we may sample every edge for sufficient times to shrink their confidence intervals in $\\Theta$,\nand feed {{\\sf LUGreedy}} with $\\Theta$ as same as solving RIM, then the performance is guaranteed by Theorem~\\ref{thm:main},\nwhich matches our intuition that {{\\sf LUGreedy}} performs well with the satisfactory $\\Theta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, our second idea is to use the multiplicative confidence interval to reduce the fluctuation of influence spread,\nthen {{\\sf LUGreedy}} still applies.\nThe next lemma is crucial to achieve this goal.\n\n\\begin{lemma}\n\t\\label{lem:mul}\n\tGiven graph $G=(V,E)$ and parameter space $\\Theta$. If there exists $\\lambda \\geq 0$, for all edge $e\\in E$, s.t.,\n\t$r_e \\leq (1+\\lambda)l_e$, then for any nonempty set $S \\subseteq V$,\n\t\n", "itemtype": "equation", "pos": 39751, "prevtext": "\n\t\n\\end{proposition}\n\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Sampling for Improving RIM}\\label{sect:sample}\n\nFrom the previous section, we propose {{\\sf LUGreedy}} algorithm to check the solution-dependent bound of the robust ratio,\nand point out the worse-case bound could be small if $\\Theta$ is not assumed to be tight enough.\n\nTheorem~\\ref{thm:3-ratios} in the previous subsection\npoints out that the best possible robust ratio $\\max_{S}g(\\Theta,S)$ can be\ntoo low so that the output for RIM could not provide us with a satisfying seed set in the worst case.\nThen a natural question is: given the input $\\Theta$, can we make efficient samples\non edges\nso that $\\Theta$ is narrowed into $\\Theta'$ (this means the true $\\theta\\in \\Theta'$ with high probability) and then output a seed set $S'$ that makes $g(\\Theta',S')$ large?\nThis problem is called \\emph{Sampling for Improving RIM}.\n\nIn this section we study both uniform sampling and adaptive sampling\nfor improving RIM.\nAccording to the Chernoff's bound, the more samples we make on an edge, the narrower\nthe confidence interval we get that guarantees the true probability\nto be located within the confidence interval with a desired probability of\nconfidence.\nAfter sampling to get a narrower parameter space, we could use \n{{\\sf LUGreedy}} algorithm \t\nto get the seed set.\n\n\n\\subsection{Uniform Sampling} \\label{sect:analysis-sub:Uniform}\n\n\n\n\n\n\nIn Sampling for improving RIM, the goal is to design a sampling and maximization algorithm ${\\mathcal{A}}$ that outputs $\\Theta'$ and $S'$ such that with high probability the robust ratio of $S'$ in $\\Theta'$ is large.\nAfter sampling edges, we can use Chernoff's bound to compute the confidence interval,\nand the confidence interval can be further narrowed down with more samples.\nHowever, the key issue is to connect the width of confidence interval with\nthe stability of influence spread.\nWe propose two ideas\nexploiting properties of additive and multiplicative confidence interval respectively\nto this issue, and incorporate into Uniform Sampling algorithm (in Algorithm~\\ref{alg:uniform-sampling})\nwith theoretical justification (in Theorem~\\ref{thm:uniform}).\n\n\n\n\n\n\n\n\nOur first idea is inspired by the following lemma from \\cite{ChenWY14a} to build the connection in the additive form.\n\n\n\\begin{lemma}[Lemma~7 in \\cite{ChenWY14a}]\n\t\\label{lem:add}\n\tGiven graph $G$ and parameter space $\\Theta$ such that $\\forall \\theta_1,\\theta_2\\in \\Theta$,  $\\left\\| {\\theta_1-\\theta_2} \\right\\|_{\\infty}\\leq \\delta$, then, $\\forall S\\subseteq V$,\n\n", "index": 31, "text": "\\[\n\t\\left| {\\sigma_{\\theta_1}(S)-\\sigma_{\\theta_2}(S)} \\right| \\leq mn\\delta \\mbox{.}\n\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\left|{\\sigma_{\\theta_{1}}(S)-\\sigma_{\\theta_{2}}(S)}\\right|\\leq mn\\delta\\mbox%&#10;{.}\" display=\"block\"><mrow><mrow><mo>|</mo><mrow><mrow><msub><mi>\u03c3</mi><msub><mi>\u03b8</mi><mn>1</mn></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c3</mi><msub><mi>\u03b8</mi><mn>2</mn></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>|</mo></mrow><mo>\u2264</mo><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>\u03b4</mi><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\tand\n\t\n", "itemtype": "equation", "pos": 41861, "prevtext": "\n\t\n\\end{lemma}\n\nWe use a tight example (in the order of $|V|$ and $|E|$) to illustrate the connection and give an insight of this lemma as follows.\n\n\n\nConsider graph $G = (V,E)$ with $|V|=n$ and $|E| = m$ ($m \\gg n$).\nLet $G$ be two disjoint cycles, each containing exactly $\\frac{n}{2}$ nodes and $\\frac{n}{2}$ edges.\nWe arbitrarily assign the rest $m-n$ edges between two cycles.\nThen, for every edge $e$ in the cycle, the interval is $l_e = r_e = 1$,\nand $l_e = 0$, $r_e = \\delta$ for those between two cycles,\nwhich constitutes $\\Theta = \\times_{e \\in E} [l_e, r_e]$.\nSuppose $\\delta > 0$ is sufficiently small, and let budget $k=1$. For any single-node set $S$,\nit is easy to check that for $\\theta^- = (l_e)_{e \\in E}$, $\\sigma_{\\theta^-}(S) = \\frac{n}{2}$,\nand for $\\theta^+ = (r_e)_{e \\in E}$,\n$\\sigma_{\\theta^+}(S) \\approx \\frac{n}{2} + \\frac{n}{2} (m - n) \\delta$,\nthus $\\left| {\\sigma_{\\theta^+}(S)-\\sigma_{\\theta^-}(S)} \\right| \\approx \\frac{1}{2} n(m-n)\\delta$ in this case.\nAs a comparison, from Lemma~\\ref{lem:add}, we know that $\\left| {\\sigma_{\\theta^+}(S)-\\sigma_{\\theta^-}(S)} \\right| \\leq m n \\delta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, the above lemma establishes the guidance that we may sample every edge for sufficient times to shrink their confidence intervals in $\\Theta$,\nand feed {{\\sf LUGreedy}} with $\\Theta$ as same as solving RIM, then the performance is guaranteed by Theorem~\\ref{thm:main},\nwhich matches our intuition that {{\\sf LUGreedy}} performs well with the satisfactory $\\Theta$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, our second idea is to use the multiplicative confidence interval to reduce the fluctuation of influence spread,\nthen {{\\sf LUGreedy}} still applies.\nThe next lemma is crucial to achieve this goal.\n\n\\begin{lemma}\n\t\\label{lem:mul}\n\tGiven graph $G=(V,E)$ and parameter space $\\Theta$. If there exists $\\lambda \\geq 0$, for all edge $e\\in E$, s.t.,\n\t$r_e \\leq (1+\\lambda)l_e$, then for any nonempty set $S \\subseteq V$,\n\t\n", "index": 33, "text": "\\begin{align}\n\t\\frac{\\sigma_{\\theta^+}(S)}{\\sigma_{\\theta^-}(S)} \\leq (1+\\lambda)^{n} \\mbox{,}\n\t\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\sigma_{\\theta^{+}}(S)}{\\sigma_{\\theta^{-}}(S)}\\leq(1+%&#10;\\lambda)^{n}\\mbox{,}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2264</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\\end{lemma}\nIn this lemma, the ratio of influence spread can be bounded based on the relation of $l_e$ and $r_e$ in the multiplicative form.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo unify both ideas mentioned above, we propose \\emph{Uniform Sampling for RIM} algorithm ({\\sf US-RIM}) in Algorithm~\\ref{alg:uniform-sampling},\nand the theoretical result is presented in Theorem~\\ref{thm:uniform}.\nBasically, the algorithm samples every edge with the same number of\ntimes, \nand use {{\\sf LUGreedy}} to obtain the seed set.\n\nWe set different $t$ and $\\delta_e$ for the two ideas.\nHenceforth,\nwe explicitly refer the first setting as {\\em Uniform Sampling with Additive form} ({\\sf US-RIM-A}), and the second one as {\\em Uniform Sampling with Multiplicative form} ({\\sf US-RIM-M}).\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G=(V,E)$, budget $k$, $(\\epsilon, \\gamma)$\n\t\t\\ENSURE Parameter space $\\Theta_{out}$, seed set $S_{out}$\n\t\t\\FORALL{$e \\in E$}\n\t\t\\STATE Sample $e$ for $t$ times, and observe $x^1_e, \\ldots, x^t_e$\n\t\t\\STATE $p_e\\gets\\frac{1}{t}\\sum_{i=1}^{t}x_e^i$, and set $\\delta_e$ according to Theorem~\\ref{thm:uniform}\n\t\t\\STATE $r_e\\gets\\min\\{1,p_e+\\delta_e\\}$, $l_e\\gets\\max\\{0,p_e-\\delta_e\\}$\n\t\t\\ENDFOR\n\t\t\\STATE \n\t\t$\\Theta_\\text{out} \\gets \\times_{e \\in E} [l_e,r_e]$\n\t\t\\STATE $S_\\text{out} \\gets {\\sf LUGreedy}(G,k,\\Theta_\\text{out})$ \n\t\t\\RETURN $(\\Theta_\\text{out}$,$S_\\text{out})$\n\t\\end{algorithmic}\n\t\\caption{{\\sf US-RIM}}\n\t\\label{alg:uniform-sampling}\n\\end{algorithm}\n\n\n\n\n\n\\begin{theorem}\\label{thm:uniform}\n\tGiven a graph $G=(V,E)$, budget $k$, and accuracy parameter $\\epsilon,\\gamma>0$, let $n=|V|$ and $m=|E|$, then for any unknown ground-truth parameter vector $\\theta=(p_e)_{e \\in E}$, Algorithm {\\sf US-RIM} outputs $(\\Theta_\\text{out}$,$S_\\text{out})$ such that\n\n", "itemtype": "equation", "pos": 41975, "prevtext": "\n\tand\n\t\n", "index": 35, "text": "\\begin{equation}\n\t\n\t\\max_{|S|= k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S_{\\theta}^{*})}\\geq (1+\\lambda)^{-n} \\mbox{.}\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\par&#10;\\max_{|S|=k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{%&#10;\\theta}(S_{\\theta}^{*})}\\geq(1+\\lambda)^{-n}\\mbox{.}\" display=\"block\"><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>\u2265</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mi>n</mi></mrow></msup><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\twith $\\Pr[\\theta \\in \\Theta_\\text{out}]\\ge 1-\\gamma$,\n\twhere the randomness is taken according to $\\theta$,\n\tif we follow either of the two settings:\n\t\\begin{enumerate}\n\t\t\\item \\label{thm-add-case}\n\t\tSet $t=\\frac{2m^2n^2 \\ln (2m/\\gamma)}{k^2\\epsilon^2}$, and for all $e$, set $\\delta_e=\\frac{k\\epsilon}{mn}$;\n\t\t\\item \\label{thm-mul-case}\n\t\tAssume we have $p'$ such that $0 < p' \\leq \\min_{e\\in E} p_e$,\n\t\tset $t=\\frac{3 \\ln (2m/\\gamma)}{ p' } \\left( \\frac{2n}{\\ln (1/1-\\epsilon)} + 1 \\right)^2$,\n\t\tand for all edge, set $\\delta_e=\\frac{1}{n} p_e\\log\\frac{1}{\\gamma}$.\n\t\\end{enumerate}\n\\end{theorem}\n\n\nIn general, the total number of samples summing up all edges is $O(\\frac{m^3n^2\\log (m/\\gamma)}{k^2\\epsilon^2})$ for {{\\sf {US-RIM-A}}},\nand $O(\\frac{mn^2 \\log (m/\\gamma)}{p' \\epsilon^2})$ for {{\\sf {US-RIM-M}}} with an additional constant $p'$, the lower bound probability on all edge probabilities.\nThe difference is that the former has a higher order of $m$, and the latter requires the knowledge of $p'$ and has an extra dependency on $O(1/p')$.\nSince the sample complexity for both settings can be calculated in advance,\none may compare the values and choose the smaller one when running the uniform sampling algorithm.\nAn intuitive interpretation is that: (1) with high probability ($\\geq 1-\\gamma$),\nthe algorithm always outputs an $(1-\\frac{1}{e}-\\epsilon)$-approximation solution guaranteed by {{\\sf {US-RIM-A}}};\n(2) if $p'=\\Omega(\\frac{k^2}{m^2})$ (it is a loose assumption naturally satisfied in practice),\nwe may choose {{\\sf {US-RIM-M}}} to achieve better sample complexity.\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Non-uniform and Adaptive Sampling} \\label{sect:heuristic}\n\nIn a real network, the importance of edges in an influence diffusion process varies significantly.\nSome edges may have larger influence probability than others or connect two important nodes in the network.\nTherefore, in sampling it is crucial to sample edges appropriately.\nMoreover, we can adapt our sampling strategy dynamically to put more sampling effort on\ncritical edges when we learn the edge probabilities\nmore accurately over time.\n\n\n\n\n\n\n\n\nFor convenience, given graph $G = (V, E)$, we define \\emph{observation set} $\\mathcal{M} = \\left\\{ M_e \\right\\}_{e \\in E}$ as a collection of sets, where\n$M_e = \\{ x^1_e, x^2_e, \\cdots, x^{t_e}_e \\}$ denotes observed values of edge $e$ via the first $t_e$ samples on edge $e$.\nWe allow that a parameter space $\\Theta_0 \\subseteq \\times_{e \\in E} [0,1]$ is given,\nwhich can be obtained by some initial samples $\\mathcal{M}_0$\n(e.g., uniformly sample each edge of the graph for a fixed number of times).\n\n\n\n\n\n\n\n\nThe following lemma is used to calculate the confidence interval, which\nis a combination of additive and multiplicative Chernoff's Bound.\nWe adopt this bound in the experiment since some edges in the graph has large influence probability while others have small ones,\nbut using either additive or multiplicative bound may not be good enough\nto obtain a small confidence interval.\n\nThe following bound is adapted from \\cite{badanidiyuru2013bandit}\nand is crucial for us in the experiment.\n\n\n\n\n\n\n\n\n\n\\begin{lemma} \\label{lem:conf}\n\t\n\tFor each $e \\in E$, let $M_e = \\left\\{ x^1_e, x^2_e, \\dots, x^{t_e}_e \\right\\}$ be samples of $e$ in $\\mathcal M = \\{ M_e \\}_{e \\in E}$, and $t_e$ be the sample number.\n\tGiven any $\\gamma > 0$, let confidence intervals for all edges be\n\t$\\Theta = \\times_{e\\in E} [l_e, r_e]$, such that, for any $e \\in E$,\n\t\n\t\n", "itemtype": "equation", "pos": 43924, "prevtext": "\n\\end{lemma}\nIn this lemma, the ratio of influence spread can be bounded based on the relation of $l_e$ and $r_e$ in the multiplicative form.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo unify both ideas mentioned above, we propose \\emph{Uniform Sampling for RIM} algorithm ({\\sf US-RIM}) in Algorithm~\\ref{alg:uniform-sampling},\nand the theoretical result is presented in Theorem~\\ref{thm:uniform}.\nBasically, the algorithm samples every edge with the same number of\ntimes, \nand use {{\\sf LUGreedy}} to obtain the seed set.\n\nWe set different $t$ and $\\delta_e$ for the two ideas.\nHenceforth,\nwe explicitly refer the first setting as {\\em Uniform Sampling with Additive form} ({\\sf US-RIM-A}), and the second one as {\\em Uniform Sampling with Multiplicative form} ({\\sf US-RIM-M}).\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G=(V,E)$, budget $k$, $(\\epsilon, \\gamma)$\n\t\t\\ENSURE Parameter space $\\Theta_{out}$, seed set $S_{out}$\n\t\t\\FORALL{$e \\in E$}\n\t\t\\STATE Sample $e$ for $t$ times, and observe $x^1_e, \\ldots, x^t_e$\n\t\t\\STATE $p_e\\gets\\frac{1}{t}\\sum_{i=1}^{t}x_e^i$, and set $\\delta_e$ according to Theorem~\\ref{thm:uniform}\n\t\t\\STATE $r_e\\gets\\min\\{1,p_e+\\delta_e\\}$, $l_e\\gets\\max\\{0,p_e-\\delta_e\\}$\n\t\t\\ENDFOR\n\t\t\\STATE \n\t\t$\\Theta_\\text{out} \\gets \\times_{e \\in E} [l_e,r_e]$\n\t\t\\STATE $S_\\text{out} \\gets {\\sf LUGreedy}(G,k,\\Theta_\\text{out})$ \n\t\t\\RETURN $(\\Theta_\\text{out}$,$S_\\text{out})$\n\t\\end{algorithmic}\n\t\\caption{{\\sf US-RIM}}\n\t\\label{alg:uniform-sampling}\n\\end{algorithm}\n\n\n\n\n\n\\begin{theorem}\\label{thm:uniform}\n\tGiven a graph $G=(V,E)$, budget $k$, and accuracy parameter $\\epsilon,\\gamma>0$, let $n=|V|$ and $m=|E|$, then for any unknown ground-truth parameter vector $\\theta=(p_e)_{e \\in E}$, Algorithm {\\sf US-RIM} outputs $(\\Theta_\\text{out}$,$S_\\text{out})$ such that\n\n", "index": 37, "text": "\\[\n\tg(\\Theta_\\text{out}, S_\\text{out})\\ge \\left(1-\\frac{1}{e}\\right)(1-\\epsilon),\n\t\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"g(\\Theta_{\\text{out}},S_{\\text{out}})\\geq\\left(1-\\frac{1}{e}\\right)(1-\\epsilon),\" display=\"block\"><mrow><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mtext>out</mtext></msub><mo>,</mo><msub><mi>S</mi><mtext>out</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>e</mi></mfrac></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\twhere $\\hat{p}_e=\\frac{ \\sum_{i=1}^{t_e} x^{i}_e }{t_e}$, $c_e = \\sqrt{\\frac{3}{t_e} \\ln\\frac{2m}{\\gamma}}$.\n\tThen, with probability at least $1-\\gamma$, the true probability $\\theta= \\left(p_e\\right)_{e\\in E}$ satisfies that $\\theta\\in\\Theta$.\n\t\n\t\n\t\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur intuition for non-uniform sampling is that the edges along the information cascade\nof important seeds determine the influence spread, and henceforth they should be estimated more accurately than other edges not along important information\ncascade paths.\nThus, we use the following \\emph{Information Cascade Sampling} method to select edges.\n\nStarting from the seed set $S$, once node $v$ is activated, $v$ will try to activate its out-neighbors.\nIn other words, \nfor every out-edge $e$ of $v$, denote $t_e$ as the number of samples,\nthen $e$ will be sampled once to generate a new observation $x_e^{t_e}$ based on the latent distribution,\nand $t_e$ will be increased by $1$.\nThe process goes on until the end of the information cascade.\n\nWe propose \\emph{Information Cascade Sampling for RIM} ({{\\sf ICS-RIM}}) algorithm in Algorithm~\\ref{alg:information-cascade-sampling},\nwhich adopts information cascade sampling described above to select edges.\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G=(V,E)$, budget $k$, initial sample $\\mathcal{M}_0$, threshold $\\kappa$, $\\gamma$.\n\t\t\\ENSURE Parameter space $\\Theta_\\text{out}$, seed set $S_\\text{out}$\n\t\t\\STATE $i\\gets 0$\n\t\t\\REPEAT\n\t\t\\STATE Get $\\Theta_i$ based on $\\mathcal{M}_i$ (see Lemma~\\ref{lem:conf}).\n\t\t\\STATE $S^{\\mathsf{LU}}_{\\Theta_i} = {\\sf LUGreedy}(G,k, \\Theta_i)$ \n\t\t\\STATE $\\mathcal{M}_{i+1} \\gets \\mathcal{M}_i$\n\t\t\\FOR{$i=1,2,\\ldots,\\tau$}\n\t\t\\STATE Do information cascade with the seed set $S^{\\mathsf{LU}}_{\\Theta_i}$ \n\t\t\\STATE During the cascade, once $v\\in V$ is activated, sample all out-edges of $v$ and update $\\mathcal{M}_{i+1}$\n\t\t\\ENDFOR\n\t\t\\STATE $i\\gets i+1$\n\t\t\\UNTIL{$\\alpha(\\Theta_i)>\\kappa$}\n\t\t\\STATE $S_\\text{out} \\gets S^{\\mathsf{LU}}_{\\Theta_{i-1}}$ \n\t\t\\STATE $\\Theta_\\text{out} \\gets \\Theta_{i-1}$\n\t\t\\RETURN $(\\Theta_\\text{out}, S_\\text{out})$\n\t\\end{algorithmic}\n\t\\caption{{{\\sf ICS-RIM}}$(\\tau)$: Information Cascade Sampling}\n\t\\label{alg:information-cascade-sampling}\n\\end{algorithm}\n\n\nAlgorithm~\\ref{alg:information-cascade-sampling} is an iterative procedure. In the $i$-th iteration,\nLemma~\\ref{lem:conf} is used to compute the confidence interval $\\Theta_i$ from observation set $\\mathcal{M}_i$.\nThen according to $\\Theta_i$, we find the lower-upper greedy set $S^{{\\mathsf{LU}}}_{\\Theta_{i}}$ and use information cascade to\nupdate observation set $\\mathcal{M}_{i+1}$ by absorbing new samples.\n\n\n\n\n\nSince the robust ratio $g(\\Theta, S^{{\\mathsf{LU}}}_{\\Theta_{i}})$ cannot be calculated efficiently, we will calculate $\\alpha(\\Theta)$ (defined in \\eqref{eq:def-alpha}) instead.\n\nIn our algorithm, we use a pre-determined threshold $\\kappa$ ($\\kappa \\in (0,1)$) as the stopping criteria.\nTherefore, for $S_\\text{out}$, the robust ratio $g(\\Theta,S_\\text{out})\\ge \\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right) > \\kappa\\left(1-\\frac{1}{e}\\right)$ is guaranteed by Theorem~\\ref{thm:main},\nand the true probability $\\theta \\in \\Theta_\\text{out}$ holds with probability at least $1-\\gamma$ due to Lemma~\\ref{lem:conf}. \n\n\n\n\n\n\n\n\n\n\n\n\nCompared with information cascade sampling method, calculating a greedy set is time-consuming.\n\nTherefore in Algorithm \\ref{alg:information-cascade-sampling}, we call ${\\sf LUGreedy}$ once every $\\tau$ rounds of information cascades\nto reduce the cost.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Empirical Evaluation} \\label{sect:experiments}\n\nWe conduct experiments on two datasets,\nFlixster\\footnote{http://www.cs.sfu.ca/$\\sim$sja25/personal/datasets/} and \nNetHEPT\\footnote{http://research.microsoft.com/en-us/people/weic/projects.aspx} \nto verify the robustness of influence maximization and our sampling methods.\n\n\n\n\n\n\n\n\\subsection{Experiment Setup}\n\n\\subsubsection{Data Description}\n\n\\paragraph*{Flixster}\nThe Flixster dataset is a network of American social movie discovery service (www.flixster.com). To transform the dataset into a weighted graph, each user is represented by a node, and a directed edge from node $u$ to $v$ is formed\nif $v$ rates one movie shortly after $u$ does so on the common movie.\nThe dataset is analyzed in \\cite{barbieri2013topic}, and the influence probability are learned by the topic-aware model.\nWe use the learning result of \\cite{barbieri2013topic} in our experiment, which is a graph containing 29357 nodes and 212614 directed edges.\nThere are 10 probabilities on each edge, and each probability represents the influence from the source user to the sink on a specific topic.\nSince most movies belong to at most two topics,\nwe only consider 3 out of 10 topics in our experiment, and get two induced graphs whose number of edges are 23252 and 64934 respectively. For the first graph, probabilities of topic 8 are directly used as the ground truth parameter (termed as Flixster(Topic~8)).\nFor the second graph, we mix the probabilities of Topic 1 and Topic 4 on each edge evenly to obtain the ground-truth probability (termed as as Flixster(Mixed)).\nAfter removing isolated nodes, the number of nodes in the two graphs are 14473 and 7118 respectively.\n\nIn~\\cite{barbieri2013topic}, the probability for every edge $(u,v)$ is learned\nby rating cascades that reach $u$ and may or may not reach $v$, and in this\ncases we view that edge $(u,v)$ are sampled.\nAccording to the data reported in~\\cite{barbieri2013topic}, on average\nevery edge is sampled $318$ times for their learning process.\nWe then use $318$ samples on each edge as our initial sample\n${\\cal M}_0$.\n\n\\paragraph*{NetHEPT}\nThe NetHEPT dataset \\cite{chen2009efficient} is extensively used in may influence\nmaximization studies.\nIt is an academic collaboration networks from the \"High Energy\nPhysics-Theory\" section of arXiv form 1991 to 2003, where nodes represent the authors\nand each edge in the network represents one paper co-authored by two nodes.\nIt contains $15233$ nodes and $58891$ undirected edges (including duplicated edges).\nWe remove those duplicated edges and obtain a directed graph $G=(V,E), |V|=15233, |E|=62774$ (directed edges).\n\nSince the NetHEPT dataset does not contain the data of influence probability on edges,\nwe set the probability on edges according to the \\emph{weighted cascade} model \\cite{kempe2003maximizing}\nas the ground truth parameter, i.e.,\n$\\forall e = (v, u)\\in E$, let $x_u$ be the in-degree of $u$ in the\nedge-duplicated graph, $y_{e}$ be the number of edges connecting node $v$ and $u$,\nthen the true probability is $p_e = 1 - (1-\\frac{1}{x_u})^{y_e}$.\n\n\n\nFollowing the same baseline of Flixster, we initially sample each edge \nfor 318 times as $\\mathcal{M}_0$. \n\n\n\\subsubsection{Algorithms}\n\\label{algorithms}\n\nWe test both the uniform sampling algorithm {{\\sf {US-RIM}}} and the adaptive sampling\nalgorithm {{\\sf ICS-RIM}}, as well as another adaptive algorithm\n{{\\sf OES-RIM}} (Out-Edge Sampling) as the baseline (to be described shortly).\nEach algorithm is given a graph $G$ and initial observation set $\\mathcal{M}_0$.\nNote that the method to estimate the parameter space based on sampling results in Algorithm~\\ref{alg:uniform-sampling} and Algorithm~\\ref{alg:information-cascade-sampling} are different. In order to make the comparison meaningful, in this section, for all three algorithms, a common method according to Lemma~\\ref{lem:conf} is used to estimate the parameter space. In all tests, we set the size of the seed set $k=50$. To reduce the running time, we use a faster approximation algorithm PMIA (proposed in \\cite{chen2010scalable}) to replace the well known greedy algorithm purposed in \\cite{kempe2003maximizing} in the whole experiment. The accuracy requirement $\\gamma=o(1)$ is set to be $\\gamma=m^{-0.5}$ where $m$ is the number of edges.\n\n\\paragraph*{{\\sf {US-RIM}}}\nThe algorithm is slightly modified from Algorithm~\\ref{alg:uniform-sampling} for a better  comparison of performance. The modified algorithm proceeds in an iterative fashion: In each iteration, the algorithm makes $\\tau_1$ \n\nsamples on each edge, updates $\\Theta$ according to Lemma~\\ref{lem:conf} and computes $\\alpha(\\Theta)$. The algorithm stops when $\\alpha(\\Theta)\\ge \\kappa=0.8$.\n$\\tau_1$ is set to 1000, 1000, 250 for NetHEPT, Flixster(Topic~8), Flixster(Mixed), respectively\nto achieve fine granularity \nand generate visually difference of $\\alpha(\\Theta)$ in our results.\n\n\n\\paragraph*{{\\sf ICS-RIM}}\nAs stated in Algorithm~\\ref{alg:information-cascade-sampling}, in each iteration, the algorithm do \n$\\tau_2 = 5000$ \n\ntimes information cascade sampling based on the seed set from the last iteration,\n\nand then it updates $\\Theta$ according to Lemma~\\ref{lem:conf}, computes $\\alpha(\\Theta)$ and uses {{\\sf LUGreedy}} algorithm to compute the seed set for the next round. The algorithm stops when $\\alpha(\\Theta)\\ge \\kappa=0.8$.\n\n\\paragraph*{{\\sf OES-RIM}}\nThis algorithm acts as a baseline, and it proceeds in the similar way to {{\\sf ICS-RIM}}.\nInstead of sampling information cascades starting from the current seed set\nas in {{\\sf ICS-RIM}}, {{\\sf OES-RIM}} only sample {\\em out-edges} from the seed set.\nMore specifically, in each iteration, the algorithm samples $5000$ \ntimes of all out-edges of the seed set from last iteration, for the three graphs respectively, and then it updates $\\Theta$ according to Lemma~\\ref{lem:conf}, computes $\\alpha(\\Theta)$ and uses {{\\sf LUGreedy}} algorithm to compute the seed set for the next round.\nNote that for {{\\sf OES-RIM}}, $\\alpha(\\Theta)$ remains small (with the increase of the number of samples) and cannot exceed the threshold $\\kappa$ even the iteration has been processed for a large number of times,\ntherefore we will terminate it when $\\alpha(\\Theta)$ is stable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{$\\bar{\\alpha}$ as a Upper Bound}\nTheorem~\\ref{thm:main} shows that $\\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right)$ is\na lower bound for the robust ratio $g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$.\nWe would also like to find some upper bound of $g(\\Theta, S^{\\mathsf{LU}}_{\\Theta})$ and\nif the upper bound is reasonably close to the lower bound or match in trend of\nchanges,  it indicates that $\\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right)$ is\na reasonable indicator of the robust ratio achieved by the {{\\sf LUGreedy}}\noutput $S^{\\mathsf{LU}}_{\\Theta}$.\nFor any $\\theta\\in \\Theta$, we define $\\bar{\\alpha}(\\Theta, \\theta)\n= \\frac{\\sigma_{\\theta}\\left( ^{\\mathsf{LU}}_{\\Theta} \\right)}{\\sigma_{\\theta}(S_{\\theta}^g)}$.\nThe following shows that $\\bar{\\alpha}(\\Theta, \\theta)$ is an upper bound for\n$g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$:\n\n", "itemtype": "equation", "pos": 47483, "prevtext": "\n\twith $\\Pr[\\theta \\in \\Theta_\\text{out}]\\ge 1-\\gamma$,\n\twhere the randomness is taken according to $\\theta$,\n\tif we follow either of the two settings:\n\t\\begin{enumerate}\n\t\t\\item \\label{thm-add-case}\n\t\tSet $t=\\frac{2m^2n^2 \\ln (2m/\\gamma)}{k^2\\epsilon^2}$, and for all $e$, set $\\delta_e=\\frac{k\\epsilon}{mn}$;\n\t\t\\item \\label{thm-mul-case}\n\t\tAssume we have $p'$ such that $0 < p' \\leq \\min_{e\\in E} p_e$,\n\t\tset $t=\\frac{3 \\ln (2m/\\gamma)}{ p' } \\left( \\frac{2n}{\\ln (1/1-\\epsilon)} + 1 \\right)^2$,\n\t\tand for all edge, set $\\delta_e=\\frac{1}{n} p_e\\log\\frac{1}{\\gamma}$.\n\t\\end{enumerate}\n\\end{theorem}\n\n\nIn general, the total number of samples summing up all edges is $O(\\frac{m^3n^2\\log (m/\\gamma)}{k^2\\epsilon^2})$ for {{\\sf {US-RIM-A}}},\nand $O(\\frac{mn^2 \\log (m/\\gamma)}{p' \\epsilon^2})$ for {{\\sf {US-RIM-M}}} with an additional constant $p'$, the lower bound probability on all edge probabilities.\nThe difference is that the former has a higher order of $m$, and the latter requires the knowledge of $p'$ and has an extra dependency on $O(1/p')$.\nSince the sample complexity for both settings can be calculated in advance,\none may compare the values and choose the smaller one when running the uniform sampling algorithm.\nAn intuitive interpretation is that: (1) with high probability ($\\geq 1-\\gamma$),\nthe algorithm always outputs an $(1-\\frac{1}{e}-\\epsilon)$-approximation solution guaranteed by {{\\sf {US-RIM-A}}};\n(2) if $p'=\\Omega(\\frac{k^2}{m^2})$ (it is a loose assumption naturally satisfied in practice),\nwe may choose {{\\sf {US-RIM-M}}} to achieve better sample complexity.\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Non-uniform and Adaptive Sampling} \\label{sect:heuristic}\n\nIn a real network, the importance of edges in an influence diffusion process varies significantly.\nSome edges may have larger influence probability than others or connect two important nodes in the network.\nTherefore, in sampling it is crucial to sample edges appropriately.\nMoreover, we can adapt our sampling strategy dynamically to put more sampling effort on\ncritical edges when we learn the edge probabilities\nmore accurately over time.\n\n\n\n\n\n\n\n\nFor convenience, given graph $G = (V, E)$, we define \\emph{observation set} $\\mathcal{M} = \\left\\{ M_e \\right\\}_{e \\in E}$ as a collection of sets, where\n$M_e = \\{ x^1_e, x^2_e, \\cdots, x^{t_e}_e \\}$ denotes observed values of edge $e$ via the first $t_e$ samples on edge $e$.\nWe allow that a parameter space $\\Theta_0 \\subseteq \\times_{e \\in E} [0,1]$ is given,\nwhich can be obtained by some initial samples $\\mathcal{M}_0$\n(e.g., uniformly sample each edge of the graph for a fixed number of times).\n\n\n\n\n\n\n\n\nThe following lemma is used to calculate the confidence interval, which\nis a combination of additive and multiplicative Chernoff's Bound.\nWe adopt this bound in the experiment since some edges in the graph has large influence probability while others have small ones,\nbut using either additive or multiplicative bound may not be good enough\nto obtain a small confidence interval.\n\nThe following bound is adapted from \\cite{badanidiyuru2013bandit}\nand is crucial for us in the experiment.\n\n\n\n\n\n\n\n\n\n\\begin{lemma} \\label{lem:conf}\n\t\n\tFor each $e \\in E$, let $M_e = \\left\\{ x^1_e, x^2_e, \\dots, x^{t_e}_e \\right\\}$ be samples of $e$ in $\\mathcal M = \\{ M_e \\}_{e \\in E}$, and $t_e$ be the sample number.\n\tGiven any $\\gamma > 0$, let confidence intervals for all edges be\n\t$\\Theta = \\times_{e\\in E} [l_e, r_e]$, such that, for any $e \\in E$,\n\t\n\t\n", "index": 39, "text": "\\begin{equation*}\n\t\\begin{aligned}\n\tl_e &= \\min\\left\\{\\hat{p}_e + \\frac{c_e^2}{2} - c_e\\sqrt{\\frac{c_e^2}{4} + \\hat{p}_e}, ~0\\right\\}\\\\\n\tr_e &= \\max\\left\\{\\hat{p}_e + \\frac{c_e^2}{2} + c_e\\sqrt{\\frac{c_e^2}{4} + \\hat{p}_e}, ~1\\right\\},\n\t\\end{aligned}\n\t\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle l_{e}\" display=\"inline\"><msub><mi>l</mi><mi>e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\min\\left\\{\\hat{p}_{e}+\\frac{c_{e}^{2}}{2}-c_{e}\\sqrt{\\frac{c_{e%&#10;}^{2}}{4}+\\hat{p}_{e}},~{}0\\right\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>2</mn></mfrac></mstyle></mrow><mo>-</mo><mrow><msub><mi>c</mi><mi>e</mi></msub><mo>\u2062</mo><msqrt><mrow><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>4</mn></mfrac></mstyle><mo>+</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub></mrow></msqrt></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mn>0</mn><mo>}</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle r_{e}\" display=\"inline\"><msub><mi>r</mi><mi>e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\max\\left\\{\\hat{p}_{e}+\\frac{c_{e}^{2}}{2}+c_{e}\\sqrt{\\frac{c_{e%&#10;}^{2}}{4}+\\hat{p}_{e}},~{}1\\right\\},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>2</mn></mfrac></mstyle><mo>+</mo><mrow><msub><mi>c</mi><mi>e</mi></msub><mo>\u2062</mo><msqrt><mrow><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>4</mn></mfrac></mstyle><mo>+</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub></mrow></msqrt></mrow></mrow><mo rspace=\"5.8pt\">,</mo><mn>1</mn><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\n\n\n\nThe next question is how to find a $\\theta=(\\theta_e)_{e\\in E}\\in \\Theta$\n\nto make the upper bound\n$\\bar{\\alpha}(\\Theta,\\theta)$ as small as possible.\nIn our experiments, we use the following two heuristics and take their minimum.\n\nThe first heuristic borrows the intuition from Example~\\ref{exp:tight}, which\nsays that the gap ratio $\\alpha(\\Theta)$ is close to the robust ratio\n$g(\\Theta, S^{\\mathsf{LU}}_{\\Theta})$ when (a) there are two disjoint seed sets with\nsimilar influence spead, (b) their cascade overlap is small, and\n(c) the reachable edges from one seed set use lower end parameters values while the reachable edges from the other seed set use upper end parameters.\nThus in our heuristic, we use PMIA algorithm to find another seed set $S'$\nof $k$ nodes\nwhen we remove all nodes in $S^{\\mathsf{LU}}_{\\Theta}$.\nWe then do information cascades from both $S^{\\mathsf{LU}}_{\\Theta}$ and $S'$ for an\nequal number of times.\nFinally, for every edge $e$, if it is sampled more in the information cascade with seed set $S^{\\mathsf{LU}}_{\\Theta}$ than with $S'$, we set $\\theta_e=l_e$, otherwise we set $\\theta_e=r_e$.\nThe second heuristic is a variant of the first one, where we run a number of\ninformation cascades from $S^{\\mathsf{LU}}_{\\Theta}$, and for any edge $e$\nthat is sampled in at least $10\\%$ of cascades, we set $\\theta_e=l_e$,\notherwise we set $\\theta_e=r_e$.\n\nOther more sophisticated heuristics are possible, but it could be a separate\nresearch topic to find tighter upper bound for the robust ratio, and thus\nwe only use the simple combination of the above two in this\npaper, which is already indicative.\nWe henceforth use $\\bar{\\alpha}(\\Theta)$ to represent\nthe upper bound found by the minimum of the above two heuristics.\n\n\\subsection{Results}\n\n\\subsubsection{The Value $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\twith Predetermined Parameter Space}\n\nIn the first experiment we explore the relationship between\nthe width of confidence interval $\\Theta=\\times_{e\\in E}[l_e,r_e]$ and $\\alpha(\\Theta)$ together\nwith $\\bar{\\alpha}(\\Theta)$.\nFor a given interval width $W$,\nwe set $l_e=\\min\\{p_e-\\frac{W}{2},0\\},r_e=\\max\\{p_e+\\frac{W}{2},1\\}$\n$\\forall e\\in E$, where $p_e$ is the ground-truth probability of $e$.\nThen we calculate $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nWe vary the width $W$ to see the trend of changes of\n$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nFigure~\\ref{fig1} reports the result on the three graphs with seed set size $k=50$.\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{AR.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different widths of confidence interval $W$.}\n\t\\label{fig1}\n\\end{figure}\n\nFirst, we observe that as the parameter space $\\Theta$ becomes wider,\nthe value of both $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ become smaller,\nwhich matches our intuition that larger uncertainty results in worse\nrobustness.\nSecond, there is a sharp decrease of $\\alpha(\\Theta)$\nbetween $W\\in [0,0.1]$ and a much slower decrease afterwards\nfor all three graphs.\nThe decrease of $\\bar{\\alpha}(\\Theta)$ is not as sharp as that of $\\alpha(\\Theta)$\nbut the decrease also slows down with larger $W$ after $0.2$.\nThe overall trend of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\nsuggests that the robust ratio may be sensitive\nto the uncertainty of the parameter space, and only when the uncertainty of\nthe parameter space reduces to a certain level that we can obtain reasonable\nguarantee on the robustness of our solution.\n\n\nAs a comparison, we know that the average number\nof samples on each edge is $318$ for the learned probabilities in the\nFlixster dataset.\nThis corresponds to an average interval width of\n0.293 for topic 8 and 0.265 for the mixed topic.\nAt these interval widths, $\\alpha(\\Theta)$ values are approximately\n$0.04$ and $0.08$ respectively for the two graphs, and\n$\\bar{\\alpha}(\\Theta)$ are approximately $0.12$ and $0.2$ respectively.\nThis means that, even considering the upper bound  $\\bar{\\alpha}(\\Theta)$,\nthe robust ratio is pretty low, and thus the learned probabilities\nreported in~\\cite{barbieri2013topic} may result in quite poor performance for\nrobust influence maximization.\n\nOf course, our result of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ is only\ntargeted at the robustness of our {{\\sf LUGreedy}} algorithm, and there could\nexist better algorithm having higher robustness performance at the same\nuncertainty level. \nFinding a better RIM algorithm seems to be a difficult task, and\nwe hope that our study could motivate more research in searching for such better\nRIM algorithms.\nBesides $S^{\\mathsf{LU}}_{\\Theta}$, we also independently test the classical\ngreedy seed set $S^g_{\\theta}$ for $\\theta=(p_e)_{e\\in E}$\non the lower parameter vector $\\theta^-$\n(that is $\\frac{\\sigma_{\\theta^-}(S^g_{\\theta})}{\\sigma_{\\theta^+}(S^g_{\\theta^+})}$ versus $\\alpha(\\Theta)$),\nand the average performance on each data point is $2.45\\%$, $1.05\\%$, $6.11\\%$ worse than $S^{\\mathsf{LU}}_{\\Theta}$\nfor Flixster(Mixed), Flixster(Topic~8) and NetHEPT, respectively.\nTherefore, it shows that $S^{\\mathsf{LU}}_{\\Theta}$ outperforms $\\sigma^g_{\\theta}$ in the worse-case scenario, \nand henceforth we only use $S^{\\mathsf{LU}}_{\\Theta}$ in the following experiments.\n\n\n\n\n\n\\subsubsection{Results for Sampling algorithms}\n\nFigures~\\ref{fig2}, \\ref{fig3} and \\ref{fig4} reports the result of\n$\\alpha=\\alpha(\\Theta)$ and $\\bar{\\alpha}=\\bar{\\alpha}(\\Theta)$ \nfor the three tested graphs respectively, \nwhen the average number of samples per edge increases. \nFor better result presentation, we trim all figures as long as \n$\\alpha(\\text{{\\sf {US-RIM}}}) = 0.7$. \n(For example, in Flixster(Topic 8), \n{{\\sf {US-RIM}}} requires $77318$ samples in average for $\\alpha$ to reach $0.8$, \nwhile {{\\sf ICS-RIM}} only needs $33033$, and for {{\\sf OES-RIM}} $\\alpha$ sticks to $0.118$.)\n\nFor the sampling algorithms, after the $i$-{th} iteration, the observation\nset is updated from $\\mathcal{M}_{i-1}$ to $\\mathcal{M}_i$,\nand the average number of samples per edge in the network is calculated.\nMarkers on each curve in these figures represent the result after one\niteration of the corresponding sampling algorithm.\t\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{NetHEPT.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph NetHEPT.}\n\t\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{Flixster8.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph Flixster(Topic~8).}\n\t\\label{fig3}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{FlixsterMix.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph Flixster(Mixed).}\n\t\\label{fig4}\n\\end{figure}\n\nThe results on all three graphs are consistent.\nFirst, for each pair of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$, even though\nthere is still some gap between the two, indicating either the lower bound\nor the upper bound may not be tight yet, the trends on\nboth $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ are consistent:\nBoth increase with the number of samples, even with similar slopes at each\npoint; and among different\nalgorithms, the ranking order and relative change are consistent\nwith both $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nAll these consistency suggests that gap ratio $\\alpha(\\Theta)$ could be used\nas an indicator for the robustness of the {{\\sf LUGreedy}} algorithm, and\nit is reasonable to use $\\alpha(\\Theta)$ in comparing the performance\nof different algorithms.\n\nSecond, comparing the performance of three algorithms, we see that\nboth {{\\sf {US-RIM}}} and {{\\sf ICS-RIM}} are helpful in improving the robust ratio of\nthe selected seed set, and {{\\sf ICS-RIM}} is better than {{\\sf {US-RIM}}}, especially\nwhen the sample size increases.\nThe baseline algorithm {{\\sf OES-RIM}}, however, performs significantly poorer\nthan the other two, even though it is also an adaptive algorithm as\n{{\\sf ICS-RIM}}.\nThe reason is that, the lower-upper greedy set $S^{{\\mathsf{LU}}}_{\\Theta}$ changes little\nafter a certain number of iterations in {{\\sf OES-RIM}},\nand thus only a small number of edges (out edges of $S^{{\\mathsf{LU}}}_{\\Theta}$)\nare repeatedly sampled.\nThe probabilities on these edges are already estimated very accurately\nwhile other edge probabilities are far from accurate.\nIt is the inaccurate edges that make $\\alpha(\\Theta)$ and the best robust ratio small.\nIn contrast, {{\\sf ICS-RIM}} uses information cascades to sample\nnot only edges directly connecting to the seed set but also edges\nthat can be potentially reached.\nThis suggests that it is important for a sampling method to balance the\nsampling between critical edges and other potentially useful edges\nin order to achieve better robustness in influence maximization.\n\nOverall, the results suggest that information cascade based sampling method\nstands out as a competitive choice to use when we can adaptively sample more\nedges to achieve better robustness.\nIf adaptive sampling is not possible, then predetermined uniform sampling\nmay also perform reasonably well.\n\n\\section{Conclusion}  \\label{sect:conclusion}\n\nIn this paper, we propose the study of robust influence maximization to address\nthe impact of uncertainty in edge probability estimates that would inevitably occur\nin practice to the influence maximization task.\nWe propose the {{\\sf LUGreedy}} algorithm with a proven solution-dependent bound,\nand further propose sampling methods, in particular information cascade\nbased adaptive sample method to effectively reduce the uncertainty and\nincrease the robustness of the {{\\sf LUGreedy}} algorithm.\nThe experimental results validate the usefulness of the {{\\sf LUGreedy}} algorithm\nand the information cascade based sampling method {{\\sf ICS-RIM}}.\nMoreover, the results indicate that robustness may be sensitive to the uncertainty\nof parameter space, and learning algorithms may need more data to achieve\naccurate learning results for robust influence maximization.\n\nOur work opens up a number of research directions.\nFirst, it is unclear what could be the upper bound of the best robust ratio given\nan actual network and learned parameter space.\nAnswering this question would help us to understand whether robust\ninfluence maximization is intrinsically\ndifficult for a particular network or it is just our algorithm that does not\nperform well.\nIf it is the latter case, then an important direction is to\ndesign better robust influence maximization algorithms.\nAnother direction is how to improve sampling methods and learning methods\nto achieve more accurate parameter learning, which seems to be crucial\nfor robust influence maximization.\nIn summary, our work indicates a big data challenge on social influence research\n--- the data on social influence analysis is still not big enough,\nsuch that the uncertainty level in model learning may result in\npoor performance for influence maximization.\nWe hope that our work could encourage further researches to meet this\nchallenge from multiple aspects including\ndata collection, data analysis, and algorithm design.\n\n\\bibliographystyle{abbrv}\n\\bibliography{sigproc}\n\n\\appendix\n\n\n\\section{Proof of Theorem~3} \n\n\n\\begin{proof}\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-1}): }\nLet $G$ be an $n$-clique and $\\Theta = \\times_{e\\in E} [0, 1]$, i.e., for every edge $e$, $l_e=0$ and $r_e=1$.\nFor arbitrary set $S=\\{v_1,\\cdots, v_k\\}$, there exists a valid parameter vector $\\theta = (p_e)_{e \\in E} \\in \\Theta$, where $p_e=0$ for all $E_S=\\{e=(u,v)\\mid u\\in S \\text{ or } v\\in S\\}$ and $p_e=1$ for all $e\\notin E_S$.\nThen, $\\sigma_{\\theta}(S)=k$ and $\\sigma_{\\theta}(S^{*}_{\\theta})=n-1$,\nwhich implies that\n$g(\\Theta, S) =\n\t\\min_{\\theta \\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}\n\t\\leq \\frac{k}{n-1}\n$.\nFor any set $S$ of size $k$, the above holds, thus we can conclude that\n", "itemtype": "equation", "pos": 58515, "prevtext": "\n\twhere $\\hat{p}_e=\\frac{ \\sum_{i=1}^{t_e} x^{i}_e }{t_e}$, $c_e = \\sqrt{\\frac{3}{t_e} \\ln\\frac{2m}{\\gamma}}$.\n\tThen, with probability at least $1-\\gamma$, the true probability $\\theta= \\left(p_e\\right)_{e\\in E}$ satisfies that $\\theta\\in\\Theta$.\n\t\n\t\n\t\n\\end{lemma}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur intuition for non-uniform sampling is that the edges along the information cascade\nof important seeds determine the influence spread, and henceforth they should be estimated more accurately than other edges not along important information\ncascade paths.\nThus, we use the following \\emph{Information Cascade Sampling} method to select edges.\n\nStarting from the seed set $S$, once node $v$ is activated, $v$ will try to activate its out-neighbors.\nIn other words, \nfor every out-edge $e$ of $v$, denote $t_e$ as the number of samples,\nthen $e$ will be sampled once to generate a new observation $x_e^{t_e}$ based on the latent distribution,\nand $t_e$ will be increased by $1$.\nThe process goes on until the end of the information cascade.\n\nWe propose \\emph{Information Cascade Sampling for RIM} ({{\\sf ICS-RIM}}) algorithm in Algorithm~\\ref{alg:information-cascade-sampling},\nwhich adopts information cascade sampling described above to select edges.\n\n\n\\begin{algorithm}[t]\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Graph $G=(V,E)$, budget $k$, initial sample $\\mathcal{M}_0$, threshold $\\kappa$, $\\gamma$.\n\t\t\\ENSURE Parameter space $\\Theta_\\text{out}$, seed set $S_\\text{out}$\n\t\t\\STATE $i\\gets 0$\n\t\t\\REPEAT\n\t\t\\STATE Get $\\Theta_i$ based on $\\mathcal{M}_i$ (see Lemma~\\ref{lem:conf}).\n\t\t\\STATE $S^{\\mathsf{LU}}_{\\Theta_i} = {\\sf LUGreedy}(G,k, \\Theta_i)$ \n\t\t\\STATE $\\mathcal{M}_{i+1} \\gets \\mathcal{M}_i$\n\t\t\\FOR{$i=1,2,\\ldots,\\tau$}\n\t\t\\STATE Do information cascade with the seed set $S^{\\mathsf{LU}}_{\\Theta_i}$ \n\t\t\\STATE During the cascade, once $v\\in V$ is activated, sample all out-edges of $v$ and update $\\mathcal{M}_{i+1}$\n\t\t\\ENDFOR\n\t\t\\STATE $i\\gets i+1$\n\t\t\\UNTIL{$\\alpha(\\Theta_i)>\\kappa$}\n\t\t\\STATE $S_\\text{out} \\gets S^{\\mathsf{LU}}_{\\Theta_{i-1}}$ \n\t\t\\STATE $\\Theta_\\text{out} \\gets \\Theta_{i-1}$\n\t\t\\RETURN $(\\Theta_\\text{out}, S_\\text{out})$\n\t\\end{algorithmic}\n\t\\caption{{{\\sf ICS-RIM}}$(\\tau)$: Information Cascade Sampling}\n\t\\label{alg:information-cascade-sampling}\n\\end{algorithm}\n\n\nAlgorithm~\\ref{alg:information-cascade-sampling} is an iterative procedure. In the $i$-th iteration,\nLemma~\\ref{lem:conf} is used to compute the confidence interval $\\Theta_i$ from observation set $\\mathcal{M}_i$.\nThen according to $\\Theta_i$, we find the lower-upper greedy set $S^{{\\mathsf{LU}}}_{\\Theta_{i}}$ and use information cascade to\nupdate observation set $\\mathcal{M}_{i+1}$ by absorbing new samples.\n\n\n\n\n\nSince the robust ratio $g(\\Theta, S^{{\\mathsf{LU}}}_{\\Theta_{i}})$ cannot be calculated efficiently, we will calculate $\\alpha(\\Theta)$ (defined in \\eqref{eq:def-alpha}) instead.\n\nIn our algorithm, we use a pre-determined threshold $\\kappa$ ($\\kappa \\in (0,1)$) as the stopping criteria.\nTherefore, for $S_\\text{out}$, the robust ratio $g(\\Theta,S_\\text{out})\\ge \\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right) > \\kappa\\left(1-\\frac{1}{e}\\right)$ is guaranteed by Theorem~\\ref{thm:main},\nand the true probability $\\theta \\in \\Theta_\\text{out}$ holds with probability at least $1-\\gamma$ due to Lemma~\\ref{lem:conf}. \n\n\n\n\n\n\n\n\n\n\n\n\nCompared with information cascade sampling method, calculating a greedy set is time-consuming.\n\nTherefore in Algorithm \\ref{alg:information-cascade-sampling}, we call ${\\sf LUGreedy}$ once every $\\tau$ rounds of information cascades\nto reduce the cost.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Empirical Evaluation} \\label{sect:experiments}\n\nWe conduct experiments on two datasets,\nFlixster\\footnote{http://www.cs.sfu.ca/$\\sim$sja25/personal/datasets/} and \nNetHEPT\\footnote{http://research.microsoft.com/en-us/people/weic/projects.aspx} \nto verify the robustness of influence maximization and our sampling methods.\n\n\n\n\n\n\n\n\\subsection{Experiment Setup}\n\n\\subsubsection{Data Description}\n\n\\paragraph*{Flixster}\nThe Flixster dataset is a network of American social movie discovery service (www.flixster.com). To transform the dataset into a weighted graph, each user is represented by a node, and a directed edge from node $u$ to $v$ is formed\nif $v$ rates one movie shortly after $u$ does so on the common movie.\nThe dataset is analyzed in \\cite{barbieri2013topic}, and the influence probability are learned by the topic-aware model.\nWe use the learning result of \\cite{barbieri2013topic} in our experiment, which is a graph containing 29357 nodes and 212614 directed edges.\nThere are 10 probabilities on each edge, and each probability represents the influence from the source user to the sink on a specific topic.\nSince most movies belong to at most two topics,\nwe only consider 3 out of 10 topics in our experiment, and get two induced graphs whose number of edges are 23252 and 64934 respectively. For the first graph, probabilities of topic 8 are directly used as the ground truth parameter (termed as Flixster(Topic~8)).\nFor the second graph, we mix the probabilities of Topic 1 and Topic 4 on each edge evenly to obtain the ground-truth probability (termed as as Flixster(Mixed)).\nAfter removing isolated nodes, the number of nodes in the two graphs are 14473 and 7118 respectively.\n\nIn~\\cite{barbieri2013topic}, the probability for every edge $(u,v)$ is learned\nby rating cascades that reach $u$ and may or may not reach $v$, and in this\ncases we view that edge $(u,v)$ are sampled.\nAccording to the data reported in~\\cite{barbieri2013topic}, on average\nevery edge is sampled $318$ times for their learning process.\nWe then use $318$ samples on each edge as our initial sample\n${\\cal M}_0$.\n\n\\paragraph*{NetHEPT}\nThe NetHEPT dataset \\cite{chen2009efficient} is extensively used in may influence\nmaximization studies.\nIt is an academic collaboration networks from the \"High Energy\nPhysics-Theory\" section of arXiv form 1991 to 2003, where nodes represent the authors\nand each edge in the network represents one paper co-authored by two nodes.\nIt contains $15233$ nodes and $58891$ undirected edges (including duplicated edges).\nWe remove those duplicated edges and obtain a directed graph $G=(V,E), |V|=15233, |E|=62774$ (directed edges).\n\nSince the NetHEPT dataset does not contain the data of influence probability on edges,\nwe set the probability on edges according to the \\emph{weighted cascade} model \\cite{kempe2003maximizing}\nas the ground truth parameter, i.e.,\n$\\forall e = (v, u)\\in E$, let $x_u$ be the in-degree of $u$ in the\nedge-duplicated graph, $y_{e}$ be the number of edges connecting node $v$ and $u$,\nthen the true probability is $p_e = 1 - (1-\\frac{1}{x_u})^{y_e}$.\n\n\n\nFollowing the same baseline of Flixster, we initially sample each edge \nfor 318 times as $\\mathcal{M}_0$. \n\n\n\\subsubsection{Algorithms}\n\\label{algorithms}\n\nWe test both the uniform sampling algorithm {{\\sf {US-RIM}}} and the adaptive sampling\nalgorithm {{\\sf ICS-RIM}}, as well as another adaptive algorithm\n{{\\sf OES-RIM}} (Out-Edge Sampling) as the baseline (to be described shortly).\nEach algorithm is given a graph $G$ and initial observation set $\\mathcal{M}_0$.\nNote that the method to estimate the parameter space based on sampling results in Algorithm~\\ref{alg:uniform-sampling} and Algorithm~\\ref{alg:information-cascade-sampling} are different. In order to make the comparison meaningful, in this section, for all three algorithms, a common method according to Lemma~\\ref{lem:conf} is used to estimate the parameter space. In all tests, we set the size of the seed set $k=50$. To reduce the running time, we use a faster approximation algorithm PMIA (proposed in \\cite{chen2010scalable}) to replace the well known greedy algorithm purposed in \\cite{kempe2003maximizing} in the whole experiment. The accuracy requirement $\\gamma=o(1)$ is set to be $\\gamma=m^{-0.5}$ where $m$ is the number of edges.\n\n\\paragraph*{{\\sf {US-RIM}}}\nThe algorithm is slightly modified from Algorithm~\\ref{alg:uniform-sampling} for a better  comparison of performance. The modified algorithm proceeds in an iterative fashion: In each iteration, the algorithm makes $\\tau_1$ \n\nsamples on each edge, updates $\\Theta$ according to Lemma~\\ref{lem:conf} and computes $\\alpha(\\Theta)$. The algorithm stops when $\\alpha(\\Theta)\\ge \\kappa=0.8$.\n$\\tau_1$ is set to 1000, 1000, 250 for NetHEPT, Flixster(Topic~8), Flixster(Mixed), respectively\nto achieve fine granularity \nand generate visually difference of $\\alpha(\\Theta)$ in our results.\n\n\n\\paragraph*{{\\sf ICS-RIM}}\nAs stated in Algorithm~\\ref{alg:information-cascade-sampling}, in each iteration, the algorithm do \n$\\tau_2 = 5000$ \n\ntimes information cascade sampling based on the seed set from the last iteration,\n\nand then it updates $\\Theta$ according to Lemma~\\ref{lem:conf}, computes $\\alpha(\\Theta)$ and uses {{\\sf LUGreedy}} algorithm to compute the seed set for the next round. The algorithm stops when $\\alpha(\\Theta)\\ge \\kappa=0.8$.\n\n\\paragraph*{{\\sf OES-RIM}}\nThis algorithm acts as a baseline, and it proceeds in the similar way to {{\\sf ICS-RIM}}.\nInstead of sampling information cascades starting from the current seed set\nas in {{\\sf ICS-RIM}}, {{\\sf OES-RIM}} only sample {\\em out-edges} from the seed set.\nMore specifically, in each iteration, the algorithm samples $5000$ \ntimes of all out-edges of the seed set from last iteration, for the three graphs respectively, and then it updates $\\Theta$ according to Lemma~\\ref{lem:conf}, computes $\\alpha(\\Theta)$ and uses {{\\sf LUGreedy}} algorithm to compute the seed set for the next round.\nNote that for {{\\sf OES-RIM}}, $\\alpha(\\Theta)$ remains small (with the increase of the number of samples) and cannot exceed the threshold $\\kappa$ even the iteration has been processed for a large number of times,\ntherefore we will terminate it when $\\alpha(\\Theta)$ is stable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{$\\bar{\\alpha}$ as a Upper Bound}\nTheorem~\\ref{thm:main} shows that $\\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right)$ is\na lower bound for the robust ratio $g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$.\nWe would also like to find some upper bound of $g(\\Theta, S^{\\mathsf{LU}}_{\\Theta})$ and\nif the upper bound is reasonably close to the lower bound or match in trend of\nchanges,  it indicates that $\\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right)$ is\na reasonable indicator of the robust ratio achieved by the {{\\sf LUGreedy}}\noutput $S^{\\mathsf{LU}}_{\\Theta}$.\nFor any $\\theta\\in \\Theta$, we define $\\bar{\\alpha}(\\Theta, \\theta)\n= \\frac{\\sigma_{\\theta}\\left( ^{\\mathsf{LU}}_{\\Theta} \\right)}{\\sigma_{\\theta}(S_{\\theta}^g)}$.\nThe following shows that $\\bar{\\alpha}(\\Theta, \\theta)$ is an upper bound for\n$g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})$:\n\n", "index": 41, "text": "\\begin{equation*}\n\\bar{\\alpha}(\\Theta, \\theta) =\n\\frac{\\sigma_{\\theta}(S^{\\mathsf{LU}}_{\\Theta})}{\\sigma_{\\theta}(S_{\\theta}^g)}\n\\ge\n\\frac{\\sigma_{\\theta}(S^{\\mathsf{LU}}_{\\Theta})}{\\sigma_{\\theta}(S^{*}_{\\theta})}\n\\ge\n\\min_{\\theta' \\in \\Theta} \\frac{\\sigma_{\\theta'}(S^{\\mathsf{LU}}_{\\Theta})}{\\sigma_{\\theta'}(S^{*}_{\\theta'})}\n=\ng(\\Theta, S^{\\mathsf{LU}}_{\\Theta}) \\mbox{.}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\bar{\\alpha}(\\Theta,\\theta)=\\frac{\\sigma_{\\theta}(S^{\\mathsf{LU}}_{\\Theta})}{%&#10;\\sigma_{\\theta}(S_{\\theta}^{g})}\\geq\\frac{\\sigma_{\\theta}(S^{\\mathsf{LU}}_{%&#10;\\Theta})}{\\sigma_{\\theta}(S^{*}_{\\theta})}\\geq\\min_{\\theta^{\\prime}\\in\\Theta}%&#10;\\frac{\\sigma_{\\theta^{\\prime}}(S^{\\mathsf{LU}}_{\\Theta})}{\\sigma_{\\theta^{%&#10;\\prime}}(S^{*}_{\\theta^{\\prime}})}=g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})\\mbox{.}\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\u03b1</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>\u03b8</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2265</mo><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2265</mo><mrow><munder><mi>min</mi><mrow><msup><mi>\u03b8</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>\u2032</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>\u2032</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>=</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\n\n\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-2}): }\nConsider graph $G=(V,E)$ such that $V=A\\cup B, |A|=|B|=\\frac{n}{2}$ and $E=\\{(u,v)\\mid u,v\\in A \\text{ or }u,v\\in B\\}$, and let $E(A)$ be the set of edges with two endpoints in $A$ and $E(B)$ defined similarly. The problem is to find a single seed ($k=1$) such that the influence spread is maximized. Let $p=\\frac{2}{n}$ and the input instance is $l_e=p-\\epsilon$ and $r_e=p+\\epsilon$ for every edge $e$ such that $[l_e,r_e]$ covers the critical interval of Erd\\H{o}s-R\\'{e}nyi random graph with $\\frac{n}{2}$ nodes.\n\nNow since every node is seemingly the same for any algorithm, suppose the algorithm chooses a seed $u\\in A$,\nthen consider the worst-case $\\theta$ where for every $e\\in E(A)$, $p_e=l_e$ and for every $e\\in E(B)$, $p_e=r_e$.\nIt can be figured out that the optimal solution is an arbitrary node $v\\in B$.\nSince $\\sigma_{\\theta}(\\{u\\}) = O(\\log n)$ and $\\sigma_{\\theta}(\\{v\\})=\\Theta(n)$, then the ratio $r = O(\\frac{\\log n}{n})$.\n\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-3}): }\nConsider graph $G=(V,E)$ such that\n\t$V$ is composed of disjoint sets $A_1, A_2,\\ldots, A_{\\sqrt{n}}$ where each $|A_i|=\\sqrt{n}$,\nand\n\t$E=\\{(u,v)\\mid u,v\\in A_i, \\forall i=1,\\cdots,\\sqrt{n}\\}$.\nLet $E(A_i)$ be the set of edges with two endpoints in $A_i$.\nThe problem is to find a single seed ($k=1$) such that the influence spread is maximized.\nLet $p=\\frac{1}{\\sqrt{n}}$, and the input instance is $l_e=p-\\epsilon$ and $r_e=p+\\epsilon$ for every edge $e$\nsuch that $[l_e,r_e]$ covers the critical interval of Erd\\H{o}s-R\\'{e}nyi random graph with $\\sqrt{n}$ nodes.\nNow every node appears to be symmetric from the input.\n\nDenote $q_{i}$ as the probability of choosing a node in $A_{i}$.\nConsider any distribution assigned on\n$A_{1}, A_{2},\\ldots,A_{\\sqrt{n}}$, i.e. $q_1+q_2+\\cdots+q_{\\sqrt{n}}=1$,\nand let the random seed set be $\\tilde{S}$.\nWithout loss of generality, let $q_1$ be the smallest one. Then consider the worst-case $\\theta$ where for every $e\\in E(A_1)$, $p_e=r_e$ and for every $e\\in E(A_i),i\\ge 2$, $p_e=l_e$. It is obvious that the optimal solution $S_{\\theta}^{*}$ is an arbitrary point $v\\in A_1$.\nSince\n", "itemtype": "equation", "pos": 70966, "prevtext": "\n\n\n\n\nThe next question is how to find a $\\theta=(\\theta_e)_{e\\in E}\\in \\Theta$\n\nto make the upper bound\n$\\bar{\\alpha}(\\Theta,\\theta)$ as small as possible.\nIn our experiments, we use the following two heuristics and take their minimum.\n\nThe first heuristic borrows the intuition from Example~\\ref{exp:tight}, which\nsays that the gap ratio $\\alpha(\\Theta)$ is close to the robust ratio\n$g(\\Theta, S^{\\mathsf{LU}}_{\\Theta})$ when (a) there are two disjoint seed sets with\nsimilar influence spead, (b) their cascade overlap is small, and\n(c) the reachable edges from one seed set use lower end parameters values while the reachable edges from the other seed set use upper end parameters.\nThus in our heuristic, we use PMIA algorithm to find another seed set $S'$\nof $k$ nodes\nwhen we remove all nodes in $S^{\\mathsf{LU}}_{\\Theta}$.\nWe then do information cascades from both $S^{\\mathsf{LU}}_{\\Theta}$ and $S'$ for an\nequal number of times.\nFinally, for every edge $e$, if it is sampled more in the information cascade with seed set $S^{\\mathsf{LU}}_{\\Theta}$ than with $S'$, we set $\\theta_e=l_e$, otherwise we set $\\theta_e=r_e$.\nThe second heuristic is a variant of the first one, where we run a number of\ninformation cascades from $S^{\\mathsf{LU}}_{\\Theta}$, and for any edge $e$\nthat is sampled in at least $10\\%$ of cascades, we set $\\theta_e=l_e$,\notherwise we set $\\theta_e=r_e$.\n\nOther more sophisticated heuristics are possible, but it could be a separate\nresearch topic to find tighter upper bound for the robust ratio, and thus\nwe only use the simple combination of the above two in this\npaper, which is already indicative.\nWe henceforth use $\\bar{\\alpha}(\\Theta)$ to represent\nthe upper bound found by the minimum of the above two heuristics.\n\n\\subsection{Results}\n\n\\subsubsection{The Value $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\twith Predetermined Parameter Space}\n\nIn the first experiment we explore the relationship between\nthe width of confidence interval $\\Theta=\\times_{e\\in E}[l_e,r_e]$ and $\\alpha(\\Theta)$ together\nwith $\\bar{\\alpha}(\\Theta)$.\nFor a given interval width $W$,\nwe set $l_e=\\min\\{p_e-\\frac{W}{2},0\\},r_e=\\max\\{p_e+\\frac{W}{2},1\\}$\n$\\forall e\\in E$, where $p_e$ is the ground-truth probability of $e$.\nThen we calculate $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nWe vary the width $W$ to see the trend of changes of\n$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nFigure~\\ref{fig1} reports the result on the three graphs with seed set size $k=50$.\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{AR.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different widths of confidence interval $W$.}\n\t\\label{fig1}\n\\end{figure}\n\nFirst, we observe that as the parameter space $\\Theta$ becomes wider,\nthe value of both $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ become smaller,\nwhich matches our intuition that larger uncertainty results in worse\nrobustness.\nSecond, there is a sharp decrease of $\\alpha(\\Theta)$\nbetween $W\\in [0,0.1]$ and a much slower decrease afterwards\nfor all three graphs.\nThe decrease of $\\bar{\\alpha}(\\Theta)$ is not as sharp as that of $\\alpha(\\Theta)$\nbut the decrease also slows down with larger $W$ after $0.2$.\nThe overall trend of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\nsuggests that the robust ratio may be sensitive\nto the uncertainty of the parameter space, and only when the uncertainty of\nthe parameter space reduces to a certain level that we can obtain reasonable\nguarantee on the robustness of our solution.\n\n\nAs a comparison, we know that the average number\nof samples on each edge is $318$ for the learned probabilities in the\nFlixster dataset.\nThis corresponds to an average interval width of\n0.293 for topic 8 and 0.265 for the mixed topic.\nAt these interval widths, $\\alpha(\\Theta)$ values are approximately\n$0.04$ and $0.08$ respectively for the two graphs, and\n$\\bar{\\alpha}(\\Theta)$ are approximately $0.12$ and $0.2$ respectively.\nThis means that, even considering the upper bound  $\\bar{\\alpha}(\\Theta)$,\nthe robust ratio is pretty low, and thus the learned probabilities\nreported in~\\cite{barbieri2013topic} may result in quite poor performance for\nrobust influence maximization.\n\nOf course, our result of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ is only\ntargeted at the robustness of our {{\\sf LUGreedy}} algorithm, and there could\nexist better algorithm having higher robustness performance at the same\nuncertainty level. \nFinding a better RIM algorithm seems to be a difficult task, and\nwe hope that our study could motivate more research in searching for such better\nRIM algorithms.\nBesides $S^{\\mathsf{LU}}_{\\Theta}$, we also independently test the classical\ngreedy seed set $S^g_{\\theta}$ for $\\theta=(p_e)_{e\\in E}$\non the lower parameter vector $\\theta^-$\n(that is $\\frac{\\sigma_{\\theta^-}(S^g_{\\theta})}{\\sigma_{\\theta^+}(S^g_{\\theta^+})}$ versus $\\alpha(\\Theta)$),\nand the average performance on each data point is $2.45\\%$, $1.05\\%$, $6.11\\%$ worse than $S^{\\mathsf{LU}}_{\\Theta}$\nfor Flixster(Mixed), Flixster(Topic~8) and NetHEPT, respectively.\nTherefore, it shows that $S^{\\mathsf{LU}}_{\\Theta}$ outperforms $\\sigma^g_{\\theta}$ in the worse-case scenario, \nand henceforth we only use $S^{\\mathsf{LU}}_{\\Theta}$ in the following experiments.\n\n\n\n\n\n\\subsubsection{Results for Sampling algorithms}\n\nFigures~\\ref{fig2}, \\ref{fig3} and \\ref{fig4} reports the result of\n$\\alpha=\\alpha(\\Theta)$ and $\\bar{\\alpha}=\\bar{\\alpha}(\\Theta)$ \nfor the three tested graphs respectively, \nwhen the average number of samples per edge increases. \nFor better result presentation, we trim all figures as long as \n$\\alpha(\\text{{\\sf {US-RIM}}}) = 0.7$. \n(For example, in Flixster(Topic 8), \n{{\\sf {US-RIM}}} requires $77318$ samples in average for $\\alpha$ to reach $0.8$, \nwhile {{\\sf ICS-RIM}} only needs $33033$, and for {{\\sf OES-RIM}} $\\alpha$ sticks to $0.118$.)\n\nFor the sampling algorithms, after the $i$-{th} iteration, the observation\nset is updated from $\\mathcal{M}_{i-1}$ to $\\mathcal{M}_i$,\nand the average number of samples per edge in the network is calculated.\nMarkers on each curve in these figures represent the result after one\niteration of the corresponding sampling algorithm.\t\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{NetHEPT.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph NetHEPT.}\n\t\\label{fig2}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{Flixster8.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph Flixster(Topic~8).}\n\t\\label{fig3}\n\\end{figure}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.45]{FlixsterMix.pdf}\n\t\\caption{$\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$\n\t\tfor different average number of samples per edge on graph Flixster(Mixed).}\n\t\\label{fig4}\n\\end{figure}\n\nThe results on all three graphs are consistent.\nFirst, for each pair of $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$, even though\nthere is still some gap between the two, indicating either the lower bound\nor the upper bound may not be tight yet, the trends on\nboth $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$ are consistent:\nBoth increase with the number of samples, even with similar slopes at each\npoint; and among different\nalgorithms, the ranking order and relative change are consistent\nwith both $\\alpha(\\Theta)$ and $\\bar{\\alpha}(\\Theta)$.\nAll these consistency suggests that gap ratio $\\alpha(\\Theta)$ could be used\nas an indicator for the robustness of the {{\\sf LUGreedy}} algorithm, and\nit is reasonable to use $\\alpha(\\Theta)$ in comparing the performance\nof different algorithms.\n\nSecond, comparing the performance of three algorithms, we see that\nboth {{\\sf {US-RIM}}} and {{\\sf ICS-RIM}} are helpful in improving the robust ratio of\nthe selected seed set, and {{\\sf ICS-RIM}} is better than {{\\sf {US-RIM}}}, especially\nwhen the sample size increases.\nThe baseline algorithm {{\\sf OES-RIM}}, however, performs significantly poorer\nthan the other two, even though it is also an adaptive algorithm as\n{{\\sf ICS-RIM}}.\nThe reason is that, the lower-upper greedy set $S^{{\\mathsf{LU}}}_{\\Theta}$ changes little\nafter a certain number of iterations in {{\\sf OES-RIM}},\nand thus only a small number of edges (out edges of $S^{{\\mathsf{LU}}}_{\\Theta}$)\nare repeatedly sampled.\nThe probabilities on these edges are already estimated very accurately\nwhile other edge probabilities are far from accurate.\nIt is the inaccurate edges that make $\\alpha(\\Theta)$ and the best robust ratio small.\nIn contrast, {{\\sf ICS-RIM}} uses information cascades to sample\nnot only edges directly connecting to the seed set but also edges\nthat can be potentially reached.\nThis suggests that it is important for a sampling method to balance the\nsampling between critical edges and other potentially useful edges\nin order to achieve better robustness in influence maximization.\n\nOverall, the results suggest that information cascade based sampling method\nstands out as a competitive choice to use when we can adaptively sample more\nedges to achieve better robustness.\nIf adaptive sampling is not possible, then predetermined uniform sampling\nmay also perform reasonably well.\n\n\\section{Conclusion}  \\label{sect:conclusion}\n\nIn this paper, we propose the study of robust influence maximization to address\nthe impact of uncertainty in edge probability estimates that would inevitably occur\nin practice to the influence maximization task.\nWe propose the {{\\sf LUGreedy}} algorithm with a proven solution-dependent bound,\nand further propose sampling methods, in particular information cascade\nbased adaptive sample method to effectively reduce the uncertainty and\nincrease the robustness of the {{\\sf LUGreedy}} algorithm.\nThe experimental results validate the usefulness of the {{\\sf LUGreedy}} algorithm\nand the information cascade based sampling method {{\\sf ICS-RIM}}.\nMoreover, the results indicate that robustness may be sensitive to the uncertainty\nof parameter space, and learning algorithms may need more data to achieve\naccurate learning results for robust influence maximization.\n\nOur work opens up a number of research directions.\nFirst, it is unclear what could be the upper bound of the best robust ratio given\nan actual network and learned parameter space.\nAnswering this question would help us to understand whether robust\ninfluence maximization is intrinsically\ndifficult for a particular network or it is just our algorithm that does not\nperform well.\nIf it is the latter case, then an important direction is to\ndesign better robust influence maximization algorithms.\nAnother direction is how to improve sampling methods and learning methods\nto achieve more accurate parameter learning, which seems to be crucial\nfor robust influence maximization.\nIn summary, our work indicates a big data challenge on social influence research\n--- the data on social influence analysis is still not big enough,\nsuch that the uncertainty level in model learning may result in\npoor performance for influence maximization.\nWe hope that our work could encourage further researches to meet this\nchallenge from multiple aspects including\ndata collection, data analysis, and algorithm design.\n\n\\bibliographystyle{abbrv}\n\\bibliography{sigproc}\n\n\\appendix\n\n\n\\section{Proof of Theorem~3} \n\n\n\\begin{proof}\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-1}): }\nLet $G$ be an $n$-clique and $\\Theta = \\times_{e\\in E} [0, 1]$, i.e., for every edge $e$, $l_e=0$ and $r_e=1$.\nFor arbitrary set $S=\\{v_1,\\cdots, v_k\\}$, there exists a valid parameter vector $\\theta = (p_e)_{e \\in E} \\in \\Theta$, where $p_e=0$ for all $E_S=\\{e=(u,v)\\mid u\\in S \\text{ or } v\\in S\\}$ and $p_e=1$ for all $e\\notin E_S$.\nThen, $\\sigma_{\\theta}(S)=k$ and $\\sigma_{\\theta}(S^{*}_{\\theta})=n-1$,\nwhich implies that\n$g(\\Theta, S) =\n\t\\min_{\\theta \\in \\Theta} \\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S^{*}_{\\theta})}\n\t\\leq \\frac{k}{n-1}\n$.\nFor any set $S$ of size $k$, the above holds, thus we can conclude that\n", "index": 43, "text": "\n\\[\n\t\\max_{|S|=k}g(\\Theta,S) = O\\left(\\frac{k}{n}\\right) \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\max_{|S|=k}g(\\Theta,S)=O\\left(\\frac{k}{n}\\right)\\mbox{.}\" display=\"block\"><mrow><mrow><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mfrac><mi>k</mi><mi>n</mi></mfrac><mo>)</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nand\n", "itemtype": "equation", "pos": 73206, "prevtext": "\n\n\n\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-2}): }\nConsider graph $G=(V,E)$ such that $V=A\\cup B, |A|=|B|=\\frac{n}{2}$ and $E=\\{(u,v)\\mid u,v\\in A \\text{ or }u,v\\in B\\}$, and let $E(A)$ be the set of edges with two endpoints in $A$ and $E(B)$ defined similarly. The problem is to find a single seed ($k=1$) such that the influence spread is maximized. Let $p=\\frac{2}{n}$ and the input instance is $l_e=p-\\epsilon$ and $r_e=p+\\epsilon$ for every edge $e$ such that $[l_e,r_e]$ covers the critical interval of Erd\\H{o}s-R\\'{e}nyi random graph with $\\frac{n}{2}$ nodes.\n\nNow since every node is seemingly the same for any algorithm, suppose the algorithm chooses a seed $u\\in A$,\nthen consider the worst-case $\\theta$ where for every $e\\in E(A)$, $p_e=l_e$ and for every $e\\in E(B)$, $p_e=r_e$.\nIt can be figured out that the optimal solution is an arbitrary node $v\\in B$.\nSince $\\sigma_{\\theta}(\\{u\\}) = O(\\log n)$ and $\\sigma_{\\theta}(\\{v\\})=\\Theta(n)$, then the ratio $r = O(\\frac{\\log n}{n})$.\n\n\n{\\flushleft \\em (Case~\\ref{pro:3-ratios-3}): }\nConsider graph $G=(V,E)$ such that\n\t$V$ is composed of disjoint sets $A_1, A_2,\\ldots, A_{\\sqrt{n}}$ where each $|A_i|=\\sqrt{n}$,\nand\n\t$E=\\{(u,v)\\mid u,v\\in A_i, \\forall i=1,\\cdots,\\sqrt{n}\\}$.\nLet $E(A_i)$ be the set of edges with two endpoints in $A_i$.\nThe problem is to find a single seed ($k=1$) such that the influence spread is maximized.\nLet $p=\\frac{1}{\\sqrt{n}}$, and the input instance is $l_e=p-\\epsilon$ and $r_e=p+\\epsilon$ for every edge $e$\nsuch that $[l_e,r_e]$ covers the critical interval of Erd\\H{o}s-R\\'{e}nyi random graph with $\\sqrt{n}$ nodes.\nNow every node appears to be symmetric from the input.\n\nDenote $q_{i}$ as the probability of choosing a node in $A_{i}$.\nConsider any distribution assigned on\n$A_{1}, A_{2},\\ldots,A_{\\sqrt{n}}$, i.e. $q_1+q_2+\\cdots+q_{\\sqrt{n}}=1$,\nand let the random seed set be $\\tilde{S}$.\nWithout loss of generality, let $q_1$ be the smallest one. Then consider the worst-case $\\theta$ where for every $e\\in E(A_1)$, $p_e=r_e$ and for every $e\\in E(A_i),i\\ge 2$, $p_e=l_e$. It is obvious that the optimal solution $S_{\\theta}^{*}$ is an arbitrary point $v\\in A_1$.\nSince\n", "index": 45, "text": "\n\\[\n\t{\\mathbb{E}}\\left[ \\sigma_{\\theta}(\\tilde{S})\\right] \\le \\frac{1}{\\sqrt{n}} \\cdot \\sqrt{n} + \\left( 1-\\frac{1}{\\sqrt{n}} \\right) O(\\log \\sqrt{n})=O(\\log n) \\mbox{,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"{\\mathbb{E}}\\left[\\sigma_{\\theta}(\\tilde{S})\\right]\\leq\\frac{1}{\\sqrt{n}}\\cdot%&#10;\\sqrt{n}+\\left(1-\\frac{1}{\\sqrt{n}}\\right)O(\\log\\sqrt{n})=O(\\log n)\\mbox{,}\" display=\"block\"><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>S</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mrow><mfrac><mn>1</mn><msqrt><mi>n</mi></msqrt></mfrac><mo>\u22c5</mo><msqrt><mi>n</mi></msqrt></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><msqrt><mi>n</mi></msqrt></mfrac></mrow><mo>)</mo></mrow><mo>\u2062</mo><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>log</mi><mo>\u2061</mo><msqrt><mi>n</mi></msqrt></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>n</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\n\n\n\\section{Proof of Lemmas}\n\n\\begin{proof}[(Proof of Lemma~\\ref{lem:mul})]\n\n\nSince when $\\sigma_{\\theta}(S)$ is regarded as a function on $\\theta$ (if $S$ is fixed), it is monotonically increasing,\nthus it suffices to consider the case that $\\forall e\\in E$, $r_e=(1+\\lambda)l_e$.\n\n\n\n\n\n\n\n\nFlipping coins for every edge according to the probability parameter $\\theta$, and we have a live-edge (random) graph $L$.\nLet $E(L)$ denote the set of edges in $L$, and $\\Pr_{\\theta}[L]$ be the probability yielding $L$.\nWe use $R_L(S)$ to denote the reachable set from $S$ in $L$. Then, the influence spread function has a linear form as follows,\n", "itemtype": "equation", "pos": 73382, "prevtext": "\nand\n", "index": 47, "text": "\n\\[\n\t\\sigma_{\\theta}(S_{\\theta}^{*})=\\Theta(\\sqrt{n}) \\mbox{,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{\\theta}(S_{\\theta}^{*})=\\Theta(\\sqrt{n})\\mbox{,}\" display=\"block\"><mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nAs a convention, for any edge $e\\in E$, we denote conditional probability\n$\\Pr_{\\theta}[L|e] = \\Pr_{\\theta}\\left[L|e\\in E(L)\\right]$,\nand\n$\\Pr_{\\theta}[L|\\bar{e}] = \\Pr_{\\theta}\\left[L|e\\notin E(L)\\right]$.\nThen, we have\n\n", "itemtype": "equation", "pos": 74126, "prevtext": "\nwhich completes the proof.\n\\end{proof}\n\n\n\n\n\n\\section{Proof of Lemmas}\n\n\\begin{proof}[(Proof of Lemma~\\ref{lem:mul})]\n\n\nSince when $\\sigma_{\\theta}(S)$ is regarded as a function on $\\theta$ (if $S$ is fixed), it is monotonically increasing,\nthus it suffices to consider the case that $\\forall e\\in E$, $r_e=(1+\\lambda)l_e$.\n\n\n\n\n\n\n\n\nFlipping coins for every edge according to the probability parameter $\\theta$, and we have a live-edge (random) graph $L$.\nLet $E(L)$ denote the set of edges in $L$, and $\\Pr_{\\theta}[L]$ be the probability yielding $L$.\nWe use $R_L(S)$ to denote the reachable set from $S$ in $L$. Then, the influence spread function has a linear form as follows,\n", "index": 49, "text": "\n\\[\n\\sigma_{\\theta}(S)=\\sum_L\\Pr_{\\theta}[L]\\cdot|R_L(S)| \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{\\theta}(S)=\\sum_{L}\\Pr_{\\theta}[L]\\cdot|R_{L}(S)|\\mbox{.}\" display=\"block\"><mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>L</mi></munder><mrow><mrow><mrow><munder><mi>Pr</mi><mi>\u03b8</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u22c5</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\n\nWhen we fixed $l_{e'}$ for all $e'\\not=e$, we have\n", "itemtype": "equation", "pos": 74418, "prevtext": "\n\nAs a convention, for any edge $e\\in E$, we denote conditional probability\n$\\Pr_{\\theta}[L|e] = \\Pr_{\\theta}\\left[L|e\\in E(L)\\right]$,\nand\n$\\Pr_{\\theta}[L|\\bar{e}] = \\Pr_{\\theta}\\left[L|e\\notin E(L)\\right]$.\nThen, we have\n\n", "index": 51, "text": "\\begin{align*}\n&\\frac{\\sigma_{\\theta^+}(S)}{\\sigma_{\\theta^-}(S)} \\\\\n=&\\frac{\\displaystyle\\sum_{L:e\\in E(L)}r_e |R_L(S)| \\Pr_{\\theta^{+}}{[L|e]} + \\sum_{L:e\\not\\in E(L)}(1 - r_e) |R_L(S)|\\Pr_{\\theta^{+}}{[L|\\bar{e}]}}\n\t{\\displaystyle\\sum_{L:e\\in E(L)}l_e |R_L(S)| \\Pr_{\\theta^{-}}{[L|e]} + \\sum_{L:e\\not\\in E(L)}(1 - l_e) |R_L(S)| \\Pr_{\\theta^{-}}{[L|\\bar{e}]}} \\mbox{.}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\sigma_{\\theta^{+}}(S)}{\\sigma_{\\theta^{-}}(S)}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\displaystyle\\sum_{L:e\\in E(L)}r_{e}|R_{L}(S)|\\Pr_{\\theta^{%&#10;+}}{[L|e]}+\\sum_{L:e\\not\\in E(L)}(1-r_{e})|R_{L}(S)|\\Pr_{\\theta^{+}}{[L|\\bar{e%&#10;}]}}{\\displaystyle\\sum_{L:e\\in E(L)}l_{e}|R_{L}(S)|\\Pr_{\\theta^{-}}{[L|e]}+%&#10;\\sum_{L:e\\not\\in E(L)}(1-l_{e})|R_{L}(S)|\\Pr_{\\theta^{-}}{[L|\\bar{e}]}}\\mbox{.}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>L</mi><mo>:</mo><mrow><mi>e</mi><mo>\u2208</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></munder></mstyle><mrow><msub><mi>r</mi><mi>e</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">|</mo><mi>e</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>L</mi><mo>:</mo><mrow><mi>e</mi><mo>\u2209</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></munder></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi>e</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow></mrow><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>L</mi><mo>:</mo><mrow><mi>e</mi><mo>\u2208</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></munder></mstyle><mrow><msub><mi>l</mi><mi>e</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">|</mo><mi>e</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>L</mi><mo>:</mo><mrow><mi>e</mi><mo>\u2209</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>L</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></munder></mstyle><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>l</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>R</mi><mi>L</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo stretchy=\"false\">|</mo><mover accent=\"true\"><mi>e</mi><mo stretchy=\"false\">\u00af</mo></mover><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>.</mtext></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhere $A,B,C,D$ are not dependent on $l_e$. It can be observed that the ratio is monotone with $l_e$, and is thus maximized either when $l_e=0$ or when $l_e=\\frac{1}{1+\\lambda}$.\n\nSimilar analysis for other edges, we can conclude that when the ratio is maximized, it must holds that $\\forall e\\in E$, $l_e=0$ or $l_e=\\frac{1}{1+\\lambda}$.\nSince when $l_e=0$, it holds that $r_e=0$, thus we can just delete this edge from the graph.\nDelete all such edges, and it ends up with a graph $G_1=(V,E_1)$ such that the probability interval on every edge is $[\\frac{1}{1+\\lambda},1]$.\nAnd it can be seen that $R_{G_1}(S)$ is determined when probability on all edges are $1$.\n\n\n\nGiven set $S$, denote the influence spread for any graph $G$ under any parameter vector $\\theta$ as $\\sigma_{\\theta}^{G}(S)$ explicitly.\nIf there exists a directed cycle $v_0\\to v_1\\to\\cdots \\to v_i\\to v_0$ in graph $G_1$. \nThen it can be seen that either all nodes in this cycle is in $R_{G_1}(S)$, or none of them is in. \nIn both cases, we can remove some edge (e.g. $v_i\\to v_0$) from $E_1$ and obtain a new graph $G_2$ (e.g. $G_2=(V,E_1 \\setminus \\{(v_i,v_0)\\})$) such that $\\sigma_{\\theta^+}^{G_1}(S)=\\sigma_{\\theta^+}^{G_2}(S)$ while $\\sigma_{\\theta^-}^{G_1}(S)\\geq\\sigma_{\\theta^-}^{G_2}(S)$. Thus,\n", "itemtype": "equation", "pos": 74854, "prevtext": "\n\n\nWhen we fixed $l_{e'}$ for all $e'\\not=e$, we have\n", "index": 53, "text": "\n\\[\n\\frac{\\sigma_{\\theta^+}(S)}{\\sigma_{\\theta^-}(S)}=\\frac{Al_e+B}{Cl_e+D} \\mbox{,}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sigma_{\\theta^{+}}(S)}{\\sigma_{\\theta^{-}}(S)}=\\frac{Al_{e}+B}{Cl_{e}+D%&#10;}\\mbox{,}\" display=\"block\"><mrow><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>=</mo><mrow><mfrac><mrow><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>l</mi><mi>e</mi></msub></mrow><mo>+</mo><mi>B</mi></mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><msub><mi>l</mi><mi>e</mi></msub></mrow><mo>+</mo><mi>D</mi></mrow></mfrac><mo>\u2062</mo><mtext>,</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nRemoving can be done since if none of the nodes are in $R_{G_1}(S)$, then deleting one edge will not change either $\\sigma_{\\theta^+}^{G_1}(S)$ or $\\sigma_{\\theta^-}^{G_1}(S)$, and if all of the nodes are in, then there must exists $v_0$ in the cycle such that $v_0\\in S$ or $v_0$ can be reached from a path directing from some node (in $S$) outside the cycle to it, then deleting the edge $(v_p,v_0)$ can be proved to satisfy the above property.\n\nRepeat deleting edges until the remaining graph is a directed acyclic graph (DAG), denoted by $G'$. Then it can be split into finite subgraphs $T_1,T_2,\\ldots,T_j$\nwhere each $T_i$ is a connected DAG, and it is immediate that\n", "itemtype": "equation", "pos": 76216, "prevtext": "\nwhere $A,B,C,D$ are not dependent on $l_e$. It can be observed that the ratio is monotone with $l_e$, and is thus maximized either when $l_e=0$ or when $l_e=\\frac{1}{1+\\lambda}$.\n\nSimilar analysis for other edges, we can conclude that when the ratio is maximized, it must holds that $\\forall e\\in E$, $l_e=0$ or $l_e=\\frac{1}{1+\\lambda}$.\nSince when $l_e=0$, it holds that $r_e=0$, thus we can just delete this edge from the graph.\nDelete all such edges, and it ends up with a graph $G_1=(V,E_1)$ such that the probability interval on every edge is $[\\frac{1}{1+\\lambda},1]$.\nAnd it can be seen that $R_{G_1}(S)$ is determined when probability on all edges are $1$.\n\n\n\nGiven set $S$, denote the influence spread for any graph $G$ under any parameter vector $\\theta$ as $\\sigma_{\\theta}^{G}(S)$ explicitly.\nIf there exists a directed cycle $v_0\\to v_1\\to\\cdots \\to v_i\\to v_0$ in graph $G_1$. \nThen it can be seen that either all nodes in this cycle is in $R_{G_1}(S)$, or none of them is in. \nIn both cases, we can remove some edge (e.g. $v_i\\to v_0$) from $E_1$ and obtain a new graph $G_2$ (e.g. $G_2=(V,E_1 \\setminus \\{(v_i,v_0)\\})$) such that $\\sigma_{\\theta^+}^{G_1}(S)=\\sigma_{\\theta^+}^{G_2}(S)$ while $\\sigma_{\\theta^-}^{G_1}(S)\\geq\\sigma_{\\theta^-}^{G_2}(S)$. Thus,\n", "index": 55, "text": "\n\\[\n\\frac{\\sigma_{\\theta^+}^{G_1}(S)}{\\sigma_{\\theta^-}^{G_1}(S)}\\leq \\frac{\\sigma_{\\theta^+}^{G_2}(S)}{\\sigma_{\\theta^-}^{G_2}(S)} \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sigma_{\\theta^{+}}^{G_{1}}(S)}{\\sigma_{\\theta^{-}}^{G_{1}}(S)}\\leq\\frac%&#10;{\\sigma_{\\theta^{+}}^{G_{2}}(S)}{\\sigma_{\\theta^{-}}^{G_{2}}(S)}\\mbox{.}\" display=\"block\"><mrow><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msub><mi>G</mi><mn>1</mn></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msub><mi>G</mi><mn>1</mn></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mrow><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msub><mi>G</mi><mn>2</mn></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msub><mi>G</mi><mn>2</mn></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nIt remains to analyze the ratio in a connected DAG $T_i$, and we need more notations before that. First, the DAG $T_i$ naturally induces a topological order on nodes (we can therefore call the nodes in $T_i$ be $V(T_i):=\\{v_1,\\cdots,v_{|T_i|}\\}$), in which every edge in $E(T_i)$ is directing from a node with smaller order to a larger order. Let $S_i=S\\cap V(T_i)$, and let $R(S_i)$ be the subset of nodes in $V(T_i)$ that is reachable with positive probability (therefore $R(S_i)$ naturally contains nodes in $S_i$). Besides, for any $v\\notin S_i$, let $d(S_i,v)$ denotes the length of shortest path directing from some node in $S_i$, and for any $v\\in S_i$, define $d(S_i,v)=0$. Thus,\n", "itemtype": "equation", "pos": 77034, "prevtext": "\n\nRemoving can be done since if none of the nodes are in $R_{G_1}(S)$, then deleting one edge will not change either $\\sigma_{\\theta^+}^{G_1}(S)$ or $\\sigma_{\\theta^-}^{G_1}(S)$, and if all of the nodes are in, then there must exists $v_0$ in the cycle such that $v_0\\in S$ or $v_0$ can be reached from a path directing from some node (in $S$) outside the cycle to it, then deleting the edge $(v_p,v_0)$ can be proved to satisfy the above property.\n\nRepeat deleting edges until the remaining graph is a directed acyclic graph (DAG), denoted by $G'$. Then it can be split into finite subgraphs $T_1,T_2,\\ldots,T_j$\nwhere each $T_i$ is a connected DAG, and it is immediate that\n", "index": 57, "text": "\n\\[\n\\frac{\\sigma_{\\theta^+}^{G'}(S)}{\\sigma_{\\theta^-}^{G'}(S)}\\le \\max_{1\\le i\\le j}\\frac{\\sigma_{\\theta^+}^{T_i}(S)}{\\sigma_{\\theta^-}^{T_i}(S)} \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sigma_{\\theta^{+}}^{G^{\\prime}}(S)}{\\sigma_{\\theta^{-}}^{G^{\\prime}}(S)%&#10;}\\leq\\max_{1\\leq i\\leq j}\\frac{\\sigma_{\\theta^{+}}^{T_{i}}(S)}{\\sigma_{\\theta^%&#10;{-}}^{T_{i}}(S)}\\mbox{.}\" display=\"block\"><mrow><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msup><mi>G</mi><mo>\u2032</mo></msup></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msup><mi>G</mi><mo>\u2032</mo></msup></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi><mo>\u2264</mo><mi>j</mi></mrow></munder><mo>\u2061</mo><mrow><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nLet $\\beta=\\frac{1}{1+\\lambda}$. For any path of length $l \\geq 0$ from $S_i$ to $v$, the activating probability of\nthat path is $\\beta^l$ under $\\theta^-$. Then, we have\n", "itemtype": "equation", "pos": 77881, "prevtext": "\n\nIt remains to analyze the ratio in a connected DAG $T_i$, and we need more notations before that. First, the DAG $T_i$ naturally induces a topological order on nodes (we can therefore call the nodes in $T_i$ be $V(T_i):=\\{v_1,\\cdots,v_{|T_i|}\\}$), in which every edge in $E(T_i)$ is directing from a node with smaller order to a larger order. Let $S_i=S\\cap V(T_i)$, and let $R(S_i)$ be the subset of nodes in $V(T_i)$ that is reachable with positive probability (therefore $R(S_i)$ naturally contains nodes in $S_i$). Besides, for any $v\\notin S_i$, let $d(S_i,v)$ denotes the length of shortest path directing from some node in $S_i$, and for any $v\\in S_i$, define $d(S_i,v)=0$. Thus,\n", "index": 59, "text": "\n\\[\n\\sigma_{\\theta^+}^{T_i}(S_i)=|R(S_i)| \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"\\sigma_{\\theta^{+}}^{T_{i}}(S_{i})=|R(S_{i})|\\mbox{.}\" display=\"block\"><mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nTherefore,\n", "itemtype": "equation", "pos": 78106, "prevtext": "\n\nLet $\\beta=\\frac{1}{1+\\lambda}$. For any path of length $l \\geq 0$ from $S_i$ to $v$, the activating probability of\nthat path is $\\beta^l$ under $\\theta^-$. Then, we have\n", "index": 61, "text": "\n\\[\n\\begin{aligned}\n\\sigma_{\\theta^{-}}^{T_i}(S_i)&=|S_i|+\\sum_{v\\in V(T_i) \\setminus S_i}\\Pr\\left[\\text{v is reached}\\right]\\\\\n&\\geq |S_i|+\\sum_{v\\in R(S_i) \\setminus  S_i}\\beta^{d(S_i,v)}\\\\\n&\\geq \\sum_{v \\in R(S_i)} \\beta^{d(S_i,v)}\\\\\n&\\geq |R(S_i)| \\beta^{|T_i|} \\mbox{.}\n\\end{aligned}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sigma_{\\theta^{-}}^{T_{i}}(S_{i})\" display=\"inline\"><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=|S_{i}|+\\sum_{v\\in V(T_{i})\\setminus S_{i}}\\Pr\\left[\\text{v is %&#10;reached}\\right]\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mrow><mrow><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2216</mo><msub><mi>S</mi><mi>i</mi></msub></mrow></mrow></munder></mstyle><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mtext>v is reached</mtext><mo>]</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq|S_{i}|+\\sum_{v\\in R(S_{i})\\setminus S_{i}}\\beta^{d(S_{i},v)}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mrow><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2216</mo><msub><mi>S</mi><mi>i</mi></msub></mrow></mrow></munder></mstyle><msup><mi>\u03b2</mi><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\sum_{v\\in R(S_{i})}\\beta^{d(S_{i},v)}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder></mstyle><msup><mi>\u03b2</mi><mrow><mi>d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msup></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq|R(S_{i})|\\beta^{|T_{i}|}\\mbox{.}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>R</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><msup><mi>\u03b2</mi><mrow><mo stretchy=\"false\">|</mo><msub><mi>T</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></msup><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nTo prove the second inequality of this lemma, by definition we have\n\t$S_{\\theta}^{*}=\\arg\\max_{|S|\\leq k}\\sigma_{\\theta}(S)$.\nNote that for all $\\theta\\in\\Theta$,\n\t$\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})\\geq\\sigma_{\\theta}(S_{\\theta}^{*})$.\nThen we have\n\n", "itemtype": "equation", "pos": 78409, "prevtext": "\n\nTherefore,\n", "index": 63, "text": "\n\\[\n\\frac{\\sigma_{\\theta^+}^{G}(S)}{\\sigma_{\\theta^-}^{G}(S)}\n\\leq \\frac{\\sigma_{\\theta^+}^{G'}(S)}{\\sigma_{\\theta^-}^{G'}(S)}\n\\leq \\max_{1\\le i\\le p}\\frac{\\sigma_{\\theta^+}^{T_i}(S)}{\\sigma_{\\theta^-}^{T_i}(S)}\n\\leq \\max_{1\\le i\\le p} \\beta^{-|T_i|}\n\\leq (1+\\lambda)^{n} \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\sigma_{\\theta^{+}}^{G}(S)}{\\sigma_{\\theta^{-}}^{G}(S)}\\leq\\frac{\\sigma_%&#10;{\\theta^{+}}^{G^{\\prime}}(S)}{\\sigma_{\\theta^{-}}^{G^{\\prime}}(S)}\\leq\\max_{1%&#10;\\leq i\\leq p}\\frac{\\sigma_{\\theta^{+}}^{T_{i}}(S)}{\\sigma_{\\theta^{-}}^{T_{i}}%&#10;(S)}\\leq\\max_{1\\leq i\\leq p}\\beta^{-|T_{i}|}\\leq(1+\\lambda)^{n}\\mbox{.}\" display=\"block\"><mrow><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>G</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><mi>G</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msup><mi>G</mi><mo>\u2032</mo></msup></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msup><mi>G</mi><mo>\u2032</mo></msup></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2264</mo><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi><mo>\u2264</mo><mi>p</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><msub><mi>T</mi><mi>i</mi></msub></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>\u2264</mo><mrow><munder><mi>max</mi><mrow><mn>1</mn><mo>\u2264</mo><mi>i</mi><mo>\u2264</mo><mi>p</mi></mrow></munder><mo>\u2061</mo><msup><mi>\u03b2</mi><mrow><mo>-</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>T</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow></msup></mrow><mo>\u2264</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nThis completes the proof for Lemma~\\ref{lem:mul}.\n\n\\end{proof}\n\n\\begin{proof}[(Proof of Lemma~\\ref{lem:conf})]\n\nFirst, we focus on one fixed edge $e$. According to Chernoff bound, we have\n", "itemtype": "equation", "pos": 78949, "prevtext": "\n\nTo prove the second inequality of this lemma, by definition we have\n\t$S_{\\theta}^{*}=\\arg\\max_{|S|\\leq k}\\sigma_{\\theta}(S)$.\nNote that for all $\\theta\\in\\Theta$,\n\t$\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})\\geq\\sigma_{\\theta}(S_{\\theta}^{*})$.\nThen we have\n\n", "index": 65, "text": "\\begin{align*}\n\\max_{|S|\\leq k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta}(S_{\\theta}^{*})}&\\geq\n\\max_{|S|\\leq k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})}\\\\\n&=\\frac{\\sigma_{\\theta^{-}}(S_{\\theta^{-}}^{*})}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})}\\\\\n&\\geq \\frac{\\sigma_{\\theta^{-}}(S_{\\theta^{+}}^{*})}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})}\\\\\n&\\geq \\min_{|S|\\subseteq V}\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}\\\\\n&\\geq \\frac{1}{(1+\\lambda)^{n}} \\mbox{.}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\max_{|S|\\leq k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S)}{%&#10;\\sigma_{\\theta}(S_{\\theta}^{*})}\" display=\"inline\"><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi>\u03b8</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\max_{|S|\\leq k}\\min_{\\theta\\in\\Theta}\\frac{\\sigma_{\\theta}(S%&#10;)}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^{*})}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mrow><munder><mi>min</mi><mrow><mi>\u03b8</mi><mo>\u2208</mo><mi mathvariant=\"normal\">\u0398</mi></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\sigma_{\\theta^{-}}(S_{\\theta^{-}}^{*})}{\\sigma_{\\theta^{+%&#10;}}(S_{\\theta^{+}}^{*})}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>-</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\frac{\\sigma_{\\theta^{-}}(S_{\\theta^{+}}^{*})}{\\sigma_{\\theta%&#10;^{+}}(S_{\\theta^{+}}^{*})}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\min_{|S|\\subseteq V}\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{%&#10;\\theta^{+}}(S)}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><munder><mi>min</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2286</mo><mi>V</mi></mrow></munder><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\frac{1}{(1+\\lambda)^{n}}\\mbox{.}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>n</mi></msup></mfrac></mstyle><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nLet $\\gamma = 2me^{-\\frac{1}{3}\\delta^2p_et_e}$, and\n\t$\\delta = \\sqrt{\\frac{3}{t_e}\\ln\\frac{2m}{\\gamma}}\\frac{1}{\\sqrt{p_e}} = \\frac{c_e}{\\sqrt{p_e}}$.\nThen, with probability no less than $1 - \\frac{\\gamma}{m}$,\nwe see that $p_e$ should satisfy the constraint\n", "itemtype": "equation", "pos": 79686, "prevtext": "\n\nThis completes the proof for Lemma~\\ref{lem:mul}.\n\n\\end{proof}\n\n\\begin{proof}[(Proof of Lemma~\\ref{lem:conf})]\n\nFirst, we focus on one fixed edge $e$. According to Chernoff bound, we have\n", "index": 67, "text": "\n\\[ \n\t\\Pr[|\\hat{p}_e - p_e|\\le p_e\\delta] \\ge 1 - 2e^{-\\frac{1}{3}\\delta^2p_et_e}. \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\Pr[|\\hat{p}_{e}-p_{e}|\\leq p_{e}\\delta]\\geq 1-2e^{-\\frac{1}{3}\\delta^{2}p_{e}%&#10;t_{e}}.\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>-</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><msub><mi>p</mi><mi>e</mi></msub><mo>\u2062</mo><mi>\u03b4</mi></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup><mo>\u2062</mo><msub><mi>p</mi><mi>e</mi></msub><mo>\u2062</mo><msub><mi>t</mi><mi>e</mi></msub></mrow></mrow></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nthus we have\n", "itemtype": "equation", "pos": 80032, "prevtext": "\nLet $\\gamma = 2me^{-\\frac{1}{3}\\delta^2p_et_e}$, and\n\t$\\delta = \\sqrt{\\frac{3}{t_e}\\ln\\frac{2m}{\\gamma}}\\frac{1}{\\sqrt{p_e}} = \\frac{c_e}{\\sqrt{p_e}}$.\nThen, with probability no less than $1 - \\frac{\\gamma}{m}$,\nwe see that $p_e$ should satisfy the constraint\n", "index": 69, "text": "\n\\[ \n\t|\\hat{p}_e - p_e|\\le c_e\\sqrt{p_e}, \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"|\\hat{p}_{e}-p_{e}|\\leq c_{e}\\sqrt{p_{e}},\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>-</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><msub><mi>c</mi><mi>e</mi></msub><mo>\u2062</mo><msqrt><msub><mi>p</mi><mi>e</mi></msub></msqrt></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nBy definition of $l_e$, $r_e$ and the fact that $p_e \\in [0,1]$,\ntherefore we have\n", "itemtype": "equation", "pos": 80090, "prevtext": "\nthus we have\n", "index": 71, "text": "\n\\[ \n\t\\hat{p}_e + \\frac{c_e^2}{2} - c_e\\sqrt{\\frac{c_e^2}{4} + \\hat{p}_e}\\le p_e \\le \\hat{p}_e + \\frac{c_e^2}{2} + c_e\\sqrt{\\frac{c_e^2}{4} + \\hat{p}_e}. \n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"\\hat{p}_{e}+\\frac{c_{e}^{2}}{2}-c_{e}\\sqrt{\\frac{c_{e}^{2}}{4}+\\hat{p}_{e}}%&#10;\\leq p_{e}\\leq\\hat{p}_{e}+\\frac{c_{e}^{2}}{2}+c_{e}\\sqrt{\\frac{c_{e}^{2}}{4}+%&#10;\\hat{p}_{e}}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>+</mo><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>2</mn></mfrac></mrow><mo>-</mo><mrow><msub><mi>c</mi><mi>e</mi></msub><mo>\u2062</mo><msqrt><mrow><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>4</mn></mfrac><mo>+</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub></mrow></msqrt></mrow></mrow><mo>\u2264</mo><msub><mi>p</mi><mi>e</mi></msub><mo>\u2264</mo><mrow><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub><mo>+</mo><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>2</mn></mfrac><mo>+</mo><mrow><msub><mi>c</mi><mi>e</mi></msub><mo>\u2062</mo><msqrt><mrow><mfrac><msubsup><mi>c</mi><mi>e</mi><mn>2</mn></msubsup><mn>4</mn></mfrac><mo>+</mo><msub><mover accent=\"true\"><mi>p</mi><mo stretchy=\"false\">^</mo></mover><mi>e</mi></msub></mrow></msqrt></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nBy union bound, we can conclude that\n", "itemtype": "equation", "pos": 80330, "prevtext": "\nBy definition of $l_e$, $r_e$ and the fact that $p_e \\in [0,1]$,\ntherefore we have\n", "index": 73, "text": "\n\\[\n\t\\Pr[l_e \\le p_e \\le r_e] \\ge 1 - \\frac{\\gamma}{m}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex37.m1\" class=\"ltx_Math\" alttext=\"\\Pr[l_{e}\\leq p_{e}\\leq r_{e}]\\geq 1-\\frac{\\gamma}{m}.\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>l</mi><mi>e</mi></msub><mo>\u2264</mo><msub><mi>p</mi><mi>e</mi></msub><mo>\u2264</mo><msub><mi>r</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mi>\u03b3</mi><mi>m</mi></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhich completes the proof of Lemma~\\ref{lem:conf}.\n\\end{proof}\n\n\n\n\\section{Proof of Theorem~6} \n\n\n\\begin{proof}\n\n\n\n{\\flushleft \\em Setting~\\ref{thm-add-case}:}\nFirst, since every $e$ is probed for $t=\\frac{2m^2n^2 \\ln \\frac{2m}{\\gamma}}{k^2\\epsilon^2}$ times, using the additive form of Chernoff-Hoeffding Inequality we have\n", "itemtype": "equation", "pos": 80425, "prevtext": "\nBy union bound, we can conclude that\n", "index": 75, "text": "\n\\[\n\t\\Pr[l_e \\le p_e \\le r_e,\\, \\forall e\\in E] \\ge 1 - \\gamma,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex38.m1\" class=\"ltx_Math\" alttext=\"\\Pr[l_{e}\\leq p_{e}\\leq r_{e},\\,\\forall e\\in E]\\geq 1-\\gamma,\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>l</mi><mi>e</mi></msub><mo>\u2264</mo><msub><mi>p</mi><mi>e</mi></msub><mo>\u2264</mo><msub><mi>r</mi><mi>e</mi></msub></mrow><mo rspace=\"4.2pt\">,</mo><mrow><mrow><mo>\u2200</mo><mi>e</mi></mrow><mo>\u2208</mo><mi>E</mi></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03b3</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nThen by union bound, it holds that\n", "itemtype": "equation", "pos": 80816, "prevtext": "\nwhich completes the proof of Lemma~\\ref{lem:conf}.\n\\end{proof}\n\n\n\n\\section{Proof of Theorem~6} \n\n\n\\begin{proof}\n\n\n\n{\\flushleft \\em Setting~\\ref{thm-add-case}:}\nFirst, since every $e$ is probed for $t=\\frac{2m^2n^2 \\ln \\frac{2m}{\\gamma}}{k^2\\epsilon^2}$ times, using the additive form of Chernoff-Hoeffding Inequality we have\n", "index": 77, "text": "\n\\[\n\\Pr\\left[ \\left| {\\frac{1}{t}\\sum_{i=1}^{t}x^i_e-p_e} \\right| >\\frac{k\\epsilon}{2mn} \\right] \\le 2\\exp\\left(-\\frac{k^2\\epsilon^2}{2m^2n^2}\\cdot t\\right)\\le \\frac{\\gamma}{m} \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex39.m1\" class=\"ltx_Math\" alttext=\"\\Pr\\left[\\left|{\\frac{1}{t}\\sum_{i=1}^{t}x^{i}_{e}-p_{e}}\\right|&gt;\\frac{k%&#10;\\epsilon}{2mn}\\right]\\leq 2\\exp\\left(-\\frac{k^{2}\\epsilon^{2}}{2m^{2}n^{2}}%&#10;\\cdot t\\right)\\leq\\frac{\\gamma}{m}\\mbox{.}\" display=\"block\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mo>|</mo><mrow><mrow><mfrac><mn>1</mn><mi>t</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow></mrow><mo>-</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo>|</mo></mrow><mo>&gt;</mo><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mfrac><mrow><msup><mi>k</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>\u03f5</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>m</mi><mn>2</mn></msup><mo>\u2062</mo><msup><mi>n</mi><mn>2</mn></msup></mrow></mfrac><mo>\u22c5</mo><mi>t</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mfrac><mi>\u03b3</mi><mi>m</mi></mfrac><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nFor every $e\\in E$, we set $l_e=\\frac{1}{t}\\sum_{i=1}^{t}x^i_e-\\frac{k\\epsilon}{2mn}$, and $r_e=\\frac{1}{t}\\sum_{i=1}^{t}x^i_e+\\frac{k\\epsilon}{2mn}$, then with probability $\\ge 1-\\gamma$, it holds that $\\theta\\in \\Theta$.\n\nTherefore, for every $S$, according to Lemma~\\ref{lem:add},\n\n\n", "itemtype": "equation", "pos": 81040, "prevtext": "\n\nThen by union bound, it holds that\n", "index": 79, "text": "\n\\[\n\\Pr\\left[\\forall e\\in E, \\left| {\\frac{1}{t}\\sum_{i=1}^{t} x^i_e-p_e} \\right| >\\frac{k\\epsilon}{2mn}\\right]\\le \\gamma \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex40.m1\" class=\"ltx_Math\" alttext=\"\\Pr\\left[\\forall e\\in E,\\left|{\\frac{1}{t}\\sum_{i=1}^{t}x^{i}_{e}-p_{e}}\\right%&#10;|&gt;\\frac{k\\epsilon}{2mn}\\right]\\leq\\gamma\\mbox{.}\" display=\"block\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mo>\u2200</mo><mi>e</mi></mrow><mo>\u2208</mo><mi>E</mi></mrow><mo>,</mo><mrow><mrow><mo>|</mo><mrow><mrow><mfrac><mn>1</mn><mi>t</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow></mrow><mo>-</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo>|</mo></mrow><mo>&gt;</mo><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow></mfrac></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nThus, $\\frac{\\sigma_{\\theta^{-}}(S^g_{\\theta^+})}{\\sigma_{\\theta^{+}}(S^g_{\\theta^+})} \\geq 1-\\epsilon$ \nalso holds.\n\nNow, since we use $S^{\\mathsf{LU}}_\\Theta$ as the solution, applying \\Cref{thm:main}, we have\n\n", "itemtype": "equation", "pos": 81460, "prevtext": "\n\nFor every $e\\in E$, we set $l_e=\\frac{1}{t}\\sum_{i=1}^{t}x^i_e-\\frac{k\\epsilon}{2mn}$, and $r_e=\\frac{1}{t}\\sum_{i=1}^{t}x^i_e+\\frac{k\\epsilon}{2mn}$, then with probability $\\ge 1-\\gamma$, it holds that $\\theta\\in \\Theta$.\n\nTherefore, for every $S$, according to Lemma~\\ref{lem:add},\n\n\n", "index": 81, "text": "\\begin{align*}\n\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}&\n\\ge 1-\\frac{\\sigma_{\\theta^{+}}(S)-\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}\\\\\n&\\ge 1-\\frac{mn\\cdot\\frac{k\\epsilon}{mn}}{k}\\\\\n&=1-\\epsilon \\mbox{.}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex41.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex41.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq 1-\\frac{\\sigma_{\\theta^{+}}(S)-\\sigma_{\\theta^{-}}(S)}{%&#10;\\sigma_{\\theta^{+}}(S)}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex42.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq 1-\\frac{mn\\cdot\\frac{k\\epsilon}{mn}}{k}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow><mo>\u22c5</mo><mfrac><mrow><mi>k</mi><mo>\u2062</mo><mi>\u03f5</mi></mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>n</mi></mrow></mfrac></mrow><mi>k</mi></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex43.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=1-\\epsilon\\mbox{.}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nwhere the second inequality holds due to $\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta) \\ge \\sigma_{\\theta^{-}}(S^g_{\\theta^+})$\n by definition of \\eqref{def:lu-greedy-solution}.\n\n\n\n{\\flushleft \\em Setting~\\ref{thm-mul-case}:}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenote $a = \\frac{\\ln \\frac{1}{1-\\epsilon}}{2n + \\ln \\frac{1}{1-\\epsilon}}$ for convenience.\nSince every edge $e$ is probed for $t=\\frac{3 \\ln \\frac{2m}{\\gamma} }{p a^2} \\geq \\frac{3 \\ln \\frac{2m}{\\gamma} }{p_e a^2}$ times,\nthe probability of upper and lower tails derived by the multiplicative form of Chernoff-Hoeffding Inequality is\n\n", "itemtype": "equation", "pos": 81910, "prevtext": "\n\nThus, $\\frac{\\sigma_{\\theta^{-}}(S^g_{\\theta^+})}{\\sigma_{\\theta^{+}}(S^g_{\\theta^+})} \\geq 1-\\epsilon$ \nalso holds.\n\nNow, since we use $S^{\\mathsf{LU}}_\\Theta$ as the solution, applying \\Cref{thm:main}, we have\n\n", "index": 83, "text": "\\begin{align*}\ng(\\Theta,S^{\\mathsf{LU}}_\\Theta) \n\t\\ge \\alpha(\\Theta)\\left(1-\\frac{1}{e}\\right)\n\t= \\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta)}{\\sigma_{\\theta^{+}}(S_{\\theta^{+}}^g)} \\left(1-\\frac{1}{e}\\right) \\\\\n\t\\ge \\frac{\\sigma_{\\theta^{-}}(S^g_{\\theta^+})}{\\sigma_{\\theta^{+}}(S^g_{\\theta^+})}  \\left(1-\\frac{1}{e}\\right)\n\t\\ge \\left( 1-\\epsilon \\right) \\left(1-\\frac{1}{e}\\right),\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex44.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle g(\\Theta,S^{\\mathsf{LU}}_{\\Theta})\\geq\\alpha(\\Theta)\\left(1-%&#10;\\frac{1}{e}\\right)=\\frac{\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_{\\Theta})}{\\sigma%&#10;_{\\theta^{+}}(S_{\\theta^{+}}^{g})}\\left(1-\\frac{1}{e}\\right)\" display=\"inline\"><mrow><mrow><mi>g</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><mi mathvariant=\"normal\">\u0398</mi><mi>\ud835\uddab\ud835\uddb4</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex45.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\frac{\\sigma_{\\theta^{-}}(S^{g}_{\\theta^{+}})}{\\sigma_{\\theta%&#10;^{+}}(S^{g}_{\\theta^{+}})}\\left(1-\\frac{1}{e}\\right)\\geq\\left(1-\\epsilon\\right%&#10;)\\left(1-\\frac{1}{e}\\right),\" display=\"inline\"><mrow><mrow><mi/><mo>\u2265</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>S</mi><msup><mi>\u03b8</mi><mo>+</mo></msup><mi>g</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow><mo>)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>e</mi></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nThen by union bound, it holds that\n", "itemtype": "equation", "pos": 82899, "prevtext": "\nwhere the second inequality holds due to $\\sigma_{\\theta^{-}}(S^{\\mathsf{LU}}_\\Theta) \\ge \\sigma_{\\theta^{-}}(S^g_{\\theta^+})$\n by definition of \\eqref{def:lu-greedy-solution}.\n\n\n\n{\\flushleft \\em Setting~\\ref{thm-mul-case}:}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenote $a = \\frac{\\ln \\frac{1}{1-\\epsilon}}{2n + \\ln \\frac{1}{1-\\epsilon}}$ for convenience.\nSince every edge $e$ is probed for $t=\\frac{3 \\ln \\frac{2m}{\\gamma} }{p a^2} \\geq \\frac{3 \\ln \\frac{2m}{\\gamma} }{p_e a^2}$ times,\nthe probability of upper and lower tails derived by the multiplicative form of Chernoff-Hoeffding Inequality is\n\n", "index": 85, "text": "\\begin{align*}\n& \\Pr\\left[\\frac{1}{t}\\sum_{i=1}^{t}x^i_e \\ge (1+a)p_e\\right]\\le e^{-\\frac{a^2}{3}\\cdot p_e t} \\le \\frac{\\gamma}{2m}\n\\\\\n& \\Pr\\left[\\frac{1}{t}\\sum_{i=1}^{t}x^i_e \\le (1-a)p_e\\right]\\le e^{-\\frac{a^2}{3}\\cdot p_e t} \\le \\frac{\\gamma}{2m} \\mbox{.}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex46.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pr\\left[\\frac{1}{t}\\sum_{i=1}^{t}x^{i}_{e}\\geq(1+a)p_{e}\\right]%&#10;\\leq e^{-\\frac{a^{2}}{3}\\cdot p_{e}t}\\leq\\frac{\\gamma}{2m}\" display=\"inline\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>t</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover></mstyle><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow></mrow><mo>\u2265</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>p</mi><mi>e</mi></msub></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mfrac><msup><mi>a</mi><mn>2</mn></msup><mn>3</mn></mfrac><mo>\u22c5</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2264</mo><mstyle displaystyle=\"true\"><mfrac><mi>\u03b3</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex47.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pr\\left[\\frac{1}{t}\\sum_{i=1}^{t}x^{i}_{e}\\leq(1-a)p_{e}\\right]%&#10;\\leq e^{-\\frac{a^{2}}{3}\\cdot p_{e}t}\\leq\\frac{\\gamma}{2m}\\mbox{.}\" display=\"inline\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>t</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover></mstyle><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow></mrow><mo>\u2264</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>a</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>p</mi><mi>e</mi></msub></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2264</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mfrac><msup><mi>a</mi><mn>2</mn></msup><mn>3</mn></mfrac><mo>\u22c5</mo><msub><mi>p</mi><mi>e</mi></msub></mrow><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b3</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi></mrow></mfrac></mstyle><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\n\nNow suppose the above bound is satisfied.\nFor every edge $e \\in E$, let $r_e = (1+a) p_e$ and $l_e = (1-a) p_e$.\nThen, we have\n$r_e \\leq \\frac{1+a}{1-a}  \\frac{\\sum_{i=1}^{t}x^i_e}{t} \\le (1 + \\frac{1}{n} \\ln \\frac{1}{1-\\epsilon}) \\frac{\\sum_{i=1}^{t}x^i_e}{t}$.\nOn the other hand, it is easy to check that $r_e = \\frac{1+a}{1-a} l_e \\le (1 + \\frac{1}{n} \\ln \\frac{1}{1-\\epsilon}) l_e$.\nAccording to Lemma~\\ref{lem:mul}, for any set $S$,\n\n", "itemtype": "equation", "pos": 83207, "prevtext": "\nThen by union bound, it holds that\n", "index": 87, "text": "\n\\[\n\t\\Pr\\left[\\forall e\\in E, \\frac{1}{1+a}\\frac{\\sum_{i=1}^{t} x^i_e}{t} \\le p_e \\le \\frac{1}{1-a} \\frac{\\sum_{i=1}^{t} x^i_e}{t} \\right] \\ge 1 - \\gamma \\mbox{.}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex48.m1\" class=\"ltx_Math\" alttext=\"\\Pr\\left[\\forall e\\in E,\\frac{1}{1+a}\\frac{\\sum_{i=1}^{t}x^{i}_{e}}{t}\\leq p_{%&#10;e}\\leq\\frac{1}{1-a}\\frac{\\sum_{i=1}^{t}x^{i}_{e}}{t}\\right]\\geq 1-\\gamma\\mbox{.}\" display=\"block\"><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mo>\u2200</mo><mi>e</mi></mrow><mo>\u2208</mo><mi>E</mi></mrow><mo>,</mo><mrow><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>a</mi></mrow></mfrac><mo>\u2062</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow><mi>t</mi></mfrac></mrow><mo>\u2264</mo><msub><mi>p</mi><mi>e</mi></msub><mo>\u2264</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>a</mi></mrow></mfrac><mo>\u2062</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></msubsup><msubsup><mi>x</mi><mi>e</mi><mi>i</mi></msubsup></mrow><mi>t</mi></mfrac></mrow></mrow><mo>]</mo></mrow></mrow><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.06551.tex", "nexttext": "\nThus, $\\frac{\\sigma_{\\theta^{-}}(S^g_{\\theta^+})}{\\sigma_{\\theta^{+}}(S^g_{\\theta^+})} \\geq 1-\\epsilon$ \nalso holds. Similar to Setting~\\ref{thm-add-case}, \nthen we can apply Theorem~\\ref{thm:main} to derive the theorem.\n\\end{proof}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 83812, "prevtext": "\n\nNow suppose the above bound is satisfied.\nFor every edge $e \\in E$, let $r_e = (1+a) p_e$ and $l_e = (1-a) p_e$.\nThen, we have\n$r_e \\leq \\frac{1+a}{1-a}  \\frac{\\sum_{i=1}^{t}x^i_e}{t} \\le (1 + \\frac{1}{n} \\ln \\frac{1}{1-\\epsilon}) \\frac{\\sum_{i=1}^{t}x^i_e}{t}$.\nOn the other hand, it is easy to check that $r_e = \\frac{1+a}{1-a} l_e \\le (1 + \\frac{1}{n} \\ln \\frac{1}{1-\\epsilon}) l_e$.\nAccording to Lemma~\\ref{lem:mul}, for any set $S$,\n\n", "index": 89, "text": "\\begin{align*}\n\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}&\n\\ge \\left( 1+\\frac{1}{n} \\ln\\frac{1}{1-\\epsilon} \\right)^{-n}\n\\ge 1-\\epsilon \\mbox{.}\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex49.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\sigma_{\\theta^{-}}(S)}{\\sigma_{\\theta^{+}}(S)}\" display=\"inline\"><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>-</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mi>\u03c3</mi><msup><mi>\u03b8</mi><mo>+</mo></msup></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex49.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\left(1+\\frac{1}{n}\\ln\\frac{1}{1-\\epsilon}\\right)^{-n}\\geq 1-%&#10;\\epsilon\\mbox{.}\" display=\"inline\"><mrow><mi/><mo>\u2265</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mi>ln</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>\u03f5</mi></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mi>n</mi></mrow></msup><mo>\u2265</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03f5</mi><mo>\u2062</mo><mtext>.</mtext></mrow></mrow></mrow></math>", "type": "latex"}]