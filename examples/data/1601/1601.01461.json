[{"file": "1601.01461.tex", "nexttext": "\n where  $u^\\dag = \\sum_{i=1}^L u_i, u_i \\in V_i$ and $V_1+\\ldots+V_L = {\\mathbb{R}}^N$ but $\\langle V_i, V_j \\rangle \\simeq 0$ for $i \\neq j$ in the sense that \n\n", "itemtype": "equation", "pos": 2465, "prevtext": "\n\n\n\\title[Optimal support recovery with multi-penalty regularization]{Conditions on optimal support recovery in unmixing problems by means of multi-penalty regularization}\n\n\\author{Markus Grasmair}\n\\address{Department of Mathematical Sciences, Norwegian University of Science and Technology, 7491 Trondheim}\n\\email{markus.grasmair@math.ntnu.no}\n\n\\author{Valeriya Naumova}\n\\address{Simula Research Laboratory AS, Martin Linges vei 25, 1364 Fornebu, Oslo}\n\\email{valeriya@simula.no}\n\n\\maketitle\n\n\\begin{abstract}\nInspired by several real-life applications in audio processing and medical image analysis, where the quantity of interest is generated by several sources to be accurately modeled and separated, as well as by recent advances in  regularization theory and optimization, we study the conditions on optimal support recovery in inverse problems of unmixing type by means of multi-penalty regularization.  \n\nWe consider and analyze a regularization functional composed of a data-fidelity term, where signal and noise are additively mixed, a non-smooth, convex, sparsity promoting term, and a quadratic penalty term to model the noise. We prove not only that  the well-established theory for sparse recovery in the single parameter case can be translated to the multi-penalty settings, but  we also demonstrate the enhanced properties of multi-penalty regularization in terms of support identification compared to sole $\\ell^1$-minimization.  We additionally confirm and support the theoretical results by extensive numerical simulations, which give a statistics of robustness of the multi-penalty regularization scheme with respect to the single-parameter counterpart. \nEventually, we confirm a significant improvement in performance compared to standard $\\ell^1$-regularization\nfor  compressive sensing problems considered in our experiments. \n\\end{abstract}\n\n\n\n\n\\section{Introduction}\n\nIn many real-life applications such as audio processing or medical image analysis, one encounters the situation when given observations (most likely noisy) have been generated by several sources $u_i$ that one wishes to reconstruct separately.  \nIn this case, the reconstruction problem can be understood as an inverse problem of unmixing type, where the solution $u^\\dag$ consists of several (two or more) components of different nature, which have to be identified and separated.  \n\nIn mathematical terms, an unmixing problem can be stated as the solution of an equation\n", "index": 1, "text": "\n\\[\n\tA u^\\dag = y,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"Au^{\\dagger}=y,\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>u</mi><mo>\u2020</mo></msup></mrow><mo>=</mo><mi>y</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nfor all $v_i \\in V_i$ and $v_j \\in V_j,$ $\\| v_i\\|_{\\ell_2} = \\| v_j \\|_{\\ell_2} = 1, i \\neq j.$ \nIn general, we are interested to acquire the minimal amount of information on $u$ so that we can selectively reconstruct with the best accuracy one of the components $u_{\\hat \\imath}$, but not necessarily also the other components $u_j$ for $j \\neq \\hat \\imath$. In this settings, we further assume that $A$ cannot be specifically tuned to recover $u_{\\hat \\imath}$ but should be suited to gain universal information to recover $u_{\\hat \\imath}$ by a specifically tuned decoder. \n\nA concrete example of this setting is the \\emph{noise folding phenomenon} arising in compressed sensing, related to noise in the signal that is eventually amplified by the measurement procedure. In this setting, it is reasonable to consider a model problem of the type\n\n", "itemtype": "equation", "pos": 2648, "prevtext": "\n where  $u^\\dag = \\sum_{i=1}^L u_i, u_i \\in V_i$ and $V_1+\\ldots+V_L = {\\mathbb{R}}^N$ but $\\langle V_i, V_j \\rangle \\simeq 0$ for $i \\neq j$ in the sense that \n\n", "index": 3, "text": "\\begin{equation}\n\t\\label{UoS}\n\t\t| \\langle v_i, v_j \\rangle | \\simeq \\delta_{ij} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"|\\langle v_{i},v_{j}\\rangle|\\simeq\\delta_{ij}\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">\u27e8</mo><msub><mi>v</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2243</mo><msub><mi>\u03b4</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nwhere $v$ is the random Gaussian noise with variance $\\sigma_v$ on the original signal $u \\in {\\mathbb{R}}^N$ and $A \\in {\\mathbb{R}}^{m \\times N}$ is the linear measurement matrix. Several recent works (see, for instance, \\cite{Arias-castro11yeldar} and the references therein) illustrate how the measurement process actually causes the noise folding phenomenon. To be more specific, one can show that  (\\ref{model_problem}) is equivalent to solving \n\n", "itemtype": "equation", "pos": 3592, "prevtext": "\nfor all $v_i \\in V_i$ and $v_j \\in V_j,$ $\\| v_i\\|_{\\ell_2} = \\| v_j \\|_{\\ell_2} = 1, i \\neq j.$ \nIn general, we are interested to acquire the minimal amount of information on $u$ so that we can selectively reconstruct with the best accuracy one of the components $u_{\\hat \\imath}$, but not necessarily also the other components $u_j$ for $j \\neq \\hat \\imath$. In this settings, we further assume that $A$ cannot be specifically tuned to recover $u_{\\hat \\imath}$ but should be suited to gain universal information to recover $u_{\\hat \\imath}$ by a specifically tuned decoder. \n\nA concrete example of this setting is the \\emph{noise folding phenomenon} arising in compressed sensing, related to noise in the signal that is eventually amplified by the measurement procedure. In this setting, it is reasonable to consider a model problem of the type\n\n", "index": 5, "text": "\\begin{equation}\n\t\\label{model_problem}\n\t\tA (u+v) = y,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"A(u+v)=y,\" display=\"block\"><mrow><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>y</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nwhere $\\omega$ is composed by i.i.d.\\ Gaussian entries with distribution $\\mathbf N(0, \\sigma_\\omega)$, and \nthe variance $\\sigma_\\omega$ is related to the variance of the original signal\nby $\\sigma_\\omega^2 = \\frac{N}{m} \\sigma_v^2$.\nThis implies that the  variance of the noise on the original signal is amplified by a factor of $N/m$.\n\nUnder the assumption that $A$ satisfies the so-called restricted isometry property,\nit is known from the work on the Dantzig selector in \\cite{CandesTao} that one can reconstruct $u^\\dagger$\nfrom measurements $y$ as in (\\ref{model_problem_eq}) such that\n\n", "itemtype": "equation", "pos": 4114, "prevtext": "\nwhere $v$ is the random Gaussian noise with variance $\\sigma_v$ on the original signal $u \\in {\\mathbb{R}}^N$ and $A \\in {\\mathbb{R}}^{m \\times N}$ is the linear measurement matrix. Several recent works (see, for instance, \\cite{Arias-castro11yeldar} and the references therein) illustrate how the measurement process actually causes the noise folding phenomenon. To be more specific, one can show that  (\\ref{model_problem}) is equivalent to solving \n\n", "index": 7, "text": "\\begin{equation}\n\t\\label{model_problem_eq}\n\t\t\\hat A u+ \\omega = y,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\hat{A}u+\\omega=y,\" display=\"block\"><mrow><mrow><mrow><mrow><mover accent=\"true\"><mi>A</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mi>u</mi></mrow><mo>+</mo><mi>\u03c9</mi></mrow><mo>=</mo><mi>y</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nwhere $k$ denotes the number of nonzero elements of the solution $u.$ The estimate (\\ref{recon_rate}) is considered (folklore) nearly-optimal in the sense that no other method can really improve the asymptotic error $\\mathcal O(\\frac{N}{m}\\sigma_v^2).$ Therefore, the noise folding phenomenon may in practice significantly reduce the potential advantages of compressed sensing in terms of the trade-off between robustness and efficient compression (given by the factor $N/m$ here) compared to other more traditional subsampling methods \\cite{6204356}.\n\nIn \\cite{arfopeXX} the authors present a two-step numerical method which allows not only to recover the large entries of the original signal $u$ accurately, but also has enhanced properties in terms of support identification over simple $\\ell^1$-minimization based algorithms. In particular, because of the lack of separation between noise and reconstructed signal components, the latter ones can easily fail to recover the support when the support is not given a priori. However, the computational cost of the second phase of the procedure presented in  \\cite{arfopeXX}, being a non-smooth and non-convex optimization problem, is too demanding to be performed on problems with realistic dimensionalities. \nIt was also shown that other methods based on a different penalization of the signal and noise can lead to higher support detection rate.  \n\nThe follow up work \\cite{naumpeter}, which addresses the noise folding scenario  by means of multi-penalty regularization, provides the first numerical evidence of the superior performance of multi-penalty regularization compared to its single parameter counterparts for problem (\\ref{model_problem}). In particular, the authors consider the functional\n\n", "itemtype": "equation", "pos": 4789, "prevtext": "\nwhere $\\omega$ is composed by i.i.d.\\ Gaussian entries with distribution $\\mathbf N(0, \\sigma_\\omega)$, and \nthe variance $\\sigma_\\omega$ is related to the variance of the original signal\nby $\\sigma_\\omega^2 = \\frac{N}{m} \\sigma_v^2$.\nThis implies that the  variance of the noise on the original signal is amplified by a factor of $N/m$.\n\nUnder the assumption that $A$ satisfies the so-called restricted isometry property,\nit is known from the work on the Dantzig selector in \\cite{CandesTao} that one can reconstruct $u^\\dagger$\nfrom measurements $y$ as in (\\ref{model_problem_eq}) such that\n\n", "index": 9, "text": "\\begin{equation}\n\t\\label{recon_rate}\n\t \\| u - u^\\dagger \\|_2^2 \\leq C^2  2\\Bigl((1+k) \\frac{N}{m} \\sigma^2_v\\Bigr)\\log N ,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\|u-u^{\\dagger}\\|_{2}^{2}\\leq C^{2}2\\Bigl{(}(1+k)\\frac{N}{m}\\sigma^{2}_{v}%&#10;\\Bigr{)}\\log N,\" display=\"block\"><mrow><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mi>u</mi><mo>-</mo><msup><mi>u</mi><mo>\u2020</mo></msup></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>\u2264</mo><mrow><msup><mi>C</mi><mn>2</mn></msup><mo>\u2062</mo><mn>2</mn><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>k</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mfrac><mi>N</mi><mi>m</mi></mfrac><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mi>v</mi><mn>2</mn></msubsup></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mi>N</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nhere  $\\alpha, \\beta, \\varepsilon \\in {\\mathbb{R}}_+$ may all be considered as regularization parameters of the problem. \nThe parameter $\\varepsilon>0 $ ensures the $\\ell_2-$coercivity of $J_{p,q}(u,\\cdot)$ also with respect to the component $v$. In the infinite dimensional setting the authors presented a numerical approach to the minimization of (\\ref{MP}) for $0 \\leq p <2, 2 \\leq q< \\infty$, based on simple iterative thresholding steps, and analyzed its convergence.\n\nThe results presented in this paper are very much inspired not only by the above-mentioned works in the signal processing and compressed sensing fields, but also by  theoretical developments in sparsity-based regularization  (see \\cite{CPA:CPA20350} and references therein) and multi-penalty regularization (\\cite{LP_NumMath, NP13}, just to mention a few). \nWhile the latter two directions are considered separately in most of the literature, there have also been some efforts to understand regularization and convergence behavior for multiple parameters and functionals, especially for image analysis \\cite{BH14, SL13}.\nHowever, to the best of our knowledge, the present paper is the first one providing a theoretical analysis  of the multi-penalty regularization with a non-smooth sparsity promoting regularization term, and an explicit comparison  with the single-parameter counterpart.\n \n \\subsection{Content of the paper}\nIn Section 2 we concisely recall the pertinent features and concepts of multi-penalty and single-penalty regularization. We further show that $\\ell^1$-regularization can be considered as the limiting case of the multi-penalty one, and thus the theory of $\\ell^1$-regularization can be applied to multi-penalty setting. \nIn Section 3 we recall and discuss conditions for exact  support recovery in the single-parameter case. \nThe main contributions of the paper are presented in Sections 4 and 5, where we extend and generalize the results from the previous sections to the multi-penalty setting. \nIn Section 5 we also open the discussion on the set of admissible parameters for the exact support recovery for unmixing problem in single-parameter as well as multi-penalty cases. In particular, we  study the sensitivity of the multi-penalty scheme with respect to the parameter choice. The theoretical findings and discussion are illustrated and  supported by extensive numerical validation tests presented in Section 6. Finally, in Section 7 we compare the performance  of the multi-penalty regularization and its single-parameter counterpart for  compressive sensing problems.\n\n\n\\section{Multi-Penalty and Single-Penalty Regularization}\n\nWe first provide a short reminder and collect some definitions of the standard notation used in this paper.   \n\nThe true solution $u^\\dagger$ of the unmixing problem~\\eqref{model_problem}\nis called $k$-sparse if it has at most $k$ non-zero entries, \ni.e., $\\#I = \\# \\mathop{{\\rm supp}}(u^\\dagger) \\leq k$, \nwhere $I := \\mathop{{\\rm supp}}(u^\\dagger) := \\bigl\\{i: u^\\dagger_i \\neq 0\\}$ denotes the support of $u^\\dagger$.\n\n\nWe propose to solve the\nunmixing problem~\\eqref{model_problem} with $k$-sparse true solution\nusing multi-penalty Tikhonov regularization\nof the form\n\n", "itemtype": "equation", "pos": 6681, "prevtext": "\nwhere $k$ denotes the number of nonzero elements of the solution $u.$ The estimate (\\ref{recon_rate}) is considered (folklore) nearly-optimal in the sense that no other method can really improve the asymptotic error $\\mathcal O(\\frac{N}{m}\\sigma_v^2).$ Therefore, the noise folding phenomenon may in practice significantly reduce the potential advantages of compressed sensing in terms of the trade-off between robustness and efficient compression (given by the factor $N/m$ here) compared to other more traditional subsampling methods \\cite{6204356}.\n\nIn \\cite{arfopeXX} the authors present a two-step numerical method which allows not only to recover the large entries of the original signal $u$ accurately, but also has enhanced properties in terms of support identification over simple $\\ell^1$-minimization based algorithms. In particular, because of the lack of separation between noise and reconstructed signal components, the latter ones can easily fail to recover the support when the support is not given a priori. However, the computational cost of the second phase of the procedure presented in  \\cite{arfopeXX}, being a non-smooth and non-convex optimization problem, is too demanding to be performed on problems with realistic dimensionalities. \nIt was also shown that other methods based on a different penalization of the signal and noise can lead to higher support detection rate.  \n\nThe follow up work \\cite{naumpeter}, which addresses the noise folding scenario  by means of multi-penalty regularization, provides the first numerical evidence of the superior performance of multi-penalty regularization compared to its single parameter counterparts for problem (\\ref{model_problem}). In particular, the authors consider the functional\n\n", "index": 11, "text": "\\begin{equation}\n\t\\label{MP}\nJ_{p,q}(u,v) : = \\| A(u+v)-y \\|^2_{2} +  \\alpha \\| u\\|_{p}^{p} + \\left (\\beta  \\| v\\|_{q}^{q} + \\varepsilon \\| v\\|_{2}^2 \\right ),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"J_{p,q}(u,v):=\\|A(u+v)-y\\|^{2}_{2}+\\alpha\\|u\\|_{p}^{p}+\\left(\\beta\\|v\\|_{q}^{q%&#10;}+\\varepsilon\\|v\\|_{2}^{2}\\right),\" display=\"block\"><mrow><mrow><mrow><msub><mi>J</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mo>+</mo><mrow><mo>(</mo><mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mi>q</mi><mi>q</mi></msubsup></mrow><mo>+</mo><mrow><mi>\u03b5</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nthe solution of which we will denote by $(u_{\\alpha,\\beta},v_{\\alpha,\\beta})$.\nWe note that we can, formally, interpret standard $\\ell^1$-regularization\nas the limiting case $\\beta = \\infty$, setting\n", "itemtype": "equation", "pos": 10068, "prevtext": "\nhere  $\\alpha, \\beta, \\varepsilon \\in {\\mathbb{R}}_+$ may all be considered as regularization parameters of the problem. \nThe parameter $\\varepsilon>0 $ ensures the $\\ell_2-$coercivity of $J_{p,q}(u,\\cdot)$ also with respect to the component $v$. In the infinite dimensional setting the authors presented a numerical approach to the minimization of (\\ref{MP}) for $0 \\leq p <2, 2 \\leq q< \\infty$, based on simple iterative thresholding steps, and analyzed its convergence.\n\nThe results presented in this paper are very much inspired not only by the above-mentioned works in the signal processing and compressed sensing fields, but also by  theoretical developments in sparsity-based regularization  (see \\cite{CPA:CPA20350} and references therein) and multi-penalty regularization (\\cite{LP_NumMath, NP13}, just to mention a few). \nWhile the latter two directions are considered separately in most of the literature, there have also been some efforts to understand regularization and convergence behavior for multiple parameters and functionals, especially for image analysis \\cite{BH14, SL13}.\nHowever, to the best of our knowledge, the present paper is the first one providing a theoretical analysis  of the multi-penalty regularization with a non-smooth sparsity promoting regularization term, and an explicit comparison  with the single-parameter counterpart.\n \n \\subsection{Content of the paper}\nIn Section 2 we concisely recall the pertinent features and concepts of multi-penalty and single-penalty regularization. We further show that $\\ell^1$-regularization can be considered as the limiting case of the multi-penalty one, and thus the theory of $\\ell^1$-regularization can be applied to multi-penalty setting. \nIn Section 3 we recall and discuss conditions for exact  support recovery in the single-parameter case. \nThe main contributions of the paper are presented in Sections 4 and 5, where we extend and generalize the results from the previous sections to the multi-penalty setting. \nIn Section 5 we also open the discussion on the set of admissible parameters for the exact support recovery for unmixing problem in single-parameter as well as multi-penalty cases. In particular, we  study the sensitivity of the multi-penalty scheme with respect to the parameter choice. The theoretical findings and discussion are illustrated and  supported by extensive numerical validation tests presented in Section 6. Finally, in Section 7 we compare the performance  of the multi-penalty regularization and its single-parameter counterpart for  compressive sensing problems.\n\n\n\\section{Multi-Penalty and Single-Penalty Regularization}\n\nWe first provide a short reminder and collect some definitions of the standard notation used in this paper.   \n\nThe true solution $u^\\dagger$ of the unmixing problem~\\eqref{model_problem}\nis called $k$-sparse if it has at most $k$ non-zero entries, \ni.e., $\\#I = \\# \\mathop{{\\rm supp}}(u^\\dagger) \\leq k$, \nwhere $I := \\mathop{{\\rm supp}}(u^\\dagger) := \\bigl\\{i: u^\\dagger_i \\neq 0\\}$ denotes the support of $u^\\dagger$.\n\n\nWe propose to solve the\nunmixing problem~\\eqref{model_problem} with $k$-sparse true solution\nusing multi-penalty Tikhonov regularization\nof the form\n\n", "index": 13, "text": "\\begin{equation}\\label{eq:multi}\nT_{\\alpha,\\beta}(u,v) := \\frac{1}{2}\\|A(u+v)-y\\|_2^2 + \\alpha\\|u\\|_1 + \\frac{\\beta}{2}\\|v\\|_2^2 \\to \\min_{u,v},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"T_{\\alpha,\\beta}(u,v):=\\frac{1}{2}\\|A(u+v)-y\\|_{2}^{2}+\\alpha\\|u\\|_{1}+\\frac{%&#10;\\beta}{2}\\|v\\|_{2}^{2}\\to\\min_{u,v},\" display=\"block\"><mrow><mrow><mrow><msub><mi>T</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>\u2192</mo><munder><mi>min</mi><mrow><mi>u</mi><mo>,</mo><mi>v</mi></mrow></munder></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nObviously, the pair of minimizers of $T_{\\alpha,\\infty}$ will always\nbe equal to $(u_\\alpha,0)$, where $u_\\alpha$\nminimizes $\\frac{1}{2}\\|Au-y\\|_2^2+\\alpha\\|u\\|_1$.\n\n\\begin{definition}\n  Let $\\beta \\in {\\mathbb{R}}_+\\cup\\{\\infty\\}$ be fixed.\n  We say that a set $\\mathcal{S} \\subset {\\mathbb{R}}^N \\times {\\mathbb{R}}^N$ is a\n  \\emph{set of exact support recovery for the unmixing\n    problem with operator $A$}, if there exists $\\alpha > 0$,\n  such that $\\mathop{{\\rm supp}}(u_{\\alpha,\\beta}) = \\mathop{{\\rm\n      supp}}(u^\\dagger)$\n  whenever the given data $y$ has the form $y = A(u^\\dagger+v)$\n  with $(u^\\dagger,v) \\in \\mathcal{S}$.\n\n  The parameters $\\alpha > 0$ for which this property holds\n  are called \\emph{admissible for $\\mathcal{S}$}.\n\\end{definition}\n\nSpecifically, we will study for $c>d>0$ the sets \n\n", "itemtype": "equation", "pos": 10427, "prevtext": "\nthe solution of which we will denote by $(u_{\\alpha,\\beta},v_{\\alpha,\\beta})$.\nWe note that we can, formally, interpret standard $\\ell^1$-regularization\nas the limiting case $\\beta = \\infty$, setting\n", "index": 15, "text": "\n\\[\nT_{\\alpha,\\infty}(u,v) := \n\\begin{cases}\n  \\frac{1}{2}\\|Au-y\\|_2^2 + \\alpha\\|u\\|_1 & \\text{if } v = 0,\\\\\n  +\\infty & \\text{if } v\\neq 0.\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"T_{\\alpha,\\infty}(u,v):=\\begin{cases}\\frac{1}{2}\\|Au-y\\|_{2}^{2}+\\alpha\\|u\\|_{%&#10;1}&amp;\\text{if }v=0,\\\\&#10;+\\infty&amp;\\text{if }v\\neq 0.\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mstyle displaystyle=\"false\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>v</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>+</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>v</mi></mrow><mo>\u2260</mo><mn>0</mn></mrow><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nand the corresponding class \n", "itemtype": "equation", "pos": 11400, "prevtext": "\nObviously, the pair of minimizers of $T_{\\alpha,\\infty}$ will always\nbe equal to $(u_\\alpha,0)$, where $u_\\alpha$\nminimizes $\\frac{1}{2}\\|Au-y\\|_2^2+\\alpha\\|u\\|_1$.\n\n\\begin{definition}\n  Let $\\beta \\in {\\mathbb{R}}_+\\cup\\{\\infty\\}$ be fixed.\n  We say that a set $\\mathcal{S} \\subset {\\mathbb{R}}^N \\times {\\mathbb{R}}^N$ is a\n  \\emph{set of exact support recovery for the unmixing\n    problem with operator $A$}, if there exists $\\alpha > 0$,\n  such that $\\mathop{{\\rm supp}}(u_{\\alpha,\\beta}) = \\mathop{{\\rm\n      supp}}(u^\\dagger)$\n  whenever the given data $y$ has the form $y = A(u^\\dagger+v)$\n  with $(u^\\dagger,v) \\in \\mathcal{S}$.\n\n  The parameters $\\alpha > 0$ for which this property holds\n  are called \\emph{admissible for $\\mathcal{S}$}.\n\\end{definition}\n\nSpecifically, we will study for $c>d>0$ the sets \n\n", "index": 17, "text": "\\begin{equation}\\label{eq:ScdI}\n\\mathcal{S}_{c,d,I} := \\bigl\\{(u,v) \\in {\\mathbb{R}}^N \\times {\\mathbb{R}}^N :\n\\mathop{{\\rm supp}}(u) = I,\\\n\\inf_{i\\in I} |u_i| > c,\\\n\\|v\\|_\\infty < d\\bigr\\}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{S}_{c,d,I}:=\\bigl{\\{}(u,v)\\in{\\mathbb{R}}^{N}\\times{\\mathbb{R}}^{N}:%&#10;\\mathop{{\\rm supp}}(u)=I,\\ \\inf_{i\\in I}|u_{i}|&gt;c,\\ \\|v\\|_{\\infty}&lt;d\\bigr{\\}}\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mi>c</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>I</mi></mrow></msub><mo>:=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><msup><mi>\u211d</mi><mi>N</mi></msup><mo>\u00d7</mo><msup><mi>\u211d</mi><mi>N</mi></msup></mrow></mrow><mo>:</mo><mrow><mrow><mrow><mo movablelimits=\"false\">supp</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>I</mi></mrow><mo rspace=\"7.5pt\">,</mo><mrow><mrow><mrow><munder><mo movablelimits=\"false\">inf</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>I</mi></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&gt;</mo><mi>c</mi></mrow><mo rspace=\"7.5pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>&lt;</mo><mi>d</mi></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nThe set $\\mathcal{S}_{c,d,I}$ is a set of exact support\nrecovery, if there exists some regularization parameter $\\alpha > 0$,\nsuch that we can apply multi-penalty regularization\nwith parameters $\\alpha$ and $\\beta$ (or single-parameter $\\ell^1$-regularization\nwith parameter $\\alpha$ in case $\\beta = \\infty$)\nto the unmixing problem~\\eqref{model_problem} and obtain a result\nwith the correct support, provided that the $\\ell^\\infty$-norm of\nthe noise is smaller than $d$ and the non-zero coefficients\nof $u^\\dagger$ are larger than $c$.\nTypical examples of real-life signals that can be modeled by signals from the set $\\mathcal{S}_{c,d,I}$ can be found in Asteroseismology, see for instance \\cite{arfopeXX}.\n\nWe note that this class of signals is very similar to the one studied\nin~\\cite{arfopeXX}. The main difference is that we focus on the case\nwhere the noise $v$ is bounded only componentwise (that is, with respect\nto the $\\ell^\\infty$-norm), whereas~\\cite{arfopeXX} deals with\nnoise that has a bounded $\\ell^p$-norm for some $1 \\le p \\le 2$.\nAdditionally, we allow the noise also to mix with the signal $u^\\dagger$\nto be identified in the sense that the supports of $v$ and $u^\\dagger$\nmay have a non-empty intersection.\nIn contrast, the signal and the noise are assumed to be strictly\nseparated in~\\cite{arfopeXX}.\n\\medskip\n\nThroughout the paper we will several times refer to the\nsign function $\\mathop{{\\rm sgn}}$, which we always\ninterpret as being the set valued function $\\mathop{{\\rm sgn}}(t) = 1$ if $t > 0$,\n$\\mathop{{\\rm sgn}}(t) = -1$ if $t < 0$, and\n$\\mathop{{\\rm sgn}}(t) = [-1,1]$ if $t = 0$, applied componentwise\nto the entries of the vector $u_\\alpha$.\n\nWe use the notation $A_I$ to denote the restriction of the operator $A$\nto the span of the support of $u^\\dagger$.\nAdditionally, we denote by\n", "itemtype": "equation", "pos": 11633, "prevtext": "\nand the corresponding class \n", "index": 19, "text": "\n\\[\n\\mathcal{S}_{c,d,k} := \\bigcup_{\\#I \\le k} \\mathcal{S}_{c,d,I}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{S}_{c,d,k}:=\\bigcup_{\\#I\\leq k}\\mathcal{S}_{c,d,I}.\" display=\"block\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mi>c</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>:=</mo><mrow><munder><mo largeop=\"true\" mathsize=\"160%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u22c3</mo><mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>I</mi></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mi>c</mi><mo>,</mo><mi>d</mi><mo>,</mo><mi>I</mi></mrow></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nthe complement of $I$, and by $A_J$ the restriction\nof $A$ to the span of $J$.\nWe note that the adjoints $A_I^*$ and $A_J^*$ are simply\nthe compositions of the adjoint $A^*$ of $A$ with the projections\nonto the spans of $I$ and $J$, respectively.\n\\medskip\n\n\n\nAs a first result, we show that the solution of \nthe multi-penalty problem~\\eqref{eq:multi} simultaneously\nsolves a related single-penalty problem.\n\n\\begin{lemma}\\label{le:single}\n  The pair $(u_{\\alpha,\\beta},v_{\\alpha,\\beta})$ solves~(\\ref{eq:multi})\n  if and only if\n \n", "itemtype": "equation", "pos": 13525, "prevtext": "\nThe set $\\mathcal{S}_{c,d,I}$ is a set of exact support\nrecovery, if there exists some regularization parameter $\\alpha > 0$,\nsuch that we can apply multi-penalty regularization\nwith parameters $\\alpha$ and $\\beta$ (or single-parameter $\\ell^1$-regularization\nwith parameter $\\alpha$ in case $\\beta = \\infty$)\nto the unmixing problem~\\eqref{model_problem} and obtain a result\nwith the correct support, provided that the $\\ell^\\infty$-norm of\nthe noise is smaller than $d$ and the non-zero coefficients\nof $u^\\dagger$ are larger than $c$.\nTypical examples of real-life signals that can be modeled by signals from the set $\\mathcal{S}_{c,d,I}$ can be found in Asteroseismology, see for instance \\cite{arfopeXX}.\n\nWe note that this class of signals is very similar to the one studied\nin~\\cite{arfopeXX}. The main difference is that we focus on the case\nwhere the noise $v$ is bounded only componentwise (that is, with respect\nto the $\\ell^\\infty$-norm), whereas~\\cite{arfopeXX} deals with\nnoise that has a bounded $\\ell^p$-norm for some $1 \\le p \\le 2$.\nAdditionally, we allow the noise also to mix with the signal $u^\\dagger$\nto be identified in the sense that the supports of $v$ and $u^\\dagger$\nmay have a non-empty intersection.\nIn contrast, the signal and the noise are assumed to be strictly\nseparated in~\\cite{arfopeXX}.\n\\medskip\n\nThroughout the paper we will several times refer to the\nsign function $\\mathop{{\\rm sgn}}$, which we always\ninterpret as being the set valued function $\\mathop{{\\rm sgn}}(t) = 1$ if $t > 0$,\n$\\mathop{{\\rm sgn}}(t) = -1$ if $t < 0$, and\n$\\mathop{{\\rm sgn}}(t) = [-1,1]$ if $t = 0$, applied componentwise\nto the entries of the vector $u_\\alpha$.\n\nWe use the notation $A_I$ to denote the restriction of the operator $A$\nto the span of the support of $u^\\dagger$.\nAdditionally, we denote by\n", "index": 21, "text": "\n\\[\nJ := \\bigl\\{i: u^\\dagger_i = 0\\}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"J:=\\bigl{\\{}i:u^{\\dagger}_{i}=0\\}\" display=\"block\"><mrow><mi>J</mi><mo>:=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mi>i</mi><mo>:</mo><mrow><msubsup><mi>u</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and $u_{\\alpha,\\beta}$ solves the optimization problem\n  \n", "itemtype": "equation", "pos": 14096, "prevtext": "\nthe complement of $I$, and by $A_J$ the restriction\nof $A$ to the span of $J$.\nWe note that the adjoints $A_I^*$ and $A_J^*$ are simply\nthe compositions of the adjoint $A^*$ of $A$ with the projections\nonto the spans of $I$ and $J$, respectively.\n\\medskip\n\n\n\nAs a first result, we show that the solution of \nthe multi-penalty problem~\\eqref{eq:multi} simultaneously\nsolves a related single-penalty problem.\n\n\\begin{lemma}\\label{le:single}\n  The pair $(u_{\\alpha,\\beta},v_{\\alpha,\\beta})$ solves~(\\ref{eq:multi})\n  if and only if\n \n", "index": 23, "text": "\\[\n  v_{\\alpha,\\beta} = (\\beta + A^*A)^{-1}(A^* y - A^*A u_{\\alpha,\\beta})\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"v_{\\alpha,\\beta}=(\\beta+A^{*}A)^{-1}(A^{*}y-A^{*}Au_{\\alpha,\\beta})\" display=\"block\"><mrow><msub><mi>v</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>+</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>y</mi></mrow><mo>-</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><msub><mi>u</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  with\n \n", "itemtype": "equation", "pos": 14235, "prevtext": "\n  and $u_{\\alpha,\\beta}$ solves the optimization problem\n  \n", "index": 25, "text": "\\begin{equation}\\label{eq:minu}\n  \\frac{1}{2}\\|B_\\beta u - y_\\beta\\|_2^2 + \\alpha\\|u\\|_1 \\to \\min\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\|B_{\\beta}u-y_{\\beta}\\|_{2}^{2}+\\alpha\\|u\\|_{1}\\to\\min\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><msub><mi>B</mi><mi>\u03b2</mi></msub><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><msub><mi>y</mi><mi>\u03b2</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2192</mo><mi>min</mi></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and\n \n", "itemtype": "equation", "pos": 14359, "prevtext": "\n  with\n \n", "index": 27, "text": "\\[\n  B_\\beta = \\Bigl({\\mathrm{Id}} + \\frac{AA^*}{\\beta}\\Bigr)^{-1/2}A\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"B_{\\beta}=\\Bigl{(}{\\mathrm{Id}}+\\frac{AA^{*}}{\\beta}\\Bigr{)}^{-1/2}A\" display=\"block\"><mrow><msub><mi>B</mi><mi>\u03b2</mi></msub><mo>=</mo><mrow><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mi>Id</mi><mo>+</mo><mfrac><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mi>\u03b2</mi></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{proof}\n  We can solve the optimization problem in~\\eqref{eq:multi} in two steps, first\n  with respect to $v$ and then with respect to $u$.\n  Assuming that $u$ is fixed, the optimality condition for $v$ in~\\eqref{eq:multi} reads\n  \n", "itemtype": "equation", "pos": 14442, "prevtext": "\n  and\n \n", "index": 29, "text": "\\[\n  y_\\beta = \\Bigl({\\mathrm{Id}}+\\frac{AA^*}{\\beta}\\Bigr)^{-1/2}y.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"y_{\\beta}=\\Bigl{(}{\\mathrm{Id}}+\\frac{AA^{*}}{\\beta}\\Bigr{)}^{-1/2}y.\" display=\"block\"><mrow><mrow><msub><mi>y</mi><mi>\u03b2</mi></msub><mo>=</mo><mrow><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mi>Id</mi><mo>+</mo><mfrac><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mi>\u03b2</mi></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo>\u2062</mo><mi>y</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  That is, for fixed $u$, the optimum in~\\eqref{eq:multi} with respect to\n  $v$ is obtained at\n \n", "itemtype": "equation", "pos": 14766, "prevtext": "\n\\end{lemma}\n\n\\begin{proof}\n  We can solve the optimization problem in~\\eqref{eq:multi} in two steps, first\n  with respect to $v$ and then with respect to $u$.\n  Assuming that $u$ is fixed, the optimality condition for $v$ in~\\eqref{eq:multi} reads\n  \n", "index": 31, "text": "\\begin{equation}\\label{eq:vu}\n  A^*(A(u+v)-y) + \\beta v = 0.\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"A^{*}(A(u+v)-y)+\\beta v=0.\" display=\"block\"><mrow><mrow><mrow><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Inserting this into the Tikhonov functional, we obtain the optimization\n  problem\n \n", "itemtype": "equation", "pos": 14941, "prevtext": "\n  That is, for fixed $u$, the optimum in~\\eqref{eq:multi} with respect to\n  $v$ is obtained at\n \n", "index": 33, "text": "\\[\n  v(u) := (\\beta + A^*A)^{-1}(A^* y - A^*Au).\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"v(u):=(\\beta+A^{*}A)^{-1}(A^{*}y-A^{*}Au).\" display=\"block\"><mrow><mrow><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>+</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>y</mi></mrow><mo>-</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Using~\\eqref{eq:vu}, we can write\n \n", "itemtype": "equation", "pos": 15081, "prevtext": "\n  Inserting this into the Tikhonov functional, we obtain the optimization\n  problem\n \n", "index": 35, "text": "\\[\n  \\frac{1}{2}\\|A(u+v(u))-y\\|_2^2 + \\alpha\\|u\\|_1 +\n  \\frac{\\beta}{2}\\|v(u)\\|_2^2 \\to \\min_u.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\|A(u+v(u))-y\\|_{2}^{2}+\\alpha\\|u\\|_{1}+\\frac{\\beta}{2}\\|v(u)\\|_{2}%&#10;^{2}\\to\\min_{u}.\" display=\"block\"><mrow><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow><mo>+</mo><mrow><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>\u2192</mo><munder><mi>min</mi><mi>u</mi></munder></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Thus the optimization problem for $u$ simplifies to\n  \n", "itemtype": "equation", "pos": 15220, "prevtext": "\n  Using~\\eqref{eq:vu}, we can write\n \n", "index": 37, "text": "\\[\n  \\begin{aligned}\n  \\frac{1}{2}\\|A(u+v(u))-y\\|_2^2\n  &= \\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle\\\\\n  &\\qquad\\qquad{}+ \\frac{1}{2}\\langle A^*(A(u+v(u))-y),v(u)\\rangle\\\\\n  &= \\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle\n  - \\frac{\\beta}{2}\\|v(u)\\|_2^2.\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2}\\|A(u+v(u))-y\\|_{2}^{2}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>,</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad\\qquad{}+\\frac{1}{2}\\langle A^{*}(A(u+v(u))-y),v(u)\\rangle\" display=\"inline\"><mrow><mo lspace=\"42.5pt\">+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle-\\frac{\\beta}{2}\\|v(u)%&#10;\\|_{2}^{2}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>,</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03b2</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Now note that\n \n", "itemtype": "equation", "pos": 15551, "prevtext": "\n  Thus the optimization problem for $u$ simplifies to\n  \n", "index": 39, "text": "\\begin{equation}\\label{eq:optu1}\n  \\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle\n  + \\alpha\\|u\\|_1 \\to \\min_u.\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\langle A(u+v(u))-y,Au-y\\rangle+\\alpha\\|u\\|_{1}\\to\\min_{u}.\" display=\"block\"><mrow><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow><mo>,</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2192</mo><munder><mi>min</mi><mi>u</mi></munder></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Inserting this equality in~\\eqref{eq:optu1}, we obtain the\n  optimization problem\n \n", "itemtype": "equation", "pos": 15694, "prevtext": "\n  Now note that\n \n", "index": 41, "text": "\\[\n  \\begin{aligned}\n    A(u+v(u))-y\n    &= A({\\mathrm{Id}}-(\\beta+A^*A)^{-1}A^*A)u - ({\\mathrm{Id}}-A(\\beta+A^*A)^{-1}A^*)y\\\\\n    &= A({\\mathrm{Id}}+A^*A/\\beta)^{-1}u - ({\\mathrm{Id}}+AA^*/\\beta)^{-1}y\\\\\n    &= ({\\mathrm{Id}}+AA^*/\\beta)^{-1}(Au-y).\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle A(u+v(u))-y\" display=\"inline\"><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>u</mi><mo>+</mo><mrow><mi>v</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>y</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=A({\\mathrm{Id}}-(\\beta+A^{*}A)^{-1}A^{*}A)u-({\\mathrm{Id}}-A(%&#10;\\beta+A^{*}A)^{-1}A^{*})y\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>-</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>+</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>-</mo><mrow><mi>A</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03b2</mi><mo>+</mo><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>y</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=A({\\mathrm{Id}}+A^{*}A/\\beta)^{-1}u-({\\mathrm{Id}}+AA^{*}/\\beta)%&#10;^{-1}y\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>+</mo><mrow><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mi>A</mi></mrow><mo>/</mo><mi>\u03b2</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>+</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mo>/</mo><mi>\u03b2</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>y</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=({\\mathrm{Id}}+AA^{*}/\\beta)^{-1}(Au-y).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>+</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mo>/</mo><mi>\u03b2</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  which is the same as~\\eqref{eq:minu}.\n\\end{proof}\n\n\n\\begin{remark}\n  As a consequence of Lemma~\\ref{le:single}, we can apply\n  the theory of $\\ell^1$-regularization also to the multi-penalty\n  setting we consider here. In particular, this yields, for fixed \n  $\\beta > 0$, estimates of the form\n \n", "itemtype": "equation", "pos": 16052, "prevtext": "\n  Inserting this equality in~\\eqref{eq:optu1}, we obtain the\n  optimization problem\n \n", "index": 43, "text": "\\[\n  \\frac{1}{2}\\bigl\\langle\n  ({\\mathrm{Id}}+AA^*/\\beta)^{-1}(Au-y),Au-y\\bigr\\rangle + \\alpha\\|u\\|_1\n  \\to \\min_u,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\bigl{\\langle}({\\mathrm{Id}}+AA^{*}/\\beta)^{-1}(Au-y),Au-y\\bigr{%&#10;\\rangle}+\\alpha\\|u\\|_{1}\\to\\min_{u},\" display=\"block\"><mrow><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e8</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Id</mi><mo>+</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mo>/</mo><mi>\u03b2</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">\u27e9</mo></mrow></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>\u2192</mo><munder><mi>min</mi><mi>u</mi></munder></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  provided that $u^\\dagger$ satisfies a source condition of the form \n  $B_\\beta^*\\eta \\in \\partial(\\|u^\\dagger\\|_1)$ \n  with $|(B_\\beta^*\\eta)_i| < 1$ for every $i\\not\\in\\mathop{{\\rm supp}}(u^\\dagger)$, \n  and the restriction of the mapping $B_\\beta$ to the span of the\n  support of $u^\\dagger$ is injective\n  (see~\\cite{CPA:CPA20350}).\n  Additionally, it is easy to show that these conditions hold\n  for $B_\\beta$ provided that they hold for $A$ and $\\beta$ is\n  sufficiently large.\n  \n\\end{remark}\n\n\n\\section{Sets of Exact Support Recovery---Single-penalty Setting}\n\nThe main focus of this paper is the question whether multi-penalty\nregularization allows for the exact recovery of the support\nof the true solution $u^\\dagger$ and how it compares to single-penalty\nregularization.\nBecause, as we have seen in Lemma~\\ref{le:single}, multi-penalty\nregularization can be rewritten as single-parameter regularization\nfor the regularized operator $B_\\beta$ and right hand side $y_\\beta$,\nwe will first discuss recovery conditions in the single-parameter setting.\n\n\nIn order to find conditions for exact support recovery,\nwe first recall the necessary and sufficient optimality\ncondition for $\\ell^1$-regularization:\n\n\\begin{lemma}\\label{le:opt}\n  The vector $u_\\alpha$ minimizes\n \n", "itemtype": "equation", "pos": 16472, "prevtext": "\n  which is the same as~\\eqref{eq:minu}.\n\\end{proof}\n\n\n\\begin{remark}\n  As a consequence of Lemma~\\ref{le:single}, we can apply\n  the theory of $\\ell^1$-regularization also to the multi-penalty\n  setting we consider here. In particular, this yields, for fixed \n  $\\beta > 0$, estimates of the form\n \n", "index": 45, "text": "\\[\n  \\|u^\\dagger-u_{\\alpha,\\beta}\\|_1 \\le C_{1,\\beta} \\alpha + \n  C_{2,\\beta} \\frac{\\|y_\\beta-B_\\beta u^\\dagger\\|_2^2}{\\alpha}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\|u^{\\dagger}-u_{\\alpha,\\beta}\\|_{1}\\leq C_{1,\\beta}\\alpha+C_{2,\\beta}\\frac{\\|%&#10;y_{\\beta}-B_{\\beta}u^{\\dagger}\\|_{2}^{2}}{\\alpha}\" display=\"block\"><mrow><msub><mrow><mo>\u2225</mo><mrow><msup><mi>u</mi><mo>\u2020</mo></msup><mo>-</mo><msub><mi>u</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub></mrow><mo>\u2225</mo></mrow><mn>1</mn></msub><mo>\u2264</mo><mrow><mrow><msub><mi>C</mi><mrow><mn>1</mn><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>+</mo><mrow><msub><mi>C</mi><mrow><mn>2</mn><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo>\u2062</mo><mfrac><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>y</mi><mi>\u03b2</mi></msub><mo>-</mo><mrow><msub><mi>B</mi><mi>\u03b2</mi></msub><mo>\u2062</mo><msup><mi>u</mi><mo>\u2020</mo></msup></mrow></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mi>\u03b1</mi></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  if and only if\n \n", "itemtype": "equation", "pos": 17883, "prevtext": "\n  provided that $u^\\dagger$ satisfies a source condition of the form \n  $B_\\beta^*\\eta \\in \\partial(\\|u^\\dagger\\|_1)$ \n  with $|(B_\\beta^*\\eta)_i| < 1$ for every $i\\not\\in\\mathop{{\\rm supp}}(u^\\dagger)$, \n  and the restriction of the mapping $B_\\beta$ to the span of the\n  support of $u^\\dagger$ is injective\n  (see~\\cite{CPA:CPA20350}).\n  Additionally, it is easy to show that these conditions hold\n  for $B_\\beta$ provided that they hold for $A$ and $\\beta$ is\n  sufficiently large.\n  \n\\end{remark}\n\n\n\\section{Sets of Exact Support Recovery---Single-penalty Setting}\n\nThe main focus of this paper is the question whether multi-penalty\nregularization allows for the exact recovery of the support\nof the true solution $u^\\dagger$ and how it compares to single-penalty\nregularization.\nBecause, as we have seen in Lemma~\\ref{le:single}, multi-penalty\nregularization can be rewritten as single-parameter regularization\nfor the regularized operator $B_\\beta$ and right hand side $y_\\beta$,\nwe will first discuss recovery conditions in the single-parameter setting.\n\n\nIn order to find conditions for exact support recovery,\nwe first recall the necessary and sufficient optimality\ncondition for $\\ell^1$-regularization:\n\n\\begin{lemma}\\label{le:opt}\n  The vector $u_\\alpha$ minimizes\n \n", "index": 47, "text": "\\[\n  T_\\alpha(u) := \\frac{1}{2}\\|Au-y\\|_2^2 + \\alpha\\|u\\|_1,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"T_{\\alpha}(u):=\\frac{1}{2}\\|Au-y\\|_{2}^{2}+\\alpha\\|u\\|_{1},\" display=\"block\"><mrow><mrow><mrow><msub><mi>T</mi><mi>\u03b1</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>u</mi></mrow><mo>-</mo><mi>y</mi></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>u</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n\\end{lemma}\n\nUsing this result, we obtain a condition that guarantees\nexact support recovery for the single-penalty method:\n\n\n\\begin{lemma}\\label{le:support}\n  We have $\\mathop{{\\rm supp}}(u_\\alpha) = I$,\n  if and only if there exists $w_\\alpha \\in ({\\mathbb{R}}\\setminus\\{0\\})^I$ such that\n \n", "itemtype": "equation", "pos": 17968, "prevtext": "\n  if and only if\n \n", "index": 49, "text": "\\[\n  A^*(Au_\\alpha-y) \\in -\\alpha\\mathop{{\\rm sgn}}(u_\\alpha).\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m1\" class=\"ltx_Math\" alttext=\"A^{*}(Au_{\\alpha}-y)\\in-\\alpha\\mathop{{\\rm sgn}}(u_{\\alpha}).\" display=\"block\"><mrow><mrow><mrow><msup><mi>A</mi><mo>*</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>A</mi><mo>\u2062</mo><msub><mi>u</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2208</mo><mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{proof}\n  This immediately follows from Lemma~\\ref{le:opt} by\n  testing the optimality conditions on the vector $u_\\alpha$\n  given by $(u_\\alpha)_i = (w_\\alpha)_i$ for $i \\in I$\n  and $(u_\\alpha)_i = 0$ else.\n\\end{proof}\n\\medskip\n\nOur main result concerning support recovery for single-parameter\nregularization is the following:\n\n\n\\begin{proposition}\\label{pr:cond_single}\n  Assume that $A_I$ is injective and that\n  \n", "itemtype": "equation", "pos": 18329, "prevtext": "\n\\end{lemma}\n\nUsing this result, we obtain a condition that guarantees\nexact support recovery for the single-penalty method:\n\n\n\\begin{lemma}\\label{le:support}\n  We have $\\mathop{{\\rm supp}}(u_\\alpha) = I$,\n  if and only if there exists $w_\\alpha \\in ({\\mathbb{R}}\\setminus\\{0\\})^I$ such that\n \n", "index": 51, "text": "\\[\n  \\begin{aligned}\n    A_I^*(A_Iw_\\alpha-y) &= -\\alpha \\mathop{{\\rm sgn}}(w_\\alpha),\\\\\n     \\|A_J^*(A_Iw_\\alpha-y)\\|_\\infty &\\le \\alpha,\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle A_{I}^{*}(A_{I}w_{\\alpha}-y)\" display=\"inline\"><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\alpha\\mathop{{\\rm sgn}}(w_{\\alpha}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|A_{J}^{*}(A_{I}w_{\\alpha}-y)\\|_{\\infty}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\alpha,\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mi>\u03b1</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Then the set $\\mathcal{S}_{c,d,I}$ defined in~\\eqref{eq:ScdI}\n  is a set of exact support recovery for the\n  unmixing problem whenever\n  \n", "itemtype": "equation", "pos": 18925, "prevtext": "\n\\end{lemma}\n\n\\begin{proof}\n  This immediately follows from Lemma~\\ref{le:opt} by\n  testing the optimality conditions on the vector $u_\\alpha$\n  given by $(u_\\alpha)_i = (w_\\alpha)_i$ for $i \\in I$\n  and $(u_\\alpha)_i = 0$ else.\n\\end{proof}\n\\medskip\n\nOur main result concerning support recovery for single-parameter\nregularization is the following:\n\n\n\\begin{proposition}\\label{pr:cond_single}\n  Assume that $A_I$ is injective and that\n  \n", "index": 53, "text": "\\begin{equation}\\label{eq:single_inftyrestriction}\n  \\|A_J^*A_I(A_I^*A_I)^{-1}\\|_\\infty < 1.\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\|A_{J}^{*}A_{I}(A_{I}^{*}A_{I})^{-1}\\|_{\\infty}&lt;1.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>&lt;</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Moreover, every parameter $\\alpha > 0$ satisfying\n  \n", "itemtype": "equation", "pos": 19174, "prevtext": "\n  Then the set $\\mathcal{S}_{c,d,I}$ defined in~\\eqref{eq:ScdI}\n  is a set of exact support recovery for the\n  unmixing problem whenever\n  \n", "index": 55, "text": "\\begin{equation}\\label{eq:cd}\n  \\frac{c}{d} >\n  \\frac{\\|A_J^*(A_I(A_I^*A_I)^{-1}A_I^*-{\\mathrm{Id}})A\\|_\\infty\\|(A_I^*A_I)^{-1}\\|_\\infty}{1-\\|A_J^*A_I(A_I^*A_I)^{-1}\\|_\\infty}\n  + \\|(A_I^*A_I)^{-1}A_I^*A\\|_{\\infty}.\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\frac{c}{d}&gt;\\frac{\\|A_{J}^{*}(A_{I}(A_{I}^{*}A_{I})^{-1}A_{I}^{*}-{\\mathrm{Id}%&#10;})A\\|_{\\infty}\\|(A_{I}^{*}A_{I})^{-1}\\|_{\\infty}}{1-\\|A_{J}^{*}A_{I}(A_{I}^{*}%&#10;A_{I})^{-1}\\|_{\\infty}}+\\|(A_{I}^{*}A_{I})^{-1}A_{I}^{*}A\\|_{\\infty}.\" display=\"block\"><mrow><mrow><mfrac><mi>c</mi><mi>d</mi></mfrac><mo>&gt;</mo><mrow><mfrac><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac><mo>+</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  is admissible on $S_{c,d,I}$.\n\\end{proposition}\n\n\\begin{proof}\n  First we note that the injectivity of $A_I$ implies that\n  the mapping $A_I^*A_I$ is invertible. Thus the\n  condition~\\eqref{eq:single_inftyrestriction} actually makes sense.\n  Moreover, the inequality~\\eqref{eq:cd} is necessary\n  and sufficient for the existence of $\\alpha$ satisfying~\\eqref{eq:single_alpha}.\n\n  Now let $(u^\\dagger,v) \\in \\mathcal{S}_{c,d,I}$ and assume that\n  $\\alpha$ satisfies~\\eqref{eq:single_alpha}.\n  We denote\n \n", "itemtype": "equation", "pos": 19461, "prevtext": "\n  Moreover, every parameter $\\alpha > 0$ satisfying\n  \n", "index": 57, "text": "\\begin{equation}\\label{eq:single_alpha}\n  \\frac{d\\|A_J^*(A_I(A_I^*A_I)^{-1}A_I^*-{\\mathrm{Id}})A\\|_\\infty}{1-\\|A_J^*A_I(A_I^*A_I)^{-1}\\|_\\infty}\n  \\le \\alpha\n  < \\frac{c-d\\|(A_I^*A_I)^{-1}A_I^*A\\|_{\\infty}}{\\|(A_I^*A_I)^{-1}\\|_\\infty}\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\frac{d\\|A_{J}^{*}(A_{I}(A_{I}^{*}A_{I})^{-1}A_{I}^{*}-{\\mathrm{Id}})A\\|_{%&#10;\\infty}}{1-\\|A_{J}^{*}A_{I}(A_{I}^{*}A_{I})^{-1}\\|_{\\infty}}\\leq\\alpha&lt;\\frac{c%&#10;-d\\|(A_{I}^{*}A_{I})^{-1}A_{I}^{*}A\\|_{\\infty}}{\\|(A_{I}^{*}A_{I})^{-1}\\|_{%&#10;\\infty}}\" display=\"block\"><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac><mo>\u2264</mo><mi>\u03b1</mi><mo>&lt;</mo><mfrac><mrow><mi>c</mi><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and define\n \n", "itemtype": "equation", "pos": 20219, "prevtext": "\n  is admissible on $S_{c,d,I}$.\n\\end{proposition}\n\n\\begin{proof}\n  First we note that the injectivity of $A_I$ implies that\n  the mapping $A_I^*A_I$ is invertible. Thus the\n  condition~\\eqref{eq:single_inftyrestriction} actually makes sense.\n  Moreover, the inequality~\\eqref{eq:cd} is necessary\n  and sufficient for the existence of $\\alpha$ satisfying~\\eqref{eq:single_alpha}.\n\n  Now let $(u^\\dagger,v) \\in \\mathcal{S}_{c,d,I}$ and assume that\n  $\\alpha$ satisfies~\\eqref{eq:single_alpha}.\n  We denote\n \n", "index": 59, "text": "\\[\n  s_i^\\dagger := \\mathop{{\\rm sgn}}(u_i^\\dagger),\n  \\qquad\n  i \\in I,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"s_{i}^{\\dagger}:=\\mathop{{\\rm sgn}}(u_{i}^{\\dagger}),\\qquad i\\in I,\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>s</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo>:=</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>u</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mi>i</mi><mo>\u2208</mo><mi>I</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Because $\\|s^\\dagger\\|_{\\infty} = 1$, it follows from the second inequality\n  in~\\eqref{eq:single_alpha} that\n \n", "itemtype": "equation", "pos": 20312, "prevtext": "\n  and define\n \n", "index": 61, "text": "\\[\n  w_\\alpha := u_I^\\dagger + (A_I^*A_I)^{-1}A_I^*Av - \\alpha (A_I^*A_I)^{-1}s^\\dagger.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"w_{\\alpha}:=u_{I}^{\\dagger}+(A_{I}^{*}A_{I})^{-1}A_{I}^{*}Av-\\alpha(A_{I}^{*}A%&#10;_{I})^{-1}s^{\\dagger}.\" display=\"block\"><mrow><mrow><msub><mi>w</mi><mi>\u03b1</mi></msub><mo>:=</mo><mrow><mrow><msubsup><mi>u</mi><mi>I</mi><mo>\u2020</mo></msubsup><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>s</mi><mo>\u2020</mo></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and therefore\n \n", "itemtype": "equation", "pos": 20520, "prevtext": "\n  Because $\\|s^\\dagger\\|_{\\infty} = 1$, it follows from the second inequality\n  in~\\eqref{eq:single_alpha} that\n \n", "index": 63, "text": "\\[\n  |(w_\\alpha)_i - u_i^\\dagger|\n  \\le \\|(A_I^*A_I)^{-1}A_I^*A\\|_\\infty\\|v\\|_\\infty + \\alpha \\|(A_I^*A_I)^{-1}\\|_\\infty\n  < c \\le |u_i^\\dagger|,\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"|(w_{\\alpha})_{i}-u_{i}^{\\dagger}|\\leq\\|(A_{I}^{*}A_{I})^{-1}A_{I}^{*}A\\|_{%&#10;\\infty}\\|v\\|_{\\infty}+\\alpha\\|(A_{I}^{*}A_{I})^{-1}\\|_{\\infty}&lt;c\\leq|u_{i}^{%&#10;\\dagger}|,\" display=\"block\"><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub><mo>-</mo><msubsup><mi>u</mi><mi>i</mi><mo>\u2020</mo></msubsup></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><mo>&lt;</mo><mi>c</mi><mo>\u2264</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>u</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo stretchy=\"false\">|</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Thus $w_\\alpha$ actually satisfies the equation\n \n", "itemtype": "equation", "pos": 20689, "prevtext": "\n  and therefore\n \n", "index": 65, "text": "\\[\n  \\mathop{{\\rm sgn}}(w_\\alpha) = \\mathop{{\\rm sgn}}(u_I^\\dagger) = s^\\dagger.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\mathop{{\\rm sgn}}(w_{\\alpha})=\\mathop{{\\rm sgn}}(u_{I}^{\\dagger})=s^{\\dagger}.\" display=\"block\"><mrow><mrow><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>u</mi><mi>I</mi><mo>\u2020</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mi>s</mi><mo>\u2020</mo></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and thus\n \n", "itemtype": "equation", "pos": 20827, "prevtext": "\n  Thus $w_\\alpha$ actually satisfies the equation\n \n", "index": 67, "text": "\\[\n  \\begin{aligned}\n  w_\\alpha &= u_I^\\dagger + (A_I^*A_I)^{-1}A_I^*Av - \\alpha (A_I^*A_I)^{-1}\\mathop{{\\rm sgn}}(w_\\alpha)\\\\\n  &= (A_I^*A_I)^{-1}A_I^*(A_Iu_I^\\dagger + Av) - \\alpha (A_I^*A_I)^{-1}\\mathop{{\\rm sgn}}(w_\\alpha)\\\\\n  &= (A_I^*A_I)^{-1}A_I^*y- \\alpha (A_I^*A_I)^{-1}\\mathop{{\\rm sgn}}(w_\\alpha),\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle w_{\\alpha}\" display=\"inline\"><msub><mi>w</mi><mi>\u03b1</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=u_{I}^{\\dagger}+(A_{I}^{*}A_{I})^{-1}A_{I}^{*}Av-\\alpha(A_{I}^{*%&#10;}A_{I})^{-1}\\mathop{{\\rm sgn}}(w_{\\alpha})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msubsup><mi>u</mi><mi>I</mi><mo>\u2020</mo></msubsup><mo>+</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(A_{I}^{*}A_{I})^{-1}A_{I}^{*}(A_{I}u_{I}^{\\dagger}+Av)-\\alpha(A%&#10;_{I}^{*}A_{I})^{-1}\\mathop{{\\rm sgn}}(w_{\\alpha})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msubsup><mi>u</mi><mi>I</mi><mo>\u2020</mo></msubsup></mrow><mo>+</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(A_{I}^{*}A_{I})^{-1}A_{I}^{*}y-\\alpha(A_{I}^{*}A_{I})^{-1}%&#10;\\mathop{{\\rm sgn}}(w_{\\alpha}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>y</mi></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  which is the first condition in Lemma~\\ref{le:support}.\n\n  It remains to show that\n \n", "itemtype": "equation", "pos": 21170, "prevtext": "\n  and thus\n \n", "index": 69, "text": "\\[\n  A_I^*(A_Iw_\\alpha-y) = -\\alpha\\mathop{{\\rm sgn}}(w_\\alpha),\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"A_{I}^{*}(A_{I}w_{\\alpha}-y)=-\\alpha\\mathop{{\\rm sgn}}(w_{\\alpha}),\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>\u03b1</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  However,\n \n", "itemtype": "equation", "pos": 21327, "prevtext": "\n  which is the first condition in Lemma~\\ref{le:support}.\n\n  It remains to show that\n \n", "index": 71, "text": "\\[\n  \\|A_J^*(A_Iw_\\alpha-y)\\|_\\infty\\le \\alpha.\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\|A_{J}^{*}(A_{I}w_{\\alpha}-y)\\|_{\\infty}\\leq\\alpha.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2264</mo><mi>\u03b1</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  and thus\n \n", "itemtype": "equation", "pos": 21393, "prevtext": "\n  However,\n \n", "index": 73, "text": "\\[\n  \\begin{aligned}\n    A_J^*(A_Iw_\\alpha-y)\n    &= A_J^*(A_Iw_\\alpha-A_Iu_I^\\dagger-Av)\\\\\n    &= A_J^*(A_I(A_I^*A_I)^{-1}A_I^*Av-Av-\\alpha A_I(A_I^*A_I)^{-1}s^\\dagger),\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle A_{J}^{*}(A_{I}w_{\\alpha}-y)\" display=\"inline\"><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{J}^{*}(A_{I}w_{\\alpha}-A_{I}u_{I}^{\\dagger}-Av)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msubsup><mi>u</mi><mi>I</mi><mo>\u2020</mo></msubsup></mrow><mo>-</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{J}^{*}(A_{I}(A_{I}^{*}A_{I})^{-1}A_{I}^{*}Av-Av-\\alpha A_{I}(%&#10;A_{I}^{*}A_{I})^{-1}s^{\\dagger}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>-</mo><mrow><mi>A</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>s</mi><mo>\u2020</mo></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Now the first inequality in~\\eqref{eq:single_alpha} implies\n  that this term is smaller than $\\alpha$.\n  Thus $w_\\alpha$ satisfies the conditions of Lemma~\\ref{le:support},\n  and thus $\\mathop{{\\rm supp}}(u_\\alpha) = I$.\n\\end{proof}\n\n\\begin{remark}\n  In the case where $A = {\\mathrm{Id}}$ is the identity operator, the conditions\n  above reduce to the conditions that $c > 2d$ and $d \\le \\alpha < c-d$.\n  Since $\\ell^1$-regularization in this setting reduces to soft \n  thresholding, these conditions are very natural and are\n  actually both sufficient and necessary:\n  Since the noise may componentwise reach the value of $d$,\n  it is necessary to choose a regularization parameter of at least\n  $d$ in order to remove it. However, on the support $I$\n  of the signal, the smallest values of the noisy signal value\n  are at least of size $c-d$. Thus they are retained as long\n  as the regularization parameter does not exceed this value.\n\n  For more complicated operators $A$, the situation is similar, i.e., a too small regularization parameter $\\alpha$ is not\n  able to remove all the noise, while a too large one destroys\n  part of the signal as well.\n  The exact bounds for the admissible regularization parameters,\n  however, are much more complicated.\n\\end{remark}\n\n\n\\section{Sets of Exact Support Recovery---Multi-Penalty Setting}\n\nWe now consider the setting of multi-penalty regularization for the\nsolution of the unmixing problem. Applying Lemma~\\ref{le:single}, we\ncan treat multi-penalty regularization with the same methods as\nsingle-penalty regularization.\nTo that end, we introduce the regularized operator\n\n", "itemtype": "equation", "pos": 21598, "prevtext": "\n  and thus\n \n", "index": 75, "text": "\\[\n  \\begin{aligned}\n  \\|A_J^*(A_Iw_\\alpha-y)\\|_\\infty\n  &\\le d\\|A_J^*(A_I(A_I^*A_I)^{-1}A_I^*-{\\mathrm{Id}})A\\|_\\infty\\\\\n  &\\qquad\\qquad\\qquad\\qquad+ \\alpha\\|A_J^*A_I(A_I^*A_I)^{-1}\\|_\\infty.\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|A_{J}^{*}(A_{I}w_{\\alpha}-y)\\|_{\\infty}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq d\\|A_{J}^{*}(A_{I}(A_{I}^{*}A_{I})^{-1}A_{I}^{*}-{\\mathrm{Id%&#10;}})A\\|_{\\infty}\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mi>d</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\alpha\\|A_{J}^{*}A_{I}(A_{I}^{*}A_{I})^{%&#10;-1}\\|_{\\infty}.\" display=\"inline\"><mrow><mrow><mo lspace=\"82.5pt\">+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nIn particular, we have with the notation of Lemma~\\ref{le:support}\nthat\n$B_\\beta^* B = A_\\beta^*A$ and $B_\\beta^*y_\\beta = A_\\beta^*y$.\n\nAs a first result, we obtain the following\nanalogon to Lemma~\\ref{le:support}:\n\n\\begin{lemma}\\label{le:support_multi}\n  We have $\\mathop{{\\rm supp}}(u_{\\alpha,\\beta}) = I$,\n  if and only if there exists $w_\\alpha \\in ({\\mathbb{R}}\\setminus\\{0\\})^I$ such that\n \n", "itemtype": "equation", "pos": 23436, "prevtext": "\n  Now the first inequality in~\\eqref{eq:single_alpha} implies\n  that this term is smaller than $\\alpha$.\n  Thus $w_\\alpha$ satisfies the conditions of Lemma~\\ref{le:support},\n  and thus $\\mathop{{\\rm supp}}(u_\\alpha) = I$.\n\\end{proof}\n\n\\begin{remark}\n  In the case where $A = {\\mathrm{Id}}$ is the identity operator, the conditions\n  above reduce to the conditions that $c > 2d$ and $d \\le \\alpha < c-d$.\n  Since $\\ell^1$-regularization in this setting reduces to soft \n  thresholding, these conditions are very natural and are\n  actually both sufficient and necessary:\n  Since the noise may componentwise reach the value of $d$,\n  it is necessary to choose a regularization parameter of at least\n  $d$ in order to remove it. However, on the support $I$\n  of the signal, the smallest values of the noisy signal value\n  are at least of size $c-d$. Thus they are retained as long\n  as the regularization parameter does not exceed this value.\n\n  For more complicated operators $A$, the situation is similar, i.e., a too small regularization parameter $\\alpha$ is not\n  able to remove all the noise, while a too large one destroys\n  part of the signal as well.\n  The exact bounds for the admissible regularization parameters,\n  however, are much more complicated.\n\\end{remark}\n\n\n\\section{Sets of Exact Support Recovery---Multi-Penalty Setting}\n\nWe now consider the setting of multi-penalty regularization for the\nsolution of the unmixing problem. Applying Lemma~\\ref{le:single}, we\ncan treat multi-penalty regularization with the same methods as\nsingle-penalty regularization.\nTo that end, we introduce the regularized operator\n\n", "index": 77, "text": "\\begin{equation}\\label{eq:Abeta}\nA_\\beta := \\Bigl( I + \\frac{AA^*}{\\beta}\\Bigr)^{-1}A.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"A_{\\beta}:=\\Bigl{(}I+\\frac{AA^{*}}{\\beta}\\Bigr{)}^{-1}A.\" display=\"block\"><mrow><mrow><msub><mi>A</mi><mi>\u03b2</mi></msub><mo>:=</mo><mrow><msup><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mi>I</mi><mo>+</mo><mfrac><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>A</mi><mo>*</mo></msup></mrow><mi>\u03b2</mi></mfrac></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>A</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n\\end{lemma}\n\n\\begin{proof}\n  Applying Lemma~\\ref{le:opt} to the single-penalty\n  problem~\\eqref{eq:minu}, we obtain the conditions\n \n", "itemtype": "equation", "pos": 23936, "prevtext": "\nIn particular, we have with the notation of Lemma~\\ref{le:support}\nthat\n$B_\\beta^* B = A_\\beta^*A$ and $B_\\beta^*y_\\beta = A_\\beta^*y$.\n\nAs a first result, we obtain the following\nanalogon to Lemma~\\ref{le:support}:\n\n\\begin{lemma}\\label{le:support_multi}\n  We have $\\mathop{{\\rm supp}}(u_{\\alpha,\\beta}) = I$,\n  if and only if there exists $w_\\alpha \\in ({\\mathbb{R}}\\setminus\\{0\\})^I$ such that\n \n", "index": 79, "text": "\\[\n  \\begin{aligned}\n    A_{\\beta,I}^*(A_Iw_{\\alpha,\\beta}-y) &= -\\alpha \\mathop{{\\rm sgn}}(w_{\\alpha,\\beta}),\\\\\n    \\|A_{\\beta,J}^*(A_Iw_\\alpha-y)\\|_\\infty &\\le \\alpha,\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle A_{\\beta,I}^{*}(A_{I}w_{\\alpha,\\beta}-y)\" display=\"inline\"><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\alpha\\mathop{{\\rm sgn}}(w_{\\alpha,\\beta}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|A_{\\beta,J}^{*}(A_{I}w_{\\alpha}-y)\\|_{\\infty}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msub><mi>w</mi><mi>\u03b1</mi></msub></mrow><mo>-</mo><mi>y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\alpha,\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mi>\u03b1</mi></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Now the claim follows from the equalities\n \n", "itemtype": "equation", "pos": 24260, "prevtext": "\n\\end{lemma}\n\n\\begin{proof}\n  Applying Lemma~\\ref{le:opt} to the single-penalty\n  problem~\\eqref{eq:minu}, we obtain the conditions\n \n", "index": 81, "text": "\\[\n  \\begin{aligned}\n    B_{\\beta,I}^*(B_{\\beta,I}w_{\\alpha,\\beta}-y_\\beta) &= -\\alpha \\mathop{{\\rm sgn}}(w_{\\alpha,\\beta}),\\\\\n    \\|B_{\\beta,J}^*(B_{\\beta,I}w_{\\alpha,\\beta}-y_\\beta)\\|_\\infty &\\le \\alpha.\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle B_{\\beta,I}^{*}(B_{\\beta,I}w_{\\alpha,\\beta}-y_{\\beta})\" display=\"inline\"><mrow><msubsup><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow></msub><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub></mrow><mo>-</mo><msub><mi>y</mi><mi>\u03b2</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=-\\alpha\\mathop{{\\rm sgn}}(w_{\\alpha,\\beta}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo movablelimits=\"false\">sgn</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\|B_{\\beta,J}^{*}(B_{\\beta,I}w_{\\alpha,\\beta}-y_{\\beta})\\|_{\\infty}\" display=\"inline\"><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow></msub><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi></mrow></msub></mrow><mo>-</mo><msub><mi>y</mi><mi>\u03b2</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\alpha.\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mi>\u03b1</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n\\end{proof}\n\nSince the proof of Proposition~\\ref{pr:cond_single} only\ndepends on the optimality conditions and the representation\nof the data as $y = Au^\\dagger + Av$, we immediately\nobtain a generalization of Proposition~\\ref{pr:cond_single}\nto the multi-penalty setting.\n\n\n\\begin{proposition}\\label{pr:cond_multi}\n  Assume that $0 < \\beta < \\infty$ is such that\n  \n", "itemtype": "equation", "pos": 24533, "prevtext": "\n  Now the claim follows from the equalities\n \n", "index": 83, "text": "\\[\n  \\begin{aligned}\n  B_{\\beta,I}^*B_{\\beta,I} &= A_{\\beta,I}^*A_I,&\n  B_{\\beta,I}^*y_\\beta &= A_{\\beta,I}^*y,\\\\\n  B_{\\beta,I}^*B_{\\beta,I} &= A_{\\beta,I}^*A_I,&\n  B_{\\beta,I}^*y_\\beta &= A_{\\beta,I}^*y.\n  \\end{aligned}\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle B_{\\beta,I}^{*}B_{\\beta,I}\" display=\"inline\"><mrow><msubsup><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{\\beta,I}^{*}A_{I},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28X.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hskip 10.0ptB_{\\beta,I}^{*}y_{\\beta}\" display=\"inline\"><mrow><msubsup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>B</mi></mpadded><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>y</mi><mi>\u03b2</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28X.m6\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{\\beta,I}^{*}y,\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>y</mi></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle B_{\\beta,I}^{*}B_{\\beta,I}\" display=\"inline\"><mrow><msubsup><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>B</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{\\beta,I}^{*}A_{I},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28Xa.m5\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hskip 10.0ptB_{\\beta,I}^{*}y_{\\beta}\" display=\"inline\"><mrow><msubsup><mpadded lspace=\"10pt\" width=\"+10pt\"><mi>B</mi></mpadded><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>y</mi><mi>\u03b2</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28Xa.m6\" class=\"ltx_Math\" alttext=\"\\displaystyle=A_{\\beta,I}^{*}y.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>y</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Then the set $\\mathcal{S}_{c,d,I}$ is a set of exact support recovery for the\n  unmixing problem in the multi-penalty setting whenever\n  \n", "itemtype": "equation", "pos": 25125, "prevtext": "\n\\end{proof}\n\nSince the proof of Proposition~\\ref{pr:cond_single} only\ndepends on the optimality conditions and the representation\nof the data as $y = Au^\\dagger + Av$, we immediately\nobtain a generalization of Proposition~\\ref{pr:cond_single}\nto the multi-penalty setting.\n\n\n\\begin{proposition}\\label{pr:cond_multi}\n  Assume that $0 < \\beta < \\infty$ is such that\n  \n", "index": 85, "text": "\\begin{equation}\\label{eq:cond_multi_nec}\n  \\|A_{\\beta,J}^*A_{I}(A_{\\beta,I}^*A_{I})^{-1}\\|_\\infty < 1.\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\|A_{\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}&lt;1.\" display=\"block\"><mrow><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>&lt;</mo><mn>1</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  Moreover, all the pairs of parameter $(\\alpha,\\beta)$\n  satisfying~\\eqref{eq:cond_multi_1} and\n  \n", "itemtype": "equation", "pos": 25385, "prevtext": "\n  Then the set $\\mathcal{S}_{c,d,I}$ is a set of exact support recovery for the\n  unmixing problem in the multi-penalty setting whenever\n  \n", "index": 87, "text": "\\begin{multline}\n    \\frac{c}{d} >\\|(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*A\\|_{\\infty}\\\\\n    + \\frac{\\|A_{\\beta,J}^*(A_I(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*-{\\mathrm{Id}})A\\|_\\infty\n      \\|(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}{1-\\|A_{\\beta,J}^*A_I(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}.\n    \\label{eq:cond_multi_1}\n  \\end{multline}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{c}{d}&gt;\\|(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}A\\|_{%&#10;\\infty}\\\\&#10;\\displaystyle+\\frac{\\|A_{\\beta,J}^{*}(A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta%&#10;,I}^{*}-{\\mathrm{Id}})A\\|_{\\infty}\\|(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}}{1-%&#10;\\|A_{\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}}.\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mfrac><mi>c</mi><mi>d</mi></mfrac><mo>&gt;</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mo>+</mo><mfrac><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  are admissible on $\\mathcal{S}_{c,d,I}$.\n\\end{proposition}\n\n\\begin{proof}\n  The proof is analogous to the proof of Proposition~\\ref{pr:cond_single}.\n\\end{proof}\n\n\\begin{remark}\n  We note that the condition\n \n", "itemtype": "equation", "pos": 25806, "prevtext": "\n  Moreover, all the pairs of parameter $(\\alpha,\\beta)$\n  satisfying~\\eqref{eq:cond_multi_1} and\n  \n", "index": 89, "text": "\\begin{equation}\\label{eq:multi_alpha}\n  \\frac{d\\|A_{\\beta,J}^*(A_I(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*-{\\mathrm{Id}})A\\|_\\infty}{1-\\|A_{\\beta,J}^*A_I(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}\n  \\le \\alpha\n  < \\frac{c-d\\|(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*A\\|_{\\infty}}{\\|(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}\n  \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\frac{d\\|A_{\\beta,J}^{*}(A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}-{%&#10;\\mathrm{Id}})A\\|_{\\infty}}{1-\\|A_{\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}%&#10;\\|_{\\infty}}\\leq\\alpha&lt;\\frac{c-d\\|(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}A%&#10;\\|_{\\infty}}{\\|(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}}\" display=\"block\"><mrow><mfrac><mrow><mi>d</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac><mo>\u2264</mo><mi>\u03b1</mi><mo>&lt;</mo><mfrac><mrow><mi>c</mi><mo>-</mo><mrow><mi>d</mi><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\n  implies the analogous inequality for $A_\\beta$ provided that $\\beta$\n  is sufficiently large. Similarly, if $\\alpha$ satisfies the\n  conditions in Proposition~\\ref{pr:cond_single} that guarantee\n  admissibility on $S_{c,d,I}$, the pair $(\\alpha,\\beta)$ will satisfy\n  the conditions for admissibility in Proposition~\\ref{pr:cond_multi}\n  provided that $\\beta$ is sufficiently large. The converse, however,\n  need not be true: If the pair $(\\alpha,\\beta)$ is admissible for exact\n  support recovery on $\\mathcal{S}_{c,d,k}$ with multi-penalty\n  regularization, it need not be true that the single parameter\n  $\\alpha$ is admissble for the single-penalty setting as well.\n  Examples where this actually happens can be found in Section~\\ref{se:valid}\n  (see in particular Table~\\ref{tb:cond}).\n\\end{remark}\n\n\n\\section{Admissible parameters}\n\nAs a consequence of Propositions~\\ref{pr:cond_single} and~\\ref{pr:cond_multi},\nwe obtain that the condition\n\n", "itemtype": "equation", "pos": 26331, "prevtext": "\n  are admissible on $\\mathcal{S}_{c,d,I}$.\n\\end{proposition}\n\n\\begin{proof}\n  The proof is analogous to the proof of Proposition~\\ref{pr:cond_single}.\n\\end{proof}\n\n\\begin{remark}\n  We note that the condition\n \n", "index": 91, "text": "\\[\n  \\|A_J^*A_I(A_I^*A_I)^{-1}\\|_\\infty < 1\n  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"\\|A_{J}^{*}A_{I}(A_{I}^{*}A_{I})^{-1}\\|_{\\infty}&lt;1\" display=\"block\"><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mi>J</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mi>I</mi><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>&lt;</mo><mn>1</mn></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nis sufficient for $S_{c,d,k}$ to be a set of exact support recovery\nfor the unmixing problem, provided that the ratio $c/d$ is sufficiently large;\nthe condition for the single-parameter case\ncan be extracted from~\\eqref{eq:cond_k} by setting $\\beta = \\infty$,\nin which case $A_\\beta$ reduces to $A$.\n\nNow define the signal-to-noise ratio of a pair $(u,v)$ as\n", "itemtype": "equation", "pos": 27329, "prevtext": "\n  implies the analogous inequality for $A_\\beta$ provided that $\\beta$\n  is sufficiently large. Similarly, if $\\alpha$ satisfies the\n  conditions in Proposition~\\ref{pr:cond_single} that guarantee\n  admissibility on $S_{c,d,I}$, the pair $(\\alpha,\\beta)$ will satisfy\n  the conditions for admissibility in Proposition~\\ref{pr:cond_multi}\n  provided that $\\beta$ is sufficiently large. The converse, however,\n  need not be true: If the pair $(\\alpha,\\beta)$ is admissible for exact\n  support recovery on $\\mathcal{S}_{c,d,k}$ with multi-penalty\n  regularization, it need not be true that the single parameter\n  $\\alpha$ is admissble for the single-penalty setting as well.\n  Examples where this actually happens can be found in Section~\\ref{se:valid}\n  (see in particular Table~\\ref{tb:cond}).\n\\end{remark}\n\n\n\\section{Admissible parameters}\n\nAs a consequence of Propositions~\\ref{pr:cond_single} and~\\ref{pr:cond_multi},\nwe obtain that the condition\n\n", "index": 93, "text": "\\begin{equation}\\label{eq:cond_k}\n\\sup_{|I| \\le k} \\|A_{\\beta,J}^*A_{I}(A_{\\beta,I}^*A_{I})^{-1}\\|_\\infty < 1\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\sup_{|I|\\leq k}\\|A_{\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}&lt;1\" display=\"block\"><mrow><mrow><munder><mo movablelimits=\"false\">sup</mo><mrow><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mo>&lt;</mo><mn>1</mn></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nThat is, $\\rho(u,v)$ is the ratio of the smallest significant value\nof the signal $u$, and the largest value of the noise $v$.\nDenote moreover\n\n", "itemtype": "equation", "pos": 27812, "prevtext": "\nis sufficient for $S_{c,d,k}$ to be a set of exact support recovery\nfor the unmixing problem, provided that the ratio $c/d$ is sufficiently large;\nthe condition for the single-parameter case\ncan be extracted from~\\eqref{eq:cond_k} by setting $\\beta = \\infty$,\nin which case $A_\\beta$ reduces to $A$.\n\nNow define the signal-to-noise ratio of a pair $(u,v)$ as\n", "index": 95, "text": "\n\\[\n\\rho(u,v) := \\frac{\\inf\\bigl\\{|u_i| : i \\in \\mathop{{\\rm supp}}(u)\\bigr\\}}{\\|v\\|_\\infty}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex30.m1\" class=\"ltx_Math\" alttext=\"\\rho(u,v):=\\frac{\\inf\\bigl{\\{}|u_{i}|:i\\in\\mathop{{\\rm supp}}(u)\\bigr{\\}}}{\\|v%&#10;\\|_{\\infty}}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mfrac><mrow><mo>inf</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>:</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mo>supp</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">}</mo></mrow></mrow><msub><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nThen the inequality~\\eqref{eq:cond_multi_1} implies that\nmulti-penalty regularization with parameter $\\beta$\nallows for the recovery of the support of\n$k$-sparse vectors $u$ from data $A(u+v)$ provided the signal-to-noise ratio\nof the pair $(u,v)$ satisfies\n", "itemtype": "equation", "pos": 28052, "prevtext": "\nThat is, $\\rho(u,v)$ is the ratio of the smallest significant value\nof the signal $u$, and the largest value of the noise $v$.\nDenote moreover\n\n", "index": 97, "text": "\\begin{multline*}\nR(\\beta,k) := \\max_{|I|\\le k} \\Biggl\\{\\frac{\\|A_{\\beta,J}^*(A_I(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*-{\\mathrm{Id}})A\\|_\\infty\n      \\|(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}{1-\\|A_{\\beta,J}^*A_I(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}\\\\\n   + \\|(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*A\\|_{\\infty}\\Biggr\\}.\n\\end{multline*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"p49.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle R(\\beta,k):=\\max_{|I|\\leq k}\\Biggl{\\{}\\frac{\\|A_{\\beta,J}^{*}(A_%&#10;{I}(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}-{\\mathrm{Id}})A\\|_{\\infty}\\|(A_{%&#10;\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}}{1-\\|A_{\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_%&#10;{I})^{-1}\\|_{\\infty}}\\\\&#10;\\displaystyle+\\|(A_{\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}A\\|_{\\infty}\\Biggr{%&#10;\\}}.\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mi>R</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo>,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><mo>:=</mo><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mrow><mo maxsize=\"260%\" minsize=\"260%\">{</mo><mfrac><mrow><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mo>+</mo><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><msub><mi>A</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mi>A</mi><msub><mo>\u2225</mo><mi mathvariant=\"normal\">\u221e</mi></msub><mo maxsize=\"260%\" minsize=\"260%\">}</mo><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nWhenever the signal-to-noise ratio is larger than $R_{\\beta,k}$,\nwe can recover the support of the vector $u$ with \\emph{some}\nregularization parameter $\\alpha$.\nThere are, however, upper and lower limits for the admissible\nparameters $\\alpha$, given by inequality~\\eqref{eq:multi_alpha}.\nIn order to visualize them, we consider instead the ratio\n", "itemtype": "equation", "pos": 28631, "prevtext": "\nThen the inequality~\\eqref{eq:cond_multi_1} implies that\nmulti-penalty regularization with parameter $\\beta$\nallows for the recovery of the support of\n$k$-sparse vectors $u$ from data $A(u+v)$ provided the signal-to-noise ratio\nof the pair $(u,v)$ satisfies\n", "index": 99, "text": "\n\\[\n\\rho(u,v) > R_{\\beta,k}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex31.m1\" class=\"ltx_Math\" alttext=\"\\rho(u,v)&gt;R_{\\beta,k}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><msub><mi>R</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nDefining\n", "itemtype": "equation", "pos": 29009, "prevtext": "\nWhenever the signal-to-noise ratio is larger than $R_{\\beta,k}$,\nwe can recover the support of the vector $u$ with \\emph{some}\nregularization parameter $\\alpha$.\nThere are, however, upper and lower limits for the admissible\nparameters $\\alpha$, given by inequality~\\eqref{eq:multi_alpha}.\nIn order to visualize them, we consider instead the ratio\n", "index": 101, "text": "\n\\[\n\\theta(\\alpha,v) := \\frac{\\alpha}{\\|v\\|_\\infty}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex32.m1\" class=\"ltx_Math\" alttext=\"\\theta(\\alpha,v):=\\frac{\\alpha}{\\|v\\|_{\\infty}}.\" display=\"block\"><mrow><mrow><mrow><mi>\u03b8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mfrac><mi>\u03b1</mi><msub><mrow><mo>\u2225</mo><mi>v</mi><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nand\n", "itemtype": "equation", "pos": 29073, "prevtext": "\nDefining\n", "index": 103, "text": "\n\\[\n\\Theta^{\\min}_{\\beta,k} := \\max_{|I|\\le k}\n\\frac{\\|A_{\\beta,J}^*(A_I(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*-{\\mathrm{Id}})A\\|_\\infty}{1-\\|A_{\\beta,J}^*A_I(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex33.m1\" class=\"ltx_Math\" alttext=\"\\Theta^{\\min}_{\\beta,k}:=\\max_{|I|\\leq k}\\frac{\\|A_{\\beta,J}^{*}(A_{I}(A_{%&#10;\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}-{\\mathrm{Id}})A\\|_{\\infty}}{1-\\|A_{%&#10;\\beta,J}^{*}A_{I}(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}}\" display=\"block\"><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow><mi>min</mi></msubsup><mo>:=</mo><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mfrac><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup></mrow><mo>-</mo><mi>Id</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub><mrow><mn>1</mn><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>J</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nwe then obtain the condition\n\n", "itemtype": "equation", "pos": 29268, "prevtext": "\nand\n", "index": 105, "text": "\n\\[\n\\Theta^{\\max}_{\\beta,k}(\\vartheta)\n:= \\min_{|I|\\le k}\\frac{\\vartheta-\\|(A_{\\beta,I}^*A_I)^{-1}A_{\\beta,I}^*A\\|_{\\infty}}{\\|(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty},\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex34.m1\" class=\"ltx_Math\" alttext=\"\\Theta^{\\max}_{\\beta,k}(\\vartheta):=\\min_{|I|\\leq k}\\frac{\\vartheta-\\|(A_{%&#10;\\beta,I}^{*}A_{I})^{-1}A_{\\beta,I}^{*}A\\|_{\\infty}}{\\|(A_{\\beta,I}^{*}A_{I})^{%&#10;-1}\\|_{\\infty}},\" display=\"block\"><mrow><mrow><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow><mi>max</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03d1</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><munder><mi>min</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2061</mo><mfrac><mrow><mi>\u03d1</mi><mo>-</mo><msub><mrow><mo>\u2225</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><mi>A</mi></mrow><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mfrac></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nfor exact support recovery.\nIf the ratio $\\theta(\\alpha,v)$ is smaller than $\\Theta^{\\min}_{\\beta,k}$,\nthen it can happen that some of the noise $v$ is not filtered\nout by the regularization method.\nOn the other hand, if $\\theta(\\alpha,v)$ is larger than\n$\\Theta^{\\max}_{\\beta,k}\\bigl(\\rho(u,v)\\bigr)$, then some parts of\nthe signal $u^\\dagger$ might actually be lost because of\nthe regularization.\n\nWe note that the function $\\Theta^{\\max}_{\\beta,k}$ is piecewise linear\nand concave, and $\\lim_{\\vartheta\\to \\infty}\\Theta^{\\max}_{\\beta,k}(\\vartheta) = +\\infty$.\nThus the region of admissible parameters defined by~\\eqref{eq:theta}\nis a convex and unbounded polyhedron.\nMoreover, we have that $\\Theta_{\\beta,k}^{\\max}(R_{\\beta,k}) = \\Theta_{\\beta,k}^{\\min}$.\nAdditionally, we note that the behaviour of the function $\\Theta_{\\beta,k}^{\\max}$\nnear infinity is determined by the term\n", "itemtype": "equation", "pos": 29462, "prevtext": "\nwe then obtain the condition\n\n", "index": 107, "text": "\\begin{equation}\\label{eq:theta}\n\\Theta^{\\min}_{\\beta,k} \\le \\theta(\\alpha,v) < \\Theta^{\\max}_{\\beta,k}\\bigl(\\rho(u,v)\\bigr)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\Theta^{\\min}_{\\beta,k}\\leq\\theta(\\alpha,v)&lt;\\Theta^{\\max}_{\\beta,k}\\bigl{(}%&#10;\\rho(u,v)\\bigr{)}\" display=\"block\"><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow><mi>min</mi></msubsup><mo>\u2264</mo><mrow><mi>\u03b8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03b1</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mrow><msubsup><mi mathvariant=\"normal\">\u0398</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow><mi>max</mi></msubsup><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nIf this value is small, then the slope of the function\n$\\Theta_{\\beta,k}^{\\max}(\\vartheta)$ for large values of $\\vartheta$\nis large, and thus the set of admissible parameter grows\nfast with increasing signal-to-noise ratio.\nIf, on the other hand, $\\Sigma_{\\beta,k}$ is large, then the\nset of admissible parameters is relatively small\neven for large signal-to-noise ratio.\nThus $\\Sigma_{\\beta,k}$ can be reasonably interpreted as\nthe sensitivity of multi-parameter regularization with\nrespect to parameter choice.\nThe larger $\\Sigma_{\\beta,k}$ is, the more precise the parameter $\\alpha$\nhas to be chosen in order to guarantee exact support recovery.\n\n\n\\section{Numerical Validation}\\label{se:valid}\n\nThe main motive behind the study and application of multi-penalty\nregularization is the problem that $\\ell^1$-regularization is often\nnot capable to identify the support of signal correctly (see \\cite{arfopeXX} and references therein).\nIncluding the additional $\\ell^2$-regularization term, however, might\nlead to an improved performance in terms of support recovery, because\nwe can expect that the $\\ell^2$-term takes care of all the small noise\ncomponents. \n\nIn order to verify  this observation, a series of numerical experiments was performed, in which we illustrate for which parameters and Gaussian matrices the conditions for support recovery derived in the previous\nsection were satisfied. In addition, we studied whether the\ninclusion of the $\\ell^2$-term indeed increases the performance.\n\nIn a first set of experiments, we have generated a set of 20 Gaussian\nrandom matrices of different sizes and have tested for each\nthree-dimensional subspace spanned by the basis elements whether the\ncondition~\\eqref{eq:cond_multi_nec} is satisfied, first for\nthe single-penalty case, and then for the multi-penalty case with\ndifferent values of $\\beta$.\nThe results for matrices of dimensions 30 times 60 and 40 times 80,\nrespectively, are summarized in Table~\\ref{tb:cond} and\nFigure~\\ref{fi:cond}.\n\nAs to be expected from the bad numerical performance of\n$\\ell^1$-regularization in terms of support recovery, the\ninequality~\\eqref{eq:single_inftyrestriction} fails in a relatively\nlarge number of cases, especially when the discrepancy between the\ndimension $N$ of the vectors to be recovered and the number of\nmeasurements $m$ is quite large. For instance, in the case $N=60$ and\n$m=30$, the condition most of the time failed for more than half of\nthe three-dimensional subspaces. In contrast, the corresponding\ncondition~\\eqref{eq:cond_multi_nec} for multi-parameter regularization\nfails in the same situation only for about an eighth of the subspaces\nif $\\beta = 1$, and in even fewer cases for $\\beta = 0.1$.\n\nFor other combinations of dimensionality of the problem and number of\nmeasurements, the situation is similar. Introducing the additional\n$\\ell^2$-penalty term always allows for the exact support\nreconstruction on a larger number of subspaces than single-penalty\nregularization. Additionally, the results indicate that the number of\nrecoverable subspaces increases with decreasing $\\beta$.\n\n\n\\begin{table}[h]\n\\begin{tabular}{r||c|ccc}\n$m=30$  &\\ Single-penalty\\ {} & \\multicolumn{3}{c}{Multi-penalty}\\\\\n$N=60$ & &\\quad $\\beta = 10$ \\quad{} &\\quad $\\beta = 1$\\quad{} &\\quad $\\beta = 0.1$\\quad{} \\\\\n  \\hline\nMedian & 0.5425 & 0.3814 & 0.1214 & 0.0623\\\\\nMean & 0.5559 & 0.3922 & 0.1225 & 0.0635\\\\\nStd.~deviation & 0.05652 & 0.04142 & 0.01518 & 0.01083\\\\\n\\end{tabular}\n\\medskip\n\n\\begin{tabular}{r||c|ccc}\n$m=40$  &\\ Single-penalty\\ {} & \\multicolumn{3}{c}{Multi-penalty}\\\\\n$N=80$  & &\\quad $\\beta = 10$ \\quad{} &\\quad $\\beta = 1$\\quad{} &\\quad $\\beta = 0.1$\\quad{} \\\\\n  \\hline\nMedian & 0.2696 & 0.1523 & 0.0396 & 0.0256\\\\\nMean &   0.2746 & 0.1547 & 0.0413 & 0.0262\\\\\nStd.~deviation & 0.03060 & 0.01848 & 0.00659 & 0.00447 \\\\\n\\end{tabular}\n\\caption{\\label{tb:cond}\n    Percentage of 3-sparse subspaces for which the condition~\\eqref{eq:cond_multi_nec}\n  failed. The condition was tested on samples of 20 Gaussian random matrices\n  of dimensions 30 times 60 (upper table) and 40 times 80 (lower table).\n  Other combinations of dimensionality and number of measurements showed\n  qualitatively similar results.\n}\n\\end{table}\n\n\\begin{figure}[h]\n  \\includegraphics[width=0.45\\textwidth]{./meas30_60.eps}\n  \\includegraphics[width=0.45\\textwidth]{./meas30_80.eps}\\\\\n  \\includegraphics[width=0.45\\textwidth]{./meas40_60.eps}\n  \\includegraphics[width=0.45\\textwidth]{./meas40_80.eps}\n  \\caption{\\label{fi:cond}\n    Percentage of 3-sparse subspaces for which the condition~\\eqref{eq:cond_multi_nec}\n    failed. For each of the different settings the condition\n    has been tested on 20 Gaussian random matrices\n    for multi-parameter regularization with $\\beta = 0.1$,\n    $\\beta = 1$ and $\\beta = 10$, and for single parameter\n    regularization.\n    \\emph{Upper left:} Dimension 30 times 60.\n    \\emph{Upper right:} Dimension 30 times 80.\n    \\emph{Lower left:} Dimension 40 times 60.\n    \\emph{Lower right:} Dimension 40 times 80.} \n\\end{figure}\n\nIn the case where $N = 80$ and $m=60$ (that is, we want to reconstruct\n$80$-dimensional vectors from 60 measurements), the sufficient\ncondition~\\eqref{eq:cond_multi_nec} for multi-penalty regularization\nwas satisfied in our numerical experiments for all $3$-sparse\nsubspaces for parameters $\\beta$ smaller than 5.\nIn this situation, we have therefore additionally computed the\nsignificant values $R_{\\beta,3}$ and $\\Sigma_{\\beta,3}$,\nthat is, the minimal recoverable signal-to-noise ratio and the\nparameter sensitivity for three-dimensional subspaces.\nThe results of these numerical experiments are shown in\nTable~\\ref{tb:beta} and Figure~\\ref{fi:beta}.\n\nThe results indicate that decreasing the regularization parameter\n$\\beta$ tends to decrease the necessary signal-to-noise ratio\n$R_{\\beta,k}$ as well. While a regularization parameter $\\beta = 5$\nrequired always an unreasonably large signal-to-noise ratio in order\nto guarantee the recoverability of the support, the ratios for $\\beta\n= 0.5$ or $\\beta = 0.1$ turned out to be much more\nreasonable. However, the results also showed that decreasing the\nregularization parameter need not necessarily have a beneficial effect\non the recoverability. For some matrices it happened that the\nnecessary signal-to-noise ratio $R_{\\beta,k}$ increased while the\nregularization parameter $\\beta$ was decreased\n(see Figure~\\ref{fi:beta}, left).\n\nAdditionally, the results indicate that the parameter sensitivity\n$\\Sigma_{\\beta,k}$ increases considerably as $\\beta$ decreases (see\nFigure~\\ref{fi:beta}, right). As a consequence, the range of\nadmissible parameters $\\alpha$ tends to be significantly smaller for\nsmaller $\\beta$, and it can happen much more easily that the combined\neffect of $\\ell^2$ and $\\ell^1$-regularization leads to a\nclassification of signals as noise. While multi-penalty regularization\nwith a small parameter $\\beta$ might therefore lead to a better\nnecessary signal-to-noise ratio for recovery, it requires at the same\ntime a better balance between the two involved parameters $\\alpha$ and\n$\\beta$.\n\n\n\n\n\\begin{figure}[h]\n  \\includegraphics[width=0.45\\textwidth]{./R_60_80_beta01_1.eps}\n  \\includegraphics[width=0.45\\textwidth]{./S_60_80_beta01_1.eps}\n  \\caption{\\label{fi:beta}\n    Influence of the regularization parameter $\\beta$\n    on the set of exact support recovery and the\n    admissible parameters.\n    \\emph{Left:} Logarithm of base 10 of the recoverable\n    signal-to-noise ratio $R_{\\beta,3}$ for different values\n    of $\\beta$.\n    \\emph{Right:} The sensitivity $\\Sigma_{\\beta,3}$ for different\n    values of $\\beta$.\\newline\n    The dimension of the matrix is in all cases 60 times 80,\n    and 20 Gaussian random matrices have been\n    used for each parameter $\\beta$.} \n\\end{figure}\n\n\n\\begin{table}[h]\n  \\begin{tabular}{rr|rrrrr}\n    & &\\multicolumn{5}{c}{$\\beta$}\\\\\n    & & $0.1$ & $0.3$ & $0.5$\\hfil & $1$\\hfil & $5$\\hfil \\\\\n    \\hline\n    & minimum & 1.623 & 5.961 & 12.05 & 49.14 & 3599.3 \\\\\n    $R_{\\beta,3}$ & median & 2.252 & 11.84 & 28.17 & 205.7 & 15041.4 \\\\\n    & maximum & 5.546 & 262.5 & 302.85 & 6178.4 & 203621.6\\\\\n    \\hline\n    & minimum & 31.07 & 13.35 & 9.379 & 6.517 & 3.793 \\\\\n    $\\Sigma_{\\beta,3}$ & median & 35.17 & 15.31 & 11.086 & 7.517 & 4.278 \\\\\n    & maximum & 44.00 & 19.02 & 13.474 & 9.154 & 4.977 \\\\\n  \\end{tabular}\n  \\caption{\\label{tb:beta}\n    Influence of the parameter $\\beta$ on the values\n    of $R_{\\beta,k}$ and $\\Sigma_{\\beta,k}$. The values have been\n    computed for 20 Gaussian random matrices of dimension\n    60 times 80 with $k=3$.}\n\\end{table}\n\n\\section{Applications. Numerical experiments}\n\n\nIn order to support our theoretical findings even further, we present in this\nsection some statistical data obtained by solving series of\ncompressive sensing problems  by means of  multi-penalty  and\n$\\ell^1$-regularization. Similarly to \\cite{naumpeter, arfopeXX} we\nconsider in our numerical experiments the model problem of the type \n", "itemtype": "equation", "pos": 30483, "prevtext": "\nfor exact support recovery.\nIf the ratio $\\theta(\\alpha,v)$ is smaller than $\\Theta^{\\min}_{\\beta,k}$,\nthen it can happen that some of the noise $v$ is not filtered\nout by the regularization method.\nOn the other hand, if $\\theta(\\alpha,v)$ is larger than\n$\\Theta^{\\max}_{\\beta,k}\\bigl(\\rho(u,v)\\bigr)$, then some parts of\nthe signal $u^\\dagger$ might actually be lost because of\nthe regularization.\n\nWe note that the function $\\Theta^{\\max}_{\\beta,k}$ is piecewise linear\nand concave, and $\\lim_{\\vartheta\\to \\infty}\\Theta^{\\max}_{\\beta,k}(\\vartheta) = +\\infty$.\nThus the region of admissible parameters defined by~\\eqref{eq:theta}\nis a convex and unbounded polyhedron.\nMoreover, we have that $\\Theta_{\\beta,k}^{\\max}(R_{\\beta,k}) = \\Theta_{\\beta,k}^{\\min}$.\nAdditionally, we note that the behaviour of the function $\\Theta_{\\beta,k}^{\\max}$\nnear infinity is determined by the term\n", "index": 109, "text": "\n\\[\n\\Sigma_{\\beta,k} := \\max_{|I| \\le k} \\|(A_{\\beta,I}^*A_I)^{-1}\\|_\\infty.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex35.m1\" class=\"ltx_Math\" alttext=\"\\Sigma_{\\beta,k}:=\\max_{|I|\\leq k}\\|(A_{\\beta,I}^{*}A_{I})^{-1}\\|_{\\infty}.\" display=\"block\"><mrow><mrow><msub><mi mathvariant=\"normal\">\u03a3</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>:=</mo><mrow><munder><mi>max</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>I</mi><mo stretchy=\"false\">|</mo></mrow><mo>\u2264</mo><mi>k</mi></mrow></munder><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><mi>\u03b2</mi><mo>,</mo><mi>I</mi></mrow><mo>*</mo></msubsup><mo>\u2062</mo><msub><mi>A</mi><mi>I</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2225</mo></mrow><mi mathvariant=\"normal\">\u221e</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01461.tex", "nexttext": "\nwhere $T \\in  {\\mathbb{R}}^{m \\times N}$ is an i.i.d.\\ Gaussian matrix, $u^\\dag$ is a sparse vector and $v$ is a noise vector. The choice of $T$ corresponds to compressed sensing measurements \\cite{FoRa13}.\n\nIn the experiments, we consider 30 problems of this type with $u^\\dag$ randomly generated, $\\inf_{i \\in I} | u^\\dag| > 1.5$  and $\\#\\mathop{{\\rm supp}}(u^\\dag) = 7,$ and $v$ is a random vector whose components are uniformly distributed on $[-1,1]$, and normalized such that $\\|v \\|_\\infty =0.3.$ We also consider the Gaussian matrices of the size $m = 50$, $N = 100.$ \n\nIn order to approximate minimizers of the multi-penalty (\\ref{eq:multi}) and the corresponding single-penalty functional  we use the iterative soft-thresholding algorithm \\cite{Fornasier07}.\nThe regularization parameters $\\alpha$ and $\\beta$ were chosen from the grid $Q_{\\alpha_0}^k \\times Q_{\\beta_0}^k$, where $Q_{\\alpha_0}^k:=\\{\\alpha= \\alpha_i = \\alpha_0 k^i\\, ,\\alpha_0 = 0.0002,k=1.25,i=0,\\ldots,50\\}$, and $Q_{\\beta_0}^k:=\\{\\beta = \\beta_i = \\beta_0 k^i,\\beta_0 = 0.01,q=1.15,i=0,\\ldots,30\\}$. For all possible combinations of  $(\\alpha,\\beta)$  we run the iterative soft-thresholding algorithm with fifty inner loop iterations and starting values $u^{(0)}=v^{(0)}=0$. \n\nIn order to assess the obtained results, we compare the performance of\nthe considered regularization schemes. We measure the approximation\nerror (AE) by $\\| u - u^\\dag \\|_{2}$, as well as by the number of\nelements in the symmetric difference (SD) $\\#(\\mathop{{\\rm supp}}(u)\n\\Delta \\mathop{{\\rm supp}}(u^\\dag))$. The SD is defined as follows: $i\n\\in \\mathop{{\\rm supp}}(u) \\Delta \\mathop{{\\rm supp}}(u^\\dag)$ if and\nonly if  either $i \\notin \\mathop{{\\rm supp}}(u)$ and\n$i\\in\\mathop{{\\rm supp}}(u^\\dag)$ or $i \\in\\mathop{{\\rm supp}}(u)$ and\n$i \\notin\\mathop{{\\rm supp}}(u^\\dagger)$.\n\nFor each problem we compute the best multi-penalty solution $u^\\dag =\nu^\\dag(\\alpha,\\beta)$ meaning that no  other pairs $(\\alpha,\\beta)\\in\nQ_{\\alpha_0}^k\\times Q_{\\beta_0}^k$ can improve the accuracy of the\nalgorithm.  \nSimultaneously, for each problem from our data set we compute the best\nmono-penalty solution $u^\\dag = u^\\dag(\\alpha)$. \nThen,  we compute the mean value of the AE and SD. The respective\nresults are shown in table~\\ref{tb:mp_sp}.\nAdditionally, figure~\\ref{fi:comp} shows an example of the typical\nresults obtained for single- and for multi-penalty regularization.\n\n\\begin{table}[h]\n  \\begin{tabular}{rr|rrrr}\n    & & $AE$ & $SD$ & $\\alpha$ & $\\beta$ \\hfil \\\\\n    \\hline\n    & minimum & 5.00  & 3 & 4.59 &  \\\\\n    $SP$ & mean &  11.21 & 6 &  5.74&  \\\\\n    & maximum &  13.74 & 8 & 11.21 &  \\\\\n    \\hline\n    & minimum & 1.05  & 1 & 3.67 & 0.76  \\\\\n    $MP$ & mean & 5.92 & 3 & 5.74 & 7.09  \\\\\n    & maximum & 8.55 & 5 & 8.97& 9.42  \\\\\n  \\end{tabular}\n  \\caption{\\label{tb:mp_sp}\n   For 30 problems for the solution of the single-penalty (upper panel) as well as multi-penalty regularization (lower panel) the minimum / maximum AE, SD and optimal values of the regularization parameters are provided. The mean values are also provided.\n}\n\\end{table}\n\n\\begin{figure}[h]\n\n   \\includegraphics[width=0.95\\textwidth]{SP.eps}\n    \\includegraphics[width=0.95\\textwidth]{MP.eps}\\\\\n  \\caption{\\label{fi:comp}\n The figure reports the results of two different decoding procedures\n of the same problem, where the circles represent the noisy signal and\n the crosses represent the original signal. \n \\emph{Upper figure:} results with single-penalty regularization.\n \\emph{Lower figure:} results with multy-penalty regularization.\n Note that multi-penalty regularization allows for a better\n reconstruction of the support of the true signal $u^\\dagger$.\n  } \n\\end{figure}\n\n\n\\begin{thebibliography}{10}\n\n\\bibitem{Arias-castro11yeldar}\nE.~Arias-Castro and Y.~Eldar.\n\\newblock Noise folding in compressed sensing.\n\\newblock {\\em IEEE Signal Process. Lett}, pages 478--481, 2011.\n\n\\bibitem{arfopeXX}\nM.~Artina, M.~Fornasier, and S.~Peter.\n\\newblock Damping noise-folding and enhanced support recovery in compressed\n  sensing.\n\\newblock {\\em IEEE Trans. Signal Proc.}, 63:5990--6002, 2015.\n\n\\bibitem{BH14}\nK.~Bredies and M.~Holler.\n\\newblock Regularization of linear inverse problems with total generalized\n  variation.\n\\newblock {\\em Journal of Inverse and Ill-posed Problems}, 1569-3945\n  (online):1--38, 2014.\n\n\\bibitem{CandesTao}\nE.~Candes and T.~Tao.\n\\newblock The dantzig selector: Statistical estimation when p is much larger\n  than n.\n\\newblock {\\em Ann. Statist.}, 35:2313--2351, 2007.\n\n\\bibitem{6204356}\nM.A. Davenport, J.N. Laska, J.~Treichler, and R.G. Baraniuk.\n\\newblock The pros and cons of compressive sensing for wideband signal\n  acquisition: Noise folding versus dynamic range.\n\\newblock {\\em Signal Processing, IEEE Transactions on}, 60(9):4628--4642, Sept\n  2012.\n\n\\bibitem{Fornasier07}\nM.~Fornasier.\n\\newblock Domain decomposition methods for linear inverse problems with\n  sparsity constraints.\n\\newblock {\\em Inverse Problems}, 23(6):2505--2526, 2007.\n\n\\bibitem{FoRa13}\nS.~Foucart and H.~Rauhut.\n\\newblock {\\em A Mathematical Introduction to Compressive Sensing}.\n\\newblock Springer, New York, 2013.\n\n\\bibitem{CPA:CPA20350}\nM.~Grasmair, O.~Scherzer, and M.~Haltmeier.\n\\newblock Necessary and sufficient conditions for linear convergence of\n  l1-regularization.\n\\newblock {\\em Communications on Pure and Applied Mathematics}, 64(2):161--182,\n  2011.\n\n\\bibitem{LP_NumMath}\nS.~Lu and S.~V. Pereverzev.\n\\newblock Multi-parameter regularization and its numerical realization.\n\\newblock {\\em Numer. Math.}, 118(1):1--31, 2011.\n\n\\bibitem{NP13}\nV.~Naumova and S.~V. Pereverzyev.\n\\newblock Multi-penalty regularization with a component-wise penalization.\n\\newblock {\\em Inverse Problems}, 29(7):075002, 2013.\n\n\\bibitem{naumpeter}\nV.~Naumova and S.~Peter.\n\\newblock Minimization of multi-penalty functionals by alternating iterative\n  thresholding and optimal parameter choices.\n\\newblock {\\em Inverse Problems}, 30:125003, 1--34, 2014.\n\n\\bibitem{SL13}\nW.~Wang, S.~Lu, H~Mao, and J.~Cheng.\n\\newblock Multi-parameter {T}ikhonov regularization with $\\ell^0$ sparsity\n  constraint.\n\\newblock {\\em Inverse Problems}, 29:065018, 2013.\n\n\\end{thebibliography}\n\n\n\n", "itemtype": "equation", "pos": 39573, "prevtext": "\nIf this value is small, then the slope of the function\n$\\Theta_{\\beta,k}^{\\max}(\\vartheta)$ for large values of $\\vartheta$\nis large, and thus the set of admissible parameter grows\nfast with increasing signal-to-noise ratio.\nIf, on the other hand, $\\Sigma_{\\beta,k}$ is large, then the\nset of admissible parameters is relatively small\neven for large signal-to-noise ratio.\nThus $\\Sigma_{\\beta,k}$ can be reasonably interpreted as\nthe sensitivity of multi-parameter regularization with\nrespect to parameter choice.\nThe larger $\\Sigma_{\\beta,k}$ is, the more precise the parameter $\\alpha$\nhas to be chosen in order to guarantee exact support recovery.\n\n\n\\section{Numerical Validation}\\label{se:valid}\n\nThe main motive behind the study and application of multi-penalty\nregularization is the problem that $\\ell^1$-regularization is often\nnot capable to identify the support of signal correctly (see \\cite{arfopeXX} and references therein).\nIncluding the additional $\\ell^2$-regularization term, however, might\nlead to an improved performance in terms of support recovery, because\nwe can expect that the $\\ell^2$-term takes care of all the small noise\ncomponents. \n\nIn order to verify  this observation, a series of numerical experiments was performed, in which we illustrate for which parameters and Gaussian matrices the conditions for support recovery derived in the previous\nsection were satisfied. In addition, we studied whether the\ninclusion of the $\\ell^2$-term indeed increases the performance.\n\nIn a first set of experiments, we have generated a set of 20 Gaussian\nrandom matrices of different sizes and have tested for each\nthree-dimensional subspace spanned by the basis elements whether the\ncondition~\\eqref{eq:cond_multi_nec} is satisfied, first for\nthe single-penalty case, and then for the multi-penalty case with\ndifferent values of $\\beta$.\nThe results for matrices of dimensions 30 times 60 and 40 times 80,\nrespectively, are summarized in Table~\\ref{tb:cond} and\nFigure~\\ref{fi:cond}.\n\nAs to be expected from the bad numerical performance of\n$\\ell^1$-regularization in terms of support recovery, the\ninequality~\\eqref{eq:single_inftyrestriction} fails in a relatively\nlarge number of cases, especially when the discrepancy between the\ndimension $N$ of the vectors to be recovered and the number of\nmeasurements $m$ is quite large. For instance, in the case $N=60$ and\n$m=30$, the condition most of the time failed for more than half of\nthe three-dimensional subspaces. In contrast, the corresponding\ncondition~\\eqref{eq:cond_multi_nec} for multi-parameter regularization\nfails in the same situation only for about an eighth of the subspaces\nif $\\beta = 1$, and in even fewer cases for $\\beta = 0.1$.\n\nFor other combinations of dimensionality of the problem and number of\nmeasurements, the situation is similar. Introducing the additional\n$\\ell^2$-penalty term always allows for the exact support\nreconstruction on a larger number of subspaces than single-penalty\nregularization. Additionally, the results indicate that the number of\nrecoverable subspaces increases with decreasing $\\beta$.\n\n\n\\begin{table}[h]\n\\begin{tabular}{r||c|ccc}\n$m=30$  &\\ Single-penalty\\ {} & \\multicolumn{3}{c}{Multi-penalty}\\\\\n$N=60$ & &\\quad $\\beta = 10$ \\quad{} &\\quad $\\beta = 1$\\quad{} &\\quad $\\beta = 0.1$\\quad{} \\\\\n  \\hline\nMedian & 0.5425 & 0.3814 & 0.1214 & 0.0623\\\\\nMean & 0.5559 & 0.3922 & 0.1225 & 0.0635\\\\\nStd.~deviation & 0.05652 & 0.04142 & 0.01518 & 0.01083\\\\\n\\end{tabular}\n\\medskip\n\n\\begin{tabular}{r||c|ccc}\n$m=40$  &\\ Single-penalty\\ {} & \\multicolumn{3}{c}{Multi-penalty}\\\\\n$N=80$  & &\\quad $\\beta = 10$ \\quad{} &\\quad $\\beta = 1$\\quad{} &\\quad $\\beta = 0.1$\\quad{} \\\\\n  \\hline\nMedian & 0.2696 & 0.1523 & 0.0396 & 0.0256\\\\\nMean &   0.2746 & 0.1547 & 0.0413 & 0.0262\\\\\nStd.~deviation & 0.03060 & 0.01848 & 0.00659 & 0.00447 \\\\\n\\end{tabular}\n\\caption{\\label{tb:cond}\n    Percentage of 3-sparse subspaces for which the condition~\\eqref{eq:cond_multi_nec}\n  failed. The condition was tested on samples of 20 Gaussian random matrices\n  of dimensions 30 times 60 (upper table) and 40 times 80 (lower table).\n  Other combinations of dimensionality and number of measurements showed\n  qualitatively similar results.\n}\n\\end{table}\n\n\\begin{figure}[h]\n  \\includegraphics[width=0.45\\textwidth]{./meas30_60.eps}\n  \\includegraphics[width=0.45\\textwidth]{./meas30_80.eps}\\\\\n  \\includegraphics[width=0.45\\textwidth]{./meas40_60.eps}\n  \\includegraphics[width=0.45\\textwidth]{./meas40_80.eps}\n  \\caption{\\label{fi:cond}\n    Percentage of 3-sparse subspaces for which the condition~\\eqref{eq:cond_multi_nec}\n    failed. For each of the different settings the condition\n    has been tested on 20 Gaussian random matrices\n    for multi-parameter regularization with $\\beta = 0.1$,\n    $\\beta = 1$ and $\\beta = 10$, and for single parameter\n    regularization.\n    \\emph{Upper left:} Dimension 30 times 60.\n    \\emph{Upper right:} Dimension 30 times 80.\n    \\emph{Lower left:} Dimension 40 times 60.\n    \\emph{Lower right:} Dimension 40 times 80.} \n\\end{figure}\n\nIn the case where $N = 80$ and $m=60$ (that is, we want to reconstruct\n$80$-dimensional vectors from 60 measurements), the sufficient\ncondition~\\eqref{eq:cond_multi_nec} for multi-penalty regularization\nwas satisfied in our numerical experiments for all $3$-sparse\nsubspaces for parameters $\\beta$ smaller than 5.\nIn this situation, we have therefore additionally computed the\nsignificant values $R_{\\beta,3}$ and $\\Sigma_{\\beta,3}$,\nthat is, the minimal recoverable signal-to-noise ratio and the\nparameter sensitivity for three-dimensional subspaces.\nThe results of these numerical experiments are shown in\nTable~\\ref{tb:beta} and Figure~\\ref{fi:beta}.\n\nThe results indicate that decreasing the regularization parameter\n$\\beta$ tends to decrease the necessary signal-to-noise ratio\n$R_{\\beta,k}$ as well. While a regularization parameter $\\beta = 5$\nrequired always an unreasonably large signal-to-noise ratio in order\nto guarantee the recoverability of the support, the ratios for $\\beta\n= 0.5$ or $\\beta = 0.1$ turned out to be much more\nreasonable. However, the results also showed that decreasing the\nregularization parameter need not necessarily have a beneficial effect\non the recoverability. For some matrices it happened that the\nnecessary signal-to-noise ratio $R_{\\beta,k}$ increased while the\nregularization parameter $\\beta$ was decreased\n(see Figure~\\ref{fi:beta}, left).\n\nAdditionally, the results indicate that the parameter sensitivity\n$\\Sigma_{\\beta,k}$ increases considerably as $\\beta$ decreases (see\nFigure~\\ref{fi:beta}, right). As a consequence, the range of\nadmissible parameters $\\alpha$ tends to be significantly smaller for\nsmaller $\\beta$, and it can happen much more easily that the combined\neffect of $\\ell^2$ and $\\ell^1$-regularization leads to a\nclassification of signals as noise. While multi-penalty regularization\nwith a small parameter $\\beta$ might therefore lead to a better\nnecessary signal-to-noise ratio for recovery, it requires at the same\ntime a better balance between the two involved parameters $\\alpha$ and\n$\\beta$.\n\n\n\n\n\\begin{figure}[h]\n  \\includegraphics[width=0.45\\textwidth]{./R_60_80_beta01_1.eps}\n  \\includegraphics[width=0.45\\textwidth]{./S_60_80_beta01_1.eps}\n  \\caption{\\label{fi:beta}\n    Influence of the regularization parameter $\\beta$\n    on the set of exact support recovery and the\n    admissible parameters.\n    \\emph{Left:} Logarithm of base 10 of the recoverable\n    signal-to-noise ratio $R_{\\beta,3}$ for different values\n    of $\\beta$.\n    \\emph{Right:} The sensitivity $\\Sigma_{\\beta,3}$ for different\n    values of $\\beta$.\\newline\n    The dimension of the matrix is in all cases 60 times 80,\n    and 20 Gaussian random matrices have been\n    used for each parameter $\\beta$.} \n\\end{figure}\n\n\n\\begin{table}[h]\n  \\begin{tabular}{rr|rrrrr}\n    & &\\multicolumn{5}{c}{$\\beta$}\\\\\n    & & $0.1$ & $0.3$ & $0.5$\\hfil & $1$\\hfil & $5$\\hfil \\\\\n    \\hline\n    & minimum & 1.623 & 5.961 & 12.05 & 49.14 & 3599.3 \\\\\n    $R_{\\beta,3}$ & median & 2.252 & 11.84 & 28.17 & 205.7 & 15041.4 \\\\\n    & maximum & 5.546 & 262.5 & 302.85 & 6178.4 & 203621.6\\\\\n    \\hline\n    & minimum & 31.07 & 13.35 & 9.379 & 6.517 & 3.793 \\\\\n    $\\Sigma_{\\beta,3}$ & median & 35.17 & 15.31 & 11.086 & 7.517 & 4.278 \\\\\n    & maximum & 44.00 & 19.02 & 13.474 & 9.154 & 4.977 \\\\\n  \\end{tabular}\n  \\caption{\\label{tb:beta}\n    Influence of the parameter $\\beta$ on the values\n    of $R_{\\beta,k}$ and $\\Sigma_{\\beta,k}$. The values have been\n    computed for 20 Gaussian random matrices of dimension\n    60 times 80 with $k=3$.}\n\\end{table}\n\n\\section{Applications. Numerical experiments}\n\n\nIn order to support our theoretical findings even further, we present in this\nsection some statistical data obtained by solving series of\ncompressive sensing problems  by means of  multi-penalty  and\n$\\ell^1$-regularization. Similarly to \\cite{naumpeter, arfopeXX} we\nconsider in our numerical experiments the model problem of the type \n", "index": 111, "text": "\n\\[\n\ty= T (u^\\dagger + v),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex36.m1\" class=\"ltx_Math\" alttext=\"y=T(u^{\\dagger}+v),\" display=\"block\"><mrow><mrow><mi>y</mi><mo>=</mo><mrow><mi>T</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>u</mi><mo>\u2020</mo></msup><mo>+</mo><mi>v</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]