[{"file": "1601.07649.tex", "nexttext": "\nwhere $\\frac{\\lambda}{2} \\ensuremath{ \\left\\| {\\boldsymbol \\theta} \\right\\|^2_{ \\mathrm{2} } } $ is the regularization term, with $\\lambda$ being the weight decay parameter.\nThe conditional log-likelihood is modelled as:\n\n", "itemtype": "equation", "pos": 13638, "prevtext": "\n\n\n\n\n\n\\title{Discriminative Training of Deep Fully-connected Continuous {{CRFs}\\xspace} \\\\\nwith Task-specific Loss}\n\n\\author{Fayao Liu, Guosheng Lin, Chunhua Shen\\thanks{Corresponding author (e-mail: chhshen@gmail.com). }\n\\\\\nSchool of Computer Science,\nUniversity of Adelaide, Australia}\n\n\n\n\n\\maketitle\n\\thispagestyle{empty}\n\n\n\n\n\n\\begin{abstract}\nRecent works on deep conditional random fields ({{CRFs}\\xspace}) have set new records on\nmany vision tasks involving structured predictions.\n\nHere we propose a fully-connected deep continuous {{CRFs}\\xspace} model for both discrete and continuous labelling\nproblems. We exemplify the usefulness of the proposed model on\nmulti-class semantic labelling (discrete) and the robust depth estimation (continuous) problems.\n\n In our framework, we model both the unary and the pairwise potential functions as deep convolutional neural networks ({{CNNs}\\xspace}), which are jointly learned in an end-to-end fashion.\nThe proposed method possesses the main advantage of continuously-valued {{CRFs}\\xspace}, which is a closed-form solution for the Maximum a posteriori (MAP) inference.\n\n To better adapt to different tasks, instead of using the commonly employed maximum likelihood {{CRFs}\\xspace} parameter learning protocol,\n we propose task-specific loss functions for learning the {{CRFs}\\xspace} parameters.\n It enables direct optimization of the quality of the MAP estimates during the course of learning.\n Specifically, we optimize the multi-class classification loss for the semantic labelling task and the Turkey's biweight loss for the robust depth estimation problem.\n Experimental results on the semantic labelling and robust depth estimation tasks demonstrate that the proposed method compare favorably against both baseline and state-of-the-art methods.\n In particular, we show that although the proposed deep {{CRFs}\\xspace} model is continuously valued,\n with the equipment of task-specific loss, it achieves impressive results even on discrete labelling tasks.\n\\end{abstract}\n\n\n\n\n\\section{Introduction}\n\n\nRecent works on combining conditional random fields ({{CRFs}\\xspace}) and deep convolutional neural networks ({{CNNs}\\xspace})\nhave set new records on many vision tasks involving structured predictions.\nPixel-level labelling tasks generally refer to assigning a discrete or continuous label to each pixel in an image, with typical examples being semantic labelling, and depth estimation.\nThey play a pivotal role in the computer vision community, and are fundamental to many high-level tasks, like recognition, scene understanding, 3D modeling.\nIn the central of many pixel labelling tasks stands the feature engineering, which has a significant impact on the final results.\nHowever, designing effective and highly discriminative features remains a challenge.\nTraditional efforts have been focusing on designing hand-crafted features, {\\emph{e.g}\\onedot}, texton, HOG, SIFT, GIST, {\\emph{etc}\\onedot}.\nLater on, unsupervised feature learning \\cite{Coates11} has been introduced and showed promises.\nRecently, supervised feature learning, {\\emph{i.e}\\onedot}, deep  {{CNNs}\\xspace} have achieved  tremendous success in various domains including image classification \\cite{AlexNet12}, pose estimation \\cite{Lecun_nips14}, semantic labelling \\cite{Long15,Zheng15,Eigen15}, depth estimation \\cite{dcnn_nips14,cvpr15}, {\\emph{etc}\\onedot}.\nAdapting {{CNNs}\\xspace} for pixel labelling tasks have since attracted a lot of attention.\nHowever, several issues need to be resolved in this procedure, {\\emph{e.g}\\onedot}, down-sampled coarse predictions, non-sharp boundary transitions.\n\nBesides the feature representation, another essential factor to be considered is the appearance and spatial consistency, which is typically captured by graphical models.\nProbabilistic graphical models, {\\emph{e.g}\\onedot}, {{CRFs}\\xspace} \\cite{Lafferty_icml01}, have long been shown as a fundamental tool in computer vision, and have been widely applied to structured prediction problems, like image denoising, semantic labelling, depth estimation, {\\emph{etc}\\onedot}.\nRecently, continuous {{CRFs}\\xspace}  have shown benefits of efficient learning and inference, while being as powerful as discrete {{CRFs}\\xspace} models \\cite{Tappen08,Samuel09,CCNF_eccv14,cvpr15,pami15}.\nIn this paper, we present a general deep {{CRFs}\\xspace} learning framework for both discrete and continuous labelling problems, which combines the benefits of {{CNNs}\\xspace} and continuous {{CRFs}\\xspace}.\n\n\n\nWhile the maximum likelihood learning is dominant in learning {{CRFs}\\xspace} parameters, it is argued being beneficial to directly take into account the quality of the MAP estimates during the course of learning.\n\n Samuel  {\\emph{et al}\\onedot} \\cite{Samuel09} pointed out that if one's goal is to use MAP estimates and evaluate the results using other criteria, then optimization of the {{MRFs}\\xspace} parameters using a probabilistic criterion, like maximum likelihood, may not necessarily lead to optimal results.\nInstead, a better strategy is to find the parameters such that the quality of the MAP estimates are directly optimized against a task-specific empirical risk.\nFor example, in  image segmentation, the most direct criterion of a good segmentation mask is the global accuracy or the Jaccard index (intersection-over-union score).\nOn the other hand, in many practical applications, it may  not be necessary to model the full conditional distribution of labels, given observations.\nInstead, modeling the marginal conditional likelihood is more preferable.\nIn \\cite{Domke13}, Domke argued that the marginal based loss generally yields better\nmaximum posterior marginal\n(MPM) inference.\nTherefore an alternative to the maximum likelihood learning is to minimize the  empirical risk loss.\n\nIn \\cite{Kakade02}, Kakade {\\emph{et al}\\onedot} propose to maximize the label-wise marginal likelihood\nrather than the joint label likelihood for sequence labelling problems.\nHere we analogously propose to maximize the label-wise labelling accuracy by optimizing the softmax classification loss for the semantic labelling task.\nFor the depth estimation problem, depths captured by  depth sensors are often noisy and contain missing values due to poor illumination and sensor limitation, {\\emph{etc}\\onedot}.\nWe  tackle this problem by proposing a robust depth estimation model, which uses the robust Turkey's biweight loss.\n\n\n\n\n\n\n\nWe summarize the main contributions of this work as follows.\n\\begin{itemize}\n\\itemsep -.12cm\n\\item We present a general deep continuous {{CRFs}\\xspace} learning model for both discrete and continuous pixel-level labelling problems.\nBoth the unary and the pairwise potentials are modeled as {{CNNs}\\xspace} networks, and jointly learned in an end-to-end fashion. There exists a closed-form solution for the MAP inference problem, largely reducing the complexity of the model.\nOur {{CRFs}\\xspace} model is fully connected and thus may better capture long-range relations.\n\\item Instead of the commonly used maximum likelihood criterion for {{CRFs}\\xspace} parameter learning, we show that a task-specific loss function can be applied to directly optimize the MAP estimates, and show two different applications, namely, multi-class semantic labelling (discrete) and robust depth estimation (continuous).\n\\item We experimentally demonstrate that the proposed framework shows advantages over the baseline methods and state-of-the-art methods on both semantic labelling and robust depth estimation tasks.\n\\end{itemize}\n\n\nThere seems to be a common understanding that for discrete labelling problems,  continuous {{CRFs}\\xspace} models  may not be suitable.  As a result, an increasingly sophisticated and computationally expensive  discrete  {{CRFs}\\xspace} methods, in terms of both training and inference, have been developed in the literature. Here we argue that {\\em a carefully-designed, yet much simpler, continuous {{CRFs}\\xspace} model is capable of solving discrete labelling problems effectively and producing competitive results, at least in the case of  deep  {{CRFs}\\xspace}}---{{CRFs}\\xspace} with deep {{CNNs}\\xspace} modeling the potentials.\n\n\n\n\n\n\n\n\n\n\n\\subsection{Related work}\n\n\nIn this section, we review some most recent progresses in {{CNNs}\\xspace} based methods for semantic labelling and depth estimation tasks, which are closely related to our work.\n\n\n\\paragraph{{{CNNs}\\xspace} for semantic labelling}\nThe most straightforward way of applying {{CNNs}\\xspace} for semantic labelling is to design fully convolutional networks to directly output the prediction map, {\\emph{e.g}\\onedot}, \\cite{Long15,Eigen15}.\nThese methods do not involve optimized structured loss, while only consider independent classification.\n\nA more appealing direction is to combine {{CNNs}\\xspace} with graphical models, which explicitly incorporates structured constraints.\nA simple strategy to do so is to incorporate consistency constraints into a trained unary model as a post-processing step. These methods first train a {{CNNs}\\xspace} model or generate {{CNNs}\\xspace} features for constructing the unary potential, then add spatial pairwise constraints to optimize the {{CRFs}\\xspace} loss.\nIn the pioneering work of \\cite{Farabet13}, Farabet {\\emph{et al}\\onedot} present a multi-scale {{CNNs}\\xspace} framework for scene labelling, which uses {{CRFs}\\xspace} as a post-processing step for local refinement.\nMost recently, Chen {\\emph{et al}\\onedot} \\cite{deeplab} propose to first train a fully convolutional neural networks for pixel classification and then separately apply a dense {{CRFs}\\xspace} to refine the semantic labelling results.\nLater on, more attempts are made towards joint learning of {{CNNs}\\xspace} and graphical models.\nIn the work of \\cite{Zheng15}, Zheng {\\emph{et al}\\onedot} propose to implement the mean field inference in {{CRFs}\\xspace} as recurrent neural networks ({{RNNs}\\xspace}) to facilitate the end-to-end joint learning.\nLin {\\emph{et al}\\onedot} \\cite{Lin15a} propose a piecewise training approach to learn the unary and pairwise {{CNNs}\\xspace} potentials in {{CRFs}\\xspace} for semantic labelling.\nThey further propose in \\cite{Lin15b} to directly learn {{CNNs}\\xspace} message estimators in the message passing inference rather than learning the potential functions.\nAll of the above-mentioned joint learning approaches exploit discrete {{CRFs}\\xspace}, which need to design approximation methods for both learning and inference.\nIn contrast, we here explore continuous {{CRFs}\\xspace}, which enjoys the benefits of exact maximum likelihood learning and closed-form solution for the MAP inference.\n\n\n\n\n\n\\paragraph{{{CNNs}\\xspace} for depth estimation}\nFor the depth estimation task, Eigen {\\emph{et al}\\onedot} \\cite{dcnn_nips14} propose to\ntrain multi-scale {{CNNs}\\xspace} to directly output the predicted depth maps by optimizing the pixel-wise least square loss for depth regression.\nDuring the course of network training, there is no explicit structured constraints involved.\nMost recently, Liu {\\emph{et al}\\onedot} \\cite{cvpr15} propose a deep convolutional neural fields model for depth estimation from single images, which jointly learns continuous {{CRFs}\\xspace} and deep {{CNNs}\\xspace} in a single framework.\nThey further propose a more effcient training approach based on fully convolutional networks\nand a superpixel pooling method in \\cite{pami15}.\nOur work here, which also jointly learn continuous {{CRFs}\\xspace} and {{CNNs}\\xspace} in a single framework,\nis mainly inspired by these two works \\cite{cvpr15,pami15}.\nHowever, our work differs from \\cite{cvpr15,pami15} in the following important aspects.\n(a) Our method  addresses both discrete and continuous labelling problems, while the works in \\cite{cvpr15,pami15} only deal with continuous labelling problem;\nApart from continuous prediction, we extend continuous deep {{CRFs}\\xspace} with closed-form MAP inference for discrete prediction.  We show that by optimizing the task-specific empirical risk, it is possible to achieve competitive  performance for discrete labelling tasks.\n(b) In contrast to the maximum likelihood optimization in \\cite{cvpr15,pami15},\n\n\nwe show the flexibility and usefulness of employing a task-specific loss in the context of deep {{CRFs}\\xspace}.\n(c) Our method here is based on fully-connected {{CRFs}\\xspace} with both the unary and the  pairwise potentials modeled as deep CNNs. In contrast,  the pairwise term  in \\cite{cvpr15,pami15} is a linear function  (or single layer) built upon\nhand-crafted image features. In our model,  the pairwise term is also a {{CNNs}\\xspace} model, and all the features are learned end-to-end.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Fully-connected deep continuous CRFs}\n\n\n\nFig.~\\ref{fig:dcnf_seg} shows an overview of our\nfully-connected deep continuous {{CRFs}\\xspace} model, which consists of a unary network and a pairwise network.\nThe unary network generates the unary prediction and is parametrized by ${\\boldsymbol \\theta^U}$,\nwhile the pairwise network outputs a similarity matrix of superpixel pairs and is parametrized by ${\\boldsymbol \\theta^V}$.\nThe final prediction is performed by combining the unary and the pairwise outputs to conduct the MAP inference.\n\n\n\\subsection{Discriminative learning of model parameters}\nGiven $N$ {{i.i.d.}\\xspace} training examples ${\\cal D}=\\{({\\bf x}^{(i)}, {\\bf y}^{(i)})\\}_{i=1}^N$, {{CRFs}\\xspace} typically learn the model parameters ${\\boldsymbol \\theta}$ by the maximum likelihood learning, {\\emph{i.e}\\onedot}, minimizing the negative conditional log-likelihood:\n\n", "index": 1, "text": "\\begin{align} \\label{eq:opt_crf}\n\\min_{{\\boldsymbol \\theta}} \\frac{\\lambda}{2} \\ensuremath{ \\left\\| {\\boldsymbol \\theta} \\right\\|^2_{ \\mathrm{2} } }  - \\sum_{i=1}^N \\log {\\mathrm{Pr}}({\\bf y}^{(i)}|{\\bf x}^{(i)}; {\\boldsymbol \\theta}),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{{\\boldsymbol{\\theta}}}\\frac{\\lambda}{2}\\left\\|{\\boldsymbol{%&#10;\\theta}}\\right\\|^{2}_{\\mathrm{2}}-\\sum_{i=1}^{N}\\log{\\mathrm{Pr}}({\\bf y}^{(i)%&#10;}|{\\bf x}^{(i)};{\\boldsymbol{\\theta}}),\" display=\"inline\"><mrow><munder><mi>min</mi><mi>\ud835\udf3d</mi></munder><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2225</mo><mi>\ud835\udf3d</mi><msubsup><mo>\u2225</mo><mn>2</mn><mn>2</mn></msubsup><mo>-</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi>log</mi><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udc32</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc31</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere $E$ is an energy function as described in Sec.~\\ref{sec:energy}; $\\mathrm{Z}$ is the partition function defined as:\n$\\mathrm{Z}({\\bf x}) = \\int_{{\\bf y}} \\exp \\{ -E({\\bf y}, {\\bf x}) \\}\\mathrm{d}{\\bf y}$ for continuous labelling ${\\bf y}$.\nDuring prediction, one performs the MAP inference:\n\n", "itemtype": "equation", "pos": 14107, "prevtext": "\nwhere $\\frac{\\lambda}{2} \\ensuremath{ \\left\\| {\\boldsymbol \\theta} \\right\\|^2_{ \\mathrm{2} } } $ is the regularization term, with $\\lambda$ being the weight decay parameter.\nThe conditional log-likelihood is modelled as:\n\n", "index": 3, "text": "\\begin{equation}\\label{eq:prob}\n\\begin{aligned}\n{\\mathrm{Pr}}({\\bf y}|{\\bf x}) = \\frac{1}{\\mathrm{Z({\\bf x})}} \\exp (- E({\\bf y}, {\\bf x})),\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathrm{Pr}}({\\bf y}|{\\bf x})=\\frac{1}{\\mathrm{Z({\\bf x})}}\\exp(%&#10;-E({\\bf y},{\\bf x})),\" display=\"inline\"><mrow><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi mathvariant=\"normal\">Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\n\n\n\n\n\n\nInstead of the traditionally optimized log-likelihood objective, we propose to directly optimize the quality of the estimated labelling (MAP solution).\nSimilar to the discriminative max-margin framework proposed by Taskar {\\emph{et al}\\onedot} \\cite{Taskar05}, our learning objective is:\n\n", "itemtype": "equation", "pos": 14574, "prevtext": "\nwhere $E$ is an energy function as described in Sec.~\\ref{sec:energy}; $\\mathrm{Z}$ is the partition function defined as:\n$\\mathrm{Z}({\\bf x}) = \\int_{{\\bf y}} \\exp \\{ -E({\\bf y}, {\\bf x}) \\}\\mathrm{d}{\\bf y}$ for continuous labelling ${\\bf y}$.\nDuring prediction, one performs the MAP inference:\n\n", "index": 5, "text": "\\begin{equation}\\label{eq:inf_map_org}\n\\begin{aligned}\n\\hat{{\\bf y}} = {\\operatornamewithlimits{argmax}}_{{\\bf y}} {\\mathrm{Pr}}({\\bf y}|{\\bf x}; {\\boldsymbol \\theta}).\n\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\bf y}}={\\operatornamewithlimits{argmax}}_{{\\bf y}}{\\mathrm%&#10;{Pr}}({\\bf y}|{\\bf x};{\\boldsymbol{\\theta}}).\" display=\"inline\"><mrow><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><munder><mo movablelimits=\"false\">argmax</mo><mi>\ud835\udc32</mi></munder><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHere ${\\cal L}(\\cdot, \\cdot)$ is a task-specific loss function, which measures the discrepancy of the predicted label with the ground-truth label;  ${\\bf y}^{(i)}$, $\\hat{{\\bf y}}^{(i)}$ are respectively the ground-truth and the predicted labels of the $i$-th training example.\nThe loss function ${\\cal L}$ provides important information on how good a potential prediction $\\hat{{\\bf y}}^{(i)}$ is with respect to the ground-truth label ${\\bf y}^{(i)}$.\nThis is generally not addressed in the traditional {{CRFs}\\xspace} learning approaches that rely on the maximum likelihood scheme.\nTo our knowledge, this has not been explored in the case of learning {\\em deep} CRFs parameters.\nNote that\nthe implicit differentiation technique can be applied to\ncompute the gradient of  $  {\\cal L}(\\hat{{\\bf y}}^{(i)}, {\\bf y}^{(i)}) $     with respect to the parameters\n$  {\\boldsymbol \\theta} $, which enables us to use stochastic gradient descent (SGD) to learn deep structured models.\n\n\n\n\n\\begin{figure*}  \\center\n      \\includegraphics[width=0.79\\textwidth]{./fig/dcnf_seg.pdf}\n\\caption{The prediction framework of our method for the semantic labelling task. The image is first over-segmented into superpixels, and then as input to the unary and the pairwise networks. The unary network outputs the unary prediction of each superpixel, and the pairwise network outputs the pairwise similarities of superpixe pairs. The final prediction is obtained by combining the outputs of the unary and the pairwise networks to solve the MAP inference (Eq. \\eqref{eq:inf_map}). Details of the unary and the pairwise networks are illustrated in Fig. \\ref{fig:cnn_net}. } \\label{fig:dcnf_seg}\n\\end{figure*}\n\n\n\\begin{figure*}[t!]  \\center\n      \\includegraphics[width=0.76\\textwidth]{./fig/unary_net.pdf} \\\\\n      \\includegraphics[width=0.45\\textwidth]{./fig/pws_net.pdf}\n\\caption{The unary (upper plot) and the pairwise (lower plot) network architectures of our method.\nThe unary network is composed of $6$ convolution blocks and $3$ fully-connected layers.\nThe pairwise network is relatively shallow, which consists of $2$ convolution blocks together with one fully-connected layer and a Gaussian kernel layer to calculate the pairwise similarity matrix ${\\bf R}$ in Eq. \\eqref{eq:def_R}.\n$m$ is the number of semantic classes. We use the superpixel pooling (denoted as sp pooling layer) method as proposed in \\cite{pami15} to obtain superpixel {{CNNs}\\xspace} features from the convolution maps.}  \\label{fig:cnn_net}\n\\end{figure*}\n\n\n\n\n\\subsection{Energy formulation}\n\\label{sec:energy}\nWe now show how to generalize the deep continuous {{CRFs}\\xspace} model in \\cite{pami15} to fully-connected multi-label discrete labelling problems, {\\emph{e.g}\\onedot}, multi-class image segmentation.\n\n\n\nFor the multi-class image segmentation task, given an image ${\\bf x}$, we represent the label of each node (superpixel) as an $m$ dimensional vector ${\\bf y}_p$ (${\\bf y}_p \\in {\\mathbb{R}}^m$), which consists of the confidence scores of superpixel $p$ being each of the $m$ classes. Here $m$ is the total number of semantic classes.\nLet ${\\bf y}={\\bf y}_1 \\odot {\\bf y}_2 \\odot \\ldots \\odot {\\bf y}_n$ denotes the overall segmentation label of the image ${\\bf x}$, with $\\odot$ stacking two vectors and $n$ being the number of superpixels in ${\\bf x}$.\nWe formulate the energy function as a typical combination of unary potentials and pairwise potentials over the nodes (superpixels) ${\\cal N}$ and edges ${\\cal S}$ of the image ${\\bf x}$:\n\n", "itemtype": "equation", "pos": 15066, "prevtext": "\n\n\n\n\n\n\nInstead of the traditionally optimized log-likelihood objective, we propose to directly optimize the quality of the estimated labelling (MAP solution).\nSimilar to the discriminative max-margin framework proposed by Taskar {\\emph{et al}\\onedot} \\cite{Taskar05}, our learning objective is:\n\n", "index": 7, "text": "\\begin{align} \\label{eq:opt_all}\n\\min_{{\\boldsymbol \\theta}} &\\frac{\\lambda}{2} \\ensuremath{ \\left\\| {\\boldsymbol \\theta} \\right\\|^2_{ \\mathrm{2} } }  + \\sum_{i=1}^N {\\cal L}(\\hat{{\\bf y}}^{(i)}, {\\bf y}^{(i)}),  \\notag \\\\\n\\text{where}\\;\\; & \\hat{{\\bf y}}^{(i)} = {\\operatornamewithlimits{argmax}}_{{\\bf y}} {\\mathrm{Pr}}({\\bf y}|{\\bf x}^{(i)}; {\\boldsymbol \\theta}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\min_{{\\boldsymbol{\\theta}}}\" display=\"inline\"><munder><mi>min</mi><mi>\ud835\udf3d</mi></munder></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\lambda}{2}\\left\\|{\\boldsymbol{\\theta}}\\right\\|^{2}_{%&#10;\\mathrm{2}}+\\sum_{i=1}^{N}{\\cal L}(\\hat{{\\bf y}}^{(i)},{\\bf y}^{(i)}),\" display=\"inline\"><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mi>\u03bb</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mi>\ud835\udf3d</mi><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>\ud835\udc32</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\bf y}}^{(i)}={\\operatornamewithlimits{argmax}}_{{\\bf y}}{%&#10;\\mathrm{Pr}}({\\bf y}|{\\bf x}^{(i)};{\\boldsymbol{\\theta}}).\" display=\"inline\"><mrow><msup><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munder><mo movablelimits=\"false\">argmax</mo><mi>\ud835\udc32</mi></munder><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udc31</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><mi>\ud835\udf3d</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere $U$, $V$ are the unary and pairwise potential function respectively.\n\n\n\n\n\nThe unary potential is formulated as:\n\n", "itemtype": "equation", "pos": 18959, "prevtext": "\nHere ${\\cal L}(\\cdot, \\cdot)$ is a task-specific loss function, which measures the discrepancy of the predicted label with the ground-truth label;  ${\\bf y}^{(i)}$, $\\hat{{\\bf y}}^{(i)}$ are respectively the ground-truth and the predicted labels of the $i$-th training example.\nThe loss function ${\\cal L}$ provides important information on how good a potential prediction $\\hat{{\\bf y}}^{(i)}$ is with respect to the ground-truth label ${\\bf y}^{(i)}$.\nThis is generally not addressed in the traditional {{CRFs}\\xspace} learning approaches that rely on the maximum likelihood scheme.\nTo our knowledge, this has not been explored in the case of learning {\\em deep} CRFs parameters.\nNote that\nthe implicit differentiation technique can be applied to\ncompute the gradient of  $  {\\cal L}(\\hat{{\\bf y}}^{(i)}, {\\bf y}^{(i)}) $     with respect to the parameters\n$  {\\boldsymbol \\theta} $, which enables us to use stochastic gradient descent (SGD) to learn deep structured models.\n\n\n\n\n\\begin{figure*}  \\center\n      \\includegraphics[width=0.79\\textwidth]{./fig/dcnf_seg.pdf}\n\\caption{The prediction framework of our method for the semantic labelling task. The image is first over-segmented into superpixels, and then as input to the unary and the pairwise networks. The unary network outputs the unary prediction of each superpixel, and the pairwise network outputs the pairwise similarities of superpixe pairs. The final prediction is obtained by combining the outputs of the unary and the pairwise networks to solve the MAP inference (Eq. \\eqref{eq:inf_map}). Details of the unary and the pairwise networks are illustrated in Fig. \\ref{fig:cnn_net}. } \\label{fig:dcnf_seg}\n\\end{figure*}\n\n\n\\begin{figure*}[t!]  \\center\n      \\includegraphics[width=0.76\\textwidth]{./fig/unary_net.pdf} \\\\\n      \\includegraphics[width=0.45\\textwidth]{./fig/pws_net.pdf}\n\\caption{The unary (upper plot) and the pairwise (lower plot) network architectures of our method.\nThe unary network is composed of $6$ convolution blocks and $3$ fully-connected layers.\nThe pairwise network is relatively shallow, which consists of $2$ convolution blocks together with one fully-connected layer and a Gaussian kernel layer to calculate the pairwise similarity matrix ${\\bf R}$ in Eq. \\eqref{eq:def_R}.\n$m$ is the number of semantic classes. We use the superpixel pooling (denoted as sp pooling layer) method as proposed in \\cite{pami15} to obtain superpixel {{CNNs}\\xspace} features from the convolution maps.}  \\label{fig:cnn_net}\n\\end{figure*}\n\n\n\n\n\\subsection{Energy formulation}\n\\label{sec:energy}\nWe now show how to generalize the deep continuous {{CRFs}\\xspace} model in \\cite{pami15} to fully-connected multi-label discrete labelling problems, {\\emph{e.g}\\onedot}, multi-class image segmentation.\n\n\n\nFor the multi-class image segmentation task, given an image ${\\bf x}$, we represent the label of each node (superpixel) as an $m$ dimensional vector ${\\bf y}_p$ (${\\bf y}_p \\in {\\mathbb{R}}^m$), which consists of the confidence scores of superpixel $p$ being each of the $m$ classes. Here $m$ is the total number of semantic classes.\nLet ${\\bf y}={\\bf y}_1 \\odot {\\bf y}_2 \\odot \\ldots \\odot {\\bf y}_n$ denotes the overall segmentation label of the image ${\\bf x}$, with $\\odot$ stacking two vectors and $n$ being the number of superpixels in ${\\bf x}$.\nWe formulate the energy function as a typical combination of unary potentials and pairwise potentials over the nodes (superpixels) ${\\cal N}$ and edges ${\\cal S}$ of the image ${\\bf x}$:\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:energy}\nE({\\bf y}, {\\bf x}) = \\sum_{p \\in {\\cal N}} U({\\bf y}_{p}, {\\bf x}) + \\sum_{(p,q) \\in {\\cal S}} V({\\bf y}_{p}, {\\bf y}_{q}, {\\bf x}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"E({\\bf y},{\\bf x})=\\sum_{p\\in{\\cal N}}U({\\bf y}_{p},{\\bf x})+\\sum_{(p,q)\\in{%&#10;\\cal S}}V({\\bf y}_{p},{\\bf y}_{q},{\\bf x}),\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder><mrow><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>,</mo><msub><mi>\ud835\udc32</mi><mi>q</mi></msub><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHere ${\\bf z}_p$ is the unary network (network architecture detailed in Fig. \\ref{fig:cnn_net}) output for the superpixel $p$, which is parametrized by ${\\boldsymbol \\theta^U}$; $\\ensuremath{ \\left\\| {\\cdot} \\right\\|^2_{ \\mathrm{2} } } $ denotes the squared $\\ell_2$ norm of a vector.\n\n\n\n\nThe pairwise potential is written as:\n\n", "itemtype": "equation", "pos": 19260, "prevtext": "\nwhere $U$, $V$ are the unary and pairwise potential function respectively.\n\n\n\n\n\nThe unary potential is formulated as:\n\n", "index": 11, "text": "\\begin{equation}\\label{eq:unary}\nU({\\bf y}_{p}, {\\bf x};{\\boldsymbol \\theta^U}) = \\ensuremath{ \\left\\| {{\\bf y}_p - {\\bf z}_p} \\right\\|^2_{ \\mathrm{2} } } . \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"U({\\bf y}_{p},{\\bf x};{\\boldsymbol{\\theta}^{U}})=\\left\\|{{\\bf y}_{p}-{\\bf z}_{%&#10;p}}\\right\\|^{2}_{\\mathrm{2}}.\" display=\"block\"><mrow><mrow><mrow><mi>U</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>,</mo><mi>\ud835\udc31</mi><mo>;</mo><msup><mi>\ud835\udf3d</mi><mi>U</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc33</mi><mi>p</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHere ${R}_{pq}$, parameterized by the pairwise network parameters ${\\boldsymbol \\theta^V}$,  is the output of the pairwise network (network architecture detailed in Fig. \\ref{fig:cnn_net}) from the superpixel pair $(p,q)$. Specifically, ${R}_{pq}$ is computed from a Gaussian kernel layer:\\footnote{Note that we are not limited to the Gaussian kernel here. We can use other similarity calculation forms such as Laplacian.\n}\n\n", "itemtype": "equation", "pos": 19760, "prevtext": "\nHere ${\\bf z}_p$ is the unary network (network architecture detailed in Fig. \\ref{fig:cnn_net}) output for the superpixel $p$, which is parametrized by ${\\boldsymbol \\theta^U}$; $\\ensuremath{ \\left\\| {\\cdot} \\right\\|^2_{ \\mathrm{2} } } $ denotes the squared $\\ell_2$ norm of a vector.\n\n\n\n\nThe pairwise potential is written as:\n\n", "index": 13, "text": "\\begin{align} \\label{eq:pairwise}\n&V({\\bf y}_{p}, {\\bf y}_{q}, {\\bf x}; {\\boldsymbol \\theta^V}) = {\\frac{1}{2}} {R}_{pq} \\ensuremath{ \\left\\| {{\\bf y}_p - {\\bf y}_q} \\right\\|^2_{ \\mathrm{2} } } . \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle V({\\bf y}_{p},{\\bf y}_{q},{\\bf x};{\\boldsymbol{\\theta}^{V}})={%&#10;\\frac{1}{2}}{R}_{pq}\\left\\|{{\\bf y}_{p}-{\\bf y}_{q}}\\right\\|^{2}_{\\mathrm{2}}.\" display=\"inline\"><mrow><mrow><mrow><mi>V</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>,</mo><msub><mi>\ud835\udc32</mi><mi>q</mi></msub><mo>,</mo><mi>\ud835\udc31</mi><mo>;</mo><msup><mi>\ud835\udf3d</mi><mi>V</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msub><mi>R</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc32</mi><mi>q</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere ${\\bf s}_p$, ${\\bf s}_q$  are the convolution features (with dimension being $128$  as illustrated in Fig. \\ref{fig:cnn_net}) for superpixel $p$, $q$ respectively; ${\\bf l}_p$, ${\\bf l}_q$ are the centroid coordinates of the superpixel $p$ and $q$ respectively (normalized to [0, 1]);\n$\\gamma$ is a hyperparameter that controls the scale of the Gaussian position kernel; $\\beta$ is a non-negative scaling factor, which is regarded as one of the  pairwise network parameters and is jointly learned in our framework.\n\n\nWith Eqs.~\\eqref{eq:unary} and  \\eqref{eq:pairwise}, the energy function in Eq. \\eqref{eq:energy} can now be written as:\n\n", "itemtype": "equation", "pos": 20393, "prevtext": "\nHere ${R}_{pq}$, parameterized by the pairwise network parameters ${\\boldsymbol \\theta^V}$,  is the output of the pairwise network (network architecture detailed in Fig. \\ref{fig:cnn_net}) from the superpixel pair $(p,q)$. Specifically, ${R}_{pq}$ is computed from a Gaussian kernel layer:\\footnote{Note that we are not limited to the Gaussian kernel here. We can use other similarity calculation forms such as Laplacian.\n}\n\n", "index": 15, "text": "\\begin{align} \\label{eq:def_R}\n{R}_{pq} ({\\boldsymbol \\theta^V})=\\beta \\exp ( - \\ensuremath{ \\left\\| {{\\bf s}_p - {\\bf s}_q } \\right\\|^2_{ \\mathrm{2} } }  - \\gamma \\ensuremath{ \\left\\| {{\\bf l}_p - {\\bf l}_q } \\right\\|^2_{ \\mathrm{2} } }  ),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{R}_{pq}({\\boldsymbol{\\theta}^{V}})=\\beta\\exp(-\\left\\|{{\\bf s}_{p%&#10;}-{\\bf s}_{q}}\\right\\|^{2}_{\\mathrm{2}}-\\gamma\\left\\|{{\\bf l}_{p}-{\\bf l}_{q}}%&#10;\\right\\|^{2}_{\\mathrm{2}}),\" display=\"inline\"><mrow><mrow><mrow><msub><mi>R</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udf3d</mi><mi>V</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mo>-</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc2c</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc2c</mi><mi>q</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc25</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc25</mi><mi>q</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 21291, "prevtext": "\nwhere ${\\bf s}_p$, ${\\bf s}_q$  are the convolution features (with dimension being $128$  as illustrated in Fig. \\ref{fig:cnn_net}) for superpixel $p$, $q$ respectively; ${\\bf l}_p$, ${\\bf l}_q$ are the centroid coordinates of the superpixel $p$ and $q$ respectively (normalized to [0, 1]);\n$\\gamma$ is a hyperparameter that controls the scale of the Gaussian position kernel; $\\beta$ is a non-negative scaling factor, which is regarded as one of the  pairwise network parameters and is jointly learned in our framework.\n\n\nWith Eqs.~\\eqref{eq:unary} and  \\eqref{eq:pairwise}, the energy function in Eq. \\eqref{eq:energy} can now be written as:\n\n", "index": 17, "text": "\\begin{align}\\label{eq:energy_expand}\nE({\\bf y}, {\\bf x}) &= \\sum_{p \\in {\\cal N} } \\ensuremath{ \\left\\| {{\\bf y}_p - {\\bf z}_p} \\right\\|^2_{ \\mathrm{2} } } \n+ \\sum_{(p,q) \\in {\\cal S}} {\\frac{1}{2}} {R}_{pq} \\ensuremath{ \\left\\| {{\\bf y}_p - {\\bf y}_q} \\right\\|^2_{ \\mathrm{2} } }   \\notag  \\\\\n&={\\bf y}^{\\!\\top} {\\bf A} {\\bf y} - 2{\\bf z}^{\\!\\top} {\\bf y} + {\\bf z}^{\\!\\top} {\\bf z},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E({\\bf y},{\\bf x})\" display=\"inline\"><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{p\\in{\\cal N}}\\left\\|{{\\bf y}_{p}-{\\bf z}_{p}}\\right\\|^{2}_%&#10;{\\mathrm{2}}+\\sum_{(p,q)\\in{\\cal S}}{\\frac{1}{2}}{R}_{pq}\\left\\|{{\\bf y}_{p}-{%&#10;\\bf y}_{q}}\\right\\|^{2}_{\\mathrm{2}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc33</mi><mi>p</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><msub><mi>R</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc32</mi><mi>p</mi></msub><mo>-</mo><msub><mi>\ud835\udc32</mi><mi>q</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bf y}^{\\!\\top}{\\bf A}{\\bf y}-2{\\bf z}^{\\!\\top}{\\bf y}+{\\bf z}^%&#10;{\\!\\top}{\\bf z},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mrow><msup><mi>\ud835\udc32</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc00\ud835\udc32</mi></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\ud835\udc33</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc33</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHere ${\\bf z}={\\bf z}_1 \\odot {\\bf z}_2 \\odot \\ldots \\odot {\\bf z}_n$;\n${\\bf I}_m$, ${\\bf I}_n$ are identity matrices of size $m \\times m$, $n \\times n$ respectively;\n$\\otimes$ denotes the Kronecker product.\n${\\bf R}$ is the $n \\times n$ matrix composed of ${R}_{pq}$;\n${\\bf D}$ is a diagonal matrix with $D_{pp}=\\sum_q {R}_{pq}$.\n\n\n\n\n\\subsection{Optimization}\n\n\\textbf{MAP Inference}\nTo predict the label of a new given image ${\\bf x}$, we perform the maximum a posterior (MAP) inference, which is:\n\n", "itemtype": "equation", "pos": 21695, "prevtext": "\nwhere\n\n", "index": 19, "text": "\\begin{align}\n{\\bf A} &={\\bf A}_0 \\otimes {\\bf I}_m, \\\\\n\\text{with}\\;\\; {\\bf A}_0 &= {\\bf I}_n + {\\bf D} - {\\bf R}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf A}\" display=\"inline\"><mi>\ud835\udc00</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bf A}_{0}\\otimes{\\bf I}_{m},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msub><mi>\ud835\udc00</mi><mn>0</mn></msub><mo>\u2297</mo><msub><mi>\ud835\udc08</mi><mi>m</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{with}\\;\\;{\\bf A}_{0}\" display=\"inline\"><mrow><mpadded width=\"+5.6pt\"><mtext>with</mtext></mpadded><mo>\u2062</mo><msub><mi>\ud835\udc00</mi><mn>0</mn></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bf I}_{n}+{\\bf D}-{\\bf R}.\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc08</mi><mi>n</mi></msub><mo>+</mo><mi>\ud835\udc03</mi></mrow><mo>-</mo><mi>\ud835\udc11</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nWith the energy formulation in Eq. \\eqref{eq:energy_expand}, we can obtain the closed-form solution for Eq. \\eqref{eq:inf_map}:\n\n", "itemtype": "equation", "pos": 22323, "prevtext": "\nHere ${\\bf z}={\\bf z}_1 \\odot {\\bf z}_2 \\odot \\ldots \\odot {\\bf z}_n$;\n${\\bf I}_m$, ${\\bf I}_n$ are identity matrices of size $m \\times m$, $n \\times n$ respectively;\n$\\otimes$ denotes the Kronecker product.\n${\\bf R}$ is the $n \\times n$ matrix composed of ${R}_{pq}$;\n${\\bf D}$ is a diagonal matrix with $D_{pp}=\\sum_q {R}_{pq}$.\n\n\n\n\n\\subsection{Optimization}\n\n\\textbf{MAP Inference}\nTo predict the label of a new given image ${\\bf x}$, we perform the maximum a posterior (MAP) inference, which is:\n\n", "index": 21, "text": "\\begin{align} \\label{eq:inf_map}\n\\hat{{\\bf y}}&={\\operatornamewithlimits{argmax}}_{{\\bf y}} {\\mathrm{Pr}}({\\bf y}|{\\bf x}) = {\\operatornamewithlimits{argmin}}_{{\\bf y}} E({\\bf y}, {\\bf x}).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\bf y}}\" display=\"inline\"><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatornamewithlimits{argmax}}_{{\\bf y}}{\\mathrm{Pr}}({\\bf y}%&#10;|{\\bf x})={\\operatornamewithlimits{argmin}}_{{\\bf y}}E({\\bf y},{\\bf x}).\" display=\"inline\"><mrow><mo>=</mo><munder><mo movablelimits=\"false\">argmax</mo><mi>\ud835\udc32</mi></munder><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo movablelimits=\"false\">argmin</mo><mi>\ud835\udc32</mi></munder><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere ${\\bf A}^{-1}=({\\bf A}_0 \\otimes {\\bf I}_m)^{-1} ={\\bf A}_0^{-1} \\otimes {\\bf I}_m$, with ${\\bf A}_0^{-1}$ can be obtained by solving a linear equation system. Note that for computational efficiency, one typically does not need to compute\nthe matrix inverse explicitly.\n\n{\\it Discussion on MAP inference of fully connected {{CRFs}\\xspace}}\n    Note that even for fully connected models, the MAP inference here can still be\n    calculated efficiently, due to the closed form of Eq. \\eqref{eq:inf_solution}.\n    In contrast in the case of discrete fully-connected {{CRFs}\\xspace} models,\n     efficient inference, even with various approximations,  is only possible for a very special case to date.\n      As indicated in\n    \\cite{krahenbuhl2012efficient}, in order to apply the approximated fast inference of \\cite{krahenbuhl2012efficient},\n    the pairwise term must be in the form of the Gaussian kernel, and the input features must be in low dimension, typically less than $20$.\n    \n    In our case, the input feature's dimension is $128$, which is much higher than what can be possibly handled by\n    \\cite{krahenbuhl2012efficient}.\n\n\n\n\n\\subsubsection{Maximum likelihood learning}\nSince here we  consider the continuous domain of ${\\bf y}$ (vector of confidence scores), the integral in the partition function in\nEq.~\\eqref{eq:prob} can be analytically calculated\nunder certain circumstances, as shown in \\cite{pami15}.\nAccording to \\cite{pami15}, the negative conditional log-likelihood can be written as:\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 22653, "prevtext": "\nWith the energy formulation in Eq. \\eqref{eq:energy_expand}, we can obtain the closed-form solution for Eq. \\eqref{eq:inf_map}:\n\n", "index": 23, "text": "\\begin{align} \\label{eq:inf_solution}\n\\hat{{\\bf y}}&={\\bf A}^{-1} {\\bf z},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{{\\bf y}}\" display=\"inline\"><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bf A}^{-1}{\\bf z},\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><msup><mi>\ud835\udc00</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nThe gradients of  $-\\log{\\mathrm{Pr}}({\\bf y}|{\\bf x})$ with respect to ${\\bf A}$ and ${\\bf z}$ can be analytically calculated as shown in \\cite{pami15}.\nThen applying the chain rule (back propagation), the gradients with respect to ${\\boldsymbol \\theta^U}$, ${\\boldsymbol \\theta^V}$ and $\\beta$ can be calculated accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Task specific learning }\n\n\\paragraph{Multi-class semantic labelling}\n\nIn the multi-class semantic labelling task, we obtain\n a prediction of $m$ dimensional vector $\\hat{{\\bf y}}_p$ for superpixel $p$, with each $\\hat{y}_{pj}$ ($j$-th element of $\\hat{{\\bf y}}_p$, $j=1,2,\\ldots,m$) indicating the confidence score of the superpixel $p$ belonging to the $j$-th category.\n For the training ground truth ${\\bf y}_p$, the confidence value for the ground truth class is $1$, and $0$ for other classes.\nDuring optimization, we prefer the confidence score of the ground-truth category being as large as possible compared to the confidence scores of incorrect categories.\nIn this circumstance, we  minimize the  softmax loss, typically employed in multi-class classification,\n {\\emph{i.e}\\onedot}, maximize the label-wise classification accuracy, to directly pursue a desirable global accuracy:\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 24262, "prevtext": "\nwhere ${\\bf A}^{-1}=({\\bf A}_0 \\otimes {\\bf I}_m)^{-1} ={\\bf A}_0^{-1} \\otimes {\\bf I}_m$, with ${\\bf A}_0^{-1}$ can be obtained by solving a linear equation system. Note that for computational efficiency, one typically does not need to compute\nthe matrix inverse explicitly.\n\n{\\it Discussion on MAP inference of fully connected {{CRFs}\\xspace}}\n    Note that even for fully connected models, the MAP inference here can still be\n    calculated efficiently, due to the closed form of Eq. \\eqref{eq:inf_solution}.\n    In contrast in the case of discrete fully-connected {{CRFs}\\xspace} models,\n     efficient inference, even with various approximations,  is only possible for a very special case to date.\n      As indicated in\n    \\cite{krahenbuhl2012efficient}, in order to apply the approximated fast inference of \\cite{krahenbuhl2012efficient},\n    the pairwise term must be in the form of the Gaussian kernel, and the input features must be in low dimension, typically less than $20$.\n    \n    In our case, the input feature's dimension is $128$, which is much higher than what can be possibly handled by\n    \\cite{krahenbuhl2012efficient}.\n\n\n\n\n\\subsubsection{Maximum likelihood learning}\nSince here we  consider the continuous domain of ${\\bf y}$ (vector of confidence scores), the integral in the partition function in\nEq.~\\eqref{eq:prob} can be analytically calculated\nunder certain circumstances, as shown in \\cite{pami15}.\nAccording to \\cite{pami15}, the negative conditional log-likelihood can be written as:\n\n\n\n\n\n\n", "index": 25, "text": "\\begin{align} \\label{eq:log-likelihood}\n-\\log{\\mathrm{Pr}}({\\bf y}|{\\bf x}) =&{\\bf y}^{\\!\\top} {\\bf A} {\\bf y} - 2{\\bf z}^{\\!\\top} {\\bf y} + {\\bf z} ^{\\!\\top} {\\bf A}^{-1} {\\bf z} \\notag \\\\\n&- {\\frac{1}{2}}\\log(|{\\bf A}|) + \\frac{n}{2}\\log(\\pi).\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\log{\\mathrm{Pr}}({\\bf y}|{\\bf x})=\" display=\"inline\"><mrow><mo>-</mo><mi>log</mi><mi>Pr</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bf y}^{\\!\\top}{\\bf A}{\\bf y}-2{\\bf z}^{\\!\\top}{\\bf y}+{\\bf z}^{%&#10;\\!\\top}{\\bf A}^{-1}{\\bf z}\" display=\"inline\"><mrow><mrow><mrow><msup><mi>\ud835\udc32</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc00\ud835\udc32</mi></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\ud835\udc33</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc33</mi><mo lspace=\"0.8pt\">\u22a4</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc00</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc33</mi></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-{\\frac{1}{2}}\\log(|{\\bf A}|)+\\frac{n}{2}\\log(\\pi).\" display=\"inline\"><mrow><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><mi>\ud835\udc00</mi><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03c0</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nThen after we get the prediction $\\hat{{\\bf y}}_p$, the predicted category label for the $p$-th superpixel is:\n\n", "itemtype": "equation", "pos": 25788, "prevtext": "\nThe gradients of  $-\\log{\\mathrm{Pr}}({\\bf y}|{\\bf x})$ with respect to ${\\bf A}$ and ${\\bf z}$ can be analytically calculated as shown in \\cite{pami15}.\nThen applying the chain rule (back propagation), the gradients with respect to ${\\boldsymbol \\theta^U}$, ${\\boldsymbol \\theta^V}$ and $\\beta$ can be calculated accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Task specific learning }\n\n\\paragraph{Multi-class semantic labelling}\n\nIn the multi-class semantic labelling task, we obtain\n a prediction of $m$ dimensional vector $\\hat{{\\bf y}}_p$ for superpixel $p$, with each $\\hat{y}_{pj}$ ($j$-th element of $\\hat{{\\bf y}}_p$, $j=1,2,\\ldots,m$) indicating the confidence score of the superpixel $p$ belonging to the $j$-th category.\n For the training ground truth ${\\bf y}_p$, the confidence value for the ground truth class is $1$, and $0$ for other classes.\nDuring optimization, we prefer the confidence score of the ground-truth category being as large as possible compared to the confidence scores of incorrect categories.\nIn this circumstance, we  minimize the  softmax loss, typically employed in multi-class classification,\n {\\emph{i.e}\\onedot}, maximize the label-wise classification accuracy, to directly pursue a desirable global accuracy:\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n", "index": 27, "text": "\\begin{align} \\label{eq:opt_seg}\n {\\cal L}(\\hat{{\\bf y}}, {\\bf y}) = - \\sum_{p \\in {\\cal N}} \\sum_{j=1}^m y_{pj} \\log \\frac{\\exp(-\\hat{y}_{pj})}{\\sum_{k=1}^m \\exp(-\\hat{y}_{pk})}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\cal L}(\\hat{{\\bf y}},{\\bf y})=-\\sum_{p\\in{\\cal N}}\\sum_{j=1}^{m%&#10;}y_{pj}\\log\\frac{\\exp(-\\hat{y}_{pj})}{\\sum_{k=1}^{m}\\exp(-\\hat{y}_{pk})}.\" display=\"inline\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msub><mi>y</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>p</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>p</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nIn the experiment section,\nwe show that the use of this classification loss, instead of the traditional maximum likelihood loss,\n is critically important in achieving good performance for multi-class labelling tasks with our deep continuously-valued  {{CRFs}\\xspace} model.\n\n\n\n\n\\paragraph{Robust depth regression}\n\nFor the depth estimation problem, one of the main criteria for a good predictions is the root mean square (rms) error.\n\nTherefore, we can train our model by directly pursuing good MAP estimates in terms of RMS measure.\nTo achieve this, the loss function for the depth estimation problem can be written as:\n\n", "itemtype": "equation", "pos": 26091, "prevtext": "\nThen after we get the prediction $\\hat{{\\bf y}}_p$, the predicted category label for the $p$-th superpixel is:\n\n", "index": 29, "text": "\\begin{align} \\label{eq:pred}\n \\hat{t}={\\operatornamewithlimits{argmax}}_j \\hat{y}_{pj}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\hat{t}={\\operatornamewithlimits{argmax}}_{j}\\hat{y}_{pj}.\" display=\"inline\"><mrow><mrow><mover accent=\"true\"><mi>t</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><munder><mo movablelimits=\"false\">argmax</mo><mi>j</mi></munder><mo>\u2061</mo><msub><mover accent=\"true\"><mi>y</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>p</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nwhere $r_p=y_p-\\hat{y}_p$  is the depth residual of the superpixel $p$ regarding to the ground truth;\n$\\rho(\\cdot)$ is a penalty function, which generally satisfies $\\rho(0)=0$ and $\\rho(r_p) > 0$ for $r_p \\neq 0$.\n\n\n\nA straightforward alternative to the log-likelihood loss is the least squares (LS) loss:\n\n", "itemtype": "equation", "pos": 26813, "prevtext": "\nIn the experiment section,\nwe show that the use of this classification loss, instead of the traditional maximum likelihood loss,\n is critically important in achieving good performance for multi-class labelling tasks with our deep continuously-valued  {{CRFs}\\xspace} model.\n\n\n\n\n\\paragraph{Robust depth regression}\n\nFor the depth estimation problem, one of the main criteria for a good predictions is the root mean square (rms) error.\n\nTherefore, we can train our model by directly pursuing good MAP estimates in terms of RMS measure.\nTo achieve this, the loss function for the depth estimation problem can be written as:\n\n", "index": 31, "text": "\\begin{align} \\label{eq:opt_depth}\n {\\cal L}(\\hat{{\\bf y}}, {\\bf y}) =\\sum_{p \\in {\\cal N}} \\rho(r_p) ,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\cal L}(\\hat{{\\bf y}},{\\bf y})=\\sum_{p\\in{\\cal N}}\\rho(r_{p}),\" display=\"inline\"><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>\ud835\udc32</mi><mo stretchy=\"false\">^</mo></mover><mo>,</mo><mi>\ud835\udc32</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>p</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi></mrow></munder></mstyle><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHowever,   LS loss is generally not robust to outliers. Actually all convex regression loss functions are considered not robust.\nFor example, for those extremely large residuals (typically outliers), the LS loss imposes too large penalty, which inevitably biases the model towards outliers.\n\nTo fully exploit the flexibility of the proposed task-specific CRF parameter learning,\nwhich allows us to choose arbitrary learning criterion, here we\n  consider a non-convex, robust regression loss function, namely, the Turkey's biweight loss:\n\n", "itemtype": "equation", "pos": 27236, "prevtext": "\nwhere $r_p=y_p-\\hat{y}_p$  is the depth residual of the superpixel $p$ regarding to the ground truth;\n$\\rho(\\cdot)$ is a penalty function, which generally satisfies $\\rho(0)=0$ and $\\rho(r_p) > 0$ for $r_p \\neq 0$.\n\n\n\nA straightforward alternative to the log-likelihood loss is the least squares (LS) loss:\n\n", "index": 33, "text": "\\begin{align} \\label{eq:lsloss}\n \\rho(r_p)  = r_p^2,\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\rho(r_{p})=r_{p}^{2},\" display=\"inline\"><mrow><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msubsup><mi>r</mi><mi>p</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07649.tex", "nexttext": "\nHere $c$ is a pre-defined constant.\nFig.~\\ref{fig:turkeyLoss} shows the Turkey's biweight loss and its first-order derivative.\n\n\n\n\\begin{figure} \\center\n   \\includegraphics[width=0.2\\textwidth]{./fig/turkeyLoss.pdf}\n   \\includegraphics[width=0.2\\textwidth]{./fig/turkeyLoss_deriv.pdf}\n\\caption{An illustration of the Turkey's biweight loss and its first order derivative.}  \\label{fig:turkeyLoss}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n \\begin{figure} \\center\n   \\scalebox{0.55}\n   {\n\\begin{tabular}{cccc}\n\n\n\n\n\n\n\n\n\n\n\n\n      \\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/img/0538.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/gt/0538.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/unary/0538.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/dcnf/0538.png}    \\\\\n\n      \\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/img/1148.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/gt/1148.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/unary/1148.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/dcnf/1148.png}  \\\\\n\n\n      \\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/img/1259.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/gt/1259.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/unary/1259.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/dcnf/1259.png}    \\\\\n\n      \\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/img/1297.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/gt/1297.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/unary/1297.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/dcnf/1297.png}     \\\\\n\n\n      \\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/img/1299.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/gt/1299.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/unary/1299.png}\n      &\\includegraphics[width=0.2\\textwidth, height=0.16\\textwidth]{./fig/nyu_seg4/dcnf/1299.png}    \\\\\n\n\n\n\n\n\nTest image &Ground-truth &Unary prediction &Our full model \\\\\n\n\\end{tabular}\n}\n\\caption{Semantic labelling (4-class) examples on the NYU v2 dataset. Compared to the unary predictions, our full model yields more\naccurate and smoother labelling.}  \\label{fig:nyud2_suppl}\n\\end{figure}\n\n\n\n\n\n\n\n\n\\begin{figure*} [t] \\center\n{\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/images/000765.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/gt/000765.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/pred/000765.png}\\\\\n\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/images/000792.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/gt/000792.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/pred/000792.png}\\\\\n\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/images/001046.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/gt/001046.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/pred/001046.png}\\\\\n\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/images/001305.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/gt/001305.png}\n\t\\includegraphics[width=0.28\\textwidth]{./fig/kitti/pred/001305.png}\\\\\n\n\\footnotesize{\nTest image \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n Ground-truth \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad Our predictions\n }\n \\\\\n}\n\\caption{Examples of depth predictions on the KITTI dataset (Best viewed on screen).\nDepths are shown in log scale and in color (red is far, blue is close).\n}  \\label{fig:kitti}\n\\end{figure*}\n\n\n\n\n\\begin{figure}[h!]  \\center\n\\begin{tabular}{c c}\n{\n      \\includegraphics[width=0.2175\\textwidth, height=0.175\\textwidth]{./fig/globalaccVSclassnum.pdf}\n      \\includegraphics[width=0.2175\\textwidth, height=0.175\\textwidth]{./fig/cataccVSclassnum.pdf}\n }\n\\end{tabular}\n\\caption{Performance comparison of the conditional log-likelihood loss and the softmax loss for image segmentation on the NYU v2 dataset.}  \\label{fig:acc_class}\n\\end{figure}\n\n\n\n\\begin{table*}\n\\center\n\\resizebox{.6\\linewidth}{!} {\n\\begin{tabular}{  l |  c  c  c c || c  c  }\n\\hline\n&Ground  &Structure &Furniture &Props  &Pix. Acc.  &Class Acc. \\\\\n\\hline\n\\hline\nGupta {\\emph{et al}\\onedot} \\cite{Gupta13} &- &- &- &- &78.0\t &-\t \\\\\nCouprie {\\emph{et al}\\onedot} \\cite{Couprie14} (RGB+D)  &87.3 &86.1 &45.3 &35.5 &64.5  &63.5  \\\\\nKhan {\\emph{et al}\\onedot} \\cite{Khan15} &87.1 &88.2 &54.7 &32.6 &69.2  &65.6 \\\\\nMuller {\\emph{et al}\\onedot} \\cite{Muller14} &\\textbf{94.9} &78.9 &71.1 &42.7 &72.3 &71.9 \\\\\nEigen {\\emph{et al}\\onedot} (RGB) \\cite{Eigen15}    &- &- &- &-  &75.3  &- \\\\\nEigen {\\emph{et al}\\onedot} (RGB+D+N) \\cite{Eigen15}\t&93.9 &87.9 &79.7 &55.1 &80.6\t&79.1\t\\\\\n\\hline\nOurs (unary) &84.3 &84.8 &77.3 &54.6  &77.1  &75.8   \\\\\nOurs (full, RGB) &88.0  &89.8  &77.7  &58.0 &80.8\t&78.7\t \\\\\nOurs (full, RGB+D)  &92.9  &\\textbf{88.7}  &\\textbf{81.7}  &\\textbf{61.7}  &\\textbf{82.5} &\\textbf{81.2} \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{Segmentation results on the NYU v2 dataset (4-class).\n  Note that our full model even outperforms  the RGB+D+N model in \\cite{Eigen15} which uses additional ground-truth depth and surface normal\n  information.\n}\n\\label{tab:seg_nyu4}\n\\end{table*}\n\n\n\n\n\\begin{table} \\center\n\\resizebox{.99\\linewidth}{!} {\n\\begin{tabular}{  l |  c  c  c c}\n\\hline\n&Pix. Acc.  &Class Acc. &Avg. Jaccard  &Freq. Jaccard\\\\\n\\hline\n\\hline\nGupta {\\emph{et al}\\onedot} \\cite{Gupta13} &59.1   &28.4   &27.4  &45.6   \\\\\nGupta {\\emph{et al}\\onedot} \\cite{Gupta14} &60.3   &35.1   &28.6  &47.0   \\\\\nKhan {\\emph{et al}\\onedot} \\cite{Khan15}  &50.7 &\\textbf{43.9}  &- &42.1\\\\\nLong {\\emph{et al}\\onedot} \\cite{Long15} (RGB) &60.0 &42.2 &29.2 &43.9 \\\\\nLong {\\emph{et al}\\onedot} \\cite{Long15} (RGB+D) &61.5  &42.4 &30.5 &45.5 \\\\\nEigen {\\emph{et al}\\onedot} \\cite{Eigen15} (RGB+D+N)      &62.9     &41.3   &\\textbf{30.8}  &47.6   \\\\\n\\hline\nOurs (unary) &58.8    &38.2   &27.1  &44.6\\\\\nOurs (full, RGB) &62.6    &42.2   &30.5  &\\textbf{49.1} \\\\\nOurs (full, RGB+D) &\\textbf{63.1}\t &39.0 &\t29.5  &48.4 \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{State-of-the-art comparison on the NYU v2 dataset (40-class).}\n\\label{tab:seg_nyu40}\n\\end{table}\n\n\n\n\n\n\\begin{table*}\n\\resizebox{.99\\linewidth}{!} {\n\\begin{tabular}{  l |  c  c  c c c  c  c c c c c  c  c c c  c  c c c c }\n\\hline\n &wall &floor &cabinet    &bed    &chair    &sofa    &table    &door    &window  &bookshelf\n &picture  &counter &blinds &desk &shelves &curtain &dresser &pillow &mirror &floor mat  \\\\\n\\hline\nGupta {\\emph{et al}\\onedot} \\cite{Gupta14} &68.0 &81.3 &44.9 &\\textbf{65.0} &\\textbf{47.9} &\\textbf{47.9} &29.9 &20.3 &32.6 &18.1 &40.3 &51.3 &42.0 &\\textbf{11.3} &3.5 &29.1 &\\textbf{34.8} &\\textbf{34.4} &16.4 &\\textbf{28.0} \\\\\nEigen {\\emph{et al}\\onedot} \\cite{Eigen15} &\\textbf{68.2} &\\textbf{83.3}  &45.0  &51.9  &46.0 &40.4 &\\textbf{32.0} &17.6 &33.0 &31.5 &45.0 &\\textbf{53.4} &43.9 &\\textbf{11.3} &\\textbf{8.9} &\\textbf{35.4} &26.3 &31.0 &\\textbf{32.6}  &27.4 \\\\\nOurs &67.7  &75.9  &\\textbf{46.8}  &52.2  &43.8  &46.3  &29.0  &\\textbf{27.3}  &\\textbf{33.2}  &\\textbf{35.9}  &\\textbf{48.6}  &41.9  &\\textbf{46.6}  &9.0  &7.8  &32.9  &28.9  &24.6  &18.1  &23.0 \\\\\n\\hline\n\\hline\n\\end{tabular}\n}\n\\resizebox{.99\\linewidth}{!} {\n\\begin{tabular}{l | c  c  c c c  c  c c c c c  c  c c c  c  c c c c}\n&clothes &ceiling    &books    &fridge    &tv    &paper    &towel  &s-curtain   &box &w-board    &person    &n-stand    &toilet    &sink    &lamp  &bathtub    &bag    &other-struc  &other-fur    &other-prop \\\\\n\\hline\nGupta {\\emph{et al}\\onedot} \\cite{Gupta14}   &4.7 &60.5 &6.4 &14.5 &31.0 &14.3 &16.3 &4.2 &2.1 &14.2 &0.2 &\\textbf{27.2} &\\textbf{55.1} &37.5 &\\textbf{34.8} &\\textbf{38.2} &0.2 &7.1 &6.1 &23.1 \\\\\nEigen {\\emph{et al}\\onedot} \\cite{Eigen15} &12.9 &\\textbf{75.2} &16.6 &15.7 &32.3 &\\textbf{20.3} &14.8 &21.7 &4.5 &6.6 &27.5 &24.2 &44.8 &\\textbf{39.4} &33.6 &29.1 &1.5 &11.7 &6.8 &\\textbf{29.4}  \\\\\nOurs  &\\textbf{15.6}  &52.6  &\\textbf{19.3}  &\\textbf{22.3}  &\\textbf{40.7}  &17.2  &\\textbf{22.5}  &\\textbf{24.1}  &\\textbf{5.6}  &\\textbf{46.9}  &\\textbf{47.6}  &19.9  &50.6  &37.8  &24.1  &10.3  &\\textbf{4.2}  &\\textbf{13.4}  &\\textbf{10.4}  &26.3 \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{The per-class pixel-wise Jaccard index for each of the 40 categories on the NYU v2 dataset.\nOur method performs the best on 18 out of the 40 classes. Note that Eigen {\\emph{et al}\\onedot} \\cite{Eigen15}\nuse extra surface normal information, and their network takes multi-scale input images.\n}\n\\label{tab:seg_nyu40_cat}\n\\end{table*}\n\n\n\n\n\n\n\\begin{figure}  \\center\n\\begin{tabular}{c c}\n{\n      \\includegraphics[width=0.2175\\textwidth, height=0.175\\textwidth]{./fig/noise_comp.pdf}\n      \\includegraphics[width=0.2175\\textwidth, height=0.175\\textwidth]{./fig/outlier_comp.pdf}\n }\n\\end{tabular}\n\\caption{Prediction accuracies of the log-likelihood loss and the robust regression loss for depth estimation with respect to different percentages of noises/outliers on the NYU v2 dataset.}  \\label{fig:acc_noise}\n\\end{figure}\n\n\n\n\\begin{table*}\n\\center\n\\resizebox{.55\\linewidth}{!} {\n\\begin{tabular}{|  l | l |  c  c  c | c  c  c |}\n\\hline\n\\multirow{3}{*}{{{Noise level}}} &\\multirow{3}{*}{{{Method}}} &\\multicolumn{3}{c|}{Error} &\\multicolumn{3}{c|}{Accuracy} \\\\\n& &\\multicolumn{3}{c|}{(lower is better)} &\\multicolumn{3}{c|}{(higher is better)} \\\\\n\\cline{3-8}\n& &rel &log10 &rms &$\\delta < 1.25$ &$\\delta < 1.25^2$ &$\\delta < 1.25^3$  \\\\\n\\hline\n\\hline\n\n\n\n\\multirow{2}{*}{{{$0\\%$}}} &Log loss  &0.223 &0.094 &0.835  &0.611& 0.891 &0.973 \\\\\n &Robust loss &\\textbf{0.219} &\\textbf{0.091} &\\textbf{0.787}  &\\textbf{0.632} &\\textbf{0.894}  &\\textbf{0.974}\t\\\\\n\\hline\n\\multirow{2}{*}{{{$10\\%$ noise}}}  &Log loss\t&0.263  &0.115 &1.023 &0.518 &0.815 &0.945  \\\\\n&Robust loss  &\\textbf{0.222}  &\\textbf{0.092} &\\textbf{0.790} &\\textbf{0.628} &\\textbf{0.896} &\\textbf{0.975}  \\\\\n\\hline\n\\multirow{2}{*}{{{$25\\%$ noise}}}  &Log loss \t &0.369 &0.130  &1.256  &0.461  &0.764 &0.915\\\\\n&Robust loss  &\\textbf{0.218}  &\\textbf{0.093} &\\textbf{0.815} &\\textbf{0.618} &\\textbf{0.894} &\\textbf{0.976} \\\\\n\\hline\n\\multirow{2}{*}{{{$10\\%$ outlier}}}  &Log loss\t&0.786 &0.211 &2.152  &0.246 &0.477 &0.685 \\\\\n&Robust loss &\\textbf{0.221} &\\textbf{0.092}\t&\\textbf{0.812}\t&\\textbf{0.617}  &\\textbf{0.895}  &\\textbf{0.974} \\\\\n\\hline\n\\multirow{2}{*}{{{$25\\%$ outlier}}}  &Log loss &0.951 &0.261 &2.503 &0.150 &0.371 &0.617 \\\\\n&Robust loss &\\textbf{0.235} &0\\textbf{0.094} &\\textbf{0.819} &\\textbf{0.611} &\\textbf{0.888} &\\textbf{0.972} \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{Comparisons of depth estimation performance of different loss functions with respect to different noise/outlier ratios on the NYU v2 dataset.} \\label{tab:depth_noisy}\n\\end{table*}\n\n\n\n\n\\begin{table}\n\\center\n\\resizebox{0.99\\linewidth}{!} {\n\\begin{tabular}{ | l |  c  c  c | c  c  c |}\n\\hline\n\\multirow{3}{*}{{{Method}}} &\\multicolumn{3}{c|}{Error} &\\multicolumn{3}{c|}{Accuracy} \\\\\n&\\multicolumn{3}{c|}{(lower is better)} &\\multicolumn{3}{c|}{(higher is better)} \\\\\n\\cline{2-7}\n&rel &log10 &rms &$\\delta < 1.25$ &$\\delta < 1.25^2$ &$\\delta < 1.25^3$  \\\\\n\\hline\n\\hline\nSaxena {\\emph{et al}\\onedot} \\cite{make3d_pami09} &0.280  &-  &8.734  &0.601 &0.820  &0.926 \\\\\nEigen {\\emph{et al}\\onedot} \\cite{dcnn_nips14} &\\textbf{0.190} &- &7.156 &\\textbf{0.692} &\\textbf{0.899} &\\textbf{0.967} \\\\\nLiu {\\emph{et al}\\onedot} \\cite{pami15} &0.217 &0.092 &7.046  &0.656 &0.881 &0.958  \\\\\n\\hline\nOurs &0.203\t&\\textbf{0.086}\t&\\textbf{6.427} &0.684  &0.894  &0.965\t \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{Comparisons of depth estimation performance on the KITTI dataset.\nThe results of \\cite{dcnn_nips14} are obtained by using millions of training images while ours are obtained by training on 700 images.}\n\\label{tab:depth_kitti}\n\\end{table}\n\n\n\n\n\\section{Experiments}\n\nTo demonstrate the effectiveness of the proposed method,\nwe perform experiments on the tasks of multi-class semantic labelling and robust depth estimation from single images.\nThe first one is discrete-valued labelling and the second one is continuously-valued labelling.\n\n\n\\paragraph{Implementation details}\nWe implement our method based on the popular {{CNNs}\\xspace} toolbox  MatConvNet\\footnote{http://www.vlfeat.org/matconvnet/}.\nThe network training is performed on a standard desktop with an NVIDIA GTX 780 GPU.\nWe set the momentum as $0.9$ and the weight decay parameter as $\\lambda=0.0005$.\nThe first $5$ convolution blocks and the first layer of the $6$th convolution block of the unary network in Fig. \\ref{fig:cnn_net} are initialized from the VGG-16 \\cite{vgg16} model.\nAll layers are trained using back-propagation.\nThe learning rate for randomly initialized layers is set to $10^{-5}$; for\nVGG-16 initialized layers it is set to a smaller value $10^{-6}$.\nThe hyperparameter of the position Gaussian kernel is set to $\\gamma=0.1$.\nThe parameter $c$ in the Turkey's biweight loss (Eq.~\\eqref{eq:turkeyloss}) is set to $1$.\nWe use SLIC \\cite{slic} to generate  $\\sim 700$ superpixels for each image.\n\n\n\n\n\n\\subsection{Multi-class semantic labelling}\nThe semantic labelling experiments are evaluated on the NYU v2 \\cite{Silberman12} and the MSRC-21 \\cite{Shotton06} datasets.\nThe NYU v2 dataset consists of 795 images for training and 654 images for test\n(we use the standard training/test split provided with the dataset).\nFor the semantic label ground-truth, we use the 4 and 40 classes described in \\cite{Silberman12}, \\cite{Gupta13} respectively.\nThe performance are evaluated using the commonly applied metrics as in \\cite{Gupta14,Couprie14,Khan15,Eigen15},\nnamely, pixel-wise accuracy (Pix.\\ Acc.) and average per-class accuracy (Class Acc.).\nFor the 40-class segmentation task, we also report the average Jaccard index (Avg.\\ Jaccard)\nand the average pixel-frequency weighted Jaccard index (Freq.\\ Jaccard) for each class.\nThe MSRC-21 dataset is a popular multi-class segmentation benchmark with 591 images containing objects from 21 categories.\nWe follow the standard split to divide the dataset into training/validation/test subsets.\n\n{\\bf NYU v2}\nSince ground-truth depth maps are provided along with this dataset, we exploit depth cues as an additional setting (denoted as RGB+D).\nSpecifically, we replicate the depth channel into 3 duplicates and regard them as images to input into our framework.\nThe {{CNNs}\\xspace} features from the RGB channels and the depth channels are concatenated after the superpixel pooling layer.\nThe compared results together with the per-class accuracy on the NYU v2 4-class segmentation task are reported in Table \\ref{tab:seg_nyu4}.\nAs we can see, {\\em our full model with incorporated depth cues outperforms state-of-the-art methods by a considerable margin}.\nIt is worth mentioning that our method outperforms Eigen {\\emph{et al}\\onedot} \\cite{Eigen15}, which used RGB images together with depths and surface normals.\nCompared with the unary-only model, our full model which jointly learns the unary and the pairwise networks shows consistent\nimprovements in terms of all the metrics, demonstrating that the continuous {{CRFs}\\xspace} model indeed improves discrete labelling.\nFig. \\ref{fig:nyud2_suppl} shows some qualitative examples of our method together with the unary baseline.\nCompare to the unary predictions, our method yields more accurate and smoother labellings.\n\n\nTable \\ref{tab:seg_nyu40} summarizes the results on the 40-class segmentation task.\nOur method generally achieves better or comparable performance against state-of-the-art methods \\cite{Long15,Eigen15}.\nSpecifically, with RGB only as the input, our method outperforms \\cite{Long15} using similar settings.\nNote that in \\cite{Long15}, Long {\\emph{et al}\\onedot} achieved better results by using HHA encoding,  which reveals that similar strategies can be applied here to further improve our results.\nIn \\cite{Eigen15}, Eigen {\\emph{et al}\\onedot} have used RGB images together with the ground-truth depths and surface normals as the input to their network training.\nYet, our method achieves comparable performance.\nWith depth channels incorporated, our method obtains the best global labelling accuracy.\nOur full model again shows improvements over the unary only model, demonstrating the usefulness of the joint learning scheme.\nThe per-class pixel-wise Jaccard indexes of the 40 object categories are presented in Table \\ref{tab:seg_nyu40_cat},\nwith our method achieves the best results on 18 out of the 40 classes.\n\n\n\n{\\bf MSRC-21}\nTable \\ref{tab:seg_msrc} reports the segmentation results on the MSRC-21 dataset.\nAs we can see, our results performs comparable to state-of-the-art methods \\cite{Roy14}, \\cite{pr15}.\n\n\\begin{table}[h!] \\center\n\\resizebox{.75\\linewidth}{!} {\n\\begin{tabular}{  l |  c  c  }\n\\hline\n&Pix.\\ Acc.\\  &Class Acc.\\   \\\\\n\\hline\n\\hline\nLucchi {\\emph{et al}\\onedot} \\cite{Lucchi13}  &83.7  &78.9 \\\\\nRoy {\\emph{et al}\\onedot} \\cite{Roy14} &91.5$^*$  &- \\\\\nLiu {\\emph{et al}\\onedot} \\cite{pr15}  &88.5 &86.7 \\\\\nLiu {\\emph{et al}\\onedot}  \\cite{pr15} (co-occur)  &91.1$^*$ &\\textbf{90.5}$^*$ \\\\\n\\hline\n\nOurs (softmax loss, unary) &89.6\t &86.4  \\\\\nOurs (softmax loss, full) &\\textbf{91.4}    &89.0  \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{Segmentation results on the MSRC-21 dataset.\n  Note that the results marked with  $ ^*$    are achieved by using additional\n   mutex/co-occurrence information that models repelling   relations.\n}\n\\label{tab:seg_msrc}\n\\end{table}\n\n\n\n\n\\paragraph{Ablation Study}\nTo show the benefits of the multi-class classification loss over the tradition log-likelihood {{CRFs}\\xspace} training,\nwe perform ablation study on the performance comparisons of the two loss functions with respect to different numbers of classes.\nIn addition to the 4-class and the 40-class segmentation tasks, we further perform experiments on a 13-class segmentation task by mapping the 40 categories into 12 main classes and a `background' class. Here the network structures are identical and the only difference is the loss function.\nFig.\\ \\ref{fig:acc_class} shows the comparative performance.\nWe report both pixel-wise accuracy and per-category accuracy.\n    As illustrated in the figure,\n    {\\em the multi-class soft-max loss considerably outperforms the log-likelihood loss}.\n    The discrepancy becomes more significant when the number of classes increases.\n    The previously-reported inferior performance of continuous {{CRFs}\\xspace} on discrete labelling may be attributed to the use of the \n    traditional maximum likelihood parameter learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Robust depth estimation}\nWe perform robust depth estimation on two datasets: the indoor NYU v2 \\cite{Silberman12} and the outdoor KITTI \\cite{kitti} datasets.\nSeveral commonly used evaluation metrics, {\\emph{i.e}\\onedot}, \nrelative error (rel), log10 error (log10), root mean square error (rms) and accuracy with different thresholds $\\delta$ (details are referred to \\cite{cvpr15,pami15}),\nare used here to report the results.\n\n\n\\paragraph{NYU v2}\nThe NYU v2 dataset \\cite{nyud2_eccv12} consists of 1449 RGB-D indoor image pairs, with 795 are used for training and 654 are for testing.\nWe have used the standard training/test split provided along with the dataset.\n\nIn addition to performance comparisons on the original dataset, we also add additive white Gaussian noises or outliers to the ground-truth depths.\nFor the noise case, we add additive white Gaussian noise with $\\sigma=0.1$ to the $[0,1]$ normalized ground-truth depths.   A certain percentage of pixels ({\\emph{e.g}\\onedot}, 10\\%, and 25\\% as shown in Table \\ref{tab:depth_noisy})\nare uniformly sampled to be added the noise.\nLikewise, for the outliers, we simply add a large value to the ground-truth of\na certain percentage of uniformly sampled pixels.\nTable \\ref{tab:depth_noisy} reports the compared results of the log-likelihood loss (denoted as `log loss')\nand the robust regression loss with respect to different noise/outlier levels.\nAs we can observe, the robust regression loss is insensitive to both noises and outliers.\nIn contrast, the performance of the log-likelihood loss degrades severely when the noise/outlier level increases.\n\n\n\n\nFig.\\ \\ref{fig:acc_noise} illustrates the compared prediction accuracies of the two loss functions.\nThe horizontal axis shows different noise (left plot) or outlier (right plot) percentages,\nand the vertical axis shows the depth prediction accuracy with $\\delta<1.25$.\n\n\n\n\n\\paragraph{KITTI}\nThe KITTI dataset \\cite{kitti} consists of outdoor videos taken from a driving vehicle equipped with a LiDaR sensor.\n\nThe ground-truth depths of the KITTI dataset are scattered at irregularly spaced points, which only consists of $\\sim 5\\%$ pixels of each image, with all others unlabelled.\nWe here treat those unlabelled regions as outliers and train the network using robust regression loss.\nThe results are reported by using the same test set, {\\emph{i.e}\\onedot}, 697 images from 28 scenes, as provided by Eigen {\\emph{et al}\\onedot} \\cite{dcnn_nips14}.\nAs for the training set, we use the same 700 images that Eigen {\\emph{et al}\\onedot} \\cite{dcnn_nips14} have used to train the method of \\cite{make3d_pami09}.\nThe compared results are reported in Table \\ref{tab:depth_kitti}.\nIt can be seen that our method outperforms the most recent work of \\cite{pami15} which have used the same training set,\nand very similar {{CNNs}\\xspace} structure as ours.\nCompared to \\cite{dcnn_nips14}, which have used millions of training images, our results are very competitive with the lowest rms error achieved.\nFig.~\\ref{fig:kitti} shows some prediction examples of our method.\n\n\n\nOn both NYU v2 and KITTI datasets, our method using robust loss outperforms  the log-likelihood loss,\neven with no noises   deliberately   added, because (a) original data may contain some noises more or less;\n(b) more importantly our method directly optimizes the task-specific empirical risk of interest.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe show some depth prediction examples on the NYU v2 dataset in Fig. \\ref{fig:nyud2_suppl}.\nAs we can see, our predictions well preserve depth discontinuities and align to local details.\n\n\n\\begin{figure*} \\center\n\\resizebox{.7\\linewidth}{!} {\n\\begin{tabular}{cccc}\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/0302.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/0302.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_302.png}   \\\\\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/0312.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/0312.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_312.png}   \\\\\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/0330.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/0330.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_330.png}   \\\\\n\n\n\n\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/0976.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/0976.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_976.png}   \\\\\n\n\t\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/1082.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/1082.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_1082.png}   \\\\\n\n\t\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/1148.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/1148.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_1148.png}   \\\\\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/1292.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/1292.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_1292.png}   \\\\\n\n      \\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/img/1314.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/gt/1314.png}\n      &\\includegraphics[width=0.3\\textwidth, height=0.22\\textwidth]{./fig/examples/nyud/pred/img_1314.png}   \\\\\n\n\nTest image &Ground-truth  &Our prediction \\\\\n\n\\end{tabular}\n}\n\\caption{Depth prediction examples of our method on the NYU v2 dataset. }  \\label{fig:nyud2_suppl}\n\\end{figure*}\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\nWe have proposed a fully-connected deep continuous {{CRFs}\\xspace} learning method for both discrete and continuous pixel-level labelling problems.\nThe proposed method inherits the advantages of continuous {{CRFs}\\xspace}, {\\emph{i.e}\\onedot}, exact maximum likelihood learning and closed-form solution for the MAP inference.\nSpecifically, it models the unary and the pairwise potentials of the {{CRFs}\\xspace} as two {{CNNs}\\xspace} and learns the network parameters in an end-to-end fashion.\nFor different tasks,  we further propose to optimize a task-specific loss function, rather than the traditional log-likelihood loss in learning {{CRFs}\\xspace} parameters,  and demonstrate two applications, which are semantic labelling and robust depth estimation.\nExperimental results show that (a) the proposed method improves the unary-only model (standard regression without considering structured constraints), indicating the effectiveness of the pairwise term of our model;\n(b) our method achieves better or on par results compared with state-of-the-art, even on discrete labelling tasks,\nshowing the promise of the proposed deep structured model.\n    We thus advocate the use deep continuous {{CRFs}\\xspace} in more structured prediction problems in vision, due\n    to its power and in particular, significantly simpler and efficient inference, compared with most discrete deep {{CRFs}\\xspace} such as \\cite{Lin15a,Lin15b}.\n\n\n\n{\\small\n   \\bibliographystyle{ieee-cs}\n   \\bibliography{seg.bib}\n}\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 27838, "prevtext": "\nHowever,   LS loss is generally not robust to outliers. Actually all convex regression loss functions are considered not robust.\nFor example, for those extremely large residuals (typically outliers), the LS loss imposes too large penalty, which inevitably biases the model towards outliers.\n\nTo fully exploit the flexibility of the proposed task-specific CRF parameter learning,\nwhich allows us to choose arbitrary learning criterion, here we\n  consider a non-convex, robust regression loss function, namely, the Turkey's biweight loss:\n\n", "index": 35, "text": "\\begin{equation}\\label{eq:turkeyloss}\n\\rho(r_p) = \\left\\{\n\\begin{array}{l l}\n     \\frac{c^2}{6} \\big[1- (1-\\frac{r_p^2}{c^2})^3 \\big],&\\text{if}\\; |r_p| < c \\\\\n    \\frac{c^2}{6}, &\\text{otherwise.}\n\\end{array}\\right.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"\\rho(r_{p})=\\left\\{\\begin{array}[]{l l}\\frac{c^{2}}{6}\\big{[}1-(1-\\frac{r_{p}^%&#10;{2}}{c^{2}})^{3}\\big{]},&amp;\\text{if}\\;|r_{p}|&lt;c\\\\&#10;\\frac{c^{2}}{6},&amp;\\text{otherwise.}\\end{array}\\right.\" display=\"block\"><mrow><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mfrac><msup><mi>c</mi><mn>2</mn></msup><mn>6</mn></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><msubsup><mi>r</mi><mi>p</mi><mn>2</mn></msubsup><msup><mi>c</mi><mn>2</mn></msup></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow><mn>3</mn></msup></mrow><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+2.8pt\"><mtext>if</mtext></mpadded><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>r</mi><mi>p</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>&lt;</mo><mi>c</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mfrac><msup><mi>c</mi><mn>2</mn></msup><mn>6</mn></mfrac><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mtext>otherwise.</mtext></mtd></mtr></mtable><mi/></mrow></mrow></math>", "type": "latex"}]