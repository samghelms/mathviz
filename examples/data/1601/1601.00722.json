[{"file": "1601.00722.tex", "nexttext": "\nwhere $\\mathbf x \\in\\mathbb{R}^I$, $\\mathbf y\\in\\mathbb{R}^K$ are the binary states of visible units   and hidden units, $\\mathbf b\\in \\mathbb{R}^I$ and $\\mathbf c\\in\\mathbb{R}^K$ are the biases, and $W \\in {\\mathbb{R}^{I \\times K}}$ represents visible-to-hidden symmetric interaction terms in the neural network. Denote by $\\Theta  = \\left\\{ \\mathbf b,\\mathbf c, W \\right\\}$ all the model parameters.\n\nTo introduce our proposed MVRBM, we define the following notations. Denote by $X = [x_{ij}]\\in\\mathbb{R}^{I\\times J}$ the binary visible matrix variate, and $Y=[y_{kl}]\\in\\mathbb{R}^{K\\times L}$ the binary hidden matrix variate. We assume that the independent random variables $x_{ij}$ and $y_{kl}$ all take values from $\\{0,\\ 1\\}$. Given the parameters of a 4-order tensor $\\mathcal{W}=[w_{ijkl}]\\in\\mathbb{R}^{I\\times J\\times K\\times L}$, bias matrices $B=[b_{ij}]\\in\\mathbb{R}^{I\\times J}$ and $C=[c_{kl}]\\in\\mathbb{R}^{K\\times L}$, we define the following energy function for a joint configuration $(X, Y)$,\n\n", "itemtype": "equation", "pos": 11180, "prevtext": "\n\n\n\\title{Matrix Variate RBM and Its Applications}\n\n\n\n\\author{Guanglei Qi$^1$, Yanfeng Sun$^1$, Junbin Gao$^2$, Yongli Hu$^1$ and Jinghua Li$^1$\\\\\n{\\small $^1$College of Metropolitan Transportation, Beijing University of Technology, Beijing 100124, China}\\\\\n{\\small $^2$Business Analytics Discipline, The University of Sydney Business School, Camperdown NSW 2006, Australia}\\\\\n{\\tt\\small qgl@emails.bjut.edu.cn; \\{huyongli,yfsun,lijinghua\\}@bjut.edu.cn; junbin.gao@sydney.edu.au}}\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nRestricted Boltzmann Machine (RBM) is an important generative model modeling vectorial data. While applying an RBM in practice to images, the data have to be vectorized. This results in high-dimensional data and valuable spatial information has got lost in vectorization. In this paper, a  Matrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed by generalizing the classic RBM to explicitly model matrix data. In the new RBM model, both input and hidden variables are in matrix forms which are connected by bilinear transforms. The MVRBM has much less model parameters, resulting in a faster training algorithm while retaining comparable performance as the classic RBM. The advantages of the MVRBM have been demonstrated on two real-world applications: Image super-resolution and handwritten digit recognition.\n\n\\end{abstract}\n\n\n\\section{Introduction}\\label{Sec:1}\nA Boltzmann machine as a type of stochastic recurrent neural network was invented by Hinton and Sejnowski in 1985~\\cite{HintonSejnowski1983}. However it is not efficient to use the generic Boltzmann machines in machine learning or inference due to its unconstrained connectivity among variable units. To make a practical model, Hinton~\\cite{Hinton2002} proposes an architecture called the \\textit{Restricted Boltzmann Machine} (RBM), only units between visible layer and hidden layer connected.\n\nWith the restricted connectivity between visible and hidden units, an RBM can be regarded as a probabilistic graphical model with bipartite graph structure. In recent years, RBMs have attracted considerable research interest in pattern recognition~\\cite{Bishop2006,SocherChenManningAndrew2013} and machine learning~\\cite{Bengio2009,HintonSalakhutdinov2009,KrizhevskyHinton2010,MemisevicHinton2010,TielemanHinton2009}, due to their strong ability in feature extraction and representation.\n\nUnits at visible and hidden layers are connected through the restricted linear mapping with weights to be trained. Given some training data, the goal of training a RBM model is to learn the weights between visible and hidden units such that the probability distribution represented by a RBM fits the training samples as well as possible. A well trained RBM can provide efficient representation for new input data following the  same distribution as training data.\n\nThe classic RBM model is mainly designed for vectorial input data or variables. However, data emerging from modern science and technology are in more general structures. For example, digital images are collected as 2D matrices, which reflect the spatial correlation or information among pixels.  In order to apply the classic RBM to such 2D image data, a typical workaround  is to vectorize 2D data. Unfortunately such as a vectorization process not only breaks the inherent high-order image structure, resulting in losing important information about interaction across modes, but also leads to increasing the number of model parameters induced by a full connection between visible and hidden units.\n\nTo extend the classic RBM for 2D matrix data, in this paper, we propose a Matrix-Variate Restricted Boltzmann Machine (MVRBM) model. Like the classic RBM, the MVRBM model also defines a probabilistic model for binary units arranged in a bipartite graph, but topologically units on the same layer (input or hidden) are organized in 2D arrays and connected through a bilinear mapping, see Section~\\ref{Sec:4}. In fact, the proposed bilinear mapping specifies a specific structure in the parameters of the model, thus gives raise to reduce the number of parameters to be learned in training process.\n\nIn summary, the new model has the following advantages which make up our contributions in this paper:\n\\begin{enumerate}\n\\item The total number of parameters to be learned is significantly less than that in the traditional RBMs, thus the computational complexity in training and inferring can be significantly improved.\n\n\\item Both the visible layer and hidden layer are organized in the matrix format, thus the spatial information in 2D matrix data can be maintained in the training and inference processes and better performance in reconstruction can be achieved.\n\n\\item The idea presented in MVRBM can be easily extended to any order tensorial data, thus the basic RBM can be applied to more complex data structures.\n\\end{enumerate}\n\nThe rest of the paper is organized as follows. In Section \\ref{Sec:2}, we summarize the related works to further highlight our contributions.  In Section \\ref{Sec:4}, the MVRBM model is introduced and a stochastic learning algorithm based on \\textit{Contrast Divergence} (CD) is proposed. In Section \\ref{Sec:5}, the performance of the proposed method is evaluated on two computer vision tasks handwritten digit recognition and image super-resolution. Finally, conclusions and suggestions for future work are provided in Section~\\ref{Sec:6}.\n\n\\section{Related Works}\\label{Sec:2}\nThere have been more and more multiway data acquired in modern scientific and engineering research, e.g., medical images~\\cite{AdaliLevinCalhoun2015,LuHaligWangChenFei2014}, multispectral images~\\cite{BernabeMarpuPlazaMuraBenediktsson2014,Garzelli2015}, and video clips~\\cite{GuyByrneRich2014} etc. \nIt is well known that vectorizing multiway data results in correlation information loss, thus downgrade the performance of learning algorithm for vectorial data like the classic RBMs. In recent years, research works on learning algorithms for multiway data modeling have attracted great attention.\n\nRovid et al.~\\cite{RovidSzeidlVarlaki2011} propose a tensor-product model based representation of neural networks in which a neural network structure for mutual interaction of variable components is introduced. It conceptually restructures the tensor product model transformation to a generalized form for fuzzy modeling and to propose new features and several new variants of the tensor product model transformation. However this type of neural networks is actually defined for vectorial data rather than tensorial variates. The similar idea can be seen in the most recent paper~\\cite{HutchinsonDengYu2013}. The key characterization for these networks is the connection weights (neural networks parameters) are in tensor format, rather than the data variables in the networks. Thus except for the nonlinearity introduced by the activation function, the neural networks offer the capacity of encoding nonlinear interaction among the hidden variable components. The so-called tensor analyzer~\\cite{TangSalakhutdinovHinton2013} also serves as such an example.\n\nSocher et al.~\\cite{SocherChenManningAndrew2013} present another similar work. It uses the similar structure as proposed in~\\cite{HutchinsonDengYu2013} to generalize several previous neural network models and provide a more powerful way to model correlation information than a standard neural network layer.\n\nThere are several works on multiple ways Boltzmann machine~\\cite{TaylorHinton2009,ZhaoAmmarRoos2013}. Taylor and  Hinton~\\cite{TaylorHinton2009} propose a factored conditional restricted Boltzmann Machines for modeling motion style. In order to capture context of motion style, this model takes  history and current information as input data, thus connections from the past to current visible units and the hidden units are increased. In this model, the input data consist of two vectors, and the output data is also in vector form and the weights between visible and hidden units are matrix.\n\nZhao et al.~\\cite{ZhaoAmmarRoos2013} use some video sequences for training RBM to get better classification performance. The video sequences are also vectorized as some vectors, used as the input to a classic RBM with a connection defined by a tensor weight.\n\nAll the above attempts aim to model the interaction between the components of vector variates. Similar to the classic RBM, these models are not appropriate for the matrix inputs. To the best of our knowledge, the first work aiming at modeling matrix variate inputs is proposed by Nguyen et al.~\\cite{NguyenTranPhungVenkatesh2015}, named \\textit{Tensor-variate Restricted Boltzmann Machines} (TvRBM) model. The authors have demonstrated the capacity of the model on three real-world applications with convincible performance.\nIn its model architecture, the input is designed for tensorial data including matrix data while the hidden units are organized as a vector. The connection between the hidden layer and the visible layer is defined by the linear combination over tensorial weights. To reduce the number of weight parameters, the weight tensors are further specified as the so-called rank-r tensors~\\cite{KoldaBader2009}. However our criticism over TvRBM is that the specification of the rank-r tensor weights is too restrictive to efficiently empower the model capability.\n\nAnother model related to our proposed MVRBM is the so-called \\textit{Replicated Softmax RBM} (RS-RBM)~\\cite{SalakhutdinovHinton2010}. Similar to TvRBM in~\\cite{NguyenTranPhungVenkatesh2015}, RS-RBM uses a linear mapping between a matrix input layer and a hidden vector layer. To model document topics in terms of word counts, an implicit condition is imposed on the matrix input, i.e., the sum of the binary entries of each row in the matrix input must be 1. Thus the Replicated Softmax model is actually equivalent to an RBM of vector softmax input units with identical weights for each unit.\n\nOur proposed MVRBM in the next section is different from both TvRBM and RS-RBM in several aspects. First, the binary entries in matrix input for MVRBM are independent as that in TvRBM while they are dependent in RS-RBM. Second, the hidden layer units in MVRBM are in a matrix format rather than in a vector format as in both RS-RBM and TvRBM. Third, the linear mapping between input and hidden layers in MVRBM is bilinear.\n\n\\section{Matrix Variate Restricted Boltzmann Machines (MVRBM)}\\label{Sec:4}\nIn this section, we will present the proposed MVRBM and investigate its learning algorithm.\n\n\\subsection{Model Definition}\n\nThe classic RBM~\\cite{FischerIgel2012,HintonSalakhutdinov2006} is a bipartite undirected probabilistic graphical model with stochastic visible units $\\mathbf x$ and stochastic hidden units $\\mathbf y$, both are in vector. The model is shown in Figure~\\ref{figure1} where each visible unit (represented by a cubic) is connected to each hidden unit (represented by a cylinder).\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.3\\textwidth]{figure1.pdf}\n\\caption{Graphical Illustration of RBM.}\n\\label{figure1}\n\\end{figure}\n\nThe RBM assigns energy for a joint configuration $(\\mathbf x,\\mathbf y)$:\n\n", "index": 1, "text": "\\begin{equation}\nE(\\mathbf x,\\mathbf y;\\Theta ) =  -\\mathbf x^T W\\mathbf y - \\mathbf b^T\\mathbf x - \\mathbf c^T \\mathbf y, \\label{Energy1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"E(\\mathbf{x},\\mathbf{y};\\Theta)=-\\mathbf{x}^{T}W\\mathbf{y}-\\mathbf{b}^{T}%&#10;\\mathbf{x}-\\mathbf{c}^{T}\\mathbf{y},\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo>,</mo><mi>\ud835\udc32</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><msup><mi>\ud835\udc31</mi><mi>T</mi></msup><mo>\u2062</mo><mi>W</mi><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc1b</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc31</mi></mrow><mo>-</mo><mrow><msup><mi>\ud835\udc1c</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc32</mi></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere $\\Theta = \\{\\mathcal{W}, B, C\\}$ collects all the parameters.\n\nThere are a total number of $I\\times J\\times K\\times L + I\\times J+ K\\times L$ free parameters in $\\Theta$. This is a huge number even for mild values of $I$, $J$, $K$ and $L$ and requires a large amount of training samples and times. In order to reduce the number of free parameters to save computational complexity in training and inference, we intend to specify a multiplicative interaction between visible units and hidden units by taking $w_{ijkl}=u_{ki} v_{lj}$. By defining two new matrices $U=[u_{ki}]\\in\\mathbb{R}^{K\\times I}$ and $V=[v_{lj}]\\in\\mathbb{R}^{L\\times J}$, we can re-write the\nenergy function \\eqref{equation3} into the following form,\n\n", "itemtype": "equation", "pos": 12349, "prevtext": "\nwhere $\\mathbf x \\in\\mathbb{R}^I$, $\\mathbf y\\in\\mathbb{R}^K$ are the binary states of visible units   and hidden units, $\\mathbf b\\in \\mathbb{R}^I$ and $\\mathbf c\\in\\mathbb{R}^K$ are the biases, and $W \\in {\\mathbb{R}^{I \\times K}}$ represents visible-to-hidden symmetric interaction terms in the neural network. Denote by $\\Theta  = \\left\\{ \\mathbf b,\\mathbf c, W \\right\\}$ all the model parameters.\n\nTo introduce our proposed MVRBM, we define the following notations. Denote by $X = [x_{ij}]\\in\\mathbb{R}^{I\\times J}$ the binary visible matrix variate, and $Y=[y_{kl}]\\in\\mathbb{R}^{K\\times L}$ the binary hidden matrix variate. We assume that the independent random variables $x_{ij}$ and $y_{kl}$ all take values from $\\{0,\\ 1\\}$. Given the parameters of a 4-order tensor $\\mathcal{W}=[w_{ijkl}]\\in\\mathbb{R}^{I\\times J\\times K\\times L}$, bias matrices $B=[b_{ij}]\\in\\mathbb{R}^{I\\times J}$ and $C=[c_{kl}]\\in\\mathbb{R}^{K\\times L}$, we define the following energy function for a joint configuration $(X, Y)$,\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{aligned}\n E(X, Y; \\Theta) &= \\sum\\limits_{i = 1}^I {\\sum\\limits_{j = 1}^J {\\sum\\limits_{k = 1}^K {\\sum\\limits_{l = 1}^L {{x_{ij}}{w_{ijkl}}{y_{kl}}} } } }  \\\\\n        &+ \\sum\\limits_{i = 1}^I {\\sum\\limits_{j = 1}^J {{x_{ij}}{b_{ij}}} }  + \\sum\\limits_{k = 1}^K {\\sum\\limits_{l = 1}^L {{y_{kl}}{c_{kl}}}},\n\\end{aligned}\\label{equation3}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle E(X,Y;\\Theta)\" display=\"inline\"><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum\\limits_{i=1}^{I}{\\sum\\limits_{j=1}^{J}{\\sum\\limits_{k=1}^{K%&#10;}{\\sum\\limits_{l=1}^{L}{{x_{ij}}{w_{ijkl}}{y_{kl}}}}}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><msub><mi>y</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum\\limits_{i=1}^{I}{\\sum\\limits_{j=1}^{J}{{x_{ij}}{b_{ij}}}}+%&#10;\\sum\\limits_{k=1}^{K}{\\sum\\limits_{l=1}^{L}{{y_{kl}}{c_{kl}}}},\" display=\"inline\"><mrow><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><msub><mi>y</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><msub><mi>c</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nBoth matrices $U$ and $V$ jointly define the interaction between input matrix $X$ and hidden matrix $Y$. The total number of free parameters in \\eqref{equation3} has been reduced to  $I\\times K+ L\\times J + I\\times J + K\\times L$ in \\eqref{equation4}.\n\n\nBased on \\eqref{equation4}, we define the following distribution:\n\n", "itemtype": "equation", "pos": 13451, "prevtext": "\nwhere $\\Theta = \\{\\mathcal{W}, B, C\\}$ collects all the parameters.\n\nThere are a total number of $I\\times J\\times K\\times L + I\\times J+ K\\times L$ free parameters in $\\Theta$. This is a huge number even for mild values of $I$, $J$, $K$ and $L$ and requires a large amount of training samples and times. In order to reduce the number of free parameters to save computational complexity in training and inference, we intend to specify a multiplicative interaction between visible units and hidden units by taking $w_{ijkl}=u_{ki} v_{lj}$. By defining two new matrices $U=[u_{ki}]\\in\\mathbb{R}^{K\\times I}$ and $V=[v_{lj}]\\in\\mathbb{R}^{L\\times J}$, we can re-write the\nenergy function \\eqref{equation3} into the following form,\n\n", "index": 5, "text": "\\begin{equation}\n E(X, Y; \\Theta)\n  =  - \\text{tr}(U^T Y V X^T) - \\text{tr}(X^TB) - \\text{tr}(Y^TC). \\label{equation4}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"E(X,Y;\\Theta)=-\\text{tr}(U^{T}YVX^{T})-\\text{tr}(X^{T}B)-\\text{tr}(Y^{T}C).\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>Y</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>\u2062</mo><mi>B</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere $\\Theta$ denotes all the model parameters $U$, $V$, $B$ and $C$ and the normalization constant $ Z(\\Theta ) $ is defined by\n\n", "itemtype": "equation", "pos": 13905, "prevtext": "\nBoth matrices $U$ and $V$ jointly define the interaction between input matrix $X$ and hidden matrix $Y$. The total number of free parameters in \\eqref{equation3} has been reduced to  $I\\times K+ L\\times J + I\\times J + K\\times L$ in \\eqref{equation4}.\n\n\nBased on \\eqref{equation4}, we define the following distribution:\n\n", "index": 7, "text": "\\begin{equation}\np(X, Y; \\Theta) = \\frac{1}{{Z(\\Theta )}}\\exp \\left\\{ { - E(X, Y; \\Theta)} \\right\\},\\label{equation5}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"p(X,Y;\\Theta)=\\frac{1}{{Z(\\Theta)}}\\exp\\left\\{{-E(X,Y;\\Theta)}\\right\\},\" display=\"block\"><mrow><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere $\\mathcal{X}$ and $\\mathcal{Y}$  are the binary value spaces of $X$ and $Y$, respectively.\n\nWe call the probabilistic model defined by \\eqref{equation5} the Matrix Variate RBM (MVRBM). The model is shown in Figure~\\ref{Figure2}.\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figure2.pdf}\nq\\caption{Graphical Illustration of MVRBM}\n\\label{Figure2}\n\\end{figure}\n\nTo facilitate exploring learning algorithm for MVRBM, we propose the following lemma regarding the conditional density of each visible and hidden entry.\n\\begin{lemma}Let the MVRBM model be defined by \\eqref{equation4} and \\eqref{equation5}, the conditional density of each visible entry $x_{ij}$ over all the other variables is given by\n\n", "itemtype": "equation", "pos": 14168, "prevtext": "\nwhere $\\Theta$ denotes all the model parameters $U$, $V$, $B$ and $C$ and the normalization constant $ Z(\\Theta ) $ is defined by\n\n", "index": 9, "text": "\\begin{equation}\nZ(\\Theta ) = \\sum\\limits_{X \\in \\mathcal{X}, Y \\in \\mathcal{Y}} {\\exp \\left\\{ { - E(X, Y; \\Theta)} \\right\\}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"Z(\\Theta)=\\sum\\limits_{X\\in\\mathcal{X},Y\\in\\mathcal{Y}}{\\exp\\left\\{{-E(X,Y;%&#10;\\Theta)}\\right\\}},\" display=\"block\"><mrow><mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>,</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></mrow></munder><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nand the conditional density of each hidden entry $y_{kl}$ over all the other variables is given by\n\n", "itemtype": "equation", "pos": 15036, "prevtext": "\nwhere $\\mathcal{X}$ and $\\mathcal{Y}$  are the binary value spaces of $X$ and $Y$, respectively.\n\nWe call the probabilistic model defined by \\eqref{equation5} the Matrix Variate RBM (MVRBM). The model is shown in Figure~\\ref{Figure2}.\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figure2.pdf}\nq\\caption{Graphical Illustration of MVRBM}\n\\label{Figure2}\n\\end{figure}\n\nTo facilitate exploring learning algorithm for MVRBM, we propose the following lemma regarding the conditional density of each visible and hidden entry.\n\\begin{lemma}Let the MVRBM model be defined by \\eqref{equation4} and \\eqref{equation5}, the conditional density of each visible entry $x_{ij}$ over all the other variables is given by\n\n", "index": 11, "text": "\\begin{equation}\np({x_{ij}} = 1\\left|Y; \\Theta \\right.) = \\sigma ({b_{ij}} + \\sum\\limits_{k = 1}^K {\\sum\\limits_{l = 1}^L {{y_{kl}}{u_{ki}}{v_{lj}}} } ) \\label{equation9}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"p({x_{ij}}=1\\left|Y;\\Theta\\right.)=\\sigma({b_{ij}}+\\sum\\limits_{k=1}^{K}{\\sum%&#10;\\limits_{l=1}^{L}{{y_{kl}}{u_{ki}}{v_{lj}}}})\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>|</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>+</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>y</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><msub><mi>u</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><msub><mi>v</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere $\\sigma$ is the sigmoid function $\\sigma (x) = 1/(1 + {e^{ - x}}).$\n\\end{lemma}\n\\begin{proof} Both $X$ and $Y$ are in symmetric position, so we only prove \\eqref{equation9} as an example. For this purpose, denote\n", "itemtype": "equation", "pos": 15321, "prevtext": "\nand the conditional density of each hidden entry $y_{kl}$ over all the other variables is given by\n\n", "index": 13, "text": "\\begin{equation}\np({y_{kl}} = 1\\left|X; \\Theta \\right.) = \\sigma ({c_{kl}} + \\sum\\limits_{i = 1}^I {\\sum\\limits_{j = 1}^J {{x_{ij}}{u_{ki}}{v_{lj}}} } ),\\label{equation9a}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"p({y_{kl}}=1\\left|X;\\Theta\\right.)=\\sigma({c_{kl}}+\\sum\\limits_{i=1}^{I}{\\sum%&#10;\\limits_{j=1}^{J}{{x_{ij}}{u_{ki}}{v_{lj}}}}),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>+</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>I</mi></munderover><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><msub><mi>u</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><msub><mi>v</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nConsider a fixed entry $x_{i_0j_0}$.\nFirst by Bayes theorem we have\n", "itemtype": "equation", "pos": 15726, "prevtext": "\nwhere $\\sigma$ is the sigmoid function $\\sigma (x) = 1/(1 + {e^{ - x}}).$\n\\end{lemma}\n\\begin{proof} Both $X$ and $Y$ are in symmetric position, so we only prove \\eqref{equation9} as an example. For this purpose, denote\n", "index": 15, "text": "\n\\[\nm_{ij} = b_{ij} + \\sum_{k = 1}^K\\sum_{l = 1}^L y_{kl}u_{ki} v_{lj}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"m_{ij}=b_{ij}+\\sum_{k=1}^{K}\\sum_{l=1}^{L}y_{kl}u_{ki}v_{lj}.\" display=\"block\"><mrow><mrow><msub><mi>m</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>+</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><msub><mi>y</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>l</mi></mrow></msub><mo>\u2062</mo><msub><mi>u</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msub><mi>v</mi><mrow><mi>l</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nClearly\n\n", "itemtype": "equation", "pos": 15868, "prevtext": "\nConsider a fixed entry $x_{i_0j_0}$.\nFirst by Bayes theorem we have\n", "index": 17, "text": "\n\\[\np(x_{i_0j_0}=1|Y; \\Theta) = \\frac{p(x_{i_0j_0}=1, Y; \\Theta)}{p(Y; \\Theta)}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"p(x_{i_{0}j_{0}}=1|Y;\\Theta)=\\frac{p(x_{i_{0}j_{0}}=1,Y;\\Theta)}{p(Y;\\Theta)}.\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nSimilarly, denoting by $X_{/i_0j_0}$ all the entries except for $x_{i_0j_0}$, we have\n\n", "itemtype": "equation", "pos": 15960, "prevtext": "\nClearly\n\n", "index": 19, "text": "\\begin{align*}\np(Y; \\Theta) & = \\frac1{Z(\\Theta)}\\sum_{X\\in\\mathcal{X}}\\exp\\{-E(X,Y; \\Theta)\\} \\\\\n&=\\frac{\\exp(\\text{tr}(Y^TV))}{Z(\\Theta)}\\sum_{X\\in\\mathcal{X}}\\prod_{(i,j)} \\exp(m_{ij}x_{ij}) \\\\\n&=\\frac{\\exp(\\text{tr}(Y^TV))}{Z(\\Theta)} \\prod_{(i,j)} \\left(1 + \\exp(m_{ij})\\right).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(Y;\\Theta)\" display=\"inline\"><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{1}{Z(\\Theta)}\\sum_{X\\in\\mathcal{X}}\\exp\\{-E(X,Y;\\Theta)\\}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mstyle><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\exp(\\text{tr}(Y^{T}V))}{Z(\\Theta)}\\sum_{X\\in\\mathcal{X}}%&#10;\\prod_{(i,j)}\\exp(m_{ij}x_{ij})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>X</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></munder></mstyle><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>m</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\exp(\\text{tr}(Y^{T}V))}{Z(\\Theta)}\\prod_{(i,j)}\\left(1+%&#10;\\exp(m_{ij})\\right).\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></munder></mstyle><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nCombining these together gives\n", "itemtype": "equation", "pos": 16343, "prevtext": "\nSimilarly, denoting by $X_{/i_0j_0}$ all the entries except for $x_{i_0j_0}$, we have\n\n", "index": 21, "text": "\\begin{align*}\n& p(x_{i_0j_0}=1, Y; \\Theta) \\\\\n= & \\frac{\\exp(\\text{tr}(Y^TV))}{Z(\\Theta)}\\exp(m_{i_0j_0}) \\sum_{X_{/i_0j_0} \\in\\mathcal{X}} \\prod_{(i,j)\\not=(i_0j_0)} \\exp ( m_{ij}x_{ij} )\\\\\n=&\\frac{\\exp(\\text{tr}(Y^TV))}{Z(\\Theta)}\\exp(m_{i_0j_0}) \\prod_{(i,j)\\not=(i_0,j_0)} \\left(1 + \\exp(m_{i_0j})\\right).\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle p(x_{i_{0}j_{0}}=1,Y;\\Theta)\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\exp(\\text{tr}(Y^{T}V))}{Z(\\Theta)}\\exp(m_{i_{0}j_{0}})\\sum%&#10;_{X_{/i_{0}j_{0}}\\in\\mathcal{X}}\\prod_{(i,j)\\not=(i_{0}j_{0})}\\exp(m_{ij}x_{ij})\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>X</mi><mrow><mi/><mo>/</mo><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></mrow></msub><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2260</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></munder></mstyle><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>m</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\exp(\\text{tr}(Y^{T}V))}{Z(\\Theta)}\\exp(m_{i_{0}j_{0}})%&#10;\\prod_{(i,j)\\not=(i_{0},j_{0})}\\left(1+\\exp(m_{i_{0}j})\\right).\" display=\"inline\"><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2260</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>i</mi><mn>0</mn></msub><mo>,</mo><msub><mi>j</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></munder></mstyle><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhich is \\eqref{equation9} for $(i,j) = (i_0,j_0)$. This completes the proof.\n\\end{proof}\n\n\\textit{Remark 1:} In terms of matrix representation, the two conditional probabilities can be written as\n\n", "itemtype": "equation", "pos": 16697, "prevtext": "\nCombining these together gives\n", "index": 23, "text": "\n\\[\np(x_{i_0j_0}=1|Y; \\Theta) = \\frac{\\exp(m_{i_0j_0})}{1 +\\exp(m_{i_0j_0}) } = \\sigma(m_{i_0j_0}),\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"p(x_{i_{0}j_{0}}=1|Y;\\Theta)=\\frac{\\exp(m_{i_{0}j_{0}})}{1+\\exp(m_{i_{0}j_{0}}%&#10;)}=\\sigma(m_{i_{0}j_{0}}),\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><msub><mi>i</mi><mn>0</mn></msub><mo>\u2062</mo><msub><mi>j</mi><mn>0</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere the sigmoid function $\\sigma$ applies on the entries of the corresponding matrices.\n\n\\subsection{The Maximum Likelihood and CD Algorithm for MVRBM}\n\nLet $\\mathcal{D} = \\{X_1, ..., X_N\\} $ be an observed dataset. Under the joint distribution \\eqref{equation5}, the log likelihood of  $\\mathcal{D}$ is defined by\n", "itemtype": "equation", "pos": 16997, "prevtext": "\nwhich is \\eqref{equation9} for $(i,j) = (i_0,j_0)$. This completes the proof.\n\\end{proof}\n\n\\textit{Remark 1:} In terms of matrix representation, the two conditional probabilities can be written as\n\n", "index": 25, "text": "\\begin{align}\n&p(X = 1 | Y; \\Theta) = \\sigma(U^T Y V + B), \\label{ConditionX}\\\\\n&p(Y = 1 | X; \\Theta) = \\sigma(UXV^T + C),\\label{ConditionY}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(X=1|Y;\\Theta)=\\sigma(U^{T}YV+B),\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>U</mi><mi>T</mi></msup><mi>Y</mi><mi>V</mi><mo>+</mo><mi>B</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p(Y=1|X;\\Theta)=\\sigma(UXV^{T}+C),\" display=\"inline\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo stretchy=\"false\">|</mo><mi>X</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><mi>X</mi><msup><mi>V</mi><mi>T</mi></msup><mo>+</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\nFor any component $\\theta $ of $\\Theta $ , we can prove that\n\n", "itemtype": "equation", "pos": 17466, "prevtext": "\nwhere the sigmoid function $\\sigma$ applies on the entries of the corresponding matrices.\n\n\\subsection{The Maximum Likelihood and CD Algorithm for MVRBM}\n\nLet $\\mathcal{D} = \\{X_1, ..., X_N\\} $ be an observed dataset. Under the joint distribution \\eqref{equation5}, the log likelihood of  $\\mathcal{D}$ is defined by\n", "index": 27, "text": "\n\\[\\ell  = \\frac{1}{N}\\sum\\limits_{n = 1}^N {\\log (\\sum\\limits_{Y\\in \\mathcal{Y}} {\\exp \\left\\{ { - E(X_n, Y)} \\right\\}} )}  - \\log Z(\\Theta ).\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\ell=\\frac{1}{N}\\sum\\limits_{n=1}^{N}{\\log(\\sum\\limits_{Y\\in\\mathcal{Y}}{\\exp%&#10;\\left\\{{-E(X_{n},Y)}\\right\\}})}-\\log Z(\\Theta).\" display=\"block\"><mrow><mrow><mi mathvariant=\"normal\">\u2113</mi><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>Z</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nWe call the first term of the right hand side of \\eqref{equation7} the data expectation and the second term the model expectation.\n\nThe main difficulty in calculating the derivative of the likelihood function with respect to a parameter is to fast compute the model expectation. The model expectation is intractable due to the summation over all the possible visible and hidden states. However, the \\textit{Contrast Divergence} (CD) procedure allows fast approximation using short Markov chains. The main idea in the CD algorithm is as follow: a Gibbs chain is initialized with a training example $X^{(0)}_n = X_n$ of the training set, then alternatively using \\eqref{ConditionX} and \\eqref{ConditionY} gives the chain $\\{(X^{(0)}_n, Y^{(0)}_n), (X^{(1)}_n, Y^{(1)}_n), ..., (X^{(k)}_n, Y^{(k)}_n), ...\\}$. The CD-k algorithm takes the samples $\\{X^{(k)}_n\\}^N_{n=1}$ at step $k$ to approximate the model expectation, that is,\n\n", "itemtype": "equation", "pos": 17674, "prevtext": "\n\nFor any component $\\theta $ of $\\Theta $ , we can prove that\n\n", "index": 29, "text": "\\begin{equation}\n\\begin{aligned}\n\\frac{{\\partial \\ell }}{{\\partial \\theta }} =& - \\frac{1}{N}\\sum\\limits_{n = 1}^N {\\sum\\limits_{Y \\in \\mathcal{Y}} {p(Y\\left| {X_n;\\Theta} \\right.)} \\frac{{\\partial E(X_n, Y; \\Theta)}}{{\\partial \\theta }}}  \\\\\n&+ \\sum\\limits_{X' \\in \\mathcal{X}, Y' \\in \\mathcal{Y}} {p(X', Y'; \\Theta)\\frac{\\partial E(X', Y'; \\Theta)}{\\partial \\theta}}.\n\\end{aligned}\\label{equation7}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{{\\partial\\ell}}{{\\partial\\theta}}=\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum\\limits_{n=1}^{N}{\\sum\\limits_{Y\\in\\mathcal{Y}}{p%&#10;(Y\\left|{X_{n};\\Theta}\\right.)}\\frac{{\\partial E(X_{n},Y;\\Theta)}}{{\\partial%&#10;\\theta}}}\" display=\"inline\"><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>|</mo><msub><mi>X</mi><mi>n</mi></msub><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\sum\\limits_{X^{\\prime}\\in\\mathcal{X},Y^{\\prime}\\in\\mathcal{Y}}{%&#10;p(X^{\\prime},Y^{\\prime};\\Theta)\\frac{\\partial E(X^{\\prime},Y^{\\prime};\\Theta)}%&#10;{\\partial\\theta}}.\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msup><mi>X</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>,</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></mrow></munder></mstyle><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\\eqref{ApproximatedExpectation} is actually the data expectation over the sampled data in the $k$-th step of all the Gibbs chains from training samples, which is similar to the data expectation in \\eqref{equation7}. Finally the CD algorithm is implemented by\n\n", "itemtype": "equation", "pos": 19017, "prevtext": "\nWe call the first term of the right hand side of \\eqref{equation7} the data expectation and the second term the model expectation.\n\nThe main difficulty in calculating the derivative of the likelihood function with respect to a parameter is to fast compute the model expectation. The model expectation is intractable due to the summation over all the possible visible and hidden states. However, the \\textit{Contrast Divergence} (CD) procedure allows fast approximation using short Markov chains. The main idea in the CD algorithm is as follow: a Gibbs chain is initialized with a training example $X^{(0)}_n = X_n$ of the training set, then alternatively using \\eqref{ConditionX} and \\eqref{ConditionY} gives the chain $\\{(X^{(0)}_n, Y^{(0)}_n), (X^{(1)}_n, Y^{(1)}_n), ..., (X^{(k)}_n, Y^{(k)}_n), ...\\}$. The CD-k algorithm takes the samples $\\{X^{(k)}_n\\}^N_{n=1}$ at step $k$ to approximate the model expectation, that is,\n\n", "index": 31, "text": "\\begin{align}\n&\\sum_{X' \\in \\mathcal{X}, Y' \\in \\mathcal{Y}} p(X', Y'; \\Theta)\\frac{\\partial E(X', Y'; \\Theta)}{\\partial \\theta} \\notag\\\\\n=& \\sum_{X'\\in\\mathcal{X}}\\left(\\sum_{Y'\\in\\mathcal{Y}} p(Y' | X';\\Theta)\\frac{\\partial E(X', Y'; \\Theta)}{\\partial \\theta}\\right)p(X';\\Theta) \\notag\\\\\n\\approx & \\frac1N\\sum^N_{n=1}\\sum_{Y'\\in\\mathcal{Y}}p(Y' | X^{(k)}_n;\\Theta)\\frac{\\partial E(X^{(k)}_n, Y'; \\Theta)}{\\partial \\theta}.\\label{ApproximatedExpectation}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{X^{\\prime}\\in\\mathcal{X},Y^{\\prime}\\in\\mathcal{Y}}p(X^{%&#10;\\prime},Y^{\\prime};\\Theta)\\frac{\\partial E(X^{\\prime},Y^{\\prime};\\Theta)}{%&#10;\\partial\\theta}\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msup><mi>X</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><mo>,</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></mrow></munder></mstyle><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><mo>=</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{X^{\\prime}\\in\\mathcal{X}}\\left(\\sum_{Y^{\\prime}\\in\\mathcal{%&#10;Y}}p(Y^{\\prime}|X^{\\prime};\\Theta)\\frac{\\partial E(X^{\\prime},Y^{\\prime};%&#10;\\Theta)}{\\partial\\theta}\\right)p(X^{\\prime};\\Theta)\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>X</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow></munder></mstyle><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle><mo>)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\approx\" display=\"inline\"><mo>\u2248</mo></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{N}\\sum^{N}_{n=1}\\sum_{Y^{\\prime}\\in\\mathcal{Y}}p(Y^{%&#10;\\prime}|X^{(k)}_{n};\\Theta)\\frac{\\partial E(X^{(k)}_{n},Y^{\\prime};\\Theta)}{%&#10;\\partial\\theta}.\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\nFor all the four parameters in the MVRBM, we take calculating $\\frac{\\partial \\ell}{\\partial U}$ as an example. The derivatives with respect to other parameters can be calculated similarly. From \\eqref{equation4}, we have\n", "itemtype": "equation", "pos": 19744, "prevtext": "\n\\eqref{ApproximatedExpectation} is actually the data expectation over the sampled data in the $k$-th step of all the Gibbs chains from training samples, which is similar to the data expectation in \\eqref{equation7}. Finally the CD algorithm is implemented by\n\n", "index": 33, "text": "\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\theta}\\approx & -\\frac1N\\sum^N_{n=1}\\sum_{Y\\in\\mathcal{Y}}p(Y | X_n;\\Theta)\\frac{\\partial E(X_n, Y; \\Theta)}{\\partial \\theta} \\label{ApproxLikelihood}\\\\\n& + \\frac1N\\sum^N_{n=1}\\sum_{Y'\\in\\mathcal{Y}}p(Y' | X^{(k)}_n;\\Theta)\\frac{\\partial E(X^{(k)}_n, Y'; \\Theta)}{\\partial \\theta}.\\notag\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial\\theta}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum^{N}_{n=1}\\sum_{Y\\in\\mathcal{Y}}p(Y|X_{n};\\Theta)%&#10;\\frac{\\partial E(X_{n},Y;\\Theta)}{\\partial\\theta}\" display=\"inline\"><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo stretchy=\"false\">|</mo><msub><mi>X</mi><mi>n</mi></msub><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{N}\\sum^{N}_{n=1}\\sum_{Y^{\\prime}\\in\\mathcal{Y}}p(Y^{%&#10;\\prime}|X^{(k)}_{n};\\Theta)\\frac{\\partial E(X^{(k)}_{n},Y^{\\prime};\\Theta)}{%&#10;\\partial\\theta}.\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03b8</mi></mrow></mfrac></mstyle><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nIn this case,  the derivative \\eqref{ApproxLikelihood} becomes\n\n", "itemtype": "equation", "pos": 20311, "prevtext": "\n\nFor all the four parameters in the MVRBM, we take calculating $\\frac{\\partial \\ell}{\\partial U}$ as an example. The derivatives with respect to other parameters can be calculated similarly. From \\eqref{equation4}, we have\n", "index": 35, "text": "\n\\[\n\\frac{\\partial E(X,Y;\\Theta)}{\\partial U} =  - YVX^T.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial E(X,Y;\\Theta)}{\\partial U}=-YVX^{T}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>E</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>U</mi></mrow></mfrac><mo>=</mo><mrow><mo>-</mo><mrow><mi>Y</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mi>X</mi><mi>T</mi></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\nFor binary variable $Y$ (or $Y'$ here), the mean value of $Y$ is equal to the probability of $Y=1$.\nHence the first term in \\eqref{equation10} is, refer to \\eqref{ConditionY},\n\n", "itemtype": "equation", "pos": 20435, "prevtext": "\nIn this case,  the derivative \\eqref{ApproxLikelihood} becomes\n\n", "index": 37, "text": "\\begin{align}\n \\frac{\\partial \\ell}{\\partial U} \\approx & - \\frac{1}{N}\\sum\\limits_{n = 1}^N \\left(\\sum\\limits_{Y \\in \\mathcal{Y}} p(Y\\left| X_n;\\Theta\\right.) \\right) V X_n^T  \\label{equation10}\\\\\n&+\\frac1N\\sum^N_{n=1} \\left(\\sum\\limits_{ Y' \\in \\mathcal{Y}} p(Y'\\left| X^{(k)}_n;\\Theta \\right.) Y'\\right) V(X^{(k)}_n)^T.\\notag\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial U}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>U</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum\\limits_{n=1}^{N}\\left(\\sum\\limits_{Y\\in\\mathcal{%&#10;Y}}p(Y\\left|X_{n};\\Theta\\right.)\\right)VX_{n}^{T}\" display=\"inline\"><mrow><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>|</mo><msub><mi>X</mi><mi>n</mi></msub><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mi>V</mi><msubsup><mi>X</mi><mi>n</mi><mi>T</mi></msubsup></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{N}\\sum^{N}_{n=1}\\left(\\sum\\limits_{Y^{\\prime}\\in%&#10;\\mathcal{Y}}p(Y^{\\prime}\\left|X^{(k)}_{n};\\Theta\\right.)Y^{\\prime}\\right)V(X^{%&#10;(k)}_{n})^{T}.\" display=\"inline\"><mrow><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mo>(</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>|</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>)</mo></mrow><mi>V</mi><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nSimilarly it applies to the second term. Thus we have\n\n", "itemtype": "equation", "pos": 20953, "prevtext": "\n\nFor binary variable $Y$ (or $Y'$ here), the mean value of $Y$ is equal to the probability of $Y=1$.\nHence the first term in \\eqref{equation10} is, refer to \\eqref{ConditionY},\n\n", "index": 39, "text": "\\begin{equation*}\n\\sum\\limits_{Y \\in \\mathcal{Y}} p(Y\\left| X_n;\\Theta\\right.) Y = \\sigma (U{X_n}{V^T} + C).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\sum\\limits_{Y\\in\\mathcal{Y}}p(Y\\left|X_{n};\\Theta\\right.)Y=\\sigma(U{X_{n}}{V^%&#10;{T}}+C).\" display=\"block\"><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>Y</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></munder><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>|</mo><msub><mi>X</mi><mi>n</mi></msub><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow><mi>Y</mi><mo>=</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><mi>U</mi><msub><mi>X</mi><mi>n</mi></msub><msup><mi>V</mi><mi>T</mi></msup><mo>+</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\nSimilarly for other parameters we obtain\n\n", "itemtype": "equation", "pos": 21132, "prevtext": "\nSimilarly it applies to the second term. Thus we have\n\n", "index": 41, "text": "\\begin{align}\n\\frac{\\partial \\ell }{\\partial U} \\approx & - \\frac{1}{N}\\sum\\limits_{n = 1}^N\\sigma(UX_nV^T + C) VX^T_n \\notag\\\\\n& + \\frac1N \\sum^N_{n=1} \\sigma(UX^{(k)}_nV^T + C) V(X^{(k)}_n)^T.  \\label{equation14}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial U}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>U</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum\\limits_{n=1}^{N}\\sigma(UX_{n}V^{T}+C)VX^{T}_{n}\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msub><mi>X</mi><mi>n</mi></msub><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msubsup><mi>X</mi><mi>n</mi><mi>T</mi></msubsup></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{N}\\sum^{N}_{n=1}\\sigma(UX^{(k)}_{n}V^{T}+C)V(X^{(k)}_{n%&#10;})^{T}.\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\n\\textit{Remark 2:} As the model only depends on the product of parameters $U$ and $V$, one parameter may go up in any scale $s$ while the other goes down to $1/s$. To avoid the issue of un-identifying model parameters, we add a penalty of $\\frac{\\beta}2(\\|U\\|^2_F + \\|V\\|^2_F)$ to the log likelihood objective $\\ell$.\n\nWe summarize the overall CD procedure for Matrix Variate RBM in Algorithm~\\ref{Algorithm1}.\nIn all our experiments, we use the special CD-$1$ algorithm for training.\n\\begin{algorithm}\n\\caption{CD-$K$ algorithm for MVRBM:}\\label{Algorithm1}\n\\begin{algorithmic}[1]\n\\REQUIRE A set of training data of $N$ matrices $\\mathcal{D} = \\{X_1, ..., X_N\\}$, the maximum iteration number $T$ (default value $=10,000$), the learning rate $\\alpha$ (default value $= 0.05$), the weight regularizer $\\beta$ (default value $= 0.01$), the momentum $\\gamma$ (default value $= 0.5$), the batch size $b$ (default value $= 100$) and the CD step $K$ (default value $=1$).\n\\ENSURE  Model parameters  $\\Theta=\\{U,V,B,C\\}$.\n\\STATE   \\textbf{Initialization}: Randomly initialize values for $U$ and $V$, set the bias $B=0$ and $C=0$ and the gradient increments $\\Delta U = \\Delta V = \\Delta B= \\Delta C = 0$.\n\\FOR{iteration step $t=1\\rightarrow T$}\n\n\\STATE Randomly divide $\\mathcal{D}$ into $M$ batches $\\mathcal{D}_1, ..., \\mathcal{D}_M$ of size $b$, then\n\\FOR{batch $m=1\\rightarrow M$}\n\\STATE For all the data $X^{(0)}=X \\in \\mathcal{D}_m$ run the Gibbs sampling at the current model parameters $\\Theta$:\n\\FOR{$k=0\\to K-1$}\n    \\STATE sample  $Y^{(k)}$ according to \\eqref{ConditionY} with the current $X^{(k)}$;\n    \\STATE sample  $X^{(k+1)}$ according to \\eqref{ConditionX} with $Y^{(k)}$;\n\\ENDFOR\n\\STATE Update the gradient increment with $\\mathcal{D}_m$ and $\\mathcal{D}^{(K)}_m (X^{(K)})$ by using \\eqref{equation14} to \\eqref{equation17}:\n\n", "itemtype": "equation", "pos": 21401, "prevtext": "\n\nSimilarly for other parameters we obtain\n\n", "index": 43, "text": "\\begin{align}\n\\frac{\\partial \\ell}{\\partial V} \\approx & - \\frac{1}{N}\\sum_{n = 1}^N \\sigma(UX_nV^T+C)^TU{X}_n \\notag\\\\\n& + \\frac{1}{N}\\sum\\limits_{n = 1}^N \\sigma(UX^{(k)}_nV^T+C)^TUX^{(k)}_n, \\label{equation15}\\\\\n\\frac{\\partial \\ell}{\\partial B} \\approx & - \\frac{1}{N}\\sum_{n = 1}^NX_n  + \\frac{1}{N}\\sum_{n = 1}^N X^{(k)}_n, \\label{partialB}\\\\\n\\frac{\\partial \\ell}{\\partial C} \\approx & - \\frac{1}{N}\\sum_{n = 1}^N \\sigma(UX_nV^T+C)  \\notag \\\\\n&+ \\frac{1}{N}\\sum_{n = 1}^N \\sigma(UX^{(k)}_nV^T+C).\n\\label{equation17}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial V}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>V</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum_{n=1}^{N}\\sigma(UX_{n}V^{T}+C)^{T}U{X}_{n}\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msub><mi>X</mi><mi>n</mi></msub><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><msub><mi>X</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{N}\\sum\\limits_{n=1}^{N}\\sigma(UX^{(k)}_{n}V^{T}+C)^{T}%&#10;UX^{(k)}_{n},\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>U</mi><mo>\u2062</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial B}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>B</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum_{n=1}^{N}X_{n}+\\frac{1}{N}\\sum_{n=1}^{N}X^{(k)}_%&#10;{n},\" display=\"inline\"><mrow><mrow><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>X</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\partial\\ell}{\\partial C}\\approx\" display=\"inline\"><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>C</mi></mrow></mfrac></mstyle><mo>\u2248</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex19.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{1}{N}\\sum_{n=1}^{N}\\sigma(UX_{n}V^{T}+C)\" display=\"inline\"><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msub><mi>X</mi><mi>n</mi></msub><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\frac{1}{N}\\sum_{n=1}^{N}\\sigma(UX^{(k)}_{n}V^{T}+C).\" display=\"inline\"><mrow><mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo>\u2062</mo><msubsup><mi>X</mi><mi>n</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><mi>C</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\\STATE Update model parameters $\\theta \\in \\Theta$ with\n", "itemtype": "equation", "pos": 23773, "prevtext": "\n\n\\textit{Remark 2:} As the model only depends on the product of parameters $U$ and $V$, one parameter may go up in any scale $s$ while the other goes down to $1/s$. To avoid the issue of un-identifying model parameters, we add a penalty of $\\frac{\\beta}2(\\|U\\|^2_F + \\|V\\|^2_F)$ to the log likelihood objective $\\ell$.\n\nWe summarize the overall CD procedure for Matrix Variate RBM in Algorithm~\\ref{Algorithm1}.\nIn all our experiments, we use the special CD-$1$ algorithm for training.\n\\begin{algorithm}\n\\caption{CD-$K$ algorithm for MVRBM:}\\label{Algorithm1}\n\\begin{algorithmic}[1]\n\\REQUIRE A set of training data of $N$ matrices $\\mathcal{D} = \\{X_1, ..., X_N\\}$, the maximum iteration number $T$ (default value $=10,000$), the learning rate $\\alpha$ (default value $= 0.05$), the weight regularizer $\\beta$ (default value $= 0.01$), the momentum $\\gamma$ (default value $= 0.5$), the batch size $b$ (default value $= 100$) and the CD step $K$ (default value $=1$).\n\\ENSURE  Model parameters  $\\Theta=\\{U,V,B,C\\}$.\n\\STATE   \\textbf{Initialization}: Randomly initialize values for $U$ and $V$, set the bias $B=0$ and $C=0$ and the gradient increments $\\Delta U = \\Delta V = \\Delta B= \\Delta C = 0$.\n\\FOR{iteration step $t=1\\rightarrow T$}\n\n\\STATE Randomly divide $\\mathcal{D}$ into $M$ batches $\\mathcal{D}_1, ..., \\mathcal{D}_M$ of size $b$, then\n\\FOR{batch $m=1\\rightarrow M$}\n\\STATE For all the data $X^{(0)}=X \\in \\mathcal{D}_m$ run the Gibbs sampling at the current model parameters $\\Theta$:\n\\FOR{$k=0\\to K-1$}\n    \\STATE sample  $Y^{(k)}$ according to \\eqref{ConditionY} with the current $X^{(k)}$;\n    \\STATE sample  $X^{(k+1)}$ according to \\eqref{ConditionX} with $Y^{(k)}$;\n\\ENDFOR\n\\STATE Update the gradient increment with $\\mathcal{D}_m$ and $\\mathcal{D}^{(K)}_m (X^{(K)})$ by using \\eqref{equation14} to \\eqref{equation17}:\n\n", "index": 45, "text": "\\begin{align*}\n\\Delta U &= \\gamma \\Delta U + \\alpha \\left(-\\left.\\frac{\\partial \\ell}{\\partial U}\\right|_{\\mathcal{D}_m,\\mathcal{D}^{(K)}_m} - \\beta U\\right); \\\\\n\\Delta V &= \\gamma \\Delta V + \\alpha \\left(- \\left.\\frac{\\partial \\ell}{\\partial V}\\right|_{\\mathcal{D}_m,\\mathcal{D}^{(K)}_m} - \\beta V\\right); \\\\\n\\Delta B  &= \\gamma \\Delta B  + \\alpha \\left(- \\left.\\frac{\\partial \\ell}{\\partial B}\\right|_{\\mathcal{D}_m,\\mathcal{D}^{(K)}_m}\\right); \\\\\n\\Delta C  &= \\gamma  \\Delta C   + \\alpha \\left(- \\left.\\frac{\\partial \\ell}{\\partial C}\\right|_{\\mathcal{D}_m,\\mathcal{D}^{(K)}_m}\\right);\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta U\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>U</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\gamma\\Delta U+\\alpha\\left(-\\left.\\frac{\\partial\\ell}{\\partial U%&#10;}\\right|_{\\mathcal{D}_{m},\\mathcal{D}^{(K)}_{m}}-\\beta U\\right);\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>U</mi></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>-</mo><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>U</mi></mrow></mfrac></mstyle><mo fence=\"true\">|</mo></mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi></msub><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></msub></mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>U</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta V\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>V</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\gamma\\Delta V+\\alpha\\left(-\\left.\\frac{\\partial\\ell}{\\partial V%&#10;}\\right|_{\\mathcal{D}_{m},\\mathcal{D}^{(K)}_{m}}-\\beta V\\right);\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>V</mi></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mrow><mo>-</mo><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>V</mi></mrow></mfrac></mstyle><mo fence=\"true\">|</mo></mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi></msub><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></msub></mrow><mo>-</mo><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>V</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta B\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>B</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex22.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\gamma\\Delta B+\\alpha\\left(-\\left.\\frac{\\partial\\ell}{\\partial B%&#10;}\\right|_{\\mathcal{D}_{m},\\mathcal{D}^{(K)}_{m}}\\right);\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>B</mi></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>-</mo><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>B</mi></mrow></mfrac></mstyle><mo fence=\"true\">|</mo></mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi></msub><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Delta C\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>C</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\gamma\\Delta C+\\alpha\\left(-\\left.\\frac{\\partial\\ell}{\\partial C%&#10;}\\right|_{\\mathcal{D}_{m},\\mathcal{D}^{(K)}_{m}}\\right);\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>C</mi></mrow><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mo>-</mo><msub><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi mathvariant=\"normal\">\u2113</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>C</mi></mrow></mfrac></mstyle><mo fence=\"true\">|</mo></mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi></msub><mo>,</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\\ENDFOR\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\textit{Remark 3:} The MVRBM model can be easily extended to any order tensorial input and hidden units. Note that the energy \\eqref{equation4}, which is equivalent to\n", "itemtype": "equation", "pos": 24430, "prevtext": "\n\\STATE Update model parameters $\\theta \\in \\Theta$ with\n", "index": 47, "text": "\n\\[\n\\theta \\leftarrow \\theta + \\Delta\\theta;\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex24.m1\" class=\"ltx_Math\" alttext=\"\\theta\\leftarrow\\theta+\\Delta\\theta;\" display=\"block\"><mrow><mrow><mi>\u03b8</mi><mo>\u2190</mo><mrow><mi>\u03b8</mi><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><mi>\u03b8</mi></mrow></mrow></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\ndetermines the bilinear mappings between $X$ and $Y$\n", "itemtype": "equation", "pos": 24696, "prevtext": "\n\\ENDFOR\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\textit{Remark 3:} The MVRBM model can be easily extended to any order tensorial input and hidden units. Note that the energy \\eqref{equation4}, which is equivalent to\n", "index": 49, "text": "\n\\[\nE(X,Y;\\Theta) = - \\langle Y, UXV^T\\rangle - \\langle B, X\\rangle - \\langle C, Y\\rangle,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex25.m1\" class=\"ltx_Math\" alttext=\"E(X,Y;\\Theta)=-\\langle Y,UXV^{T}\\rangle-\\langle B,X\\rangle-\\langle C,Y\\rangle,\" display=\"block\"><mrow><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>Y</mi><mo>,</mo><mrow><mi>U</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>B</mi><mo>,</mo><mi>X</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>C</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nwhere $\\times_n$ means $n$-mode product of a tensor and a matrix~\\cite{KoldaBader2009} and $E$'s are the logistic error matrices. For $D$-order tensorial binary variates $\\mathcal{X}$ and $\\mathcal{Y}$, a Tucker decomposition~\\cite{KoldaBader2009}\n$\n\\mathcal{Y} = \\mathcal{X}\\times_1 U_1\\times_2 \\cdots \\times_D U_D + \\mathcal{E}\n$\nsuggests the following energy function\n", "itemtype": "equation", "pos": 24842, "prevtext": "\ndetermines the bilinear mappings between $X$ and $Y$\n", "index": 51, "text": "\n\\[\nY = X\\times_1 U\\times_2 V + E_y\n\\text{ and }\nX = Y\\times_1 U^T\\times_2 V^T + E_x,\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex26.m1\" class=\"ltx_Math\" alttext=\"Y=X\\times_{1}U\\times_{2}V+E_{y}\\text{ and }X=Y\\times_{1}U^{T}\\times_{2}V^{T}+E%&#10;_{x},\" display=\"block\"><mrow><mrow><mi>Y</mi><mo>=</mo><mrow><mrow><mrow><mi>X</mi><msub><mo>\u00d7</mo><mn>1</mn></msub><mi>U</mi></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><mi>V</mi></mrow><mo>+</mo><mrow><msub><mi>E</mi><mi>y</mi></msub><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>X</mi></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>Y</mi><msub><mo>\u00d7</mo><mn>1</mn></msub><msup><mi>U</mi><mi>T</mi></msup></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><msup><mi>V</mi><mi>T</mi></msup></mrow><mo>+</mo><msub><mi>E</mi><mi>x</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\nfor a Tensorial Variate RBM (TV-RBM). Algorithm and its applications of such a TV-RBM will be investigated in our forthcoming paper~\\cite{QiSunGaoHu2015}.\n\n\\subsection{Multimodal MVRBM}\nInformation in the real world comes through multiple input channels. For example, in image super-resolution, the lower resolution images are associated with different types of features. The classic RBM has been engineered to handle multimodal data~\\cite{SalakhutdinovHinton2010}.\n\nAs a proof of the concept, in this subsection, we simply give an outline to describe how the newly proposed MVRBM can be generalized to process multimodal matrix variates. We assume that the visible layer consists of two separate matrices $X\\in\\mathbb{R}^{I\\times J}$ and $Z\\in\\mathbb{R}^{H\\times W}$. Both $X$ and $Z$ could be in the same dimension and will be connected to the hidden layer given by a matrix variate $Y\\in\\mathbb{R}^{K\\times L}$. The connection is specified in the following energy function\n\n", "itemtype": "equation", "pos": 25301, "prevtext": "\nwhere $\\times_n$ means $n$-mode product of a tensor and a matrix~\\cite{KoldaBader2009} and $E$'s are the logistic error matrices. For $D$-order tensorial binary variates $\\mathcal{X}$ and $\\mathcal{Y}$, a Tucker decomposition~\\cite{KoldaBader2009}\n$\n\\mathcal{Y} = \\mathcal{X}\\times_1 U_1\\times_2 \\cdots \\times_D U_D + \\mathcal{E}\n$\nsuggests the following energy function\n", "index": 53, "text": "\n\\[\nE(\\mathcal{X}, \\mathcal{Y}; \\Theta) = -\\langle \\mathcal{Y}, \\mathcal{X}\\times_1 U_1\\times_2 \\cdots \\times_D U_D\\rangle - \\langle \\mathcal{B}, \\mathcal{X}\\rangle - \\langle \\mathcal{C}, \\mathcal{Y}\\rangle\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex27.m1\" class=\"ltx_Math\" alttext=\"E(\\mathcal{X},\\mathcal{Y};\\Theta)=-\\langle\\mathcal{Y},\\mathcal{X}\\times_{1}U_{%&#10;1}\\times_{2}\\cdots\\times_{D}U_{D}\\rangle-\\langle\\mathcal{B},\\mathcal{X}\\rangle%&#10;-\\langle\\mathcal{C},\\mathcal{Y}\\rangle\" display=\"block\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo>,</mo><mrow><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><msub><mo>\u00d7</mo><mn>1</mn></msub><msub><mi>U</mi><mn>1</mn></msub></mrow><msub><mo>\u00d7</mo><mn>2</mn></msub><mi mathvariant=\"normal\">\u22ef</mi></mrow><msub><mo>\u00d7</mo><mi>D</mi></msub><msub><mi>U</mi><mi>D</mi></msub></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi class=\"ltx_font_mathcaligraphic\">\u212c</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo>-</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9e</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo stretchy=\"false\">\u27e9</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\n\nGiven the energy function in \\eqref{EnergyMultiview}, the following joint distribution\n", "itemtype": "equation", "pos": 26487, "prevtext": "\nfor a Tensorial Variate RBM (TV-RBM). Algorithm and its applications of such a TV-RBM will be investigated in our forthcoming paper~\\cite{QiSunGaoHu2015}.\n\n\\subsection{Multimodal MVRBM}\nInformation in the real world comes through multiple input channels. For example, in image super-resolution, the lower resolution images are associated with different types of features. The classic RBM has been engineered to handle multimodal data~\\cite{SalakhutdinovHinton2010}.\n\nAs a proof of the concept, in this subsection, we simply give an outline to describe how the newly proposed MVRBM can be generalized to process multimodal matrix variates. We assume that the visible layer consists of two separate matrices $X\\in\\mathbb{R}^{I\\times J}$ and $Z\\in\\mathbb{R}^{H\\times W}$. Both $X$ and $Z$ could be in the same dimension and will be connected to the hidden layer given by a matrix variate $Y\\in\\mathbb{R}^{K\\times L}$. The connection is specified in the following energy function\n\n", "index": 55, "text": "\\begin{align}\n E(X, Z, Y; \\Theta) =&  - \\text{tr}(U^TYVX^T) - \\text{tr}(X^TB) - \\text{tr}(Y^TV)  \\notag\\\\\n&- \\text{tr}(Q^TYRZ^T) - \\text{tr}(Z^TA). \\label{EnergyMultiview}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle E(X,Z,Y;\\Theta)=\" display=\"inline\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Z</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex28.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\text{tr}(U^{T}YVX^{T})-\\text{tr}(X^{T}B)-\\text{tr}(Y^{T}V)\" display=\"inline\"><mrow><mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>U</mi><mi>T</mi></msup><mo>\u2062</mo><mi>Y</mi><mo>\u2062</mo><mi>V</mi><mo>\u2062</mo><msup><mi>X</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>\u2062</mo><mi>B</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>T</mi></msup><mo>\u2062</mo><mi>V</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\text{tr}(Q^{T}YRZ^{T})-\\text{tr}(Z^{T}A).\" display=\"inline\"><mrow><mrow><mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Q</mi><mi>T</mi></msup><mo>\u2062</mo><mi>Y</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><msup><mi>Z</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mtext>tr</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Z</mi><mi>T</mi></msup><mo>\u2062</mo><mi>A</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00722.tex", "nexttext": "\ndefines a graphical model, called \\textit{Multimodal} MVRBM (MMVRBM). The learning algorithm based on CD approximation can be easily derived. In experiment, we will use this model for image super-resolution.\n\n\n\\section{Experimental Results and Analysis}\\label{Sec:5}\nIn this section, we implement Algorithm 1 and conduct a number of experiments on some  public databases to assess the proposed MVRBM model. These experiments are designed to demonstrate the performance of MVRBM in feature extraction and reconstruction by comparing with the existing RBM model. The algorithm is coded by Matlab, and is run on a machine with a 2.50GHz Intel Xeon Processor and 128GB of installed memory.\n\n\\subsection{Experiment 1: Denoising and Reconstruction}\nIn the first experiment, our goal is to show that a well trained MVRBM can be used to data denoising and dimension reduction for reconstruction. The experiment is based on the MNIST handwritten digit database, downloadable from  \\texttt{http://yann.lecun.com/exdb/mnist/.} The dataset contains 70000 labeled images of handwritten digits which are in 28 by 28 pixels and gray scale. In general, the training set contains 60,000 images and the testing set 10,000 images.\n\nIn the first attempt, we wish to demonstrate that the MVRBM model actually learns the information from data. For this purpose, we randomly select 5,000 images of digit 9 from the training dataset, set the hidden matrix variate to size $15\\times 15$ and use most of default parameter values set in Algorithm~\\ref{Algorithm1}. The training process terminates after $T = 3,000$ epoches.\nWith the trained MVRBM, we conduct a simple denoising test. We randomly add 10\\% salt \\& pepper  noises to some testing images of digit 9. The noised testing images are shown in Figure~\\ref{Figure1Gao}(a) while the denoised counterparts in Figure~\\ref{Figure1Gao}(b). The denoising result is visually pleasing.\n\n\\begin{figure}\n\\centering\n\\subfigure[noised digits]{\\includegraphics[width=0.22\\textwidth]{noised_nine.pdf}}\\;\\;\n\\subfigure[denoised digits]{\\includegraphics[width=0.22\\textwidth]{denoised_nine.pdf}}\n\\caption{The denoising demonstration of the trained MVRBM over the images of digit 9.}\n\\label{Figure1Gao}\n\\end{figure}\n\nIn another attempt, we train a MVRBM with $N=20,000$ training samples ($2,000$ samples for each digit) and $T=3,000$ epoches, but the hidden size is set to 25. Based on the bilinear model of the MVRBM, the trained model parameters $U$ and $V$ can be jointly used as filters or feature extractor in terms of dictionary $U^T\\otimes V^T$. We show some filters in Figure~\\ref{Figure2Gao}(a), from which we can see that the learned filters are quite close to the Haar filters used in image processing.\n\nThen we test the capacity of the learned MVRBM in dimensionality reduction and reconstruction. Figure~\\ref{Figure2Gao}(b) shows several examples of the original and its corresponding reconstruction from low dimension representation. Average reconstruction error is 10.8488/(28*28).\n\\begin{figure}\n\\centering\n\\subfigure[The Trained Filters]{\\includegraphics[width=0.15\\textwidth]{dictionary_digits.pdf}}\\;\n\\subfigure[The Original Images]{\\includegraphics[width=0.15\\textwidth]{origenal2.pdf}}\\;\n\\subfigure[The Corresponding Reconstruction]{\\includegraphics[width=0.15\\textwidth]{reconstructions2.pdf}}\n\\caption{The learned MVRBM filters and the reconstruction demonstration of the trained MVRBM.}\n\\label{Figure2Gao}\n\\end{figure}\n\n\n\\subsection{Experiment 2: Handwritten Digit Classification}\n\nThe dataset used in Experiment 1 has been widely used for testing and evaluating classification or clustering algorithms~\\cite{ChazalTapsonSchaik2015,Deng2012,KangGonugondlaMin-SunShanbhag2015}. In this experiment, we use this dataset to evaluate how well the proposed MVRBM is in feature extraction. In fact, the states over hidden layer can be regarded as new features of observed data. These new features will be piped into a process to train a classifier. As most existing classifiers are designed for vectorial data, in this experiment, the hidden matrix features given by MVRBM will be concatenated into vectors and then use the $K$ Nearest Neighbor ($K$-NN) Classifier with $K=1$ for classification.\n\nWe assess how different model training settings impact the 1-NN classifier performance. Under different training settings, we first train an MVRBM, use it as a feature extractor and then conduct 1-NN classification over all the testing digits. Finally we report the classification error rates.\n\nIn the first test, we fix the size of hidden units at $25\\times25$ and iteration number for $T=2,000$ while varying the number of training samples from $100$ to $20,000$. We show the results of classification errors in Figure~\\ref{Figure12}(a). Sufficient training samples lead to better classification performance.   In another test, we randomly choose $10,000$ training samples while varying the iteration number from $10$ to $3,000$ for training. The curve in Figure~\\ref{Figure12}(b) shows the change of classification errors over the iteration numbers. We can observe that learning MVRBM model has been stable after 70 iterations. Particularly for the iteration numbers from $300$ to $3,000$, the classification error rate only changes from $0.0571$ to $0.0520$. Based on these observations, we recommend $N=20,000$ and $T=3,000$ in the most of the following experiments while comparing with other models.  In our experiment, we also note that the MVRBM with $50,000$ training samples gives a quite good accuracy of $0.0359$ although this accuracy goes up to $0.1387$ with only 600 training samples.\n\n\n\\begin{figure}\n \\centering\n \\subfigure[fixed $T=2000$] {\\includegraphics[width=0.23\\textwidth]{figure12}}\\;\n \\subfigure[fixed $N=10000$]{\\includegraphics[width=0.23\\textwidth]{figure13.pdf}}\n \\caption{Classification Errors vs $N$ (a) and $T$ (b).}\n \\label{Figure12}\n\\end{figure}\n\nFinally we compare the performance of the proposed model against other state-of-the-art methods which include the Deep Neural Network based drop-out method (DNN)~\\cite{SrivastavaHintonKrizhevskySutskeverSalakhutdinov2014}, the Deep Belief Networks (DBN)~\\cite{HintonS.Teh2006}, the Convolutional Neural Networks (CNN)~\\cite{LeCunKavukcuogluFarabet2010} and the Sparse Autoencoder (SAE)~\\cite{Andrew2011}.\nThese compared methods are implemented in the DeepLearn Toolbox which is available online at \\url{https://github.com/rasmusbergpalm/DeepLearnToolbox} and we use their default parameter settings.\nFigure~\\ref{figure11}(a) and (b) show the overall results for which we fix the iteration number $T=3,000$ as suggested for MVRBM. The observation we can make from two figures is that with the sufficient training samples the newly proposed MVRBM is comparable to all the other methods while the new model outperforms others in the cases of few samples (less than 10,000). We believe this is due to the fact that the MVRBM has much less model parameters than other models, thus it is much more immune to overfitting.\n \\begin{figure}\n\\centering\n \\subfigure[Smaller $N$'s]{\\includegraphics[width=0.23\\textwidth]{figure11.pdf}}\n \\subfigure[Larger $N$'s]{\\includegraphics[width=0.23\\textwidth]{figure10.pdf}}\n\\caption{The Classification Errors vs $N$.}\n\\label{figure11}\n\\end{figure}\n\n\\subsection{Experiment 3: Image Super-resolution}\n\nIn this experiment we apply our Mutlimodal MVRBM model for image super-resolution. We follow the same setting used in~\\cite{YangWrightHuangM2010} to prepare training data. The training patches are randomly taken from 69 Miscellaneous color images which are available online at \\texttt{http://decsai.ugr.es/cvg/dbimagenes/}.\n\nEach training sample consists of a high resolution patch $X$ (the raw image from luminance Y channel in the YCbCr color space) and four low resolution patches which are the directives of images in x-, y-, xx- and yy-directions, denoted by $Z^1$, $Z^2$, $Z^3$ and $Z^4$ of the Y channel. Hence at the visible layer we have five matrix patches $(X, Z^1, Z^2, Z^3,Z^4)$. The energy function defined in \\eqref{EnergyMultiview} can be appropriately extended to cope with this case.\n\nWe select $N$ training samples from the Miscelaneous database, denoted by\n$\\mathcal{D}=\\{ (X_1, Z^1_1, Z^2_1, Z^3_1,$ $Z^4_1),  \\cdots, (X_N, Z^1_N, Z^2_N, Z^3_N, Z^4_N)\\}$. In this experiment, we randomly sample $N=10,000$ training patches and try different image patch sizes of $10\\times 10$, $15\\times15$, $20\\times20$, $30\\times $ and $35\\times 35$, all with a magnification factor of 2. The hidden size is fixed to 20 to demonstrate the potential of using overcomplete dictionary. We found this hidden size gives better results in all the tests conducted.\nFigure~\\ref{Figure3Gao}(a) shows sample patches $X$ of size 15 and Figure~\\ref{Figure3Gao}(b) shows the learned $U$ and $V$ in terms of $U^T\\otimes V^T$.\n\nAfter training MMVRBM for each case, we use the following strategy to conduct super-resolution inference. Given a low resolution feature input $Z= (Z^1, Z^2, Z^3, Z^4)$, we first use any simple super-resolution algorithm\nsuch as interpolation based methods to get an estimate $X^0$ of the desired super-resolution patch $X$. Then take as the input(s) both $Z$ and $X^0$ to the visible layer of a trained MMVRBM and run the MMVRBM training algorithm to transfer message from\nthe visible layer to the hidden layer to get the variable $Y$. Following that, the message $Y$ can be transferred back from the hidden layer to the visible layer and the super-resolution results can be taken from those $X$ units. In general, this gives a faster inference algorithm as demonstrated in our experiments. While necessary, this process can be run several cycles to reach equilibrium. The similar idea has been used in~\\cite{YangWangLinCohenScottHuang2012} where a neural network is trained as a separate post-procedure.\n\nWe apply this inference for the super-resolution on Lena image in size $256\\times 256$. Table~\\ref{Table3Gao} shows the reconstruction errors for each case. Based on the results we recommend using patch size 30 for general super-resolution. In Figure~\\ref{figure5lena}, we compare our method with several other methods on  Lena for super-resolution. In this experiment we fix the patch size to $15\\times15$ and hidden size to $20\\times20$. The size of low-resolution input image is $256\\times256$. The PSNR of our method is $35.3006$dB, much higher than $34.1282$dB from the classic bicubic interpolation. As our model is in bilinear format which is a sub-model of the full linear model such as the \\textit{Super-resolution via Sparse Representation} (SR) algorithm in~\\cite{YangWrightHuangM2010}, the PSNR of our method is slightly inferior to the SR method, however the reconstruction time is much better than theirs\\footnote{As the bicubic interpolation is implemented in Matlab and highly optimized, we did not report its super-resolution recovery time.}.\n\\begin{figure}\n\\centering\n\\subfigure[High Resolution Patches]{\\includegraphics[width=0.20\\textwidth]{dictionarypatch15.pdf}}\\;\n\\subfigure[The learned $U^T\\otimes V^T$]{\\includegraphics[width=0.20\\textwidth]{dictionary.pdf}}\n\\caption{The selected training patches and the learned filters or dictionary in terms of $U^T\\otimes V^T$. }\n\\label{Figure3Gao}\n\\end{figure}\n\\begin{table}\n  \\centering\n  \\begin{tabular}{|c|c|c|}\n\\hline\nPatch Size& Hidden Size & PSNR(dB)\\[0.5ex]\\hline\n\n\n$10\\times10$&$20\\times20$&\t35.1621\\\\\\hline\n$15\\times15$&$20\\times20$&\t35.3227\\\\\\hline\n$20\\times20$&$20\\times20$&\t35.3555\\\\\\hline\n$30\\times30$&$20\\times20$&\t35.3606\\\\\\hline\n$35\\times35$&$20\\times20$&\t35.3564\\\\\\hline\n\\end{tabular}\n  \\caption{MVRBM models for different patches}\\label{Table3Gao}\n\\end{table}\n\n\nMore tests have been conducted for natural image super-resolution, which are in the supplementary document to save the space of the paper. Here we only present one set of experiment as an example in Figure~\\ref{natural}. The results show that the faster reconstruction of the proposed model against the SR algorithm while maintaining comparable reconstruction accuracy.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.065\\textwidth]{Figure5_1.pdf}\n\\includegraphics[width=0.13\\textwidth]{Figure5_2.pdf}\n\\includegraphics[width=0.13\\textwidth]{Figure5_3.pdf}\n\\includegraphics[width=0.13\\textwidth]{Figure5_4.pdf}\n\n\\caption{Results of Lena image magnified by a factor of 2  and the corresponding RMSEs, Testing time and PSNR. Left to right: Input $(256\\times256)$, Bicubic Interpolation (RMSE: 5.0134; PSNR: 34.1282dB), SR~\\cite{YangWrightHuangM2010} (RMSE: 4.0900; Time: 679.529s; PSNR: 35.8963dB) and our Method (RMSE: 4.3804; Time: 36.116s; PSNR: 35.3006dB).}\n\\label{figure5lena}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.065\\textwidth]{10-8-1.pdf}\n\\includegraphics[width=0.13\\textwidth]{10-8-3.pdf}\n\\includegraphics[width=0.13\\textwidth]{10-8-4.pdf}\n\\includegraphics[width=0.13\\textwidth]{10-8-5.pdf}\n\n\\caption{Results of Bird image magnified by a factor of 2  and the corresponding RMSEs, Testing time and PSNR. Left to right: Input$(256\\times256)$, Bicubic Interpolation (RMSE: 4.7932; PSNR: 34.5184 dB), SR~\\cite{YangWrightHuangM2010} (RMSE: 3.7975; Time: 748.225s; PSNR: 36.5409dB) and our Method (RMSE: 3.9459; Time: 35.196s; PSNR: 36.2514dB).}\n\\label{natural}\n\\end{figure}\n\n\n\n\\section{Conclusions}\\label{Sec:6}\nIn this paper, we proposed a novel model called Matrix Variate Restricted Boltzmann Machine (MVRBM) for 2D matrix variate data  by defining a bilinear connection between matrix variate visible layer and matrix variate hidden layer. Different from the traditional RBM which vectorizes the 2D matrix variate, this new model can make good use of spatial information in 2D matrix data, and be easily extended to any higher order tensor variate data.\n\nIn order to learn model parameters in MVRBM, we express the multiplicative interaction between visible and hidden units as a specified structure, and thus the number of free parameters in the model is significantly reduced, compared to the corresponding vectorized RBM models. The relevant learning algorithm for the new model has been investigated.\n\nThe experiments have demonstrated the new model is comparable to the classic RBM while maintaining good training and inferring computational complexity. This has been particularly demonstrated in the application to the image super-resolution problem.\nOur model can be easily incorporated into a deeper structure. Using the deeper structure we may get more abstract feature and better performance. Our future work is to apply the method into the field of the deep learning and the construction of the deep neural network structures.\n\n{\\small\n\\begin{thebibliography}{1}\n\n\\bibitem{AdaliLevinCalhoun2015}\nT. Adali, S. Y. Levin, and V. D. Calhoun. Multimodal data fusionusingsourceseparation: Applicationtomedicalimaging. IEEE Proceedings, 103(9):1494\u00a8C1506, 2015.\n\n\\bibitem{Andrew2011}\nN. Andrew. Sparse autoencoder. CS294A Lecture notes, 72,2011.\n\n\\bibitem{Bengio2009}\nY. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1\u00a8C127, 2009.\n\n\\bibitem{BernabeMarpuPlazaMuraBenediktsson2014}\nS. Bernabe, P. R. Marpu, A. Plaza, M. D. Mura, and J. A.Benediktsson. Spectral\u00a8Cspatialclassificationofmultispectral images using kernel feature space representation. IEEE Geoscience and Remote Sensing Letters, 11(1):288\u00a8C292, 2014.\n\n\\bibitem{Bishop2006}\nC. Bishop.Pattern Recognition and Machine Learning.Springer, 2006.\n\n\\bibitem{ChazalTapsonSchaik2015}\nP. de Chazal, J. Tapson, and A. van Schaik. A comparison of extreme learning machines and back-propagation trained\nfeed-forward networks processing the MINST database. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2165\u00a8C2168. IEEE, 2015.\n\n\\bibitem{Deng2012}\nL. Deng. The MNIST database of handwritten digit images formachinelearningresearch. IEEESignalProcessingMagazine, 29(6):141\u00a8C142, 2012.\n\n\\bibitem{FischerIgel2012}\nA. Fischer and C. Igel. An introduction to restricted Boltzmann machines. In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, pages 14\u00a8C36.Springer, 2012.\n\n\\bibitem{Garzelli2015}\nA. Garzelli. Pansharpening of multispectral images based\non nonlocal parameter optimization. IEEE Transactions on\nGeoscience and Remote Sensing, 53(4):2096\u00a8C2107, 2015.\n\n\\bibitem{GuyByrneRich2014}\nR. Guy, B. Byrne, and P. Rich. Supporting physiology learn-\ning: thedevelopmentofinteractiveconcept-basedvideoclip-\ns. Advances in Physiology Education, 38(1):96\u00a8C98, 2014.\n\n\\bibitem{Hinton2002}\nG. E. Hinton. Training products of experts by minimizing\ncontrastive divergence. Neural Computation, 14(8):1771\u00a8C\n1800, 2002.\n\n\\bibitem{HintonOsinderoTeh2006}\nG. E. Hinton, S. S., and Y. W. Teh. A fast learning algorithm\nfor deep belief nets. Neural Computation, 18(7):1527\u00a8C1554,\n2006.\n\n\\bibitem{HintonSalakhutdinov2006}\nG. E. Hinton and R. Salakhutdinov. Reducing the dimension-\nality of data with neural networks. Science, 313(5786):504\u00a8C\n507, 2006.\n\n\\bibitem{HintonSalakhutdinov2009}\nG. E. Hinton and R. Salakhutdinov. Replicated softmax: an\nundirected topic model. In Advances in Neural Information\nProcessing Systems, pages 1607\u00a8C1614, 2009.\n\n\\bibitem{HintonSejnowski1983}\nG. E. Hinton and T. J. Sejnowski. Optimal perceptual infer-\nence. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 448\u00a8C453. Citeseer, 1983.\n\n\\bibitem{HutchinsonDengYu2013}\nB. Hutchinson, L. Deng, and D. Yu. Tensor deep stacking\nnetworks. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 35(8):1944\u00a8C1957, 2013.\n\n\\bibitem{KangGonugondlaMin-SunShanbhag2015}\nM. Kang, S. K. Gonugondla, K. Min-Sun, and N. R.\nShanbhag.\nAn energy-efficient memory-based high-\nthroughput vlsi architecture for convolutional networks. In\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 1037\u00a8C1041. IEEE, 2015.\n\n\\bibitem{KoldaBader2009}\nT. G. Kolda and B. W. Bader. Tensor decompositions and\napplications. SIAM Review, 51(3):455\u00a8C500, 2009.\n\n\\bibitem{KrizhevskyHinton2010}\nA. Krizhevsky and G. E. Hinton. Factored 3-way restricted\nBoltzmann machines for modeling natural images. In Inter-\nnational Conference on Artificial Intelligence and Statistics,\npages 621\u00a8C628, 2010.\n\n\\bibitem{LeCunKavukcuogluFarabet2010}\nY. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional\nnetworks and applications in vision. In IEEE Internation-\nal Symposium on Circuits and Systems (ISCAS), pages 253\u00a8C\n256. IEEE, 2010.\n\n\\bibitem{LuHaligWangChenFei2014}\nG. Lu, L. Halig, D. Wang, Z. G. Chen, and B. Fei. Spectral-\nspatial classification using tensor modeling for cancer detec-\ntion of hyperspectral imaging. In SPIE Medical Imaging,\npages 903413\u00a8C903413, 2014.\n\n\\bibitem{MemisevicHinton2010}\nR. Memisevic and G. E. Hinton. Learning to represent s-\npatial transformations with factored higher-order Boltzmann\nmachines. Neural Computation, 22(6):1473\u00a8C1492, 2010.\n\n\\bibitem{NguyenTranPhungVenkatesh2015}\nT. D. Nguyen, T. Tran, D. Phung, and S. Venkatesh. Tensor-\nvariate restricted Boltzmann machines.\nIn Twenty-Ninth\nAAAI Conference on Artificial Intelligence, 2015.\n\n\\bibitem{RovidSzeidlVarlaki2011}\n A. R?vid, L. Szeidl, and P. V\u00a8\u00a2rlaki. On tensor-product mod-\nel based representation of neural networks. In IEEE Interna-\ntional Conference on Intelligent Engineering Systems, pages\n69\u00a8C72, 2011.\n\n\\bibitem{SalakhutdinovHinton2010}\nR. Salakhutdinov and G. Hinton. Replicated softmax: an\nundirected topic model. In Neural Information Processing\nSystems, volume 23, 2010.\n\n\\bibitem{SocherChenManningAndrew2013}\nR. Socher, D. Chen, C. Manning, and N. Andrew. Reason-\ning with neural tensor networks for knowledge base comple-\ntion. In Advances in Neural Information Processing Systems,\npages 926\u00a8C934, 2013.\n\n\\bibitem{SrivastavaHintonKrizhevskySutskeverSalakhutdinov2014}\nN.Srivastava, G.E. Hinton, A.Krizhevsky, L. Sutskever, and\nR. Salakhutdinov. Dropout: A simple way to prevent neural\nnetworks from overfitting. The Journal of Machine Learning\nResearch, 15(1):1929\u00a8C1958, 2014.\n\n\\bibitem{TangSalakhutdinovHinton2013}\nY. Tang, R. Salakhutdinov, and G. Hinton. Tensor analyzer-\ns. In International Conference on Machine Learning, pages\n163\u00a8C171, 2013.\n\n\\bibitem{TaylorHinton2009}\nG. W. Taylor and G. E. Hinton. Factored conditional restrict-\ned Boltzmann machines for modeling motion style. In In-\nternational Conference on Machine Learning, pages 1025\u00a8C\n1032, 2009.\n\n\\bibitem{QiSunGaoHu2015}\nTheAuthors. Tensorial variate RBMs and applications. IEEE\nTran. Pattern. Analy. Mach. Intell., to be submitted, 2015.\n\n\\bibitem{TielemanHinton2009}\nT. Tieleman and G. Hinton. Using fast weights to improve\npersistent contrastive divergence. In The 26th International\nConference on Machine Learning, pages 1033\u00a8C1040, 2009.\n\n\\bibitem{YangWangLinCohenScottHuang2012}\nJ. Yang, Z. Wang, Z. Lin, S. Cohen, Scott, and T. Huang.\nCoupled dictionary training for image super-resolution.\nIEEE Transactions on Image Processing, 21(8):3467\u00a8C3478,\n2012.\n\n\\bibitem{YangWrightHuangM2008}\nJ. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing, 19(11):2861\u00a8C2873, 2010.\n\n\\bibitem{ZhaoAmmarRoos2013}\nW. Zhao, H. B. Ammar, and N. Roos. Dynamic object recognition using sparse coded three-way conditional restricted boltzmann machines. In The 25th Benelux Conference on\nArtificial Intelligence (BNAIC), 2013.\n\n\n\\end{thebibliography}\n\n}\n\n\n\n", "itemtype": "equation", "pos": 26758, "prevtext": "\n\nGiven the energy function in \\eqref{EnergyMultiview}, the following joint distribution\n", "index": 57, "text": "\n\\[\np(X,Z,Y;\\Theta) = \\frac1{Z(\\Theta)} \\exp \\{-E(X, Z, Y; \\Theta)\\}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex29.m1\" class=\"ltx_Math\" alttext=\"p(X,Z,Y;\\Theta)=\\frac{1}{Z(\\Theta)}\\exp\\{-E(X,Z,Y;\\Theta)\\}\" display=\"block\"><mrow><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Z</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo>-</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Z</mi><mo>,</mo><mi>Y</mi><mo>;</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}]