[{"file": "1601.00710.tex", "nexttext": "E$, $G\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\nWe build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.  Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\newcite{kay2000} points out that if a document is translated once, it is likely to be translated again and again into other languages.  This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT).   The translation system now has two strings as input, which can reduce ambiguity via ``triangulation'' (Kay's term).  For example, the normally ambiguous English word ``bank'' may be more easily translated into French in the presence of a second, German input string containing the word ``Flussufer'' (river bank).\n\n\\newcite{och01} describe such a {\\em multi-source} MT system.  They first train separate bilingual MT systems $F\n", "index": 1, "text": "$$\\rightarrow$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"block\"><mo>\u2192</mo></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "E$, $G\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\nWe build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.  Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\newcite{kay2000} points out that if a document is translated once, it is likely to be translated again and again into other languages.  This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT).   The translation system now has two strings as input, which can reduce ambiguity via ``triangulation'' (Kay's term).  For example, the normally ambiguous English word ``bank'' may be more easily translated into French in the presence of a second, German input string containing the word ``Flussufer'' (river bank).\n\n\\newcite{och01} describe such a {\\em multi-source} MT system.  They first train separate bilingual MT systems $F\n", "index": 1, "text": "$$\\rightarrow$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"block\"><mo>\u2192</mo></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "E$, $G\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\nWe build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.  Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\newcite{kay2000} points out that if a document is translated once, it is likely to be translated again and again into other languages.  This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT).   The translation system now has two strings as input, which can reduce ambiguity via ``triangulation'' (Kay's term).  For example, the normally ambiguous English word ``bank'' may be more easily translated into French in the presence of a second, German input string containing the word ``Flussufer'' (river bank).\n\n\\newcite{och01} describe such a {\\em multi-source} MT system.  They first train separate bilingual MT systems $F\n", "index": 1, "text": "$$\\rightarrow$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"block\"><mo>\u2192</mo></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "E$, $G\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\nWe build a multi-source machine translation model and train it to maximize the probability of a target English string given French and German sources.  Using the neural encoder-decoder framework, we explore several combination methods and report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\newcite{kay2000} points out that if a document is translated once, it is likely to be translated again and again into other languages.  This gives rise to an interesting idea: a human does the first translation by hand, then turns the rest over to machine translation (MT).   The translation system now has two strings as input, which can reduce ambiguity via ``triangulation'' (Kay's term).  For example, the normally ambiguous English word ``bank'' may be more easily translated into French in the presence of a second, German input string containing the word ``Flussufer'' (river bank).\n\n\\newcite{och01} describe such a {\\em multi-source} MT system.  They first train separate bilingual MT systems $F\n", "index": 1, "text": "$$\\rightarrow$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"block\"><mo>\u2192</mo></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\\noindent $W_c$ and all other weights in the network are learned from example string triples drawn from a trilingual training corpus. \n\nThe new cell state is simply the sum of the two cell states from the encoders. \n\n\n", "itemtype": "equation", "pos": 6070, "prevtext": "E$ translations.  \\newcite{ccb02} exploits hypothesis selection in multi-source MT to expand available corpora, via co-training.\n\nOthers use system combination techniques to merge hypotheses at the word level, creating the ability to synthesize new translations outside those proposed by the single-source translators.  These methods include confusion networks \\cite{Matusov_computingconsensus,Schroeder:2009:WLM:1609067.1609147}, source-side string combination \\cite{Schroeder:2009:WLM:1609067.1609147}, and median strings \\cite{conf/icpr/Gonzalez-RubioC10}.\n\nThe above work all relies on base MT systems trained on {\\em bilingual data}, using traditional methods.  This follows early work in sentence alignment \\cite{gale1993program} and word alignment \\cite{simard1999text}, which exploited trilingual text, but did not build trilingual models.  Previous authors possibly considered a three-dimensional translation table t($e|f,g$) to be prohibitive.\n\nIn this paper, by contrast, we train a P($e|f,g$) model directly on  {\\em trilingual data}, and we use that model to decode an ($f,g$) pair simultaneously.  We view this as a kind of multi-tape transduction \\cite{elgot1965relations,kaplan1994regular,derimake} with two input tapes and one output tape.  Our contributions are as follows:\n\n\\begin{itemize}\n\\item We train a P($e|f,g$) model directly on trilingual data, and we use it to decode a new source string pair ($f,g$) into target string $e$.\n\\item We show positive Bleu improvements over strong single-source baselines.\n\\item We show that improvements are best when the two source languages are more distant from each other.\n\\end{itemize}\n\nWe are able to achieve these results using the framework of neural encoder-decoder models, where multi-target MT \\cite{dong15} and multi-source, cross-modal mappings have been explored \\cite{luong15}.\n\n\\section{Multi-Source Neural MT}\n \n\\begin{figure*}\n\n\n\\begin{center}\n\\includegraphics[width=12cm]{four-layer-crop.pdf}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{The encoder-decoder framework for neural machine translation (NMT) \\cite{sutskever2014sequence}.  Here, a source sentence C B A (presented in reverse order as A B C) is translated into a target sentence W X Y Z.  At each step, an evolving real-valued vector summarizes the state of the encoder (white) and decoder (gray).}\n\\label{enc-dec}\n\\end{figure*}\n\nIn the neural encoder-decoder framework for MT \\cite{neco1997asynchronous,castano1997connectionist,sutskever2014sequence,bahdanau2014neural,luong2015effective}, we use a recurrent neural network ({\\em encoder}) to convert a source sentence into a dense, fixed-length vector.  We then use another recurrent network ({\\em decoder}) to convert that vector in a target sentence.\\footnote{We follow previous authors in presenting the source sentence to the encoder in reverse order.} \n\n\nIn this paper, we use a four-layer encoder-decoder system (Figure~\\ref{enc-dec}) with long short-term memory (LSTM) units \\cite{lstm} trained for maximum likelihood (via a softmax layer) with back-propagation through time \\cite{btt}. \n\n\nFor our baseline single-source MT system we use two different models, one of which implements the local attention plus feed-input model from \\newcite{luong2015effective}.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=12cm]{four-layer-crop2.pdf}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{Multi-source encoder-decoder model for MT\\@.  We have two source sentences (C B A and K J I) in different languages. Each language has its own encoder; it passes its final hidden and cell state to a set of {\\em combiners} (in black). The output of a combiner is a hidden state and cell state of the same dimension.}\n\\label{enc-dec-multi}\n\\end{figure*}\n\nFigure~\\ref{enc-dec-multi} shows our approach to multi-source MT\\@. Each source language has its own encoder. The question is how to combine the hidden states and cell states from each encoder, to pass on to the decoder. Black {\\em combiner} blocks implement a function whose input is two hidden states ($h_1$ and $h_2$) and two cell states ($c_1$ and $c_2$), and whose output is a single hidden state $h$ and cell state $c$. We propose two combination methods.\n\n\n\n\n\n\n\n\\subsection{Basic Combination Method}\n\nThe Basic method works by concatenating the two hidden states from the source encoders, applying a linear transformation $W_c$ (size 2000 x 1000), then sending its output through a tanh non-linearity. This operation is represented by the equation:\n\n\n", "index": 9, "text": "\\begin{equation} \nh = \\mathrm{tanh}\\Big( W_{c}\\big[ h_{1} ; h_{2} \\big]  \\Big) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"h=\\mathrm{tanh}\\Big{(}W_{c}\\big{[}h_{1};h_{2}\\big{]}\\Big{)}\" display=\"block\"><mrow><mi>h</mi><mo>=</mo><mrow><mi>tanh</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msub><mi>W</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><msub><mi>h</mi><mn>1</mn></msub><mo>;</mo><msub><mi>h</mi><mn>2</mn></msub><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\\noindent\nWe also attempted to concatenate cell states and apply a linear transformation, but training diverges due to large cell values.\n\n\\subsection{Child-Sum Method}\n\nOur second combination method is inspired by the  Child-Sum Tree-LSTMs of \\newcite{treeLSTM}. Here, we use an LSTM variant to combine the two hidden states and cells. The standard LSTM input, output, and new cell value are all calculated. Then cell states from each encoder get their own forget gates. The final cell state and hidden state are calculated as in a normal LSTM.  More precisely:\n\n\n", "itemtype": "equation", "pos": 6383, "prevtext": "\n\n\\noindent $W_c$ and all other weights in the network are learned from example string triples drawn from a trilingual training corpus. \n\nThe new cell state is simply the sum of the two cell states from the encoders. \n\n\n", "index": 11, "text": "\\begin{equation} \nc = c_{1} + c_{2} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"c=c_{1}+c_{2}\" display=\"block\"><mrow><mi>c</mi><mo>=</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>+</mo><msub><mi>c</mi><mn>2</mn></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 7000, "prevtext": "\n\n\\noindent\nWe also attempted to concatenate cell states and apply a linear transformation, but training diverges due to large cell values.\n\n\\subsection{Child-Sum Method}\n\nOur second combination method is inspired by the  Child-Sum Tree-LSTMs of \\newcite{treeLSTM}. Here, we use an LSTM variant to combine the two hidden states and cells. The standard LSTM input, output, and new cell value are all calculated. Then cell states from each encoder get their own forget gates. The final cell state and hidden state are calculated as in a normal LSTM.  More precisely:\n\n\n", "index": 13, "text": "\\begin{equation} i = \\mathrm{sigmoid} \\Big(  W_{1}^{i}h_{1} + W_{2}^{i}h_{2}  \\Big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"i=\\mathrm{sigmoid}\\Big{(}W_{1}^{i}h_{1}+W_{2}^{i}h_{2}\\Big{)}\" display=\"block\"><mrow><mi>i</mi><mo>=</mo><mrow><mi>sigmoid</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><msubsup><mi>W</mi><mn>1</mn><mi>i</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msubsup><mi>W</mi><mn>2</mn><mi>i</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 7099, "prevtext": "\n\n", "index": 15, "text": "\\begin{equation} f = \\mathrm{sigmoid} \\Big(  W_{i}^{f}h_{i} \\Big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"f=\\mathrm{sigmoid}\\Big{(}W_{i}^{f}h_{i}\\Big{)}\" display=\"block\"><mrow><mi>f</mi><mo>=</mo><mrow><mi>sigmoid</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msubsup><mi>W</mi><mi>i</mi><mi>f</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 7180, "prevtext": "\n\n", "index": 17, "text": "\\begin{equation} o = \\mathrm{sigmoid} \\Big(  W_{1}^{o}h_{1} + W_{2}^{o}h_{2}  \\Big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"o=\\mathrm{sigmoid}\\Big{(}W_{1}^{o}h_{1}+W_{2}^{o}h_{2}\\Big{)}\" display=\"block\"><mrow><mi>o</mi><mo>=</mo><mrow><mi>sigmoid</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><msubsup><mi>W</mi><mn>1</mn><mi>o</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msubsup><mi>W</mi><mn>2</mn><mi>o</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 7279, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation} u = \\mathrm{tanh} \\Big(  W_{1}^{u}h_{1} + W_{2}^{u}h_{2}  \\Big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"u=\\mathrm{tanh}\\Big{(}W_{1}^{u}h_{1}+W_{2}^{u}h_{2}\\Big{)}\" display=\"block\"><mrow><mi>u</mi><mo>=</mo><mrow><mi>tanh</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><mrow><msubsup><mi>W</mi><mn>1</mn><mi>u</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msubsup><mi>W</mi><mn>2</mn><mi>u</mi></msubsup><mo>\u2062</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 7375, "prevtext": "\n\n", "index": 21, "text": "\\begin{equation} c = i_{f} \\odot u_{f} + f_{1} \\odot c_{1} + f_{2} \\odot c_{2} \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"c=i_{f}\\odot u_{f}+f_{1}\\odot c_{1}+f_{2}\\odot c_{2}\" display=\"block\"><mrow><mi>c</mi><mo>=</mo><mrow><mrow><msub><mi>i</mi><mi>f</mi></msub><mo>\u2299</mo><msub><mi>u</mi><mi>f</mi></msub></mrow><mo>+</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>\u2299</mo><msub><mi>c</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>f</mi><mn>2</mn></msub><mo>\u2299</mo><msub><mi>c</mi><mn>2</mn></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\nThis method employs eight new matrices (the $W$'s in the above equations), each of size 1000~x~1000. The $\\odot$ symbol represents an elementwise multiplication. In equation~3, $i$ represents the input gate of a typical LSTM cell. In equation~4, there are two forget gates indexed by the subscript $i$ that serve as the forget gates for each of the incoming cells for each of the encoders. In equation~5, $o$ represents the output gate of a normal LSTM. $i$, $f$, $o$, and $u$ are all size-1000 vectors.\n\n\\subsection{Multi-Source Attention}\n\n\nOur single-source attention model is modeled off the local-p attention model with feed input from \\newcite{luong2015effective}, where hidden states from the top decoder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hidden states, to make a better hidden state vector ($\\tilde{h_{t}}$), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step.  \n\nThe local-p attention model from \\newcite{luong2015effective} works as follows. First, a position to look at in the source encoder is predicted by equation~9:\n\n\n", "itemtype": "equation", "pos": 7469, "prevtext": "\n\n", "index": 23, "text": "\\begin{equation} h = o_{f} \\odot \\mathrm{tanh}(c_{f}) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"h=o_{f}\\odot\\mathrm{tanh}(c_{f})\" display=\"block\"><mrow><mi>h</mi><mo>=</mo><mrow><mrow><msub><mi>o</mi><mi>f</mi></msub><mo>\u2299</mo><mi>tanh</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>c</mi><mi>f</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\\noindent\n$S$ is the source sentence length, and $v_p$ and $W_p$ are learned parameters, with $v_p$ being a vector of dimension 1000, and $W_p$ being a matrix of dimension 1000~x~1000.\n\nAfter $p_t$ is computed, a window of size $2D+1$ is looked at in the top layer of the source encoder centered around $p_t$ ($D=10$). \nFor each hidden state in this window, we compute an alignment score $a_t\\big(s\\big)$, between 0 and 1. This alignment score is computed by equations~10, 11 and 12:\n\n\n", "itemtype": "equation", "pos": 8795, "prevtext": "\n\nThis method employs eight new matrices (the $W$'s in the above equations), each of size 1000~x~1000. The $\\odot$ symbol represents an elementwise multiplication. In equation~3, $i$ represents the input gate of a typical LSTM cell. In equation~4, there are two forget gates indexed by the subscript $i$ that serve as the forget gates for each of the incoming cells for each of the encoders. In equation~5, $o$ represents the output gate of a normal LSTM. $i$, $f$, $o$, and $u$ are all size-1000 vectors.\n\n\\subsection{Multi-Source Attention}\n\n\nOur single-source attention model is modeled off the local-p attention model with feed input from \\newcite{luong2015effective}, where hidden states from the top decoder layer can look back at the top hidden states from the encoder. The top decoder hidden state is combined with a weighted sum of the encoder hidden states, to make a better hidden state vector ($\\tilde{h_{t}}$), which is passed to the softmax output layer. With input-feeding, the hidden state from the attention model is sent down to the bottom decoder layer at the next time step.  \n\nThe local-p attention model from \\newcite{luong2015effective} works as follows. First, a position to look at in the source encoder is predicted by equation~9:\n\n\n", "index": 25, "text": "\\begin{equation}\np_t = S \\cdot \\mathrm{sigmoid} \\big( v_p^T \\mathrm{tanh}\\big( W_p h_t \\big) \\big )\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"p_{t}=S\\cdot\\mathrm{sigmoid}\\big{(}v_{p}^{T}\\mathrm{tanh}\\big{(}W_{p}h_{t}\\big%&#10;{)}\\big{)}\" display=\"block\"><mrow><msub><mi>p</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mi>S</mi><mo>\u22c5</mo><mi>sigmoid</mi></mrow><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msubsup><mi>v</mi><mi>p</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>tanh</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mi>W</mi><mi>p</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>t</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 9396, "prevtext": "\n\n\\noindent\n$S$ is the source sentence length, and $v_p$ and $W_p$ are learned parameters, with $v_p$ being a vector of dimension 1000, and $W_p$ being a matrix of dimension 1000~x~1000.\n\nAfter $p_t$ is computed, a window of size $2D+1$ is looked at in the top layer of the source encoder centered around $p_t$ ($D=10$). \nFor each hidden state in this window, we compute an alignment score $a_t\\big(s\\big)$, between 0 and 1. This alignment score is computed by equations~10, 11 and 12:\n\n\n", "index": 27, "text": "\\begin{equation}\na_t\\big(s\\big) = \\mathrm{align}\\big( h_t , h_s \\big) \\mathrm{exp} \\Big( \\frac{-\\big( s - p_t \\big)^2}{2\\sigma^2} \\Big)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"a_{t}\\big{(}s\\big{)}=\\mathrm{align}\\big{(}h_{t},h_{s}\\big{)}\\mathrm{exp}\\Big{(%&#10;}\\frac{-\\big{(}s-p_{t}\\big{)}^{2}}{2\\sigma^{2}}\\Big{)}\" display=\"block\"><mrow><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>s</mi><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>align</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mi>s</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><mi>exp</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mfrac><mrow><mo>-</mo><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>s</mi><mo>-</mo><msub><mi>p</mi><mi>t</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03c3</mi><mn>2</mn></msup></mrow></mfrac><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 9548, "prevtext": "\n\n\n", "index": 29, "text": "\\begin{equation}\n\\mathrm{align}\\big( h_t , h_s \\big) = \\frac{\\mathrm{exp}\\big(\\mathrm{score}\\big( h_t , h_s \\big) \\big) }{\\sum_{s'}\\mathrm{exp}\\big(\\mathrm{score}\\big( h_t , h_{s'} \\big) \\big)}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{align}\\big{(}h_{t},h_{s}\\big{)}=\\frac{\\mathrm{exp}\\big{(}\\mathrm{score%&#10;}\\big{(}h_{t},h_{s}\\big{)}\\big{)}}{\\sum_{s^{\\prime}}\\mathrm{exp}\\big{(}\\mathrm%&#10;{score}\\big{(}h_{t},h_{s^{\\prime}}\\big{)}\\big{)}}\" display=\"block\"><mrow><mrow><mi>align</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mi>s</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>score</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mi>s</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><msup><mi>s</mi><mo>\u2032</mo></msup></msub><mrow><mi>exp</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>score</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><msup><mi>s</mi><mo>\u2032</mo></msup></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\nIn equation~10, $\\sigma$ is set to be $D/2$ and $s$ is the source index for that hidden state. $W_a$ is a learnable parameter of dimension 1000~x~1000. \n\nOnce all of the alignments are calculated, $c_t$ is created by taking a weighted sum of all source hidden states multiplied by their alignment weight. \n\nThe final hidden state sent to the softmax layer is given by:\n\n\n", "itemtype": "equation", "pos": 9758, "prevtext": "\n\n\n", "index": 31, "text": "\\begin{equation}\n\\mathrm{score}\\big( h_t , h_s \\big) = h_t^T W_a h_s\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{score}\\big{(}h_{t},h_{s}\\big{)}=h_{t}^{T}W_{a}h_{s}\" display=\"block\"><mrow><mrow><mi>score</mi><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mi>s</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>h</mi><mi>t</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>W</mi><mi>a</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>s</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\n\n\nWe modify this attention model to look at both source encoders simultaneously. We create a context vector from each source encoder named $c_{t}^{1}$ and $c_{t}^{2}$ instead of the just $c_t$ in the single-source attention model:\n\n\n", "itemtype": "equation", "pos": 10213, "prevtext": "\n\nIn equation~10, $\\sigma$ is set to be $D/2$ and $s$ is the source index for that hidden state. $W_a$ is a learnable parameter of dimension 1000~x~1000. \n\nOnce all of the alignments are calculated, $c_t$ is created by taking a weighted sum of all source hidden states multiplied by their alignment weight. \n\nThe final hidden state sent to the softmax layer is given by:\n\n\n", "index": 33, "text": "\\begin{equation} \n\\tilde{h_{t}} = tanh \\Big( W_{c} \\big[ h_{t} ; c_{t} \\big] \\Big) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h_{t}}=tanh\\Big{(}W_{c}\\big{[}h_{t};c_{t}\\big{]}\\Big{)}\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>h</mi><mi>t</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msub><mi>W</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><msub><mi>h</mi><mi>t</mi></msub><mo>;</mo><msub><mi>c</mi><mi>t</mi></msub><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00710.tex", "nexttext": "\n\n\n\n\\begin{figure}\n\\begin{tabular}{|l|r|r|r|} \\hline\n& French & English & German\\\\ \\hline\n\nWord tokens & 66.2m & 59.4m & 57.0m \\\\ \\hline\nWord types & 424,832 & 381,062 & 865,806 \\\\ \\hline\nSegment pairs & \\multicolumn{3}{c|}{2,378,112} \\\\ \\hline\nAve. segment & 27.8 & 25.0 & 24.0 \\\\ \nlength (tokens) & & & \\\\ \\hline\n\\end{tabular}\n\\caption{Trilingual corpus statistics.}\n\\label{corpus-stats}\n\\end{figure}\n\nIn our multi-source attention model we now have two $p_t$ variables, one for each source encoder. We also have two separate sets of alignments and therefore now have two $c_t$ values denoted by $c_t^1$ and $c_t^2$ as mentioned above. We also have distinct $W_a$, $v_p$, and $W_p$ parameters for each encoder.\n\n\\section{Experiments}\n\nWe use English, French, and German data from a subset of the WMT 2014 dataset \\cite{wmt2014}.  Figure~\\ref{corpus-stats} shows statistics for our training set. For development, we use the 3000 sentences supplied by WMT. For testing, we use a 1503-line trilingual subset of the WMT test set.\n\n\n\nFor the single-source models, we follow the training procedure used in \\newcite{luong2015effective}, but with 15 epochs and halving the learning rate every full epoch after the 10th epoch.  We also re-scale the normalized gradient when norm~$>$~5. For training, we use a minibatch size of 128, a hidden state size of 1000, and dropout as in \\newcite{dropout}. The dropout rate is 0.2, the initial parameter range is \\mbox{$[$-0.1,~+0.1$]$}, and the learning rate is 1.0. For the normal and multi-source attention models, we adjust these parameters to 0.3, \\mbox{$[$-0.08,~+0.08$]$}, and 0.7, respectively, to adjust for overfitting.\n\n\\begin{figure}\n\\begin{tabular}{|l|l|r|r|} \\hline\n\\multicolumn{4}{|c|}{Target = English} \\\\ \\hline\n{\\bf Source} & {\\bf Method} & {\\bf Ppl} & {\\bf BLEU} \\\\ \\hline\nFrench & --- & 10.3 & 21.0 \\\\ \\hline\nGerman & --- &  15.9 & 17.3 \\\\ \\hline\nFrench+German &  Basic & 8.7 & 23.2 \\\\ \\hline \\hline\nFrench+German &  Child-Sum & 9.0 & 22.5 \\\\ \\hline \nFrench+French & Child-Sum & 10.9 & 20.7 \\\\ \\hline \\hline\nFrench & Attention & 8.1 & 25.2 \\\\ \\hline\nFrench+German & B-Attent. & 5.7 & 30.0 \\\\ \\hline\nFrench+German & CS-Attent. & 6.0 & 29.6\\\\ \\hline\n\\end{tabular}\n\\caption{Multi-source MT for target English, with source languages French and German.  Ppl reports test-set perplexity  as the system predicts English tokens.  BLEU is scored using the multi-bleu.perl script from Moses. For our evaluation we use a single reference and they are case sensitive.}\n\\label{results-english}\n\\end{figure}\n\nFigure~\\ref{results-english} show our results for target English, with source languages being French and German).\nWe see that the Basic combination method yields a +4.8 Bleu improvement over the strongest single-source, attention-based system.  It also improves Bleu by +2.2 over the non-attention baseline.  The Child-Sum method gives improvements of +4.4 and +1.4. We also confirm that two copies of the same French input yields no BLEU improvement.  \n\nFigure~\\ref{align} shows the action of the multi-attention model during decoding.\n\nWhen our source languages are English and French (Figure~\\ref{results-german}), we observe smaller BLEU gains (up to +1.1).  This is evidence that the more distinct the source languages, the better they disambiguate each other.\n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=7.7cm]{alignment3.pdf}\n\\end{center}\n\\vspace{-0.2in}\n\\caption{Action of the multi-attention model as the neural decoder generates target English from French/German sources (test set).  Lines show strengths of $a_t(s)$.}\n\\label{align}\n\\end{figure}\n\n\\begin{figure}\n\\begin{tabular}{|l|l|r|r|} \\hline\n\\multicolumn{4}{|c|}{Target = German} \\\\ \\hline\n{\\bf Source} & {\\bf Method} & {\\bf Ppl} & {\\bf BLEU} \\\\ \\hline\nFrench & --- & 12.3 & 10.6 \\\\ \\hline\nEnglish & --- & 9.6 & 13.4 \\\\ \\hline \\hline\nFrench+English &  Basic & 9.1 & 14.5 \\\\ \\hline \nFrench+English &  Child-Sum & 9.5 & 14.4 \\\\ \\hline \\hline \n English & Attention & 7.3 & 17.6 \\\\ \\hline\n French+English & B-Attent. & 6.9 & 18.6 \\\\ \\hline\n French+English & CS-Attent. & 7.1 & 18.2\\\\ \\hline\n\\end{tabular}\n\\caption{Multi-source MT results for target German, with source languages French and English.}\n\\label{results-german}\n\\end{figure}\n\n\\section{Conclusion}\n\nWe describe a multi-source neural MT system that gets up to +4.8 Bleu gains over a very strong attention-based, single-source baseline.  We obtain this result through a novel encoder-vector combination method and a novel multi-attention system. We release the code for these experiments at \\url{https://github.com/isi-nlp/Zoph_RNN}.\n\n\n\n\n\n\n\\bibliographystyle{acl}\n\\bibliography{multisource}\n\n\n", "itemtype": "equation", "pos": 10546, "prevtext": "\n\n\n\n\nWe modify this attention model to look at both source encoders simultaneously. We create a context vector from each source encoder named $c_{t}^{1}$ and $c_{t}^{2}$ instead of the just $c_t$ in the single-source attention model:\n\n\n", "index": 35, "text": "\\begin{equation}\n\\tilde{h_{t}} = tanh \\Big( W_{c} \\big[ h_{t} ; c_{t}^{1} ; c_{t}^{2} \\big] \\Big)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\tilde{h_{t}}=tanh\\Big{(}W_{c}\\big{[}h_{t};c_{t}^{1};c_{t}^{2}\\big{]}\\Big{)}\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>h</mi><mi>t</mi></msub><mo stretchy=\"false\">~</mo></mover><mo>=</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msub><mi>W</mi><mi>c</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">[</mo><msub><mi>h</mi><mi>t</mi></msub><mo>;</mo><msubsup><mi>c</mi><mi>t</mi><mn>1</mn></msubsup><mo>;</mo><msubsup><mi>c</mi><mi>t</mi><mn>2</mn></msubsup><mo maxsize=\"120%\" minsize=\"120%\">]</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}]