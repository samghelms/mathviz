[{"file": "1601.07047.tex", "nexttext": "\n\n\n\\protect\\caption{Value Calculation}\n\n\n\\label{value-algorithm}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n$\\mathtt{factor}\\left(\\textrm{curve}\\;C^{i},\\mathtt{SLR}\\right):$\n\n\\quad{}$\\mathtt{t\\_sort=}\\left[\\mathtt{p}\\left[0\\right]\\mathtt{\\:for\\:p\\:in\\:}C^{i}\\right]$\n\n\\quad{}$\\mathtt{v\\_sort=}\\left[\\mathtt{p}\\left[1\\right]\\mathtt{\\:for\\:p\\:in\\:}C^{i}\\right]$\n\n\\quad{}$\\mathtt{low\\_index=}\\left|\\left[\\mathtt{t\\:for\\:t\\:in\\:t\\_sort\\:if\\:t}\\leq\\mathtt{SLR}\\right]\\right|$\n\n\\quad{}$\\mathtt{t\\_low=t\\_sort}\\left[\\mathtt{low\\_index}\\right]$\n\n\\quad{}$\\mathtt{t\\_high=t\\_sort}\\left[\\mathtt{low\\_index}+1\\right]$\n\n\\quad{}$\\mathtt{v\\_low=v\\_sort}\\left[\\mathtt{low\\_index}\\right]$\n\n\\quad{}$\\mathtt{v\\_high=v\\_sort}\\left[\\mathtt{low\\_index}+1\\right]$\n\n\\quad{}$F=\\mathtt{v\\_low}+\\left(\\frac{\\mathtt{SLR-t\\_low}}{\\mathtt{t\\_high-t\\_low}}\\times\\left(\\mathtt{v\\_high-v\\_low}\\right)\\right)$\n\n\\quad{}$\\mathtt{return}\\:F$\n\n\\protect\\caption{Value Factor Calculation\\label{value_factor_algorithm}}\n\\end{algorithm}\n\n\n\n\\section{Bidding Policies}\n\nIn a market, independent agents need to make bids for the services\nthey require. In this context of market-based scheduling, tasks will\neach bid for computational resource. In this market, it is necessary\nto have policies that calculate the bids for the tasks. These bids\ncan be based a number of underlying attributes of the task.\n\nBaseline policies are explicitly intended to be simplistic or naive,\nto show what is achievable with little processing work. Two baseline\npolicies are considered, \\emph{Random} and \\emph{First In First Out}\n(FIFO). Random places a random bid for each task in the queue at each\nround of bidding. FIFO places bids that are proportional to the arrival\ntime of each task's parent job $T^{i}\\in J^{k},\\;FIFO=J_{\\mathtt{arrive}}^{k}$,\nwith the smallest numerical bid being the highest priority.\n\nSeveral policies are considered that do not make use of the value\ncurves. These instead use the upward ranks of tasks $T_{R}^{i}$ within\nthe dependency graph of their parent job. \\emph{Shortest Remaining\nTime First} (SRTF) and \\emph{Longest Remaining Time First} (LRTF)\n\\cite{zhaosakellariou06,Topcuoglu2002} bid for tasks using their\nupward rank, with the smallest and largest bids being given priority,\nrespectively.\n\nThe \\emph{Projected Schedule Length Ratio} (P-SLR) policy bids the\nP-SLR value, with the highest value being highest priority \\cite{burkimsher12}.\nThis is so that the tasks that are most late relative to their execution\ntime are given the highest priority, which is intended to ensure that\nall jobs will have a waiting time proportional to the length of their\ncritical path, a desirable attribute for fairness and responsiveness\n\\cite{saule10}. The P-SLR as defined above is starvation-free as\nlong as overloads are transient (overall load is below 100\\%). However,\nto ensure P-SLR is starvation-free under extremely high load, a further\nterm is added to the equation. The equation as used for P-SLR bidding\nin the simulation is shown in Algorithm \\ref{Proj-slr-algo}. One\ndiscrete time unit is also added to the P-SLR calculation in order\nto preferentially prioritise smaller tasks that arrive at the same\ninstant as larger ones.\n\n\\begin{algorithm}\n$\\mathtt{projected\\_slr}(\\textrm{task}\\;T^{i},\\;\\textrm{job\\ }\\;J^{k},\\mathtt{curr\\_time},\\;\\textrm{queue}\\;Q)=$\n", "itemtype": "equation", "pos": 16736, "prevtext": "\n\n\\title{Bidding policies for market-based HPC workflow scheduling}\n\n\n\\author{Andrew Burkimsher, Leandro Soares Indrusiak\\\\\\{andrew.burkimsher, leandro.indrusiak\\}@york.ac.uk\\\\ Department of Computer Science, University of York, Heslington, York, YO10 5GH, UK}\n\\maketitle\n\\begin{abstract}\nThis paper considers the scheduling of jobs on distributed, heterogeneous\nHigh Performance Computing (HPC) clusters. Market-based approaches\nare known to be efficient for allocating limited resources to those\nthat are most prepared to pay. This context is applicable to an HPC\nor cloud computing scenario where the platform is overloaded. \n\nIn this paper, jobs are composed of dependent tasks. Each job has\na non-increasing time-value curve associated with it. Jobs are submitted\nto and scheduled by a market-clearing centralised auctioneer. This\npaper compares the performance of several policies for generating\ntask bids. The aim investigated here is to maximise the value for\nthe platform provider while minimising the number of jobs that do\nnot complete (or starve).\n\nIt is found that the Projected Value Remaining bidding policy gives\nthe highest level of value under a typical overload situation, and\ngives the lowest number of starved tasks across the space of utilisation\nexamined. It does this by attempting to capture the urgency of tasks\nin the queue. At high levels of overload, some alternative algorithms\nproduce slightly higher value, but at the cost of a hugely higher\nnumber of starved workflows. \\end{abstract}\n\n\\begin{IEEEkeywords}\nMarket-Based, Scheduling, Value, HPC, Value, Value-Curves, Value Remaining,\nWorkflows, Starvation, Responsiveness\n\\end{IEEEkeywords}\n\n\n\\section{Introduction}\n\nIn recent years, computational performance has increasingly been achieved\nthrough increasing parallelism rather than increased single-core performance.\nThis is mainly due to the exponential increase in power consumption\nrequired to run processors at higher clock speeds \\cite{intel04}.\nProcessors have become multicore, and multicore processors have been\ngrouped into clusters and networks of clusters, known as grids \\cite{kesselman98}.\nThese High Performance Computing (HPC) systems have grown to be huge\nin scale, especially those that support the operations of cloud computing\ncorporations.\n\nMany kinds of work run on HPC systems are not independent. Instead,\nworkflows (or \\emph{jobs}) are run that are made up of several individual\npieces of software (\\emph{tasks}), each of which takes some input\nand produces some output. This leads to the notion of \\emph{dependencies}\nbetween tasks, which restrict tasks to starting only once all their\ndependencies have completed and their input data transmitted.\n\nLarge-scale systems such as these can experience periods of high demand.\nThis can even extend into periods of overload, where work is arriving\nfaster than it can be executed. HPC providers need to be able to effectively\nprioritise work during these periods to ensure the most important\nwork is run. Cloud providers who sell computing capacity on the open\nmarket will wish to maximise their profits by running the work that\nis most valuable \\cite{cloudintro11}. There are real costs to HPC\nproviders for running work, so it may sometimes be more worthwhile\nto discard work whose value is too low than to run it.\n\nWork is run on computing clusters because it is valuable to the users\nand organisations who submit it. This value can be represented as\na real amount of money that users are willing to pay for their results.\nThis value is not always fixed. This is because work returned earlier\nmay allow users or organisations to be more productive, whereas some\nwork may have no value at all if it is returned too late \\cite{aburkimsherEngD14}.\n\nScheduling large-scale computing systems has traditionally been done\nthrough list scheduling \\cite{graham69,Maheswaran99}. This is where\nall the work in a queue is ranked by an appropriate metric, and allocated\nto hardware in sorted order. Where value is assigned to work, market-based\nprinciples can be applied to scheduling.\n\nThe usefulness of work on list scheduling can be employed in a market-based\nway. The values calculated for ranking in a list scheduler can instead\nbe used as bids that each piece of work submits to an auctioneer.\nPast research has investigated using value as bids \\cite{irwin04}.\nHowever, the bids can be different than just the values of jobs at\na given moment in time. It has been shown \\cite{aburkimsherEngD14}\nthat in the context of list scheduling, ordering tasks by total job\nvalue does not necessarily give the highest overall value under overload.\n\nThis paper explores the effectiveness of different bidding policies\nin a market-based scheduling scenario. This is to evaluate which provide\nthe highest value, especially in periods of overload when not all\nwork is able to run.\n\n\n\\section{Literature Survey}\n\nScheduling using market-based techniques has been studied over a long\nperiod, with pioneering early work by Sutherland in the 1960s \\cite{Sutherland68}.\nSince then, a great deal of work has been done on the structure of\nmarkets, investigating which structures best promote economic efficiency.\nAuction-based markets have been looked at by Waldspurger et al. \\cite{waldspurger02}\nwhereas market-clearing techniques where resources are allocated to\nwork until one or other is consumed were investigated by Miller and\nDrexler \\cite{millerdrexler88}. Lai et al. have examined doing the\nallocation of resources in proportion to the value of bids \\cite{Lai05admissons}.\nPopovici and Wilkes \\cite{popoviciwilkes05} investigated market-inspired\nadmission control. Dube \\cite{dube08} gives a good survey of the\nstate of this research and concludes that various different market\nstructures all hold promise, showing the benefits of better allocation\nand greater decentralisaion (and hence scalability and fault-tolerance)\nthan traditional schedulers.\n\nA necessary part of a market based system is that jobs have a certain\nvalue to users, and that this value is represented in the market by\ntheir bids \\cite{irwin04}. There has been much past work on scheduling\nto maximise the value of a workload \\cite{Lee99}. The problem of\nmaximising workload value where job value is a function of time was\nextensively researched by Locke \\cite{Locke1986} for systems schedulable\nby the Earliest Deadline First list scheduling policy, and has subsequently\nbeen applied to more general scheduling problems \\cite{aburkimsherEngD14,chen96}.\n\nIt seems that there has been little research into using value curves\nrather than fixed job values when scheduling in a market-based context.\nThis is likely because markets need a single price for each bid submitted\nby a job, not a curve. Yet in previous work \\cite{aburkimsherEngD14},\nit has been shown that value curves can be used to create single ranking\nvalues for list scheduling at given times in a simulation. This paper\nwill extend this approach by using these values as bids in a market-based\nframework.\n\n\n\\section{Models}\n\nTo evaluate which bidding policies are best, the simulator previously\ndeveloped by the authors \\cite{aburkimsherEngD14} was extended to\nuse a market-based scheduling system. This simulator implements several\nmodels representing the applications, the platform, the scheduling\nscheme and the means of representing and calculating value. The simulation\ntakes place in a discrete-time environment, where all events happen\non time ticks $\\tau\\in\\mathbb{N^{\\mathrm{0}}}$. \n\n\n\\subsection{Application Model}\n\nThe application model represents the work to be run on the cluster.\nTasks are non-pre-emptive, running to completion once they have begun.\nThis represents the behaviour of the LSF/GridEngine systems commonly\nused to manage large HPC systems. This lack of pre-emption is less\nof a problem at scale than it might seem, because in large-scale systems,\nthe turnover of work is sufficiently high. This means that it is very\nunlikely that any job will ever have to wait too long for something\nto finish so that it can start \\cite{burkimsher14}. \n\nFor the purposes of this paper, a single, non-preemptible piece of\nwork will be known as a \\emph{task}, denoted $T^{i}$. Each task will\nrun for a duration $T_{\\mathtt{exec}}^{i}\\in\\mathbb{N^{\\star}}$ on\na number of cores $T_{\\mathtt{cores}}^{i}\\in\\mathbb{N^{\\star}}$ concurrently.\nA set of tasks is known as a \\emph{job}, denoted $J^{k}$. Tasks can\nonly depend on other tasks within the same job and the successors\nof each task will be known as $T_{\\mathtt{succ}}^{i}$ . The structure\nof the dependencies will be that of a Directed Acyclic Graph (DAG),\nwhich defines a topological ordering over the tasks in a job: a partial\norder that they must be run in.\n\nHaving a dependency graph means several other useful metrics about\na job can be calculated. The upward rank \\cite{Topcuoglu2002} of\ntasks, $T_{\\mathtt{R}}^{i}$, is defined as the longest route from\nany task to its latest-finishing successor: $\\forall\\,T^{j}\\in T_{\\mathtt{succ}}^{i}:T_{\\mathtt{R}}^{i}=T_{\\mathtt{exec}}^{i}+\\max(T_{\\mathtt{R}}^{j})$.\nSorting tasks by their upward rank will give an ordering that respects\nthe partial order of the dependencies. The upward rank is also useful\nto estimate the finish time of a job if a task is run immediately.\nAs jobs only realise their value once they have finished, this estimate\nis useful for scheduling comparisons at runtime.\n\nThe critical path \\cite{kelley61} of a job, $J_{\\mathtt{CP}}^{k}$,\nis equivalent to the largest upward rank of any of its component tasks:\n$\\forall\\,T^{i}\\in J_{\\mathtt{}}^{k}:J_{\\mathtt{CP}}^{k}=\\max\\left(T_{R}^{i}\\right)$\n\\cite{Topcuoglu2002}. This is the length of time that the job would\ntake to execute if the number of cores requied was unbounded, which\nis equivalent to its minimum execution time. \n\n\n\\subsection{Platform Model}\n\nThe basic unit of the platform model is an execution core. Each cluster\nis made up of a number of cores. Multicore tasks must execute within\na single cluster, and consume a number of cores for the duration of\ntheir execution. Cores are not shared between executing tasks. Within\na cluster, cores are assumed to have negligible communication delays\nbetween them. For example, this could be due to all cores sharing\na single networked file system. In the HPC context considered, execution\ntimes are measured in hours to days, so the highest-speed communications\nwithin cores on the same multicore processor or within the same server\ncan reasonably be assumed to be negligible.\n\nClusters communicate through a central router. Delays are present\nfor data transfers between clusters, and are adjustable using the\nCommunication to Computation Ratio (CCR), supplied as an input to\nthe simulations. For a task executing for $T_{\\mathtt{exec}}^{i}$,\nthe time taken for data transfer would be $T_{\\mathtt{exec}}^{i}\\times CCR$.\n\nHeterogoneity is present in the model to a limited degree. Tasks and\ncluster cores have \\emph{Kinds}, which must match for a task to be\nable to run on a cluster. A task will always run for the same amount\nof time on clusters of the same kind. Each cluster is made up of cores\nof only one kind. This means that for jobs where tasks require different\nkinds, some network communications are unavoidable, as some tasks\nmust run on different clusters. Where this is the case, the network\ndelays are considered in the calculation of the critical path as well\nas just the execution times.\n\n\n\\subsection{Scheduling Model}\n\nThe scheduling model is market-based and works by using a central\nauctioneer running on the central router. All jobs are submitted to\nthe central router, which maintains the queue of work. Jobs are immediately\ndecomposed into their component tasks and, once their dependencies\nare satisfied, are added to the single global queue. This is different\nto previous work by the authors \\cite{aburkimsherEngD14}, where work\nis load-balanced on submission and then spends time queueing on the\nclusters.\n\nScheduling takes place at each scheduling instant, which can be the\narrival of a job or the completion of a task on any of the clusters.\nAt each instant, the tasks and the clusters submit bids to the auctioneer.\nThe tasks bid according to a bidding policy, and these bidding policies\nwill be described below. The clusters bid according to the number\nof cores they have available. The auctioneer then assigns the highest-bidding\ntask to the cluster with the most cores free until there are no tasks\nleft or there are no clusters with sufficient cores free to run the\nhighest-bidding task. This is an example of a market-clearing architecture.\n\nWhen a task is assigned to a cluster, it is run immediately, because\nthere are guaranteed to be sufficient cores free on a cluster to run\ndue to the bidding mechanism.\n\nBackfilling is not used where tasks other than the highest bidder\ncould fit onto a cluster even where the highest bidder cannot. This\nis for several reasons. Most importantly, when running large-scale\nclusters, there is a high turnover of work. This means that the delay\nuntil the next running task finishes is likely to be small \\cite{burkimsher14}.\nThis means there is less likely to be much of a penalty without doing\nbackfilling. Secondly, where execution times are only estimates, backfilling\ncannot be perfectly precise. This may mean that some previously lower-bidding\ntasks are still running when the previously high-bidding task would\nhave started. This can delay the execution of the high-bidding task,\npenalising the overall value achievable.\n\n\n\\subsection{Value Model}\n\nUsers submit work to HPC systems because running the work on their\nown computers is likely to take a prohibitively long time. Furthermore,\nusers tend to not really care how busy the systems are overall, but\ninstead care most about the responsiveness of their jobs. Therefore,\na model of value curves is required that captures these perceptions\nabout responsiveness. It is assumed that users will define these curves\nand submit them along with their jobs. This is because the decision\nand allocation of appropriate time and value is context-dependent\nand is fundamentally a stakeholder issue.\n\nThe authors previously concluded \\cite{burkimsher12} that the most\nappropriate measure of responsiveness for jobs with dependencies was\nTopcuoglu's Schedule Length Ratio or \\emph{SLR} \\cite{Topcuoglu2002}.\nThis is the ratio of the job's actual response time to the length\nof its critical path: $SLR=\\left(J_{\\mathtt{finish}}^{k}-J_{\\mathtt{arrive}}^{k}\\right)\\div J_{\\mathtt{CP}}^{k}$.\nA particularly useful feature of the SLR is that job SLR can be estimated\n(or projected, hence P-SLR) for tasks in advance if the submission\ntime of the job, the current time and the upward rank of the task\nis known: $T^{i}\\in J^{k}:\\;PSLR\\left(T^{i}\\right)=\\left(\\left(T_{R}^{i}+\\tau_{\\mathtt{current}}\\right)-J_{\\mathtt{arrive}}^{k}\\right)\\div J_{\\mathtt{CP}}^{k}$.\n\nEach job submitted will also be submitted with a value curve, $J_{C^{i}}^{K}$.\nThe curve $C^{i}$ is an array of coordinates of the points that define\nthe curve. The time-axis of the value curve is defined using the SLR\nof the job. This means that the initial and final deadlines along\nwith the time coordinates of points on the curve are defined in terms\nof SLR. This is so that the value curve can easily be scaled to jobs\nof different sizes, total values or lengths of critical path. This\nis also designed so that the future value of a task can be estimated\nusing its P-SLR. The value curve is undefined before an SLR of 1,\nas no job can finish before the length of its critical path. \n\nFigure \\ref{value-curve-template-pic} shows the template used to\ndefine value curves. Every job is assigned a maximum value $V_{\\mathtt{max}}$\nthat it can return to the user. A value curve is defined as a piecewise\nfunction with three subdomains, punctuated by an initial ($D_{\\mathtt{initial}}$)\nand a final ($D_{\\mathtt{final}}$) deadline, as defined in Algorithm\n\\ref{value-algorithm}. Before the initial deadline, the value returned\nis always the maximum $V_{\\mathtt{max}}$. Between the initial and\nfinal deadlines, the value is calculated using linear interpolation\nbetween a sequence of points which reach 0 at $D_{\\mathtt{final}}$.\nOnce the final deadline has been reached, the value is 0. The algorithm\nfor calculating this factor is given in Algorithm \\ref{value_factor_algorithm}.\n\n\\begin{figure}\n\\noindent \\begin{centering}\n\\includegraphics[bb=0bp 0bp 523bp 282bp,clip,width=1\\columnwidth]{valuediagramcropped}\n\\par\\end{centering}\n\n\\protect\\caption{Value Curve Template}\n\n\n\\label{value-curve-template-pic}\n\\end{figure}\n\n\n\\begin{algorithm}\n$\\mathtt{Value}\\left(\\textrm{job}\\;J^{k},\\mathtt{SLR}\\right)=$\n\n", "index": 1, "text": "\n\\[\n\\begin{cases}\nJ_{\\mathtt{VMax}}^{k} & \\mathtt{if\\;SLR}\\leq D_{\\mathtt{initial}}\\\\\n0 & \\mathtt{if\\;SLR}\\geq D_{\\mathtt{final}}\\\\\nJ_{\\mathtt{VMax}}^{k}\\times\\mathtt{factor}\\left(J_{C^{i}}^{k},\\:\\mathtt{SLR}\\right) & \\mathtt{if}\\;D_{\\mathtt{initial}}<\\mathtt{SLR}<D_{\\mathtt{final}}\n\\end{cases}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\begin{cases}J_{\\mathtt{VMax}}^{k}&amp;\\mathtt{if\\;SLR}\\leq D_{\\mathtt{initial}}\\\\&#10;0&amp;\\mathtt{if\\;SLR}\\geq D_{\\mathtt{final}}\\\\&#10;J_{\\mathtt{VMax}}^{k}\\times\\mathtt{factor}\\left(J_{C^{i}}^{k},\\&gt;\\mathtt{SLR}%&#10;\\right)&amp;\\mathtt{if}\\;D_{\\mathtt{initial}}&lt;\\mathtt{SLR}&lt;D_{\\mathtt{final}}\\end{cases}\" display=\"block\"><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><msubsup><mi>J</mi><mi>\ud835\ude85\ud835\ude7c\ud835\ude8a\ud835\udea1</mi><mi>k</mi></msubsup></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+2.8pt\"><mi>\ud835\ude92\ud835\ude8f</mi></mpadded><mo>\u2062</mo><mi>\ud835\ude82\ud835\ude7b\ud835\ude81</mi></mrow><mo>\u2264</mo><msub><mi>D</mi><mi>\ud835\ude92\ud835\ude97\ud835\ude92\ud835\ude9d\ud835\ude92\ud835\ude8a\ud835\ude95</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+2.8pt\"><mi>\ud835\ude92\ud835\ude8f</mi></mpadded><mo>\u2062</mo><mi>\ud835\ude82\ud835\ude7b\ud835\ude81</mi></mrow><mo>\u2265</mo><msub><mi>D</mi><mi>\ud835\ude8f\ud835\ude92\ud835\ude97\ud835\ude8a\ud835\ude95</mi></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msubsup><mi>J</mi><mi>\ud835\ude85\ud835\ude7c\ud835\ude8a\ud835\udea1</mi><mi>k</mi></msubsup><mo>\u00d7</mo><mi>\ud835\ude8f\ud835\ude8a\ud835\ude8c\ud835\ude9d\ud835\ude98\ud835\ude9b</mi></mrow><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>J</mi><msup><mi>C</mi><mi>i</mi></msup><mi>k</mi></msubsup><mo rspace=\"4.7pt\">,</mo><mi>\ud835\ude82\ud835\ude7b\ud835\ude81</mi><mo>)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mpadded width=\"+2.8pt\"><mi>\ud835\ude92\ud835\ude8f</mi></mpadded><mo>\u2062</mo><msub><mi>D</mi><mi>\ud835\ude92\ud835\ude97\ud835\ude92\ud835\ude9d\ud835\ude92\ud835\ude8a\ud835\ude95</mi></msub></mrow><mo>&lt;</mo><mi>\ud835\ude82\ud835\ude7b\ud835\ude81</mi><mo>&lt;</mo><msub><mi>D</mi><mi>\ud835\ude8f\ud835\ude92\ud835\ude97\ud835\ude8a\ud835\ude95</mi></msub></mrow></mtd></mtr></mtable></mrow></math>", "type": "latex"}, {"file": "1601.07047.tex", "nexttext": "\n\n\n\\protect\\caption{\\selectlanguage{english}\nProjected SLR algorithm\\selectlanguage{british}\n}\n\\label{Proj-slr-algo}\n\\end{algorithm}\n\n\nSeveral policies based on value are considered. \\emph{Projected Value}\n(PV) uses the estimated P-SLR to determine the parent job's value\nif it were to finish after the task's upward rank. PV then uses the\nhighest bids as the most preferable. This is similar to previous market-based\npolicies where work has fixed values rather than value curves $T^{i}\\in J^{k}:\\;PV\\left(T^{i}\\right)=\\mathtt{Value}\\left(J^{k},\\;PSLR\\left(T^{i}\\right)\\right)$.\n\nThe issue with PV is that tasks that promise a large amount a value\nmay also take a large amount of resource. Instead, the \\emph{Projected\nValue Density} (PVD) policy \\cite{Locke1986} looks at how profitable\nrunning each task may be, by comparing the value gained with the resource\nrequired to achieve this value. The resource required, $T_{\\mathtt{succ\\_sum}}^{i}$,\nis the sum of the execution times, $T_{\\mathtt{exec}}^{i}$, multiplied\nby the cores they require, $T_{\\mathtt{cores}}^{i}$, of all the tasks\nthat depend on the considered task $\\forall\\,T^{j}\\in T_{\\mathtt{succ}}^{i}:T_{\\mathtt{succ\\_sum}}^{i}=\\left(T_{\\mathtt{exec}}^{i}\\times T_{\\mathtt{cores}}^{i}\\right)+\\sum T_{\\mathtt{succ\\_sum}}^{j}$.\nHaving defined the resource required, we can define the value density\nas the value divided by the resource required $T^{i}\\in J^{k}:\\;PVD\\left(T^{i}\\right)=\\mathtt{Value}\\left(J^{k},\\;PSLR\\left(T^{i}\\right)\\right)\\div T_{\\mathtt{succ\\_sum}}^{i}$\n. Higher density is prioritised as it will be most profitable.\n\nAldarmi and Burns \\cite{aldarmi1999} suggested that squaring the\nvalue density gives a better separation between tasks that will be\nprofitable and those that should be starved to avoid wasting resources.\nThis is termed the \\emph{Projected Value Density SQuared} (PVDSQ)\npolicy, $PVDSQ\\left(T^{i}\\right)=\\left(PVD\\left(T^{i}\\right)\\right)^{2}$.\n\nThe Projected Value Remaining (PVR) policy \\cite{aburkimsherEngD14}\nis designed to capture the relative urgency of a task at a given moment.\nThe other policies may project tasks to be valuable or not, but are\nunable to give an indication of whether that value is likely to decrease\nwith waiting much longer. PVR uses the P-SLR to determine the earliest\npossible time a task could finish if it were run immediately. The\nPVR is then the area under the value curve remaining between the P-SLR\nat the current time and the final deadline $D_{\\mathtt{final}}$ of\nthe job. This is illustrated graphically in Figure \\ref{fig:pvr-diag}.\n\n\\begin{figure}\n\\noindent \\begin{centering}\n\\includegraphics[bb=0bp 0bp 525bp 220bp,width=1\\columnwidth]{valuediagram2pdfcropped}\n\\par\\end{centering}\n\n\\protect\\caption{Value Curve showing Projected Value Remaining }\n\n\n\\label{fig:pvr-diag}\n\\end{figure}\n\n\nThe tasks with the smallest value remaining are run first. Urgent\ntasks would have a steeply sloping value curve, which would give only\na small area under the curve. Tasks about to time out would also have\nonly a small area remaining. Prioritising these tasks would reduce\nstarvation and loss of value.\n\nThe value curves were designed using linear interpolation between\nthe points so that the definite integral of this curve would be quickly\nand exactly calculable using the trapezoidal method \\cite{alvarado79}.\nHowever, the policy method generalises to any value curves where value\ncan only decrease over time, as long as a final deadline is present.\nThe algorithm to calculate PVR is given in Algorithm \\ref{projected-value-remaining-algo}.\n\n\\begin{algorithm}\n$\\mathtt{PVR}\\left(T^{i},\\:\\mathtt{\\tau_{current}}\\right):$\n\n\\begin{eqnarray*}\nJ^{k} & = & T_{\\mathtt{parent\\_job}}^{i}\\\\\nD_{\\mathtt{final}} & = & J_{C_{D_{\\mathtt{final}}}^{i}}^{k}\\\\\n\\mathtt{P\\_SLR} & = & \\frac{\\left(T_{\\mathtt{R}}^{i}+\\tau_{\\mathtt{current}}\\right)-J_{\\mathtt{arrive}}^{k}}{J_{\\mathtt{CP}}^{k}}\\\\\nPVR & = & \\int_{\\mathtt{P\\_SLR}}^{D_{\\mathtt{final}}}\\mathtt{Value}\\left(J^{k},s\\right)\\:ds\\\\\n\\mathtt{return} & PVR\n\\end{eqnarray*}\n\n\n\\protect\\caption{Algorithm to compute Projected Value Remaining }\n\n\n\\label{projected-value-remaining-algo}\n\\end{algorithm}\n\n\n\n\\section{Experimental Method}\n\nIn order to be able to compare different schedules, appropriate metrics\nare required. For this work, the pertinent metric is that of the proportion\nof maximum value achieved. The maximum value of a workload is the\nsum of the maximum value of all possible jobs, and is the value that\nwould be achievable if the number of processors available was unbounded,\nthere was no contention between work and no network delays. Because\ndifferent workloads may have different maximum values, it is necessary\nto normalise these values between workloads. In this work, the value\nachieved under a given set of circumstances is divided by the maximum\npossible value to give a normalised figure.\n\nUsers are likely to not only care about their work, but also care\nwhether it was completed or not. Therefore the number of jobs starved\nin a given set of circumstances will also be examined. A job is considered\nto have starved (and is removed from the queue) once it has passed\nits final deadline $D_{\\mathtt{final}}$ as defined by its value curve.\n\nThe bidding policies are evaluated in simulation, using a synthetic\nworkload running over a synthetic platform. The simulations take place\nwithin a discrete-time environment, implemented using the SimPy library\n\\cite{simpy} in the Python language. The platform consists of 4000\ncores total, organised into clusters of 1000 cores each. Three clusters\nare of architecture Kind1, and one of Kind2. The clusters are all\nconnected directly to the central router, where the auction is run.\nNetwork delays between the clusters is accounted for by using a Communication\nto Computation ratio of 0.2.\n\nTen synthetic workloads were used for evaluation. To ensure sufficient\nscale was present, each workload was made up of 10,000 jobs with 5-20\ntasks per job. This gives approximately 100,000 tasks in each workload.\nThe execution time distribution for jobs and tasks followed a log-uniform\ndistribution. 80\\% of the tasks in the workload were Kind1, with 20\\%\nbeing of Kind2. The dependencies between jobs followed an exponential\ndistribution in node degree. These synthetic workloads were created\nusing the methods published by Burkimsher et al. \\cite{burkimsher14}.\nValue curves were also generated synthetically, and had $D_{\\mathtt{initial}}$\nvalues randomly selected between 2 and 4, $D_{\\mathtt{final}}$ values\nbetween 6 and 10, and 5-10 random, non-increasing points in between.\n\nIn simulation, real allocations of value are unavailable. Therefore,\nit was assumed that the value of jobs was proportional to the number\nof core-minutes their execution would consume. That is, that their\nvalue density at $V_{\\mathtt{max}}$ is identical. In periods of overload,\nhowever, jobs may have very different value densities because the\nvalue curves specified will lead to them having very different relative\nurgency.\n\nLoad can be varied for the same workload by adjusting the inter-arrival\ntimes of jobs, according to the method in Burkimsher et al. \\cite{burkimsher14}.\nThe arrival rates are also adjusted to give peaks during working hours\nand quieter periods overnight and at the weekend. This ensures that\nmany cycles of overload and catching-up are present, a more challenging\nscenario for a scheduler than a constant arrival rate of work and\none that is more representative of real systems. Using this method,\nload is varied between 70\\% and 140\\% of saturation, the state where\nthe system must operate at full capacity to service the work arriving.\nLoad above 100\\% represents overload, where not all the work can be\nrun. With the daily and weekly cycles of load, transient overloads\nwill be present even below 100\\% total load.\n\n\n\\section{Results and Discussion}\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[clip,width=1\\columnwidth]{Fig1}\n\\par\\end{centering}\n\n\\protect\\caption{Value against load for different bidding policies\\label{fig:Value-against-load}}\n\n\n\\end{figure}\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[clip,width=1\\columnwidth]{Fig2}\n\\par\\end{centering}\n\n\\protect\\caption{Value against load for different bidding policies (Detail)\\label{fig:Value-against-load-zoom}}\n\\end{figure}\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[clip,width=1\\columnwidth]{Fig3}\n\\par\\end{centering}\n\n\\protect\\caption{Starvation against load for different bidding policies\\label{fig:Starvation-against-load}}\n\\end{figure}\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[clip,width=1\\columnwidth]{Fig4}\n\\par\\end{centering}\n\n\\protect\\caption{Starvation against load for different bidding policies (Detail)\\label{fig:Starvation-against-load-zoom}}\n\\end{figure}\n\n\n\\begin{figure}\n\\begin{centering}\n\\includegraphics[clip,width=1\\columnwidth]{Fig5}\n\\par\\end{centering}\n\n\\protect\\caption{Responsiveness (SLR) across the spectrum of job execution times at\n120\\% load\\label{fig:Responsiveness-decile}}\n\\end{figure}\n\n\nThe experiments were set up such that there will always be transient\noverloads, so that not all work is able to run. This is why the proportion\nof maximum value is below 1.0 even at 70\\% load. As load is increased,\nthe proportion of value decreases for all bidding policies. This is\nbound to happen because the more work there is to do in a given time,\nthe more there will be that is not done. The challenge is to find\nthe policy that suffers the least loss of value given the increase\nin load, with the results shown in Figure \\ref{fig:Value-against-load}.\n\nLRTF is a policy commonly used in static scheduling as it is good\nat bin-packing, and hence at reducing workload makespan \\cite{Topcuoglu2002}.\nHowever, it is clear that it is a poor choice for a dynamic scheduling\nscenario, because it will tend to run the most recently arrived jobs\nfirst. This is because the jobs most recently arrived will have lots\nof work left to do, and so be prioritised over those that are soon\nto finish. In an overload situation, jobs will be started continuously\nbut never be able to finish. Its value achieved in this dynamic case\nis even worse than the baseline random policy.\n\nThe FIFO policy is a fair improvement on the random policy, but FIFO\nis still a long way behind the other policies under overload. This\nis natural because it does not consider the size or the relative urgency\nof jobs.\n\nAt load levels below overload, the PV policy performs surprisingly\npoorly, even worse than the random policy. PV goes for the largest\nand most valuable tasks first, meaning that many smaller ones will\nstarve. This affects the total value achievable, and it also leads\nto the most tasks starving across the spectrum of load, with only\nLRTF starving more. At very high levels of load, PV does not fall\nas fast as some other policies, though it is still beaten by the flavours\nof Value Density, SRTF and PVR.\n\nP-SLR is designed to ensure that the SLR of jobs across the execution\ntime spectrum is equal (fairness with respect to responsiveness) \\cite{burkimsher12}.\nUnder high levels of load, however, this can mean that many jobs may\nhave their final deadline before the SLR that is currently being achieved,\nmeaning that many will starve (Figure \\ref{fig:Starvation-against-load}).\nP-SLR is unable to take into account the fact that jobs may have varying\nlevels of urgency.\n\nThe two flavours of value density bidding (PVD and PVDSQ) are clearly\nbetter than simply using value-based bidding. Locke \\cite{Locke1986}\nshowed that for uniprocessor scheduling of workloads that are also\nschedulable using EDF, value density based scheduling was optimal.\nNaturally, workloads that overload their platform are not schedulable\nusing EDF, so these scenarios are outside those proved optimal. This\nevaluation shows that in the overload scenarios considered, other\npolicies can do better than PVD.\n\nInterestingly, PVDSQ gives higher value than PVD (Figure \\ref{fig:Value-against-load}).\nThis is because PVDSQ starves many fewer tasks than PVD (Figure \\ref{fig:Starvation-against-load}).\nBy squaring the value density, jobs that are not worth running are\nmore cleanly starved, leaving the rest to be able to run to completion.\nPVDSQ also starves a few more of the largest jobs, which gives a lot\nmore space for other jobs to run. Interestingly, PVDSQ and SRTF give\nalmost indistinguishable levels of value across the load spectrum.\nHowever, SRTF starves many fewer tasks overall because SRTF prioritises\nsmall tasks with little time remaining.\n\nEDF attains the second-highest level of value below 110\\% load (Figure\n\\ref{fig:Value-against-load-zoom}). However, its performance falls\nsteeply after that. This is because once the earliest deadline that\nis being run gets into the past, every job starts missing its deadline\nand lateness starts cascading. Above 110\\% load, the value attained\nby EDF falls rapidly behind the alternative policies evaluated. After\n120\\% load, the number of tasks starved by EDF also increases rapidly\n(Figure \\ref{fig:Starvation-against-load-zoom}).\n\nBelow saturation and up to 110\\% load, PVR attains the highest value\nof all the policies evaluated. This is due to its ability to take\ninto account the urgency of tasks. Small, urgent tasks are able to\nrun, which ensures their value is captured before they starve. PVR\nactually has the highest responsiveness (lowest SLR) of all the policies\nevaluated for smaller jobs (Figure \\ref{fig:Responsiveness-decile}).\nIt does this by only letting the largest jobs have their SLR increase.\nHowever, the largest jobs can often also be those that are most tolerant\nto waiting time: if a job takes months to run, it may be able to take\na few more days and users will not mind. This means that the value\ncurves are likely to decrease relatively gradually.\n\nAt very high levels of load, PVR's lead is eroded and it falls behind\nSRTF and PVDSQ and is equalled by PVD at the highest level of load\nevaluated. When there is too much work to run, PVR tends to keep most\njobs running by starving the largest ones. Yet the largest jobs can\nalso be the ones that supply the most value. As can be seen from Figure\n\\ref{fig:Responsiveness-decile}, SRTF penalises the largest jobs\na little less, meaning that it keeps more value overall under extreme\noverload. Saying that, PVR has the lowest number of starved jobs across\nthe spectrum of load (Figure \\ref{fig:Starvation-against-load-zoom}).\nPVR manages to ensure that only 5\\% of jobs are incomplete by their\nfinal deadline, even at 140\\% load. This low level of starvation is\nlikely to be attractive to grid operators because it will keep user\nsatisfaction high, as no user wishes to have their job starve.\n\n\n\\section{Conclusion}\n\nIn this paper, a number of bidding policies for market-based scheduling\nin a distributed grid computing scenario were examined. The Projected\nValue Remaining (PVR) policy was shown to achieve the highest levels\nof value as long as overload was not excessive. Even under extreme\noverload, the relatively low levels of starved jobs are likely to\nbe more acceptable to users than the slight improvement in overall\nvalue offered by the PVDSQ policy. PVR is also likely to be preferable\nto SRTF under extreme overload, because most jobs still have high\nresponsiveness, and it is the largest jobs, for which responsiveness\nis usually less critical anyway because their execution times are\nso long, that must wait longer. PVR's ability to capture urgency in\nits bidding metric is what enables it to give these results.\n\nA further notable conclusion is that the differences between the leading\npolicies are really quite close in terms of the value achieved, even\nwhen the number of jobs they starve is wildly different. This evaluation\nshows that different policies achieve similar levels of value using\nquite different behaviour. Some policies run just the largest jobs\nand starve everything else (PV), others the smallest (SRTF), while\nothers try to achieve balance across the space of execution times\n(EDF, PVDSQ, PVR).\n\nThis work considers the scenario where all work is admitted but may\nbe left in the queue to starve. This may be less useful to users than\na system with an admission controller. Users could then know up-front\nwhether their jobs were accepted, and find out sooner if their jobs\nwere not going to be run. A natural direction for future work would\nbe to see whether integrating the current approaches with the addition\nof an admission controller could achieve similar levels of value.\n\n\\smallskip{}\n\n\nThe research described in this paper is partially funded by the EU\nFP7 DreamCloud Project (611411).\n\n\\bibliographystyle{plain}\n\\bibliography{biblob3}\n\n\n", "itemtype": "equation", "pos": 20358, "prevtext": "\n\n\n\\protect\\caption{Value Calculation}\n\n\n\\label{value-algorithm}\n\\end{algorithm}\n\n\n\\begin{algorithm}\n$\\mathtt{factor}\\left(\\textrm{curve}\\;C^{i},\\mathtt{SLR}\\right):$\n\n\\quad{}$\\mathtt{t\\_sort=}\\left[\\mathtt{p}\\left[0\\right]\\mathtt{\\:for\\:p\\:in\\:}C^{i}\\right]$\n\n\\quad{}$\\mathtt{v\\_sort=}\\left[\\mathtt{p}\\left[1\\right]\\mathtt{\\:for\\:p\\:in\\:}C^{i}\\right]$\n\n\\quad{}$\\mathtt{low\\_index=}\\left|\\left[\\mathtt{t\\:for\\:t\\:in\\:t\\_sort\\:if\\:t}\\leq\\mathtt{SLR}\\right]\\right|$\n\n\\quad{}$\\mathtt{t\\_low=t\\_sort}\\left[\\mathtt{low\\_index}\\right]$\n\n\\quad{}$\\mathtt{t\\_high=t\\_sort}\\left[\\mathtt{low\\_index}+1\\right]$\n\n\\quad{}$\\mathtt{v\\_low=v\\_sort}\\left[\\mathtt{low\\_index}\\right]$\n\n\\quad{}$\\mathtt{v\\_high=v\\_sort}\\left[\\mathtt{low\\_index}+1\\right]$\n\n\\quad{}$F=\\mathtt{v\\_low}+\\left(\\frac{\\mathtt{SLR-t\\_low}}{\\mathtt{t\\_high-t\\_low}}\\times\\left(\\mathtt{v\\_high-v\\_low}\\right)\\right)$\n\n\\quad{}$\\mathtt{return}\\:F$\n\n\\protect\\caption{Value Factor Calculation\\label{value_factor_algorithm}}\n\\end{algorithm}\n\n\n\n\\section{Bidding Policies}\n\nIn a market, independent agents need to make bids for the services\nthey require. In this context of market-based scheduling, tasks will\neach bid for computational resource. In this market, it is necessary\nto have policies that calculate the bids for the tasks. These bids\ncan be based a number of underlying attributes of the task.\n\nBaseline policies are explicitly intended to be simplistic or naive,\nto show what is achievable with little processing work. Two baseline\npolicies are considered, \\emph{Random} and \\emph{First In First Out}\n(FIFO). Random places a random bid for each task in the queue at each\nround of bidding. FIFO places bids that are proportional to the arrival\ntime of each task's parent job $T^{i}\\in J^{k},\\;FIFO=J_{\\mathtt{arrive}}^{k}$,\nwith the smallest numerical bid being the highest priority.\n\nSeveral policies are considered that do not make use of the value\ncurves. These instead use the upward ranks of tasks $T_{R}^{i}$ within\nthe dependency graph of their parent job. \\emph{Shortest Remaining\nTime First} (SRTF) and \\emph{Longest Remaining Time First} (LRTF)\n\\cite{zhaosakellariou06,Topcuoglu2002} bid for tasks using their\nupward rank, with the smallest and largest bids being given priority,\nrespectively.\n\nThe \\emph{Projected Schedule Length Ratio} (P-SLR) policy bids the\nP-SLR value, with the highest value being highest priority \\cite{burkimsher12}.\nThis is so that the tasks that are most late relative to their execution\ntime are given the highest priority, which is intended to ensure that\nall jobs will have a waiting time proportional to the length of their\ncritical path, a desirable attribute for fairness and responsiveness\n\\cite{saule10}. The P-SLR as defined above is starvation-free as\nlong as overloads are transient (overall load is below 100\\%). However,\nto ensure P-SLR is starvation-free under extremely high load, a further\nterm is added to the equation. The equation as used for P-SLR bidding\nin the simulation is shown in Algorithm \\ref{Proj-slr-algo}. One\ndiscrete time unit is also added to the P-SLR calculation in order\nto preferentially prioritise smaller tasks that arrive at the same\ninstant as larger ones.\n\n\\begin{algorithm}\n$\\mathtt{projected\\_slr}(\\textrm{task}\\;T^{i},\\;\\textrm{job\\ }\\;J^{k},\\mathtt{curr\\_time},\\;\\textrm{queue}\\;Q)=$\n", "index": 3, "text": "\n\\[\n\\frac{\\left(T_{R}^{i}+\\mathtt{curr\\_time}+1\\right)-J_{\\mathtt{arrive}}^{k}}{J_{\\mathtt{CP}}^{k}}+\\left\\lfloor \\frac{\\mathtt{curr\\_time}-J_{\\mathtt{arrive}}^{k}}{\\forall J^{n}\\in Q:\\max\\left(J_{\\mathtt{CP}}^{n}\\right)}\\right\\rfloor ^{2}\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\left(T_{R}^{i}+\\mathtt{curr\\_time}+1\\right)-J_{\\mathtt{arrive}}^{k}}{J_%&#10;{\\mathtt{CP}}^{k}}+\\left\\lfloor\\frac{\\mathtt{curr\\_time}-J_{\\mathtt{arrive}}^{%&#10;k}}{\\forall J^{n}\\in Q:\\max\\left(J_{\\mathtt{CP}}^{n}\\right)}\\right\\rfloor^{2}\" display=\"block\"><mrow><mfrac><mrow><mrow><mo>(</mo><mrow><msubsup><mi>T</mi><mi>R</mi><mi>i</mi></msubsup><mo>+</mo><mrow><mi>\ud835\ude8c\ud835\ude9e\ud835\ude9b\ud835\ude9b</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>\ud835\ude9d\ud835\ude92\ud835\ude96\ud835\ude8e</mi></mrow><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo>-</mo><msubsup><mi>J</mi><mi>\ud835\ude8a\ud835\ude9b\ud835\ude9b\ud835\ude92\ud835\ude9f\ud835\ude8e</mi><mi>k</mi></msubsup></mrow><msubsup><mi>J</mi><mi>\ud835\ude72\ud835\ude7f</mi><mi>k</mi></msubsup></mfrac><mo>+</mo><msup><mrow><mo>\u230a</mo><mfrac><mrow><mrow><mi>\ud835\ude8c\ud835\ude9e\ud835\ude9b\ud835\ude9b</mi><mo>\u2062</mo><mi mathvariant=\"normal\">_</mi><mo>\u2062</mo><mi>\ud835\ude9d\ud835\ude92\ud835\ude96\ud835\ude8e</mi></mrow><mo>-</mo><msubsup><mi>J</mi><mi>\ud835\ude8a\ud835\ude9b\ud835\ude9b\ud835\ude92\ud835\ude9f\ud835\ude8e</mi><mi>k</mi></msubsup></mrow><mrow><mrow><mrow><mo>\u2200</mo><msup><mi>J</mi><mi>n</mi></msup></mrow><mo>\u2208</mo><mi>Q</mi></mrow><mo>:</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>(</mo><msubsup><mi>J</mi><mi>\ud835\ude72\ud835\ude7f</mi><mi>n</mi></msubsup><mo>)</mo></mrow></mrow></mrow></mfrac><mo>\u230b</mo></mrow><mn>2</mn></msup></mrow></math>", "type": "latex"}]