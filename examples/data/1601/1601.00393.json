[{"file": "1601.00393.tex", "nexttext": "\nProblem 1 can be exactly solved in polynomial time \\cite{orlin2009faster}, while Problem 2 is NP-hard since some of its special cases (e.g., Max Cut) are NP-hard.\n\nFor convenience of the presentation, we will use P1 and P2 to refer to Problem 1 and Problem 2, respectively. Similarly, for the following algorithms, we will use A1 and A2 to refer to Algorithm 1 and Algorithm 2. The reference holds for P3, P4, A3 and A4.\n\nDefine $\\mathcal{X}_{min} \\triangleq \\{ X_* \\subseteq N \\ |\\ f(X_*) \\le f(X), \\forall X \\subseteq N \\}$ as the optima set of P1. Similarly, define $\\mathcal{X}_{max} \\triangleq \\{ X^* \\subseteq N \\ |\\ f(X^*) \\ge f(X), \\forall X \\subseteq N \\}$. Obviously, we have $\\mathcal{X}_{min} \\subseteq [\\emptyset,N]$ and $\\mathcal{X}_{max} \\subseteq [\\emptyset,N]$.\n\nWe now give the definition of reducibility. For P1 (P2), we say the objective function $f$ is \\emph{reducible for minimization (maximization)} if $\\exists \\ [S,T] \\subset [\\emptyset,N]$, where $[S,T]$ can be obtained in $\\mathcal{O}(n^p)$ function evaluations, such that $\\mathcal{X}_{min} \\subseteq [S,T]$ ($\\mathcal{X}_{max} \\subseteq [S,T]$). Note that if we can only find $[S,T]$ in $\\mathcal{O}(2^n)$ time, the reduction is meaningless since $\\mathcal{O}(2^n)$ time is enough for us to find all the optima. The ratio $1- \\frac{|T \\setminus S|}{|N|} \\in (0,1]$ is called the \\emph{reduction rate}.\n\n\\subsection{Algorithms}\n\nExisting works on reduction for unconstrained submodular optimization can be summarized by the following two algorithms, both of which terminate in $\\mathcal{O}(n^2)$ time. The brief review of existing works can be found in Section 6.\n\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Reduction for Minimization}\n\\label{algo1}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $X_0 \\leftarrow \\emptyset$, $Y_0 \\leftarrow N$, $t \\leftarrow 0$.\n\\Ensure\n$[X_t,Y_t]$.\n\\State Find $U_t = \\{i \\in Y_t \\setminus X_t \\ |\\  f(i|X_t) < 0\\}$.\n\\State $X_{t+1} \\leftarrow X_t \\cup U_t$.\n\\State Find $D_t = \\{j \\in Y_t \\setminus X_t \\ |\\ f(j|Y_t - j) > 0\\}$.\n\\State $Y_{t+1} \\leftarrow Y_t \\setminus D_t$.\n\\State If $X_{t+1} = X_t$ and $Y_{t+1} = Y_t$, terminate.\n\\State $t \\leftarrow t+1$. Go to Step $1$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Reduction for Maximization}\n\\label{algo2}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $X_0 \\leftarrow \\emptyset$, $Y_0 \\leftarrow N$, $t \\leftarrow 0$.\n\\Ensure\n$[X_t,Y_t]$.\n\\State Find $U_t = \\{i \\in Y_t \\setminus X_t \\ |\\  f(i|X_t) < 0\\}$.\n\\State $Y_{t+1} \\leftarrow Y_t \\setminus U_t$.\n\\State Find $D_t = \\{j \\in Y_t \\setminus X_t \\ |\\ f(j|Y_t - j) > 0\\}$.\n\\State $X_{t+1} \\leftarrow X_t \\cup D_t$.\n\\State If $X_{t+1} = X_t$ and $Y_{t+1} = Y_t$, terminate.\n\\State $t \\leftarrow t+1$. Go to Step $1$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\n\\begin{prop}\n\\label{prop1}\nSuppose $f: 2^N \\mapsto \\mathbb{R}$ is submodular. After each iteration of A1 (A2), we have $\\mathcal{X}_{min} \\subseteq [X_t,Y_t]$ ($\\mathcal{X}_{max} \\subseteq [X_t,Y_t]$).\n\\end{prop}\n\nWe prove Proposition \\ref{prop1} in the supplementary material. According to Proposition \\ref{prop1}, if the output of A1 (A2) statifies $[X_t,Y_t] \\subset [\\emptyset,N]$, then $f$ is reducible.\n\nAccording to A1 (A2), if $U_0 = D_0 = \\emptyset$, then we have $X_1 = X_0$ and $Y_1 = Y_0$. The algorithm will terminate after the first iteration and the output is $[X_0,Y_0] = [\\emptyset,N]$, which provides a vacuous reduction. In this case, we say that $f$ is \\emph{irreducible with respect to A1 (A2)}. For convenience, we directly say $f$ is \\emph{irreducible}.\n\nThereby, we conclude two points from the above algorithms. First, by the definition of $U_t$ and $D_t$, the reducibility of $f$ can be determined by the signs of marginal gains with respect to the endpoint sets of the current working lattice. Second, the reducibility of $f$ for minimization and maximization are actually the same property. Specially, suppose in a certain iteration, A1 and A2 have the same working lattice $[S,T]$. According to the algorithms, they also have the same $U_t$ and $D_t$, which determine whether $f$ is reducible after the current iteration.\n\n\\begin{prop}\n\\label{prop2}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, and a lattice $[S,T]$. $\\forall i \\in T \\setminus S$, Define $K_i = {\\operatorname{sgn}}\\{f(i|S)\\} \\cdot {\\operatorname{sgn}}\\{f(i|T-i)\\}$. Then $f$ is reducible on $[S,T]$ with respect to A1 (A2) if and only if\n\n", "itemtype": "equation", "pos": 6858, "prevtext": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\runningauthor{Jincheng Mei, Hao Zhang, Bao-Liang Lu}\n\n\\twocolumn[\n\n\\aistatstitle{On the Reducibility of Submodular Functions}\n\n\\aistatsauthor{ Jincheng Mei \\And Hao Zhang \\And Bao-Liang Lu$^\\star$ }\n\n\\aistatsaddress{ Department of Computing Science \\\\ University of Alberta \\\\ Edmonton, AB, Canada, T6G 2E8 \\\\ \\And The Robotics Institute \\\\ School of Computer Science \\\\ Carnegie Mellon University \\\\ Pittsburgh, PA 15213, USA \\And Key Lab of SMEC for IICE \\\\ Dept. of Computer Sci. and Eng. \\\\ Shanghai Jiao Tong University \\\\ Shanghai 200240, China} ]\n\n\\begin{abstract}\nThe scalability of submodular optimization methods is critical for their usability in practice. In this paper, we study the \\emph{reducibility} of submodular functions, a property that enables us to reduce the solution space of submodular optimization problems without performance loss. We introduce the concept of reducibility using \\emph{marginal gains}. Then we show that by adding perturbation, we can endow irreducible functions with reducibility, based on which we propose the \\emph{perturbation-reduction} optimization framework. Our theoretical analysis proves that given the perturbation scales, the reducibility gain could be computed, and the performance loss has additive upper bounds. We further conduct empirical studies and the results demonstrate that our proposed framework significantly accelerates existing optimization methods for irreducible submodular functions with a cost of only small performance losses.\n\\end{abstract}\n\n\\section{INTRODUCTION}\n\\label{sec:intro}\nSubmodularity naturally arises in a number of machine learning problems, such as active learning \\cite{golovin2011adaptive}, clustering \\cite{narasimhan2005q}, and dictionary selection \\cite{das2011submodular}. The scalability of submodular optimization methods is critical in practice, thus has drawn much attention from the research community. For example, Iyer et al. \\cite{iyer2013fast} propose $\\mathcal{O}(n^2)$ general optimization methods based on the semidifferential. Wei et al. \\cite{wei2014fast} combine approximation with pruning to accelerate the greedy algorithm for uniform matroid constrained submodular maximization. Mirzasoleiman et al. \\cite{mirzasoleiman2013distributed} and Pan et al. \\cite{pan2014parallel} use distributed implementation to accelerate existing optimization methods. Other techniques, including stochastic sampling \\cite{mirzasoleiman2013lazier} and decomposable assumption \\cite{jegelka2013reflection}, are also applied to scale up submodular optimization methods.\nWhile in this paper, we focus on the reducibility of submodular functions, a favourable property that can substantially improve the scalability of submodular optimization methods. The reducibility can directly reduce the solution space of the submodular optimization problems, while preserve all the optima in the reduced space, thereby enables us to accelerate the optimization process without incurring performance loss.\n\nRecent research shows that for some submodular functions, by evaluating marginal gains, reduction can be applied for unconstrained maximization \\cite{goldengorin2009maximization}, unconstrained minimization \\cite{fujishige2005submodular,iyer2013fast}, and uniform matroid constrained maximization \\cite{wei2014fast}. By leveraging the reducibility, a variety of methods have been developed to scale up the optimization of \\emph{reducible} submodular functions \\cite{fujishige2005submodular,goldengorin2009maximization,iyer2013fast,mei2015unconstrained},\nWhile existing works mainly focus on reducible functions, there exist a number of \\emph{irreducible} submodular functions widely applied in practice, for which existing methods can only provide vacuous reduction.\n\nIn this paper, we investigate the problem that whether irreducible functions can also exploit this favorable property.\nWe firstly introduce the concept of reducibility using marginal gains over the endpoint sets of a given lattice. Then for irreducible functions, we transform them to reducible functions by adding random noise to perturb the marginal gains, after which we perform lattice reduction for the perturbed functions and solve the original functions on the reduced lattice. Theoretical results show that given the perturbation scales, the reducibility gain is lower bounded, and the performance loss has additive upper bounds. The empirical results demonstrate that there exist useful perturbation scale intervals in practice, which enables us to significantly accelerate existing optimization methods with small performance losses.\n\nIn summary, this paper has the following contributions. Firstly, we introduce the concept of reducibility, and propose the perturbation-reduction framework. Secondly, we theoretically analyze our proposed method. In particular, for the reducibility gain, we propose a lower bound in terms of the perturbation scale. For the performance loss, we propose both deterministic and probabilistic upper bounds. The deterministic bound provides the understanding of relationship between the reducibility gain and performance loss, while the probabilistic bound can explain the experimental results. Finally, we empirically show that the proposed method is applicable for a variety of commonly used irreducible submodular functions.\n\nIn the sequel, we organize the paper as follows. In Section \\ref{sec:reducibility}, we introduce the definitions and the existing reduction algorithms. In Section \\ref{sec:perturbation}, we propose our perturbation based method. Theoretical analysis and empirical results are presented in Section \\ref{sec:theory} and Section \\ref{sec:results}, respectively. In Section \\ref{sec:related_work}, we review some related works. Section \\ref{sec:conclusion} comes to our conclusion.\n\n\\section{REDUCIBILITY}\n\\label{sec:reducibility}\n\\subsection{Notations and Definitions}\n\nGiven a finite set $N = \\{ 1, 2, \\dots, n\\}$, and a set function $f: 2^N \\mapsto \\mathbb{R}$. $f$ is said to be \\emph{submodular} \\cite{fujishige2005submodular} if $\\forall X, Y \\subseteq N$, $f(X) + f(Y) \\ge f(X \\cap Y) + f(X \\cup Y)$. An equivalent definition of submodularity is the \\emph{diminishing return} property: $\\forall A \\subseteq B \\subseteq N$, $\\forall i \\in N \\setminus B$, $f(i | A) \\ge f(i | B)$, where $f(i | A) \\triangleq f(A + i) - f(A)$ is called the \\emph{marginal gain} of element $i$ with respect to set $A$. To simplify the notation, we denote $A \\cup \\{ i \\}$ by $A + i$, and $A \\setminus \\{ i \\}$ by $A - i$. Given $ A \\subseteq B \\subseteq N$, the \\emph{(set interval) lattice} is defined as $[A,B] \\triangleq \\{ S \\ | \\ A \\subseteq S \\subseteq B \\}$.\n\nSuppose $N = \\{ 1, 2, \\dots, n\\}$, and $f: 2^N \\mapsto \\mathbb{R}$ is a submodular function. In this paper, we focus on unconstrained submodular optimization problems,\n\n", "index": 1, "text": "\\begin{equation*}\n    \\text{Problem 1}: \\ \\min_{X \\subseteq N}{f(X)}, \\quad \\text{Problem 2}: \\ \\max_{X \\subseteq N}{f(X)}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\text{Problem 1}:\\ \\min_{X\\subseteq N}{f(X)},\\quad\\text{Problem 2}:\\ \\max_{X%&#10;\\subseteq N}{f(X)}.\" display=\"block\"><mrow><mrow><mtext>Problem 1</mtext><mo rspace=\"7.5pt\">:</mo><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mtext>Problem 2</mtext></mrow><mo rspace=\"7.5pt\">:</mo><mrow><mrow><munder><mi>max</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n\\end{prop}\n\\begin{proof}\nSuppose $[X_0,Y_0] = [S,T]$ in A1 (A2). Then $f$ is reducible if and only if the algorithm does not terminate after its first iteration, \\emph{i.e.}, $U_0 \\not= \\emptyset$ or $D_0 \\not= \\emptyset$. Suppose $U_0 \\not= \\emptyset$ happens, \\emph{i.e.}, $\\exists i \\in T \\setminus S$, $f(i|S) < 0$. According to submodularity, $f(i|T-i) \\le f(i|S) < 0$. We have $K \\ge K_i = {\\operatorname{sgn}}\\{f(i|S)\\} \\cdot {\\operatorname{sgn}}\\{f(i|T-i)\\} = 1 > 0$. Suppose $D_0 \\not= \\emptyset$ happens, \\emph{i.e.}, $\\exists j \\in T \\setminus S$, $f(j|T-j) > 0$. According to submodularity, $f(j|S) \\ge f(j|T-j) > 0$. We have $K \\ge K_j = {\\operatorname{sgn}}\\{f(j|S)\\} \\cdot {\\operatorname{sgn}}\\{f(j|T-j)\\} = 1 > 0$.\n\\end{proof}\n\nAccording to Proposition \\ref{prop2}, the reducibility of $f$ for minimization (maximization) can be obtained by (\\ref{eq1}). Thus we say $f$ is \\emph{reducible with respect to A1 (A2)} if (\\ref{eq1}) holds. Similarly, without ambiguity in this paper, we directly say $f$ is \\emph{reducible} if (\\ref{eq1}) holds.\n\n\\section{PERTURBATION REDUCTION}\n\\label{sec:perturbation}\nGiven a reducible submodular function, we can use A1 and A2 to provide useful reduction. Unfortunately, there still exist many irreducible submodular functions, some of which are listed in the experimental section. Given a submodular function $f$, which is irreducible on $[S,T]$. According to Proposition \\ref{prop2}, $\\forall i \\in T \\setminus S$, we have $f(i|S) \\ge 0$ and $f(i|T-i) \\le 0$.\n\nIf we expect A1 and A2 to provide nontrivial reduction, we need to guarantee that (\\ref{eq1}) holds for some elements without changing the submodularity of the objective function. A natural way is to add random noise $r$\\footnote{$r: 2^N \\mapsto \\mathbb{R}$ is a modular function, and $r(X) \\triangleq \\sum\\limits_{i \\in X}{r(i)}$.} to perturb the original function as follows,\n\n", "itemtype": "equation", "pos": 11521, "prevtext": "\nProblem 1 can be exactly solved in polynomial time \\cite{orlin2009faster}, while Problem 2 is NP-hard since some of its special cases (e.g., Max Cut) are NP-hard.\n\nFor convenience of the presentation, we will use P1 and P2 to refer to Problem 1 and Problem 2, respectively. Similarly, for the following algorithms, we will use A1 and A2 to refer to Algorithm 1 and Algorithm 2. The reference holds for P3, P4, A3 and A4.\n\nDefine $\\mathcal{X}_{min} \\triangleq \\{ X_* \\subseteq N \\ |\\ f(X_*) \\le f(X), \\forall X \\subseteq N \\}$ as the optima set of P1. Similarly, define $\\mathcal{X}_{max} \\triangleq \\{ X^* \\subseteq N \\ |\\ f(X^*) \\ge f(X), \\forall X \\subseteq N \\}$. Obviously, we have $\\mathcal{X}_{min} \\subseteq [\\emptyset,N]$ and $\\mathcal{X}_{max} \\subseteq [\\emptyset,N]$.\n\nWe now give the definition of reducibility. For P1 (P2), we say the objective function $f$ is \\emph{reducible for minimization (maximization)} if $\\exists \\ [S,T] \\subset [\\emptyset,N]$, where $[S,T]$ can be obtained in $\\mathcal{O}(n^p)$ function evaluations, such that $\\mathcal{X}_{min} \\subseteq [S,T]$ ($\\mathcal{X}_{max} \\subseteq [S,T]$). Note that if we can only find $[S,T]$ in $\\mathcal{O}(2^n)$ time, the reduction is meaningless since $\\mathcal{O}(2^n)$ time is enough for us to find all the optima. The ratio $1- \\frac{|T \\setminus S|}{|N|} \\in (0,1]$ is called the \\emph{reduction rate}.\n\n\\subsection{Algorithms}\n\nExisting works on reduction for unconstrained submodular optimization can be summarized by the following two algorithms, both of which terminate in $\\mathcal{O}(n^2)$ time. The brief review of existing works can be found in Section 6.\n\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Reduction for Minimization}\n\\label{algo1}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $X_0 \\leftarrow \\emptyset$, $Y_0 \\leftarrow N$, $t \\leftarrow 0$.\n\\Ensure\n$[X_t,Y_t]$.\n\\State Find $U_t = \\{i \\in Y_t \\setminus X_t \\ |\\  f(i|X_t) < 0\\}$.\n\\State $X_{t+1} \\leftarrow X_t \\cup U_t$.\n\\State Find $D_t = \\{j \\in Y_t \\setminus X_t \\ |\\ f(j|Y_t - j) > 0\\}$.\n\\State $Y_{t+1} \\leftarrow Y_t \\setminus D_t$.\n\\State If $X_{t+1} = X_t$ and $Y_{t+1} = Y_t$, terminate.\n\\State $t \\leftarrow t+1$. Go to Step $1$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Reduction for Maximization}\n\\label{algo2}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $X_0 \\leftarrow \\emptyset$, $Y_0 \\leftarrow N$, $t \\leftarrow 0$.\n\\Ensure\n$[X_t,Y_t]$.\n\\State Find $U_t = \\{i \\in Y_t \\setminus X_t \\ |\\  f(i|X_t) < 0\\}$.\n\\State $Y_{t+1} \\leftarrow Y_t \\setminus U_t$.\n\\State Find $D_t = \\{j \\in Y_t \\setminus X_t \\ |\\ f(j|Y_t - j) > 0\\}$.\n\\State $X_{t+1} \\leftarrow X_t \\cup D_t$.\n\\State If $X_{t+1} = X_t$ and $Y_{t+1} = Y_t$, terminate.\n\\State $t \\leftarrow t+1$. Go to Step $1$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\n\\begin{prop}\n\\label{prop1}\nSuppose $f: 2^N \\mapsto \\mathbb{R}$ is submodular. After each iteration of A1 (A2), we have $\\mathcal{X}_{min} \\subseteq [X_t,Y_t]$ ($\\mathcal{X}_{max} \\subseteq [X_t,Y_t]$).\n\\end{prop}\n\nWe prove Proposition \\ref{prop1} in the supplementary material. According to Proposition \\ref{prop1}, if the output of A1 (A2) statifies $[X_t,Y_t] \\subset [\\emptyset,N]$, then $f$ is reducible.\n\nAccording to A1 (A2), if $U_0 = D_0 = \\emptyset$, then we have $X_1 = X_0$ and $Y_1 = Y_0$. The algorithm will terminate after the first iteration and the output is $[X_0,Y_0] = [\\emptyset,N]$, which provides a vacuous reduction. In this case, we say that $f$ is \\emph{irreducible with respect to A1 (A2)}. For convenience, we directly say $f$ is \\emph{irreducible}.\n\nThereby, we conclude two points from the above algorithms. First, by the definition of $U_t$ and $D_t$, the reducibility of $f$ can be determined by the signs of marginal gains with respect to the endpoint sets of the current working lattice. Second, the reducibility of $f$ for minimization and maximization are actually the same property. Specially, suppose in a certain iteration, A1 and A2 have the same working lattice $[S,T]$. According to the algorithms, they also have the same $U_t$ and $D_t$, which determine whether $f$ is reducible after the current iteration.\n\n\\begin{prop}\n\\label{prop2}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, and a lattice $[S,T]$. $\\forall i \\in T \\setminus S$, Define $K_i = {\\operatorname{sgn}}\\{f(i|S)\\} \\cdot {\\operatorname{sgn}}\\{f(i|T-i)\\}$. Then $f$ is reducible on $[S,T]$ with respect to A1 (A2) if and only if\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq1}\nK = \\max_{i \\in T \\setminus S}{K_i} > 0.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"K=\\max_{i\\in T\\setminus S}{K_{i}}&gt;0.\" display=\"block\"><mrow><mrow><mi>K</mi><mo>=</mo><mrow><munder><mi>max</mi><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2216</mo><mi>S</mi></mrow></mrow></munder><mo>\u2061</mo><msub><mi>K</mi><mi>i</mi></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nwhere $\\forall i \\in N$, $r(i) \\in \\mathbb{R}$ is generated uniformly at random in $[-t,t]$ for some $t \\ge 0$. By appropriately choosing the value of $t$, we can ensure $g(i|S) < 0$ or $g(i|T-i) > 0$ hold for some $i \\in T \\setminus S$. Thus we have (\\ref{eq1}) holds, indicating that $g$ is reducible. At the same time, as $r$ is a modular function, the submodularity of $g$ still holds.\n\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Perturbation-Reduction Minimization}\n\\label{algo3}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $[S,T]$ where $\\mathcal{X}_{min} \\subseteq [S,T]$.\n\\Ensure An approximate solution $X_*^p$.\n\\State If $f$ is reducible on $[S,T]$, $[X_0,Y_0] \\leftarrow [S,T]$, run A1 for $f$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Generate $r$. Let $g = f + r$. $[X_0,Y_0] \\leftarrow [S,T]$, run A1 for $g$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Solve $X_*^p \\in {\\operatorname{arg\\,min}}_{X \\in [S,T]}{f(X)}$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Perturbation-Reduction Maximization}\n\\label{algo4}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $[S,T]$ where $\\mathcal{X}_{max} \\subseteq [S,T]$.\n\\Ensure An approximate solution $X_p^*$.\n\\State If $f$ is reducible on $[S,T]$, $[X_0,Y_0] \\leftarrow [S,T]$, run A2 for $f$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Generate $r$. Let $g = f + r$. $[X_0,Y_0] \\leftarrow [S,T]$, run A2 for $g$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Solve $X_p^* \\in {\\operatorname{arg\\,max}}_{X \\in [S,T]}{f(X)}$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\nWe propose our perturbation based method for minimization and maximization in A3 and A4, respectively. For an irreducible submodular function $f$ on a given lattice $[S,T]$, we first perturb the objective function to make it reducible, \\emph{i.e.}, $g \\triangleq f + r$. A1 or A2 are then employed to obtain the reduced lattice of $g$. Finally we solve the original problems of $f$ on the reduced lattice exactly or approximately using existing methods.\n\nIt is worth mentioning that, though we mainly focus on irreducible functions, our methods also work for reducible ones, as they are special cases of irreducible functions. Particularly, given a reducible function $f$ on $[S,T]$, of which the reduction rate is less than $1$, after A1 (A2) terminates, we can get a sublattice $[P,Q] \\subset [S,T]$ so that $f$ is irreducible on $[P,Q]$.\n\n\\section{THEORETICAL ANALYSIS}\n\\label{sec:theory}\nBy perturbing the irreducible submodular function, we transform P1 (P2) into P3 (P4). This makes the objective reducible while leads the solution to be inexact. Correspondingly, our theoretical analysis of the method focuses on two main aspects: the reducibility gain and the performance loss incurred by perturbation.\n\n\\subsection{Reducibility Gain}\n\\label{reducibility_gain}\n\nSuppose $f: 2^N \\mapsto \\mathbb{R}$ is an irreducible submodular function on $[S,T]$, and $g \\triangleq f + r$ as defined in P3 and P4. Since $f$ is irreducible, $\\forall i \\in T \\setminus S$, we have $f(i|S) \\ge 0$ and $f(i|T-i) \\le 0$.\n\n\\begin{prop}\n\\label{prop3}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, which is irreducible on $[S,T]$. Define $m\\{f,[S,T]\\} \\triangleq \\min_{i \\in T \\setminus S}{\\min\\{f(i|S), -f(i|T-i)\\}}$. If $t \\le m\\{f,[S,T]\\}$, then $g$ is irreducible on $[S,T]$.\n\\end{prop}\n\\begin{proof}\nSince $m\\{f,[S,T]\\} \\ge 0$, we suppose $0 \\le t \\le m\\{f,[S,T]\\}$. $\\forall i \\in T \\setminus S$, we have $g(i|S) = f(i|S) + r(i) \\ge f(i|S) - t \\ge f(i|S) - m\\{f,[S,T]\\} \\ge 0$, and $g(i|T-i) = f(i|T-i) + r(i) \\le f(i|T-i) + t \\le f(i|T-i) + m\\{f,[S,T]\\} \\le 0$, which implies that $g$ is also irreducible on $[S,T]$.\n\\end{proof}\n\nProposition \\ref{prop3} indicates that if the perturbation scale $t$ is small enough, there is no reducibility gain. This is intuitively reasonable since we have $g \\rightarrow f$ when $t \\rightarrow 0$.\n\nTo lower bound the reducibility gain of adding perturbation, we generalize the concept of \\emph{curvature} \\cite{conforti1984submodular,iyer2013curvature} for non-monotone irreducible submodular functions.\n\n\\begin{defi}\n\\label{defi1}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, the curvature of $f$ on $[S,T]$ is defined as,\n\n", "itemtype": "equation", "pos": 13497, "prevtext": "\n\\end{prop}\n\\begin{proof}\nSuppose $[X_0,Y_0] = [S,T]$ in A1 (A2). Then $f$ is reducible if and only if the algorithm does not terminate after its first iteration, \\emph{i.e.}, $U_0 \\not= \\emptyset$ or $D_0 \\not= \\emptyset$. Suppose $U_0 \\not= \\emptyset$ happens, \\emph{i.e.}, $\\exists i \\in T \\setminus S$, $f(i|S) < 0$. According to submodularity, $f(i|T-i) \\le f(i|S) < 0$. We have $K \\ge K_i = {\\operatorname{sgn}}\\{f(i|S)\\} \\cdot {\\operatorname{sgn}}\\{f(i|T-i)\\} = 1 > 0$. Suppose $D_0 \\not= \\emptyset$ happens, \\emph{i.e.}, $\\exists j \\in T \\setminus S$, $f(j|T-j) > 0$. According to submodularity, $f(j|S) \\ge f(j|T-j) > 0$. We have $K \\ge K_j = {\\operatorname{sgn}}\\{f(j|S)\\} \\cdot {\\operatorname{sgn}}\\{f(j|T-j)\\} = 1 > 0$.\n\\end{proof}\n\nAccording to Proposition \\ref{prop2}, the reducibility of $f$ for minimization (maximization) can be obtained by (\\ref{eq1}). Thus we say $f$ is \\emph{reducible with respect to A1 (A2)} if (\\ref{eq1}) holds. Similarly, without ambiguity in this paper, we directly say $f$ is \\emph{reducible} if (\\ref{eq1}) holds.\n\n\\section{PERTURBATION REDUCTION}\n\\label{sec:perturbation}\nGiven a reducible submodular function, we can use A1 and A2 to provide useful reduction. Unfortunately, there still exist many irreducible submodular functions, some of which are listed in the experimental section. Given a submodular function $f$, which is irreducible on $[S,T]$. According to Proposition \\ref{prop2}, $\\forall i \\in T \\setminus S$, we have $f(i|S) \\ge 0$ and $f(i|T-i) \\le 0$.\n\nIf we expect A1 and A2 to provide nontrivial reduction, we need to guarantee that (\\ref{eq1}) holds for some elements without changing the submodularity of the objective function. A natural way is to add random noise $r$\\footnote{$r: 2^N \\mapsto \\mathbb{R}$ is a modular function, and $r(X) \\triangleq \\sum\\limits_{i \\in X}{r(i)}$.} to perturb the original function as follows,\n\n", "index": 5, "text": "\\begin{align*}\n    &\\text{Problem 3}: \\ \\min_{X \\subseteq N}{g(X)} \\triangleq \\min_{X \\subseteq N}{f(X) + r(X)}, \\\\\n    &\\text{Problem 4}: \\ \\max_{X \\subseteq N}{g(X)} \\triangleq \\max_{X \\subseteq N}{f(X) + r(X)},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{Problem 3}:\\ \\min_{X\\subseteq N}{g(X)}\\triangleq\\min_{X%&#10;\\subseteq N}{f(X)+r(X)},\" display=\"inline\"><mrow><mrow><mtext>Problem 3</mtext><mo rspace=\"7.5pt\">:</mo><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{Problem 4}:\\ \\max_{X\\subseteq N}{g(X)}\\triangleq\\max_{X%&#10;\\subseteq N}{f(X)+r(X)},\" display=\"inline\"><mrow><mrow><mtext>Problem 4</mtext><mo rspace=\"7.5pt\">:</mo><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>g</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><mrow><mrow><munder><mi>max</mi><mrow><mi>X</mi><mo>\u2286</mo><mi>N</mi></mrow></munder><mo>\u2061</mo><mi>f</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n\\end{defi}\n\nNote that for any irreducible submodular function $f$ on $[S,T]$, we have $c\\{f,[S,T]\\} \\ge 1$.\n\n\\begin{thm}\n\\label{thm1}\nSuppose $t > m\\{f,[S,T]\\}$, denote $s = |T \\setminus S| > 0$, $k = \\sum_{i \\in T \\setminus S}{f(i|S)}$, $c = c\\{f,[S,T]\\}$. The reduction rate in expectation of $g$ is at least $1 - \\frac{ck}{2ts}$.\n\\end{thm}\n\\begin{proof}\nSuppose $T \\setminus S = \\{ 1, 2, \\dots, s \\}$, $\\forall i \\in T \\setminus S$, we define a random variable $H_i$ as,\n\n", "itemtype": "equation", "pos": 17993, "prevtext": "\nwhere $\\forall i \\in N$, $r(i) \\in \\mathbb{R}$ is generated uniformly at random in $[-t,t]$ for some $t \\ge 0$. By appropriately choosing the value of $t$, we can ensure $g(i|S) < 0$ or $g(i|T-i) > 0$ hold for some $i \\in T \\setminus S$. Thus we have (\\ref{eq1}) holds, indicating that $g$ is reducible. At the same time, as $r$ is a modular function, the submodularity of $g$ still holds.\n\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Perturbation-Reduction Minimization}\n\\label{algo3}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $[S,T]$ where $\\mathcal{X}_{min} \\subseteq [S,T]$.\n\\Ensure An approximate solution $X_*^p$.\n\\State If $f$ is reducible on $[S,T]$, $[X_0,Y_0] \\leftarrow [S,T]$, run A1 for $f$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Generate $r$. Let $g = f + r$. $[X_0,Y_0] \\leftarrow [S,T]$, run A1 for $g$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Solve $X_*^p \\in {\\operatorname{arg\\,min}}_{X \\in [S,T]}{f(X)}$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\begin{minipage}[h]{0.48\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Perturbation-Reduction Maximization}\n\\label{algo4}\n\\begin{algorithmic}[1]\n\\Require\n$N$, $f$, $[S,T]$ where $\\mathcal{X}_{max} \\subseteq [S,T]$.\n\\Ensure An approximate solution $X_p^*$.\n\\State If $f$ is reducible on $[S,T]$, $[X_0,Y_0] \\leftarrow [S,T]$, run A2 for $f$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Generate $r$. Let $g = f + r$. $[X_0,Y_0] \\leftarrow [S,T]$, run A2 for $g$, $[S,T] \\leftarrow [X_t,Y_t]$.\n\\State Solve $X_p^* \\in {\\operatorname{arg\\,max}}_{X \\in [S,T]}{f(X)}$.\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\nWe propose our perturbation based method for minimization and maximization in A3 and A4, respectively. For an irreducible submodular function $f$ on a given lattice $[S,T]$, we first perturb the objective function to make it reducible, \\emph{i.e.}, $g \\triangleq f + r$. A1 or A2 are then employed to obtain the reduced lattice of $g$. Finally we solve the original problems of $f$ on the reduced lattice exactly or approximately using existing methods.\n\nIt is worth mentioning that, though we mainly focus on irreducible functions, our methods also work for reducible ones, as they are special cases of irreducible functions. Particularly, given a reducible function $f$ on $[S,T]$, of which the reduction rate is less than $1$, after A1 (A2) terminates, we can get a sublattice $[P,Q] \\subset [S,T]$ so that $f$ is irreducible on $[P,Q]$.\n\n\\section{THEORETICAL ANALYSIS}\n\\label{sec:theory}\nBy perturbing the irreducible submodular function, we transform P1 (P2) into P3 (P4). This makes the objective reducible while leads the solution to be inexact. Correspondingly, our theoretical analysis of the method focuses on two main aspects: the reducibility gain and the performance loss incurred by perturbation.\n\n\\subsection{Reducibility Gain}\n\\label{reducibility_gain}\n\nSuppose $f: 2^N \\mapsto \\mathbb{R}$ is an irreducible submodular function on $[S,T]$, and $g \\triangleq f + r$ as defined in P3 and P4. Since $f$ is irreducible, $\\forall i \\in T \\setminus S$, we have $f(i|S) \\ge 0$ and $f(i|T-i) \\le 0$.\n\n\\begin{prop}\n\\label{prop3}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, which is irreducible on $[S,T]$. Define $m\\{f,[S,T]\\} \\triangleq \\min_{i \\in T \\setminus S}{\\min\\{f(i|S), -f(i|T-i)\\}}$. If $t \\le m\\{f,[S,T]\\}$, then $g$ is irreducible on $[S,T]$.\n\\end{prop}\n\\begin{proof}\nSince $m\\{f,[S,T]\\} \\ge 0$, we suppose $0 \\le t \\le m\\{f,[S,T]\\}$. $\\forall i \\in T \\setminus S$, we have $g(i|S) = f(i|S) + r(i) \\ge f(i|S) - t \\ge f(i|S) - m\\{f,[S,T]\\} \\ge 0$, and $g(i|T-i) = f(i|T-i) + r(i) \\le f(i|T-i) + t \\le f(i|T-i) + m\\{f,[S,T]\\} \\le 0$, which implies that $g$ is also irreducible on $[S,T]$.\n\\end{proof}\n\nProposition \\ref{prop3} indicates that if the perturbation scale $t$ is small enough, there is no reducibility gain. This is intuitively reasonable since we have $g \\rightarrow f$ when $t \\rightarrow 0$.\n\nTo lower bound the reducibility gain of adding perturbation, we generalize the concept of \\emph{curvature} \\cite{conforti1984submodular,iyer2013curvature} for non-monotone irreducible submodular functions.\n\n\\begin{defi}\n\\label{defi1}\nGiven a submodular function $f: 2^N \\mapsto \\mathbb{R}$, the curvature of $f$ on $[S,T]$ is defined as,\n\n", "index": 7, "text": "\\begin{equation*}\n    c\\{f,[S,T]\\} = \\max_{i \\in T \\setminus S, f(i|S) > 0}{\\frac{f(i|S)-f(i|T-i)}{f(i|S)}}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"c\\{f,[S,T]\\}=\\max_{i\\in T\\setminus S,f(i|S)&gt;0}{\\frac{f(i|S)-f(i|T-i)}{f(i|S)}}.\" display=\"block\"><mrow><mrow><mrow><mi>c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mi>f</mi><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><mi>S</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>max</mi><mrow><mi>i</mi><mo>\u2208</mo><mi>T</mi><mo>\u2216</mo><mi>S</mi><mo>,</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><mn>0</mn></mrow></munder><mo>\u2061</mo><mfrac><mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>T</mi><mo>-</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n$H_i$ indicates whether $i$ can be reduced from $T \\setminus S$ or not. Define $H = \\sum_{i \\in T \\setminus S}{H_i}$ as the total number of the reduced elements. We firstly lower bound $\\mathbb{E}(H)$ by the total number of the reduced elements after the first iteration round of A1(A2),\n\n", "itemtype": "equation", "pos": 18592, "prevtext": "\n\\end{defi}\n\nNote that for any irreducible submodular function $f$ on $[S,T]$, we have $c\\{f,[S,T]\\} \\ge 1$.\n\n\\begin{thm}\n\\label{thm1}\nSuppose $t > m\\{f,[S,T]\\}$, denote $s = |T \\setminus S| > 0$, $k = \\sum_{i \\in T \\setminus S}{f(i|S)}$, $c = c\\{f,[S,T]\\}$. The reduction rate in expectation of $g$ is at least $1 - \\frac{ck}{2ts}$.\n\\end{thm}\n\\begin{proof}\nSuppose $T \\setminus S = \\{ 1, 2, \\dots, s \\}$, $\\forall i \\in T \\setminus S$, we define a random variable $H_i$ as,\n\n", "index": 9, "text": "\\begin{equation*}\n    H_i=\n    \\begin{cases}\n        1 &\\text{if } K_i > 0, \\\\\n        0 &\\text{otherwise}.\n    \\end{cases}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"H_{i}=\\begin{cases}1&amp;\\text{if }K_{i}&gt;0,\\\\&#10;0&amp;\\text{otherwise}.\\end{cases}\" display=\"block\"><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><msub><mi>K</mi><mi>i</mi></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mn>0</mn></mtd><mtd columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo>.</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nConsequently, the reduction rate in expectation is $\\frac{\\mathbb{E}(H)}{s} \\ge 1 - \\frac{ck}{2ts}$.\n\\end{proof}\nTheorem \\ref{thm1} implies that the reduction rate in expectation approaches $1$ as the perturbation scale $t$ increases. This is also consistent with our intuition since $g \\rightarrow r$ when $t \\rightarrow \\infty$. Note that $r$ is a modular function, which always has the highest reduction rate $1$.\n\n\\subsection{Performance Loss}\n\nSuppose $X_* \\in \\mathcal{X}_{min}$ ($X^* \\in \\mathcal{X}_{max}$), \\emph{i.e.}, $X_*$ ($X^*$) is an optimum of P1 (P2). Recall that $X_*^p$ ($X_p^* $) is the output of A3 (A4), for P1 (P2), we define $f(X_*^p) - f(X_*)$ ($f(X^*) - f(X_p^*)$) as the \\emph{performance loss} incurred by perturbation. For P1 (P2), the following result shows that the performance loss is upper bounded by the total perturbation of the ``mistakenly'' reduced elements, which will be explained later on.\n\n\\begin{thm}\n\\label{thm2}\nGiven an irreducible submodular function $f: 2^N \\mapsto \\mathbb{R}$. Suppose $t$ is the perturbation scale in A3 (A4), and $R_t$ is the reduction rate. We have,\n{\n\\vspace{-3pt}\n\n", "itemtype": "equation", "pos": 19020, "prevtext": "\n$H_i$ indicates whether $i$ can be reduced from $T \\setminus S$ or not. Define $H = \\sum_{i \\in T \\setminus S}{H_i}$ as the total number of the reduced elements. We firstly lower bound $\\mathbb{E}(H)$ by the total number of the reduced elements after the first iteration round of A1(A2),\n\n", "index": 11, "text": "\\begin{align*}\n\\small\n    &\\mathbb{E}(H) = \\sum_{i \\in T \\setminus S}{\\mathbb{E}(H_i)} = \\sum_{i \\in T \\setminus S} {{\\operatorname{Pr}}\\{H_i = 1\\}}\\\\\n    &\\ge \\frac{1}{2t} \\cdot \\sum_{i \\in T \\setminus S}{\\max\\{0, t - f(i|S)\\} + \\max\\{0, t + f(i|T-i)\\}} \\\\\n    &\\ge s - \\frac{c}{2t} \\cdot \\sum_{i \\in T \\setminus S}{f(i|S)} = s - \\frac{ck}{2t}.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}(H)=\\sum_{i\\in T\\setminus S}{\\mathbb{E}(H_{i})}=\\sum_{i%&#10;\\in T\\setminus S}{{\\operatorname{Pr}}\\{H_{i}=1\\}}\" display=\"inline\"><mrow><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2216</mo><mi>S</mi></mrow></mrow></munder></mstyle><mrow><mi>\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>H</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2216</mo><mi>S</mi></mrow></mrow></munder></mstyle><mrow><mo>Pr</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>H</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\frac{1}{2t}\\cdot\\sum_{i\\in T\\setminus S}{\\max\\{0,t-f(i|S)\\}+%&#10;\\max\\{0,t+f(i|T-i)\\}}\" display=\"inline\"><mrow><mo>\u2265</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mstyle><mo>\u22c5</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2216</mo><mi>S</mi></mrow></mrow></munder></mstyle><mi>max</mi><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi>t</mi><mo>-</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mo>+</mo><mi>max</mi><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi>t</mi><mo>+</mo><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>T</mi><mo>-</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq s-\\frac{c}{2t}\\cdot\\sum_{i\\in T\\setminus S}{f(i|S)}=s-\\frac{%&#10;ck}{2t}.\" display=\"inline\"><mrow><mo>\u2265</mo><mi>s</mi><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mi>c</mi><mrow><mn>2</mn><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mstyle><mo>\u22c5</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>\u2208</mo><mrow><mi>T</mi><mo>\u2216</mo><mi>S</mi></mrow></mrow></munder></mstyle><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">|</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>s</mi><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>c</mi><mo>\u2062</mo><mi>k</mi></mrow><mrow><mn>2</mn><mo>\u2062</mo><mi>t</mi></mrow></mfrac></mstyle><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n}\n\\end{thm}\n\\vspace{-12pt}\n\\begin{proof}\nWe prove the maximization case. In general, we have $X^* \\not\\in [X_t, Y_t]$, otherwise the loss is zero. Note that $X_p^* \\in [X_t, Y_t]$ according to A4. So we firstly introduce an intermediate set $X^* \\cup X_t \\cap Y_t$, \\emph{i.e.}, the contraction of $X^*$ in $[X_t,Y_t]$ for our analysis. Given the fact that $f(X^* \\cup X_t \\cap Y_t) \\le f(X_p^*)$, if we can upper bound $f(X^*) - f(X^* \\cup X_t \\cap Y_t)$, then the total performance loss is also upper bounded.\nIn A2, we have $[X_t,Y_t] \\subset \\cdots \\subset [X_1,Y_1] \\subset [X_0,Y_0] = [\\emptyset,N]$. By definition, $\\forall \\ 0 \\le k \\le t-1$, $X_{k+1} = X_k \\cup D_k$, and $Y_{k+1} = Y_k \\setminus U_k$. We have,\n\n", "itemtype": "equation", "pos": 20514, "prevtext": "\nConsequently, the reduction rate in expectation is $\\frac{\\mathbb{E}(H)}{s} \\ge 1 - \\frac{ck}{2ts}$.\n\\end{proof}\nTheorem \\ref{thm1} implies that the reduction rate in expectation approaches $1$ as the perturbation scale $t$ increases. This is also consistent with our intuition since $g \\rightarrow r$ when $t \\rightarrow \\infty$. Note that $r$ is a modular function, which always has the highest reduction rate $1$.\n\n\\subsection{Performance Loss}\n\nSuppose $X_* \\in \\mathcal{X}_{min}$ ($X^* \\in \\mathcal{X}_{max}$), \\emph{i.e.}, $X_*$ ($X^*$) is an optimum of P1 (P2). Recall that $X_*^p$ ($X_p^* $) is the output of A3 (A4), for P1 (P2), we define $f(X_*^p) - f(X_*)$ ($f(X^*) - f(X_p^*)$) as the \\emph{performance loss} incurred by perturbation. For P1 (P2), the following result shows that the performance loss is upper bounded by the total perturbation of the ``mistakenly'' reduced elements, which will be explained later on.\n\n\\begin{thm}\n\\label{thm2}\nGiven an irreducible submodular function $f: 2^N \\mapsto \\mathbb{R}$. Suppose $t$ is the perturbation scale in A3 (A4), and $R_t$ is the reduction rate. We have,\n{\n\\vspace{-3pt}\n\n", "index": 13, "text": "\\begin{align*}\n\\small\n  f(X_*^p) - f(X_*) &< - r(X_t \\setminus X_*) + r(X_* \\setminus Y_t) < n t R_t, \\\\\n  f(X^*) - f(X_p^*) &< r(X_t \\setminus X^*) - r(X^* \\setminus Y_t) < n t R_t.\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\small f(X_{*}^{p})-f(X_{*})\" display=\"inline\"><mrow><mrow><mi mathsize=\"90%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><msubsup><mi mathsize=\"90%\">X</mi><mo mathsize=\"90%\" stretchy=\"false\">*</mo><mi mathsize=\"90%\">p</mi></msubsup><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"90%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><msub><mi mathsize=\"90%\">X</mi><mo mathsize=\"90%\" stretchy=\"false\">*</mo></msub><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;-r(X_{t}\\setminus X_{*})+r(X_{*}\\setminus Y_{t})&lt;ntR_{t},\" display=\"inline\"><mrow><mrow><mi/><mo>&lt;</mo><mrow><mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msub><mi>X</mi><mo>*</mo></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mo>*</mo></msub><mo>\u2216</mo><msub><mi>Y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msub><mi>R</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f(X^{*})-f(X_{p}^{*})\" display=\"inline\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>p</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;r(X_{t}\\setminus X^{*})-r(X^{*}\\setminus Y_{t})&lt;ntR_{t}.\" display=\"inline\"><mrow><mrow><mi/><mo>&lt;</mo><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u2216</mo><msub><mi>Y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><msub><mi>R</mi><mi>t</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nwhere (\\ref{eq3}) is the telescopic version of (\\ref{eq2}). According to submodularity, we have (\\ref{eq4}) holds, and (\\ref{eq5}) comes from the third step of A2. Similarly, we have,\n\n", "itemtype": "equation", "pos": 21431, "prevtext": "\n}\n\\end{thm}\n\\vspace{-12pt}\n\\begin{proof}\nWe prove the maximization case. In general, we have $X^* \\not\\in [X_t, Y_t]$, otherwise the loss is zero. Note that $X_p^* \\in [X_t, Y_t]$ according to A4. So we firstly introduce an intermediate set $X^* \\cup X_t \\cap Y_t$, \\emph{i.e.}, the contraction of $X^*$ in $[X_t,Y_t]$ for our analysis. Given the fact that $f(X^* \\cup X_t \\cap Y_t) \\le f(X_p^*)$, if we can upper bound $f(X^*) - f(X^* \\cup X_t \\cap Y_t)$, then the total performance loss is also upper bounded.\nIn A2, we have $[X_t,Y_t] \\subset \\cdots \\subset [X_1,Y_1] \\subset [X_0,Y_0] = [\\emptyset,N]$. By definition, $\\forall \\ 0 \\le k \\le t-1$, $X_{k+1} = X_k \\cup D_k$, and $Y_{k+1} = Y_k \\setminus U_k$. We have,\n\n", "index": 15, "text": "\\begin{align}\n\\small\n\\label{eq2}\n  &f(X^* \\cup X_t) - f(X^*) \\\\\n\\label{eq3}\n  &= \\sum\\limits_{s = 1, x_s \\in X_t \\setminus X^*}^{|X_t \\setminus X^*|}{f(x_s | X^* + x_1 + \\cdots + x_{s-1})} \\\\\n\\label{eq4}\n  &\\ge \\sum\\limits_{i = 0}^{t-1} \\sum\\limits_{d \\in D_i \\setminus X^*}{f(d|Y_i-d)} \\\\\n\\label{eq5}\n  &> - \\sum\\limits_{i = 0}^{t-1} \\sum\\limits_{d \\in D_i \\setminus X^*}{r(d)} \\\\\n\\label{eq6}\n  &= - r(X_t \\setminus X^*),\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle f(X^{*}\\cup X_{t})-f(X^{*})\" display=\"inline\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u222a</mo><msub><mi>X</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum\\limits_{s=1,x_{s}\\in X_{t}\\setminus X^{*}}^{|X_{t}\\setminus&#10;X%&#10;^{*}|}{f(x_{s}|X^{*}+x_{1}+\\cdots+x_{s-1})}\" display=\"inline\"><mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>s</mi></msub><mo>\u2208</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">|</mo></mrow></munderover></mstyle><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>s</mi></msub><mo stretchy=\"false\">|</mo><msup><mi>X</mi><mo>*</mo></msup><mo>+</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>+</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\geq\\sum\\limits_{i=0}^{t-1}\\sum\\limits_{d\\in D_{i}\\setminus X^{*}%&#10;}{f(d|Y_{i}-d)}\" display=\"inline\"><mrow><mo>\u2265</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>d</mi><mo>\u2208</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></mrow></munder></mstyle><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">|</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>-</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&gt;-\\sum\\limits_{i=0}^{t-1}\\sum\\limits_{d\\in D_{i}\\setminus X^{*}}{%&#10;r(d)}\" display=\"inline\"><mrow><mi/><mo>&gt;</mo><mrow><mo>-</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>d</mi><mo>\u2208</mo><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow></mrow></munder></mstyle><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=-r(X_{t}\\setminus X^{*}),\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nCombining (\\ref{eq6}) with (\\ref{eq7}), and noting $X^* \\cup X_t \\cap Y_t \\in [X_t, Y_t]$ and $f(X_p^*) \\ge f(X^* \\cup X_t \\cap Y_t)$, we have,\n\n", "itemtype": "equation", "pos": 22050, "prevtext": "\nwhere (\\ref{eq3}) is the telescopic version of (\\ref{eq2}). According to submodularity, we have (\\ref{eq4}) holds, and (\\ref{eq5}) comes from the third step of A2. Similarly, we have,\n\n", "index": 17, "text": "\\begin{equation}\n\\label{eq7}\n  f(X^* \\cup X_t) - f(X^* \\cup X_t \\cap Y_t) < -r(X^* \\setminus Y_t).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"f(X^{*}\\cup X_{t})-f(X^{*}\\cup X_{t}\\cap Y_{t})&lt;-r(X^{*}\\setminus Y_{t}).\" display=\"block\"><mrow><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u222a</mo><msub><mi>X</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u222a</mo><msub><mi>X</mi><mi>t</mi></msub></mrow><mo>\u2229</mo><msub><mi>Y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u2216</mo><msub><mi>Y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nWe note in (\\ref{eq8}), $X_t \\setminus X^*$ is actually the set of all the elements which are not in $X^*$ but added by A2. Similarly, $X^* \\setminus Y_t$ is the set of all the elements which are in $X^*$ but eliminated by A2. Consequently, the performance loss is upper bounded by the total perturbation value of all the mistakenly reduced elements. Since the number of all the mistakenly reduced elements is no more than the number of all the reduced elements $n R_t$, and the perturbation is generated in $[-t, t]$, we have $r(X_t \\setminus X^*) - r(X^* \\setminus Y_t) \\le n t R_t$.\n\nFor the minimization case, the proof is similar.\n\\end{proof}\n\nNote that in Theorem \\ref{thm2}, the performance loss is upper bounded by the sum of random variables, which means we can obtain high probability bounds using some concentration inequalities, such as \\cite{hoeffding1963probability}.\n\n\\begin{thm}(\\textbf{Hoeffding})\n\\label{thm3}\nLet $X_1, X_2, \\dots, X_n$ be independent real-valued random variables such that $\\forall i \\in \\{ 1, 2, \\dots, n\\}$, $|X_i| \\le t$. Then with probability $1 - \\delta$,\n\n", "itemtype": "equation", "pos": 22308, "prevtext": "\nCombining (\\ref{eq6}) with (\\ref{eq7}), and noting $X^* \\cup X_t \\cap Y_t \\in [X_t, Y_t]$ and $f(X_p^*) \\ge f(X^* \\cup X_t \\cap Y_t)$, we have,\n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq8}\n  f(X^*) - f(X_p^*) < r(X_t \\setminus X^*) - r(X^* \\setminus Y_t).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"f(X^{*})-f(X_{p}^{*})&lt;r(X_{t}\\setminus X^{*})-r(X^{*}\\setminus Y_{t}).\" display=\"block\"><mrow><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>p</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>\u2216</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>\u2216</mo><msub><mi>Y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n\\end{thm}\n\n\\begin{thm}\n\\label{thm4}\nDefine $X_*^c \\triangleq X_* \\cup X_t \\cap Y_t$, and $X_c^* \\triangleq X^* \\cup X_t \\cap Y_t$. Denote $M_r \\triangleq |X_*^c \\triangle X_*|$, and  $N_r \\triangleq |X_c^* \\triangle X^*|$, where $A \\triangle B \\triangleq (A \\setminus B) \\cup (B \\setminus A)$ is the symmetric difference between the two sets $A$ and $B$. Then with probability at least $1 - \\delta$,\n\n", "itemtype": "equation", "pos": 23516, "prevtext": "\nWe note in (\\ref{eq8}), $X_t \\setminus X^*$ is actually the set of all the elements which are not in $X^*$ but added by A2. Similarly, $X^* \\setminus Y_t$ is the set of all the elements which are in $X^*$ but eliminated by A2. Consequently, the performance loss is upper bounded by the total perturbation value of all the mistakenly reduced elements. Since the number of all the mistakenly reduced elements is no more than the number of all the reduced elements $n R_t$, and the perturbation is generated in $[-t, t]$, we have $r(X_t \\setminus X^*) - r(X^* \\setminus Y_t) \\le n t R_t$.\n\nFor the minimization case, the proof is similar.\n\\end{proof}\n\nNote that in Theorem \\ref{thm2}, the performance loss is upper bounded by the sum of random variables, which means we can obtain high probability bounds using some concentration inequalities, such as \\cite{hoeffding1963probability}.\n\n\\begin{thm}(\\textbf{Hoeffding})\n\\label{thm3}\nLet $X_1, X_2, \\dots, X_n$ be independent real-valued random variables such that $\\forall i \\in \\{ 1, 2, \\dots, n\\}$, $|X_i| \\le t$. Then with probability $1 - \\delta$,\n\n", "index": 21, "text": "\\begin{equation*}\n\\small\n    \\sum_{i=1}^n{X_i} - \\mathbb{E}\\left[\\sum_{i=1}^n{X_i}\\right] <  t \\sqrt{2n \\log{(1/\\delta)}}.\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex11.m1\" class=\"ltx_Math\" alttext=\"\\small\\sum_{i=1}^{n}{X_{i}}-\\mathbb{E}\\left[\\sum_{i=1}^{n}{X_{i}}\\right]&lt;t%&#10;\\sqrt{2n\\log{(1/\\delta)}}.\" display=\"block\"><mrow><mrow><mrow><mrow><munderover><mo largeop=\"true\" mathsize=\"90%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"90%\">i</mi><mo mathsize=\"90%\" stretchy=\"false\">=</mo><mn mathsize=\"90%\">1</mn></mrow><mi mathsize=\"90%\">n</mi></munderover><msub><mi mathsize=\"90%\">X</mi><mi mathsize=\"90%\">i</mi></msub></mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"90%\">\ud835\udd3c</mi><mo>\u2062</mo><mrow><mo>[</mo><mrow><munderover><mo largeop=\"true\" mathsize=\"90%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"90%\">i</mi><mo mathsize=\"90%\" stretchy=\"false\">=</mo><mn mathsize=\"90%\">1</mn></mrow><mi mathsize=\"90%\">n</mi></munderover><msub><mi mathsize=\"90%\">X</mi><mi mathsize=\"90%\">i</mi></msub></mrow><mo>]</mo></mrow></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">&lt;</mo><mrow><mi mathsize=\"90%\">t</mi><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"90%\">2</mn><mo>\u2062</mo><mi mathsize=\"90%\">n</mi><mo>\u2062</mo><mrow><mi mathsize=\"90%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><mrow><mn mathsize=\"90%\">1</mn><mo mathsize=\"90%\" stretchy=\"false\">/</mo><mi mathsize=\"90%\">\u03b4</mi></mrow><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n\\end{thm}\n\\begin{proof}\nWe prove (\\ref{eq10}). Since the perturbation vector $r$ has zero expectation value, and each element of $r$ is independently generated. For any fixed $X \\subseteq N$, according to Theorem \\ref{thm3},  with probability at least $1 - \\delta$,\n\n", "itemtype": "equation", "pos": 24055, "prevtext": "\n\\end{thm}\n\n\\begin{thm}\n\\label{thm4}\nDefine $X_*^c \\triangleq X_* \\cup X_t \\cap Y_t$, and $X_c^* \\triangleq X^* \\cup X_t \\cap Y_t$. Denote $M_r \\triangleq |X_*^c \\triangle X_*|$, and  $N_r \\triangleq |X_c^* \\triangle X^*|$, where $A \\triangle B \\triangleq (A \\setminus B) \\cup (B \\setminus A)$ is the symmetric difference between the two sets $A$ and $B$. Then with probability at least $1 - \\delta$,\n\n", "index": 23, "text": "\\begin{align}\n\\small\n\\label{eq9}\n    f(X_*^p) - f(X_*) &< t \\sqrt{2 M_r (n + \\log{(1/\\delta)})}, \\\\\n\\label{eq10}\n    f(X^*) - f(X_p^*) &< t \\sqrt{2 N_r (n + \\log{(1/\\delta)})}.\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\small f(X_{*}^{p})-f(X_{*})\" display=\"inline\"><mrow><mrow><mi mathsize=\"90%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><msubsup><mi mathsize=\"90%\">X</mi><mo mathsize=\"90%\" stretchy=\"false\">*</mo><mi mathsize=\"90%\">p</mi></msubsup><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow><mo mathsize=\"90%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"90%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"90%\" minsize=\"90%\">(</mo><msub><mi mathsize=\"90%\">X</mi><mo mathsize=\"90%\" stretchy=\"false\">*</mo></msub><mo maxsize=\"90%\" minsize=\"90%\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;t\\sqrt{2M_{r}(n+\\log{(1/\\delta)})},\" display=\"inline\"><mrow><mrow><mi/><mo>&lt;</mo><mrow><mi>t</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>M</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f(X^{*})-f(X_{p}^{*})\" display=\"inline\"><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>p</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle&lt;t\\sqrt{2N_{r}(n+\\log{(1/\\delta)})}.\" display=\"inline\"><mrow><mrow><mi/><mo>&lt;</mo><mrow><mi>t</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>N</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nSuppose $X^* \\in [S,T]$, and define $m \\triangleq |[S,T]|$. Obviously, we have $m \\le 2^n$. Hence,\n\n", "itemtype": "equation", "pos": 24510, "prevtext": "\n\\end{thm}\n\\begin{proof}\nWe prove (\\ref{eq10}). Since the perturbation vector $r$ has zero expectation value, and each element of $r$ is independently generated. For any fixed $X \\subseteq N$, according to Theorem \\ref{thm3},  with probability at least $1 - \\delta$,\n\n", "index": 25, "text": "\\begin{equation}\n\\label{eq11}\n    r(X) - r(X^*) \\le t \\sqrt{2 |X \\triangle X^*| \\log{(1/\\delta)}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"r(X)-r(X^{*})\\leq t\\sqrt{2|X\\triangle X^{*}|\\log{(1/\\delta)}}.\" display=\"block\"><mrow><mrow><mrow><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>r</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mrow><mi>t</mi><mo>\u2062</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>X</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u25b3</mi><mo>\u2062</mo><msup><mi>X</mi><mo>*</mo></msup></mrow><mo stretchy=\"false\">|</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nwhere the first equality holds by the law of total probability. The second equality holds because replacing $X_c^*$ with $X$ in the first expression does not change the event. The first inequality comes from dropping the event $X_c^* = X$ increases the probability. The last line results from (\\ref{eq11}) and the definition of $m$.\nCombining the above result with Theorem \\ref{thm2}, and note $r(X_t \\setminus X^*) - r(X^* \\setminus Y_t) = r(X_c^*) - r(X^*)$, we have, with probability at least $1 - \\delta$,\n\\vspace{-2pt}\n\n", "itemtype": "equation", "pos": 24723, "prevtext": "\nSuppose $X^* \\in [S,T]$, and define $m \\triangleq |[S,T]|$. Obviously, we have $m \\le 2^n$. Hence,\n\n", "index": 27, "text": "\\begin{equation*}\n\\footnotesize\n\\begin{aligned}\n    &{\\operatorname{Pr}}{\\left[ r(X_c^*) - r(X^*) \\ge t \\sqrt{2 N_r \\log{(m/\\delta)}} \\right]} \\\\\n    &= \\hspace{-8pt} \\sum\\limits_{X \\in [S,T]} \\hspace{-5pt} {\\operatorname{Pr}}{\\left[ r(X_c^*) - r(X^*) \\hspace{-2pt} \\ge \\hspace{-2pt} t \\hspace{-1pt} \\sqrt{2 |X_c^* \\triangle X^*| \\log{(\\frac{m}{\\delta})}}, X_c^* \\hspace{-2pt}  = \\hspace{-2pt}  X \\right]} \\\\\n    &= \\hspace{-8pt} \\sum\\limits_{X \\in [S,T]} \\hspace{-5pt} {\\operatorname{Pr}}{\\left[ r(X) - r(X^*) \\hspace{-2pt} \\ge \\hspace{-2pt} t \\sqrt{2 |X \\triangle X^*| \\log{(\\frac{m}{\\delta})}}, X_c^* \\hspace{-2pt} = \\hspace{-2pt} X \\right]} \\\\\n    &\\le \\hspace{-8pt} \\sum\\limits_{X \\in [S,T]} \\hspace{-5pt} {\\operatorname{Pr}}{\\left[ r(X) - r(X^*) \\hspace{-2pt} \\ge \\hspace{-2pt} t \\sqrt{2 |X \\triangle X^*| \\log{(\\frac{m}{\\delta})}} \\right]} \\\\\n    &\\le \\hspace{-8pt} \\sum\\limits_{X \\in [S,T]} \\frac{\\delta}{m} = m \\frac{\\delta}{m} = \\delta,\n\\end{aligned}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\operatorname{Pr}}{\\left[r(X_{c}^{*})-r(X^{*})\\geq t\\sqrt{2N_{r}%&#10;\\log{(m/\\delta)}}\\right]}\" display=\"inline\"><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2265</mo><mrow><mi mathsize=\"80%\">t</mi><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><msub><mi mathsize=\"80%\">N</mi><mi mathsize=\"80%\">r</mi></msub><mo>\u2062</mo><mrow><mi mathsize=\"80%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mi mathsize=\"80%\">m</mi><mo mathsize=\"80%\" stretchy=\"false\">/</mo><mi mathsize=\"80%\">\u03b4</mi></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\hskip-8.0pt\\sum\\limits_{X\\in[S,T]}\\hskip-5.0pt{\\operatorname{Pr%&#10;}}{\\left[r(X_{c}^{*})-r(X^{*})\\hskip-2.0pt\\geq\\hskip-2.0ptt\\hskip-1.0pt\\sqrt{2%&#10;|X_{c}^{*}\\triangle X^{*}|\\log{(\\frac{m}{\\delta})}},X_{c}^{*}\\hskip-2.0pt=%&#10;\\hskip-2.0ptX\\right]}\" display=\"inline\"><mrow><mi/><mo mathsize=\"80%\" rspace=\"0pt\" stretchy=\"false\">=</mo><mrow><mpadded width=\"-5pt\"><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">[</mo><mi mathsize=\"80%\">S</mi><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mi mathsize=\"80%\">T</mi><mo maxsize=\"80%\" minsize=\"80%\">]</mo></mrow></mrow></munder></mstyle></mpadded><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\" rspace=\"0.5pt\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">\u2265</mo><mrow><mpadded width=\"-1pt\"><mi mathsize=\"80%\">t</mi></mpadded><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mrow><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup><mo>\u2062</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u25b3</mi><mo>\u2062</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo></mrow><mo>\u2062</mo><mrow><mi mathsize=\"80%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">m</mi><mi mathsize=\"80%\">\u03b4</mi></mfrac></mstyle><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mrow><mpadded width=\"-2pt\"><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup></mpadded><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">=</mo><mi mathsize=\"80%\">X</mi></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\hskip-8.0pt\\sum\\limits_{X\\in[S,T]}\\hskip-5.0pt{\\operatorname{Pr%&#10;}}{\\left[r(X)-r(X^{*})\\hskip-2.0pt\\geq\\hskip-2.0ptt\\sqrt{2|X\\triangle X^{*}|%&#10;\\log{(\\frac{m}{\\delta})}},X_{c}^{*}\\hskip-2.0pt=\\hskip-2.0ptX\\right]}\" display=\"inline\"><mrow><mi/><mo mathsize=\"80%\" rspace=\"0pt\" stretchy=\"false\">=</mo><mrow><mpadded width=\"-5pt\"><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">[</mo><mi mathsize=\"80%\">S</mi><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mi mathsize=\"80%\">T</mi><mo maxsize=\"80%\" minsize=\"80%\">]</mo></mrow></mrow></munder></mstyle></mpadded><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">X</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\" rspace=\"0.5pt\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">\u2265</mo><mrow><mi mathsize=\"80%\">t</mi><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mrow><mi mathsize=\"80%\">X</mi><mo>\u2062</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u25b3</mi><mo>\u2062</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo></mrow><mo>\u2062</mo><mrow><mi mathsize=\"80%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">m</mi><mi mathsize=\"80%\">\u03b4</mi></mfrac></mstyle><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mrow><mpadded width=\"-2pt\"><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup></mpadded><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">=</mo><mi mathsize=\"80%\">X</mi></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\hskip-8.0pt\\sum\\limits_{X\\in[S,T]}\\hskip-5.0pt{\\operatorname%&#10;{Pr}}{\\left[r(X)-r(X^{*})\\hskip-2.0pt\\geq\\hskip-2.0ptt\\sqrt{2|X\\triangle X^{*}%&#10;|\\log{(\\frac{m}{\\delta})}}\\right]}\" display=\"inline\"><mrow><mi/><mo mathsize=\"80%\" rspace=\"0pt\" stretchy=\"false\">\u2264</mo><mrow><mpadded width=\"-5pt\"><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">[</mo><mi mathsize=\"80%\">S</mi><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mi mathsize=\"80%\">T</mi><mo maxsize=\"80%\" minsize=\"80%\">]</mo></mrow></mrow></munder></mstyle></mpadded><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">X</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\" rspace=\"0.5pt\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">\u2265</mo><mrow><mi mathsize=\"80%\">t</mi><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mrow><mi mathsize=\"80%\">X</mi><mo>\u2062</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u25b3</mi><mo>\u2062</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo></mrow><mo>\u2062</mo><mrow><mi mathsize=\"80%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">m</mi><mi mathsize=\"80%\">\u03b4</mi></mfrac></mstyle><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></msqrt></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex12Xd.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\hskip-8.0pt\\sum\\limits_{X\\in[S,T]}\\frac{\\delta}{m}=m\\frac{%&#10;\\delta}{m}=\\delta,\" display=\"inline\"><mrow><mrow><mi/><mo mathsize=\"80%\" rspace=\"0pt\" stretchy=\"false\">\u2264</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">[</mo><mi mathsize=\"80%\">S</mi><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mi mathsize=\"80%\">T</mi><mo maxsize=\"80%\" minsize=\"80%\">]</mo></mrow></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">\u03b4</mi><mi mathsize=\"80%\">m</mi></mfrac></mstyle></mrow><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mrow><mi mathsize=\"80%\">m</mi><mo>\u2062</mo><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">\u03b4</mi><mi mathsize=\"80%\">m</mi></mfrac></mstyle></mrow><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mi mathsize=\"80%\">\u03b4</mi></mrow><mo mathsize=\"80%\" stretchy=\"false\">,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nUsing a similar method, (\\ref{eq9}) can also be proved.\n\\end{proof}\n\nTheorem \\ref{thm4} has an intuitive interpretation. Take P2 and P4 as examples, $\\forall Y \\subseteq N$, if $f(X^*) - f(Y)$ is large, then it is unlikely that $Y$ is an optimum of P4. Suppose $f(X^*) - f(Y) = \\sigma > 0$, then we have,\n\n", "itemtype": "equation", "pos": 26224, "prevtext": "\nwhere the first equality holds by the law of total probability. The second equality holds because replacing $X_c^*$ with $X$ in the first expression does not change the event. The first inequality comes from dropping the event $X_c^* = X$ increases the probability. The last line results from (\\ref{eq11}) and the definition of $m$.\nCombining the above result with Theorem \\ref{thm2}, and note $r(X_t \\setminus X^*) - r(X^* \\setminus Y_t) = r(X_c^*) - r(X^*)$, we have, with probability at least $1 - \\delta$,\n\\vspace{-2pt}\n\n", "index": 29, "text": "\\begin{equation*}\n\\footnotesize\n    f(X^*) - f(X_p^*) \\hspace{-2pt} < \\hspace{-2pt} r(X_c^*) - r(X^*) \\hspace{-2pt} < \\hspace{-2pt} t \\sqrt{2 N_r (n + \\log{(1/\\delta)})}.\n\\vspace{-2pt}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex13.m1\" class=\"ltx_Math\" alttext=\"\\footnotesize f(X^{*})-f(X_{p}^{*})\\hskip-2.0pt&lt;\\hskip-2.0ptr(X_{c}^{*})-r(X^{%&#10;*})\\hskip-2.0pt&lt;\\hskip-2.0ptt\\sqrt{2N_{r}(n+\\log{(1/\\delta)})}.\\vspace{-2pt}\" display=\"block\"><mrow><mrow><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">p</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup><mo maxsize=\"80%\" minsize=\"80%\" rspace=\"0.5pt\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">&lt;</mo><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msubsup><mi mathsize=\"80%\">X</mi><mi mathsize=\"80%\">c</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msubsup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\" rspace=\"0.5pt\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" rspace=\"0.5pt\" stretchy=\"false\">&lt;</mo><mrow><mi mathsize=\"80%\">t</mi><mo>\u2062</mo><msqrt><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><msub><mi mathsize=\"80%\">N</mi><mi mathsize=\"80%\">r</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mi mathsize=\"80%\">n</mi><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">log</mi><mo>\u2061</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mrow><mn mathsize=\"80%\">1</mn><mo mathsize=\"80%\" stretchy=\"false\">/</mo><mi mathsize=\"80%\">\u03b4</mi></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></msqrt></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nTotally, the probability of $Y$ being an optimum of P4 is upper bounded by the probability that the perturbation difference $r(Y) - r(X^*)$ can compensate the function value difference $\\sigma$, where the later probability is small when $\\sigma$ is large.\n\nFinally, we show that the $M_r$ and $N_r$ in Theorem \\ref{thm4}, which are the numbers of mistakenly reduced elements, can also be upper bounded by functions of $f$ and $t$.\n\n\\begin{thm}\n\\label{thm5}\nDenote the total number of the mistakenly reduced elements in the first iteration of A1 and A2 as $M^1_r$ and $N^1_r$,  respectively.\nWe have,\n{\\footnotesize\n\n", "itemtype": "equation", "pos": 26730, "prevtext": "\nUsing a similar method, (\\ref{eq9}) can also be proved.\n\\end{proof}\n\nTheorem \\ref{thm4} has an intuitive interpretation. Take P2 and P4 as examples, $\\forall Y \\subseteq N$, if $f(X^*) - f(Y)$ is large, then it is unlikely that $Y$ is an optimum of P4. Suppose $f(X^*) - f(Y) = \\sigma > 0$, then we have,\n\n", "index": 31, "text": "\\begin{equation*}\n\\footnotesize\n\\begin{aligned}\n    &{\\operatorname{Pr}}{\\left[f(Y) + r(Y) \\ge f(X) + r(X), \\forall X \\subseteq N\\right]} \\\\\n    &\\le {\\operatorname{Pr}}{\\left[f(Y) + r(Y) \\ge f(X^*) + r(X^*)\\right]} \\\\\n    &= {\\operatorname{Pr}}{\\left[r(Y) - r(X^*) \\ge \\sigma \\right]}.\n\\end{aligned}\n\\vspace{-2pt}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\operatorname{Pr}}{\\left[f(Y)+r(Y)\\geq f(X)+r(X),\\forall X%&#10;\\subseteq N\\right]}\" display=\"inline\"><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">Y</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">Y</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2265</mo><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">X</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">X</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">,</mo><mrow><mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2200</mo><mi mathsize=\"80%\">X</mi></mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2286</mo><mi mathsize=\"80%\">N</mi></mrow><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq{\\operatorname{Pr}}{\\left[f(Y)+r(Y)\\geq f(X^{*})+r(X^{*})%&#10;\\right]}\" display=\"inline\"><mrow><mi/><mo mathsize=\"80%\" stretchy=\"false\">\u2264</mo><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">Y</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">Y</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2265</mo><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex14Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{Pr}}{\\left[r(Y)-r(X^{*})\\geq\\sigma\\right]}.\" display=\"inline\"><mrow><mrow><mi/><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mrow><mo mathsize=\"80%\" stretchy=\"false\">Pr</mo><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">Y</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">r</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">\u2265</mo><mi mathsize=\"80%\">\u03c3</mi></mrow><mo>]</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\n}\nwhere $F \\triangleq \\frac{1}{2}(f(\\emptyset) + f(N)) \\in [f(X_*), f(X^*)]$.\n\\end{thm}\n\\begin{proof}\nFor (\\ref{eq13}), we calculate the total mistakenly reduced element number in expectation in the first iteration of A2. According to the definition of symmetric difference, $N^1_r = |X^* \\setminus Y_1| + |X_1 \\setminus X^*|$.\n\n$\\forall i \\in X^*$, $f(i | \\emptyset) \\ge f(i | X^* - i) = f(X^*) - f(X^* - i) \\ge 0$. And $i \\in X^* \\setminus Y_1$ iff $f(i | \\emptyset) + t < 0$. Similarly, $\\forall j \\not\\in X^*$, $f(j | N - j) \\le f(j | X^*) = f(X^* + j) - f(X^*) \\le 0$. And $j \\in X_1 \\setminus X^*$ iff $f(j | N - j) + t > 0$. Thus we have,\n\n", "itemtype": "equation", "pos": 27676, "prevtext": "\nTotally, the probability of $Y$ being an optimum of P4 is upper bounded by the probability that the perturbation difference $r(Y) - r(X^*)$ can compensate the function value difference $\\sigma$, where the later probability is small when $\\sigma$ is large.\n\nFinally, we show that the $M_r$ and $N_r$ in Theorem \\ref{thm4}, which are the numbers of mistakenly reduced elements, can also be upper bounded by functions of $f$ and $t$.\n\n\\begin{thm}\n\\label{thm5}\nDenote the total number of the mistakenly reduced elements in the first iteration of A1 and A2 as $M^1_r$ and $N^1_r$,  respectively.\nWe have,\n{\\footnotesize\n\n", "index": 33, "text": "\\begin{align}\n\\label{eq12}\n    \\mathbb{E}_r[M^1_r] &\\le \\frac{n}{2} - \\frac{F - f(X_*)}{t}, \\\\\n\\label{eq13}\n    \\mathbb{E}_r[N^1_r] &\\le \\frac{n}{2} - \\frac{f(X^*) - F}{t},\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}_{r}[M^{1}_{r}]\" display=\"inline\"><mrow><msub><mi>\ud835\udd3c</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>M</mi><mi>r</mi><mn>1</mn></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{n}{2}-\\frac{F-f(X_{*})}{t},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>F</mi><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mo>*</mo></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mi>t</mi></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}_{r}[N^{1}_{r}]\" display=\"inline\"><mrow><msub><mi>\ud835\udd3c</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>N</mi><mi>r</mi><mn>1</mn></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{n}{2}-\\frac{f(X^{*})-F}{t},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi>n</mi><mn>2</mn></mfrac></mstyle><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>F</mi></mrow><mi>t</mi></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nFor (\\ref{eq12}), the proof is similar.\n\\end{proof}\n\nUsing similar methods we can obtain the following results, which recover Theorem \\ref{thm5} as a special case.\n\n\\begin{thm}\n\\label{thm6}\nDenote the total number of the mistakenly reduced elements in the $k$th iteration as $M^k_r$, and  $N^k_r$,  respectively. We have,\n\n", "itemtype": "equation", "pos": 28507, "prevtext": "\n}\nwhere $F \\triangleq \\frac{1}{2}(f(\\emptyset) + f(N)) \\in [f(X_*), f(X^*)]$.\n\\end{thm}\n\\begin{proof}\nFor (\\ref{eq13}), we calculate the total mistakenly reduced element number in expectation in the first iteration of A2. According to the definition of symmetric difference, $N^1_r = |X^* \\setminus Y_1| + |X_1 \\setminus X^*|$.\n\n$\\forall i \\in X^*$, $f(i | \\emptyset) \\ge f(i | X^* - i) = f(X^*) - f(X^* - i) \\ge 0$. And $i \\in X^* \\setminus Y_1$ iff $f(i | \\emptyset) + t < 0$. Similarly, $\\forall j \\not\\in X^*$, $f(j | N - j) \\le f(j | X^*) = f(X^* + j) - f(X^*) \\le 0$. And $j \\in X_1 \\setminus X^*$ iff $f(j | N - j) + t > 0$. Thus we have,\n\n", "index": 35, "text": "\\begin{equation*}\n\\footnotesize\n\\begin{aligned}\n    &\\mathbb{E}_r[N^1_r] = \\mathbb{E}_r|X^* \\setminus Y_1| + \\mathbb{E}_r|X_1 \\setminus X^*| \\\\\n    &=\\sum_{i \\in X^*}{\\frac{t - f(i | \\emptyset)}{2t}} + \\sum_{j \\not\\in X^*}{\\frac{t + f(j | N - j)}{2t}} \\\\\n    &=\\frac{n}{2} - \\frac{1}{2t}\\left[ \\sum_{i \\in X^*}{f(i | \\emptyset)} - \\sum_{j \\not\\in X^*}{f(j | N - j)} \\right] \\\\\n    &\\le \\frac{n}{2} - \\frac{f(X^*) - f(\\emptyset) + f(X^*) - f(N)}{2t}.\n\\end{aligned}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}_{r}[N^{1}_{r}]=\\mathbb{E}_{r}|X^{*}\\setminus Y_{1}|+%&#10;\\mathbb{E}_{r}|X_{1}\\setminus X^{*}|\" display=\"inline\"><mrow><mrow><msub><mi mathsize=\"80%\">\ud835\udd3c</mi><mi mathsize=\"80%\">r</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">[</mo><msubsup><mi mathsize=\"80%\">N</mi><mi mathsize=\"80%\">r</mi><mn mathsize=\"80%\">1</mn></msubsup><mo maxsize=\"80%\" minsize=\"80%\">]</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mrow><mrow><msub><mi mathsize=\"80%\">\ud835\udd3c</mi><mi mathsize=\"80%\">r</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mrow><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo mathsize=\"80%\" stretchy=\"false\">\u2216</mo><msub><mi mathsize=\"80%\">Y</mi><mn mathsize=\"80%\">1</mn></msub></mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><msub><mi mathsize=\"80%\">\ud835\udd3c</mi><mi mathsize=\"80%\">r</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mrow><msub><mi mathsize=\"80%\">X</mi><mn mathsize=\"80%\">1</mn></msub><mo mathsize=\"80%\" stretchy=\"false\">\u2216</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow><mo maxsize=\"80%\" minsize=\"80%\">|</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i\\in X^{*}}{\\frac{t-f(i|\\emptyset)}{2t}}+\\sum_{j\\not\\in X^%&#10;{*}}{\\frac{t+f(j|N-j)}{2t}}\" display=\"inline\"><mrow><mi/><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mrow><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">i</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathsize=\"80%\">t</mi><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mi mathsize=\"80%\">f</mi><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">i</mi><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u2205</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mi mathsize=\"80%\">t</mi></mrow></mfrac></mstyle></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">j</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2209</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow></munder></mstyle><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathsize=\"80%\">t</mi><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mi mathsize=\"80%\">f</mi><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">j</mi><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mi mathsize=\"80%\">N</mi><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mi mathsize=\"80%\">j</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mi mathsize=\"80%\">t</mi></mrow></mfrac></mstyle></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{n}{2}-\\frac{1}{2t}\\left[\\sum_{i\\in X^{*}}{f(i|\\emptyset)}-%&#10;\\sum_{j\\not\\in X^{*}}{f(j|N-j)}\\right]\" display=\"inline\"><mrow><mo mathsize=\"80%\" stretchy=\"false\">=</mo><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">n</mi><mn mathsize=\"80%\">2</mn></mfrac></mstyle><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mstyle displaystyle=\"true\"><mfrac><mn mathsize=\"80%\">1</mn><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mi mathsize=\"80%\">t</mi></mrow></mfrac></mstyle><mrow><mo>[</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">i</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2208</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow></munder></mstyle><mi mathsize=\"80%\">f</mi><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">i</mi><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u2205</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" mathsize=\"80%\" movablelimits=\"false\" stretchy=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi mathsize=\"80%\">j</mi><mo mathsize=\"80%\" stretchy=\"false\">\u2209</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup></mrow></munder></mstyle><mi mathsize=\"80%\">f</mi><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">j</mi><mo maxsize=\"80%\" minsize=\"80%\">|</mo><mi mathsize=\"80%\">N</mi><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mi mathsize=\"80%\">j</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow><mo>]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex15Xc.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{n}{2}-\\frac{f(X^{*})-f(\\emptyset)+f(X^{*})-f(N)}{2t}.\" display=\"inline\"><mrow><mrow><mi/><mo mathsize=\"80%\" stretchy=\"false\">\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mi mathsize=\"80%\">n</mi><mn mathsize=\"80%\">2</mn></mfrac></mstyle><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mrow><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\" mathvariant=\"normal\">\u2205</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">+</mo><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><msup><mi mathsize=\"80%\">X</mi><mo mathsize=\"80%\" stretchy=\"false\">*</mo></msup><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">-</mo><mrow><mi mathsize=\"80%\">f</mi><mo>\u2062</mo><mrow><mo maxsize=\"80%\" minsize=\"80%\">(</mo><mi mathsize=\"80%\">N</mi><mo maxsize=\"80%\" minsize=\"80%\">)</mo></mrow></mrow></mrow><mrow><mn mathsize=\"80%\">2</mn><mo>\u2062</mo><mi mathsize=\"80%\">t</mi></mrow></mfrac></mstyle></mrow></mrow><mo mathsize=\"80%\" stretchy=\"false\">.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nwhere $n_{k-1} \\triangleq |Y_{k-1} \\setminus X_{k-1}|$, and $F_{k-1} \\triangleq \\frac{1}{2}(f(X_{k-1}) + f(Y_{k-1})) \\in [f(X_*), f(X^*)]$.\n\\end{thm}\n\nTheorem \\ref{thm5} implies that the expected number of mistakenly reduced elements in the first iteration will approach $\\frac{n}{2}$ as the perturbation scale $t$ increases. This is consistent with the intuition. Let $t \\rightarrow \\infty$, then $g \\rightarrow r$. Each element will be randomly selected to be added or eliminated with probability $\\frac{1}{2}$, so the expected number of mistakenly reduced elements is $\\frac{n}{2}$.\n\n\\begin{remk}\n\\label{remk1}\nWhen $t$ is large enough, most elements will be reduced in the first iteration, i.e., $N_r \\approx N^1_r$. Let $t =  \\frac{2(f(X^*) - F)}{n(1-2\\varepsilon)}$, where $\\varepsilon > 0$, by (\\ref{eq13}) we have $\\mathbb{E}_r[N_r] \\approx \\mathbb{E}_r[N^1_r] \\le \\varepsilon n$, which indicates that if $t = \\frac{2(f(X^*) - F)}{n(1-2\\varepsilon)}$ is a large enough perturbation scale, then the number of  mistakenly reduced elements can be desirably upper bounded.\n\\end{remk}\n\n\\begin{remk}\n\\label{remk2}\nSuppose $t$ is large enough and $N_r \\approx N^1_r$. With the result of Theorem \\ref{thm2}, we have\n\n", "itemtype": "equation", "pos": 29309, "prevtext": "\nFor (\\ref{eq12}), the proof is similar.\n\\end{proof}\n\nUsing similar methods we can obtain the following results, which recover Theorem \\ref{thm5} as a special case.\n\n\\begin{thm}\n\\label{thm6}\nDenote the total number of the mistakenly reduced elements in the $k$th iteration as $M^k_r$, and  $N^k_r$,  respectively. We have,\n\n", "index": 37, "text": "\\begin{align*}\n    \\mathbb{E}_r[M^k_r] &\\le \\frac{n_{k-1}}{2} - \\frac{F_{k-1} - f(X_*)}{t}, \\\\\n    \\mathbb{E}_r[N^k_r] &\\le \\frac{n_{k-1}}{2} - \\frac{f(X^*) - F_{k-1}}{t},\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}_{r}[M^{k}_{r}]\" display=\"inline\"><mrow><msub><mi>\ud835\udd3c</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>M</mi><mi>r</mi><mi>k</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex16.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{n_{k-1}}{2}-\\frac{F_{k-1}-f(X_{*})}{t},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mn>2</mn></mfrac></mstyle><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>F</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mo>*</mo></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mi>t</mi></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathbb{E}_{r}[N^{k}_{r}]\" display=\"inline\"><mrow><msub><mi>\ud835\udd3c</mi><mi>r</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>N</mi><mi>r</mi><mi>k</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex17.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\frac{n_{k-1}}{2}-\\frac{f(X^{*})-F_{k-1}}{t},\" display=\"inline\"><mrow><mrow><mi/><mo>\u2264</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msub><mi>n</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub><mn>2</mn></mfrac></mstyle><mo>-</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><msub><mi>F</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mi>t</mi></mfrac></mstyle></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.00393.tex", "nexttext": "\nLet $t = \\frac{2[(1+\\delta)f(X^*) - F]}{n}$ where $\\delta > 0$, we have $f(X_p^*) > (1-\\delta)f(X^*)$ from above. This means if there is some relationship between the optimum and the perturbation ($t = \\frac{2[(1+\\delta)f(X^*) - F]}{n}$ is a large perturbation scale), then the previous performance loss results can be transformed into approximation ratios.\n\\end{remk}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/rate_max.pdf}\n\\caption{Average Reduction Rates of Maximization}\n\\label{fig1}\n\\end{figure*}\n\n\\section{EXPERIMENTAL RESULTS}\n\\label{sec:results}\nFor reducible submodular functions, by incorporating reduction into optimization methods, favorable performance has been achieved \\cite{fujishige2005submodular,goldengorin2009maximization,iyer2013fast,mei2015unconstrained}. In our experiments, we mainly focus on (nearly) irreducible submodular functions, as listed below.\n\n\\paragraph{Subset Selection Function.}\n\nThe objective function \\cite{lin2009select,iyer2013fast} is irreducible. Given $M \\in \\mathbb{R}_+^{n \\times n}$, $f(X) \\triangleq \\sum_{i \\in N}\\sum_{j \\in X}{M_{ij}} - \\lambda\\sum_{i,j \\in X}{M_{ij}}$, where $\\lambda \\in [0.5,1]$. We set $n = 100$, $\\lambda = 0.7$, and randomly generate symmetric matrix $M$ in $(0,1)^{n \\times n}$, and set $M_{ii} = 1$, $\\forall i \\in N$.\n\n\\paragraph{Mutual Information Function.}\n\nGiven $n$ random vectors $X_1, X_2, \\dots, X_n$, define $h(X)$ as the entropy of random variables $\\{X_i | i \\in N\\}$, which is a highly reducible submodular function. The symmetrization \\cite{bach2013learning} of $h$ leads to the mutual information $f(X) \\triangleq h(X) + h(N \\setminus X)$, which is irreducible. We set $n = 100$, and randomly generate $\\{X_i \\ | \\ i = 1, 2, \\dots, n\\}$.\n\n\\paragraph{Log-Determinant Function.}\n\nGiven a positive definite matrix $K \\in \\mathbb{S}_{++}^n$, the determinant \\cite{kulesza2012determinantal} is log-submodular. The symmetrization of log-determinant is $f(X) \\triangleq \\log{\\det{(K_X)}} + \\log{\\det{(K_{N \\setminus X})}}$, where $K_X \\triangleq [K_{ij}]_{i,j \\in X}$, $\\forall X \\subseteq N$. We set $n = 100$. We randomly generate $n$ data points and compute the $n \\times n$ similarity matrix as the positive definite matrix $K$.\n\n\\paragraph{Negative Half-Products Function.}\n\nThe objective \\cite{boros2002pseudo} is $f(X) \\triangleq c(X) - \\sum_{i,j \\in X, i<j}{a(i)b(j)}$, where $a, b, c$ are non-negative vertors. When $c$ is not non-negative, $f$ can be highly reducible \\cite{mei2015unconstrained}. Here $c$ is non-negative, and $f$ is nearly irreducible. The reduction rate of A1 (A2) is about $1\\%$. We set $n = 100$, and randomly generate $a, b$ in $(0.1,0.5)^n$ and $c$ in $(1,5)^n$.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/max_curve_dca.pdf}\n\\caption{Maximization Results Using Branch-and-Bound Method \\cite{goldengorin1999data} (Exact Solver)}\n\\label{fig2}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{figs/max_curve_rbg.pdf}\n\\caption{Maximization Results Using Random Bi-directional Greedy \\cite{buchbinder2012tight} (Approximate Solver)}\n\\label{fig3}\n\\end{figure*}\n\n\\subsection{Perturbation Scale}\n\nIn Theorem \\ref{thm1}, we lower bound the expectation of reduction rate using the expectation of reduction rate after the first iteration of A1 (A2). It is recently reported that for reducible submodular minimization, this bound is often not tight in practice \\cite{iyer2013fast}. Given that our method is actually transforming irreducible functions to reducible ones, it is reasonable to borrow experience from reducible cases. We conjecture that relatively small reduction rates after the first iteration would be sufficient for high reduction rates after the last iteration, thereby we only need to choose small perturbation scales to obtain desirable reducibility gains.\nWe empirically verify the conjecture as shown in Figure \\ref{fig1}. Appropriate perturbation scales $t$ are chosen so that the reduction rates after the last iteration are changing from $0$ to nearly $1$. Given a certain perturbation scale, we repeatedly generate $r$ for $10$ times and record the average reduction rates of A2 after iteration $1$-$4$ and the last iteration. We observe that A2 terminates within $10$ iterations for all objective functions.\n\nWe defer similar results for minimization to the supplementary material. As conjectured, we learn from Figure \\ref{fig1} that the gap between the average reduction rates after the first iteration and the last iteration is always large in practice. Hence, we can choose $t$ to get appropriate reduction rates in expectation (e.g., $0.3$) after the first iteration, so as to obtain potentially high final reduction rates.\nAlthough we can empirically utilize the gap of reducibility gain to choose relative small perturbation scales, we would like to point out that theoretically determining the reduction rates in expectation after the last iteration given certain perturbation scales is still an open problem.\n\n\\subsection{Optimization Results}\n\nWe implement our method using SFO toolbox \\cite{krause2010sfo}.\n\nFor maximization, we compare A4 with both exact and approximate methods, as exact methods usually cannot terminate in acceptable time with larger input scales. Denote the outputs of the proposed A4 and the existing method as $X^p$ and $X^e$, respectively. Also denote the running time as $T_p$ and $T_e$. We measure the performance loss using \\emph{relative error}, which is defined as $E_r \\triangleq \\frac{|f(X^e)-f(X^p)|}{|f(X^e)|}$. When $X^e$ is exact, $1 - E_r$ is the approximation ratio. We measure the reducibility gain using both the reduction rate and the time ratio $T_p/T_e$. Small time ratios and relative errors indicate large reducibility gains and small performance losses, respectively.\n\nWe employ the branch-and-bound method \\cite{goldengorin1999data} as the exact solver. Since it has exponential time complexity, we reset $n = 20$ so that it terminates within acceptable time. The results are shown in Figure \\ref{fig2}. For comparison, we normalize the perturbation scale as follows. We define $M\\{f,[S,T]\\} \\triangleq \\max_{i \\in T \\setminus S}{\\max\\{f(i|S), -f(i|T-i)\\}}$, and define the \\emph{perturbation scale ratio} as $P(t) \\triangleq \\frac{t-m}{M-m}$. We change the perturbation scale $t$ in $[m,M]$ by varying $P(t)$ in $[0,1]$. We then randomly generate $10$ cases for each objective function and record the average relative errors, average reduction rates, and average time ratios for each perturbation scale ratio.\nFigure \\ref{fig3} shows the results compared with the random bi-directional greedy method \\cite{buchbinder2012tight}, which is used as the approximate solver. Note that $n$ is set to 100. For each case, we firstly run A2 once, and then run the random method $5$ times on both the original and the reduced lattice, and record the best solutions.\n\nAccording to Figure \\ref{fig2} and Figure \\ref{fig3}, when the perturbation scale ratio is smaller than $0.3$, the time ratio is larger than $1$. This is because the small reducibility gain cannot make the combination methods more efficient than before. As the perturbation scale ratio increases, the reduction rate increases and the time ratio decreases as expected. Meanwhile, the relative error increases gently when $P(t)$ increases, indicating that there exist useful intervals, in which the perturbation scales can lead to large reducibility gains and small performance losses.\n\nFor minimization, since the subset selection and the mutual information function have trivial zero optimal values, \\emph{i.e.}, $f(X_*) = f(\\emptyset) = 0$, we use the later two as objective functions. We employ the Fujishige-Wolfe minimum-norm point algorithm \\cite{fujishige2011submodular} as the exact solver. All the settings are the same as those of maximization. The results of minimization are shown in Figure \\ref{fig4}. We note that for negative half-products function, the useful interval of perturbation scales is smaller than those of other functions. According to Remark \\ref{remk2}, as the marginal gains are relatively large compared to the optimal value, it is inappropriate to choose large perturbation scales in this case.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{figs/min_curve.pdf}\n\\caption{Minimization Results}\n\\label{fig4}\n\\end{figure}\n\n\\section{RELATED WORK}\n\\label{sec:related_work}\nIn this section, we review some existing works related to solution space reduction for submodular optimization. For P1, Fujishige \\cite{fujishige2005submodular} firstly proves $\\mathcal{X}_{min} \\subseteq [A,B]$, where $A = \\{ i \\in N \\ | \\ f(i|\\emptyset) < 0 \\}$ and $B = \\{ j \\in N \\ | \\ f(j|N-j) \\le 0\\}$. Note that actually $[A,B] = [X_1,Y_1]$, which is the working lattice of A1 after its first iteration. Recently, Iyer et al. \\cite{iyer2013fast} propose the discrete Majorization-Minimization (MMin) framework for P1. They prove that by choosing appropriate supergradients, MMin is identical with A1. For P2, Goldengorin \\cite{goldengorin2009maximization} proposes the Preliminary Preservation Algorithm (PPA), which is identical with A2. For general cases, Mei et al. \\cite{mei2015unconstrained} prove that the two algorithms work for quasi-submodular functions. Beyond unconstrained problems, for uniform matroid constrained monotone submodular function optimization, Wei et al. \\cite{wei2014fast} propose similar pruning method in which the reduced ground set contains all the original solutions of the \\emph{greedy algorithm}.\n\n\\section{CONCLUSIONS}\n\\label{sec:conclusion}\nIn this paper, we introduce the reducibility of submodularity, which can improve the efficiency of submodular optimization methods. We then propose the perturbation-reduction framework, and demonstrate its advantages theoretically and empirically. We analyze the reducibility gain and performance loss given perturbation scales. Experimental results show that there exists practically useful intervals, and choosing perturbation scales from them enables us to significantly accelerate the existing methods with only small performance loss. For the future work, we would like to study the reducibility of submodular functions in constrained problems.\n\n\\subsubsection*{Acknowledgements}\n\nJincheng Mei would like to thank Csaba Szepesv{\\'a}ri for fixing the proof of Theorem \\ref{thm4}. Bao-Liang Lu was supported by the National Basic Research Program of China (No. 2013CB329401), the National Natural Science Foundation of China (No. 61272248) and the Science and Technology Commission of Shanghai Municipality (No. 13511500200). Asterisk indicates the corresponding author.\n\n{\\small\n\\bibliographystyle{plain}\n\\bibliography{aistats2016bib}\n}\n\n\\newpage\n\\appendix\n\\section*{Appendix}\n\\section{Proof of Proposition \\ref{prop1}}\nFor Algorithm \\ref{algo1}, the proof can be found in \\cite{iyer2013fast}. For Algorithm \\ref{algo2}, the proof can be found in \\cite{goldengorin2009maximization}. A proof using weaker assumption of quasi-submodular function $f$ can be found in \\cite{mei2015unconstrained}. We prove Proposition \\ref{prop1} here for completeness.\n\n\\begin{proof}\n\\textbf{Algorithm \\ref{algo1}}. Obviously $\\mathcal{X}_{min} \\subseteq [X_0,Y_0]$. Suppose $\\mathcal{X}_{min} \\subseteq [X_k,Y_k]$, we now prove $\\mathcal{X}_{min} \\subseteq [X_{k+1},Y_{k+1}]$. Suppose $X_* \\in \\mathcal{X}_{min}$ is a minimum of $f$, then we have $X_k \\subseteq X_* \\subseteq Y_k$. For $\\forall i \\in U_k$, if $i \\not\\in X_*$, by submodularity, we have $f(i|X_*) \\le f(i|X_k) < 0$, \\emph{i.e.}, $f(X_*+i) < f(X_*)$, which contradicts with the optimality of $X_*$. So we have $U_k \\subseteq X_*$, and $X_{k+1} = X_k \\cup U_k \\subseteq X_*$.\n$\\forall j \\in D_k$, if $j \\in X_*$, by submodularity, we have $f(j|X_*-j) \\ge f(j|Y_k-j) > 0$, \\emph{i.e.}, $f(X_*) > f(X_*-j)$, which also contradicts with the optimality of $X_*$. Therefore we have $D_k \\subseteq N \\setminus X_*$, and $X_* \\subseteq Y_{k+1} = Y_k \\setminus D_k$.\n\nNow we have $X_{k+1} \\subseteq X_* \\subseteq Y_{k+1}$. Since $X_*$ can be an arbitrary element of $\\mathcal{X}_{min}$, we have $\\mathcal{X}_{min} \\subseteq [X_{k+1},Y_{k+1}]$.\n\n\\textbf{Algorithm \\ref{algo2}}. Obviously $\\mathcal{X}_{max} \\subseteq [X_0,Y_0]$. Suppose $\\mathcal{X}_{max} \\subseteq [X_k,Y_k]$, we now prove $\\mathcal{X}_{max} \\subseteq [X_{k+1},Y_{k+1}]$. Suppose $X^* \\in \\mathcal{X}_{max}$ is a maximum of $f$, then we have $X_k \\subseteq X^* \\subseteq Y_k$. $\\forall i \\in U_k$, if $i \\in X^*$, by submodularity, we have $f(i|X^*-i) \\le f(i|X_k) < 0$, \\emph{i.e.}, $f(X^*) < f(X^*-i)$, which contradicts with the optimality of $X^*$. So we have $U_k \\subseteq N \\setminus X^*$, and $X^* \\subseteq Y_{k+1} = Y_k \\setminus U_k$.\n$\\forall j \\in D_k$, if $j \\not\\in X^*$, by submodularity, we have $f(j|X^*) \\ge f(j|Y_k-j) > 0$, \\emph{i.e.}, $f(X^*+j) > f(X^*)$, which also contradicts with the optimality of $X^*$. So we have $D_k \\subseteq X^*$, and $X_{k+1} = X_k \\cup D_k \\subseteq X^*$.\n\nNow we have $X_{k+1} \\subseteq X^* \\subseteq Y_{k+1}$. Since $X^*$ can be an arbitrary element of $\\mathcal{X}_{max}$, we have $\\mathcal{X}_{max} \\subseteq [X_{k+1},Y_{k+1}]$.\n\\end{proof}\n\n\\section{Reduction Rate of Algorithm \\ref{algo1}}\n\nFigure \\ref{fig1_supp} shows the reduction rates of Algorithm \\ref{algo1}. All the settings are the same as those of Algorithm \\ref{algo2} in the paper.\n\n\\section{More Experimental Results}\n\n\\subsection{Results of Maximization}\n\nIn the paper we use the random bi-directional greedy method as the approximate solver for maximization. We also report the results of random permutation \\cite{iyer2013fast} and random local search \\cite{iyer2013fast}. The settings are the same as those in the paper. The results are shown in Figure \\ref{fig2_supp} and Figure \\ref{fig3_supp}.\n\n\\subsection{Results Using Real Data}\n\nFinally, we compare the results on real data. The objective function is the log-determinant function. For each test case, we randomly select $100$ samples from the CIFAR dataset \\cite{krizhevsky2009learning}, and then we compute the similarity matrix as the positive definite matrix $K$. Other settings are the same as those in the paper. The results are shown in Figure \\ref{fig4_supp}.\n\nIn Figure \\ref{fig4_supp}, the first three subfigures show the results of maximization using random local search, random permutation, and random bi-directional greedy, respectively. The last subfigure presents the results of minimization using the Fujishige-Wolfe minimum-norm point algorithm \\cite{fujishige2011submodular}.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/rate_min.pdf}\n\\caption{Average Reduction Rates of Minimization}\n\\label{fig1_supp}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/max_curve_rp.pdf}\n\\caption{Maximization Results Using Random Permutation \\cite{iyer2013fast}}\n\\label{fig2_supp}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/max_curve_rls.pdf}\n\\caption{Maximization Results Using Random Local Search \\cite{iyer2013fast}}\n\\label{fig3_supp}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figs/curve_log-det-cifar.pdf}\n\\caption{Results of Log-determinant Function Using CIFAR Dataset}\n\\label{fig4_supp}\n\\end{figure*}\n\n\n\n", "itemtype": "equation", "pos": 30710, "prevtext": "\nwhere $n_{k-1} \\triangleq |Y_{k-1} \\setminus X_{k-1}|$, and $F_{k-1} \\triangleq \\frac{1}{2}(f(X_{k-1}) + f(Y_{k-1})) \\in [f(X_*), f(X^*)]$.\n\\end{thm}\n\nTheorem \\ref{thm5} implies that the expected number of mistakenly reduced elements in the first iteration will approach $\\frac{n}{2}$ as the perturbation scale $t$ increases. This is consistent with the intuition. Let $t \\rightarrow \\infty$, then $g \\rightarrow r$. Each element will be randomly selected to be added or eliminated with probability $\\frac{1}{2}$, so the expected number of mistakenly reduced elements is $\\frac{n}{2}$.\n\n\\begin{remk}\n\\label{remk1}\nWhen $t$ is large enough, most elements will be reduced in the first iteration, i.e., $N_r \\approx N^1_r$. Let $t =  \\frac{2(f(X^*) - F)}{n(1-2\\varepsilon)}$, where $\\varepsilon > 0$, by (\\ref{eq13}) we have $\\mathbb{E}_r[N_r] \\approx \\mathbb{E}_r[N^1_r] \\le \\varepsilon n$, which indicates that if $t = \\frac{2(f(X^*) - F)}{n(1-2\\varepsilon)}$ is a large enough perturbation scale, then the number of  mistakenly reduced elements can be desirably upper bounded.\n\\end{remk}\n\n\\begin{remk}\n\\label{remk2}\nSuppose $t$ is large enough and $N_r \\approx N^1_r$. With the result of Theorem \\ref{thm2}, we have\n\n", "index": 39, "text": "\\begin{equation*}\n    f(X^*) - f(X_p^*) < tN_r \\approx \\frac{nt}{2} - (f(X^*) - F).\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex18.m1\" class=\"ltx_Math\" alttext=\"f(X^{*})-f(X_{p}^{*})&lt;tN_{r}\\approx\\frac{nt}{2}-(f(X^{*})-F).\" display=\"block\"><mrow><mrow><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>X</mi><mi>p</mi><mo>*</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&lt;</mo><mrow><mi>t</mi><mo>\u2062</mo><msub><mi>N</mi><mi>r</mi></msub></mrow><mo>\u2248</mo><mrow><mfrac><mrow><mi>n</mi><mo>\u2062</mo><mi>t</mi></mrow><mn>2</mn></mfrac><mo>-</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>X</mi><mo>*</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mi>F</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]