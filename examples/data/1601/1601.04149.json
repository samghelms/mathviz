[{"file": "1601.04149.tex", "nexttext": "\nwhere $y_t \\in R^m$ is the DCT coefficient block for $x_t$. $\\alpha \\in R^{p_\\Phi}$\nand $\\beta \\in R^{p_\\Psi}$ are sparse codes in the DCT and pixel domains,\nrespectively. $T^{-1}$ denotes the inverse discrete cosine\ntransform (IDCT) operator.  $\\lambda_1$, $\\lambda_2$ and\n$\\lambda_3$ are positive scalars. One noteworthy point is the\ninequality constraint, where $q^L$ and $q^U$ represents the\n(pre-known) quantization intervals according to the JPEG\nquantization table \\cite{JPEG}. The constraint incorporates the\nimportant side information and further confines the solution\nspace. Finally, $\\bm{\\Psi} \\beta$ provides an estimate of the original uncompressed pixel block $\\hat{x}_t$.\n\nSuch a sparsity-based dual-domain model (\\ref{dual}) exploits\nresidual redundancies (e,g, inter-DCT-block correlations) in the\nDCT domain without spreading errors into the pixel domain, and at\nthe same time recovers high-frequency information driven by a\nlarge training set. However, note that the inference process of\n(\\ref{dual}) relies on iterative algorithms, and is computational\nexpensive. Also in (\\ref{dual}), the three parameters $\\lambda_1$,\n$\\lambda_2$ and $\\lambda_3$ have to be manually tuned. The authors\nof \\cite{Liu} simply set them all equal, which may hamper the performance. In\naddition, the dictionaries $\\bm{\\Phi}$ and $\\bm{\\Psi}$ have to be\nindividually learned for each patch, which allows for extra\nflexibility but also brings in heavy computation load.\n\n\n\\subsection{$\\mathbf{D^3}$: A Feed-Forward Network Formulation}\n\n\n\n\n\n\n\n\n\n\nIn training, we have the pixel-domain blocks \\{$x_i$\\} after JPEG\ncompression, as well as their corresponding\noriginal blocks \\{$\\hat{x}_i$\\}. During testing, for an input compressed block $x_t$, our goal is to estimate the original $\\hat{x}_t$, using the redundancies in both DCT and pixel domains, as well as\nJPEG prior knowledge.\n\n\n\n\n\n\n\n\n\nAs illustrated in Fig. \\ref{DDD}, the input $x_t$ is first\ntransformed into its DCT coefficient block $y_t$, by feeding through the\nconstant 2-D DCT matrix layer $T$. The subsequent two layers aim\nto enforce DCT domain sparsity, where we refer to the concepts of\nanalysis and synthesis dictionaries in sparse coding \\cite{Lei}.\nThe Sparse Coding (SC) Analysis Module 1 is implemented to solve\nthe following type of sparse inference problem in the DCT domain\n($\\lambda$ is a positive coefficient):\n\n", "itemtype": "equation", "pos": 9905, "prevtext": "\n\n\n\\title{$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images}\n\n\\author{Zhangyang Wang\\dag, Ding Liu\\dag, Shiyu Chang\\dag, Qing Ling\\ddag, and Thomas S. Huang\\dag\n \\\\\n\\dag Beckman Institute, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA\\\\\n\\ddag Department of Automation, University of Science and Technology of China, Hefei, 230027, China\n\\\\\n{\\tt\\small \\{zwang119, dingliu2, chang87, t-huang1\\}@illinois.edu} $\\qquad$ {\\tt\\small qingling@mail.ustc.edu.cn}\n\n\n\n\n\n}\n\\maketitle\n\n\n\n\n\\begin{abstract}\n\nIn this paper, we design a Deep Dual-Domain ($\\mathbf{D^3}$) based\nfast restoration model to remove artifacts of JPEG compressed\nimages. It leverages the large learning capacity of deep networks,\nas well as the problem-specific expertise that was hardly\nincorporated in the past design of deep architectures. For the\nlatter, we take into consideration both the prior knowledge of the\nJPEG compression scheme, and the successful practice of the\nsparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and light-weighted feed-forward approximation of sparse coding. Extensive experiments\nverify the superiority of the proposed $D^3$ model over several\nstate-of-the-art methods. Specifically, our best model is capable\nof outperforming the latest deep model for around 1 dB in PSNR,\nand is 30 times faster.\n\n\\end{abstract}\n\n\n\\section{Introduction}\n\nIn visual communication and computing systems, the most common\ncause of image degradation is arguably compression. Lossy\ncompression, such as JPEG \\cite{JPEG} and HEVC-MSP \\cite{HEVC}, is\nwidely adopted in image and video codecs for saving both bandwidth\nand in-device storage. It exploits inexact approximations for\nrepresenting the encoded content compactly. Inevitably, it will\nintroduce undesired complex artifacts, such as blockiness, ringing\neffects, and blurs. They are usually caused by the discontinuities\narising from batch-wise processing, the loss of high-frequency\ncomponents by coarse quantization, and so on. These artifacts not\nonly degrade perceptual visual quality, but also adversely affect\nvarious low-level image processing routines that take compressed\nimages as input \\cite{Dong}.\n\nAs practical image compression methods are not information\ntheoretically optimal \\cite{Liu}, the resulting compression code\nstreams still possess residual redundancies, which makes the\nrestoration of the original signals possible. Different from\ngeneral image restoration problems, compression artifact\nrestoration has problem-specific properties that can be utilized\nas powerful priors. For example, JPEG compression first\ndivides an image into 8 $\\times$ 8 pixel blocks, followed by\ndiscrete cosine transformation (DCT) on every block. Quantization\nis applied on the DCT coefficients of every block, with pre-known\nquantization levels \\cite{JPEG}. Moreover, the\ncompression noises are more difficult to model than other common\nnoise types. In contrast to the tradition of assuming noise to be\nwhite and signal independent \\cite{KSVD}, the non-linearity of\nquantization operations makes quantization noises non-stationary\nand signal-dependent.\n\nVarious approaches have been proposed to suppress compression\nartifacts. Early works \\cite{Liu5,Liu16} utilized filtering-based\nmethods to remove simple artifacts. Data-driven methods were then considered to avoid inaccurate\nempirical modeling of compression degradations. Sparsity-based\nimage restoration approaches have been discussed in\n\\cite{chang2014reducing, choi2013learning, Dong12,\n Xianming, rothe2015efficient}\nto produce sharpened images, but they are often accompanied with\nartifacts along edges, and unnatural smooth regions. In \\cite{Liu}, Liu et.al. proposed a sparse coding process carried out jointly in the DCT and pixel\ndomains, to simultaneously exploit residual redundancies of JPEG\ncode streams and sparsity properties of latent images. More recently, Dong et. al. \\cite{Dong} first introduced deep learning\ntechniques \\cite{imagenet} into this problem, by\nspecifically adapting their SR-CNN model in \\cite{SRCNN}. However,\nit does not incorporate much problem-specific prior knowledge.\n\n\n\nThe time constraint is often stringent in image\nor video codec post-processing scenarios. Low-complexity or even\nreal-time attenuation of compression artifacts is highly desirable\n\\cite{shen1999real}. The inference process of traditional\napproaches, for example, sparse coding, usually involves iterative\noptimization algorithms, whose inherently sequential structure as\nwell as the data-dependent complexity and latency often constitute\na major bottleneck in the computational efficiency \\cite{LISTA}.\nDeep networks benefit from the feed-forward structure and enjoy\nmuch faster inference. However, to maintain their competitive performances, deep networks show demands for increased width (numbers of\nfilters) and depth (number of layers), as well as smaller strides,\nall leading to growing computational costs\n\\cite{he2015convolutional}.\n\n\n\n\\begin{figure*}[htbp]\n\\centering\n\\begin{minipage}{0.97\\textwidth}\n\\centering{\n\\includegraphics[width=\\textwidth]{DDD.pdf}\n}\\end{minipage}\n\\caption{The illustration of Deep Dual-Domain ($\\mathbf{D^3}$) based model (all subscripts are omitted for simplicity). The black solid lines denote the network inter-layer connections, while the black dash lines connect to the loss functions. The two red dash-line boxes depict the two stages that incorporate DCT and pixel domain sparsity priors, respectively. The two grey blocks denote constant DCT and IDCT layers, respectively. The notations within parentheses along the pipeline are to remind the corresponding variables in (\\ref{dual}).}\n\\label{DDD}\n\\end{figure*}\n\nIn the paper, we focus on removing artifacts in JPEG compressed\nimages. Our major innovation is to explicitly combine both\n\\textbf{the prior knowledge in the JPEG compression scheme} and\n\\textbf{the successful practice of dual-domain sparse coding}\n\\cite{Liu}, for designing a task-specific deep architecture. Furthermore, we introduce a One-Step Sparse Inference\n\\textbf{(1-SI)} module, that acts as a highly efficient and light-weighted approximation of the sparse coding inference \\cite{Reckless}. 1-SI\nalso reveals important inner connections between sparse coding and\ndeep learning. The proposed model, named Deep Dual-Domain\n($\\mathbf{D^3}$) based fast restoration, proves to be more\neffective and interpretable than general deep models. It gains\nremarkable margins over several state-of-the-art methods, in terms\nof both \\textbf{restoration performance} and \\textbf{time\nefficiency}.\n\n\\section{Related Work}\n\nOur work is inspired by the prior wisdom in \\cite{Liu}. Most\nprevious works restored compressed images in either the pixel\ndomain \\cite{KSVD} or the DCT domain \\cite{JPEG} solely. However,\nan isolated quantization error of one single DCT coefficient is propagated to all pixels of the same block. An aggressively\nquantized DCT coefficient can further produce structured errors in\nthe pixel-domain that correlate to the latent signal. On the other\nhand, the compression process sets most high frequency\ncoefficients to zero, making it impossible to recover details from\nonly the DCT domain. In view of their complementary characteristics, the dual-domain model\nwas proposed in \\cite{Liu}. While the spatial redundancies in the\npixel domain were exploited by a learned dictionary \\cite{KSVD},\nthe residual redundancies in the DCT domain were also utilized to\ndirectly restore DCT coefficients. In this way, quantization noises\nwere suppressed without propagating errors. The final objective\n(see Section 3.1) is a combination of DCT- and pixel-domain sparse\nrepresentations, which could cross validate each other.\n\n\nTo date, deep learning \\cite{imagenet} has shown impressive\nresults on both high-level and low-level vision problems. The\nSR-CNN proposed by Dong et al. \\cite{SRCNN} showed the great\npotential of end-to-end trained networks in image super resolution (SR).\nTheir recent work \\cite{Dong} proposed a four-layer convolutional\nnetwork that was tuned based on SR-CNN, named Artifacts Reduction\nConvolutional Neural Networks (AR-CNN), which was\neffective in dealing with various compression artifacts.\n\n\n\n\n\nIn \\cite{LISTA}, the authors leveraged fast trainable regressors\nand constructed feed-forward network approximations of the learned\nsparse models. By turning sparse coding into deep networks, one\nmay expect faster inference, larger learning capacity, and better\nscalability. Similar views were adopted in\n\\cite{PAMI2015} to develop a fixed-complexity algorithm for solving structured sparse and\nrobust low rank models. The paper \\cite{unfold} summarized the\nmethodology of ``deep unfolding''. Very recently, \\cite{Zhaowen}\nproposed deeply improved sparse coding for SR, which can\nbe incarnated as an end-to-end neural network. Our task-specific\narchitecture shares similar spirits with these works.\n\n\n\n\n\n\n\\section{Deep Dual-Domain ($\\mathbf{D^3}$) based Restoration}\n\n\\subsection{Sparsity-based Dual-Domain Formulation}\n\nWe first review the sparsity-based dual-domain restoration model\nestablished in \\cite{Liu}. Considering a training set of JPEG\ncompressed images, pixel-domain blocks \\{$x_i$\\} $\\in R^m$ (vectorized from a $\\sqrt{m} \\times \\sqrt{m}$ patch; $m$ = 64 for JPEG) are drawn\nfor training, along with their (quantized) DCT coefficient blocks\n\\{$y_i$\\}$\\in R^m$. For each testing input (JPEG-coded) $x_t$ $\\in R^m$, two dictionaries\n$\\bm{\\Phi}\\in R^{m \\times p_\\Phi}$ and $\\bm{\\Psi} \\in R^{m \\times p_\\Psi}$ ($p_\\Phi$ and $p_\\Psi$ denote the dictionary sizes) are constructed from training data\n\\{$y_i$\\} and \\{$x_i$\\}, in the DCT and pixel domains,\nrespectively, via locally adaptive feature selection and\nprojection. The following optimization model is then solved during the testing stage:\n\n", "index": 1, "text": "\\begin{equation}\n\\begin{array}{l}\\label{dual}\n\\min_{\\{\\alpha, \\beta\\}} ||y_t - \\bm{\\Phi} \\alpha||_2^2 + \\lambda_1 ||\\alpha||_1 \\\\   \\qquad  \\quad  +  \\lambda_2 ||T^{-1}  \\bm{\\Phi} \\alpha - \\bm{\\Psi} \\beta||_2^2 + \\lambda_3 ||\\beta||_1,\\\\\n \\qquad \\quad s.t.  \\quad q^L \\preceq \\bm{\\Phi} \\alpha \\preceq\n q^U.\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\min_{\\{\\alpha,\\beta\\}}||y_{t}-\\bm{\\Phi}\\alpha||_{2}^{2}+%&#10;\\lambda_{1}||\\alpha||_{1}\\\\&#10;\\qquad\\quad+\\lambda_{2}||T^{-1}\\bm{\\Phi}\\alpha-\\bm{\\Psi}\\beta||_{2}^{2}+%&#10;\\lambda_{3}||\\beta||_{1},\\\\&#10;\\qquad\\quad s.t.\\quad q^{L}\\preceq\\bm{\\Phi}\\alpha\\preceq q^{U}.\\end{array}\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><munder><mi>min</mi><mrow><mo stretchy=\"false\">{</mo><mi>\u03b1</mi><mo>,</mo><mi>\u03b2</mi><mo stretchy=\"false\">}</mo></mrow></munder><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\u03b1</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mo lspace=\"32.5pt\">+</mo><mrow><msub><mi>\u03bb</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mi>T</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>-</mo><mrow><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\u03b2</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mn>3</mn></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\u03b2</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mpadded lspace=\"30pt\" width=\"+30pt\"><mi>s</mi></mpadded><mo>.</mo><mi>t</mi><mo>.</mo><mo separator=\"true\">\u2003</mo><msup><mi>q</mi><mi>L</mi></msup><mo>\u2aaf</mo><mi>\ud835\udebd</mi><mi>\u03b1</mi><mo>\u2aaf</mo><msup><mi>q</mi><mi>U</mi></msup><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nThe Sparse Coding (SC) Synthesis Module 1 outputs the\nDCT-domain sparsity-based reconstruction in (\\ref{dual}), i.e.,\n$\\bm{\\Phi} \\alpha$.\n\nThe intermediate output $\\bm{\\Phi} \\alpha$ is further constrained by\nan auxiliary loss, which encodes the inequality constraint in\n(\\ref{dual}): $ q^L \\preceq \\bm{\\Phi} \\alpha \\preceq q^U$. We\ndesign the following \\textbf{signal-dependent, box-constrained\n\\cite{box} loss}:\n\n", "itemtype": "equation", "pos": 12620, "prevtext": "\nwhere $y_t \\in R^m$ is the DCT coefficient block for $x_t$. $\\alpha \\in R^{p_\\Phi}$\nand $\\beta \\in R^{p_\\Psi}$ are sparse codes in the DCT and pixel domains,\nrespectively. $T^{-1}$ denotes the inverse discrete cosine\ntransform (IDCT) operator.  $\\lambda_1$, $\\lambda_2$ and\n$\\lambda_3$ are positive scalars. One noteworthy point is the\ninequality constraint, where $q^L$ and $q^U$ represents the\n(pre-known) quantization intervals according to the JPEG\nquantization table \\cite{JPEG}. The constraint incorporates the\nimportant side information and further confines the solution\nspace. Finally, $\\bm{\\Psi} \\beta$ provides an estimate of the original uncompressed pixel block $\\hat{x}_t$.\n\nSuch a sparsity-based dual-domain model (\\ref{dual}) exploits\nresidual redundancies (e,g, inter-DCT-block correlations) in the\nDCT domain without spreading errors into the pixel domain, and at\nthe same time recovers high-frequency information driven by a\nlarge training set. However, note that the inference process of\n(\\ref{dual}) relies on iterative algorithms, and is computational\nexpensive. Also in (\\ref{dual}), the three parameters $\\lambda_1$,\n$\\lambda_2$ and $\\lambda_3$ have to be manually tuned. The authors\nof \\cite{Liu} simply set them all equal, which may hamper the performance. In\naddition, the dictionaries $\\bm{\\Phi}$ and $\\bm{\\Psi}$ have to be\nindividually learned for each patch, which allows for extra\nflexibility but also brings in heavy computation load.\n\n\n\\subsection{$\\mathbf{D^3}$: A Feed-Forward Network Formulation}\n\n\n\n\n\n\n\n\n\n\nIn training, we have the pixel-domain blocks \\{$x_i$\\} after JPEG\ncompression, as well as their corresponding\noriginal blocks \\{$\\hat{x}_i$\\}. During testing, for an input compressed block $x_t$, our goal is to estimate the original $\\hat{x}_t$, using the redundancies in both DCT and pixel domains, as well as\nJPEG prior knowledge.\n\n\n\n\n\n\n\n\n\nAs illustrated in Fig. \\ref{DDD}, the input $x_t$ is first\ntransformed into its DCT coefficient block $y_t$, by feeding through the\nconstant 2-D DCT matrix layer $T$. The subsequent two layers aim\nto enforce DCT domain sparsity, where we refer to the concepts of\nanalysis and synthesis dictionaries in sparse coding \\cite{Lei}.\nThe Sparse Coding (SC) Analysis Module 1 is implemented to solve\nthe following type of sparse inference problem in the DCT domain\n($\\lambda$ is a positive coefficient):\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{array}{l}\\label{alpha}\n\\min_\\alpha \\frac{1}{2}||y_t - \\bm{\\Phi} \\alpha||_2^2 + \\lambda ||\\alpha||_1.\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\min_{\\alpha}\\frac{1}{2}||y_{t}-\\bm{\\Phi}\\alpha||_{2}^{2}+%&#10;\\lambda||\\alpha||_{1}.\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>\u03b1</mi></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\u03b1</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nNote it takes not only $\\bm{\\Phi} \\alpha$, but also $x$ as inputs, since the actual JPEG quantization interval [$q^L$, $q^U$] depends on $x$. The operator $[\\quad]_+$ keeps the nonnegative elements unchanged while setting others to zero. Eqn. (\\ref{box}) will thus only penalize the coefficients falling out of the quantization interval.\n\n\nAfter passing through the constant IDCT matrix layer $T^{-1}$, the\nDCT-domain reconstruction $\\bm{\\Phi} \\alpha$ is transformed back\nto the pixel domain for one more sparse representation. The SC\nAnalysis Module 2 solves ($\\gamma$ is a positive coefficient):\n\n", "itemtype": "equation", "pos": 13185, "prevtext": "\nThe Sparse Coding (SC) Synthesis Module 1 outputs the\nDCT-domain sparsity-based reconstruction in (\\ref{dual}), i.e.,\n$\\bm{\\Phi} \\alpha$.\n\nThe intermediate output $\\bm{\\Phi} \\alpha$ is further constrained by\nan auxiliary loss, which encodes the inequality constraint in\n(\\ref{dual}): $ q^L \\preceq \\bm{\\Phi} \\alpha \\preceq q^U$. We\ndesign the following \\textbf{signal-dependent, box-constrained\n\\cite{box} loss}:\n\n", "index": 5, "text": "\\begin{equation}\n\\begin{array}{l}\\label{box}\nL_B(\\bm{\\Phi} \\alpha, x) = ||[\\bm{\\Phi} \\alpha - q^U(x)]_+||_2^2 + ||[q^L(x) - \\bm{\\Phi} \\alpha]_+||_2^2.\n\n\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}L_{B}(\\bm{\\Phi}\\alpha,x)=||[\\bm{\\Phi}\\alpha-q^{U}(x)]_{+}||_%&#10;{2}^{2}+||[q^{L}(x)-\\bm{\\Phi}\\alpha]_{+}||_{2}^{2}.\\par&#10;\\par&#10;\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><msub><mi>L</mi><mi>B</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>-</mo><mrow><msup><mi>q</mi><mi>U</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mo>+</mo></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mrow><mo fence=\"true\">||</mo><msub><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><msup><mi>q</mi><mi>L</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mo>+</mo></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nwhile the SC Synthesis Module 2 produces the final pixel-domain\nreconstruction $\\bm{\\Psi} \\beta$. Finally, the $L_2$ loss between $\\bm{\\Psi}\n\\beta$ and $\\hat{x}_i$ is enforced.\n\nNote that in the above, we try to correspond the intermediate\noutputs of $\\mathbf{D^3}$ with the variables in\n(\\ref{dual}), in order to help understand the close analytical\nrelationship between the proposed deep architecture with the sparse\ncoding-based model. That does not necessarily imply any exact\nnumerical equivalence, since $\\mathbf{D^3}$ allows for end-to-end\nlearning of all parameters (including $\\lambda$ in (\\ref{alpha})\nand $\\gamma$ in (\\ref{beta})). However, we will see in experiments\nthat such enforcement of the specific problem structure improves\nthe network performance and efficiency remarkably. In addition,\nthe above relationships remind us that the deep model could be well initialized from the sparse coding components.\n\n\\subsection{One-Step Sparse Inference Module}\n\n\n\nThe implementation of SC Analysis and Synthesis Modules appears to be the core of $\\mathbf{D^3}$. While the synthesis process is naturally feed-forward by multiplying the dictionary, it is less straightforward to transform the sparse analysis (or inference) process into a feed-forward network. \n\nWe take (\\ref{alpha}) as an example, while the same solution applies\nto (\\ref{beta}). Such a sparse inference problem could be solved\nby the iterative shrinkage and thresholding algorithm (ISTA)\n\\cite{iterative}, each iteration of which updates as follows:\n\n", "itemtype": "equation", "pos": 13963, "prevtext": "\nNote it takes not only $\\bm{\\Phi} \\alpha$, but also $x$ as inputs, since the actual JPEG quantization interval [$q^L$, $q^U$] depends on $x$. The operator $[\\quad]_+$ keeps the nonnegative elements unchanged while setting others to zero. Eqn. (\\ref{box}) will thus only penalize the coefficients falling out of the quantization interval.\n\n\nAfter passing through the constant IDCT matrix layer $T^{-1}$, the\nDCT-domain reconstruction $\\bm{\\Phi} \\alpha$ is transformed back\nto the pixel domain for one more sparse representation. The SC\nAnalysis Module 2 solves ($\\gamma$ is a positive coefficient):\n\n", "index": 7, "text": "\\begin{equation}\n\\begin{array}{l}\\label{beta}\n\\min_\\beta \\frac{1}{2}||T^{-1}  \\bm{\\Phi} \\alpha - \\bm{\\Psi} \\beta||_2^2 + \\gamma ||\\beta||_1,\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\min_{\\beta}\\frac{1}{2}||T^{-1}\\bm{\\Phi}\\alpha-\\bm{\\Psi}%&#10;\\beta||_{2}^{2}+\\gamma||\\beta||_{1},\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mi>\u03b2</mi></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mi>T</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udebd</mi><mo>\u2062</mo><mi>\u03b1</mi></mrow><mo>-</mo><mrow><mi>\ud835\udebf</mi><mo>\u2062</mo><mi>\u03b2</mi></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mi>\u03b2</mi><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nwhere $\\bm{\\alpha}^k$ denotes the intermediate result of the $k$-th iteration, and where $s_{\\lambda}$ is an element-wise shrinkage function ($\\mathbf{u}$ is a vector and $\\mathbf{u}_i$ is its $i$-th element, $i = 1, 2, ..., p$):\n\n", "itemtype": "equation", "pos": 15658, "prevtext": "\nwhile the SC Synthesis Module 2 produces the final pixel-domain\nreconstruction $\\bm{\\Psi} \\beta$. Finally, the $L_2$ loss between $\\bm{\\Psi}\n\\beta$ and $\\hat{x}_i$ is enforced.\n\nNote that in the above, we try to correspond the intermediate\noutputs of $\\mathbf{D^3}$ with the variables in\n(\\ref{dual}), in order to help understand the close analytical\nrelationship between the proposed deep architecture with the sparse\ncoding-based model. That does not necessarily imply any exact\nnumerical equivalence, since $\\mathbf{D^3}$ allows for end-to-end\nlearning of all parameters (including $\\lambda$ in (\\ref{alpha})\nand $\\gamma$ in (\\ref{beta})). However, we will see in experiments\nthat such enforcement of the specific problem structure improves\nthe network performance and efficiency remarkably. In addition,\nthe above relationships remind us that the deep model could be well initialized from the sparse coding components.\n\n\\subsection{One-Step Sparse Inference Module}\n\n\n\nThe implementation of SC Analysis and Synthesis Modules appears to be the core of $\\mathbf{D^3}$. While the synthesis process is naturally feed-forward by multiplying the dictionary, it is less straightforward to transform the sparse analysis (or inference) process into a feed-forward network. \n\nWe take (\\ref{alpha}) as an example, while the same solution applies\nto (\\ref{beta}). Such a sparse inference problem could be solved\nby the iterative shrinkage and thresholding algorithm (ISTA)\n\\cite{iterative}, each iteration of which updates as follows:\n\n", "index": 9, "text": "\\begin{equation}\n\\begin{array}{l}\\label{ISTA}\n\\bm{\\alpha}^{k+1} = s_{\\lambda}(\\bm{\\alpha}^k + \\bm{\\Phi}^T (y_t - \\bm{\\Phi} \\bm{\\alpha}^k)),\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\bm{\\alpha}^{k+1}=s_{\\lambda}(\\bm{\\alpha}^{k}+\\bm{\\Phi}^{T}(%&#10;y_{t}-\\bm{\\Phi}\\bm{\\alpha}^{k})),\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msup><mi>\ud835\udf36</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><msub><mi>s</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udf36</mi><mi>k</mi></msup><mo>+</mo><mrow><msup><mi>\ud835\udebd</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>-</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><msup><mi>\ud835\udf36</mi><mi>k</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nThe learned ISTA (LISTA) \\cite{LISTA} parameterized encoder\nfurther proposed a natural network implementation of ISTA. The\nauthors time-unfolded and truncated (\\ref{ISTA}) into a fixed\nnumber of stages (more than 2), and then jointly tuned all\nparameters with training data, for a good feed-forward\napproximation of sparse inference. The similar unfolding\nmethodology has been lately exploited in \\cite{unfold},\n\\cite{PAMI2015}, \\cite{sprechmann2013supervised}.\n\nIn our work, we launch a more aggressive approximation, by only\nkeeping one iteration of (\\ref{ISTA}), leading to a One-Step\nSparse Inference (\\textbf{1-SI}) Module. Our major motivation lies\nin the same observation as in \\cite{Dong} that overly deep\nnetworks could adversely affect the performance in low-level\nvision tasks. Note that we have two SC Analysis modules where the\noriginal LISTA applies, and two more SC Synthesis modules (each\nwith one learnable layer). Even only two iterations are kept as in\n\\cite{LISTA}, we end up with a six-layer network, that suffers\nfrom both difficulties in training \\cite{Dong} and fragility in\ngeneralization \\cite{srivastava2014dropout} for this task.\n\nA 1-SI module takes the following simplest form:\n\n", "itemtype": "equation", "pos": 16055, "prevtext": "\nwhere $\\bm{\\alpha}^k$ denotes the intermediate result of the $k$-th iteration, and where $s_{\\lambda}$ is an element-wise shrinkage function ($\\mathbf{u}$ is a vector and $\\mathbf{u}_i$ is its $i$-th element, $i = 1, 2, ..., p$):\n\n", "index": 11, "text": "\\begin{equation}\n\\begin{array}{l}\\label{threshold}\n[s_{\\lambda}(\\mathbf{u})]_i = \\text{sign}(\\mathbf{u}_i)[|\\mathbf{u}_i| - \\lambda_i]_{+}.\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}[s_{\\lambda}(\\mathbf{u})]_{i}=\\text{sign}(\\mathbf{u}_{i})[|%&#10;\\mathbf{u}_{i}|-\\lambda_{i}]_{+}.\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>s</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mi>i</mi></msub><mo>=</mo><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>+</mo></msub></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nwhich could be viewed as first passing through a fully-connected\nlayer ($\\bm{\\Phi}$), followed by neurons that take the form of\n$s_{\\lambda}$. We further rewrite (\\ref{threshold}) as\n\\cite{Zhaowen} did\\footnote{In (\\ref{threshold1}), we slightly\nabuse notations, and set $\\lambda$ to be a vector of the same\ndimension as $\\mathbf{u}$, in order for extra element-wise\nflexibility.}:\n\n", "itemtype": "equation", "pos": 17430, "prevtext": "\nThe learned ISTA (LISTA) \\cite{LISTA} parameterized encoder\nfurther proposed a natural network implementation of ISTA. The\nauthors time-unfolded and truncated (\\ref{ISTA}) into a fixed\nnumber of stages (more than 2), and then jointly tuned all\nparameters with training data, for a good feed-forward\napproximation of sparse inference. The similar unfolding\nmethodology has been lately exploited in \\cite{unfold},\n\\cite{PAMI2015}, \\cite{sprechmann2013supervised}.\n\nIn our work, we launch a more aggressive approximation, by only\nkeeping one iteration of (\\ref{ISTA}), leading to a One-Step\nSparse Inference (\\textbf{1-SI}) Module. Our major motivation lies\nin the same observation as in \\cite{Dong} that overly deep\nnetworks could adversely affect the performance in low-level\nvision tasks. Note that we have two SC Analysis modules where the\noriginal LISTA applies, and two more SC Synthesis modules (each\nwith one learnable layer). Even only two iterations are kept as in\n\\cite{LISTA}, we end up with a six-layer network, that suffers\nfrom both difficulties in training \\cite{Dong} and fragility in\ngeneralization \\cite{srivastava2014dropout} for this task.\n\nA 1-SI module takes the following simplest form:\n\n", "index": 13, "text": "\\begin{equation}\n\\begin{array}{l}\\label{1step}\n\\bm{\\alpha}= s_{\\lambda}(\\bm{\\Phi} y_t),\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\bm{\\alpha}=s_{\\lambda}(\\bm{\\Phi}y_{t}),\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi>\ud835\udf36</mi><mo>=</mo><mrow><msub><mi>s</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nEqn. (\\ref{threshold1}) indicates that the original neuron with\ntrainable thresholds can be decomposed into two linear scaling\nlayers plus a unit-threshold neuron. The weights of the two\nscaling layers are diagonal matrices defined by $\\bm{\\theta}$ and\nits element-wise reciprocal, respectively. The unit-threshold\nneuron $s_1$ could in essence be viewed as a double-sided and\ntranslated variant of ReLU \\cite{imagenet}.\n\n\nA related form to (\\ref{1step}) was obtained in \\cite{Reckless} on\na different case of non-negative sparse coding. The authors\nstudied its connections with the soft-threshold feature for\nclassification, but did not correlate it with network architectures.\n\n\n\n\n\\subsection{Model Overview}\n\nBy plugging in the 1-SI module (\\ref{1step}), we are ready to\nobtain the SC Analysis and Synthesis Modules, as in Fig.\n\\ref{1SI}. By comparing Fig. \\ref{1SI} with Eqn. (\\ref{alpha}) (or\n(\\ref{beta})), it is easy to notice the analytical relationships\nbetween $D_A$ and $\\bm{\\Phi}^T$ (or $\\bm{\\Psi}^T$), $D_S$ and\n$\\bm{\\Phi}$ (or $\\bm{\\Psi}$), as well as $\\theta$ and $\\lambda$\n(or $\\gamma$). In fact, those network hyperparamters could be well\ninitialized from the sparse coding parameters, which\ncould be obtained easily. The entire $D^3$ model, consisting\nof four learnable fully-connected weight layers (except for the\ndiagonal layers), are then trained from end to end \\footnote{From\nthe analytical perspective, $\\mathbf{D}_S$ is the transpose of\n$\\mathbf{D}_A$, but we untie them during training for larger\nlearning capability.}.\n\\begin{figure}[htbp]\n\\centering\n\\begin{minipage}{0.45\\textwidth}\n\\centering{\n\\includegraphics[width=\\textwidth]{1SI.pdf}\n}\\end{minipage}\n\\caption{The illustration of SC Analysis and Synthesis Modules. The former is implemented by the proposed 1-SI module (\\ref{1step}). Both $D_A$ and $D_S$ are fully-connected layers, while diag($\\theta$) and diag($1/\\theta$) denotes the two diagonal scaling layers.}\n\\label{1SI}\n\\end{figure}\n\nIn Fig. \\ref{1SI}, we intentionally do not combine $\\theta$ into\n$\\mathbf{D}_A$ layer (also $1/\\theta$ into $\\mathbf{D}_S$ layer ),\nfor the reason that we still wish to keep $\\theta$ and $1/\\theta$\nlayers tied as element-wise reciprocal. That proves to have\npositive implications in our experiments. If we absorb the two\ndiagonal layers into $\\mathbf{D}_A$ and $\\mathbf{D}_S$, Fig.\n\\ref{1SI} is reduced to two fully connected weight matrices,\nconcatenated by one layer of hidden neurons (\\ref{threshold1}).\nHowever, keeping the ``decomposed'' model architecture facilitates\nthe incorporation of problem-specific structures.\n\n\n\n\n\n\n\n\\subsection{Complexity Analysis}\n\n\n\nFrom the clear correspondences between the sparsity-based\nformulation and the $D^3$ model, we immediately derive the dimensions of weight layers, as in Table\n\\ref{dim}.\n\n \\begin{table}[htbp]\n  \n \\begin{center}\n \\caption{Dimensions of all layers in the $D^3$ model}\n \\label{dim}\n \\vspace{-1em}\n \\begin{tabular}{|c|c|c|c|}\n \\hline\nLayer & $\\mathbf{D}_A$ &  $\\mathbf{D}_S$ &  diag($\\theta$) \\\\\n\\hline\nStage I (DCT Domain) & $p_\\Phi \\times m$ & $m \\times p_\\Phi$ & $p_\\Phi$ \\\\\n\\hline\nStage II (Pixel Domain) & $p_\\Psi \\times m$ & $m \\times p_\\Psi$ & $p_\\Psi$ \\\\\n\\hline\n \\end{tabular}\n \\end{center}\n \\end{table}\n\n\\begin{figure*}[tbp]\n\\centering\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Original] {\n\\includegraphics[width=\\textwidth]{ori_bike.pdf}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Compressed (PSNR = 21.72 dB)] {\n\\includegraphics[width=\\textwidth]{Compressed_bike.jpg}\n}\\\\\n\\centering \\subfigure[S-D$^2$ (PSNR = 22.87 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q10.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[AR-CNN (PSNR = 23.27 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q5.png}\n}\\\\\n\\centering \\subfigure[D$^3$-128 (PSNR = 23.94 dB)] {\n\\includegraphics[width=\\textwidth]{D-128-bike.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[D$^3$-256 (PSNR = 24.30 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q20.png}\n}\\\\\n\\centering \\subfigure[D-Base-256 (PSNR = 23.48 dB)] {\n\\includegraphics[width=\\textwidth]{D-base-256-bike.png}\n}\\end{minipage}\n\\caption{Visual comparison of various methods on \\textit{Bike} at Q = 5. The corresponding PSNR values (in dB) are also shown.}\n\\label{bike}\n\\end{figure*}\n\n\\subsubsection{Time Complexity}\n\nDuring training, deep learning with the aid of gradient descent\nscales linearly in time and space with the number of train\nsamples. We are primarily concerned with the time complexity\nduring testing (inference), which is more relevant to practical\nusages. Since all learnable layers in the $D^3$ model are\nfully-connected, the inference process of $D^3$ is nothing more\nthan a series of matrix multiplications. The multiplication times\nare counted as: $p_\\Phi m$ ($D_A$ in Stage I) + $2p_\\Phi$ (two\ndiagonal layers) + $p_\\Phi m$ ($D_S$ in Stage I)  + $p_\\Psi m$\n($D_A$ in Stage II) + $2p_\\Psi$ (two diagonal layers) + $p_\\Psi m$\n($D_S$ in Stage II). The 2D DCT and IDCT each takes $\\frac{1}{2}\nm\\log(m)$ multiplications \\cite{JPEG} . Therefore, the total\ninference time complexity of $D^3$ is:\n\n\n", "itemtype": "equation", "pos": 17927, "prevtext": "\nwhich could be viewed as first passing through a fully-connected\nlayer ($\\bm{\\Phi}$), followed by neurons that take the form of\n$s_{\\lambda}$. We further rewrite (\\ref{threshold}) as\n\\cite{Zhaowen} did\\footnote{In (\\ref{threshold1}), we slightly\nabuse notations, and set $\\lambda$ to be a vector of the same\ndimension as $\\mathbf{u}$, in order for extra element-wise\nflexibility.}:\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{array}{l}\\label{threshold1}\n[s_{\\lambda}(\\mathbf{u})]_i = \\lambda_i \\cdot \\text{sign}(\\mathbf{u}_i)(|\\mathbf{u}_i|/\\lambda_i - 1)_{+} = \\lambda_i s_1(\\mathbf{u}_i/\\lambda_i)\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}[s_{\\lambda}(\\mathbf{u})]_{i}=\\lambda_{i}\\cdot\\text{sign}(%&#10;\\mathbf{u}_{i})(|\\mathbf{u}_{i}|/\\lambda_{i}-1)_{+}=\\lambda_{i}s_{1}(\\mathbf{u%&#10;}_{i}/\\lambda_{i})\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><msub><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>s</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u22c5</mo><mtext>sign</mtext></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>/</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></msub></mrow><mo>=</mo><mrow><msub><mi>\u03bb</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>s</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo>/</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nThe complexity could also be expressed as $O(p_\\Phi+ p_\\Psi)$.\n\nIt is obvious that the sparse coding inference \\cite{Liu} has\ndramatically higher time complexity. We are also interested in the\ninference time complexity of other competitive deep models,\nespecially AR-CNN \\cite{Dong}. For their fully convolutional\narchitecture, the total complexity \\cite{he2015convolutional} is:\n\n", "itemtype": "equation", "pos": 23386, "prevtext": "\nEqn. (\\ref{threshold1}) indicates that the original neuron with\ntrainable thresholds can be decomposed into two linear scaling\nlayers plus a unit-threshold neuron. The weights of the two\nscaling layers are diagonal matrices defined by $\\bm{\\theta}$ and\nits element-wise reciprocal, respectively. The unit-threshold\nneuron $s_1$ could in essence be viewed as a double-sided and\ntranslated variant of ReLU \\cite{imagenet}.\n\n\nA related form to (\\ref{1step}) was obtained in \\cite{Reckless} on\na different case of non-negative sparse coding. The authors\nstudied its connections with the soft-threshold feature for\nclassification, but did not correlate it with network architectures.\n\n\n\n\n\\subsection{Model Overview}\n\nBy plugging in the 1-SI module (\\ref{1step}), we are ready to\nobtain the SC Analysis and Synthesis Modules, as in Fig.\n\\ref{1SI}. By comparing Fig. \\ref{1SI} with Eqn. (\\ref{alpha}) (or\n(\\ref{beta})), it is easy to notice the analytical relationships\nbetween $D_A$ and $\\bm{\\Phi}^T$ (or $\\bm{\\Psi}^T$), $D_S$ and\n$\\bm{\\Phi}$ (or $\\bm{\\Psi}$), as well as $\\theta$ and $\\lambda$\n(or $\\gamma$). In fact, those network hyperparamters could be well\ninitialized from the sparse coding parameters, which\ncould be obtained easily. The entire $D^3$ model, consisting\nof four learnable fully-connected weight layers (except for the\ndiagonal layers), are then trained from end to end \\footnote{From\nthe analytical perspective, $\\mathbf{D}_S$ is the transpose of\n$\\mathbf{D}_A$, but we untie them during training for larger\nlearning capability.}.\n\\begin{figure}[htbp]\n\\centering\n\\begin{minipage}{0.45\\textwidth}\n\\centering{\n\\includegraphics[width=\\textwidth]{1SI.pdf}\n}\\end{minipage}\n\\caption{The illustration of SC Analysis and Synthesis Modules. The former is implemented by the proposed 1-SI module (\\ref{1step}). Both $D_A$ and $D_S$ are fully-connected layers, while diag($\\theta$) and diag($1/\\theta$) denotes the two diagonal scaling layers.}\n\\label{1SI}\n\\end{figure}\n\nIn Fig. \\ref{1SI}, we intentionally do not combine $\\theta$ into\n$\\mathbf{D}_A$ layer (also $1/\\theta$ into $\\mathbf{D}_S$ layer ),\nfor the reason that we still wish to keep $\\theta$ and $1/\\theta$\nlayers tied as element-wise reciprocal. That proves to have\npositive implications in our experiments. If we absorb the two\ndiagonal layers into $\\mathbf{D}_A$ and $\\mathbf{D}_S$, Fig.\n\\ref{1SI} is reduced to two fully connected weight matrices,\nconcatenated by one layer of hidden neurons (\\ref{threshold1}).\nHowever, keeping the ``decomposed'' model architecture facilitates\nthe incorporation of problem-specific structures.\n\n\n\n\n\n\n\n\\subsection{Complexity Analysis}\n\n\n\nFrom the clear correspondences between the sparsity-based\nformulation and the $D^3$ model, we immediately derive the dimensions of weight layers, as in Table\n\\ref{dim}.\n\n \\begin{table}[htbp]\n  \n \\begin{center}\n \\caption{Dimensions of all layers in the $D^3$ model}\n \\label{dim}\n \\vspace{-1em}\n \\begin{tabular}{|c|c|c|c|}\n \\hline\nLayer & $\\mathbf{D}_A$ &  $\\mathbf{D}_S$ &  diag($\\theta$) \\\\\n\\hline\nStage I (DCT Domain) & $p_\\Phi \\times m$ & $m \\times p_\\Phi$ & $p_\\Phi$ \\\\\n\\hline\nStage II (Pixel Domain) & $p_\\Psi \\times m$ & $m \\times p_\\Psi$ & $p_\\Psi$ \\\\\n\\hline\n \\end{tabular}\n \\end{center}\n \\end{table}\n\n\\begin{figure*}[tbp]\n\\centering\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Original] {\n\\includegraphics[width=\\textwidth]{ori_bike.pdf}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Compressed (PSNR = 21.72 dB)] {\n\\includegraphics[width=\\textwidth]{Compressed_bike.jpg}\n}\\\\\n\\centering \\subfigure[S-D$^2$ (PSNR = 22.87 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q10.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[AR-CNN (PSNR = 23.27 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q5.png}\n}\\\\\n\\centering \\subfigure[D$^3$-128 (PSNR = 23.94 dB)] {\n\\includegraphics[width=\\textwidth]{D-128-bike.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[D$^3$-256 (PSNR = 24.30 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_bike_arcnn_9715_q20.png}\n}\\\\\n\\centering \\subfigure[D-Base-256 (PSNR = 23.48 dB)] {\n\\includegraphics[width=\\textwidth]{D-base-256-bike.png}\n}\\end{minipage}\n\\caption{Visual comparison of various methods on \\textit{Bike} at Q = 5. The corresponding PSNR values (in dB) are also shown.}\n\\label{bike}\n\\end{figure*}\n\n\\subsubsection{Time Complexity}\n\nDuring training, deep learning with the aid of gradient descent\nscales linearly in time and space with the number of train\nsamples. We are primarily concerned with the time complexity\nduring testing (inference), which is more relevant to practical\nusages. Since all learnable layers in the $D^3$ model are\nfully-connected, the inference process of $D^3$ is nothing more\nthan a series of matrix multiplications. The multiplication times\nare counted as: $p_\\Phi m$ ($D_A$ in Stage I) + $2p_\\Phi$ (two\ndiagonal layers) + $p_\\Phi m$ ($D_S$ in Stage I)  + $p_\\Psi m$\n($D_A$ in Stage II) + $2p_\\Psi$ (two diagonal layers) + $p_\\Psi m$\n($D_S$ in Stage II). The 2D DCT and IDCT each takes $\\frac{1}{2}\nm\\log(m)$ multiplications \\cite{JPEG} . Therefore, the total\ninference time complexity of $D^3$ is:\n\n\n", "index": 17, "text": "\\begin{equation}\n\\begin{array}{l}\\label{D3complexity}\nC_{D^3} = 2 (p_\\Phi+ p_\\Psi) (m + 1) + m\\log(m) \\approx 2m(p_\\Phi+ p_\\Psi).\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}C_{D^{3}}=2(p_{\\Phi}+p_{\\Psi})(m+1)+m\\log(m)\\approx 2m(p_{%&#10;\\Phi}+p_{\\Psi}).\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>C</mi><msup><mi>D</mi><mn>3</mn></msup></msub><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a6</mi></msub><mo>+</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a8</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>m</mi><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>\u2248</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a6</mi></msub><mo>+</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a8</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nwhere $l$ is the layer index, $d$ is the total depth, $n_l$ is the\nnumber of filters in the $l$-th layer, $s_l$ is the spatial size\nof the filter, and $m_l$ is the spatial size of the output feature\nmap.\n\nThe theoretical time complexities in (\\ref{D3complexity}) and\n(\\ref{convcomplexity}) do not represent  the actual running time,\nas they depend on different configurations and can be sensitive to\nimplementations and hardware. Yet, our actual running time scales\nnicely with those theoretical results.\n\n \\subsubsection{Parameter Complexity}\nThe total number of free parameters in $D^3$ is:\n\n", "itemtype": "equation", "pos": 23923, "prevtext": "\nThe complexity could also be expressed as $O(p_\\Phi+ p_\\Psi)$.\n\nIt is obvious that the sparse coding inference \\cite{Liu} has\ndramatically higher time complexity. We are also interested in the\ninference time complexity of other competitive deep models,\nespecially AR-CNN \\cite{Dong}. For their fully convolutional\narchitecture, the total complexity \\cite{he2015convolutional} is:\n\n", "index": 19, "text": "\\begin{equation}\n\\begin{array}{l}\\label{convcomplexity}\nC_{conv} = \\sum_{l=1}^d n_{l-1} \\cdot s_l^2 \\cdot n_l \\cdot m_l^2,\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}C_{conv}=\\sum_{l=1}^{d}n_{l-1}\\cdot s_{l}^{2}\\cdot n_{l}%&#10;\\cdot m_{l}^{2},\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>C</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>n</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u22c5</mo><msubsup><mi>s</mi><mi>l</mi><mn>2</mn></msubsup><mo>\u22c5</mo><msub><mi>n</mi><mi>l</mi></msub><mo>\u22c5</mo><msubsup><mi>m</mi><mi>l</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nAs a comparison, the AR-CNN model \\cite{Dong} contains:\n\n", "itemtype": "equation", "pos": 24666, "prevtext": "\nwhere $l$ is the layer index, $d$ is the total depth, $n_l$ is the\nnumber of filters in the $l$-th layer, $s_l$ is the spatial size\nof the filter, and $m_l$ is the spatial size of the output feature\nmap.\n\nThe theoretical time complexities in (\\ref{D3complexity}) and\n(\\ref{convcomplexity}) do not represent  the actual running time,\nas they depend on different configurations and can be sensitive to\nimplementations and hardware. Yet, our actual running time scales\nnicely with those theoretical results.\n\n \\subsubsection{Parameter Complexity}\nThe total number of free parameters in $D^3$ is:\n\n", "index": 21, "text": "\\begin{equation}\n\\begin{array}{l}\\label{D3para}\nN_{D^3} = 2 p_\\Phi m + p_\\Phi + 2 p_\\Psi m + p_\\Psi = 2 (p_\\Phi+\np_\\Psi) (m + 1).\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}N_{D^{3}}=2p_{\\Phi}m+p_{\\Phi}+2p_{\\Psi}m+p_{\\Psi}=2(p_{\\Phi}%&#10;+p_{\\Psi})(m+1).\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>N</mi><msup><mi>D</mi><mn>3</mn></msup></msub><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a6</mi></msub><mo>\u2062</mo><mi>m</mi></mrow><mo>+</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a6</mi></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a8</mi></msub><mo>\u2062</mo><mi>m</mi></mrow><mo>+</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a8</mi></msub></mrow><mo>=</mo><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a6</mi></msub><mo>+</mo><msub><mi>p</mi><mi mathvariant=\"normal\">\u03a8</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\n\n\n\n\\subsection{Remark: A Hidden Gem}\n\nA hidden gem in the design process of D$^3$ is the discovery of\ninner connections between popular deep network modules and\nwell-studied sparse coding algorithms. The current model concerns\nsolving conventional sparse coding (\\ref{alpha}). By enforcing an\nextra nonnegative constraint on $\\alpha$, (\\ref{alpha}) becomes\nthe non-negative sparse coding problem that is found to be\nmeaningful in many visual recognition tasks \\cite{KSVD}. Following\nthe same routine in Section 3.3, the only change of 1-SI module\noccurs in the thresholding operator :\n\n", "itemtype": "equation", "pos": 24879, "prevtext": "\nAs a comparison, the AR-CNN model \\cite{Dong} contains:\n\n", "index": 23, "text": "\\begin{equation}\n\\begin{array}{l}\\label{convpara}\nN_{conv} =  \\sum_{l=1}^d n_{l-1} \\cdot n_l \\cdot s_l^2.\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}N_{conv}=\\sum_{l=1}^{d}n_{l-1}\\cdot n_{l}\\cdot s_{l}^{2}.%&#10;\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>N</mi><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>n</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mrow><msub><mi>n</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>\u22c5</mo><msub><mi>n</mi><mi>l</mi></msub><mo>\u22c5</mo><msubsup><mi>s</mi><mi>l</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04149.tex", "nexttext": "\nThe new $r_{\\lambda}$ is exactly a ReLU \\cite{imagenet} neuron\nwith a translation bias $\\lambda$. Similarly, if we substitute the\nformulation (\\ref{alpha}) with the convolutional sparse coding\nmodel \\cite{bristow2013fast}, $\\mathbf{D}_A$ in Fig. \\ref{1SI}\nwill be replaced by a convolutional layer.\n\nThe above analogies provide us with some new clues on interpreting\nand designing deep architectures. Deep networks constituted by popular modules (fully-connected and convolutional\nlayers, ReLU, etc) could be viewed as a hierarchy of roughly-solved\nsparse coding models, and further enables end-to-end training. Although a\nmore in-depth discussion is beyond the focus of this paper, we\nhope those observations can evoke more interests from the\ncommunity.\n\n \n\n\n\n\\begin{figure*}[tbp]\n\\centering\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Original] {\n\\includegraphics[width=\\textwidth]{ori_Butterfly.pdf}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Compressed (PSNR = 22.65 dB)] {\n\\includegraphics[width=\\textwidth]{Compressed_Butterfly.jpg}\n}\\\\\n\\centering \\subfigure[S-D$^2$ (PSNR = 24.87 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Butterfly_arcnn_9715_q10.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[AR-CNN (PSNR = 25.81 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Butterfly_arcnn_9715_q5.png}\n}\\\\\n\\centering \\subfigure[D$^3$-128 (PSNR = 24.74 dB)] {\n\\includegraphics[width=\\textwidth]{D-128-Butterfly.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[D$^3$-256 (PSNR = 26.30 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Butterfly_arcnn_9715_q20.png}\n}\\\\\n\\centering \\subfigure[D-Base-256 (PSNR = 24.28 dB)] {\n\\includegraphics[width=\\textwidth]{D-base-256-Butterfly.png}\n}\\end{minipage}\n\\caption{Visual comparison of various methods on \\textit{Monarch} at Q = 5. The corresponding PSNR values (in dB) are also shown.}\n\\label{butterfly}\n\\end{figure*}\n\n\n \\begin{table*}[htbp]\n \n \\begin{center}\n \\caption{The average results of PSNR (dB), SSIM, PSNR-B (dB) on the LIVE1 dataset.}\n \\label{allcompare}\n \\begin{tabular}{|c|c|c|c|c|c|c|c|}\n \\hline\n& & Compressed & S-D$^2$ &  AR-CNN & D$^3$-128 & D$^3$-256& D-Base-256 \\\\\n  \\hline\n$\\multirow{3}{*}{Q = 5}$ & PSNR & 24.61 & 25.83 & 26.64  & 26.26 & \\textbf{27.37} & 25.83 \\\\\n\\cline{2-8}\n & SSIM & 0.7020 & 0.7170& 0.7274 & 0.7203 & \\textbf{0.7303} & 0.7186  \\\\\n \\cline{2-8}\n& PSNR-B & 22.01 & 25.64 & 26.46   & 25.86 & \\textbf{26.95} & 25.51  \\\\\n  \\hline\n$\\multirow{3}{*}{Q = 10}$ & PSNR & 27.77 & 28.88 & 29.03   & 28.62 & \\textbf{29.96} & 28.24 \\\\\n\\cline{2-8}\n & SSIM & 0.7905 & 0.8195& 0.8218  & 0.8198 & \\textbf{0.8233} & 0.8161  \\\\\n \\cline{2-8}\n& PSNR-B & 25.33 & 27.96& 28.76 & 28.33 & \\textbf{29.45} &  27.57 \\\\\n  \\hline\n$\\multirow{3}{*}{Q = 20}$ & PSNR & 30.07 & 31.62 & 31.30  & 31.20 & \\textbf{32.21} & 31.27 \\\\\n\\cline{2-8}\n & SSIM & 0.8683 & 0.8830 & 0.8871  & 0.8829 & \\textbf{0.8903} & 0.8868 \\\\\n \\cline{2-8}\n& PSNR-B & 27.57 & 29.73 & 30.80  & 30.56 & \\textbf{31.35} & 29.25 \\\\\n\n\n\n\n\n\n\n\n \\hline\n  \\hline\n\\#Param &  & $\\backslash$ & NA & 106,448 & 33, 280 & 66, 560 & 66, 560 \\\\\n  \\hline\n \\end{tabular}\n \\end{center}\n \\end{table*}\n\n\n\\begin{figure*}[tbp]\n\\centering\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Original] {\n\\includegraphics[width=\\textwidth]{ori_Parrots.pdf}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[Compressed (PSNR = 26.15 dB)] {\n\\includegraphics[width=\\textwidth]{Compressed_Parrots.jpg}\n}\\\\\n\\centering \\subfigure[S-D$^2$ (PSNR = 27.92 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Parrots_arcnn_9715_q10.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[AR-CNN (PSNR = 28.20 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Parrots_arcnn_9715_q5.png}\n}\\\\\n\\centering \\subfigure[D$^3$-128 (PSNR = 27.52 dB)] {\n\\includegraphics[width=\\textwidth]{D-128-Parrots.png}\n}\\end{minipage}\n\\begin{minipage}{0.225\\textwidth}\n\\centering \\subfigure[D$^3$-256 (PSNR = 28.84 dB)] {\n\\includegraphics[width=\\textwidth]{Gray_Parrots_arcnn_9715_q20.png}\n}\\\\\n\\centering \\subfigure[D-Base-256 (PSNR = 27.21 dB)] {\n\\includegraphics[width=\\textwidth]{D-base-256-Parrots.png}\n}\\end{minipage}\n\\caption{Visual comparison of various methods on \\textit{Parrots} at Q = 5. The corresponding PSNR values are also shown.}\n\\label{parrot}\n\\end{figure*}\n\n\n\n\n\\section{Experiments}\n\n\\subsection{Implementation and Setting}\n\nWe use the disjoint training set (200 images) and test set\n(200 images) of BSDS500 database \\cite{BSD}, as our training set; its validation set (100 images) is used for validation, which follows \\cite{Dong}.\nFor training the D$^3$ model, we first divide each original image\ninto overlapped $8 \\times 8$ patches, and subtract the pixel\nvalues by 128 as in the JPEG mean shifting process. We then\nperform JPEG encoding on them by MATLAB JPEG encoder with a\nspecific quality factor $Q$, to generate the corresponding\ncompressed samples. Whereas JPEG works on non-overlapping patches,\nwe emphasize that the training patches are overlapped and\nextracted from arbitrary positions. For a testing image, we sample\n$8 \\times 8$ blocks with a stride of 4, and apply the D$^3$ model in a\npatch-wise manner. For a patch that misaligns with the original\nJPEG block boundaries, we find its most similar coding block from\nits $16 \\times 16$ local neighborhood, whose quantization\nintervals are then applied to the misaligned patch. We find this\npractice effective and important for removing blocking artifacts\nand ensuring the neighborhood consistency. The final result is\nobtained via aggregating all patches, with the overlapping\nregions averaged.\n\nThe proposed networks are implemented using the cuda-convnet\npackage \\cite{imagenet}. We apply a constant learning rate of\n0.01, a batch size of 128, with no momentum. Experiments run on a workstation with 12 Intel Xeon\n2.67GHz CPUs and 1 GTX680 GPU. The two losses, $L_B$ and $L_2$, are equally weighted. For the parameters in Table\n\\ref{dim}, $m$ is fixed as 64. We try different values of $p_\\Phi$ and $p_\\Psi$ in experiments.\n\nBased on the solved Eqn. (\\ref{dual}), one could initialize $D_A$, $D_S$, and $\\theta$  from $\\Phi$, $\\Phi^T$ and $\\lambda$ in the DCT domain block of Fig. \\ref{DDD}, and from $\\Psi$, $\\Psi^T$ and $\\gamma$ in the pixel domain block, respectively. In practice, we find such an initialization strategy has a marginal yet positive impact on the performances. What is more, it usually leads to faster training convergences.\n\n\nWe test the quality factor $Q$ = 5, 10, and\n20. For each $Q$, we train a dedicated model. We further find the\neasy-hard transfer suggested by \\cite{Dong} useful. As images of\nlow $Q$ values (heavily compressed) contain more complex\nartifacts, it is helpful to use the features learned from images\nof high $Q$ values (lightly compressed) as a starting point. In\npractice, we first train the D$^3$ model on JPEG compressed images\nwith $Q = 20$ (the highest quality). We then initialize the $Q =\n10$ model with the $Q = 20$ model, and similarly, initialize $Q =\n5$ model from the $Q = 10$ one.\n\n\n\\subsection{Restoration Performance Comparison}\nWe include the following two relevant, state-of-the-art methods for comparison:\n\n\\begin{itemize}\n\\item \\textbf{Sparsity-based Dual-Domain Method (S-D$^2$)}\n\\cite{Liu} could be viewed as the ``shallow'' counterpart of\nD$^3$. It has outperformed most traditional methods\n\\cite{Liu}, such as BM3D \\cite{BM3D} and DicTV\n\\cite{chang2014reducing}, with which we thus do not compare again.\nThe algorithm has a few parameters to be manually\ntuned. Especially, their dictionary atoms are adaptively selected by a nearest-neighbour type algorithm; the number of selected atoms varies for every testing patch. Therefore, the parameter complexity of\nS-D$^2$ cannot be exactly computed.\n\\item \\textbf{AR-CNN} has been\nthe latest deep model resolving the JPEG compression artifact\nremoval problem. In \\cite{Dong}, the authors show its advantage\nover SA-DCT \\cite{foi2007pointwise}, RTF \\cite{jancsary2012loss},\nand SR-CNN \\cite{SRCNN}. We adopt the default network\nconfiguration in \\cite{Dong}: $s_1$ = 9, $s_2$ = 7, $s_3$ = 1,\n$s_4$ = 5; $n_1$ = 64, $n_2$ = 32, $n_3$ = 16, $n_4$ = 1. The\nauthors adopted the easy-hard transfer in training.\n    \n\n\n\n\n\\end{itemize}\nFor D$^3$, we test $p_\\Phi$ =\n$p_\\Psi$ = 128 and 256 \\footnote{from the common experiences of choosing dictionary\nsizes \\cite{KSVD}}. The resulting D$^3$ models\nare denoted as D$^3$-128 and D$^3$-256, respectively.\nIn addition, to verify the superiority of our task-specific\ndesign, we construct a fully-connected Deep Baseline Model\n(D-Base), of the same complexity with D$^3$-256, named D-Base-256.\nIt consists of four weight matrices of the same dimensions as\nD$^3$-256's four trainable layers\\footnote{The diagonal layers\ncontain a very small portion of parameters and are ignored here.}. D-Base-256 utilizes ReLU \\cite{imagenet} neurons and the dropout technique.\n\n\\begin{figure*}[htbp]\n\\centering\n\\begin{minipage}{0.89\\textwidth}\n\\centering \\includegraphics[width = \\textwidth]{time.png}\n\\end{minipage}\n\\caption{The running time comparison (s) on each individual image in the LIVE1 dataset (Q = 5).}\n\\label{time}\n\\end{figure*}\n\n\nWe use the 29 images in the LIVE1 dataset \\cite{live} (converted to the gray scale) to evaluate both the quantitative and qualitative performances. Three quality assessment criteria: PSNR, structural similarity (SSIM) \\cite{SSIM}, and PSNR-B \\cite{PSNRB}, are evaluated, the last of which is designed specifically to assess blocky images. The averaged results on the LIVE1 dataset are list in Table \\ref{allcompare}. \n\nCompared to S-D$^2$, both D$^3$-128 and D$^3$-256 gain remarkable advantages, thanks to the end-to-end training as deep architectures. As $p_\\Phi$ and $p_\\Psi$ grow from 128 to 256, one observes clear improvements in PSNR/SSIM/PSNR-B. D$^3$-256 has outperformed the state-of-the-art ARCNN, for around 1 dB in PSNR. Moreover, D$^3$-256 also demonstrates a notable performance margin over D-Base-256, although they possess the same number of parameters. We thus verify our important argument that D$^3$ benefits from its task-specific architecture inspired by the sparse coding process (\\ref{dual}), rather than just the large learning capacity of generic deep models. The parameter numbers of different models are compared in the last row of Table \\ref{allcompare}. It is impressive to see that D$^3$-256 also takes less parameters than AR-CNN.\n\nWe display three groups of visual results, on \\textit{Bike}, \\textit{Monarch} and \\textit{Parrots} images, when $Q$ = 5, in Figs. \\ref{bike}, \\ref{butterfly} and \\ref{parrot}, respectively. AR-CNN tends to generate over-smoothness, such as in the edge regions of butterfly wings and parrot head. S-D$^2$ is capable of restoring sharper edges and textures. The D$^3$ models further reduce the unnatural artifacts occurring in S-D$^2$ results.\nEspecially, while D$^3$-128 results still suffer from a small amount of visible ringing artifacts, D$^3$-256 not only shows superior in preserving details, but also suppresses artifacts well.\n\n\n\nIn addition, we re-train the D$^3$-256 model without the\nintermediate loss $L_B$ (\\ref{box}), in the $Q = 5$ case. We find\nthat the resulting model is less capable in generating sharp\ndetails, accompanied with degraded PSNR/SSIM/PSNR-B. That verifies\nthe significance of the $L_B$ loss, without which the energy tends to focus more on the low-frequency bands.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Running Time Comparison}\n\nThe image or video codecs desire highly efficient compression\nartifact removal algorithms as the post-processing tool.\nTraditional TV and digital cinema business uses frame rate\nstandards such as 24p (i.e., 24 frames per second), 25p, and 30p.\nEmerging standards require much higher rates. For example,\nhigh-end High-Definition (HD) TV systems adopt 50p or 60p; the\nUltra-HD (UHD) TV standard advocates 100p/119.88p/120p; the HEVC\nformat could reach the maximum frame rate of 300p \\cite{url}. To\nthis end, higher time efficiency is as desirable as improved\nperformances.\n\nWe compare the averaged testing times of AR-CNN and the proposed\nD$^3$ models in Table \\ref{timetable}, on 29 images in the LIVE29\ndataset, all using the same machine and software environment \\footnote{We do not compare\nwith S-D$^2$ because the sparse coding inference is well-known to\nbe much slower.}. The time costs of AR-CNN and D$^3$-256 running on\neach individual image are also profiled in Fig. \\ref{time}. Our best\nmodel, D$^3$-256, takes approximately 12 ms per image; that is more\nthan \\textbf{30 times faster} than AR-CNN. The speed difference is NOT mainly\ncaused by the different implementations. Both being completely\nfeed-forward, AR-CNN relies on the time-consuming convolution\noperations while ours takes only a few matrix multiplications.\nThat is in accordance with the theoretical time complexities\ncomputed from (\\ref{D3complexity}) and (\\ref{convcomplexity}), too. As\na result, D$^3$-256 is able to process 80p image sequences (or\neven higher). To our best knowledge, D$^3$ is the \\textbf{fastest}\namong all state-of-the-art algorithms, and proves to be a\npractical choice for HDTV industrial usage.\n\n\n \\begin{table}[htbp]\n \\begin{center}\n \\caption{Averaged running time comparison (ms) on LIVE1.}\n \\label{timetable}\n \\begin{tabular}{|c|c|c|c|c|}\n \\hline\n& AR-CNN &  D$^3$-128 &  D$^3$-256  & D-Base-256 \\\\\n \\hline\n  Q = 5 & 396.76 & 7.62 & 12.20 & 9.85 \\\\\n  \\hline\n  Q = 10 & 400.34 & 8.84 & 12.79 & 10.27 \\\\\n  \\hline\n  Q = 20 & 394.61 & 8.42 & 12.02 & 9.97 \\\\\n  \\hline\n \\end{tabular}\n \\end{center}\n \\end{table}\n\n\n\n\n\n\\section{Conclusion}\nWe introduce the Deep Dual-Domain (D$^3$) based fast restoration\nmodel to remove artifacts in JPEG compressed images. The\nsuccessful combination of both JPEG prior knowledge and sparse\ncoding expertise helps our proposed deep architecture to be highly\neffective and efficient. In the future, we aim to further reduce\nthe complexity of D$^3$, as well as extend our model to\nmore related applications, such as denoising and super-resolution.\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{cvprd4}\n}\n\n\n", "itemtype": "equation", "pos": 25597, "prevtext": "\n\n\n\n\\subsection{Remark: A Hidden Gem}\n\nA hidden gem in the design process of D$^3$ is the discovery of\ninner connections between popular deep network modules and\nwell-studied sparse coding algorithms. The current model concerns\nsolving conventional sparse coding (\\ref{alpha}). By enforcing an\nextra nonnegative constraint on $\\alpha$, (\\ref{alpha}) becomes\nthe non-negative sparse coding problem that is found to be\nmeaningful in many visual recognition tasks \\cite{KSVD}. Following\nthe same routine in Section 3.3, the only change of 1-SI module\noccurs in the thresholding operator :\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{array}{l}\\label{1stepRELU}\n\\bm{\\alpha}= r_{\\lambda}(\\bm{\\Phi} y_t), \\quad \\text{where} \\quad\n[r_{\\lambda}(\\mathbf{u})]_i = [\\mathbf{u}_i - \\lambda_i]_{+}.\n\\end{array}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\begin{array}[]{l}\\bm{\\alpha}=r_{\\lambda}(\\bm{\\Phi}y_{t}),\\quad\\text{where}%&#10;\\quad[r_{\\lambda}(\\mathbf{u})]_{i}=[\\mathbf{u}_{i}-\\lambda_{i}]_{+}.\\end{array}\" display=\"block\"><mtable displaystyle=\"true\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mrow><mi>\ud835\udf36</mi><mo>=</mo><mrow><mrow><msub><mi>r</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udebd</mi><mo>\u2062</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mtext>where</mtext></mrow></mrow><mo separator=\"true\">\u2003</mo><mrow><msub><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>r</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2e</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mi>\ud835\udc2e</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow><mo>+</mo></msub></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}]