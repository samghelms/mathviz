[{"file": "1601.01343.tex", "nexttext": "\nwhere $c$ is the size of the context window, $w_t$ denotes the target word, and $w_{t+j}$ is its context word.\nThe conditional probability $P(w_{t+j}|w_t)$ is computed using the following softmax function:\n\n", "itemtype": "equation", "pos": 5938, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nNamed Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia).\nIn this paper, we propose a novel embedding method specifically designed for NED.\nThe proposed method \\textit{jointly} maps words and entities into the same continuous vector space.\nWe extend the \\textit{skip-gram} model by using two models.\nThe \\textit{KB graph model} learns the relatedness of entities using the link structure of the KB, whereas the \\textit{anchor context model} aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words.\nBy combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1\\% on the standard CoNLL dataset and 85.2\\% on the TAC 2010 dataset.\nOur code and pre-trained vectors will be made available online.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nNamed Entity Disambiguation (NED) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g., Wikipedia).\nNED has lately been extensively studied \\cite{Cucerzan2007,Mihalcea2007,Milne2008,Ratinov2011} and used as a fundamental component in numerous tasks, such as information extraction, knowledge base population \\cite{McNamee2009,Ji2010}, and semantic search \\cite{Blanco2015a}.\nWe use Wikipedia as our KB in this paper.\n\nThe main difficulty in NED is ambiguity in the meaning of entity mentions.\nFor example, the mention ``Washington'' in a document can refer to various entities, such as the state, or the capital in the US, the actor \\textsf{Denzel Washington}, the first US president \\textsf{George Washington}, and so on.\nIn order to resolve these ambiguous mentions into references to the correct entities, early approaches focused on modeling \\textit{textual} context, such as the similarity between contextual words and encyclopedic descriptions of a candidate entity \\cite{Bunescu2006,Mihalcea2007}.\nMost state-of-the-art methods use more sophisticated \\textit{global} approaches, wherein all mentions in a document are simultaneously disambiguated based on global \\textit{coherence} among disambiguation decisions.\n\nWord embedding methods are also becoming increasingly popular \\cite{Mikolov2013,Mikolov2013a,Pennington2014}.\nThese involve learning continuous vector representations of words from large, unstructured text corpora.\nThe vectors are designed to capture the semantic similarity of words when similar words are placed near one another in a relatively low-dimensional vector space.\n\nIn this paper, we propose a method to construct a novel embedding that \\textit{jointly} maps words and entities into the same continuous vector space.\nIn this model, similar words and entities are placed close to one another in a vector space.\nHence, we can measure the similarity between any pairs of items (i.e., words, entities, and a word and an entity) by simply computing their cosine similarity.\nThis enables us to easily measure the contextual information for NED, such as the similarity between a context word and a candidate entity, and the relatedness of entities required to model coherence.\n\nOur model is based on the skip-gram model \\cite{Mikolov2013,Mikolov2013a}, a recently proposed embedding model that learns to predict each context word given the target word.\nOur model consists of the following three models based on the skip-gram model:\n1) the conventional skip-gram model that learns to predict the neighboring words given the target word in the text corpora,\n2) the \\textit{KB graph model} that learns to estimate the neighboring entities given the target entity in the link graph of the KB,\nand 3) the \\textit{anchor context model} that learns to predict the neighboring words given the target entity using the anchors and their context words in the KB.\nBy jointly optimizing these models, our method simultaneously learns the embedding of words and entities.\n\nBased on our proposed embedding, we also develop a straightforward NED method that computes two contexts using the proposed embedding: textual context similarity, and coherence.\nTextual context similarity is measured based on vector similarity between an entity and words in the document.\nCoherence is measured based on the relatedness between the target entity and other entities in the document.\nOur NED method combines these contexts with several standard features (e.g., prior probability) using supervised machine learning.\n\nWe tested the proposed method using two standard NED datasets: the CoNLL dataset and the TAC 2010 dataset.\nExperimental results revealed that our method outperformed state-of-the-art methods on both datasets by significant margins.\nMoreover, we conducted experiments to separately assess the quality of the vector representations of words and entities using several standard word similarity datasets and an entity relatedness dataset, and discovered that our method successfully learns the quality representations of words and entities.\n\n\\section{Joint Embedding of Words and Entities}\n\nIn this section, we first describe the conventional skip-gram model for learning word embedding.\nWe then explain our method to construct an embedding that jointly maps words and entities into the same continuous $d$-dimensional vector space.\nWe extend the skip-gram model by adding the \\textit{KB graph model} and the \\textit{anchor context model}.\n\n\\subsection{Skip-gram Model for Word Similarity}\n\\label{subsec:skip-gram-model}\n\nThe training objective of the skip-gram model is to find word representations that are useful to predict context words given the target word.\nFormally, given a sequence of $T$ words $w_1, w_2, ..., w_T$, the model aims to maximize the following objective function:\n\n", "index": 1, "text": "\\begin{equation}\n\\mathcal{L}_w = \\sum_{t=1}^{T}\\sum_{-c \\leq j \\leq c,j \\neq 0}\\log P(w_{t+j}|w_t)\n\\label{objective_skipgram}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{w}=\\sum_{t=1}^{T}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log P(w_{t+j}|w_{t})\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>w</mi></msub><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mrow><mo>-</mo><mi>c</mi></mrow><mo>\u2264</mo><mi>j</mi><mo>\u2264</mo><mi>c</mi></mrow><mo>,</mo><mrow><mi>j</mi><mo>\u2260</mo><mn>0</mn></mrow></mrow></munder><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $W$ is a set containing all words in the vocabulary, and $\\mathbf{V}_w \\in \\mathbb{R}^d$ and $\\mathbf{U}_w \\in \\mathbb{R}^d$ denote the vectors of word $w$ in matrices $\\mathbf{V}$ and $\\mathbf{U}$, respectively.\n\nThe skip-gram model is trained to optimize the above function $\\mathcal{L}_w$, and $\\mathbf{V}$ are used as the resulting vector representations of words.\n\n\\subsection{Extending the Skip-gram Model}\n\nWe extend the skip-gram model to learn the vector representations of entities.\nWe expand matrices $\\textbf{V}$ and $\\textbf{U}$ to include the vectors of entities $\\mathbf{V}_e \\in \\mathbb{R}^d$ and $\\mathbf{U}_e \\in \\mathbb{R}^d$ in addition to the vectors for words.\n\n\\subsubsection{KB Graph Model}\n\nWe use an internal link structure in KB to enable the model to learn the relatedness between pairs of entities.\nWikipedia Link-based Measure (WLM) \\cite{DavidMilne} is a method to measure entity relatedness based on its link structure.\nIt has been used as a standard method to compute the relatedness of entities for modeling coherence in past NED studies.\nThe relatedness between two entities is computed using the following function:\n\n", "itemtype": "equation", "pos": 6285, "prevtext": "\nwhere $c$ is the size of the context window, $w_t$ denotes the target word, and $w_{t+j}$ is its context word.\nThe conditional probability $P(w_{t+j}|w_t)$ is computed using the following softmax function:\n\n", "index": 3, "text": "\\begin{equation}\nP(w_{t+j}|w_t) = \\frac{\\exp(\\mathbf{V}_{w_t}\\!^\\top \\mathbf{U}_{w_{t+j}})}{\\sum_{w \\in W}\\exp(\\mathbf{V}_{w_t}\\!^\\top \\mathbf{U}_w)}\n\\label{softmax_skipgram}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"P(w_{t+j}|w_{t})=\\frac{\\exp(\\mathbf{V}_{w_{t}}\\!^{\\top}\\mathbf{U}_{w_{t+j}})}{%&#10;\\sum_{w\\in W}\\exp(\\mathbf{V}_{w_{t}}\\!^{\\top}\\mathbf{U}_{w})}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">|</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>w</mi><mi>t</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>w</mi><mo>\u2208</mo><mi>W</mi></mrow></msub><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>w</mi><mi>t</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><mi>w</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $E$ is the set of all entities in KB and $C_e$ is the set of entities with a link to an entity $e$.\nIntuitively, WLM assumes that entities with similar incoming links are related.\nDespite its simplicity, WLM yields state-of-the-art performance \\cite{Hoffart2012}.\n\nInspired by WLM, the KB graph model simply learns to place entities with similar incoming links near one another in the vector space.\nWe formalize this as the following objective function:\n\n", "itemtype": "equation", "pos": 7633, "prevtext": "\nwhere $W$ is a set containing all words in the vocabulary, and $\\mathbf{V}_w \\in \\mathbb{R}^d$ and $\\mathbf{U}_w \\in \\mathbb{R}^d$ denote the vectors of word $w$ in matrices $\\mathbf{V}$ and $\\mathbf{U}$, respectively.\n\nThe skip-gram model is trained to optimize the above function $\\mathcal{L}_w$, and $\\mathbf{V}$ are used as the resulting vector representations of words.\n\n\\subsection{Extending the Skip-gram Model}\n\nWe extend the skip-gram model to learn the vector representations of entities.\nWe expand matrices $\\textbf{V}$ and $\\textbf{U}$ to include the vectors of entities $\\mathbf{V}_e \\in \\mathbb{R}^d$ and $\\mathbf{U}_e \\in \\mathbb{R}^d$ in addition to the vectors for words.\n\n\\subsubsection{KB Graph Model}\n\nWe use an internal link structure in KB to enable the model to learn the relatedness between pairs of entities.\nWikipedia Link-based Measure (WLM) \\cite{DavidMilne} is a method to measure entity relatedness based on its link structure.\nIt has been used as a standard method to compute the relatedness of entities for modeling coherence in past NED studies.\nThe relatedness between two entities is computed using the following function:\n\n", "index": 5, "text": "\\begin{equation}\n\\resizebox{.89\\hsize}{!}{$\nWLM(e_1, e_2) = 1 - \\frac{\\log\\,\\max(|C_{e_1}|, |C_{e_2}|) - \\log|C_{e_1} \\cap C_{e_2}|}{\\log|E| - \\log\\,\\min(|C_{e_1}|, |C_{e_2}|)}\n$}\n\\label{wlm}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1.m1\" class=\"ltx_Math\" alttext=\"WLM(e_{1},e_{2})=1-\\frac{\\log\\,\\max(|C_{e_{1}}|,|C_{e_{2}}|)-\\log|C_{e_{1}}%&#10;\\cap C_{e_{2}}|}{\\log|E|-\\log\\,\\min(|C_{e_{1}}|,|C_{e_{2}}|)}\" display=\"inline\"><mrow><mrow><mi>W</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>e</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mrow><mpadded width=\"+1.7pt\"><mi>log</mi></mpadded><mo>\u2061</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><msub><mi>e</mi><mn>1</mn></msub></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><msub><mi>e</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mrow><msub><mi>C</mi><msub><mi>e</mi><mn>1</mn></msub></msub><mo>\u2229</mo><msub><mi>C</mi><msub><mi>e</mi><mn>2</mn></msub></msub></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mrow><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mi>E</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo>-</mo><mrow><mpadded width=\"+1.7pt\"><mi>log</mi></mpadded><mo>\u2061</mo><mrow><mi>min</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><msub><mi>e</mi><mn>1</mn></msub></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>C</mi><msub><mi>e</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mfrac></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nWe compute the conditional probability $P(e_o|e_i)$ using the following softmax function:\n\n", "itemtype": "equation", "pos": 8300, "prevtext": "\nwhere $E$ is the set of all entities in KB and $C_e$ is the set of entities with a link to an entity $e$.\nIntuitively, WLM assumes that entities with similar incoming links are related.\nDespite its simplicity, WLM yields state-of-the-art performance \\cite{Hoffart2012}.\n\nInspired by WLM, the KB graph model simply learns to place entities with similar incoming links near one another in the vector space.\nWe formalize this as the following objective function:\n\n", "index": 7, "text": "\\begin{equation}\n\\mathcal{L}_e = \\sum_{e_i \\in E}\\sum_{e_o \\in C_{e_i}, e_i \\neq e_o}\\log P(e_o|e_i)\n\\label{objective_entity_rel}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{e}=\\sum_{e_{i}\\in E}\\sum_{e_{o}\\in C_{e_{i}},e_{i}\\neq e_{o}}\\log&#10;P%&#10;(e_{o}|e_{i})\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>e</mi></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>\u2208</mo><mi>E</mi></mrow></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><msub><mi>e</mi><mi>o</mi></msub><mo>\u2208</mo><msub><mi>C</mi><msub><mi>e</mi><mi>i</mi></msub></msub></mrow><mo>,</mo><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>\u2260</mo><msub><mi>e</mi><mi>o</mi></msub></mrow></mrow></munder><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>o</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nWe train the model to predict the incoming links $C_{e}$ given an entity $e$.\nTherefore, $C_{e}$ plays a similar role to context words in the skip-gram model.\n\n\\subsubsection{Anchor Context Model}\n\nIf we add only the KB graph model to the skip-gram model, the vectors of words and entities do not interact, and can be placed in different subspaces of the vector space.\nTo address this issue, we introduce the anchor context model to place similar words and entities near one another in the vector space.\n\nThe idea underlying this model is to leverage KB anchors and their context words to train the model.\nAs mentioned in Section \\ref{sec:introduction}, we use Wikipedia as a KB.\nIt contains many internal anchors that can be safely treated as unambiguous occurrences of referent KB entities.\nBy using these anchors, we can easily obtain many occurrences of entities and their corresponding context words directly from the KB.\n\nAs in the skip-gram model, we simply train the model to predict the context words of an entity pointed to by the target anchor.\nThe objective function is as follows:\n\n", "itemtype": "equation", "pos": 8535, "prevtext": "\nWe compute the conditional probability $P(e_o|e_i)$ using the following softmax function:\n\n", "index": 9, "text": "\\begin{equation}\nP(e_o|e_i) = \\frac{\\exp(\\mathbf{V}_{e_i}\\!^\\top \\mathbf{U}_{e_o})}{\\sum_{e \\in E}\\exp(\\mathbf{V}_{e_i}\\!^\\top \\mathbf{U}_e)}\n\\label{softmax_entity_rel}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"P(e_{o}|e_{i})=\\frac{\\exp(\\mathbf{V}_{e_{i}}\\!^{\\top}\\mathbf{U}_{e_{o}})}{\\sum%&#10;_{e\\in E}\\exp(\\mathbf{V}_{e_{i}}\\!^{\\top}\\mathbf{U}_{e})}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>o</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>e</mi><mi>i</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><msub><mi>e</mi><mi>o</mi></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>e</mi><mo>\u2208</mo><mi>E</mi></mrow></msub><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>e</mi><mi>i</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $A$ denotes a set of anchors in the KB, each of which contains a pair of a referent entity $e_i$ and a set of its context words $Q$.\nHere, $Q$ contains the previous $c$ words and the next $c$ words.\nNote that $|A|$ equals the number of internal anchors in the KB.\nAs in past models, the conditional probability $P(w_o|e_i)$ is computed using the softmax function:\n\n", "itemtype": "equation", "pos": 9813, "prevtext": "\nWe train the model to predict the incoming links $C_{e}$ given an entity $e$.\nTherefore, $C_{e}$ plays a similar role to context words in the skip-gram model.\n\n\\subsubsection{Anchor Context Model}\n\nIf we add only the KB graph model to the skip-gram model, the vectors of words and entities do not interact, and can be placed in different subspaces of the vector space.\nTo address this issue, we introduce the anchor context model to place similar words and entities near one another in the vector space.\n\nThe idea underlying this model is to leverage KB anchors and their context words to train the model.\nAs mentioned in Section \\ref{sec:introduction}, we use Wikipedia as a KB.\nIt contains many internal anchors that can be safely treated as unambiguous occurrences of referent KB entities.\nBy using these anchors, we can easily obtain many occurrences of entities and their corresponding context words directly from the KB.\n\nAs in the skip-gram model, we simply train the model to predict the context words of an entity pointed to by the target anchor.\nThe objective function is as follows:\n\n", "index": 11, "text": "\\begin{equation}\n\\mathcal{L}_a = \\sum_{(e_i, Q) \\in A}\\sum_{w_o \\in Q}\\log P(w_o|e_i)\n\\label{objective_alignment}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{a}=\\sum_{(e_{i},Q)\\in A}\\sum_{w_{o}\\in Q}\\log P(w_{o}|e_{i})\" display=\"block\"><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>a</mi></msub><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mi>i</mi></msub><mo>,</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi>A</mi></mrow></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>w</mi><mi>o</mi></msub><mo>\u2208</mo><mi>Q</mi></mrow></munder><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>o</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nUsing the proposed model, we align the vector representations of words and entities by placing words and entities with similar context words close to one another in the vector space.\n\n\\subsection{Training}\n\nConsidering the three model components mentioned above, we propose the following objective function by linearly combining the above objective functions:\n\n", "itemtype": "equation", "pos": 10312, "prevtext": "\nwhere $A$ denotes a set of anchors in the KB, each of which contains a pair of a referent entity $e_i$ and a set of its context words $Q$.\nHere, $Q$ contains the previous $c$ words and the next $c$ words.\nNote that $|A|$ equals the number of internal anchors in the KB.\nAs in past models, the conditional probability $P(w_o|e_i)$ is computed using the softmax function:\n\n", "index": 13, "text": "\\begin{equation}\nP(w_o|e_i) = \\frac{\\exp(\\mathbf{V}_{e_i}\\!^\\top \\mathbf{U}_{w_o})}{\\sum_{w \\in W}\\exp(\\mathbf{V}_{e_i}\\!^\\top \\mathbf{U}_w)}\n\\label{softmax_alignment}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"P(w_{o}|e_{i})=\\frac{\\exp(\\mathbf{V}_{e_{i}}\\!^{\\top}\\mathbf{U}_{w_{o}})}{\\sum%&#10;_{w\\in W}\\exp(\\mathbf{V}_{e_{i}}\\!^{\\top}\\mathbf{U}_{w})}\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>o</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>e</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>e</mi><mi>i</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><msub><mi>w</mi><mi>o</mi></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>w</mi><mo>\u2208</mo><mi>W</mi></mrow></msub><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>e</mi><mi>i</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><mi>w</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nThe training of the model is intended to maximize the above function, and the resulting matrix $\\mathbf{V}$ is used to embed words and entities.\n\nOne of the problems in training our model is that the normalizers contained in the softmax functions $P(w_{t+j}|w_t)$, $P(e_o|e_i)$, and $P(w_o|e_i)$ are computationally very expensive because they involve summation over all words $W$ or entities $E$.\nTo address this problem, we use \\textit{negative sampling (NEG)} \\cite{Mikolov2013a} to convert original objective functions into computationally feasible ones.\nNEG is defined by the following objective function:\n\n", "itemtype": "equation", "pos": 10855, "prevtext": "\nUsing the proposed model, we align the vector representations of words and entities by placing words and entities with similar context words close to one another in the vector space.\n\n\\subsection{Training}\n\nConsidering the three model components mentioned above, we propose the following objective function by linearly combining the above objective functions:\n\n", "index": 15, "text": "\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_w + \\mathcal{L}_e + \\mathcal{L}_a\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}=\\mathcal{L}_{w}+\\mathcal{L}_{e}+\\mathcal{L}_{a}\" display=\"block\"><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>w</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>e</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mi>a</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $\\sigma(x) = 1/(1 + \\exp(-x))$ and $g$ is the number of negative samples.\nWe replace the $\\log P(w_{t+j}|w_t)$ term in Eq. \\eqref{objective_skipgram} with the above objective function.\nConsequently, the objective function is transformed from that in Eq. \\eqref{objective_skipgram} to a simple objective function of the binary classification to distinguish the observed word $w_t$ from words drawn from noise distribution $P_{neg}(w)$.\nWe also replace $\\log P(e_o|e_i)$ in Eq. \\eqref{objective_entity_rel} and $\\log P(w_o|e_i)$ in Eq. \\eqref{objective_alignment} in the same manner.\n\nNote that NEG takes a negative distribution $P_{neg}(w)$ as a free parameter.\nFollowing \\cite{Mikolov2013a}, we use the unigram distribution of words ($U(w)$) raised to the $3/4^{th}$ power (i.e., $U(w)^{3/4}/Z$, where $Z$ is a normalization constant) in the skip-gram model and the anchor context model.\nIn the KB graph model, we use a uniform distribution over KB entities $E$ as the negative distribution.\n\nWe use Wikipedia to train all the above models.\nOptimization is carried out simultaneously to maximize the transformed objective function by iterating over Wikipedia pages several times.\nWe use stochastic gradient descent (SGD) for the optimization.\nThe optimization is performed using a multiprocess-based implementation of our model using Python, Cython, and NumPy configured with OpenBLAS with storing matrices $\\mathbf{V}$ and $\\mathbf{U}$ in the shared memory.\nTo improve speed, we decide not to introduce locks to the shared matrices.\n\n\\section{Named Entity Disambiguation Using Embedding}\n\nIn this section, we explain our NED method using our proposed embedding.\nLet us formally define the task.\nGiven a set of entity mentions $M = \\{m_1, m_2, ..., m_N\\}$ in a document $d$ with an entity set $E = \\{e_1, e_2, ..., e_K\\}$ in the KB, the task is defined as resolving mentions (e.g., ``Washington'') into their referent entities (e.g., \\textsf{Washington D.C.}).\n\nWe introduce two measures that have been frequently observed in past NED studies: \\textit{entity prior} $P(e)$ and \\textit{prior probability} $P(e|m)$.\nWe define entity prior $P(e) = |A_{e,*}| / |A_{*,*}|$ where $A_{*,*}$ denotes all anchors in the KB and $A_{e,*}$ is the set of anchors that point to entity $e$.\nPrior probability is defined as $P(e|m) = |A_{e,m}| / |A_{*,m}|$ where $A_{*,m}$ represents all anchors with the same surface as mention $m$ in KB and $A_{e, m}$ is a subset of $A_{*,m}$ that points to entity $e$.\n\nWe separate the NED task into two sub-tasks: \\textit{candidate generation} and \\textit{mention disambiguation}.\n\nIn candidate generation, candidates of referent entities are generated for each mention.\nDetails of candidate generation are provided in Section \\ref{subsubsec:ned-setup}.\n\n\n\n\n\n\n\n\n\n\\subsection{Mention Disambiguation}\n\nGiven a document $d$ and mention $m$ with its candidate referent entities $\\{e_1, e_2, ..., e_k\\}$ generated in the candidate generation step, the task is to disambiguate mention $m$ by selecting the most relevant entity from the candidate entities.\n\nThe key to improving the performance of this task is to effectively model the context.\nWe propose two novel methods to model the context using the proposed embedding.\nFurther, we combine these two models with several standard NED features using supervised machine learning.\n\n\\subsubsection{Modeling Textual Context}\n\\label{subsubsec:textual-context}\n\nTextual context is designed based on the assumption that an entity is more likely to appear if the context of a given mention is similar to that of the entity.\n\nWe propose a method to measure the similarity between textual context and entity using the proposed embedding by first deriving the vector representation of the context and then computing the similarity between the context and the entity using cosine similarity.\nTo derive the vector of context, we average the vectors of context words:\n\n", "itemtype": "equation", "pos": 11558, "prevtext": "\nThe training of the model is intended to maximize the above function, and the resulting matrix $\\mathbf{V}$ is used to embed words and entities.\n\nOne of the problems in training our model is that the normalizers contained in the softmax functions $P(w_{t+j}|w_t)$, $P(e_o|e_i)$, and $P(w_o|e_i)$ are computationally very expensive because they involve summation over all words $W$ or entities $E$.\nTo address this problem, we use \\textit{negative sampling (NEG)} \\cite{Mikolov2013a} to convert original objective functions into computationally feasible ones.\nNEG is defined by the following objective function:\n\n", "index": 17, "text": "\\begin{equation}\n\\resizebox{.89\\hsize}{!}{$\n\\log\\sigma(\\mathbf{V}_{w_t} \\!^\\top \\mathbf{U}_{w_{t+j}}) + \\sum_{i=1}^{g}\\mathbb{E}_{w_i \\sim P_{neg}(w)} \\Big[\\log\\sigma(-\\mathbf{V}_{w_t} \\!^\\top \\mathbf{U}_{w_i})\\Big]\n$}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1.m1\" class=\"ltx_Math\" alttext=\"\\log\\sigma(\\mathbf{V}_{w_{t}}\\!^{\\top}\\mathbf{U}_{w_{t+j}})+\\sum_{i=1}^{g}%&#10;\\mathbb{E}_{w_{i}\\sim P_{neg}(w)}\\Big{[}\\log\\sigma(-\\mathbf{V}_{w_{t}}\\!^{\\top%&#10;}\\mathbf{U}_{w_{i}})\\Big{]}\" display=\"inline\"><mrow><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>w</mi><mi>t</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>g</mi></msubsup><mrow><msub><mi>\ud835\udd3c</mi><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>\u223c</mo><mrow><msub><mi>P</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>g</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">[</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mrow><msubsup><mi>\ud835\udc15</mi><msub><mi>w</mi><mi>t</mi></msub><mo>\u22a4</mo></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc14</mi><msub><mi>w</mi><mi>i</mi></msub></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">]</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $W_{c_m}$ is a set of the context words of mention $m$ and $\\vec{v_w} \\in \\mathbf{V}$ denotes the vector representation of word $w$.\nWe use all noun words in document $d$ as context words.\\footnote{We used Apache OpenNLP tagger to detect nouns. {\\url{https://opennlp.apache.org/}}}\nMoreover, we ignore a context word if the surface of mention $m$ contains it.\n\nWe then measure the similarity between candidate entity and the derived textual context by using cosine similarity between $\\vec{v_{c_w}}$ and the vector of entity $\\vec{v_e}$.\n\n\\subsubsection{Modeling Coherence}\n\\label{subsubsec:coherence}\n\nIt has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED.\nHowever, this is a chicken-and-egg problem because the assignment of entities to mentions, which is required to measure coherence, is not possible prior to performing NED.\n\nTo address this problem, we introduce a simple \\textit{two-step} approach:\nwe first train the machine learning model using the coherence score among unambiguous mentions\\footnote{We consider that mention $m$ unambiguously refers to entity $e$ if its prior probability $P(e|m)$ is greater than 0.95.}, in addition to other features, and then retrain the model using the coherence score among the predicted entity assignments instead.\n\nTo estimate coherence, we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity $e$.\nNote that context entities are unambiguous entities in the first step, and predicted entities are used instead in the second step.\n\nTo derive the vector representation of context entities, we average their vector representations:\n\n", "itemtype": "equation", "pos": 15720, "prevtext": "\nwhere $\\sigma(x) = 1/(1 + \\exp(-x))$ and $g$ is the number of negative samples.\nWe replace the $\\log P(w_{t+j}|w_t)$ term in Eq. \\eqref{objective_skipgram} with the above objective function.\nConsequently, the objective function is transformed from that in Eq. \\eqref{objective_skipgram} to a simple objective function of the binary classification to distinguish the observed word $w_t$ from words drawn from noise distribution $P_{neg}(w)$.\nWe also replace $\\log P(e_o|e_i)$ in Eq. \\eqref{objective_entity_rel} and $\\log P(w_o|e_i)$ in Eq. \\eqref{objective_alignment} in the same manner.\n\nNote that NEG takes a negative distribution $P_{neg}(w)$ as a free parameter.\nFollowing \\cite{Mikolov2013a}, we use the unigram distribution of words ($U(w)$) raised to the $3/4^{th}$ power (i.e., $U(w)^{3/4}/Z$, where $Z$ is a normalization constant) in the skip-gram model and the anchor context model.\nIn the KB graph model, we use a uniform distribution over KB entities $E$ as the negative distribution.\n\nWe use Wikipedia to train all the above models.\nOptimization is carried out simultaneously to maximize the transformed objective function by iterating over Wikipedia pages several times.\nWe use stochastic gradient descent (SGD) for the optimization.\nThe optimization is performed using a multiprocess-based implementation of our model using Python, Cython, and NumPy configured with OpenBLAS with storing matrices $\\mathbf{V}$ and $\\mathbf{U}$ in the shared memory.\nTo improve speed, we decide not to introduce locks to the shared matrices.\n\n\\section{Named Entity Disambiguation Using Embedding}\n\nIn this section, we explain our NED method using our proposed embedding.\nLet us formally define the task.\nGiven a set of entity mentions $M = \\{m_1, m_2, ..., m_N\\}$ in a document $d$ with an entity set $E = \\{e_1, e_2, ..., e_K\\}$ in the KB, the task is defined as resolving mentions (e.g., ``Washington'') into their referent entities (e.g., \\textsf{Washington D.C.}).\n\nWe introduce two measures that have been frequently observed in past NED studies: \\textit{entity prior} $P(e)$ and \\textit{prior probability} $P(e|m)$.\nWe define entity prior $P(e) = |A_{e,*}| / |A_{*,*}|$ where $A_{*,*}$ denotes all anchors in the KB and $A_{e,*}$ is the set of anchors that point to entity $e$.\nPrior probability is defined as $P(e|m) = |A_{e,m}| / |A_{*,m}|$ where $A_{*,m}$ represents all anchors with the same surface as mention $m$ in KB and $A_{e, m}$ is a subset of $A_{*,m}$ that points to entity $e$.\n\nWe separate the NED task into two sub-tasks: \\textit{candidate generation} and \\textit{mention disambiguation}.\n\nIn candidate generation, candidates of referent entities are generated for each mention.\nDetails of candidate generation are provided in Section \\ref{subsubsec:ned-setup}.\n\n\n\n\n\n\n\n\n\n\\subsection{Mention Disambiguation}\n\nGiven a document $d$ and mention $m$ with its candidate referent entities $\\{e_1, e_2, ..., e_k\\}$ generated in the candidate generation step, the task is to disambiguate mention $m$ by selecting the most relevant entity from the candidate entities.\n\nThe key to improving the performance of this task is to effectively model the context.\nWe propose two novel methods to model the context using the proposed embedding.\nFurther, we combine these two models with several standard NED features using supervised machine learning.\n\n\\subsubsection{Modeling Textual Context}\n\\label{subsubsec:textual-context}\n\nTextual context is designed based on the assumption that an entity is more likely to appear if the context of a given mention is similar to that of the entity.\n\nWe propose a method to measure the similarity between textual context and entity using the proposed embedding by first deriving the vector representation of the context and then computing the similarity between the context and the entity using cosine similarity.\nTo derive the vector of context, we average the vectors of context words:\n\n", "index": 19, "text": "\\begin{equation}\n\\vec{v_{c_w}} = \\frac{1}{|W_{c_m}|}\\sum_{w \\in W_{c_m}}\\vec{v_{w}}\n\\label{eq:word-vector-average}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\vec{v_{c_{w}}}=\\frac{1}{|W_{c_{m}}|}\\sum_{w\\in W_{c_{m}}}\\vec{v_{w}}\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>v</mi><msub><mi>c</mi><mi>w</mi></msub></msub><mo stretchy=\"false\">\u2192</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>W</mi><msub><mi>c</mi><mi>m</mi></msub></msub><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>w</mi><mo>\u2208</mo><msub><mi>W</mi><msub><mi>c</mi><mi>m</mi></msub></msub></mrow></munder><mover accent=\"true\"><msub><mi>v</mi><mi>w</mi></msub><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01343.tex", "nexttext": "\nwhere $E_{c_m}$ denotes the set of context entities described above.\n\nTo estimate the coherence score, we again use cosine similarity between the vector of entity $\\vec{v}_e$ and that of context entities $\\vec{v}_{c_e}$.\n\n\\subsubsection{Learning to Rank}\n\\label{subsubsec:learning-to-rank}\n\nTo combine the proposed contextual information described above with standard NED features, we employ a method of supervised machine learning to rank the candidate entities given mention $m$ and document $d$.\n\nIn particular, we use Gradient Boosted Regression Trees (GBRT) \\cite{Friedman2001}, a state-of-the-art point-wise learning-to-rank algorithm widely used for various tasks, which has been recently adopted for the sort of tasks for which we employ it here \\cite{Meij2012}.\nGBRT consists of an ensemble of regression trees, and predicts a relevance score given an instance.\nWe use the GBRT implementation in \\textit{scikit-learn}\\footnote{\\url{http://scikit-learn.org/}} and the logistic loss is used as the loss function.\nThe main parameters of GBRT are the number of iterations $\\eta$, the learning rate $\\beta$, and the maximum depth of the decision trees $\\xi$.\n\nWith regard to the features of machine learning, we first use prior probability ($P(e|m)$) and entity prior ($P(e)$).\nFurther, we include a feature representing the maximum prior probability of the candidate entity $e$ of all mentions in the document.\nWe also add the number of entity candidates for mention $m$ as a feature.\nThe above set of four features is called \\textit{base} features in the rest of the paper.\n\nWe also use several \\textit{string similarity} features used in past work on NED \\cite{Meij2012}.\nThese features aim to capture the similarity between the title of entity $e$ and the surface of mention $m$, and consist of the edit distance, whether the title of entity $e$ exactly equals or contains the surface of mention $m$, and whether the title of entity $e$ starts or ends with the surface of mention $m$.\n\nFinally, we include contextual features measured using the proposed embedding.\nWe use cosine similarity between the candidate entity and the textual context (see Section \\ref{subsubsec:textual-context}), and similarity between an entity and contextual entities (see Section \\ref{subsubsec:coherence}).\nFurthermore, we include the rank of entity $e$ among candidate entities of mention $m$, sorted according to these two similarity scores in descending order.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{l|ccc}\n\\hline\n& WordSim-353 & MC & RG \\\\\n\\hline\nOur Method & 0.66 & \\textbf{0.78} & \\textbf{0.77} \\\\\n\n\n\n\n\n\n\n\n\nSkip-gram & \\textbf{0.67} & 0.77 & 0.76 \\\\\n\\hline\n\\end{tabular}\n\\caption{Results of the word similarity task.}\n\\label{tb:word-sim-scores}\n\\end{table}\n\nIn this section, we describe the setup and results of our experiments.\nIn addition to experiments on the NED task, we conducted two experiments---one involving a \\textit{word similarity} and another involving an \\textit{entity relatedness}---in order to test the effectiveness of our method in capturing pairwise similarity between pairs of words as well as pairs of entities.\nWe first describe the details of the training of the embedding and then present the experimental results.\n\n\\subsection{Training for the Proposed Embedding}\n\nTo train the proposed embedding, we used the December 2014 version of the Wikipedia dump\\footnote{The dump was retrieved from Wikimedia Downloads. \\url{http://dumps.wikimedia.org/}}.\nWe first removed the pages for navigation, maintenance, and discussion, and used the remaining 4.9 million pages.\n\n\n\n\nWe parsed the Wikipedia pages and extracted text and anchors from each page.\nWe further tokenized the text using the \\textit{Apache OpenNLP} tokenizer.\nWe also filtered out rare words that appeared fewer than five times in the corpus.\nWe thus obtained approximately 2 billion tokens and 73 million anchors.\n\n\n\n\n\n\n\n\n\n\n\n\nThe total number of words and entities in the embedding were approximately 2.1 million and 5 million, respectively.\nConsequently, the number of rows of matrices $\\mathbf{V}$ and $\\mathbf{U}$ were 7.1 million.\n\nThe number of dimensions $d$ of the embedding was set to 500.\nFollowing \\cite{Mikolov2013a}, we also used learning rate $\\alpha = 0.025$ which linearly decreased with the iterations of the Wikipedia dump.\nRegarding the other parameters, we set the size of the context window $c = 10$ and the negative samples $g = 30$.\nThe model was trained online by iterating over pages in the Wikipedia dump 10 times.\nThe training lasted approximately five days using a server with a 40-core CPU on Amazon EC2.\n\n\\subsection{Word Similarity}\n\nIn order to test the quality of vector representations of words, we used three standard word similarity datasets: the \\textit{WordSim-353} dataset \\cite{Finkelstein2002}, the \\textit{MC} dataset \\cite{Miller1991}, and the \\textit{RG} dataset \\cite{Rubenstein1965} that contain 353, 65, and 30 word pairs, respectively.\nEach word pair has a \\textit{gold-standard} similarity score assigned by human judges.\n\nWe used cosine similarity to calculate similarity score between any pair of words.\nFollowing past work, we computed the correlation between similarity scores through human judgments on a set of word pairs using Spearman's rank correlation coefficient.\nHere, we adopted the skip-gram model as baseline.\n\nWe used our implementation to train the skip-gram model.\nFurthermore, the following parameters were used to train the model: $d = 500$, $c = 10$, $g = 30$, and $\\alpha = 0.025$.\nWe trained the model by iterating over the Wikipedia dump 10 times.\n\nTable \\ref{tb:word-sim-scores} shows the results.\nCompared to the skip-gram model, our method performed comparably on the WordSim-353 dataset, and slightly better on other datasets, thus showing that the proposed extension to the skip-gram model can be also beneficial for improving word representations.\n\n\\subsection{Entity Relatedness}\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{l|cccccc}\n\\hline\n& \\scriptsize{NDCG@1} & \\scriptsize{NDCG@5} & \\scriptsize{NDCG@10} & \\scriptsize{MAP} \\\\\n\\hline\n\\footnotesize{Our Method} & \\textbf{0.59} & \\textbf{0.56} & \\textbf{0.59} & \\textbf{0.52} \\\\\n\\footnotesize{WLM} & 0.54 & 0.52 & 0.55 & 0.48 \\\\\n\n\n\n\n\n\n\n\n\n\\hline\n\\end{tabular}\n\\caption{Results of the entity relatedness task.}\n\\label{tb:entity-rel-scores}\n\\end{table}\nTo test the quality of the vector representation of entities, we conducted an experiment using a dataset for entity relatedness created by Ceccarelli et al. \\cite{Ceccarelli2013}.\nThe dataset consists of training, test, and validation sets, and we only use the test set.\nThe test set contains 3,314 entities, where each entity has 91 candidate entities with \\textit{gold-standard} labels indicating whether the two entities are related.\nFollowing \\cite{DBLP:journals/corr/HuangHJ15}, we obtained the ranked order of the candidate entities using cosine similarity between the target entity and each of the candidate entities, and computed the two standard measures: normalized discounted cumulative gain (NDCG) \\cite{Jarvelin2002} and mean average precision (MAP) \\cite{Manning2008}.\nWe adopted WLM as baseline.\n\n\nTable \\ref{tb:entity-rel-scores} shows the results.\nThe score for WLM was obtained from Huang et al. \\cite{DBLP:journals/corr/HuangHJ15}.\nOur method clearly outperformed WLM.\nThe results show that our method accurately captures pairwise entity relatedness.\n\n\\subsection{Named Entity Disambiguation}\n\n\\subsubsection{Setup}\n\\label{subsubsec:ned-setup}\n\nWe now explain our experimental setup for the NED task.\nWe tested the performance of our proposed method on two standard NED datasets: the \\textit{CoNLL} dataset and the \\textit{TAC 2010} dataset.\nThe details of these datasets are provided below.\nMoreover, as with the corpus used in the embedding, we used the December 2014 version of the Wikipedia dump as the referent KB, and to derive the prior probability as well as the entity prior.\n\nTo find the best parameters for our machine learning model, we ran a parameter search on the CoNLL development set.\nWe used $\\eta = 10,000$ trees, and tested all combinations of the learning rate $\\beta = \\{0.01, 0.02, 0.03, 0.05\\}$ and the maximum depth of the decision trees $\\xi = \\{3, 4, 5\\}$.\nWe computed their accuracy on the dataset, and found that the parameters did not significantly affect performance (1.0\\% at most).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe used $\\beta = 0.02$ and $\\xi = 4$ which yielded the best performance.\n\n\\paragraph*{CoNLL}\n\nThe CoNLL dataset is a popular NED dataset constructed by Hoffart et al. \\cite{Hoffart2011}.\nThe dataset is based on NER data from the CoNLL 2003 shared task, and consists of training, development, and test sets, containing 946, 216, and 231 documents, respectively.\nWe trained our machine learning model using the training set and reported its performance using the test set.\nWe also used the development set for the parameter tuning described above.\nFollowing \\cite{Hoffart2011}, we only used 27,816 mentions with valid entries in the KB and reported the standard micro- (aggregates over all mentions) and macro- (aggregates over all documents) accuracies of the top-ranked candidate entities to assess disambiguation performance.\nFor candidate generation, we used a public dataset\\footnote{\\url{https://github.com/masha-p/PPRforNED}} built by Pershina et al. \\cite{pershina-he-grishman:2015:NAACL-HLT}.\n\n\\paragraph*{TAC 2010}\n\nThe TAC 2010 dataset is another popular NED dataset constructed for the Text Analysis Conference (TAC)\\footnote{\\url{http://www.nist.gov/tac/}} \\cite{Ji2010}.\nThe dataset is based on news articles from various agencies and Web log data, and consists of a training and a test set containing 1,043 and 1,013 documents, respectively.\nFollowing past work \\cite{he-EtAl:2013:Short,TACL494}, we used mentions only with a valid entry in the KB, and reported the micro-accuracy score of the top-ranked candidate entities. We trained our model using the training set and assessed its performance using the test set.\nWe trained our model using the training set and assessed its performance using the test set.\nConsequently, we evaluated our model on 1,020 mentions contained in the test set.\nFor candidate generation, we used a dictionary that was directly built from the Wikipedia dump mentioned previously.\nSimilar to past work, we retrieved possible mention surfaces of an entity from (1) the title of the entity, (2) the title of another entity redirecting to the entity, and (3) the names of anchors that point to the entity.\nFurther, we retained the top 50 candidates through their entity priors for computational efficiency.\n\n\\subsubsection{Comparison with State-of-the-art Methods}\n\nWe compared our method with the following recently proposed state-of-the-art methods:\n\\begin{itemize}[itemsep=0em,topsep=0.3em]\n\\item Hoffart et al. \\cite{Hoffart2011} is a graph-based approach that finds a dense subgraph of entities in a document to address NED.\n\\item He et al. \\cite{he-EtAl:2013:Short} uses deep neural networks to derive the representations of entities and mention contexts and applies them to NED.\n\\item Chisholm and Hachey \\cite{TACL494} uses a Wikilinks dataset \\cite{singh12:wiki-links} to improve the performance of NED.\n\n\n\n\n\n\n\n\\end{itemize}\n\\subsubsection{Results}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{l|ccc}\n\\hline\n& \\begin{tabular}{@{}c@{}} \\footnotesize{CoNLL} \\\\ \\footnotesize{(Micro)}\\end{tabular} & \\begin{tabular}{@{}c@{}}\\footnotesize{CoNLL} \\\\ \\footnotesize{(Macro)}\\end{tabular} & \\footnotesize{TAC10} \\\\\n\\hline\n\\footnotesize{Our Method} & \\textbf{93.1} & \\textbf{92.6} & \\textbf{85.2} \\\\\n\\hline\n\\footnotesize{Hoffart et al., 2011} & 82.5 & 81.7 & - \\\\\n\\footnotesize{He et al., 2013} & 85.6 & 84.0 & 81.0 \\\\\n\\footnotesize{Chisholm \\& Hachey, 2015} & 88.7 & - & 80.7 \\\\\n\n\\hline\n\\end{tabular}\n\\caption{Experimental results of NED using the proposed method and state-of-the-art methods.}\n\\label{tb:ned-results}\n\\end{table}\n\nTable \\ref{tb:ned-results} shows the experimental results of our proposed method as well as those of state-of-the-art methods.\nOur proposed method achieved a 93.1\\% micro-accuracy and 92.6\\% macro-accuracy on the CoNLL dataset, and 85.2\\% micro-accuracy on the TAC 2010 dataset.\nOur method significantly outperformed all the other state-of-the-art methods on both datasets by significant margins.\n\n\n\\subsubsection{Feature Study}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{p{2.8cm}|cc}\n\\hline\n& \\begin{tabular}{@{}c@{}}\\small{Micro}\\\\\\small{accuracy}\\end{tabular} & \\begin{tabular}{@{}c@{}}\\small{Macro} \\\\ \\small{accuracy}\\end{tabular} \\\\\n\\hline\n\\textbf{CoNLL:}\\\\\n\\hline\n\\small{Base} & 85.4 & 87.4 \\\\\n\n\n\n\n\n\n\n\\small{+String similarity} & 85.8 & 87.8 \\\\\n\n\n\n\n\n\n\n\\small{+Textual context} & 90.9 & 92.4 \\\\\n\n\n\n\n\n\n\n\\small{+Coherence} & 91.4 & 92.1 \\\\\n\n\n\n\n\n\n\n\\small{Two-step} & \\textbf{93.1} & \\textbf{92.6} \\\\\n\n\n\n\n\n\n\n\\hline\n\\textbf{TAC 2010:} \\\\\n\\hline\n\\small{Base} & 80.1 & - \\\\\n\n\n\n\n\n\n\n\\small{+String similarity} & 81.7 & - \\\\\n\n\n\n\n\n\n\n\\small{+Textual context} & 84.6 & - \\\\\n\n\n\n\n\n\n\n\\small{+Coherence} & \\textbf{85.5} & - \\\\\n\n\n\n\n\n\n\n\\small{Two-step} & 85.2 & - \\\\\n\n\n\n\n\n\n\n\\hline\n\\end{tabular}\n\\caption{The results of our feature study.}\n\\label{tb:feature-study}\n\\end{table}\n\nWe conducted a feature study on our method.\nWe began with base features, added various features to our system incrementally, and reported their impact on performance.\nWe then introduced our two-step approach to achieve the final results.\n\nTable \\ref{tb:feature-study} shows the results.\nSurprisingly, we attained results comparable with those of most state-of-the-art methods on the both datasets by only using base features.\nAdding string similarity features slightly further improved performance.\n\nWe observed significant improvement when adding textual context features based on our proposed embedding.\nOur method outperformed other state-of-the-art methods without using coherence.\n\nFurther, coherence based on unambiguous entity mentions and our two-step approach significantly improved performance on the CoNLL dataset.\nHowever, it did not contribute to performance on the TAC 2010 dataset.\nThis was because of the significant difference in the density of entity mentions between the datasets.\nThe CoNLL dataset contains approximately 20 entity mentions per document, but the TAC 2010 only contains approximately one mention per document which is unarguably insufficient to model coherence.\n\n\n\\subsubsection{Error Analysis}\n\nWe also conducted an error analysis on the CoNLL test set.\nWe observed that approximately 48.6\\% errors were caused by \\textit{metonymy} mentions \\cite{Ling2015} (i.e., mentions with more than one plausible annotation).\nIn particular, our NED method often erred when an incorrect entity was highly popular and exactly matched the mention surface (e.g., ``South Africa'' referring to the entity \\textsf{South Africa national rugby union team} rather than the entity \\textsf{South Africa}).\nThis makes sense because our machine learning model uses the popularity statistics of the KB (i.e., prior probability and entity prior), and the string similarity between the title of the entity and the mention surface.\nThis problem is discussed further in \\cite{Ling2015}.\n\n\\section{Related Work}\n\nEarly NED methods addressed the problem as a well-studied \\textit{word sense disambiguation} problem \\cite{Mihalcea2007}.\nThese methods primarily focused on modeling the similarity of \\textit{textual} (\\textit{local}) context.\nMost recent state-of-the-art methods focus on modeling \\textit{coherence} among disambiguated entities in the same document \\cite{Cucerzan2007,Milne2008,Hoffart2011,Ratinov2011}.\nThese approaches have also been called \\textit{collective} or \\textit{global} approaches in the literature.\n\n\n\n\nLearning the representations of entities for NED has been addressed in past literature.\nGuo and Barbosa \\cite{Guo2014b} used random walks on KB graphs to construct vector representations of entities and documents to address NED.\nBlanco et al. \\cite{Blanco2015a} proposed a method to map entities into the word embedding (i.e., Word2vec \\cite{Mikolov2013a}) space using entity descriptions in the KB and applied it for NED.\nHe et al. \\cite{he-EtAl:2013:Short} used deep neural networks to compute representations of entities and contexts of mentions directly from the KB.\nSimilarly, Sun et al. \\cite{Sum2015} proposed a method based on deep neural networks to model representations of mentions, contexts of mentions, and entities.\nHuang et al. \\cite{DBLP:journals/corr/HuangHJ15} also leveraged deep neural networks to learn entity representations such that the consequent pairwise entity relatedness was more suitable than of a standard method (i.e., WLM) for NED.\nFurther, Hu et al. \\cite{hu-EtAl:2015:ACL-IJCNLP} used hierarchical information in the KB to build entity embedding and applied it to model coherence.\nUnlike these methods, our proposed approach involves jointly learning vector representations of entities as well as words, hence enabling the accurate computation of the semantic similarity among its items to model both the textual context and coherence.\n\n\n\n\n\n\n\n\n\n\n\nFurthermore, in the context of \\textit{knowledge graph embedding}, another tenor of recent works has been published \\cite{AAAI113659,NIPS2013_5028,AAAI159571}.\nThese methods focus on learning vector representations of entities to primarily address the \\textit{link prediction} task that aims to predict a new fact based on existing facts in KB.\nParticularly, Wang et al. \\cite{wang-EtAl:2014:EMNLP20145} have recently revealed that the joint modeling of the embedding of words and entities can improve performance in several tasks including the link prediction task, which is somewhat analogous to our experimental results.\n\n\\section{Conclusions}\n\nIn this paper, we proposed an embedding method to jointly map words and entities into the same continuous vector space.\nOur method enables us to effectively model both \\textit{textual} and \\textit{global} contexts.\nFurther, armed with these context models, our NED method significantly outperforms state-of-the-art NED methods.\n\nIn future work, we intend to improve our model by leveraging relevant knowledge, such as relations in a knowledge graph (e.g., Freebase).\nWe would also like to seek applications of our proposed embedding other than NED.\n\nThe code of the proposed embedding method and the pre-trained vectors used in our experiments will be made publicly available before the conference.\n\n\\bibliographystyle{naaclhlt2016}\n\\bibliography{library}\n\n\n", "itemtype": "equation", "pos": 17614, "prevtext": "\nwhere $W_{c_m}$ is a set of the context words of mention $m$ and $\\vec{v_w} \\in \\mathbf{V}$ denotes the vector representation of word $w$.\nWe use all noun words in document $d$ as context words.\\footnote{We used Apache OpenNLP tagger to detect nouns. {\\url{https://opennlp.apache.org/}}}\nMoreover, we ignore a context word if the surface of mention $m$ contains it.\n\nWe then measure the similarity between candidate entity and the derived textual context by using cosine similarity between $\\vec{v_{c_w}}$ and the vector of entity $\\vec{v_e}$.\n\n\\subsubsection{Modeling Coherence}\n\\label{subsubsec:coherence}\n\nIt has been revealed that effectively modeling coherence in the assignment of entities to mentions is important for NED.\nHowever, this is a chicken-and-egg problem because the assignment of entities to mentions, which is required to measure coherence, is not possible prior to performing NED.\n\nTo address this problem, we introduce a simple \\textit{two-step} approach:\nwe first train the machine learning model using the coherence score among unambiguous mentions\\footnote{We consider that mention $m$ unambiguously refers to entity $e$ if its prior probability $P(e|m)$ is greater than 0.95.}, in addition to other features, and then retrain the model using the coherence score among the predicted entity assignments instead.\n\nTo estimate coherence, we first calculate the vector representation of the context entities and measure the similarity between the vector of the context entities and that of the target entity $e$.\nNote that context entities are unambiguous entities in the first step, and predicted entities are used instead in the second step.\n\nTo derive the vector representation of context entities, we average their vector representations:\n\n", "index": 21, "text": "\\begin{equation}\n\\vec{v_{c_e}} = \\frac{1}{|E_{c_m}|}\\sum_{e^* \\in E_{c_m}}\\vec{v_{e^*}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\vec{v_{c_{e}}}=\\frac{1}{|E_{c_{m}}|}\\sum_{e^{*}\\in E_{c_{m}}}\\vec{v_{e^{*}}}\" display=\"block\"><mrow><mover accent=\"true\"><msub><mi>v</mi><msub><mi>c</mi><mi>e</mi></msub></msub><mo stretchy=\"false\">\u2192</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>E</mi><msub><mi>c</mi><mi>m</mi></msub></msub><mo stretchy=\"false\">|</mo></mrow></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>e</mi><mo>*</mo></msup><mo>\u2208</mo><msub><mi>E</mi><msub><mi>c</mi><mi>m</mi></msub></msub></mrow></munder><mover accent=\"true\"><msub><mi>v</mi><msup><mi>e</mi><mo>*</mo></msup></msub><mo stretchy=\"false\">\u2192</mo></mover></mrow></mrow></mrow></math>", "type": "latex"}]