[{"file": "1601.04366.tex", "nexttext": "\n\nWe propose an innovative view on the pivot column selection criterion,\nwhich is closely associated to the selection of variables in least-angle\nregression (LAR)~\\cite{Efron2004}. At each step, the method keeps a current\nestimate of the regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$, which is obtained by moving along the\ndirection of the vectors within the current approximation(s).\n\nIn comparison to existing methods, it has the following advantages. First, the\nmethod is aware of multiple kernel functions. In each step, the next pivot to\nbe added will be chosen greedily from all remaining pivots from all kernels.\nAccordingly, kernels that give more information about the current residual are\nmore likely to be selected. In contrast to methods that assume access to the\ncomplete kernel matrices, the importance of a kernel is determined already at the time of\napproximation.\n\nSecond, the explicit notion of the approximation error is completely abolished\nin pivot selection criterion.  Since in the limit the approximations are exact\n(Eq.~\\ref{e:icd_sequence}), we consider only the gain with respect to the\ncurrent residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$. Although accurate approximation is\nproportional to the similarity of the model using the full kernel\nfunction~\\cite{cortes2010two}, it was recently shown that i) the expected\ngeneralization error is related to maximal marginal degrees of freedom rather\nthan the approximation error and ii) empirically, low-rank approximation can\nresult in a regularization-like effect~\\cite{Bach2012}.\n\nThe high-level pseudo code of the \\texttt{mklaren} algorithm is shown in\nAlgorithm~\\ref{a:mklaren}.  Each step is described in detail in the following.\n\n\n\\subsection*{Simultaneous Incomplete Cholesky decompositions}\n\nEach kernel function $k_q$ is associated to its corresponding kernel matrix\n${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}_q$, which is in turn associated to its low-rank approximation ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$.\nLet {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}} denote any of the matrices ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p$. \n\nInitially, ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$ is initialized to ${\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$, the diagonal vector ${{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}} =\n\\text{diag}({\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}})}$ and the active set $\\mathcal{A} = \\emptyset$. At step $j$,\na pivot $i$ is selected from the remaining set $\\mathcal{J} = \\{1, 2,\n...,n \\} \\setminus \\mathcal{A}$ and its corresponding \\emph{pivot column} ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_j =\n{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, j)$ is computed as follows:\n\n", "itemtype": "equation", "pos": 9737, "prevtext": "\n\n\n\n\\title{Learning the kernel matrix via predictive low-rank approximations}\n\\author{\\IEEEauthorblockN{Martin Stra\u00c5\u00bear}\n\\IEEEauthorblockA{Faculty of Computer and Information Science \\\\ University of Ljubljana,\\\\ Ve\u00c4\u008dna pot 113, 1000 Ljubljana, Slovenia \\\\ Email: martin.strazar@fri.uni-lj.si}\n\\and\n\\IEEEauthorblockN{Toma\u00c5\u00be Curk}\n\\IEEEauthorblockA{Faculty of Computer and Information Science \\\\ University of Ljubljana,\\\\ Ve\u00c4\u008dna pot 113, 1000 Ljubljana, Slovenia \\\\ Email: tomaz.curk@fri.uni-lj.si}\n}\n\n\n\\maketitle\n\n\\begin{abstract} Efficient and accurate low-rank approximations to multiple\ndata sources are essential in the era  of big data. The scaling of kernel-based\nlearning algorithms to large datasets is limited by the $O(n^2)$ complexity\nassociated with computation and storage of the kernel matrix, which is assumed\nto be available in most recent multiple kernel learning algorithms. We propose\na method to learn simultaneous low-rank approximations of a set of base\nkernels in regression tasks. \n\nWe present the {\\texttt{mklaren}}\\ algorithm, which approximates \\texttt{m}ultiple\n\\texttt{k}ernel matrices with \\texttt{l}east \\texttt{a}ngle\n\\texttt{re}gressio\\texttt{n} in the low-dimensional feature space. The idea is\nbased on geometrical concepts and does not assume access to full\nkernel matrices. The algorithm achieves linear complexity in the number of\ndata points as well as kernels, while it accounts for the correlations between\nkernel matrices.  When explicit feature space representation is available for\nkernels, we use the relation between primal and dual regression weights to gain\nmodel interpretation.  Our approach outperforms contemporary kernel matrix\napproximation approaches when learning with multiple kernels on standard\nregression datasets, as well as improves selection of relevant kernels in\ncomparison to multiple kernel learning methods.  \\end{abstract}\n\n\n\n\n\\section*{Introduction}\n\n\nKernel methods are popular in machine learning as they model relations between\nobjects in feature spaces of arbitrary, even infinite\ndimension~\\cite{scholkopf2002learning}. To avoid working with explicit\nrepresentations, algorithms are phrased to depend only on \\emph{inner products}\nin such feature spaces. Any symmetric positive semidefinite matrix can be\nassociated to inner products between training data in some feature space. Such\n\\emph{kernel matrix} is used in learning algorithms via the kernel trick. Along\nwith gain in accuracy of the rich representation, it is useful for learning\nwith objects not representble as vectors. Examples include structured objects,\nstrings or trees.  The availability of the inner product values for all pairs\nof data points presents a substantial drawback as the computation and storage\nof the kernel matrix typically scales as $O(n^2)$ in the number of data\ninstances. Kernel approximations are thus indispensable when learning on large\ndatasets and can be roughly classified in two families: approximations of the\n\\emph{kernel function} or approximations of the \\emph{kernel matrix}.\n\n\nDirect approximation of the kernel function can achieve significant performance\ngains. A large body of work relies on approximating the frequently used\nGaussian kernel, which has a rapidly decaying eigenspectrum, as proved by the\nBochner's theorem and the concept of random features\\cite{Rahimi2007}. Recently,\nthe property of matrices generated by the Gaussian kernel were further\nexploited to achieve sublinear complexity in the approximation\nrank~\\cite{Yang2014,Si2014,Le2013a}.  In another line of work, low-dimensional\nfeatures can be derived for translation-invariant kernels based on their\nFourier transform~\\cite{Vedaldi2012}. The mentioned family of methods currently\npresents the most space- and time- efficient approximations, but is\nnevertheless limited to kernels of particular functional forms.\n \n\nApproximations to the kernel matrix are applicable to any symmetric\npositive-definite matrix even if the underlying kernel function is unknown.\nThey are termed \\emph{data dependent} and can be obtained via the eigenvalue\ndecomposition, selection a subset of datapoints (the Nystr\\\"om method) or\nCholesky decomposition in order to minimize the difference between the original\nmatrix and its low-rank\napproximation~\\cite{rudi2015less,Xu2015,Li2015,Gittens2013,Williams2001,\nFine2001}. However, side information such as target variables is commonly\ndisregarded. Predictive decompositions use the target variables to obtain a\nsupervised low-rank approximation via the Incomplete Cholesky\ndecomposition~\\cite{Bach2005} or minimization of Bregman divergence\nmeasures~\\cite{Kulis2009}.  \n\n\nThe choice of optimal kernel for a given learning task is often non-trivial.\nMultiple kernel learning (MKL) methods learn the optimal weighted sum of kernel\nmatrices with respect to the target variables, such as class\nlabels~\\cite{Gonen2011}. Different kernels can thus be used to model the data\nand their relative importance is assessed via the predictive accuracy, offering\ninsights into the domain of interest.  Depending on the user-defined\nconstraints, the resulting optimization problems are quadratic (QP) or\nsemidefinite programs (SDP), assuming the complete knowledge of the kernel\nmatrices.  Low-rank approximations have been used for MKL, e.g., by performing\nGram-Schmidt orthogonalization and subsequently MKL~\\cite{Kandola2002}. In a\nrecent attempt, the combined kernel matrix is learned via efficient generalized\nForward-backward algorithm, however assuming that low-rank\napproximations are available beforehand~\\cite{Rakotomamonjy2014}. G\\\"{o}nen et\nal. develop a Bayesian treatment for joint dimensionality reduction and\nclassification, solved via gradient descent~\\cite{Gonen2010} or variational\napproximation~\\cite{Kaski2014}, while assuming access to full kernel matrices\nand not exploiting their symmetric structure. \n\n\nIn this work, we propose a joint treatment of low-rank kernel approximations\nand MKL. We design {\\texttt{mklaren}}, a greedy algorithm that couples Incomplete Cholesky\nDecomposition and Least-angle regression to simultaneously learn low-rank\napproximations and regression coefficients. Multiple kernel learning is\nperformed implicitly by iteratively selecting the appropriate portions of the\napproximations to the feature space induced by the set of kernels. Finally, when\nexplicit feature space representation is available for kernels, the relation\nbetween primal and dual regression weights to achieve model interpretation. In\ncontrast to contemporary approaches, which rely on convex optimization or\nBayesian methods to learn the combination of kernels, our approach\nrelies on geometrical principles solely, enhancing simplicity and\ncomputational complexity.\n\n\n\nA common assumption when applying matrix approximation or MKL is that the\nresulting decomposition can only be applied in \\emph{transductive\nlearning}~\\cite{Zhang2012, Lanckriet2004a}, i.e., prediction is possible only if\ntest examples are available at the training phase.  Here, we show that by means\nof relating Incomplete Cholesky decomposition and the Nystr\\\"om method, it is\npossible to circumvent this apparent limitation and predict the value of any\ndata point, even if it was not part of the training set.\n\n\nThe resulting method assumes linear time computational complexity in the number\nof data points as well as kernels, and compared favorable against related\nlow-rank kernel matrix approximation and state-of-the-art MKL approaches. The\npredictive performance and model interpretation is evaluated empirically on\nmultiple benchmark regression datasets. Additionally, we isolate the effect of\nlow-rank approximation and compare the method with full kernel matrix MKL\nmethods on a very large set of rank-one kernels.\n\n\n\\section*{Multiple kernel learning with least-angle regression}\n\n\nLet $\\{{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_n\\}$ be a set of points in a Hilbert\nspace $\\mathcal{X}$, associated with targets ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\in \\mathbb{R}^n$. Let the\nHilbert spaces $\\mathcal{X}_1, \\mathcal{X}_2, ..., \\mathcal{X}_p$ be isomorphic\nto $\\mathcal{X}$ and endowed with respective inner product (kernel) functions\n$k_1, k_2, ... k_p$. The kernels $k_q$ are positive definite and map from\n$\\mathcal{X}_q \\times \\mathcal{X}_q$ to $\\mathbb{R}$. Hence, a data point\n${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}} \\in \\mathcal{X}_q$ can be represented in multiple inner product spaces,\nwhich could be related to different data views or representations.  The goal of\na supervised approximation algorithm is to learn the corresponding low-rank\napproximations ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p$ and the regression line\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} \\in \\text{span}\\{{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p\\}$ such that\n$\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}\\|$ is minimal.\n\nThe \\texttt{mklaren} algorithm simultaneously learns low-rank approximations to\nkernel matrices ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}_q$ associated to each kernel function $k_q$ using a\nvariant of the well-known Incomplete Cholesky Decomposition (ICD).  This family\nof methods produces a finite sequence of low-rank approximations ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^{(1)},\n{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^{(2)}, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^{(i)}$, such that \n\n\n", "index": 1, "text": "\\begin{equation}    \n{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^{(i)} {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^{(i)\\ T} \\rightarrow {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} \\text{ as } i\n\\rightarrow n.  \n\\label{e:icd_sequence}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{G}}}}}^{(i)}{\\bm{\\mathrm{{{G}}}}}^{(i)\\ T}\\rightarrow{\\bm{%&#10;\\mathrm{{{K}}}}}\\text{ as }i\\rightarrow n.\" display=\"block\"><mrow><mrow><mrow><msup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc06</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>T</mi></mrow></msup></mrow><mo>\u2192</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mtext>\u00a0as\u00a0</mtext><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2192</mo><mi>n</mi></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\nThe selected pivot is added to the active set, the counter $j$ and the diagonal vector are updated:\n\n\n", "itemtype": "equation", "pos": 12846, "prevtext": "\n\nWe propose an innovative view on the pivot column selection criterion,\nwhich is closely associated to the selection of variables in least-angle\nregression (LAR)~\\cite{Efron2004}. At each step, the method keeps a current\nestimate of the regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$, which is obtained by moving along the\ndirection of the vectors within the current approximation(s).\n\nIn comparison to existing methods, it has the following advantages. First, the\nmethod is aware of multiple kernel functions. In each step, the next pivot to\nbe added will be chosen greedily from all remaining pivots from all kernels.\nAccordingly, kernels that give more information about the current residual are\nmore likely to be selected. In contrast to methods that assume access to the\ncomplete kernel matrices, the importance of a kernel is determined already at the time of\napproximation.\n\nSecond, the explicit notion of the approximation error is completely abolished\nin pivot selection criterion.  Since in the limit the approximations are exact\n(Eq.~\\ref{e:icd_sequence}), we consider only the gain with respect to the\ncurrent residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$. Although accurate approximation is\nproportional to the similarity of the model using the full kernel\nfunction~\\cite{cortes2010two}, it was recently shown that i) the expected\ngeneralization error is related to maximal marginal degrees of freedom rather\nthan the approximation error and ii) empirically, low-rank approximation can\nresult in a regularization-like effect~\\cite{Bach2012}.\n\nThe high-level pseudo code of the \\texttt{mklaren} algorithm is shown in\nAlgorithm~\\ref{a:mklaren}.  Each step is described in detail in the following.\n\n\n\\subsection*{Simultaneous Incomplete Cholesky decompositions}\n\nEach kernel function $k_q$ is associated to its corresponding kernel matrix\n${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}_q$, which is in turn associated to its low-rank approximation ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$.\nLet {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}} denote any of the matrices ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p$. \n\nInitially, ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$ is initialized to ${\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$, the diagonal vector ${{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}} =\n\\text{diag}({\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}})}$ and the active set $\\mathcal{A} = \\emptyset$. At step $j$,\na pivot $i$ is selected from the remaining set $\\mathcal{J} = \\{1, 2,\n...,n \\} \\setminus \\mathcal{A}$ and its corresponding \\emph{pivot column} ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_j =\n{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, j)$ is computed as follows:\n\n", "index": 3, "text": "\\begin{equation}\n\\begin{split}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(i, j) &= \\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i)} \\\\\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(\\mathcal{J}, j) &= \\frac{1}{{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(i, j)} \n    \\bigg({\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{J}, i) - \\sum_{l=1}^{j-1}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(\\mathcal{J}, l){\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(i, l) \\bigg) \\\\\n\\label{e:icd}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{G}}}}}(i,j)&amp;\\displaystyle=\\sqrt{{\\bm{%&#10;\\mathrm{{{d}}}}}(i)}\\\\&#10;\\displaystyle{\\bm{\\mathrm{{{G}}}}}(\\mathcal{J},j)&amp;\\displaystyle=\\frac{1}{{\\bm{%&#10;\\mathrm{{{G}}}}}(i,j)}\\bigg{(}{\\bm{\\mathrm{{{K}}}}}(\\mathcal{J},i)-\\sum_{l=1}^%&#10;{j-1}{\\bm{\\mathrm{{{G}}}}}(\\mathcal{J},l){\\bm{\\mathrm{{{G}}}}}(i,l)\\bigg{)}\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><msqrt><mrow><mi>\ud835\udc1d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mrow><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nImportantly, only the information on one column of ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}$ is required at each\nstep and ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T$ is never computed explicitly. The\nquality of approximation is influenced by the subset of the pivot columns used\nfor approximation. The pivots are selected based on a heuristic in a greedy\nmanner. Various options exist, such as the lower-bound on the gain in reduction\nof the approximation error~\\cite{Fine2001}, or a weighted cost function\nconsidering approximation error and targets~\\cite{Bach2005}.\n\nIn the method proposed by Fine \\& Scheinberg~\\cite{Fine2001}, $i$ is selected according to maximum value in {\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}},\nwhich represents the lower bound on the gain in approximation\nerror. One could perform the decomposition for each kernel\n$k_q$ independently and subsequently weight kernels in spirit of MKL (e.g., see~\\cite{Kandola2002}). Instead, our pivot selection criterion takes into\naccount the current residual as well as all other kernels. This is achieved as\nfollows; consider the subspace spanned by the union of columns within all $p$\napproximations \n\n", "itemtype": "equation", "pos": 13466, "prevtext": "\nThe selected pivot is added to the active set, the counter $j$ and the diagonal vector are updated:\n\n\n", "index": 5, "text": "\\begin{equation}\n\\begin{split}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}  &= {\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_j^2 \\\\\n    \\mathcal{A} &= \\mathcal{A} \\cup \\{i\\}\\\\\n    j &= j + 1\n\\label{e:icd_2}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{d}}}}}&amp;\\displaystyle={\\bm{\\mathrm{{{d%&#10;}}}}}-{\\bm{\\mathrm{{{g}}}}}_{j}^{2}\\\\&#10;\\displaystyle\\mathcal{A}&amp;\\displaystyle=\\mathcal{A}\\cup\\{i\\}\\\\&#10;\\displaystyle j&amp;\\displaystyle=j+1\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mi>\ud835\udc1d</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udc1d</mi><mo>-</mo><msubsup><mi>\ud835\udc20</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>\u222a</mo><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>j</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n $\\text{span}({\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}) \\subset \\mathbb{R}^n$ and ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}} \\in \\mathbb{R}^{n\n\\times \\sum_{q}j_q}$.  Treating ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}$ as a feature space in context of\nregression enables greedy selection of further columns, to be added using the\nleast-angle regression criterion.\n\n\n\\subsection*{Least-angle regression based pivot selection}\n\nLeast-angle regression (LAR) is an \\emph{active set method}, originally\ndesigned for feature subset selection in linear regression~\\cite{Efron2004}.\nThe original method assumes the access to all candidate columns to be added to\nthe feature space, making the combination of ICD and LAR without significant\nincrease in complexity non-trivial. Thorough description of LAR is hereby\nskipped for brevity and can be found in the Appendix.  The following is an\nadaptation of the LAR method to Cholesky decomposition, where the exact values\nof pivot columns to be added are unknown.\n\nConsider the matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}$ in\nEq.~\\ref{def:bh} and the set $\\mathcal{J}_q = \\{1, 2, ..., n \\} \\setminus\n\\mathcal{A}_q$ containing \\emph{candidate} pivot columns yet to be included in\ndecomposition of $k_q$.  The regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ is initialized to {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}.\nFor all columns ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_i$ in {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}, define vectors\n\n\n", "itemtype": "equation", "pos": 14960, "prevtext": "\n\nImportantly, only the information on one column of ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}$ is required at each\nstep and ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T$ is never computed explicitly. The\nquality of approximation is influenced by the subset of the pivot columns used\nfor approximation. The pivots are selected based on a heuristic in a greedy\nmanner. Various options exist, such as the lower-bound on the gain in reduction\nof the approximation error~\\cite{Fine2001}, or a weighted cost function\nconsidering approximation error and targets~\\cite{Bach2005}.\n\nIn the method proposed by Fine \\& Scheinberg~\\cite{Fine2001}, $i$ is selected according to maximum value in {\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}},\nwhich represents the lower bound on the gain in approximation\nerror. One could perform the decomposition for each kernel\n$k_q$ independently and subsequently weight kernels in spirit of MKL (e.g., see~\\cite{Kandola2002}). Instead, our pivot selection criterion takes into\naccount the current residual as well as all other kernels. This is achieved as\nfollows; consider the subspace spanned by the union of columns within all $p$\napproximations \n\n", "index": 7, "text": "\\begin{equation}\n\\begin{split} \n    {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}} &= \\big( {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1 \\ {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2 \\ \\cdots\u00c2\u00a0\\ {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p \\big) = \\\\\n           &= \\big({\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{1,1} \\  \\cdots {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{1,j_1} \\    \n               {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{2,1} \\cdots {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{2,j_2} \\cdots\n               {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{p,1}  \\cdots {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{p,j_p}  \\big) \n    \\label{def:bh}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{H}}}}}&amp;\\displaystyle=\\big{(}{\\bm{%&#10;\\mathrm{{{G}}}}}_{1}\\ {\\bm{\\mathrm{{{G}}}}}_{2}\\ \\cdots\\unichar{194}\\unichar{1%&#10;60}\\ {\\bm{\\mathrm{{{G}}}}}_{p}\\big{)}=\\\\&#10;&amp;\\displaystyle=\\big{(}{\\bm{\\mathrm{{{g}}}}}_{1,1}\\ \\cdots{\\bm{\\mathrm{{{g}}}}}%&#10;_{1,j_{1}}\\ {\\bm{\\mathrm{{{g}}}}}_{2,1}\\cdots{\\bm{\\mathrm{{{g}}}}}_{2,j_{2}}%&#10;\\cdots{\\bm{\\mathrm{{{g}}}}}_{p,1}\\cdots{\\bm{\\mathrm{{{g}}}}}_{p,j_{p}}\\big{)}%&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mi>\ud835\udc07</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mpadded width=\"+5pt\"><msub><mi>\ud835\udc06</mi><mn>1</mn></msub></mpadded><mo>\u2062</mo><mpadded width=\"+5pt\"><msub><mi>\ud835\udc06</mi><mn>2</mn></msub></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mn>194</mn><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mpadded width=\"+5pt\"><mn>160</mn></mpadded><mo>\u2062</mo><msub><mi>\ud835\udc06</mi><mi>p</mi></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mi/></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mpadded width=\"+5pt\"><msub><mi>\ud835\udc20</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mpadded><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><mpadded width=\"+5pt\"><msub><mi>\ud835\udc20</mi><mrow><mn>1</mn><mo>,</mo><msub><mi>j</mi><mn>1</mn></msub></mrow></msub></mpadded><mo>\u2062</mo><msub><mi>\ud835\udc20</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>\ud835\udc20</mi><mrow><mn>2</mn><mo>,</mo><msub><mi>j</mi><mn>2</mn></msub></mrow></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>\ud835\udc20</mi><mrow><mi>p</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>\ud835\udc20</mi><mrow><mi>p</mi><mo>,</mo><msub><mi>j</mi><mi>p</mi></msub></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nwhere {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} is the centering projection matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} = ({\\ensuremath{\\boldsymbol{\\mathrm{{{I}}}}}} -\n\\frac{{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T}{n})$ and $s_i$ is the sign of the correlation\n$({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_i)^T({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}})$. The active matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$\nrepresenting the modified feature space equals \n", "itemtype": "equation", "pos": 17156, "prevtext": "\n $\\text{span}({\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}) \\subset \\mathbb{R}^n$ and ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}} \\in \\mathbb{R}^{n\n\\times \\sum_{q}j_q}$.  Treating ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}$ as a feature space in context of\nregression enables greedy selection of further columns, to be added using the\nleast-angle regression criterion.\n\n\n\\subsection*{Least-angle regression based pivot selection}\n\nLeast-angle regression (LAR) is an \\emph{active set method}, originally\ndesigned for feature subset selection in linear regression~\\cite{Efron2004}.\nThe original method assumes the access to all candidate columns to be added to\nthe feature space, making the combination of ICD and LAR without significant\nincrease in complexity non-trivial. Thorough description of LAR is hereby\nskipped for brevity and can be found in the Appendix.  The following is an\nadaptation of the LAR method to Cholesky decomposition, where the exact values\nof pivot columns to be added are unknown.\n\nConsider the matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}$ in\nEq.~\\ref{def:bh} and the set $\\mathcal{J}_q = \\{1, 2, ..., n \\} \\setminus\n\\mathcal{A}_q$ containing \\emph{candidate} pivot columns yet to be included in\ndecomposition of $k_q$.  The regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ is initialized to {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}.\nFor all columns ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_i$ in {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}, define vectors\n\n\n", "index": 9, "text": "\\begin{equation}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i = s_i {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} {\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_i\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{h}}}}}_{i}=s_{i}{\\bm{\\mathrm{{{P}}}}}{\\bm{\\mathrm{{{g}}}}}_{i}\" display=\"block\"><mrow><msub><mi>\ud835\udc21</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc0f\ud835\udc20</mi><mi>i</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": " \n \nBy elementary linear algebra, there exist an equiangular vector ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$,\nhaving $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A\\| = 1$ and spanning equal angles, less than 90 degrees,\nbetween the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ and vectors in ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$.  Moving the\nregression line\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ along ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nwhere {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} is the centering projection matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} = ({\\ensuremath{\\boldsymbol{\\mathrm{{{I}}}}}} -\n\\frac{{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T}{n})$ and $s_i$ is the sign of the correlation\n$({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_i)^T({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}})$. The active matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$\nrepresenting the modified feature space equals \n", "index": 11, "text": "\\begin{equation} {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A = \\big( \\cdots\n{\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i \\cdots \\big) \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{H}}}}}_{A}=\\big{(}\\cdots{\\bm{\\mathrm{{{h}}}}}_{i}\\cdots\\big{)}\" display=\"block\"><mrow><msub><mi>\ud835\udc07</mi><mi>A</mi></msub><mo>=</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>\ud835\udc21</mi><mi>i</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\ncauses the correlations $c_i = {\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i^T ({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}) / \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i\\|$ to change equally\nfor all ${\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i$.\n\n\nLet $q$ be an index of any kernel having nonempty remaining set $\\mathcal{J}_q$.\nDefine the following quantities $C = \\text{max}_i\\{c_i\\}$ and A =\n$({\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}})^{-1/2}$.  The gradient $\\gamma_q$ is then\nselected as follows. \n\n\n", "itemtype": "equation", "pos": 18619, "prevtext": " \n \nBy elementary linear algebra, there exist an equiangular vector ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$,\nhaving $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A\\| = 1$ and spanning equal angles, less than 90 degrees,\nbetween the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ and vectors in ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$.  Moving the\nregression line\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ along ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$\n\n", "index": 13, "text": "\\begin{equation}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}^{\\text{new}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} + \\gamma {\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A \n    \\label{e:lar_update}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{\\mu}}}}}^{\\text{new}}={\\bm{\\mathrm{{{\\mu}}}}}+\\gamma{\\bm{%&#10;\\mathrm{{{u}}}}}_{A}\" display=\"block\"><mrow><msup><mi>\ud835\udf41</mi><mtext>new</mtext></msup><mo>=</mo><mrow><mi>\ud835\udf41</mi><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc2e</mi><mi>A</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nHere, $\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}$ is the \\emph{approximation to the pivot column}\n${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q,l}$ if the pivot $l$ would be chosen to generate a new column in\ndecomposition of $k_q$ using Eq.~\\ref{e:icd};  $\\text{min}^+$ is the minimum\nover positive arguments for each choice of $l$.  The selected pivot is the\nminimizer $l$ of Eq.~\\ref{e:lar_gamma}.  Finally, the gradient $\\gamma$ and\nnext kernel $q$ are selected by $\\gamma = \\text{min}_q \\{ \\gamma_q \\}$ and the\npivot index $l$ is the solution of Eq.~\\ref{e:lar_gamma} for selected $q$.\n\n\nThe approximation $\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}$ to ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q,l}$  is necessary to\ncompute Eq.~\\ref{e:lar_gamma} efficiently and is further described below. The\nexact value of ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$, selected as described above, remains unknown\nuntil the actual step of Cholesky decomposition is computed using\nEq.~\\ref{e:icd}.  This renders the value of $\\gamma$ invalid, i.e., the change\nin the correlations of the active set would not be the same after applying\nEq.~\\ref{e:lar_update}.  To obtain the correct $\\gamma$ at this point, we\nrecompute Eq.~\\ref{e:lar_gamma} with fixed kernel and pivot indices $q$ and $l$\nrespectively and using the true value of ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$.\n\n\nIn the last step of Algorithm~\\ref{a:mklaren}, as $\\sum_{q} j_q$ reaches maximum allowed rank $K$,\nthe same selection procedure is followed. After the true value is computed for last \n${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$ to be added to ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$ and consequently {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}},\nthe gradient simplifies to $\\gamma = C / A$, yielding the least-squares solution\nfor ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$ and {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}.\n\n\n\n\\subsection*{Look-ahead decompositions}\n\n\nThe Eq.~\\ref{e:lar_gamma} assumes the availability of correlations $c_j$ for\neach of the candidate pivots in the sets $\\mathcal{J}_q$. To avoid \nrecomputing the Cholesky steps, we adapt look-ahead decompositions, proposed by\nBach \\& Jordan~\\cite{Bach2005}.\n\nBy definition of ICD in Eq.~\\ref{e:icd}, the values of a newly added pivot column\nin step $j$ and pivot $i$ are:\n\n", "itemtype": "equation", "pos": 19484, "prevtext": "\ncauses the correlations $c_i = {\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i^T ({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}) / \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i\\|$ to change equally\nfor all ${\\ensuremath{\\boldsymbol{\\mathrm{{{h}}}}}}_i$.\n\n\nLet $q$ be an index of any kernel having nonempty remaining set $\\mathcal{J}_q$.\nDefine the following quantities $C = \\text{max}_i\\{c_i\\}$ and A =\n$({\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}})^{-1/2}$.  The gradient $\\gamma_q$ is then\nselected as follows. \n\n\n", "index": 15, "text": "\\begin{equation}\n\\begin{split}\n\\gamma_q &= \\text{min}^+_{l \\in \\mathcal{J}^q} \\bigg\\{\\frac{C -\nc_l}{A_A - a_l}, \\frac{C + c_l}{A_A + a_l} \\bigg\\} , \\text{where} \\\\\nc_l &= ({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} \\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l})^T({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}) / \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}\\| \\\\\na_l &= ({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} \\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l})^T{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A / \\|\u00c2\u00a0{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}\\|  \\\\\n\\end{split}\n\\label{e:lar_gamma}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\gamma_{q}&amp;\\displaystyle=\\text{min}^{+}_{l\\in%&#10;\\mathcal{J}^{q}}\\bigg{\\{}\\frac{C-c_{l}}{A_{A}-a_{l}},\\frac{C+c_{l}}{A_{A}+a_{l%&#10;}}\\bigg{\\}},\\text{where}\\\\&#10;\\displaystyle c_{l}&amp;\\displaystyle=({\\bm{\\mathrm{{{P}}}}}\\hat{{\\bm{\\mathrm{{{g}%&#10;}}}}}_{q,l})^{T}({\\bm{\\mathrm{{{y}}}}}-{\\bm{\\mathrm{{{\\mu}}}}})/\\|{\\bm{\\mathrm%&#10;{{{P}}}}}\\hat{{\\bm{\\mathrm{{{g}}}}}}_{q,l}\\|\\\\&#10;\\displaystyle a_{l}&amp;\\displaystyle=({\\bm{\\mathrm{{{P}}}}}\\hat{{\\bm{\\mathrm{{{g}%&#10;}}}}}_{q,l})^{T}{\\bm{\\mathrm{{{u}}}}}_{A}/\\|\\unichar{194}\\unichar{160}{\\bm{%&#10;\\mathrm{{{P}}}}}\\hat{{\\bm{\\mathrm{{{g}}}}}}_{q,l}\\|\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\u03b3</mi><mi>q</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><msubsup><mtext>min</mtext><mrow><mi>l</mi><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mi>q</mi></msup></mrow><mo>+</mo></msubsup><mo>\u2062</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">{</mo><mfrac><mrow><mi>C</mi><mo>-</mo><msub><mi>c</mi><mi>l</mi></msub></mrow><mrow><msub><mi>A</mi><mi>A</mi></msub><mo>-</mo><msub><mi>a</mi><mi>l</mi></msub></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>C</mi><mo>+</mo><msub><mi>c</mi><mi>l</mi></msub></mrow><mrow><msub><mi>A</mi><mi>A</mi></msub><mo>+</mo><msub><mi>a</mi><mi>l</mi></msub></mrow></mfrac><mo maxsize=\"210%\" minsize=\"210%\">}</mo></mrow></mrow><mo>,</mo><mtext>where</mtext></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>c</mi><mi>l</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>q</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc32</mi><mo>-</mo><mi>\ud835\udf41</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>q</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>\u2225</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>a</mi><mi>l</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>q</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2e</mi><mi>A</mi></msub></mrow><mo>/</mo><mrow><mo>\u2225</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mn>194</mn><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mn>160</mn><mo>\u2062</mo><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>q</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow><mo>\u2225</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe computation of $c_j$ is made more efficient by replacing ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}$ in\nEq.~\\ref{e:chol_step} by an \\emph{look-ahead} approximation ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}' \\in\n\\mathbb{R}^{n \\times (j + \\delta)}$, using $\\delta$ additional columns.\n\n\n", "itemtype": "equation", "pos": 22633, "prevtext": "\n\nHere, $\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}$ is the \\emph{approximation to the pivot column}\n${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q,l}$ if the pivot $l$ would be chosen to generate a new column in\ndecomposition of $k_q$ using Eq.~\\ref{e:icd};  $\\text{min}^+$ is the minimum\nover positive arguments for each choice of $l$.  The selected pivot is the\nminimizer $l$ of Eq.~\\ref{e:lar_gamma}.  Finally, the gradient $\\gamma$ and\nnext kernel $q$ are selected by $\\gamma = \\text{min}_q \\{ \\gamma_q \\}$ and the\npivot index $l$ is the solution of Eq.~\\ref{e:lar_gamma} for selected $q$.\n\n\nThe approximation $\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}}_{q,l}$ to ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q,l}$  is necessary to\ncompute Eq.~\\ref{e:lar_gamma} efficiently and is further described below. The\nexact value of ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$, selected as described above, remains unknown\nuntil the actual step of Cholesky decomposition is computed using\nEq.~\\ref{e:icd}.  This renders the value of $\\gamma$ invalid, i.e., the change\nin the correlations of the active set would not be the same after applying\nEq.~\\ref{e:lar_update}.  To obtain the correct $\\gamma$ at this point, we\nrecompute Eq.~\\ref{e:lar_gamma} with fixed kernel and pivot indices $q$ and $l$\nrespectively and using the true value of ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$.\n\n\nIn the last step of Algorithm~\\ref{a:mklaren}, as $\\sum_{q} j_q$ reaches maximum allowed rank $K$,\nthe same selection procedure is followed. After the true value is computed for last \n${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{q, l}$ to be added to ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$ and consequently {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}},\nthe gradient simplifies to $\\gamma = C / A$, yielding the least-squares solution\nfor ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$ and {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}.\n\n\n\n\\subsection*{Look-ahead decompositions}\n\n\nThe Eq.~\\ref{e:lar_gamma} assumes the availability of correlations $c_j$ for\neach of the candidate pivots in the sets $\\mathcal{J}_q$. To avoid \nrecomputing the Cholesky steps, we adapt look-ahead decompositions, proposed by\nBach \\& Jordan~\\cite{Bach2005}.\n\nBy definition of ICD in Eq.~\\ref{e:icd}, the values of a newly added pivot column\nin step $j$ and pivot $i$ are:\n\n", "index": 17, "text": "\\begin{equation}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_j = \\frac{({\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} - \\sum_{l=1}^{j-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, l){\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, l)^T)(:, i)}{\\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i)}}\n\\label{e:chol_step}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{g}}}}}_{j}=\\frac{({\\bm{\\mathrm{{{K}}}}}-\\sum_{l=1}^{j-1}{\\bm{%&#10;\\mathrm{{{G}}}}}(:,l){\\bm{\\mathrm{{{G}}}}}(:,l)^{T})(:,i)}{\\sqrt{{\\bm{\\mathrm{%&#10;{{d}}}}}(i)}}\" display=\"block\"><mrow><msub><mi>\ud835\udc20</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0a</mi><mo>-</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc06</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msqrt><mrow><mi>\ud835\udc1d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThis simplification significantly reduces the computation complexity in\ncomputing the quantities $c_l$ and $a_l$ in Eq.~\\ref{e:lar_gamma}. To see this,\nconsider the computation of correlation with the equiangular vector, $a_l$ for\nany (normalized) candidate pivot column ${\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_{l}$.\n\nBy assumption of the LAR algorithm, the predictor variables\nhave mean zero and norm one, which is not the case with pivot columns. This is\nsolved by using the centering projection matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} = ({\\ensuremath{\\boldsymbol{\\mathrm{{{I}}}}}} -\n\\frac{{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T}{n})$, and letting\n\n\n\n", "itemtype": "equation", "pos": 23253, "prevtext": "\n\nThe computation of $c_j$ is made more efficient by replacing ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}$ in\nEq.~\\ref{e:chol_step} by an \\emph{look-ahead} approximation ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}' \\in\n\\mathbb{R}^{n \\times (j + \\delta)}$, using $\\delta$ additional columns.\n\n\n", "index": 19, "text": "\\begin{equation}\n\\begin{split}\n{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_i &= \\frac{({\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'^T- \\sum_{l=1}^{j-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, l){\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:, l)^T)(:, i)}{\\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i)}} \\\\\n            &= \\frac{{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'(:,\\ j{+}1{:}j{+}\\delta) \\  {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'^T (j{+}1{:}j{+}\\delta,\\ i)}{\\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i)}}\n\\label{e:chol_step_lookahead}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\hat{{\\bm{\\mathrm{{{{g}}}}}}}}_{i}&amp;\\displaystyle=%&#10;\\frac{({\\bm{\\mathrm{{{G}}}}}^{\\prime}{\\bm{\\mathrm{{{G}}}}}^{\\prime T}-\\sum_{l=%&#10;1}^{j-1}{\\bm{\\mathrm{{{G}}}}}(:,l){\\bm{\\mathrm{{{G}}}}}(:,l)^{T})(:,i)}{\\sqrt{%&#10;{\\bm{\\mathrm{{{d}}}}}(i)}}\\\\&#10;&amp;\\displaystyle=\\frac{{\\bm{\\mathrm{{{G}}}}}^{\\prime}(:,\\ j{+}1{:}j{+}\\delta)\\ {%&#10;\\bm{\\mathrm{{{G}}}}}^{\\prime T}(j{+}1{:}j{+}\\delta,\\ i)}{\\sqrt{{\\bm{\\mathrm{{{%&#10;d}}}}}(i)}}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mi>i</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mfrac><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msup><mi>\ud835\udc06</mi><mrow><mi mathsize=\"142%\" mathvariant=\"normal\">\u2032</mi><mo>\u2063</mo><mi>T</mi></mrow></msup></mrow><mo>-</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mi>\ud835\udc06</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc06</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msqrt><mrow><mi>\ud835\udc1d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mfrac><mrow><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><msup><mi>\ud835\udc06</mi><mrow><mi mathsize=\"142%\" mathvariant=\"normal\">\u2032</mi><mo>\u2063</mo><mi>T</mi></mrow></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><msqrt><mrow><mi>\ud835\udc1d</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msqrt></mfrac></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nInserting ${\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l$ as in in Eq.~\\ref{e:chol_step_lookahead}, the\ndenominator $1/\\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i, i)}$  cancels out. The norm $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}\n{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l\\|$ can be computed as:\n\n", "itemtype": "equation", "pos": 24646, "prevtext": "\n\nThis simplification significantly reduces the computation complexity in\ncomputing the quantities $c_l$ and $a_l$ in Eq.~\\ref{e:lar_gamma}. To see this,\nconsider the computation of correlation with the equiangular vector, $a_l$ for\nany (normalized) candidate pivot column ${\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_{l}$.\n\nBy assumption of the LAR algorithm, the predictor variables\nhave mean zero and norm one, which is not the case with pivot columns. This is\nsolved by using the centering projection matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} = ({\\ensuremath{\\boldsymbol{\\mathrm{{{I}}}}}} -\n\\frac{{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T}{n})$, and letting\n\n\n\n", "index": 21, "text": "\\begin{equation}\na_l = \\frac{({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} {\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l)^T{\\ensuremath{\\boldsymbol{\\mathrm{{{u_A}}}}}}}{\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} {\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}\\|} \\\\\n\\label{e:low_rank_bisector}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"a_{l}=\\frac{({\\bm{\\mathrm{{{P}}}}}{\\hat{{\\bm{\\mathrm{{{{g}}}}}}}}_{l})^{T}{\\bm%&#10;{\\mathrm{{{u_{A}}}}}}}{\\|{\\bm{\\mathrm{{{P}}}}}{\\hat{{\\bm{\\mathrm{{{{g}}}}}}}}%&#10;\\|}\\\\&#10;\" display=\"block\"><mrow><msub><mi>a</mi><mi>l</mi></msub><mo>=</mo><mfrac><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mi>l</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc2e</mi><mi>\ud835\udc00</mi></msub></mrow><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>\u2225</mo></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nSimilarly, dot product with a vector is computed as:\n\n", "itemtype": "equation", "pos": 25354, "prevtext": "\n\nInserting ${\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l$ as in in Eq.~\\ref{e:chol_step_lookahead}, the\ndenominator $1/\\sqrt{{\\ensuremath{\\boldsymbol{\\mathrm{{{d}}}}}}(i, i)}$  cancels out. The norm $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}}\n{\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l\\|$ can be computed as:\n\n", "index": 23, "text": "\\begin{equation}\n\\begin{split}\n       \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} {\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l\\|^2 \n                    &= \\bigg( {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} \\  {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'(:,\\ j{+}1{:}j{+}\\delta)\\  {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'^T (j{+}1{:}j{+}\\delta,\\ l) \\bigg)^T \\  \\\\\n                       &\\ \\ \\ \\  \\bigg( {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} \\ {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'(:,\\ j{+}1{:}j{+}\\delta) \\ {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'^T (j{+}1{:}j{+}\\delta,\\ l) \\bigg) \\\\\n                    &= {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(l, :) \\bigg( {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(j{+}1{:}j{+}\\delta,\\ j{+}1{:}j{+}\\delta) \n                                         \\\\\n                    &\\ \\ \\  - {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(j{+}1{:}j{+}\\delta,\\ j{+}1{:}j{+}\\delta)\n                         \\bigg) {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(l, :)^T\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\|{\\bm{\\mathrm{{{P}}}}}{\\hat{{\\bm{\\mathrm{{{{g}}}}}}%&#10;}}_{l}\\|^{2}&amp;\\displaystyle=\\bigg{(}{\\bm{\\mathrm{{{P}}}}}\\ {\\bm{\\mathrm{{{G}}}}%&#10;}^{\\prime}(:,\\ j{+}1{:}j{+}\\delta)\\ {\\bm{\\mathrm{{{G}}}}}^{\\prime T}(j{+}1{:}j%&#10;{+}\\delta,\\ l)\\bigg{)}^{T}\\\\&#10;&amp;\\displaystyle\\ \\ \\ \\ \\bigg{(}{\\bm{\\mathrm{{{P}}}}}\\ {\\bm{\\mathrm{{{G}}}}}^{%&#10;\\prime}(:,\\ j{+}1{:}j{+}\\delta)\\ {\\bm{\\mathrm{{{G}}}}}^{\\prime T}(j{+}1{:}j{+}%&#10;\\delta,\\ l)\\bigg{)}\\\\&#10;&amp;\\displaystyle={\\bm{\\mathrm{{{G}}}}}(l,:)\\bigg{(}{\\bm{\\mathrm{{{G}}}}}^{T}{\\bm%&#10;{\\mathrm{{{G}}}}}(j{+}1{:}j{+}\\delta,\\ j{+}1{:}j{+}\\delta)\\\\&#10;&amp;\\displaystyle\\ \\ \\ -{\\bm{\\mathrm{{{G}}}}}^{T}{\\bm{\\mathrm{{{1}}}}}{\\bm{%&#10;\\mathrm{{{1}}}}}^{T}{\\bm{\\mathrm{{{G}}}}}(j{+}1{:}j{+}\\delta,\\ j{+}1{:}j{+}%&#10;\\delta)\\bigg{)}{\\bm{\\mathrm{{{G}}}}}(l,:)^{T}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msup><mrow><mo>\u2225</mo><mrow><mi>\ud835\udc0f</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mi>l</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn></msup></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+5pt\"><mi>\ud835\udc0f</mi></mpadded><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><msup><mi>\ud835\udc06</mi><mrow><mi mathsize=\"142%\" mathvariant=\"normal\">\u2032</mi><mo>\u2063</mo><mi>T</mi></mrow></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mi>T</mi></msup></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo lspace=\"22.5pt\" maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+5pt\"><mi>\ud835\udc0f</mi></mpadded><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><msup><mi>\ud835\udc06</mi><mrow><mi mathsize=\"142%\" mathvariant=\"normal\">\u2032</mi><mo>\u2063</mo><mi>T</mi></mrow></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mo>:</mo><mo stretchy=\"false\">)</mo></mrow><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><msup><mi>\ud835\udc06</mi><mi>T</mi></msup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo lspace=\"17.5pt\">-</mo><msup><mi>\ud835\udc06</mi><mi>T</mi></msup><msup><mn>\ud835\udfcf\ud835\udfcf</mn><mi>T</mi></msup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo><mi>\ud835\udc06</mi><mo stretchy=\"false\">(</mo><mi>l</mi><mo>,</mo><mo>:</mo><mo stretchy=\"false\">)</mo><msup><mi/><mi>T</mi></msup></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nCorrectly ordering the order of computation yields the computational complexity\n$O(\\delta^2)$ per column.  Note that matrices ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$,\n${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$, ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$, ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$ are the same for all columns\n(independent of $l$) and need to be computed only once. Computation of correlations\nwith the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ are computed in an analogous manner.\n\n\n\n\\begin{algorithm}\n\\SetKwData{Variables}{variables}\n\n\\KwIn{ \\\\\n\\hspace{5mm} $ \\{{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_n\\}$ training set of objects in $\\mathcal{X}$, \\\\ \n \\hspace{5mm} $k_1, k_2, ... k_p$ kernel functions on $\\mathcal{X} \\times \\mathcal{X}$, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\in \\mathbb{R}^n$ regression targets, \\\\\n \\hspace{5mm} $\\lambda$ regularization parameter, \\\\\n \\hspace{5mm} $\\delta$ number of look-ahead columns,  \\\\\n \\hspace{5mm} $K$ maximum total rank. \\\\\n} \n\n\\BlankLine\n\\KwResult{ \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1 \\in \\mathbb{R}^{n \\times k_1}, \n {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2 \\in  \\mathbb{R}^{n \\times k_2}, ...\n {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p \\in \\mathbb{R}^{n \\times k_p} $, \\\\ \\hspace{10mm} low-rank approximations, \\\\\n \\hspace{5mm} $\\mathcal{A}_1, \\mathcal{A}_2, ..., \\mathcal{A}_p$ active sets of object indices, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} \\in \\mathbb{R}^{n}$ regression vector on the training set, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} \\in \\mathbb{R}^K$ regression weights. \\\\\n}\n\n\\BlankLine\n\\BlankLine\n\nInitialize:\u00c2\u00a0\\\\\n\\hspace{5mm} residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - \\bar{{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}}$, \n\n\\hspace{5mm} bisector ${\\ensuremath{\\boldsymbol{\\mathrm{{{b}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$, \n\n\\hspace{5mm} regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$,\n\n\\hspace{5mm} active sets $\\mathcal{A}_q = \\emptyset$ and counters $j_q = 0$ for $q \\in \\{1, ..., p\\}$,\n\n\n\\BlankLine\n    Compute standard Cholesky Decompositions with $\\delta$ look-ahead columns for ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_p$.\n\n\n\\BlankLine\n\\BlankLine\n\\While{$\\sum_{q=1}^{p} j_q < K$}{\n\n    \\BlankLine\n    Select (kernel, pivot index) pair \n    $(k_q, j)$ that maximizes the LAR criterion (Eq.~\\ref{e:lar_gamma}).\n    \n    \n\n    Compute ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{qj}$ with one Cholesky step for pivot $j$ w.r.t ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$ and $k_q(.,.)$.\n\n    Add ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{qj}$ to ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$.\n\n    Recompute Cholesky steps for ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_q$ at columns $k_q...k_q + \\delta$.\n    \\BlankLine\n\n\n    Compute bisector ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ of all active columns in ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p$ (Eq.~\\ref{e:low_rank_bisector}).\n    \n    Compute gradient step $\\gamma$ using the LAR method (Eq.~\\ref{e:lar_gamma}). \n    The step is computed so that the resulting correlations remain all equal.\n\n    Update regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} + \\gamma {\\ensuremath{\\boldsymbol{\\mathrm{{{b}}}}}}$.\n\n    \\BlankLine\n    Increase column counter $j_q = j_q + 1$.    \n\n    Add index $j$ to active set $\\mathcal{A}_q$.\n\n}\n\n\\BlankLine\nCompute ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ using the QR decomposition with Eq.~\\ref{e:beta_qr}.\n\nCompute linear transform {\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}} using the Nystr\\\"om approximation using Eq.~\\ref{e:nystrom_cholesky}.\n\n\n\\caption{The {\\texttt{mklaren}}\\ algorithm.}\n\\label{a:mklaren}\n\\end{algorithm}\n\n\n\\subsection*{Computing the regression weights}\n\nThe regression weights ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} \\in \\mathbb{R}^K$ are computed from the\nregression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ and the active feature space {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}} as defined in\nEq.~\\ref{def:bh}.\nusing the relation\n\n", "itemtype": "equation", "pos": 26638, "prevtext": "\n\nSimilarly, dot product with a vector is computed as:\n\n", "index": 25, "text": "\\begin{equation}\n\\begin{split}\n    &({\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} {\\ensuremath{\\hat{{\\ensuremath{\\boldsymbol{\\mathrm{{{{g}}}}}}}}}}_l)^T{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A = {\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T \\bigg( {\\ensuremath{\\boldsymbol{\\mathrm{{{P}}}}}} \\ {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'(:,\\ j{+}1{:}j{+}\\delta) {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'^T(j{+}1{:}j{+}\\delta,\\ l) \\bigg)^T = \\\\\n         &=  \\bigg( {\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:,\\ j{+}1{:}j{+}\\delta) - {\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(:,\\ j{+}1{:}j{+}\\delta) \\bigg) \\\\ &\\ \\ \\ \\ \\ \\  {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}(j{+}1{:}j{+}\\delta,\\ l)  \\\\\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle({\\bm{\\mathrm{{{P}}}}}{\\hat{{\\bm{\\mathrm{{{{g}}}}}}%&#10;}}_{l})^{T}{\\bm{\\mathrm{{{u}}}}}_{A}={\\bm{\\mathrm{{{u}}}}}_{A}^{T}\\bigg{(}{\\bm%&#10;{\\mathrm{{{P}}}}}\\ {\\bm{\\mathrm{{{G}}}}}^{\\prime}(:,\\ j{+}1{:}j{+}\\delta){\\bm{%&#10;\\mathrm{{{G}}}}}^{\\prime T}(j{+}1{:}j{+}\\delta,\\ l)\\bigg{)}^{T}=\\\\&#10;&amp;\\displaystyle=\\bigg{(}{\\bm{\\mathrm{{{u}}}}}_{A}^{T}{\\bm{\\mathrm{{{G}}}}}(:,\\ %&#10;j{+}1{:}j{+}\\delta)-{\\bm{\\mathrm{{{u}}}}}_{A}^{T}{\\bm{\\mathrm{{{1}}}}}{\\bm{%&#10;\\mathrm{{{1}}}}}^{T}{\\bm{\\mathrm{{{G}}}}}(:,\\ j{+}1{:}j{+}\\delta)\\bigg{)}\\\\&#10;&amp;\\displaystyle\\ \\ \\ \\ \\ \\ {\\bm{\\mathrm{{{G}}}}}(j{+}1{:}j{+}\\delta,\\ l)\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc0f</mi><msub><mover accent=\"true\"><mi>\ud835\udc20</mi><mo stretchy=\"false\">^</mo></mover><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><msub><mi>\ud835\udc2e</mi><mi>A</mi></msub><mo>=</mo><msubsup><mi>\ud835\udc2e</mi><mi>A</mi><mi>T</mi></msubsup><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><mpadded width=\"+5pt\"><mi>\ud835\udc0f</mi></mpadded><msup><mi>\ud835\udc06</mi><mo>\u2032</mo></msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow><msup><mi>\ud835\udc06</mi><mrow><mi mathsize=\"142%\" mathvariant=\"normal\">\u2032</mi><mo>\u2063</mo><mi>T</mi></mrow></msup><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow><mi>T</mi></msup><mo>=</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mo>=</mo><mrow><mo maxsize=\"210%\" minsize=\"210%\">(</mo><msubsup><mi>\ud835\udc2e</mi><mi>A</mi><mi>T</mi></msubsup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><msubsup><mi>\ud835\udc2e</mi><mi>A</mi><mi>T</mi></msubsup><msup><mn>\ud835\udfcf\ud835\udfcf</mn><mi>T</mi></msup><mi>\ud835\udc06</mi><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo rspace=\"7.5pt\">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"210%\" minsize=\"210%\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mpadded lspace=\"30pt\" width=\"+30pt\"><mi>\ud835\udc06</mi></mpadded><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>:</mo><mi>j</mi><mo>+</mo><mi>\u03b4</mi><mo rspace=\"7.5pt\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nwhere ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A = {\\ensuremath{\\boldsymbol{\\mathrm{{{QR}}}}}}$ is the thin QR decomposition~\\cite{golub2012matrix}.\n\n\\subsection*{Computing the dual coefficients}\nKernel ridge regression is often stated in terms of dual coefficients ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} \\in \\mathbb{R}^n$,\nsatisfying the relation:\n\n\n", "itemtype": "equation", "pos": 32594, "prevtext": "\n\nCorrectly ordering the order of computation yields the computational complexity\n$O(\\delta^2)$ per column.  Note that matrices ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$,\n${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$, ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$, ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}^T {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}$ are the same for all columns\n(independent of $l$) and need to be computed only once. Computation of correlations\nwith the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ are computed in an analogous manner.\n\n\n\n\\begin{algorithm}\n\\SetKwData{Variables}{variables}\n\n\\KwIn{ \\\\\n\\hspace{5mm} $ \\{{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_n\\}$ training set of objects in $\\mathcal{X}$, \\\\ \n \\hspace{5mm} $k_1, k_2, ... k_p$ kernel functions on $\\mathcal{X} \\times \\mathcal{X}$, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\in \\mathbb{R}^n$ regression targets, \\\\\n \\hspace{5mm} $\\lambda$ regularization parameter, \\\\\n \\hspace{5mm} $\\delta$ number of look-ahead columns,  \\\\\n \\hspace{5mm} $K$ maximum total rank. \\\\\n} \n\n\\BlankLine\n\\KwResult{ \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1 \\in \\mathbb{R}^{n \\times k_1}, \n {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2 \\in  \\mathbb{R}^{n \\times k_2}, ...\n {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p \\in \\mathbb{R}^{n \\times k_p} $, \\\\ \\hspace{10mm} low-rank approximations, \\\\\n \\hspace{5mm} $\\mathcal{A}_1, \\mathcal{A}_2, ..., \\mathcal{A}_p$ active sets of object indices, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} \\in \\mathbb{R}^{n}$ regression vector on the training set, \\\\\n \\hspace{5mm} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} \\in \\mathbb{R}^K$ regression weights. \\\\\n}\n\n\\BlankLine\n\\BlankLine\n\nInitialize:\u00c2\u00a0\\\\\n\\hspace{5mm} residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - \\bar{{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}}$, \n\n\\hspace{5mm} bisector ${\\ensuremath{\\boldsymbol{\\mathrm{{{b}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$, \n\n\\hspace{5mm} regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}}$,\n\n\\hspace{5mm} active sets $\\mathcal{A}_q = \\emptyset$ and counters $j_q = 0$ for $q \\in \\{1, ..., p\\}$,\n\n\n\\BlankLine\n    Compute standard Cholesky Decompositions with $\\delta$ look-ahead columns for ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_p$.\n\n\n\\BlankLine\n\\BlankLine\n\\While{$\\sum_{q=1}^{p} j_q < K$}{\n\n    \\BlankLine\n    Select (kernel, pivot index) pair \n    $(k_q, j)$ that maximizes the LAR criterion (Eq.~\\ref{e:lar_gamma}).\n    \n    \n\n    Compute ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{qj}$ with one Cholesky step for pivot $j$ w.r.t ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$ and $k_q(.,.)$.\n\n    Add ${\\ensuremath{\\boldsymbol{\\mathrm{{{g}}}}}}_{qj}$ to ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_q$.\n\n    Recompute Cholesky steps for ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}'_q$ at columns $k_q...k_q + \\delta$.\n    \\BlankLine\n\n\n    Compute bisector ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ of all active columns in ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}_p$ (Eq.~\\ref{e:low_rank_bisector}).\n    \n    Compute gradient step $\\gamma$ using the LAR method (Eq.~\\ref{e:lar_gamma}). \n    The step is computed so that the resulting correlations remain all equal.\n\n    Update regression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} + \\gamma {\\ensuremath{\\boldsymbol{\\mathrm{{{b}}}}}}$.\n\n    \\BlankLine\n    Increase column counter $j_q = j_q + 1$.    \n\n    Add index $j$ to active set $\\mathcal{A}_q$.\n\n}\n\n\\BlankLine\nCompute ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ using the QR decomposition with Eq.~\\ref{e:beta_qr}.\n\nCompute linear transform {\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}} using the Nystr\\\"om approximation using Eq.~\\ref{e:nystrom_cholesky}.\n\n\n\\caption{The {\\texttt{mklaren}}\\ algorithm.}\n\\label{a:mklaren}\n\\end{algorithm}\n\n\n\\subsection*{Computing the regression weights}\n\nThe regression weights ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} \\in \\mathbb{R}^K$ are computed from the\nregression line ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ and the active feature space {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}} as defined in\nEq.~\\ref{def:bh}.\nusing the relation\n\n", "index": 27, "text": "\\begin{equation}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}  \\implies {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = ({\\ensuremath{\\boldsymbol{\\mathrm{{{R}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{R}}}}}})^{-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{Q}}}}}}^T {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}},\n\\label{e:beta_qr}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{H}}}}}_{A}{\\bm{\\mathrm{{{\\beta}}}}}={\\bm{\\mathrm{{{\\mu}}}}}%&#10;\\implies{\\bm{\\mathrm{{{\\mu}}}}}=({\\bm{\\mathrm{{{R}}}}}^{T}{\\bm{\\mathrm{{{R}}}}%&#10;})^{-1}{\\bm{\\mathrm{{{Q}}}}}^{T}{\\bm{\\mathrm{{{\\mu}}}}},\" display=\"block\"><mrow><mrow><mrow><msub><mi>\ud835\udc07</mi><mi>A</mi></msub><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow><mo>=</mo><mi>\ud835\udf41</mi><mo>\u27f9</mo><mi>\ud835\udf41</mi><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc11</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc11</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc10</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udf41</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThis is an overdetermined system of equations. The vector {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} with\nminimal norm can be obtained by solving the following least-norm problem:\n\n", "itemtype": "equation", "pos": 33387, "prevtext": "\n\nwhere ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A = {\\ensuremath{\\boldsymbol{\\mathrm{{{QR}}}}}}$ is the thin QR decomposition~\\cite{golub2012matrix}.\n\n\\subsection*{Computing the dual coefficients}\nKernel ridge regression is often stated in terms of dual coefficients ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} \\in \\mathbb{R}^n$,\nsatisfying the relation:\n\n\n", "index": 29, "text": "\\begin{equation}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} =  {\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{H}}}}}_{A}^{T}{\\bm{\\mathrm{{{\\alpha}}}}}={\\bm{\\mathrm{{{\\beta}}%&#10;}}}\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udc07</mi><mi>A</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udf36</mi></mrow><mo>=</mo><mi>\ud835\udf37</mi></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe problem has an analytical solution equal to\n\n", "itemtype": "equation", "pos": 33750, "prevtext": "\n\nThis is an overdetermined system of equations. The vector {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} with\nminimal norm can be obtained by solving the following least-norm problem:\n\n", "index": 31, "text": "\\begin{equation}\n\\begin{split}\n    \\text{minimize }     &\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}\\|_2 \\\\\n    \\text{subject to }   &{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\text{minimize }&amp;\\displaystyle\\|{\\bm{\\mathrm{{{%&#10;\\alpha}}}}}\\|_{2}\\\\&#10;\\displaystyle\\text{subject to }&amp;\\displaystyle{\\bm{\\mathrm{{{H}}}}}_{A}^{T}{\\bm%&#10;{\\mathrm{{{\\alpha}}}}}={\\bm{\\mathrm{{{\\beta}}}}}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mtext>minimize\u00a0</mtext></mtd><mtd columnalign=\"left\"><msub><mrow><mo>\u2225</mo><mi>\ud835\udf36</mi><mo>\u2225</mo></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign=\"right\"><mtext>subject to\u00a0</mtext></mtd><mtd columnalign=\"left\"><mrow><mrow><msubsup><mi>\ud835\udc07</mi><mi>A</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udf36</mi></mrow><mo>=</mo><mi>\ud835\udf37</mi></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nObtaining dual coefficients ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}$ can be useful if the explicit feature\nspace ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\Phi}}}}}}_q$ is available for a kernel $q$, which is the case for\nlinear, polynomial, string and other kernels. An interpretation of regression\ncoefficients can be done in that feature space by considering\n\n", "itemtype": "equation", "pos": 34110, "prevtext": "\n\nThe problem has an analytical solution equal to\n\n", "index": 33, "text": "\\begin{equation}\n   {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A ({\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}})^{-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{\\alpha}}}}}={\\bm{\\mathrm{{{H}}}}}_{A}({\\bm{\\mathrm{{{H}}}}}^{T}%&#10;{\\bm{\\mathrm{{{H}}}}})^{-1}{\\bm{\\mathrm{{{\\beta}}}}}\" display=\"block\"><mrow><mi>\ud835\udf36</mi><mo>=</mo><mrow><msub><mi>\ud835\udc07</mi><mi>A</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc07</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc07</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udf37</mi></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nMoreover, if the vector {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} is sparse, only the relevant portions of\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\Phi}}}}}}_q$ need to be computed.  This condition can be additionally enforced\nby using techniques like matching pursuit when solving for {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}~\\cite{bach2010sparse}.\n\n\n\\subsection*{Prediction of unseen samples}\n\nWe show a way to infer the Cholesky factors for examples not used during\ntraining, without explicitly repeating the Cholesky steps, so that learned\nweights ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ can predict responses for test samples. In order to\nsimplify notation, we show the principle for one kernel matrix and its\ncorresponding Cholesky factors, but note that the idea is trivially extended to\nthe case of multiple kernels.\n\n\\textbf{Nystr\\\"om approximation}. Let $\\mathcal{A} = \\{i_1, i_2, ..., i_k\\}$ be\nthe active set of pivot indices. The Nystr\\\"om aproximation~\\cite{Williams2001} of the\nkernel matrix {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} is defined as follows:\n\n\n", "itemtype": "equation", "pos": 34746, "prevtext": "\n\nObtaining dual coefficients ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}$ can be useful if the explicit feature\nspace ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\Phi}}}}}}_q$ is available for a kernel $q$, which is the case for\nlinear, polynomial, string and other kernels. An interpretation of regression\ncoefficients can be done in that feature space by considering\n\n", "index": 35, "text": "\\begin{equation}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}_{\\Phi_q} ={\\ensuremath{\\boldsymbol{\\mathrm{{{\\Phi}}}}}}_q {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{\\beta}}}}}_{\\Phi_{q}}={\\bm{\\mathrm{{{\\Phi}}}}}_{q}{\\bm{\\mathrm{%&#10;{{\\alpha}}}}}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udf37</mi><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>q</mi></msub></msub><mo>=</mo><mrow><msub><mi>\ud835\udebd</mi><mi>q</mi></msub><mo>\u2062</mo><mi>\ud835\udf36</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe selection of the indices constituting $\\mathcal{A}$ crucially influences the\nperformance of learning. Note that {\\texttt{mklaren}}\\\ndefines a method to construct this set.\n\nLet ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T$ be an (incomplete) Cholesky decomposition using the same\npivot indices in the set $\\mathcal{A}$. Consider the following proposition:\n\n\\vspace{3mm}\n\\textbf{Proposition.} \\emph{The incomplete Cholesky decomposition with pivots\n$\\mathcal{A} = \\{i_1, i_2, ..., i_k\\}$ yields the same approximation as the\nNystr\\\"om approximation using the active set $\\mathcal{A}$.}\n\\vspace{3mm}\n\nThe proof follows directly from~\\cite{Bach2005}. There is an unique matrix\n{\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}} that is: \\emph{(i) symmetric,  (ii) has the column space spanned by ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:,\n\\mathcal{A})$ and (iii) ${\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}}(:, \\mathcal{A}) = {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:, \\mathcal{A})$.} It\nfollows that both incomplete Cholesky decomposition and the Nystr\\\"om approximation\nresult in the same approximation matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}}$.\n\n\\vspace{3mm}\n\\textbf{Corollary.} Let {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}} be the Cholesky factors obtained on the training\nset. Cholesky factors ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ for unseen examples ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2^*, ...\n{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}^*_{nt}$ can be inferred using the linear transform:\n\\vspace{3mm}\n\n\n", "itemtype": "equation", "pos": 36029, "prevtext": "\n\nMoreover, if the vector {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}} is sparse, only the relevant portions of\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\Phi}}}}}}_q$ need to be computed.  This condition can be additionally enforced\nby using techniques like matching pursuit when solving for {\\ensuremath{\\boldsymbol{\\mathrm{{{\\alpha}}}}}}~\\cite{bach2010sparse}.\n\n\n\\subsection*{Prediction of unseen samples}\n\nWe show a way to infer the Cholesky factors for examples not used during\ntraining, without explicitly repeating the Cholesky steps, so that learned\nweights ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ can predict responses for test samples. In order to\nsimplify notation, we show the principle for one kernel matrix and its\ncorresponding Cholesky factors, but note that the idea is trivially extended to\nthe case of multiple kernels.\n\n\\textbf{Nystr\\\"om approximation}. Let $\\mathcal{A} = \\{i_1, i_2, ..., i_k\\}$ be\nthe active set of pivot indices. The Nystr\\\"om aproximation~\\cite{Williams2001} of the\nkernel matrix {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} is defined as follows:\n\n\n", "index": 37, "text": "\\begin{equation}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:,\\mathcal{A}){\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A},\\mathcal{A})^{-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:,\\mathcal{A})^T\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{K}}}}}={\\bm{\\mathrm{{{K}}}}}(:,\\mathcal{A}){\\bm{\\mathrm{{{K}}}}%&#10;}(\\mathcal{A},\\mathcal{A})^{-1}{\\bm{\\mathrm{{{K}}}}}(:,\\mathcal{A})^{T}\" display=\"block\"><mrow><mi>\ud835\udc0a</mi><mo>=</mo><mrow><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}} \\in\u00c2\u00a0\\mathbb{R}^{K\u00c2\u00a0\\times K}$ is inexpensive to compute and\ncan be stored permanently after training. Hence, the Cholesky factors\n${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ can be computed from the inner product between the test and the\nactive sets ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A}, *)$ and the existing regression weights\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ can be applied on ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ to obtain predictions.\n\n\n\\subsection*{$\\ell_2$ norm regularization}\n\nRegularization is achieved by constraining the norm of weights $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}\\|$.\nZou and Hastie~\\cite{Zou2005} prove the following lemma, which shows that\n$\\ell_2$ regularized regression problem can be stated as ordinary least\nsquares via appropriate augmentation of the dataset ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$.\n\n\\vspace{2mm} \\textbf{Lemma}. Define the augmented data set ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}^*$\nto equal\n\n\n", "itemtype": "equation", "pos": 37929, "prevtext": "\n\nThe selection of the indices constituting $\\mathcal{A}$ crucially influences the\nperformance of learning. Note that {\\texttt{mklaren}}\\\ndefines a method to construct this set.\n\nLet ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T$ be an (incomplete) Cholesky decomposition using the same\npivot indices in the set $\\mathcal{A}$. Consider the following proposition:\n\n\\vspace{3mm}\n\\textbf{Proposition.} \\emph{The incomplete Cholesky decomposition with pivots\n$\\mathcal{A} = \\{i_1, i_2, ..., i_k\\}$ yields the same approximation as the\nNystr\\\"om approximation using the active set $\\mathcal{A}$.}\n\\vspace{3mm}\n\nThe proof follows directly from~\\cite{Bach2005}. There is an unique matrix\n{\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}} that is: \\emph{(i) symmetric,  (ii) has the column space spanned by ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:,\n\\mathcal{A})$ and (iii) ${\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}}(:, \\mathcal{A}) = {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:, \\mathcal{A})$.} It\nfollows that both incomplete Cholesky decomposition and the Nystr\\\"om approximation\nresult in the same approximation matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{L}}}}}}$.\n\n\\vspace{3mm}\n\\textbf{Corollary.} Let {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}} be the Cholesky factors obtained on the training\nset. Cholesky factors ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ for unseen examples ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2^*, ...\n{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}^*_{nt}$ can be inferred using the linear transform:\n\\vspace{3mm}\n\n\n", "index": 39, "text": "\\begin{equation}\n\\begin{split}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^* &= ({\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}})^{-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^T    {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(:, \\mathcal{A})  {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A},\\mathcal{A})^{-1} {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A}, *) \\\\\n             &= {\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}} {\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A}, *)\n\\end{split}\n\\label{e:nystrom_cholesky}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{G}}}}}^{*}&amp;\\displaystyle=({\\bm{%&#10;\\mathrm{{{G}}}}}^{T}{\\bm{\\mathrm{{{G}}}}})^{-1}{\\bm{\\mathrm{{{G}}}}}^{T}{\\bm{%&#10;\\mathrm{{{K}}}}}(:,\\mathcal{A}){\\bm{\\mathrm{{{K}}}}}(\\mathcal{A},\\mathcal{A})^%&#10;{-1}{\\bm{\\mathrm{{{K}}}}}(\\mathcal{A},*)\\\\&#10;&amp;\\displaystyle={\\bm{\\mathrm{{{T}}}}}{\\bm{\\mathrm{{{K}}}}}(\\mathcal{A},*)\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msup><mi>\ud835\udc06</mi><mo>*</mo></msup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc06</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udc06</mi><mi>T</mi></msup><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mo>:</mo><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>,</mo><mo>*</mo><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udc13\ud835\udc0a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mo>,</mo><mo>*</mo><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": " \n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nThe matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}} \\in\u00c2\u00a0\\mathbb{R}^{K\u00c2\u00a0\\times K}$ is inexpensive to compute and\ncan be stored permanently after training. Hence, the Cholesky factors\n${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ can be computed from the inner product between the test and the\nactive sets ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}}(\\mathcal{A}, *)$ and the existing regression weights\n${\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ can be applied on ${\\ensuremath{\\boldsymbol{\\mathrm{{{G}}}}}}^*$ to obtain predictions.\n\n\n\\subsection*{$\\ell_2$ norm regularization}\n\nRegularization is achieved by constraining the norm of weights $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}\\|$.\nZou and Hastie~\\cite{Zou2005} prove the following lemma, which shows that\n$\\ell_2$ regularized regression problem can be stated as ordinary least\nsquares via appropriate augmentation of the dataset ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$.\n\n\\vspace{2mm} \\textbf{Lemma}. Define the augmented data set ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}^*$\nto equal\n\n\n", "index": 41, "text": "\\begin{equation*} {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^* = \\sqrt{(1+\\lambda)} \\begin{pmatrix}{\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}} \\\\\n\\sqrt{\\lambda}{\\ensuremath{\\boldsymbol{\\mathrm{{{I}}}}}}\\end{pmatrix} \\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{X}}}}}^{*}=\\sqrt{(1+\\lambda)}\\begin{pmatrix}{\\bm{\\mathrm{{{X}}}%&#10;}}\\\\&#10;\\sqrt{\\lambda}{\\bm{\\mathrm{{{I}}}}}\\end{pmatrix}\" display=\"block\"><mrow><msup><mi>\ud835\udc17</mi><mo>*</mo></msup><mo>=</mo><mrow><msqrt><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>\u03bb</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msqrt><mo>\u2062</mo><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc17</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msqrt><mi>\u03bb</mi></msqrt><mo>\u2062</mo><mi>\ud835\udc08</mi></mrow></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe least-squares solution of ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}^*$ is then equivalent to Ridge\nregression of the original data ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$ with parameter $\\lambda$. For\nproof, see Zou \\& Hastie~\\cite{Zou2005}. The augmented dataset can be included in LAR to achieve the $\\ell_2$-regularized solution.\n\n\\subsubsection*{Computational complexity} \nIt is straightforward to analyze the exact upper bound on computational\ncomplexity of the {\\texttt{mklaren}}\\ algorithm, described in Algorithm~\\ref{a:mklaren}.\nThe look-ahead Cholesky decompositions are standard Cholesky decompositions\nwith $\\delta$ pivots and complexity $O(n\\delta^2)$. The main loop is executed\n$K$ times. The selection of kernel and pivot pairs is based on the LAR\ncriterion, which includes inverting ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$ of size $K \\times K$,\nthus having a complexity of $K^3$. However, as each step is a rank-one\nmodification to ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$, the Morrison-Sherman-Woodbury\nlemma~\\cite{meyer2000matrix} can be used to achieve complexity $O(K^2)$ per\nupdate. The computation of correlations with the bisector in\nEq.~\\ref{e:low_rank_bisector} and residuals are computed for $p$ kernels in\n$O(np\\delta^2)$.  Recomputation of $\\delta$ Cholesky factors requires standard\nCholesky steps of complexity $n\\delta^2$. The computation of the gradient step\nis of the same complexity as the gradient step. Updating the regression line is\n$O(n)$. The QR decomposition in Eq~\\ref{e:beta_qr} takes $O(nK^2)$ and the\ncomputation of linear transform ${\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}}$ in Eq.~\\ref{e:nystrom_cholesky} is of\n$O(K^3 + nK^2)$ complexity. Together, the final complexity is \n\n", "itemtype": "equation", "pos": 39909, "prevtext": " \n", "index": 43, "text": "\\begin{equation*} {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}^* =\n\\begin{pmatrix} {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\\\ {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}} \\end{pmatrix}.  \\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{y}}}}}^{*}=\\begin{pmatrix}{\\bm{\\mathrm{{{y}}}}}\\\\&#10;{\\bm{\\mathrm{{{0}}}}}\\end{pmatrix}.\" display=\"block\"><mrow><mrow><msup><mi>\ud835\udc32</mi><mo>*</mo></msup><mo>=</mo><mrow><mo>(</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc32</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mn/></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\nthus being linear in the number of data points and kernels.\n\n\n\\section*{Experiments}\n\nIn this section, we provide an empirical evaluation of the proposed methodology\non known regression datasets. We first compare {\\texttt{mklaren}}\\ with several benchmark\nlow-rank matrix approximation methods: Incomplete Cholesky Decomposition ({\\texttt{icd}},\n\\cite{Fine2001}), Cholesky with side Information ({\\texttt{csi}}, \\cite{Bach2005}) and\nthe Nystr\\\"om method ({\\texttt{Nystr\\\"om}}, \\cite{Williams2001}). In the interest of\ngenerality, we omit the comparison with kernel function approximation methods\nlimited to kernels of a particular functional form~\\cite{Rahimi2007,\nVedaldi2012} and the approximation of kernel matrices limited only to the Gaussian\nkernel~\\cite{Si2014, Le2013a, Yang2014}. Finally, a comparison is provided\nwith a family of state-of-the-art multiple kernel learning methods using the\nfull-kernel matrix on a sentiment analysis data set using a large number of\nrank-one kernels~\\cite{Cortes2012}. \n\n\\subsection*{Comparison with low-rank approximations}\n\n\\begin{table*}[ht!]\n\\centering\n\n\\small\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{4.393 $\\pm$ 0.432} & $4.762 \\pm 0.598$ & $6.703 \\pm 0.354$ & $6.611 \\pm 1.272$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.018 $\\pm$ 0.000} & $0.025 \\pm 0.003$ & $0.067 \\pm 0.006$ & $0.065 \\pm 0.006$ & $0.013 \\pm 0.000$\\\\ \npumadyn & \\textbf{1.252 $\\pm$ 0.032} & $1.650 \\pm 0.169$ & $4.024 \\pm 0.655$ & $3.882 \\pm 0.803$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.638 $\\pm$ 0.116} & $2.768 \\pm 0.187$ & $2.906 \\pm 0.222$ & $2.939 \\pm 0.197$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{5.288 $\\pm$ 0.461} & $7.520 \\pm 1.852$ & $14.111 \\pm 1.123$ & $13.763 \\pm 0.580$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.283 $\\pm$ 0.017} & $0.310 \\pm 0.022$ & $0.380 \\pm 0.010$ & $0.377 \\pm 0.015$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.036 $\\pm$ 0.001} & $0.046 \\pm 0.005$ & $0.101 \\pm 0.011$ & $0.128 \\pm 0.010$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & \\textbf{54.680 $\\pm$ 3.61} & $54.953 \\pm 3.018$ & $63.715 \\pm 5.970$ & $68.117 \\pm 3.947$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\\vspace{2mm}\n\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{3.792 $\\pm$ 0.454} & $4.481 \\pm 0.689$ & $5.499 \\pm 0.680$ & $5.677 \\pm 0.609$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.016 $\\pm$ 0.001} & $0.018 \\pm 0.000$ & $0.059 \\pm 0.008$ & $0.054 \\pm 0.009$ & $0.013 \\pm 0.000$\\\\ \npumadyn & \\textbf{1.257 $\\pm$ 0.032} & $1.268 \\pm 0.035$ & $3.552 \\pm 0.767$ & $3.581 \\pm 0.660$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.526 $\\pm$ 0.097} & $2.591 \\pm 0.111$ & $2.777 \\pm 0.194$ & $2.820 \\pm 0.220$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{3.100 $\\pm$ 0.942} & $5.318 \\pm 1.298$ & $12.646 \\pm 0.548$ & $11.288 \\pm 2.365$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.234 $\\pm$ 0.028} & $0.254 \\pm 0.030$ & $0.341 \\pm 0.012$ & $0.331 \\pm 0.016$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.035 $\\pm$ 0.001} & $0.036 \\pm 0.001$ & $0.067 \\pm 0.005$ & $0.110 \\pm 0.012$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & $55.580 \\pm 3.634$ & \\textbf{55.220 $\\pm$ 3.56} & $58.793 \\pm 5.606$ & $60.747 \\pm 2.377$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\n\\vspace{2mm}\n\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{3.493 $\\pm$ 0.489} & $4.191 \\pm 0.878$ & $4.657 \\pm 0.664$ & $5.220 \\pm 0.751$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.014 $\\pm$ 0.000} & $0.018 \\pm 0.000$ & $0.043 \\pm 0.018$ & $0.040 \\pm 0.014$ & $0.013 \\pm 0.000$\\\\ \npumadyn & $1.255 \\pm 0.038$ & \\textbf{1.251 $\\pm$ 0.027} & $3.015 \\pm 0.702$ & $2.448 \\pm 0.742$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.500 $\\pm$ 0.110} & $2.545 \\pm 0.095$ & $2.597 \\pm 0.128$ & $2.702 \\pm 0.159$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{1.330 $\\pm$ 0.409} & $4.791 \\pm 2.805$ & $9.845 \\pm 2.085$ & $9.744 \\pm 2.005$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.221 $\\pm$ 0.012} & $0.228 \\pm 0.018$ & $0.304 \\pm 0.024$ & $0.266 \\pm 0.033$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.034 $\\pm$ 0.002} & $0.035 \\pm 0.001$ & $0.042 \\pm 0.009$ & $0.101 \\pm 0.020$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & $55.628 \\pm 3.597$ & \\textbf{55.214 $\\pm$ 4.03} & $56.608 \\pm 4.488$ & $57.560 \\pm 2.425$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\n\\vspace{2mm}\n\n\n\\caption{Comparison of regression performance (RMSE) on test sets via 5-fold cross-validation for different values of rank ($K$).\nShown in bold is the low-rank approximation method with lowest RMSE. \\textbf{Top} K=14. \\textbf{Middle} K=28. \\textbf{Bottom} K=42.}\n\\label{t:rmse}\n\\end{table*}\nThe main proposed advantage of {\\texttt{mklaren}}\\ over established kernel matrix\napproximation methods is simultaneous approximation of multiple kernels, which\nconsiders the current approximation to the regression line and greedily selects\nthe next kernel and pivot to be included in the decomposition. To elucidate\nthis, we performed a comparison on eight known regression datasets: abalone,\nbank, boston, comp-active, diabetes, ionosphere, kinematics, pumadyn\\footnote{\\url{http://archive.ics.uci.edu/ml/}}\\footnote{\\url{http://www.cs.toronto.edu/~delve/data/datasets.html}}. \n\nSimilar to Cortes \\emph{et al.}~\\cite{Cortes 2012}, seven Gaussian kernels with different\nlengthscale parameters are used. The Gaussian kernel function is defined as\n$k(x, y) = \\text{exp}\\{-\\gamma\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}\\|^2\\}$, where the lengthscale\nparameter $\\gamma$ is in range $2^{-3}, 2^{-2}, ..., 2^0, ...,\u00c2\u00a02^3$. For\napproximation methods {\\texttt{icd}}, {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}, each kernel matrix was\napproximated independently using a fixed maximum rank $K$. The combined feature\nspace of seven kernel matrices was used with ridge regression.\n\nFor {\\texttt{mklaren}}, the approximation is defined simultaneously for all kernels and\nthe maximum rank was set to $7K$, i.e., seven times the maximum rank of\nindividual kernels used in {\\texttt{icd}}, {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}. Thus, the low-rank feature\nspace of all four methods had exactly the same dimension. The uniform kernel\ncombination ({\\texttt{uniform}}) using the full-kernel matrix was included as an\nempirical lower bound.\n\nThe performance was assessed using 5-fold cross-validation as follows.\nInitially up to 1000 data points were selected randomly from the dataset. For\neach random split of the data set, a \\emph{training set} containing 60\\% of the\ndata was used for kernel matrix approximation and fitting the regression model.\nA \\emph{validation set} containing 20\\% of the data was used to select the\nregularization parameter $\\lambda$ from range $10^{-3}, 10^{-2}, ...  10^0,\n...\u00c2\u00a010^3$. The final reported performance using root mean square error (RMSE)\nwas obtained on the \\emph{test set} with remaining 20\\% of the data.  All\nvariables were standardized and the targets {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} were centered to the mean. The look-ahead parameter $\\delta$ was set to 10 for {\\texttt{mklaren}}\\ and {\\texttt{csi}}.\n\n\nThe results using different settings of $K$ are shown in Table~\\ref{t:rmse}.\nNot surprisingly, the performance of supervised {\\texttt{mklaren}}\\ and {\\texttt{csi}}\\ is\nconsistently superior to unsupervised {\\texttt{icd}}\\ and {\\texttt{Nystr\\\"om}}. Moreover, {\\texttt{mklaren}}\\\noutperforms {\\texttt{csi}}\\ on the majority of regression tasks, especially at lower\nsettings of $K$. At higher settings of $K$, the difference begins to vanish as\nall approximation methods obtain sufficient information of the feature\nspace induced by the kernels. \n\n\\begin{table}[ht!]\n\\centering\n\\begin{tabular}{|ll||c|c|c|c|} \n\\hline \nDataset & $n$ & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}}\\\\ \n\\hline \nboston & 506 & \\textbf{42} & 63 & $>140$ & 119\\\\ \nkin & 1000 & \\textbf{63} & $>140$ & $>140$ & $>140$\\\\ \npumadyn & 1000 & \\textbf{49} & $>140$ & 56 & 98\\\\ \nabalone & 1000 & \\textbf{21} & 28 & 35 & 49\\\\ \ncomp & 1000 & \\textbf{49} & 63 & $>140$ & $>140$\\\\ \nionosphere & 351 & \\textbf{14} & \\textbf{14} & 42 & 35\\\\ \nbank & 1000 & \\textbf{21} & 42 & 42 & 112\\\\ \ndiabetes & 442 & \\textbf{14} & \\textbf{14} & \\textbf{14} & 21\\\\ \n\\hline\n\\end{tabular}\n\\vspace{2mm}\n\\caption{Comparison of minimal rank for which the RMSE differs by at most one\nstandard deviation to RMSE obtained with the full kernel matrices using uniform\nalignment. $n$, number of features. Shown in bold is the method with lowest maximal rank\n $K$ to achieve equivalent performance to {\\texttt{uniform}}.} \n\\label{t:ranks}\n\\end{table}\n\nIt is interesting to compare the utilization of the vectors in the\nlow-dimensional feature space. Table~\\ref{t:ranks} shows the minimal\nsetting of $K$ where the performance is at most one standard deviation away\nfrom the performance obtained by {\\texttt{uniform}}. On seven out of eight datasets,\n{\\texttt{mklaren}}\\ reaches equivalent performance to {\\texttt{uniform}}\\ at the smallest setting of\n$K$. On only the diabetes dataset, the unsupervised {\\texttt{icd}}\\ and {\\texttt{Nystr\\\"om}}\\ appear to\noutperform the supervised methods at low ranks. However, at higher setting of\n$K$ the performance of {\\texttt{csi}}\\ and {\\texttt{mklaren}}\\ overtakes {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}\\ as can be\nseen in Table~\\ref{t:rmse}.\n\nOverall the results confirm the utility of the greedy approach to select not\nonly pivots, but also the kernels to be approximated and suggest {\\texttt{mklaren}}\\ to\nbe the method of choice when competitive performance at very low-rank feature\nspaces is desired. Moreover, as irrelevant kernels that are never selected to\nbe added to the decompositions need never be evaluated at test stage. This\npoint is discussed further in the next subsection.\n\n\n\\subsection*{Comparison with MKL methods on rank-one kernels}\n\nThe comparison of {\\texttt{mklaren}}\\ to multiple kernel learning methods using the full\nkernel matrix is challenging as it is unrealistic to expect improved\nperformance with low-rank approximation methods. Although the restriction to\nlow-rank feature spaces may result in implicit regularization and improved\nperformance as a consequence~\\cite{Bach2012}, the difference in implicit\ndimension of the feature space makes the comparison difficult.\n\nWe focus on the ability of {\\texttt{mklaren}}\\ to select from a set of kernels in a way\nthat takes into account the implicit correlations between the kernels. To this\nend, we built on the empirical analysis of Cortes et. al~\\cite{Cortes2012}. The\nmentioned reference used four well-known sentiment analysis datasets compiled\nby Blitzer et. al~\\cite{blitzer2007biographies}. In each dataset, the examples are user\nreviews of products and the target is the product rating in a discrete range\n$1..5$. The features are counts of 4000 most frequent bigrams in each dataset.\nEach bigram was represented by a rank-one kernel, thus enabling the use of\nmultiple kernel learning for feature selection and explicit control over\nfeature space dimension. The datasets contain moderate number of examples:\nbooks ($n=5501$), electronics ($n=5901$), kitchen ($n=5149$) and dvd ($n=5118$).\nThe splits into \\emph{training} and \\emph{test} part were readily included as a\npart of the data set. \n\nThree state-of-the-art multiple kernel methods were used for comparison. All\nmethods are based on maximizing centered kernel\nalignent~\\cite{Cortes2012}. The {\\texttt{align}}\\ method infers the kernel weights\nindependently, while {\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ consider the between-kernel\ncorrelations when maximizing the alignment. The combined kernel learned by all\nthree methods was used with kernel ridge regression model. The {\\texttt{align}}\\ method\nis linear in the number of kernels ($p$), while {\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ are\ncubic as the include solving a full $p \\times p$ system of equations\n({\\texttt{alignf}}) or a QP ({\\texttt{alignfc}}).\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-books.set-test}.pdf}\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-dvd.set-test}.pdf}\\\\\n\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-electronics.set-test}.pdf}\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-kitchen.set-test}.pdf}\n\n\\caption{RMSE on the test set for MKL methods. The rank $K$ is equal to the number of kernels included.}\n\\label{f:rmse_align_test}\n\\end{figure*}\n\nWhen testing for different ranks $K$, the bigrams were first filtered according\nto the descending centered alignment metric for {\\texttt{align}}, {\\texttt{alignf}}, {\\texttt{alignfc}}\\\nprior to optimization.  When using {\\texttt{mklaren}}\\ the $K$ pivot columns were\nselected from the complete set of 4000 bigrams and $\\delta$ was set to 1.\nNote that in this scenario, {\\texttt{mklaren}}\\ is equivalent to the original LAR\nalgorithm, thus excluding the effect of low-rank approximation and comparing\nonly the kernel selection part. This way, the same dimension of the feature\nspace was ensured.\n\nThe performance was measured via 5-fold crossvalidation. At each step, 80\\% of\nthe \\emph{training} was used for kernel matrix approximation ({\\texttt{mklaren}}) or\ndetermining kernel weights ({\\texttt{align}}, {\\texttt{alignf}}, {\\texttt{alignfc}}). The remaining\n20\\% of the training set was used for selecting regularization parameter\n$\\lambda$ from range $10^{-3}, 10^{-2}, ... 10^3$ and the final performance was\nreported on the \\emph{test} set using RMSE.\n\nThe results for different settings of $K$ are shown in\nFigure~\\ref{f:rmse_align_test}. For low setting of $K$, {\\texttt{mklaren}}\\ outperforms\nall three MKL methods that assume full kernel-matrices. At higher settings of\n$K$ and the number of kernels considered, {\\texttt{mklaren}}\\ turns equivalent to\n{\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ in terms on performance, showing that the greedy\nkernel and pivot selection criterion considers implicit correlations between\nkernels. However, there is an important difference in computational complexity.\nNote that {\\texttt{mklaren}}\\ is linear in the number of kernels $p$, the fact that can\nmake it preferable over {\\texttt{alignf}}\\ and {\\texttt{alignfc}}. \n\n\n\nThus, the same effect for accounting of between kernel\ncorrelations is achived at a significanlty lower cost.\n \nWe finally examine the utility of {\\texttt{mklaren}}\\ for model interpretation. We\ncompare the kernels (features) selected by the greedy criterion and the\ncorresponding regression weights for the books dataset on\nFigure~\\ref{f:features_align}. In comparison with the features selected by\n{\\texttt{align}}\\, the bigrams appear to represent explicit expression of sentiment\nrelated to reviews. For example, the words generally expressing positive\nsentiment \\emph{great}, \\emph{wonderful} or \\emph{excellent} were detected by\n{\\texttt{mklaren}}\\, as well as negatively sounding \\emph{disappointing},\n\\emph{ridiculous}, or \\emph{sorry}. All the words were missed by {\\texttt{align}} ,\nexplaining the possible gap in performance seen in\nFigure~\\ref{f:rmse_align_test}. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{{features.dset-books.rank-80.mname-Mklaren}.pdf}\n\\includegraphics[width=0.40\\textwidth]{{features.dset-books.rank-80.mname-align}.pdf} \\\\\n\n\n\n\n\n\n\n\n\n\n\\caption{Weights of top 10 positive and top 10 negative features for {\\texttt{mklaren}}\\ (top) and {\\texttt{align}}\\ (below) on the sentiment analysis task for the books dataset.}\n\\label{f:features_align}\n\\end{figure}\n\n\n\n\n\n\n\\section*{Conclusion}\n\nSubquadratic complexity in the number of training examples is essential in\nlarge-scale application of kernel methods. Learning the kernel matrix\nefficiently from the data and the selection of relevant portions on the data\nearly can reduce time and storage requirements further up the machine learning\npipeline. The complexity with respect to the number of kernels should not be\ndisregarded when the number of kernels is large. Using a greedy low-rank\napproximation to multiple kernels, we achieve linear complexity in the number of\nkernels and data points without sacrificing the consideration of in-between\nkernel correlations.  Moreover, the approach learns a regression model, but is\nnevertheless applicable in any kernel-based model. The extension to classification\nor ranking tasks is an interesting subject for future work. Contrary to the recent\nkernel matrix approximations, we present an idea based entirely on geometric principles,\nwhich is also not limited to transductive learning.  With the abundance of\ndifferent data representations, we expect kernels to remain essential in\nmachine learning applications.   \n\n\n\n\n\\section*{Appendix}\n\n\\subsubsection*{Least-angle regression}\n\\label{app:lar}\n\nWe briefly describe Least-angle regression (LAR, see\nref.~\\cite{friedman2001elements,Hesterberg2008}) for completeness. We use LAR\nas an alternative to the pivot selection criterion in the ICD and to\nsimultaneously learn the regression coefficients. \n\nIn the LAR method, a new column is chosen from the set of candidates such that\nthe correlations with the residual are equal for all active variables.  This is\npossible because all variables (columns) are known \\emph{a priori}, which\nclearly does not hold for candidate pivot columns. The monotonically decreasing\nmaximal correlation in the active set is therefore not guaranteed. Moreover,\nthe addition of a column to the active set potentially affects the values in\nall further columns.  Naively recomputing these values\nat each iteration would yield a computational complexity of order $O(n^2)$.\n\nLet the predictor variables ${{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_p}$ be vectors in\n$\\mathbb{R}^{n}$, arranged in a matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}} \\in \\mathbb{R}^{n \\times p}$.\nThe associated response vector is ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\in \\mathbb{R}^{n}$.  The LAR method\niteratively selects the predictor variables ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ and the corresponding\ncoefficients $\\beta_j$ are updated at the same time as they are moved towards\ntheir least-squares coefficients. At last step, the method reaches the\nleast-squares solution ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$.\n\n\nThe high-level pseudo code is as follows:\n\n\n\\begin{enumerate}\n\n\\item Start with the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - \\bar{{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}}$, and regression\ncoefficients $\\beta_1, \\beta_2, ... \\beta_p = 0$.\n\n\\item Find the variable ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ most correlated with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Move $\\beta_j$ towards its least-squares coefficient until another\n${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_k$ has as much correlation with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Move $\\beta_j$ and $\\beta_k$ in the direction towards their joint\nleast-sq. coeff., until some new ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_l$ has as much correlation with\n${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Repeat until all variables have been entered, reaching the least-sq.\nsolution.  \n\n\\end{enumerate}\n\nNote that the method is easily modified to include early stopping, after a\nmaximum number of selected predictor variables are included.  Importantly, the\nmethod can be viewed as a version of supervised Incomplete Cholesky\nDecomposition of the \\emph{linear kernel} ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^T$ which\ncorresponds to the usual inner product in $\\mathbb{R}^p$.  \n\n\n\nAssume that the predictor variables are standardized and response has had its mean subtracted off:\n\n", "itemtype": "equation", "pos": 42042, "prevtext": "\n\nThe least-squares solution of ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^*, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}^*$ is then equivalent to Ridge\nregression of the original data ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}, {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$ with parameter $\\lambda$. For\nproof, see Zou \\& Hastie~\\cite{Zou2005}. The augmented dataset can be included in LAR to achieve the $\\ell_2$-regularized solution.\n\n\\subsubsection*{Computational complexity} \nIt is straightforward to analyze the exact upper bound on computational\ncomplexity of the {\\texttt{mklaren}}\\ algorithm, described in Algorithm~\\ref{a:mklaren}.\nThe look-ahead Cholesky decompositions are standard Cholesky decompositions\nwith $\\delta$ pivots and complexity $O(n\\delta^2)$. The main loop is executed\n$K$ times. The selection of kernel and pivot pairs is based on the LAR\ncriterion, which includes inverting ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$ of size $K \\times K$,\nthus having a complexity of $K^3$. However, as each step is a rank-one\nmodification to ${\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{H}}}}}}_A$, the Morrison-Sherman-Woodbury\nlemma~\\cite{meyer2000matrix} can be used to achieve complexity $O(K^2)$ per\nupdate. The computation of correlations with the bisector in\nEq.~\\ref{e:low_rank_bisector} and residuals are computed for $p$ kernels in\n$O(np\\delta^2)$.  Recomputation of $\\delta$ Cholesky factors requires standard\nCholesky steps of complexity $n\\delta^2$. The computation of the gradient step\nis of the same complexity as the gradient step. Updating the regression line is\n$O(n)$. The QR decomposition in Eq~\\ref{e:beta_qr} takes $O(nK^2)$ and the\ncomputation of linear transform ${\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}}$ in Eq.~\\ref{e:nystrom_cholesky} is of\n$O(K^3 + nK^2)$ complexity. Together, the final complexity is \n\n", "index": 45, "text": "\\begin{equation}\nO(n\\delta^2 + K(K^2 + np\\delta^2 + n\\delta^2) + nK^2 + K^3) =\nO(K^3 + npK\\delta^2),\n\\label{e:complexity}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"O(n\\delta^{2}+K(K^{2}+np\\delta^{2}+n\\delta^{2})+nK^{2}+K^{3})=O(K^{3}+npK%&#10;\\delta^{2}),\" display=\"block\"><mrow><mrow><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>K</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>K</mi><mn>2</mn></msup><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><msup><mi>K</mi><mn>2</mn></msup></mrow><mo>+</mo><msup><mi>K</mi><mn>3</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>K</mi><mn>3</mn></msup><mo>+</mo><mrow><mi>n</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>K</mi><mo>\u2062</mo><msup><mi>\u03b4</mi><mn>2</mn></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nInitialize the \\emph{regression line} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$, the \\emph{residual} {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} and the\n\\emph{active set} $\\mathcal{A}$: \n", "itemtype": "equation", "pos": 62603, "prevtext": "\nthus being linear in the number of data points and kernels.\n\n\n\\section*{Experiments}\n\nIn this section, we provide an empirical evaluation of the proposed methodology\non known regression datasets. We first compare {\\texttt{mklaren}}\\ with several benchmark\nlow-rank matrix approximation methods: Incomplete Cholesky Decomposition ({\\texttt{icd}},\n\\cite{Fine2001}), Cholesky with side Information ({\\texttt{csi}}, \\cite{Bach2005}) and\nthe Nystr\\\"om method ({\\texttt{Nystr\\\"om}}, \\cite{Williams2001}). In the interest of\ngenerality, we omit the comparison with kernel function approximation methods\nlimited to kernels of a particular functional form~\\cite{Rahimi2007,\nVedaldi2012} and the approximation of kernel matrices limited only to the Gaussian\nkernel~\\cite{Si2014, Le2013a, Yang2014}. Finally, a comparison is provided\nwith a family of state-of-the-art multiple kernel learning methods using the\nfull-kernel matrix on a sentiment analysis data set using a large number of\nrank-one kernels~\\cite{Cortes2012}. \n\n\\subsection*{Comparison with low-rank approximations}\n\n\\begin{table*}[ht!]\n\\centering\n\n\\small\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{4.393 $\\pm$ 0.432} & $4.762 \\pm 0.598$ & $6.703 \\pm 0.354$ & $6.611 \\pm 1.272$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.018 $\\pm$ 0.000} & $0.025 \\pm 0.003$ & $0.067 \\pm 0.006$ & $0.065 \\pm 0.006$ & $0.013 \\pm 0.000$\\\\ \npumadyn & \\textbf{1.252 $\\pm$ 0.032} & $1.650 \\pm 0.169$ & $4.024 \\pm 0.655$ & $3.882 \\pm 0.803$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.638 $\\pm$ 0.116} & $2.768 \\pm 0.187$ & $2.906 \\pm 0.222$ & $2.939 \\pm 0.197$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{5.288 $\\pm$ 0.461} & $7.520 \\pm 1.852$ & $14.111 \\pm 1.123$ & $13.763 \\pm 0.580$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.283 $\\pm$ 0.017} & $0.310 \\pm 0.022$ & $0.380 \\pm 0.010$ & $0.377 \\pm 0.015$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.036 $\\pm$ 0.001} & $0.046 \\pm 0.005$ & $0.101 \\pm 0.011$ & $0.128 \\pm 0.010$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & \\textbf{54.680 $\\pm$ 3.61} & $54.953 \\pm 3.018$ & $63.715 \\pm 5.970$ & $68.117 \\pm 3.947$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\\vspace{2mm}\n\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{3.792 $\\pm$ 0.454} & $4.481 \\pm 0.689$ & $5.499 \\pm 0.680$ & $5.677 \\pm 0.609$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.016 $\\pm$ 0.001} & $0.018 \\pm 0.000$ & $0.059 \\pm 0.008$ & $0.054 \\pm 0.009$ & $0.013 \\pm 0.000$\\\\ \npumadyn & \\textbf{1.257 $\\pm$ 0.032} & $1.268 \\pm 0.035$ & $3.552 \\pm 0.767$ & $3.581 \\pm 0.660$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.526 $\\pm$ 0.097} & $2.591 \\pm 0.111$ & $2.777 \\pm 0.194$ & $2.820 \\pm 0.220$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{3.100 $\\pm$ 0.942} & $5.318 \\pm 1.298$ & $12.646 \\pm 0.548$ & $11.288 \\pm 2.365$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.234 $\\pm$ 0.028} & $0.254 \\pm 0.030$ & $0.341 \\pm 0.012$ & $0.331 \\pm 0.016$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.035 $\\pm$ 0.001} & $0.036 \\pm 0.001$ & $0.067 \\pm 0.005$ & $0.110 \\pm 0.012$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & $55.580 \\pm 3.634$ & \\textbf{55.220 $\\pm$ 3.56} & $58.793 \\pm 5.606$ & $60.747 \\pm 2.377$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\n\\vspace{2mm}\n\n\\begin{tabular}{|p{1.5cm}||p{2.3cm}|p{2.3cm}|p{2.1cm}|p{2.1cm}||p{2.1cm}|} \n\\hline \nDataset & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}} & {\\texttt{uniform}}\\\\ \n\\hline \nboston & \\textbf{3.493 $\\pm$ 0.489} & $4.191 \\pm 0.878$ & $4.657 \\pm 0.664$ & $5.220 \\pm 0.751$ & $3.109 \\pm 0.274$\\\\ \nkin & \\textbf{0.014 $\\pm$ 0.000} & $0.018 \\pm 0.000$ & $0.043 \\pm 0.018$ & $0.040 \\pm 0.014$ & $0.013 \\pm 0.000$\\\\ \npumadyn & $1.255 \\pm 0.038$ & \\textbf{1.251 $\\pm$ 0.027} & $3.015 \\pm 0.702$ & $2.448 \\pm 0.742$ & $1.210 \\pm 0.070$\\\\ \nabalone & \\textbf{2.500 $\\pm$ 0.110} & $2.545 \\pm 0.095$ & $2.597 \\pm 0.128$ & $2.702 \\pm 0.159$ & $2.499 \\pm 0.118$\\\\ \ncomp & \\textbf{1.330 $\\pm$ 0.409} & $4.791 \\pm 2.805$ & $9.845 \\pm 2.085$ & $9.744 \\pm 2.005$ & $0.750 \\pm 0.203$\\\\ \nionosphere & \\textbf{0.221 $\\pm$ 0.012} & $0.228 \\pm 0.018$ & $0.304 \\pm 0.024$ & $0.266 \\pm 0.033$ & $0.292 \\pm 0.025$\\\\ \nbank & \\textbf{0.034 $\\pm$ 0.002} & $0.035 \\pm 0.001$ & $0.042 \\pm 0.009$ & $0.101 \\pm 0.020$ & $0.034 \\pm 0.001$\\\\ \ndiabetes & $55.628 \\pm 3.597$ & \\textbf{55.214 $\\pm$ 4.03} & $56.608 \\pm 4.488$ & $57.560 \\pm 2.425$ & $62.142 \\pm 3.991$\\\\ \n\\hline\n\\end{tabular}\n\n\\vspace{2mm}\n\n\n\\caption{Comparison of regression performance (RMSE) on test sets via 5-fold cross-validation for different values of rank ($K$).\nShown in bold is the low-rank approximation method with lowest RMSE. \\textbf{Top} K=14. \\textbf{Middle} K=28. \\textbf{Bottom} K=42.}\n\\label{t:rmse}\n\\end{table*}\nThe main proposed advantage of {\\texttt{mklaren}}\\ over established kernel matrix\napproximation methods is simultaneous approximation of multiple kernels, which\nconsiders the current approximation to the regression line and greedily selects\nthe next kernel and pivot to be included in the decomposition. To elucidate\nthis, we performed a comparison on eight known regression datasets: abalone,\nbank, boston, comp-active, diabetes, ionosphere, kinematics, pumadyn\\footnote{\\url{http://archive.ics.uci.edu/ml/}}\\footnote{\\url{http://www.cs.toronto.edu/~delve/data/datasets.html}}. \n\nSimilar to Cortes \\emph{et al.}~\\cite{Cortes 2012}, seven Gaussian kernels with different\nlengthscale parameters are used. The Gaussian kernel function is defined as\n$k(x, y) = \\text{exp}\\{-\\gamma\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}-{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}\\|^2\\}$, where the lengthscale\nparameter $\\gamma$ is in range $2^{-3}, 2^{-2}, ..., 2^0, ...,\u00c2\u00a02^3$. For\napproximation methods {\\texttt{icd}}, {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}, each kernel matrix was\napproximated independently using a fixed maximum rank $K$. The combined feature\nspace of seven kernel matrices was used with ridge regression.\n\nFor {\\texttt{mklaren}}, the approximation is defined simultaneously for all kernels and\nthe maximum rank was set to $7K$, i.e., seven times the maximum rank of\nindividual kernels used in {\\texttt{icd}}, {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}. Thus, the low-rank feature\nspace of all four methods had exactly the same dimension. The uniform kernel\ncombination ({\\texttt{uniform}}) using the full-kernel matrix was included as an\nempirical lower bound.\n\nThe performance was assessed using 5-fold cross-validation as follows.\nInitially up to 1000 data points were selected randomly from the dataset. For\neach random split of the data set, a \\emph{training set} containing 60\\% of the\ndata was used for kernel matrix approximation and fitting the regression model.\nA \\emph{validation set} containing 20\\% of the data was used to select the\nregularization parameter $\\lambda$ from range $10^{-3}, 10^{-2}, ...  10^0,\n...\u00c2\u00a010^3$. The final reported performance using root mean square error (RMSE)\nwas obtained on the \\emph{test set} with remaining 20\\% of the data.  All\nvariables were standardized and the targets {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} were centered to the mean. The look-ahead parameter $\\delta$ was set to 10 for {\\texttt{mklaren}}\\ and {\\texttt{csi}}.\n\n\nThe results using different settings of $K$ are shown in Table~\\ref{t:rmse}.\nNot surprisingly, the performance of supervised {\\texttt{mklaren}}\\ and {\\texttt{csi}}\\ is\nconsistently superior to unsupervised {\\texttt{icd}}\\ and {\\texttt{Nystr\\\"om}}. Moreover, {\\texttt{mklaren}}\\\noutperforms {\\texttt{csi}}\\ on the majority of regression tasks, especially at lower\nsettings of $K$. At higher settings of $K$, the difference begins to vanish as\nall approximation methods obtain sufficient information of the feature\nspace induced by the kernels. \n\n\\begin{table}[ht!]\n\\centering\n\\begin{tabular}{|ll||c|c|c|c|} \n\\hline \nDataset & $n$ & {\\texttt{mklaren}} & {\\texttt{csi}} & {\\texttt{icd}} & {\\texttt{Nystr\\\"om}}\\\\ \n\\hline \nboston & 506 & \\textbf{42} & 63 & $>140$ & 119\\\\ \nkin & 1000 & \\textbf{63} & $>140$ & $>140$ & $>140$\\\\ \npumadyn & 1000 & \\textbf{49} & $>140$ & 56 & 98\\\\ \nabalone & 1000 & \\textbf{21} & 28 & 35 & 49\\\\ \ncomp & 1000 & \\textbf{49} & 63 & $>140$ & $>140$\\\\ \nionosphere & 351 & \\textbf{14} & \\textbf{14} & 42 & 35\\\\ \nbank & 1000 & \\textbf{21} & 42 & 42 & 112\\\\ \ndiabetes & 442 & \\textbf{14} & \\textbf{14} & \\textbf{14} & 21\\\\ \n\\hline\n\\end{tabular}\n\\vspace{2mm}\n\\caption{Comparison of minimal rank for which the RMSE differs by at most one\nstandard deviation to RMSE obtained with the full kernel matrices using uniform\nalignment. $n$, number of features. Shown in bold is the method with lowest maximal rank\n $K$ to achieve equivalent performance to {\\texttt{uniform}}.} \n\\label{t:ranks}\n\\end{table}\n\nIt is interesting to compare the utilization of the vectors in the\nlow-dimensional feature space. Table~\\ref{t:ranks} shows the minimal\nsetting of $K$ where the performance is at most one standard deviation away\nfrom the performance obtained by {\\texttt{uniform}}. On seven out of eight datasets,\n{\\texttt{mklaren}}\\ reaches equivalent performance to {\\texttt{uniform}}\\ at the smallest setting of\n$K$. On only the diabetes dataset, the unsupervised {\\texttt{icd}}\\ and {\\texttt{Nystr\\\"om}}\\ appear to\noutperform the supervised methods at low ranks. However, at higher setting of\n$K$ the performance of {\\texttt{csi}}\\ and {\\texttt{mklaren}}\\ overtakes {\\texttt{csi}}\\ and {\\texttt{Nystr\\\"om}}\\ as can be\nseen in Table~\\ref{t:rmse}.\n\nOverall the results confirm the utility of the greedy approach to select not\nonly pivots, but also the kernels to be approximated and suggest {\\texttt{mklaren}}\\ to\nbe the method of choice when competitive performance at very low-rank feature\nspaces is desired. Moreover, as irrelevant kernels that are never selected to\nbe added to the decompositions need never be evaluated at test stage. This\npoint is discussed further in the next subsection.\n\n\n\\subsection*{Comparison with MKL methods on rank-one kernels}\n\nThe comparison of {\\texttt{mklaren}}\\ to multiple kernel learning methods using the full\nkernel matrix is challenging as it is unrealistic to expect improved\nperformance with low-rank approximation methods. Although the restriction to\nlow-rank feature spaces may result in implicit regularization and improved\nperformance as a consequence~\\cite{Bach2012}, the difference in implicit\ndimension of the feature space makes the comparison difficult.\n\nWe focus on the ability of {\\texttt{mklaren}}\\ to select from a set of kernels in a way\nthat takes into account the implicit correlations between the kernels. To this\nend, we built on the empirical analysis of Cortes et. al~\\cite{Cortes2012}. The\nmentioned reference used four well-known sentiment analysis datasets compiled\nby Blitzer et. al~\\cite{blitzer2007biographies}. In each dataset, the examples are user\nreviews of products and the target is the product rating in a discrete range\n$1..5$. The features are counts of 4000 most frequent bigrams in each dataset.\nEach bigram was represented by a rank-one kernel, thus enabling the use of\nmultiple kernel learning for feature selection and explicit control over\nfeature space dimension. The datasets contain moderate number of examples:\nbooks ($n=5501$), electronics ($n=5901$), kitchen ($n=5149$) and dvd ($n=5118$).\nThe splits into \\emph{training} and \\emph{test} part were readily included as a\npart of the data set. \n\nThree state-of-the-art multiple kernel methods were used for comparison. All\nmethods are based on maximizing centered kernel\nalignent~\\cite{Cortes2012}. The {\\texttt{align}}\\ method infers the kernel weights\nindependently, while {\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ consider the between-kernel\ncorrelations when maximizing the alignment. The combined kernel learned by all\nthree methods was used with kernel ridge regression model. The {\\texttt{align}}\\ method\nis linear in the number of kernels ($p$), while {\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ are\ncubic as the include solving a full $p \\times p$ system of equations\n({\\texttt{alignf}}) or a QP ({\\texttt{alignfc}}).\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-books.set-test}.pdf}\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-dvd.set-test}.pdf}\\\\\n\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-electronics.set-test}.pdf}\n\\includegraphics[width=0.45\\textwidth]{{errorbar_rmse_dataset.dset-kitchen.set-test}.pdf}\n\n\\caption{RMSE on the test set for MKL methods. The rank $K$ is equal to the number of kernels included.}\n\\label{f:rmse_align_test}\n\\end{figure*}\n\nWhen testing for different ranks $K$, the bigrams were first filtered according\nto the descending centered alignment metric for {\\texttt{align}}, {\\texttt{alignf}}, {\\texttt{alignfc}}\\\nprior to optimization.  When using {\\texttt{mklaren}}\\ the $K$ pivot columns were\nselected from the complete set of 4000 bigrams and $\\delta$ was set to 1.\nNote that in this scenario, {\\texttt{mklaren}}\\ is equivalent to the original LAR\nalgorithm, thus excluding the effect of low-rank approximation and comparing\nonly the kernel selection part. This way, the same dimension of the feature\nspace was ensured.\n\nThe performance was measured via 5-fold crossvalidation. At each step, 80\\% of\nthe \\emph{training} was used for kernel matrix approximation ({\\texttt{mklaren}}) or\ndetermining kernel weights ({\\texttt{align}}, {\\texttt{alignf}}, {\\texttt{alignfc}}). The remaining\n20\\% of the training set was used for selecting regularization parameter\n$\\lambda$ from range $10^{-3}, 10^{-2}, ... 10^3$ and the final performance was\nreported on the \\emph{test} set using RMSE.\n\nThe results for different settings of $K$ are shown in\nFigure~\\ref{f:rmse_align_test}. For low setting of $K$, {\\texttt{mklaren}}\\ outperforms\nall three MKL methods that assume full kernel-matrices. At higher settings of\n$K$ and the number of kernels considered, {\\texttt{mklaren}}\\ turns equivalent to\n{\\texttt{alignf}}\\ and {\\texttt{alignfc}}\\ in terms on performance, showing that the greedy\nkernel and pivot selection criterion considers implicit correlations between\nkernels. However, there is an important difference in computational complexity.\nNote that {\\texttt{mklaren}}\\ is linear in the number of kernels $p$, the fact that can\nmake it preferable over {\\texttt{alignf}}\\ and {\\texttt{alignfc}}. \n\n\n\nThus, the same effect for accounting of between kernel\ncorrelations is achived at a significanlty lower cost.\n \nWe finally examine the utility of {\\texttt{mklaren}}\\ for model interpretation. We\ncompare the kernels (features) selected by the greedy criterion and the\ncorresponding regression weights for the books dataset on\nFigure~\\ref{f:features_align}. In comparison with the features selected by\n{\\texttt{align}}\\, the bigrams appear to represent explicit expression of sentiment\nrelated to reviews. For example, the words generally expressing positive\nsentiment \\emph{great}, \\emph{wonderful} or \\emph{excellent} were detected by\n{\\texttt{mklaren}}\\, as well as negatively sounding \\emph{disappointing},\n\\emph{ridiculous}, or \\emph{sorry}. All the words were missed by {\\texttt{align}} ,\nexplaining the possible gap in performance seen in\nFigure~\\ref{f:rmse_align_test}. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{{features.dset-books.rank-80.mname-Mklaren}.pdf}\n\\includegraphics[width=0.40\\textwidth]{{features.dset-books.rank-80.mname-align}.pdf} \\\\\n\n\n\n\n\n\n\n\n\n\n\\caption{Weights of top 10 positive and top 10 negative features for {\\texttt{mklaren}}\\ (top) and {\\texttt{align}}\\ (below) on the sentiment analysis task for the books dataset.}\n\\label{f:features_align}\n\\end{figure}\n\n\n\n\n\n\n\\section*{Conclusion}\n\nSubquadratic complexity in the number of training examples is essential in\nlarge-scale application of kernel methods. Learning the kernel matrix\nefficiently from the data and the selection of relevant portions on the data\nearly can reduce time and storage requirements further up the machine learning\npipeline. The complexity with respect to the number of kernels should not be\ndisregarded when the number of kernels is large. Using a greedy low-rank\napproximation to multiple kernels, we achieve linear complexity in the number of\nkernels and data points without sacrificing the consideration of in-between\nkernel correlations.  Moreover, the approach learns a regression model, but is\nnevertheless applicable in any kernel-based model. The extension to classification\nor ranking tasks is an interesting subject for future work. Contrary to the recent\nkernel matrix approximations, we present an idea based entirely on geometric principles,\nwhich is also not limited to transductive learning.  With the abundance of\ndifferent data representations, we expect kernels to remain essential in\nmachine learning applications.   \n\n\n\n\n\\section*{Appendix}\n\n\\subsubsection*{Least-angle regression}\n\\label{app:lar}\n\nWe briefly describe Least-angle regression (LAR, see\nref.~\\cite{friedman2001elements,Hesterberg2008}) for completeness. We use LAR\nas an alternative to the pivot selection criterion in the ICD and to\nsimultaneously learn the regression coefficients. \n\nIn the LAR method, a new column is chosen from the set of candidates such that\nthe correlations with the residual are equal for all active variables.  This is\npossible because all variables (columns) are known \\emph{a priori}, which\nclearly does not hold for candidate pivot columns. The monotonically decreasing\nmaximal correlation in the active set is therefore not guaranteed. Moreover,\nthe addition of a column to the active set potentially affects the values in\nall further columns.  Naively recomputing these values\nat each iteration would yield a computational complexity of order $O(n^2)$.\n\nLet the predictor variables ${{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_1, {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_2, ..., {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_p}$ be vectors in\n$\\mathbb{R}^{n}$, arranged in a matrix ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}} \\in \\mathbb{R}^{n \\times p}$.\nThe associated response vector is ${\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\in \\mathbb{R}^{n}$.  The LAR method\niteratively selects the predictor variables ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ and the corresponding\ncoefficients $\\beta_j$ are updated at the same time as they are moved towards\ntheir least-squares coefficients. At last step, the method reaches the\nleast-squares solution ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}$.\n\n\nThe high-level pseudo code is as follows:\n\n\n\\begin{enumerate}\n\n\\item Start with the residual ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - \\bar{{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}}$, and regression\ncoefficients $\\beta_1, \\beta_2, ... \\beta_p = 0$.\n\n\\item Find the variable ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ most correlated with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Move $\\beta_j$ towards its least-squares coefficient until another\n${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_k$ has as much correlation with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Move $\\beta_j$ and $\\beta_k$ in the direction towards their joint\nleast-sq. coeff., until some new ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_l$ has as much correlation with\n${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$.\n\n\\item Repeat until all variables have been entered, reaching the least-sq.\nsolution.  \n\n\\end{enumerate}\n\nNote that the method is easily modified to include early stopping, after a\nmaximum number of selected predictor variables are included.  Importantly, the\nmethod can be viewed as a version of supervised Incomplete Cholesky\nDecomposition of the \\emph{linear kernel} ${\\ensuremath{\\boldsymbol{\\mathrm{{{K}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}^T$ which\ncorresponds to the usual inner product in $\\mathbb{R}^p$.  \n\n\n\nAssume that the predictor variables are standardized and response has had its mean subtracted off:\n\n", "index": 47, "text": "\\begin{equation}\n\\begin{split}\n    \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j\\|_1 &= 0 \\text{ and }   \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j\\|_2 = 1 \\text { for } j = 1, 2, ..., p .\u00c2\u00a0\\\\\n    \\|{\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}}\\|_1 & = 0 \n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\|{\\bm{\\mathrm{{{x}}}}}_{j}\\|_{1}&amp;\\displaystyle=0%&#10;\\text{ and }\\|{\\bm{\\mathrm{{{x}}}}}_{j}\\|_{2}=1\\text{ for }j=1,2,...,p.%&#10;\\unichar{194}\\unichar{160}\\\\&#10;\\displaystyle\\|{\\bm{\\mathrm{{{y}}}}}\\|_{1}&amp;\\displaystyle=0\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>\u2225</mo></mrow><mn>1</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mrow><mi/><mo>=</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>p</mi></mrow><mo>.</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mn>194</mn><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\unichar</mtext></merror><mo>\u2062</mo><mn>160</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mrow><mo>\u2225</mo><mi>\ud835\udc32</mi><mo>\u2225</mo></mrow><mn>1</mn></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mn>0</mn></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\n\nThe LAR algorithm estimates ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ in successive steps.\nSay the predictor ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_i$ has the largest correlation with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$. Then,\nthe index $i$ is added to the active set $\\mathcal{A}$ and the regression line\nand residual are updated:\n\n\n\n", "itemtype": "equation", "pos": 63079, "prevtext": "\n\nInitialize the \\emph{regression line} ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$, the \\emph{residual} {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} and the\n\\emph{active set} $\\mathcal{A}$: \n", "index": 49, "text": "\\begin{equation}\n    {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{0}}}}}} \\text{,  } {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} \\text{ and } \\mathcal{A} = \\emptyset \\text{ .} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\mathrm{{{\\mu}}}}}={\\bm{\\mathrm{{{0}}}}}\\text{, }{\\bm{\\mathrm{{{r}}}}}={%&#10;\\bm{\\mathrm{{{y}}}}}\\text{ and }\\mathcal{A}=\\emptyset\\text{ .}\" display=\"block\"><mrow><mi>\ud835\udf41</mi><mo>=</mo><mrow><mn/><mo>\u2062</mo><mtext>,\u00a0</mtext><mo>\u2062</mo><mi>\ud835\udc2b</mi></mrow><mo>=</mo><mrow><mi>\ud835\udc32</mi><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">\u2205</mi><mo>\u2062</mo><mtext>\u00a0.</mtext></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe gradient $\\gamma$ is set such that a new predictor ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ will enter\nthe model after ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ is updated and all predictors in the active set as\nwell as ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ will be equally correlated to {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}. The key parts are the\nselection of predictors added to the model and the calculation of gradient.\n\nThe active matrix for a subset of indices $j$ with sign $s_j$ is defined as\n\n", "itemtype": "equation", "pos": 63798, "prevtext": "\n\n\nThe LAR algorithm estimates ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}{\\ensuremath{\\boldsymbol{\\mathrm{{{\\beta}}}}}}$ in successive steps.\nSay the predictor ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_i$ has the largest correlation with ${\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}$. Then,\nthe index $i$ is added to the active set $\\mathcal{A}$ and the regression line\nand residual are updated:\n\n\n\n", "index": 51, "text": "\\begin{equation}\n\\begin{split}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}^{\\text{new}} &= {\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}} + \\gamma {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_i \\\\\n{\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}^{\\text{new}} &= {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} - \\gamma {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_i\n\\end{split}\n\\label{se:lar_update}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{\\mu}}}}}^{\\text{new}}&amp;\\displaystyle={%&#10;\\bm{\\mathrm{{{\\mu}}}}}+\\gamma{\\bm{\\mathrm{{{x}}}}}_{i}\\\\&#10;\\displaystyle{\\bm{\\mathrm{{{r}}}}}^{\\text{new}}&amp;\\displaystyle={\\bm{\\mathrm{{{r%&#10;}}}}}-\\gamma{\\bm{\\mathrm{{{x}}}}}_{i}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msup><mi>\ud835\udf41</mi><mtext>new</mtext></msup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udf41</mi><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msup><mi>\ud835\udc2b</mi><mtext>new</mtext></msup></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udc2b</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nBy elementary linear algebra, there exist a \\emph{bisector} ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ - an\nequiangular vector, having $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A\\|_2 = 1$ and spanning equal angles, less\nthan 90 degrees, with vectors in ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$. Define the following quantities\nrespectively: ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$ the active matrix, $A$ the normalization scalar,\n${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ the bisector, and {\\ensuremath{\\boldsymbol{\\mathrm{{{\\omega}}}}}} the vector making equal angles with\nthe columns of ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$. The bisector is obtained as follows.\n\n\n", "itemtype": "equation", "pos": 64729, "prevtext": "\n\nThe gradient $\\gamma$ is set such that a new predictor ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ will enter\nthe model after ${\\ensuremath{\\boldsymbol{\\mathrm{{{\\mu}}}}}}$ is updated and all predictors in the active set as\nwell as ${\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j$ will be equally correlated to {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}. The key parts are the\nselection of predictors added to the model and the calculation of gradient.\n\nThe active matrix for a subset of indices $j$ with sign $s_j$ is defined as\n\n", "index": 53, "text": "\\begin{equation}\n\\begin{split}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A &= \\big( \\cdots s_j {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j \\cdots \\big)  \\text{ for } j \\in \\mathcal{A} \\\\\ns_j &= \\text{sign}\\{{\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j^T{\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}\\}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{X}}}}}_{A}&amp;\\displaystyle=\\big{(}%&#10;\\cdots s_{j}{\\bm{\\mathrm{{{x}}}}}_{j}\\cdots\\big{)}\\text{ for }j\\in\\mathcal{A}%&#10;\\\\&#10;\\displaystyle s_{j}&amp;\\displaystyle=\\text{sign}\\{{\\bm{\\mathrm{{{x}}}}}_{j}^{T}{%&#10;\\bm{\\mathrm{{{r}}}}}\\}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\ud835\udc17</mi><mi>A</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>s</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2062</mo><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>s</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mtext>sign</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc2b</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nThe calculation of gradient proceeds as follows. Get the maximum vector of\ncorrelations. Active set contains variables with highest absolute correlations.\n\n\n\n", "itemtype": "equation", "pos": 65758, "prevtext": "\n\nBy elementary linear algebra, there exist a \\emph{bisector} ${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ - an\nequiangular vector, having $\\|{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A\\|_2 = 1$ and spanning equal angles, less\nthan 90 degrees, with vectors in ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$. Define the following quantities\nrespectively: ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$ the active matrix, $A$ the normalization scalar,\n${\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A$ the bisector, and {\\ensuremath{\\boldsymbol{\\mathrm{{{\\omega}}}}}} the vector making equal angles with\nthe columns of ${\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A$. The bisector is obtained as follows.\n\n\n", "index": 55, "text": "\\begin{equation}\n\\begin{split}\n{\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}}_A &= {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A \\\\\nA &= ({\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}_A^T{\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}}_A{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}_A)^{-1/2} \\\\\n{\\ensuremath{\\boldsymbol{\\mathrm{{{\\omega}}}}}} &= A {\\ensuremath{\\boldsymbol{\\mathrm{{{T}}}}}}_A^{-1}{\\ensuremath{\\boldsymbol{\\mathrm{{{1}}}}}}_A \\\\ \n{\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A &= {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A {\\ensuremath{\\boldsymbol{\\mathrm{{{\\omega}}}}}}_A \n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle{\\bm{\\mathrm{{{T}}}}}_{A}&amp;\\displaystyle={\\bm{\\mathrm%&#10;{{{X}}}}}_{A}^{T}{\\bm{\\mathrm{{{X}}}}}_{A}\\\\&#10;\\displaystyle A&amp;\\displaystyle=({\\bm{\\mathrm{{{1}}}}}_{A}^{T}{\\bm{\\mathrm{{{T}}%&#10;}}}_{A}{\\bm{\\mathrm{{{1}}}}}_{A})^{-1/2}\\\\&#10;\\displaystyle{\\bm{\\mathrm{{{\\omega}}}}}&amp;\\displaystyle=A{\\bm{\\mathrm{{{T}}}}}_{%&#10;A}^{-1}{\\bm{\\mathrm{{{1}}}}}_{A}\\\\&#10;\\displaystyle{\\bm{\\mathrm{{{u}}}}}_{A}&amp;\\displaystyle={\\bm{\\mathrm{{{X}}}}}_{A}%&#10;{\\bm{\\mathrm{{{\\omega}}}}}_{A}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>\ud835\udc13</mi><mi>A</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>\ud835\udc17</mi><mi>A</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc17</mi><mi>A</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>A</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mn>\ud835\udfcf</mn><mi>A</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc13</mi><mi>A</mi></msub><mo>\u2062</mo><msub><mn>\ud835\udfcf</mn><mi>A</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>\ud835\udf4e</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><mi>A</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc13</mi><mi>A</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><msub><mn>\ud835\udfcf</mn><mi>A</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><msub><mi>\ud835\udc2e</mi><mi>A</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msub><mi>\ud835\udc17</mi><mi>A</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udf4e</mi><mi>A</mi></msub></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n where $\\text{min}^+$ is the minimum over positive components.  \n\nBy Eq.~\\ref{se:lar_update}, we the change in correlations within the active set\ncan be expressed.\n\n", "itemtype": "equation", "pos": 66563, "prevtext": "\n\nThe calculation of gradient proceeds as follows. Get the maximum vector of\ncorrelations. Active set contains variables with highest absolute correlations.\n\n\n\n", "index": 57, "text": "\\begin{equation}\n\\begin{split}\nc_j &= {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j^T{\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}} \\\\\nC &= \\text{max}_j\\{c_j\\} \\\\\n{\\ensuremath{\\boldsymbol{\\mathrm{{{a}}}}}} &= {\\ensuremath{\\boldsymbol{\\mathrm{{{X}}}}}}_A^T {\\ensuremath{\\boldsymbol{\\mathrm{{{u}}}}}}_A \\\\\n \\\\\n\\gamma &= \\text{min}^+_{j \\in \\mathcal{A}^c} \\{\\frac{C - c_j}{A_A - a_j}, \\frac{C + c_j}{A_A + a_j}\\}\n\\label{se:lar_minimum}\n\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle c_{j}&amp;\\displaystyle={\\bm{\\mathrm{{{x}}}}}_{j}^{T}{%&#10;\\bm{\\mathrm{{{r}}}}}\\\\&#10;\\displaystyle C&amp;\\displaystyle=\\text{max}_{j}\\{c_{j}\\}\\\\&#10;\\displaystyle{\\bm{\\mathrm{{{a}}}}}&amp;\\displaystyle={\\bm{\\mathrm{{{X}}}}}_{A}^{T}%&#10;{\\bm{\\mathrm{{{u}}}}}_{A}\\\\&#10;\\\\&#10;\\displaystyle\\gamma&amp;\\displaystyle=\\text{min}^{+}_{j\\in\\mathcal{A}^{c}}\\{\\frac{%&#10;C-c_{j}}{A_{A}-a_{j}},\\frac{C+c_{j}}{A_{A}+a_{j}}\\}\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><msub><mi>c</mi><mi>j</mi></msub></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc2b</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>C</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msub><mtext>max</mtext><mi>j</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>c</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>\ud835\udc1a</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>\ud835\udc17</mi><mi>A</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc2e</mi><mi>A</mi></msub></mrow></mrow></mtd></mtr><mtr><mtd/></mtr><mtr><mtd columnalign=\"right\"><mi>\u03b3</mi></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msubsup><mtext>min</mtext><mrow><mi>j</mi><mo>\u2208</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi><mi>c</mi></msup></mrow><mo>+</mo></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">{</mo><mfrac><mrow><mi>C</mi><mo>-</mo><msub><mi>c</mi><mi>j</mi></msub></mrow><mrow><msub><mi>A</mi><mi>A</mi></msub><mo>-</mo><msub><mi>a</mi><mi>j</mi></msub></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>C</mi><mo>+</mo><msub><mi>c</mi><mi>j</mi></msub></mrow><mrow><msub><mi>A</mi><mi>A</mi></msub><mo>+</mo><msub><mi>a</mi><mi>j</mi></msub></mrow></mfrac><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\nFor the predictors in active set, we have\n\n", "itemtype": "equation", "pos": 67182, "prevtext": "\n where $\\text{min}^+$ is the minimum over positive components.  \n\nBy Eq.~\\ref{se:lar_update}, we the change in correlations within the active set\ncan be expressed.\n\n", "index": 59, "text": "\\begin{equation}\n    c_j^{\\text{new}} = {\\ensuremath{\\boldsymbol{\\mathrm{{{x}}}}}}_j^T({\\ensuremath{\\boldsymbol{\\mathrm{{{y}}}}}} - {\\ensuremath{\\boldsymbol{\\mathrm{{{r}}}}}}^{\\text{new}}) = c_j - \\gamma a_j\n    \\label{se:lar_new_correlation}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"c_{j}^{\\text{new}}={\\bm{\\mathrm{{{x}}}}}_{j}^{T}({\\bm{\\mathrm{{{y}}}}}-{\\bm{%&#10;\\mathrm{{{r}}}}}^{\\text{new}})=c_{j}-\\gamma a_{j}\" display=\"block\"><mrow><msubsup><mi>c</mi><mi>j</mi><mtext>new</mtext></msubsup><mo>=</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>j</mi><mi>T</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc32</mi><mo>-</mo><msup><mi>\ud835\udc2b</mi><mtext>new</mtext></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>c</mi><mi>j</mi></msub><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><msub><mi>a</mi><mi>j</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04366.tex", "nexttext": "\n\nA variable is selected from the remaining variables in $\\mathcal{A}^c$, such\nthat $c_j^{\\text{new}}$ is maximal.  Equaling Eq.~\\ref{se:lar_new_correlation}\nand Eq.~\\ref{se:lar_new_correlation_a}, and maximizing yields $\\gamma = \\frac{C -\nc_j}{A - a_j}$. Similarly, $-c_j^{\\text{new}}$ for the reverse covariate is\nmaximal at $\\gamma = \\frac{C + c_j}{A + a_j}$. Hence, $\\gamma$ is chosen in\nEq.~\\ref{se:lar_minimum} as a minimal value for which an variable joins the\nactive set.\n\n\n\n\n\\bibliographystyle{IEEEtran}\n\\addcontentsline{toc}{section}{References}\n\\bibliography{ref}\n\n\n", "itemtype": "equation", "pos": 67482, "prevtext": "\nFor the predictors in active set, we have\n\n", "index": 61, "text": "\\begin{equation}\n|c_j^{\\text{new}}| = C - \\gamma A ,  \\text{  for } j \\in \\mathcal{A}.\n\\label{se:lar_new_correlation_a}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"|c_{j}^{\\text{new}}|=C-\\gamma A,\\text{ for }j\\in\\mathcal{A}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>c</mi><mi>j</mi><mtext>new</mtext></msubsup><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mi>C</mi><mo>-</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mi>A</mi></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mtext>\u00a0for\u00a0</mtext><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9c</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]