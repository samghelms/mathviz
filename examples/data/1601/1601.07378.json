[{"file": "1601.07378.tex", "nexttext": "\nwhere $-\\frac{1}{2}\\Delta$ is the kinetic energy operator,\n$V_{\\mathrm{loc}}$ the local potential,\n$V_{\\mathrm{nl}}$ the nonlocal term,\nand $S$ the overlapping operator.\nIn the case of the norm-conserving pseudopotential,\n$S$ could simply be interpreted as the identity operator.\nIn this paper, we refer to the pseudo wave function simply as the\nwave function.\n\n\\iffalse\nWe always use the periodic boundary condition, and the translational symmetry\nimplies that electronic eigenstate $(\\epsilon_i, \\tilde{\\Psi}_i)$ could be\ncharacterized by a good quantum number $k$ which is a vector in the Brillouin\nzone \\cite{Martin04}. \nIntroducing a second index corresponding to the energy level, $n$,\nand the following the Bloch theorem \\cite{Martin04}, we can write the wave function\n$\\tilde{\\Psi}_{n{\\bf k}}$ into:\n", "itemtype": "equation", "pos": 7597, "prevtext": "\n\n\\title{Parallel 3-dim fast Fourier transforms with load balancing of the plane waves  }\n\\author{Xingyu Gao}\n\\affiliation{Laboratory of Computational Physics, Huayuan Road 6, Beijing 100088, P.R.~China}\n\\affiliation{Institute of Applied Physics and Computational Mathematics, Fenghao East Road 2, Beijing 100094, P.R.~China}\n\\affiliation{CAEP Software Center for High Performance Numerical Simulation, Huayuan Road 6, Beijing 100088, P.R.~China}\n\\author{Zeyao Mo}\n\\affiliation{Laboratory of Computational Physics, Huayuan Road 6, Beijing 100088, P.R.~China}\n\\affiliation{Institute of Applied Physics and Computational Mathematics, Fenghao East Road 2, Beijing 100094, P.R.~China}\n\\affiliation{CAEP Software Center for High Performance Numerical Simulation, Huayuan Road 6, Beijing 100088, P.R.~China}\n\\author{Jun Fang}\n\\affiliation{Institute of Applied Physics and Computational Mathematics, Fenghao East Road 2, Beijing 100094, P.R.~China}\n\\affiliation{CAEP Software Center for High Performance Numerical Simulation, Huayuan Road 6, Beijing 100088, P.R.~China}\n\\author{Han Wang}\n\\email{wang_han@iapcm.ac.cn}\n\\affiliation{Institute of Applied Physics and Computational Mathematics, Fenghao East Road 2, Beijing 100094, P.R.~China}\n\\affiliation{CAEP Software Center for High Performance Numerical Simulation, Huayuan Road 6, Beijing 100088, P.R.~China}\n\n\\begin{abstract}\n\\noindent\nThe plane wave method is most widely used for solving the Kohn--Sham equations\nin first-principles materials science computations.  \nIn this procedure, the three-dimensional (3-dim) trial wave functions' fast Fourier transform (FFT) is a regular\noperation and one of the most demanding algorithms in terms of \nthe scalability on a parallel machine. We propose a new partitioning\nalgorithm for the 3-dim FFT grid to accomplish the trade-off between the\ncommunication overhead and load balancing of the plane waves. It is shown by\nqualitative analysis and numerical results that our approach could scale the plane wave \nfirst-principles calculations up to more nodes.\n\n\\vspace{5pt}\n\\noindent\\textbf{Keywords}: first-principles calculation, Kohn--Sham equation, plane wave, FFT, load balancing.\n\\end{abstract}\n\n\\maketitle\n\n\n\n\\section{Introduction}\n\\noindent In the context of Density Functional Theory (DFT),\nsolving the Kohn--Sham equation is the most time-consuming part\nof the first-principles materials science computations~\\cite{Kohn65, Kresse96CMS, Kresse96PRB}.\nThe plane wave method, which is a widely used numerical approach~\\cite{Payne92},\n{could lead} to a large-scale dense algebraic eigenvalue problem. \nThis problem is usually solved by the iterative \ndiagonalization methods such as Davidson's\\cite{Liu1978}, \nRMM-DIIS\\cite{Kresse96PRB}, LOBPCG\\cite{Knyazev01SIAM}, Chebyshev\npolynomial filtering subspace iteration\\cite{Zhou06JCP}, etc.\nThe elementary operation of the iteration methods is the matrix-vector multiplication.\nSince the large-scale dense matrix is not suitable for explicit assembly, \nthe matrix-vector multiplication is realized\nby applying the Hamiltonian operator on trial wave functions. \n{The local term of the effective potential is one part of the Hamiltonian operator.}\nIn order to compute its action in a lower time complexity, \nwe perform 3-dim FFT twice on one trial wave function in each matrix-vector \nmultiplication. \n\n\n\nThere are three features to make the trial wave function's FFT one of the most\ndemanding algorithms to scale on a parallel machine. The first is the \nmoderate sized FFT grid rather than a large one. The ratio of computation \nto communication of the parallel 3-dim FFT is of order\n$\\log N$ where $N$, the single dimension of the FFT grid, is usually \n$\\mathcal{O}(10^2)$ in most first-principles calculations of bulk materials. \nThe second is the accumulated communication overhead led by many execution times corresponding to many wave functions in large-scale problems. \nThousands of FFTs may run at each step of iterative diagonalization. \nThe third is the all-to-all type communication required by the data\ntransposes. {This can limit the parallel scaling due to the large number of\nsmall messages in the network resulting in competition as well as latency\nissues.} \n\n\n\nIt has already been recognized that making fewer and larger messages can\nspeed up parallel trial wave functions' FFTs. \nThe hybrid OpenMP/MPI implementation \\cite{Goedecker03CPC, Canning2012CSC} can \nlead to fewer and larger messages compared to a pure MPI version. \nAnd a blocked version \\cite{Canning2012CSC} performs a number of trial\nwave functions' FFTs at the same time to aggregate the message sizes and reduce\nthe latency problem.\n\nIn first-principles calculations, \nwe should consider not only the parallel scaling of wave functions' FFTs, \nbut also the load balancing of intensive computations on the plane waves \nthat expand the wave functions. The workload of these computations\nare usually inhomogeneously distributed on a standard 3-dim FFT grid.\nThus a greedy algorithm is usually used to optimize the load balancing.\nHowever, this algorithm results in global all-to-all communications across all\nthe processes, thus the latency overhead would grow in proportion to the number \nof processors and might contribute substantially to the total simulation time.\nHaynes et.~al.~\\cite{Haynes00CPC} present a partitioning approach \nfor the 3-dim FFT grid that minimizes the latency cost. Their method depends\ncritically on the Danielson-Lanczos Lemma \\cite{Danielson1942} and requires a particular data\ndistribution, which limits the possibilities to improve the load balancing of\nthe plane waves.\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{5pt}\nIn this paper, we propose a new partitioning method for the 3-dim FFT grid,\n  with which we need independent local all-to-all communications for each data transpose rather than\n  one global all-to-all communication.\n\n\n\n\nWith this communication pattern preserved, we develop the method to improve \nthe load balancing by adjusting the data distribution among the working processors.\n\n\n\nBy numerical examples, we show that\nalthough its load balancing is not as perfect as that of the greedy algorithm,\nthe new approach can be more favorable for \nparallel scaling by making the fewer and larger messages.\nHence we are allowed to accomplish \nthe trade-off between the load balancing of the plane waves and communication \noverhead in the trial wave functions' FFTs. And such a trade-off \ncould scale the plane wave first-principles calculations up to more nodes.\nWith the proposed partitioning method, we design a compact parallel 3-dim FFT to \nreduce the amount of calculations and passing messages without lost of accuracy. \n\n\\vspace{5pt}\nThe rest of this paper is organized as follows. In Section 2 we introduce\nthe elemental role of trial wave functions' FFTs in the plane wave method.\nIn Section 3 we introduce the greedy algorithm for load balancing of the plane waves\nand analyze the resulting communication cost. In Section 4 we describe\nthe new partitioning algorithms and implementations. In Section 5 we show\nthe numerical results. The last section gives concluding remarks.\n\n\n\\section{Role of trial wave functions' FFT}\nIn this section, we explain the elemental role of trial wave functions' FFTs\nin solving the Kohn--Sham equation using a plane wave basis set.\n\nIn the pseudopotential\n(norm-conserving \\cite{HSC79} or ultrasoft \\cite{Vanderbilt90} pseudopotential)\nsetting or the projector augmented wave (PAW) \\cite{Blochl94,Kresse99PRB}\napproach, the pseudo wave function $\\tilde{\\Psi}_i$ satisfies \nthe Kohn--Sham equation which looks like:\n\n", "index": 1, "text": "\\begin{equation}\n    \\left(-\\frac{1}{2}\\Delta + V_{\\mathrm{loc}} + V_{\\mathrm{nl}}\n    \\right)\\tilde{\\Psi}_i = \\epsilon_i S\\tilde{\\Psi}_i,\n    \\label{eq:Kohn-Sham}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\left(-\\frac{1}{2}\\Delta+V_{\\mathrm{loc}}+V_{\\mathrm{nl}}\\right)\\tilde{\\Psi}_{%&#10;i}=\\epsilon_{i}S\\tilde{\\Psi}_{i},\" display=\"block\"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mi mathvariant=\"normal\">\u0394</mi></mrow></mrow><mo>+</mo><msub><mi>V</mi><mi>loc</mi></msub><mo>+</mo><msub><mi>V</mi><mi>nl</mi></msub></mrow><mo>)</mo></mrow><mo>\u2062</mo><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mo>=</mo><mrow><msub><mi>\u03f5</mi><mi>i</mi></msub><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nwhere $u_{n{\\bf k}}({\\bf r})$ is a cell periodic function. The cell \nwe consider the (unit) cell as a parallelpiped domain with basis vectors\n$\\vec{a}_1$, $\\vec{a}_2$, and $\\vec{a}_3$, which are associated with\nreciprocal basis vectors $\\vec{b}_1$, $\\vec{b}_2$, and $\\vec{b}_3$ through\n$\\vec{a}_i\\cdot\\vec{b}_j=2\\pi\\delta_{ij},\\,i,j=1,2,3$.\n\nThe cell periodic function $u_{n{\\bf k}}({\\bf r})$ is now written as\na sum of plane waves:\n", "itemtype": "equation", "pos": 8583, "prevtext": "\nwhere $-\\frac{1}{2}\\Delta$ is the kinetic energy operator,\n$V_{\\mathrm{loc}}$ the local potential,\n$V_{\\mathrm{nl}}$ the nonlocal term,\nand $S$ the overlapping operator.\nIn the case of the norm-conserving pseudopotential,\n$S$ could simply be interpreted as the identity operator.\nIn this paper, we refer to the pseudo wave function simply as the\nwave function.\n\n\\iffalse\nWe always use the periodic boundary condition, and the translational symmetry\nimplies that electronic eigenstate $(\\epsilon_i, \\tilde{\\Psi}_i)$ could be\ncharacterized by a good quantum number $k$ which is a vector in the Brillouin\nzone \\cite{Martin04}. \nIntroducing a second index corresponding to the energy level, $n$,\nand the following the Bloch theorem \\cite{Martin04}, we can write the wave function\n$\\tilde{\\Psi}_{n{\\bf k}}$ into:\n", "index": 3, "text": "\n\\[ \\tilde{\\Psi}_{n{\\bf k}}({\\bf r}) = e^{-\\imath{\\bf k}\\cdot{\\bf r}}\\,\nu_{n{\\bf k}}({\\bf r}), \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\Psi}_{n{\\bf k}}({\\bf r})=e^{-\\imath{\\bf k}\\cdot{\\bf r}}\\,u_{n{\\bf k}}(%&#10;{\\bf r}),\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>\u0131</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow><mo>\u22c5</mo><mi>\ud835\udc2b</mi></mrow></mrow></msup></mpadded><mo>\u2062</mo><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nwhere $\\bf G$ runs over reciprocal lattice vectors that can be written as\nintegral multiples of basis vectors $\\vec{b}_1$, $\\vec{b}_2$, and $\\vec{b}_3$.\nIt follows that the wave function\n", "itemtype": "equation", "pos": 9113, "prevtext": "\nwhere $u_{n{\\bf k}}({\\bf r})$ is a cell periodic function. The cell \nwe consider the (unit) cell as a parallelpiped domain with basis vectors\n$\\vec{a}_1$, $\\vec{a}_2$, and $\\vec{a}_3$, which are associated with\nreciprocal basis vectors $\\vec{b}_1$, $\\vec{b}_2$, and $\\vec{b}_3$ through\n$\\vec{a}_i\\cdot\\vec{b}_j=2\\pi\\delta_{ij},\\,i,j=1,2,3$.\n\nThe cell periodic function $u_{n{\\bf k}}({\\bf r})$ is now written as\na sum of plane waves:\n", "index": 5, "text": "\n\\[ u_{n{\\bf k}}({\\bf r}) = \\sum_{\\bf G} u_{n{\\bf k}}({\\bf G}) \\,\ne^{-\\imath{\\bf G}\\cdot{\\bf r}}, \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"u_{n{\\bf k}}({\\bf r})=\\sum_{\\bf G}u_{n{\\bf k}}({\\bf G})\\,e^{-\\imath{\\bf G}%&#10;\\cdot{\\bf r}},\" display=\"block\"><mrow><mrow><mrow><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>\ud835\udc06</mi></munder><mrow><msub><mi>u</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc06</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>\u0131</mi><mo>\u2062</mo><mi>\ud835\udc06</mi></mrow><mo>\u22c5</mo><mi>\ud835\udc2b</mi></mrow></mrow></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nIn practice, only those plane waves satisfying\n\n", "itemtype": "equation", "pos": 9400, "prevtext": "\nwhere $\\bf G$ runs over reciprocal lattice vectors that can be written as\nintegral multiples of basis vectors $\\vec{b}_1$, $\\vec{b}_2$, and $\\vec{b}_3$.\nIt follows that the wave function\n", "index": 7, "text": "\n\\[ \\tilde{\\Psi}_{n{\\bf k}}({\\bf r}) = \\sum_{\\bf G} \\tilde{\\Psi}_{n{\\bf k}}({\\bf G}) \\,\ne^{-\\imath({\\bf k}+{\\bf G})\\cdot{\\bf r}}. \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\Psi}_{n{\\bf k}}({\\bf r})=\\sum_{\\bf G}\\tilde{\\Psi}_{n{\\bf k}}({\\bf G})%&#10;\\,e^{-\\imath({\\bf k}+{\\bf G})\\cdot{\\bf r}}.\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>\ud835\udc06</mi></munder><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc06</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>\u0131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc24</mi><mo>+</mo><mi>\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>\ud835\udc2b</mi></mrow></mrow></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nare included in the computation.\n\\fi\n\n{\nWe use always the periodic boundary condition and expand the wave functions in\nplane waves:\n\n", "itemtype": "equation", "pos": 9580, "prevtext": "\nIn practice, only those plane waves satisfying\n\n", "index": 9, "text": "\\begin{equation}\n    \\left|{\\bf k}+{\\bf G}\\right| < \\sqrt{2E_{\\mathrm{cut}}} \\equiv\n    G_{\\mathrm{cut}}.\n    \\label{eq:Ecut}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\left|{\\bf k}+{\\bf G}\\right|&lt;\\sqrt{2E_{\\mathrm{cut}}}\\equiv G_{\\mathrm{cut}}.\" display=\"block\"><mrow><mrow><mrow><mo>|</mo><mrow><mi>\ud835\udc24</mi><mo>+</mo><mi>\ud835\udc06</mi></mrow><mo>|</mo></mrow><mo>&lt;</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>E</mi><mi>cut</mi></msub></mrow></msqrt><mo>\u2261</mo><msub><mi>G</mi><mi>cut</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nwhere the ${\\bf k}$'s are vectors sampling the first Brillouin zone,\n$n$ is an index of the energy level with given $k$, \nand ${\\bf G}$'s are the reciprocal lattice vectors.} The expansion\n\\eqref{eq:pw_expansion} only includes the plane waves satisfying\n\n", "itemtype": "equation", "pos": 9853, "prevtext": "\nare included in the computation.\n\\fi\n\n{\nWe use always the periodic boundary condition and expand the wave functions in\nplane waves:\n\n", "index": 11, "text": "\\begin{equation}\n    \\tilde{\\Psi}_{n{\\bf k}}({\\bf r}) = \\sum_{\\bf G} \\tilde{\\Psi}_{n{\\bf k}}({\\bf G}) \\,\ne^{-\\imath({\\bf k}+{\\bf G})\\cdot{\\bf r}},\n    \\label{eq:pw_expansion}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{\\Psi}_{n{\\bf k}}({\\bf r})=\\sum_{\\bf G}\\tilde{\\Psi}_{n{\\bf k}}({\\bf G})%&#10;\\,e^{-\\imath({\\bf k}+{\\bf G})\\cdot{\\bf r}},\" display=\"block\"><mrow><mrow><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc2b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>\ud835\udc06</mi></munder><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">\u03a8</mi><mo stretchy=\"false\">~</mo></mover><mrow><mi>n</mi><mo>\u2062</mo><mi>\ud835\udc24</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc06</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>e</mi><mrow><mo>-</mo><mrow><mrow><mi>\u0131</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc24</mi><mo>+</mo><mi>\ud835\udc06</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u22c5</mo><mi>\ud835\udc2b</mi></mrow></mrow></msup></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nare included in the computation.\n\\fi\n\n{\nWe use always the periodic boundary condition and expand the wave functions in\nplane waves:\n\n", "itemtype": "equation", "pos": 9580, "prevtext": "\nIn practice, only those plane waves satisfying\n\n", "index": 9, "text": "\\begin{equation}\n    \\left|{\\bf k}+{\\bf G}\\right| < \\sqrt{2E_{\\mathrm{cut}}} \\equiv\n    G_{\\mathrm{cut}}.\n    \\label{eq:Ecut}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\left|{\\bf k}+{\\bf G}\\right|&lt;\\sqrt{2E_{\\mathrm{cut}}}\\equiv G_{\\mathrm{cut}}.\" display=\"block\"><mrow><mrow><mrow><mo>|</mo><mrow><mi>\ud835\udc24</mi><mo>+</mo><mi>\ud835\udc06</mi></mrow><mo>|</mo></mrow><mo>&lt;</mo><msqrt><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>E</mi><mi>cut</mi></msub></mrow></msqrt><mo>\u2261</mo><msub><mi>G</mi><mi>cut</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\nFor a fixed $N_{\\mathrm{FFT}}$, the latency overhead grows linearly with respect to the number of\nprocessors, which will probably result in a limited parallel scaling.\n\n\n\\section{The new partitioning algorithm and its implementation}\nIn this section, we present a new partitioning algorithm of the 3-dim FFT grid\nto avoid global all-to-all communications required by the data transposes,\nso that the latency cost is alleviated at a cost of small loss of load balancing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{The idea of the basic algorithm}\\label{sec:basic}\n\n\n\n\n\n\nWe assume that the number of processors $p$ can be factorized by $m\\times n$, where\nthe difference between $m$ and $n$, i.e.~$|m-n|$, should be as small as possible.\n\n\nThen the $p$ processors are grouped into $m$ rows by $n$ columns (a $3\\times2$ case is illustrated by Fig.~\\ref{fig:procs}).\nTake the reciprocal space layout for example. We distribute the complete columns\nof data along the $x$ direction following two rules: \n1, The data columns with the same $y$-index are distributed within the same column group of processors.\n2, The data columns with the same $z$-index are distributed within the same row group of processors.\nThe intermediate and the real space data layout can be established in a similar way.\nThe only restrictions are that the intermediate layout \nshares the same data distribution with the reciprocal space layout along the $z$\ndirection and with the real space layout along the $x$ direction.\nIn another word, each $xy$ plane in the intermediate layout\n  is distributed among the same row of processors as the $xy$ plane in the reciprocal space layout with the same $z$ index, \n  and each $yz$ plane in the intermediate layout\n  is distributed among the same column of processors as the $yz$ plane in the real space layout with the same $x$ index.\n  \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{procs.eps}\n    \\caption{The illustration of a $3\\times2$ grid of processors.}\n    \\label{fig:procs}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{5pt}\nA direct implementation of the algorithm is to distribute the data columns in\na cyclic fashion. As an illustration, we show how to use it to distribute a 3-dim FFT\ngrid of size $5\\times5\\times5$ among 6 processors. The 6 processors \nare grouped into 3 rows by 2 columns, as shown by \nFig.~\\ref{fig:procs}.\nThe reciprocal space, intermediate and the real space layouts established by our method are shown, from left to right respectively, by\n\nFig.~\\ref{fig:layout_noredis}.\nWe call this implementation the ``basic algorithm'' in this manuscript.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{layout_noredis.eps}\n    \\caption{The resulting three layouts by using the basic algorithm\n    to partition the 3-dim FFT grid of $5\\times5\\times5$ among 6 processors.}\n    \\label{fig:layout_noredis}\n\\end{figure}\n\n\\vspace{5pt}\nIn general, the first data transpose (between reciprocal space and intermediate layouts) requires $m$ local all-to-all \ncommunications which can be carried out \\emph{independently} within row groups of $n$ processors.\nSimilarly, the second transpose (between intermediate and real space layouts) requires $n$ local all-to-all communications \nwhich can be  carried out \\emph{independently} within column groups of $m$ processors.\nAs shown by Fig.~\\ref{fig:layout_noredis}, \nthe first data transpose \nrequires local all-to-all communications within row groups of two processors,\nand the second data transpose requires local all-to-all communications within column groups of three processors.\nWhen $m$ and $n$ are roughly equal to $\\sqrt{p}$, the communication overhead can be\nestimated as: \n\n", "itemtype": "equation", "pos": 17842, "prevtext": "\n\n\n{\nIn the plane wave discretization of one large-scale problem,\nthe Hamiltonian matrix should never be assembled explicitly.\nInstead, iterative diagonalization techniques are employed together with the\nimplicit matrix-vector multiplication that is realized as the action of the\nHamiltonian operator on the trial wave functions.\nIt is noticed that the local potential is diagonal in the real space.\n\nIn order to obtain efficiently the action of the local potential on\nthe wave function, we should first transform $\\tilde{\\Psi}_{n{\\bf k}}({\\bf\nG})$ to the real space representation\n$\\tilde{\\Psi}_{n{\\bf k}}({\\bf r})$ by one FFT,\nmultiply with the local potential term,\nand then transform the product back to the reciprocal space.\nConsequently, two 3-dim FFTs are required by each action on a trial wave\nfunction.}\n\n\n\\section{The Load balancing issue and the greedy algorithm}\n\n\n\n\n\\subsection{The load balancing issue}\\label{sec:layout}\n\nAs mentioned in the previous section, the plane waves \nare truncated at a certain cut-off radius $G_{\\mathrm{cut}}$.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the charge density $\\rho$ is the sum of squares of the wave functions in the\nreal space, the corresponding cut-off radius for the charge density is $2G_{\\mathrm{cut}}$.\nThe cut-off radius of the local potential $V_{\\mathrm{loc}}$ \ncan be regarded the same as that of $\\rho$, because $V_{\\mathrm{loc}}$ is a\nfunctional of $\\rho$. \nThus, the cut-off radius of $V_{\\mathrm{loc}}\\tilde{\\Psi}_{n{\\bf k}}$ is $3G_{\\mathrm{cut}}$.\nIt should be noted that\n\nthe Kohn--Sham equation \\eqref{eq:Kohn-Sham} is discretized by the\nplane waves lying in the sphere of radius\n$G_{\\mathrm{cut}}$. As illustrated in Fig.~\\ref{fig:wrap-error}, it is\nsufficient to take the FFT grid with only $2G_{\\mathrm{cut}}$ for preventing the wave\nfunctions from the wrap-around error and solving \\eqref{eq:Kohn-Sham} correctly.  \n\n\n\n\n\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{wrap-error.eps}\n    \\caption{A two dimensional sketch of the wrap-around errors in \n    the periodic reciprocal space. The wave functions $|\\Psi\\rangle$ \n    is sampled within a sphere with the radius $G_{\\mathrm{cut}}$ \n    (the innermost circle 1). The charge density $\\rho$\n    and the local potential $V_{\\mathrm{loc}}$ are\n    defined inside a sphere with the radius $2G_{\\mathrm{cut}}$ (circle 2).\n    We would require a sphere with the radius $3G_{\\mathrm{cut}}$ to\n    accurately estimate the operation of the local potential on the wave function.\n    If we apply a smaller FFT grid with only \n    $2G_{\\mathrm{cut}}$, the artificial wrap-around error between \n    $2G_{\\mathrm{cut}}$ and $3G_{\\mathrm{cut}}$ would occur and be folded back into \n    the 2rd circle due to the periodicity. Hence it is sufficient to\n    approximate the wave functions and gradients correctly in circle 1.\n    }\n    \\label{fig:wrap-error}\n\\end{figure}\n\n\\vspace{5pt}\nOn one hand, we compute the operation of the local potential on the trial wave\nfunctions by the 3-dim FFTs on the standard grid determined by the cut-off\nradius $2G_{\\mathrm{cut}}$.\n\n\n{On the other hand, we carry out intensive computations \ntime complexities of which are in proportion to the number of the plane waves in a sphere of radius $G_{\\mathrm{cut}}$,\nincluding the assembly of the matrix on the subspace, the orthogonalization of wave functions, and\nthe actions of other parts of the Hamiltonian operator.} Thus the\nworkload of the intensive calculations is not homogeneously distributed on the grid.\nIn partitioning the grid, one should consider\nnot only the parallel scaling of 3-dim FFTs, but also\nthe load balancing issue of the plane waves calculations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{The greedy algorithm}\nOne 3-dim FFT consists of three successive sets of 1-dim FFTs along the $x$, $y$ and $z$\ndirections. For each set of 1-dim FFT, the data layout guarantees that each processor holds the complete columns of data along the FFT direction.\n\n\nTherefore, there are three data layouts of the 1-dim FFTs along the $x$, $y$ and $z$ directions. We call them\nthe reciprocal space, intermediate and real space layouts, respectively. \n\nThe greedy algorithm is used to build the reciprocal space layout for the sake of load\n  balancing. {In the reciprocal space layout, each processor holds the complete\n  columns along the $x$ direction.}  \n\n\n\nThe workload of each complete column is\nestimated by the number of plane waves within the\ncut-off radius $G_{\\mathrm{cut}}$. As illustrated by Fig. \\ref{fig:greedy-distrib}, \nwe sort these columns in the descending order of workload and\ndistribute the individual columns among processors in a round robin fashion.\nIn this way, the reciprocal space layout is established and each processor holds a set \nof complete columns with approximately equal workload.\nDue to the uniform distribution of workload, we could directly distribute the\nindividual columns in a cyclic way to establish the intermediate and real space \nlayouts. \n\n\n\n\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{greedy_distrib.eps}\n    \\caption{A two dimensional illustration of the greedy algorithm for\n    building the reciprocal space layout.}\n    \\label{fig:greedy-distrib}\n\\end{figure}\n\n\\subsection{The communication pattern and overhead}\n\nWhen finishing one set of 1-dim FFTs along one direction, \nwe transpose the data from the current layout to the next one \nfor the successive set of 1-dim FFTs.\nThe first transpose is between the reciprocal space layout and intermediate\nlayout, while the second one is between the intermediate layout and real space\nlayout. With the reciprocal space layout established by the greedy algorithm, \nthe first transpose typically requires the all-to-all communication.  \nThe second transpose may require no communications if each processor \nholds complete planes (perpendicular to the $x$ direction), or limited local communications if each processor has a\nsection of a plane.\n\n\n\n\n\nIn general, the overhead of the all-to-all data communication mainly consists of two parts: the data transmission and the network latency.\n\nThe transmission cost is proportional to the total size of the data packets, and inversely proportional to the internode bandwidth denoted by $\\beta$.\nThe latency cost is proportional to the number of data transmissions initiated.\nWe denote the latency of one data transmission by $\\alpha$.\nIt worth noting that $\\alpha$ and $\\beta$ are defined for the situation that a node is sending a data \npacket to another node and simultaneously receiving a packet from another node.\n\n\\vspace{5pt}\nWithout lost of generality, we assume that the all-to-all communications is implemented by the\npairwise data exchanges. Alternative implementations can be found in Ref.~\\cite{Rao10}.\nThus the all-to-all communication of $p$ processors can be achieved in \n$p-1$ phases. In each phase, each processor simultaneously sends a data packet\nto one processor and receives a packet from another (usually different) processor. \nThough the sizes of data packets are not uniform (in an Alltoallv operation), \nthe average size of one packet can be estimated by $\\mu N_{\\mathrm{FFT}}/p^2$, where \n$\\mu$ is the size of a single element (typically 16 bytes for a double precision \ncomplex data type), and $N_{\\mathrm{FFT}}$ is number of the FFT grids.\n\nHence we estimate the total cost of one all-to-all communication as:\n\n", "index": 15, "text": "\\begin{equation}\n    t_1 = (p-1)\\left(\\alpha+\\frac{\\mu N_{\\mathrm{FFT}}}{\\beta p^2}\\right).\n    \\label{eq:comm_t1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"t_{1}=(p-1)\\left(\\alpha+\\frac{\\mu N_{\\mathrm{FFT}}}{\\beta p^{2}}\\right).\" display=\"block\"><mrow><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\u03b1</mi><mo>+</mo><mfrac><mrow><mi>\u03bc</mi><mo>\u2062</mo><msub><mi>N</mi><mi>FFT</mi></msub></mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><msup><mi>p</mi><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.07378.tex", "nexttext": "\n\n\nCompared with the estimated cost \\eqref{eq:comm_t1} of the\nglobal all-to-all communication, the growth rate of the latency cost with respect to the number of processors is decreased from $p$ to $\\sqrt p$. \n\n\\vspace{5pt}\nIt should be clarified that both \\eqref{eq:comm_t1} and \\eqref{eq:comm_t2} are\nused to qualitatively illustrate how the communication overhead is decreased \nrather than to give an quantitative interpretation of actual running time. \nCompared with a global all-to-all communication, the new local all-to-all\ncommunications make fewer ($p(\\sqrt p - 1)$ v.s.~$p(p-1)$) and\nlarger ($\\mu N_{\\mathrm{FFT}}/(p\\sqrt p)$ v.s.~$\\mu N_{\\mathrm{FFT}}/p^2$) messages,\nwhich alleviates the competition as well as  latency issues in the network.\n\nSo the proposed partitioning algorithm offers the \nprospect of scaling the plane wave first-principles calculations up to more nodes.\n\n\\subsection{The improved algorithm considering the load balancing}\\label{sec:adv}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe local all-to-all communications can be kept if \nthe aforementioned partitioning restrictions are satisfied, i.e. the intermediate layout \nshares the same data distribution with the reciprocal space layout along the $z$\ndirection, and the same data distribution with the real space layout along the $x$ direction.\nSo we are allowed to improve the reciprocal space layout considering the load\nbalancing issue.\n\n{Here we present one possibility:} \nFirstly the workload of each $xz$-plane and $xy$-plane is estimated by the \nnumber of plane waves in the sphere of radius $G_{\\mathrm{cut}}$.\nSecondly, the $xz$-planes are sorted in the descending order with respect \nto the workload, and then the reordered $xz$-planes are distributed\nto the column groups of processors in a round robin fashion.\nFinally, \nthe $xy$-planes are sorted in the descending order with respect to the workload,\nand then the reordered $xy$-planes are distributed\nto the row groups of processors in a round robin fashion.\nWe will call this implementation the ``improved algorithm'' in this manuscript.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{The compact 3-dim FFT}\n\nAs we have discussed in Sec.~\\ref{sec:layout},\nthe standard 3-dim FFT grid is determined by the cut-off radius \n$2G_{\\mathrm{cut}}$, while the wave functions are represented by plane waves\nwithin the sphere of radius $G_{\\mathrm{cut}}$  (Fig.~\\ref{fig:compact-FFT} (a)).\nThus, one can pick up the complete $x$-data columns\nthat intersect with the sphere to perform 1-dim FFTs along the $x$ direction, \nbecause all other $x$-data columns contain only vanishing values.\nAll the selected columns constitute a cylinder, as shown in \nFig.~\\ref{fig:compact-FFT} (b).\nAfter the $x$ direction FFTs, only this cylinder contains non-zero data.\nThen one can select the complete $y$-data columns that intersect with\nthe cylinder to perform the $y$ direction 1-dim FFTs,\nand the resulting non-zero-data region is a cuboid, as shown in Fig.~\\ref{fig:compact-FFT} (c).\nThe last set of 1-dim FFTs along the $z$ direction is performed on the whole cube \nshown as in Fig.~\\ref{fig:compact-FFT} (d). \n\n\n\n\n\nSuch a compact 3-dim FFT can also reduce the \namount of passing messages and calculations compared to the standard \n3-dim FFT implementation that performs 1-dim FFTs on all $x$ and $y$ data\ncolumns in the cube.\n\n\nIf only the $\\Gamma$-point is used \nfor the ${\\bf k}$-point sampling, we can implement a real mode where the\nreciprocal space and intermediate layouts can be cut by half since we take\ninto account that $\\tilde{\\Psi}_n({\\bf G})=\\tilde{\\Psi}^*_n(-{\\bf G})$.\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.65\\textwidth]{compact3DFFT.eps}\n    \\caption{The illustration of a compact 3-dim FFT.\n      (a): The aqua region presents the sphere of cut-off radius $G_{\\mathrm{cut}}$.\n      (b) -- (d):\n      The aqua regions present union of all data columns selected to perform the $x$, $y$ and $z$ 1-dim FFTs, respectively.\n    }\n    \\label{fig:compact-FFT}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Numerical results}\nWe implement the parallel compact 3-dim FFT for the trial wave functions \nin the in-house plane wave code package CESSP developed on the infrastructure \nJASMIN \\cite{Mo10}. Our implementation is a pure MPI version including \nthe greedy algorithm, basic algorithm and improved algorithm.\n{With these partitioning algorithms, the parallel scaling of solving the\nKohn--Sham equation \\eqref{eq:Kohn-Sham} in PAW approach} are tested on a domestic parallel machine.\nEach node of the machine consists of 2\nIntel Xeon E5540 CPUs (8 cores) and the nodes are connected by the infiniband\nwith double data rate (DDR). \n\n\nThe testing system, {which is sampled by only the $\\Gamma$-point, is defined on an \nFCC (face centered cubic) supercell} consisting of 500 Al (aluminum) atoms. \nThe self-consistent field iteration runs 7 cycles, and in each cycle the RMM-DIIS algorithm\n\\cite{Kresse99PRB} is employed to solve the lowest 1001 eigenstates. \nIn this process, 38038 FFTs of the trial wave functions are executed one by one. \nThe size of the 3-dim FFT grid is $80\\times80\\times80$,\nwhile the sphere of radius $G_{\\mathrm{cut}}$ consists of 35160 plane waves\nfor the expansion of wave functions.\n\n\n\n\n\n\n\n \n\\begin{table}\n  \\centering\n    \\caption{Comparison on the parallel scaling of three partitioning algorithms.}\n    \\label{tab:statlrealF}\n    \\includegraphics[width=0.95\\textwidth]{statlrealF.eps}\n\\end{table}\n\n\\vspace{5pt}\nIn all tests, we launch 8 pure MPI processes per node and count the number \nof data transposes, the total wall time as well as communication time of the \ntrial wave functions' FFTs. The results of the tests are summarized in Tab.~\\ref{tab:statlrealF}.\nIn the greedy algorithm, no data transposes are required between the\nintermediate and real space layouts since the intermediate layout holds the\ncomplete $yz$-planes on each processor. So the greedy algorithm needs half\nnumber of data transposes as our new algorithms.  \nNevertheless, as shown by Tab.~\\ref{tab:statlrealF}, with increasing number of \nprocessors, the greedy algorithm leads to a rapid growth in the communication cost,\nwhich finally takes more than one third of the total computational cost, while \nthe new algorithms (both the basic and its improved versions) can \neffectively suppress the growth in the communication cost.\nWhen the number of processors is less than 24, the greedy algorithm is preferable, while\nwhen the number of processors is more than 24, our algorithms could provide better overall performance.\n\n\n\\begin{table}\n    \\centering\n    \\caption{Comparison on the load balancing of three partitioning\n    algorithms}\n    \\label{fig:loadbalancing}\n    \\includegraphics[width=0.95\\textwidth]{loadbalancing.eps}\n\\end{table}\n\nIn Tab.~\\ref{fig:loadbalancing} we represent the load balancing in the simulations by comparing the maximum\nand minimum numbers of plane waves distributed on a single processor.\nIt is obvious that the load balancing of the greedy algorithm is almost perfect.\n{Although not as perfect as the greedy algorithm, the load balancing\nof our improved algorithm is still acceptable.}\nComparing with the basic algorithm,\nthe improved version can effectively reduce the gap between the maximum and\nminimum number of plane waves on a processor.\nCombining Tab.~\\ref{tab:statlrealF} and \\ref{fig:loadbalancing} together, we\npresent an example of { \nthe trade-off between the load balancing and communication cost:\nthe greedy algorithm has best load balancing but could lead to very limited parallel\nscaling; our new algorithms would achieve much better scaling at a moderate loss of load balancing.}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusion}\nWe present a new partitioning algorithm for the 3-dim FFT grid used in \nthe plane wave first-principles calculations. Compared with the greedy algorithm \n{biased toward} load balancing of the plane wave computations,\nour approach primarily {suppresses the growth in communication overhead {with respect to increasing number of processors} by\nperforming local all-to-all communications during data transposes. \n\n\n{Then we adjust the} data distribution to improve the load\nbalancing with the communication pattern preserved.\n\n\nIn the numerical examples, we present a trade-off:\na much lower communication overhead on a relatively large number of\nprocessors is achieved at a moderate loss of load balancing.\nBy using the new algorithm, we could scale the whole plane wave codes up to more processors than the greedy algorithm.\nFor a better performance, our approach can be seamlessly combined with \nother techniques such as the hybrid OpenMP/MPI implementation or simultaneously\nperforming a large number of FFTs.\n\n\n\n\n\n\n\n\n\\section*{Acknowledgment}\nThe authors would like to thank Hong Guo and Zhang Yang for useful\ndiscussions. And the first author is especially thankful to Xiaowen Xu for his\nencouragement.\nThis work was partially supported by the National Science Foundation of China under\nGrants 91430218 and 61300012, and the National High Technology Research and Development \nProgram of China under Grant 2015AA01A304.\nH.W. is supported by the National Science Foundation of China under\nGrants 11501039 and 91530322.\n\n\n\n\\begin{thebibliography}{17}\n\\expandafter\\ifx\\csname natexlab\\endcsname\\relax\\def\\natexlab#1{#1}\\fi\n\\expandafter\\ifx\\csname bibnamefont\\endcsname\\relax\n  \\def\\bibnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname bibfnamefont\\endcsname\\relax\n  \\def\\bibfnamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname citenamefont\\endcsname\\relax\n  \\def\\citenamefont#1{#1}\\fi\n\\expandafter\\ifx\\csname url\\endcsname\\relax\n  \\def\\url#1{\\texttt{#1}}\\fi\n\\expandafter\\ifx\\csname urlprefix\\endcsname\\relax\\def\\urlprefix{URL }\\fi\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\eprint}[2][]{\\url{#2}}\n\n\\bibitem[{\\citenamefont{Kohn and Sham}(1965)}]{Kohn65}\n\\bibinfo{author}{\\bibfnamefont{W.}~\\bibnamefont{Kohn}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{L.~J.} \\bibnamefont{Sham}},\n  \\bibinfo{journal}{Phys. Rev.} \\textbf{\\bibinfo{volume}{140}},\n  \\bibinfo{pages}{A1133} (\\bibinfo{year}{1965}).\n\n\\bibitem[{\\citenamefont{Kresse and\n  Furthm\\\"uller}(1996{\\natexlab{a}})}]{Kresse96CMS}\n\\bibinfo{author}{\\bibfnamefont{G.}~\\bibnamefont{Kresse}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Furthm\\\"uller}},\n  \\bibinfo{journal}{Comput. Mat. Sci.} \\textbf{\\bibinfo{volume}{6}},\n  \\bibinfo{pages}{15} (\\bibinfo{year}{1996}{\\natexlab{a}}).\n\n\\bibitem[{\\citenamefont{Kresse and\n  Furthm\\\"uller}(1996{\\natexlab{b}})}]{Kresse96PRB}\n\\bibinfo{author}{\\bibfnamefont{G.}~\\bibnamefont{Kresse}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Furthm\\\"uller}},\n  \\bibinfo{journal}{Phys. Rev. B} \\textbf{\\bibinfo{volume}{54}},\n  \\bibinfo{pages}{11169} (\\bibinfo{year}{1996}{\\natexlab{b}}).\n\n\\bibitem[{\\citenamefont{Payne et~al.}(1992)\\citenamefont{Payne, Teter, and\n  Allan}}]{Payne92}\n\\bibinfo{author}{\\bibfnamefont{M.~C.} \\bibnamefont{Payne}},\n  \\bibinfo{author}{\\bibfnamefont{M.~P.} \\bibnamefont{Teter}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{D.~C.} \\bibnamefont{Allan}},\n  \\bibinfo{journal}{Rev. Mod. Phys.} \\textbf{\\bibinfo{volume}{64}},\n  \\bibinfo{pages}{1045} (\\bibinfo{year}{1992}).\n\n\\bibitem[{\\citenamefont{Liu}(1978)}]{Liu1978}\n\\bibinfo{author}{\\bibfnamefont{B.}~\\bibnamefont{Liu}}, in\n  \\emph{\\bibinfo{booktitle}{Numerical Algorithms in Chemistry: Algebraic\n  Methods}}, edited by \\bibinfo{editor}{\\bibfnamefont{E.}~\\bibnamefont{Moler}}\n  \\bibnamefont{and} \\bibinfo{editor}{\\bibfnamefont{I.}~\\bibnamefont{Shavitt}}\n  (\\bibinfo{publisher}{Lawrence Berkley Lab. Univ. of California},\n  \\bibinfo{year}{1978}), p.~\\bibinfo{pages}{49}.\n\n\\bibitem[{\\citenamefont{Knyazev}(2001)}]{Knyazev01SIAM}\n\\bibinfo{author}{\\bibfnamefont{A.~V.} \\bibnamefont{Knyazev}},\n  \\bibinfo{journal}{SIAM J. Sci. Comput.} \\textbf{\\bibinfo{volume}{23}},\n  \\bibinfo{pages}{517} (\\bibinfo{year}{2001}).\n\n\\bibitem[{\\citenamefont{Zhou et~al.}(2001)\\citenamefont{Zhou, Saad, Tiago, and\n  Chelikowsky}}]{Zhou06JCP}\n\\bibinfo{author}{\\bibfnamefont{Y.}~\\bibnamefont{Zhou}},\n  \\bibinfo{author}{\\bibfnamefont{Y.}~\\bibnamefont{Saad}},\n  \\bibinfo{author}{\\bibfnamefont{M.~L.} \\bibnamefont{Tiago}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.~R.} \\bibnamefont{Chelikowsky}},\n  \\bibinfo{journal}{J. Comput. Phys.} \\textbf{\\bibinfo{volume}{219}},\n  \\bibinfo{pages}{172} (\\bibinfo{year}{2001}).\n\n\\bibitem[{\\citenamefont{Goedecker et~al.}(2003)\\citenamefont{Goedecker, Boulet,\n  and Deutsch}}]{Goedecker03CPC}\n\\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{Goedecker}},\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{Boulet}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{T.}~\\bibnamefont{Deutsch}},\n  \\bibinfo{journal}{Comput. Phys. Comm.} \\textbf{\\bibinfo{volume}{154}},\n  \\bibinfo{pages}{105} (\\bibinfo{year}{2003}).\n\n\\bibitem[{\\citenamefont{Canning et~al.}(2012)\\citenamefont{Canning, Shalf,\n  Wright, Anderson, and Gajbe}}]{Canning2012CSC}\n\\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Canning}},\n  \\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Shalf}},\n  \\bibinfo{author}{\\bibfnamefont{N.~J.} \\bibnamefont{Wright}},\n  \\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{Anderson}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{Gajbe}}, in\n  \\emph{\\bibinfo{booktitle}{The 9th International Conference on Scientific\n  Computing}} (\\bibinfo{year}{2012}).\n\n\\bibitem[{\\citenamefont{Haynes and C\\^ot\\'e}(2000)}]{Haynes00CPC}\n\\bibinfo{author}{\\bibfnamefont{P.~D.} \\bibnamefont{Haynes}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{C\\^ot\\'e}},\n  \\bibinfo{journal}{Comput. Phys. Comm.} \\textbf{\\bibinfo{volume}{130}},\n  \\bibinfo{pages}{130} (\\bibinfo{year}{2000}).\n\n\\bibitem[{\\citenamefont{Danielson and Lanczos}(1942)}]{Danielson1942}\n\\bibinfo{author}{\\bibfnamefont{G.~C.} \\bibnamefont{Danielson}}\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{C.}~\\bibnamefont{Lanczos}},\n  \\bibinfo{journal}{J. Franklin Inst.} \\textbf{\\bibinfo{volume}{233}},\n  \\bibinfo{pages}{365} (\\bibinfo{year}{1942}).\n\n\\bibitem[{\\citenamefont{Hamann et~al.}(1979)\\citenamefont{Hamann, Schl\\\"uter,\n  and Chiang}}]{HSC79}\n\\bibinfo{author}{\\bibfnamefont{D.~R.} \\bibnamefont{Hamann}},\n  \\bibinfo{author}{\\bibfnamefont{M.}~\\bibnamefont{Schl\\\"uter}},\n  \\bibnamefont{and} \\bibinfo{author}{\\bibfnamefont{C.}~\\bibnamefont{Chiang}},\n  \\bibinfo{journal}{Phys. Rev. Lett.} \\textbf{\\bibinfo{volume}{43}},\n  \\bibinfo{pages}{1494} (\\bibinfo{year}{1979}).\n\n\\bibitem[{\\citenamefont{Vanderbilt}(1990)}]{Vanderbilt90}\n\\bibinfo{author}{\\bibfnamefont{D.}~\\bibnamefont{Vanderbilt}},\n  \\bibinfo{journal}{Phys. Rev. B} \\textbf{\\bibinfo{volume}{41}},\n  \\bibinfo{pages}{7892} (\\bibinfo{year}{1990}).\n\n\\bibitem[{\\citenamefont{Bl\\\"ochl}(1994)}]{Blochl94}\n\\bibinfo{author}{\\bibfnamefont{P.~E.} \\bibnamefont{Bl\\\"ochl}},\n  \\bibinfo{journal}{Phys. Rev, B} \\textbf{\\bibinfo{volume}{50}},\n  \\bibinfo{pages}{17953} (\\bibinfo{year}{1994}).\n\n\\bibitem[{\\citenamefont{Kresse and Joubert}(1999)}]{Kresse99PRB}\n\\bibinfo{author}{\\bibfnamefont{G.}~\\bibnamefont{Kresse}} \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{J.}~\\bibnamefont{Joubert}},\n  \\bibinfo{journal}{Phys. Rev. B} \\textbf{\\bibinfo{volume}{59}}\n  (\\bibinfo{year}{1999}).\n\n\\bibitem[{\\citenamefont{Rao et~al.}(2010)\\citenamefont{Rao, Zhang, and\n  Li}}]{Rao10}\n\\bibinfo{author}{\\bibfnamefont{L.}~\\bibnamefont{Rao}},\n  \\bibinfo{author}{\\bibfnamefont{Y.}~\\bibnamefont{Zhang}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{Y.}~\\bibnamefont{Li}},\n  \\bibinfo{journal}{Computer Science} \\textbf{\\bibinfo{volume}{37}},\n  \\bibinfo{pages}{186} (\\bibinfo{year}{2010}).\n\n\\bibitem[{\\citenamefont{Mo et~al.}(2010)\\citenamefont{Mo, Zhang, Cao, Liu, Xu,\n  An, Pei, and Zhu}}]{Mo10}\n\\bibinfo{author}{\\bibfnamefont{Z.}~\\bibnamefont{Mo}},\n  \\bibinfo{author}{\\bibfnamefont{A.}~\\bibnamefont{Zhang}},\n  \\bibinfo{author}{\\bibfnamefont{X.}~\\bibnamefont{Cao}},\n  \\bibinfo{author}{\\bibfnamefont{Q.}~\\bibnamefont{Liu}},\n  \\bibinfo{author}{\\bibfnamefont{X.}~\\bibnamefont{Xu}},\n  \\bibinfo{author}{\\bibfnamefont{H.}~\\bibnamefont{An}},\n  \\bibinfo{author}{\\bibfnamefont{W.}~\\bibnamefont{Pei}}, \\bibnamefont{and}\n  \\bibinfo{author}{\\bibfnamefont{S.}~\\bibnamefont{Zhu}},\n  \\bibinfo{journal}{Frontiers of Computer Science in China}\n  \\textbf{\\bibinfo{volume}{4}}, \\bibinfo{pages}{480} (\\bibinfo{year}{2010}).\n\n\\end{thebibliography}\n\n\n\n\n", "itemtype": "equation", "pos": 21660, "prevtext": "\nFor a fixed $N_{\\mathrm{FFT}}$, the latency overhead grows linearly with respect to the number of\nprocessors, which will probably result in a limited parallel scaling.\n\n\n\\section{The new partitioning algorithm and its implementation}\nIn this section, we present a new partitioning algorithm of the 3-dim FFT grid\nto avoid global all-to-all communications required by the data transposes,\nso that the latency cost is alleviated at a cost of small loss of load balancing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{The idea of the basic algorithm}\\label{sec:basic}\n\n\n\n\n\n\nWe assume that the number of processors $p$ can be factorized by $m\\times n$, where\nthe difference between $m$ and $n$, i.e.~$|m-n|$, should be as small as possible.\n\n\nThen the $p$ processors are grouped into $m$ rows by $n$ columns (a $3\\times2$ case is illustrated by Fig.~\\ref{fig:procs}).\nTake the reciprocal space layout for example. We distribute the complete columns\nof data along the $x$ direction following two rules: \n1, The data columns with the same $y$-index are distributed within the same column group of processors.\n2, The data columns with the same $z$-index are distributed within the same row group of processors.\nThe intermediate and the real space data layout can be established in a similar way.\nThe only restrictions are that the intermediate layout \nshares the same data distribution with the reciprocal space layout along the $z$\ndirection and with the real space layout along the $x$ direction.\nIn another word, each $xy$ plane in the intermediate layout\n  is distributed among the same row of processors as the $xy$ plane in the reciprocal space layout with the same $z$ index, \n  and each $yz$ plane in the intermediate layout\n  is distributed among the same column of processors as the $yz$ plane in the real space layout with the same $x$ index.\n  \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{procs.eps}\n    \\caption{The illustration of a $3\\times2$ grid of processors.}\n    \\label{fig:procs}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{5pt}\nA direct implementation of the algorithm is to distribute the data columns in\na cyclic fashion. As an illustration, we show how to use it to distribute a 3-dim FFT\ngrid of size $5\\times5\\times5$ among 6 processors. The 6 processors \nare grouped into 3 rows by 2 columns, as shown by \nFig.~\\ref{fig:procs}.\nThe reciprocal space, intermediate and the real space layouts established by our method are shown, from left to right respectively, by\n\nFig.~\\ref{fig:layout_noredis}.\nWe call this implementation the ``basic algorithm'' in this manuscript.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{layout_noredis.eps}\n    \\caption{The resulting three layouts by using the basic algorithm\n    to partition the 3-dim FFT grid of $5\\times5\\times5$ among 6 processors.}\n    \\label{fig:layout_noredis}\n\\end{figure}\n\n\\vspace{5pt}\nIn general, the first data transpose (between reciprocal space and intermediate layouts) requires $m$ local all-to-all \ncommunications which can be carried out \\emph{independently} within row groups of $n$ processors.\nSimilarly, the second transpose (between intermediate and real space layouts) requires $n$ local all-to-all communications \nwhich can be  carried out \\emph{independently} within column groups of $m$ processors.\nAs shown by Fig.~\\ref{fig:layout_noredis}, \nthe first data transpose \nrequires local all-to-all communications within row groups of two processors,\nand the second data transpose requires local all-to-all communications within column groups of three processors.\nWhen $m$ and $n$ are roughly equal to $\\sqrt{p}$, the communication overhead can be\nestimated as: \n\n", "index": 17, "text": "\\begin{equation}\n    t_2 = (\\sqrt{p}-1)\\left(\\alpha+\\frac{\\mu N_{\\mathrm{FFT}}}{\\beta\n    p\\sqrt{p}}\\right).\n    \\label{eq:comm_t2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"t_{2}=(\\sqrt{p}-1)\\left(\\alpha+\\frac{\\mu N_{\\mathrm{FFT}}}{\\beta p\\sqrt{p}}%&#10;\\right).\" display=\"block\"><mrow><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msqrt><mi>p</mi></msqrt><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\u03b1</mi><mo>+</mo><mfrac><mrow><mi>\u03bc</mi><mo>\u2062</mo><msub><mi>N</mi><mi>FFT</mi></msub></mrow><mrow><mi>\u03b2</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><msqrt><mi>p</mi></msqrt></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]