[{"file": "1601.04691.tex", "nexttext": "\nwhere the amplitude $\\alpha_u(t) \\in \\mathbb{C}$ and $\\Ket{\\psi(t)} \\in \\mathbb{C}^{|V|}$ are both complex. Moreover, we have that $\\alpha_u(t) \\alpha_u^*(t)$ gives the probability that at time $t$ the walker is at the vertex $u$, and thus $\\sum_{u \\in V} \\alpha_u(t) \\alpha^{*}_u(t) = 1$ and $\\alpha_u(t) \\alpha^{*}_u(t) \\in [0,1]$, for all $u \\in V$, $t \\in \\mathbb{R}^{+}$.\n\nThe evolution of the walk is governed by the Schr\\\"{o}dinger equation\n\n", "itemtype": "equation", "pos": 5245, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\nA number of recent studies have investigated the introduction of decoherence in quantum walks and the resulting transition to classical random walks. Interestingly, it has been shown that algorithmic properties of quantum walks with decoherence such as the spreading rate are sometimes better than their purely quantum counterparts. Not only quantum walks with decoherence provide a generalization of quantum walks that naturally encompasses both the quantum and classical case, but they also give rise to new and different probability distribution. The application of quantum walks with decoherence to large graphs is limited by the necessity of evolving a state vector whose size is quadratic in the number of nodes of the graph, as opposed to the linear state vector of the purely quantum (or classical) case. In this technical report, we show how to use perturbation theory to reduce the computational complexity of evolving a continuous-time quantum walk subject to decoherence. More specifically, given a graph over $n$ nodes, we show how to approximate the eigendecomposition of the $n^2 \\times n^2$ Lindblad super-operator from the eigendecomposition of the $n \\times n$ graph Hamiltonian.\n\\end{abstract}\n\n\\section{Introduction}\nQuantum walks on graphs represent the quantum mechanical analogue of classical random walks~\\cite{farhi1998quantum,aharonov2001quantum,kempe2003quantum}. Despite being similar in the definition, the dynamics of the two types of walks can be remarkably different, with quantum walks possessing a number of interesting properties not exhibited by their classical counterparts. In the classical case, the evolution of the walk is described by a real-valued probability vector. In the quantum case, the state is characterized by a complex-valued amplitude vector. An interesting consequence of this is that different paths are naturally allowed to destructively (constructively) interfere with each other.\n\nMost of the work in the literature has considered pure quantum dynamics~\\cite{farhi1998quantum,aharonov2001quantum,kempe2003quantum,childs2009universal,emms2009graph1,rossi2013characterizing,rossi2015measuring}, i.e, fully coherent quantum walks. However, it has been shown that the introduction of decoherence can result in some algorithmic properties of the walk, such as the spreading rate, being better than in the purely quantum case case~\\cite{kendon2007decoherence,whitfield2010quantum}. Most importantly, quantum walks with decoherence represent a generalization of quantum walks that encompasses both classical and quantum walks, as well as new types of walks that result in different probability distributions~\\cite{whitfield2010quantum}.\n\nRecall that decoherence is the process by which a quantum system is altered by its interaction with the environment. The result of this process is a transition of the system from quantum to classical. For example, a quantum walk subject to decoherence transitions to a classical random walk, with a speed that depends on the decoherence rate. Unfortunately, while in the fully classical and fully quantum cases the size of the state vector is $n$, where $n$ denotes the number of nodes of the graph, in the decoherent case the size of the state vector is $n^2$. The Hamiltonian operator acting on the state vector of a unitary quantum walk is represented by a $n \\times n$ matrix. With the addition of decoherence, on the other hand, the evolution is defined by the Lindblad super-operator, which is represented by a $n^2 \\times n^2$ matrix. This clearly limits the possibility of analysing large graph structures using decoherent quantum walks.\n\nIn this technical report we propose to use perturbation theory~\\cite{trefethen1997numerical,van2007computation} to reduce the computational complexity of evolving a continuous-time quantum walk subject to decoherence. In Section~\\ref{quantumwalk} we introduce the necessary quantum mechanical background. In Section~\\ref{perturbation} we review the eigenvalue perturbation problem and in Section~\\ref{decoherence} we show how this can be applied to the problem at hand.\n\n\\section{Continuous-time Quantum Walks with Decoherence}\\label{quantumwalk}\n\\subsection{Continuous-Time Quantum Walks}\nThe continuous-time quantum walk is the quantum analogous of the continuous-time random walk~\\cite{farhi1998quantum}. Let $G = (V,E)$ denote an undirected graph with $n$ nodes. If ${\\mathbf{{p}}}(t) \\in \\mathbb{R}^n$ denotes the state of walk at time $t$, in a continuous-time random walk the state vector evolves according to the equation ${\\mathbf{{p}}}(t) = e^{-Lt} {\\mathbf{{p}}}(0)$, where the graph Laplacian $L$ is the infinitesimal generator matrix of the underlying continuous-time Markov process.\n\nSimilarly to its classical counterpart, the state space of the continuous-time quantum walks is the vertex set of the graph. The classical state vector is replaced by a vector of complex amplitudes over $V$ whose squared norm sums to unity, and as such the state of the system is not constrained to lie in a probability space, thus allowing interference to take place. The general state of the walk at time $t$ is a complex linear combination of the basis states $\\Ket{u}$, i.e.,\n\n", "index": 1, "text": "\\begin{equation}\n\\Ket{\\psi(t)} = \\sum_{u\\in V} \\alpha_u(t) \\Ket{u},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\Ket{\\psi(t)}=\\sum_{u\\in V}\\alpha_{u}(t)\\Ket{u},\" display=\"block\"><mrow><mrow><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>u</mi><mo>\u2208</mo><mi>V</mi></mrow></munder><mrow><msub><mi>\u03b1</mi><mi>u</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo fence=\"true\">|</mo><mi>u</mi><mo>\u27e9</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere we denote the time-independent Hamiltonian as $H$. Generally speaking, a continuous-time quantum walk is induced whenever the structure of the graphs is reflected by the (0,1) pattern of the Hamiltonian. For example, we could take the adjacency matrix or the Laplacian. In the following we assume $H=L$.\n\nGiven an initial state $\\Ket{\\psi(0)}$, solving the Schr\\\"{o}dinger equation gives the expression of the state vector at time $t$,\n\n", "itemtype": "equation", "pos": 5776, "prevtext": "\nwhere the amplitude $\\alpha_u(t) \\in \\mathbb{C}$ and $\\Ket{\\psi(t)} \\in \\mathbb{C}^{|V|}$ are both complex. Moreover, we have that $\\alpha_u(t) \\alpha_u^*(t)$ gives the probability that at time $t$ the walker is at the vertex $u$, and thus $\\sum_{u \\in V} \\alpha_u(t) \\alpha^{*}_u(t) = 1$ and $\\alpha_u(t) \\alpha^{*}_u(t) \\in [0,1]$, for all $u \\in V$, $t \\in \\mathbb{R}^{+}$.\n\nThe evolution of the walk is governed by the Schr\\\"{o}dinger equation\n\n", "index": 3, "text": "\\begin{equation}\n\\frac{\\partial}{\\partial t}\\Ket{\\psi(t)} = -iH\\Ket{\\psi(t)},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial}{\\partial t}\\Ket{\\psi(t)}=-iH\\Ket{\\psi(t)},\" display=\"block\"><mrow><mrow><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nThis can be conveniently expressed in terms of the spectral decomposition of the Hamiltonian $H = \\Phi \\Lambda \\Phi^\\top$, i.e., $\\Ket{\\psi(t)} = \\Phi^\\top e^{-i \\Lambda t} \\Phi \\Ket{\\psi(0)}$,\nwhere $\\Phi$ denotes the $n \\times n$ matrix $\\Phi=(\\phi_1 | \\phi_2 | ... | \\phi_j | ... | \\phi_n)$ with the ordered eigenvectors $\\phi_j$s of $H$ as columns and $\\Lambda = \\mbox{diag}(\\lambda_1, \\lambda_2, ..., \\lambda_j, ..., \\lambda_n)$ is the $n \\times n$ diagonal matrix with the ordered eigenvalues $\\lambda_j$ of $H$ as elements, and we have made use of the fact that $\\mbox{exp}[-iLt]=\\Phi^\\top \\mbox{exp}[-i \\Lambda t] \\Phi$.\n\n\\subsection{Quantum Walks with Decoherence}\n\nThe density matrix is introduced in quantum mechanics to describe a system whose state is an ensemble of pure quantum states $\\Ket{\\psi(i)}$, each with probability $p(i)$~\\cite{nielsen2010quantum}. The density operator of such a system is defined as\n\n", "itemtype": "equation", "pos": 6311, "prevtext": "\nwhere we denote the time-independent Hamiltonian as $H$. Generally speaking, a continuous-time quantum walk is induced whenever the structure of the graphs is reflected by the (0,1) pattern of the Hamiltonian. For example, we could take the adjacency matrix or the Laplacian. In the following we assume $H=L$.\n\nGiven an initial state $\\Ket{\\psi(0)}$, solving the Schr\\\"{o}dinger equation gives the expression of the state vector at time $t$,\n\n", "index": 5, "text": "\\begin{equation}\n\\Ket{\\psi(t)} = e^{-iLt}\\Ket{\\psi(0)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\Ket{\\psi(t)}=e^{-iLt}\\Ket{\\psi(0)}.\" display=\"block\"><mrow><mrow><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow><mo>=</mo><mrow><msup><mi>e</mi><mrow><mo>-</mo><mrow><mi>i</mi><mo>\u2062</mo><mi>L</mi><mo>\u2062</mo><mi>t</mi></mrow></mrow></msup><mo>\u2062</mo><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nFor a quantum walk with state vector $\\Ket{\\psi(t)}$, the corresponding density matrix at time $t$ is $\\rho(t) = \\Braket{\\psi(t) | \\psi(t)}$. Similarly to the Schr\\\"{o}dinger equation, the Liouville-von Neumann equation describes how a density operator evolves in time\n\n", "itemtype": "equation", "pos": 7307, "prevtext": "\nThis can be conveniently expressed in terms of the spectral decomposition of the Hamiltonian $H = \\Phi \\Lambda \\Phi^\\top$, i.e., $\\Ket{\\psi(t)} = \\Phi^\\top e^{-i \\Lambda t} \\Phi \\Ket{\\psi(0)}$,\nwhere $\\Phi$ denotes the $n \\times n$ matrix $\\Phi=(\\phi_1 | \\phi_2 | ... | \\phi_j | ... | \\phi_n)$ with the ordered eigenvectors $\\phi_j$s of $H$ as columns and $\\Lambda = \\mbox{diag}(\\lambda_1, \\lambda_2, ..., \\lambda_j, ..., \\lambda_n)$ is the $n \\times n$ diagonal matrix with the ordered eigenvalues $\\lambda_j$ of $H$ as elements, and we have made use of the fact that $\\mbox{exp}[-iLt]=\\Phi^\\top \\mbox{exp}[-i \\Lambda t] \\Phi$.\n\n\\subsection{Quantum Walks with Decoherence}\n\nThe density matrix is introduced in quantum mechanics to describe a system whose state is an ensemble of pure quantum states $\\Ket{\\psi(i)}$, each with probability $p(i)$~\\cite{nielsen2010quantum}. The density operator of such a system is defined as\n\n", "index": 7, "text": "\\begin{equation}\n\\rho = \\sum_i p(i) \\Ket{\\psi(i)}\\Bra{\\psi(i)}\\,.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\rho=\\sum_{i}p(i)\\Ket{\\psi(i)}\\Bra{\\psi(i)}\\,.\" display=\"block\"><mrow><mrow><mi>\u03c1</mi><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo fence=\"true\">|</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u27e9</mo></mrow><mo>\u2062</mo><mpadded width=\"+1.7pt\"><mrow><mo>\u27e8</mo><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">|</mo></mrow></mpadded></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $L$ is the graph Laplacian and $[A, B] = AB - BA$ denotes the commutator.\n\nWe can add non-unitary decoherence to the system by writing~\\cite{kendon2007decoherence}\n\n", "itemtype": "equation", "pos": 7657, "prevtext": "\nFor a quantum walk with state vector $\\Ket{\\psi(t)}$, the corresponding density matrix at time $t$ is $\\rho(t) = \\Braket{\\psi(t) | \\psi(t)}$. Similarly to the Schr\\\"{o}dinger equation, the Liouville-von Neumann equation describes how a density operator evolves in time\n\n", "index": 9, "text": "\\begin{equation}\n\\frac{\\partial}{\\partial t} \\rho(t) = -i[L,\\rho(t)],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial}{\\partial t}\\rho(t)=-i[L,\\rho(t)],\" display=\"block\"><mrow><mrow><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo>,</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $p$ is the rate per unit time with which we add decoherence to the walk, and $\\mathcal{P}\\rho(t) = \\sum_j P_j \\rho(t) P_j^{\\dag}$ represents the effect of noise on $\\rho(t)$, where $\\lbrace P_j \\rbrace$ is a set of projectors. Intuitively, the effect of the extra terms is to reduce the off-diagonal elements of $\\rho(t)$, i.e., the coherence terms, at a rate $p$ per unit time~\\cite{kendon2007decoherence}, while leaving the diagonal elements unaffected. More specifically, let $\\operatorname{vec}\\big(\\rho(t)\\big)$ be the vectorization of the density matrix $\\rho(t)$. Then we can write\n\n", "itemtype": "equation", "pos": 7912, "prevtext": "\nwhere $L$ is the graph Laplacian and $[A, B] = AB - BA$ denotes the commutator.\n\nWe can add non-unitary decoherence to the system by writing~\\cite{kendon2007decoherence}\n\n", "index": 11, "text": "\\begin{equation}\n\\frac{\\partial}{\\partial t} \\rho(t) = -i[L,\\rho(t)] -p\\rho(t)+p\\mathcal{P}\\rho(t),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial}{\\partial t}\\rho(t)=-i[L,\\rho(t)]-p\\rho(t)+p\\mathcal{P}\\rho(t),\" display=\"block\"><mrow><mrow><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mo>-</mo><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mi>L</mi><mo>,</mo><mrow><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>-</mo><mrow><mi>p</mi><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>p</mi><mo>\u2062</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcab</mi><mo>\u2062</mo><mi>\u03c1</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $E_{vv}$ is the matrix which is 1 in $(v,v)$ and 0 elsewhere, i.e., the projector on the node $v$.\n\n\\section{Eigenvalue Perturbation}\\label{perturbation}\n\nLet $A(t)$ be a $n\\times n$ complex matrix parametrized by $t \\in T\\subseteq\\mathbb{R}$. Further, assume that  $A(t)$ is diagonalizable for all values of $t$, i.e., there exist two $n\\times n$ parametric matrices $X(t)$, $Y(t)$, and a diagonal  $n\\times n$ matrix $\\Lambda(t)$ such that for all $t\\in T$\n\\begin{eqnarray}\n A(t) X(t) &=& X(t)\\Lambda(t)\\label{eq:right}\\\\\n Y(t) A(t) &=& \\Lambda(t)Y(t)\\label{eq:left}.\n\\end{eqnarray}\n\nWithout lack of generality, assume\n\\begin{eqnarray}\nY(t)X(t) &=& I\\label{eq:inv}\\\\\n{\\operatorname{diag}}\\big(X(t)^\\dagger X(t)\\big) &=& \\mathbf{1}\\label{eq:rvecnorm}.\n\n\\end{eqnarray}\n\nWe want to reconstruct $X(t)$, $Y(t)$, and $\\Lambda(t)$ to the first order: \n\\begin{eqnarray}\nX(t) &\\approx& X + t X^\\prime\\\\\nY(t) &\\approx& Y + t Y^\\prime\\\\\n\\Lambda(t) &\\approx&  \\Lambda + t \\Lambda^\\prime,\n\\end{eqnarray}\nwhere $X$, $X^\\prime$, $Y$, $Y^\\prime$, $\\Lambda$, and $\\Lambda^\\prime$ are computed at time $t=0$.\n\nTo this end, since $A(t)$ is diagonalizable, $X$ and $Y$ have full rank, so we can write\n\\begin{eqnarray}\nX^\\prime = X B\\\\\nY^\\prime = C Y,\n\\end{eqnarray}\nfor some matrices $B$ and $C$.\n\n\\subsection{Distinct Eigenvalues}\n\nIn the case where all the eigenvalues are distinct one can compute the eigenvalue and eigenvector derivatives directly. Differentiating (\\ref{eq:right}), we have\n\n", "itemtype": "equation", "pos": 8622, "prevtext": "\nwhere $p$ is the rate per unit time with which we add decoherence to the walk, and $\\mathcal{P}\\rho(t) = \\sum_j P_j \\rho(t) P_j^{\\dag}$ represents the effect of noise on $\\rho(t)$, where $\\lbrace P_j \\rbrace$ is a set of projectors. Intuitively, the effect of the extra terms is to reduce the off-diagonal elements of $\\rho(t)$, i.e., the coherence terms, at a rate $p$ per unit time~\\cite{kendon2007decoherence}, while leaving the diagonal elements unaffected. More specifically, let $\\operatorname{vec}\\big(\\rho(t)\\big)$ be the vectorization of the density matrix $\\rho(t)$. Then we can write\n\n", "index": 13, "text": "\\begin{equation}\\label{decoherence_eq}\n\\frac{\\partial}{\\partial t}\\operatorname{vec}\\big(\\rho(t)\\big) = \\left[-i \\left(L\\otimes I + I\\otimes-L\\right) + p\\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right)\\right] \\operatorname{vec}\\big(\\rho(t)\\big),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial}{\\partial t}\\operatorname{vec}\\big{(}\\rho(t)\\big{)}=\\left[-i%&#10;\\left(L\\otimes I+I\\otimes-L\\right)+p\\left(\\sum_{v\\in V}E_{vv}\\otimes E_{vv}-I%&#10;\\otimes I\\right)\\right]\\operatorname{vec}\\big{(}\\rho(t)\\big{)},\" display=\"block\"><mrow><mfrac><mo>\u2202</mo><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>t</mi></mrow></mfrac><mo>vec</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>=</mo><mrow><mo>[</mo><mo>-</mo><mi>i</mi><mrow><mo>(</mo><mi>L</mi><mo>\u2297</mo><mi>I</mi><mo>+</mo><mi>I</mi><mo>\u2297</mo><mo>-</mo><mi>L</mi><mo>)</mo></mrow><mo>+</mo><mi>p</mi><mrow><mo>(</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mi>V</mi></mrow></munder><msub><mi>E</mi><mrow><mi>v</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>\u2297</mo><msub><mi>E</mi><mrow><mi>v</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>-</mo><mi>I</mi><mo>\u2297</mo><mi>I</mi><mo>)</mo></mrow><mo>]</mo></mrow><mo>vec</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nLeft-multiplying both sides by $Y$ and recalling that $YAX=\\Lambda$, we have\n\n", "itemtype": "equation", "pos": 10379, "prevtext": "\nwhere $E_{vv}$ is the matrix which is 1 in $(v,v)$ and 0 elsewhere, i.e., the projector on the node $v$.\n\n\\section{Eigenvalue Perturbation}\\label{perturbation}\n\nLet $A(t)$ be a $n\\times n$ complex matrix parametrized by $t \\in T\\subseteq\\mathbb{R}$. Further, assume that  $A(t)$ is diagonalizable for all values of $t$, i.e., there exist two $n\\times n$ parametric matrices $X(t)$, $Y(t)$, and a diagonal  $n\\times n$ matrix $\\Lambda(t)$ such that for all $t\\in T$\n\\begin{eqnarray}\n A(t) X(t) &=& X(t)\\Lambda(t)\\label{eq:right}\\\\\n Y(t) A(t) &=& \\Lambda(t)Y(t)\\label{eq:left}.\n\\end{eqnarray}\n\nWithout lack of generality, assume\n\\begin{eqnarray}\nY(t)X(t) &=& I\\label{eq:inv}\\\\\n{\\operatorname{diag}}\\big(X(t)^\\dagger X(t)\\big) &=& \\mathbf{1}\\label{eq:rvecnorm}.\n\n\\end{eqnarray}\n\nWe want to reconstruct $X(t)$, $Y(t)$, and $\\Lambda(t)$ to the first order: \n\\begin{eqnarray}\nX(t) &\\approx& X + t X^\\prime\\\\\nY(t) &\\approx& Y + t Y^\\prime\\\\\n\\Lambda(t) &\\approx&  \\Lambda + t \\Lambda^\\prime,\n\\end{eqnarray}\nwhere $X$, $X^\\prime$, $Y$, $Y^\\prime$, $\\Lambda$, and $\\Lambda^\\prime$ are computed at time $t=0$.\n\nTo this end, since $A(t)$ is diagonalizable, $X$ and $Y$ have full rank, so we can write\n\\begin{eqnarray}\nX^\\prime = X B\\\\\nY^\\prime = C Y,\n\\end{eqnarray}\nfor some matrices $B$ and $C$.\n\n\\subsection{Distinct Eigenvalues}\n\nIn the case where all the eigenvalues are distinct one can compute the eigenvalue and eigenvector derivatives directly. Differentiating (\\ref{eq:right}), we have\n\n", "index": 15, "text": "\\begin{equation}\\label{eq:diff}\nA^\\prime X + A X^\\prime = X^\\prime \\Lambda + X \\Lambda^\\prime.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"A^{\\prime}X+AX^{\\prime}=X^{\\prime}\\Lambda+X\\Lambda^{\\prime}.\" display=\"block\"><mrow><mrow><mrow><mrow><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>+</mo><mrow><mi>A</mi><mo>\u2062</mo><msup><mi>X</mi><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><mrow><msup><mi>X</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi></mrow><mo>+</mo><mrow><mi>X</mi><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u039b</mi><mo>\u2032</mo></msup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nfrom which\n\n", "itemtype": "equation", "pos": 10566, "prevtext": "\nLeft-multiplying both sides by $Y$ and recalling that $YAX=\\Lambda$, we have\n\n", "index": 17, "text": "\\begin{equation}\\label{eq:diff2}\nY A^\\prime X + \\Lambda B = B\\Lambda + \\Lambda^\\prime,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"YA^{\\prime}X+\\Lambda B=B\\Lambda+\\Lambda^{\\prime},\" display=\"block\"><mrow><mrow><mrow><mrow><mi>Y</mi><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u039b</mi><mo>\u2062</mo><mi>B</mi></mrow></mrow><mo>=</mo><mrow><mrow><mi>B</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi></mrow><mo>+</mo><msup><mi mathvariant=\"normal\">\u039b</mi><mo>\u2032</mo></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nfrom which we have the eigenvalue derivatives\n\n", "itemtype": "equation", "pos": 10679, "prevtext": "\nfrom which\n\n", "index": 19, "text": "\\begin{equation}\n{\\operatorname{diag}}(\\Lambda^\\prime) = {\\operatorname{diag}}(Y A^\\prime X) + {\\operatorname{diag}}(\\Lambda B - B\\Lambda) = {\\operatorname{diag}}(Y A^\\prime X),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{diag}}(\\Lambda^{\\prime})={\\operatorname{diag}}(YA^{\\prime}X)+{%&#10;\\operatorname{diag}}(\\Lambda B-B\\Lambda)={\\operatorname{diag}}(YA^{\\prime}X),\" display=\"block\"><mrow><mrow><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi mathvariant=\"normal\">\u039b</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Y</mi><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi mathvariant=\"normal\">\u039b</mi><mo>\u2062</mo><mi>B</mi></mrow><mo>-</mo><mrow><mi>B</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Y</mi><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $x_i$ and $y_i$ are respectively the right and left eigenvectors corresponding to eigenvalue $\\lambda_i$ of $A(0)$.\n\nFor the Eigenvectors, from (\\ref{eq:diff2}) we have, for $i\\neq j$\n\\begin{eqnarray}\n(Y A^\\prime X)_{ij} + (\\Lambda B)_{ij} &=& (B\\Lambda)_{ij} + (\\Lambda^\\prime)_{ij}\\\\\ny_i^\\dagger A^\\prime x_j + \\lambda_i b_{ij} &=& b_{ij}\\lambda_j + 0,\n\\end{eqnarray}\nfrom which\n\n", "itemtype": "equation", "pos": 10918, "prevtext": "\nfrom which we have the eigenvalue derivatives\n\n", "index": 21, "text": "\\begin{equation}\\label{eq:valdiff}\n \\lambda^\\prime_i = y_i^\\dagger A^\\prime x_i,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\lambda^{\\prime}_{i}=y_{i}^{\\dagger}A^{\\prime}x_{i},\" display=\"block\"><mrow><mrow><msubsup><mi>\u03bb</mi><mi>i</mi><mo>\u2032</mo></msubsup><mo>=</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>x</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\n\nDifferentiating (\\ref{eq:rvecnorm}), we obtain\n\\begin{eqnarray}\n \\mathbf{0} &=& {\\operatorname{diag}}\\big(X(t)^\\dagger X(t)\\big)^\\prime = {\\operatorname{diag}}\\big((X^\\prime)^\\dagger X + X^\\dagger X^\\prime\\big) \\\\\\nonumber\n &=& {\\operatorname{diag}}(B^\\dagger X^\\dagger X +  X^\\dagger X B) = 2 \\operatorname{Re}\\big({\\operatorname{diag}}( X^\\dagger X B ) \\big)\n\\end{eqnarray}\nso, for all $i$, we have\n\n", "itemtype": "equation", "pos": 11401, "prevtext": "\nwhere $x_i$ and $y_i$ are respectively the right and left eigenvectors corresponding to eigenvalue $\\lambda_i$ of $A(0)$.\n\nFor the Eigenvectors, from (\\ref{eq:diff2}) we have, for $i\\neq j$\n\\begin{eqnarray}\n(Y A^\\prime X)_{ij} + (\\Lambda B)_{ij} &=& (B\\Lambda)_{ij} + (\\Lambda^\\prime)_{ij}\\\\\ny_i^\\dagger A^\\prime x_j + \\lambda_i b_{ij} &=& b_{ij}\\lambda_j + 0,\n\\end{eqnarray}\nfrom which\n\n", "index": 23, "text": "\\begin{equation}\\label{eq:mix}\n b_{ij} = \\frac{y_i^\\dagger A^\\prime x_j}{\\lambda_j-\\lambda_i}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"b_{ij}=\\frac{y_{i}^{\\dagger}A^{\\prime}x_{j}}{\\lambda_{j}-\\lambda_{i}}.\" display=\"block\"><mrow><mrow><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msubsup><mi>y</mi><mi>i</mi><mo>\u2020</mo></msubsup><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mrow><msub><mi>\u03bb</mi><mi>j</mi></msub><mo>-</mo><msub><mi>\u03bb</mi><mi>i</mi></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nor, extracting the term for $k=i$, and recalling that $(X^\\dagger X)_{ii}$=1\n\n", "itemtype": "equation", "pos": 11913, "prevtext": "\n\nDifferentiating (\\ref{eq:rvecnorm}), we obtain\n\\begin{eqnarray}\n \\mathbf{0} &=& {\\operatorname{diag}}\\big(X(t)^\\dagger X(t)\\big)^\\prime = {\\operatorname{diag}}\\big((X^\\prime)^\\dagger X + X^\\dagger X^\\prime\\big) \\\\\\nonumber\n &=& {\\operatorname{diag}}(B^\\dagger X^\\dagger X +  X^\\dagger X B) = 2 \\operatorname{Re}\\big({\\operatorname{diag}}( X^\\dagger X B ) \\big)\n\\end{eqnarray}\nso, for all $i$, we have\n\n", "index": 25, "text": "\\begin{equation}\n \\sum_k \\operatorname{Re}\\big((X^\\dagger X)_{ik} b_{ki}\\big) = 0\\,,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\sum_{k}\\operatorname{Re}\\big{(}(X^{\\dagger}X)_{ik}b_{ki}\\big{)}=0\\,,\" display=\"block\"><mrow><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>k</mi></munder><mrow><mo>Re</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>b</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow><mo>=</mo><mpadded width=\"+1.7pt\"><mn>0</mn></mpadded></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nAs for the imaginary part of $b_{ii}$, recall that even after the normalization constraint (\\ref{eq:rvecnorm}) there is still a degree of freedom in the choice of the global phase of the eigenvectors $x_i$ which is reflected in an arbitrariness in the choice of $\\operatorname{Im}(b_{ii})$. Here we set $\\operatorname{Im}(b_{ii})=0.$\n\nNote, also, that differentiating (\\ref{eq:inv}) we obtain\n\n", "itemtype": "equation", "pos": 12090, "prevtext": "\nor, extracting the term for $k=i$, and recalling that $(X^\\dagger X)_{ii}$=1\n\n", "index": 27, "text": "\\begin{equation}\n \\operatorname{Re}(b_{ii})= -\\sum_{k\\neq i} \\operatorname{Re}\\big( (X^\\dagger X)_{ik} b_{ki}\\big) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\operatorname{Re}(b_{ii})=-\\sum_{k\\neq i}\\operatorname{Re}\\big{(}(X^{\\dagger}X%&#10;)_{ik}b_{ki}\\big{)}\" display=\"block\"><mrow><mrow><mo>Re</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>i</mi></mrow></munder><mrow><mo>Re</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>X</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>b</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nfrom which we obtain\n\n", "itemtype": "equation", "pos": 12614, "prevtext": "\nAs for the imaginary part of $b_{ii}$, recall that even after the normalization constraint (\\ref{eq:rvecnorm}) there is still a degree of freedom in the choice of the global phase of the eigenvectors $x_i$ which is reflected in an arbitrariness in the choice of $\\operatorname{Im}(b_{ii})$. Here we set $\\operatorname{Im}(b_{ii})=0.$\n\nNote, also, that differentiating (\\ref{eq:inv}) we obtain\n\n", "index": 29, "text": "\\begin{equation}\n \\mathbf{0} = \\big(Y(t)X(t)\\big)^\\prime = Y^\\prime X + Y X^\\prime = CYX + YXB = C+B,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{0}=\\big{(}Y(t)X(t)\\big{)}^{\\prime}=Y^{\\prime}X+YX^{\\prime}=CYX+YXB=C+B,\" display=\"block\"><mrow><mrow><mn/><mo>=</mo><msup><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>Y</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u2032</mo></msup><mo>=</mo><mrow><mrow><msup><mi>Y</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mi>X</mi></mrow><mo>+</mo><mrow><mi>Y</mi><mo>\u2062</mo><msup><mi>X</mi><mo>\u2032</mo></msup></mrow></mrow><mo>=</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>Y</mi><mo>\u2062</mo><mi>X</mi></mrow><mo>+</mo><mrow><mi>Y</mi><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi>B</mi></mrow></mrow><mo>=</mo><mrow><mi>C</mi><mo>+</mo><mi>B</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\n\n\\subsection{Repeated Eigenvalues}\n\nIn the case of repeated eigenvalues we have an additional degree of freedom from the choice of the eigenbasis. Any linear combination of eigenvectors corresponding to the same eigenvalue is still an eigenvector of the matrix, thus the observed eigenvectors $x_i$ can indeed be linear combinations of the limiting eigenvectors of $A(t)$ as $t \\rightarrow 0$, resulting in a discontinuity. This can be solved by assuming that there is an unknown eigenvector basis $X$ that is continuous in $t$ and expressing it in terms of the observed eigenvector matrix $\\hat{X}$:\n\n", "itemtype": "equation", "pos": 12752, "prevtext": "\nfrom which we obtain\n\n", "index": 31, "text": "\\begin{equation}\nC=-B.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"C=-B.\" display=\"block\"><mrow><mrow><mi>C</mi><mo>=</mo><mrow><mo>-</mo><mi>B</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\n\nSubstituting into (\\ref{eq:right}) and left-multiplying by $\\hat{Y}=\\hat{X}^{-1}$ we have\n\\begin{eqnarray}\n \\hat{Y} A \\hat{X}\\Gamma &=& \\hat{X}\\Gamma\\Lambda\\\\\n \\Lambda \\Gamma &=& \\Gamma \\Lambda,\n\\end{eqnarray}\nHence, $\\Gamma$ is co-diagonalizable with $\\Lambda$. Recall that $\\Lambda$ is diagonal, thus $\\Gamma$ must be block diagonal with the blocks corresponding to the repeated values of $\\Lambda$.\n\nLet $\\bar{\\lambda}$ be one such repeated eigenvalue, repeated with multiplicity $r$. We can partition the eigenvalue/eigenvector matrices as follows:\n\\begin{eqnarray}\n&\\Lambda = \\left(\\begin{array}{c c}\\Lambda_1&\\mathbf{0}\\\\\\mathbf{0}&\\bar{\\lambda}I\\end{array}\\right) \\qquad\n\\Gamma = \\left(\\begin{array}{c c}\\Gamma_1&\\mathbf{0}\\\\\\mathbf{0}&\\Gamma_2\\end{array}\\right) \\qquad\nB = \\left(\\begin{array}{c c}B_{11}&B_{12}\\\\B_{21}&B_{22}\\end{array}\\right)&\\\\\n&\\hat{X} = \\left(X_1\\; X_2\\right) \\qquad\n\\hat{Y} = \\left(\\begin{array}{c}Y_1\\\\Y_2\\end{array}\\right) \\qquad&.\n\\end{eqnarray}\n\nFrom (\\ref{eq:diff2}) we obtain\n\n", "itemtype": "equation", "pos": 13391, "prevtext": "\n\n\\subsection{Repeated Eigenvalues}\n\nIn the case of repeated eigenvalues we have an additional degree of freedom from the choice of the eigenbasis. Any linear combination of eigenvectors corresponding to the same eigenvalue is still an eigenvector of the matrix, thus the observed eigenvectors $x_i$ can indeed be linear combinations of the limiting eigenvectors of $A(t)$ as $t \\rightarrow 0$, resulting in a discontinuity. This can be solved by assuming that there is an unknown eigenvector basis $X$ that is continuous in $t$ and expressing it in terms of the observed eigenvector matrix $\\hat{X}$:\n\n", "index": 33, "text": "\\begin{equation}\nX = \\hat{X}\\Gamma.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"X=\\hat{X}\\Gamma.\" display=\"block\"><mrow><mrow><mi>X</mi><mo>=</mo><mrow><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">^</mo></mover><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\n\nHence, the block-diagonal element $\\Gamma_2$ can be obtained by solving the eigenvalue problem\n\n", "itemtype": "equation", "pos": 14455, "prevtext": "\n\nSubstituting into (\\ref{eq:right}) and left-multiplying by $\\hat{Y}=\\hat{X}^{-1}$ we have\n\\begin{eqnarray}\n \\hat{Y} A \\hat{X}\\Gamma &=& \\hat{X}\\Gamma\\Lambda\\\\\n \\Lambda \\Gamma &=& \\Gamma \\Lambda,\n\\end{eqnarray}\nHence, $\\Gamma$ is co-diagonalizable with $\\Lambda$. Recall that $\\Lambda$ is diagonal, thus $\\Gamma$ must be block diagonal with the blocks corresponding to the repeated values of $\\Lambda$.\n\nLet $\\bar{\\lambda}$ be one such repeated eigenvalue, repeated with multiplicity $r$. We can partition the eigenvalue/eigenvector matrices as follows:\n\\begin{eqnarray}\n&\\Lambda = \\left(\\begin{array}{c c}\\Lambda_1&\\mathbf{0}\\\\\\mathbf{0}&\\bar{\\lambda}I\\end{array}\\right) \\qquad\n\\Gamma = \\left(\\begin{array}{c c}\\Gamma_1&\\mathbf{0}\\\\\\mathbf{0}&\\Gamma_2\\end{array}\\right) \\qquad\nB = \\left(\\begin{array}{c c}B_{11}&B_{12}\\\\B_{21}&B_{22}\\end{array}\\right)&\\\\\n&\\hat{X} = \\left(X_1\\; X_2\\right) \\qquad\n\\hat{Y} = \\left(\\begin{array}{c}Y_1\\\\Y_2\\end{array}\\right) \\qquad&.\n\\end{eqnarray}\n\nFrom (\\ref{eq:diff2}) we obtain\n\n", "index": 35, "text": "\\begin{equation}\n\\left(\\begin{array}{c c}\nY_1 A^\\prime X_1\\Gamma_1 & Y_1 A^\\prime X_2\\Gamma_2\\\\\nY_2 A^\\prime X_1\\Gamma_1 & Y_2 A^\\prime X_2\\Gamma_2\n\\end{array}\\right) \n=\n\\left(\\begin{array}{c c}\n\\Gamma_1 (B_{11} \\Lambda_1 - \\Lambda_1 B_{11} - \\Lambda_1^\\prime) &\n\\Gamma_1 (\\bar{\\lambda}I-\\Lambda_1)B_{12} \\\\\n-\\Gamma_2 B_{21} (\\bar{\\lambda}I-\\Lambda_1) &\n\\Gamma_2 \\Lambda_2^\\prime\n\\end{array}\\right).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\left(\\begin{array}[]{c c}Y_{1}A^{\\prime}X_{1}\\Gamma_{1}&amp;Y_{1}A^{\\prime}X_{2}%&#10;\\Gamma_{2}\\\\&#10;Y_{2}A^{\\prime}X_{1}\\Gamma_{1}&amp;Y_{2}A^{\\prime}X_{2}\\Gamma_{2}\\end{array}\\right%&#10;)=\\left(\\begin{array}[]{c c}\\Gamma_{1}(B_{11}\\Lambda_{1}-\\Lambda_{1}B_{11}-%&#10;\\Lambda_{1}^{\\prime})&amp;\\Gamma_{1}(\\bar{\\lambda}I-\\Lambda_{1})B_{12}\\\\&#10;-\\Gamma_{2}B_{21}(\\bar{\\lambda}I-\\Lambda_{1})&amp;\\Gamma_{2}\\Lambda_{2}^{\\prime}%&#10;\\end{array}\\right).\" display=\"block\"><mrow><mrow><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi>Y</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>Y</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow></mtd></mtr></mtable><mo>)</mo></mrow><mo>=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>B</mi><mn>11</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>11</mn></msub></mrow><mo>-</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn><mo>\u2032</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>I</mi></mrow><mo>-</mo><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>B</mi><mn>12</mn></msub></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mo>-</mo><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>21</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><mi>I</mi></mrow><mo>-</mo><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup></mrow></mtd></mtr></mtable><mo>)</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere the derivatives $\\Lambda_2^\\prime$ of the repeated eigenvalues $\\bar{\\lambda}$ are the eigenvalues of $Y_2 A^\\prime X_2\\Gamma_2$. If these eigenvalues are distinct the matrix $\\Gamma_2$ is unique up to a multiplicative factor.\n\nIf we assume that both the observed and the continuous eigenvectors are normalized, i.e.,\n\n", "itemtype": "equation", "pos": 14966, "prevtext": "\n\nHence, the block-diagonal element $\\Gamma_2$ can be obtained by solving the eigenvalue problem\n\n", "index": 37, "text": "\\begin{equation}\\label{eq:eig2}\n Y_2 A^\\prime X_2\\Gamma_2 = \\Gamma_2 \\Lambda_2^\\prime,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"Y_{2}A^{\\prime}X_{2}\\Gamma_{2}=\\Gamma_{2}\\Lambda_{2}^{\\prime},\" display=\"block\"><mrow><mrow><mrow><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nit is clear that on non-repeated eigenvalues, the corresponding diagonal element of $\\Gamma$ must have norm 1. As usual the phase remains arbitrary, but we can pick $\\gamma_{ii}=1$ without loss of generality.\n\nThe equation\n\n", "itemtype": "equation", "pos": 15392, "prevtext": "\nwhere the derivatives $\\Lambda_2^\\prime$ of the repeated eigenvalues $\\bar{\\lambda}$ are the eigenvalues of $Y_2 A^\\prime X_2\\Gamma_2$. If these eigenvalues are distinct the matrix $\\Gamma_2$ is unique up to a multiplicative factor.\n\nIf we assume that both the observed and the continuous eigenvectors are normalized, i.e.,\n\n", "index": 39, "text": "\\begin{equation}\n {\\operatorname{diag}}\\big(X(t)^\\dagger X(t)\\big) = \\mathbf{1} \\qquad {\\operatorname{diag}}\\big(\\Gamma^\\dagger X(t)^\\dagger X(t)\\Gamma\\big) = \\mathbf{1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{diag}}\\big{(}X(t)^{\\dagger}X(t)\\big{)}=\\mathbf{1}\\qquad{%&#10;\\operatorname{diag}}\\big{(}\\Gamma^{\\dagger}X(t)^{\\dagger}X(t)\\Gamma\\big{)}=%&#10;\\mathbf{1},\" display=\"block\"><mrow><mrow><mrow><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mi>X</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mn>\ud835\udfcf</mn></mrow><mo mathvariant=\"italic\" separator=\"true\">\u2003\u2003</mo><mrow><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mn>\ud835\udfcf</mn></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nis equivalent to (\\ref{eq:diff2}) on the reduced eigenvalue set and can be recursively partitioned if $\\Lambda_1$ still contains repeated eigenvalues and solved as in the case of non-repeated eigenvalues. On the other hand, the values of $B_{12}$ and $B_{21}$ can be computed from the following equations\n\\begin{eqnarray}\n Y_1 A^\\prime X_2\\Gamma_2 &=& \\Gamma_1 (\\bar{\\lambda}I-\\Lambda_1)B_{12}\\label{eq38} \\\\\n Y_2 A^\\prime X_1\\Gamma_1 &=& -\\Gamma_2 B_{21} (\\bar{\\lambda}I-\\Lambda_1)\\label{eq39}.\n\\end{eqnarray}\n\nTo compute $B_{22}$, we differentiate (\\ref{eq:diff}) one more time, setting $X^{\\prime\\prime}=XD$, we left-multiply by $Y$ and we concentrate on the sub-matrix corresponding to the repeated eigenvalues:\n\n", "itemtype": "equation", "pos": 15801, "prevtext": "\nit is clear that on non-repeated eigenvalues, the corresponding diagonal element of $\\Gamma$ must have norm 1. As usual the phase remains arbitrary, but we can pick $\\gamma_{ii}=1$ without loss of generality.\n\nThe equation\n\n", "index": 41, "text": "\\begin{equation}\n Y_1 A^\\prime X_1\\Gamma_1  = \\Gamma_1 (B_{11} \\Lambda_1 - \\Lambda_1 B_{11} - \\Lambda_1^\\prime)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"Y_{1}A^{\\prime}X_{1}\\Gamma_{1}=\\Gamma_{1}(B_{11}\\Lambda_{1}-\\Lambda_{1}B_{11}-%&#10;\\Lambda_{1}^{\\prime})\" display=\"block\"><mrow><mrow><msub><mi>Y</mi><mn>1</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi>X</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub></mrow><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>B</mi><mn>11</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub></mrow><mo>-</mo><mrow><msub><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>11</mn></msub></mrow><mo>-</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>1</mn><mo>\u2032</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nRecalling that $\\Gamma_2$ is a solution to the eigenvalue problem (\\ref{eq:eig2}), we have\n\n", "itemtype": "equation", "pos": 16644, "prevtext": "\nis equivalent to (\\ref{eq:diff2}) on the reduced eigenvalue set and can be recursively partitioned if $\\Lambda_1$ still contains repeated eigenvalues and solved as in the case of non-repeated eigenvalues. On the other hand, the values of $B_{12}$ and $B_{21}$ can be computed from the following equations\n\\begin{eqnarray}\n Y_1 A^\\prime X_2\\Gamma_2 &=& \\Gamma_1 (\\bar{\\lambda}I-\\Lambda_1)B_{12}\\label{eq38} \\\\\n Y_2 A^\\prime X_1\\Gamma_1 &=& -\\Gamma_2 B_{21} (\\bar{\\lambda}I-\\Lambda_1)\\label{eq39}.\n\\end{eqnarray}\n\nTo compute $B_{22}$, we differentiate (\\ref{eq:diff}) one more time, setting $X^{\\prime\\prime}=XD$, we left-multiply by $Y$ and we concentrate on the sub-matrix corresponding to the repeated eigenvalues:\n\n", "index": 43, "text": "\\begin{equation}\n Y_2 A^{\\prime\\prime} X_2 \\Gamma_2 + 2 Y_2 A^\\prime \\Gamma_2 B_{22} + \\bar{\\lambda}\\Gamma_2 D = \\Gamma_2 D \\bar{\\lambda} + 2 \\Gamma_2 B_{22} \\Lambda_2^\\prime + \\gamma_2\\Lambda_2^{\\prime\\prime}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"Y_{2}A^{\\prime\\prime}X_{2}\\Gamma_{2}+2Y_{2}A^{\\prime}\\Gamma_{2}B_{22}+\\bar{%&#10;\\lambda}\\Gamma_{2}D=\\Gamma_{2}D\\bar{\\lambda}+2\\Gamma_{2}B_{22}\\Lambda_{2}^{%&#10;\\prime}+\\gamma_{2}\\Lambda_{2}^{\\prime\\prime}.\" display=\"block\"><mrow><mrow><mrow><mrow><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mo>\u2032</mo></msup><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>22</mn></msub></mrow><mo>+</mo><mrow><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><mi>D</mi><mo>\u2062</mo><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>22</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mi>\u2032\u2032</mi></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nor\n\n", "itemtype": "equation", "pos": 16961, "prevtext": "\nRecalling that $\\Gamma_2$ is a solution to the eigenvalue problem (\\ref{eq:eig2}), we have\n\n", "index": 45, "text": "\\begin{equation}\n  Y_2 A^{\\prime\\prime} X_2 \\Gamma_2 + 2 \\Gamma_2 \\Lambda_2^\\prime B_{22} =  2 \\Gamma_2 B_{22} \\Lambda_2^\\prime + \\gamma_2\\Lambda_2^{\\prime\\prime}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"Y_{2}A^{\\prime\\prime}X_{2}\\Gamma_{2}+2\\Gamma_{2}\\Lambda_{2}^{\\prime}B_{22}=2%&#10;\\Gamma_{2}B_{22}\\Lambda_{2}^{\\prime}+\\gamma_{2}\\Lambda_{2}^{\\prime\\prime}\" display=\"block\"><mrow><mrow><mrow><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>B</mi><mn>22</mn></msub></mrow></mrow><mo>=</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>B</mi><mn>22</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mi>\u2032\u2032</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nfrom which we can extract the off-diagonal elements of $B_{22}$:\n\n", "itemtype": "equation", "pos": 17142, "prevtext": "\nor\n\n", "index": 47, "text": "\\begin{equation}\n 2(B_{22} \\Lambda_2^\\prime - \\Lambda_2^\\prime B_{22} ) = \\Gamma_2^{-1} Y_2 A^{\\prime\\prime} X_2 \\Gamma_2  - \\Lambda_2^{\\prime\\prime},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"2(B_{22}\\Lambda_{2}^{\\prime}-\\Lambda_{2}^{\\prime}B_{22})=\\Gamma_{2}^{-1}Y_{2}A%&#10;^{\\prime\\prime}X_{2}\\Gamma_{2}-\\Lambda_{2}^{\\prime\\prime},\" display=\"block\"><mrow><mrow><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>B</mi><mn>22</mn></msub><mo>\u2062</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup></mrow><mo>-</mo><mrow><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mo>\u2032</mo></msubsup><mo>\u2062</mo><msub><mi>B</mi><mn>22</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow><mo>-</mo><msubsup><mi mathvariant=\"normal\">\u039b</mi><mn>2</mn><mi>\u2032\u2032</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nNote that in the special case where the matrix $A(t)$ is a linear function of $t$, {\\em i.e.},  $A(t) = A + t A^\\prime$, then $A^{\\prime\\prime}=\\mathbf{0}$ and thus $(B_{22})_{ij} = 0$.\n\n\nAs for the non-repeated eigenvalue case, the diagonal of $B_{22}$ is computed from the constraint \n\n", "itemtype": "equation", "pos": 17373, "prevtext": "\nfrom which we can extract the off-diagonal elements of $B_{22}$:\n\n", "index": 49, "text": "\\begin{equation}\\label{eq43}\n (B_{22})_{ij} = \\frac{\\Gamma_2^{-1} Y_2 A^{\\prime\\prime} X_2 \\Gamma_2}{2(\\bar{\\lambda}_j^\\prime-\\bar{\\lambda}_i^\\prime)}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"(B_{22})_{ij}=\\frac{\\Gamma_{2}^{-1}Y_{2}A^{\\prime\\prime}X_{2}\\Gamma_{2}}{2(%&#10;\\bar{\\lambda}_{j}^{\\prime}-\\bar{\\lambda}_{i}^{\\prime})}.\" display=\"block\"><mrow><mrow><msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>B</mi><mn>22</mn></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msubsup><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>A</mi><mi>\u2032\u2032</mi></msup><mo>\u2062</mo><msub><mi>X</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi mathvariant=\"normal\">\u0393</mi><mn>2</mn></msub></mrow><mrow><mn>2</mn><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>j</mi><mo>\u2032</mo></msubsup><mo>-</mo><msubsup><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">\u00af</mo></mover><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nresulting in \n\n", "itemtype": "equation", "pos": 17827, "prevtext": "\nNote that in the special case where the matrix $A(t)$ is a linear function of $t$, {\\em i.e.},  $A(t) = A + t A^\\prime$, then $A^{\\prime\\prime}=\\mathbf{0}$ and thus $(B_{22})_{ij} = 0$.\n\n\nAs for the non-repeated eigenvalue case, the diagonal of $B_{22}$ is computed from the constraint \n\n", "index": 51, "text": "\\begin{equation}\n {\\operatorname{diag}}\\big(\\Gamma^\\dagger X(t)^\\dagger X(t)\\Gamma\\big) = \\mathbf{1},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{diag}}\\big{(}\\Gamma^{\\dagger}X(t)^{\\dagger}X(t)\\Gamma\\big{)}=%&#10;\\mathbf{1},\" display=\"block\"><mrow><mrow><mrow><mo>diag</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow><mo>=</mo><mn>\ud835\udfcf</mn></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nand, without loss of generality $\\operatorname{Im}(b_{ii})=0.$\n\n\\subsection{Hermitian Matrices}\nIf $A$ is Hermitian, then $X$ is unitary and $Y=X^\\dagger$. With this in mind, in the distinct eigenvalue case, we have\n\\begin{eqnarray}\n \\lambda_i^\\prime &=& x_i^\\dagger A^\\prime x_i\\\\\n b_{ij} &=& \\frac{x_i^\\dagger A^\\prime x_j}{\\lambda_j-\\lambda_i}\\\\\n b_{ii} &=& 0.\n\\end{eqnarray}\nThus, if also $A^\\prime$ is Hermitian, $B$ is skew-symmetric.\n\n\n\\section{Application to Quantum Walks with Decoherence}\\label{decoherence}\n\nRecall that the evolution of a quantum walk with decoherence expressed in terms of the density matrix $\\rho$ is given by Eq.~\\ref{decoherence_eq}. In order to compute the evolution of $\\rho(t)$, we analyze the behavior of the eigenvalues and eigenvectors of \n\n", "itemtype": "equation", "pos": 17958, "prevtext": "\nresulting in \n\n", "index": 53, "text": "\\begin{equation}\n \\operatorname{Re}(b_{ii})= -\\sum_{k\\neq i} \\operatorname{Re}\\big( (\\Gamma^\\dagger X^\\dagger X \\Gamma)_{ik} b_{ki}\\big) \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E27.m1\" class=\"ltx_Math\" alttext=\"\\operatorname{Re}(b_{ii})=-\\sum_{k\\neq i}\\operatorname{Re}\\big{(}(\\Gamma^{%&#10;\\dagger}X^{\\dagger}X\\Gamma)_{ik}b_{ki}\\big{)}\" display=\"block\"><mrow><mrow><mo>Re</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>\u2260</mo><mi>i</mi></mrow></munder><mrow><mo>Re</mo><mo>\u2061</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><msub><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi mathvariant=\"normal\">\u0393</mi><mo>\u2020</mo></msup><mo>\u2062</mo><msup><mi>X</mi><mo>\u2020</mo></msup><mo>\u2062</mo><mi>X</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msub><mi>b</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nas a function of the decoherence rate $p$.\n\nFor $p=0$ the eigenvalues of $A(p)$ are all of the form \n\n", "itemtype": "equation", "pos": 18889, "prevtext": "\nand, without loss of generality $\\operatorname{Im}(b_{ii})=0.$\n\n\\subsection{Hermitian Matrices}\nIf $A$ is Hermitian, then $X$ is unitary and $Y=X^\\dagger$. With this in mind, in the distinct eigenvalue case, we have\n\\begin{eqnarray}\n \\lambda_i^\\prime &=& x_i^\\dagger A^\\prime x_i\\\\\n b_{ij} &=& \\frac{x_i^\\dagger A^\\prime x_j}{\\lambda_j-\\lambda_i}\\\\\n b_{ii} &=& 0.\n\\end{eqnarray}\nThus, if also $A^\\prime$ is Hermitian, $B$ is skew-symmetric.\n\n\n\\section{Application to Quantum Walks with Decoherence}\\label{decoherence}\n\nRecall that the evolution of a quantum walk with decoherence expressed in terms of the density matrix $\\rho$ is given by Eq.~\\ref{decoherence_eq}. In order to compute the evolution of $\\rho(t)$, we analyze the behavior of the eigenvalues and eigenvectors of \n\n", "index": 55, "text": "\\begin{equation}\nA(p) = -i \\left(L\\otimes I + I\\otimes-L\\right) + p\\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) = iA+pA^\\prime\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E28.m1\" class=\"ltx_Math\" alttext=\"A(p)=-i\\left(L\\otimes I+I\\otimes-L\\right)+p\\left(\\sum_{v\\in V}E_{vv}\\otimes E_%&#10;{vv}-I\\otimes I\\right)=iA+pA^{\\prime}\" display=\"block\"><mrow><mi>A</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mo>-</mo><mi>i</mi><mrow><mo>(</mo><mi>L</mi><mo>\u2297</mo><mi>I</mi><mo>+</mo><mi>I</mi><mo>\u2297</mo><mo>-</mo><mi>L</mi><mo>)</mo></mrow><mo>+</mo><mi>p</mi><mrow><mo>(</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mi>V</mi></mrow></munder><msub><mi>E</mi><mrow><mi>v</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>\u2297</mo><msub><mi>E</mi><mrow><mi>v</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>-</mo><mi>I</mi><mo>\u2297</mo><mi>I</mi><mo>)</mo></mrow><mo>=</mo><mi>i</mi><mi>A</mi><mo>+</mo><mi>p</mi><msup><mi>A</mi><mo>\u2032</mo></msup></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $\\lambda_k$ and $\\lambda_j$ are eigenvalues of $L$. The corresponding eigenvectors are of the form \n\n", "itemtype": "equation", "pos": 19149, "prevtext": "\nas a function of the decoherence rate $p$.\n\nFor $p=0$ the eigenvalues of $A(p)$ are all of the form \n\n", "index": 57, "text": "\\begin{equation}\n\\pi_{jk} = i(\\lambda_k - \\lambda_j),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E29.m1\" class=\"ltx_Math\" alttext=\"\\pi_{jk}=i(\\lambda_{k}-\\lambda_{j}),\" display=\"block\"><mrow><mrow><msub><mi>\u03c0</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><mi>i</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\u03bb</mi><mi>k</mi></msub><mo>-</mo><msub><mi>\u03bb</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwhere $\\phi_j$ is an eigenvector of $L$ corresponding to $\\lambda_j$ and $\\phi_k$ is an eigenvector  corresponding to $\\lambda_k$.\n\nNote that there is at least one  repeated eigenvalue in $A(0)$, namely $0$ with multiplicity at least $n$. In fact, for all $i=1,\\ldots,n$ we have $\\pi_{jj}=i(\\lambda_j - \\lambda_j)=0$ which is an eigenvalue with eigenvector $\\xi_{jj}=\\phi_j\\otimes\\phi_j$. \nIn the following we make the simplifying assumption that this is the only case of repeated eigenvalue, namely that the eigenvalue gaps $\\lambda_j - \\lambda_k$  in $L$ are all unique for $j\\neq k$.\n\nUsing (\\ref{eq:valdiff}) we can compute the eigenvalue derivatives $\\pi_{jk}\\prime$ for $j\\neq k$:\n\\begin{eqnarray}\n\\pi_{jk}\\prime &=& \\xi_{jk}^T A^\\prime \\xi_{jk} = (\\phi_j\\otimes\\phi_k)^T \\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) (\\phi_j\\otimes\\phi_k) \\nonumber\\\\\n&=& \\sum_{v\\in V} (\\phi_j^T E_{vv}\\phi_j)\\otimes (\\phi_k^T E_{vv} \\phi_k) - 1 = -\\left(1-\\sum_{v\\in V} \\phi_{jv}^2 \\phi_{kv}^2 \\right),\n\\end{eqnarray}\nwhere the quantity \n\n", "itemtype": "equation", "pos": 19324, "prevtext": "\nwhere $\\lambda_k$ and $\\lambda_j$ are eigenvalues of $L$. The corresponding eigenvectors are of the form \n\n", "index": 59, "text": "\\begin{equation}\n\\xi_{jk} = \\phi_j\\otimes\\phi_k,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E30.m1\" class=\"ltx_Math\" alttext=\"\\xi_{jk}=\\phi_{j}\\otimes\\phi_{k},\" display=\"block\"><mrow><mrow><msub><mi>\u03be</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><msub><mi>\u03d5</mi><mi>j</mi></msub><mo>\u2297</mo><msub><mi>\u03d5</mi><mi>k</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nis the probability of co-observation of the standing waves $\\phi_j$ and $\\phi_k$. This means that the (real) decay of the mixed eigenvector $\\xi_{jk}$ introduced by the decoherence is proportional to the probability that the two components $\\phi_j$ and $\\phi_k$ are not observed on the same node.\n\nFor the eigenvector derivative, we compute the mixing proportion $b_{jk}^{lm}$ for $j\\neq k$, $l\\neq m$, and $(j,k)\\neq(l,m)$. Intuitively, this tells us how much of $\\xi_{jk}$ goes into $\\xi_{lm}^\\prime$\n\\begin{eqnarray}\\label{eq54}\n b_{jk}^{lm} &=& \\frac{\\xi_{jk}^T A^\\prime \\xi_{lm}}{\\pi_{lm}-\\pi_{jk}} = \\frac{(\\phi_j\\otimes\\phi_k)^T \\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) (\\phi_l\\otimes\\phi_m)}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)}\\nonumber\\\\\n &=& \\frac{\\sum_{v\\in V} (\\phi_j^T E_{vv}\\phi_l)\\otimes (\\phi_k^T E_{vv} \\phi_m)}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)} = \\frac{\\sum_{v\\in V} \\phi_{jv}\\phi_{lv}\\phi_{kv}\\phi_{mv}}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)}.\n\\end{eqnarray}\nHence, the mixing is proportional to the probability of co-observation of the standing waves $\\phi_j$, $\\phi_k$, $\\phi_l$, and $\\phi_m$.\n\n\nFor $j=k$ we have repeated eigenvalues, so we need to solve the following eigensystem:\n\n", "itemtype": "equation", "pos": 20433, "prevtext": "\nwhere $\\phi_j$ is an eigenvector of $L$ corresponding to $\\lambda_j$ and $\\phi_k$ is an eigenvector  corresponding to $\\lambda_k$.\n\nNote that there is at least one  repeated eigenvalue in $A(0)$, namely $0$ with multiplicity at least $n$. In fact, for all $i=1,\\ldots,n$ we have $\\pi_{jj}=i(\\lambda_j - \\lambda_j)=0$ which is an eigenvalue with eigenvector $\\xi_{jj}=\\phi_j\\otimes\\phi_j$. \nIn the following we make the simplifying assumption that this is the only case of repeated eigenvalue, namely that the eigenvalue gaps $\\lambda_j - \\lambda_k$  in $L$ are all unique for $j\\neq k$.\n\nUsing (\\ref{eq:valdiff}) we can compute the eigenvalue derivatives $\\pi_{jk}\\prime$ for $j\\neq k$:\n\\begin{eqnarray}\n\\pi_{jk}\\prime &=& \\xi_{jk}^T A^\\prime \\xi_{jk} = (\\phi_j\\otimes\\phi_k)^T \\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) (\\phi_j\\otimes\\phi_k) \\nonumber\\\\\n&=& \\sum_{v\\in V} (\\phi_j^T E_{vv}\\phi_j)\\otimes (\\phi_k^T E_{vv} \\phi_k) - 1 = -\\left(1-\\sum_{v\\in V} \\phi_{jv}^2 \\phi_{kv}^2 \\right),\n\\end{eqnarray}\nwhere the quantity \n\n", "index": 61, "text": "\\begin{equation}\no_{jk}=\\sum_{v\\in V} \\phi_{jv}^2 \\phi_{kv}^2\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E31.m1\" class=\"ltx_Math\" alttext=\"o_{jk}=\\sum_{v\\in V}\\phi_{jv}^{2}\\phi_{kv}^{2}\" display=\"block\"><mrow><msub><mi>o</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>v</mi><mo>\u2208</mo><mi>V</mi></mrow></munder><mrow><msubsup><mi>\u03d5</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>v</mi></mrow><mn>2</mn></msubsup><mo>\u2062</mo><msubsup><mi>\u03d5</mi><mrow><mi>k</mi><mo>\u2062</mo><mi>v</mi></mrow><mn>2</mn></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04691.tex", "nexttext": "\nwith $\\Xi=(\\xi_{jk})$\n\\begin{eqnarray}\n\\xi_{jk} =  \\xi_{jj} A^\\prime \\xi_{kk} &=& (\\phi_j\\otimes\\phi_j)^T \\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) (\\phi_k\\otimes\\phi_k) \\nonumber\\\\\n&=& \\sum_{v\\in V} \\phi_{jv}^2\\phi_{kv}^2 - \\delta_{jk},\n\\end{eqnarray}\nthus we have that $\\Xi = O - I$ where $O$ is the matrix of co-observations of the standing waves.\nIt is easy to show that $O$ is doubly stochastic, in fact\n\\begin{eqnarray}\n \\sum_j o_{jk} &=&   \\sum_j \\sum_{v\\in V} \\phi_{jv}^2\\phi_{kv}^2 = \\sum_{v\\in V} \\left( \\sum_j \\phi_{jv}^2\\right) \\phi_{kv}^2 = \\sum_{v\\in V}  \\phi_{kv}^2 =1\\\\\n  \\sum_k o_{jk} &=&  \\sum_k \\sum_{v\\in V} \\phi_{jv}^2\\phi_{kv}^2 = \\sum_{v\\in V}  \\phi_{jv}^2 \\left(\\sum_k \\phi_{kv}^2\\right) = \\sum_{v\\in V}  \\phi_{jv}^2 =1,\n\\end{eqnarray}\nthus, $\\Xi$ has all real negative eigenvalue with the exception of at least one zero eigenvalue corresponding to the steady state of $O$.\n\n\\subsection{Computational Complexity}\nWe conclude this technical report with some remarks on the computational complexity of the proposed approach. To this end, note that we first need to compute the eigendecomposition of the Laplacian matrix $L$, which has complexity $O(n^3)$, where $n$ is the number of nodes of the graph. Similarly, solving the eigensystem of Eq.~\\ref{eq55} has complexity $O(n^3)$, where $\\Xi$ is a real-valued symmetric matrix and $\\Gamma$ is orthogonal.\n\nThe computation of $B_{12}$ in Eq.~\\ref{eq38} requires inverting $\\Gamma_1$, which in our case is the identity matrix, and a diagonal matrix, i.e., $(\\bar{\\lambda}I-\\Lambda_1)$. Similarly, solving Eq.~\\ref{eq39} for $B_{21}$ requires inverting $(\\bar{\\lambda}I-\\Lambda_1)$ and $\\Gamma_2$. Since $\\Gamma$ is orthogonal and block-diagonal, we conclude that $\\Gamma_2$ is an orthogonal matrix. In general, note that $B$ is an $n^2 \\times n^2$ matrix and therefore the complexity of constructing it is at least $O(n^4)$. In particular, from Eq.~\\ref{eq54} it follows that the complexity of computing the $n^4$ elements of $B$ is $O(n^5)$. We should stress, however, that the computation of the $b_{jk}^{lm}$ can be easily parallelized. \n\nAs a result, we conclude that the complexity of the proposed approach is dominated by the $O(n^5)$ computation of the matrix $B$. This should be contrasted with the cost of directly computing the eigendecomposition of the $n^2 \\times n^2$ super-operator $A(p)$, which is $O(n^6)$. Finally, note that for a generic $p > 0$, $A(p)$ is not Hermitian and therefore techniques like singular value decomposition cannot be employed.\n\n\\bibliography{biblio}\n\\bibliographystyle{unsrt}\n\n\n", "itemtype": "equation", "pos": 21776, "prevtext": "\nis the probability of co-observation of the standing waves $\\phi_j$ and $\\phi_k$. This means that the (real) decay of the mixed eigenvector $\\xi_{jk}$ introduced by the decoherence is proportional to the probability that the two components $\\phi_j$ and $\\phi_k$ are not observed on the same node.\n\nFor the eigenvector derivative, we compute the mixing proportion $b_{jk}^{lm}$ for $j\\neq k$, $l\\neq m$, and $(j,k)\\neq(l,m)$. Intuitively, this tells us how much of $\\xi_{jk}$ goes into $\\xi_{lm}^\\prime$\n\\begin{eqnarray}\\label{eq54}\n b_{jk}^{lm} &=& \\frac{\\xi_{jk}^T A^\\prime \\xi_{lm}}{\\pi_{lm}-\\pi_{jk}} = \\frac{(\\phi_j\\otimes\\phi_k)^T \\left( \\sum_{v\\in V} E_{vv}\\otimes E_{vv} - I\\otimes I\\right) (\\phi_l\\otimes\\phi_m)}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)}\\nonumber\\\\\n &=& \\frac{\\sum_{v\\in V} (\\phi_j^T E_{vv}\\phi_l)\\otimes (\\phi_k^T E_{vv} \\phi_m)}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)} = \\frac{\\sum_{v\\in V} \\phi_{jv}\\phi_{lv}\\phi_{kv}\\phi_{mv}}{i(\\lambda_m + \\lambda_j - \\lambda_l - \\lambda_k)}.\n\\end{eqnarray}\nHence, the mixing is proportional to the probability of co-observation of the standing waves $\\phi_j$, $\\phi_k$, $\\phi_l$, and $\\phi_m$.\n\n\nFor $j=k$ we have repeated eigenvalues, so we need to solve the following eigensystem:\n\n", "index": 63, "text": "\\begin{equation}\\label{eq55}\n \\Xi \\Gamma = \\Gamma \\Lambda(0)^\\prime,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E32.m1\" class=\"ltx_Math\" alttext=\"\\Xi\\Gamma=\\Gamma\\Lambda(0)^{\\prime},\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u039e</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u039b</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><mo>\u2032</mo></msup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]