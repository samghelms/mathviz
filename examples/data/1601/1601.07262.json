[{"file": "1601.07262.tex", "nexttext": "\n\nwhere $I_x(x,y)$ and $I_y(x,y)$ respective represents pixel gradient in the $x$ and $y$ direction at point $(x,y)$. If $\\lambda_1\\approx\\lambda_2$ and $\\lambda_1\\lambda_2\\gg0$, the point $(x,y)$ is considered as a corner point. Therefore, corner response can be measured by the following:\n\n", "itemtype": "equation", "pos": 6755, "prevtext": "\n\n\n\\maketitle\n\n\\begin{abstract}\nMany images, of natural or man-made scenes often contain \\emph{Similar but Genuine Objects} (SGO). This poses a challenge to existing \\emph{Copy-Move Forgery Detection} (CMFD) methods which match the key points / blocks, solely based on the pair similarity in the scene. To address such issue, we propose a novel CMFD method using \\emph{Scaled Harris Feature Descriptors} (SHFD) that preform consistently well on forged images with SGO. It involves the following main steps: (i) Pyramid scale space and orientation assignment are used to keep scaling and rotation invariance; (ii) Combined features are applied for precise texture description; (iii) Similar features of two points are matched and RANSAC is used to remove the false matches. The experimental results indicate that the proposed algorithm is effective in detecting SGO and copy-move forgery, which compares favorably to existing methods. Our method exhibits high robustness even when an image is operated by geometric transformation and post-processing.\n\\end{abstract}\n\n\\begin{keywords}\nImage copy-move forgery, similar but genuine objects, scaled Harris feature descriptors\n\\end{keywords}\n\n\n\n\\begin{figure*} [htb]\n\\centering\n\\includegraphics[width=3.5cm,height=3cm]{16}\\hspace{0.02cm}\n\\includegraphics[width=3.5cm,height=3cm]{16sift}\\hspace{0.02cm}\n\\includegraphics[width=3.5cm,height=3cm]{16lbpsurf}\\hspace{0.02cm}\n\\includegraphics[width=3.5cm,height=3cm]{16lbpsvddct}\\hspace{0.02cm}\n\n\\vspace{.2cm}\n\\includegraphics[width=3.5cm,height=3cm]{16gai}\\hspace{0.02cm}\n\n\\includegraphics[width=3.5cm,height=3cm]{16gaisift}\\hspace{0.02cm}\n\\includegraphics[width=3.5cm,height=3cm]{16gaisurf}\\hspace{0.02cm}\n\\includegraphics[width=3.5cm,height=3cm]{16gailbpsvddct}\\hspace{0.02cm}\n\n\\caption{Top and bottom: exemplar results of SGO and copy-move forgery images. From left to right: (Column 1) Original and tampered images, (Column 2-4) the results based on SIFT\\cite{14_amerini2011sift}, SURF\\cite{17_bo2010image} and SHFD. The green and blue points in column 2 respectively indicate two clusters, the red lines denote two matching points. }\n\\vspace{-.1in}\n\\label{f1}\n\\end{figure*}\n\n\n\n\\section{Introduction}\n\\label{sec:intro}\n\nCopy-move image forgery, as the most commonly occurring forgery type, copies part of the image, and paste it into another part of the same image. Various \\emph{Copy-Move Forgery Detection} (CMFD) methods have been proposed, which can be categorized as block-based and key-point-based matching methods.\nThe first block-based CMFD algorithm by Fridrich \\cite{1_fridrich_copymove03}, makes use of Discrete Cosine Transform (DCT) and lexicographical order. Many improved DCT algorithms were subsequently proposed \\cite{2_wang2011dwt, 3_6047099}. Muhammad proposed a passive method based on Dyadic Wavelet Transform (DyWT), which combined approximation and detail subbands \\cite{4_Muhammad201249}. In addition, some algorithms focus on dimensionality reduction, such as Principal Component Analysis (PCA) \\cite{5_popescu2004exposing}. To keep geometric transformation invariance, efforts have been devoted in recent works, such as Fourier-Mellin Transform (FMT) \\cite{7_li2010rotation}, invariant moment \\cite{8_liu2011passive,10_ryu2010detection}, and Local Binary Patterns (LBP) \\cite{12_davarzani2013copy}.\nOn the category of key-points based methods, Huang proposed CMFD algorithm based on Scale Invariant Feature Transform (SIFT) \\cite{13_huang2008detection}, and subsequently, SIFT-improved approaches are proposed\\cite{14_amerini2011sift,15_pan2010detecting}. Speeded-Up Robust Feature (SURF) was applied for improving computational efficiency by Xu\\cite{17_bo2010image} and Shivakumar \\cite{shivakumar2011detection}. Furthermore, some other methods based on DAISY descriptor \\cite{18_guo2013duplication} and Harris points\\cite{19_chen2013region} are recently proposed.\n\n\n\nExisting CMFD methods devote to finding similar\nareas to locate tampering, while ignoring that most realistic\nscenes are likely to contain \\emph{Similar but Genuine Objects} (SGO). With such ambiguity, the performance of CMFD usually degrades when applying to image with SGO. In this work, we proposed a novel CMFD method using \\emph{Scaled Harris Feature Descriptors} (SHFD), which performs consistently well and is robust to images containing SGO. Fig.\\ref{f1} illustrates an example of CMFD performance degradation using SIFT \\cite{14_amerini2011sift} and SURF \\cite{17_bo2010image}. Whereas the proposed SHFD method demonstrates promising results. Some important features of our work are as follows,\n\n\n\n\\begin{enumerate}\n\\item  Key points are extracted using scaled Harris features, which are scaling invariant. Orientation is assigned to the neighborhood of each key points, in order to achieve rotational invariance.\n\\item  SHFD performs consistently well for images with \\emph{naive}, \\emph{rotation}, \\emph{scaling} and \\emph{free-form distortion} tempering. Empirically, it outperforms SIFT and SURF methods for images from COVERAGE database\\cite{24_dataset}. Furthermore, SHFD is robust to post-processing including \\emph{blurring}, \\emph{noise}, and \\emph{jpeg compression} operations.\n\n\\end{enumerate}\n\n\n\n\\section{PROPOSED METHOD}\n\\label{sec:pagestyle}\n\n\n\nThere are four main steps in our proposed SHFD algorithm: Scaled Harris points extraction (Section \\ref{sec21}), Orientation assignment (Section \\ref{sec22}), features extraction (Section \\ref{sec23}) and feature matching (Section \\ref{sec24}).\n\n\n\\subsection{Scaled Harris points extraction} \\label{sec21}\n\nDetecting locations that are invariant\nto scale change of the image can be accomplished by\nsearching for stable features across all scales,\nusing a continuous function of scale known as scale\nspace. The pyramid scale space \\cite{witkin1984scale} is the most useful model to achieve scale invariance, where octave and interval are used for multi-resolution analysis and image continuity maintenance. Since Harris~\\cite{21_harris1988combined} is a classical way to extract corner points with none scale invariance, scaled Harris points are built in our approach by combining pyramid scale.\n\nThe pyramid intervals are obtained by Gaussian smoothing and sub-sampling is used to build octaves. For given image $I(x,y)$, the intervals in the same octave are given by: $L(x,y,\\sigma)=I(x,y)*G(x,y,\\sigma)$, and the first interval of next octave is $L_{oc}(x,y,\\sigma) = \\text{\\textit{sampling}}(L_{oc-1}(x,y,\\sigma), \\beta)$. Where $G(x,y,\\sigma)$ is Gaussian blur, ${\\textit{sampling}}$ is the function of down sampling, $\\sigma$ and $\\beta$ is the scale and sampling factor.\n\n\nHarris points are classified by using eigen values, $\\lambda_1$ and $\\lambda_2$ of the second moment matrix $M(x,y)$ as:\n\n", "index": 1, "text": "\\begin{equation}\\label{eqn:harris}\nM(x,y) = \\begin{bmatrix}\n  I_x(x,y)^2 & I_x(x,y)I_y(x,y) \\\\\n  I_x(x,y)I_y(x,y) & I_y(x,y)^2 \\\\\n\\end{bmatrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"M(x,y)=\\begin{bmatrix}I_{x}(x,y)^{2}&amp;I_{x}(x,y)I_{y}(x,y)\\\\&#10;I_{x}(x,y)I_{y}(x,y)&amp;I_{y}(x,y)^{2}\\\\&#10;\\end{bmatrix}\" display=\"block\"><mrow><mrow><mi>M</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><msub><mi>I</mi><mi>x</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>I</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mi>y</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><msub><mi>I</mi><mi>x</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>I</mi><mi>y</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"center\"><mrow><msub><mi>I</mi><mi>y</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\nwhere $det(M)=\\lambda_1\\lambda_2 = I_x^2I_y^2-(I_xI_y)^2$, $tr(M)=\\lambda_1+\\lambda_2 = I_x^2+I_y^2$, and $k$ is the weight value, $t\\_\\{CR\\}$ is the threshold value. In our method, the scaled Harris key points are extracted on every scale $L(x,y,\\sigma)$ .\n\n\n\\subsection{Orientation assignment} \\label{sec22}\n\nTo achieve rotation invariance, the most important issue lies in locating the correct neighborhood region. Based on this, the orientation of each key point is a way to help find same region. In our approach, the oriented gradient is used to assign orientation for the neighborhood of each key point. The gradient magnitude and orientation, denoted as $m(x,y)$ and $\\theta(x,y)$, are computed on their octaves by using pixel differences as shown in the following equations:\n\n\n", "itemtype": "equation", "pos": 7204, "prevtext": "\n\nwhere $I_x(x,y)$ and $I_y(x,y)$ respective represents pixel gradient in the $x$ and $y$ direction at point $(x,y)$. If $\\lambda_1\\approx\\lambda_2$ and $\\lambda_1\\lambda_2\\gg0$, the point $(x,y)$ is considered as a corner point. Therefore, corner response can be measured by the following:\n\n", "index": 3, "text": "\\begin{equation}\nCR = det(M) - k(tr(M)^2)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"CR=det(M)-k(tr(M)^{2})\" display=\"block\"><mrow><mrow><mi>C</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>k</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\n\nwhere $\\Delta L_x = L(x+1, y) - L(x-1, y)$, $\\Delta L_x = L(x, y+1) - L(x, y-1)$. And then, through dividing $[0, 2\\pi]$ to ten regions and getting the histogram of gradient, the maximal magnitude is the orientation of point $(x,y)$. The rotation invariance neighborhood region of point $(x,y)$ is given in the following equation:\n\n", "itemtype": "equation", "pos": 8046, "prevtext": "\nwhere $det(M)=\\lambda_1\\lambda_2 = I_x^2I_y^2-(I_xI_y)^2$, $tr(M)=\\lambda_1+\\lambda_2 = I_x^2+I_y^2$, and $k$ is the weight value, $t\\_\\{CR\\}$ is the threshold value. In our method, the scaled Harris key points are extracted on every scale $L(x,y,\\sigma)$ .\n\n\n\\subsection{Orientation assignment} \\label{sec22}\n\nTo achieve rotation invariance, the most important issue lies in locating the correct neighborhood region. Based on this, the orientation of each key point is a way to help find same region. In our approach, the oriented gradient is used to assign orientation for the neighborhood of each key point. The gradient magnitude and orientation, denoted as $m(x,y)$ and $\\theta(x,y)$, are computed on their octaves by using pixel differences as shown in the following equations:\n\n\n", "index": 5, "text": "\\begin{align}\nm(x,y) &= \\sqrt{\\Delta L_x^2 + \\Delta L_y^2 }\\\\\n\\theta(x,y) &=\\tan^{-1}(\\Delta L_y / \\Delta L_x)\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle m(x,y)\" display=\"inline\"><mrow><mi>m</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sqrt{\\Delta L_{x}^{2}+\\Delta L_{y}^{2}}\" display=\"inline\"><mrow><mi/><mo>=</mo><msqrt><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>L</mi><mi>x</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msubsup><mi>L</mi><mi>y</mi><mn>2</mn></msubsup></mrow></mrow></msqrt></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\theta(x,y)\" display=\"inline\"><mrow><mi>\u03b8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\tan^{-1}(\\Delta L_{y}/\\Delta L_{x})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msup><mi>tan</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mi mathvariant=\"normal\">\u0394</mi><mo>\u2062</mo><msub><mi>L</mi><mi>y</mi></msub></mrow><mo>/</mo><mi mathvariant=\"normal\">\u0394</mi></mrow><mo>\u2062</mo><msub><mi>L</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\n\nwhere $\\begin{bmatrix}\n               x \\\\\n               y \\\\\n             \\end{bmatrix}$ represent pixels in a 4$\\times$4 square neighborhood among the center $(x,y)$,\n             $\\begin{bmatrix}\n               $\\~{x}$ \\\\\n               $\\~{y}$\\\\\n             \\end{bmatrix}$ are the coordinates of oriented region, and $\\theta$ is the orientation of key point $(x,y)$.\n\n\n\\subsection{Feature descriptor extraction} \\label{sec23}\n\n\nAccurate feature descriptions are able to capture the weeny image details are the key of robustness on SGO. Local Binary Patterns (LBP) is an effective texture feature descriptor due to its low computational complexity, invariance to monotonic gray-scale changes and texture description ability\\cite{22_ojala2002multiresolution}. Besides, DCT is a well known sparsifying transform for image regions where information is highly concentrated in its low-frequency component. Additionally, Singular Value Decomposition (SVD) is the tool commonly used in the dimensionality reduction methods. Therefore, coefficients in the DCT domain, and singular values of the image data are good features which are robust to noise and interference.\n\nIn our method, Suppose $M$ is the neighborhood region of point $(x,y)$, which is a 4$\\times$4 square matrix. Uniform LBP $(LBP_{P,R}^{u2})$ and the rotation invariant uniform LBP $(LBP_{P,R}^{riu2})$ keep the rotation invariance through starting from minimum LBP value and remove the redundancy, where $P$ is the number of pixels in neighborhood on a circle of radius $R$.  DCT coefficient could be extracted from $M$ and reshaped as a vector $dct$ with dimensions of 16. The diagonal entries of singular matrix $M$ in descending order are recorded as SVD vectors $svd$.\nIn total, the descriptor of point $(x,y)$ is presented with four feature vectors $V=[V_{1}(LBP_{8,1}^{u2})$, $V_{2}(LBP_{16,2}^{riu2})$, $V_{3}(dct)$, $V_{4}(svd)]$ with dimensions of 93($V_1(59)+V_2(14)+V_3(16)+V_4(4)$).\n\n\n\n \n \n \n \n \n\n\n\n\n\\subsection{Feature matching} \\label{sec24}\nSince the next octave is the down sampling by factor $\\beta$, the equation of mapping the Harris point $(x, y)$ to original image $(X, Y)$ is:\n\n", "itemtype": "equation", "pos": 8501, "prevtext": "\n\nwhere $\\Delta L_x = L(x+1, y) - L(x-1, y)$, $\\Delta L_x = L(x, y+1) - L(x, y-1)$. And then, through dividing $[0, 2\\pi]$ to ten regions and getting the histogram of gradient, the maximal magnitude is the orientation of point $(x,y)$. The rotation invariance neighborhood region of point $(x,y)$ is given in the following equation:\n\n", "index": 7, "text": "\\begin{equation}\n\\begin{bmatrix}\n               $\\~{x}$ \\\\\n               $\\~{y}$ \\\\\n             \\end{bmatrix}\n             = \\begin{bmatrix}\n                 \\cos\\theta & -\\sin\\theta \\\\\n                 \\sin\\theta & \\cos\\theta \\\\\n               \\end{bmatrix} \\begin{bmatrix}\n               x \\\\\n               y \\\\\n             \\end{bmatrix}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{bmatrix}$\\~{x}$\\\\&#10;$\\~{y}$\\\\&#10;\\end{bmatrix}=\\begin{bmatrix}\\cos\\theta&amp;-\\sin\\theta\\\\&#10;\\sin\\theta&amp;\\cos\\theta\\\\&#10;\\end{bmatrix}\\begin{bmatrix}x\\\\&#10;y\\\\&#10;\\end{bmatrix}\" display=\"block\"><mrow><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mtext>x\u0303</mtext></mtd></mtr><mtr><mtd columnalign=\"center\"><mtext>\u1ef9</mtext></mtd></mtr></mtable><mo>]</mo></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mrow><mi>cos</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mtd><mtd columnalign=\"center\"><mrow><mo>-</mo><mrow><mi>sin</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"center\"><mrow><mi>sin</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mtd><mtd columnalign=\"center\"><mrow><mi>cos</mi><mo>\u2061</mo><mi>\u03b8</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo>\u2062</mo><mrow><mo>[</mo><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>x</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>y</mi></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\n\nIf Euclidean distances of every mixed feature are less than the threshold $\\epsilon$, the point pairs $(i,j$) are regarded as candidate matching pairs. After this we obtain the matrix of matching pairs, recorded as follows:\n\n", "itemtype": "equation", "pos": 11024, "prevtext": "\n\nwhere $\\begin{bmatrix}\n               x \\\\\n               y \\\\\n             \\end{bmatrix}$ represent pixels in a 4$\\times$4 square neighborhood among the center $(x,y)$,\n             $\\begin{bmatrix}\n               $\\~{x}$ \\\\\n               $\\~{y}$\\\\\n             \\end{bmatrix}$ are the coordinates of oriented region, and $\\theta$ is the orientation of key point $(x,y)$.\n\n\n\\subsection{Feature descriptor extraction} \\label{sec23}\n\n\nAccurate feature descriptions are able to capture the weeny image details are the key of robustness on SGO. Local Binary Patterns (LBP) is an effective texture feature descriptor due to its low computational complexity, invariance to monotonic gray-scale changes and texture description ability\\cite{22_ojala2002multiresolution}. Besides, DCT is a well known sparsifying transform for image regions where information is highly concentrated in its low-frequency component. Additionally, Singular Value Decomposition (SVD) is the tool commonly used in the dimensionality reduction methods. Therefore, coefficients in the DCT domain, and singular values of the image data are good features which are robust to noise and interference.\n\nIn our method, Suppose $M$ is the neighborhood region of point $(x,y)$, which is a 4$\\times$4 square matrix. Uniform LBP $(LBP_{P,R}^{u2})$ and the rotation invariant uniform LBP $(LBP_{P,R}^{riu2})$ keep the rotation invariance through starting from minimum LBP value and remove the redundancy, where $P$ is the number of pixels in neighborhood on a circle of radius $R$.  DCT coefficient could be extracted from $M$ and reshaped as a vector $dct$ with dimensions of 16. The diagonal entries of singular matrix $M$ in descending order are recorded as SVD vectors $svd$.\nIn total, the descriptor of point $(x,y)$ is presented with four feature vectors $V=[V_{1}(LBP_{8,1}^{u2})$, $V_{2}(LBP_{16,2}^{riu2})$, $V_{3}(dct)$, $V_{4}(svd)]$ with dimensions of 93($V_1(59)+V_2(14)+V_3(16)+V_4(4)$).\n\n\n\n \n \n \n \n \n\n\n\n\n\\subsection{Feature matching} \\label{sec24}\nSince the next octave is the down sampling by factor $\\beta$, the equation of mapping the Harris point $(x, y)$ to original image $(X, Y)$ is:\n\n", "index": 9, "text": "\\begin{equation}\nX = x\\times(1/\\beta)^{oc-1},  \\hskip 0.3cm Y = y\\times(1/\\beta)^{oc-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"X=x\\times(1/\\beta)^{oc-1},\\hskip 8.535827ptY=y\\times(1/\\beta)^{oc-1}\" display=\"block\"><mrow><mrow><mi>X</mi><mo>=</mo><mrow><mi>x</mi><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mi>o</mi><mo>\u2062</mo><mi>c</mi></mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow><mo rspace=\"11pt\">,</mo><mrow><mi>Y</mi><mo>=</mo><mrow><mi>y</mi><mo>\u00d7</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b2</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mrow><mi>o</mi><mo>\u2062</mo><mi>c</mi></mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\n\nSince there is a significant amount of false matches, a way of removing false matches is applied. RANdom SAmple Consensus (RANSAC)\\cite{23_RANSAC} is an iterative method to remove false matches. In our method, RANSAC evaluates a translation matrix model on the dataset $match(i,j)$ and removes false matches that are not compatible with it.\n\n\n\\begin{figure} [b]\n\\centering\n\\includegraphics[width=4cm,height=2.8cm]{roctf}\\hspace{0.01cm}\n\\includegraphics[width=4cm,height=2.8cm]{rocplot1}\n\\caption{From left to right: ROC curves respect to tampering factors and COVERAGE database.}\n\\vspace{-.2in}\n\\label{f2}\n\\end{figure}\n\n\\begin{figure*} [htb]\n\\centering\n\\includegraphics[width=3.5cm,height=2.8cm]{61blur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61siftblur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61surfblur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61Rblur1}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1sift}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1surf}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1R}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\includegraphics[width=3.5cm,height=2.8cm]{72}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72siftjpeg80}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72surfjpeg80}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72Rjpeg80}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\caption{From top to bottom: Exemplar results of two SGO and one tampered images varying with respect to \\emph{blurring}, \\emph{noise} and \\emph{jpeg compression}. From left to right: (Column 1) tested images, (Column 2-4) the results based on SIFT\\cite{14_amerini2011sift}, SURF\\cite{17_bo2010image} and SHFD.}\n\\vspace{-.1in}\n\\label{f3}\n\\end{figure*}\n\n\\begin{figure*} [htb]\n\n\\centering\n\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotblur1}\\hspace{0.01cm}\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotnoise1}\\hspace{0.01cm}\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotjpeg}\\hspace{0.01cm}\n\\vspace{.1cm}\n\\setlength{\\abovecaptionskip}{-1.3cm}\n\\caption{From left to right: ROC curves varying with respect to \\emph{blurring}, \\emph{noise} and \\emph{jpeg compression}}\n\\vspace{-.1in}\n\\label{f4}\n\\end{figure*}\n\n\n\\section{EXPERIMENTS}\n\n\\subsection{Database}\nWe evaluate the proposed SHFD algorithm, as well as popular SIFT\\cite{14_amerini2011sift} and SURF \\cite{17_bo2010image} methods over the newly proposed COVERAGE database \\cite{24_dataset}. The original images of the forgery pairs from COVERAGE all contain SGO. The selected images are forged with 6 different tampering factors, respectively \\emph{naive}, \\emph{rotation}, \\emph{scaling}, \\emph{illumination}, \\emph{free-form distortion} and \\emph{combined factors}.\n\n\n\\subsection{Parameters and Metrics}\n\nThe input parameters required by the\nmethods are set as follows: $oc=4$ (number of pyramid octaves), $in=4$ (number of pyramid intervals), $\\beta=1.25$ (sampling factor in pyramid space), $t\\_\\{CR\\} =0.02*max(CR)$ (threshold for the corner response) and $k=0.05$ (weight in corner response).\n\nTo evaluate the performance of CMFD on images with SGO, true positive rate (TPR) and false positive rate (FPR) are used as evaluation metrics. They are defined as follows,\n\n", "itemtype": "equation", "pos": 11352, "prevtext": "\n\nIf Euclidean distances of every mixed feature are less than the threshold $\\epsilon$, the point pairs $(i,j$) are regarded as candidate matching pairs. After this we obtain the matrix of matching pairs, recorded as follows:\n\n", "index": 11, "text": "\\begin{equation}\nmatch(i,j) = [(X_i, Y_i), (X_j, Y_j)]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"match(i,j)=[(X_{i},Y_{i}),(X_{j},Y_{j})]\" display=\"block\"><mrow><mrow><mi>m</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>j</mi></msub><mo>,</mo><msub><mi>Y</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.07262.tex", "nexttext": "\n\n\n\n\\subsection{Numerical Results}\n\nIn this part, we compare empirical performance using the proposed SHFD algorithm to popular CMFD methods, including SIFT \\cite{14_amerini2011sift}, and SURF \\cite{17_bo2010image} algorithms. We present and analyze the numerical results with different tampering factors and post-processing methods.\n\n\n\\subsubsection{Performance on tampering factors discussion}\n\nTo minimize the visible traces of forgery, various types of tempering factors are applied in the forged image. We now analyze how CMFD performances varied with different tempering factors. We evaluate TPR and FPR values by applying SHFD algorithm to 100 images from COVERAGE. Fig. \\ref{f2} in the left side illustrates ROC curves subject to different tampering factors. Empirically, our proposed SHFD algorithm performs consistently well for images with \\emph{naive}, \\emph{rotation}, \\emph{scaling} and \\emph{free-form distortion} tempering. However, we also observed reasonable performance degradation for images with complicated tempering factors such as \\emph{illumination}, and \\emph{combined factors}. More sophisticated features which impose illumination variance are required to handle more complex tempering factor in the future work.\n\nTo compare the proposed SHFD method to the popular SIFT and SURF methods, we also evaluate TPR and FPR values using SIFT\\cite{14_amerini2011sift}, and SURF \\cite{17_bo2010image} methods over COVERAGE database. The overall ROC curves are plotted in the right of Fig.\\ref{f2}. The empirical results obtained by the proposed SHFD method demonstrate promising performance compared to SIFT and SURF methods.\n\n\n\n\n\n\n\\subsubsection{Post-processing experiments}\n\nIt is important to study the CMFD behavior subject to post-processing operations such as blurring, noise corruption and JPEG compression, since similar effects usually occur during image transmission and processing. We now artificially edit images from COVERAGE databases with operations including Gaussian blurring (window size, $w=3$, and sigma, $\\sigma=[0.5, 1,2 ]$), Gaussian noise corruption (mean, $m=0$, and variance, $var=[1, 3, 5]$) and JPEG compression with a decreasing quality factor of $[80, 60, 40]$. Fig.\\ref{f3} shows three examples of processed images with blurring, noise corruption and JPEG compression, as well as their CMFD matching results using SIFT, SURF and SHFD methods.\nThe corresponding ROC curves are plotted in Fig.\\ref{f4} with blurring, noise corruption and JPEG compression respectively. From the plotted curves, we observe promising robustness of the proposed SHFD method subject to \\emph{blurring}, \\emph{noise}, and \\emph{jpeg compression} operations.\n\n\n\n\n\\section{CONCLUSIONS}\n\nDetecting images with SGO and copy-move is a common issue in CMFD. However, it is often overlooked in existing methods. In this paper, an efficient method called SHFD was proposed and evaluated against two state-of-art methods. The results show that SHFD is the only one that could distinguish images with SGO and copy-move forgery. Furthermore, it also determines the geometric transformations and post-processing applied to the forged regions. However, our method preforms unsatisfied in the illumination variance, which will be continued to work in the future study.\n\n\n\\label{sec:refs}\n\n\n\n\n\n\n\\bibliographystyle{IEEEbib}\n\\bibliography{icassp_v2_Bihan}\n\n\n", "itemtype": "equation", "pos": 14791, "prevtext": "\n\nSince there is a significant amount of false matches, a way of removing false matches is applied. RANdom SAmple Consensus (RANSAC)\\cite{23_RANSAC} is an iterative method to remove false matches. In our method, RANSAC evaluates a translation matrix model on the dataset $match(i,j)$ and removes false matches that are not compatible with it.\n\n\n\\begin{figure} [b]\n\\centering\n\\includegraphics[width=4cm,height=2.8cm]{roctf}\\hspace{0.01cm}\n\\includegraphics[width=4cm,height=2.8cm]{rocplot1}\n\\caption{From left to right: ROC curves respect to tampering factors and COVERAGE database.}\n\\vspace{-.2in}\n\\label{f2}\n\\end{figure}\n\n\\begin{figure*} [htb]\n\\centering\n\\includegraphics[width=3.5cm,height=2.8cm]{61blur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61siftblur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61surfblur1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{61Rblur1}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1sift}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1surf}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{23noise1R}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\includegraphics[width=3.5cm,height=2.8cm]{72}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72siftjpeg80}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72surfjpeg80}\\hspace{0.01cm}\n\\includegraphics[width=3.5cm,height=2.8cm]{72Rjpeg80}\\hspace{0.01cm}\n\\vspace{.1cm}\n\n\\caption{From top to bottom: Exemplar results of two SGO and one tampered images varying with respect to \\emph{blurring}, \\emph{noise} and \\emph{jpeg compression}. From left to right: (Column 1) tested images, (Column 2-4) the results based on SIFT\\cite{14_amerini2011sift}, SURF\\cite{17_bo2010image} and SHFD.}\n\\vspace{-.1in}\n\\label{f3}\n\\end{figure*}\n\n\\begin{figure*} [htb]\n\n\\centering\n\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotblur1}\\hspace{0.01cm}\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotnoise1}\\hspace{0.01cm}\n\\includegraphics[width=5.5cm,height=3.5cm]{rocplotjpeg}\\hspace{0.01cm}\n\\vspace{.1cm}\n\\setlength{\\abovecaptionskip}{-1.3cm}\n\\caption{From left to right: ROC curves varying with respect to \\emph{blurring}, \\emph{noise} and \\emph{jpeg compression}}\n\\vspace{-.1in}\n\\label{f4}\n\\end{figure*}\n\n\n\\section{EXPERIMENTS}\n\n\\subsection{Database}\nWe evaluate the proposed SHFD algorithm, as well as popular SIFT\\cite{14_amerini2011sift} and SURF \\cite{17_bo2010image} methods over the newly proposed COVERAGE database \\cite{24_dataset}. The original images of the forgery pairs from COVERAGE all contain SGO. The selected images are forged with 6 different tampering factors, respectively \\emph{naive}, \\emph{rotation}, \\emph{scaling}, \\emph{illumination}, \\emph{free-form distortion} and \\emph{combined factors}.\n\n\n\\subsection{Parameters and Metrics}\n\nThe input parameters required by the\nmethods are set as follows: $oc=4$ (number of pyramid octaves), $in=4$ (number of pyramid intervals), $\\beta=1.25$ (sampling factor in pyramid space), $t\\_\\{CR\\} =0.02*max(CR)$ (threshold for the corner response) and $k=0.05$ (weight in corner response).\n\nTo evaluate the performance of CMFD on images with SGO, true positive rate (TPR) and false positive rate (FPR) are used as evaluation metrics. They are defined as follows,\n\n", "index": 13, "text": "\\begin{align}\nTPR &= \\frac{\\#\\text{image detected as forgery being forgery}}{\\#\\text{forgery images}}\\\\\nFPR &= \\frac{\\#\\text{image detected as forgery being origin}}{\\#\\text{origin images}}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle TPR\" display=\"inline\"><mrow><mi>T</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\#\\text{image detected as forgery being forgery}}{\\#\\text{%&#10;forgery images}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mtext>image detected as forgery being forgery</mtext></mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mtext>forgery images</mtext></mrow></mfrac></mstyle></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle FPR\" display=\"inline\"><mrow><mi>F</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\frac{\\#\\text{image detected as forgery being origin}}{\\#\\text{%&#10;origin images}}\" display=\"inline\"><mrow><mi/><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mtext>image detected as forgery being origin</mtext></mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mtext>origin images</mtext></mrow></mfrac></mstyle></mrow></math>", "type": "latex"}]