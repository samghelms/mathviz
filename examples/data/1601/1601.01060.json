[{"file": "1601.01060.tex", "nexttext": "\nand the \\textit{complete log-likelihood function} is\n\\begin{eqnarray}\n\tl^{C}(\\mathbf{\\Theta})&=&\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\nonumber \\\\\n\t&=&\\sum_{i,j\\in \\Omega}\\sum_{k=1}^{K}z_{ijk}[\\log{\\pi_{k}}+\\log{f_{p_{k}}(e_{ij};0,\\eta_{k})}].\n\\end{eqnarray}\n\nAs aforementioned in introduction, determining the number of components $K$ is an important problem for the mixture model. Thus, various model selection techniques can be readily employed to resolve this issue. Most conventional methods are based on the likelihood function and some information theoretic criteria, such as AIC and BIC. However, Leroux~\\cite{leroux1992consistent} showed that these criteria may overestimate the true number of components. On the other hand, Bayesian approaches~\\cite{ormoneit1998averaging,zivkovic2004recursive} have also been used to find a suitable number of components of the finite mixture model. But the computation burden and statistical properties of the Bayesian method limit its use to a certain extent. Here we adopt a recently proposed method by Huang et al.~\\cite{huang2013model} for this aim of selecting EP mixture number, and construct the following  penalized MoEP (PMoEP) model:\n\\begin{eqnarray}\\label{PMoEPmodel}\n\t\\max_{\\mathbf{\\Theta}}\\left\\{l_{P}^{C}(\\mathbf{\\Theta})=l^{C}(\\mathbf{\\Theta})-P(\\boldsymbol{\\pi};\\lambda)\\right\\},\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n\tP(\\boldsymbol{\\pi};\\lambda)=n\\lambda \\sum_{k=1}^{K}D_{k}\\log{\\frac{\\epsilon+\\pi_{k}}{\\epsilon}},\n\\end{eqnarray}\nwith $\\epsilon$ being a very small positive number, $\\lambda$ being a tuning parameter ($\\lambda>0$), and $D_{k}$ being the number of free parameters for the $k^{th}$ component. In the proposed PMoEP model, $D_{k}$ equals 2 (for $\\pi_{k}$ and $\\eta_{k}$).\n\n\\subsection{EM algorithm for PMoEP model}\nIn this subsection, we propose an EM algorithm to solve the proposed PMoEP model (\\ref{PMoEPmodel}). The EM algorithm is an iterative procedure and thus we assume that $\\mathbf{\\Theta}^{(t)}=\\{\\{\\boldsymbol{\\pi}^{(t)}\\},\\{\\boldsymbol{\\eta}^{(t)}\\},\\mathbf{U}^{(t)},\\mathbf{V}^{(t)}\\}$ is the estimation at the $t^{th}$ iteration. In the following, we will introduce the two steps of the proposed EM algorithm.\n\nIn the E step, we compute the conditional expectation of $z_{ijk}$ given $e_{ij}$ by the Bayes' rule:\n\\begin{eqnarray}\\label{updategamma}\n\t\\gamma_{ijk}^{(t+1)} = \\frac{\\pi_{k}^{(t)}f_{p_k}(y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T})|0,\\eta_{k}^{(t)})}\n\t{\\sum_{l=1}^{K}\\pi_{l}^{(t)}f_{p_l}(y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T})|0,\\eta_{l}^{(t)}))}.\n\\end{eqnarray}\nThen, it is easy to construct the so-called $Q$ function:\n\\vspace{-2pt}\n\\begin{eqnarray}\n\t\\begin{split}\n\t\tQ(\\mathbf{\\Theta},\\mathbf{\\Theta}^{(t)})\\!&=\\!\\sum_{i,j\\in \\Omega,k}\\gamma_{ijk}^{(t+1)}[\\log{f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};\\eta_{k})}\\!+\\!\\log{\\pi_{k}}]\\nonumber\\\\\n\t\t& - n\\lambda \\sum_{k=1}^{K}D_{k}\\log{\\frac{\\epsilon+\\pi_{k}}{\\epsilon}}.\n\t\\end{split}\n\\end{eqnarray}\n\nIn the M-step, we update $\\mathbf{\\Theta}$ by maximizing the $Q$ function.\nFor $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\eta}$, it is easy to obtain the update equations by taking the first derivative of $Q$ with respect to them respectively, and finding the zero points through:\n\n", "itemtype": "equation", "pos": 16996, "prevtext": "\n\n\n\n\n\\title{Low-rank Matrix Factorization under General Mixture Noise Distributions}\n\n\n\n\n\n\n\n\n\n\n\n\\author{Xiangyong Cao,\n    Qian~Zhao,\n\tDeyu~Meng$^\\ast$,~\\IEEEmembership{Member,~IEEE,}\n\tYang~Chen,~Zongben~Xu\n\\thanks{Xiangyong Cao, Qian Zhao, Deyu Meng, Yang Chen, and Zongben Xu are with the School of Mathematics and Statistics and Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi'an Jiaotong University, Xi'an 710049, China (caoxiangyong45@gmail.com, timmy.zhaoqian@gmail.com, dymeng@mail.xjtu.edu.cn, chengyang9103@gmail.com, zbxu@mail.xjtu.edu.cn)}.\n\\thanks{$^\\ast$Deyu Meng is the corresponding author}.\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\maketitle\n\n\n\n\\begin{abstract}\nMany computer vision problems can be posed as learning a low-dimensional subspace from high dimensional data. The low rank matrix factorization (LRMF) represents a commonly utilized subspace learning strategy. Most of the current LRMF techniques are constructed on the optimization problems using $L_1$-norm and $L_2$-norm losses, which mainly deal with Laplacian and Gaussian noises, respectively. To make LRMF capable of adapting more complex noise, this paper proposes a new LRMF model by assuming noise as Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining the penalized likelihood method with MoEP distributions. Such setting facilitates the learned LRMF model capable of automatically fitting the real noise through MoEP distributions. Each component in this mixture is adapted from a series of preliminary  super- or sub-Gaussian candidates. Moreover, by facilitating the local continuity of noise components, we embed Markov random field into the PMoEP model and further propose the advanced PMoEP-MRF model. An Expectation Maximization (EM) algorithm and a variational EM (VEM) algorithm are also designed to infer the parameters involved in the proposed PMoEP and the PMoEP-MRF model, respectively. The superseniority of our methods is demonstrated by extensive experiments on synthetic data, face modeling, hyperspectral image restoration and background subtraction.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nLow-rank matrix factorization, mixture of exponential power distributions, Expectation Maximization algorithm, face modeling, hyperspectral image restoration, background subtraction.\n\\end{IEEEkeywords}\n\n\\IEEEpeerreviewmaketitle\n\\section{Introduction}\nMany computer vision, machine learning, data mining and statistical problems can be formulated as the problem of extracting the intrinsic low dimensional subspace from input high-dimensional data. The extracted subspace tends to deliver the refined latent knowledge underlying data and thus has a wide range of applications including\nstructure from motion~\\cite{tomasi1992shape}, face recognition~\\cite{wright2009robust}, collaborative filtering~\\cite{koren2008factorization}, information retrieval~\\cite{deerwester1990indexing}, social networks~\\cite{cheng2012fused}, object recognition~\\cite{turk1991eigenfaces}, layer extraction~\\cite{ke2001subspace} and plane-based pose estimation~\\cite{sturm2000algorithms}.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{Intro_fig1}\n\t\\caption{From left to right: Original hyperspectral image (HSI), reconstructed image, two extracted noise images with their histograms by the proposed methods. (Top: $EP_{0.2}$ noise image and histogram. Bottom: $EP_{1.8}$ noise image and histogram).}\\label{intro_fig}\n\t\\vspace{-0mm}\n\\end{figure}\n\nLow rank matrix factorization (LRMF) is one of the most commonly utilized techniques for subspace learning. Given a data matrix $\\mathbf{Y}\\in\\mathcal{R}^{m\\times n}$ with entries $y_{ij}s$, the LRMF problem can be mathematically formulated as\n\\begin{eqnarray}\\label{LRMF}\n\\min_{\\mathbf{U},\\mathbf{V}}||\\mathbf{W}\\odot(\\mathbf{Y}-\\mathbf{U}\\mathbf{V}^{T})||,\n\\end{eqnarray}\nwhere $\\mathbf{W}$ is the indicator matrix with $w_{ij} = 0$ if $y_{ij}$ is missing and 1 otherwise, and $\\mathbf{U}\\in \\mathcal{R}^{m\\times r}$ and $\\mathbf{V}\\in \\mathcal{R}^{n\\times r}$ are low-rank matrices  ($r<\\min(m,n)$). The operator $\\odot$ denotes the Hadamard product (the component-wise multiplication) and $||\\cdot||$ corresponds to a certain noise measure.\n\nUnder the assumption of Gaussian noise, it is natural to utilize the $L_2$-norm (Frobenius norm) as the noise measure, which has been extensively studied in LRMF literatures~\\cite{srebro2003weighted,buchanan2005damped,okatani2007wiberg, aguiar2008spectrally, zhao2010successively, okatani2011efficient, wen2012solving, mitra2010large}. However, it has been recognized in many real applications that these methods constructed on $L_2$ norm are sensitive to outliers and non-Gaussian noise. In order to introduce robustness, the $L_1$-norm based models~\\cite{ke2005robust, eriksson2010efficient,zheng2012practical,kwak2008principal,shu2014robust, ji2010robust} have attracted much attention recently. However, the $L_1$-norm is only optimal for Laplace-like noise and still very limited for handling various types of noise encountered in real problems. Taking the hyper-spectral image (HSI) as an example, it has been investigated in~\\cite{zhang2014hyperspectral} that there are mainly two kinds of noise embedded in such type of data, i.e., sparse noise (stripe and deadline) and Gaussian-like noise, as depicted in Fig.~\\ref{intro_fig}. The stripe noise is produced by the non-uniform sensor response which conducts the deviation of gray values of the original image continuously towards one direction. This noise always very sparsely located on edges and in texture areas of an image. The deadline noise, which is induced by some damaged sensor, results in zero or very small pixel values of entire columns of images along some HSI bands. The Gaussian-like noise is induced by some random disturbation during the transmission process of hyper-spectral signals. It is easy to see that such kind of complex noise cannot be well fit by either Laplace or Gaussian, which means that neither $L_1$-norm nor $L_2$-norm LRMF models are proper for this type of data.\n\nVery recently, some novel models were presented to expand the availability of LRMF under more complex noise. The key idea is assuming that the noise follows a more complicated mixture of Gaussians (MoG)~\\cite{meng2013robust}, which is expected to better fit real noise, since the MoG constructs a universal approximator to any continuous density function in theory~\\cite{maz1996approximate}. However, this method still cannot finely adapt real data noise. On one hand, MoG can approximate a complex distribution, e.g. Laplace, only under the assumption that the number of components goes to infinity, while in applications only a finite number of components can be specified. On the other hand, it also lacks a theoretically sound manner to properly select the number of Gaussian mixture components based on the practical noise extent mixed in data. Thus, it is crucial to construct a better strategy with more adaptive distribution modeling capability on data noises beyond MoG.\n\nIn this paper, we propose a new LRMF method with a more general noise model to address the aforementioned issues. Specifically, we encode the noise as a mixture distribution of a series of sub- and super-Gaussians (i.e., general exponential power (EP) distribution), and formulate LRMF as a penalized MLE model, called PMoEP model \\cite{cao2015PMoEP}. Moreover, by facilitating the local continuity of noise components, we embed Markov random field into the PMoEP model and propose the PMoEP-MRF model. Then we design an Expectation Maximization (EM) algorithm and a variational EM (VEM) algorithm to estimate the parameters involved in the proposed PMoEP model and PMoEP-MRF model, respectively, and prove their convergence. The two new methods are not only capable of adaptively fitting complex real noise by EP noise components with proper parameters, but also able to automatically learn the proper number of noise components from data, and thus can better recover the true low-rank matrix from corrupted data as verified by extensive experiments.\n\nThe rest of the paper is organized as follows. In Section II, the related work regarding LRMF is discussed. In Section III, we first present the PMoEP model and the corresponding EM algorithm, and then conduct the convergence analysis of the proposed algorithm. The PMoEP-MRF model and the corresponding variational EM algorithm are proposed in Section IV. In Section V, extensive experiments are conducted to substantiate the superiority of the proposed models over previous methods. Finally, conclusions are drawn in Section VI. Throughout the paper, we denote scalars, vectors, and matrices as the non-bold letters, bold lower case letters, and bold upper case letters, respectively.\n\n\n\\section{Related work}\nThe $L_2$ norm LRMF with missing data has been studied for decades. Gabriel and Zamir~\\cite{gabriel1979lower} proposed a weighted SVD method as the early attempt for this task. They used alternated minimization to find the principal subspace underlying the data. Srebro and Jaakkola~\\cite{srebro2003weighted} proposed the Weighted Low-rank Approximation (WLRA) algorithm to enhance efficiency of LRMF calculation. Buchanan and Fitzgibbon~\\cite{buchanan2005damped} further proposed a regularized model that adds a regularization term and then adopts the damped newton algorithm to estimate the subspaces. However, it cannot handle large-scale problems due to the infeasibility of computing the Hessian matrix over a large number of variables. Okatani and Deguchi~\\cite{okatani2007wiberg} showed that a Wiberg marginalization strategy on $\\mathbf{U}$ and $\\mathbf{V}$ can provide a better and robust initialization and proposed the Wiberg algorithm that updates $\\mathbf{U}$ via least squares while updates $\\mathbf{V}$ by a Gauss-Newton step in each iteration. Later, the Wiberg algorithm was extended to a damped version to achieve better convergence by Okatani et al.~\\cite{okatani2011efficient}. Aguiar et al.~\\cite{aguiar2008spectrally} deduced a globally optimal solution to $L_2$-LRMF with missing data under the assumption that the missing data has a special Young diagram structure. Zhao and Zhang~\\cite{zhao2010successively} formulated the $L_2$- norm LRMF as a constrained model to improve its stability in real applications.\nWen et al.~\\cite{wen2012solving} adopted the alternating strategy to solve the $L_2$-norm LRMF problem. Mitra et al.~\\cite{mitra2010large} proposed an augmented Lagrangian method to solve the $L_2$-norm LRMF problem for higher accuracy. However, all of these methods minimize the $L_2$-norm or its variations and is only optimal for Gaussian-like noise.\n\nTo make subspace learning method less sensitive to outliers, some robust loss functions have been investigated. For example, De la Torre and Black~\\cite{de2003framework} adopted the Geman-McClure function and then used the iterative reweighted least square (IRLS) method to solve the induced optimization problem. In the last decade, the $L_1$-norm has become the most popular robust loss function along this research line. Ke and Kanade~\\cite{ke2005robust} initially replaced the $L_2$-norm with the $L_1$-norm for LRMF, and then solved the optimization by alternated convex programming (ACP) method. Kwak~\\cite{kwak2008principal} later proposed to maximize the $L_1$-norm of the projection of data points onto the unknown principal directions instead of minimizing the residue. Eriksson and Hengel~\\cite{eriksson2010efficient}\nexperimentally showed that the ACP approach does not converge to the desired point with high probability, and thus introduced the $L_1$-Wiberg approach to address this issue. Zheng et al.~\\cite{zheng2012practical} added more constraints to the factors $\\mathbf{U}$ and $\\mathbf{V}$ for $L_1$-norm LRMF, and solved the optimization by\nALM, which improved the performance in structure from motion application. Within the probabilistic framework, Wang et al.~\\cite{wang2012probabilistic} proposed probabilistic robust matrix factorization (PRMF) that modeled the noise as a Laplace distribution, which has been later extended to fully Bayesian settings by Wang and Yeung~\\cite{wang2013bayesian}. However, these methods optimize the $L_1$-norm and thus are only optimal for Laplace-like noise.\n\nBeyond Gaussian or Laplace, other types of noise assumptions have also been attempted recently to make the model adaptable to more complex noise scenarios. Lakshminarayanan et al.~\\cite{lakshminarayanan2011robust} assumed that the noise is drawn from a student-t distribution. Babacan et al.~\\cite{babacan2012sparse} proposed a Bayesian methods for low-rank matrix estimation modeling the noise as a combination of sparse and Gaussian. To handle more complex noise, Meng and De la Torre~\\cite{meng2013robust} modeled the noise as a MoG distribution for LRMF, and later was extended to the Bayesian framework by Chen et al.~\\cite{chen2015bayesian} and to RPCA by Zhao et al.~\\cite{zhao2014robust}. Although better than traditional methods, these methods are still very limited in dealing with complex noise in real scenarios.\n\n\\section{LRMF with MoEP noise}\nIn this section, we first present the new LRMF model with MoEP noise, called PMoEP model, and then design an EM algorithm to solve it. Finally, we give the convergence analysis of the proposed EM algorithm and the implementation issues.\n\n\\subsection{PMoEP model}\nIn LRMF, from a generative perspective, each element $y_{ij}(i=1,2,\\dots,m,j=1,2,\\dots,n)$ of the data matrix $\\mathbf{Y}$ can be modeled as\n\\begin{eqnarray}\n\ty_{ij}=\\mathbf{u}_{i}\\mathbf{v}_{j}^{T} + e_{ij},\n\\end{eqnarray}\nwhere $\\mathbf{u}_{i}$ and $\\mathbf{v}_{i}$ represent the $i^{th}$ row vectors of $\\mathbf{U}$ and $\\mathbf{V}$, respectively, and $e_{ij}$ is the noise embedded in $y_{ij}$. Instead of assuming that the noise obeys Gaussian~\\cite{srebro2003weighted}, Laplace~\\cite{ke2005robust} or MoG~\\cite{meng2013robust} distributions as previous methods, we assume that the noise $e_{ij}$ follows more flexible mixture of Exponential Power (EP) distributions:\n\\begin{eqnarray}\\label{MoEP}\n\t\\mathbb{P}(e_{ij}) = \\sum_{k=1}^K \\pi_{k} f_{p_{k}}(e_{ij};0,\\eta_k),\n\\end{eqnarray}\nwhere $\\pi_{k}$ is the mixing proportion with $\\pi_{k}\\geq 0$ and $\\sum_{k=1}^{K}\\pi_{k}=1$, $K$ is the number of the mixture components and $f_{p_{k}}(e_{ij};0,\\eta_k)$ denotes the $k^{th}$ EP distribution with parameter $\\eta_{k}$ and $p_{k} (p_{k}>0)$. Let $\\mathbf{p}=[p_1,p_2,\\dots,p_{K}]$, in which each $p_{k}$ can be variously specified. As defined in~\\cite{mineo2005software}, the density function of the EP distribution ($p>0$) with zero mean is\n\\begin{eqnarray}\\label{EP_pdf}\n\tf_{p}(e;0,\\eta) &=& \\frac{p\\eta^{\\frac{1}{p}}}{2\\Gamma(\\frac{1}{p})}\\exp\\{-\\eta|e|^{p}\\},\n\\end{eqnarray}\nwhere $\\eta$ is the precision parameter, $p$ is the shape parameter and $\\Gamma(\\cdot)$ is the Gamma function. By changing the shape parameter $p$, the EP distribution describes both leptokurtic ($0<p<2$) and platykurtic ($p>2$) distributions. In particular, we obtain the Laplace distribution with $p=1$, the Gaussian distribution with $p=2$ and the Uniform distribution with $p\\rightarrow \\infty$ (see Fig. \\ref{EPpdf}). Therefore, all previous cases including $L_2$, $L_1$, MoG and any combinations of them are just special cases of MoEP. By setting $\\eta=1/(p\\sigma^{p})$, the EP distribution (\\ref{EP_pdf}) can be equivalently written as $EP_{p}(e;0,p\\sigma^{p})$.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.85\\linewidth]{EPpdf}\n\t\\caption{The probability density function of EP distributions.}\\label{EPpdf}\n\\end{figure}\n\nIn our model, we assume that each noise $e_{ij}$ is equipped with an indicator variable $\\mathbf{z}_{ij}=[z_{ij1},z_{ij2},\\dots,z_{ijK}]^{T}$, where $z_{ijk}\\in \\{0,1\\}$ and $\\sum_{k=1}^{K}z_{ijk}=1$. $z_{ijk}=1$ implies that the noise $e_{ij}$ is drawn from the $k^{th}$ EP distribution. $\\mathbf{z}_{ij}$ obeys a multinomial distribution $\\mathbf{z}_{ij}\\sim \\mathcal{M}(\\boldsymbol{\\pi})$, where $\\boldsymbol{\\pi}=[\\pi_1,\\pi_2,\\dots,\\pi_{K}]^{T}$. Then we have:\n\\begin{eqnarray}\n\t\\mathbb{P}(e_{ij}|\\mathbf{z}_{ij}) &=& \\prod_{k=1}^{K} f_{p_{k}}(e_{ij};0,\\eta_{k})^{z_{ijk}},\\\\\n\t\\mathbb{P}(\\mathbf{z}_{ij};\\boldsymbol{\\pi}) &=& \\prod_{k=1}^{K}\\pi_{k}^{z_{ijk}}.\n\\end{eqnarray}\nDenoting $\\mathbf{E}=(e_{ij})_{m\\times n}$, $\\mathbf{Z}=(\\mathbf{z}_{ij})_{m\\times n}$ and $\\mathbf{\\Theta}=\\{\\boldsymbol{\\pi},\\boldsymbol{\\eta},\\mathbf{U},\\mathbf{V}\\}$ with $\\boldsymbol{\\eta}=[\\eta_1,\\eta_2,\\dots,\\eta_K]^{T}$, the \\textit{complete likelihood function} can then be written as\n\\begin{eqnarray}\n\t\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})&=& \\prod_{i,j\\in \\Omega}\\prod_{k=1}^{K}[\\pi_{k}f_{p_{k}}(e_{ij};0,\\eta_{k})]^{z_{ijk}},\n\\end{eqnarray}\nwhere $\\Omega$ is the index set of the non-missing entries in $\\mathbf{Y}$.\nThen the \\textit{log-likelihood function} is\n\n", "index": 1, "text": "\\begin{equation}\\label{loglikelihood}\n\tl(\\mathbf{\\Theta})=\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta})}=\\log{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"l(\\mathbf{\\Theta})=\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta})}=\\log{\\sum_{%&#10;\\mathbf{Z}}\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})},\" display=\"block\"><mrow><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><mi>\u2119</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo>;</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>\ud835\udc19</mi></munder><mrow><mi>\u2119</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo>,</mo><mi>\ud835\udc19</mi><mo>;</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 20470, "prevtext": "\nand the \\textit{complete log-likelihood function} is\n\\begin{eqnarray}\n\tl^{C}(\\mathbf{\\Theta})&=&\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\nonumber \\\\\n\t&=&\\sum_{i,j\\in \\Omega}\\sum_{k=1}^{K}z_{ijk}[\\log{\\pi_{k}}+\\log{f_{p_{k}}(e_{ij};0,\\eta_{k})}].\n\\end{eqnarray}\n\nAs aforementioned in introduction, determining the number of components $K$ is an important problem for the mixture model. Thus, various model selection techniques can be readily employed to resolve this issue. Most conventional methods are based on the likelihood function and some information theoretic criteria, such as AIC and BIC. However, Leroux~\\cite{leroux1992consistent} showed that these criteria may overestimate the true number of components. On the other hand, Bayesian approaches~\\cite{ormoneit1998averaging,zivkovic2004recursive} have also been used to find a suitable number of components of the finite mixture model. But the computation burden and statistical properties of the Bayesian method limit its use to a certain extent. Here we adopt a recently proposed method by Huang et al.~\\cite{huang2013model} for this aim of selecting EP mixture number, and construct the following  penalized MoEP (PMoEP) model:\n\\begin{eqnarray}\\label{PMoEPmodel}\n\t\\max_{\\mathbf{\\Theta}}\\left\\{l_{P}^{C}(\\mathbf{\\Theta})=l^{C}(\\mathbf{\\Theta})-P(\\boldsymbol{\\pi};\\lambda)\\right\\},\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n\tP(\\boldsymbol{\\pi};\\lambda)=n\\lambda \\sum_{k=1}^{K}D_{k}\\log{\\frac{\\epsilon+\\pi_{k}}{\\epsilon}},\n\\end{eqnarray}\nwith $\\epsilon$ being a very small positive number, $\\lambda$ being a tuning parameter ($\\lambda>0$), and $D_{k}$ being the number of free parameters for the $k^{th}$ component. In the proposed PMoEP model, $D_{k}$ equals 2 (for $\\pi_{k}$ and $\\eta_{k}$).\n\n\\subsection{EM algorithm for PMoEP model}\nIn this subsection, we propose an EM algorithm to solve the proposed PMoEP model (\\ref{PMoEPmodel}). The EM algorithm is an iterative procedure and thus we assume that $\\mathbf{\\Theta}^{(t)}=\\{\\{\\boldsymbol{\\pi}^{(t)}\\},\\{\\boldsymbol{\\eta}^{(t)}\\},\\mathbf{U}^{(t)},\\mathbf{V}^{(t)}\\}$ is the estimation at the $t^{th}$ iteration. In the following, we will introduce the two steps of the proposed EM algorithm.\n\nIn the E step, we compute the conditional expectation of $z_{ijk}$ given $e_{ij}$ by the Bayes' rule:\n\\begin{eqnarray}\\label{updategamma}\n\t\\gamma_{ijk}^{(t+1)} = \\frac{\\pi_{k}^{(t)}f_{p_k}(y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T})|0,\\eta_{k}^{(t)})}\n\t{\\sum_{l=1}^{K}\\pi_{l}^{(t)}f_{p_l}(y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T})|0,\\eta_{l}^{(t)}))}.\n\\end{eqnarray}\nThen, it is easy to construct the so-called $Q$ function:\n\\vspace{-2pt}\n\\begin{eqnarray}\n\t\\begin{split}\n\t\tQ(\\mathbf{\\Theta},\\mathbf{\\Theta}^{(t)})\\!&=\\!\\sum_{i,j\\in \\Omega,k}\\gamma_{ijk}^{(t+1)}[\\log{f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};\\eta_{k})}\\!+\\!\\log{\\pi_{k}}]\\nonumber\\\\\n\t\t& - n\\lambda \\sum_{k=1}^{K}D_{k}\\log{\\frac{\\epsilon+\\pi_{k}}{\\epsilon}}.\n\t\\end{split}\n\\end{eqnarray}\n\nIn the M-step, we update $\\mathbf{\\Theta}$ by maximizing the $Q$ function.\nFor $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\eta}$, it is easy to obtain the update equations by taking the first derivative of $Q$ with respect to them respectively, and finding the zero points through:\n\n", "index": 3, "text": "\\begin{equation}\\label{updatepi}\n\t\\pi_{k}^{(t+1)}\\!=\\!\\max\\left\\{0,\\frac{1}{1\\!-\\!\\lambda \\hat{D}}\\left[\\frac{\\sum_{i,j\\in \\Omega}\\gamma_{ijk}^{(t+1)}}{|\\Omega|}\\!-\\!\\lambda D_{k}\\right]\\right\\},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\pi_{k}^{(t+1)}\\!=\\!\\max\\left\\{0,\\frac{1}{1\\!-\\!\\lambda\\hat{D}}\\left[\\frac{%&#10;\\sum_{i,j\\in\\Omega}\\gamma_{ijk}^{(t+1)}}{|\\Omega|}\\!-\\!\\lambda D_{k}\\right]%&#10;\\right\\},\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>\u03c0</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mpadded><mo rspace=\"0.8pt\">=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mrow><mfrac><mn>1</mn><mrow><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded><mo rspace=\"0.8pt\">-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mo>[</mo><mrow><mpadded width=\"-1.7pt\"><mfrac><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow></msub><msubsup><mi>\u03b3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mrow><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a9</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mpadded><mo rspace=\"0.8pt\">-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>D</mi><mi>k</mi></msub></mrow></mrow><mo>]</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $\\hat{D} = \\sum_{k=1}^{K}D_{k} = 2K$, $N_{k} = \\sum_{i,j\\in \\Omega}\\gamma_{ijk}^{(t+1)}$ and $|\\Omega|$ is the number of non-missing elements. To update $\\mathbf{U}, \\mathbf{V}$, we need to maximize the following function:\n\\begin{eqnarray}\\label{update_s}\n\t-\\sum_{i,j\\in \\Omega}\\sum_{k=1}^{K}\\gamma_{ijk}^{(t+1)}\n\t\\eta_{k}^{(t+1)}|y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T}|^{p_k},\n\\end{eqnarray}\nwhich is equivalent to solving\\footnote{The $p$-norm of a matrix is defined as $||\\mathbf{X}||_{p}=(\\sum_{i,j}|x_{ij}|^{p})^{\\frac{1}{p}}$.}\n\n", "itemtype": "equation", "pos": 20681, "prevtext": "\n\n", "index": 5, "text": "\\begin{equation}\\label{updatetheta}\n\t\\eta_{k}^{(t+1)}\\!=\\!\\frac{N_{k}}{p_{k}\\sum_{i,j\\in \\Omega}\\gamma_{ijk}^{(t+1)}|y_{ij}\\!-\\!\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T}|^{p_{k}}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\eta_{k}^{(t+1)}\\!=\\!\\frac{N_{k}}{p_{k}\\sum_{i,j\\in\\Omega}\\gamma_{ijk}^{(t+1)}%&#10;|y_{ij}\\!-\\!\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T}|^{p_{k}}},\" display=\"block\"><mrow><mrow><mpadded width=\"-1.7pt\"><msubsup><mi>\u03b7</mi><mi>k</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup></mpadded><mo rspace=\"0.8pt\">=</mo><mfrac><msub><mi>N</mi><mi>k</mi></msub><mrow><msub><mi>p</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow></msub><mrow><msubsup><mi>\u03b3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mpadded width=\"-1.7pt\"><msub><mi>y</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mpadded><mo rspace=\"0.8pt\">-</mo><mrow><msubsup><mi>\ud835\udc2e</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc2f</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><msub><mi>p</mi><mi>k</mi></msub></msup></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere the element $w_{(k)ij}$ of $\\mathbf{W}_{(k)}\\in \\mathcal{R}^{m\\times n}(k=1,\\dots,K)$ is\\\\\n\\begin{displaymath}\n\tw_{(k)ij}=\\begin{cases}(\\eta_{k}^{(t+1)}\\gamma_{ijk}^{(t+1)})^{\\frac{1}{p_{k}}}, \\quad i,j\\in\\Omega\\\\~~~~~~~~~~0, ~~~~~~~~~~~ i,j\\notin\\Omega \\end{cases}.\n\\end{displaymath}\nTo solve (\\ref{subproblem_uv}), we resort to augmented Lagrange multipliers (ALM) method. By introducing auxiliary variable $\\mathbf{L}=\\mathbf{U}\\mathbf{V}^{T}$, (\\ref{subproblem_uv}) can be equivalently rewritten as\n\n", "itemtype": "equation", "pos": 21437, "prevtext": "\nwhere $\\hat{D} = \\sum_{k=1}^{K}D_{k} = 2K$, $N_{k} = \\sum_{i,j\\in \\Omega}\\gamma_{ijk}^{(t+1)}$ and $|\\Omega|$ is the number of non-missing elements. To update $\\mathbf{U}, \\mathbf{V}$, we need to maximize the following function:\n\\begin{eqnarray}\\label{update_s}\n\t-\\sum_{i,j\\in \\Omega}\\sum_{k=1}^{K}\\gamma_{ijk}^{(t+1)}\n\t\\eta_{k}^{(t+1)}|y_{ij}-\\mathbf{u}_{i}^{(t)}(\\mathbf{v}_{j}^{(t)})^{T}|^{p_k},\n\\end{eqnarray}\nwhich is equivalent to solving\\footnote{The $p$-norm of a matrix is defined as $||\\mathbf{X}||_{p}=(\\sum_{i,j}|x_{ij}|^{p})^{\\frac{1}{p}}$.}\n\n", "index": 7, "text": "\\begin{equation}\\label{subproblem_uv}\n\t\\min_{\\mathbf{U}, \\mathbf{V}}\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\odot (\\mathbf{Y}-\\mathbf{U}\\mathbf{V}^{T})||_{p_{k}}^{p_{k}},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{U},\\mathbf{V}}\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\odot(\\mathbf{Y}-%&#10;\\mathbf{U}\\mathbf{V}^{T})||_{p_{k}}^{p_{k}},\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc14</mi><mo>,</mo><mi>\ud835\udc15</mi></mrow></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2299</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc18</mi><mo>-</mo><msup><mi>\ud835\udc14\ud835\udc15</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><msub><mi>p</mi><mi>k</mi></msub><msub><mi>p</mi><mi>k</mi></msub></msubsup></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nThe augmented Lagrangian function can be written as:\n\\vspace{-2pt}\n\n", "itemtype": "equation", "pos": 22124, "prevtext": "\nwhere the element $w_{(k)ij}$ of $\\mathbf{W}_{(k)}\\in \\mathcal{R}^{m\\times n}(k=1,\\dots,K)$ is\\\\\n\\begin{displaymath}\n\tw_{(k)ij}=\\begin{cases}(\\eta_{k}^{(t+1)}\\gamma_{ijk}^{(t+1)})^{\\frac{1}{p_{k}}}, \\quad i,j\\in\\Omega\\\\~~~~~~~~~~0, ~~~~~~~~~~~ i,j\\notin\\Omega \\end{cases}.\n\\end{displaymath}\nTo solve (\\ref{subproblem_uv}), we resort to augmented Lagrange multipliers (ALM) method. By introducing auxiliary variable $\\mathbf{L}=\\mathbf{U}\\mathbf{V}^{T}$, (\\ref{subproblem_uv}) can be equivalently rewritten as\n\n", "index": 9, "text": "\\begin{equation}\\label{subproblem_uv2}\n\t\\begin{split}\n\t\t&\\min_{\\mathbf{U},\\mathbf{V}}\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\odot (\\mathbf{Y}-\\mathbf{L})||_{p_{k}}^{p_{k}},\n\t\t\\quad s.t~\\mathbf{L} = \\mathbf{U}\\mathbf{V}^{T}.\n\t\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle\\min_{\\mathbf{U},\\mathbf{V}}\\sum_{k=1}^{K}||\\mathbf%&#10;{W}_{(k)}\\odot(\\mathbf{Y}-\\mathbf{L})||_{p_{k}}^{p_{k}},\\quad s.t~{}\\mathbf{L}%&#10;=\\mathbf{U}\\mathbf{V}^{T}.\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc14</mi><mo>,</mo><mi>\ud835\udc15</mi></mrow></munder><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2299</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc18</mi><mo>-</mo><mi>\ud835\udc0b</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><msub><mi>p</mi><mi>k</mi></msub><msub><mi>p</mi><mi>k</mi></msub></msubsup></mrow></mrow><mo rspace=\"12.5pt\">,</mo><mi>s</mi></mrow><mo>.</mo><mrow><mrow><mpadded width=\"+3.3pt\"><mi>t</mi></mpadded><mo>\u2062</mo><mi>\ud835\udc0b</mi></mrow><mo>=</mo><msup><mi>\ud835\udc14\ud835\udc15</mi><mi>T</mi></msup></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $\\mathbf{\\Lambda}\\in \\mathcal{R}^{m\\times n}$ is the Lagrange multiplier and $\\rho$ is a positive scalar. Then the optimization (\\ref{subproblem_uv2}) can be solved by alternatively updating all involved variables and multipliers as follows\n\\begin{eqnarray}\\label{optProcess}\n\\begin{cases}\n\t&\\!\\left(\\mathbf{U}^{(s+1)},\\mathbf{V}^{(s+1)}\\right)\\!=\\! \\underset{\\mathbf{U},\\mathbf{V}}{\\arg\\min} L(\\mathbf{U},\\mathbf{V},\\mathbf{L}^{(s)},\\mathbf{\\Lambda}^{(s)},\\rho^{(s)}),\\\\\n\t&\\!\\mathbf{L}^{(s+1)}\\!=\\! \\underset{\\mathbf{L}}{\\arg\\min} L(\\mathbf{U}^{(s+1)},\\mathbf{V}^{(s+1)},\\mathbf{L},\\mathbf{\\Lambda}^{(s)},\\rho^{(s)}),\\\\\n\t&\\!\\mathbf{\\Lambda}^{(s+1)}\\!=\\!\\mathbf{\\Lambda}^{(s)} + \\rho^{(s)}(\\mathbf{L}^{(s+1)}\\!-\\!\\mathbf{U}^{(s+1)}(\\mathbf{V}^{(s+1)})^{T}),\\label{alg1:eq3}\\\\\n\t&\\!\\rho^{(s+1)} \\!=\\! \\alpha\\rho^{(s)}\\label{alg1:eq4},\n\\end{cases}\n\\end{eqnarray}\nwhere $\\alpha$ is a preset constant which is slightly larger than 1, guaranteeing the gradually increasing value for $\\rho$ in each iteration. Now we discuss how to solve the subproblems involved in the above procedure.\n\n(1) \\textit{Update} $\\mathbf{U},\\mathbf{V}$. The following subproblem needs to be solved:\n\n", "itemtype": "equation", "pos": 22436, "prevtext": "\nThe augmented Lagrangian function can be written as:\n\\vspace{-2pt}\n\n", "index": 11, "text": "\\begin{equation}\\label{lagrangefunc}\n\t\\begin{split}\n\t\tL(\\mathbf{U},\\mathbf{V},\\mathbf{L},\\mathbf{Y},\\rho)&=\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\odot (\\mathbf{Y}\\!-\\!\\mathbf{L})||_{p_{k}}^{p_{k}}\\\\\n\t\t&+\\langle\\mathbf{\\Lambda},\\mathbf{L}\\!-\\!\\mathbf{U}\\mathbf{V}^{T}\\rangle+\\frac{\\rho}{2}||\\mathbf{L}\\!-\\!\\mathbf{U}\\mathbf{V}^{T}||_{F}^{2},\n\t\\end{split}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle L(\\mathbf{U},\\mathbf{V},\\mathbf{L},\\mathbf{Y},\\rho)%&#10;&amp;\\displaystyle=\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\odot(\\mathbf{Y}\\!-\\!\\mathbf{L}%&#10;)||_{p_{k}}^{p_{k}}\\\\&#10;&amp;\\displaystyle+\\langle\\mathbf{\\Lambda},\\mathbf{L}\\!-\\!\\mathbf{U}\\mathbf{V}^{T}%&#10;\\rangle+\\frac{\\rho}{2}||\\mathbf{L}\\!-\\!\\mathbf{U}\\mathbf{V}^{T}||_{F}^{2},\\end%&#10;{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>L</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc14</mi><mo>,</mo><mi>\ud835\udc15</mi><mo>,</mo><mi>\ud835\udc0b</mi><mo>,</mo><mi>\ud835\udc18</mi><mo>,</mo><mi>\u03c1</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>\u2299</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><mi>\ud835\udc18</mi></mpadded><mo rspace=\"0.8pt\">-</mo><mi>\ud835\udc0b</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><msub><mi>p</mi><mi>k</mi></msub><msub><mi>p</mi><mi>k</mi></msub></msubsup></mrow></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mo>+</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\ud835\udeb2</mi><mo>,</mo><mrow><mpadded width=\"-1.7pt\"><mi>\ud835\udc0b</mi></mpadded><mo rspace=\"0.8pt\">-</mo><msup><mi>\ud835\udc14\ud835\udc15</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">\u27e9</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mi>\u03c1</mi><mn>2</mn></mfrac><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mpadded width=\"-1.7pt\"><mi>\ud835\udc0b</mi></mpadded><mo rspace=\"0.8pt\">-</mo><msup><mi>\ud835\udc14\ud835\udc15</mi><mi>T</mi></msup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhich can be accurately and efficiently solved by the SVD method.\n\n(2) \\textit{Update} $\\mathbf{L}$. We need to solve the following problem:\n\\begin{eqnarray}\\label{subproblemL}\n\t\\begin{split}\n\t\t&\\min_{\\mathbf{L}}\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\!\\odot\\! (\\mathbf{Y}\\!-\\!\\mathbf{L})||_{p_{k}}^{p_{k}}\\!+\\!\\langle\\mathbf{\\Lambda}^{(s)},\\mathbf{L}\\rangle\\\\\n\t\t&+\\frac{\\rho^{(s)}}{2}||\\mathbf{L}-\\mathbf{U}^{(s+1)}(\\mathbf{V}^{(s+1)})^{T}||_{F}^{2}.\n\t\\end{split}\n\\end{eqnarray}\nThis problem seems to be more difficult due to its non-convexity and non-smoothness. However, we can divide it into $mn$ independent scalar optimization problems as follows:\n\\begin{eqnarray}\\label{updateE}\n\t\\begin{cases}\n\t\t\\begin{split}\n\t\t\t&\\min_{l_{ij}}\\sum_{k}\\eta_{k}\\gamma_{ijk}|y_{ij}-l_{ij}|^{p_{k}}\n\t\t\t+\\frac{\\rho^{(s)}}{2}l_{ij}^{2}\\\\\n\t\t\t&~~~~~~~~~+((\\mathbf{\\Lambda}_{ij}^{(s)})-\\rho^{(s)}\\mathbf{u}_{i}\\mathbf{v}_{j}^{T})l_{ij},~~~~~~~~(i,j)\\in\\Omega\\\\\n\t\t\t&\\underset{l_{ij}}{\\min}\\frac{\\rho^{(s)}}{2}l_{ij}^{2}+ ((\\mathbf{\\Lambda}^{(s)})_{ij}-\\rho^{(s)}\\mathbf{u}_{i}\\mathbf{v}_{j}^{T})l_{ij}.~~~(i,j)\\notin\\Omega\n\t\t\\end{split}\n\t\\end{cases}\n\\end{eqnarray}\nLetting $s_{ij} = y_{ij}-l_{ij}$, (\\ref{updateE}) is equivalent to\n\\begin{eqnarray}\\label{sij}\n\t\\begin{cases}\n\t\t&\\underset{s_{ij}}{\\min}\\frac{1}{2}(t_{ij}-s_{ij})^{2}+ \\frac{1}{\\rho^{(s)}}\\sum_{l}\\eta_{l}\\gamma_{ijl}|s_{ij}|^{p_{l}},~(i,j)\\in\\Omega\\\\\n\t\t&\\underset{s_{ij}}{\\min}\\frac{1}{2}(t_{ij}-s_{ij})^{2},~(i,j)\\notin\\Omega\n\t\\end{cases}\n\\end{eqnarray}\nwhere $t_{ij}=-\\mathbf{u}_{i}\\mathbf{v}_{j}^{T}+y_{ij}+\\frac{1}{\\rho^{(s)}}(\\mathbf{\\Lambda}_{ij}^{(s)})$. Then,  for each $(i,j)\\in\\Omega$, (\\ref{sij}) is equivalent to the following subproblem:\n\\begin{eqnarray}\\label{subproblem_e}\n\t\\min_{s_{ij}} \\frac{1}{2}(t_{ij}-s_{ij})^{2} + \\frac{1}{\\rho}\\sum_{l=1}^{K}\\eta_{l}\\gamma_{ijl}|s_{ij}|^{p_{l}}.\n\\end{eqnarray}\nThis problem requires to optimize a scalar variable, and we take its first derivative with respect to $s_{ij}$ and then adopt the well-known Newton method to easily approach a local minimum of it. The procedure of updating $\\mathbf{L}$ by ALM method can then be listed in Algorithm~\\ref{alg1}.\n\\begin{algorithm}[H]\n\t\\caption{ALM method for solving~(\\ref{subproblem_uv}).}\n\t\\label{alg1}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE The initialization of $\\mathbf{L}^{(0)}$, $\\mathbf{\\Lambda}^{(0)}$ and $s=0$.\n\t\t\\ENSURE $\\mathbf{U}$ and $\\mathbf{V}$.\n\t\t\\WHILE{ not converged }\n\t\t\\STATE Updating $\\mathbf{U}^{(s+1)}$ and  $\\mathbf{V}^{(s+1)}$ via Eq.~(\\ref{uvupdate});\n\t\t\\STATE Updating $\\mathbf{L}^{(s+1)}$ via Eqs.~(\\ref{sij}) and (\\ref{subproblem_e}).\n\t\t\\STATE Updating $\\mathbf{\\Lambda}^{(s+1)}$ via Eq.~(\\ref{alg1:eq3}).\n\t\t\\STATE Updating $\\alpha^{(s+1)}$ via Eq.~(\\ref{alg1:eq4}).\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n{\\bf{Remark:}} If $f_{k}$ is specified as the density of a Gaussian distribution, the PMoEP model degenerates to the penalized MoG (PMoG) model. The optimization process of the PMoG model is almost the same as the PMoEP except the minimization form of (\\ref{subproblem_uv}). In this case, the optimization problem (18) has the following form\n\\begin{eqnarray}\\label{PMoG_dif}\n\t\\min_{\\mathbf{U}, \\mathbf{V}}||\\mathbf{\\tilde{W}}\\odot (\\mathbf{Y}-\\mathbf{U}\\mathbf{V}^{T})||_{2}^{2},\n\\end{eqnarray}\nand then any off-the-shelf weighted $L_2$ norm LRMF method can be adopted to solve it. It should be noted that the PMoG method so conducted is different from the previous MoG method~\\cite{meng2013robust} due to its augmented automatic mixture-component-number learning capability.\n\nThe proposed EM algorithm for PMoEP model can now be summarized in Algorithm~\\ref{alg2}.\n\n\\begin{algorithm}[H]\n\t\\caption{EM Algorithm for PMoEP LRMF.} \\label{alg2}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Data $\\mathbf{Y}$; The algorithm parameters: rank $r$ and $\\lambda$.\n\t\t\\ENSURE Parameter $\\mathbf{\\Theta}$, the number of mixture components $K_{final}$ and posterior probability $\\boldsymbol{\\gamma}=(\\gamma_{ijk})_{m\\times n\\times K_{final}}$.\n\t\t\\REQUIRE $\\mathbf{\\Theta}^{(t)}\\!=\\!\\{\\boldsymbol{\\pi}^{(t)},\\boldsymbol{\\eta}^{(t)}, \\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}\\}$, the number of initial mixture components $K_{start}$, preset candidates $\\mathbf{p}=[p_1,\\dots,p_{K_{start}}]$, tolerance $\\epsilon$ and $t=0$.\n\t\t\\WHILE { not converged }\n\t\t\\STATE Updating $\\boldsymbol{\\gamma}^{(t)}$ via Eq.~\\eqref{updategamma};\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\pi}^{(t)}$ via Eq.~\\eqref{updatepi}, and removing the component with $\\pi_{k}^{(t)}=0$;\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\eta}^{(t)}$ via Eq.~\\eqref{updatetheta};\\\\\n\t\t\\STATE Updating $\\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}$ via Algorithm~\\ref{alg1}.\\\\\n\t\t\\STATE $t = t + 1;$\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Convergence Analysis of EM algorithm}\nIn this subsection, we show the convergence property of the proposed EM algorithm for PMoEP model.\n\n\\begin{theorem}\\label{theorem1}\n\tLet $l_{P}^{C}(\\mathbf{\\Theta}) = l(\\mathbf{\\Theta})-P(\\boldsymbol{\\pi};\\lambda)$, where $l(\\Theta)$ is defined in (\\ref{loglikelihood}). If we assume that $\\{\\mathbf{\\Theta}^{(t)}\\}$ is the sequence generated by Algorithm \\ref{alg2} and the sequence of likelihood values $\\{l_{P}^{C}(\\mathbf{\\Theta}^{(t)})\\}$ is bounded above, then there exits a constant $l^{\\star}$ such that\n\t\n", "itemtype": "equation", "pos": 23976, "prevtext": "\nwhere $\\mathbf{\\Lambda}\\in \\mathcal{R}^{m\\times n}$ is the Lagrange multiplier and $\\rho$ is a positive scalar. Then the optimization (\\ref{subproblem_uv2}) can be solved by alternatively updating all involved variables and multipliers as follows\n\\begin{eqnarray}\\label{optProcess}\n\\begin{cases}\n\t&\\!\\left(\\mathbf{U}^{(s+1)},\\mathbf{V}^{(s+1)}\\right)\\!=\\! \\underset{\\mathbf{U},\\mathbf{V}}{\\arg\\min} L(\\mathbf{U},\\mathbf{V},\\mathbf{L}^{(s)},\\mathbf{\\Lambda}^{(s)},\\rho^{(s)}),\\\\\n\t&\\!\\mathbf{L}^{(s+1)}\\!=\\! \\underset{\\mathbf{L}}{\\arg\\min} L(\\mathbf{U}^{(s+1)},\\mathbf{V}^{(s+1)},\\mathbf{L},\\mathbf{\\Lambda}^{(s)},\\rho^{(s)}),\\\\\n\t&\\!\\mathbf{\\Lambda}^{(s+1)}\\!=\\!\\mathbf{\\Lambda}^{(s)} + \\rho^{(s)}(\\mathbf{L}^{(s+1)}\\!-\\!\\mathbf{U}^{(s+1)}(\\mathbf{V}^{(s+1)})^{T}),\\label{alg1:eq3}\\\\\n\t&\\!\\rho^{(s+1)} \\!=\\! \\alpha\\rho^{(s)}\\label{alg1:eq4},\n\\end{cases}\n\\end{eqnarray}\nwhere $\\alpha$ is a preset constant which is slightly larger than 1, guaranteeing the gradually increasing value for $\\rho$ in each iteration. Now we discuss how to solve the subproblems involved in the above procedure.\n\n(1) \\textit{Update} $\\mathbf{U},\\mathbf{V}$. The following subproblem needs to be solved:\n\n", "index": 13, "text": "\\begin{equation}\\label{uvupdate}\n\t\\min_{\\mathbf{U},\\mathbf{V}}||\\mathbf{L}^{(s)}+\\frac{1}{\\rho^{(s)}}\\mathbf{\\Lambda}^{(s)}-\\mathbf{U}\\mathbf{V}^{T}||_{F}^{2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{U},\\mathbf{V}}||\\mathbf{L}^{(s)}+\\frac{1}{\\rho^{(s)}}\\mathbf{%&#10;\\Lambda}^{(s)}-\\mathbf{U}\\mathbf{V}^{T}||_{F}^{2},\" display=\"block\"><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc14</mi><mo>,</mo><mi>\ud835\udc15</mi></mrow></munder><mo>\u2062</mo><msubsup><mrow><mo fence=\"true\">||</mo><mrow><mrow><msup><mi>\ud835\udc0b</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>+</mo><mrow><mfrac><mn>1</mn><msup><mi>\u03c1</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></msup></mfrac><mo>\u2062</mo><msup><mi>\ud835\udeb2</mi><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo>-</mo><msup><mi>\ud835\udc14\ud835\udc15</mi><mi>T</mi></msup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\twhere\n\t\n", "itemtype": "equation", "pos": 29439, "prevtext": "\nwhich can be accurately and efficiently solved by the SVD method.\n\n(2) \\textit{Update} $\\mathbf{L}$. We need to solve the following problem:\n\\begin{eqnarray}\\label{subproblemL}\n\t\\begin{split}\n\t\t&\\min_{\\mathbf{L}}\\sum_{k=1}^{K}||\\mathbf{W}_{(k)}\\!\\odot\\! (\\mathbf{Y}\\!-\\!\\mathbf{L})||_{p_{k}}^{p_{k}}\\!+\\!\\langle\\mathbf{\\Lambda}^{(s)},\\mathbf{L}\\rangle\\\\\n\t\t&+\\frac{\\rho^{(s)}}{2}||\\mathbf{L}-\\mathbf{U}^{(s+1)}(\\mathbf{V}^{(s+1)})^{T}||_{F}^{2}.\n\t\\end{split}\n\\end{eqnarray}\nThis problem seems to be more difficult due to its non-convexity and non-smoothness. However, we can divide it into $mn$ independent scalar optimization problems as follows:\n\\begin{eqnarray}\\label{updateE}\n\t\\begin{cases}\n\t\t\\begin{split}\n\t\t\t&\\min_{l_{ij}}\\sum_{k}\\eta_{k}\\gamma_{ijk}|y_{ij}-l_{ij}|^{p_{k}}\n\t\t\t+\\frac{\\rho^{(s)}}{2}l_{ij}^{2}\\\\\n\t\t\t&~~~~~~~~~+((\\mathbf{\\Lambda}_{ij}^{(s)})-\\rho^{(s)}\\mathbf{u}_{i}\\mathbf{v}_{j}^{T})l_{ij},~~~~~~~~(i,j)\\in\\Omega\\\\\n\t\t\t&\\underset{l_{ij}}{\\min}\\frac{\\rho^{(s)}}{2}l_{ij}^{2}+ ((\\mathbf{\\Lambda}^{(s)})_{ij}-\\rho^{(s)}\\mathbf{u}_{i}\\mathbf{v}_{j}^{T})l_{ij}.~~~(i,j)\\notin\\Omega\n\t\t\\end{split}\n\t\\end{cases}\n\\end{eqnarray}\nLetting $s_{ij} = y_{ij}-l_{ij}$, (\\ref{updateE}) is equivalent to\n\\begin{eqnarray}\\label{sij}\n\t\\begin{cases}\n\t\t&\\underset{s_{ij}}{\\min}\\frac{1}{2}(t_{ij}-s_{ij})^{2}+ \\frac{1}{\\rho^{(s)}}\\sum_{l}\\eta_{l}\\gamma_{ijl}|s_{ij}|^{p_{l}},~(i,j)\\in\\Omega\\\\\n\t\t&\\underset{s_{ij}}{\\min}\\frac{1}{2}(t_{ij}-s_{ij})^{2},~(i,j)\\notin\\Omega\n\t\\end{cases}\n\\end{eqnarray}\nwhere $t_{ij}=-\\mathbf{u}_{i}\\mathbf{v}_{j}^{T}+y_{ij}+\\frac{1}{\\rho^{(s)}}(\\mathbf{\\Lambda}_{ij}^{(s)})$. Then,  for each $(i,j)\\in\\Omega$, (\\ref{sij}) is equivalent to the following subproblem:\n\\begin{eqnarray}\\label{subproblem_e}\n\t\\min_{s_{ij}} \\frac{1}{2}(t_{ij}-s_{ij})^{2} + \\frac{1}{\\rho}\\sum_{l=1}^{K}\\eta_{l}\\gamma_{ijl}|s_{ij}|^{p_{l}}.\n\\end{eqnarray}\nThis problem requires to optimize a scalar variable, and we take its first derivative with respect to $s_{ij}$ and then adopt the well-known Newton method to easily approach a local minimum of it. The procedure of updating $\\mathbf{L}$ by ALM method can then be listed in Algorithm~\\ref{alg1}.\n\\begin{algorithm}[H]\n\t\\caption{ALM method for solving~(\\ref{subproblem_uv}).}\n\t\\label{alg1}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE The initialization of $\\mathbf{L}^{(0)}$, $\\mathbf{\\Lambda}^{(0)}$ and $s=0$.\n\t\t\\ENSURE $\\mathbf{U}$ and $\\mathbf{V}$.\n\t\t\\WHILE{ not converged }\n\t\t\\STATE Updating $\\mathbf{U}^{(s+1)}$ and  $\\mathbf{V}^{(s+1)}$ via Eq.~(\\ref{uvupdate});\n\t\t\\STATE Updating $\\mathbf{L}^{(s+1)}$ via Eqs.~(\\ref{sij}) and (\\ref{subproblem_e}).\n\t\t\\STATE Updating $\\mathbf{\\Lambda}^{(s+1)}$ via Eq.~(\\ref{alg1:eq3}).\n\t\t\\STATE Updating $\\alpha^{(s+1)}$ via Eq.~(\\ref{alg1:eq4}).\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n{\\bf{Remark:}} If $f_{k}$ is specified as the density of a Gaussian distribution, the PMoEP model degenerates to the penalized MoG (PMoG) model. The optimization process of the PMoG model is almost the same as the PMoEP except the minimization form of (\\ref{subproblem_uv}). In this case, the optimization problem (18) has the following form\n\\begin{eqnarray}\\label{PMoG_dif}\n\t\\min_{\\mathbf{U}, \\mathbf{V}}||\\mathbf{\\tilde{W}}\\odot (\\mathbf{Y}-\\mathbf{U}\\mathbf{V}^{T})||_{2}^{2},\n\\end{eqnarray}\nand then any off-the-shelf weighted $L_2$ norm LRMF method can be adopted to solve it. It should be noted that the PMoG method so conducted is different from the previous MoG method~\\cite{meng2013robust} due to its augmented automatic mixture-component-number learning capability.\n\nThe proposed EM algorithm for PMoEP model can now be summarized in Algorithm~\\ref{alg2}.\n\n\\begin{algorithm}[H]\n\t\\caption{EM Algorithm for PMoEP LRMF.} \\label{alg2}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Data $\\mathbf{Y}$; The algorithm parameters: rank $r$ and $\\lambda$.\n\t\t\\ENSURE Parameter $\\mathbf{\\Theta}$, the number of mixture components $K_{final}$ and posterior probability $\\boldsymbol{\\gamma}=(\\gamma_{ijk})_{m\\times n\\times K_{final}}$.\n\t\t\\REQUIRE $\\mathbf{\\Theta}^{(t)}\\!=\\!\\{\\boldsymbol{\\pi}^{(t)},\\boldsymbol{\\eta}^{(t)}, \\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}\\}$, the number of initial mixture components $K_{start}$, preset candidates $\\mathbf{p}=[p_1,\\dots,p_{K_{start}}]$, tolerance $\\epsilon$ and $t=0$.\n\t\t\\WHILE { not converged }\n\t\t\\STATE Updating $\\boldsymbol{\\gamma}^{(t)}$ via Eq.~\\eqref{updategamma};\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\pi}^{(t)}$ via Eq.~\\eqref{updatepi}, and removing the component with $\\pi_{k}^{(t)}=0$;\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\eta}^{(t)}$ via Eq.~\\eqref{updatetheta};\\\\\n\t\t\\STATE Updating $\\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}$ via Algorithm~\\ref{alg1}.\\\\\n\t\t\\STATE $t = t + 1;$\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Convergence Analysis of EM algorithm}\nIn this subsection, we show the convergence property of the proposed EM algorithm for PMoEP model.\n\n\\begin{theorem}\\label{theorem1}\n\tLet $l_{P}^{C}(\\mathbf{\\Theta}) = l(\\mathbf{\\Theta})-P(\\boldsymbol{\\pi};\\lambda)$, where $l(\\Theta)$ is defined in (\\ref{loglikelihood}). If we assume that $\\{\\mathbf{\\Theta}^{(t)}\\}$ is the sequence generated by Algorithm \\ref{alg2} and the sequence of likelihood values $\\{l_{P}^{C}(\\mathbf{\\Theta}^{(t)})\\}$ is bounded above, then there exits a constant $l^{\\star}$ such that\n\t\n", "index": 15, "text": "\\begin{equation}\n\t\t\\lim_{t\\rightarrow \\infty}l_{P}^{C}(\\mathbf{\\Theta}^{(t)}) = l^{\\star},\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\lim_{t\\rightarrow\\infty}l_{P}^{C}(\\mathbf{\\Theta}^{(t)})=l^{\\star},\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>t</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><msup><mi>l</mi><mo>\u22c6</mo></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\tand\n\t\n", "itemtype": "equation", "pos": 29554, "prevtext": "\n\twhere\n\t\n", "index": 17, "text": "\\begin{equation}\n\t\t \\mathbf{\\Theta}^{(t)}\\!=\\!\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\left\\{\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t-1)})\\!+\\!P(\\boldsymbol{\\pi}^{(t-1)};\\lambda)\\!-\\!P(\\boldsymbol{\\pi};\\lambda)\\right\\},\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\Theta}^{(t)}\\!=\\!\\underset{\\mathbf{\\Theta}}{\\arg\\max}\\left\\{\\Omega(%&#10;\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t-1)})\\!+\\!P(\\boldsymbol{\\pi}^{(t-1)};%&#10;\\lambda)\\!-\\!P(\\boldsymbol{\\pi};\\lambda)\\right\\},\" display=\"block\"><mrow><mpadded width=\"-1.7pt\"><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mpadded><mo rspace=\"0.8pt\">=</mo><munder accentunder=\"true\"><mrow><mi>arg</mi><mo>\u2061</mo><mi>max</mi></mrow><mo>\ud835\udeaf</mo></munder><mrow><mo>{</mo><mi mathvariant=\"normal\">\u03a9</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.8pt\">+</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udf45</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><mi>\u03bb</mi><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.8pt\">-</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udf45</mi><mo>;</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>}</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\\end{theorem}\nThe proof is listed in Appendix A.\n\n\\subsection{Implementation Issues}\nIn the proposed PMoEP algorithm, there are three involved preset parameters, $K_{start}$, $p$ and $\\lambda$. Throughout all our experiments, we just simply set $K_{start}$ as a not large number as $4-10$ based on a coarse empirical estimate on the noise complexity inside data. Once $K_{start}$ is initialized, the length of vector $\\mathbf{p}=[p_{1},p_{2},\\dots,p_{K_{start}}]$ in PMoEP is determined. In all our experiments, the elements in $\\mathbf{p}$ are selected ranging over the interval between 0.1 and 2. For the setting of parameter $\\lambda$ , we first provide a series of candidates $\\lambda$ and then adopt the modified BIC to select a good $\\lambda$ among these candidates based on the modified BIC criterion. This criterion has been proven to be able to yield consistent component number estimation of the finite Gaussian mixture model~\\cite{huang2013model}. Specifically, the modified BIC criterion is defined as\n\n", "itemtype": "equation", "pos": 29794, "prevtext": "\n\tand\n\t\n", "index": 19, "text": "\\begin{equation}\n\t\t\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t-1)})\\!=\\!\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t-1)})\n\t\t\\log{\\frac{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta}^{(t-1)})}}.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t-1)})\\!=\\!\\sum_{\\mathbf{Z}}\\mathbb{P%&#10;}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t-1)})\\log{\\frac{\\mathbb{P}(\\mathbf{%&#10;E},\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{%&#10;\\Theta}^{(t-1)})}}.\" display=\"block\"><mrow><mi mathvariant=\"normal\">\u03a9</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.8pt\">=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>\ud835\udc19</mi></munder><mi>\u2119</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc19</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc04</mi><mo>;</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mi>log</mi><mfrac><mrow><mi>\u2119</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo>,</mo><mi>\ud835\udc19</mi><mo>;</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>\u2119</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo>,</mo><mi>\ud835\udc19</mi><mo>;</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nThen we can select the proper $\\hat{\\lambda}$ by\n\n", "itemtype": "equation", "pos": 31095, "prevtext": "\n\\end{theorem}\nThe proof is listed in Appendix A.\n\n\\subsection{Implementation Issues}\nIn the proposed PMoEP algorithm, there are three involved preset parameters, $K_{start}$, $p$ and $\\lambda$. Throughout all our experiments, we just simply set $K_{start}$ as a not large number as $4-10$ based on a coarse empirical estimate on the noise complexity inside data. Once $K_{start}$ is initialized, the length of vector $\\mathbf{p}=[p_{1},p_{2},\\dots,p_{K_{start}}]$ in PMoEP is determined. In all our experiments, the elements in $\\mathbf{p}$ are selected ranging over the interval between 0.1 and 2. For the setting of parameter $\\lambda$ , we first provide a series of candidates $\\lambda$ and then adopt the modified BIC to select a good $\\lambda$ among these candidates based on the modified BIC criterion. This criterion has been proven to be able to yield consistent component number estimation of the finite Gaussian mixture model~\\cite{huang2013model}. Specifically, the modified BIC criterion is defined as\n\n", "index": 21, "text": "\\begin{equation}\\label{bic}\n\\mbox{BIC}(\\lambda)\\!=\\! \\sum_{i,j\\in\\Omega}\\log{\\{\\sum_{k=1}^{\\hat{K}}\\hat{\\pi}_{k}f_{k}(e_{ij};\\hat{\\eta}_{k})\\}}\\!-\\! \\frac{1}{2}(\\sum_{k=1}^{\\hat{K}}D_{k})\\log{|\\Omega|}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\mbox{BIC}(\\lambda)\\!=\\!\\sum_{i,j\\in\\Omega}\\log{\\{\\sum_{k=1}^{\\hat{K}}\\hat{\\pi%&#10;}_{k}f_{k}(e_{ij};\\hat{\\eta}_{k})\\}}\\!-\\!\\frac{1}{2}(\\sum_{k=1}^{\\hat{K}}D_{k}%&#10;)\\log{|\\Omega|}.\" display=\"block\"><mrow><mrow><mrow><mtext>BIC</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo rspace=\"0.8pt\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.8pt\">=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>\u2208</mo><mi mathvariant=\"normal\">\u03a9</mi></mrow></munder><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mover accent=\"true\"><mi>K</mi><mo stretchy=\"false\">^</mo></mover></munderover><mrow><msub><mover accent=\"true\"><mi>\u03c0</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo>\u2062</mo><msub><mi>f</mi><mi>k</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>;</mo><msub><mover accent=\"true\"><mi>\u03b7</mi><mo stretchy=\"false\">^</mo></mover><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"0.8pt\" stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"0.8pt\">-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mover accent=\"true\"><mi>K</mi><mo stretchy=\"false\">^</mo></mover></munderover><msub><mi>D</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u03a9</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $|\\Omega|$ is the number of non-missing elements, $\\hat{K}$ is the estimate of the number of components, $\\hat{\\pi}_{k}$ is the estimate of parameter $\\pi_{k}$, and $\\hat{\\eta}_{k}$ is the estimate of parameter $\\eta_{k}$ for maximizing (\\ref{PMoEPmodel}) for a given $\\lambda$.\n\n\\section{PMoEP with Markov Random Field}\nIn this section, we first propose an advanced PMoEP-MRF model. Then, we introduce a variational EM (VEM) algorithm to solve it. Finally, we also show the convergence analysis for the proposed algorithm.\n\\subsection{PMoEP-MRF Model}\nIn some practical applications, we often have certain noise prior knowledge. By introducing the prior into modeling, noise can be more appropriately modeled and thus the performance of the model is expected to be further improved. In video data, we can utilize the spatial and temporal smoothness prior. Specifically, for a certain pixel in one video frame, the pixels located near it both spatially and temporally tend to have similar distribution to it. Therefore, by facilitating the local continuity of noise components, we can embed Markov Random Field (MRF) into the PMoEP model. Note that the random variable $\\mathbf{z}_{ij}$ determines the cluster label of noise $e_{ij}$ in PMoEP model, and the aforementioned spatial and temporal relationships among adjacent pixels imply that they incline to possess similar $\\mathbf{z}_{ij}$ values. Therefore, we integrate into the distribution of  $\\mathbf{z}_{ij}$ with such prior smoothness knowledge as:\n\n", "itemtype": "equation", "pos": 31362, "prevtext": "\nThen we can select the proper $\\hat{\\lambda}$ by\n\n", "index": 23, "text": "\\begin{equation}\n\\hat{\\lambda} = \\arg\\max_{\\lambda}\\mbox{BIC}(\\lambda),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"\\hat{\\lambda}=\\arg\\max_{\\lambda}\\mbox{BIC}(\\lambda),\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mi>\u03bb</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><mrow><mrow><mi>arg</mi><mo>\u2061</mo><mrow><munder><mi>max</mi><mi>\u03bb</mi></munder><mo>\u2061</mo><mtext>BIC</mtext></mrow></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 32963, "prevtext": "\nwhere $|\\Omega|$ is the number of non-missing elements, $\\hat{K}$ is the estimate of the number of components, $\\hat{\\pi}_{k}$ is the estimate of parameter $\\pi_{k}$, and $\\hat{\\eta}_{k}$ is the estimate of parameter $\\eta_{k}$ for maximizing (\\ref{PMoEPmodel}) for a given $\\lambda$.\n\n\\section{PMoEP with Markov Random Field}\nIn this section, we first propose an advanced PMoEP-MRF model. Then, we introduce a variational EM (VEM) algorithm to solve it. Finally, we also show the convergence analysis for the proposed algorithm.\n\\subsection{PMoEP-MRF Model}\nIn some practical applications, we often have certain noise prior knowledge. By introducing the prior into modeling, noise can be more appropriately modeled and thus the performance of the model is expected to be further improved. In video data, we can utilize the spatial and temporal smoothness prior. Specifically, for a certain pixel in one video frame, the pixels located near it both spatially and temporally tend to have similar distribution to it. Therefore, by facilitating the local continuity of noise components, we can embed Markov Random Field (MRF) into the PMoEP model. Note that the random variable $\\mathbf{z}_{ij}$ determines the cluster label of noise $e_{ij}$ in PMoEP model, and the aforementioned spatial and temporal relationships among adjacent pixels imply that they incline to possess similar $\\mathbf{z}_{ij}$ values. Therefore, we integrate into the distribution of  $\\mathbf{z}_{ij}$ with such prior smoothness knowledge as:\n\n", "index": 25, "text": "\\begin{equation}\n\\mathbf{z}_{ij}\\sim \\mathcal{M}(\\mathbf{z}_{ij};\\boldsymbol{\\pi})\\prod_{(p,q)\\in\\mathcal{N}(i,j)}\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq}),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{z}_{ij}\\sim\\mathcal{M}(\\mathbf{z}_{ij};\\boldsymbol{\\pi})\\prod_{(p,q)%&#10;\\in\\mathcal{N}(i,j)}\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq}),\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc33</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u223c</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>;</mo><mi>\ud835\udf45</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $\\tau$ is a positive scalar parameter (we set $\\tau=10$ in experiments), $C$ is a normalization constant of $\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})$ and $\\mathcal{N}(i,j)$ is the neighborhood of the $(i,j)$ entry.\nSpecifically, when $z_{ijk}$ and $z_{pqk}$ achieve the same value (0 or 1), $\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})$ will have higher value, and thus this term readily encode the expected prior information. After defining the new distribution of $\\mathbf{z}_{ij}$, the  distribution of $\\mathbf{Z}$ can be written as\n\\begin{eqnarray}\n\\begin{split}\n\\mathbb{P}(\\mathbf{Z};\\boldsymbol{\\pi})&=\\frac{1}{C}\\prod_{i,j\\in\\Omega,k}\\pi_{k}^{z_{ijk}}\\\\\n&\\prod_{i,j\\in\\Omega,k}\\prod_{(p,q)\\in \\mathcal{N}(i,j)}\\exp\\left[\\tau(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right].\n\\end{split}\n\\end{eqnarray}\nThen, the \\textit{complete likelihood function} can be written as\n\\begin{eqnarray}\n\\begin{split}\n\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})&=\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\boldsymbol{\\eta})\\mathbb{P}(\\mathbf{Z};\\boldsymbol{\\pi})\\\\\n&= \\frac{1}{C}\\prod_{i,j\\in \\Omega,k}[\\pi_{k}f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};0,\\eta_{k})]^{z_{ijk}}\\\\\n&\\prod_{i,j\\in\\Omega,k}\\prod_{(p,q)\\in \\mathcal{N}(i,j)}\\!\\!\\!\\!\\!\\exp\\!\\left[\\tau(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right],\n\\end{split}\n\\end{eqnarray}\nand the \\textit{complete log-likelihood function} is\n\\begin{eqnarray}\n\\begin{split}\nl^{C}(\\mathbf{\\Theta})&\\!=\\!\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\\\\n&\\!=\\!\\sum_{i,j\\in \\Omega,k}z_{ijk}[\\log{\\pi_{k}}\\!+\\!\\log{f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};0,\\eta_{k})}]\\\\\n\\quad &\\!+\\!\\tau\\sum_{i,j\\in\\Omega,k}\\sum_{(p,q)\\in \\mathcal{N}(i,j)}\\!\\!\\!(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1) \\!+\\!const.\n\\end{split}\n\\end{eqnarray}\nIn the next section, we will introduce a variational EM algorithm to solve this PMoEP-MRF model in detail.\n\\vspace{-5pt}\n\\subsection{Variational EM algorithm for PMoEP-MRF model}\nSince EM requires the computation of conditional distribution $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$ which is not tractable. In such PMoEP-MRF model, we resort to the variational method that aims at optimizing a lower bound of $\\log{\\mathcal{L}(\\mathbf{E})}$, denoted by\n\\begin{eqnarray}\n\\mathcal{J}(R_{\\mathbf{E}}) = \\log{\\mathcal{L}(\\mathbf{E})} - KL[R_{\\mathbf{E}}(\\mathbf{Z}),\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})],\n\\end{eqnarray}\nwhere $KL$ denotes the Kullback$-$Leibler divergence, $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$ is the true conditional distribution of the indicator variables $\\mathbf{Z}$ given $\\mathbf{E}$, and $R_{\\mathbf{E}}(\\mathbf{Z})$ is an approximation of the conditional distribution. $\\mathcal{J}(R_{\\mathbf{E}})$ equals to $\\log{\\mathcal{L}(\\mathbf{E})}$ if and only if $R_{\\mathbf{E}}(\\mathbf{Z})=\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$.\n\nAs shown above, we are not able to calculate $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$,\nso we will look for the best (in terms of $KL$\ndivergence) $R_{\\mathbf{E}}(\\mathbf{Z})$ in a certain class of distributions. Specifically, we constrain the variational distribution $R_{\\mathbf{E}}(\\mathbf{Z})$ to have the following form:\n\\begin{eqnarray}\nR_{\\mathbf{E}}(\\mathbf{Z})&=&\\prod_{i,j}R(\\mathbf{z}_{ij};\\boldsymbol{\\gamma}_{ij}),\n\\end{eqnarray}\nwhere $R(\\mathbf{z}_{ij};\\boldsymbol{\\gamma}_{ij})=\\prod_{ij}\\prod_{k}\\gamma_{ijk}^{z_{ijk}},\\sum_k \\gamma_{ijk} = 1$, and $\\boldsymbol{\\gamma}$ is the variational parameter. Then, the lower bound $\\mathcal{J}(R_{\\mathbf{E}})$ to be maximized can be written as\n\\begin{eqnarray}\\label{lowbound}\n\\begin{split}\n\\mathcal{J}(R_{\\mathbf{E}}) &= E_{R_{\\mathbf{E}}(\\mathbf{Z})}\\{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z})}\\} - E_{R_{\\mathbf{E}}(\\mathbf{Z})}\\{R_{\\mathbf{E}}(\\mathbf{Z})\\},\\\\\n&= \\sum_{i,j\\in\\Omega,k}\\left[\\log{\\pi_{k}+\\log{f_{p_k}(e_{ij};0,\\eta_{k})}}\\right]\\\\\n&+\\tau\\sum_{i,j\\in\\Omega,k}\\sum_{(p,q)\\in \\mathcal{N}(i,j)}(2\\gamma_{ijk}-1)(2\\gamma_{pqk}-1)\\\\\n&-\\sum_{i,j\\in\\Omega,k}\\gamma_{ijk}\\log{\\gamma_{ijk}}+const.\n\\end{split}\n\\end{eqnarray}\nWe can easily adopt alternative search strategy for the maximization problem on $\\mathcal{J}(R_{\\mathbf{E}})$ by alternatively solving the sub-problems: (i) with respect to $R_{\\mathbf{E}}$ and (ii) with respect to parameters $\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}$. The following Proposition~\\ref{pro1} and~\\ref{pro2} provide the solutions of optimization problem (i) and (ii), respectively.\n\\begin{proposition}\\label{pro1}\n\t\\textit{(Variational E-step)}~ Given parameters $\\mathbf{\\Theta}=\\{\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}\\}$, the optimal variational parameters $\\hat{\\gamma}_{ij} = \\underset{\\boldsymbol{\\gamma}}{\\arg\\max}~\\mathcal{J}(R_{\\mathbf{E}})$ satisfy the following fixed point relation:\n\t\n", "itemtype": "equation", "pos": 33137, "prevtext": "\nwhere\n\n", "index": 27, "text": "\\begin{equation}\n\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})=\n\\frac{1}{C}\\prod_{k}\\exp\\left[\\tau(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right],\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})=\\frac{1}{C}\\prod_{k}\\exp\\left[\\tau(2z_{%&#10;ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right],\" display=\"block\"><mrow><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc33</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>\ud835\udc33</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>C</mi></mfrac><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>k</mi></munder><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mi>z</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mpadded></mrow><mo rspace=\"0.8pt\">-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mn>2</mn><mo>\u2062</mo><mpadded width=\"-1.7pt\"><msub><mi>z</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mpadded></mrow><mo rspace=\"0.8pt\">-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\t\n\\end{proposition}\n\\begin{proof}\nBased on (\\ref{lowbound}), we maximize $\\mathcal{J}(R_{\\mathbf{E}})$ with respect to $\\boldsymbol{\\gamma}_{ij}s$, subject to $\\sum_k\\gamma_{ijk}=1$, for all $i,j$, i.e. to maximize $\\mathcal{J}(R_{\\mathbf{E}})+\\sum_{ij}[\\lambda_{ij}(\\sum_{k}\\gamma_{ijk}-1)]$ where $\\lambda_{ij}$ is the Lagrangian multiplier. The derivative with respect to $\\gamma_{ijk}$ is\n\t\\begin{displaymath}\n\t\\log{\\pi_{k}\\!+\\!\\log{f_{p_k}(e_{ij};0,\\eta_{k})}}\\!+\\!\\tau\\!\\!\\!\\!\\sum_{(p,q)\\in \\mathcal{N}(i,j)}\\gamma_{pqk}\\!-\\!\\log{\\gamma_{ijk}}\\!-\\!1\\!+\\!\\lambda_{ij}.\n\t\\end{displaymath}\n\tThis derivative is null iff $\\gamma_{ijk}$ satisfy the relation given in the\n\tproposition, and $\\exp(-1+\\lambda_{ij})$ is the the normalizing constant.\n\\end{proof}\n\\vspace{-1pt}\n\\begin{proposition}\\label{pro2}\n\t\\textit{(Variational M-step)}~ Given the variational parameters $\\boldsymbol{\\gamma}_{ij}s$, the values of parameters $\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}$ that maximize $\\mathcal{J}(R_{\\mathbf{E}})$ can be calculated in the same way as the M step in the EM algorithm of PMoEP model.\n\\end{proposition}\n\nThe proposed variational EM algorithm for PMoEP-MRF model can then be summarized in Algorithm~\\ref{alg3}.\n\n\\begin{algorithm}[H]\n\t\\caption{VEM Algorithm for the PMoEP-MRF Model.} \\label{alg3}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Data $\\mathbf{Y}$, rank $r$, $\\tau$ and  $\\lambda$.\n\t\t\\ENSURE Parameter $\\mathbf{\\Theta}$, mixture components number $K_{final}$ and $\\boldsymbol{\\gamma}=(\\gamma_{ijk})_{m\\times n\\times K_{final}}$.\n\t\t\\REQUIRE $\\mathbf{\\Theta}^{(0)}\\!=\\!\\{\\boldsymbol{\\pi}^{(0)},\\boldsymbol{\\eta}^{(0)}, \\mathbf{U}^{(0)}, \\mathbf{V}^{(0)}\\}$, the initial mixture components number $K_{start}$,  preset candidates $\\mathbf{p}=[p_1,\\dots,p_{K_{start}}]$, tolerance $\\epsilon$ and $t=0$.\n\t\t\\WHILE { not converged }\n\t\t\\STATE Updating $\\boldsymbol{\\gamma}^{(t)}$ via the fixed-point Eq.~\\eqref{updategamma_fixpoint};\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\pi}^{(t)}$ via Eq.~\\eqref{updatepi}, and removing the component with $\\pi_{k}^{(t)}=0$;\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\eta}^{(t)}$ via Eq.~\\eqref{updatetheta};\\\\\n\t\t\\STATE Updating $\\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}$ via Algorithm~\\ref{alg1};\\\\\n\t\t\\STATE $t = t + 1.$\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Convergence Analysis of Variational EM algorithm}\nIn this subsection, we show the convergence property of the proposed EM algorithm for PMoEP model.\n\n\\begin{theorem}\\label{theorem2}\n\tGiven $\\lambda$, Algorithm~\\ref{alg3} generates a sequence $\\{\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\}, \\mathbf{\\Theta}^{(t)}\\}\\}_{t=1}^{\\infty}$ which increases $\\mathcal{J}(R_{\\mathbf{E}})$ such that\n\t\n", "itemtype": "equation", "pos": -1, "prevtext": "\nwhere $\\tau$ is a positive scalar parameter (we set $\\tau=10$ in experiments), $C$ is a normalization constant of $\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})$ and $\\mathcal{N}(i,j)$ is the neighborhood of the $(i,j)$ entry.\nSpecifically, when $z_{ijk}$ and $z_{pqk}$ achieve the same value (0 or 1), $\\psi(\\mathbf{z}_{ij},\\mathbf{z}_{pq})$ will have higher value, and thus this term readily encode the expected prior information. After defining the new distribution of $\\mathbf{z}_{ij}$, the  distribution of $\\mathbf{Z}$ can be written as\n\\begin{eqnarray}\n\\begin{split}\n\\mathbb{P}(\\mathbf{Z};\\boldsymbol{\\pi})&=\\frac{1}{C}\\prod_{i,j\\in\\Omega,k}\\pi_{k}^{z_{ijk}}\\\\\n&\\prod_{i,j\\in\\Omega,k}\\prod_{(p,q)\\in \\mathcal{N}(i,j)}\\exp\\left[\\tau(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right].\n\\end{split}\n\\end{eqnarray}\nThen, the \\textit{complete likelihood function} can be written as\n\\begin{eqnarray}\n\\begin{split}\n\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})&=\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\boldsymbol{\\eta})\\mathbb{P}(\\mathbf{Z};\\boldsymbol{\\pi})\\\\\n&= \\frac{1}{C}\\prod_{i,j\\in \\Omega,k}[\\pi_{k}f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};0,\\eta_{k})]^{z_{ijk}}\\\\\n&\\prod_{i,j\\in\\Omega,k}\\prod_{(p,q)\\in \\mathcal{N}(i,j)}\\!\\!\\!\\!\\!\\exp\\!\\left[\\tau(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1)\\right],\n\\end{split}\n\\end{eqnarray}\nand the \\textit{complete log-likelihood function} is\n\\begin{eqnarray}\n\\begin{split}\nl^{C}(\\mathbf{\\Theta})&\\!=\\!\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\\\\n&\\!=\\!\\sum_{i,j\\in \\Omega,k}z_{ijk}[\\log{\\pi_{k}}\\!+\\!\\log{f_{p_k}(y_{ij}\\!-\\!\\mathbf{u}_{i}\\mathbf{v}_{j}^{T};0,\\eta_{k})}]\\\\\n\\quad &\\!+\\!\\tau\\sum_{i,j\\in\\Omega,k}\\sum_{(p,q)\\in \\mathcal{N}(i,j)}\\!\\!\\!(2z_{ijk}\\!-\\!1)(2z_{pqk}\\!-\\!1) \\!+\\!const.\n\\end{split}\n\\end{eqnarray}\nIn the next section, we will introduce a variational EM algorithm to solve this PMoEP-MRF model in detail.\n\\vspace{-5pt}\n\\subsection{Variational EM algorithm for PMoEP-MRF model}\nSince EM requires the computation of conditional distribution $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$ which is not tractable. In such PMoEP-MRF model, we resort to the variational method that aims at optimizing a lower bound of $\\log{\\mathcal{L}(\\mathbf{E})}$, denoted by\n\\begin{eqnarray}\n\\mathcal{J}(R_{\\mathbf{E}}) = \\log{\\mathcal{L}(\\mathbf{E})} - KL[R_{\\mathbf{E}}(\\mathbf{Z}),\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})],\n\\end{eqnarray}\nwhere $KL$ denotes the Kullback$-$Leibler divergence, $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$ is the true conditional distribution of the indicator variables $\\mathbf{Z}$ given $\\mathbf{E}$, and $R_{\\mathbf{E}}(\\mathbf{Z})$ is an approximation of the conditional distribution. $\\mathcal{J}(R_{\\mathbf{E}})$ equals to $\\log{\\mathcal{L}(\\mathbf{E})}$ if and only if $R_{\\mathbf{E}}(\\mathbf{Z})=\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$.\n\nAs shown above, we are not able to calculate $\\mathbb{P}(\\mathbf{Z}|\\mathbf{E})$,\nso we will look for the best (in terms of $KL$\ndivergence) $R_{\\mathbf{E}}(\\mathbf{Z})$ in a certain class of distributions. Specifically, we constrain the variational distribution $R_{\\mathbf{E}}(\\mathbf{Z})$ to have the following form:\n\\begin{eqnarray}\nR_{\\mathbf{E}}(\\mathbf{Z})&=&\\prod_{i,j}R(\\mathbf{z}_{ij};\\boldsymbol{\\gamma}_{ij}),\n\\end{eqnarray}\nwhere $R(\\mathbf{z}_{ij};\\boldsymbol{\\gamma}_{ij})=\\prod_{ij}\\prod_{k}\\gamma_{ijk}^{z_{ijk}},\\sum_k \\gamma_{ijk} = 1$, and $\\boldsymbol{\\gamma}$ is the variational parameter. Then, the lower bound $\\mathcal{J}(R_{\\mathbf{E}})$ to be maximized can be written as\n\\begin{eqnarray}\\label{lowbound}\n\\begin{split}\n\\mathcal{J}(R_{\\mathbf{E}}) &= E_{R_{\\mathbf{E}}(\\mathbf{Z})}\\{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z})}\\} - E_{R_{\\mathbf{E}}(\\mathbf{Z})}\\{R_{\\mathbf{E}}(\\mathbf{Z})\\},\\\\\n&= \\sum_{i,j\\in\\Omega,k}\\left[\\log{\\pi_{k}+\\log{f_{p_k}(e_{ij};0,\\eta_{k})}}\\right]\\\\\n&+\\tau\\sum_{i,j\\in\\Omega,k}\\sum_{(p,q)\\in \\mathcal{N}(i,j)}(2\\gamma_{ijk}-1)(2\\gamma_{pqk}-1)\\\\\n&-\\sum_{i,j\\in\\Omega,k}\\gamma_{ijk}\\log{\\gamma_{ijk}}+const.\n\\end{split}\n\\end{eqnarray}\nWe can easily adopt alternative search strategy for the maximization problem on $\\mathcal{J}(R_{\\mathbf{E}})$ by alternatively solving the sub-problems: (i) with respect to $R_{\\mathbf{E}}$ and (ii) with respect to parameters $\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}$. The following Proposition~\\ref{pro1} and~\\ref{pro2} provide the solutions of optimization problem (i) and (ii), respectively.\n\\begin{proposition}\\label{pro1}\n\t\\textit{(Variational E-step)}~ Given parameters $\\mathbf{\\Theta}=\\{\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}\\}$, the optimal variational parameters $\\hat{\\gamma}_{ij} = \\underset{\\boldsymbol{\\gamma}}{\\arg\\max}~\\mathcal{J}(R_{\\mathbf{E}})$ satisfy the following fixed point relation:\n\t\n", "index": 29, "text": "\\begin{equation}\\label{updategamma_fixpoint}\n\t\\gamma_{ijk}\\propto\\pi_{k}f_{p_k}(e_{ij};0,\\eta_{k})\\exp\\{\\tau\\sum_{(p,q)\\in \\mathcal{N}(i,j)}\\gamma_{pqk}\\}.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"\\gamma_{ijk}\\propto\\pi_{k}f_{p_{k}}(e_{ij};0,\\eta_{k})\\exp\\{\\tau\\sum_{(p,q)\\in%&#10;\\mathcal{N}(i,j)}\\gamma_{pqk}\\}.\" display=\"block\"><mrow><mrow><msub><mi>\u03b3</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>k</mi></mrow></msub><mo>\u221d</mo><mrow><msub><mi>\u03c0</mi><mi>k</mi></msub><mo>\u2062</mo><msub><mi>f</mi><msub><mi>p</mi><mi>k</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>;</mo><mn>0</mn><mo>,</mo><msub><mi>\u03b7</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>\u03c4</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><msub><mi>\u03b3</mi><mrow><mi>p</mi><mo>\u2062</mo><mi>q</mi><mo>\u2062</mo><mi>k</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\\end{theorem}\n\\begin{proof}\n\tThis is a direct consequence of Propositions \\ref{pro1} and \\ref{pro2}, which both guarantee that $\\mathcal{J}(R_{\\mathbf{E}})$ monotonically increases in iteration.\n\\end{proof}\n\nIt is easy to see that $\\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\},\\mathbf{\\Theta}^{(t)}\\})$ is upper bounded, and thus the convergence of Algorithm 3 can be guaranteed.\n\n\\section{Experimental Results}\nTo evaluate the performance of the proposed PMoEP method, its special case PMoG and the PMoEP-MRF method, we conducted a series of experiments on both synthetic and real data. Five state-of-the-art LRMF methods were considered for comparison, including Mixture of Gaussion method (MoG~\\cite{meng2013robust}), Laplace noise methods (CWM~\\cite{meng2013cyclic}, RegL1ALM~\\cite{zheng2012practical}) and Gaussian noise methods (Damped Wiberg (DW)~\\cite{okatani2011efficient} and SVD). All experiments were implemented in Matlab R2014a on a PC with 3.60GHz CPU and 12GB RAM.\n\\begin{table}\n\t\\newcommand{\\tabincell}[2]{\\begin{tabular}{@{}#1@{}}#2\\end{tabular}}\n\t\\centering\n\t\\caption{\\label{table1}The Parameter Selection for PMoG and PMoEP.}\n\t{\\scriptsize\n\t\t\\scalebox{1}[1]{\n\t\t\t\\begin{tabular}{c|c|c}\n\t\t\t\t\\hline\n\t\t\t\t&\\multicolumn{2}{c}{Parameter Selection}\\\\\n\t\t\t\t\\cline{2-3}\n\t\t\t\t& PMoG & PMoEP\\\\\n\t\t\t\t\\hline\n\t\t\t\tGaussian & \\tabincell{c}{  $K_{final}=1$,\\\\ $\\lambda_{select}=0.01$ } &\\tabincell{c}{  $K_{final}=1, p_{select} = 2$,\\\\$\\lambda_{select}=0.15$ }\\\\\n\t\t\t\t\\hline\n\t\t\t\tExponential Power &\n\t\t\t\t\\tabincell{c}{ $K_{final}=3$,\\\\ $\\lambda_{select}=0.001$ } &\\tabincell{c}{$K_{final}=1,p_{select}=0.2$,\\\\ $\\lambda_{select}=0.3$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tLaplace  &\n\t\t\t\t\\tabincell{c}{  $K_{final}=3$,\\\\ $\\lambda_{select}=0.001$  } &\n\t\t\t\t\\tabincell{c}{$K_{final}=1,p_{select}=1$,\\\\ $\\lambda_{select}=0.1$ }\\\\\n\t\t\t\t\\hline\n\t\t\t\tSparse  &\n\t\t\t\t\\tabincell{c}{  $K_{final}=2$, \\\\$\\lambda_{select}=0.005$ } &\n\t\t\t\t\\tabincell{c}{ $K_{final}=2,p_{select}=[2,2]$\\\\, $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tMixuture 1 & \\tabincell{c}{ $K_{final}=2$,\\\\ $\\lambda_{select}=0.01$  }  &\\tabincell{c}{$K_{final}=2,p_{select}=[1.5,2]$,\\\\ $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tMixuture 2 & \\tabincell{c}{ $K_{final}=1$,\\\\ $\\lambda_{select}=0.001$  }  &\\tabincell{c}{$K_{final}=2,p_{select}=[0.5,2]$,\\\\ $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}}\n\t\\end{table}\n\\subsection{Synthetic simulations}\nSeveral synthetic experiments with different noise settings were designed to compare the performance of the proposed methods and other competing methods. We first randomly generated $30$ low rank matrices with size $40\\times20$ and rank 4. Each of these matrices was generated by the multiplication of two low-rank matrices $\\mathbf{U}_{gt}\\in \\mathcal{R}^{40\\times 4}$ and $\\mathbf{V}_{gt}\\in \\mathcal{R}^{20\\times 4}$, and $\\mathbf{Y}_{gt}=\\mathbf{U}_{gt}\\mathbf{V}_{gt}^{T}$ is the ground truth matrix. Then, we randomly specified 20\\% elements of $\\mathbf{Y}_{gt}$ as missing entries. Next, we added different types of noise to the non-missing entries as follows: (1) \\textit{Gaussian noise}: $\\mathcal{N}(0,0.04)$. (2) \\textit{Exponential power noise}:\\footnote{The method of drawing samples from a general exponential power distribution is introduced in Appendix B.} $EP_{0.2}(0,0.2^{p}p), p=0.2$. (3) \\textit{Laplace noise}: $\\mathcal{L}(0,0.2)$. (4) \\textit{Sparse noise}: 12.5\\% of the non-missing entries were corrupted with uniformly distributed noise on [-20,20]. (5) \\textit{Mixture noise 1}: 25\\% of the entries were corrupted with uniformly distributed noise on [-5,5], 25\\% were contaminated with Gaussian noise $\\mathcal{N}(0,0.04)$ and the remaining 50\\% are corrupted with Gaussian noise $\\mathcal{N}(0,0.01)$. (6) \\textit{Mixture noise 2}: 37.5\\% of the entries were corrupted with $EP(0,0.1^{p}p), p=0.5$, 50\\% were contaminated with Laplace noise $\\mathcal{L}(0,0.3)$ and the remaining 50\\% were corrupted with Gaussian noise $\\mathcal{N}(0,0.01)$. Then we get the noisy matrix $\\mathbf{Y}_{no}$. Six measures were utilized for performance assessment:\n\n", "itemtype": "equation", "pos": 40865, "prevtext": "\t\n\\end{proposition}\n\\begin{proof}\nBased on (\\ref{lowbound}), we maximize $\\mathcal{J}(R_{\\mathbf{E}})$ with respect to $\\boldsymbol{\\gamma}_{ij}s$, subject to $\\sum_k\\gamma_{ijk}=1$, for all $i,j$, i.e. to maximize $\\mathcal{J}(R_{\\mathbf{E}})+\\sum_{ij}[\\lambda_{ij}(\\sum_{k}\\gamma_{ijk}-1)]$ where $\\lambda_{ij}$ is the Lagrangian multiplier. The derivative with respect to $\\gamma_{ijk}$ is\n\t\\begin{displaymath}\n\t\\log{\\pi_{k}\\!+\\!\\log{f_{p_k}(e_{ij};0,\\eta_{k})}}\\!+\\!\\tau\\!\\!\\!\\!\\sum_{(p,q)\\in \\mathcal{N}(i,j)}\\gamma_{pqk}\\!-\\!\\log{\\gamma_{ijk}}\\!-\\!1\\!+\\!\\lambda_{ij}.\n\t\\end{displaymath}\n\tThis derivative is null iff $\\gamma_{ijk}$ satisfy the relation given in the\n\tproposition, and $\\exp(-1+\\lambda_{ij})$ is the the normalizing constant.\n\\end{proof}\n\\vspace{-1pt}\n\\begin{proposition}\\label{pro2}\n\t\\textit{(Variational M-step)}~ Given the variational parameters $\\boldsymbol{\\gamma}_{ij}s$, the values of parameters $\\mathbf{U},\\mathbf{V},\\boldsymbol{\\pi},\\boldsymbol{\\eta}$ that maximize $\\mathcal{J}(R_{\\mathbf{E}})$ can be calculated in the same way as the M step in the EM algorithm of PMoEP model.\n\\end{proposition}\n\nThe proposed variational EM algorithm for PMoEP-MRF model can then be summarized in Algorithm~\\ref{alg3}.\n\n\\begin{algorithm}[H]\n\t\\caption{VEM Algorithm for the PMoEP-MRF Model.} \\label{alg3}\n\t\\begin{algorithmic}[1]\n\t\t\\REQUIRE Data $\\mathbf{Y}$, rank $r$, $\\tau$ and  $\\lambda$.\n\t\t\\ENSURE Parameter $\\mathbf{\\Theta}$, mixture components number $K_{final}$ and $\\boldsymbol{\\gamma}=(\\gamma_{ijk})_{m\\times n\\times K_{final}}$.\n\t\t\\REQUIRE $\\mathbf{\\Theta}^{(0)}\\!=\\!\\{\\boldsymbol{\\pi}^{(0)},\\boldsymbol{\\eta}^{(0)}, \\mathbf{U}^{(0)}, \\mathbf{V}^{(0)}\\}$, the initial mixture components number $K_{start}$,  preset candidates $\\mathbf{p}=[p_1,\\dots,p_{K_{start}}]$, tolerance $\\epsilon$ and $t=0$.\n\t\t\\WHILE { not converged }\n\t\t\\STATE Updating $\\boldsymbol{\\gamma}^{(t)}$ via the fixed-point Eq.~\\eqref{updategamma_fixpoint};\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\pi}^{(t)}$ via Eq.~\\eqref{updatepi}, and removing the component with $\\pi_{k}^{(t)}=0$;\\\\\n\t\t\\STATE Updating $\\boldsymbol{\\eta}^{(t)}$ via Eq.~\\eqref{updatetheta};\\\\\n\t\t\\STATE Updating $\\mathbf{U}^{(t)}, \\mathbf{V}^{(t)}$ via Algorithm~\\ref{alg1};\\\\\n\t\t\\STATE $t = t + 1.$\n\t\t\\ENDWHILE\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Convergence Analysis of Variational EM algorithm}\nIn this subsection, we show the convergence property of the proposed EM algorithm for PMoEP model.\n\n\\begin{theorem}\\label{theorem2}\n\tGiven $\\lambda$, Algorithm~\\ref{alg3} generates a sequence $\\{\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\}, \\mathbf{\\Theta}^{(t)}\\}\\}_{t=1}^{\\infty}$ which increases $\\mathcal{J}(R_{\\mathbf{E}})$ such that\n\t\n", "index": 31, "text": "\\begin{equation}\n\t\\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t+1)}\\}, \\mathbf{\\Theta}^{(t+1)}\\})\\geq \\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\},\\mathbf{\\Theta}^{(t)}\\}).\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t+1)}\\},\\mathbf{\\Theta%&#10;}^{(t+1)}\\})\\geq\\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\},%&#10;\\mathbf{\\Theta}^{(t)}\\}).\" display=\"block\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>\ud835\udc04</mi></msub><mo>;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>\ud835\udf38</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>,</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca5</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>R</mi><mi>\ud835\udc04</mi></msub><mo>;</mo><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>\ud835\udf38</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>,</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">)</mo><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $\\tilde{\\mathbf{U}},\\tilde{\\mathbf{V}}$ are the outputs of the corresponding competing method, and $subspace(\\mathbf{U}_1$,$\\mathbf{U}_2$) denotes the angle between\nsubspaces spanned by the columns of $\\mathbf{U}_1$ and $\\mathbf{U}_2$. Note that $C1$ and $C2$ are the optimization objective function for $L_1$ and $L_2$ norm LRMF problems, while the latter four measures ($C3-C6$) are more faithful to evaluate whether a method recovers the correct subspaces.\n\t\\begin{table}[htp]\n\t\t\\caption{\\label{simuTab1} Performance evaluation on synthetic data. The best results in terms of each criterion are highlighted in bold.}\n\t\t\\begin{center}\n\t\t\t{\\scriptsize\n\t\t\t\t\\scalebox{1}[0.9]{\n\t\t\t\t\t\\begin{tabular}{ccccccc}\n\t\t\t\t\t\t\\toprule[2pt]\n\t\t\t\t\t\t& PMoEP & PMoG & MoG & DW & CWM & RegL1ALM\\\\\n\t\t\t\t\t\t\\midrule[2pt]\n\t\t\t\t\t\t\\multicolumn{7}{c}{Gaussian Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &   40.97         & 41.00   & 41.00  &  41.00  &  39.23  & {\\bf{36.60}} \\\\\n\t\t\t\t\t\tC2 &   {\\bf{4.16}}    & {\\bf{4.16}}   & {\\bf{4.16}}    &  {\\bf{4.16}}   &  5.67   & 5.27\\\\\n\t\t\t\t\t\tC3 &  {\\bf{3.27}}     & {\\bf{3.27}}     & {\\bf{3.27}}    &  {\\bf{3.27}}     &  6.01     & 4.94\\\\\n\t\t\t\t\t\tC4 &  {\\bf{3.90e+1}}   & 3.91e+1  & 3.91e+1        &  3.91e+1         &  5.09e+1  & 5.09e+1 \\\\\n\t\t\t\t\t\tC5 &  {\\bf{4.22e-2}}   & {\\bf{4.22e-2}}  & {\\bf{4.22e-2}} &  {\\bf{4.22e-2}}  &  5.71e-2  & 5.33e-2 \\\\\n\t\t\t\t\t\tC6 &  {\\bf{3.01e-2}}  & {\\bf{3.01e-2}}  & {\\bf{3.01e-2}} &  {\\bf{3.01e-2}}  &  4.55e-2  & 3.79e-2\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Exponential Power Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &  3.60e+2          & 3.42e+2  &  3.23e+2  &  4.30e+2  & {\\bf{3.21e+2}}  & 3.65e+2\\\\\n\t\t\t\t\t\tC2 &  1.30e+3          & 1.04e+3  &  1.18e+3  &  {\\bf{6.27e+2}}  & 1.17e+3  & 8.51e+2\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.72e+2}}   & 4.49e+4  &  2.17e+3   &  5.06e+3  & 1.73e+2  & 7.77e+4 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{2.32e+2}}   & 4.67e+2  &  2.60e+2   &  6.29e+2  & 2.40e+2  & 9.68e+2\\\\\n\t\t\t\t\t\tC5 &  {\\bf{3.31e-1}}   & 5.67e-1  &  4.11e-1   &  9.19e-1  & 3.39e-1  & 1.16\\\\\n\t\t\t\t\t\tC6 &  {\\bf{2.19e-1}}   & 4.97e-1  &  2.31e-1   &  8.94e-1  & 2.61e-1  & 1.11\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Laplacian Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &  7.63e+1         & 7.29e+1   &  7.13e+1   &  7.76e+1       & 7.24e+1   & {\\bf{6.80e+1}}\\\\\n\t\t\t\t\t\tC2 &  1.72e+1         & 2.57e+1   &  2.44e+1   &  \\bf{1.68e+1}  & 2.16e+1   & 2.10e+1\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.27e+1}}  & 2.02e+1   &  1.84e+1   &  1.31e+1       & 1.69e+1   & 1.42e+1\\\\\n\t\t\t\t\t\tC4 &  {\\bf{7.54e+1}}  & 9.37e+1   &  8.99e+1   &  7.69e+1       & 8.33e+1   & 7.85e+1\\\\\n\t\t\t\t\t\tC5 &  {\\bf{9.17e-2}}  & 1.15e-1   &  1.07e-1   &  9.22e-2       & 1.07e-1   & 9.80e-2\\\\\n\t\t\t\t\t\tC6 &  {\\bf{6.30e-2}}  & 8.25e-2   &  7.84e-2   &  6.49e-2       & 8.24e-2   & 6.56e-2\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Sparse Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 & {\\bf{8.12e+2}}  & {\\bf{8.12e+2}}  & {\\bf{8.12e+2}}  &  1.17e+3   & 8.20e+2   & 8.73e+2\\\\\n\t\t\t\t\t\tC2 & 1.08e+4  & 1.08e+4  & 1.08e+4  &  {\\bf{5.12e+3}}   & 1.06e+4   & 5.95e+3\\\\\n\t\t\t\t\t\tC3 & {\\bf{2.37e-12}} & {\\bf{2.37e-12}} & 7.94e-12 &  3.09e+4   & 9.75e+1   & 1.59e+6\\\\\n\t\t\t\t\t\tC4 & {\\bf{2.54e-5}}  & 2.55e-5  & 3.48e-5  &  2.12e+3   & 6.03e+1   & 4.89e+3 \\\\\n\t\t\t\t\t\tC5 & {\\bf{3.87e-8}}  & 3.87e-8  & 6.63e-8  &  1.48      & 2.83e-1   & 1.47\\\\\n\t\t\t\t\t\tC6 & {\\bf{2.28e-8}}  & 2.29e-8  & 4.44e-8  &  1.39      & 6.25e-2   & 1.54\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Mixture Noise1}\\\\ \\hline\n\t\t\t\t\t\tC1 &  4.49e+2        & 4.55e+2   & 5.25e+2        &  5.25e+2  & {\\bf{4.33e+2}}  & 4.35e+2\\\\\n\t\t\t\t\t\tC2 &  1.36e+3        & 1.25e+3   & {\\bf{8.49e+2}} &  8.51e+2  & 1.12e+3         & 1.16e+3\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.53e+2}} & 6.52e+4   & 8.98e+2        &  8.93e+2  & 3.01e+2         & 1.56e+4 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{1.66e+2}} & 4.38e+2   & 6.02e+2        &  6.00e+2  & 2.87e+2         & 5.15e+2\\\\\n\t\t\t\t\t\tC5 &  {\\bf{3.28e-1}} & 5.79e-1   & 6.47e-1        &  6.60e-1  & 4.30e-1         & 7.88e-1\\\\\n\t\t\t\t\t\tC6 &  {\\bf{1.18e-1}} & 3.78e-1   & 5.01e-1        &  5.01e-1  & 2.93e-1         & 6.84e-1\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Mixture Noise2}\\\\ \\hline\n\t\t\t\t\t\tC1 &  9.01e+1         & 8.93e+1 &  8.76e+1  &  9.60e+1        & 8.83e+1  & {\\bf{8.32e+1}}\\\\\n\t\t\t\t\t\tC2 &  3.37e+1         & 4.04e+1 &  3.99e+1  &  {\\bf{2.72e+1}} & 3.53e+1  & 3.42e+1\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.72e+01}} & 2.62e+1 &  2.49e+1  &  2.13e+1        & 2.40e+1  & 1.87e+1 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{8.57e+01}} & 1.04e+2 &  1.01e+2  &  9.71e+1        & 9.77e+1  & 8.87e+1 \\\\\n\t\t\t\t\t\tC5 &  {\\bf{1.02e-01}} & 1.23e-1 &  1.24e-1  &  1.09e-1        & 1.21e-1  & 1.07e-1\\\\\n\t\t\t\t\t\tC6 &  {\\bf{6.39e-02}} & 8.41e-2 &  8.14e-2  &  7.02e-2        & 8.96e-2  & 6.62e-2\\\\\n\t\t\t\t\t\t\\bottomrule[2pt]\n\t\t\t\t\t\\end{tabular}}\n\t\t\t\t}\n\t\t\t\\end{center}\n\t\t\\end{table}\t\n\t\t\t\\begin{figure}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=1\\linewidth]{PDF_draw1}\n\t\t\t\t\\caption{Visual comparison of the ground truth (denote by True) noise probability density functions and those estimated (denote by Est) by the PMoEP method in the synthetic experiments. The embedded sub-figures depict the zoom-in of the indicated portions.}\\label{pdfdraw}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\t\\begin{figure}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=1\\linewidth]{Face_Recover1}\n\t\t\t\t\\caption{From left to right: original face images, reconstructed faces by PMoEP, PMoG, MoG, RegL1ALM, DW, CWM and SVD.}\\label{FaceRecover}\n\t\t\t\\end{figure}\nWe set the rank of all the competing methods to $4$ and adopt the random initialization strategy for all the methods. For each method, we first run with 20 random initializations and then select the best result with respect to the corresponding objective value of the method. The  performance of each method was evaluated as the average results over the 30 random matrices in terms of the six measures, and the results are summarized in Table \\ref{simuTab1}. We also report the final selections of the mixture number $K_{final}$ and the corresponding parameter $\\lambda_{select}$ for PMoG and PMoEP in Table \\ref{table1}.\n\t\n\tFrom Table \\ref{simuTab1}, we can observe that $L_2$-norm methods DW, MoG, PMoG and our proposed PMoEP methods achieve the best performance than others in Gaussian noise case. In Laplace noise case, our PMoEP method performs best and $L_1$ method RegL1ALM achieves similar results. When the noise is Exponential Power, PMoEP evidently outperforms other competing methods in term of criteria C3$-$C6. In sparse noise case, PMoEP and PMoG perfom the best and MoG achieves comparable good results with PMoEP. Moreover, when the noise gets more complex, PMoEP achieves the best performance, which attributes to the high flexibility of PMoEP to model unknown complex noise. These results then substantiate that our proposed PMoEP method can estimate a better subspace from the noisy data than other competing methods.\n\t\n    The promising performance of PMoEP method in these cases can be easily explained by Fig. \\ref{pdfdraw}, which compares the ground truth noise distributions and the estimated ones by the PMoEP method. It can be easily observed that the estimated noise distributions well match the true ones, which naturally conducts its good reconstruction capability to the true low-rank matrix.\n\n\t\\subsection{Face modeling}\n\tThis experiment aims to test the effectiveness of PMoG and PMoEP methods in face modeling application. We choose the first and the second subset of the Extended Yale B database\\footnote{http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html}, and each subset consists of 64 faces of one person with size $192\\times168$ and then generate two data matrices, each of which is with size $32256\\times 64$. Typical images are shown in the first column of Fig. \\ref{FaceRecover}.\n\t\n\t\n\tWe set the rank as $4$~\\cite{basri2003lambertian} and adopt two initialization strategies, namely random and SVD for all competing methods. Then we report the best result among the results in terms of the object value of the corresponding model utilized by each method. Some reconstructed faces of different methods are visually compared in Fig. \\ref{FaceRecover}.\n\t\n\tFrom Fig. \\ref{FaceRecover}, it is easy to observe that,  the proposed PMoEP and PMoG methods, as well as the other competing ones, can remove the cast shadows and saturations in faces. However, our PMoEP and PMoG methods perform better than other ones on faces containing a large dark region. Such face images contain both significant cast shadow and saturation noises, which correspond to the highly dark and bright areas in face, and camera noise~\\cite{nakamura2005image}, which is much amplified in the dark areas. Compared with other competing methods, PMoEP method is capable of better extracting such complex noise configurations, and thus leads to its better face reconstruction performance.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{103_test_cp}\n\t\\caption{ Restoration results of band 103 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{103bandRec}\n\\end{figure}\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{206_test_cp}\n\t\\caption{ Restoration results of band 206 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{206bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{207_test_cp}\n\t\\caption{ Restoration results of band 207 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{207bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{107_test_cp}\n\t\\caption{Restoration results of band 107 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{107bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_152_cp}\n\t\\caption{ Restoration results of band 152 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{152Rec}\n\\end{figure}\n\n\\subsection{Hyperspectral Image Restoration}\nIn this section, we evaluate the performance of our proposed PMoEP method on hyperspectral image restoration problem. Two real hyperspectral image (HSI) data sets\\footnote{http://www.tec.army.mil/hypercube.}  were used.\n\nThe first dataset is Urban HSI data. This dataset contains $210$ bands, each of which  is $307\\times307$, and some bands are seriously polluted by atmosphere and water and corrupted by noises with complex structures, as shown in Fig. \\ref{intro_fig}. We reshape each band as a vector, and stack all the vectors into a matrix, resulting in the final data matrix with size $94249\\times210$. The second one is the Terrain dataset. The original images are of size $500\\times307\\times210$. We use all the bands in our experiments and thus generate a $153500\\times210$ data matrix. Therefore, we get two data matrices used to test our methods. All the competing methods were implemented, except DW method which encounters the `out of memory' problem.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_206_cp}\n\t\\caption{ Restoration results of band 206 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{206Rec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_139_cp}\n\t\\caption{ Restoration results of band 139 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{139Rec}\n\\end{figure}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{Mean_207}\n\t\\caption{ Vertical (left) and Horizontal (right) mean profiles of band 207 in the Urban data set: (a) original, (b) PMoEP, (c) PMoG, (d) MoG, (e) RegL1ALM, (f) CWM, (g) SVD.}\\label{M207}\n\\end{figure*}\n\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{Mean_Profiles_152}\n\t\\caption{Vertical (left) and Horizontal (right) mean profiles of band 152 in the Terrain data set: (a) original, (b) PMoEP, (c) PMoG, (d) MoG, (e) RegL1ALM, (f) CWM, (g) SVD.}\\label{M152}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{HSI_Noise}\n\t\\caption{From top to bottom, band 204, 206, 207 of Urban, band 208 of Terrain. From left to right: original bands, and extracted noise by RegL1ALM, CWM, SVD, MoG, PMoG and PMoEP. The noises with positive and negative values are depicted in yellow and blue, respectively. This figure should be viewed in color and the details are better seen by zooming on a computer screen.}\\label{UrbanNoise}\n\\end{figure*}\n\n\n\t\n\t\n\n\nThe reconstructed hyperspectral images of bands $103$, $206$, $207$ and $107$ in Urban dataset and bands $152$, $206$ and $139$ in Terrain dataset are shown in Fig. \\ref{103bandRec}$-$\\ref{107bandRec} and  Fig. \\ref{152Rec}$-$\\ref{139Rec}, respectively. For easy observation, an area of interest is amplified in the restored images obtained by all the competing methods. It can be easily seen from the figures that for some bands containing evident stripes and deadlines, the image restored by the proposed PMoEP method is clean and smooth, while the results obtained by the other competing ones contain evident stripe area. In addition, as is demonstrated in Fig. \\ref{107bandRec} and  Fig. \\ref{139Rec}, the PMoEP method can effectively recover the seriously polluted bands, while the other methods failed on them. These results show that our proposed PMoEP method can not only remove complicated noises embedded in HSI, but also can perform robust in the presence of extreme outlier cases like in Fig. \\ref{107bandRec} and Fig. \\ref{139Rec}.\n\nThen we give more quantitative comparison by showing the vertical mean profiles and horizontal mean profiles of band 207 in Urban dataset and band 152 in Terrain dataset before and after reconstruction in Fig. \\ref{M207} and Fig. \\ref{M152}. The horizontal axis of Fig. \\ref{M207} represents the column (left) and row (right) number, and the vertical axis represents the mean DN value of each column (left) and row (right). It is easy to observe that the curves in Fig. \\ref{M207}(a) and \\ref{M152}(a) (right) have drastic fluctuations for the original image. This is deviated from the prior knowledge that the adjacent bands should possess similar shapes since they are captured under relatively similar sensor settings. After the reconstruction, the fluctuations in vertical direction have been reduced by most of the methods. While in the horizontal direction (see Fig. \\ref{M207} (right) and Fig. \\ref{M152} (right)), the PMoEP method provides evidently smoother curves, which indicates that the stripes in the horizontal direction have been removed more effectively by our method. The results are consistent with the recovered HSIs in Fig. \\ref{207bandRec} and Fig. \\ref{152Rec}.\n\nThe better performance of PMoEP over other methods is due to its more powerful ability in noise modeling. Specifically, as depicted in Fig. \\ref{UrbanNoise}, PMoEP can more properly extract noise information from the corrupted images with physical meanings, such as sparse strips, sparse deadlines, and dense Gaussian noise, while other competing methods fail to do so.\n\n\\begin{figure*}\\label{ForDet}\n\t\\centering\n\t\\includegraphics[width=0.85\\linewidth]{Final_Exp}\n\t\\caption{Foreground Detection results of different methods on sample frames.}\\label{watersurface}\n\\end{figure*}\n\n\\subsection{Background Subtraction}\nIn this section, we evaluate the performance of our proposed methods on background subtraction problem. The background subtraction from a video sequence captured by a static camera can be modeled as a low-rank matrix analysis problem~\\cite{wright2009robust}. All the nine standard video sequences\\footnote{http://perception.i2r.a-star.edu.sg/bk\\_model/bk\\_index.html} provided by Li et.al~\\cite{li2004statistical} were adopted in our evaluation, including simple and complex background. Ground truth foreground regions of 20 frames were provided for each sequence.\n\nWe compared our PMoEP and PMoEP-MRF methods with the state-of-the-art LRMF methods: SVD, RegL1ALM, CWM and MoG methods. To conduct the experiments, we first ran each method on each video sequence to estimate the background. Then we obtained the recovered foreground by calculating the absolute values of the difference between the original frame and the estimated background. For MoG, PMoEP and PMoEP-MRF methods, we obtained the foreground by selecting the noise component with largest variance.\n\nFor quantitative evaluation, we first introduce some evaluation indices. We measure the recovery accuracy of the support in the foreground by comparing the true support $S$ with the detected support $\\tilde{S}$. We regard it as a classification problem and thus can evaluate the results using precision and recall, which are defined as:\n\\begin{displaymath}\nprecision = \\frac{TP}{TP+FP},~~~ recall = \\frac{TP}{TP+FN},\n\\end{displaymath}\nwhere $TP$, $FP$, $TN$ and $FN$ represent the numbers of true positive,\nfalse positive, true negative and false negative, respectively.\nFor simplicity, we adopt $F\\textrm{-}measure$ that combines\nprecision and recall together:\n\\begin{displaymath}\nF\\textrm{-}measure = 2\\times \\frac{precision\\times recall}{precision+recall}.\n\\end{displaymath}\nThe higher F\\textrm{-}measure value means the\nbetter recovery accuracy of the support. Additionally, the\nrecovered support $\\tilde{S}$ is obtained by thresholding the recovered foreground  $E$ with a threshold value that gives the maximal F\\textrm{-}measure. For all competing methods, we adopt two initialization strategies, namely, random and SVD. Then we report the best result among the two initializations. The results are summarized in Table \\ref{simuTab2}.\n\n\t\\begin{table}[htp]\n\t\t\\caption{\\label{simuTab2}Performance evaluation on Video data. The best and second best results for each video dataset are highlighted in bold and in italic bold, respectively.}\n\t\t\\begin{center}\n\t\t\t{\\normalsize\n\t\t\t\t\\scalebox{0.7}[0.75]{\n\t\t\t\t\t\\begin{tabular}{cccccccc}\n\t\t\t\t\t\t\\toprule[2pt]\n\t\t\t\t\t\tVideo  & SVD & RegL1ALM & CWM & MoG & PMoEP &  PMoEP-MRF \\\\\n\t\t\t\t\t\t\\midrule[2pt]\n\t\t\t\t\t\t\\multicolumn{7}{c}{$F\\textrm{-}measure$}\\\\ \\hline\n\t\t\t\t\t\tCampus       & 0.4716  & {\\bf{0.5308}} & \\textbf{\\emph{0.5301}} &  0.4633  & 0.5065 & 0.5115\\\\\n\t\t\t\t\t\tLobby        & 0.7623  & 0.7679 & \\textbf{\\emph{0.7681}} &   {\\bf{0.7724}}  & 0.7650 & 0.7444\\\\\n\t\t\t\t\t\tShoppingMall & 0.6990  & \\textbf{\\emph{0.7138}} & {\\bf{0.7173}}  &   0.6387  &  0.7037 & 0.7015\\\\\n\t\t\t\t\t\tBootstrap    & 0.6234  & {\\bf{0.6749}}  & 0.6533 &   0.4234  &  0.6404 &  \\textbf{\\emph{0.6635}} \\\\\n\t\t\t\t\t\tHall         & 0.4104  & 0.4659   &  0.4624  &  0.4523  &  \\textbf{\\emph{0.5372}}  & {\\bf{0.5438}}\\\\\n\t\t\t\t\t\tCurtain      & 0.5273  & 0.5342  &  0.5316  &  0.7869  & {\\bf{0.7895}} & \\textbf{\\emph{0.7888}}\\\\\n\t\t\t\t\t\tFountain     & 0.4989  & 0.5298 & 0.5262 &   0.5782  & \\textbf{\\emph{0.6843}} &  {\\bf{0.7295}}\\\\\n\t\t\t\t\t\tWaterSurface & 0.3416  & 0.2840 & 0.2920 &   0.5979  & \\textbf{\\emph{0.8515}} & {\\bf{0.8651}}\\\\\n\t\t\t\t\t\tEscalator    & 0.2675  & 0.2998 & 0.2972 &   0.2675  & \\textbf{\\emph{0.3255}} & {\\bf{0.3408}}\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\tAverage      & 0.5113  & 0.5334 & 0.5309 &   0.5534  & \\textbf{\\emph{0.6448}} & {\\bf{0.6543}}\\\\\n\t\t\t\t\t\t\\bottomrule[2pt]\n\t\t\t\t\t\\end{tabular}}\n\t\t\t\t}\n\t\t\t\\end{center}\n\t\t\\end{table}\n\nFrom Table \\ref{simuTab2}, it can be easily seen that our proposed PMoEP and PMoEP-MRF methods outperform other methods in the sequences of Hall, Curtain, Fountain, WaterSurface and Escalator, of which the background is with complex shapes. For the sequences with simple background, including Bootstrap, ShoppingMall, Campus and Lobby, the performances of all the methods are almost the same. On average, the PMoEP method achieves the second best performance. Compared with the PMoEP method, the PMoEP-MRF method slightly improves the average performance due to the modeling of spatial and temporal smoothness prior knowledge under foreground using Markov random field.\n\nThe better performance of PMoEP and PMoEP-MRF methods can be visually shown in Fig. 15. It can be easily seen from the figure that the proposed PMoEP and PMoEP-MRF can perform comparably well as other methods in simple foreground cases, while evidently better in much complicated scenarios, e.g., videos with dynamic background.\n\n\\section{Conclusions}\nIn this paper, we model the noise of the LRMF problem as a Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining the penalized likelihood method with the MoEP distributions. Moreover, by facilitating the local continuity of noise components along both space and time of a video, we embed Markov random field into PMoEP and then propose the PMoEP-MRF model. Compared with the current LRMF methods, our PMoEP method performs better in a wide variety of synthetic and real complex noise scenarios including face modeling, hyperspectral image restoration, and background subtraction applications. Additionally, our methods are capable of automatically learning the number of components from data, and thus can be used to deal with more complex applications. In the future, we'll attempt to extend the noise modeling methodology under PMoEP to more computer vision and machine learning tasks, e.g., the high-order low rank tensor factorization problems.\n\n\\section*{Acknowledgements}\nThis research was supported by 973 Program of China\nwith No.3202013CB329404, the NSFC projects with No.11131006, 91330204 and 61373114.\n\n\\appendices\n\\section{Proof of Theorem 1}\n\\begin{proof}\n\t(i) First, we calculate that\n\t\\footnotesize\n\t\\begin{eqnarray}\n\t\\begin{split}\n\tl_{P}^{\\textsc{C}}(\\mathbf{\\Theta})-l_{P}^{C}(\\mathbf{\\Theta}^{(t)})\n\t&=l(\\mathbf{\\Theta})-l(\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t &=\\log{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~-\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t&\\geq \\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~-\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t &=\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda).\\nonumber\n\t\\end{split}\n\t\\end{eqnarray}\n\tLet $\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t)})=\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}\n\t\t{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}$, then\n\t\n", "itemtype": "equation", "pos": 45163, "prevtext": "\n\\end{theorem}\n\\begin{proof}\n\tThis is a direct consequence of Propositions \\ref{pro1} and \\ref{pro2}, which both guarantee that $\\mathcal{J}(R_{\\mathbf{E}})$ monotonically increases in iteration.\n\\end{proof}\n\nIt is easy to see that $\\mathcal{J}(R_{\\mathbf{E}};\\{\\boldsymbol{\\gamma}_{ij}^{(t)}\\},\\mathbf{\\Theta}^{(t)}\\})$ is upper bounded, and thus the convergence of Algorithm 3 can be guaranteed.\n\n\\section{Experimental Results}\nTo evaluate the performance of the proposed PMoEP method, its special case PMoG and the PMoEP-MRF method, we conducted a series of experiments on both synthetic and real data. Five state-of-the-art LRMF methods were considered for comparison, including Mixture of Gaussion method (MoG~\\cite{meng2013robust}), Laplace noise methods (CWM~\\cite{meng2013cyclic}, RegL1ALM~\\cite{zheng2012practical}) and Gaussian noise methods (Damped Wiberg (DW)~\\cite{okatani2011efficient} and SVD). All experiments were implemented in Matlab R2014a on a PC with 3.60GHz CPU and 12GB RAM.\n\\begin{table}\n\t\\newcommand{\\tabincell}[2]{\\begin{tabular}{@{}#1@{}}#2\\end{tabular}}\n\t\\centering\n\t\\caption{\\label{table1}The Parameter Selection for PMoG and PMoEP.}\n\t{\\scriptsize\n\t\t\\scalebox{1}[1]{\n\t\t\t\\begin{tabular}{c|c|c}\n\t\t\t\t\\hline\n\t\t\t\t&\\multicolumn{2}{c}{Parameter Selection}\\\\\n\t\t\t\t\\cline{2-3}\n\t\t\t\t& PMoG & PMoEP\\\\\n\t\t\t\t\\hline\n\t\t\t\tGaussian & \\tabincell{c}{  $K_{final}=1$,\\\\ $\\lambda_{select}=0.01$ } &\\tabincell{c}{  $K_{final}=1, p_{select} = 2$,\\\\$\\lambda_{select}=0.15$ }\\\\\n\t\t\t\t\\hline\n\t\t\t\tExponential Power &\n\t\t\t\t\\tabincell{c}{ $K_{final}=3$,\\\\ $\\lambda_{select}=0.001$ } &\\tabincell{c}{$K_{final}=1,p_{select}=0.2$,\\\\ $\\lambda_{select}=0.3$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tLaplace  &\n\t\t\t\t\\tabincell{c}{  $K_{final}=3$,\\\\ $\\lambda_{select}=0.001$  } &\n\t\t\t\t\\tabincell{c}{$K_{final}=1,p_{select}=1$,\\\\ $\\lambda_{select}=0.1$ }\\\\\n\t\t\t\t\\hline\n\t\t\t\tSparse  &\n\t\t\t\t\\tabincell{c}{  $K_{final}=2$, \\\\$\\lambda_{select}=0.005$ } &\n\t\t\t\t\\tabincell{c}{ $K_{final}=2,p_{select}=[2,2]$\\\\, $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tMixuture 1 & \\tabincell{c}{ $K_{final}=2$,\\\\ $\\lambda_{select}=0.01$  }  &\\tabincell{c}{$K_{final}=2,p_{select}=[1.5,2]$,\\\\ $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\tMixuture 2 & \\tabincell{c}{ $K_{final}=1$,\\\\ $\\lambda_{select}=0.001$  }  &\\tabincell{c}{$K_{final}=2,p_{select}=[0.5,2]$,\\\\ $\\lambda_{select}=0.005$}\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t}}\n\t\\end{table}\n\\subsection{Synthetic simulations}\nSeveral synthetic experiments with different noise settings were designed to compare the performance of the proposed methods and other competing methods. We first randomly generated $30$ low rank matrices with size $40\\times20$ and rank 4. Each of these matrices was generated by the multiplication of two low-rank matrices $\\mathbf{U}_{gt}\\in \\mathcal{R}^{40\\times 4}$ and $\\mathbf{V}_{gt}\\in \\mathcal{R}^{20\\times 4}$, and $\\mathbf{Y}_{gt}=\\mathbf{U}_{gt}\\mathbf{V}_{gt}^{T}$ is the ground truth matrix. Then, we randomly specified 20\\% elements of $\\mathbf{Y}_{gt}$ as missing entries. Next, we added different types of noise to the non-missing entries as follows: (1) \\textit{Gaussian noise}: $\\mathcal{N}(0,0.04)$. (2) \\textit{Exponential power noise}:\\footnote{The method of drawing samples from a general exponential power distribution is introduced in Appendix B.} $EP_{0.2}(0,0.2^{p}p), p=0.2$. (3) \\textit{Laplace noise}: $\\mathcal{L}(0,0.2)$. (4) \\textit{Sparse noise}: 12.5\\% of the non-missing entries were corrupted with uniformly distributed noise on [-20,20]. (5) \\textit{Mixture noise 1}: 25\\% of the entries were corrupted with uniformly distributed noise on [-5,5], 25\\% were contaminated with Gaussian noise $\\mathcal{N}(0,0.04)$ and the remaining 50\\% are corrupted with Gaussian noise $\\mathcal{N}(0,0.01)$. (6) \\textit{Mixture noise 2}: 37.5\\% of the entries were corrupted with $EP(0,0.1^{p}p), p=0.5$, 50\\% were contaminated with Laplace noise $\\mathcal{L}(0,0.3)$ and the remaining 50\\% were corrupted with Gaussian noise $\\mathcal{N}(0,0.01)$. Then we get the noisy matrix $\\mathbf{Y}_{no}$. Six measures were utilized for performance assessment:\n\n", "index": 33, "text": "\\begin{equation*}\n\\begin{split}\n&C1 \\!=\\! ||\\mathbf{W}\\!\\odot\\!(\\mathbf{Y}_{no}\\!-\\!\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T})||_{1},\\nonumber~C2 \\!=\\! ||\\mathbf{W}\\!\\odot\\! (\\mathbf{Y}_{no}\\!-\\!\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T})||_{2}, \\nonumber\\\\\n&C3 = ||\\mathbf{Y}_{gt}-\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T}||_{1}, \\nonumber~~C4 = ||\\mathbf{Y}_{gt}-\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T}||_{2}, \\nonumber\\\\\n&C5 = subspace(\\mathbf{U}_{gt},\\tilde{\\mathbf{U}}), \\nonumber~~C6 = subspace(\\mathbf{V}_{gt},\\tilde{\\mathbf{V}}),\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}&amp;\\displaystyle C1\\!=\\!||\\mathbf{W}\\!\\odot\\!(\\mathbf{Y}_{no}\\!-\\!%&#10;\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T})||_{1},~{}C2\\!=\\!||\\mathbf{W}\\!\\odot%&#10;\\!(\\mathbf{Y}_{no}\\!-\\!\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T})||_{2},\\\\&#10;&amp;\\displaystyle C3=||\\mathbf{Y}_{gt}-\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T}||%&#10;_{1},~{}~{}C4=||\\mathbf{Y}_{gt}-\\tilde{\\mathbf{U}}\\tilde{\\mathbf{V}}^{T}||_{2}%&#10;,\\\\&#10;&amp;\\displaystyle C5=subspace(\\mathbf{U}_{gt},\\tilde{\\mathbf{U}}),~{}~{}C6=%&#10;subspace(\\mathbf{V}_{gt},\\tilde{\\mathbf{V}}),\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mn>1</mn></mpadded></mrow><mo rspace=\"0.8pt\">=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mpadded width=\"-1.7pt\"><mi>\ud835\udc16</mi></mpadded><mo rspace=\"0.8pt\">\u2299</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>o</mi></mrow></msub></mpadded><mo rspace=\"0.8pt\">-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc14</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc15</mi><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo rspace=\"5.8pt\">,</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mpadded width=\"-1.7pt\"><mn>2</mn></mpadded></mrow><mo rspace=\"0.8pt\">=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mpadded width=\"-1.7pt\"><mi>\ud835\udc16</mi></mpadded><mo rspace=\"0.8pt\">\u2299</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mpadded width=\"-1.7pt\"><msub><mi>\ud835\udc18</mi><mrow><mi>n</mi><mo>\u2062</mo><mi>o</mi></mrow></msub></mpadded><mo rspace=\"0.8pt\">-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc14</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc15</mi><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mn>3</mn></mrow><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc18</mi><mrow><mi>g</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc14</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc15</mi><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>1</mn></msub></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mn>4</mn></mrow><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc18</mi><mrow><mi>g</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>-</mo><mrow><mover accent=\"true\"><mi>\ud835\udc14</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><msup><mover accent=\"true\"><mi>\ud835\udc15</mi><mo stretchy=\"false\">~</mo></mover><mi>T</mi></msup></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd/><mtd columnalign=\"left\"><mrow><mrow><mrow><mrow><mi>C</mi><mo>\u2062</mo><mn>5</mn></mrow><mo>=</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc14</mi><mrow><mi>g</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc14</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"9.1pt\">,</mo><mrow><mrow><mi>C</mi><mo>\u2062</mo><mn>6</mn></mrow><mo>=</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc15</mi><mrow><mi>g</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>,</mo><mover accent=\"true\"><mi>\ud835\udc15</mi><mo stretchy=\"false\">~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\t\\normalsize\n\t\\\\\n\t(ii) In the M step of Algorithm 1, it is obvious that\n\t\\footnotesize\n\t\\begin{eqnarray}\n\t\\mathbf{\\Theta}^{(t+1)}\\!&=&\\! \\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\!-\\!P(\\pi;\\lambda)\\right\\}\\nonumber\\\\\n\t \\!&=&\\!\\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\frac{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}}\n\t{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta}^{(t)})}}\\!-\\!P(\\pi;\\lambda)\\right\\}\\nonumber\\\\\n\t \\!&=&\\!\\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\right\\}.\\nonumber\n\t\\end{eqnarray}\n\t\\normalsize\n\tThus, we have\n\t\\begin{eqnarray}\n\t\\begin{split}\n\t&\\Omega(\\mathbf{\\Theta}^{(t+1)}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi^{(t+1)};\\lambda)\\\\\n\t&\\geq\n\t\\Omega(\\mathbf{\\Theta}^{(t)}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi^{(t)};\\lambda)=0\n\t\\end{split}\n\t\\end{eqnarray}\n\tThen, we can easily derive that\n\t\n", "itemtype": "equation", "pos": 68945, "prevtext": "\nwhere $\\tilde{\\mathbf{U}},\\tilde{\\mathbf{V}}$ are the outputs of the corresponding competing method, and $subspace(\\mathbf{U}_1$,$\\mathbf{U}_2$) denotes the angle between\nsubspaces spanned by the columns of $\\mathbf{U}_1$ and $\\mathbf{U}_2$. Note that $C1$ and $C2$ are the optimization objective function for $L_1$ and $L_2$ norm LRMF problems, while the latter four measures ($C3-C6$) are more faithful to evaluate whether a method recovers the correct subspaces.\n\t\\begin{table}[htp]\n\t\t\\caption{\\label{simuTab1} Performance evaluation on synthetic data. The best results in terms of each criterion are highlighted in bold.}\n\t\t\\begin{center}\n\t\t\t{\\scriptsize\n\t\t\t\t\\scalebox{1}[0.9]{\n\t\t\t\t\t\\begin{tabular}{ccccccc}\n\t\t\t\t\t\t\\toprule[2pt]\n\t\t\t\t\t\t& PMoEP & PMoG & MoG & DW & CWM & RegL1ALM\\\\\n\t\t\t\t\t\t\\midrule[2pt]\n\t\t\t\t\t\t\\multicolumn{7}{c}{Gaussian Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &   40.97         & 41.00   & 41.00  &  41.00  &  39.23  & {\\bf{36.60}} \\\\\n\t\t\t\t\t\tC2 &   {\\bf{4.16}}    & {\\bf{4.16}}   & {\\bf{4.16}}    &  {\\bf{4.16}}   &  5.67   & 5.27\\\\\n\t\t\t\t\t\tC3 &  {\\bf{3.27}}     & {\\bf{3.27}}     & {\\bf{3.27}}    &  {\\bf{3.27}}     &  6.01     & 4.94\\\\\n\t\t\t\t\t\tC4 &  {\\bf{3.90e+1}}   & 3.91e+1  & 3.91e+1        &  3.91e+1         &  5.09e+1  & 5.09e+1 \\\\\n\t\t\t\t\t\tC5 &  {\\bf{4.22e-2}}   & {\\bf{4.22e-2}}  & {\\bf{4.22e-2}} &  {\\bf{4.22e-2}}  &  5.71e-2  & 5.33e-2 \\\\\n\t\t\t\t\t\tC6 &  {\\bf{3.01e-2}}  & {\\bf{3.01e-2}}  & {\\bf{3.01e-2}} &  {\\bf{3.01e-2}}  &  4.55e-2  & 3.79e-2\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Exponential Power Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &  3.60e+2          & 3.42e+2  &  3.23e+2  &  4.30e+2  & {\\bf{3.21e+2}}  & 3.65e+2\\\\\n\t\t\t\t\t\tC2 &  1.30e+3          & 1.04e+3  &  1.18e+3  &  {\\bf{6.27e+2}}  & 1.17e+3  & 8.51e+2\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.72e+2}}   & 4.49e+4  &  2.17e+3   &  5.06e+3  & 1.73e+2  & 7.77e+4 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{2.32e+2}}   & 4.67e+2  &  2.60e+2   &  6.29e+2  & 2.40e+2  & 9.68e+2\\\\\n\t\t\t\t\t\tC5 &  {\\bf{3.31e-1}}   & 5.67e-1  &  4.11e-1   &  9.19e-1  & 3.39e-1  & 1.16\\\\\n\t\t\t\t\t\tC6 &  {\\bf{2.19e-1}}   & 4.97e-1  &  2.31e-1   &  8.94e-1  & 2.61e-1  & 1.11\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Laplacian Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 &  7.63e+1         & 7.29e+1   &  7.13e+1   &  7.76e+1       & 7.24e+1   & {\\bf{6.80e+1}}\\\\\n\t\t\t\t\t\tC2 &  1.72e+1         & 2.57e+1   &  2.44e+1   &  \\bf{1.68e+1}  & 2.16e+1   & 2.10e+1\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.27e+1}}  & 2.02e+1   &  1.84e+1   &  1.31e+1       & 1.69e+1   & 1.42e+1\\\\\n\t\t\t\t\t\tC4 &  {\\bf{7.54e+1}}  & 9.37e+1   &  8.99e+1   &  7.69e+1       & 8.33e+1   & 7.85e+1\\\\\n\t\t\t\t\t\tC5 &  {\\bf{9.17e-2}}  & 1.15e-1   &  1.07e-1   &  9.22e-2       & 1.07e-1   & 9.80e-2\\\\\n\t\t\t\t\t\tC6 &  {\\bf{6.30e-2}}  & 8.25e-2   &  7.84e-2   &  6.49e-2       & 8.24e-2   & 6.56e-2\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Sparse Noise}\\\\ \\hline\n\t\t\t\t\t\tC1 & {\\bf{8.12e+2}}  & {\\bf{8.12e+2}}  & {\\bf{8.12e+2}}  &  1.17e+3   & 8.20e+2   & 8.73e+2\\\\\n\t\t\t\t\t\tC2 & 1.08e+4  & 1.08e+4  & 1.08e+4  &  {\\bf{5.12e+3}}   & 1.06e+4   & 5.95e+3\\\\\n\t\t\t\t\t\tC3 & {\\bf{2.37e-12}} & {\\bf{2.37e-12}} & 7.94e-12 &  3.09e+4   & 9.75e+1   & 1.59e+6\\\\\n\t\t\t\t\t\tC4 & {\\bf{2.54e-5}}  & 2.55e-5  & 3.48e-5  &  2.12e+3   & 6.03e+1   & 4.89e+3 \\\\\n\t\t\t\t\t\tC5 & {\\bf{3.87e-8}}  & 3.87e-8  & 6.63e-8  &  1.48      & 2.83e-1   & 1.47\\\\\n\t\t\t\t\t\tC6 & {\\bf{2.28e-8}}  & 2.29e-8  & 4.44e-8  &  1.39      & 6.25e-2   & 1.54\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Mixture Noise1}\\\\ \\hline\n\t\t\t\t\t\tC1 &  4.49e+2        & 4.55e+2   & 5.25e+2        &  5.25e+2  & {\\bf{4.33e+2}}  & 4.35e+2\\\\\n\t\t\t\t\t\tC2 &  1.36e+3        & 1.25e+3   & {\\bf{8.49e+2}} &  8.51e+2  & 1.12e+3         & 1.16e+3\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.53e+2}} & 6.52e+4   & 8.98e+2        &  8.93e+2  & 3.01e+2         & 1.56e+4 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{1.66e+2}} & 4.38e+2   & 6.02e+2        &  6.00e+2  & 2.87e+2         & 5.15e+2\\\\\n\t\t\t\t\t\tC5 &  {\\bf{3.28e-1}} & 5.79e-1   & 6.47e-1        &  6.60e-1  & 4.30e-1         & 7.88e-1\\\\\n\t\t\t\t\t\tC6 &  {\\bf{1.18e-1}} & 3.78e-1   & 5.01e-1        &  5.01e-1  & 2.93e-1         & 6.84e-1\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\t\\multicolumn{7}{c}{Mixture Noise2}\\\\ \\hline\n\t\t\t\t\t\tC1 &  9.01e+1         & 8.93e+1 &  8.76e+1  &  9.60e+1        & 8.83e+1  & {\\bf{8.32e+1}}\\\\\n\t\t\t\t\t\tC2 &  3.37e+1         & 4.04e+1 &  3.99e+1  &  {\\bf{2.72e+1}} & 3.53e+1  & 3.42e+1\\\\\n\t\t\t\t\t\tC3 &  {\\bf{1.72e+01}} & 2.62e+1 &  2.49e+1  &  2.13e+1        & 2.40e+1  & 1.87e+1 \\\\\n\t\t\t\t\t\tC4 &  {\\bf{8.57e+01}} & 1.04e+2 &  1.01e+2  &  9.71e+1        & 9.77e+1  & 8.87e+1 \\\\\n\t\t\t\t\t\tC5 &  {\\bf{1.02e-01}} & 1.23e-1 &  1.24e-1  &  1.09e-1        & 1.21e-1  & 1.07e-1\\\\\n\t\t\t\t\t\tC6 &  {\\bf{6.39e-02}} & 8.41e-2 &  8.14e-2  &  7.02e-2        & 8.96e-2  & 6.62e-2\\\\\n\t\t\t\t\t\t\\bottomrule[2pt]\n\t\t\t\t\t\\end{tabular}}\n\t\t\t\t}\n\t\t\t\\end{center}\n\t\t\\end{table}\t\n\t\t\t\\begin{figure}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=1\\linewidth]{PDF_draw1}\n\t\t\t\t\\caption{Visual comparison of the ground truth (denote by True) noise probability density functions and those estimated (denote by Est) by the PMoEP method in the synthetic experiments. The embedded sub-figures depict the zoom-in of the indicated portions.}\\label{pdfdraw}\n\t\t\t\\end{figure}\n\t\t\t\n\t\t\t\\begin{figure}\n\t\t\t\t\\centering\n\t\t\t\t\\includegraphics[width=1\\linewidth]{Face_Recover1}\n\t\t\t\t\\caption{From left to right: original face images, reconstructed faces by PMoEP, PMoG, MoG, RegL1ALM, DW, CWM and SVD.}\\label{FaceRecover}\n\t\t\t\\end{figure}\nWe set the rank of all the competing methods to $4$ and adopt the random initialization strategy for all the methods. For each method, we first run with 20 random initializations and then select the best result with respect to the corresponding objective value of the method. The  performance of each method was evaluated as the average results over the 30 random matrices in terms of the six measures, and the results are summarized in Table \\ref{simuTab1}. We also report the final selections of the mixture number $K_{final}$ and the corresponding parameter $\\lambda_{select}$ for PMoG and PMoEP in Table \\ref{table1}.\n\t\n\tFrom Table \\ref{simuTab1}, we can observe that $L_2$-norm methods DW, MoG, PMoG and our proposed PMoEP methods achieve the best performance than others in Gaussian noise case. In Laplace noise case, our PMoEP method performs best and $L_1$ method RegL1ALM achieves similar results. When the noise is Exponential Power, PMoEP evidently outperforms other competing methods in term of criteria C3$-$C6. In sparse noise case, PMoEP and PMoG perfom the best and MoG achieves comparable good results with PMoEP. Moreover, when the noise gets more complex, PMoEP achieves the best performance, which attributes to the high flexibility of PMoEP to model unknown complex noise. These results then substantiate that our proposed PMoEP method can estimate a better subspace from the noisy data than other competing methods.\n\t\n    The promising performance of PMoEP method in these cases can be easily explained by Fig. \\ref{pdfdraw}, which compares the ground truth noise distributions and the estimated ones by the PMoEP method. It can be easily observed that the estimated noise distributions well match the true ones, which naturally conducts its good reconstruction capability to the true low-rank matrix.\n\n\t\\subsection{Face modeling}\n\tThis experiment aims to test the effectiveness of PMoG and PMoEP methods in face modeling application. We choose the first and the second subset of the Extended Yale B database\\footnote{http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html}, and each subset consists of 64 faces of one person with size $192\\times168$ and then generate two data matrices, each of which is with size $32256\\times 64$. Typical images are shown in the first column of Fig. \\ref{FaceRecover}.\n\t\n\t\n\tWe set the rank as $4$~\\cite{basri2003lambertian} and adopt two initialization strategies, namely random and SVD for all competing methods. Then we report the best result among the results in terms of the object value of the corresponding model utilized by each method. Some reconstructed faces of different methods are visually compared in Fig. \\ref{FaceRecover}.\n\t\n\tFrom Fig. \\ref{FaceRecover}, it is easy to observe that,  the proposed PMoEP and PMoG methods, as well as the other competing ones, can remove the cast shadows and saturations in faces. However, our PMoEP and PMoG methods perform better than other ones on faces containing a large dark region. Such face images contain both significant cast shadow and saturation noises, which correspond to the highly dark and bright areas in face, and camera noise~\\cite{nakamura2005image}, which is much amplified in the dark areas. Compared with other competing methods, PMoEP method is capable of better extracting such complex noise configurations, and thus leads to its better face reconstruction performance.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{103_test_cp}\n\t\\caption{ Restoration results of band 103 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{103bandRec}\n\\end{figure}\n\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{206_test_cp}\n\t\\caption{ Restoration results of band 206 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{206bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{207_test_cp}\n\t\\caption{ Restoration results of band 207 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{207bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{107_test_cp}\n\t\\caption{Restoration results of band 107 in Urban data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{107bandRec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_152_cp}\n\t\\caption{ Restoration results of band 152 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{152Rec}\n\\end{figure}\n\n\\subsection{Hyperspectral Image Restoration}\nIn this section, we evaluate the performance of our proposed PMoEP method on hyperspectral image restoration problem. Two real hyperspectral image (HSI) data sets\\footnote{http://www.tec.army.mil/hypercube.}  were used.\n\nThe first dataset is Urban HSI data. This dataset contains $210$ bands, each of which  is $307\\times307$, and some bands are seriously polluted by atmosphere and water and corrupted by noises with complex structures, as shown in Fig. \\ref{intro_fig}. We reshape each band as a vector, and stack all the vectors into a matrix, resulting in the final data matrix with size $94249\\times210$. The second one is the Terrain dataset. The original images are of size $500\\times307\\times210$. We use all the bands in our experiments and thus generate a $153500\\times210$ data matrix. Therefore, we get two data matrices used to test our methods. All the competing methods were implemented, except DW method which encounters the `out of memory' problem.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_206_cp}\n\t\\caption{ Restoration results of band 206 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{206Rec}\n\\end{figure}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{Rec_band_139_cp}\n\t\\caption{ Restoration results of band 139 in Terrain data set: (a) original bands. (b)-(g) reconstructed bands by PMoEP, PMoG, MoG, RegL1ALM, CWM and SVD.}\\label{139Rec}\n\\end{figure}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{Mean_207}\n\t\\caption{ Vertical (left) and Horizontal (right) mean profiles of band 207 in the Urban data set: (a) original, (b) PMoEP, (c) PMoG, (d) MoG, (e) RegL1ALM, (f) CWM, (g) SVD.}\\label{M207}\n\\end{figure*}\n\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{Mean_Profiles_152}\n\t\\caption{Vertical (left) and Horizontal (right) mean profiles of band 152 in the Terrain data set: (a) original, (b) PMoEP, (c) PMoG, (d) MoG, (e) RegL1ALM, (f) CWM, (g) SVD.}\\label{M152}\n\\end{figure*}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{HSI_Noise}\n\t\\caption{From top to bottom, band 204, 206, 207 of Urban, band 208 of Terrain. From left to right: original bands, and extracted noise by RegL1ALM, CWM, SVD, MoG, PMoG and PMoEP. The noises with positive and negative values are depicted in yellow and blue, respectively. This figure should be viewed in color and the details are better seen by zooming on a computer screen.}\\label{UrbanNoise}\n\\end{figure*}\n\n\n\t\n\t\n\n\nThe reconstructed hyperspectral images of bands $103$, $206$, $207$ and $107$ in Urban dataset and bands $152$, $206$ and $139$ in Terrain dataset are shown in Fig. \\ref{103bandRec}$-$\\ref{107bandRec} and  Fig. \\ref{152Rec}$-$\\ref{139Rec}, respectively. For easy observation, an area of interest is amplified in the restored images obtained by all the competing methods. It can be easily seen from the figures that for some bands containing evident stripes and deadlines, the image restored by the proposed PMoEP method is clean and smooth, while the results obtained by the other competing ones contain evident stripe area. In addition, as is demonstrated in Fig. \\ref{107bandRec} and  Fig. \\ref{139Rec}, the PMoEP method can effectively recover the seriously polluted bands, while the other methods failed on them. These results show that our proposed PMoEP method can not only remove complicated noises embedded in HSI, but also can perform robust in the presence of extreme outlier cases like in Fig. \\ref{107bandRec} and Fig. \\ref{139Rec}.\n\nThen we give more quantitative comparison by showing the vertical mean profiles and horizontal mean profiles of band 207 in Urban dataset and band 152 in Terrain dataset before and after reconstruction in Fig. \\ref{M207} and Fig. \\ref{M152}. The horizontal axis of Fig. \\ref{M207} represents the column (left) and row (right) number, and the vertical axis represents the mean DN value of each column (left) and row (right). It is easy to observe that the curves in Fig. \\ref{M207}(a) and \\ref{M152}(a) (right) have drastic fluctuations for the original image. This is deviated from the prior knowledge that the adjacent bands should possess similar shapes since they are captured under relatively similar sensor settings. After the reconstruction, the fluctuations in vertical direction have been reduced by most of the methods. While in the horizontal direction (see Fig. \\ref{M207} (right) and Fig. \\ref{M152} (right)), the PMoEP method provides evidently smoother curves, which indicates that the stripes in the horizontal direction have been removed more effectively by our method. The results are consistent with the recovered HSIs in Fig. \\ref{207bandRec} and Fig. \\ref{152Rec}.\n\nThe better performance of PMoEP over other methods is due to its more powerful ability in noise modeling. Specifically, as depicted in Fig. \\ref{UrbanNoise}, PMoEP can more properly extract noise information from the corrupted images with physical meanings, such as sparse strips, sparse deadlines, and dense Gaussian noise, while other competing methods fail to do so.\n\n\\begin{figure*}\\label{ForDet}\n\t\\centering\n\t\\includegraphics[width=0.85\\linewidth]{Final_Exp}\n\t\\caption{Foreground Detection results of different methods on sample frames.}\\label{watersurface}\n\\end{figure*}\n\n\\subsection{Background Subtraction}\nIn this section, we evaluate the performance of our proposed methods on background subtraction problem. The background subtraction from a video sequence captured by a static camera can be modeled as a low-rank matrix analysis problem~\\cite{wright2009robust}. All the nine standard video sequences\\footnote{http://perception.i2r.a-star.edu.sg/bk\\_model/bk\\_index.html} provided by Li et.al~\\cite{li2004statistical} were adopted in our evaluation, including simple and complex background. Ground truth foreground regions of 20 frames were provided for each sequence.\n\nWe compared our PMoEP and PMoEP-MRF methods with the state-of-the-art LRMF methods: SVD, RegL1ALM, CWM and MoG methods. To conduct the experiments, we first ran each method on each video sequence to estimate the background. Then we obtained the recovered foreground by calculating the absolute values of the difference between the original frame and the estimated background. For MoG, PMoEP and PMoEP-MRF methods, we obtained the foreground by selecting the noise component with largest variance.\n\nFor quantitative evaluation, we first introduce some evaluation indices. We measure the recovery accuracy of the support in the foreground by comparing the true support $S$ with the detected support $\\tilde{S}$. We regard it as a classification problem and thus can evaluate the results using precision and recall, which are defined as:\n\\begin{displaymath}\nprecision = \\frac{TP}{TP+FP},~~~ recall = \\frac{TP}{TP+FN},\n\\end{displaymath}\nwhere $TP$, $FP$, $TN$ and $FN$ represent the numbers of true positive,\nfalse positive, true negative and false negative, respectively.\nFor simplicity, we adopt $F\\textrm{-}measure$ that combines\nprecision and recall together:\n\\begin{displaymath}\nF\\textrm{-}measure = 2\\times \\frac{precision\\times recall}{precision+recall}.\n\\end{displaymath}\nThe higher F\\textrm{-}measure value means the\nbetter recovery accuracy of the support. Additionally, the\nrecovered support $\\tilde{S}$ is obtained by thresholding the recovered foreground  $E$ with a threshold value that gives the maximal F\\textrm{-}measure. For all competing methods, we adopt two initialization strategies, namely, random and SVD. Then we report the best result among the two initializations. The results are summarized in Table \\ref{simuTab2}.\n\n\t\\begin{table}[htp]\n\t\t\\caption{\\label{simuTab2}Performance evaluation on Video data. The best and second best results for each video dataset are highlighted in bold and in italic bold, respectively.}\n\t\t\\begin{center}\n\t\t\t{\\normalsize\n\t\t\t\t\\scalebox{0.7}[0.75]{\n\t\t\t\t\t\\begin{tabular}{cccccccc}\n\t\t\t\t\t\t\\toprule[2pt]\n\t\t\t\t\t\tVideo  & SVD & RegL1ALM & CWM & MoG & PMoEP &  PMoEP-MRF \\\\\n\t\t\t\t\t\t\\midrule[2pt]\n\t\t\t\t\t\t\\multicolumn{7}{c}{$F\\textrm{-}measure$}\\\\ \\hline\n\t\t\t\t\t\tCampus       & 0.4716  & {\\bf{0.5308}} & \\textbf{\\emph{0.5301}} &  0.4633  & 0.5065 & 0.5115\\\\\n\t\t\t\t\t\tLobby        & 0.7623  & 0.7679 & \\textbf{\\emph{0.7681}} &   {\\bf{0.7724}}  & 0.7650 & 0.7444\\\\\n\t\t\t\t\t\tShoppingMall & 0.6990  & \\textbf{\\emph{0.7138}} & {\\bf{0.7173}}  &   0.6387  &  0.7037 & 0.7015\\\\\n\t\t\t\t\t\tBootstrap    & 0.6234  & {\\bf{0.6749}}  & 0.6533 &   0.4234  &  0.6404 &  \\textbf{\\emph{0.6635}} \\\\\n\t\t\t\t\t\tHall         & 0.4104  & 0.4659   &  0.4624  &  0.4523  &  \\textbf{\\emph{0.5372}}  & {\\bf{0.5438}}\\\\\n\t\t\t\t\t\tCurtain      & 0.5273  & 0.5342  &  0.5316  &  0.7869  & {\\bf{0.7895}} & \\textbf{\\emph{0.7888}}\\\\\n\t\t\t\t\t\tFountain     & 0.4989  & 0.5298 & 0.5262 &   0.5782  & \\textbf{\\emph{0.6843}} &  {\\bf{0.7295}}\\\\\n\t\t\t\t\t\tWaterSurface & 0.3416  & 0.2840 & 0.2920 &   0.5979  & \\textbf{\\emph{0.8515}} & {\\bf{0.8651}}\\\\\n\t\t\t\t\t\tEscalator    & 0.2675  & 0.2998 & 0.2972 &   0.2675  & \\textbf{\\emph{0.3255}} & {\\bf{0.3408}}\\\\\n\t\t\t\t\t\t\\hline\n\t\t\t\t\t\tAverage      & 0.5113  & 0.5334 & 0.5309 &   0.5534  & \\textbf{\\emph{0.6448}} & {\\bf{0.6543}}\\\\\n\t\t\t\t\t\t\\bottomrule[2pt]\n\t\t\t\t\t\\end{tabular}}\n\t\t\t\t}\n\t\t\t\\end{center}\n\t\t\\end{table}\n\nFrom Table \\ref{simuTab2}, it can be easily seen that our proposed PMoEP and PMoEP-MRF methods outperform other methods in the sequences of Hall, Curtain, Fountain, WaterSurface and Escalator, of which the background is with complex shapes. For the sequences with simple background, including Bootstrap, ShoppingMall, Campus and Lobby, the performances of all the methods are almost the same. On average, the PMoEP method achieves the second best performance. Compared with the PMoEP method, the PMoEP-MRF method slightly improves the average performance due to the modeling of spatial and temporal smoothness prior knowledge under foreground using Markov random field.\n\nThe better performance of PMoEP and PMoEP-MRF methods can be visually shown in Fig. 15. It can be easily seen from the figure that the proposed PMoEP and PMoEP-MRF can perform comparably well as other methods in simple foreground cases, while evidently better in much complicated scenarios, e.g., videos with dynamic background.\n\n\\section{Conclusions}\nIn this paper, we model the noise of the LRMF problem as a Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combining the penalized likelihood method with the MoEP distributions. Moreover, by facilitating the local continuity of noise components along both space and time of a video, we embed Markov random field into PMoEP and then propose the PMoEP-MRF model. Compared with the current LRMF methods, our PMoEP method performs better in a wide variety of synthetic and real complex noise scenarios including face modeling, hyperspectral image restoration, and background subtraction applications. Additionally, our methods are capable of automatically learning the number of components from data, and thus can be used to deal with more complex applications. In the future, we'll attempt to extend the noise modeling methodology under PMoEP to more computer vision and machine learning tasks, e.g., the high-order low rank tensor factorization problems.\n\n\\section*{Acknowledgements}\nThis research was supported by 973 Program of China\nwith No.3202013CB329404, the NSFC projects with No.11131006, 91330204 and 61373114.\n\n\\appendices\n\\section{Proof of Theorem 1}\n\\begin{proof}\n\t(i) First, we calculate that\n\t\\footnotesize\n\t\\begin{eqnarray}\n\t\\begin{split}\n\tl_{P}^{\\textsc{C}}(\\mathbf{\\Theta})-l_{P}^{C}(\\mathbf{\\Theta}^{(t)})\n\t&=l(\\mathbf{\\Theta})-l(\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t &=\\log{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~-\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t&\\geq \\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~-\\log{\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber \\\\\n\t &=\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}\\\\\n\t&~~~+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda).\\nonumber\n\t\\end{split}\n\t\\end{eqnarray}\n\tLet $\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t)})=\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\frac{\\mathbb{P}(\\mathbf{E}|\\mathbf{Z};\\mathbf{\\Theta})\\mathbb{P}(\\mathbf{Z};\\mathbf{\\Theta})}\n\t\t{\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\mathbb{P}(\\mathbf{E};\\mathbf{\\Theta}^{(t)})}}$, then\n\t\n", "index": 35, "text": "\\begin{equation}\\label{lemma1}\n\tl_{P}^{C}(\\mathbf{\\Theta})\\geq l_{P}^{C}(\\mathbf{\\Theta}^{(t)}) + \\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\nonumber.\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"l_{P}^{C}(\\mathbf{\\Theta})\\geq l_{P}^{C}(\\mathbf{\\Theta}^{(t)})+\\Omega(\\mathbf%&#10;{\\Theta}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda).\" display=\"block\"><mrow><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi mathvariant=\"normal\">\u03a9</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udeaf</mi><mo stretchy=\"false\">|</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>\u03c0</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>-</mo><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\u03c0</mi><mo>;</mo><mi>\u03bb</mi><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\tBased on (\\ref{inequ}), the sequence $\\{l_{P}^{G}(\\mathbf{\\Theta}^{(t)})\\}_{t=1}^{\\infty}$ is nondecreasing and bounded above. Therefore, there exits a constant\n\t$l^{\\star}$ such that\n\t\n", "itemtype": "equation", "pos": 70291, "prevtext": "\n\t\\normalsize\n\t\\\\\n\t(ii) In the M step of Algorithm 1, it is obvious that\n\t\\footnotesize\n\t\\begin{eqnarray}\n\t\\mathbf{\\Theta}^{(t+1)}\\!&=&\\! \\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}\\!-\\!P(\\pi;\\lambda)\\right\\}\\nonumber\\\\\n\t \\!&=&\\!\\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\sum_{\\mathbf{Z}}\\mathbb{P}(\\mathbf{Z}|\\mathbf{E};\\mathbf{\\Theta}^{(t)})\\frac{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta})}}\n\t{\\log{\\mathbb{P}(\\mathbf{E},\\mathbf{Z};\\mathbf{\\Theta}^{(t)})}}\\!-\\!P(\\pi;\\lambda)\\right\\}\\nonumber\\\\\n\t \\!&=&\\!\\underset{\\mathbf{\\Theta}}{\\!\\arg\\max}\\left\\{\\Omega(\\mathbf{\\Theta}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi;\\lambda)\\right\\}.\\nonumber\n\t\\end{eqnarray}\n\t\\normalsize\n\tThus, we have\n\t\\begin{eqnarray}\n\t\\begin{split}\n\t&\\Omega(\\mathbf{\\Theta}^{(t+1)}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi^{(t+1)};\\lambda)\\\\\n\t&\\geq\n\t\\Omega(\\mathbf{\\Theta}^{(t)}|\\mathbf{\\Theta}^{(t)})+P(\\pi^{(t)};\\lambda)-P(\\pi^{(t)};\\lambda)=0\n\t\\end{split}\n\t\\end{eqnarray}\n\tThen, we can easily derive that\n\t\n", "index": 37, "text": "\\begin{equation}\\label{inequ}\n\tl_{P}^{C}(\\mathbf{\\Theta}^{(t+1)})\\geq l_{P}^{C}(\\mathbf{\\Theta}^{(t)}).\\nonumber\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"l_{P}^{C}(\\mathbf{\\Theta}^{(t+1)})\\geq l_{P}^{C}(\\mathbf{\\Theta}^{(t)}).\" display=\"block\"><mrow><mrow><mrow><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\\end{proof}\n\n\\section{Exponential Power Distribution}\n\\subsection{Three different forms of Exponential Power Distribution}\nThe Exponential Power Distribution ($\\mu=0$) has the following three equivalent forms:\n\n", "itemtype": "equation", "pos": 70606, "prevtext": "\n\tBased on (\\ref{inequ}), the sequence $\\{l_{P}^{G}(\\mathbf{\\Theta}^{(t)})\\}_{t=1}^{\\infty}$ is nondecreasing and bounded above. Therefore, there exits a constant\n\t$l^{\\star}$ such that\n\t\n", "index": 39, "text": "\\begin{equation}\n\t\\lim_{t\\rightarrow \\infty}l_{P}^{C}(\\mathbf{\\Theta}^{(t)}) = l^{\\star}.\\nonumber\n\t\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\lim_{t\\rightarrow\\infty}l_{P}^{C}(\\mathbf{\\Theta}^{(t)})=l^{\\star}.\" display=\"block\"><mrow><mrow><mrow><munder><mo movablelimits=\"false\">lim</mo><mrow><mi>t</mi><mo>\u2192</mo><mi mathvariant=\"normal\">\u221e</mi></mrow></munder><mo>\u2061</mo><mrow><msubsup><mi>l</mi><mi>P</mi><mi>C</mi></msubsup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>\ud835\udeaf</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><msup><mi>l</mi><mo>\u22c6</mo></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nLet $\\tau = (p\\sigma^{p})^{\\frac{1}{p}}$, then\n\n", "itemtype": "equation", "pos": 70931, "prevtext": "\n\\end{proof}\n\n\\section{Exponential Power Distribution}\n\\subsection{Three different forms of Exponential Power Distribution}\nThe Exponential Power Distribution ($\\mu=0$) has the following three equivalent forms:\n\n", "index": 41, "text": "\\begin{equation}\\label{form1}\nf_{p}(x;0,\\sigma)=\\frac{1}{2\\sigma p^{\\frac{1}{p}}\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-\\frac{|x|^{p}}{p\\sigma^{p}}\\right\\}.\\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"f_{p}(x;0,\\sigma)=\\frac{1}{2\\sigma p^{\\frac{1}{p}}\\Gamma(1+\\frac{1}{p})}\\exp%&#10;\\left\\{-\\frac{|x|^{p}}{p\\sigma^{p}}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mn>0</mn><mo>,</mo><mi>\u03c3</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c3</mi><mo>\u2062</mo><msup><mi>p</mi><mfrac><mn>1</mn><mi>p</mi></mfrac></msup><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mfrac><msup><mrow><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup><mrow><mi>p</mi><mo>\u2062</mo><msup><mi>\u03c3</mi><mi>p</mi></msup></mrow></mfrac></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nLet $\\eta = \\frac{1}{\\tau^{p}}$, then\n\n", "itemtype": "equation", "pos": 71153, "prevtext": "\nLet $\\tau = (p\\sigma^{p})^{\\frac{1}{p}}$, then\n\n", "index": 43, "text": "\\begin{equation}\\label{form2}\nf_{p}(x;0,\\tau)=\\frac{1}{2\\tau\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-|\\frac{x}{\\tau}|^{p}\\right\\}.\\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"f_{p}(x;0,\\tau)=\\frac{1}{2\\tau\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-|\\frac{x}{\\tau%&#10;}|^{p}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mn>0</mn><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c4</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><msup><mrow><mo stretchy=\"false\">|</mo><mfrac><mi>x</mi><mi>\u03c4</mi></mfrac><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nNoting that $\\Gamma(1+\\frac{1}{p})=\\frac{1}{p}\\Gamma(\\frac{1}{p})$, then we can represent the above three forms in equivalent forms.\n\n\n\\subsection{Draw Samples from Exponential Power Distribution}\nThe second form of exponential power distribution is\n\n", "itemtype": "equation", "pos": 71339, "prevtext": "\nLet $\\eta = \\frac{1}{\\tau^{p}}$, then\n\n", "index": 45, "text": "\\begin{equation}\\label{form3}\nf_{p}(x;0,\\eta)=\\frac{\\eta^{\\frac{1}{p}}}{2\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-\\eta|x|^{p}\\right\\}.\\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"f_{p}(x;0,\\eta)=\\frac{\\eta^{\\frac{1}{p}}}{2\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-%&#10;\\eta|x|^{p}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mn>0</mn><mo>,</mo><mi>\u03b7</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><msup><mi>\u03b7</mi><mfrac><mn>1</mn><mi>p</mi></mfrac></msup><mrow><mn>2</mn><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><mrow><mi>\u03b7</mi><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">|</mo><mi>x</mi><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nLet $\\eta = \\frac{1}{\\tau^{p}}$, then\n\n", "itemtype": "equation", "pos": 71153, "prevtext": "\nLet $\\tau = (p\\sigma^{p})^{\\frac{1}{p}}$, then\n\n", "index": 43, "text": "\\begin{equation}\\label{form2}\nf_{p}(x;0,\\tau)=\\frac{1}{2\\tau\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-|\\frac{x}{\\tau}|^{p}\\right\\}.\\nonumber\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"f_{p}(x;0,\\tau)=\\frac{1}{2\\tau\\Gamma(1+\\frac{1}{p})}\\exp\\left\\{-|\\frac{x}{\\tau%&#10;}|^{p}\\right\\}.\" display=\"block\"><mrow><mrow><mrow><msub><mi>f</mi><mi>p</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mn>0</mn><mo>,</mo><mi>\u03c4</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>\u2062</mo><mi>\u03c4</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u0393</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo>{</mo><mrow><mo>-</mo><msup><mrow><mo stretchy=\"false\">|</mo><mfrac><mi>x</mi><mi>\u03c4</mi></mfrac><mo stretchy=\"false\">|</mo></mrow><mi>p</mi></msup></mrow><mo>}</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 72263, "prevtext": "\nSampling from the exponential power distribution contains two cases: $p\\geq 1$ and $0<p<1$.\n\\subsubsection{case 1: $p\\geq 1$}\nWe adopt the method proposed in \\cite{chiodi1995generation,marsaglia1964convenient,mineo2005software}.\n\\subsubsection{case 2: $0<p<1$}\nWhen $0<p<1$, the method proposed in \\cite{polson2014bayesian} is used. We sample the distribution in two steps:\n\n", "index": 49, "text": "\\begin{equation}\\label{samplew}\n(w|p) \\sim \\frac{1+p}{2}Ga(2+\\frac{1}{p},1) + \\frac{1-p}{2}Ga(1+\\frac{1}{p},1),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"(w|p)\\sim\\frac{1+p}{2}Ga(2+\\frac{1}{p},1)+\\frac{1-p}{2}Ga(1+\\frac{1}{p},1),\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">|</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mi>p</mi></mrow><mn>2</mn></mfrac><mi>G</mi><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow><mn>2</mn></mfrac><mi>G</mi><mi>a</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mi>p</mi></mfrac><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.01060.tex", "nexttext": "\nwhere $w$ is a intermediate variable. (\\ref{samplew}) can be sampled directly but (\\ref{samplebeta}) is difficult. Therefore, we\nadopt the slice sampling strategy in \\cite{bishop2006pattern}.\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n\\newpage\n\\fi\n\n\\bibliographystyle{ieee}\n\\bibliography{mybibfile}\n\n\n", "itemtype": "equation", "pos": 72390, "prevtext": "\n\n", "index": 51, "text": "\\begin{equation}\\label{samplebeta}\n(\\beta|\\tau,w,p) \\sim \\frac{1}{\\tau w^{\\frac{1}{p}}}\\left\\{1-|\\frac{\\beta}{\\tau w^{\\frac{1}{p}}}|\\right\\}_{+},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"(\\beta|\\tau,w,p)\\sim\\frac{1}{\\tau w^{\\frac{1}{p}}}\\left\\{1-|\\frac{\\beta}{\\tau w%&#10;^{\\frac{1}{p}}}|\\right\\}_{+},\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">(</mo><mi>\u03b2</mi><mo stretchy=\"false\">|</mo><mi>\u03c4</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mfrac><mn>1</mn><mrow><mi>\u03c4</mi><mo>\u2062</mo><msup><mi>w</mi><mfrac><mn>1</mn><mi>p</mi></mfrac></msup></mrow></mfrac><msub><mrow><mo>{</mo><mn>1</mn><mo>-</mo><mo stretchy=\"false\">|</mo><mfrac><mi>\u03b2</mi><mrow><mi>\u03c4</mi><mo>\u2062</mo><msup><mi>w</mi><mfrac><mn>1</mn><mi>p</mi></mfrac></msup></mrow></mfrac><mo stretchy=\"false\">|</mo><mo>}</mo></mrow><mo>+</mo></msub><mo>,</mo></mrow></math>", "type": "latex"}]