[{"file": "1601.08221.tex", "nexttext": "\n\\begin{table}\n\\centering\n\\begin{tabular}{| r | l |}\n\\hline\n\\bf Symbol & \\bf Meaning \\\\\n \\hline\n$R$              & performance goal \\\\\n$S$              & workload schedule \\\\\n$C_i$            & query class $i$ \\\\\n$q^i_j$          & $j$th query (of class $i$) \\\\\n$f^i_s$          & start-up cost of VM type $i$ \\\\\n$f^i_r$          & cost of renting VM type $i$ per unit time\\\\\n$l(q^j, i)$      & latency of query class $j$ on VM type $i$\\\\\n${p({R}, {S})}$ & penalty of $S$ under $R$\\\\\n${cost({R}, {S})}$    & total cost of $S$ under $R$\\\\\n$vm^i_j$         & $j$th VM (of type $i$) \\\\\n$vm^i$           &  VM of type $i$ \\\\\n$v_s$            & partial schedule at vertex $v$\\\\\n$v_u$            & unassigned queries at vertex $v$\\\\\n\\hline\n\\end{tabular}\n\\vspace{-0.5em}\n\\caption{Notation table.}\n\\vspace{-4mm}\n\\label{tbl:symbols}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n{\\bf Problem Complexity} Under certain conditions, the above optimization problem becomes the bin packing problem, where we try to ``pack'' each query  into one of the available ``VM-bins''. For this reduction, we need to assume that (1) the number of query classes is unbounded, (2) infinite penalty, ${p({R}, {S})} = \\infty$, if the performance goal is violated, and (3) the start-up cost $f^i_s$ is uniform across all VM types. Under these conditions, the problem is {\\tt NP-Hard}. However, these assumptions are not valid in our system. Limiting the number of query classes relaxes the problem to one that is of polynomial complexity, but is still not computationally feasible~\\cite{appalg}.\n\n\nOne can imagine a large number of greedy approximations to this optimization problem. One common approximation is the first-fit decreasing algorithm (FFD)~\\cite{ffd}, which sorts queries in decreasing order of latency and places each query on the first VM where the query ``fits'' (incurs no penalty). If the query will not fit on any VM, a new VM is created. Existing cloud management systems have used this approach (e.g.,~\\cite{opennebula,sci_place,hadoop_place}) for provisioning resources and scheduling queries. However, this approach is agnostic to the performance metric and thus fails to offer good schedules  for various performance metrics. For example, when applied for the workload and per query latency constraints shown in Figure~\\ref{fig:example_sla}, FFD schedules all queries on their own VM, which offer the same performance as Scenario 1 but uses an additional VM. A better approximation would be first-fit increasing (FFI), which is identical to FFD except it sorts the queries in ascending order. FFI produces the schedule that is depicted in Scenario 1, scheduling the four queries across three VMs without violating the performance goal.\n\n{WiSeDB\\xspace} departs from this ``one-heuristic-fits-all'' scheduling approach used by existing system~\\cite{opennebula}. Instead, its goal is to identify custom scheduling heuristics for application-defined performance goals. We next describe how {WiSeDB\\xspace} identifies such heuristics. \n\n\n\n\n\\section{Model Generation}\\label{s:model}\n{WiSeDB\\xspace} uses a rigorous learning framework to addresses the workload management problem we defined. Given application-defined query classes $C$ and a performance goal $R$, it trains a decision tree classifier that can be used to schedule workloads within the application's performance goal. Our  learning framework is independent of the specifics of the performance goal as well as the workload specifications (e.g., number of classes, query distribution across classes, query performance statistics, etc). The framework collects features from sample workloads and uses them to learn cost-effective query management strategies (aka \\emph{models}) customized to the application's goal and workload specification. In this section, we detail the model generation process. \n\n\\subsection{Approach Overview}\n\n\n\nOur goal is to identify a workload management strategy that  approximates the optimal schedule, i.e., the schedule that minimizes the total cost as defined in Equation~\\ref{eq:cost}. Towards this end, our approach collects training data from samples of optimal schedules and trains its decision tree classifier on these optimal solutions. \n\nFigure~\\ref{f:training} gives an overview of our approach.  Initially, we create a large number of \\emph{random sample workloads} based on query class definitions. Second, we represent the problem of scheduling workloads as a \\emph{graph navigation} problem. On this graph, edges represents query assignment decisions or resource provisioning decisions, and the weight of each edge is equal to the cost of that decision. Hence each path through the graph represents decisions that compose some workload schedule. Finding the optimal schedule is thus reduced to finding the shortest path through the graph. \n\nFor each decision within the optimal solution (i.e., optimal path), we extract a set of features to characterize the decision. We generate a  training set which includes all  collected features from all the optimal paths across all random workloads. Finally, we train a \\emph{decision tree model} on this training set. This model can then be used to schedule new workloads with close to minimum cost. Next we describe these steps in detail.\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figs/training_system.pdf}\n\\vspace{-7mm}\n\\caption{ {Generation of the decision model.}}\n\\vspace{-4mm}\n\\label{f:training}\n\\end{figure}\n\n\n\n\t\\subsection{Workload Sampling}\nIn order to obtain a training set, {WiSeDB\\xspace} first generates sample workloads based on the application-provided query classes $C$. We create $N$ random sample workloads each containing $m$ queries. Here, $N$ must be sufficiently large so that query interactions patterns emerge and the decision tree model can be properly trained. The number of queries in each workload, $m$,  must be  large enough so that interactions between query types can be amply explored.  However, it must also be sufficiently small so that for each workload we can identify the optimal schedule in a timely manner. In our experiments, we use $N = 3000$ and $m = 18$. \n\n\nIt is also important that our sampling covers the space of possible workloads. If the sampling is not uniform, the decision tree may not be able to learn how two query classes interact, or the decision tree may have very little information about certain classes. Hence, we generate sample workloads via uniform direct sampling of the query classes: for each of the $N$ workloads, we add a query of a randomly selected query class until each workload contains $m$ queries. While our training will be based on uniform sampling of the query classes, we show experimentally that {WiSeDB\\xspace} is capable of handling skewed workloads (Section~\\ref{s_sensitivity}).\n\n\n\n\\subsection{Optimal Schedule Generation}\\label{sec:graph}\n\nGiven a set of random workloads, our goal is to learn a model based on the optimal schedule for these workloads. To achieve that, we represent the workload management schedules as paths on a weighted graph and we find the best path on this graph. \n\n\n\n\n\n{\\bf Graph Construction} Given a random workload $Q=\\{q_1^x, q_2^y, \\dots\\}$, we construct a directed, weighted graph $G(V,E)$ where vertices represent intermediate steps in schedule generation, i.e., partial schedules and a set of remaining queries to be scheduled. Edges represent actions, such as renting a new VM or assigning a query to a VM. The cost (weight) of each edge will be the cost of performing a particular action (e.g., the cost of starting a new VM). We refer to this as a \\emph{scheduling graph}. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/graph.pdf}\n\\vspace{-1.0em}\n\\caption{ A subgraph of a scheduling graph for two query classes and $Q=\\{q^1_1, q^2_2, q^2_3\\}$. $G$ is one goal vertex.}\n\\vspace{-5mm}\n\\label{fig:graph}\n\\end{figure}\n\n\n\n\nSpecifically, each vertex $v\\in V$  has a schedule for some queries of the workload $Q$, $v_s=[vm^i_1, vm^k_2,\\dots]$, which represents the set of VMs to be rented for these queries. Each VM $j$ of type $i$, $vm^i_j$, is a queue of queries of some class that will be processed on that VM, $vm^i_j=[q^x_k, q^y_m,...]$. Hence, $v_s$ represents possible (potentially partial) schedules for the given workload $Q$. Each vertex $v \\in V$ also has a set of unassigned queries from $Q$, $v_u$, that still need to be placed onto some VMs. \n\nThe \\emph{start vertex}, $A \\in V$, represents the initial state where all queries are unassigned. Therefore, $A_u$ includes all the queries in the given workload and $A_s$ is empty. If a vertex $g \\in V$ has no unassigned queries, we say that vertex $g$ is a \\emph{goal vertex} and its schedule $g_s$ is a \\emph{complete} schedule. \n\nAn edge in $E$ represents one of two possible actions:\n\n\\begin{CompactEnumerate}\n\\item{A \\emph{start-up edge} $(u, v, i)$ represents renting a new VM of type $i$, $vm^i_j$. It connects a vertex $u$ to $v$ where $v$ has an additional empty VM, i.e., $v_s = u_s \\cup vm^i_j$. It does not assign any queries, so $u_u = v_u$. The weight of a start-up edge is the cost to provision a new VM of type $i$: $w(u, v) = f^i_s$.}\n\n\\item{A \\emph{placement edge} $(u, v, q^x_y)$ represents placing an unassigned query $q^x_y \\in u_u$ into the queue of a rented VM in $v_s$. It follows that $v_u = u_u - q^x_y$. {Because {WiSeDB\\xspace} is agnostic to the specifics of any particular query, the placement of an instance of query class $C_x$ is equivalent to placing any other instance of $C_x$.} Therefore, we include only a single placement edge per query class even if the class appears more than once in $u_u$. The cost of an edge that places query $q^x_y$ into a VM of type $i$ is the latency of query multiplied by the cost per unit time of the VM, plus any additionally incurred penalties: $w(u, v) = l(q^x_y, i) \\times f^i_r + {p({R}, {v_s})} - {p({R}, {u_s})}.$}\n\n\n\\end{CompactEnumerate}\n\nFigure~\\ref{fig:graph} shows part of a scheduling graph for a workload $Q=\\{q_1^1,q^2_2, q^2_3\\}$. $A$ represents the start vertex and $G$ represents a goal vertex. At each edge, either a query is assigned to an existing VM ($\\overline{AB}$, $\\overline{AC}$ ,$\\overline{BE}$, $\\overline{EG}$) or a new VM is created ($\\overline{BD}$, $\\overline{EF}$). The path $\\overline{ABEG}$ represents assigning the three queries, $q_1^1,q^2_2, q^2_3$, to execute in that order on $vm_1$.\n\nBased on the above definitions, the weight of a path from the start vertex to a goal vertex $g$ will be equal to the cost of the complete schedule of the goal vertex, ${cost({R}, {g_s})}$. Since all possible complete schedules are represented by some goal state, searching for the minimum cost path from a start vertex to any of the goal vertices will provide an optimal solution. We then use the A* search algorithm~\\cite{astar} to find the optimal path through the graph, noting the scheduling decision made at every step, i.e., which edge was selected at each vertex.  \n\n\n\n\n{\\bf Graph Reduction} To improve the runtime of the search algorithm, we reduce the graph in a number of ways. {First, we include a start-up edge only if the last VM provisioned has some queries assigned to it, i.e.,  we rent a new VM \\emph{only if the most recently provisioned VM is used by some queries}. {This eliminates paths that provision VMs that are never used.} Second, queries are assigned only to the most recently provisioned VM, i.e., each vertex has outgoing placement edges that assign a query \\emph{only to the most recently added VM}.} {This reduces the number of redundant paths in the graph, since each {combination of VM types and query orderings} is accessible by only a single path instead of by many paths.} \n\nWe now prove that this reduction can be applied without loss of optimality. Specifically, given a graph $G$ and a reduced graph $G_r$, all goal vertices with no empty VMs in $G$ are reachable in $G_r$.\n\n\\begin{proof}\nConsider an arbitrary goal vertex with no empty VMs $g \\in G$. For any vertex $v$, let $head(v_s)$ be the most recently created VM in $v_s$ (the head of the queue).\n\nLet us assume $head(g_s) = vm^i_j$ and $q^x_y$ is the last query scheduled in $vm^i_j$. Then there is a placement edge $e_p = (v, g, q^x_y)$ connecting some vertex $v$ with query $q^x_y$ in its set of unassigned queries, i.e., $q^x_y \\in v_u$, to $g$. Further, we know that $e_p$ is an edge of $G_r$ because $e_p$ is an assignment of a query to the most recently created VM. If $head(v_s)$ is non-empty, and $q^m_k$ is the last query on $head(v_s$), then there is another vertex $u$ connected to $v$ via a placement edge $e_p = (u, v, q^m_k)$ in $G_r$ by the same argument. If $head(v_s) = vm^i_j$ is empty, then there is a start-up edge $e_s = (y, v, i)$ connecting some vertex $y$, with the same set of unassigned queries as $v$, i.e., $v_u = y_u$, to $v$. We know that $e_s$ must be an edge of $G_r$ because $v$ can have at most one empty VM. This process can be repeated until the start vertex is reached. Therefore, there is a path from the start vertex in $G_r$ to any goal vertex with no empty VMs $g \\in G$.\n\\end{proof}\n\n\n{\\bf A* Search Acceleration}  We run the A* search algorithm~\\cite{astar} on this graph in order to find the optimal path from the start vertex to a goal vertex. {If $g$ is the goal vertex found on the optimal path, then $g_s$ is the optimal schedule for the workload.} \n\n {The A* algorithm can optionally take advantage of a heuristic function $h(v)$ in order to find the optimal path faster. The function $h(v)$ must approximate the cost from a vertex $v$ to the optimal goal vertex. In order for A* to guarantee correctness, the heuristic must be \\emph{admissible}, meaning that $h(v)$ never overestimates the cost.}  The heuristic function is problem specific. In our system, the cost of a path to $v$ is the cost of the schedule in $v_s$, $cost(R,v_s)$, and hence it  is calculated differently for different performance goals $R$. Therefore satisfying this requirement depends on the semantics of the performance metric. \n\n\n Here, we distinguish performance metrics that are \\emph{monotonically increasing} from those that are not. A performance goal is monotonically increasing if and only if the penalty incurred by a schedule $u_s$ never decreases when adding queries. Formally, at any assignment edge connecting $u$ to $v$, ${p({R}, {v_s})} \\geq {p({R}, {u_s})}$. Maximum query latency is an example of a monotonically increasing performance metric, since adding an additional query on the queue of the last provisioned VM will never decrease the penalty. Average latency is not monotonically increasing, since adding a short  query may decrease the average latency and thus the penalty.\n\nFor monotonically increasing performance goals, we use the following heuristic function\\footnote{\\small For performance goals that are not monotonically increasing, we do not use a heuristic, which is equivalent to using the null heuristic, $h(v) = 0$.}:\n\n", "itemtype": "equation", "pos": 21144, "prevtext": "\n\\begin{sloppypar}\n\n\\title{WiSeDB: A Learning-based Workload Management \\\\ Advisor for Cloud Databases}\n\n\n\\author{\nRyan Marcus \\\\\nBrandeis University \\\\\n\\texttt{rcmarcus@brandeis.edu}\n\\and\nOlga Papaemmanouil \\\\\nBrandeis University \\\\\n\\texttt{olga@cs.brandeis.edu}\n}\n\n\n\\maketitle\n\n\\begin{abstract}\nWorkload management for cloud databases deals with the tasks of resource provisioning, query placement and query scheduling in a manner that meets the application's performance goals while minimizing the cost of using cloud resources. Existing solutions have approached these challenges in isolation and with only a particular type of performance goal in mind. In this paper, we introduce, {WiSeDB\\xspace}, a learning-based framework for generating \\emph{end-to-end} workload management solutions customized to application-defined performance metrics and workload characteristics. Our approach relies on decision tree learning to  train offline cost-effective models for guiding query placement, scheduling, and resource provisioning decisions. These models can be used for both batch and online scheduling of incoming workloads.  A unique feature of our system is that it can adapt its offline model to  stricter/looser performance goals with minimal re-training. This  allows us to present to the user alternative workload management solutions that address the typical performance vs  cost trade-off of cloud services. Experimental results show that our approach has very low training overhead while it discovers near optimal solutions for a variety of performance goals and workload characteristics.\n\n\\end{abstract}\n\n\\section{Introduction}\\label{s:intro}\n\nCloud computing has transformed the way data-centric applications are deployed by reducing data processing services to commodities that can be acquired and paid for on-demand. Despite the increased adoption of cloud databases,\nchallenges related to workload management still exist. These challenges include tasks such as provisioning cloud resources (e.g., virtual machines (VMs)), assigning incoming queries to provisioned VMs, and query scheduling within a VM in order to meet  performance goals. \nThese tasks strongly depend on  application-specific workload characteristics and performance goals, and they are typically addressed by ad-hoc implementations at the application-level.\n \n A growing number of efforts in cloud databases attempt to tackle these challenges (e.g., ~\\cite{cloud-optimizer, yun_sla-tree11, smartsla11, pmax, icbs, q-cop, slo_jignesh12, bazaar, activesla_hakan11}).  However, these techniques suffer from two main limitations. First, they do not provide an end-to-end workload management solution but instead focus on individual aspects of the problem, such as  query admission~\\cite{activesla_hakan11, q-cop},  query placement to VMs~\\cite{yun_sla-tree11, pmax, slo_jignesh12}, query scheduling within a VM~\\cite{yun_sla-tree11, icbs}, or VM provisioning~\\cite{slo_jignesh12, bazaar,cloud-optimizer,smartsla11}. Since  these solutions are developed independently of each other, their integration into a unified framework requires substantial effort and investment to ``get it right'' for each specific case.\nSecond, while  a broad range of performance criteria are covered by these  systems, (e.g., query response time ~\\cite{yun_sla-tree11, smartsla11, pmax, icbs}, average query latency of a workload~\\cite{pmax}) each one offers  solutions tuned  for specific performance metrics and adapting them to support custom application-specific metrics is not a straightforward task. \n\nWe argue that cloud-based databases could benefit from a workload management advisor service that removes the burden of the above challenges from the application developers. Applications should be able to specify their workload characteristics and performance objectives to this service which returns a set of low-cost and performance-efficient strategies for executing their workloads on a cloud infrastructure. \n\nWe have identified three main design goals for such an advisor service. First, given an incoming query workload and a performance goal, the service  should provide \\emph{end-to-end solutions} for deploying the  workload on a cloud infrastructure ({\\bf Design Goal 1}). Each solution should indicate: (a) the cloud resources  to be provisioned (e.g., number/type of VMs), (b) the distribution of resources among the workload queries (e.g., which VM will execute a given query) and (c) the execution order of these queries. We refer to these solutions collectively as {\\em workload schedules}.\n\nSecond, since cloud providers offer their resources for some cost (i.e., price/hour for renting a VM), optimizing workload schedules for this cost is vital for cloud-based applications. Hence, any workload management advisor should be \\emph{cost-aware} (\\textbf{Design Goal 2}). In the cloud computing environment, cost functions are available through contracts between the service providers and their customers, in the form of service level agreements (SLAs). These cost functions define the price for renting cloud resources, the performance goals, as well as the penalty the  provider has to pay if the agreed-upon performance is not met. A workload management service should address all these cost factors while assisting  applications in exploring performance vs. cost trade-offs. \n\n\nFinally, the diversity of the data processing applications and workloads (e.g., scientific, financial, e-commerce, business-intelligence, etc) unavoidably implies the need for customizable workload management service that support equally diverse performance criteria. Therefore we envision a \\emph{metric-independent }service that allows applications to define \\emph{custom application-level goals} ({\\bf Design Goal 3}). Since different workload schedules offer different performance vs. cost trade-offs for different performance metrics, the system should be able to discover the ``best'' heuristic for executing a given workload under an application-specific performance goal. In all cases,  low cost workload schedules should be discovered independently of the performance metric. \n\nThis paper introduces \\emph{{WiSeDB\\xspace}} ([W]orkload management [Se]rvice for cloud [DB]s), a workload management advisor for cloud databases designed to satisfy the above requirements. {WiSeDB\\xspace} relies on a learning framework that ``learns'' close-to-optimal heuristics for executing incoming workloads under specific performance metrics and constraints. Here, we measure optimality based on the monetary cost of executing these workloads.  \n\nWiseDB's heuristics are expressed as \\emph{decision models} generated by a decision tree classifier and tuned for application-defined performance goals and workload characteristics ({\\bf Goal 2}). For a given workload, {WiSeDB\\xspace} can parse the model to identify  the number/type of VMs to provision,  the  query assignment to these VMs and execution order within each VM in order to execute the workload with low cost ({\\bf Goal 1}). Each model is cost-aware: it is trained on a set of performance and cost-related features collected from mininum cost (optimal) schedules of random workload samples (\\textbf{Goal 3}). This cost  accounts for resource provisioning as well as any penalties paid due to violations of the performance goals. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven the workload specification and performance goal, {WiSeDB\\xspace} generates offline a custom decision model which it can  then leverage in multiple directions. First, given an incoming \\emph{batch workload} and a decision model to use, {WiSeDB\\xspace} parses the model to generate (a) a low cost  schedule for executing the  workload on cloud resources and (b) a cost estimation for this schedule. Although the model is trained offline once, it can be used at runtime to generate schedules for \\emph{any} workload that matches the model's workload specifications. \n\nSecond, the training set of the  model can be re-used to generate with small training overhead a set of \\emph{alternative decision models} for the same workload specification but stricter and more relaxed performance constraints. Several models are presented to the user along with the cost estimations as functions of the workload size. This allows applications to explore the performance vs. cost trade-off for their workloads (\\textbf{Goal 3}). Finally, the model can be adjusted with small overhead during runtime to support \\emph{online scheduling} of queries arriving one at a time (as opposed to batch workloads). \n\n \n\n\n\n\n\n\n \n\n\n\n \n\n\n\nThe contributions of this work can be summarized as follows:\n\n\\begin{CompactEnumerate}\n\\item We introduce {\\em {WiSeDB\\xspace}} a workload management advisor for cloud databases. We discuss its  design and rich functionality, which ranges from recommendations of alternative workload execution strategies that cover the space of performance vs. cost trade-offs to resource provisioning and query scheduling recommendations for both batch and online processing. All recommendations are tuned for application-specific performance goals and workload characteristics. \n\\item We propose a novel learning approach to the workload management problem. {WiSeDB\\xspace} expresses its optimization problem as a path search problem on a weighted graph in order to learn from the decisions that lead to the  optimal workload schedules for a given performance goal. It then extracts performance and cost related features from these decisions to learn the best heuristic for executing any workload under the same performance objective. \n\\item We propose an {\\em adaptive} modeling technique that allows {WiSeDB\\xspace} to generate with minimum  overhead (a) a set of alternative models for batch scheduling and (b) new low-cost models upon arrival of a new query  for online scheduling.  \n\\item We present experimental results that prove our ability to learn good heuristics for a number of performance metrics with very small training overhead. These heuristics offer close-to-optimal  solutions for both batch and online scheduling independently of the size and skewness of the workload. \n\\end{CompactEnumerate}\n\n\n\nThis paper first discusses the system model of our workload management service (Section~\\ref{s:system}). We formally define our optimization problem in Section~\\ref{s:problem}. We introduce our modeling framework in Section~\\ref{s:model} and the adaptive modeling approach in Section~\\ref{sec:shift}. Section~\\ref{s:runtime} discusses {WiSeDB\\xspace}'s runtime functionality. Section~\\ref{sec:expr} includes our experimental results. Section~\\ref{s:related} discusses related work, and our final remarks and  plans for future work are presented in Section~\\ref{s:conclusions}. \n\n\n\n\n\n\n\n\n\n\\section{System Model}\\label{s:system}\n\nOur system is designed for data management applications running on an Infrastructure-as-a-Service (IaaS) cloud (e.g.,~\\cite{url-amazonAWS,url-msazzure}). These providers typically offer VMs of different types (i.e., resource configurations) for a certain fee per renting period. We assume this deployment is realized by renting virtual machines (VMs) with preloaded database engines (e.g., MySQL~\\cite{url_mysql}, PostgreSQL~\\cite{url_prostgres}) and that queries can be executed locally on any of the rented VMs. This property is  offered by fully replicated databases\\footnote{\\small Partial replication/data partitioning models can also supported by specifying which data partitions can serve a given query.}. \n\n\n\\begin{figure}\n\t\\centering \n\t \\includegraphics[width=0.45\\textwidth] {figs/system.pdf}\n\t\n\t\\caption{{The {WiSeDB\\xspace} system model.} }\n\t\\label{fig:system-mod}\n\t\n\t\\vspace{-3mm}\n\\end{figure}\n\n\nUsers start their interaction with {WiSeDB\\xspace} by providing a \\emph{workload specification} which indicates groups of queries with similar performance characteristics. Specifically, queries  should be  grouped into \\emph{query classes}, such that each class represents queries with similar execution times  when processed on the same execution environment (e.g., resource configurations, concurrent queries, etc). Example groupings include  assigning to the same class instances of the same or similar query templates. These latency estimates can be provided by either the application itself (e.g., by executing representative queries a-priori on various VM types) or by using existing latency prediction models (e.g.,~\\cite{jennie_contender_edbt14,jennie_sigmod11}).  \n\n\n{WiSeDB\\xspace} users also specify their performance goals for their incoming workloads as functions of query latency. Performance goals can be defined either at the query class level or workload level and currently we support the following four types of  metrics. (1) {\\em Per Query Deadline}: users can specify an upper bound for the latency of each query in a class. (2) \\emph{Max Latency Deadline}: the users express an upper bound on the worst query response time in a given workload. (3) {\\em Average Deadline}: sets an upper limit on the average query latency of a  workload. (4) {\\em Percentile Deadline}: specifies that at  least $x\\%$ of the workload's queries must be completed within $t$ seconds. These metrics cover a range of performance goals typically used for database applications (e.g.,~\\cite{icbs,pmax,icbs,q-cop, activesla_hakan11}). However, {WiSeDB\\xspace} is the first system to support them within a single workload management framework. \n\nPerformance goals are expressed as part of an Service-Level-Agreement (SLA) between the IaaS provider and the application. The agreement states the workload specification (i.e., query classes), the expected performance goal, and a penalty function that defines the penalty to be paid to the application if that goal is not met. Our system is agnostic to the details of the penalty function but accounts for it in its cost-driven approach.  \n\n\nWorkload and performance specifications are submitted to {WiSeDB\\xspace} which collects a set of performance, cost and workload related features to train offline a decision model for guiding a low-cost execution of  future workloads (of the same specification) under the given performance goal (\\emph{Model Generation}). The training set for this model is then leveraged to generate alternative decision models for stricter and more relaxed performance goals (\\emph{Recommendations}). These models are presented to the user (along with their expected cost), who chooses the one that better fits their performance and budget constraints.   \n\nDuring runtime, given a decision model and an incoming workload, the model can be used to advise the application on (a) the type/number of VMs to be provisioned, (b) the assignment of queries  to these VMs and (c) the execution order of the queries assigned to each VM. These actions collectively can reduce penalties due to performance goal violations while minimizing the VM provisioning cost. This step is executed by the {\\em Scheduler} component which can generate {\\em schedules} (i.e., advice) for both batch workloads  as well as single queries as they arrive (online scheduling).  \n\n\n\n\n\n\nApplications  execute their workloads according to {WiSeDB\\xspace} recommendations. They rent VMs as needed and add queries in the processing queue of each VM according to the proposed schedule. VMs are released upon completion of the workload's execution. \n\n{\\bf Example} Figure~\\ref{fig:example_sla} compares two  workload management strategies. The workload specification includes two query classes, $C_1$ and $C_2$. Queries in the first class take two minutes to execute and queries in the second class take one minute with deadline of three and one minute respectively. Scenario 1 illustrates a possible scheduling of a workload consisting of four queries ($q^1_1$ from $C_1$ and $\\{q^2_2,q^2_3,q^2_4\\}$ from $C_1$)  across three VMs that meets the performance goal. Here, each query of class $C_1$ is assigned to its own VM since this class has the strictest deadline. The single query of class $C_2$ is scheduled to be executed after one of these queries completes.  Scenario 2 uses a different strategy for the same workload. Here, the query of class $C_1$ is scheduled first on one VM while a second VM executes a query of $C_2$. The remaining two queries of $C_2$ will be executed once these queries complete. This strategy requires fewer VMs, but it exhibits a violation and thus incurs a penalty. While the cost of VM provisioning is the same, (all machines are of the same type and the total rent period is 3 minutes), the presence of violations will increase the cost.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/example_sla.pdf}\n\\vspace{-1mm}\n\\caption{\\small {Two different schedules for $Q=\\{q^1_1, q^2_2, q^2_3, q^2_4\\}$. Queries in class $C_1$ and $C_2$ have deadlines of 1min and and 3min respectively.}}\n\\label{fig:example_sla}\n\\vspace{-4mm}\n\\end{figure}\n \n\n\n\n\\section{Problem definition}\\label{s:problem}\n\nHere, we formally define our system's optimization goal and discuss the problem's complexity. {Applications provide a set of \\textit{query classes} $C= \\{C_1, C_2, \\dots\\}$ and a \\textit{performance goal} $R$.} Each query class $C_j \\in C$ represent groups of queries that have similar latencies when executed with the same resources. We denote the latency of a query $q$ executed on a VM of type $i$, $vm^i$, as $l(q, i)$\n\n\n\n\n\nGiven the above, query classes are defined by the expected latencies $l(q, i)$ of its queries for each available VM type $i$. Obviously prediction errors in latency estimates can lead to ``noisy'' query class definitions (e.g., not all queries in the class demonstrate identical latency). However, {WiSeDB\\xspace} can gracefully adapt to a certain level of  prediction errors, as shown in Section~\\ref{s_sensitivity}.\n\nQueries assigned to a VM can be executed immediately or can be placed in a VM's processing queue if no more concurrent queries can be processed\\footnote{\\small Most DBMSes put an upper limit on the number of concurrent queries, referred to as multiprogramming level.}. In the latter case, the query's execution time includes queued wait time.\n\nLet us assume a workload $Q=\\{q_1^x, q_2^y, \\dots\\}$ where each query $q_i^j \\in Q$ belongs to query class $C_j \\in C$. \\cut{For convenience, we will write $q_i^j$ as $q^j$ when only the query class is relevant, {\\color{magenta}[\\textit{{and we will write $q_i^j$ as $q_i$ when the query class is not relevant. }}]}} \nWe  represent a VM of type $i$, $vm^i$, as a queue $vm^i=[q_1^x, q_2^y, \\dots]$ of queries to process in that order and a \\emph{schedule} $S=\\left[vm^i_1, vm^j_2, \\dots \\right]$ of the workload $Q$ as a list of VMs such that each VM contains only queries from $Q$.  A \\emph{complete} schedule assigns each query in $Q$ to one VM. \n\nGiven the set of query classes $C$, {WiSeDB\\xspace} generates decision models for scheduling workloads with queries instances from $C$. At runtime, given an incoming workload $Q$, {WiSeDB\\xspace} uses these models and identifies a  \\emph{workload schedule} $S$  for executing this workload. Schedules indicates (1) the number and type of VMs needed,  (2) the assignment of each query $q^i_j \\in Q$ to these VMs and (3) the query execution order on each VM, $vm^i_j \\in S$.\n\n\n{\\bf Cost Model} To calculate the monetary cost of processing a workload, we assume each VM of type $i$ has a fixed start-up cost $f^i_s$, as well as a running cost $f^i_r$ per unit of time (i.e. the price for renting the VM for that time unit). We also assume a penalty function ${p({R}, {S})}$ that estimates the penalty for a given schedule $S$ and performance goal $R$. Without loss of generality, we assume penalties are calculated based on the violation period, i.e., the IaaS will pay a fixed amount per second of violation within a schedule $S$. \n\nThe \\emph{violation period} is the duration of time that the performance objective $R$ was not met. Figure~\\ref{fig:example_sla} shows an example violation period in which $R$ constrains the execution time of each query. Here, the violation period (Scenario 2) for each query is measured from the time point it missed its deadline until its completion. {For the maximum latency metric the violation period is computed in the same way but now each query has the same deadline.} For an average latency performance goal the duration of the violation period is the difference between the desired average latency and the  actual average latency of each query. For a percentile performance goal that requires $x\\%$ of queries to be complete in $y$ minutes, the violation period is the amount of time in which $100-x\\%$ of the queries had latencies exceeding $y$ minutes.\n\n\n\n\n\n\n\n\n\n\n{\\it \\underline{Problem Definition}} For a workload $Q$, where each query $q_i^j$ belongs to a class $C_j$, and a performance goal $R$, our goal is to  find a schedule that minimizes the total monetary cost (provisioning, processing, and penalty payouts) of executing the workload. Formally, our goal is to identify a {complete schedule} $S$  that minimizes the total cost ${cost({R}, {S})}$, defined as:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:cost}\n{cost({R}, {S})} = \\sum_{vm^i_j \\in S} \\left[ f^i_s +\\sum_{q^m_k \\in vm^i_j} f^i_r \\times l(q^m_k, i) \\right] + {p({R}, {S})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"{cost({R},{S})}=\\sum_{vm^{i}_{j}\\in S}\\left[f^{i}_{s}+\\sum_{q^{m}_{k}\\in vm^{i%&#10;}_{j}}f^{i}_{r}\\times l(q^{m}_{k},i)\\right]+{p({R},{S})}\" display=\"block\"><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mi>v</mi><mo>\u2062</mo><msubsup><mi>m</mi><mi>j</mi><mi>i</mi></msubsup></mrow><mo>\u2208</mo><mi>S</mi></mrow></munder><mrow><mo>[</mo><mrow><msubsup><mi>f</mi><mi>s</mi><mi>i</mi></msubsup><mo>+</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>q</mi><mi>k</mi><mi>m</mi></msubsup><mo>\u2208</mo><mrow><mi>v</mi><mo>\u2062</mo><msubsup><mi>m</mi><mi>j</mi><mi>i</mi></msubsup></mrow></mrow></munder><mrow><mrow><msubsup><mi>f</mi><mi>r</mi><mi>i</mi></msubsup><mo>\u00d7</mo><mi>l</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>q</mi><mi>k</mi><mi>m</mi></msubsup><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08221.tex", "nexttext": "\nwhich calculates the cheapest possible runtime for the queries that are still unassigned at $v$. In other words, $h(v)$ sums up the cost of the cheapest way to process each remaining query, assuming VMs could be created for free.\n\nEquation~\\ref{eq:h} will never overestimate the optimal cost to a goal vertex. Regardless of performance goals, query classes, or VM performance, one always has to pay the cost of renting a VM for the duration of the queries. More formally, every path from an arbitrary vertex $v$ to a goal vertex must include an assignment edge, with cost $l(q^x_y, i) \\times f^i_r + {p({R}, {v_s})} - {p({R}, {u_s})}$, for each query in $v_u$ and for some VM type $i$. When the performance goal is monotonically increasing, the term ${p({R}, {v_s})} - {p({R}, {u_s})}$ is never negative, so $h(v)$ is never larger than the actual cost to the goal vertex.\n\n\n\\subsection{Feature Extraction}\nAfter running the A* algorithm on each of the sampled query workloads, we will have the optimal schedules (i.e., optimal graph paths) for $N$ different workloads. In order to build a training set, we extract features from each of the vertices in the optimal paths we discovered. These features are extracted from \\emph{all} the paths we collected for all of the random workloads we used. \n\nThe training set consists of $(decision, features)$ pairs indicating the decisions that was made by A* while calculating optimal schedules and performance/workload related features at the time of the decision. Each decision represents an edge in the search graph, and is therefore a decision to either (a) create a new VM of type $i$, or (b) assign a query of class $C_j$ to the most recently created VM. Thus, the domain of possible decisions is equal to the sum of the number of query classes and the number of VM types. We map each decision (edge) $(u,v)$ in the optimal path to a set of features of its origin vertex $u$. \n\nWe collect features from each vertex because there is a correspondence between a vertex and the optimal decision made at that vertex. Specifically, for a given vertex $v$, the edge selected in the optimal path is independent of $v$'s parents or children but depends on the unassigned queries $v_u$ and a schedule so far $v_s$. However, the domains of $v_u$ and $v_s$ are far too large to enumerate and do not lend themselves to machine learning algorithms ($v_u$ and $v_s$ are neither numeric or categorical). Therefore, we extract features from $v$ that provide information about $v_s$ and $v_u$, but have much smaller domains and are either numeric or categorical.\n\nFor each vertex $v$ along the optimal path, we extract the features:\n\\begin{CompactEnumerate}\n\\item{\\texttt{wait-time}: the amount of time that a query would have to wait before being processed if it were placed on the most recently created VM. {Formally, \\texttt{wait-time} is equal to execution time of all the queries already assigned to the last VM. \\cut{ $\\sum_{q^x_y \\in head(v_s)} l(q^x_y, i)$ where $i$ is the type of the most recently provisioned VM, $head(v_s)$.} This feature can help our model decide which queries should be placed on the last added VM based on their deadline. For example, if a machine's wait time has exceeded the deadline of a query class, it is likely that no more queries from this class should be assigned to that VM. Alternatively, if a machine's wait time is very high, it is likely that only short queries should be assigned.}} \n\n\\item{\\texttt{proportion-of-X}: the proportion of queries on the most recently created VM that belong to query class \\texttt{X}. {In other words, \\texttt{proportion-of-X} is the ratio between the number queries of class \\texttt{X} assigned to the VM and the total number of queries assigned to the VM. For example, if the VM currently has four queries assigned to it, with one of those queries being of class $C_1$ and three being of class $C_2$ , then we extract the features \\texttt{proportion-of-C1=0.25} and \\texttt{proportion-of-C2=0.75}. We only need to consider to most recently created VM because the assignment edges in the reduced graph only assign queries to the most recent VM. Since each sample workload contains only a limited number of queries, keeping track of the exact number of instances of each query class would not scale to large workloads. Therefore, we track the proportion instead.}}\n\n\n\\item{\\texttt{cost-of-X}: the cost incurred (including any penalty costs) by placing a query of class \\texttt{X} on the most recently created VM. \n\n{\\texttt{cost-of-X} is equal to the weight of the outgoing assignment edge for class \\texttt{X}. This allows our model to check the cost of placing an instance of a certain query class and based on its value decide on whether to assign another query class to the last rented VM or create a new VM.}}\n\\item{\\texttt{have-X}: whether or not a query of class \\texttt{X} is still unassigned.  This feature helps our model understand how the class of the unassigned queries affects the decisions on the optimal path. If there is no instance of some query class $C_j$ available to place, then the model places one of the remaining classes, while if an instance of $C_j$ exists, the model might prefer to schedule that as opposed to any other class.}\n\\end{CompactEnumerate}\n\nWhile these features are not enough to \\emph{uniquely} identify a vertex and hence learn the exact conditions that lead to the optimal schedule, they can shed light on the workload/performance conditions related to the optimal decision. Learning based on these features helps {WiSeDB\\xspace} predict when to provision a new VM and how to assign queries and eventually offer close-to-optimal schedules.\n\n\n\n\n\n\n\n\n\\subsection{Workload Management Model}\n\\label{sec:model}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{figs/dt.pdf}\n\t\\vspace{-1mm}\n\t\\caption{An example decision model.}\n\t\\vspace{-4mm}\n\t\\label{fig:dt}\n\\end{figure}\n\nAfter building up a training feature set, we create a decision tree model using the J48 algorithm~\\cite{weka}. Figure~\\ref{fig:dt} shows a  decision model.  Each feature node (orange nodes) of the decision tree represents either a binary split on a numeric feature, or an $n$-way split on a categorical feature, where $n$ is the number of categories. The decision nodes (white nodes) represent the suggested actions. \\cut{The decision tree maps a vertex $v$ to an action $a$ by extracting features from $v$ and descending through the tree. Since each action $a$ is represented by a single out-edge of $v$, one can navigate the decision tree by traversing the edge corresponding to the action selected.}\n\n\n\n\nWe demonstrate how to traverse the decision tree with an example. The right side of Figure~\\ref{fig:dt} shows how we use the decision tree comes up with the schedule in Scenario 1 of Figure~\\ref{fig:example_sla} (minus one query). Here we have two query classes, $C_1$ and $C_2$. Each query in $C_1$ is expected to have a latency of two minutes and the performance goal is to offer a latency of no more than three minutes per query. Instances of $C_2$ will have a latency of one minute and the performance goal is for each query in this class to be completed within one minute.  {WiSeDB\\xspace} (assuming VMs of a single type and that queries are executed in isolation)  generates the decision tree in  Figure~\\ref{fig:dt}.\n\nGiven a workload $Q=\\{q^1_1,q^2_2,q^2_3\\}$,  the tree is parsed as follows. In the first node (1), we check the wait time, which is zero since all queries are unassigned, hence we proceed to node (3). The workload has queries of class $C_2$ and therefore we proceed to node (4). Here we calculate the  cost of placing an instance of $C_2$. Let us assume the cost is is less than 100 (no penalty is incurred), which leads to node (6) which assigns an instance of $C_2$ to a new VM. Since we have more queries in the workload we next re-parse the decision tree. In (1) the wait time on the most recent VM is now 1 minute (the runtime of queries in $C_2$) so we move to node (3). Since we have one more query of $C_2$ unassigned, we move to (4). Now the cost of assigning a query of $C_2$ is more than 100 since the new query would need to wait for $q^2_1$ to complete (and thus incur a penalty). Hence, we move to node (7) and we check if there are any  unassigned instances of $C_1$ in our workload. Since there is ($q^1_1$), we assign $q^1_1$ to the last VM. We re-parse the tree in the same way and {by following nodes (1)$\\rightarrow$(2)}, then again as (1)$\\rightarrow$(3)$\\rightarrow$(4)$\\rightarrow$(7)$\\rightarrow$(9), so we assign the remaining query $q^2_3$ onto a new VM. \n\nIn general, the decision model will place an instance of $C_2$, then an instance of $C_1$, and then create a new VM. This process will repeat itself until queries of $C_1$ or $C_2$ are depleted. When one query class is depleted, single instances of the remaining class will be placed on new VMs until none remain. For the case of these two query classes, the heuristic learned by {WiSeDB\\xspace} is equivalent to first-fit increasing, which sorts queries in decreasing order of latency and places each query on the first VM where the query ``fits'' (i.e., incurs no penalty).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Adaptive Modeling}\\label{sec:shift}\n\n\n\n\n\n\n\n\nIt is often desirable to allow the user to explore potential performance and cost trade-offs within the space of possible performance goals for their specific workload~\\cite{pslas}. This can be achieved by generating different models for the same workload with stricter/looser performance constraints and thus higher/lower costs. However, {WiSeDB\\xspace} tunes its decision model for a specific performance goal. Changes in this goal will trigger {WiSeDB\\xspace} to re-train the decision model, since changes in performance constraints lead to different optimal schedules and hence to different training sets. Therefore, generating a set of alternative decision models for the same workload and different performance constraints could impose significant training overhead.  \n\n\nTo address this challenge, {WiSeDB\\xspace} employs a technique that adapts an existing model trained for a given workload and performance goal to a new model for the same workload and stricter performance goals with minimal re-training. The main idea is that if two decision models are trained for the same query classes $C$ and similar performance goals, their training sets will share significant information. Our approach relies on the adaptive A* algorithm~\\cite{adaptiveastar} which reuses information from one graph to create a better search heuristic, $h(v)$, to parse  another graph with identical structure but increased edge weights. We refer to this as \\emph{adaptive modeling}.\n\n\n\n\n \n\n\n\n\n\n Let us assume a model $\\alpha(C,R)$ and a desired new model $\\alpha^\\prime (C, R^\\prime)$  with the same query classes $C$ but a slightly different {performance goal} $R^\\prime$. \nWithout loss of generality, we only consider cases where the performance goal $R^\\prime$ is more strict that $R$. Of course, one can always start with a substantially loose performance goal than the one requested and restrict it incrementally. \n\nFor any given schedule $S$ we generate with model $\\alpha$, Equation~\\ref{eq:cost} indicates that the new performance goal will only affect the penalty. Specifically, since stricter performance constraints on the same schedule can lead only to more violations we have that:\n\n", "itemtype": "equation", "pos": 36364, "prevtext": "\n\\begin{table}\n\\centering\n\\begin{tabular}{| r | l |}\n\\hline\n\\bf Symbol & \\bf Meaning \\\\\n \\hline\n$R$              & performance goal \\\\\n$S$              & workload schedule \\\\\n$C_i$            & query class $i$ \\\\\n$q^i_j$          & $j$th query (of class $i$) \\\\\n$f^i_s$          & start-up cost of VM type $i$ \\\\\n$f^i_r$          & cost of renting VM type $i$ per unit time\\\\\n$l(q^j, i)$      & latency of query class $j$ on VM type $i$\\\\\n${p({R}, {S})}$ & penalty of $S$ under $R$\\\\\n${cost({R}, {S})}$    & total cost of $S$ under $R$\\\\\n$vm^i_j$         & $j$th VM (of type $i$) \\\\\n$vm^i$           &  VM of type $i$ \\\\\n$v_s$            & partial schedule at vertex $v$\\\\\n$v_u$            & unassigned queries at vertex $v$\\\\\n\\hline\n\\end{tabular}\n\\vspace{-0.5em}\n\\caption{Notation table.}\n\\vspace{-4mm}\n\\label{tbl:symbols}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n{\\bf Problem Complexity} Under certain conditions, the above optimization problem becomes the bin packing problem, where we try to ``pack'' each query  into one of the available ``VM-bins''. For this reduction, we need to assume that (1) the number of query classes is unbounded, (2) infinite penalty, ${p({R}, {S})} = \\infty$, if the performance goal is violated, and (3) the start-up cost $f^i_s$ is uniform across all VM types. Under these conditions, the problem is {\\tt NP-Hard}. However, these assumptions are not valid in our system. Limiting the number of query classes relaxes the problem to one that is of polynomial complexity, but is still not computationally feasible~\\cite{appalg}.\n\n\nOne can imagine a large number of greedy approximations to this optimization problem. One common approximation is the first-fit decreasing algorithm (FFD)~\\cite{ffd}, which sorts queries in decreasing order of latency and places each query on the first VM where the query ``fits'' (incurs no penalty). If the query will not fit on any VM, a new VM is created. Existing cloud management systems have used this approach (e.g.,~\\cite{opennebula,sci_place,hadoop_place}) for provisioning resources and scheduling queries. However, this approach is agnostic to the performance metric and thus fails to offer good schedules  for various performance metrics. For example, when applied for the workload and per query latency constraints shown in Figure~\\ref{fig:example_sla}, FFD schedules all queries on their own VM, which offer the same performance as Scenario 1 but uses an additional VM. A better approximation would be first-fit increasing (FFI), which is identical to FFD except it sorts the queries in ascending order. FFI produces the schedule that is depicted in Scenario 1, scheduling the four queries across three VMs without violating the performance goal.\n\n{WiSeDB\\xspace} departs from this ``one-heuristic-fits-all'' scheduling approach used by existing system~\\cite{opennebula}. Instead, its goal is to identify custom scheduling heuristics for application-defined performance goals. We next describe how {WiSeDB\\xspace} identifies such heuristics. \n\n\n\n\n\\section{Model Generation}\\label{s:model}\n{WiSeDB\\xspace} uses a rigorous learning framework to addresses the workload management problem we defined. Given application-defined query classes $C$ and a performance goal $R$, it trains a decision tree classifier that can be used to schedule workloads within the application's performance goal. Our  learning framework is independent of the specifics of the performance goal as well as the workload specifications (e.g., number of classes, query distribution across classes, query performance statistics, etc). The framework collects features from sample workloads and uses them to learn cost-effective query management strategies (aka \\emph{models}) customized to the application's goal and workload specification. In this section, we detail the model generation process. \n\n\\subsection{Approach Overview}\n\n\n\nOur goal is to identify a workload management strategy that  approximates the optimal schedule, i.e., the schedule that minimizes the total cost as defined in Equation~\\ref{eq:cost}. Towards this end, our approach collects training data from samples of optimal schedules and trains its decision tree classifier on these optimal solutions. \n\nFigure~\\ref{f:training} gives an overview of our approach.  Initially, we create a large number of \\emph{random sample workloads} based on query class definitions. Second, we represent the problem of scheduling workloads as a \\emph{graph navigation} problem. On this graph, edges represents query assignment decisions or resource provisioning decisions, and the weight of each edge is equal to the cost of that decision. Hence each path through the graph represents decisions that compose some workload schedule. Finding the optimal schedule is thus reduced to finding the shortest path through the graph. \n\nFor each decision within the optimal solution (i.e., optimal path), we extract a set of features to characterize the decision. We generate a  training set which includes all  collected features from all the optimal paths across all random workloads. Finally, we train a \\emph{decision tree model} on this training set. This model can then be used to schedule new workloads with close to minimum cost. Next we describe these steps in detail.\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figs/training_system.pdf}\n\\vspace{-7mm}\n\\caption{ {Generation of the decision model.}}\n\\vspace{-4mm}\n\\label{f:training}\n\\end{figure}\n\n\n\n\t\\subsection{Workload Sampling}\nIn order to obtain a training set, {WiSeDB\\xspace} first generates sample workloads based on the application-provided query classes $C$. We create $N$ random sample workloads each containing $m$ queries. Here, $N$ must be sufficiently large so that query interactions patterns emerge and the decision tree model can be properly trained. The number of queries in each workload, $m$,  must be  large enough so that interactions between query types can be amply explored.  However, it must also be sufficiently small so that for each workload we can identify the optimal schedule in a timely manner. In our experiments, we use $N = 3000$ and $m = 18$. \n\n\nIt is also important that our sampling covers the space of possible workloads. If the sampling is not uniform, the decision tree may not be able to learn how two query classes interact, or the decision tree may have very little information about certain classes. Hence, we generate sample workloads via uniform direct sampling of the query classes: for each of the $N$ workloads, we add a query of a randomly selected query class until each workload contains $m$ queries. While our training will be based on uniform sampling of the query classes, we show experimentally that {WiSeDB\\xspace} is capable of handling skewed workloads (Section~\\ref{s_sensitivity}).\n\n\n\n\\subsection{Optimal Schedule Generation}\\label{sec:graph}\n\nGiven a set of random workloads, our goal is to learn a model based on the optimal schedule for these workloads. To achieve that, we represent the workload management schedules as paths on a weighted graph and we find the best path on this graph. \n\n\n\n\n\n{\\bf Graph Construction} Given a random workload $Q=\\{q_1^x, q_2^y, \\dots\\}$, we construct a directed, weighted graph $G(V,E)$ where vertices represent intermediate steps in schedule generation, i.e., partial schedules and a set of remaining queries to be scheduled. Edges represent actions, such as renting a new VM or assigning a query to a VM. The cost (weight) of each edge will be the cost of performing a particular action (e.g., the cost of starting a new VM). We refer to this as a \\emph{scheduling graph}. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/graph.pdf}\n\\vspace{-1.0em}\n\\caption{ A subgraph of a scheduling graph for two query classes and $Q=\\{q^1_1, q^2_2, q^2_3\\}$. $G$ is one goal vertex.}\n\\vspace{-5mm}\n\\label{fig:graph}\n\\end{figure}\n\n\n\n\nSpecifically, each vertex $v\\in V$  has a schedule for some queries of the workload $Q$, $v_s=[vm^i_1, vm^k_2,\\dots]$, which represents the set of VMs to be rented for these queries. Each VM $j$ of type $i$, $vm^i_j$, is a queue of queries of some class that will be processed on that VM, $vm^i_j=[q^x_k, q^y_m,...]$. Hence, $v_s$ represents possible (potentially partial) schedules for the given workload $Q$. Each vertex $v \\in V$ also has a set of unassigned queries from $Q$, $v_u$, that still need to be placed onto some VMs. \n\nThe \\emph{start vertex}, $A \\in V$, represents the initial state where all queries are unassigned. Therefore, $A_u$ includes all the queries in the given workload and $A_s$ is empty. If a vertex $g \\in V$ has no unassigned queries, we say that vertex $g$ is a \\emph{goal vertex} and its schedule $g_s$ is a \\emph{complete} schedule. \n\nAn edge in $E$ represents one of two possible actions:\n\n\\begin{CompactEnumerate}\n\\item{A \\emph{start-up edge} $(u, v, i)$ represents renting a new VM of type $i$, $vm^i_j$. It connects a vertex $u$ to $v$ where $v$ has an additional empty VM, i.e., $v_s = u_s \\cup vm^i_j$. It does not assign any queries, so $u_u = v_u$. The weight of a start-up edge is the cost to provision a new VM of type $i$: $w(u, v) = f^i_s$.}\n\n\\item{A \\emph{placement edge} $(u, v, q^x_y)$ represents placing an unassigned query $q^x_y \\in u_u$ into the queue of a rented VM in $v_s$. It follows that $v_u = u_u - q^x_y$. {Because {WiSeDB\\xspace} is agnostic to the specifics of any particular query, the placement of an instance of query class $C_x$ is equivalent to placing any other instance of $C_x$.} Therefore, we include only a single placement edge per query class even if the class appears more than once in $u_u$. The cost of an edge that places query $q^x_y$ into a VM of type $i$ is the latency of query multiplied by the cost per unit time of the VM, plus any additionally incurred penalties: $w(u, v) = l(q^x_y, i) \\times f^i_r + {p({R}, {v_s})} - {p({R}, {u_s})}.$}\n\n\n\\end{CompactEnumerate}\n\nFigure~\\ref{fig:graph} shows part of a scheduling graph for a workload $Q=\\{q_1^1,q^2_2, q^2_3\\}$. $A$ represents the start vertex and $G$ represents a goal vertex. At each edge, either a query is assigned to an existing VM ($\\overline{AB}$, $\\overline{AC}$ ,$\\overline{BE}$, $\\overline{EG}$) or a new VM is created ($\\overline{BD}$, $\\overline{EF}$). The path $\\overline{ABEG}$ represents assigning the three queries, $q_1^1,q^2_2, q^2_3$, to execute in that order on $vm_1$.\n\nBased on the above definitions, the weight of a path from the start vertex to a goal vertex $g$ will be equal to the cost of the complete schedule of the goal vertex, ${cost({R}, {g_s})}$. Since all possible complete schedules are represented by some goal state, searching for the minimum cost path from a start vertex to any of the goal vertices will provide an optimal solution. We then use the A* search algorithm~\\cite{astar} to find the optimal path through the graph, noting the scheduling decision made at every step, i.e., which edge was selected at each vertex.  \n\n\n\n\n{\\bf Graph Reduction} To improve the runtime of the search algorithm, we reduce the graph in a number of ways. {First, we include a start-up edge only if the last VM provisioned has some queries assigned to it, i.e.,  we rent a new VM \\emph{only if the most recently provisioned VM is used by some queries}. {This eliminates paths that provision VMs that are never used.} Second, queries are assigned only to the most recently provisioned VM, i.e., each vertex has outgoing placement edges that assign a query \\emph{only to the most recently added VM}.} {This reduces the number of redundant paths in the graph, since each {combination of VM types and query orderings} is accessible by only a single path instead of by many paths.} \n\nWe now prove that this reduction can be applied without loss of optimality. Specifically, given a graph $G$ and a reduced graph $G_r$, all goal vertices with no empty VMs in $G$ are reachable in $G_r$.\n\n\\begin{proof}\nConsider an arbitrary goal vertex with no empty VMs $g \\in G$. For any vertex $v$, let $head(v_s)$ be the most recently created VM in $v_s$ (the head of the queue).\n\nLet us assume $head(g_s) = vm^i_j$ and $q^x_y$ is the last query scheduled in $vm^i_j$. Then there is a placement edge $e_p = (v, g, q^x_y)$ connecting some vertex $v$ with query $q^x_y$ in its set of unassigned queries, i.e., $q^x_y \\in v_u$, to $g$. Further, we know that $e_p$ is an edge of $G_r$ because $e_p$ is an assignment of a query to the most recently created VM. If $head(v_s)$ is non-empty, and $q^m_k$ is the last query on $head(v_s$), then there is another vertex $u$ connected to $v$ via a placement edge $e_p = (u, v, q^m_k)$ in $G_r$ by the same argument. If $head(v_s) = vm^i_j$ is empty, then there is a start-up edge $e_s = (y, v, i)$ connecting some vertex $y$, with the same set of unassigned queries as $v$, i.e., $v_u = y_u$, to $v$. We know that $e_s$ must be an edge of $G_r$ because $v$ can have at most one empty VM. This process can be repeated until the start vertex is reached. Therefore, there is a path from the start vertex in $G_r$ to any goal vertex with no empty VMs $g \\in G$.\n\\end{proof}\n\n\n{\\bf A* Search Acceleration}  We run the A* search algorithm~\\cite{astar} on this graph in order to find the optimal path from the start vertex to a goal vertex. {If $g$ is the goal vertex found on the optimal path, then $g_s$ is the optimal schedule for the workload.} \n\n {The A* algorithm can optionally take advantage of a heuristic function $h(v)$ in order to find the optimal path faster. The function $h(v)$ must approximate the cost from a vertex $v$ to the optimal goal vertex. In order for A* to guarantee correctness, the heuristic must be \\emph{admissible}, meaning that $h(v)$ never overestimates the cost.}  The heuristic function is problem specific. In our system, the cost of a path to $v$ is the cost of the schedule in $v_s$, $cost(R,v_s)$, and hence it  is calculated differently for different performance goals $R$. Therefore satisfying this requirement depends on the semantics of the performance metric. \n\n\n Here, we distinguish performance metrics that are \\emph{monotonically increasing} from those that are not. A performance goal is monotonically increasing if and only if the penalty incurred by a schedule $u_s$ never decreases when adding queries. Formally, at any assignment edge connecting $u$ to $v$, ${p({R}, {v_s})} \\geq {p({R}, {u_s})}$. Maximum query latency is an example of a monotonically increasing performance metric, since adding an additional query on the queue of the last provisioned VM will never decrease the penalty. Average latency is not monotonically increasing, since adding a short  query may decrease the average latency and thus the penalty.\n\nFor monotonically increasing performance goals, we use the following heuristic function\\footnote{\\small For performance goals that are not monotonically increasing, we do not use a heuristic, which is equivalent to using the null heuristic, $h(v) = 0$.}:\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:h}\nh(v) = \\sum_{q^x_y \\in v_u} \\min_i \\left[ f^i_r \\times l(q^x_y, i) \\right]\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"h(v)=\\sum_{q^{x}_{y}\\in v_{u}}\\min_{i}\\left[f^{i}_{r}\\times l(q^{x}_{y},i)\\right]\" display=\"block\"><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msubsup><mi>q</mi><mi>y</mi><mi>x</mi></msubsup><mo>\u2208</mo><msub><mi>v</mi><mi>u</mi></msub></mrow></munder><mrow><munder><mi>min</mi><mi>i</mi></munder><mo>\u2061</mo><mrow><mo>[</mo><mrow><mrow><msubsup><mi>f</mi><mi>r</mi><mi>i</mi></msubsup><mo>\u00d7</mo><mi>l</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>q</mi><mi>y</mi><mi>x</mi></msubsup><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08221.tex", "nexttext": "\nTo map this into our scheduling graph, let\t us consider the set of vertices along an optimal path to a goal vertex $g$ in one training workload discovered during the training phase for $\\alpha$. For any particular vertex $v$, we know from Equations ~\\ref{eq:cost} and~\\ref{eq:stricter} that ${cost({R^\\prime}, {v})} \\geq {cost({R}, {v})}$, in other words, the cost of that vertex's partial schedule will cost more if we have a stricter performance goal.  Since this holds for all vertices on the optimal path, it also holds for the optimal goal vertex $g$ under $\\alpha$. Hence, if the performance goal becomes stricter, the cost of the final schedule can only increase (Equation~\\ref{eq:stricter}).\n\n\n\nFurthermore, for any vertex $v$ along the optimal path to $g$, we know that the minimum possible cost to go from $v$ to $g$ is exactly ${cost({R}, {g})} - {cost({R}, {v})}$. If the edge weight can only increase due to a stricter performance goal, then the cost to go from $v$ to $g$ under $R$ must be less than or equal to the cost to go form $v$ to $g$ under $R^\\prime$. Formally, since ${cost({R^\\prime}, {v})} \\geq {cost({R}, {v})}$, it follows that ${cost({R^\\prime}, {g})} - {cost({R^\\prime}, {v})} \\geq {cost({R}, {g})} - {cost({R}, {v})}$. \n\n{For example, in the graph shown in Figure~\\ref{fig:graph}, ${cost({R}, {B})} = 6$ and ${cost({R}, {G})} = 26$. The cost to get from $B$ to $G$ is thus $20$. If $R^\\prime$ resulted in the assignment $\\overline{EG}$ having a higher cost of $100$ (because $q^2_3$ misses its deadline), then it holds that the cost of going from $B$ to $G$ under $R^\\prime$ (110) is greater than the cost of going from $B$ to $G$ under $R$: ${cost({R^\\prime}, {G})} - {cost({R^\\prime}, {G})} \\geq {cost({R}, {G})} - {cost({R}, {B})}$. }\n\nWhile the optimal goal vertex for a set of queries may be different under $R$ than under $R^\\prime$, we know that \\emph{the cost of the optimal vertex under $R^\\prime$ is no less than the cost of the optimal vertex under $R$}, because $g_s$ was optimal under $R$. {Intuitively, if there was some complete schedule $\\gamma_s$ of a workload under $R^\\prime$ with a lower cost than $g_s$, then $\\gamma_s$ would also have a lower cost than $g_s$ under $R$, which contradicts that $g_s$ is optimal.}\n\nTherefore, we know that the cost from an arbitrary vertex $v$ under $R^\\prime$ to the unknown optimal goal vertex $g^\\prime$ is no less than the cost to get from $v$ to $g$ under $R$:\n\n", "itemtype": "equation", "pos": 47956, "prevtext": "\nwhich calculates the cheapest possible runtime for the queries that are still unassigned at $v$. In other words, $h(v)$ sums up the cost of the cheapest way to process each remaining query, assuming VMs could be created for free.\n\nEquation~\\ref{eq:h} will never overestimate the optimal cost to a goal vertex. Regardless of performance goals, query classes, or VM performance, one always has to pay the cost of renting a VM for the duration of the queries. More formally, every path from an arbitrary vertex $v$ to a goal vertex must include an assignment edge, with cost $l(q^x_y, i) \\times f^i_r + {p({R}, {v_s})} - {p({R}, {u_s})}$, for each query in $v_u$ and for some VM type $i$. When the performance goal is monotonically increasing, the term ${p({R}, {v_s})} - {p({R}, {u_s})}$ is never negative, so $h(v)$ is never larger than the actual cost to the goal vertex.\n\n\n\\subsection{Feature Extraction}\nAfter running the A* algorithm on each of the sampled query workloads, we will have the optimal schedules (i.e., optimal graph paths) for $N$ different workloads. In order to build a training set, we extract features from each of the vertices in the optimal paths we discovered. These features are extracted from \\emph{all} the paths we collected for all of the random workloads we used. \n\nThe training set consists of $(decision, features)$ pairs indicating the decisions that was made by A* while calculating optimal schedules and performance/workload related features at the time of the decision. Each decision represents an edge in the search graph, and is therefore a decision to either (a) create a new VM of type $i$, or (b) assign a query of class $C_j$ to the most recently created VM. Thus, the domain of possible decisions is equal to the sum of the number of query classes and the number of VM types. We map each decision (edge) $(u,v)$ in the optimal path to a set of features of its origin vertex $u$. \n\nWe collect features from each vertex because there is a correspondence between a vertex and the optimal decision made at that vertex. Specifically, for a given vertex $v$, the edge selected in the optimal path is independent of $v$'s parents or children but depends on the unassigned queries $v_u$ and a schedule so far $v_s$. However, the domains of $v_u$ and $v_s$ are far too large to enumerate and do not lend themselves to machine learning algorithms ($v_u$ and $v_s$ are neither numeric or categorical). Therefore, we extract features from $v$ that provide information about $v_s$ and $v_u$, but have much smaller domains and are either numeric or categorical.\n\nFor each vertex $v$ along the optimal path, we extract the features:\n\\begin{CompactEnumerate}\n\\item{\\texttt{wait-time}: the amount of time that a query would have to wait before being processed if it were placed on the most recently created VM. {Formally, \\texttt{wait-time} is equal to execution time of all the queries already assigned to the last VM. \\cut{ $\\sum_{q^x_y \\in head(v_s)} l(q^x_y, i)$ where $i$ is the type of the most recently provisioned VM, $head(v_s)$.} This feature can help our model decide which queries should be placed on the last added VM based on their deadline. For example, if a machine's wait time has exceeded the deadline of a query class, it is likely that no more queries from this class should be assigned to that VM. Alternatively, if a machine's wait time is very high, it is likely that only short queries should be assigned.}} \n\n\\item{\\texttt{proportion-of-X}: the proportion of queries on the most recently created VM that belong to query class \\texttt{X}. {In other words, \\texttt{proportion-of-X} is the ratio between the number queries of class \\texttt{X} assigned to the VM and the total number of queries assigned to the VM. For example, if the VM currently has four queries assigned to it, with one of those queries being of class $C_1$ and three being of class $C_2$ , then we extract the features \\texttt{proportion-of-C1=0.25} and \\texttt{proportion-of-C2=0.75}. We only need to consider to most recently created VM because the assignment edges in the reduced graph only assign queries to the most recent VM. Since each sample workload contains only a limited number of queries, keeping track of the exact number of instances of each query class would not scale to large workloads. Therefore, we track the proportion instead.}}\n\n\n\\item{\\texttt{cost-of-X}: the cost incurred (including any penalty costs) by placing a query of class \\texttt{X} on the most recently created VM. \n\n{\\texttt{cost-of-X} is equal to the weight of the outgoing assignment edge for class \\texttt{X}. This allows our model to check the cost of placing an instance of a certain query class and based on its value decide on whether to assign another query class to the last rented VM or create a new VM.}}\n\\item{\\texttt{have-X}: whether or not a query of class \\texttt{X} is still unassigned.  This feature helps our model understand how the class of the unassigned queries affects the decisions on the optimal path. If there is no instance of some query class $C_j$ available to place, then the model places one of the remaining classes, while if an instance of $C_j$ exists, the model might prefer to schedule that as opposed to any other class.}\n\\end{CompactEnumerate}\n\nWhile these features are not enough to \\emph{uniquely} identify a vertex and hence learn the exact conditions that lead to the optimal schedule, they can shed light on the workload/performance conditions related to the optimal decision. Learning based on these features helps {WiSeDB\\xspace} predict when to provision a new VM and how to assign queries and eventually offer close-to-optimal schedules.\n\n\n\n\n\n\n\n\n\\subsection{Workload Management Model}\n\\label{sec:model}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{figs/dt.pdf}\n\t\\vspace{-1mm}\n\t\\caption{An example decision model.}\n\t\\vspace{-4mm}\n\t\\label{fig:dt}\n\\end{figure}\n\nAfter building up a training feature set, we create a decision tree model using the J48 algorithm~\\cite{weka}. Figure~\\ref{fig:dt} shows a  decision model.  Each feature node (orange nodes) of the decision tree represents either a binary split on a numeric feature, or an $n$-way split on a categorical feature, where $n$ is the number of categories. The decision nodes (white nodes) represent the suggested actions. \\cut{The decision tree maps a vertex $v$ to an action $a$ by extracting features from $v$ and descending through the tree. Since each action $a$ is represented by a single out-edge of $v$, one can navigate the decision tree by traversing the edge corresponding to the action selected.}\n\n\n\n\nWe demonstrate how to traverse the decision tree with an example. The right side of Figure~\\ref{fig:dt} shows how we use the decision tree comes up with the schedule in Scenario 1 of Figure~\\ref{fig:example_sla} (minus one query). Here we have two query classes, $C_1$ and $C_2$. Each query in $C_1$ is expected to have a latency of two minutes and the performance goal is to offer a latency of no more than three minutes per query. Instances of $C_2$ will have a latency of one minute and the performance goal is for each query in this class to be completed within one minute.  {WiSeDB\\xspace} (assuming VMs of a single type and that queries are executed in isolation)  generates the decision tree in  Figure~\\ref{fig:dt}.\n\nGiven a workload $Q=\\{q^1_1,q^2_2,q^2_3\\}$,  the tree is parsed as follows. In the first node (1), we check the wait time, which is zero since all queries are unassigned, hence we proceed to node (3). The workload has queries of class $C_2$ and therefore we proceed to node (4). Here we calculate the  cost of placing an instance of $C_2$. Let us assume the cost is is less than 100 (no penalty is incurred), which leads to node (6) which assigns an instance of $C_2$ to a new VM. Since we have more queries in the workload we next re-parse the decision tree. In (1) the wait time on the most recent VM is now 1 minute (the runtime of queries in $C_2$) so we move to node (3). Since we have one more query of $C_2$ unassigned, we move to (4). Now the cost of assigning a query of $C_2$ is more than 100 since the new query would need to wait for $q^2_1$ to complete (and thus incur a penalty). Hence, we move to node (7) and we check if there are any  unassigned instances of $C_1$ in our workload. Since there is ($q^1_1$), we assign $q^1_1$ to the last VM. We re-parse the tree in the same way and {by following nodes (1)$\\rightarrow$(2)}, then again as (1)$\\rightarrow$(3)$\\rightarrow$(4)$\\rightarrow$(7)$\\rightarrow$(9), so we assign the remaining query $q^2_3$ onto a new VM. \n\nIn general, the decision model will place an instance of $C_2$, then an instance of $C_1$, and then create a new VM. This process will repeat itself until queries of $C_1$ or $C_2$ are depleted. When one query class is depleted, single instances of the remaining class will be placed on new VMs until none remain. For the case of these two query classes, the heuristic learned by {WiSeDB\\xspace} is equivalent to first-fit increasing, which sorts queries in decreasing order of latency and places each query on the first VM where the query ``fits'' (i.e., incurs no penalty).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Adaptive Modeling}\\label{sec:shift}\n\n\n\n\n\n\n\n\nIt is often desirable to allow the user to explore potential performance and cost trade-offs within the space of possible performance goals for their specific workload~\\cite{pslas}. This can be achieved by generating different models for the same workload with stricter/looser performance constraints and thus higher/lower costs. However, {WiSeDB\\xspace} tunes its decision model for a specific performance goal. Changes in this goal will trigger {WiSeDB\\xspace} to re-train the decision model, since changes in performance constraints lead to different optimal schedules and hence to different training sets. Therefore, generating a set of alternative decision models for the same workload and different performance constraints could impose significant training overhead.  \n\n\nTo address this challenge, {WiSeDB\\xspace} employs a technique that adapts an existing model trained for a given workload and performance goal to a new model for the same workload and stricter performance goals with minimal re-training. The main idea is that if two decision models are trained for the same query classes $C$ and similar performance goals, their training sets will share significant information. Our approach relies on the adaptive A* algorithm~\\cite{adaptiveastar} which reuses information from one graph to create a better search heuristic, $h(v)$, to parse  another graph with identical structure but increased edge weights. We refer to this as \\emph{adaptive modeling}.\n\n\n\n\n \n\n\n\n\n\n Let us assume a model $\\alpha(C,R)$ and a desired new model $\\alpha^\\prime (C, R^\\prime)$  with the same query classes $C$ but a slightly different {performance goal} $R^\\prime$. \nWithout loss of generality, we only consider cases where the performance goal $R^\\prime$ is more strict that $R$. Of course, one can always start with a substantially loose performance goal than the one requested and restrict it incrementally. \n\nFor any given schedule $S$ we generate with model $\\alpha$, Equation~\\ref{eq:cost} indicates that the new performance goal will only affect the penalty. Specifically, since stricter performance constraints on the same schedule can lead only to more violations we have that:\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:stricter}\n\\forall s ({p({R^\\prime}, {s})} \\geq {p({R}, {s})})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\forall s({p({R^{\\prime}},{s})}\\geq{p({R},{s})})\" display=\"block\"><mrow><mo>\u2200</mo><mi>s</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msup><mi>R</mi><mo>\u2032</mo></msup><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2265</mo><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08221.tex", "nexttext": "\nWe can thus use ${cost({R}, {g})} - {cost({R}, {v})}$ as a search heuristic for identifying the optimal goal vertex for the model $\\alpha^\\prime$. \n\nFor metrics that are monotonically increasing, we write the new search heuristic $h^\\prime$ in terms of the original heuristic $h$:\n\n", "itemtype": "equation", "pos": 50510, "prevtext": "\nTo map this into our scheduling graph, let\t us consider the set of vertices along an optimal path to a goal vertex $g$ in one training workload discovered during the training phase for $\\alpha$. For any particular vertex $v$, we know from Equations ~\\ref{eq:cost} and~\\ref{eq:stricter} that ${cost({R^\\prime}, {v})} \\geq {cost({R}, {v})}$, in other words, the cost of that vertex's partial schedule will cost more if we have a stricter performance goal.  Since this holds for all vertices on the optimal path, it also holds for the optimal goal vertex $g$ under $\\alpha$. Hence, if the performance goal becomes stricter, the cost of the final schedule can only increase (Equation~\\ref{eq:stricter}).\n\n\n\nFurthermore, for any vertex $v$ along the optimal path to $g$, we know that the minimum possible cost to go from $v$ to $g$ is exactly ${cost({R}, {g})} - {cost({R}, {v})}$. If the edge weight can only increase due to a stricter performance goal, then the cost to go from $v$ to $g$ under $R$ must be less than or equal to the cost to go form $v$ to $g$ under $R^\\prime$. Formally, since ${cost({R^\\prime}, {v})} \\geq {cost({R}, {v})}$, it follows that ${cost({R^\\prime}, {g})} - {cost({R^\\prime}, {v})} \\geq {cost({R}, {g})} - {cost({R}, {v})}$. \n\n{For example, in the graph shown in Figure~\\ref{fig:graph}, ${cost({R}, {B})} = 6$ and ${cost({R}, {G})} = 26$. The cost to get from $B$ to $G$ is thus $20$. If $R^\\prime$ resulted in the assignment $\\overline{EG}$ having a higher cost of $100$ (because $q^2_3$ misses its deadline), then it holds that the cost of going from $B$ to $G$ under $R^\\prime$ (110) is greater than the cost of going from $B$ to $G$ under $R$: ${cost({R^\\prime}, {G})} - {cost({R^\\prime}, {G})} \\geq {cost({R}, {G})} - {cost({R}, {B})}$. }\n\nWhile the optimal goal vertex for a set of queries may be different under $R$ than under $R^\\prime$, we know that \\emph{the cost of the optimal vertex under $R^\\prime$ is no less than the cost of the optimal vertex under $R$}, because $g_s$ was optimal under $R$. {Intuitively, if there was some complete schedule $\\gamma_s$ of a workload under $R^\\prime$ with a lower cost than $g_s$, then $\\gamma_s$ would also have a lower cost than $g_s$ under $R$, which contradicts that $g_s$ is optimal.}\n\nTherefore, we know that the cost from an arbitrary vertex $v$ under $R^\\prime$ to the unknown optimal goal vertex $g^\\prime$ is no less than the cost to get from $v$ to $g$ under $R$:\n\n", "index": 7, "text": "\\begin{equation*}\n{cost({R^\\prime}, {g^\\prime})} - {cost({R^\\prime}, {v})} \\geq {cost({R}, {g})} - {cost({R}, {v})}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{cost({R^{\\prime}},{g^{\\prime}})}-{cost({R^{\\prime}},{v})}\\geq{cost({R},{g})}-%&#10;{cost({R},{v})}\" display=\"block\"><mrow><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>R</mi><mo>\u2032</mo></msup><mo>,</mo><msup><mi>g</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>R</mi><mo>\u2032</mo></msup><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08221.tex", "nexttext": "\nFor {performance goals} that are not monotonically increasing, {like average latency,} we simply drop the $h(v)$ term, giving $h^\\prime(v) = {cost({R}, {g})} - {cost({R}, {v})}$. This yields a substantially improved heuristic that is able to find the optimal solution much faster. \n\n{\\bf Graph Reuse} In order to generate a new model $\\alpha^\\prime$ with a stricter performance goal from the training data of an existing model $\\alpha$, we use the new search heuristic $h^\\prime$ to search the new optimal path using the scheduling  graph we generated for each of $\\alpha$'s training workloads.  The new, recalculated paths serve as a training sample for $\\alpha^\\prime$, and (since $h^\\prime(v)$ is admissible) are guaranteed by the A* algorithm to be correct~\\cite{astar}. {This new training set is then used to learn a decision tree model for the stricter performance goal $R^\\prime$.}  This approach saves {WiSeDB\\xspace} from constructing the scheduling graph for each new model which is the step with the dominating overhead. \n\n\n\n\n\n\n\n\n\n\n\\section{Run Time Functionality}\\label{s:runtime}\n\n{A user interacts with {WiSeDB\\xspace}  in two phases. First, in the \\emph{model generation phase}, the user submits their query classes definition, associated latencies and desired performance goal. {WiSeDB\\xspace} will then produce a decision model for scheduling queries under this performance goal as well as several models with stricter/looser goals for the user to examine. After selecting a model with a performance vs. cost trade-off that the user finds desirable, the \\emph{processing phase} beings. In this phase, the user may submit batch workloads, which {WiSeDB\\xspace} will schedule using the user's picked decision model. The same model can also be used for online scheduling where queries arrive one at a time. The system remains in this mode until the user either wishes to switch models or has no more queries to process.}\n\n\n\n\n\n\\subsection{Model Recommendations}\n\nWhile generating alternative decision models to recommend to the user, our goal is to identify a small set of models that represent significantly different performance vs. cost trade-offs. To achieve this, we use a similar approach as in~\\cite{pslas}. We first create a sequence of performance goals in increasing order of strictness\\footnote{\\small This is easily done for most SLA types: for a total latency SLA, push back the final deadline; for an average latency SLA, increase the average latency before a penalty is incurred.}, $\\mathscr{R} = R_1, R_2, \\dots, R_n$, {in which the application-defined goal $R$ is the median}.  For each goal $R_i \\in \\mathscr{R}$, we train a decision model (by shifting the original model as described in Section~\\ref{sec:shift}) and calculate the average cost of each query class over a large random sample workload $Q$. Then, we compute the pairwise differences between the average cost of queries per class of each performance goal $R_i \\in \\mathscr{R}$ using Earth Mover's Distance~\\cite{emd}. We find the smallest such distance between any pairs of performance goals, $EMD(R_i, R_{i+1})$, and we remove $R_{i+1}$ from the sequence. We repeat this process until only $k$ performance goals remain. It has been shown in~\\cite{pslas} that this technique produces a desirable distribution of performance goals (or ``service tiers'') that represent the performance/cost trade-offs of the cloud infrastructure. \n\nHowever, unlike in~\\cite{pslas}, we do not need to \\emph{explicitly} execute any workloads in order to compute the cost of a particular model. Instead, for each decision model {WiSeDB\\xspace} provides a cost estimation function that takes as parameters the size of each query class. Users can easily experiment with different query classes sizes for the incoming workloads and they can estimate the expected cost of executing these workloads using our proposed models. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Batch query scheduling}\nAfter a decision model has been selected by the user, the user can opt to send a batch of queries to be scheduled based on this model. {WiSeDB\\xspace} navigates the decision tree and produces a strategy for executing this work. A  detailed example was given in Section~\\ref{sec:model}.  The strategy indicates the number and types of VMs needed, the assignment of queries to VM and the query execution  order in each VM. The user is then responsible for executing these scheduling decisions on the IaaS cloud infrastructure. \n\n\n\n\n\\subsection{Online Query Scheduling}\n\\label{sec:online}\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/example_sla_online.pdf}\n\\vspace{-1mm}\n\\caption{Online scheduling example}\n\\vspace{-5mm}\n\\label{fig:example_sla_online}\n\\end{figure}\n\n{WiSeDB\\xspace} can also handle non-preemptive online query scheduling. We will describe how {WiSeDB\\xspace} handles the general case of online scheduling and then we will give two optimizations.\n\n\n\n\n{\\bf General Online Scheduling} Let us  assume an available decision model for a performance goal $R$ and query classes $C$. We also assume that a user submits queries, $\\{q^x_1, q^y_2, \\dots,\\}$ at  times $\\{t_1, t_2, \\dots\\}$ respectively  and each query $q^x_i$ belongs to one class $C_i \\in C$. Online scheduling can be viewed as a series of successive batch scheduling tasks where each batch includes a single additional query. The first query is scheduled at time $t_1$ as if it were a batch. In general, when query  $q^x_i$ arrives at $t_i$, we create a new batch $B_i$ containing the queries \\cut{from $\\{q_j,\\dots,q_{i}\\}$} that have not started executing by $t_i$ and use the model to re-schedule them along with query $q^x_i$.\n\nHowever, scheduling $B_i$ as a batch using the same decision model is not guaranteed to offer low-cost solutions. This is because all the queries from $B_{i-1}$ represent queries that have been sitting in the queue since at least the previous timestep, and hence their expected latency is now higher than what was initially assumed. Ideally, a new query class should be created to capture the new expected latency (if this latency is not captured by existing classes), and, in order to discover low-cost solutions that include this new class, a new decision model must be trained. \n\nFormally, any $q^x_y \\in B_{i-1}$ that has yet to run will have a final latency of, at minimum, $l(q^x_y, k) + (t_i - t_y)$, where $k$ is the type of the VM on which $q^x_y$ was assigned and $t_y$ is the arrival time of $q^x_y$. Of course, the latency could be even greater if it is not scheduled to execute immediately at time $t_i$. In order to handle these new query classes, {WiSeDB\\xspace} trains a new decision model using the same performance goal and the new set of query classes. This new model can then be used to schedule $B_i$ as a batch. \n\n\n\n\n\n\n\nFigure~\\ref{fig:example_sla_online} illustrates the results of online scheduling in {WiSeDB\\xspace} with two query classes. Queries with latency of $1$ minute are in class $C_1$ and those with latency of $0.5$ minutes are assigned to class $C_2$. Each query has a deadline of  twice its latency. Figure~\\ref{fig:example_sla_online} shows how different scenarios for the  queries $\\{q^1_1, q^2_2, q^1_3, q^2_4, q^1_5, q^1_6\\}$. {WiSeDB\\xspace} first scheduled query $q^1_1$, which arrived at $t_0$, as a batch. After half a minute ($t_3$), $q^2_2$ and $q^1_3$ arrive and {WiSeDB\\xspace} creates a batch $B_{2-3}$ for them and schedule using the existing model. The model assigns them to the first VM, right after the first query. At time $t_5$, when two more queries, $q^2_4, q^1_5$ arrive, the first query has not yet completed so  we need to create a new batch that includes $q^2_2,q^1_3,q^2_4,q^1_5$. However even though $q_5^1$ and $q^1_3$ are of the same class, they have different performance goals: $q_5^1$ is due within two minutes, and $q^1_3$ is due to complete within $0.5$ minute (since it has been waiting for $0.5$ minutes already). Hence, the existing model cannot be used and a new model needs to be train. This model will include an extra class for queries  with latency of $1$ minute and deadline of $0.5$ minutes.\n\n\n\n\n\n\n\n\n\n\nA substantially fast query arrival rate could create a very large number of query classes, causing an increase in training time and a high frequency of retraining. Since training can be expensive, we offer two optimizations to prevent or accelerate retraining.\n\n\n\n\n\n\n\n\n\n\n{\\bf Model Reuse} As explained above, in Figure~\\ref{fig:example_sla_online}, at $t_5$ we had to train a new model with a new query class of with execution time of $0.5$ minutes and deadline of $0.5$ minutes. When query $q^1_6$ arrives at $t_6$, $q^1_5$'s deadline is also $0.5$ minutes. However, the previous model is already trained for a class of queries with runtime of $0.5$ minutes and deadline of $0.5$ minutes.  We can thus use the same decision model we used at $t_5$ at $t_6$, since the query classes at these two time steps are equivalent. More obviously, we can use the same decision model at $t_7$ as we did at $t_1$, since both $t_1$ and $t_7$ contain only a single query in their respective batches. At times $(t_5, t_6)$ and at times $(t_2, t_7)$, {the difference in time between the newest and the oldest query is the same.} \n\n\n\n\n\n\n\n\nNext, consider the general case. We define $\\omega(i)$ to be the difference in time between the oldest query that has not started processing at time $t_i$ and the newest query at time $t_i$. Formally,\n\n", "itemtype": "equation", "pos": 50923, "prevtext": "\nWe can thus use ${cost({R}, {g})} - {cost({R}, {v})}$ as a search heuristic for identifying the optimal goal vertex for the model $\\alpha^\\prime$. \n\nFor metrics that are monotonically increasing, we write the new search heuristic $h^\\prime$ in terms of the original heuristic $h$:\n\n", "index": 9, "text": "\\begin{equation*}\nh^\\prime(v) = \\max \\left[h(v), {cost({R}, {g})} - {cost({R}, {v})}\\right]\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"h^{\\prime}(v)=\\max\\left[h(v),{cost({R},{g})}-{cost({R},{v})}\\right]\" display=\"block\"><mrow><mrow><msup><mi>h</mi><mo>\u2032</mo></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo>[</mo><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>g</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mi>c</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.08221.tex", "nexttext": "\n\nClearly, the absolute time of a workload's submission is irrelevant to the scheduling model: only the difference between the current time and the time a workload was submitted matters. Formally, if $\\omega(i) = \\omega(j)$, then $B_i$ and $B_j$ can be scheduled using the same model. For example, for $t_5$ and $t_6$ in Figure~\\ref{fig:example_sla_online}, $\\omega(5) = 0.5$ and $\\omega(6) = 0.5$. Therefore, $B_5$ and $B_6$ can be scheduled using the same model.  {In practice, $B_i$ and $B_j$ can be scheduled using the same model if the difference between $\\omega(B_i)$ and $\\omega(B_j)$ is less than the error in the query latency prediction model.} By keeping a mapping of $\\omega(x)$ to decision models, {WiSeDB\\xspace} can significantly reduce the amount of training that occurs during online scheduling. \n\n\n\n\n{\\bf Linear Shifting} For some performance metrics, scheduling queries that have been waiting in the queue for a certain amount of time is the same as scheduling with a stricter deadline (Section~\\ref{sec:shift}). This will significantly reduce the training overhead as the new models could reuse the scheduling graph of the previous ones.  Consider a performance goal that puts a deadline on each query latency of three minutes. If a query is scheduled one minute after submission, that query can be scheduled as if it were scheduled immediately with a deadline of two minutes. \n\n\n\nThis optimization can be applied only to \\emph{linearly shiftable} performance metric. \nIn general, we say that a metric $R$ is {linearly shiftable} if the penalty  incurred under $R$ for a schedule which starts queries after a delay of $n$ seconds is the same as the penalty incurred under $R^\\prime$ by a schedule that starts queries immediately, and where $R^\\prime$ is a tightening of $R$ by some known function of $n$. For the metric of maximum query latency in a workload, this function of $n$ is the identity: the penalty incurred under some goal $R$ after a delay of $n$ seconds is equal to the penalty incurred under $R^\\prime$, where $R^\\prime$ is $R$ tightened by $n$ seconds. For linear shiftable performance metrics, training a new model when new queries arrive can be reduced to shifting a decision model, as described in Section~\\ref{sec:shift}.\n\n\n  \n\n\n\n\n\n\\section{Experimental Results}\\label{sec:expr}\nNext we present our experimental results which focus on evaluating {WiSeDB\\xspace}'s \\emph{effectiveness} to learn low-cost workload schedules for a variety of performance goals, as well as its runtime \\emph{efficiency}.\n\n\n\\subsection{Experimental Setup}\\label{s_setup}\n\nWe implemented a  prototype of {WiSeDB\\xspace} using Java 8 and installed it on a server equipped with a Intel Xeon E5-2430 and 32GB of RAM. The service is used to support a data management application deployed on a private cloud of 3 Intel Xeon E5-2430 servers with the ability of support up to 12 VMs (4 VMs per server). This private cloud emulates Amazon AWS~\\cite{url-amazonAWS} by using query latencies and VM start up times (measured on {t2.medium} EC2 instances). By default our experiments assume a single type of VM unless otherwise specified. \n\nOur database application and query workload is based on the {TPC-H} benchmark~\\cite{url_tpch}. We deployed a database of size 10GB and we created workloads by uniformly sampling query templates 1 through 10. These templates have a response time ranging from 2 to 6 minutes  with an average latency of 4 minutes on the \\texttt{t2.medium} EC2 instance.  We set the query processing cost to be the price of that instance ($f_r = \\$0.052$ per hour), and measured its start-up cost experimentally based on how long it took for the VM to become available via connections after it entered a ``running'' state ($f_s = \\$0.0008$). \n\nOur approach trains a decision tree classifier on sample workloads. Our training was performed with $N = 3000$ sample workloads with a size of $m = 18$ queries in each workload. We defined the query classes for these workload to represent query templates, therefore all queries in the same class have the same expected latency when processed on the same VM type. Our workload specifications include 10 query classes (TPC-H queries 1-10). We also assume queries are executed in isolation. Given a training model, we then use the model to generated schedules for incoming workload. Our experiments vary the size of these workloads as well as the distribution of queries across the different classes. Each experiment  averages costs over 5 workloads of a given size and query distributions. Finally, our violation penalty was set to be one cent per each second of violation period.  \n\n\n\n\n{\\bf Performance Metrics}. Our experiments are based on four  performance goals: (1) {\\bf Max} requires the maximum query latency in the workload to be less than $x$ minutes. By default we set $x=15$  which is 2.5 times the latency of the longest query in our workload. (2) {\\bf PerQuery} requires that each query in a given class cannot exceed its expected latency by x times. By default we set $x=3$ so that the average of these deadlines is approximately $15$ minutes, which is  2.5 times the latency of the longest query. (3) {\\bf Average} requires that the average latency of a workload is x minutes. We set $x=10$ so that this deadline is 2.5 times the average query class latency. (4) {\\bf Percent} requires that $y\\%$ of the queries in the workload finish within $x$ minutes. By default we set $y=90$ and $x=10$. \n\nViolation periods for each of these performance goals were calculated as we described in Section~\\ref{s:problem}. Specifically, for {\\tt Max}, schedules are charged an additional cent per second for any query whose latency exceeds x minutes and for {\\tt PerQuery} schedules are charged an additional cent per second in which any query exceeds $x$ times its predicted latency. For {\\tt Average},  schedules are charged an additional number of cents equal to the difference between the average latency of the scheduled queries and x. In other words, schedules are charged if their average latency exceeds x minutes. For {\\tt  Percent} if $90\\%$ of queries finish within ten minutes, there is no penalty. Otherwise, a penalty of one cent per second is charged for time exceeding ten minutes.   \n\n\n\n\n\\subsection{Effectiveness Results}\n\n\nIn order to demonstrate the effectiveness and versatility, we compare {WiSeDB\\xspace} against an optimal scheduler and against hand-written heuristics that work well for specific performance metrics. \n\n{\\bf Optimality} Since our scheduling problem is computationally expensive, calculating the optimal schedule for large workload sizes is not computationally feasible. However, we can compare {WiSeDB\\xspace} against the optimal for smaller workload sizes for which we can exhaustively search for the optimal solution. Figure~\\ref{fig:performance} shows the final cost for scheduling workloads of 30 queries uniformly distributed across 10 query classes. Here we compare schedules generated by {\\it {WiSeDB\\xspace}} with the optimal schedule  ({\\tt Optimal}). {{WiSeDB\\xspace}'s schedules are within $8\\%$ of the optimal for all performance metrics}. Figure~\\ref{fig:performance_scale} shows the percent-increase in cost over the optimal for three different workload sizes. {WiSeDB\\xspace} consistently performs within 8\\% from the optimal independently of the size of the workload, while for the {\\tt Percent} metric is learns scheduling heuristics that cost less than 2\\% from the best solution.\n\n{Figure~\\ref{fig:strict} shows the percent-increase in cost over the optimal when we tighten the constraints of the performance goals. To capture this parameter we introduce the \\emph{strictness factor}.  {A strictness factor of} $0$ means a performance goal equal to those described in Section~\\ref{s_setup}, whereas a strictness factor $x<0$ represents a performance goal that is only $x$ times as strict as those described above (so it has a more relaxed constraint). A strictness factor of $x>0$ corresponds to a performance goal that is $x$ times stricter than those above (so it has a tighter constraint). Figure~\\ref{fig:strict} shows that the strictness factor does not affect the effectiveness of {WiSeDB\\xspace} relative to the optimal. In the rest of the experiments we will use performance goals with a strictness factor of $0$.} \n\n\\emph{We conclude that {WiSeDB\\xspace} is able to learn effective heuristics for scheduling incoming workloads for various performance metrics. These heuristics offer low-cost scheduling solutions (within $8\\%$ of the optimal)  independently of the size of the runtime workloads and the strictness of the performance goal.}\n\n\n\n\\begin{figure*}[t]\n\t\\centering\n\t\\begin{subfigure}{0.3\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{experiments/performance.pdf}\n\t\t\\caption{\\small Optimality for various performance metrics}\n\t\t\\label{fig:performance}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.3\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{experiments/opt_scale.pdf}\n\t\t\\caption{\\small Optimality for varying workload sizes}\n\t\t\\label{fig:performance_scale}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.3\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{experiments/strictness.pdf}\n\t\t\\caption{\\small {Optimality for varying constraints}}\n\t\t\\label{fig:strict}\n\t\\end{subfigure}\n\t\\vspace{-1mm}\n\t\\caption{ Comparison with the optimal scheduling solution. \\cut{{WiSeDB\\xspace} learns a strategy that performs within 8\\% from the optimal solution.}}\n\t\\vspace{-1mm}\n\n\\end{figure*}\n\n{\\bf Multiple VM types} Since many IaaS providers (e.g.,~\\cite{url-amazonAWS,url-msazure}) offer different types of machines at different costs, it is critical that for cost-driven workload management service to take advantage of multiple VM types.  With the TPC-H benchmark, queries that require less RAM tend to have similar performance on \\texttt{t2.medium} and \\texttt{t2.small} EC2 instances. Since \\texttt{t2.small} instances are cheaper than \\texttt{t2.medium} instances, it makes sense to place low-RAM queries on \\texttt{t2.small} instances. {WiSeDB\\xspace} is able to learn this, and generate heuristics that take these affinities into account.\n\n\n\nFigure~\\ref{fig:multi_vm} shows the cost of {WiSeDB\\xspace} schedules against the optimal schedule with one {\\tt ({WiSeDB\\xspace} 1T)} and two VM types {\\tt ({WiSeDB\\xspace} 2T)}. The second VM type is a \\texttt{t2.small} EC2 instance, which has a better price/performance ratio for some of the lighter queries in the TPC-H benchmark. \nThe results reveal that even when the learning task is more complex (using more VM types adds more edges in our search graph and thus more complexity to the problem), {WiSeDB\\xspace} is able to learn good scheduling heuristics that perform within 6\\% of the optimal on average.\n\n Additionally, Figure~\\ref{fig:multi_vm} shows that {WiSeDB\\xspace} performs better with access to a larger number of VM types  and for every performance goal, {WiSeDB\\xspace} with two VM types performed better than the optimal with only a single VM type. \\emph{Hence we can conclude that {WiSeDB\\xspace} is able to leverage the availability of cheaper VMs, learn their impact on the specific performance goal and adapt its decision model to produce low-cost schedules}. \n\n\n\n\n{\\bf Metric-specific Heuristics} We now focus our evaluation on larger workloads of $5000$ queries. Here, exhaustively searching the optimal solution is infeasible, hence we compare {WiSeDB\\xspace}'s schedules with the ones produced by heuristics designed to offer good solutions for each of our performance goals. \\emph{First-Fit Decreasing} ({\\tt FFD}) sorts the queries in descending order and then places each query into the first VM with sufficient space. If no such VMs can be found a new one is created. FFD is often used as a heuristic for classic bin-packing problems~\\cite{ffd}, indicating that it should perform well for the {\\tt Max} metric. \\emph{First-Fit Increasing} ({\\tt FFI}) does the same but first sorts the queries in ascending order, {which, like shortest job first, should reduce the {\\tt PerQuery} goal and therefore the {\\tt Average} query latency metric~\\cite{os}. {\\tt Pack9} first sorts the queries in ascending order, then repeatedly places the 9 shortest remaining queries followed by the largest remaining query. Pack9 should perform well with the {\\tt Percent} performance goal since it will put as many of the most expensive queries into the $10\\%$ margin allowed.}\n\n\\begin{figure*}\n\\centering\n\\begin{subfigure}{0.30\\textwidth}\n\t\\includegraphics[width=\\textwidth]{experiments/multi_vm.pdf}\n\t\\caption{ Optimality with different VM types}\n\t\\label{fig:multi_vm}\n\\end{subfigure}\n\\begin{subfigure}{0.30\\textwidth}\n\t\\includegraphics[width=\\textwidth]{experiments/performance_5000.pdf}\n\t\\caption{ Comparison with metric-specific heuristics}\n\t\\label{fig:performance5000}\n\\end{subfigure}\n\\begin{subfigure}{0.30\\textwidth}\n\t\\centering\n\t\\includegraphics[width=\\textwidth]{experiments/training_time.pdf}\n\t\\caption{\\small Training time vs \\# of query classes}\n\t\\label{fig:training}\n\\end{subfigure}\n\\vspace{-1mm}\n\\caption{ Effectiveness and training overhead of {WiSeDB\\xspace}.}\n\\vspace{-2mm}\n\\end{figure*}\n\n\n\n Figure~\\ref{fig:performance5000} shows the performance of {WiSeDB\\xspace} compared to hard-coded heuristics. \nOne can easily see that there is no single simple heuristic that is  sufficient to handle diverse performance goals. FFD offers the best alternative for the  Max metric, FFI provides good schedules only for the PerQuery and Average metrics and Pack9 works well for the Percent metric. However, our service offers schedules that consistently perform better than all other heuristics.  \\emph{This indicates that {WiSeDB\\xspace} outperforms standard metric-specific heuristics, i.e., its training features are effective in characterizing optimal decisions and learning  special cases of query assignments and orderings with the VMs the fit better with each performance goal}. \n\n\n\n\n\n\n\\subsection{Sensitivity Analysis}\\label{s_sensitivity}\n\n\\begin{figure*}\n\t\\centering\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{experiments/skew_cost_diff.pdf}\n\t\t\\caption{\\small Sensitivity to skewed runtime workloads}\n\t\t\\label{fig:avg_skew}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{experiments/skew_tl.pdf}\n\t\t\\caption{\\small Workload skewness vs cost range}\n\t\t\\label{fig:range_skew}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\centering\n\t\t\\includegraphics[width=\\textwidth]{experiments/cost_model_sens.pdf}\n\t\t\\caption{\\small Optimality for varying cost errors}\n\t\t\\label{fig:cm_sens}\n\t\\end{subfigure}\n\t\\vspace{-1mm}\n\t\\caption{Sensitivity analysis results. }\n\t\\vspace{-2mm}\n\\end{figure*}\n\n\n\n\n\n{\\bf Skewed Workloads} While the training data for the decision tree model is drawn uniformly from the provided query classes, {WiSeDB\\xspace} can still effectively schedule query workloads that are heavily skewed towards one query class or another. Figure~\\ref{fig:avg_skew} shows the average percent increase over the optimal schedule cost for a workload with varying $\\chi^2$ statistics~\\cite{chi2}, which indicates the skewness factor. {The $\\chi^2$ statistic measures the likelihood that a distribution of queries was not drawn randomly. In other words, $0$ represents a uniform distribution of queries to classes and $1$ represents the most skewed. The $\\chi^2$ statistics were calculated with the null hypothesis that each query class would be equally represented. Thus, the value on the x-axis is the confidence with which that hypothesis can be rejected.} \nThe results demonstrate that even with highly skewed workloads consisting of almost exclusively a single query type ($\\chi^2 \\approx 1$), the average percent-increase over the optimal changes by less than $2\\%$. \\emph{Hence, {WiSeDB\\xspace}'s decision models perform effectively even in the presence of  skewed query workloads.} \n\nFigure~\\ref{fig:range_skew} shows both the average and the range of  the cost of 1000 skewed workloads we scheduled and executed for the \\texttt{Max} performance metric. While the mean cost remains relatively constant across different $\\chi^2$  values, the range of the costs is proportional to the amount of skew. {In other words, while the average cost remains constant across these workloads, the variance of the cost increases as skew increases.} This is because a very skewed workload could contain a disproportionate number of cheap or expensive queries, whereas a more uniform workload will contain approximately equal numbers of each. {WiSeDB\\xspace}'s decision models have variance approximately equal to that of an optimal scheduler.\n\n{\\bf Cost Prediction Accuracy} Further, while {WiSeDB\\xspace} assumes that the cost model is entirely accurate when generating training data, {WiSeDB\\xspace} can tolerate significant error in the cost model. These error could be due to errors in estimating the latency of query which could lead to wrong estimations of the penalty costs. Figure~\\ref{fig:cm_sens} shows how the effectiveness of schedules produced by {WiSeDB\\xspace} decrease as cost model errors increase\t. Each $\\sigma^2$ value refers to the error (standard deviation) in the cost model {as a percentage of actual query latency.} {WiSeDB\\xspace} tolerates cost model errors less than $30\\%$ very well. When error exceeds $30\\%$, performance becomes very poor. Given that the state of on performance prediction for cloud databases~\\cite{jennie_contender_edbt14, jennie_sigmod11} offers a prediction error of roughly 20\\%, \\emph{{WiSeDB\\xspace} can still be used to  provide low-cost solutions even in the presence of these prediction errors}. \n\n\\subsection{Efficiency Results}\n\nWhen first given a set of query classes and an performance goal, {WiSeDB\\xspace} must run a training process in order to build the initial decision model. Figure~\\ref{fig:training} shows how long it takes to train models for our four performance metrics and varying numbers of query classes. Here, the higher the number of query classes  the longer the training process since additional query classes represent additional edges that must be explored in the search graph. In the most extreme cases, training can take around 60 seconds. In more tame cases, training takes less than 20 seconds. \\emph{Hence, {WiSeDB\\xspace} can learn metric-specific solution in timely manner (<60sec). This should be sufficiently fast, since the model only needs to be trained once and can be applied to any number of workloads}. \n\nAfter training is complete, {WiSeDB\\xspace} generates a set of models very quickly by   tightening the performance goal  of its original model. Figure~\\ref{fig:retrain} shows how much time is needed to tighten various metrics by certain percentages. Tightening is easiest to understand in the case of the \\texttt{Max} performance goal: initially, the model is trained with a deadline of 15 minutes. Since the longest running query class takes 12 minutes, we say that a 12 minute deadline is the tightest possible goal. Thus, tightening a 15 minute constraint by 100\\% results in a 12 minute constraint. Tightening by 50\\% would result in a 13.5 minute constraint.\n\nFigure~\\ref{fig:retrain} shows that all four metrics can be tightened by up to 40\\% in less than a second. Tightening a constraint by a larger percentage takes more time. This is because the number of training samples that have to be retrained will increase as one makes the constraints tighter. This is most immediately clear for the \\texttt{Max} metric. The jump at $37\\%$ shift represents a large portion of the training set that needs to be recalculated. With no tightening at all ($0\\%$ tightness), or with tightening by only $20\\%$, it was optimal to have many VMs with four queries scheduled on them. However, tightening by $40\\%$ causes the fourth query to miss its deadline, incurring an violation penalty. Since the cost of that assignment has now changed, all the training samples containing that popular assignment must be recalculated. The \\texttt{PerQuery}  and \\texttt{Percent} performance goals have  curves that behave similarly.\n\nThe \\texttt{Average} metric is slightly different. Since the query workloads are drawn uniformly from the set of query classes, their sum is normally distributed (central limit theorem). The average latency of each training sample is thus also normally distributed (since a normal distribution divided by a constant is still normal). Consider two tightenings of $X\\%$ and $Y\\%$, where $X \\geq Y$. Any training sample that must be retrained when tightening by $Y\\%$ will also need to be retrained when tightening by $X\\%$. Each point on the \\texttt{Average} curve in Figure~\\ref{fig:retrain} can be thought of as being the previous point plus a small $\\delta$, where $\\delta$ is the number of additional samples to retrain. The curve is thus the sum of these $\\delta$s, and thus it approximates a Gaussian cumulative density function. \n\n\\emph{In most cases, when a user submits a performance goal  and a set of query classes, {WiSeDB\\xspace} generates a set of alternative trained models that explore the performance vs cost trade off though stricter or more relaxed performance goals in under a minute}.\n\n\\subsection{Online scheduling}\n\n\\begin{figure*}\n\t\\centering\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{experiments/retrain_time.pdf}\n\t\t\\caption{\\small Time to tighten an existing model}\n\t\t\\label{fig:retrain}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{experiments/arrival_rate.pdf}\n\t\t\\caption{\\small Optimality of online scheduling}\n\t\t\\label{fig:arrival_rate}\n\t\\end{subfigure}\n\t\\begin{subfigure}{0.30\\textwidth}\n\t\t\\includegraphics[width=\\textwidth]{experiments/wait_time.pdf}\n\t\t\\caption{\\small Average overhead for online scheduling}\n\t\t\\label{fig:wait_time}\n\t\\end{subfigure}\n\t\\vspace{-1mm}\n\t\\caption{Effectiveness and efficiency of the adaptive modeling and online scheduling.}\n\t\\vspace{-3mm}\n\\end{figure*}\n\n\n{WiSeDB\\xspace} handles the online scheduling of queries by training a new model with additional query classes when a new arrives. Section~\\ref{sec:online} describes the technique used and two optimizations that we apply. The model reuse optimization ({\\tt Reuse}) can be applied to all four performance metric. The linear shifting optimization ({\\tt Shift}) can only be  used  for the \\texttt{Max} and {\\tt PerQuery} metrics. \n\nFigure~\\ref{fig:arrival_rate} shows the performance (cost) of {WiSeDB\\xspace} compared to an optimal scheduler for various query arrival rates and performance metrics. For each metrics, we generate a set of $30$ queries and run them in a random order, varying the time between queries. More time between queries means that the scheduler can use fewer parallel VMs, thus reducing cost. The results demonstrate that {WiSeDB\\xspace} compares favorably with the optimal. In all cases it generates schedules with costs that are within 10\\% of the optimal. \n\n{Figure~\\ref{fig:wait_time} shows the average scheduling overhead when a query arrives for each performance goal type and the impact of our optimizations. This correspond to the average time a query needs to wait before being assigned to a VM. Here, {\\tt Shift} refers to the linear shifting optimization, {\\tt Reuse} refers to the model reuse optimization, and {\\tt Shift+Reuse} refers to using both optimizations. We compare these with {\\tt None} which retrains a new model each time a query arrives.  We assume that, at any particular time of day, the query arrival rate is normally distributed~\\cite{capacity_planning} with a mean of $\\frac{1}{4}$ seconds and standard deviation of $\\frac{1}{8}$ seconds. We also assume a $5\\%$ error in the query latency prediction model. If a query arrives before the last query was scheduled, it waits. \n\nFigure~\\ref{fig:wait_time} shows that the average query wait time can be reduced to below a second for the {\\tt PerQuery} and {\\tt Max} performance goals using both the linear shifting and the model reuse optimization. The {\\tt Average} and {\\tt Percent} performance goals have substantially larger wait times, at around 5 seconds, but a 5 second delay represents only a $2\\%$ slowdown for the average query.} \\emph{Hence, our optimizations are very effective in reducing the query wait time, allowing {WiSeDB\\xspace} to efficiently reuse its generated models and offer online scheduling solutions in a timely manner}.  \n\\\\~\n\n\n\\section{Related Work}\\label{s:related}\nA wide body of research efforts address the problem of workload management in cloud databases. \niCBS~\\cite{icbs} offers a generalized heuristic for scheduling queries based on the convex hull problem, but the algorithm considers only a single machine and performance goals are limited to piecewise-linear functions (which cannot express percentile performance metrics like {WiSeDB\\xspace}). \nIn~\\cite{yun_sla-tree11} they propose a data structure for supporting profit-oriented workload allocation decisions, including scheduling and provisioning. However, their work supports only step-wise SLAs which cannot express average or percentile metrics.\nIn~\\cite{pmax,slo_jignesh12,cloud-optimizer,delphi_pythia} they consider the problem of mapping each tenant (customer) to a single physical machine to meet performance goals, but ordering and scheduling queries is left up to the user. SmartSLA~\\cite{smartsla11} considers the number of VMs to allocate to certain users but focuses on performance trade-offs between users. Again, scheduling queries is left up the user. {WiSeDB\\xspace} learns decision models tuned to specific performance metrics. It supports a wider range of metrics than the above systems while its learning-based approach offers end-to-end solutions that indicate the VMs to be provisioned, the query assignment to VMs as well as their execution order within each VM. \n\n\nActiveSLA~\\cite{q-cop, activesla_hakan11} proposes admission control solutions that reject queries that might cause an SLA violation at runtime, whereas our system seeks to minimize the cost of scheduling every query and to inform the user of performance/cost trade-offs. ~\\cite{pslas} proposes multiple SLAs with different prices for various query workloads, but leave scheduling those queries up to the user and supports only per-query latency SLAs whereas we allow applications to define their own query \\emph{and} workload-level performance metrics.  \n\nIn~\\cite{sqlvm} they propose monitoring mechanism for resource-level SLAs and in~\\cite{bazaar} the propose an approach for translating query-level performance goals to resource requirements, but both assume only a single performance type and leave scheduling up to the user. ~\\cite{opennebula} proposes an end-to-end cloud infrastructure management tool, but it supports only constrains on the maximum query latency within a workload and uses a simple heuristic for task scheduling. {WiSeDB\\xspace} learns specific heuristics for application-specific performance goal by training on optimal schedules for representative workload samples. In~\\cite{sci_place} they use a hypergraph partitioning approach to schedule scientific tasks expressed as directed acyclic graphs on cloud infrastructures, but they do not consider performance goals of any type nor do they consider provisioning of additional resources. Finally, ~\\cite{hadoop_place} proposes a system for scheduling Hadoop jobs on clouds which takes resource provisioning and deadlines into account. However, it considers only simple per-task deadlines and is not easily generalized beyond Hadoop.\n\n\n\\section{conclusions}\n\\label{s:conclusions}\n\n{This work introduces {WiSeDB\\xspace}, a workload management advisor for cloud databases. To the best of our knowledge, {WiSeDB\\xspace} is the first system to address workload management in an \\emph{end-to-end} fashion, handling the tasks of resource provisioning, query placement and query scheduling for arbitrary application-specified performance goals.} \n{{WiSeDB\\xspace} leverages machine learning techniques to create decision models for guiding the low-cost execution of incoming queries under certain performance goals. We have shown that these decision models can efficiently and effectively schedule a wide range of workloads. Furthermore, these  models can be quickly adapted to enable exploration of the performance vs cost trade-offs inherent in cloud computing, as well as provide online query scheduling. Our experiments demonstrate that XCloud can gracefully adapt to errors in cost prediction models, take advantage of multiple VM types, process skewed workloads, and outperform several well-known heuristics with small training overhead.}\n\n{We have a full research agenda moving forward. We are currently investigating alternative features for characterizing the optimal assignment decision as well as alternative learning techniques (e.g., neural networks) for the workload assignment problem. We are also looking into multi metric performance goals that combine workload and query level constraints.}\n\n\\clearpage\n\\newpage\n\\bibliographystyle{abbrv}\n\\bibliography{xcloud-sigmod16}  \n\n\\end{sloppypar}\n\n", "itemtype": "equation", "pos": 60524, "prevtext": "\nFor {performance goals} that are not monotonically increasing, {like average latency,} we simply drop the $h(v)$ term, giving $h^\\prime(v) = {cost({R}, {g})} - {cost({R}, {v})}$. This yields a substantially improved heuristic that is able to find the optimal solution much faster. \n\n{\\bf Graph Reuse} In order to generate a new model $\\alpha^\\prime$ with a stricter performance goal from the training data of an existing model $\\alpha$, we use the new search heuristic $h^\\prime$ to search the new optimal path using the scheduling  graph we generated for each of $\\alpha$'s training workloads.  The new, recalculated paths serve as a training sample for $\\alpha^\\prime$, and (since $h^\\prime(v)$ is admissible) are guaranteed by the A* algorithm to be correct~\\cite{astar}. {This new training set is then used to learn a decision tree model for the stricter performance goal $R^\\prime$.}  This approach saves {WiSeDB\\xspace} from constructing the scheduling graph for each new model which is the step with the dominating overhead. \n\n\n\n\n\n\n\n\n\n\n\\section{Run Time Functionality}\\label{s:runtime}\n\n{A user interacts with {WiSeDB\\xspace}  in two phases. First, in the \\emph{model generation phase}, the user submits their query classes definition, associated latencies and desired performance goal. {WiSeDB\\xspace} will then produce a decision model for scheduling queries under this performance goal as well as several models with stricter/looser goals for the user to examine. After selecting a model with a performance vs. cost trade-off that the user finds desirable, the \\emph{processing phase} beings. In this phase, the user may submit batch workloads, which {WiSeDB\\xspace} will schedule using the user's picked decision model. The same model can also be used for online scheduling where queries arrive one at a time. The system remains in this mode until the user either wishes to switch models or has no more queries to process.}\n\n\n\n\n\n\\subsection{Model Recommendations}\n\nWhile generating alternative decision models to recommend to the user, our goal is to identify a small set of models that represent significantly different performance vs. cost trade-offs. To achieve this, we use a similar approach as in~\\cite{pslas}. We first create a sequence of performance goals in increasing order of strictness\\footnote{\\small This is easily done for most SLA types: for a total latency SLA, push back the final deadline; for an average latency SLA, increase the average latency before a penalty is incurred.}, $\\mathscr{R} = R_1, R_2, \\dots, R_n$, {in which the application-defined goal $R$ is the median}.  For each goal $R_i \\in \\mathscr{R}$, we train a decision model (by shifting the original model as described in Section~\\ref{sec:shift}) and calculate the average cost of each query class over a large random sample workload $Q$. Then, we compute the pairwise differences between the average cost of queries per class of each performance goal $R_i \\in \\mathscr{R}$ using Earth Mover's Distance~\\cite{emd}. We find the smallest such distance between any pairs of performance goals, $EMD(R_i, R_{i+1})$, and we remove $R_{i+1}$ from the sequence. We repeat this process until only $k$ performance goals remain. It has been shown in~\\cite{pslas} that this technique produces a desirable distribution of performance goals (or ``service tiers'') that represent the performance/cost trade-offs of the cloud infrastructure. \n\nHowever, unlike in~\\cite{pslas}, we do not need to \\emph{explicitly} execute any workloads in order to compute the cost of a particular model. Instead, for each decision model {WiSeDB\\xspace} provides a cost estimation function that takes as parameters the size of each query class. Users can easily experiment with different query classes sizes for the incoming workloads and they can estimate the expected cost of executing these workloads using our proposed models. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Batch query scheduling}\nAfter a decision model has been selected by the user, the user can opt to send a batch of queries to be scheduled based on this model. {WiSeDB\\xspace} navigates the decision tree and produces a strategy for executing this work. A  detailed example was given in Section~\\ref{sec:model}.  The strategy indicates the number and types of VMs needed, the assignment of queries to VM and the query execution  order in each VM. The user is then responsible for executing these scheduling decisions on the IaaS cloud infrastructure. \n\n\n\n\n\\subsection{Online Query Scheduling}\n\\label{sec:online}\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/example_sla_online.pdf}\n\\vspace{-1mm}\n\\caption{Online scheduling example}\n\\vspace{-5mm}\n\\label{fig:example_sla_online}\n\\end{figure}\n\n{WiSeDB\\xspace} can also handle non-preemptive online query scheduling. We will describe how {WiSeDB\\xspace} handles the general case of online scheduling and then we will give two optimizations.\n\n\n\n\n{\\bf General Online Scheduling} Let us  assume an available decision model for a performance goal $R$ and query classes $C$. We also assume that a user submits queries, $\\{q^x_1, q^y_2, \\dots,\\}$ at  times $\\{t_1, t_2, \\dots\\}$ respectively  and each query $q^x_i$ belongs to one class $C_i \\in C$. Online scheduling can be viewed as a series of successive batch scheduling tasks where each batch includes a single additional query. The first query is scheduled at time $t_1$ as if it were a batch. In general, when query  $q^x_i$ arrives at $t_i$, we create a new batch $B_i$ containing the queries \\cut{from $\\{q_j,\\dots,q_{i}\\}$} that have not started executing by $t_i$ and use the model to re-schedule them along with query $q^x_i$.\n\nHowever, scheduling $B_i$ as a batch using the same decision model is not guaranteed to offer low-cost solutions. This is because all the queries from $B_{i-1}$ represent queries that have been sitting in the queue since at least the previous timestep, and hence their expected latency is now higher than what was initially assumed. Ideally, a new query class should be created to capture the new expected latency (if this latency is not captured by existing classes), and, in order to discover low-cost solutions that include this new class, a new decision model must be trained. \n\nFormally, any $q^x_y \\in B_{i-1}$ that has yet to run will have a final latency of, at minimum, $l(q^x_y, k) + (t_i - t_y)$, where $k$ is the type of the VM on which $q^x_y$ was assigned and $t_y$ is the arrival time of $q^x_y$. Of course, the latency could be even greater if it is not scheduled to execute immediately at time $t_i$. In order to handle these new query classes, {WiSeDB\\xspace} trains a new decision model using the same performance goal and the new set of query classes. This new model can then be used to schedule $B_i$ as a batch. \n\n\n\n\n\n\n\nFigure~\\ref{fig:example_sla_online} illustrates the results of online scheduling in {WiSeDB\\xspace} with two query classes. Queries with latency of $1$ minute are in class $C_1$ and those with latency of $0.5$ minutes are assigned to class $C_2$. Each query has a deadline of  twice its latency. Figure~\\ref{fig:example_sla_online} shows how different scenarios for the  queries $\\{q^1_1, q^2_2, q^1_3, q^2_4, q^1_5, q^1_6\\}$. {WiSeDB\\xspace} first scheduled query $q^1_1$, which arrived at $t_0$, as a batch. After half a minute ($t_3$), $q^2_2$ and $q^1_3$ arrive and {WiSeDB\\xspace} creates a batch $B_{2-3}$ for them and schedule using the existing model. The model assigns them to the first VM, right after the first query. At time $t_5$, when two more queries, $q^2_4, q^1_5$ arrive, the first query has not yet completed so  we need to create a new batch that includes $q^2_2,q^1_3,q^2_4,q^1_5$. However even though $q_5^1$ and $q^1_3$ are of the same class, they have different performance goals: $q_5^1$ is due within two minutes, and $q^1_3$ is due to complete within $0.5$ minute (since it has been waiting for $0.5$ minutes already). Hence, the existing model cannot be used and a new model needs to be train. This model will include an extra class for queries  with latency of $1$ minute and deadline of $0.5$ minutes.\n\n\n\n\n\n\n\n\n\n\nA substantially fast query arrival rate could create a very large number of query classes, causing an increase in training time and a high frequency of retraining. Since training can be expensive, we offer two optimizations to prevent or accelerate retraining.\n\n\n\n\n\n\n\n\n\n\n{\\bf Model Reuse} As explained above, in Figure~\\ref{fig:example_sla_online}, at $t_5$ we had to train a new model with a new query class of with execution time of $0.5$ minutes and deadline of $0.5$ minutes. When query $q^1_6$ arrives at $t_6$, $q^1_5$'s deadline is also $0.5$ minutes. However, the previous model is already trained for a class of queries with runtime of $0.5$ minutes and deadline of $0.5$ minutes.  We can thus use the same decision model we used at $t_5$ at $t_6$, since the query classes at these two time steps are equivalent. More obviously, we can use the same decision model at $t_7$ as we did at $t_1$, since both $t_1$ and $t_7$ contain only a single query in their respective batches. At times $(t_5, t_6)$ and at times $(t_2, t_7)$, {the difference in time between the newest and the oldest query is the same.} \n\n\n\n\n\n\n\n\nNext, consider the general case. We define $\\omega(i)$ to be the difference in time between the oldest query that has not started processing at time $t_i$ and the newest query at time $t_i$. Formally,\n\n", "index": 11, "text": "\\begin{equation*}\n\\begin{split}\n\\omega(i) & =  t_i - t_j \\mbox{ where } \\\\\nj         & =  \\min \\{ j \\mid j \\leq i \\land \\exists q^x_y (q^x_y \\in B_j \\land q^x_y \\in B_i)\\}\\\\\n\\end{split}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\begin{split}\\displaystyle\\omega(i)&amp;\\displaystyle=t_{i}-t_{j}\\mbox{ where }\\\\&#10;\\displaystyle j&amp;\\displaystyle=\\min\\{j\\mid j\\leq i\\land\\exists q^{x}_{y}(q^{x}_%&#10;{y}\\in B_{j}\\land q^{x}_{y}\\in B_{i})\\}\\\\&#10;\\end{split}\" display=\"block\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"right\"><mrow><mi>\u03c9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mrow><mi/><mo>=</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>-</mo><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>\u2062</mo><mtext>\u00a0where\u00a0</mtext></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mi>j</mi></mtd><mtd columnalign=\"left\"><mrow><mo>=</mo><mi>min</mi><mrow><mo stretchy=\"false\">{</mo><mi>j</mi><mo>\u2223</mo><mi>j</mi><mo>\u2264</mo><mi>i</mi><mo>\u2227</mo><mo>\u2203</mo><msubsup><mi>q</mi><mi>y</mi><mi>x</mi></msubsup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>q</mi><mi>y</mi><mi>x</mi></msubsup><mo>\u2208</mo><msub><mi>B</mi><mi>j</mi></msub><mo>\u2227</mo><msubsup><mi>q</mi><mi>y</mi><mi>x</mi></msubsup><mo>\u2208</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mtd></mtr></mtable></math>", "type": "latex"}]