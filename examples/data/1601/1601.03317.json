[{"file": "1601.03317.tex", "nexttext": "\n\nLet $T$ be the length of the source sentence. We further penalize the condition by adding the following two costs to the categorical cross-entropy cost of the translation model:\n\n\\paragraph{Step-decay cost} We restrict the decay gate from extracting too much information from the condition. So we add a cost term:\n\n", "itemtype": "equation", "pos": 10846, "prevtext": "\n\\maketitle\n\n{\\let\\thefootnote\\relax\\footnotetext{$^\\dag$Work done while Shi was an intern at Microsoft Research.}}\n\n\\begin{abstract}\nNeural machine translation has shown very promising results lately.\nMost NMT models follow the encoder-decoder framework.\nTo make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning.\n\nWe observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models.\nAiming to resolve these problems, we propose new variations of attention-based encoder-decoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attention-based encoder-decoder.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{section:intro}\n\nNeural machine translation has shown promising results lately. Most NMT methods follow the encoder-decoder framework proposed by \\cite{cho2014learning}, which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word. The decoder will stop once a special symbol denoting the end of the sentence is generated. This encoder-decoder framework can be used on general sequence-to-sequence tasks \\cite{sutskever2014sequence}, like question answering and text summarization. After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning \\cite{vinyals2014show,xu2015show}. In the following discussion, we focus on the task of machine translation.\n\nIn the original encoder-decoder model, although the encoder RNN generates a set of hidden states, one at each position of the source sentence, the decoder only takes the last one. This design in effect compresses the variable-length source sentence into a fixed-length context vector, with the information of each source word implicitly stored in the context vector. Thus the decoder cannot easily make full use of the whole sequence of encoder hidden states. To make it more flexible and generalize the fixed-length representation to a variable-length one, it was proposed to use attention mechanism for machine translation \\cite{bahdanau2014neural}.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{figures/intro_align_1}\n\\caption{Alignment by attention mechanism.\n    Each row is a distribution of how the corresponding target word (English) is aligned to source word.\n    Darker color denotes higher weight.\n}\n\\label{fig:intro_align_1}\n\\end{figure}\n\nAttention mechanism was first proposed to allow models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem \\cite{mnih2014recurrent}. \n\nIn \\cite{bahdanau2014neural}, attention mechanism was applied to machine translation to learn an alignment between source words and target words. Fig.~\\ref{fig:intro_align_1} shows a sample alignment given by attention mechanism. \n\nWith the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering \\cite{hermann2015teaching}, speech recognition \\cite{bahdanau2015end,chorowski2014end}, image captioning \\cite{xu2015show} and visual question answering \\cite{xu2015ask,chen2015abc,shih2015look}. In these applications, variations of attention mechanism were proposed to enhance its performance. \n\n\n\n\n\n\\section{Problems of Attention Mechanism}\n\\label{section:problems}\n\nBy training the encoder-decoder model with attention mechanism, we get an alignment from target word to source word. This alignment helps translation by allowing re-ordering. But since the alignment by attention is not always accurate, we observed that in many cases where the alignment is incorrect, the translation quality is significantly damaged. We attribute this kind of problem to the lack of explicit distortion and fertility models in attention-based encoder-decoder model.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/prob_distortion_1}\n\\caption{Wrong alignment due to lack of distortion model.}\n\\label{fig:prob_distortion_1}\n\\end{figure}\n\n\\subsection{Lack of Distortion Model}\n\\label{section:problem_distortion}\n\nIn SMT, the distortion model controls how the words are re-ordered. In Fig.~\\ref{fig:prob_distortion_1} we show an example alignment given by attention mechanism where incorrect alignment in the middle of the sentence caused the translation to go wrong afterwards. We focus on the later part of the sentence where the correct translation should be:\n\n``...\\emph{and warned that the election to be held on january 30th next year would not be and end to serious violence in iraq.}\"\n\nwhich get translated into:\n\n``...\\emph{and warned that it would not be the end of iraq's serious violence {\\bf {next year}}.}\"\n\nFrom the alignment matrix we can see that, the word ``\\begin{CJK}{UTF8}{gbsn}\u00e9\u00a2\u0084\u00e5\u00ae\u009a\\end{CJK}\" (means ``scheduled\") in source sentence is attended to by ``\\emph{would}\", but the next attention jumped to ``\\begin{CJK}{UTF8}{gbsn}\u00e5\u00a4\u00a7\u00e9\u0080\u0089\\end{CJK} (means ``election\"), while it should focus on ``\\begin{CJK}{UTF8}{gbsn}\u00e6\u0098\u008e\u00e5\u00b9\u00b4\\end{CJK}\" (means ``next year\") or on the date. After this incorrect re-ordering, the meaning of the source sentence is twisted in the translation. If the attention is aware of the previous alignment, it should be able to order ``\\emph{next year}\" after ``\\emph{election}\" and reserve the mearning of source sentence correctly.\n\nIn this kind of casese, the translation can go wrong due to incorrect re-ordering. We attribute this kind of problem due to the lack of distortion model in attention-based encoder-decoder.\n\n\\subsection{Lack of Fertility Model}\n\\label{section:problem_fertility}\n\nIn SMT, the fertility model controls how many target words are translated from a source word.\nWe observe two phenomena related to the lack of fertility model in attention-based encoder-decoder: the problem of repetition and the problem of coverage.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/prob_repetition_1}\n\\caption{Problem of repetition in alignment.}\n\\label{fig:prob_repetition_1}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/prob_coverage_1}\n\\caption{Problem of coverage in alignment.}\n\\label{fig:prob_coverage_1}\n\\end{figure}\n\n\\paragraph{Problem of Repetition}\nFig.~\\ref{fig:prob_repetition_1} shows an example of repetition problem in alignment. For consecutive words, the attention mechanism focused on the same position in the source sentence, resulting in repetition in the translation, ``\\emph{the organization of europe and the organization of cooperation in europe}\".\n\n\\paragraph{Problem of Coverage}\nFig.~\\ref{fig:prob_coverage_1} shows an example of coverage problem in alignment. We see that some part of the source sentence was not attended to, resulting in significant loss of content in the translation. \n\nThese two problems are due to the lack of fertility model in NMT: in the first case, some source words are translated into too many target words, while in the second case, some sources words are translted into too few target words.\n\nAlthough attention mechanism already makes the encoder-decoder more flexible by allowing re-ordering, the observed problems demonstrated some restrictions of it. Motivated by these observations, we propose additions of implicit distortion and fertility models to attention-based encoder decoder. In Sec.~\\ref{section:recurrent_attention}, we introduce \\textsc{RecAtt} and \\textsc{RNNAtt} which are designed as an implicit distortion models. In Sec.~\\ref{section:conditioned_decoder}, we introduce \\textsc{CondDec} which is designed as an implicit fertility model. We verify that the proposed methods can resolve the observed problems in our experiments in Sec.~\\ref{section:qualitative}.\n\n\\section{Attention-based Encoder-Decoder}\n\\label{section:methods}\n\nWe start by reviewing the RNN used in NMT papers and the encoder-decoder with attention mechanism from \\cite{bahdanau2014neural}.\n\n\\input{sections/gru.tex}\n\n\\input{sections/rnnsearch.tex}\n\n\n\\section{Recurrent Attention Mechanisms}\n\\label{section:recurrent_attention}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{figures/models/attention_decoder}\n\\caption{Attention Decoder.\n    The dashed lines show how the current hidden state is passed to the next decoding step.\n    The current attention is computed with the previous hidden state.\n}\n\\label{fig:attention_decoder}\n\\end{figure}\n\nIn Fig.~\\ref{fig:attention_decoder} we show an abstraction of the decoder-attention structure.\n\nWe note that attention mechanism treats the encoder states as a set, not a sequence, while the source sentence order is crucial to re-ordering. And the state of re-ordering given to the attention unit is all embedded in the hidden state of the decoder - the attention unit itself does not have memory. \n\nMotivated by the analysis in Sec.~\\ref{section:problem_distortion}, we propose to add recurrent paths to the decoder-attention structure to provide the attention unit with more information the re-ordering. With these recurrent paths, instead of making the decoder remembering what the state of re-ordering is, recurrent attention mechanism explicitly keeps track of this information. \n\n\\input{sections/recatt.tex}\n\\input{sections/rnnatt.tex}\n\n\\section{Conditioned Decoder}\n\\label{section:conditioned_decoder}\n\nAs analyzed in Sec.~\\ref{section:problem_fertility}, attention mechanism might produce incorrect alignment and low-quality translation due to the lack of explicit distortion and fertility models. To address this issue, we propose conditioned decoder, \\textsc{CondDec}, which uses a condition vector to represent what information has been extracted from the source sentence. This can be seen as an implicit fertility model, the condition vector can keep track of how many target words are translated from each source word. We use a structure similar to \\cite{wen2015semantically} where a predefined condition is used to guide natural language generation. Different from that method, we use a trainable condition initialized with the last encoder hidden state. At each decoding step, the condition is updated with the decoder state and used to compute the next decoder state. The decoder GRU with attention and condition ${\\bm{{sd}}}_i$ is defined as adding an extra decay gate $vd_i$ to decoder:\n\n", "index": 1, "text": "\\begin{align*}\n  {\\bm{{r}}}_i &= \\sigma({\\bm{{W}}}^r{\\bm{{x}}}_{i} + {\\bm{{U}}}^r{\\bm{{h}}}_{i-1} + {\\bm{{V}}}^r{\\bm{{c}}}_i) \\\\\n  {\\bm{{h}}}'_i &= \\tanh({\\bm{{r}}}_i \\circ {\\bm{{U}}}{\\bm{{h}}}_{i-1} + {\\bm{{W}}}{\\bm{{x}}}_{i} + {\\bm{{V}}}{\\bm{{c}}}_i) \\\\\n  {\\bm{{z}}}_i &= \\sigma({\\bm{{W}}}^z{\\bm{{x}}}_{i} + {\\bm{{U}}}^z{\\bm{{h}}}_{i-1} + {\\bm{{V}}}^z{\\bm{{c}}}_i) \\\\\n  {\\bm{{d}}}_i &= \\sigma({\\bm{{W}}}^d{\\bm{{x}}}_{i} + {\\bm{{U}}}^d{\\bm{{h}}}_{i-1} + {\\bm{{V}}}^d{\\bm{{c}}}_i) \\\\\n  {\\bm{{sd}}}_i &= {\\bm{{d}}}_i \\circ {\\bm{{sd}}}_{i-1} \\\\\n  {\\bm{{h}}}_i &= (1 - {\\bm{{z}}}_i) \\circ {\\bm{{h}}}'_i + {\\bm{{z}}}_i \\circ {\\bm{{h}}}_{i-1} + \\tanh({\\bm{{V}}}^h{\\bm{{sd}}}_i)\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{r}}}_{i}\" display=\"inline\"><msub><mi>\ud835\udc93</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\bm{{W}}}^{r}{\\bm{{x}}}_{i}+{\\bm{{U}}}^{r}{\\bm{{h}}}_{i-%&#10;1}+{\\bm{{V}}}^{r}{\\bm{{c}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc7e</mi><mi>r</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc99</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7c</mi><mi>r</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7d</mi><mi>r</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{h}}}^{\\prime}_{i}\" display=\"inline\"><msubsup><mi>\ud835\udc89</mi><mi>i</mi><mo>\u2032</mo></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\tanh({\\bm{{r}}}_{i}\\circ{\\bm{{U}}}{\\bm{{h}}}_{i-1}+{\\bm{{W}}}{%&#10;\\bm{{x}}}_{i}+{\\bm{{V}}}{\\bm{{c}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><msub><mi>\ud835\udc93</mi><mi>i</mi></msub><mo>\u2218</mo><mi>\ud835\udc7c</mi></mrow><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>\ud835\udc7e</mi><mo>\u2062</mo><msub><mi>\ud835\udc99</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><mi>\ud835\udc7d</mi><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{z}}}_{i}\" display=\"inline\"><msub><mi>\ud835\udc9b</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\bm{{W}}}^{z}{\\bm{{x}}}_{i}+{\\bm{{U}}}^{z}{\\bm{{h}}}_{i-%&#10;1}+{\\bm{{V}}}^{z}{\\bm{{c}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc7e</mi><mi>z</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc99</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7c</mi><mi>z</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7d</mi><mi>z</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{d}}}_{i}\" display=\"inline\"><msub><mi>\ud835\udc85</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sigma({\\bm{{W}}}^{d}{\\bm{{x}}}_{i}+{\\bm{{U}}}^{d}{\\bm{{h}}}_{i-%&#10;1}+{\\bm{{V}}}^{d}{\\bm{{c}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msup><mi>\ud835\udc7e</mi><mi>d</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc99</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7c</mi><mi>d</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc7d</mi><mi>d</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>i</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{sd}}}_{i}\" display=\"inline\"><mrow><mi>\ud835\udc94</mi><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mi>i</mi></msub></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bm{{d}}}_{i}\\circ{\\bm{{sd}}}_{i-1}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><msub><mi>\ud835\udc85</mi><mi>i</mi></msub><mo>\u2218</mo><mi>\ud835\udc94</mi></mrow><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{h}}}_{i}\" display=\"inline\"><msub><mi>\ud835\udc89</mi><mi>i</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=(1-{\\bm{{z}}}_{i})\\circ{\\bm{{h}}}^{\\prime}_{i}+{\\bm{{z}}}_{i}%&#10;\\circ{\\bm{{h}}}_{i-1}+\\tanh({\\bm{{V}}}^{h}{\\bm{{sd}}}_{i})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msub><mi>\ud835\udc9b</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2218</mo><msubsup><mi>\ud835\udc89</mi><mi>i</mi><mo>\u2032</mo></msubsup></mrow><mo>+</mo><mrow><msub><mi>\ud835\udc9b</mi><mi>i</mi></msub><mo>\u2218</mo><msub><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc7d</mi><mi>h</mi></msup><mo>\u2062</mo><mi>\ud835\udc94</mi><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03317.tex", "nexttext": "\n\n\\paragraph{Left-over cost} We want the decoder to extract as much information as possible from the condition after reading the source sentence. So we add a cost term:\n\n", "itemtype": "equation", "pos": 11847, "prevtext": "\n\nLet $T$ be the length of the source sentence. We further penalize the condition by adding the following two costs to the categorical cross-entropy cost of the translation model:\n\n\\paragraph{Step-decay cost} We restrict the decay gate from extracting too much information from the condition. So we add a cost term:\n\n", "index": 3, "text": "\\begin{align*}\n\\text{cost}_{decay} = \\frac{1}{T}\\sum_{j=1}^{T}||{\\bm{{sd}}}_j - {\\bm{{sd}}}_{j-1}||_2\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{cost}_{decay}=\\frac{1}{T}\\sum_{j=1}^{T}||{\\bm{{sd}}}_{j}-{%&#10;\\bm{{sd}}}_{j-1}||_{2}\" display=\"inline\"><mrow><msub><mtext>cost</mtext><mrow><mi>d</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>y</mi></mrow></msub><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>T</mi></mfrac></mstyle><mo>\u2062</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><msub><mrow><mo fence=\"true\">||</mo><mrow><mrow><mi>\ud835\udc94</mi><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mi>j</mi></msub></mrow><mo>-</mo><mrow><mi>\ud835\udc94</mi><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03317.tex", "nexttext": "\n\nThese two costs are added to the categorical cross-entropy cost of the translation model. At training time, the costs are used to enforce a fertility model and are ignored at test time.\n\n\\section{Related work}\nThere are variations of attention mechanism that have recurrent paths similar to that of \\textsc{RecAtt}. In this section, we review these models and compare those decoder-attention structures.\n\\input{sections/inputfeed.tex}\n\\input{sections/hybridatt.tex}\n\nWe note that \\textsc{HybridAtt1} and \\textsc{HybridAtt2} were proposed for speech recognition task, which requires less re-ordering compared to translation. Although these models improved the performance of encoder-decoder on speech recognition, they not necessarily will on machine translation. We included these models because they have structures similar to \\textsc{RecAtt}.\n\nOther variations of attention mechanism with similar recurrent paths include \\cite{mnih2014recurrent}, \\cite{chen2015abc}. In these works, the authors used attention mechanism on image classification and visual question answering respectively. The variations of attention mechanism they used are location-based attention, which is more reasonable for image-related tasks. Due to these reasons we do not review or compare their methods in this work.\n\n\\section{Experimental Setup}\n\\label{section:exp_setup}\n\nIn this section we describe the data used in our experiments, our evaluation methods and our validation procedure. \n\n\\begin{description}[leftmargin=0pt]\n\n\\item[Datasets]\nFor training, we use NIST Chinese-English training set excluding the Hong Kong Law and Hong Kong Hansard (0.5m sentence pairs after exclusion). For testing, we use Nist2005 dataset (1082 sentence pairs). For validation, we use Nist2003 dataset (913 sentence pairs). Validation set is only used for early-stopping and training process monitoring. \n\n\nFollowing \\cite{bahdanau2014neural}, we use source and target dictionaries of size $30000$, covering 97.4\\% and 98.9\\% of the vocabularies respectively. Out-of-vocabulary words are replaced with a special token {$\\langle$UNK$\\rangle$\\xspace}.\n\n\\item[Post-processing]\nWe perform post-processing based on the alignment given by attention mechanism.  \nFor each translated target word, we choose the source word assigned with the highest attention weight as the aligned word.\n\n\n\n{$\\langle$UNK$\\rangle$\\xspace}'s in the translated sentence are replaced with the correct translation of the aligned source word. We make a simple word-level translation table from the alignment result given by GIZA++ \\cite{och2003systematic} from the training set: for each source word, we choose the most frequently aligned target word.\n\n\\item[Evaluation] \n\nPerformance is evaluated by BLEU score \\cite{papineni2002bleu} over the test set. \n\n\nWe compare 6 models, \\textsc{RNNSearch} \\cite{cho2014learning}, \\textsc{HybridAtt2} \\cite{bahdanau2015end}, \\textsc{InputFeed} \\cite{luong2015effective}, and three proposed models, \\textsc{RecAtt}, \\textsc{RNNAtt} and \\textsc{CondDec}. We skip \\textsc{HybridAtt1} because we have \\textsc{HybridAtt2} as an improved version.\n\nWe benchmark the 6 NMT models with our implementation of hierarchical phrase-based SMT from \\cite{chiang2007hierarchical}, with standard features, denoted as \\textsc{SMT}.\n\n\\item[Validation]\nValidation is done by calculating the BLEU score over the validation set without post-processing, using {\\tt {MultiBleu.perl}} script from \\cite{bahdanau2014neural}.\nFor each model, we choose the parameters of the highest validation score.\n\n\\item[Model Training]\nThe encoder and decoder have $1000$ hidden units each. The dimension of source and target word embedding is $620$.\nFollowing \\cite{bahdanau2014neural}, we use dropout rate $0.5$.\n\nWe remove sentences of length over 50 words from the training set. \nWe use batch size of 80 with 12 batches pre-fetched and sorted by the sentence length.\n\nEach model is trained with AdaGrad \\cite{duchi2011adaptive} on K40m GPU for approximately 4 days, finishing over 400000 updates, equivalent to 640 epochs.\n\nWhen testing trained models, we use beam search \\cite{graves2012sequence,boulanger2013audio,sutskever2014sequence} with beam size of 12.\n\n\\end{description}\n\n\\section{Results}\n\\label{section:results}\n\n\\subsection{Quantitative}\n\\label{section:quantitative}\n\n\\begin{table}\n\\centering\n\\begin{tabular}{l r r r}\n           &\tBefore  &\tAfter   & Improvement \\\\\\hline\n  \\textsc{SMT}        &    /    & 32.25 & / \\\\\n  \\textsc{RNNSearch}  &\t26.65\t  &\t31.02\t& 4.37 \\\\\\hline\n  \\textsc{HybridAtt2} &\t24.60\t  &\t29.14\t& 4.54 \\\\\n  \\textsc{InputFeed}  &\t25.44\t  &\t29.02\t& 3.58 \\\\\\hline\n  \\textsc{RecAtt}     &\t{\\bf {28.10}}\t&\t{\\bf {33.14}} & {\\bf {5.04}}\t\\\\\n  \\textsc{RNNAtt}     &\t25.04\t  &\t30.02\t& 4.98 \\\\\n  \\textsc{CondDec}    & 27.48   & 32.21 & 4.73 \\\\\\hline\n\\end{tabular}\n\\caption{BLEU scores w/o post-processing and the improvement from post-processing}\n\\label{table:overview_results}\n\\end{table}\n\nBLEU scores on the test set are shown in (Table~\\ref{table:overview_results}).\n\n\\textsc{RecAtt} performed best among NMT models, with and without post-processing. \\textsc{RecAtt} achieved a $2.1$ BLEU score improvement over the original \\textsc{RNNSearch}. \n\nNote that \\textsc{RecAtt} also gained the most improvement from post-processing, $5.04$ BLEU points. In the post-processing, we use a naive translation table which is generated purely from the training data so the effect of post-processing depends largely on the quality of the alignment. Thus the gain from post-processing can be seen as a measurement of the quality of attention-generated alignment, and from this we see that \\textsc{RecAtt} improved attention mechanism.\n\n\\textsc{CondDec} out-performed \\textsc{RNNSearch} by 1 BLEU point, both with and without post-processing.\n\nAll three of our proposed models out-performed the phrase-based SMT baseline.\n\nThe combination of \\textsc{CondDec} with \\textsc{RecAtt} and \\textsc{RNNAtt} is a work in progress.\n\n\\subsection{Qualitative}\n\\label{section:qualitative}\n\nAs mentioned in Sec.~\\ref{section:problems}, the original attention-based encoder-decoder has some problems due to the lack of distortion and fertility models. In this section we will qualitatively evaluate how our models resolved these problems.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_distortion_1}\n\\caption{Effect of implicit distortion model.}\n\\label{fig:expr_distortion_1}\n\\end{figure}\n\n\\paragraph{Distortion}\n\nWe show the alignment and translation by \\textsc{RecAtt} in Fig.~\\ref{fig:expr_distortion_1} on the same sentence of Fig.~\\ref{fig:prob_distortion_1}. In the alignment by \\textsc{RecAtt}, it can be seen that ``\\emph{will not}\" are correctly aligned to ``\\begin{CJK}{UTF8}{gbsn}\u00e4\u00b8\u008d\u00e4\u00bc\u009a\\end{CJK}\" (means ``will not\") and ``\\emph{next year}\" is correctly ordered to describe ``\\emph{the election to be held}\" instead of ``\\emph{riot in iraq}\". The transltion quality of the whole sentence is also higher.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_coverage_1}\n\\caption{Example of coverage problem. \\\\\nLeft: \\textsc{RNNSearch}.\nRight: \\textsc{CondDec}.}\n\\label{fig:expr_coverage_1}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_coverage_2}\n\\caption{Example of coverage problem. \\\\\nLeft: \\textsc{RNNSearch}.\nRight: \\textsc{RecAtt}.}\n\\label{fig:expr_coverage_2}\n\\end{figure}\n\n\n\\paragraph{Fertility: Coverage} In Fig.~\\ref{fig:expr_coverage_1} we show the alignments given by \\textsc{RNNSearch} and \\textsc{RecAtt}. From the alignment of \\textsc{RNNSearch}, we can observe the problem of coverage where the later part of the source sentence is lost in the translation, while the alignment given by \\textsc{RecAtt} does not have this problem and covered the whole source sentence. \n\nWe observed that \\textsc{RecAtt} can also resolve the coverage problem. This is because a correct alignment can be very helpful in preventing the incorrect generation of end-of-sentence symbol. In Fig.~\\ref{fig:expr_coverage_2} we show an example. In the alignment by \\textsc{RNNSearch}, when generating the word after ``,\" (last row), the attention is not very concentrated, leading to the generation of end-of-sentence symbol. While in the alignment by \\textsc{RecAtt} when generating that word, the attention correctly focused on ``\\begin{CJK}{UTF8}{gbsn}\u00e8\u00be\u0083\\end{CJK}\" (means ``more\") with high confidence, leading to the correct generation of ``\\emph{more}\".\n\n\\begin{figure}[t]\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_repetition_1}\n\\caption{Example of repetition problem. \\\\\nLeft: \\textsc{RNNSearch}.\nRight: \\textsc{CondDec}.}\n\\label{fig:expr_repetition_1}\n\\end{figure}\n\n\\paragraph{Fertility: Repetition} In Fig.~\\ref{fig:expr_repetition_1} we see that the problem of repetition occurred in the alignment by \\textsc{RNNSearch}. ``\\begin{CJK}{UTF8}{gbsn}\u00e4\u00b8\u009c\u00e6\u0096\u00b9 \u00e5\u00bf\u00ab\u00e8\u00bd\u00a6\\end{CJK}\" (means ``midnight express\") is repeatedly focused on and translated into ``\\emph{moon ice}\" and ``\\emph{night of the midnight of the night}\". \\textsc{CondDec} produces both the correct alignment and the correct translation ``\\emph{midnight express}\".\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_long_repetition_1}\n\\caption{Long repetition.}\n\\label{fig:expr_long_repetition_1}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1.0\\columnwidth]{figures/expr_long_repetition_2}\n\\caption{Example of long repetition. \\\\\nLeft: \\textsc{RNNSearch}.\nRight: \\textsc{RNNAtt}.}\n\\label{fig:expr_long_repetition_2}\n\\end{figure}\n\n\\paragraph{Long Repetition} \nWe observed that \\textsc{RecAtt} can also resolve the repetition problem. Because the previous attention-generated context was passed to the attention unit, the attention can decide not to focus on the same position as last time. But since it only has a short-term memory, in some cases the alignment by \\textsc{RecAtt} has long repetitions as shown in Fig.~\\ref{fig:expr_long_repetition_1}. \n\nAlthough \\textsc{RNNAtt} did not perform as well as \\textsc{RecAtt} in terms of BLEU score, we observe that it can resolve the long repetition problem that is hard for \\textsc{RecAtt} to handle. In Fig.~\\ref{fig:expr_long_repetition_2} we show the alignments by \\textsc{RNNSearch} and \\textsc{RNNAtt}. First we can see that \\textsc{RNNSearch} did not handle this sentence well, with incorrect alignment and low-quality translation, which shows that this sentence is hard to translate. However the alignment by \\textsc{RNNAtt} is more accurate and the translation quality is higher than both \\textsc{RNNSearch} and \\textsc{RecAtt}, with no long repetition problem.\n\nOne possible reason for the low BLEU score of \\textsc{RNNAtt} is that the path from translation cost to the attention recurrent unit is too long and the model is hard to train. Improving end-to-end performance and exploring alternative structures for \\textsc{RNNAtt} is a work in progress.\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions}\n\nIn this paper we noted some problems occurred in neural machine translation due to the lack of distortion and fertility model.\n\nTo resolve these problems, we proposed to add implicit distortion and fertility models to attention-based encoder-decoder. We proposed recurrent attention mechanism, \\textsc{RecAtt} and \\textsc{RNNAtt} for distortion model, and \\textsc{CondDec} for fertility model. We compared our models with other related variations.\n\nWe evaluated our methods both quantitatively and qualitatively. In Chinese-English translation, \\textsc{RecAtt} gained an improvement of 2 BLEU points over the original attention mechanism and out-performed all related models. \\textsc{CondDec} also out-performed the original attention mechanism by 1 BLEU point.\nBy analyzing the alignment matrix produced by attention mechanism, we demonstrated that our proposed methods help resolve the observed problems.\n\nWe are working on the combination of \\textsc{CondDec} with \\textsc{RecAtt} and \\textsc{RNNAtt}. We will also explore alternative structures of recurrent attention mechanism and try to improve the end-to-end performance of \\textsc{RNNAtt}.\n\n\n\n\n\n\n\n\\bibliographystyle{acl2015}\n\\bibliography{master}\n\n\n", "itemtype": "equation", "pos": 12130, "prevtext": "\n\n\\paragraph{Left-over cost} We want the decoder to extract as much information as possible from the condition after reading the source sentence. So we add a cost term:\n\n", "index": 5, "text": "\\begin{align*}\n\\text{cost}_{left} = ||{\\bm{{sd}}}_T||_2\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\text{cost}_{left}=||{\\bm{{sd}}}_{T}||_{2}\" display=\"inline\"><mrow><msub><mtext>cost</mtext><mrow><mi>l</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mi>t</mi></mrow></msub><mo>=</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><mi>\ud835\udc94</mi><mo>\u2062</mo><msub><mi>\ud835\udc85</mi><mi>T</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></math>", "type": "latex"}]