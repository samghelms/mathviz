[{"file": "1601.01280.tex", "nexttext": "\nwhere $y_{<t} = \\left(y_1, \\cdots, y_{t-1} \\right)$. In the following\nwe describe the details of how~$p\\left( a | q \\right)$ is computed.\n\n\n\\subsection{Sequence-to-Sequence Model}\n\\label{sec:sequ-sequ-model}\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{seq2seq.pdf}\n\t\\caption{A sequence-to-sequence model with two-layer recurrent\n\t\tneural networks.  }\n\t\\label{fig:seq2seq}\n\\end{figure}\n\nOur model consists of an \\textbf{encoder} which encodes natural\nlanguage input $q$ into a vector representation and a \\textbf{decoder}\nwhich learns to generate $y_1, \\cdots, y_{|a|}$ conditioned on the\nencoding vector.\n\nAs shown in Figure~\\ref{fig:seq2seq}, the encoder and decoder are two\ndifferent $L$-layer recurrent neural networks with Long Short-Term\nMemory (LSTM) units which recursively process tokens one by one.  The\nfirst $|q|$ time steps belong to the encoder, while the following~$|a|$ time\nsteps belong to the decoder.  Let ${\\mathbf{h} }_{ t }^{ l } \\in\n\\mathbb{R}^{n}$ denote an $n$-dimensional hidden vector in time\nstep~$t$ and layer~$l$. ${\\mathbf{h} }_{ t }^{ l }$ is computed by:\n\n", "itemtype": "equation", "pos": 7348, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\n\t\n\tSemantic parsing aims at mapping natural language to machine\n\tinterpretable meaning representations. Traditional approaches rely\n\ton high-quality lexicons, manually-built templates, and linguistic\n\tfeatures which are either domain- or representation-specific.\n\t\n\tIn this paper, we present a general method based on an\n\tattention-enhanced sequence-to-sequence model.  We encode input\n\tsentences into vector representations using recurrent neural\n\tnetworks, and generate their logical forms by conditioning the\n\toutput on the encoding vectors.  The model is trained in an\n\tend-to-end fashion to maximize the likelihood of target logical\n\tforms given the natural language inputs.\n\t\n\tExperimental results on four datasets show that our approach\n\tperforms competitively without using hand-engineered features and is\n\teasy to adapt across domains and meaning representations.\n\t\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\nSemantic parsing is the task of translating text to a formal meaning\nrepresentation such as logical forms or structured queries.  There has\nrecently been a surge of interest in developing machine learning\nmethods for semantic parsing (see the references in\nSection~\\ref{sec:related-work}), due in part to the existence of\ncorpora containing utterances annotated with formal meaning\nrepresentations. Figure~\\ref{fig:introduction} shows an example of a\nquestion (left hand-side) and its annotated logical form (right\nhand-side), taken from \\textsc{Jobs}~\\cite{cocktail}, a well-known\nsemantic parsing benchmark. In order to predict the correct logical\nform for a given input utterance, most previous systems rely on\npredefined templates and manually designed features, which often\nrender parsers domain- or representation-specific.  In this work, we\naim to use a simple yet effective method to bridge the gap between\nnatural language and logical form with minimal domain knowledge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{overview.pdf}\n\t\\caption{We treat both input utterances and their logical forms as sequences which are encoded and decoded with different recurrent neural networks. The attention layer is used to learn soft alignments between natural language and logical form.}\n\t\\label{fig:introduction}\n\\end{figure}\n\n\nSequence transduction architectures based on recurrent neural networks\nhave been successfully applied to a variety of NLP tasks ranging from\nsyntactic parsing~\\cite{grammar:foreign:language}, to machine\ntranslation~\\cite{mt:kalchbrenner13emnlp,mt:cho-EtAl:2014:EMNLP2014,mt:seq2seq},\nand image description\ngeneration~\\cite{imagecaption:rnn:stanford,imagecaption:lstm:google}.\nAn attention mechanism is often used to improve performance for long\nsequences~\\cite{mt:jointly:align:translate,grammar:foreign:language}.\nWe adapt the general sequence-to-sequence modeling paradigm to the\nsemantic parsing task.  As shown in Figure~\\ref{fig:introduction}, our\nmodel learns from natural language descriptions paired with meaning\nrepresentations; it encodes sentences and decodes logical forms using\nrecurrent neural networks with long short-term memory (LSTM) units.\nWe also introduce an attention\nmechanism~\\cite{mt:jointly:align:translate,luong-pham-manning:2015:EMNLP}\nwhich allows the model to learn soft alignments between natural\nlanguage and logical forms.  Rare words present a problem to sequence\ntransduction models, and semantic parsing is no exception. In order to\nhandle rare words, we mask entities and numbers with their types and\nrecover them in a post-processing step.\n\nEvaluation results demonstrate that our model achieves similar or\nbetter performance when compared to previous methods across different\ndomains and meaning representations, despite using no hand-engineered\nfeatures, templates, domain or linguistic knowledge.\n\n\nThe contributions of our work are three-fold: (1)~we present a\ngeneral-purpose sequence-to-sequence model for semantic parsing;\n(2)~propose a method to address the rare word problem for our neural\nmodel; and (3)~conduct extensive experiments on several benchmark\ndatasets providing detailed analysis on what our model learns.\n\n\\section{Related Work}\n\\label{sec:related-work}\n\nOur work synthesizes two strands of research, namely semantic parsing\nand the encoder-decoder architecture with neural networks.\n\nThe problem of learning semantic parsers has received significant\nattention, dating back to \\newcite{lunar}. A large\nnumber of approaches learn from sentences paired with logical forms\nfollowing different modeling strategies. Examples include the use of\nparsing models \\cite{miller96,scissor,lnlz08,tisp}, inductive logic\nprogramming \\cite{chill,tang-mooney:2000:EMNLP,Thomson:Mooney:2003},\nprobabilistic automata \\cite{He:Young:2006}, string/tree-to-tree\ntransformation rules \\cite{silt}, classifiers based on string kernels\n\\cite{krisp}, machine translation \\cite{wasp,lambdawasp,sp:as:mt}, and\ncombinatory categorial grammar induction techniques\n\\cite{zc05,zc07,ubl,fubl}.  Other work learns semantic parsers without\nrelying on logical-from annotations, e.g.,~from sentences paired with\nconversational logs \\cite{artzi-zettlemoyer:2011:EMNLP}, system\ndemonstrations\n\\cite{Chen:Mooney:2011,Goldwasser:Roth:2011,Artzi:Zettlemoyer:2013},\nquestion-answer pairs \\cite{clarke2010driving,dcs}, and distant\nsupervision\n\\cite{krishnamurthy2012weakly,cai2013semantic,reddy2014semanticparsing}.\n\n\nOur model learns from natural language descriptions paired with\nmeaning representations. Most previous systems rely on high-quality\nlexicons, manually-built templates, and features which are either\ndomain- or representation-specific. We instead present a general\nmethod that can be easily adapted to different domains and meaning\nrepresentations. We adopt the general encoder-decoder framework based\non neural networks which has been recently repurposed for various NLP\ntasks such as syntactic parsing~\\cite{grammar:foreign:language},\nmachine\ntranslation~\\cite{mt:kalchbrenner13emnlp,mt:cho-EtAl:2014:EMNLP2014,mt:seq2seq},\nimage description\ngeneration~\\cite{imagecaption:rnn:stanford,imagecaption:lstm:google},\nquestion answering \\cite{hermann2015teaching}, and summarization\n\\cite{rush2015neural}.\n\n\n\n\n\n\n\n\n\\newcite{neural:instruction} use a sequence-to-sequence model to map\nnavigational instructions to actions. Our model works on more\nwell-defined meaning representations (such as Prolog and lambda\ncalculus) and is conceptually simpler; it does not employ\nbidirectionality or multi-level alignments. \\newcite{deep:sp} propose\na different architecture for semantic parsing based on the combination\nof two neural network models.  The first model learns shared\nrepresentations from pairs of questions and their translations into\nknowledge base queries, whereas the second model generates the queries\nconditioned on the learned representations. However, they do not\nreport empirical evaluation results.\n\n\n\n\n\\section{Model}\n\\label{sec:model}\n\nOur goal is to learn a model which maps natural language input $q =\n\\left( x_1 , \\cdots , x_{|q|} \\right)$ to a logical form\nrepresentation of its meaning~$a = \\left( y_1, \\cdots, y_{|a|}\n\\right)$. We regard both input~$q$ and output~$a$ as sequences and\nwish to estimate the conditional probability~$p\\left( a | q \\right)$\nwhich is decomposed as follows:\n\n", "index": 1, "text": "\\begin{equation}\n\t\\label{eq:prob:whole:seq}\n\tp\\left( a | q \\right) = \\prod _{ t = 1 }^{ |a| }{ p\\left( y_t | y_{<t} , q \\right) }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"p\\left(a|q\\right)=\\prod_{t=1}^{|a|}{p\\left(y_{t}|y_{&lt;t},q\\right)}\" display=\"block\"><mrow><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mo stretchy=\"false\">|</mo><mi>q</mi><mo>)</mo></mrow><mo>=</mo><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>a</mi><mo stretchy=\"false\">|</mo></mrow></munderover><mi>p</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>,</mo><mi>q</mi><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere ${\\mathbf{h} }_{ t }^{ 0 }$ is the word vector of the current\ntoken for the encoder or the previously predicted word for the decoder.\nSpecifically, we follow the definition of the LSTM unit used\nin~\\newcite{zaremba2014recurrent} to recursively compute the hidden\nvectors:\n\n", "itemtype": "equation", "pos": 8608, "prevtext": "\nwhere $y_{<t} = \\left(y_1, \\cdots, y_{t-1} \\right)$. In the following\nwe describe the details of how~$p\\left( a | q \\right)$ is computed.\n\n\n\\subsection{Sequence-to-Sequence Model}\n\\label{sec:sequ-sequ-model}\n\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{seq2seq.pdf}\n\t\\caption{A sequence-to-sequence model with two-layer recurrent\n\t\tneural networks.  }\n\t\\label{fig:seq2seq}\n\\end{figure}\n\nOur model consists of an \\textbf{encoder} which encodes natural\nlanguage input $q$ into a vector representation and a \\textbf{decoder}\nwhich learns to generate $y_1, \\cdots, y_{|a|}$ conditioned on the\nencoding vector.\n\nAs shown in Figure~\\ref{fig:seq2seq}, the encoder and decoder are two\ndifferent $L$-layer recurrent neural networks with Long Short-Term\nMemory (LSTM) units which recursively process tokens one by one.  The\nfirst $|q|$ time steps belong to the encoder, while the following~$|a|$ time\nsteps belong to the decoder.  Let ${\\mathbf{h} }_{ t }^{ l } \\in\n\\mathbb{R}^{n}$ denote an $n$-dimensional hidden vector in time\nstep~$t$ and layer~$l$. ${\\mathbf{h} }_{ t }^{ l }$ is computed by:\n\n", "index": 3, "text": "\\begin{equation}\n\t{\\mathbf{h} }_{ t }^{ l } = f \\left( {\\mathbf{h} }_{ t-1 }^{ l }, {\\mathbf{h} }_{ t }^{ l-1 } \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{h}}_{t}^{l}=f\\left({\\mathbf{h}}_{t-1}^{l},{\\mathbf{h}}_{t}^{l-1}\\right)\" display=\"block\"><mrow><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere~$\\tanh$ and ${\\operatorname{\\textrm{sigm}}}$~are applied element-wise, $\\odot$~is\nelement-wise multiplication, and $W^{l} \\in \\mathbb{R}^{4n \\times 2n}$\nis a parameter matrix for the~$l$-th layer.  Notice that the encoder and\ndecoder have different parameters.\n\n\n\nOnce the tokens of the input sequence $x_1, \\cdots, x_{|q|}$ are\nencoded into vectors, they are used to initialize the hidden states of\nthe first time step in the decoder.  Next, the hidden vector of the\ntopmost LSTM~${\\mathbf{h} }_{ |q| + t }^{ L }$ is used to predict the\n$t$-th~output token as:\n\n", "itemtype": "equation", "pos": 9021, "prevtext": "\nwhere ${\\mathbf{h} }_{ t }^{ 0 }$ is the word vector of the current\ntoken for the encoder or the previously predicted word for the decoder.\nSpecifically, we follow the definition of the LSTM unit used\nin~\\newcite{zaremba2014recurrent} to recursively compute the hidden\nvectors:\n\n", "index": 5, "text": "\\begin{equation}\n\t\\label{eq:lstm}\n\t\\begin{aligned}\n\t\t\\begin{pmatrix} \\mathbf{i} \\\\ \\mathbf{f} \\\\ \\mathbf{o} \\\\ \\mathbf{g} \\end{pmatrix} &= \\begin{pmatrix} {\\operatorname{\\textrm{sigm}}} \\\\ {\\operatorname{\\textrm{sigm}}} \\\\ {\\operatorname{\\textrm{sigm}}} \\\\ \\tanh \\end{pmatrix} W^{l} \\begin{pmatrix}  {\\mathbf{h} }_{ t }^{ l-1 } \\\\ { \\mathbf{h} }_{ t-1 }^{ l } \\end{pmatrix} \\\\\n\t\t{ \\mathbf{m} }_{ t }^{ l } &= \\mathbf{f}\\odot { \\mathbf{m} }_{ t-1 }^{ l }+\\mathbf{i}\\odot \\mathbf{g} \\\\\n\t\t{ \\mathbf{h} }_{ t }^{ l } &= \\mathbf{o}\\odot \\tanh\\left( { \\mathbf{m} }_{ t }^{ l } \\right)\n\t\\end{aligned}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{pmatrix}\\mathbf{i}\\\\&#10;\\mathbf{f}\\\\&#10;\\mathbf{o}\\\\&#10;\\mathbf{g}\\end{pmatrix}\" display=\"inline\"><mrow><mo>(</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mi>\ud835\udc22</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc1f</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc28</mi></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>\ud835\udc20</mi></mtd></mtr></mtable><mo>)</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\begin{pmatrix}{\\operatorname{\\textrm{sigm}}}\\\\&#10;{\\operatorname{\\textrm{sigm}}}\\\\&#10;{\\operatorname{\\textrm{sigm}}}\\\\&#10;\\tanh\\end{pmatrix}W^{l}\\begin{pmatrix}{\\mathbf{h}}_{t}^{l-1}\\\\&#10;{\\mathbf{h}}_{t-1}^{l}\\end{pmatrix}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mo>(</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><mtext>sigm</mtext></mtd></mtr><mtr><mtd columnalign=\"center\"><mtext>sigm</mtext></mtd></mtr><mtr><mtd columnalign=\"center\"><mtext>sigm</mtext></mtd></mtr><mtr><mtd columnalign=\"center\"><mi>tanh</mi></mtd></mtr></mtable><mo>)</mo></mrow><mo>\u2062</mo><msup><mi>W</mi><mi>l</mi></msup><mo>\u2062</mo><mrow><mo>(</mo><mtable rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udc21</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>l</mi></msubsup></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{m}}_{t}^{l}\" display=\"inline\"><msubsup><mi>\ud835\udc26</mi><mi>t</mi><mi>l</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xa.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathbf{f}\\odot{\\mathbf{m}}_{t-1}^{l}+\\mathbf{i}\\odot\\mathbf{g}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mrow><mi>\ud835\udc1f</mi><mo>\u2299</mo><msubsup><mi>\ud835\udc26</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow><mo>+</mo><mrow><mi>\ud835\udc22</mi><mo>\u2299</mo><mi>\ud835\udc20</mi></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{h}}_{t}^{l}\" display=\"inline\"><msubsup><mi>\ud835\udc21</mi><mi>t</mi><mi>l</mi></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3Xb.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\mathbf{o}\\odot\\tanh\\left({\\mathbf{m}}_{t}^{l}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>\ud835\udc28</mi><mo>\u2299</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><msubsup><mi>\ud835\udc26</mi><mi>t</mi><mi>l</mi></msubsup><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere $W_p \\in \\mathbb{R}^{|V_a| \\times n}$ is a parameter matrix, and\n$\\mathbf{e}\\left( y_t \\right) \\in \\{0,1\\}^{|V_a|}$ is a one-hot vector used to obtain\n$y_t$'s~probability from the predicted distribution.\n\nWe augment every sequence with a\n``start-of-sequence''~\\textit{\\textless s\\textgreater} and\n``end-of-sequence'' \\textit{\\textless e\\textgreater}~token. The\ngeneration process terminates once~\\textit{\\textless e\\textgreater} is\npredicted.  Then, the conditional probability of generating the whole\nsequence $p\\left( a | q \\right)$ is computed using\nEquation~\\eqref{eq:prob:whole:seq}.\n\n\\subsection{Attention Mechanism}\n\\label{sec:attention-mechanism}\n\nAs shown in Equation~\\eqref{eq:seq2seq:decoder:predict}, the hidden\nvectors of the input sequence are not directly used in the decoding\nprocess. However, it makes intuitively sense to consider relevant\ninformation from the input to better predict the current\ntoken. Following this idea, various techniques have been proposed to\nintegrate encoder-side information (in the form of a context vector)\ninto each time step of the decoder\n\\cite{mt:jointly:align:translate,luong-pham-manning:2015:EMNLP,imagecaption:attend,hermann2015teaching}.\nWe use the global attention architecture\ndescribed in~\\newcite{luong-pham-manning:2015:EMNLP}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{attention.pdf}\n\t\\caption{Attention scores are computed using the current\n\t\thidden vector of the decoder and all hidden vectors of the\n\t\tencoder obtained in the form of a weighted sum. The new\n\t\thidden vector ${\\mathbf{h} }_{ |q| + 3 }^{ att }$ is used to\n\t\tpredict $y_3$ for the current time step.}\n\t\\label{fig:attention}\n\\end{figure}\n\nAs shown in Figure~\\ref{fig:attention}, in order to find relevant\nencoder-side context for the current hidden state~${\\mathbf{h} }_{ |q|\n\t+ t }^{ L }$, we compute its attention score with the\n\\mbox{$k$-th}~hidden state in the encoder as:\n\n", "itemtype": "equation", "pos": 10198, "prevtext": "\nwhere~$\\tanh$ and ${\\operatorname{\\textrm{sigm}}}$~are applied element-wise, $\\odot$~is\nelement-wise multiplication, and $W^{l} \\in \\mathbb{R}^{4n \\times 2n}$\nis a parameter matrix for the~$l$-th layer.  Notice that the encoder and\ndecoder have different parameters.\n\n\n\nOnce the tokens of the input sequence $x_1, \\cdots, x_{|q|}$ are\nencoded into vectors, they are used to initialize the hidden states of\nthe first time step in the decoder.  Next, the hidden vector of the\ntopmost LSTM~${\\mathbf{h} }_{ |q| + t }^{ L }$ is used to predict the\n$t$-th~output token as:\n\n", "index": 7, "text": "\\begin{equation}\n\t\\label{eq:seq2seq:decoder:predict}\n\tp\\left( y_t | y_{<t} , q \\right) = {{\\operatorname{\\textrm{softmax}}} \\left( W_p {\\mathbf{h} }_{ |q| + t }^{ L } \\right)}^{\\intercal} \\mathbf{e}\\left( y_t \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"p\\left(y_{t}|y_{&lt;t},q\\right)={{\\operatorname{\\textrm{softmax}}}\\left(W_{p}{%&#10;\\mathbf{h}}_{|q|+t}^{L}\\right)}^{\\intercal}\\mathbf{e}\\left(y_{t}\\right)\" display=\"block\"><mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>,</mo><mi>q</mi><mo>)</mo></mrow><mo>=</mo><mtext>softmax</mtext><msup><mrow><mo>(</mo><msub><mi>W</mi><mi>p</mi></msub><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mi>L</mi></msubsup><mo>)</mo></mrow><mo>\u22ba</mo></msup><mi>\ud835\udc1e</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere only the top-layer hidden vectors are utilized.  Then, the\ncontext vector is the weighted sum of the hidden vectors in the\nencoder:\n\n", "itemtype": "equation", "pos": 12369, "prevtext": "\nwhere $W_p \\in \\mathbb{R}^{|V_a| \\times n}$ is a parameter matrix, and\n$\\mathbf{e}\\left( y_t \\right) \\in \\{0,1\\}^{|V_a|}$ is a one-hot vector used to obtain\n$y_t$'s~probability from the predicted distribution.\n\nWe augment every sequence with a\n``start-of-sequence''~\\textit{\\textless s\\textgreater} and\n``end-of-sequence'' \\textit{\\textless e\\textgreater}~token. The\ngeneration process terminates once~\\textit{\\textless e\\textgreater} is\npredicted.  Then, the conditional probability of generating the whole\nsequence $p\\left( a | q \\right)$ is computed using\nEquation~\\eqref{eq:prob:whole:seq}.\n\n\\subsection{Attention Mechanism}\n\\label{sec:attention-mechanism}\n\nAs shown in Equation~\\eqref{eq:seq2seq:decoder:predict}, the hidden\nvectors of the input sequence are not directly used in the decoding\nprocess. However, it makes intuitively sense to consider relevant\ninformation from the input to better predict the current\ntoken. Following this idea, various techniques have been proposed to\nintegrate encoder-side information (in the form of a context vector)\ninto each time step of the decoder\n\\cite{mt:jointly:align:translate,luong-pham-manning:2015:EMNLP,imagecaption:attend,hermann2015teaching}.\nWe use the global attention architecture\ndescribed in~\\newcite{luong-pham-manning:2015:EMNLP}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.45\\textwidth]{attention.pdf}\n\t\\caption{Attention scores are computed using the current\n\t\thidden vector of the decoder and all hidden vectors of the\n\t\tencoder obtained in the form of a weighted sum. The new\n\t\thidden vector ${\\mathbf{h} }_{ |q| + 3 }^{ att }$ is used to\n\t\tpredict $y_3$ for the current time step.}\n\t\\label{fig:attention}\n\\end{figure}\n\nAs shown in Figure~\\ref{fig:attention}, in order to find relevant\nencoder-side context for the current hidden state~${\\mathbf{h} }_{ |q|\n\t+ t }^{ L }$, we compute its attention score with the\n\\mbox{$k$-th}~hidden state in the encoder as:\n\n", "index": 9, "text": "\\begin{equation}\n\t\\label{eq:attention:score}\n\t{ s }_{ k }^{ |q|+t } = \\frac { \\exp \\{ {({\\mathbf{h} }_{ k }^{ L })}^{\\intercal} {\\mathbf{h} }_{ |q| + t }^{ L } \\} }{ \\sum _{ j=1 }^{ |q| }{ \\exp \\{ {({\\mathbf{h} }_{ j }^{ L })}^{\\intercal} {\\mathbf{h} }_{ |q| + t }^{ L } \\} }  } \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{s}_{k}^{|q|+t}=\\frac{\\exp\\{{({\\mathbf{h}}_{k}^{L})}^{\\intercal}{\\mathbf{h}}_{%&#10;|q|+t}^{L}\\}}{\\sum_{j=1}^{|q|}{\\exp\\{{({\\mathbf{h}}_{j}^{L})}^{\\intercal}{%&#10;\\mathbf{h}}_{|q|+t}^{L}\\}}}\" display=\"block\"><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mi>k</mi><mi>L</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u22ba</mo></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mi>L</mi></msubsup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc21</mi><mi>j</mi><mi>L</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u22ba</mo></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mi>L</mi></msubsup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\n\nIn lieu of Equation~\\eqref{eq:seq2seq:decoder:predict}, we further use\nthis context vector which acts as a summary of the encoder to compute\nthe probability of generating $y_t$ as:\n\n", "itemtype": "equation", "pos": 12802, "prevtext": "\nwhere only the top-layer hidden vectors are utilized.  Then, the\ncontext vector is the weighted sum of the hidden vectors in the\nencoder:\n\n", "index": 11, "text": "\\begin{equation}\n\t\\mathbf{c}^{|q|+t} = \\sum_{ k=1 }^{ |q| }{ { s }_{ k }^{ |q|+t } {\\mathbf{h} }_{ k }^{ L } }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{c}^{|q|+t}=\\sum_{k=1}^{|q|}{{s}_{k}^{|q|+t}{\\mathbf{h}}_{k}^{L}}\" display=\"block\"><mrow><msup><mi>\ud835\udc1c</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow></munderover><mrow><msubsup><mi>s</mi><mi>k</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc21</mi><mi>k</mi><mi>L</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 13110, "prevtext": "\n\nIn lieu of Equation~\\eqref{eq:seq2seq:decoder:predict}, we further use\nthis context vector which acts as a summary of the encoder to compute\nthe probability of generating $y_t$ as:\n\n", "index": 13, "text": "\\begin{equation}\n\t\\label{eq:attention:new:hidden}\n\t{\\mathbf{h} }_{ |q| + t }^{ att } = \\tanh \\left( W_1 {\\mathbf{h} }_{ |q| + t }^{ L } + W_2 \\mathbf{c}^{|q|+t} \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{h}}_{|q|+t}^{att}=\\tanh\\left(W_{1}{\\mathbf{h}}_{|q|+t}^{L}+W_{2}%&#10;\\mathbf{c}^{|q|+t}\\right)\" display=\"block\"><mrow><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>t</mi></mrow></msubsup><mo>=</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mi>L</mi></msubsup></mrow><mo>+</mo><mrow><msub><mi>W</mi><mn>2</mn></msub><mo>\u2062</mo><msup><mi>\ud835\udc1c</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere $W_p \\in \\mathbb{R}^{|V_a| \\times n}$ and $W_1 , W_2 \\in\n\\mathbb{R}^{n \\times n}$ are three parameter matrices, and\n$\\mathbf{e}\\left( y_t \\right)$ is a one-hot vector used to obtain\n$y_t$'s~probability.\n\n\n\n\\subsection{Model Training}\n\\label{sec:model-training}\n\nOur goal is to maximize the likelihood of generated logical forms\ngiven natural language utterances as input. So the objective function\nis defined as:\n\n", "itemtype": "equation", "pos": 13294, "prevtext": "\n\n", "index": 15, "text": "\\begin{equation}\n\t\\label{eq:attention:decoder:predict}\n\tp\\left( y_t | y_{<t} , q \\right) = {{\\operatorname{\\textrm{softmax}}} \\left( W_p {\\mathbf{h} }_{ |q| + t }^{ att } \\right)}^{\\intercal} \\mathbf{e}\\left( y_t \\right)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"p\\left(y_{t}|y_{&lt;t},q\\right)={{\\operatorname{\\textrm{softmax}}}\\left(W_{p}{%&#10;\\mathbf{h}}_{|q|+t}^{att}\\right)}^{\\intercal}\\mathbf{e}\\left(y_{t}\\right)\" display=\"block\"><mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>,</mo><mi>q</mi><mo>)</mo></mrow><mo>=</mo><mtext>softmax</mtext><msup><mrow><mo>(</mo><msub><mi>W</mi><mi>p</mi></msub><msubsup><mi>\ud835\udc21</mi><mrow><mrow><mo stretchy=\"false\">|</mo><mi>q</mi><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>t</mi></mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>t</mi></mrow></msubsup><mo>)</mo></mrow><mo>\u22ba</mo></msup><mi>\ud835\udc1e</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere $\\mathcal{D}$ is the set of all natural language-logical form\ntraining pairs, and~$p \\left( a | q \\right)$ is computed as shown in\nEquation~\\eqref{eq:prob:whole:seq}.  The RMSProp\nalgorithm~\\cite{rmsprop} is employed to solve this non-convex\noptimization problem.  Moreover, dropout is used for regularizing the\nmodel as suggested in~\\newcite{zaremba2014recurrent}. Specifically,\ndropout operators are used between different LSTM layers and for\nthe hidden layers before the softmax classifiers. This technique can\nsubstantially reduce overfitting, especially on datasets of small size.\n\n\\subsection{Inference}\n\\label{sec:inference}\n\nAt test time, the model predicts the logical form for an input\nutterance~$q$ maximizing the conditional probability:\n\n", "itemtype": "equation", "pos": 13949, "prevtext": "\nwhere $W_p \\in \\mathbb{R}^{|V_a| \\times n}$ and $W_1 , W_2 \\in\n\\mathbb{R}^{n \\times n}$ are three parameter matrices, and\n$\\mathbf{e}\\left( y_t \\right)$ is a one-hot vector used to obtain\n$y_t$'s~probability.\n\n\n\n\\subsection{Model Training}\n\\label{sec:model-training}\n\nOur goal is to maximize the likelihood of generated logical forms\ngiven natural language utterances as input. So the objective function\nis defined as:\n\n", "index": 17, "text": "\\begin{equation}\n\t\\label{eq:objective}\n\t\\min -\\sum_{(q , a) \\in \\mathcal{D} }{ \\log{p \\left( a | q \\right)}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\min-\\sum_{(q,a)\\in\\mathcal{D}}{\\log{p\\left(a|q\\right)}}\" display=\"block\"><mrow><mi>min</mi><mo>-</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow></munder><mi>log</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mo stretchy=\"false\">|</mo><mi>q</mi><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01280.tex", "nexttext": "\nwhere~$a'$ represents a candidate logical form sequence.  However, it\nis not practical to iterate over all possible sequences to obtain the\noptimal prediction.  According to Equation~\\eqref{eq:prob:whole:seq},\nwe decompose the probability~$p\\left( a | q \\right)$ so that we can\nuse beam search or greedy search to predict tokens one by one.\nWe add special tokens~\\textit{\\textless\n\ts\\textgreater}~and~\\textit{\\textless e\\textgreater} to the beginning\nand end of every sequence. The generation process terminates once\ntoken~\\textit{\\textless e\\textgreater} is predicted.\n\n\\subsection{Handling Rare Words}\n\\label{sec:handling-rare-words}\n\nSome entities and numbers are rare or even do not appear in the training set at all, especially on small-scale datasets.\nConventional sequence-to-sequence models replace rare words with the unknown word symbol~\\cite{mt:rare:word:google,mt:rare:word:cho}. This strategy, however, would adversely affect our model's accuracy, because a prediction result is correct only if all tokens are parsed correctly.\n\n\n\\begin{table*}[t]\n\t\\centering\n\t\\small\n\t\\begin{tabular}{l c l}\n\t\t\\hline\n\t\tDataset & Length & Example \\\\ \\hline\\hline\n\t\t\\textsc{Jobs} & \\tabincell{r}{9.80 \\\\ 22.90} & \\tabincell{l}{\\textit{what microsoft jobs do not require a bscs?} \\\\ {answer(company(J,'microsoft'),job(J),not((req\\_deg(J,'bscs'))))}} \\\\ \\hline\n\t\t\\textsc{Geo} & \\tabincell{r}{7.60 \\\\ 19.10} & \\tabincell{l}{\\textit{what is the population of the state with the largest area?} \\\\ {(population:i (argmax \\$0 (state:t \\$0) (area:i \\$0)))}} \\\\ \\hline\n\t\t\\textsc{Atis} & \\tabincell{r}{11.10 \\\\ 28.10} & \\tabincell{l}{\\textit{dallas to san francisco leaving after 4 in the afternoon please} \\\\{\\small {(lambda \\$0 e (and (\\textgreater (departure\\_time \\$0) 1600:ti) (from \\$0 dallas:ci) (to \\$0 san\\_francisco:ci)))}}} \\\\ \\hline\n\t\t\\textsc{Ifttt} & \\tabincell{r}{6.95 \\\\ 21.80} & \\tabincell{l}{\\textit{Turn on heater when temperature drops below 58 degree} \\\\ {\\footnotesize {TRIGGER: Weather - Current\\_temperature\\_drops\\_below - ((Temperature (58)) (Degrees\\_in (f))) } } \\\\ {\\footnotesize {ACTION: WeMo\\_Insight\\_Switch - Turn\\_on - ((Which\\_switch? (\"\")))} }} \\\\ \\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{Examples of natural language descriptions and their meaning representations from four datasets. The average length of input and output sequences is shown in the second column.}\n\t\\label{table:dataset}\n\\end{table*}\n\n\nOur solution is to link entities and numbers in\nutterances to their corresponding logical constants, and replace them\nwith their type names and unique IDs.  For instance, we pre-process\nthe training example ``\\textit{jobs with a salary of 40000}'' and its\nlogical form ``job(ANS), salary\\_greater\\_than(ANS, 40000, year)'' as\n``\\textit{jobs with a salary of {\\underline{\\textit{\\texttt{{num}}}$_{{0}}$}}}'' and ``job(ANS),\nsalary\\_greater\\_than(ANS, {\\underline{\\textit{\\texttt{{num}}}$_{{0}}$}}, year)''.  We use the\npre-processed examples as training data. At inference time, we\nalso mask entities and numbers with their types and IDs. Once we\nobtain the decoding result, a post-processing step recovers all the\nmarkers {\\underline{\\textit{\\texttt{{type}}}$_{{i}}$}} to their corresponding logical constants.\n\n\n\n\n\n\n\n\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nWe compare our method against multiple previous systems on four\ndatasets.  We describe these datasets below, and present our\nexperimental settings and results. Finally, we conduct model\nanalysis in order to understand what the model learns.\n\n\\subsection{Datasets}\n\\label{sec:datasets}\n\nOur model was trained on the following datasets, covering different\ndomains and using different meaning representations.  Examples for\neach domain are shown in Table~\\ref{table:dataset}.\n\n\n\\paragraph{\\textsc{Jobs}} This benchmark dataset contains~$640$\nqueries to a database of job listings.  Specifically, questions are\npaired with Prolog-style queries. We used the same\ntraining-test split as~\\newcite{zc05} which contains $500$~training\nand $140$~test instances.  Values for the variables company, degree,\nlanguage, platform, location, job area, and number are considered as rare\nwords.\n\n\n\\noindent\n\\paragraph{\\textsc{Geo}} This is a standard semantic parsing benchmark\nwhich contains~$880$ queries to a database of\nU.S. geography. \\textsc{Geo} has 880~sentence logical-form pairs split\ninto a training set of $680$ training examples and $200$ test\nexamples~\\cite{zc05}. We used the same meaning representation based on\nlambda-calculus as~\\newcite{fubl}.  Values for the varibles city,\nstate, country, river, and number are handled as rare words.\n\n\\noindent\n\\paragraph{\\textsc{Atis}} This dataset has $5,410$ queries to a flight\nbooking system. The standard split~\\cite{fubl} has $4,480$ training\ninstances, $480$ development instances, and $450$ test\ninstances. Sentences are paired with lambda-calculus expressions.\nValues for the variables date, time, city, aircraft code, airport,\nairline, and number are treated as rare words.\n\n\\noindent\n\\paragraph{\\textsc{Ifttt}} \\newcite{ifttt} created this dataset by\nextracting a large number of if-this-then-that recipes from the IFTTT\nwebsite\\footnote{\\url{http://www.ifttt.com}}. Recipes are simple\nprograms with exactly one trigger and one action which users specify\non the site. Whenever the conditions of the trigger are satisfied, the\naction is performed. Actions typically revolve around home security\n(e.g.,~``\\textit{turn on my lights when I arrive home}''), automation\n(e.g.,~``\\textit{text me if the door opens}''), well-being\n(e.g.,~``\\textit{remind me to drink water if I've been at a bar for\n\tmore than two hours}''), and so on.  Triggers and actions are\nselected from different channels ($160$ in total) representing various\ntypes of services, devices (e.g., Android), and knowledge sources\n(such as ESPN or Gmail).  In the dataset, there are~$552$~trigger\nfunctions from~$128$ channels, and~$229$ action functions from~$99$\nchannels.  We used Quirk et al.'s \\shortcite{ifttt} original split\nwhich contains~$77,495$ training, $5,171$~development, and\n$4,294$~test examples.  The \\textsc{Ifttt} programs are represented as\nabstract syntax trees and are paired with natural language\ndescriptions provided by users (see Table~\\ref{table:dataset}).\nHere, numbers and URLs are handled as rare words.\n\n\n\n\n\n\\subsection{Settings}\n\\label{sec:settings}\n\nWe converted natural language sentences to lowercase and corrected\nmisspelled words using a dictionary based on the Wikipedia list of\ncommon misspellings. To reduce sparsity, words were stemmed using the\nSnowball Stemmer within NLTK~\\cite{nltk}. For \\textsc{Ifttt}, we\nfiltered tokens, channels and functions which appeared less than five\ntimes in the training set. For the other datasets, we filtered input\nwords which did not occur at least two times in the training set, but\nkept all tokens in the logical forms.\nPlain string matching was employed to identify entities for handling rare words as described in Section~\\ref{sec:handling-rare-words}. More sophisticated approaches could be used, however we leave this future work.\n\nModel hyper-parameters were cross-validated on the training set for\n\\textsc{Jobs} and \\textsc{Geo}. We used the standard development sets\nfor \\textsc{Atis} and \\textsc{Ifttt}. Our model was trained with\nmini-batch stochastic gradient descent using the RMSProp algorithm\n(with batch size set to~$20$). The smoothing constant of RMSProp was\nset to~$0.95$. The base learning rate was~$0.002$ for \\textsc{Ifttt}\nand $0.01$~for the other domains. After $5$~epochs, the learning rate\nwas decreased by a factor of~$0.95$ at every epoch as suggested\nin~\\newcite{karpathy2015visualizing}.  Gradients were clipped at $5$\nto alleviate the exploding gradient\nproblem~\\cite{pascanu2013difficulty}.  Parameters were initialized by\nrandomly sampling values from a uniform distribution\n$\\mathcal{U}\\left(-0.08,0.08\\right)$.  A two-layer LSTM was used for\n\\textsc{Ifttt}, while a one-layer LSTM was employed for the other\ndomains.  We also used dropout regularization; the dropout rate was\nselected from $\\{0.2,0.3,0.4,0.5\\}$. Dimensions of the hidden vector\nand the word embedding were the same, and selected from\n$\\{150,200,250\\}$.  Early stopping was employed to determine the number of epochs.  The input sentences were reversed before feeding\ninto the sequence encoder as suggested by~\\newcite{mt:seq2seq}.\nGreedy search was used to generate logical forms during inference.\nNotice that two decoders with shared word embeddings were used to predict triggers and actions for \\textsc{Ifttt}.\n\nAll experiments were conducted on a single GTX 980 GPU device.\n\n\n\n\\subsection{Results}\n\\label{sec:results}\n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{tabular}{l c}\n\t\t\\hline\n\t\tMethod                   & Accuracy \\\\ \\hline\\hline\n\t\tCOCKTAIL~\\cite{cocktail} & 79.4     \\\\\n\t\tPRECISE~\\cite{precise}   & 88.0     \\\\\n\t\tZC05~\\cite{zc05}         & 79.3     \\\\\n\t\tDCS+L~\\cite{dcs}         & 90.7     \\\\\n\t\tTISP~\\cite{tisp}         & 85.0     \\\\ \\hline\n\t\tseq2seq $-$ rare          & 70.7     \\\\\n\t\tseq2seq $-$ attention     & 77.9     \\\\\n\t\tseq2seq (ours)           & 87.1     \\\\ \\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{Evaluation results on \\textsc{Jobs}.}\n\t\\label{table:results:jobs}\n\\end{table}\n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{tabular}{l c}\n\t\t\\hline\n\t\tMethod                           & Accuracy \\\\ \\hline\\hline\n\t\tSCISSOR~\\cite{scissor}           & 72.3     \\\\\n\t\tSILT~\\cite{silt}                 & 54.1     \\\\\n\t\tKRISP~\\cite{krisp}               & 71.7     \\\\\n\t\tWASP~\\cite{wasp}                 & 74.8     \\\\\n\t\t$\\lambda$-WASP~\\cite{lambdawasp} & 86.6     \\\\\n\t\tLNLZ08~\\cite{lnlz08}             & 81.8     \\\\ \\hline\n\t\tZC05~\\cite{zc05}                 & 79.3     \\\\\n\t\tZC07~\\cite{zc07}                 & 86.1     \\\\\n\t\tUBL~\\cite{ubl}                   & 87.9     \\\\\n\t\tFUBL~\\cite{fubl}                 & 88.6     \\\\\n\t\tKCAZ13~\\cite{onthefly13}         & 89.0     \\\\\n\t\tDCS+L~\\cite{dcs}                 & 87.9     \\\\\n\t\tTISP~\\cite{tisp}                 & 88.9     \\\\ \\hline\n\t\tseq2seq $-$ rare                  & 68.6     \\\\\n\t\tseq2seq $-$ attention             & 72.9     \\\\\n\t\tseq2seq (ours)                   & 84.6     \\\\ \\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{Evaluation results on \\textsc{Geo}. 10-fold\n\t\tcross-validation is used for the systems shown in the top\n\t\thalf of the table. The standard split of ZC05 is used\n\t\tfor all other systems.}\n\t\\label{table:results:GeoQueries}\n\\end{table}\n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{tabular}{l c}\n\t\t\\hline\n\t\tMethod                & Accuracy \\\\ \\hline\\hline\n\t\tZC07~\\cite{zc07}      & 84.6     \\\\\n\t\tUBL~\\cite{ubl}        & 71.4     \\\\\n\t\tFUBL~\\cite{fubl}      & 82.8     \\\\\n\t\tGUSP-FULL~\\cite{gusp} & 74.8     \\\\\n\t\tGUSP++~\\cite{gusp}    & 83.5     \\\\\n\t\tTISP~\\cite{tisp}      & 84.2     \\\\ \\hline\n\t\tseq2seq $-$ rare       & 74.1     \\\\\n\t\tseq2seq $-$ attention  & 78.5     \\\\\n\t\tseq2seq (ours)        & 84.2     \\\\ \\hline\n\t\\end{tabular}\n\t\\normalsize\n\t\\caption{Evaluation results on \\textsc{Atis}.}\n\t\\label{table:results:ATIS}\n\\end{table}\n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\subfloat[Omit non-English. \\label{table:results:IFTTT:1}] {\n\t\t\\begin{tabular}{l c c c}\n\t\t\t\\hline\n\t\t\tMethod               & Channel & +Func & F1   \\\\ \\hline\\hline\n\t\t\tretrieval            & 28.9    & 20.2  & 41.7 \\\\\n\t\t\tphrasal              & 19.3    & 11.3  & 35.3 \\\\\n\t\t\tsync                 & 18.1    & 10.6  & 35.1 \\\\\n\t\t\tclassifier           & 48.8    & 35.2  & 48.4 \\\\\n\t\t\tposclass             & 50.0    & 36.9  & 49.3 \\\\ \\hline\n\t\t\tseq2seq $-$ rare      & 53.9    & 38.6  & 49.7 \\\\\n\t\t\tseq2seq $-$ attention & 54.0    & 37.9  & 49.8 \\\\\n\t\t\tseq2seq (ours)       & 54.3    & 39.2  & 50.1 \\\\ \\hline\n\t\t\\end{tabular}\n\t}\\\\%\n\t\\subfloat[Omit non-English \\& unintelligible.\n\t\\label{table:results:IFTTT:2}]{\n\t\t\\begin{tabular}{l c c c}\n\t\t\t\\hline\n\t\t\tMethod               & Channel & +Func & F1   \\\\ \\hline\\hline\n\t\t\tretrieval            & 36.8    & 25.4  & 49.0 \\\\\n\t\t\tphrasal              & 27.8    & 16.4  & 39.9 \\\\\n\t\t\tsync                 & 26.7    & 15.5  & 37.6 \\\\\n\t\t\tclassifier           & 64.8    & 47.2  & 56.5 \\\\\n\t\t\tposclass             & 67.2    & 50.4  & 57.7 \\\\ \\hline\n\t\t\tseq2seq $-$ rare      & 68.8    & 50.4  & 59.7 \\\\\n\t\t\tseq2seq $-$ attention & 68.7    & 48.9  & 59.5 \\\\\n\t\t\tseq2seq (ours)       & 68.8    & 50.5  & 60.3 \\\\ \\hline\n\t\t\\end{tabular}\n\t}\\\\%\n\t\\subfloat[$\\ge 3$ turkers agree with gold.\n\t\\label{table:results:IFTTT:3}]{\n\t\t\\begin{tabular}{l c c c}\n\t\t\t\\hline\n\t\t\tMethod               & Channel & +Func & F1   \\\\ \\hline\\hline\n\t\t\tretrieval            & 43.3    & 32.3  & 56.2 \\\\\n\t\t\tphrasal              & 37.2    & 23.5  & 45.5 \\\\\n\t\t\tsync                 & 36.5    & 24.1  & 42.8 \\\\\n\t\t\tclassifier           & 79.3    & 66.2  & 65.0 \\\\\n\t\t\tposclass             & 81.4    & 71.0  & 66.5 \\\\ \\hline\n\t\t\tseq2seq $-$ rare      & 86.8    & 74.9  & 70.8 \\\\\n\t\t\tseq2seq $-$ attention & 88.3    & 73.8  & 72.9 \\\\\n\t\t\tseq2seq (ours)       & 87.8    & 75.2  & 73.7 \\\\ \\hline\n\t\t\\end{tabular}\n\t}\n\t\\normalsize\n\t\\caption{Evaluation results on \\textsc{Ifttt}.}\n\t\\label{table:results:IFTTT}\n\\end{table}\n\nWe first discuss the performance of our model on \\textsc{Jobs},\n\\textsc{Geo}, and \\textsc{Atis}, and then examine our results on\n\\textsc{Ifttt}. Tables~\\ref{table:results:jobs}--\\ref{table:results:ATIS}\ncompare our results against a variety of systems previously described\nin the literature. In addition to our model (seq2seq), we present two\nablation variants, namely without using an attention\nmechanism ($-$attention) and without handing rare words ($-$rare).  We\nreport accuracy which is defined as the proportion of input sentences\nthat are correctly parsed to their gold standard logical forms.\nNotice that DCS+L, KCAZ13 and GUSP output answers directly, so\naccuracy in this setting is defined as the percentage of correct\nanswers.\n\nWe find that adding attention\nsubstantially improves performance on all three datasets. This\nunderlines the importance of utilizing soft alignments between inputs\nand outputs. We further analyze what the attention layer learns in\nSection~\\ref{sec:model-analysis}.\nMoreover, the results show that handling rare words is critical for small-scale datasets. For example, about $92\\%$ of city names appear less than $4$ times in the  \\textsc{Geo} training set, so it is difficult to learn reliable parameters for these words.\nIn relation to previous models, the seq2seq framework achieves comparable or better performance without\nrelying on manually defined templates or language/domain specific\nfeatures.  Importantly, we use the same model across datasets and\nmeaning representations (Prolog-style logical forms in \\textsc{Jobs}\nand lambda calculus in the other two datasets), without\nmodification. Despite this relatively simple approach, we observe that\nseq2seq ranks third on \\textsc{Jobs}, and second on \\textsc{Atis}.\n\n\n\n\\begin{figure*}[t]\n\t\\centering\n\t\n\t\\subfloat[c][which jobs pay num0 that do not \\\\ require a degid0\\label{fig:exp:attention:a}]{\\includegraphics[scale=0.48]{attention_score_90.pdf}}\n\t\n\t\\subfloat[c][what's first class fare \\\\ round trip from ci0 to ci1\\label{fig:exp:attention:b}]{\\includegraphics[scale=0.48]{attention_score_236.pdf}}\n\t\n\t\n\t\n\t\n\t\\subfloat[c][what afternoon flights \\\\ are available from ci0 to ci1\\label{fig:exp:attention:c}]{\\includegraphics[scale=.48]{attention_score_166.pdf}}\n\t\n\t\n\t\n\t\n\t\\subfloat[][what is the highest elevation in the co0\\label{fig:exp:attention:d}]{\\includegraphics[scale=.48]{attention_score_162.pdf}}\n\t\n\t\n\t\n\t\\caption{Examples of alignments (same color rectangles)\n\t\tproduced by the model's attention mechanism (darker color\n\t\trepresents higher attention score). Input sentences are\n\t\treversed and stemmed. }\n\t\\label{fig:exp:attention}\n\\end{figure*}\n\n\nFor \\textsc{Ifttt}, we follow the same evaluation protocol introduced\nin~\\newcite{ifttt}. The dataset is extremely noisy and measuring\naccuracy is problematic since predicted abstract syntax trees (ASTs) almost\nnever exactly match the gold standard. Quirk et al. view an AST as a\nset of productions and compute balanced F1 instead which we also\nadopt. The first column in Table~\\ref{table:results:IFTTT} shows the\npercentage of channels selected correctly for both triggers and\nactions. The second column measures accuracy for \\emph{both} channels\nand functions. The last column shows balanced F1-measure against the\ngold tree over all productions in the proposed derivation.  We compare\nour model against posclass, the method introduced in Quirk et al. and\nseveral of their baselines. posclass is reminiscent of\nKRISP~\\cite{krisp}, it learns distributions over productions given\ninput sentences represented as a bag of linguistic features. The\nretrieval baseline searches for the closest description in the\ntraining data based on character string-edit-distance and returns the\nrecipe for that training program.  The phrasal method uses\nphrase-based machine translation to generate the recipe, whereas sync\nextracts synchronous grammar rules from the data, essentially\nrecreating WASP~\\cite{wasp}. Finally, the classifier approach uses a\nbinary classifier to predict whether a production should be present in\nthe derivation tree corresponding to the description.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\hspace*{-.3cm}\\subfloat[][\\label{fig:exp:length:a}]{\\includegraphics[scale=.225]{length_input.pdf}}\n\t\\subfloat[][\\label{fig:exp:length:b}]{\\includegraphics[scale=.225]{length_logical_form.pdf}}\n\t\\caption{Accuracy as a function of (a)~input and (b)~output length\n\t\t(\\textsc{Atis}, development\n\t\tset).}\n\t\n\t\\label{fig:exp:length}\n\\end{figure}\n\n\\newcite{ifttt} report results on the full test data and smaller\nsubsets after noise filtering, e.g.,~when non-English and\nunintelligible descriptions are removed\n(Tables~\\ref{table:results:IFTTT:1}\nand~\\ref{table:results:IFTTT:2}). They also ran their system on a\nhigh-quality subset of description-program pairs which were found in\nthe gold standard and at least three humans managed to independently\nreproduce (Table~\\ref{table:results:IFTTT:3}). As can be seen, across\nall subsets seq2seq outperforms posclass and related baselines.\nCompared to the previous datasets, the attention mechanism and the method used to handle rare words yield less of an improvement here. This may be due to the size of  \\newcite{ifttt} and the way it was created -- user curated descriptions are often of low quality, and thus align very loosely to their corresponding ASTs.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Model Analysis}\n\\label{sec:model-analysis}\n\n\nFigure~\\ref{fig:exp:attention} illustrates examples of alignments\nproduced by our model. Matrices of attention scores are computed using\nEquation~\\eqref{eq:attention:score} and are represented in\ngrayscale. Aligned input words and logical form predicates are\nenclosed in (same color) rectangles whose overlapping areas contain\nthe attention scores.  Input sentences are reversed \\cite{mt:seq2seq}\nfor better performance. Also notice that attention scores are computed\nby LSTM hidden vectors which encode context information rather than\njust the words in their current positions.  The examples demonstrate\nthat the attention mechanism learns soft alignments between input\nsentences and output logical forms, performing surprisingly well, even\nin cases of reordering (Figure~\\ref{fig:exp:attention:b},~\\ref{fig:exp:attention:c} and~\\ref{fig:exp:attention:d}) and one-to-many alignments (Figure~\\ref{fig:exp:attention:c}).\n\n\n\n\n\n\nWe also explore whether the length of natural language descriptions and\nlogical forms influences performance. Figure~\\ref{fig:exp:length} shows\nhow accuracy varies with different length ranges on the \\textsc{Atis}\ndevelopment set. Overall, we observe that accuracy drops as sequence\nlength becomes larger, because longer sequences tend to have more\ncomplex meanings and be more compositional.  We also find that the\nattention mechanism consistently boosts performance across all ranges\nof length.  Moreover, adding attention allows the system to better cope\nwith longer sequences compared to the vanilla seq2seq method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nIn this paper we presented a sequence-to-sequence model for semantic\nparsing. Given natural language descriptions as input our model\npredicts meaning representations as output. Both input sentences and\noutput logical forms are regarded as sequences, and are encoded and\ndecoded by different recurrent neural networks.  Our experimental\nresults show that enhancing the model with an attention mechanism\nimproves performance across the board, especially for long sequences.\nWe also show that our method of handling rare words (via masking and\npost-processing) is effective and boosts performance. Extensive\ncomparisons with previous approaches demonstrate that our model\nperforms competitively, without recourse to domain- or\nrepresentation-specific features.\n\nDirections for future work are many and varied.  For example, it would\nbe interesting to learn a model from question-answer pairs without\naccess to target logical forms.  We would also like to use the model\nfor question answering over a knowledge base, e.g.,~by ranking the\npredicate paths between entities (figuring in the queries) and\ncandidate answers.\n\n\n\n\n\\bibliography{lang2logic}\n\\bibliographystyle{naaclhlt2016}\n\t\n\n", "itemtype": "equation", "pos": 14829, "prevtext": "\nwhere $\\mathcal{D}$ is the set of all natural language-logical form\ntraining pairs, and~$p \\left( a | q \\right)$ is computed as shown in\nEquation~\\eqref{eq:prob:whole:seq}.  The RMSProp\nalgorithm~\\cite{rmsprop} is employed to solve this non-convex\noptimization problem.  Moreover, dropout is used for regularizing the\nmodel as suggested in~\\newcite{zaremba2014recurrent}. Specifically,\ndropout operators are used between different LSTM layers and for\nthe hidden layers before the softmax classifiers. This technique can\nsubstantially reduce overfitting, especially on datasets of small size.\n\n\\subsection{Inference}\n\\label{sec:inference}\n\nAt test time, the model predicts the logical form for an input\nutterance~$q$ maximizing the conditional probability:\n\n", "index": 19, "text": "\\begin{equation}\n\t\\label{eq:inference}\n\t\\hat{a} = {\\operatorname{arg\\,max}}_{a'}{ p \\left( a' | q \\right) }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\hat{a}={\\operatorname{arg\\,max}}_{a^{\\prime}}{p\\left(a^{\\prime}|q\\right)}\" display=\"block\"><mrow><mover accent=\"true\"><mi>a</mi><mo stretchy=\"false\">^</mo></mover><mo>=</mo><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><msup><mi>a</mi><mo>\u2032</mo></msup></msub><mi>p</mi><mrow><mo>(</mo><msup><mi>a</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">|</mo><mi>q</mi><mo>)</mo></mrow></mrow></math>", "type": "latex"}]