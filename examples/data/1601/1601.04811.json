[{"file": "1601.04811.tex", "nexttext": "\nwhere $s_i$ is an decoder hidden state for time step $i$, computed by\\\n\n", "itemtype": "equation", "pos": 5482, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\n\\noindent Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate.\nHowever, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems.\nIn response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words.\nExperiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.\n\\end{abstract}\n\n\n\n\\section{Introduction}\n\nThe past several years have witnessed the rapid development of end-to-end neural machine translation (NMT) \\cite{Kalchbrenner:2013:EMNLP,Sutskever:2014:NIPS,Bahdanau:2015:ICLR}. Unlike conventional statistical machine translation (SMT) \\cite{Brown:1993:CL,Koehn:2003:NAACL,Chiang:2007:CL}, NMT proposes to use a single, large neural network instead of latent structures to model the translation process. This leads to the following benefits. First, the use of distributed representations of words proves to alleviate the curse of dimensionality problem \\cite{Bengio:2003:JMLR}. Second, there is no need to design features to  capture translation regularities explicitly, which is very tricky in SMT. Instead, NMT is capable of learning representations directly from the training data. Third, Long Short-Term Memory \\cite{Hochreite:1997} enables NMT to capture long-distance reordering, which is a notorious challenge in SMT.\n\nHowever, a serious problem with NMT is the lack of {\\em coverage}. In phrase-based SMT \\cite{Koehn:2003:NAACL}, a decoder maintains a coverage vector to indicate whether a source word is translated or not.  This is important for ensuring that each source word is translated exactly in decoding. The decoding process is completed when all source words are translated. In NMT, there is no such coverage vector and the decoding process ends only when the end-of-sentence tag is produced. We believe that lacking coverage might result in following problems in NMT:\n\\begin{enumerate}\n\\item Over-translation: some words are unnecessarily translated for multiple times;\n\\item Under-translation: some words are wrongly untranslated.\n\\end{enumerate}\nSpecifically, in the state-of-the-art attentional NMT model \\cite{Bahdanau:2015:ICLR}, generating a target word heavily depends on the relevant parts on the source side. As each source word is involved in calculating the attention for all target words, over-translation and under-translation inevitably happen because of the inappropriate imbalance of the ``fertility'' (i.e., the number of target words generated) of source words. Figure~\\ref{figure-nmt-examples} shows examples: the Chinese phrase ``zhudao zuoyong'' is over translated to ``{\\em leading role}'' twice (left panel), while ``qunian'' (means ``{\\em last year}'') is wrongly untranslated (right panel).\n\n\\begin{figure*}[t]\n\\begin{center}\n            \\includegraphics[width=0.55\\textwidth]{figures/over_translation/nmt.png}\\hspace{8pt}\n            \\includegraphics[width=0.35\\textwidth]{figures/under_translation/nmt.png}\\\\\n            \n\\end{center}\n\\caption{Examples of over-translation (left panel) and under-translation (right-panel) generated by attentional NMT.}\n\\label{figure-nmt-examples}\n\\end{figure*}\n\nIn this work, we propose a coverage-based approach to NMT to alleviate the over-translation and under-translation problems. Basically, we append annotation vectors to the intermediate representation of NMT models, which is updated after each attentive read during the decoding process to keep track of the attention history. Those annotation vectors, when entering into attention model, can help adjust the future attention and significantly improve the alignment between source and target. This design potentially contains many particular cases for coverage modeling with contrasting characteristics,  which all share a clear linguistic intuition and yet can be trained in a data driven fashion. Notably, in a simple and effective case, we achieve by far the best performance by re-defining the concept of fertility, as a successful example of re-introducing linguistic knowledge into neural network-based NLP models. Experiments on large-scale Chinese-English datasets show that our coverage-based NMT system outperforms conventional attentional NMT significantly on both translation and alignment tasks.\n\n\n\\section{Background}\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.25\\textwidth]{figures/nmt.pdf}\n\\caption{Architecture of NMT with alignment model. The alignment model calculates the alignment probability $\\alpha_{i,j}$ between $y_i$ and $x_j$, which is based on the decoder hidden state $s_{i-1}$ and the encoder annotation $h_j$.}\n\\label{figure-nmt}\n\\end{figure}\n\nOur work is built on attention-based NMT (RNNSearch)~\\cite{Bahdanau:2015:ICLR}, which simultaneously conducts dynamic alignment and generation of the target sentence, as illustrated in Figure~\\ref{figure-nmt}. It produces the translation by generating one target word at every time step conditioned on a context vector, the previous hidden state and the previously generated word.\nGiven an input sentence ${\\bf x}=\\{x_1, \\dots, x_{T_x}\\}$ and previous translated words $\\{y_1, \\dots, y_{i-1}\\}$, the probability of next word $y_i$ is:\n\n", "index": 1, "text": "\\begin{equation}\nP(y_i|y_1,\\dots, y_{i-1}, {\\bf x}) = g(y_{i-1}, s_i, c_i)\n\\label{eqn-nmt-prediction}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"P(y_{i}|y_{1},\\dots,y_{i-1},{\\bf x})=g(y_{i-1},s_{i},c_{i})\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nHere the activation function $f(\\cdot)$ is a gated recurrent unit (GRU)~\\cite{Cho:2014:EMNLP}, and $c_i$ is a distinct context vector for time $i$, which is calculated as a weighted sum of the input annotations $h_j$:\n\n", "itemtype": "equation", "pos": 5670, "prevtext": "\nwhere $s_i$ is an decoder hidden state for time step $i$, computed by\\\n\n", "index": 3, "text": "\\begin{equation}\ns_i = f(s_{i-1}, y_{i-1}, c_i)\n\\label{eqn-nmt-state}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"s_{i}=f(s_{i-1},y_{i-1},c_{i})\" display=\"block\"><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere $h_j={[\\overrightarrow{h}_j^{\\top};\\overleftarrow{h}_j^{\\top}]}^\\top$ is the annotation of $x_j$ from a bi-directional RNN~\\cite{Schuster:1997:TSP}, and its weight $\\alpha_{i,j}$ is computed by\n\n", "itemtype": "equation", "pos": 5973, "prevtext": "\nHere the activation function $f(\\cdot)$ is a gated recurrent unit (GRU)~\\cite{Cho:2014:EMNLP}, and $c_i$ is a distinct context vector for time $i$, which is calculated as a weighted sum of the input annotations $h_j$:\n\n", "index": 5, "text": "\\begin{equation}\nc_i = \\sum_{j=1}^{T_x}{\\alpha_{i,j}\\cdot h_j}\n\\label{eqn-context}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"c_{i}=\\sum_{j=1}^{T_{x}}{\\alpha_{i,j}\\cdot h_{j}}\" display=\"block\"><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><mrow><msub><mi>\u03b1</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u22c5</mo><msub><mi>h</mi><mi>j</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere \n\\begin{eqnarray}\ne_{i,j} &=& a(s_{i-1}, h_j) \\nonumber \\\\\n           &=& v_a^{\\top} \\tanh (W_a s_{i-1} + U_a h_j)\n\\label{eqn-alignment-model}\n\\end{eqnarray}\nis an {\\em alignment model} that scores how well $y_i$ and $h_j$ match.\nWith the alignment model, it avoids the need to represent the entire source sentence with a fixed-length vector. Instead, the decoder selects parts of the source sentence to pay attention to, thus exploits an {\\em expected annotation} $c_i$ over possible alignments $\\alpha_{i,j}$ for each time step $i$. \n\nThe parameters are trained to maximize the likelihood of the training data\n\n", "itemtype": "equation", "pos": 6271, "prevtext": "\nwhere $h_j={[\\overrightarrow{h}_j^{\\top};\\overleftarrow{h}_j^{\\top}]}^\\top$ is the annotation of $x_j$ from a bi-directional RNN~\\cite{Schuster:1997:TSP}, and its weight $\\alpha_{i,j}$ is computed by\n\n", "index": 7, "text": "\\begin{equation}\n\\alpha_{i,j} = \\frac{\\exp(e_{i,j})}{\\sum_{k=1}^{T_x} \\exp(e_{i,k})} \n\\label{eqn-alignment-probability} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\alpha_{i,j}=\\frac{\\exp(e_{i,j})}{\\sum_{k=1}^{T_{x}}\\exp(e_{i,k})}\" display=\"block\"><mrow><msub><mi>\u03b1</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></msubsup><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\n\nHowever, the alignment model misses the opportunity to take advantage of past alignment information, which proves useful in traditional statistical machine translation~\\cite{Koehn:2003:NAACL}. For example, if a source word is translated in the past, it is less likely to be translated again, thus should be assigned a lower probability.\n\n\n\n\\section{Coverage Model for NMT}\n\\label{sec-coverage-model}\n\nIn SMT, a coverage set is maintained to keep track of which source words have been translated (``covered'') in the past. Take an input sentence ${\\bf x}=\\{x_1, x_2, x_3, x_4\\}$ as an example, the initial coverage set is $\\mathcal{C}=\\{0, 0, 0, 0\\}$ which denotes no source word is yet translated. When a translation rule $bp = (x_2 x_3, y_my_{m+1})$ is used to generate translation, we produce one hypothesis labelled with coverage $\\mathcal{C}=\\{0, 1, 1, 0\\}$. It means that the second and third source words are translated. The goal is to generate translation with full coverage $\\mathcal{C}=\\{1, 1, 1, 1\\}$. \nA source word is translated when it is covered by one translation rule, and it is not allowed to be translated again in the future  (i.e. {\\em hard coverage}).\nIn this way, each source word is guaranteed to be translated and only be translated once.\nAs shown, coverage is essential for SMT since it avoids gaps and overlap when translating source words.\n\n\nModeling coverage is also useful for neural machine translators with automatic alignment, since they generally lack a mechanism to tell whether a certain segment of source sentence is translated, and therefore prone to the  ``coverage'' mistakes: some part of source sentence is translated more than once or not translated. For neural machine translation model, directly modeling coverage is less straightforward, but the problem can be significantly alleviated by keeping track of the attention signal during the decoding process. The most natural way for doing that is to append an annotation vector $\\beta_j$  to every $h_j$, which is uniformly initialized but updated after every attentive read of the corresponding hidden state. This annotation vector will enter the soft attention model for alignment, as illustrated in Figure~\\ref{figure-coverage-alignment}. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.25\\textwidth]{figures/coverage_alignment.pdf}\n\\caption{Architecture of coverage-based alignment model. A coverage set ${\\bf\\beta}_{i-1}$ is maintained to keep track of which source words have been translated before time $i$. Alignment decisions ({$\\alpha_{i,j}$}) are made jointly taking into account $\\beta_{i-1,j}$ to give a bias to untranslated source words.}\n\\label{figure-coverage-alignment}\n\\end{figure}\n\nRoughly speaking, since  \n$\\beta_{i-1,j}$ summarizes the attention record for $h_j$ ( and therefore for a small neighbor centering at the $j^{th}$ source word), it will discourage further attention to it if it has been heavily attended, and implicitly push the attention to the less attended segments of the source sentence since the attention weights are normalized to one. This could potentially solve both coverage mistakes mentioned above, when modeled and learned properly. \n\nFormally annotation model is given by \n\n", "itemtype": "equation", "pos": 7025, "prevtext": "\nwhere \n\\begin{eqnarray}\ne_{i,j} &=& a(s_{i-1}, h_j) \\nonumber \\\\\n           &=& v_a^{\\top} \\tanh (W_a s_{i-1} + U_a h_j)\n\\label{eqn-alignment-model}\n\\end{eqnarray}\nis an {\\em alignment model} that scores how well $y_i$ and $h_j$ match.\nWith the alignment model, it avoids the need to represent the entire source sentence with a fixed-length vector. Instead, the decoder selects parts of the source sentence to pay attention to, thus exploits an {\\em expected annotation} $c_i$ over possible alignments $\\alpha_{i,j}$ for each time step $i$. \n\nThe parameters are trained to maximize the likelihood of the training data\n\n", "index": 9, "text": "\\begin{equation}\n{\\operatorname{arg\\,max}}\\sum_{n=1}^{N}log P({\\bf y}_n|{\\bf x_n})\n\\label{eqn-standard-training}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{arg\\,max}}\\sum_{n=1}^{N}logP({\\bf y}_{n}|{\\bf x_{n}})\" display=\"block\"><mrow><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>\ud835\udc27</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere\n\\begin{itemize}\n  \\item $g_{update}(\\cdot)$ is the function that updates $\\beta_{i,j}$ after the new attention at time step $i$ in the decoding process;\n  \\item $\\beta_{i,j}$ is a $d$-dimensional annotation vector summarizing the history of attention till time step $i$ on $h_j$;\n  \\item $\\Phi_i(h_j)$ is a word-specific feature with its own parameters;\n  \\item $auxs$ are auxiliary inputs exploited in different sorts of coverage models;\n\\end{itemize}\nEquation~\\ref{eqn-alignment-model} gives a rather general model, which could take different function forms for $g_{update}(\\cdot)$ and $\\Phi(\\cdot)$, and different auxiliary inputs $auxs$ (e.g. previous decoding state $s_{i-1}$). In the rest of this section, we will give a number of representative implementations of the annotation model, which either resort to the flexibility of neural network function approximation (Section~\\ref{sec-nonlinear-coverage}) or bear more linguistic intuition (Section~\\ref{sec-linear-coverage}).   \n\n\n\\subsection{Neural Network-based Coverage Model} \\label{sec-nonlinear-coverage}\n\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.3\\textwidth]{figures/rnn_coverage.pdf}\n\\caption{NN-based coverage model.}\n\\label{figure-recurrent}\n\\end{figure}\n\nWhen $\\beta_j$ is a vector ($d>1$) and $g_{update}(\\cdot)$  takes a neural network (NN) form, we actually have a recurrent neural network (RNN) model for annotation, as illustrated by Figure~\\ref{figure-recurrent}. In our work, we take the following form\n\\begin{eqnarray}\n\\beta_{i,j} &=& g_{update} (\\beta_{i-1,j}, \\alpha_{i,j}, h_j, s_{i-1}) \\nonumber\\\\\n                  &=& \\tanh (U \\beta_{i-1,j} + V \\alpha_{i,j} + B h_j + W s_{i-1})\n\\label{eqn-coverage-general}\n\\end{eqnarray}\nwhere $U, V, B, W$ are weights and $s_{i-1}$ is the auxiliary input that encodes past translation information.\nNote that we leave out the the word-specific feature function $\\Phi(\\cdot)$ and only take the input annotation $h_j$ as the input to the annotation RNN. It is important to emphasize that the NN-based annotation is able to be fed with arbitrary auxiliary inputs, such as the previous attentional context $c_{i-1}$. Here we only employ $\\alpha_{i-1}$ for past alignment information, $s_{i-1}$ for past translation information, and $h_j$ for word-specific bias.\n\n\n\\paragraph{Gating}\nTo capture long-distance dependencies on past alignment information, we can employ gating activation function for $g_{update}$, such as Long Short-Term Memory (LSTM) \\cite{Hochreite:1997} or Gated Recurrent Unit (GRU)~\\cite{Cho:2014:EMNLP}. In this work, we adopt GRU since it is simple yet powerful. Then the coverage $\\beta_{i}$ is computed by\n\\begin{eqnarray}\n\\beta_{i,j} &=& (1-z_i) \\circ \\beta_{i-1,j} + z_i \\circ \\tilde{\\beta}_{i,j} \\nonumber\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n\\tilde{\\beta}_{i,j} &=& \\tanh (U [r_i \\circ \\beta_{i-1,j}] + V \\alpha_{i,j} + B h_j + W s_{i-1})\\nonumber\\\\\nz_i &=& \\sigma (U_z \\beta_{i-1,j} + V_z \\alpha_{i,j} + B_z h_j + W_z s_{i-1})\\nonumber\\\\\nr_i &=& \\sigma (U_r \\beta_{i-1,j} + V_r \\alpha_{i,j} + B_r h_j + W_r s_{i-1})\\nonumber\n\\label{eqn-coverage-general}\n\\end{eqnarray}\nwhere $\\sigma(\\cdot)$ is a logistic sigmoid function, and $z_i$ and $r_i$ are update and reset gates respectively.\n\n\n\\vspace{5pt}\n\\noindent Although the NN-based annotation model enjoys the flexibility brought by the recurrent nonlinear form, its lack of clear linguistic meaning may render it hard to train: the annotation model can only be trained along with the attention model and get the supervision signal from it in back-propagation, which could be weak (relatively distant from the decoding process) and noisy (after the distortion from other under-trained components in the decoder RNN). Partially to overcome this problem, we also propose the linguistically inspired model which has much clearer interpretation but much less parameters. \n\n\n\n\n\n\\subsection{Linguistic Coverage Model}\\label{sec-linear-coverage}\n\nWhile linguistically-inspired coverage in NMT is similar in spirit to that in SMT, there is one key difference: \nit indicates what percentage of source words have been translated (i.e. {\\em soft coverage}). \nIn NMT, each target word $y_i$ is generated from all source words with probabilities $\\alpha_{i,j}$ for source word $x_j$. \nIn other words, each source word $x_j$ involves in generating all target words and generates $\\alpha_{i,j}$ target word at time step $i$.\nNote that unlike in SMT where each source word is not {\\em fully translated} at one decoding step, $x_j$ is {\\em partially translated} at each decoding step in NMT .\nTherefore, the coverage at time step $i$ denotes the translated ratio of that each source word is translated.\n\nWe use a scalar ($d=1$) to represent linguistic coverages for each source word and employ an accumulate operation for $g_{update}$. We iteratively construct linguistic coverages through an accumulation of alignment probabilities generated by the attention model, each of which is normalized by a distinct context-dependent weight.\nThe coverage of source word $x_j$ at time step $i$ is computed by\n\n", "itemtype": "equation", "pos": 10377, "prevtext": "\n\nHowever, the alignment model misses the opportunity to take advantage of past alignment information, which proves useful in traditional statistical machine translation~\\cite{Koehn:2003:NAACL}. For example, if a source word is translated in the past, it is less likely to be translated again, thus should be assigned a lower probability.\n\n\n\n\\section{Coverage Model for NMT}\n\\label{sec-coverage-model}\n\nIn SMT, a coverage set is maintained to keep track of which source words have been translated (``covered'') in the past. Take an input sentence ${\\bf x}=\\{x_1, x_2, x_3, x_4\\}$ as an example, the initial coverage set is $\\mathcal{C}=\\{0, 0, 0, 0\\}$ which denotes no source word is yet translated. When a translation rule $bp = (x_2 x_3, y_my_{m+1})$ is used to generate translation, we produce one hypothesis labelled with coverage $\\mathcal{C}=\\{0, 1, 1, 0\\}$. It means that the second and third source words are translated. The goal is to generate translation with full coverage $\\mathcal{C}=\\{1, 1, 1, 1\\}$. \nA source word is translated when it is covered by one translation rule, and it is not allowed to be translated again in the future  (i.e. {\\em hard coverage}).\nIn this way, each source word is guaranteed to be translated and only be translated once.\nAs shown, coverage is essential for SMT since it avoids gaps and overlap when translating source words.\n\n\nModeling coverage is also useful for neural machine translators with automatic alignment, since they generally lack a mechanism to tell whether a certain segment of source sentence is translated, and therefore prone to the  ``coverage'' mistakes: some part of source sentence is translated more than once or not translated. For neural machine translation model, directly modeling coverage is less straightforward, but the problem can be significantly alleviated by keeping track of the attention signal during the decoding process. The most natural way for doing that is to append an annotation vector $\\beta_j$  to every $h_j$, which is uniformly initialized but updated after every attentive read of the corresponding hidden state. This annotation vector will enter the soft attention model for alignment, as illustrated in Figure~\\ref{figure-coverage-alignment}. \n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.25\\textwidth]{figures/coverage_alignment.pdf}\n\\caption{Architecture of coverage-based alignment model. A coverage set ${\\bf\\beta}_{i-1}$ is maintained to keep track of which source words have been translated before time $i$. Alignment decisions ({$\\alpha_{i,j}$}) are made jointly taking into account $\\beta_{i-1,j}$ to give a bias to untranslated source words.}\n\\label{figure-coverage-alignment}\n\\end{figure}\n\nRoughly speaking, since  \n$\\beta_{i-1,j}$ summarizes the attention record for $h_j$ ( and therefore for a small neighbor centering at the $j^{th}$ source word), it will discourage further attention to it if it has been heavily attended, and implicitly push the attention to the less attended segments of the source sentence since the attention weights are normalized to one. This could potentially solve both coverage mistakes mentioned above, when modeled and learned properly. \n\nFormally annotation model is given by \n\n", "index": 11, "text": "\\begin{equation}\n\\beta_{i,j} = g_{update}\\big(\\beta_{i-1,j}, \\alpha_{i,j}, \\Phi(h_j), auxs\\big)\n\\label{eqn-coverage-general}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\beta_{i,j}=g_{update}\\big{(}\\beta_{i-1,j},\\alpha_{i,j},\\Phi(h_{j}),auxs\\big{)}\" display=\"block\"><mrow><msub><mi>\u03b2</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>g</mi><mrow><mi>u</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>e</mi></mrow></msub><mo>\u2062</mo><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>\u03b2</mi><mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>\u03b1</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mrow><mi mathvariant=\"normal\">\u03a6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>a</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>s</mi></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere $\\Phi_j$ is a pre-defined weight which indicates the number of target words $x_j$ is expected to generate. \nThe simplest way is to follow Xu et al.~\\shortcite{Xu:2015:ICML} in image-to-caption translation to fix $\\Phi=1$ for all source words, which means that we directly use the sum of previous alignment probabilities without normalization as coverage for each word, as done in~\\cite{Cohn:2016:arXiv}. \n\nHowever, in natural languages, different types of source words contributes differently to the generation of translation. Take the sentence pairs in Figure~\\ref{figure-nmt-examples} as an example, the adjective on the source side ``{\\em zhudao}'' is translated into one target word ``{\\em leading}'', while the noun ``{\\em jinnian}'' is translated into two words ``{\\em this year}''. Therefore, we need to assign a distinct $\\Phi_j$ for each source word.\nIdeally, we expect $\\Phi_j = \\sum_{k=1}^{T_y} \\alpha_{k,j}$ with $T_y$ be the total number of time steps in decoding. However, such desired value is not available before decoding, thus is not suitable in this scenario. \n\n\\paragraph{Fertility}\nTo predict $\\Phi_j$, \nwe introduce the concept of {\\em fertility}, which is firstly proposed in word-level SMT~\\cite{Brown:1993:CL}. \nFertility of source word $x_j$ tells how many target words $x_j$ produces. In SMT, the fertility is a random variable $\\Phi_j$, whose distribution $p(\\Phi_j=\\phi)$ is determined by the parameters of word alignment models (e.g. IBM models).\\textbf{}\nIn this work, we compute the fertility $\\Phi_j$ by\n\n", "itemtype": "equation", "pos": 15613, "prevtext": "\nwhere\n\\begin{itemize}\n  \\item $g_{update}(\\cdot)$ is the function that updates $\\beta_{i,j}$ after the new attention at time step $i$ in the decoding process;\n  \\item $\\beta_{i,j}$ is a $d$-dimensional annotation vector summarizing the history of attention till time step $i$ on $h_j$;\n  \\item $\\Phi_i(h_j)$ is a word-specific feature with its own parameters;\n  \\item $auxs$ are auxiliary inputs exploited in different sorts of coverage models;\n\\end{itemize}\nEquation~\\ref{eqn-alignment-model} gives a rather general model, which could take different function forms for $g_{update}(\\cdot)$ and $\\Phi(\\cdot)$, and different auxiliary inputs $auxs$ (e.g. previous decoding state $s_{i-1}$). In the rest of this section, we will give a number of representative implementations of the annotation model, which either resort to the flexibility of neural network function approximation (Section~\\ref{sec-nonlinear-coverage}) or bear more linguistic intuition (Section~\\ref{sec-linear-coverage}).   \n\n\n\\subsection{Neural Network-based Coverage Model} \\label{sec-nonlinear-coverage}\n\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.3\\textwidth]{figures/rnn_coverage.pdf}\n\\caption{NN-based coverage model.}\n\\label{figure-recurrent}\n\\end{figure}\n\nWhen $\\beta_j$ is a vector ($d>1$) and $g_{update}(\\cdot)$  takes a neural network (NN) form, we actually have a recurrent neural network (RNN) model for annotation, as illustrated by Figure~\\ref{figure-recurrent}. In our work, we take the following form\n\\begin{eqnarray}\n\\beta_{i,j} &=& g_{update} (\\beta_{i-1,j}, \\alpha_{i,j}, h_j, s_{i-1}) \\nonumber\\\\\n                  &=& \\tanh (U \\beta_{i-1,j} + V \\alpha_{i,j} + B h_j + W s_{i-1})\n\\label{eqn-coverage-general}\n\\end{eqnarray}\nwhere $U, V, B, W$ are weights and $s_{i-1}$ is the auxiliary input that encodes past translation information.\nNote that we leave out the the word-specific feature function $\\Phi(\\cdot)$ and only take the input annotation $h_j$ as the input to the annotation RNN. It is important to emphasize that the NN-based annotation is able to be fed with arbitrary auxiliary inputs, such as the previous attentional context $c_{i-1}$. Here we only employ $\\alpha_{i-1}$ for past alignment information, $s_{i-1}$ for past translation information, and $h_j$ for word-specific bias.\n\n\n\\paragraph{Gating}\nTo capture long-distance dependencies on past alignment information, we can employ gating activation function for $g_{update}$, such as Long Short-Term Memory (LSTM) \\cite{Hochreite:1997} or Gated Recurrent Unit (GRU)~\\cite{Cho:2014:EMNLP}. In this work, we adopt GRU since it is simple yet powerful. Then the coverage $\\beta_{i}$ is computed by\n\\begin{eqnarray}\n\\beta_{i,j} &=& (1-z_i) \\circ \\beta_{i-1,j} + z_i \\circ \\tilde{\\beta}_{i,j} \\nonumber\n\\end{eqnarray}\nwhere\n\\begin{eqnarray}\n\\tilde{\\beta}_{i,j} &=& \\tanh (U [r_i \\circ \\beta_{i-1,j}] + V \\alpha_{i,j} + B h_j + W s_{i-1})\\nonumber\\\\\nz_i &=& \\sigma (U_z \\beta_{i-1,j} + V_z \\alpha_{i,j} + B_z h_j + W_z s_{i-1})\\nonumber\\\\\nr_i &=& \\sigma (U_r \\beta_{i-1,j} + V_r \\alpha_{i,j} + B_r h_j + W_r s_{i-1})\\nonumber\n\\label{eqn-coverage-general}\n\\end{eqnarray}\nwhere $\\sigma(\\cdot)$ is a logistic sigmoid function, and $z_i$ and $r_i$ are update and reset gates respectively.\n\n\n\\vspace{5pt}\n\\noindent Although the NN-based annotation model enjoys the flexibility brought by the recurrent nonlinear form, its lack of clear linguistic meaning may render it hard to train: the annotation model can only be trained along with the attention model and get the supervision signal from it in back-propagation, which could be weak (relatively distant from the decoding process) and noisy (after the distortion from other under-trained components in the decoder RNN). Partially to overcome this problem, we also propose the linguistically inspired model which has much clearer interpretation but much less parameters. \n\n\n\n\n\n\\subsection{Linguistic Coverage Model}\\label{sec-linear-coverage}\n\nWhile linguistically-inspired coverage in NMT is similar in spirit to that in SMT, there is one key difference: \nit indicates what percentage of source words have been translated (i.e. {\\em soft coverage}). \nIn NMT, each target word $y_i$ is generated from all source words with probabilities $\\alpha_{i,j}$ for source word $x_j$. \nIn other words, each source word $x_j$ involves in generating all target words and generates $\\alpha_{i,j}$ target word at time step $i$.\nNote that unlike in SMT where each source word is not {\\em fully translated} at one decoding step, $x_j$ is {\\em partially translated} at each decoding step in NMT .\nTherefore, the coverage at time step $i$ denotes the translated ratio of that each source word is translated.\n\nWe use a scalar ($d=1$) to represent linguistic coverages for each source word and employ an accumulate operation for $g_{update}$. We iteratively construct linguistic coverages through an accumulation of alignment probabilities generated by the attention model, each of which is normalized by a distinct context-dependent weight.\nThe coverage of source word $x_j$ at time step $i$ is computed by\n\n", "index": 13, "text": "\\begin{equation}\n\\beta_{i,j}= \\frac{1}{\\Phi_j} \\sum_{k=1}^{i} \\alpha_{k,j}\n\\label{eqn-fertility-coverage}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\beta_{i,j}=\\frac{1}{\\Phi_{j}}\\sum_{k=1}^{i}\\alpha_{k,j}\" display=\"block\"><mrow><msub><mi>\u03b2</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>j</mi></msub></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></munderover><msub><mi>\u03b1</mi><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere $N \\in \\mathbb{R}$ is a predefined constant\nto denoting the maximum number of target words one source word can produce, $\\sigma(\\cdot)$ is a logistic sigmoid function, and $U_f \\in \\mathbb{R}^{1 \\times 2n}$ is the weight matrix.\nHere we use $h_j$ to denote $(x_j|{\\bf x})$ since $h_j$ contains information about the whole input sentence with a strong focus on the parts surrounding $x_j$~\\cite{Bahdanau:2015:ICLR}.\nSince $\\Phi_j$ does not depend on $i$, we can pre-compute it before decoding to minimize the computational cost.\n\nWhile our fertility model is similar in spirit to that in SMT, there are two key differences which reflect how we simplify and adapt from the original model. \nFirst, fertility in SMT is a random variable with a set of fertility probabilities, $n(\\Phi_j|x_j) = p(\\Phi_{1}^{j-1}, {\\bf x})$, which depends on the fertilities of previous source words. To simplify the calculation and adapt it to the attention model in NMT, we define the fertility in NMT as a constant number,  which is independent of previous fertilities.\nSecond, $\\Phi_j$  in SMT is an {\\em integer} sum over binary alignment decisions whereas that  in NMT is a {\\em real} sum over soft alignment probabilities.\n\n\n\n\n\n\\subsection{Integrating Coverage into NMT}\n\\label{sec-coverage-alignment-model}\n\n\\begin{figure*}[t]\n\\begin{center}\n\n\n            \n            \\includegraphics[width=0.45\\textwidth]{figures/over_translation/additive_align.png}\\hspace{8pt}\n            \\includegraphics[width=0.45\\textwidth]{figures/under_translation/additive_align.png}\\\\\n           \n\\end{center}\n\n\\caption{Example translations of coverage-based NMT.  Coverage model alleviates the problems of over-translation and under-translation shown in Figure~\\ref{figure-nmt-examples}.}\n\\label{figure-coverage-examples}\n\\end{figure*}\n\nAlthough the introduction of alignment model has advanced the state-of-the-art of NMT, it computes soft alignment probabilities without considering useful information in the past. For example, a source word that contributed a lot to the predicted target words in the past, should be assigned lower alignment probabilities in the following decoding. Motivated by this observation, in this work, we propose to calculate the alignment probability by jointly taking into account past alignment information (e.g. which source words have been translated).\n\nIntuitively, at each time step $i$ in the decoding phase, coverage from time step ($i-1$) serves as input to the attention model, which provides complementary information of that how likely the source words are translated in the past.\nWe expect the coverage information would guide the attention model to focus more on untranslated source words (i.e. assign higher probabilities). In practice, we find that the coverage model does come up to expectation (see Section~\\ref{sec-experiments}). The translated ratios of source words from linguistic coverages negatively correlate to the corresponding alignment probabilities. Figure~\\ref{figure-coverage-examples} shows an example, in which coverage-based NMT indeed alleviates the problems of over-translation and under-translation shown in Figure~\\ref{figure-nmt-examples}.\n\n \nMore formally, we rewrite the alignment model in Equation~\\ref{eqn-alignment-model} as\n\n", "itemtype": "equation", "pos": 17277, "prevtext": "\nwhere $\\Phi_j$ is a pre-defined weight which indicates the number of target words $x_j$ is expected to generate. \nThe simplest way is to follow Xu et al.~\\shortcite{Xu:2015:ICML} in image-to-caption translation to fix $\\Phi=1$ for all source words, which means that we directly use the sum of previous alignment probabilities without normalization as coverage for each word, as done in~\\cite{Cohn:2016:arXiv}. \n\nHowever, in natural languages, different types of source words contributes differently to the generation of translation. Take the sentence pairs in Figure~\\ref{figure-nmt-examples} as an example, the adjective on the source side ``{\\em zhudao}'' is translated into one target word ``{\\em leading}'', while the noun ``{\\em jinnian}'' is translated into two words ``{\\em this year}''. Therefore, we need to assign a distinct $\\Phi_j$ for each source word.\nIdeally, we expect $\\Phi_j = \\sum_{k=1}^{T_y} \\alpha_{k,j}$ with $T_y$ be the total number of time steps in decoding. However, such desired value is not available before decoding, thus is not suitable in this scenario. \n\n\\paragraph{Fertility}\nTo predict $\\Phi_j$, \nwe introduce the concept of {\\em fertility}, which is firstly proposed in word-level SMT~\\cite{Brown:1993:CL}. \nFertility of source word $x_j$ tells how many target words $x_j$ produces. In SMT, the fertility is a random variable $\\Phi_j$, whose distribution $p(\\Phi_j=\\phi)$ is determined by the parameters of word alignment models (e.g. IBM models).\\textbf{}\nIn this work, we compute the fertility $\\Phi_j$ by\n\n", "index": 15, "text": "\\begin{equation}\n\\Phi_{j} = \\mathcal{N} (x_j | {\\bf x}) = \\mathcal{N} (h_j) = N \\cdot \\sigma(U_f h_j)\n\\label{eqn-fertility}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\Phi_{j}=\\mathcal{N}(x_{j}|{\\bf x})=\\mathcal{N}(h_{j})=N\\cdot\\sigma(U_{f}h_{j})\" display=\"block\"><mrow><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>j</mi></msub><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">|</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>N</mi><mo>\u22c5</mo><mi>\u03c3</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>U</mi><mi>f</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere $\\beta_{i-1,j}$ is the translated ratio of source word $x_j$ before time $i$. $B_d \\in \\mathbb{R}^{n\\times 1}$ is the additional weight matrix for coverage with $n$ and $d$ be the numbers of hidden units and coverage units respectively. \n\n\n\n\n\n\\section{Training}\n\\label{sec-training}\nIn this paper, we take end-to-end learning for our coverage-based NMT model, which jointly learns not only the parameters for the ``original\" RNNsearch (i.e., those for encoding RNN, decoding RNN, and attention model) but also the parameters for coverage modeling (i.e., those for annotation and its role in guiding the attention) . More specially, we choose to maximize the likelihood of reference sentences as most other neural machine translator (see, however\\cite{Shen:2015:arXiv})    \n\n", "itemtype": "equation", "pos": 20686, "prevtext": "\nwhere $N \\in \\mathbb{R}$ is a predefined constant\nto denoting the maximum number of target words one source word can produce, $\\sigma(\\cdot)$ is a logistic sigmoid function, and $U_f \\in \\mathbb{R}^{1 \\times 2n}$ is the weight matrix.\nHere we use $h_j$ to denote $(x_j|{\\bf x})$ since $h_j$ contains information about the whole input sentence with a strong focus on the parts surrounding $x_j$~\\cite{Bahdanau:2015:ICLR}.\nSince $\\Phi_j$ does not depend on $i$, we can pre-compute it before decoding to minimize the computational cost.\n\nWhile our fertility model is similar in spirit to that in SMT, there are two key differences which reflect how we simplify and adapt from the original model. \nFirst, fertility in SMT is a random variable with a set of fertility probabilities, $n(\\Phi_j|x_j) = p(\\Phi_{1}^{j-1}, {\\bf x})$, which depends on the fertilities of previous source words. To simplify the calculation and adapt it to the attention model in NMT, we define the fertility in NMT as a constant number,  which is independent of previous fertilities.\nSecond, $\\Phi_j$  in SMT is an {\\em integer} sum over binary alignment decisions whereas that  in NMT is a {\\em real} sum over soft alignment probabilities.\n\n\n\n\n\n\\subsection{Integrating Coverage into NMT}\n\\label{sec-coverage-alignment-model}\n\n\\begin{figure*}[t]\n\\begin{center}\n\n\n            \n            \\includegraphics[width=0.45\\textwidth]{figures/over_translation/additive_align.png}\\hspace{8pt}\n            \\includegraphics[width=0.45\\textwidth]{figures/under_translation/additive_align.png}\\\\\n           \n\\end{center}\n\n\\caption{Example translations of coverage-based NMT.  Coverage model alleviates the problems of over-translation and under-translation shown in Figure~\\ref{figure-nmt-examples}.}\n\\label{figure-coverage-examples}\n\\end{figure*}\n\nAlthough the introduction of alignment model has advanced the state-of-the-art of NMT, it computes soft alignment probabilities without considering useful information in the past. For example, a source word that contributed a lot to the predicted target words in the past, should be assigned lower alignment probabilities in the following decoding. Motivated by this observation, in this work, we propose to calculate the alignment probability by jointly taking into account past alignment information (e.g. which source words have been translated).\n\nIntuitively, at each time step $i$ in the decoding phase, coverage from time step ($i-1$) serves as input to the attention model, which provides complementary information of that how likely the source words are translated in the past.\nWe expect the coverage information would guide the attention model to focus more on untranslated source words (i.e. assign higher probabilities). In practice, we find that the coverage model does come up to expectation (see Section~\\ref{sec-experiments}). The translated ratios of source words from linguistic coverages negatively correlate to the corresponding alignment probabilities. Figure~\\ref{figure-coverage-examples} shows an example, in which coverage-based NMT indeed alleviates the problems of over-translation and under-translation shown in Figure~\\ref{figure-nmt-examples}.\n\n \nMore formally, we rewrite the alignment model in Equation~\\ref{eqn-alignment-model} as\n\n", "index": 17, "text": "\\begin{flalign}\n e_{i,j} &= a(s_{i-1}, h_j, \\beta_{i-1, j})   \\nonumber \\\\\n& = v_a^{\\top} \\tanh (W_a s_{i-1} + U_a h_j + B_a \\beta_{i-1, j})\n\\end{flalign}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle e_{i,j}\" display=\"inline\"><msub><mi>e</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=a(s_{i-1},h_{j},\\beta_{i-1,j})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>a</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mi>j</mi></msub><mo>,</mo><msub><mi>\u03b2</mi><mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=v_{a}^{\\top}\\tanh(W_{a}s_{i-1}+U_{a}h_{j}+B_{a}\\beta_{i-1,j})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msubsup><mi>v</mi><mi>a</mi><mo>\u22a4</mo></msubsup><mo>\u2062</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>W</mi><mi>a</mi></msub><mo>\u2062</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>U</mi><mi>a</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><msub><mi>B</mi><mi>a</mi></msub><mo>\u2062</mo><msub><mi>\u03b2</mi><mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\n\n\nFor the coverage model with a clearer linguistic interpretation (Section \\ref{sec-linear-coverage}), it is possible to inject an auxiliary objective function on some intermediate representation. More specifically, we have the following objective\n\n", "itemtype": "equation", "pos": 21620, "prevtext": "\nwhere $\\beta_{i-1,j}$ is the translated ratio of source word $x_j$ before time $i$. $B_d \\in \\mathbb{R}^{n\\times 1}$ is the additional weight matrix for coverage with $n$ and $d$ be the numbers of hidden units and coverage units respectively. \n\n\n\n\n\n\\section{Training}\n\\label{sec-training}\nIn this paper, we take end-to-end learning for our coverage-based NMT model, which jointly learns not only the parameters for the ``original\" RNNsearch (i.e., those for encoding RNN, decoding RNN, and attention model) but also the parameters for coverage modeling (i.e., those for annotation and its role in guiding the attention) . More specially, we choose to maximize the likelihood of reference sentences as most other neural machine translator (see, however\\cite{Shen:2015:arXiv})    \n\n", "index": 19, "text": "\\begin{flalign}\n\n{\\operatorname{arg\\,max}}\\sum_{n=1}^{N}  \\log P({\\bf y}_n|{\\bf x}_n).\n\\label{eqn-coverage-training}\n\\end{flalign}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;{\\operatorname{arg\\,max}}\\sum_{n=1}^{N}\\log P({\\bf y}_{n}|{%&#10;\\bf x}_{n}).\" display=\"inline\"><mrow><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04811.tex", "nexttext": "\nwhere the term $\\sum_{j=1}^{|{\\bf x_n}|}(\\Phi_j - \\sum_{i=1}^{|{\\bf y_n}|}{\\alpha_{i,j}})^2 $  penalizes the discrepancy between the sum of attention and the expect fertility for linguistic coverage. This is similar to the more explicit training for fertility as in  Xu et al.~\\shortcite{Xu:2015:ICML}, which directly penalizes the discrepancy between each $\\Phi_j$ and the sum of attention to the corresponding $h_j$. \n\n\nOur end-to-end training strategy poses less constraints on the dependency between $\\{\\Phi_j\\}$ and the attention than a more explicit strategy taken in~\\cite{Xu:2015:ICML}, and let the objective associated with the translation quality (i.e., the likelihood) drive the training. This strategy is arguably advantageous, since the attention weight on a hidden state $h_j$ cannot be interpreted as the proportion of the corresponding word being translated on the target side. For one thing, the hidden state $\\{ h_j\\}$, after the transformation from encoding RNN, bear the contextual information from other parts of the source sentence and therefore lose the rigid correspondence with the corresponding words. Our empirical study shows that a combined objective as in Equation{eqn-coverage-training} consistently worsens the translation quality (BLEU score) while gaining slightly on the alignment. \n\n\\section{Experiments}\n\\label{sec-experiments}\n\n\nWe report our empirical study on applying coverage-based NMT to Chinese-to-English translation, and compare it against state-of-the-art NMT and SMT models.\n\n\\subsection{Setup}\n\\paragraph{Dataset and Evaluation Metrics} Our training data consists of 1.25M sentence pairs extracted from LDC corpora\\footnote{The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.}, with 27.9M Chinese words and 34.5M English words respectively.\nWe choose NIST 2002 (MT02) dataset as our development set, and the NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) datasets as our test sets.\nWe use the case-insensitive 4-gram NIST BLEU score~\\cite{Papineni:2002} \nas our evaluation metric, and \\emph{sign-test}~\\cite{Collins:2005} as statistical significance test.\nIn addition to BLEU score to evaluate the translation quality, we also specifically check the alignment quality with alignment error rate (AER)~\\cite{Och:2003}.\n\n\\paragraph{Training Neural Networks} In training of the neural networks, we limit the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8\\% and 98.3\\% of the two corpora respectively. All the out-of-vocabulary words are mapped to a special token \\texttt{\\small UNK}.\nWe train each model with the sentences of length up to 50 words in training data. The word embedding dimension is 620 and the size of a hidden layer is 1000. We set the dimension of coverage $d=1$ for both NN-based\\footnote{In a pilot study, increasing the dimension of NN-based coverage did not improve the translation performance.}  and linguistic coverage models and set $N=2$ for the fertility model. We train our models until the BLEU score on the development set stopped improving.\n\nWe compare our method with two state-of-the-art SMT and NMT\\footnote{There are recent progress on aggregating multiple models or enlarging the vocabulary(e.g., in ~\\cite{Jean:2015:ACL}), but here we focus on the generic models.} models:\n\\begin{itemize}\n    \\item {\\bf Moses}~\\cite{Koehn:2007:ACL}: an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data;\n    \\item {\\bf RNNsearch}~\\cite{Bahdanau:2015:ICLR}:  an attentional NMT model with default setting.\n\\end{itemize}\nWe use  the RNNsearch as the NMT baseline, for it represents the state-of-the-art neural machine translation methods with a small vocabulary and modest parameter size (30M$\\sim$50M).\n\n\n\\subsection{Translation Quality}\n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{l|llll}\n    {\\bf System}\t&\t{\\bf MT05}  &  {\\bf MT06}\t  &\t{\\bf MT08}  &  {\\bf Ave.}\\\\\n    \\hline\n    Moses\t              &\t31.37\t\t&\t\t30.85\t\t&\t23.01\t\t&\t28.41\\\\\n    \\hline\n    RNNSearch\t\t\t\t\t&  28.63  &  28.92  &  21.04\t&  26.20\\\\\n    \\hdashline\n    + NN-based coverage w/o gating\t\t\t&  29.77  &  29.20  &  21.53  &  26.83\\\\\n    + NN-based coverage w/ gating           &  29.89  &  29.38  &  22.15  &  27.14\\\\\n    \\hdashline\n    + Linguistic coverage w/o fertility              \t\t&  29.41  &  29.78  &  23.03  &  27.41\\\\\n    + Linguistic coverage w/ fertility\t\t&  30.11  &  30.08  &  22.91  &  27.70\\\\\n\\end{tabular}\n\\caption{Evaluation of translation quality. }\n\\label{table-translation-results}\n\\end{table*}\n\nTable~\\ref{table-translation-results} shows the translation performances measured in BLEU score. \nClearly the proposed \\textsc{Coverage-based NMT} significantly improves the translation quality in all cases, although there are still considerable differences among different variants. More specifically,\n\\begin{itemize}\n  \\item {\\bf NN-based Coverages} (Rows 3 and 4 in Table~\\ref{table-translation-results}): Both variants of NN-based coverages outperform RNNSearch with averaged gains of 0.63 and 0.94 BLEU points, respectively. Introducing gating activation function improves the performance of coverage models, which is consistent with the results in other tasks (e.g. ~\\cite{Cho:2014:EMNLP}).\n  \\item {\\bf Linguistic Coverages} (Rows 5 and 6 in Table~\\ref{table-translation-results}): Two observations can be made. First, linguistic coverages overall outperforms its NN-based counterparts, indicating that explicitly linguistic regularities are very important to the attention model. This is further verified on the alignment task (Section~\\ref{sec-alignment-quality}). Second, incorporating fertility model boosts performance by better estimating the covered ratios of source words.  \n\\end{itemize}\n\n\\subsection{Alignment Quality}\n\\label{sec-alignment-quality}\n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{l|cc}\n    {\\bf System}\t\t\t& SAER &  AER\\\\\n    \\hline\n    RNNsearch\t\t\t\t\t\t&  69.84  &  56.78\\\\\n    \\hdashline\n    + NN-based coverage w/o gating \t\t&  69.37  &  56.44\\\\\n    + NN-based coverage w/ gating                &  68.49  &  56.17\\\\\n    \\hdashline\n    + Linguistic coverage w/o fertility\t\t&  69.25  &  55.88\\\\\n    + Linguistic coverage w/ fertility\t\t&  67.89  &  54.91\\\\\n\\end{tabular}\n\\caption{Evaluation of alignment quality. For both metrics, the lower the score, the better the alignment quality.}\n\\label{table-alignment-results}\n\\end{table*}\n\n\n\\begin{figure*}[t]\n\\begin{center}\n            \\includegraphics[width=0.45\\textwidth]{figures/align1/nmt.png}\\hspace{8pt}\n            \\includegraphics[width=0.45\\textwidth]{figures/align1/additive_align.png}\\\\\n            \n            \\includegraphics[width=0.45\\textwidth]{figures/align2/nmt.png}\\hspace{8pt}\n            \\includegraphics[width=0.45\\textwidth]{figures/align2/additive_align.png}\\\\\n            \n\\end{center}\n\\caption{Example alignments of NMT and (linguistically) coverage-based NMT.}\n\\label{figure-align-cases}\n\\end{figure*}\n\n\nIn this section, we investigate the quality of different alignments on the Chinese-English language pair data. We carried out experiments on the evaluation dataset from~\\cite{Liu:2015:AAAI}, which contains 900 manually aligned sentence pairs. \nWe evaluate alignments in terms of AER:\n\\begin{eqnarray}\nAER(S,P,A)=1-\\frac{|A \\cap S|+|A \\cap P|}{|A|+|S|} \\nonumber\n\\end{eqnarray}\nwhere $S$ is a set of sure links in a hand-aligned reference alignment, $P$ is a set of possible links in the reference alignment, and $A$ is a candidate alignment. Note that $S$ is a subset of $P$: $S\\subseteq P$.\n\nGiven that AER is designed specifically for binary alignments in SMT, we design a variant of AER for soft alignments in NMT, naming {\\em SAER}:\n\\begin{eqnarray}\nSAER(S,P,A)=1-\\frac{|M_{A} \\times M_{S}|+|M_{A} \\times M_{P}|}{|M_{A}|+|M_{S}|} \\nonumber\n\\end{eqnarray}\nwhere $M$ denotes alignment matrix, and for both $M_{S}$ and $M_{P}$ we assign the links in $S$ and $P$ with probabilities $1.0$ while assign the other links with probabilities $0.0$. In this way, we are able to better evaluate the quality of the soft alignments produced by attentional NMT.\n\nWe follow Luong et al.~\\shortcite{Luong:2015:EMNLP} to ``force'' decode NMT models to produce translations that match references.  \nWe extract both (1) one-to-one alignments by selecting the source word with the highest alignment probability for each target word, and (2) alignment matrices that consist of alignment probabilities from all source words for each target word. We measure their qualities with AER and SAER respectively, as shown in Table~\\ref{table-alignment-results}.\\footnote{Our results are basically consistent with Cheng et al.~\\shortcite{Cheng:2015:arXiv} on the same evaluation data. The overall error rates in Table~\\ref{table-alignment-results} are around 2 points higher than theirs for two reasons: (1) the size of our training data is half as much as theirs, and (2) we don't implement the technique in~\\cite{Jean:2015:ACL} to address unknown words while they did.}\n\nWe find that coverage information improves attention model as expected by maintain an annotation summarizing the log of previous attention on each source word. More specifically, linguistic coverage with fertility significantly reduces alignment errors under both metrics, in which fertility plays an important role. Figure~\\ref{figure-align-cases} shows example alignment matrices, which shows linguistic coverages significantly improves the alignment accuracy.\nNN-based coverages, however, only slightly reduces alignment errors, which is consistent with the performance on the translation task. It reconfirms our claim that linguistic coverages provide more explicit signals to the attention model, which is the key to the success.\n\n\n\n\\subsection{Effects on Long Sentences}\n\n\\begin{figure}[t]\n\\begin{center}\n            \\includegraphics[width=0.45\\textwidth]{figures/bleu_sentence_len.pdf}\\hspace{8pt}\n            \\includegraphics[width=0.45\\textwidth]{figures/tran_sentence_len.pdf}\\\\\n      \\caption{Performance of the generated translations on the test set with respect to the lengths of the input sentences. The results are on the full test data by merging the three test sets. Coverage-based NMT alleviates the problem of under-translation by producing longer translations on long sentences, leading to better translation performances.}\n    \\label{figure-sentence-len}\n  \\end{center}\n\\end{figure}\n\nWe follow Bahdanau et al.~\\shortcite{Bahdanau:2015:ICLR} to group sentences of similar lengths together and compute a BLEU score and an averaged length of translation per group, as shown in Figure~\\ref{figure-sentence-len}. Cho et al.~\\shortcite{Cho:2014:SSST} shows that the performance of NMT drops rapidly when the length of input sentence increases. Our results confirm these findings. One main reason is that NMT produces much shorter translations on longer sentences (e.g. $>40$, see right panel in Figure~\\ref{figure-sentence-len}), thus faces a serious under-translation problem. \nCoverage-based NMT alleviates this problem through incorporating  coverage information into the attention model, which in general pushes the attention to untranslated parts of the input sentence and implicitly discourages the early stop of the decoding process.\n\n\n\n\n\n\n\\section{Related Work}\n\nOur work is inspired by recent works on improving attentional NMT.\nAttention mechanism advanced state of the art NMT by jointly learning to align and translate~\\cite{Bahdanau:2015:ICLR,Luong:2015:EMNLP}. \nThe notion of attention corresponds well to that of alignment in traditional word-based SMT~\\cite{Brown:1993:CL}, giving the opportunities to be further improved with techniques that have been applied with success in SMT.\nFollowing the success of minimum risk training (MRT) in conventional SMT~\\cite{Och:2003b}, Shen et al.~\\shortcite{Shen:2015:arXiv} proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics.\nBased on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al.~\\shortcite{Cheng:2015:arXiv} proposed an agreement-based learning~\\cite{Liang:2006:NAACL} to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and under-translation problems.\n\nConcurrent with our work, Cohn et al.~\\shortcite{Cohn:2016:arXiv} and Feng et al.~\\shortcite{Feng:2016:arXiv} made use of the concept of ``fertility'' for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility.\n\nCohn et al.~\\shortcite{Cohn:2016:arXiv} introduced a feature-based fertility that includes the total alignment scores for the surrounding source words. In contrast, we build a prediction of fertility to decide how many target words each source produces before decoding. The expected fertility then works as a normalizer to better estimate the covered ratio of each source word, which guides the alignment model to pay more attention to uncovered words.\nFeng et al.~\\shortcite{Feng:2016:arXiv} used the previous attentional context to represent {\\em implicit fertility} and directly passed it to the decoder , which is in essence similar to the input-feed method proposed in~\\cite{Luong:2015:EMNLP}. Comparatively, we predict {\\em explicit fertility} for each source word based on its encoding annotation, and incorporate it into the linguistic-inspired coverage for attention model. In this work, we show that the explicitly designed fertility (or coverage) outperforms its implicit neural network-based counterpart in both translation and alignment tasks.\nThere is one minor difference as well: we validate the effectiveness of our approach on a large-scale corpus while both Cohn et al.~\\shortcite{Cohn:2016:arXiv} and Feng et al.~\\shortcite{Feng:2016:arXiv} did on small-scale corpora.\n\n\\section{Conclusion}\n\nWe have presented an approach to maintain a coverage vector for NMT to indicate whether each source word is translated or not. By encouraging attentional NMT to pay more attention to untranslated words and less attention to translated words, coverage-based NMT alleviates the serious over-translation and under-translation problems that attentional NMT suffers. Experimental results show that coverage-based NMT achieves significant improvements in terms of alignment and translation quality over NMT without coverage.\n\nIn the future, we plan to further validate the effectiveness of our approach on more language pairs. Further directions also include better designs of coverages model and making better use of the coverage information (e.g. directly pass it to the decoder). \n\n\\bibliography{all}\n\\bibliographystyle{acl}\n\n\n", "itemtype": "equation", "pos": 21999, "prevtext": "\n\n\nFor the coverage model with a clearer linguistic interpretation (Section \\ref{sec-linear-coverage}), it is possible to inject an auxiliary objective function on some intermediate representation. More specifically, we have the following objective\n\n", "index": 21, "text": "\\begin{flalign}\n\n{\\operatorname{arg\\,max}}\\sum_{n=1}^{N} \\big( & \\log P({\\bf y}_n|{\\bf x}_n)  \\nonumber \\\\\n                                                         & - \\lambda \\sum_{j=1}^{|{\\bf x_n}|}(\\Phi_j - \\sum_{i=1}^{|{\\bf y_n}|}{\\alpha_{i,j}})^2 \\big)\n\\label{eqn-coverage-training}\n\\end{flalign}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\par&#10;{\\operatorname{arg\\,max}}\\sum_{n=1}^{N}\\big{(}\" display=\"inline\"><mrow><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>max</mi></mrow><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mo maxsize=\"120%\" minsize=\"120%\">(</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\log P({\\bf y}_{n}|{\\bf x}_{n})\" display=\"inline\"><mrow><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc32</mi><mi>n</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\lambda\\sum_{j=1}^{|{\\bf x_{n}}|}(\\Phi_{j}-\\sum_{i=1}^{|{\\bf y_{%&#10;n}}|}{\\alpha_{i,j}})^{2}\\big{)}\" display=\"inline\"><mrow><mo>-</mo><mi>\u03bb</mi><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc31</mi><mi>\ud835\udc27</mi></msub><mo stretchy=\"false\">|</mo></mrow></munderover></mstyle><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"normal\">\u03a6</mi><mi>j</mi></msub><mo>-</mo><mstyle displaystyle=\"true\"><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>\ud835\udc32</mi><mi>\ud835\udc27</mi></msub><mo stretchy=\"false\">|</mo></mrow></munderover></mstyle><msub><mi>\u03b1</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></math>", "type": "latex"}]