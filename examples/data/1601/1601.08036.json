[{"file": "1601.08036.tex", "nexttext": "\n\nThe fact that the application case and benchmark variables are known with limited precision is taken into account by treating ${\\mathbf{y}}$ as a random vector, and the prior distribution $\\rm p({\\mathbf{y}})$, reflecting the uncertainty of ${\\mathbf{y}}$ due to uncertainties of nuclear data, technological parameters and operational parameters, is assessed through Mon\\-te Carlo sampling of these parameters and subsequent computation of ${\\mathbf{y}}$ for each random sample. Then $\\rm p({\\mathbf{y}})$ is estimated from the resulting statistics of ${\\mathbf{y}}$ computations. For a normal distribution model, this means to estimate the prior mean vector ${\\mathbf{y}}_0$ and the prior covariance matrix ${\\boldsymbol{\\Sigma}}_0$ from the Monte Carlo data~\\citep{mocaba}:\\footnote{A more general class of distribution models can be accessed by making use of invertible variable transformations~\\citep{mocaba}.}\n\n\n", "itemtype": "equation", "pos": 13691, "prevtext": "\n\n\\begin{frontmatter}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\title{Improving PWR core simulations by  \\linebreak[9] Monte Carlo uncertainty analysis and Bayesian inference}\n\n\n\\author[UPM,AREVA]{E.~Castro}\n\\ead{emilio.castro@upm.es}\n\\author[UPM]{C.~Ahnert}\n\\author[AREVA]{O.~Buss}\n\\author[UPM]{N.~Garc\\'{i}a-Herranz}\n\\author[AREVA]{A.~Hoefer}\n\\author[AREVA]{D.~Porsch}\n\n\\address[UPM]{Universidad Polit\\'{e}cnica de Madrid - Dpto. de Ingenier\\'{i}a Energ\\'{e}tica, \\'{a}rea de Ingenier\\'{i}a Nuclear\\\\\n\tC. Jos\\'{e} Guti\\'{e}rrez Abascal 2, 28006 Madrid, Spain.}\n\\address[AREVA]{AREVA~GmbH, Paul-Gossen-Strasse 100, 91052 Erlangen, Germany}\n\n\n\\begin{abstract}\nA Monte Carlo-based Bayesian inference model is applied to the prediction of reactor operation parameters of a PWR nuclear power plant. In this non-perturbative framework, high-dimensional covariance information describing the uncertainty of microscopic nuclear data is combined with measured reactor operation data in order to provide statistically sound, well founded uncertainty estimates of integral parameters, such as the boron letdown curve and the burnup-dependent reactor power distribution. The performance of this methodology is assessed in a blind test approach, where we use measurements of a given reactor cycle to improve the prediction of the subsequent cycle. As it turns out, the resulting improvement of the prediction quality is impressive. In particular, the prediction uncertainty of the boron letdown curve, which is of utmost importance for the planning of the reactor cycle length, can be reduced by one order of magnitude by including the boron concentration measurement information of the previous cycle in the analysis. Additionally, we present first results of non-perturbative nuclear-data updating and show that predictions obtained with the updated libraries are consistent with those induced by Bayesian inference applied directly to the integral observables.\n\\end{abstract}\n\n\\begin{keyword}\n\nUncertainty analysis \\sep Nuclear data \\sep Monte Carlo methods \\sep PWR core analysis \\sep Bayesian inference\n\n\n\n\\end{keyword}\n\n\\end{frontmatter}\n\n\n\n\n\n\n\\newpage\n\n\n\n\n\\section{Introduction}\n\\label{sect::intro}\n\n\n\n\nBest estimate plus uncertainty methodologies allow for a reliable quantification of safety margins for nuclear power plant operation and for the manufacturing, handling, storage and transport of nuclear fuel \\citep{margins_IAEA,be_IAEA,uam_vol1,uacsa_report}. This opens up the possibility to eliminate unnecessary conservatism, which leads to improved plant performance through optimized core design and greater operational flexibility and to reduced fuel management costs. However, the evaluation of statistical confidence bounds for nuclear safety parameters requires a consistent statistical framework to combine and propagate uncertainties in nuclear data, technological data, and operational data.\n\nTwo different approaches are currently used to propagate nuclear data uncertainties to integral observable uncertainties: perturbation theory and Monte Carlo sampling. \n\nThe perturbation theory approach has been successfully used for more than a half a century \\citep{uchasev,gandini} and has been implemented in many different computer codes. Within this framework, integral functions of nuclear data are approximated by their first order series expansions, which implies that the integral variable uncertainties are expressed as linear transformations of nuclear data covariances defined by the sensitivities of the integral observables to the nuclear data; see e.g.~\\citep{broadhead}. Typically, the applied nuclear transport codes have to be upgraded for computing the sensitivity coefficients. As a first order approximation, this approach yields sufficiently accurate uncertainty estimates only under the condition that the relevant nuclear data uncertainties are not too large. \n\nIn recent years, the Monte Carlo sampling approach has been playing an increasingly important role in the uncertainty propagation of nuclear data to integral observables \\citep{koning_tmc, acab_2008, xsusa, nuduna,xsusa_scale, psi}. This method consists in random sampling of nuclear data parameters - optionally together with technological and operational parameters - from their joint uncertainty distribution, where each random sample is used in a different computation of the integral variable of interest. Finally, uncertainty estimations for the integral variables are obtained from the statistics of Monte Carlo computations. Since this approach does not rely on perturbation theory, any inaccuracies due to omitting higher order effects can be ruled out a priori. Additionally, the transport codes can be used as so-called black boxes and do not need to be adapted for computing sensitivity coefficients, typically by implementing adjoint flux computation capabilities.\n\nA strong point of first order perturbation theory is that it can be combined with the Generalized Linear Least Squares (GLLS) method \n\\citep{cecchini,humi,hemment,broadhead,saintjean,salvatores} to include measurements of integral variables in order to improve the knowledge about the nuclear data and, consequently, about the integral variables depending on them. \n\nThe Monte Carlo approach, in contrast, has in the past been limited by the fact that an equally rigorous framework as the GLLS method was missing. However, this limitation has been removed recently by the development of the MOCABA methodology \\citep{mocaba}. As for the GLLS method, MOCABA is based on a Bayesian model that permits the inclusion of information from integral measurements to improve the prediction of integral observables. A major advantage of this approach is that MOCABA updating can be applied directly to the integral observables without taking the detour via nuclear data updating. On the other hand, since nuclear data can be seen as just a special case of an integral observable,  MOCABA can also be used for non-perturbative nuclear data updating, which may be an attractive alternative to the GLLS approach for the generation of adjusted nuclear data libraries \\citep{wpec_mocaba}. Mathematically, MOCABA can be seen as a non-perturbative generalization of the GLLS framework, which is shown by the fact that applying first order perturbation theory to the MOCABA model yields the well-known GLLS formulas \\citep{mocaba}.\n\nIn this work, the performance of Monte Carlo-based Bayesian updating is tested for application in reactor physics. The question we address is what we can learn from previous reactor measurements for the prediction of future reactor cycles.  As a test case, we consider two consecutive burnup cycles of a Spanish  PWR nuclear power plant, denoted as Cycle~A and Cycle~B, where we attempt to predict Cycle~B based on measurements of Cycle~A. Here we focus on two reactor observables: the boron letdown curve, representing the critical boron concentration in the reactor as a function of burnup, and the reactor power distribution defined by the power values of the individual fuel assemblies in the reactor core. For the propagation of nuclear data uncertainties to the considered reactor observables, random samples of ENDF/B-VII.1 nuclear data files~\\citep{endfb71} are generated with the aid of  the NUDUNA Monte Carlo code~\\citep{nuduna}, which are then converted to random libraries for core simulations with the extensively validated SEANAP PWR analysis system~\\citep{SEANAP2,SEANAP3}. The prior uncertainty distributions of the boron concentration and the assembly-wise power distribution are obtained from a statistical evaluation of the SEANAP results calculated with the different random libraries. Subsequently, the MOCABA updating procedure is used to improve the prior predictions of the reactor observables of Cycle~B by including measurements of Cycle~A. The predictive power of this approach is verified by comparing the predicted reactor observables of Cycle~B to the actual measured values. Finally, we compare the predictions from direct integral observable updating to those obtained via MOCABA updating of the SEANAP nuclear data library.\n\nThe paper is structured as follows. In Section~\\ref{sect::descmethod}, we give a brief introduction to the SEANAP core analysis system, the NUDUNA nuclear data Monte Carlo code and the MOCABA updating scheme, and we describe how they are combined in our reactor uncertainty analysis. In Section~\\ref{sect::applications} we apply this methodology to the PWR test case described above. After a discussion of the results, we end with conclusions and outlook.\n\n\n\n\\section{Description of the NUDUNA / MOCABA methodology applied to SEANAP core analysis} \n\\label{sect::descmethod}\n\n\n\n\nThe procedure we use for reactor cycle prediction is divided in three steps:\n\\begin{itemize}\n\\item \nGeneration of random nuclear data libraries with the aid of the NUDUNA Monte Carlo code,\n\\item\nPerforming SEANAP burnup simulations for Cycles A and B for each of the random library inputs,\n\\item\nMOCABA updating of the prediction of Cycle~B using measurements of Cycle~A.\n\\end{itemize}\nIn the following, the codes involved in this procedure are described, together with the applied methodology.\n\n\n\n\n\\subsection{SEANAP: PWR core analysis}\n\\label{sect::SEANAP}\n\n\n\n\nSEANAP is a system for the simulation and analysis of PWR cores, developed at Universidad Polit\\'{e}cnica de Madrid \\citep{SEANAP2,SEANAP3}. It consists of a chain of concatenated codes covering the different tasks needed to perform PWR core simulations, and it has been used for the simulation of many cycles of Spanish PWRs for the last 25 years, with good agreement between the simulated and the measured values. This system can calculate a wide variety of parameters: boron concentration, peak factors, axial offset, reactivity coefficients, power per fuel assembly, among many others. The SEANAP system is a fast simulation tool that allows the computation of many different observables at the core level. Hence, it is well suited to analyze the impact of uncertainties in the basic nuclear data on global core parameters.\n\nThe main components of this system are presented in Figure~\\ref{fig::seanapfigure}. The MARIA subsystem consists of the PREWIM, WIMS-D4 and POSWIM codes. The PREWIM code generates the input files required by WIMS-D4 for all the fuel assemblies, covering the parameter space of the local physical variables, using a cylindricalized model of the square PWR assembly, which provides an efficient treatment of the PWR fuel assembly. The WIMS-D4 lattice code \\citep{WIMS_traca,WIMSDinput} calculates the PWR fuel assembly in the annular cluster geometry by $S_N$ neutron transport calculation using a 69 energy group nuclear data library. As a result, the cross sections for each fuel assembly are obtained, homogenized at the assembly level and at the pin level.\n\nCOBAYA~2 is a 2D two-group pin-by-pin diffusion code, with a fine-mesh finite-difference method. Its purpose is to compute nodal discontinuity factors and hot-pin to node average power ratios, at some reference conditions, that are used in the SIMULA core simulator. SIMULA is a 3D nodal code, using four nodes per fuel assembly, an axial mesh of 34 nodes and simplified closed-channel thermal hydraulics. The solver is a linear-discontinuous finite-difference scheme for synthetic coarse-mesh few group diffusion calculation \\citep{SEANAP1}. It uses the nodal discontinuity factors provided by COBAYA~2 for each node as a function of the burnup.\n\n\\begin{figure}[t!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0001_seanap.png}\n    \\caption[]{\\label{fig::seanapfigure} Scheme of the PWR core analysis system SEANAP.}\n  \\end{center}\n\\end{figure}\n\n\n\n\n\\subsection{NUDUNA: nuclear data random sampling}\n\\label{sect::NUDUNA}\n\n\n\n\nThe NUDUNA (NUclear Data UNcertainty Analysis) program, developed by AREVA GmbH \\citep{nuduna}, provides Monte Carlo sampling \nof nuclear data, generating random libraries that can be used in core simulations.\n\nThis code reads evaluated nuclear data files in ENDF-6 format~\\citep{endf6_format}, and provides random libraries based on the covariance information included in the evaluations for the following data:\n\n\\begin{itemize}\n\\item\naverage fission neutron multiplicities (File 1)\n\\item\nresonance parameters (File 2)\n\\item\ncross sections (File 3)\n\\item\nangular distributions (File 4)\n\\item\ndecay data (File 8, Section 457)\n\\end{itemize}\n\n\nNUDUNA is coupled to the NJOY nuclear data processing system \\citep{njoy_manual} in order to automatically generate nuclear data inputs for different transport codes. For this work, the processing of the files to the WIMSD format has been implemented using the WIMSR module of NJOY, based on \\citep{WIMSDupdate} and \\citep{MacFarlaneProcessingEndf}. {NUDUNA also supports the ACE format of MCNP~\\citep{mcnp5}, the AMPX format of  SCALE~\\citep{scale60}, and the APOLLO~II neutron library format \\citep{apollo2}.\n\nHere we apply WIMSD-formatted NUDUNA samples in SEANAP transport calculations. The resulting statistics of reactor operation parameters reflects their uncertainty due to nuclear data uncertainties.\n\n\n\n\n\n\\subsection{MOCABA: Bayesian updating of predictions}\n\\label{sect::MOCABA}\n\n\n\n\nMOCABA is a mathematical framework developed by AREVA GmbH to combine Monte Carlo sampling and Bayesian updating in order to achieve improved predictions of integral functions of nuclear data~\\citep{mocaba}.\nIn this framework, the vector of application case variables ${\\mathbf{y}}_A$, i.e.~the variables we want to predict, and the vector\nof benchmark variables ${\\mathbf{y}}_B$, for which measurements are available, are collected in a combined vector \n\n\n", "index": 1, "text": "\\begin{align}\n  {\\mathbf{y}} &\\,=\\, \\left( {\\mathbf{y}}_A^T  , {\\mathbf{y}}_B^T \\right)^T .\n\\label{eq::partition_y}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{y}}\" display=\"inline\"><mi>\ud835\udc32</mi></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\,=\\,\\left({\\mathbf{y}}_{A}^{T},{\\mathbf{y}}_{B}^{T}\\right)^{T}.\" display=\"inline\"><mrow><mrow><mi mathvariant=\"normal\">\u2009</mi><mo rspace=\"4.2pt\">=</mo><msup><mrow><mo>(</mo><msubsup><mi>\ud835\udc32</mi><mi>A</mi><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc32</mi><mi>B</mi><mi>T</mi></msubsup><mo>)</mo></mrow><mi>T</mi></msup></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.08036.tex", "nexttext": "\n\nMeasurements of the benchmark vector ${\\mathbf{y}}_B$ and/or linear constraints on ${\\mathbf{y}}$, represented by a vector ${\\mathbf{v}}$, are expressed in terms of a likelihood function ${\\rm p}({\\mathbf{v}} \\,|\\, {\\mathbf{y}})$. To add this information to the prior information, Bayes' theorem is applied, which yields the posterior distribution:\n\n\n", "itemtype": "equation", "pos": 14736, "prevtext": "\n\nThe fact that the application case and benchmark variables are known with limited precision is taken into account by treating ${\\mathbf{y}}$ as a random vector, and the prior distribution $\\rm p({\\mathbf{y}})$, reflecting the uncertainty of ${\\mathbf{y}}$ due to uncertainties of nuclear data, technological parameters and operational parameters, is assessed through Mon\\-te Carlo sampling of these parameters and subsequent computation of ${\\mathbf{y}}$ for each random sample. Then $\\rm p({\\mathbf{y}})$ is estimated from the resulting statistics of ${\\mathbf{y}}$ computations. For a normal distribution model, this means to estimate the prior mean vector ${\\mathbf{y}}_0$ and the prior covariance matrix ${\\boldsymbol{\\Sigma}}_0$ from the Monte Carlo data~\\citep{mocaba}:\\footnote{A more general class of distribution models can be accessed by making use of invertible variable transformations~\\citep{mocaba}.}\n\n\n", "index": 3, "text": "\\begin{align}\n  {\\mathbf{y}}_0 & \\,=\\, \\left( {\\mathbf{y}}_{0A}^T  , {\\mathbf{y}}_{0B}^T \\right)^T \\,,\\quad\n {\\boldsymbol{\\Sigma}}_0 \\,=\\,\n\\left(\n \\begin{matrix}\n {\\boldsymbol{\\Sigma}}_{0A} & {\\boldsymbol{\\Sigma}}_{0AB} \\\\\n {\\boldsymbol{\\Sigma}}_{0AB}^T & {\\boldsymbol{\\Sigma}}_{0B}\n \\end{matrix}\n \\right) .\n\\label{eq::prior_mp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{y}}_{0}\" display=\"inline\"><msub><mi>\ud835\udc32</mi><mn>0</mn></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\,=\\,\\left({\\mathbf{y}}_{0A}^{T},{\\mathbf{y}}_{0B}^{T}\\right)^{T}%&#10;\\,,\\quad{\\boldsymbol{\\Sigma}}_{0}\\,=\\,\\left(\\begin{matrix}{\\boldsymbol{\\Sigma}%&#10;}_{0A}&amp;{\\boldsymbol{\\Sigma}}_{0AB}\\\\&#10;{\\boldsymbol{\\Sigma}}_{0AB}^{T}&amp;{\\boldsymbol{\\Sigma}}_{0B}\\end{matrix}\\right).\" display=\"inline\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u2009</mi><mo rspace=\"4.2pt\">=</mo><mpadded width=\"+1.7pt\"><msup><mrow><mo>(</mo><msubsup><mi>\ud835\udc32</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>A</mi></mrow><mi>T</mi></msubsup><mo>,</mo><msubsup><mi>\ud835\udc32</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>B</mi></mrow><mi>T</mi></msubsup><mo>)</mo></mrow><mi>T</mi></msup></mpadded></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mpadded width=\"+1.7pt\"><msub><mi>\ud835\udeba</mi><mn>0</mn></msub></mpadded><mo rspace=\"4.2pt\">=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msub><mi>\ud835\udeba</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>A</mi></mrow></msub></mtd><mtd columnalign=\"center\"><msub><mi>\ud835\udeba</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>B</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeba</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>A</mi><mo>\u2062</mo><mi>B</mi></mrow><mi>T</mi></msubsup></mtd><mtd columnalign=\"center\"><msub><mi>\ud835\udeba</mi><mrow><mn>0</mn><mo>\u2062</mo><mi>B</mi></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.08036.tex", "nexttext": "\n\nFor a normal distribution model, the Bayesian updating replaces the prior mean vector ${\\mathbf{y}}_0$ and the prior covariance matrix ${\\boldsymbol{\\Sigma}}_0$ by the posterior mean vector ${\\mathbf{y}}^*$ and the posterior covariance matrix ${\\boldsymbol{\\Sigma}}^*$:\n\n\n", "itemtype": "equation", "pos": 15428, "prevtext": "\n\nMeasurements of the benchmark vector ${\\mathbf{y}}_B$ and/or linear constraints on ${\\mathbf{y}}$, represented by a vector ${\\mathbf{v}}$, are expressed in terms of a likelihood function ${\\rm p}({\\mathbf{v}} \\,|\\, {\\mathbf{y}})$. To add this information to the prior information, Bayes' theorem is applied, which yields the posterior distribution:\n\n\n", "index": 5, "text": "\\begin{align}\n{\\rm p}({\\mathbf{y}} \\,|\\, {\\mathbf{v}}) & \\,\\propto\\, {\\rm p}({\\mathbf{v}}\\, |\\, {\\mathbf{y}}) \\, {\\rm p}({\\mathbf{y}})\\,.\n\\label{eq::post_y}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\rm p}({\\mathbf{y}}\\,|\\,{\\mathbf{v}})\" display=\"inline\"><mrow><mi mathvariant=\"normal\">p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc32</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\ud835\udc2f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\,\\propto\\,{\\rm p}({\\mathbf{v}}\\,|\\,{\\mathbf{y}})\\,{\\rm p}({%&#10;\\mathbf{y}})\\,.\" display=\"inline\"><mrow><mi mathvariant=\"normal\">\u2009</mi><mo rspace=\"4.2pt\">\u221d</mo><mi mathvariant=\"normal\">p</mi><mrow><mo stretchy=\"false\">(</mo><mpadded width=\"+1.7pt\"><mi>\ud835\udc2f</mi></mpadded><mo rspace=\"4.2pt\" stretchy=\"false\">|</mo><mi>\ud835\udc32</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mi mathvariant=\"normal\">p</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc32</mi><mo rspace=\"4.2pt\" stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.08036.tex", "nexttext": "\n\nThe impact of the benchmark measurements on the prediction of the application case variables is determined by the matrix ${\\boldsymbol{\\Sigma}}_{0AB}$ of prior covariances between application case and benchmark variables. If the corresponding correlations are close to 1, measurements of ${\\mathbf{y}}_B$ have the potential to significantly improve the prediction of ${\\mathbf{y}}_A$. This is generally the case if application case and benchmarks are similar, in the sense that they show similar responses to variations in the input parameters, in particular nuclear data. Due to its general formulation, the MOCABA framework can be applied to the prediction of any integral observable, and any integral measurement can be used to update the predictions. As mentioned above, MOCABA can also be used for non-perturbative updating of nuclear data libraries. For a more detailed description see \\citep{mocaba}.\n\n\n\n\n\\subsection{Methodology of reactor cycle prediction}\n\\label{sect::methodology}\n\n\n\n\nIn the following, SEANAP, NUDUNA and MOCABA are used in combination to improve the predictions of reactor operation parameters in a given burnup cycle of a PWR (Cycle~B) based on measurements and simulations of the previous cycle (Cycle~A). \n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0002_methodology.pdf}\n    \\caption[]{\\label{fig::methodology} Scheme of the methodology.}\n  \\end{center}\n\\end{figure}\n\n\n{{Figure}}{}~\\ref{fig::methodology} shows a scheme of the applied methodology, which is divided in three steps. \nFirst, a sufficient number (we use 200) of random nuclear data libraries are generated with NUDUNA, using covariance data from the ENDF/B-VII.1 evaluation \\citep{endfb71} for the most important isotopes, being \\textsuperscript{235}U, \\textsuperscript{238}U, \\textsuperscript{239}Pu, \\textsuperscript{1}H, \\textsuperscript{16}O and \\textsuperscript{10}B. These random samples are combined with the reference library of WIMS using the  WILLIE code \\citep{WILLIE}, yielding a set of random nuclear data libraries to be used in core simulations with SEANAP, which is the second step. The core simulation results are then used in a statistical analysis to estimate the prior mean values and covariances of the Cycle~A and Cycle~B observables.\n\nIn the last step, measurement information of Cycle~A is added by applying MOCABA updating to the prior Cycle~A and Cycle~B predictions. Hence, Cycle~A is used as a benchmark to improve the prediction of Cycle~B.\n\n\n\n\\section{Applications} \n\\label{sect::applications}\n\n\n\n\nHere we consider two different reactor observables: the boron letdown curve, defining the time-dependent critical boron concentration in the reactor coolant during a burnup cycle, and the fuel assembly-wise reactor power distribution. For the boron letdown-curve, the components of the integral observable vectors are the boron concentrations, and for the reactor power distribution they are the power values of the individual fuel assemblies in the reactor core, at different times during the burnup  cycle.\n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.3\\textwidth]{fig_0003_graphic.png}\n    \\caption[]{\\label{fig::refueling_pattern} Refueling pattern for the two cycles In blue: fresh fuel without burnable absorber. In red: \n\tfresh fuel with burnable absorber, the number indicates the number of rods with poison. In white: old fuel.}\n  \\end{center}\n\\end{figure}\n\nThe analyzed reactor contains 157 fuel assemblies, with a rated thermal power of 2775 MWt, using fresh and previously used fuel, and WABA rods as burnable absorber. The refueling pattern for both cycles is quite similar, although the number of burnable absorber rods in the fuel assemblies is higher in Cycle~A. The refueling patterns for both cycles are presented in {{Figure}}{}~\\ref{fig::refueling_pattern}, where all fresh fuel assemblies have an initial enrichment of 3.6~w/o, except for the fuel assemblies with 20 WABA rods in Cycle~A, whose enrichment is 3.24 w/o.\n\nMeasurements at 10 different burnup points of Cycle~A are used to update the Cycle~B predictions for 12 different burnup values. It is important to emphasize that only measurements of Cycle~A are used in the updating procedure, and comparisons of Cycle~B predictions to Cycle~B measurements are only made afterwards to assess the predictive power of the methodology. The uncertainty in the boron concentration measurements is 6~ppm,\\footnote{The boron concentration values presented here refer to mass ratios of natural boron in the reactor coolant.} that is 0.6~\\% at the beginning of the cycle (BOC); and the uncertainty in the power measurements is 5~\\%.\n\n\n\n\n\n\\subsection{Boron letdown curve of a burnup cycle}\n\\label{sect::appBoron}\n\n\n\n\n\\begin{table*}[htb!]\n  \\begin{center}\n    \\caption{\\label{tab::boronupd}Boron concentrations for Cycle~B as a function of burnup.}\n    \\footnotesize\n    \\begin{tabular}{ccccccc}\n    \\toprule\n    Burnup (GWd/t) & $BC_{prior}$ & $\\sigma_{prior}$ & $BC_{posterior}$ & $\\sigma_{posterior}$ & $\\sigma_{prior}/\\sigma_{posterior}$ & $B_{measured}$\\\\\n    \\midrule\n    0.13  & 986   & 46    & 986   & 4.2  & 11 & 983 \\\\\n    1.34  & 867   & 45    & 868   & 3.3  & 13 & 874 \\\\  \n    2.49  & 763   & 44    & 767   & 2.7  & 16 & 771 \\\\ \n    2.84  & 733   & 44    & 737   & 2.6  & 17 & 740 \\\\\n    3.59  & 664   & 43    & 668   & 2.4  & 18 & 672 \\\\  \n    4.44  & 590   & 43    & 595   & 2.3  & 18 & 596 \\\\ \n    5.55  & 487   & 42    & 494   & 2.2  & 19 & 496 \\\\\n    6.69  & 383   & 42    & 391   & 2.4  & 18 & 394 \\\\  \n    7.72  & 290   & 42    & 298   & 2.6  & 16 & 402 \\\\ \n    8.82  & 197   & 41    & 206   & 2.8  & 15 & 202 \\\\ \n    10.28 & 74    & 41    & 85    & 3.2  & 13 & 87 \\\\ \n    11.35 & -15   & 40    & -4    & 3.5  & 11 & 3 \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table*}\n\nThe evolution of the critical boron concentration during a reactor cycle is a measure of the cycle length. Hence, its prediction is essential for reactor operation planning. {{Figure}}{}~\\ref{fig::priorboron} shows the prior uncertainty in the predicted critical boron concentration of Cycle B due to nuclear data, obtained by application of the NUDUNA random libraries to the SEANAP calculations.\n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0004_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::priorboron} Prior results for the standard deviation of the boron concentration for Cycle~B as a function of burnup.}\n  \\end{center}\n\\end{figure}\n\n\nThe total standard deviation due to all considered isotopes decreases slightly with burnup from a maximum value of 46~ppm at BOC to a value of 40~ppm at the end of the cycle (EOC).\n\n\\textsuperscript{238}U is  the principal uncertainty contributor for burnup values up to 8~GWd/t heavy metal. Its contribution decreases continuously from 36~ppm at BOC to 25~ppm at EOC. The uncertainties due to \\textsuperscript{235}U and \\textsuperscript{239}Pu have opposite behaviours. \\textsuperscript{235}U is consumed during the cycle, so its importance is decreasing, with an effect of \n22~ppm at BOC and 14~ppm at EOC. On the other hand, \\textsuperscript{239}Pu is formed from \\textsuperscript{238}U during the cycle, so its uncertainty \ncontribution increases from 19~ppm at BOC to 29~ppm at EOC.\n\nThe remaining materials have a much lower impact on the uncertainty of the boron concentration. Hydrogen and oxygen (in moderator and fuel) provide a basically constant uncertainty contribution of approx.~1~ppm and 5~ppm, respectively;  \\textsuperscript{10}B causes 1~ppm uncertainty at BOC, and its impact is fully negligible at EOC.\n\nA similar analysis of the prior uncertainties in the boron concentration was performed by \\cite{SeanapUncert} with the Total Monte Carlo (TMC) code \\citep{koning_tmc}, which was based on the TENDL-2012 nuclear data evaluation \\citep{talys}. Comparing the results derived by \\cite{SeanapUncert} to the results presented above, shows that uncertainties in the boron concentration are much larger for TENDL-2012/TMC than for ENDF/B-VII.1/NUDUNA, and also the relative importance of the isotopes differs. This confirms that the choice of a nuclear data evaluation may have a large impact on predictions and their corresponding uncertainties, as has already been demonstrated by \\cite{CJDiezComparison}. \n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0005_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::nucdatupd}Differences between measured boron concentrations and respective prior predictions, posterior predictions, and predictions obtained with a MOCABA-updated nuclear data library. The error bars represent the respective 1$\\sigma$ uncertainties.}\n  \\end{center}\n\\end{figure}\n\n\nFinally, Bayesian updating is applied to the prior boron letdown curve with the MOCABA procedure, taking into account measurements and simulations of the previous cycle, i.e.~Cycle A. The corresponding outcomes are shown in Table~\\ref{tab::boronupd} and {{Figure}}{}~\\ref{fig::nucdatupd}. Two conclusions can be drawn from these results. First, for all burnup values, considering the previous cycle measurements leads to a better prediction of the boron letdown curve. Second, the uncertainty in the boron calculation is reduced by one order of magnitude, from values higher than 40~ppm to values around 3~ppm, which means a huge improvement for the prediction of a reactor cycle and, hence, for reactor operation planning.\n\nThe impressive improvement in the prediction of the boron letdown curve by including measurements of the previous cycle is explained by {{Figure}}{}~\\ref{fig::correlations}. It shows large correlations between the boron values of Cycle~A and Cycle~B in the range 0.92 - 0.99, which reflects a high similarity between application case and benchmark, i.e.~Cycle B and Cycle A.\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0006_graphic.png}\n    \\caption[]{\\label{fig::correlations} Correlation matrix of boron concentration values for Cycle~A and Cycle~B at different burnup steps.}\n  \\end{center}\n\\end{figure}\n\n\n{{Figure}}{}~\\ref{fig::app7convergence} shows the convergence of the posterior boron concentration at a burnup of 7.72~GWd/t as a function of the number of benchmarks included in the Bayesian updating. The uncertainty is reduced with increasing number of benchmarks taken into account and thus the amount of information considered. Already including a single benchmark leads to a dramatic uncertainty reduction. Adding further measurements includes additional information, which leads to even better predictions. However, the simulation results for the different burnup steps are correlated, and thus the amount of new information of an additional measurement is limited. Hence, the impact of the first measurement is most prominent.\n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0007_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::app7convergence}Convergence of the MOCABA updating procedure for increasing numbers of considered benchmarks for a burnup of 7.72~GWd/t.}\n  \\end{center}\n\\end{figure}\n\n\n\\subsection{Effects of nuclear data updating on boron letdown curve}\nThere is a different way of obtaining updated boron concentrations using MOCABA, which is updating the WIMS nuclear data library using the measurements of Cycle~A, and performing after that the SEANAP simulation for Cycle~B with the updated library as input. Here, we update the information of \\textsuperscript{235}U, \\textsuperscript{238}U, \\textsuperscript{239}Pu and \\textsuperscript{1}H in the 69 energy groups. In {{Figure}}{}~\\ref{fig::nucdatupd} the results obtained with direct updating of the boron letdown curve are compared to results obtained with the new nuclear data library. Both methods give consistent results which differ by less than 2~ppm.\n\nNote that our study does not include other uncertainty contributions than nuclear data uncertainties, e.g., no uncertainties related to technological parameters or to neutronics and thermal hydraulic models are taken into account. This ignorance could result in adapting nuclear data in order to balance deficiencies in the description of other input values, which might result in unphysical updates of the nuclear data. To check this effect, we compare in {{Figure}}{}~\\ref{fig::Pu9upd} the differences between original and updated library and normalize the difference to the standard deviation of the original library. The largest deviations can be observed for the \\textsuperscript{239}Pu fission cross sections, and the modifications are most prominent in the low-energy groups. However, the differences are always smaller than the standard deviation of the original values and can be explained by its uncertainties. Nevertheless, the update could compensate other deficiencies, and thus it is to be checked in future studies whether the updated library obtained for one specific reactor also improves the description of other reactors.\n\n\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0008_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::Pu9upd} Cross section updates normalized to one standard deviation.}\n  \\end{center}\n\\end{figure}\n\n\n\n\n\n\\subsection{Power per fuel assembly in a burnup cycle}\n\\label{sect::appPower}\n\n\n\n\n\n{{Figure}}{}~\\ref{fig::powerBOCEOC} shows on the left-hand side NUDUNA/\\linebreak SEANAP uncertainty estimates for the relative power per fuel assembly in Cycle~B at BOC and EOC. Here, the relative power values are the respective fuel assembly-wise power values divided by the mean power per fuel assembly in the reactor core. The uncertainty is largest in the center of the core and near the boundary, as has also been observed by \\cite{santamarinapowermaps} and attributed to the radial swing of flux. \n\n\\begin{figure}[]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0009_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::powermean2842} Relative deviations of the prior and posterior powers per fuel assembly of Cycle~B to its measurements at 2842~MWd/t.}\n  \\end{center}\n\\end{figure}\n\n\\begin{figure*}[]\n  \\begin{center}\n    \\includegraphics[width=0.7\\textwidth]{fig_0010_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::powerBOCEOC}Relative standard deviations for prior and posterior powers per fuel assembly for cycle burnups of 2842~MWd/t (top) and 10284~MWd/t (bottom) during Cycle~B.}\n  \\end{center}\n\\end{figure*}\nThe relative powers per fuel assembly have been determined during Cycle~A for each of the 47 fuel assembly positions in the considered quarter of the core and for 10 burnup steps, so there exist 470 power benchmark values. The right-hand graphs of {{Figure}}{}~\\ref{fig::powerBOCEOC} show the Cycle~B posterior uncertainties, which have been obtained with MOCABA based on the 470 Cycle~A benchmarks. The maximum uncertainty reduction amounts to approximately 50~\\%. The average reduction depends on burnup (for 2842~MWd/t: 47~\\%, for 10284~MWd/t: 29~\\%), due to the fact that Cycle~A was shorter than Cycle~B and, consequently, the  correlations between Cycle~A and Cycle~B EOC simulations turn out to be less prominent. \n\n{{Figure}}{}~\\ref{fig::powermean2842} compares the simulation results to the Cycle~B measurements. However, due to the low precision of the measurements, it cannot be judged whether the posterior or prior estimates give a better description. Both are consistent with the measurements.\n\nThe analysis shows that the uncertainty reduction for the power predictions by MOCABA is much less favorable than for the boron letdown curve predictions. This feature results from the high uncertainties in the power measurements of 5~\\%, which are far less precise than the boron measurements (6 ppm $\\equiv$ 0.6~\\%  uncertainty at BOC). The low precision of measurements can also not be balanced by their large quantity (470 power measurements instead of 10 boron concentration measurements). \n\n\\subsection{Combining benchmarks and predictions of different responses}\nWith MOCABA, the prediction of a response can be improved by measurements of any benchmark observable. Thus it is also possible to update the power predictions by the boron concentration measurements and vice versa. In order to study this feature, we consider three scenarios for improving the power predictions:\n\\begin{itemize}\n\t\\item Updating based on power measurements (see previous section),\n\t\\item Updating based on boron concentration measurements,\n\t\\item Updating based on all measurements.\n\\end{itemize}\n{{Figure}}{}~\\ref{fig::powerWithAll} shows the results for a burnup of 2842~MWd/t during Cycle~B. If the power distributions are updated only with boron concentration measurements, then the uncertainty is reduced in maximum by 33~\\% and on average by 13~\\%. This rather modest improvement is explained by fairly low correlations of local power and boron concentration values. If the update considers both boron concentration and power measurements, then the uncertainty reduction amounts to on average 53~\\% which is approx.~6~\\% better than for considering only the power measurements. This demonstrates that different responses can be combined in the Bayesian updating, and all of them can provide useful information.\n\n\\begin{figure}[ht!]\n  \\begin{center}\n    \\includegraphics[width=0.45\\textwidth]{fig_0011_graphic-eps-converted-to.pdf}\n    \\caption[]{\\label{fig::powerWithAll} Relative standard deviation of the power of each fuel assembly at 2842 MWd/t. Updates performed using only power per fuel assembly measurements, only boron concentration measurements, and both.}\n  \\end{center}\n\\end{figure}\n\n\n\n}\n\n\n\n\\section{Conclusions} \n\\label{sect::conclusions}\n\n\n\n\nThe nuclear data Monte Carlo code NUDUNA has been applied to PWR core simulations with SEANAP to provide both best estimates and their uncertainties for the boron letdown curve and the fuel assembly-wise power distribution. Next, the predictions for a given reactor cycle (Cycle~B) have been updated by applying the Bayesian inference model MOCABA, utilizing measurement information obtained in the preceding reactor cycle (Cycle~A). The resulting updated best estimates and their uncertainties have been compared afterwards to the measurements during Cycle~B in order to verify the predictive power of the procedure.\n\nFor the boron letdown curve, the MOCABA updating leads to major improvements for the best estimates and to massive uncertainty reductions. The nuclear-data-induced uncertainty of 40 to 45~ppm is reduced to 2 to 4~ppm after applying MOCABA, i.e.~to up to a 20 times lower uncertainty. \n\nFor the power per fuel assembly, the 5~\\% relative uncertainty of the power measurements of Cycle~A limits the possible reduction of the prediction uncertainty of Cycle~B. Still, MOCABA provides a major uncertainty reduction of 53~\\% on average. This analysis also demonstrates the ability of the MOCABA methodology to combine simulations and measurements of different response variables.\n\nMOCABA has also been applied to generate an updated nuclear data library in WIMS format based on the boron concentration measurements of Cycle~A. Computing the boron letdown curve for Cycle~B with this updated library gives results that are almost identical to the ones obtained by direct MOCABA updating of the boron concentrations (differences are smaller than 2 ppm). The nuclear data in the resulting library lie within the one-standard-deviation range of the original data, and thus are compatible with them. However, the updating might also compensate for calculation code deficiencies or technological parameter uncertainties of the considered PWR plant. Future studies need to address the question whether the updated nuclear data obtained for one specific reactor with one specific reactor code suite can improve the description of other analyzes.\n\nThe blind tests presented in this paper show very good performance of the NUDUNA/MOCABA best estimate plus uncertainty methodology. The obtained gain in precision, especially for the boron letdown curve, is impressive and promises major economical benefits. Compared to the traditional perturbative GLLS method, our Monte Carlo-Bayes procedure has four major advantages: it can be easily implemented since the transport codes used for the computation of the integral observables of interest can be treated as black boxes; it can easily address any integral function of nuclear data, such as boron concentration, power per fuel assembly, axial offset, and peak factors; integral observables can be directly updated without taking the detour via nuclear data updating, although also updated nuclear data libraries can be generated; and, due to its non-perturbative nature, it is not limited to the regime where nuclear data uncertainties are small.\n\n\n\n\n\n\n\n\n\\section*{Acknowledgements}\nThis work was supported by AREVA GmbH and conducted in the framework of the agreement in the area of Propagation of Uncertainties for Neutronic Calculations in Criticality Safety Analysis between the Spanish Nuclear Safety Council (CSN) and Universidad Polit\\'ecnica de Madrid (UPM).\n\n\n\n\n\n\n\n\n\n\n\\section*{References}\n\\bibliographystyle{elsarticle-harv}\n\\bibliography{nuduna_mocaba_seanap}\n\n\n", "itemtype": "equation", "pos": 15869, "prevtext": "\n\nFor a normal distribution model, the Bayesian updating replaces the prior mean vector ${\\mathbf{y}}_0$ and the prior covariance matrix ${\\boldsymbol{\\Sigma}}_0$ by the posterior mean vector ${\\mathbf{y}}^*$ and the posterior covariance matrix ${\\boldsymbol{\\Sigma}}^*$:\n\n\n", "index": 7, "text": "\\begin{align}\n{\\mathbf{y}}^* \\,=\\, \\left( {\\mathbf{y}}_A^{*T}  , {\\mathbf{y}}_B^{*T} \\right)^T\\,,\\quad\n{\\boldsymbol{\\Sigma}}^* \\,=\\,\n\\left(\n\\begin{matrix}\n{\\boldsymbol{\\Sigma}}_A^* & {\\boldsymbol{\\Sigma}}_{AB}^* \\\\\n{\\boldsymbol{\\Sigma}}_{AB}^{*T} & {\\boldsymbol{\\Sigma}}_B^*\n\\end{matrix}\n\\right).\n\\label{eq::posterior_mp}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{y}}^{*}\\,=\\,\\left({\\mathbf{y}}_{A}^{*T},{\\mathbf{y}}_{B}%&#10;^{*T}\\right)^{T}\\,,\\quad{\\boldsymbol{\\Sigma}}^{*}\\,=\\,\\left(\\begin{matrix}{%&#10;\\boldsymbol{\\Sigma}}_{A}^{*}&amp;{\\boldsymbol{\\Sigma}}_{AB}^{*}\\\\&#10;{\\boldsymbol{\\Sigma}}_{AB}^{*T}&amp;{\\boldsymbol{\\Sigma}}_{B}^{*}\\end{matrix}%&#10;\\right).\" display=\"inline\"><mrow><mrow><mrow><mpadded width=\"+1.7pt\"><msup><mi>\ud835\udc32</mi><mo>*</mo></msup></mpadded><mo rspace=\"4.2pt\">=</mo><mpadded width=\"+1.7pt\"><msup><mrow><mo>(</mo><msubsup><mi>\ud835\udc32</mi><mi>A</mi><mrow><mi/><mo>*</mo><mi>T</mi></mrow></msubsup><mo>,</mo><msubsup><mi>\ud835\udc32</mi><mi>B</mi><mrow><mi/><mo>*</mo><mi>T</mi></mrow></msubsup><mo>)</mo></mrow><mi>T</mi></msup></mpadded></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mpadded width=\"+1.7pt\"><msup><mi>\ud835\udeba</mi><mo>*</mo></msup></mpadded><mo rspace=\"4.2pt\">=</mo><mrow><mo>(</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeba</mi><mi>A</mi><mo>*</mo></msubsup></mtd><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeba</mi><mrow><mi>A</mi><mo>\u2062</mo><mi>B</mi></mrow><mo>*</mo></msubsup></mtd></mtr><mtr><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeba</mi><mrow><mi>A</mi><mo>\u2062</mo><mi>B</mi></mrow><mrow><mi/><mo>*</mo><mi>T</mi></mrow></msubsup></mtd><mtd columnalign=\"center\"><msubsup><mi>\ud835\udeba</mi><mi>B</mi><mo>*</mo></msubsup></mtd></mtr></mtable><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]