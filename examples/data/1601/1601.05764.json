[{"file": "1601.05764.tex", "nexttext": " The bias of a hypothesis $h$ is the\nsame quantity with $h(x)$ replacing $l(x)$.  If a hypothesis has low bias in\nabsolute value we say it achieves \\emph{statistical parity.} Note that $S$\nrepresents the group we wish to protect from discrimination, and the bias\nrepresents the degree to which they have been discriminated against.  The sign\nof bias indicates whether $S$ or $S^C$ is discriminated against.  A similar\nstatistical measure called \\emph{disparate impact} was introduced and studied\nby Friedler et al.~\\cite{FriedlerSV14} based on the ``$80\\%$ rule'' used in\nUnited States hiring law.\n\nDwork~et~al.~\\cite{DworkHPR12} point out that statistical parity is only a\nmeasure of population-wide fairness. They provide a laundry list\nof ways one could achieve statistical parity while still exhibiting serious and\nunlawful discrimination. In particular, one can achieve statistical parity by\nflipping the labels of a certain number of arbitrarily chosen members of the\ndisadvantaged group, regardless of the relation between the individuals and the\nclassification task. In our experiments we show this already outperforms some\nof the leading algorithms in the fairness literature.\n\nDespite this, it is important to study the ability for learning algorithms to\nachieve statistical parity.  For example, it might be\nreasonable to flip the labels of the ``most qualified'' individuals of the\ndisadvantaged group who are classified negatively. Some previous approaches\nassume the existence of a ranking or metric on individuals, or try to learn\nthis ranking from data~\\cite{KamiranC09,DworkHPR12}. By contrast, our SDB\nachieves statistical parity without the need for such a ranking.\n\n\\emph{$kNN$-consistency:} The second notion, due to~\\cite{DworkHPR12}, calls a\nclassifier ``individually fair'' if it classifies similar individuals\nsimilarly. They use $k$-nearest-neighbor to measure the consistency of labels\nof similar individuals. Note that ``closeness'' is defined with respect to a\nmetric chosen as part of the data cleaning and feature selection process. By\ncontrast SDB does not require a metric on individuals.\n\n\\subsection{Previous work on fair algorithms} Learning algorithms studied\npreviously in the context of fairness include naive Bayes~\\cite{CaldersV10},\ndecision trees~\\cite{KamiranCP10}, and logistic\nregression~\\cite{KamishimaAAS12}.  To the best of our knowledge we are the\nfirst to study boosting and SVM in this context, and our confidence-based\nanalysis is new for both these and logistic regression. \n\nThe two main approaches in the literature are massaging and regularization.\nMassaging means changing the biased dataset before training to remove the bias\nin the hope that the learning algorithm trained on the now unbiased data will\nbe fair.  Massaging is done in the previous literature based on a ranking\nlearned from the biased data~\\cite{KamiranC09}. The regularization\napproach consists of adding a regularizer to an optimization objective which\npenalizes the classifier for discrimination~\\cite{KamashimaAS11}. While SDB can\nbe thought of as a post-processing regularization, it does so in a way that\nmakes the trade-off between bias and accuracy transparent and easily\ncontrolled. \n\nThere are two other notable approaches in the fairness literature. The first,\nintroduced in \\cite{DworkHPR12}, is a framework for maximizing the utility of a\nclassification with the constraint that similar people be treated similarly.\nOne shortcoming of this approach is that it relies on a metric on the data that\ntells us the similarity of individuals with respect to the classification task.\nMoreover, the work in~\\cite{DworkHPR12} suggests that learning a suitably fair\nsimilarity metric from the data is as hard as the original problem of finding a\nfair classifier. Our SDB method does not require such a metric.\n\nThe ``Learning Fair Representations'' method of Zemel et al.~\\cite{ZemelWSPD13}\nformulates the problem of fairness in terms of intermediate representations:\nthe goal is to find a representation of the data which preserves as much\ninformation as possible from the original data while simultaneously obfuscating\nmembership in the protected class.  Given that in this paper we seek to make\nexplicit the trade-off between bias and accuracy, we will not be able to hide\nmembership in the protected class as Zemel et al.~seeks to do. Rather, we align\nwith the central thesis of~\\cite{DworkHPR12}, that knowing the protected\nfeature is useful to promote fairness. \n\n\\subsection{Margins}\n\nThe theory of margins has provided a deep, foundational explanation for the\ngeneralization properties of algorithms such as AdaBoost and soft-margin\nSVMs~\\cite{CortesV95,SchapireFBL98}. A hypothesis $f: X \\to [-1,1]$ induces a\n\\emph{margin} for a labeled example $\\textup{margin}_f(x,y) = y\\cdot f(x)$,\nwhere $x\\in X$ is a data point and $y\\in\\{-1,1\\}$ is the correct label for $x$.\nThe sign of the margin is positive if and only if $f$ correctly labels $x$, and\nthe magnitude indicates how confident $f$ is in its prediction.\n\nAs an example of the power of margins, we quote a celebrated theorem on the\ngeneralization accuracy of weighted majority voting schemes in PAC-learning.\nHere a weighted majority vote is a function $f(x) = \\sum_{i=1}^N \\alpha_i\nh_i(x)$ for some hypotheses $h_i \\in H$ and $\\alpha_i \\geq 0, \\sum_i \\alpha_i =\n1$.\n\n\\begin{theorem}[Schapire et al.~\\cite{SchapireFBL98}]\\label{thm:margin-generalization}\nLet $D$ be a distribution over $X \\times \\{ -1,1\\}$ and $S$ be a sample of $m$\nexamples chosen i.i.d. at random according to $D$. Let $H$ be a set of\nhypotheses of VC-dimension $d$. Then for any $\\delta > 0$, with probability at\nleast $1-\\delta$ \\emph{every} weighted majority voting scheme satisfies the\nfollowing for every $\\theta > 0$:\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract} \n\nWe study three classical machine learning algorithms in the context of\nalgorithmic fairness: adaptive boosting, support vector machines, and logistic\nregression.  Our goal is to maintain the high accuracy of these learning\nalgorithms while reducing the degree to which they discriminate against\nindividuals because of their membership in a protected group. \n\nOur first contribution is a method for achieving fairness by shifting the decision\nboundary for the protected group.  The method is based on the theory of margins\nfor boosting.  Our method performs comparably to or outperforms previous algorithms\nin the fairness literature in terms of accuracy and low discrimination,\nwhile simultaneously allowing for a fast and transparent quantification\nof the trade-off between bias and error. \n\nOur second contribution addresses the shortcomings of the bias-error trade-off\nstudied in most of the algorithmic fairness literature. We demonstrate that\neven hopelessly naive modifications of a biased algorithm, which cannot be\nreasonably said to be fair, can still achieve low bias and high accuracy.  To\nhelp to distinguish between these naive algorithms and more sensible algorithms\nwe propose a new measure of fairness, called \\emph{resilience to random bias}\n(RRB). We demonstrate that RRB distinguishes well between our naive and\nsensible fairness algorithms.  RRB together with bias and accuracy provides a\nmore complete picture of the fairness of an algorithm. \n\n\\end{abstract}\n\n\\section{Background and Motivation} \\label{sec:background}\n\n\\subsection{Motivation}\n\nMachine learning algorithms assume an increasingly large role in making\ndecisions across many different areas of industry, finance, and government,\nfrom facial recognition and social network analysis to self-driving cars to\ndata-based approaches in commerce, education, and policing. The decisions made\nby algorithms in these domains directly affect individual people, and not\nalways for the better.  Consequently, there has been a growing concern that\nmachine learning algorithms, which are often poorly understood by those that\nuse them, make discriminatory decisions.\n\nIf the data used for training the algorithm is biased, a machine learning\nalgorithm will learn the bias and perpetuate discriminatory decisions against\ngroups that are protected by law, even in the absence of ``discriminatory\nintent'' by the designers. A typical example is an algorithm serving predatory\nads to protected groups. Such issues resulted in a 2014 report from the US\nExecutive Office~\\cite{PodestaPMHZ14} which voiced concerns about\ndiscrimination in machine learning. The primary question we study in this paper\nis\n\\begin{center}\nHow can we maintain high accuracy of a learning algorithm while reducing\ndiscriminatory biases?\n\\end{center}\nIn this paper we will focus on the issue of biased training data, which is one\nof the several possible causes of discriminatory outcomes in machine learning.\nIn this setting, we have a protected attribute (e.g. race or gender) which we\nassert should be independent from the target attribute.  For example, if the\ngoal is to decide creditworthiness for loans and the protected attribute is\ngender, a classifier's prediction should not correlate with an applicant's\ngender. We say that the classifier achieves \\emph{statistical parity} if the\nprotected subgroup is as likely as the broader population to have a given\nlabel.\n\nOf course, there might be situations where the target label depends on\nlegitimate factors that correlate with the protected attribute. For example,\nif the protected attribute is gender and the target label is income, some argue\nthat lower salaries for women can be partly explained by the fact that on\naverage, men work longer hours than women. In this paper we assume that this\nis not the case. The issue of ``explainable discrimination'' in machine\nlearning was studied in \\cite{KamiranZC13}.\n\nIn our setting, since we only have biased data, we cannot evaluate our\nclassifiers against an unbiased ground truth. In particular only a biased\nclassifier could achieve perfect accuracy; to achieve statistical parity in\ngeneral one must be willing to reduce accuracy. Hence the natural goal is to\nfind a classifier that achieves statistical parity while minimizing error, or\nmore generally to study the trade-off between bias and accuracy so as to make\nfavorable trade-offs.\n\\subsection{Contributions}\nOur first contribution in this paper is a method for optimizing this trade-off\nwhich we call the \\emph{Shifted Decision Boundary} (SDB). SDB is a generic\nmethod based on the theory of margins~\\cite{CortesV95,SchapireFBL98}, and it\ncan be combined with any learning algorithm that produces a measure of\nconfidence in its prediction  (Section~\\ref{sec:sdb}). In particular we combine SDB with boosting,\nsupport vector machines, and logistic regression, and it performs comparably to\nor outperforms previous algorithms in the fair learning literature.  See Section~\\ref{sec:experiments} for its empirical evaluation.\nWe also give a\ntheorem based on the analysis in~\\cite{SchapireFBL98} bounding the loss of\naccuracy for SDB under weighted majority schemes (Section~\\ref{sec:sdbtheory}). \nSDB makes the assumptions on the bias explicit and\ntransparent, so that the trade-off can be understood without a detailed\nunderstanding of the learning algorithm itself. \n\nUnfortunately, studying the bias-error trade-off is an incomplete picture of\nthe fairness of an algorithm. The shortcomings were discussed\nin~\\cite{DworkHPR12}, e.g., in terms of how an adversary could achieve\nstatistical parity while still targeting the protected group unfairly. We\ndemonstrate these shortcomings in action even in the absence of adversarial\nmanipulation. Among other methods, we show that modifying a classifier by\nrandomly flipping certain output labels with a certain probability already\noutperforms much of the prior fairness literature in both accuracy and bias.\nSuch a naive algorithm is obviously unfair because the relabeling is\nindependent of the classification task. Our second contribution is a measure of\nfairness that addresses this shortcoming, which we call \\emph{resilience to\nrandom bias}. We define it in Section~\\ref{sec:rrb} and demonstrate that it\ndistinguishes well between our naive baseline algorithms and SDB.\n\n\\subsection{Existing notions of fairness} \n\nThe study of fairness in machine learning is young, but there has been a lot of\ndisparate work studying notions of what it means for data to be fair. Finding \nthe ``right'' definition of fairness is a major challenge; see the extensive\nsurvey of~\\cite{RomeiR14} for a detailed discussion. Two prominent definitions\nof fairness that have emerged are \\emph{statistical parity} and\n\\emph{$k$-nearest-neighbor consistency.} We review them briefly now.\n\n\\emph{Statistical parity:} Let $D$ be a distribution over a set of labeled\nexamples $X$ with labels $l : X \\to \\{-1, 1\\}$ and a protected subset $S \\subset\nX$. The \\emph{bias} of $l$ with respect to $D$ is defined as the difference in\nprobability of an example in $S$ having label 1 and the probability of an\nexample in $S^C$ having label 1, i.e.  \n", "index": 1, "text": "$$ B(D, S) = \\Pr_{x \\sim D|_{S^C}}[l(x)\n= 1] - \\Pr_{x \\sim D|_{S}}[l(x) = 1].  $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"B(D,S)=\\Pr_{x\\sim D|_{S^{C}}}[l(x)=1]-\\Pr_{x\\sim D|_{S}}[l(x)=1].\" display=\"block\"><mrow><mrow><mrow><mi>B</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mi>Pr</mi><mrow><mi>x</mi><mo>\u223c</mo><msub><mrow><mi>D</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><msup><mi>S</mi><mi>C</mi></msup></msub></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>-</mo><mrow><munder><mi>Pr</mi><mrow><mi>x</mi><mo>\u223c</mo><msub><mrow><mi>D</mi><mo fence=\"true\" stretchy=\"false\">|</mo></mrow><mi>S</mi></msub></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": "\n\\end{theorem}\n\nIn other words, the generalization error is bounded by the probability of a\nsmall margin \\emph{on the sample}. One can go on to show\nAdaBoost~\\cite{SchapireF12}, a popular algorithm that produces a weighted\nvoting scheme, performs well in this respect. Recall that the output of\nAdaBoost is a hypothesis which outputs the sign of a weighted majority vote\n$\\sum_i \\alpha_i, h_i(x)$. Rather than measure the margin we measure the\n\\emph{signed confidence} of the boosting hypothesis on an unlabeled example $x$\nas \n", "itemtype": "equation", "pos": 13066, "prevtext": " The bias of a hypothesis $h$ is the\nsame quantity with $h(x)$ replacing $l(x)$.  If a hypothesis has low bias in\nabsolute value we say it achieves \\emph{statistical parity.} Note that $S$\nrepresents the group we wish to protect from discrimination, and the bias\nrepresents the degree to which they have been discriminated against.  The sign\nof bias indicates whether $S$ or $S^C$ is discriminated against.  A similar\nstatistical measure called \\emph{disparate impact} was introduced and studied\nby Friedler et al.~\\cite{FriedlerSV14} based on the ``$80\\%$ rule'' used in\nUnited States hiring law.\n\nDwork~et~al.~\\cite{DworkHPR12} point out that statistical parity is only a\nmeasure of population-wide fairness. They provide a laundry list\nof ways one could achieve statistical parity while still exhibiting serious and\nunlawful discrimination. In particular, one can achieve statistical parity by\nflipping the labels of a certain number of arbitrarily chosen members of the\ndisadvantaged group, regardless of the relation between the individuals and the\nclassification task. In our experiments we show this already outperforms some\nof the leading algorithms in the fairness literature.\n\nDespite this, it is important to study the ability for learning algorithms to\nachieve statistical parity.  For example, it might be\nreasonable to flip the labels of the ``most qualified'' individuals of the\ndisadvantaged group who are classified negatively. Some previous approaches\nassume the existence of a ranking or metric on individuals, or try to learn\nthis ranking from data~\\cite{KamiranC09,DworkHPR12}. By contrast, our SDB\nachieves statistical parity without the need for such a ranking.\n\n\\emph{$kNN$-consistency:} The second notion, due to~\\cite{DworkHPR12}, calls a\nclassifier ``individually fair'' if it classifies similar individuals\nsimilarly. They use $k$-nearest-neighbor to measure the consistency of labels\nof similar individuals. Note that ``closeness'' is defined with respect to a\nmetric chosen as part of the data cleaning and feature selection process. By\ncontrast SDB does not require a metric on individuals.\n\n\\subsection{Previous work on fair algorithms} Learning algorithms studied\npreviously in the context of fairness include naive Bayes~\\cite{CaldersV10},\ndecision trees~\\cite{KamiranCP10}, and logistic\nregression~\\cite{KamishimaAAS12}.  To the best of our knowledge we are the\nfirst to study boosting and SVM in this context, and our confidence-based\nanalysis is new for both these and logistic regression. \n\nThe two main approaches in the literature are massaging and regularization.\nMassaging means changing the biased dataset before training to remove the bias\nin the hope that the learning algorithm trained on the now unbiased data will\nbe fair.  Massaging is done in the previous literature based on a ranking\nlearned from the biased data~\\cite{KamiranC09}. The regularization\napproach consists of adding a regularizer to an optimization objective which\npenalizes the classifier for discrimination~\\cite{KamashimaAS11}. While SDB can\nbe thought of as a post-processing regularization, it does so in a way that\nmakes the trade-off between bias and accuracy transparent and easily\ncontrolled. \n\nThere are two other notable approaches in the fairness literature. The first,\nintroduced in \\cite{DworkHPR12}, is a framework for maximizing the utility of a\nclassification with the constraint that similar people be treated similarly.\nOne shortcoming of this approach is that it relies on a metric on the data that\ntells us the similarity of individuals with respect to the classification task.\nMoreover, the work in~\\cite{DworkHPR12} suggests that learning a suitably fair\nsimilarity metric from the data is as hard as the original problem of finding a\nfair classifier. Our SDB method does not require such a metric.\n\nThe ``Learning Fair Representations'' method of Zemel et al.~\\cite{ZemelWSPD13}\nformulates the problem of fairness in terms of intermediate representations:\nthe goal is to find a representation of the data which preserves as much\ninformation as possible from the original data while simultaneously obfuscating\nmembership in the protected class.  Given that in this paper we seek to make\nexplicit the trade-off between bias and accuracy, we will not be able to hide\nmembership in the protected class as Zemel et al.~seeks to do. Rather, we align\nwith the central thesis of~\\cite{DworkHPR12}, that knowing the protected\nfeature is useful to promote fairness. \n\n\\subsection{Margins}\n\nThe theory of margins has provided a deep, foundational explanation for the\ngeneralization properties of algorithms such as AdaBoost and soft-margin\nSVMs~\\cite{CortesV95,SchapireFBL98}. A hypothesis $f: X \\to [-1,1]$ induces a\n\\emph{margin} for a labeled example $\\textup{margin}_f(x,y) = y\\cdot f(x)$,\nwhere $x\\in X$ is a data point and $y\\in\\{-1,1\\}$ is the correct label for $x$.\nThe sign of the margin is positive if and only if $f$ correctly labels $x$, and\nthe magnitude indicates how confident $f$ is in its prediction.\n\nAs an example of the power of margins, we quote a celebrated theorem on the\ngeneralization accuracy of weighted majority voting schemes in PAC-learning.\nHere a weighted majority vote is a function $f(x) = \\sum_{i=1}^N \\alpha_i\nh_i(x)$ for some hypotheses $h_i \\in H$ and $\\alpha_i \\geq 0, \\sum_i \\alpha_i =\n1$.\n\n\\begin{theorem}[Schapire et al.~\\cite{SchapireFBL98}]\\label{thm:margin-generalization}\nLet $D$ be a distribution over $X \\times \\{ -1,1\\}$ and $S$ be a sample of $m$\nexamples chosen i.i.d. at random according to $D$. Let $H$ be a set of\nhypotheses of VC-dimension $d$. Then for any $\\delta > 0$, with probability at\nleast $1-\\delta$ \\emph{every} weighted majority voting scheme satisfies the\nfollowing for every $\\theta > 0$:\n\n", "index": 3, "text": "$$\n\\begin{aligned}\\Pr_D[yf(x) \\leq 0] &\\leq \\Pr_S[yf(x) \\leq \\theta] + \\\\ & O \\left (\n\\frac{1}{\\sqrt{m}} \\left ( \\frac{d \\log^2(m/d)}{\\theta^2} + \\log (1/\\delta) \\right )^{1/2} \\right )\n\\end{aligned}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pr_{D}[yf(x)\\leq 0]\" display=\"inline\"><mrow><munder><mi>Pr</mi><mi>D</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\Pr_{S}[yf(x)\\leq\\theta]+\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><munder><mi>Pr</mi><mi>S</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><mi>f</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle O\\left(\\frac{1}{\\sqrt{m}}\\left(\\frac{d\\log^{2}(m/d)}{\\theta^{2}}%&#10;+\\log(1/\\delta)\\right)^{1/2}\\right)\" display=\"inline\"><mrow><mi>O</mi><mo>\u2062</mo><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msqrt><mi>m</mi></msqrt></mfrac></mstyle><mo>\u2062</mo><msup><mrow><mo>(</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>d</mi><mo>\u2062</mo><mrow><msup><mi>log</mi><mn>2</mn></msup><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>/</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msup><mi>\u03b8</mi><mn>2</mn></msup></mfrac></mstyle><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>/</mo><mi>\u03b4</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow><mo>)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": " The magnitude of the confidence measures the\nagreement of the voters in their classification of an example.\n\nThe theoretical work on margins for boosting suggests that examples with small\nconfidence are more likely to have incorrect labels than examples with large\nconfidence. For example, we display in Figure~\\ref{fig:boosting-margins} the\nsigned confidence values for all examples and incorrectly predicted examples respectively.\nThe incorrect examples have confidence centered around zero. One\ncan leverage this for fairness by flipping negative labels of members of the\nprotected class with a small confidence value. This is a rough sketch of the SDB\nmethod. The empirical results of SDB suggest that SDB achieves statistical\nparity with relatively little loss in accuracy. Indeed, we state a similar guarantee\nto Theorem~\\ref{thm:margin-generalization} in Section~\\ref{sec:sdbtheory} that\nsolidifies this intuition.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{adult-confidence-histogram.pdf}\n\\caption{Histogram of boosting confidences for the Census data set. The top\nhistogram shows the distribution of confidence values for the entire dataset,\nand the bottom shows the confidence for only mislabeled examples. The vast\nmajority of women are classified as $-1$, and the incorrect classifications are\ncloser to the decision boundary.}\n\\label{fig:boosting-margins}\n\\end{figure}\n\nThe idea of a signed confidence generalizes nicely to other machine\nlearning algorithms. We study support vector machines (SVM) which have a\nnatural geometric notion of margin, and logistic regression which outputs a\nconfidence in its prediction. For background on SVM, logistic regression, and\nAdaBoost, see~\\cite{ShalevShwartzBD14}.\n\n\n\\subsection{Interpretations of signed confidence}\nHere we state how signed confidence is defined for each of the learning\nmethods.\n\n\\subsubsection{AdaBoost}\nBoosting algorithms work by combining \\emph{base hypotheses}, ``rules of\nthumb'' that have a fixed edge over random guessing, into highly accurate\npredictors.  In each round, a boosting algorithm finds the base hypothesis that\nachieves the smallest weighted error on the sample. It then increases the\nweights of the incorrectly classified examples, thus forcing the base learner\nto improve the classification of difficult examples. In this paper we study\nAdaBoost, a ubiquitous boosting algorithm. For more on boosting, we refer the\nreader to~\\cite{SchapireF12}. \n\nLet $H$ be a set of base classifiers, and let $(\\alpha_t, h_t)_{t=1}^T$ be the\nweights and hypotheses output by AdaBoost after $T$ rounds.  The signed\nconfidence of the hypothesis is ${\\operatorname{conf}}(\\mathbf x) = \\frac{\\sum_{i=1}^T \\alpha_i\nh_i(\\mathbf x)}{\\sum_{i=1}^T \\alpha_i}.$ In all of our experiments we boost\ndecision stumps for $T=20$ rounds.\n\n\\subsubsection{SVM} The soft-margin SVM of Vapnik~\\cite{CortesV95} outputs a\nmaximum margin hyperplane $\\mathbf{w}$ in a high-dimensional space implicitly\ndefined by a kernel $K$, and $\\mathbf{w}$ can be expressed implicitly as a\nlinear combination of the input vectors, say $\\mathbf{w'}$.  We define the\nconfidence as the distance of a point from the separating hyperplane, i.e.\n${\\operatorname{conf}}(\\mathbf x) = K (\\mathbf{w'}, \\mathbf x)$. For the Census Income and\nSingles datasets we use the standard Gaussian kernel, and for the German\ndataset we use a linear kernel (the datasets are described in\nSection~\\ref{sec:experiments}). \n\n\\subsubsection{Logistic regression}\nThe classifier output by logistic regression has the form \n", "itemtype": "equation", "pos": -1, "prevtext": "\n\\end{theorem}\n\nIn other words, the generalization error is bounded by the probability of a\nsmall margin \\emph{on the sample}. One can go on to show\nAdaBoost~\\cite{SchapireF12}, a popular algorithm that produces a weighted\nvoting scheme, performs well in this respect. Recall that the output of\nAdaBoost is a hypothesis which outputs the sign of a weighted majority vote\n$\\sum_i \\alpha_i, h_i(x)$. Rather than measure the margin we measure the\n\\emph{signed confidence} of the boosting hypothesis on an unlabeled example $x$\nas \n", "index": 5, "text": "$$ {\\operatorname{conf}}(\\mathbf x) = \\frac{\\sum_{i=1}^T \\alpha_i h_i(\\mathbf\nx)}{\\sum_{i=1}^T \\alpha_i}.$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"{\\operatorname{conf}}(\\mathbf{x})=\\frac{\\sum_{i=1}^{T}\\alpha_{i}h_{i}(\\mathbf{%&#10;x})}{\\sum_{i=1}^{T}\\alpha_{i}}.\" display=\"block\"><mrow><mrow><mrow><mo>conf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mrow><msub><mi>\u03b1</mi><mi>i</mi></msub><mo>\u2062</mo><msub><mi>h</mi><mi>i</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><msub><mi>\u03b1</mi><mi>i</mi></msub></mrow></mfrac></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": " where\n$\\phi(z)=\\frac{1}{1+e^{-z}}$ is the logistic function, and the vector $\\mathbf\nw$ is found by empirical risk minimization (ERM) with the standard logistic\nloss $\\ell(\\mathbf w, (\\mathbf x,y))=\\log(1+e^{-y\\langle \\mathbf w, \\mathbf\nx\\rangle})$ and $L_2$ regularization. Here we define the confidence of logistic\nregression simply as the value that the classifier takes before rounding:\n${\\operatorname{conf}}(\\mathbf x) = \\phi(\\langle \\mathbf w, \\mathbf x\\rangle).$\n\n\\section{Methods and Technical Solutions} \\label{sec:methods}\n\n\\subsection{Shifted decision boundary}\\label{sec:sdb}\n\nIn this section we define our methods. In what follows $X$ is a labeled\ndataset, $l(x)$ are the given labels, and $S \\subset X$ is the protected group.\nWe further assume that members of $S$ are less likely than $S^C$ to have label\n1.\nFirst we describe our proposed method, called \\emph{shifted decision\nboundary} (SDB), and then we describe three techniques we use for baseline\ncomparisons (in addition to comparing to previous literature).\n\nLet ${\\operatorname{conf}}:X \\to [-1,1]$ be a function corresponding to a classifier $h(x)={\\operatorname{sign}}({\\operatorname{conf}}(x))$,\nand define the \\emph{decision boundary shift of $\\lambda$ for\n$S$} as the classifier $h_\\lambda: X \\to \\{-1,1\\}$, defined as\n\n", "itemtype": "equation", "pos": -1, "prevtext": " The magnitude of the confidence measures the\nagreement of the voters in their classification of an example.\n\nThe theoretical work on margins for boosting suggests that examples with small\nconfidence are more likely to have incorrect labels than examples with large\nconfidence. For example, we display in Figure~\\ref{fig:boosting-margins} the\nsigned confidence values for all examples and incorrectly predicted examples respectively.\nThe incorrect examples have confidence centered around zero. One\ncan leverage this for fairness by flipping negative labels of members of the\nprotected class with a small confidence value. This is a rough sketch of the SDB\nmethod. The empirical results of SDB suggest that SDB achieves statistical\nparity with relatively little loss in accuracy. Indeed, we state a similar guarantee\nto Theorem~\\ref{thm:margin-generalization} in Section~\\ref{sec:sdbtheory} that\nsolidifies this intuition.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\columnwidth]{adult-confidence-histogram.pdf}\n\\caption{Histogram of boosting confidences for the Census data set. The top\nhistogram shows the distribution of confidence values for the entire dataset,\nand the bottom shows the confidence for only mislabeled examples. The vast\nmajority of women are classified as $-1$, and the incorrect classifications are\ncloser to the decision boundary.}\n\\label{fig:boosting-margins}\n\\end{figure}\n\nThe idea of a signed confidence generalizes nicely to other machine\nlearning algorithms. We study support vector machines (SVM) which have a\nnatural geometric notion of margin, and logistic regression which outputs a\nconfidence in its prediction. For background on SVM, logistic regression, and\nAdaBoost, see~\\cite{ShalevShwartzBD14}.\n\n\n\\subsection{Interpretations of signed confidence}\nHere we state how signed confidence is defined for each of the learning\nmethods.\n\n\\subsubsection{AdaBoost}\nBoosting algorithms work by combining \\emph{base hypotheses}, ``rules of\nthumb'' that have a fixed edge over random guessing, into highly accurate\npredictors.  In each round, a boosting algorithm finds the base hypothesis that\nachieves the smallest weighted error on the sample. It then increases the\nweights of the incorrectly classified examples, thus forcing the base learner\nto improve the classification of difficult examples. In this paper we study\nAdaBoost, a ubiquitous boosting algorithm. For more on boosting, we refer the\nreader to~\\cite{SchapireF12}. \n\nLet $H$ be a set of base classifiers, and let $(\\alpha_t, h_t)_{t=1}^T$ be the\nweights and hypotheses output by AdaBoost after $T$ rounds.  The signed\nconfidence of the hypothesis is ${\\operatorname{conf}}(\\mathbf x) = \\frac{\\sum_{i=1}^T \\alpha_i\nh_i(\\mathbf x)}{\\sum_{i=1}^T \\alpha_i}.$ In all of our experiments we boost\ndecision stumps for $T=20$ rounds.\n\n\\subsubsection{SVM} The soft-margin SVM of Vapnik~\\cite{CortesV95} outputs a\nmaximum margin hyperplane $\\mathbf{w}$ in a high-dimensional space implicitly\ndefined by a kernel $K$, and $\\mathbf{w}$ can be expressed implicitly as a\nlinear combination of the input vectors, say $\\mathbf{w'}$.  We define the\nconfidence as the distance of a point from the separating hyperplane, i.e.\n${\\operatorname{conf}}(\\mathbf x) = K (\\mathbf{w'}, \\mathbf x)$. For the Census Income and\nSingles datasets we use the standard Gaussian kernel, and for the German\ndataset we use a linear kernel (the datasets are described in\nSection~\\ref{sec:experiments}). \n\n\\subsubsection{Logistic regression}\nThe classifier output by logistic regression has the form \n", "index": 7, "text": "$$h(\\mathbf\nx)={\\operatorname{sign}}(\\phi(\\langle \\mathbf w, \\mathbf x\\rangle) - 1/2)$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"h(\\mathbf{x})={\\operatorname{sign}}(\\phi(\\langle\\mathbf{w},\\mathbf{x}\\rangle)-%&#10;1/2)\" display=\"block\"><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>sign</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>\u03d5</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">\u27e8</mo><mi>\ud835\udc30</mi><mo>,</mo><mi>\ud835\udc31</mi><mo stretchy=\"false\">\u27e9</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": "\nThe SDB algorithm accepts as input confidences ${\\operatorname{conf}}$ and finds the minimal error\ndecision boundary shift for $S$ that achieves statistical parity. That is,\ngiven ${\\operatorname{conf}}$ and $\\varepsilon > 0$, it produces a value $\\lambda$ such that $h_\\lambda$\nhas minimal error subject to achieving statistical parity up to bias\n$\\varepsilon$. \n\n\\subsection{Naive baseline algorithms}\n\nWe define two naive baseline methods which are intended to be both baseline\ncomparisons for our SDB algorithm and illustrations of the shortcomings of the\nbias-error trade-off.\n\nSimilarly to SDB, the \\emph{random relabeling} (RR) algorithm modifies a given\nhypothesis $h$ by flipping labels. In particular, RR computes the probability\n$p$ for which, if members of $S$ with label $-1$ under $h$ are flipped by $h'$\nto $+1$ randomly and independently with probability $p$, the bias of $h'$ is\nzero in expectation. The classifier $h'$ is then defined as the randomized\nclassifier that flips members of $S$ with label $-1$ with probability $p$ and\notherwise is the same as $h$.\n\nNext, we define \\emph{random massaging} (RM).  Massaging strategies, introduced\nby~\\cite{KamiranC09}, involve eliminating the bias of the training data by\nmodifying the labels of data points, and then training a classifier on this\ndata in the hope that the statistical parity of the training data will\ngeneralize to the test set as well.  In our experiment, we massage the data\nrandomly; i.e.~we flip the labels of $S$ from $-1$ to $+1$ independently at\nrandom with the probability needed to achieve statistical parity in\nexpectation, as in RR.\n\nAs we have already noted, these two baseline methods perform comparably to much\nof the previous literature in both bias and error. This illustrates that the\nsemantics of \\emph{why} an algorithm achieves statistical parity is crucial\npart of its evaluation. As such, these two baselines can be useful for any\nanalysis that measures bias and accuracy. Moreover, they can\nbe used to determine the suitability of a new proposed measure of fairness.\n\n\\subsection{Fair weak learning}\n\nFinally, we include a method which is based on a natural idea but is\nempirically suboptimal to SDB. Recall that boosting works by combining weak\nlearners into a ``strong'' classifier.  It is natural to ask whether boosting\nkeeps the fairness properties of the weak learners. Weak learners used in\npractice, such as decision stumps, have very low complexity, therefore it is\neasy to impose fairness constraints on them. In our \\emph{fair weak learning}\n(FWL) baseline we replace a standard boosting weak learner with one which tries\nto minimize a linear combination of error and bias and run the resulting\nboosting algorithm unchanged. The weak learner we use computes the decision\nstump which minimizes the sum of label error and bias of its induced\nhypothesis. \n\n\\subsection{Theoretical properties of SDB}\\label{sec:sdbtheory}\n\nBecause the SDB method only flips the labels of examples with small signed\nconfidence, margin theory implies that it will not increase the error too much.\nWe formalize this precisely below. This theorem, a direct corollary of\nTheorem~\\ref{thm:margin-generalization}, provides strong theoretical\njustification for our SDB method. To the best of our knowledge, SDB is the\nfirst empirically tested method for fair learning that has any specific\nguarantees for its accuracy.\n\nInformally, the theorem says that when a majority voting scheme is\npost-processed by the SDB technique, the resulting hypothesis maintains the\ngeneralization accuracy bounds in terms of the margin on the sample when the\nshift is small ($\\lambda \\leq \\theta$). But as the shift grows, the error bound\nincreases proportionally to the fraction of the protected population that has\nlarge enough negative margins (i.e., in $[-\\lambda, -\\theta]$).\n\n\\begin{theorem} \nLet $X$ be finite and $D,S,m,H$, and $d$ be as in\nTheorem~\\ref{thm:margin-generalization}. Let $T \\subset S$ be the subset of the\nsample in the protected class. Let $\\delta > 0$. Let $\\textup{err}(m)$ be the\ntail error function from Theorem~\\ref{thm:margin-generalization}. For any $A\n\\subset X$ let $A_{\\lambda, \\theta} = \\{ a \\in A : -\\lambda \\leq\n\\textup{conf}(a) \\leq -\\theta \\}$. \n\nThen with probability at least $1-\\delta$,\nevery function $h_\\lambda$ post-processed by SDB with weighted majority vote ${\\operatorname{conf}}(x)$ and shift\n$\\lambda > 0$ satisfies the following for every $\\theta > 0$:\n\n", "itemtype": "equation", "pos": 18848, "prevtext": " where\n$\\phi(z)=\\frac{1}{1+e^{-z}}$ is the logistic function, and the vector $\\mathbf\nw$ is found by empirical risk minimization (ERM) with the standard logistic\nloss $\\ell(\\mathbf w, (\\mathbf x,y))=\\log(1+e^{-y\\langle \\mathbf w, \\mathbf\nx\\rangle})$ and $L_2$ regularization. Here we define the confidence of logistic\nregression simply as the value that the classifier takes before rounding:\n${\\operatorname{conf}}(\\mathbf x) = \\phi(\\langle \\mathbf w, \\mathbf x\\rangle).$\n\n\\section{Methods and Technical Solutions} \\label{sec:methods}\n\n\\subsection{Shifted decision boundary}\\label{sec:sdb}\n\nIn this section we define our methods. In what follows $X$ is a labeled\ndataset, $l(x)$ are the given labels, and $S \\subset X$ is the protected group.\nWe further assume that members of $S$ are less likely than $S^C$ to have label\n1.\nFirst we describe our proposed method, called \\emph{shifted decision\nboundary} (SDB), and then we describe three techniques we use for baseline\ncomparisons (in addition to comparing to previous literature).\n\nLet ${\\operatorname{conf}}:X \\to [-1,1]$ be a function corresponding to a classifier $h(x)={\\operatorname{sign}}({\\operatorname{conf}}(x))$,\nand define the \\emph{decision boundary shift of $\\lambda$ for\n$S$} as the classifier $h_\\lambda: X \\to \\{-1,1\\}$, defined as\n\n", "index": 9, "text": "$$\nh_\\lambda(x) = \\begin{cases}\n1    & \\textup{ if } x \\in S, {\\operatorname{conf}}(x) \\geq -\\lambda \\\\ \n{\\operatorname{sign}}({\\operatorname{conf}}(x)) & \\textup{ otherwise.}\n\\end{cases}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"h_{\\lambda}(x)=\\begin{cases}1&amp;\\textup{ if }x\\in S,{\\operatorname{conf}}(x)\\geq%&#10;-\\lambda\\\\&#10;{\\operatorname{sign}}({\\operatorname{conf}}(x))&amp;\\textup{ otherwise.}\\end{cases}\" display=\"block\"><mrow><mrow><msub><mi>h</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><mi>x</mi></mrow><mo>\u2208</mo><mi>S</mi></mrow><mo>,</mo><mrow><mrow><mo>conf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mo>-</mo><mi>\u03bb</mi></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mo>sign</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>conf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mtd><mtd columnalign=\"left\"><mtext>\u00a0otherwise.</mtext></mtd></mtr></mtable></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": "  \n\\end{theorem}\n\\begin{proof}\nThe bound follows by conditioning on the event that $h_\\lambda$ flips the\nlabel, noticing $-{\\operatorname{conf}}(x)$ is also a majority function, and applying\nTheorem~\\ref{thm:margin-generalization} twice.  \n\\end{proof}\n\n\\subsection{Resilience to random bias}\\label{sec:rrb}\nOne of the biggest challenges for designers of fair learning algorithms is\nthe lack of good measures of fairness. The most popular measures are\nstatistical measures of bias such as statistical parity. As Dwork et\nal.~\\cite{DworkHPR12} have pointed out, statistical parity fails to capture all\nimportant aspects of fairness. In particular, it is easy to achieve statistical\nparity simply by flipping the labels of an arbitrary set of individuals in the\nprotected class. A real-world example would be giving a raise to a random group\nof women to eliminate the gender disparity in wages.  The root cause of this\nproblem is that one does not have access to reliable (unbiased) ground truth\nlabels. We propose to compensate for this by evaluating algorithms on synthetic\nbias. In doing this we make transparent the \\emph{kind} of bias a claimed\n``fair'' algorithm protects against, and we can accurately measure its\nresilience to said bias.\n\nWe introduce a new notion of fairness called \\emph{resilience to random bias}\n(RRB). Informally we introduce a new, random feature which has no correlation\nwith the target attribute, and then we introduce bias against individuals which\nhave a certain value for this new feature.  We call an algorithm fair if it can\nrecover the original, unbiased labels. For RRB in particular, the synthetic\nbias is i.i.d. random against the protected group.  \n\nCertainly, in practice, bias may not be of this form and we do not pretend that this notion captures all forms of bias.\nRather, this notion seeks to model a comparatively mild form of bias --\nif an algorithm cannot recover from this type of random bias against a protected class then\nthere is little reason to think it can handle other types of bias.\nIn other words, we propose this as a minimally necessary condition but not necessarily a sufficient condition for individual fairness.\nRelating our RRB measure more formally to other notions of individual fairness is left for future work.\n\nWe formally define RRB as follows. Let $X$ be a set of examples and $D$ be a\ndistribution over examples, with $l:X \\to \\{-1,1\\}$ a target labeling function.\nWe first define a randomized process mapping $(X,D,l) \\to (\\tilde X, \\tilde D,\n\\tilde l)$. Let $\\tilde X = X \\times \\{ -1,1 \\}$ and $\\tilde D$ be the\ndistribution on $\\tilde X$ which is independently $D$ on the $X$ coordinate and\nuniform on the $\\{-1,1\\}$ coordinate. Denote by $\\tilde X_0 = \\{ (x,b) \\in\n\\tilde X \\mid b=0 \\}$ and call this the \\emph{protected set}. Finally, $\\tilde\nl(x,b)$ is \\emph{fixed} to either $l(x)$ or $-l(x)$ independently at random\nfor each $(x,b) \\in \\tilde X$ according to the following: \n", "itemtype": "equation", "pos": -1, "prevtext": "\nThe SDB algorithm accepts as input confidences ${\\operatorname{conf}}$ and finds the minimal error\ndecision boundary shift for $S$ that achieves statistical parity. That is,\ngiven ${\\operatorname{conf}}$ and $\\varepsilon > 0$, it produces a value $\\lambda$ such that $h_\\lambda$\nhas minimal error subject to achieving statistical parity up to bias\n$\\varepsilon$. \n\n\\subsection{Naive baseline algorithms}\n\nWe define two naive baseline methods which are intended to be both baseline\ncomparisons for our SDB algorithm and illustrations of the shortcomings of the\nbias-error trade-off.\n\nSimilarly to SDB, the \\emph{random relabeling} (RR) algorithm modifies a given\nhypothesis $h$ by flipping labels. In particular, RR computes the probability\n$p$ for which, if members of $S$ with label $-1$ under $h$ are flipped by $h'$\nto $+1$ randomly and independently with probability $p$, the bias of $h'$ is\nzero in expectation. The classifier $h'$ is then defined as the randomized\nclassifier that flips members of $S$ with label $-1$ with probability $p$ and\notherwise is the same as $h$.\n\nNext, we define \\emph{random massaging} (RM).  Massaging strategies, introduced\nby~\\cite{KamiranC09}, involve eliminating the bias of the training data by\nmodifying the labels of data points, and then training a classifier on this\ndata in the hope that the statistical parity of the training data will\ngeneralize to the test set as well.  In our experiment, we massage the data\nrandomly; i.e.~we flip the labels of $S$ from $-1$ to $+1$ independently at\nrandom with the probability needed to achieve statistical parity in\nexpectation, as in RR.\n\nAs we have already noted, these two baseline methods perform comparably to much\nof the previous literature in both bias and error. This illustrates that the\nsemantics of \\emph{why} an algorithm achieves statistical parity is crucial\npart of its evaluation. As such, these two baselines can be useful for any\nanalysis that measures bias and accuracy. Moreover, they can\nbe used to determine the suitability of a new proposed measure of fairness.\n\n\\subsection{Fair weak learning}\n\nFinally, we include a method which is based on a natural idea but is\nempirically suboptimal to SDB. Recall that boosting works by combining weak\nlearners into a ``strong'' classifier.  It is natural to ask whether boosting\nkeeps the fairness properties of the weak learners. Weak learners used in\npractice, such as decision stumps, have very low complexity, therefore it is\neasy to impose fairness constraints on them. In our \\emph{fair weak learning}\n(FWL) baseline we replace a standard boosting weak learner with one which tries\nto minimize a linear combination of error and bias and run the resulting\nboosting algorithm unchanged. The weak learner we use computes the decision\nstump which minimizes the sum of label error and bias of its induced\nhypothesis. \n\n\\subsection{Theoretical properties of SDB}\\label{sec:sdbtheory}\n\nBecause the SDB method only flips the labels of examples with small signed\nconfidence, margin theory implies that it will not increase the error too much.\nWe formalize this precisely below. This theorem, a direct corollary of\nTheorem~\\ref{thm:margin-generalization}, provides strong theoretical\njustification for our SDB method. To the best of our knowledge, SDB is the\nfirst empirically tested method for fair learning that has any specific\nguarantees for its accuracy.\n\nInformally, the theorem says that when a majority voting scheme is\npost-processed by the SDB technique, the resulting hypothesis maintains the\ngeneralization accuracy bounds in terms of the margin on the sample when the\nshift is small ($\\lambda \\leq \\theta$). But as the shift grows, the error bound\nincreases proportionally to the fraction of the protected population that has\nlarge enough negative margins (i.e., in $[-\\lambda, -\\theta]$).\n\n\\begin{theorem} \nLet $X$ be finite and $D,S,m,H$, and $d$ be as in\nTheorem~\\ref{thm:margin-generalization}. Let $T \\subset S$ be the subset of the\nsample in the protected class. Let $\\delta > 0$. Let $\\textup{err}(m)$ be the\ntail error function from Theorem~\\ref{thm:margin-generalization}. For any $A\n\\subset X$ let $A_{\\lambda, \\theta} = \\{ a \\in A : -\\lambda \\leq\n\\textup{conf}(a) \\leq -\\theta \\}$. \n\nThen with probability at least $1-\\delta$,\nevery function $h_\\lambda$ post-processed by SDB with weighted majority vote ${\\operatorname{conf}}(x)$ and shift\n$\\lambda > 0$ satisfies the following for every $\\theta > 0$:\n\n", "index": 11, "text": "$$\n\\begin{aligned}\\Pr_D[yh_\\lambda(x) \\leq 0] &\\leq \n\\Pr_{T_{\\lambda, \\theta}}[y\\cdot{\\operatorname{conf}}(x) \\geq -\\theta] \\Pr_{S}[x \\in T_{\\lambda,\n\\theta}] \\\\ & + \n\\Pr_{S - T_{\\lambda, \\theta}}[y\\cdot{\\operatorname{conf}}(x) \\leq \\theta] \\Pr_{S}[x \\not \\in T_{\\lambda,\n\\theta}] \\\\ & + \n\\max ( \\textup{err}(|T_{\\lambda, \\theta}|), \\textup{err}(|T^C_{\\lambda,\n\\theta}|))\n\\end{aligned}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6X.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Pr_{D}[yh_{\\lambda}(x)\\leq 0]\" display=\"inline\"><mrow><munder><mi>Pr</mi><mi>D</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>y</mi><mo>\u2062</mo><msub><mi>h</mi><mi>\u03bb</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2264</mo><mn>0</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6X.m3\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\\Pr_{T_{\\lambda,\\theta}}[y\\cdot{\\operatorname{conf}}(x)\\geq-%&#10;\\theta]\\Pr_{S}[x\\in T_{\\lambda,\\theta}]\" display=\"inline\"><mrow><mi/><mo>\u2264</mo><mrow><mrow><munder><mi>Pr</mi><msub><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow></msub></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>y</mi><mo>\u22c5</mo><mrow><mo>conf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2265</mo><mrow><mo>-</mo><mi>\u03b8</mi></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><mi>S</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>x</mi><mo>\u2208</mo><msub><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6Xa.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\Pr_{S-T_{\\lambda,\\theta}}[y\\cdot{\\operatorname{conf}}(x)\\leq%&#10;\\theta]\\Pr_{S}[x\\not\\in T_{\\lambda,\\theta}]\" display=\"inline\"><mrow><mo>+</mo><mrow><mrow><munder><mi>Pr</mi><mrow><mi>S</mi><mo>-</mo><msub><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow></msub></mrow></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>y</mi><mo>\u22c5</mo><mrow><mo>conf</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2264</mo><mi>\u03b8</mi></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>\u2062</mo><mrow><munder><mi>Pr</mi><mi>S</mi></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mi>x</mi><mo>\u2209</mo><msub><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6Xb.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle+\\max(\\textup{err}(|T_{\\lambda,\\theta}|),\\textup{err}(|T^{C}_{%&#10;\\lambda,\\theta}|))\" display=\"inline\"><mrow><mo>+</mo><mrow><mi>max</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mtext>err</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mtext>err</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>T</mi><mrow><mi>\u03bb</mi><mo>,</mo><mi>\u03b8</mi></mrow><mi>C</mi></msubsup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": "\n\nIn other words, the positive labels of a randomly chosen protected subgroup are\nflipped to negative independently at random with probability $\\eta$. We\nemphasize that the process mapping $l \\mapsto \\tilde l$ is randomized, but the\nmap $\\tilde l(x,b)$ itself is fixed and deterministic. So an algorithm which\nqueries labels from $\\tilde l$ is given consistent answers. Now we define the\nresilience to random bias as follows:\n\n\\begin{definition} \\label{def:rrb}\nLet $(X,D,l), (\\tilde X, \\tilde D, \\tilde l)$ be as above. Let $h = A(\\tilde D,\n\\tilde l)$ be the output classifier of a learning algorithm $A$ when given\nbiased data as input. The \\emph{resilience to random bias} (RRB) of $A$ with\nrespect to $(X,D,l)$ and discrimination rate $0 \\leq \\eta < 1/2$, denoted\n$\\textup{RRB}_{\\eta}(A)$, is\n\n", "itemtype": "equation", "pos": 26855, "prevtext": "  \n\\end{theorem}\n\\begin{proof}\nThe bound follows by conditioning on the event that $h_\\lambda$ flips the\nlabel, noticing $-{\\operatorname{conf}}(x)$ is also a majority function, and applying\nTheorem~\\ref{thm:margin-generalization} twice.  \n\\end{proof}\n\n\\subsection{Resilience to random bias}\\label{sec:rrb}\nOne of the biggest challenges for designers of fair learning algorithms is\nthe lack of good measures of fairness. The most popular measures are\nstatistical measures of bias such as statistical parity. As Dwork et\nal.~\\cite{DworkHPR12} have pointed out, statistical parity fails to capture all\nimportant aspects of fairness. In particular, it is easy to achieve statistical\nparity simply by flipping the labels of an arbitrary set of individuals in the\nprotected class. A real-world example would be giving a raise to a random group\nof women to eliminate the gender disparity in wages.  The root cause of this\nproblem is that one does not have access to reliable (unbiased) ground truth\nlabels. We propose to compensate for this by evaluating algorithms on synthetic\nbias. In doing this we make transparent the \\emph{kind} of bias a claimed\n``fair'' algorithm protects against, and we can accurately measure its\nresilience to said bias.\n\nWe introduce a new notion of fairness called \\emph{resilience to random bias}\n(RRB). Informally we introduce a new, random feature which has no correlation\nwith the target attribute, and then we introduce bias against individuals which\nhave a certain value for this new feature.  We call an algorithm fair if it can\nrecover the original, unbiased labels. For RRB in particular, the synthetic\nbias is i.i.d. random against the protected group.  \n\nCertainly, in practice, bias may not be of this form and we do not pretend that this notion captures all forms of bias.\nRather, this notion seeks to model a comparatively mild form of bias --\nif an algorithm cannot recover from this type of random bias against a protected class then\nthere is little reason to think it can handle other types of bias.\nIn other words, we propose this as a minimally necessary condition but not necessarily a sufficient condition for individual fairness.\nRelating our RRB measure more formally to other notions of individual fairness is left for future work.\n\nWe formally define RRB as follows. Let $X$ be a set of examples and $D$ be a\ndistribution over examples, with $l:X \\to \\{-1,1\\}$ a target labeling function.\nWe first define a randomized process mapping $(X,D,l) \\to (\\tilde X, \\tilde D,\n\\tilde l)$. Let $\\tilde X = X \\times \\{ -1,1 \\}$ and $\\tilde D$ be the\ndistribution on $\\tilde X$ which is independently $D$ on the $X$ coordinate and\nuniform on the $\\{-1,1\\}$ coordinate. Denote by $\\tilde X_0 = \\{ (x,b) \\in\n\\tilde X \\mid b=0 \\}$ and call this the \\emph{protected set}. Finally, $\\tilde\nl(x,b)$ is \\emph{fixed} to either $l(x)$ or $-l(x)$ independently at random\nfor each $(x,b) \\in \\tilde X$ according to the following: \n", "index": 13, "text": "$$ \\Pr[\\tilde l(x,b)\n= l(x)] = \\begin{cases} 1      & \\textup{ if } b=1 \\textup{ or } l(x) = -1 \\\\\n1-\\eta & \\textup{ if } b=0 \\textup{ and } l(x) = 1 \\\\ \\end{cases}.  $$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\Pr[\\tilde{l}(x,b)=l(x)]=\\begin{cases}1&amp;\\textup{ if }b=1\\textup{ or }l(x)=-1\\\\&#10;1-\\eta&amp;\\textup{ if }b=0\\textup{ and }l(x)=1\\\\&#10;\\end{cases}.\" display=\"block\"><mrow><mrow><mrow><mi>Pr</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mover accent=\"true\"><mi>l</mi><mo stretchy=\"false\">~</mo></mover><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mn>1</mn></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><mi>b</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>\u2062</mo><mtext>\u00a0or\u00a0</mtext><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mn>1</mn><mo>-</mo><mi>\u03b7</mi></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>\u00a0if\u00a0</mtext><mo>\u2062</mo><mi>b</mi></mrow><mo>=</mo><mrow><mn>0</mn><mo>\u2062</mo><mtext>\u00a0and\u00a0</mtext><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr></mtable></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.05764.tex", "nexttext": "\n\\end{definition}\n\nSimilarly to calculating statistical parity, RRB is estimated on a fixed\ndataset by simulating the process described above and outputing an empirical\naverage. \n\n\\section{Empirical Evaluation} \\label{sec:experiments}\n\nWe measure our methods on label error, statistical parity, and RRB with $\\eta =\n0.2$. In all of our experiments we split the datasets randomly into training,\ntest, and model-selection subsets, and we output the average of 10\nexperiments.\\footnote{The code is available for reproducibility\nat~\\url{https://github.com/j2kun/fkl-SDM16}}\n\n\\subsection{Datasets}\n\nThe Census Income dataset~\\cite{Lichman13}, extracted from the 1994 Census\ndatabase, contains demographic information about $48842$ American adults.  The\nprediction task is to determine whether a person earns over \\$50K a year.  The\ndataset contains $16,192$ females ($33\\%$) and $32,650$ males. Note $30.38\\%$\nof men and $10.93\\%$ of women reported earnings of more than \\$50K, therefore\nthe bias of the dataset is $19.45\\%$.  \n\nThe German credit dataset~\\cite{Lichman13} contains financial information about\n1000 individuals who are classified into groups of good and bad credit risk.\nThe ``good'' credit group contains $699$ individuals. Following the work\nof~\\cite{KamiranC09}, we consider age as the protected attribute with a cut-off\nat $25$. Only $59\\%$ of the younger people are considered good credit risk,\nwhereas of the $25$ or older group, $72\\%$ are creditworthy, making the bias\n$13\\%$.\n\nIn the Singles dataset, extracted from the marketing dataset\nof~\\cite{HastieTF09} by taking all respondents who identified as ``single,''\nthe goal is to predict whether annual income of a household is greater than\n\\$25K from 13 other demographic attributes.  The protected attribute is gender.\nThe dataset contains $3,653$ data points, $1,756$ ($48\\%$) of which belong to\nthe protected group. $34\\%$ of the dataset has a positive label. The bias is\n$9.8\\%$.\n\n\\subsection{Results and analysis}\\label{sec:results}\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{0.65\\columnwidth}\n\\includegraphics[width=\\columnwidth]{adult_scatter_plot.pdf}\n\\end{subfigure}\n\\hspace{1mm}\n\\begin{subfigure}{0.65\\columnwidth}\n\\includegraphics[width=\\columnwidth]{german_scatter_plot.pdf}\n\\end{subfigure}\n\\hspace{1mm}\n\\begin{subfigure}{0.65\\columnwidth}\n\\includegraphics[width=\\columnwidth]{singles_scatter_plot.pdf}\n\\end{subfigure}\n\\caption{A summary of our experimental results for our three datasets, from\nleft to right: Census Income, German, Singles.\nLabels show which learning algorithm is used and the colors give which method for achieving fairness was used.\nThe parameters of each algorithm\nwere chosen to minimize bias. The size of a point is proportional to the RRB of\nthe learner (only for those algorithms for which we have the RRB numbers),\nwhere larger dots mean there is a larger probability of correcting\na label.} \n\\label{fig:scatterplots} \n\\end{figure*}\n\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{| c | ccc |}\n\\hline\n               Method    & Census & German & Singles \\\\ \n\\hline \n               SVM       & 0.2702 & 0.6756 & 0.2424  \\\\ \n               SVM (RR)  & 0.2821 & 0.7827 & 0.2588  \\\\ \n               SVM (RM)  & 0.2545 & 0.6232 & 0.2552  \\\\ \n               SVM (SDB) & \\textbf{0.3172} & \\textbf{0.8619} & \\textbf{0.3064}  \\\\ \n\\hline\n               LR        & 0.4647 & 0.3070 & 0.1971  \\\\ \n               LR (RR)   & 0.4696 & 0.8564 & 0.3213  \\\\ \n               LR (RM)   & 0.4282 & 0.6741 & 0.2117  \\\\ \n               LR (SDB)  & \\textbf{0.5402} & \\textbf{0.8687} & \\textbf{0.8596}  \\\\ \n\\hline\n               AB        & 0.4372 & 0.6774 & 0.2864  \\\\ \n               AB (RR)   & 0.4661 & \\textbf{0.8629} & 0.3996  \\\\ \n               AB (RM)   & 0.4410 & 0.6965 & 0.3325  \\\\ \n               AB (SDB)  & \\textbf{0.5461} & 0.8596 & \\textbf{0.4027}  \\\\ \n               AB (FWL)  & 0.5174 & 0.6879 & 0.2971  \\\\ \n\\hline\n\\end{tabular}\n\\caption{The RRB numbers for each of our methods and baselines. In each column\nand section the largest values are shown in bold, and they are almost always\nSDB.}\n\\label{table:rrb}\n\\end{table}\n\n\nIn this section we state our experimental results. They are summarized in\nFigure~\\ref{fig:scatterplots} for the Census Income, German, and Singles\ndatasets, and the full set of numbers are in Tables~\\ref{table:census_results},~\\ref{table:german_results}, and~\\ref{table:singles_results} respectively.  For comparison,\nwe also included the numbers for the Learning Fair Representations (LFR) method\nof~\\cite{ZemelWSPD13} for the Census Income dataset, for Classification with No\nDiscrimination (CND) method of~\\cite{KamiranC09}, and for the Discrimination\nAware Decision Tree (DADT) technique of~\\cite{KamiranCP10} (specifically we use\nthe numbers for the ``IGC+IGS\\_Relab'' method). In~\\cite{ZemelWSPD13} the\nauthors implemented three other learning algorithms, these are unregularized\nlogistic regression, Fair Naive-Bayes \\cite{KamiranC09}, and Regularized\nLogistic Regression \\cite{KamashimaAS11}. These methods all had errors above\n$20\\%$ on the Census dataset and so we omit them for brevity.\nIn~\\cite{KamiranCP10} the authors implemented variations on the decision tree\nlearning scheme, and the one we include has the highest accuracy, though they\nare all closely comparable.  We reported all biases as unsigned. We were unable to access\nimplementations of the prior authors' algorithms, so we were not able to\nreproduce their results or measure their algorithms with respect to RRB.\n\nTo investigate the trade-offs made by our SDB\nmethod more closely,\nFigures~\\ref{fig:adult_tradeoffs},~\\ref{fig:german_tradeoffs},\nand~\\ref{fig:singles_tradeoffs} show the rate at which error increases as bias\ngoes to zero.  In many cases, a substantial reduction in bias can be achieved before there is any significant drop-off in accuracy.\n\nFor the Census Income dataset, the three SDB techniques outperform the\nbaselines and outperform all the prior literature except for DADT. Both SDB\nalgorithms achieve statistical parity with about $18\\%$ error. Moreover, these\ntwo SDB algorithms have the highest RRB, while SVM appears to overfit the\nrandom bias introduced by RRB more than the other algorithms. While DADT\nappears to achieve lower label error and comparable bias, we note that the\nstandard deviation of the bias reported in~\\cite{KamiranCP10} is 0.015 while for SDB (on the Census Income dataset) the standard deviations are at\nleast one order of magnitude smaller. \n\nThe singles dataset shows a similar pattern, with SDB combined with logistic\nregression outperforming all other baselines. Note that in the instances where\nthe baselines perform comparably to SDB, SDB tends to have a much larger\nresilience to random bias.\n\nThe German dataset is more puzzling. While two of the SDB techniques\noutperform the prior literature by a moderate margin, they do not \noutperform random relabeling or random massaging by a significant\nmargin (and these baselines already outperform CND). Another curious\nobservation is that label error stays\nconstant as the decision boundary is shifted, as Figure~\\ref{fig:german_tradeoffs} shows.\n\n\n\n\n\n\nNote again the difference in SVM kernels between the datasets. The Gaussian\nkernel performs well for the Census Income and Singles dataset. However, in\nthe case of the German dataset, which is the smallest of the three, with the\nGaussian kernel every point becomes a support vector. This is not only a clear\nsign of overfitting but it also makes SDB useless since the model gives the\nsame confidence for almost every data point.\n\nThese facts seem to be\nevidence that the German dataset (which has only about a thousand records) is\ntoo small to draw a significant conclusion. We nevertheless include it here for\ncompleteness and to show comparison with the previous literature.\n\nFair weak learning (FWL) does empirically reduce bias but does not achieve\nstatistical parity in two of the three data sets.  FWL performs worse\non either label error or bias on each of the data sets and the trade-off\nbetween label error and bias cannot easily be controlled. It also does not seem\neasy to control this trade-off using either random massaging and random\nrelabeling.\n \nOne notable advantage of SDB is that the trade-off between label error and bias\ncan be controlled \\emph{after} training.  To decide how much bias and error we\nwant to allow, we do not have to pick a hyper-parameter before training the\nalgorithm, unlike for most other fair learning methods. This means that the\ncomputational cost of choosing the best point on the trade-off curve is very\nlow, and the trade-off is transparent. \n\nThe results also highlight the usefulness of RRB as a measure of fairness. The\nRRB values across all datasets and algorithms we studied are in\nTable~\\ref{table:rrb}.  In cases where random relabeling or random massaging\nperforms comparably to SDB, the RRB measure is able to distinguish them, giving\na lower score to the less reasonable baselines and a higher score to SDB.\n\nThis suggests that\nthe performance of fair learning algorithms should not be evaluated solely by\ntheir accuracy and bias.\n\n\\section{Significance and Impact}\\label{sec:discussion}\nIn this paper, we introduced a general method for balancing discrimination and\nlabel error. This method, which we call shifted decision boundary (SDB), is\napplicable to any learning algorithm which has an efficiently computable\nmeasure of confidence. We studied three such algorithms -- AdaBoost, support\nvector machines, and linear regression -- compared our methods to other methods\nproposed in the earlier literature and our own baselines, and empirically\nevaluated our methods' performances in terms of their resilience to random bias. \n\nOur method, in addition to outperforming much of the previous literature, has\nseveral other desirable properties. Unlike most other fair learning algorithms,\nSDB applied to AdaBoost has theoretical bounds on generalization error.\n\nAlso, since the margin shift can be specified after the\noriginal learner has been trained on the data, a practitioner can easily\nevaluate the trade-off between error and bias and choose the most desirable\npoint on the trade-off curve. This makes SDB a fast and transparent way to study\nthe fairness properties of an algorithm.\n\nOur resilience to random bias (RRB) measure is a novel approach to evaluate the\nfairness of a learning algorithm. Although i.i.d.~random bias is a simplified\nmodel of real-world discrimination, we posit that any algorithm which can be\nconsidered fair must be fair with respect to RRB. Moreover, RRB generalizes to\nan arbitrary distribution over the input data, and one could adapt it to\nwell-studied models of bias in social science.\n\n\\section*{Acknowledments}\nWe would like to thank Lev Reyzin for helpful discussions.\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{adult-boosting-T.pdf}\n\\caption{Boosting}\n\\label{fig:adult_boosting_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{adult-lr-T.pdf}\n\\caption{Logistic Regression}\n\\label{fig:adult_lr_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{adult-svm-T.pdf}\n\\caption{SVM}\n\\label{fig:adult_svm_tradeoff}\n\\end{subfigure}\n\\caption{Trade-off between (signed) bias and error for SDB on the Census Income data. The horizontal axis is the threshold used for SDB.}\n\\label{fig:adult_tradeoffs}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{german-boosting-T.pdf}\n\\caption{Boosting}\n\\label{fig:german_boosting_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{german-lr-T.pdf}\n\\caption{Logistic Regression}\n\\label{fig:german_lr_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{german-svmlinear-T.pdf}\n\\caption{SVM}\n\\label{fig:german_svm_tradeoff}\n\\end{subfigure}\n\\caption{Trade-off between (signed) bias and error for SDB on the German data. The horizontal axis is the threshold used for SDB.}\n\\label{fig:german_tradeoffs}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{singles-boosting-T.pdf}\n\\caption{Boosting}\n\\label{fig:singles_boosting_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{singles-lr-T.pdf}\n\\caption{Logistic Regression}\n\\label{fig:singles_lr_tradeoff}\n\\end{subfigure}\n\\begin{subfigure}{.7\\columnwidth}\n\\includegraphics[width=\\columnwidth]{singles-svm-T.pdf}\n\\caption{SVM}\n\\label{fig:singles_svm_tradeoff}\n\\end{subfigure}\n\\caption{Trade-off between (signed) bias and error for SDB on the Singles data. The horizontal axis is the threshold used for SDB.}\n\\label{fig:singles_tradeoffs}\n\\end{figure*}\n\n\\bibliographystyle{plain}\n\\bibliography{main}\n\n\\clearpage\n\\appendix\n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{| c | ccccc |}\n\\hline\n               & SVM & SVM (RR) & SVM (SDB) & SVM (RM) & LFR \\cite{ZemelWSPD13} \\\\\n\\hline\nlabel error    & 0.1471 (5.7e-17) & 0.2007 (0.002) & 0.1869 (0.004) & 0.1740 (0.003) & 0.2299 \\\\\nbias           & 0.1689 (5.7e-17) & 0.0050 (0.003) & 0.0036 (0.009) & 0.0795 (0.010) & 0.0020 \\\\\nRRB            & 0.2702 (0.014) & 0.2926 (0.004) & 0.3172 (0.025) & 0.2545 (0.007) & n/a \\\\\n\\hline\n               & LR & LR (RR) & LR (SDB) & LR (RM) & DADT \\cite{KamiranCP10} \\\\\n\\hline\nlabel error    & 0.1478 (4.8e-04) & 0.2077 (0.004) & 0.1802 (0.002) & 0.1810 (0.003) & 0.1600 \\\\\nbias           & 0.1968 (0.003) & 0.0044 (0.006) & 0.0060 (0.011) & 0.0262 (0.008) & 0.0090 (0.015) \\\\\nRRB            & 0.4647 (0.013) & 0.4696 (0.009) & 0.5402 (0.011) & 0.4282 (0.019) & n/a \\\\\n\\hline\n               & AdaBoost & AB (RR) & AB (SDB) & AB (RM) & AB (FWL)  \\\\\n\\hline\nlabel error    & 0.1529 (0.002) & 0.2078 (0.004) & 0.1822 (0.005) & 0.1864 (0.004) & 0.1860 (0.004) \\\\\nbias           & 0.1856 (0.012) & 0.0091 (0.006) & 0.0013 (0.007) & 0.0381 (0.013) & 0.0682 (0.004)  \\\\\nRRB            & 0.4372 (0.032) & 0.4661 (0.019) & 0.5461 (0.015) & 0.4410 (0.013) & 0.4321 (0.016)  \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\caption{A summary of our experimental results for the Census Income data for\nrelabeling, massaging, and the fair weak learner. The threshold for SDB was\nchosen to achieve perfect statistical parity on the training data. Standard\ndeviations are reported in parentheses when known.}\n\\label{table:census_results}\n\\end{table*}\n\n\\begin{table*}[h]\n\\centering\n\\begin{tabular}{| c | ccccc |}\n\\hline\n               & SVM & SVM (RR) & SVM (SDB) & SVM (RM) & CND \\cite{KamiranC09} \\\\\n\\hline\nlabel error    & 0.2823 (0) & 0.2778 (0.025) & 0.2979 (0.022) & 0.3000 (0.017) & 0.2757  \\\\\nbias           & 0.0886 (4.2e-17) & 0.0732 (0.066) & 0.0266 (0.085) & 0.0445 (0.028) & 0.0327   \\\\\nRRB            & 0.6756 (0.081) & 0.7827 (0.054) & 0.8619 (0.041) & 0.6232 (0.070) & n/a  \\\\\n\\hline\n               & LR & LR (RR) & LR (SDB) & LR (RM) &  \\\\\n\\hline\nlabel error    & 0.2541 (0.005) & 0.2656 (0.020) & 0.2685 (0.021) & 0.2625 (0.011) &\\\\\nbias           & 0.1383 (0.014) & 0.0095 (0.064) & 0.0142 (0.219) & 0.0202 (0.566) &\\\\\nRRB            & 0.3070 (0.067) & 0.8564 (0.045) & 0.8687 (0.042) & 0.6741 (0.045) & \\\\\n\\hline\n               & AdaBoost & AB (RR)  & AB (SDB)  & AB (RM)   & AB (FWL)  \\\\\n\\hline\nlabel error    & 0.2602 (0.009) & 0.2429 (0.010) & 0.2745 (0.010) & 0.2637 (0.019) & 0.2859 (0.016)\\\\\nbias           & 0.2617 (0.272) & 0.0376 (0.044) & 0.0034 (0.064) & 0.0391 (0.023) & 0.0093 (0.035)\\\\\nRRB            & 0.6774 (0.219) & 0.8629 (0.051) & 0.8596 (0.067) & 0.6965 (0.037) & 0.6879 (0.042)\\\\\n\\hline\n\\hline\n\\end{tabular}\n\\caption{A summary of our experimental results for the German data for\nrelabeling, massaging, and the fair weak learner. The threshold for SDB was\nchosen to achieve perfect statistical parity on the training data. On this\ndataset SVM was run with a linear kernel. Standard deviations are reported in\nparentheses when known. }\n\\label{table:german_results}\n\\end{table*}\n\n\n\\begin{table*}[h]\n\\centering\n\\begin{tabular}{| c | ccccc |}\n\\hline\n               & SVM & SVM (RR) & SVM (SDB) & SVM (RM) & \\\\\n\\hline\nlabel error    & 0.2718 (5.7e-17)& 0.2793 (0.009) & 0.2716 (0.013) & 0.2876 (0.015)& \\\\\nbias           & 0.0550 (1.4e-17)& 0.1460 (0.017) & 0.0106 (0.035) & 0.0260 (0.047)& \\\\\nRRB            & 0.2424 (0.045)& 0.2588 (0.009) & 0.3064 (0.042) & 0.2552 (0.032)&\\\\\n\\hline\n               & LR & LR (RR)  & LR (SDB)  & LR (RM)  &  \\\\\n\\hline\nlabel error    & 0.2742 (1.14e-16)& 0.3130 (0.011) & 0.2745 (0.010) & 0.2966 (0.008)&\\\\\nbias           & 0.1468 (9.99e-18)& 0.3025 (0.040) & 0.0034 (0.640) & 0.0732 (0.024)&\\\\\nRRB            & 0.1971 (0.036)& 0.3213 (0.035) & 0.8596 (0.067) & 0.2117 (0.036)& \\\\\n\\hline\n               & AdaBoost & AB (RR) & AB (SDB) & AB (RM) & AB (FWL)   \\\\\n\\hline\nlabel error    & 0.2690 (0.004)& 0.3088 (0.009) & 0.2990 (0.008) & 0.2860 (0.019) & 0.2687 (0.008)\\\\\nbias           & 0.0966 (0.020)& 0.2123 (0.013) & 0.0140 (0.017) & 0.0180 (0.037) & 0.0463 (0.016)\\\\\nRRB            & 0.2864 (0.057)& 0.3996 (0.105) & 0.4027 (0.061) & 0.3325 (0.060) & 0.2971 (0.028)\\\\\n\\hline\n\\hline\n\\end{tabular}\n\\caption{A summary of our experimental results for the Singles data for\nrelabeling, massaging, and the fair weak learner. The threshold for SDB was\nchosen to achieve perfect statistical parity on the training data. Standard\ndeviations are reported in parentheses when known. }\n\\label{table:singles_results}\n\\end{table*}\n\n\n", "itemtype": "equation", "pos": 27821, "prevtext": "\n\nIn other words, the positive labels of a randomly chosen protected subgroup are\nflipped to negative independently at random with probability $\\eta$. We\nemphasize that the process mapping $l \\mapsto \\tilde l$ is randomized, but the\nmap $\\tilde l(x,b)$ itself is fixed and deterministic. So an algorithm which\nqueries labels from $\\tilde l$ is given consistent answers. Now we define the\nresilience to random bias as follows:\n\n\\begin{definition} \\label{def:rrb}\nLet $(X,D,l), (\\tilde X, \\tilde D, \\tilde l)$ be as above. Let $h = A(\\tilde D,\n\\tilde l)$ be the output classifier of a learning algorithm $A$ when given\nbiased data as input. The \\emph{resilience to random bias} (RRB) of $A$ with\nrespect to $(X,D,l)$ and discrimination rate $0 \\leq \\eta < 1/2$, denoted\n$\\textup{RRB}_{\\eta}(A)$, is\n\n", "index": 15, "text": "$$\n\\textup{RRB}_{\\eta}(A) = \\Pr_{\\tilde D} [h(x,b) = l(x) \\mid b = 0, l(x) = 1]\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\textup{RRB}_{\\eta}(A)=\\Pr_{\\tilde{D}}[h(x,b)=l(x)\\mid b=0,l(x)=1]\" display=\"block\"><mrow><mrow><msub><mtext>RRB</mtext><mi>\u03b7</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>A</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><munder><mi>Pr</mi><mover accent=\"true\"><mi>D</mi><mo stretchy=\"false\">~</mo></mover></munder><mo>\u2061</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>h</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>\u2223</mo><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><mo>,</mo><mrow><mrow><mi>l</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></math>", "type": "latex"}]