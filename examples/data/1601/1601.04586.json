[{"file": "1601.04586.tex", "nexttext": "\n where $A_{i\\cdot}$ is the $i$-th row of $\\mathbf{A}$ and $\\|\\cdot\\|_{q}$ is the $L_{q}$-norm of a vector with $q \\in \\{1,2,\\infty\\}$. Note that both k-means clustering and hierarchical\nclustering consider $L_{0}$-norm in the second term, which leads to\na non-convex optimization problem \\citep{Hocking2011,Tan2015}. Therefore, convex\nclustering can be viewed as a convex relaxation of k-means clustering and hierarchical\nclustering, and the convex relaxation ensures that it achieves a unique\nglobal minimizer.\n\nDue to the fused-lasso penalty \\citep{Tibshirani.ea:2005} in the second term of (\\ref{eq:obj-cc}),\nthe above formulation encourages that some of the rows of the solution $\\widehat{\\bfA}$\nare identical. If $\\widehat{A}_{i_{1}\\cdot}=\\widehat{A}_{i_{2}\\cdot}$,\nthen observation $i_{1}$ and observation $i_{2}$ are said to belong\nto the same cluster. The tuning parameter $\\gamma$ in (\\ref{eq:obj-cc})\ncontrols the number of unique rows of $\\widehat{\\bfA}$, that is,\nthe number of estimated clusters. When $\\gamma=0$, $\\widehat{\\bfA}={\\bfX}$,\nand therefore each observation by itself is a cluster. As $\\gamma$\nincreases, some of the rows of $\\widehat{\\bfA}$ become identical,\nwhich demonstrates a fusion process. For sufficiently large $\\gamma$,\nall the rows of $\\widehat{\\bfA}$ will be identical, implying that\nall the observations are estimated to belong to a single cluster.\nCompared to traditional non-convex clustering methods, the\nsolution $\\widehat{\\bfA}$ from convex clustering is unique for\neach given $\\gamma$ since the objective function in (\\ref{eq:obj-cc})\nis strictly convex.\n\nIn recent years, the computational and statistical properties of convex\nclustering have been investigated. In particular, \\citet{Zhu2014} provided conditions for convex clustering\nto recover the true clusters, \\citet{Chi2015} proposed efficient\nand scalable implementations for convex clustering. \\citet{Tan2015}\nstudied several statistical properties of convex clustering. While convex clustering enjoys nice theoretical properties and is\ncomputationally efficient, its performance can be severely deteriorated\nwhen clustering high-dimensional data where the number of features\nbecomes large and many of them may contain no information about the\nclustering structure. Our extensive experimental studies demonstrate\nthat in high-dimensional scenarios the performance of convex clustering\nis unsatisfactory when the uninformative features are included in\nthe clustering. To overcome such a difficulty, a more appropriate convex\nclustering algorithm that can simultaneously perform cluster analysis\nand select informative variables is in demand.\n\nIn this article, we introduce a new clustering method, \\textit{Sparse\nConvex Clustering}, to incorporate the sparsity into convex clustering of high dimensional data. The key idea is to formulate convex clustering\nin a form of regularization, with an adaptive group-lasso penalty\nterm on cluster centers to encourage the sparsity. Despite its simplicity,\nthis regularization operator demands more challenging computational\nand statistical analysis than those in original convex clustering.\nIn particular, computationally, we need to reformulate the sparse convex\nclustering into a few sub-optimization problems and then solve each\nindividual one via a pseudo regression formulation. To prove an unbiased\nestimator for the degrees of freedom of the proposed sparse convex clustering\nmethod, we need to carefully quantify the impact of variable selection\ndue to the group lasso penalty. Our sparse convex clustering method is not only theoretical sound, but also practically promising. The superior performance\nof our procedure is demonstrated in extensive simulated examples and\na real application of hand movement clustering.\n\n\n\\subsection{Related Work}\n\nA related paper on convex clustering is its efficient implementations proposed by \\citet{Chi2015}.  Two efficient algorithms ADMM and AMA are introduced while they are mainly designed for clustering low-dimensional data. In order\nto address high dimensionality, one key ingredient of our sparse convex\nclustering method is a new regularization penalty built upon their\nADMM and AMA algorithms to encourage the sparsity structure of the\nclustering centers. As will be shown in experimental studies, such\nregularization step is able to significantly improve the clustering\naccuracy in high-dimensional clustering problems.\n\nAnother line of research focuses on simultaneous clustering and feature\nselection. Some approaches are model-based clustering methods, such\nas \\citet{Raftery.Dean:2006}, \\citet{Pan.Shen:2007}, \\citet{Wang.Zhu:2008}, \\citet{Xie.ea:2008}, and \\citet{Guo.ea:2010}. In contrast,\nsome approaches are model-free, such as \\citet{Witten.Tibshirani:2010}, \\citet{Sun2012}, and \\citet{Wang.ea:2013}. One common building block of\nthese sparse clustering approaches is the usage of a lasso-type penalty\nfor feature selection. For example, \\citet{Witten.Tibshirani:2010}\ndeveloped a unified framework for feature selection in clustering using the\nlasso penalty \\citep{tibshirani:1996}. \\citet{Sun2012} proposed a sparse k-means using the group-lasso penalty \\citep{Yuan2006}.\nWe refer readers to \\citet{Alelyani.ea:2013} for a thorough overview. In spite of their good numeric performance, these\nsparse clustering procedures still suffer from instabilities due to\nthe non-convex optimization formulations. To overcome it, our sparse\nconvex clustering solves a convex optimization problem and ensures\na unique global solution.\n\n\n\\subsection{Paper Organization}\n\nThe rest of the manuscript is organized as follows. Section \\ref{sec:main}\nintroduces the sparse convex clustering as well as its two efficient algorithms\nand studies its statistical properties. Section \\ref{sec:practical}\ndiscusses practical choices of parameters in the proposed implementations.\nSection \\ref{sec:experiment} evaluates the superior numeric performance\nof the proposed methods through extensive simulations and a real data\napplication. Technical details are provided in Appendix.\n\n\n\\section{Sparse Convex Clustering}\n\n\\label{sec:main}\n\nThis section introduces main results of our sparse convex clustering.\nWe first consider a reformulation of the traditional convex clustering\nand then discuss its sparse extension in Section \\ref{sec:model}.\nThen we provide two efficient algorithms for solving the sparse convex\nclustering in Section \\ref{sec:algorithm} and demonstrate its statistical properties in Section \\ref{sec:thm}.\n\n\n\\subsection{Model}\n\n\\label{sec:model}\n\nTo allow an adaptive penalization, we consider a modification of convex\nclustering (\\ref{eq:obj-cc}),\n\n\n", "itemtype": "equation", "pos": 2867, "prevtext": "\n\\pagenumbering{arabic}\n\\setcounter{page}{1}\n\\baselineskip=14pt\n\n\\begin{center}\n{\\Large {Sparse Convex Clustering}.arg} \\\\\n\n\\vskip 3mm\n\n{ \\medskip Binhuan Wang$^{1\\ast}$, Yilong Zhang$^1$, Wei Sun$^2$, Yixin Fang$^1$\\\\ \\medskip {\\it $^1$New York University School of Medicine}\\\\ {\\it $^2$ Yahoo! Lab} }.arg\n\\end{center}\n\n\\footnote\n{ Correspondence to: 650 First Avenue Rm 578, New York, NY 10016; Email: Binhuan.Wang@nyumc.org}\n\n\n\\vskip 3mm\n\n\\date{}\n\n\n\\begin{abstract}\nConvex clustering, a convex relaxation of k-means clustering and hierarchical\nclustering, has drawn recent attentions since it nicely addresses\nthe instability issue of traditional non-convex clustering methods.\nAlthough its computational and statistical properties have been recently\nstudied, the performance of convex clustering has not yet been investigated\nin the high-dimensional clustering scenario, where the data contains a\nlarge number of features and many of them carry no information about\nthe clustering structure. In this paper, we demonstrate that the performance\nof convex clustering could be distorted when the uninformative features\nare included in the clustering. To overcome it, we introduce a new\nclustering method, referred to as \\textit{Sparse Convex Clustering},\nto simultaneously cluster observations and conduct feature selection.\nThe key idea is to formulate convex clustering in a form of regularization,\nwith an adaptive group-lasso penalty term on cluster centers. In order\nto optimally balance the trade-off between the cluster fitting and\nsparsity, a tuning criterion based on clustering stability is developed.\nIn theory, we provide an unbiased estimator for the degrees of freedom of the proposed sparse convex clustering method. Finally, the effectiveness\nof the sparse convex clustering is examined through a variety of numerical\nexperiments and a real data application.\n\\end{abstract}\n\\vskip .1 in {\\textbf{Key words}: \\textit{Convex clustering; Group LASSO; High-dimensionality;\nK-means; Sparsity}}\n\n\\noindent\n\n\\doublespacing\n\n\n\\section{Introduction}\n\nCluster analysis is an unsupervised learning method and aims to assign\nobservations into a number of clusters such that observations in the\nsame group are similar to each other. Traditional clustering methods\nsuch as k-means clustering, hierarchical clustering, and Gaussian mixture\nmodels take a greedy approach and suffer from instabilities due to\ntheir non-convex optimization formulations.\n\nTo overcome the instability issues of these traditional clustering methods, a new clustering algorithm, \\textit{Convex Clustering},\nhas been recently proposed \\citep{Lindsten.ea:2004,Hocking2011}. Let\n$\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ be a data matrix with $n$\nobservations $X_{i\\cdot}$, $i=1,\\cdots,n$, and $p$ features. Convex\nclustering for these $n$ observations solves the following minimization\nproblem:\n\n", "index": 1, "text": "\\begin{equation}\n\\min_{\\mathbf{A}\\in \\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{i=1}^{n}||X_{i\\cdot}-A_{i\\cdot}||_{2}^{2}+\\gamma \\sum_{i_{1}<i_{2}}||A_{i_{1}\\cdot}-A_{i_{2}\\cdot}||_{q},\\label{eq:obj-cc}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{A}\\in\\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{i=1}^{n}||X_{i\\cdot%&#10;}-A_{i\\cdot}||_{2}^{2}+\\gamma\\sum_{i_{1}&lt;i_{2}}||A_{i_{1}\\cdot}-A_{i_{2}\\cdot}%&#10;||_{q},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc00</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><mi>i</mi><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>&lt;</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></munder><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>2</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>q</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nwhere the weight $w_{i_{1},i_{2}}\\ge0$. \\citet{Hocking2011} considered a pairwise\naffinity weight $w_{i_{1},i_{2}}=\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$\nand \\citet{Chi2015} suggested $w_{i_{1},i_{2}}=\\iota_{i_{1},i_{2}}^{m}\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$,\nwhere $\\iota_{i_{1},i_{2}}^{m}$ is 1 if observation $i_{2}$ is among\n$i_{1}$'s $m$ nearest neighbors or vice verse, and 0 otherwise.\n\nTo introduce a reformulation of (\\ref{eq:obj-cc-w}), we write the\ndata matrix $\\mathbf{X}$ in feature-level as column vector $\\mathbf{X}=(\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{p})$,\nwhere $\\mathbf{x}_{j}=(X_{1j},\\cdots,X_{nj})^{\\trans}$, $j=1,\\ldots,p$\nand denote $\\mathbf{A}$ in feature-level as column vector $\\mathbf{A}=(\\mathbf{a}_{1},\\cdots,\\mathbf{a}_{p})$.\nWithout loss of generality, we assume the feature vectors are centered,\ni.e., $\\sum_{i=1}^{n}X_{ij}=0$ for each $j=1,\\ldots, p$. Simple algebra implies that (\\ref{eq:obj-cc-w})\ncan be reformulated as\n\n", "itemtype": "equation", "pos": 9738, "prevtext": "\n where $A_{i\\cdot}$ is the $i$-th row of $\\mathbf{A}$ and $\\|\\cdot\\|_{q}$ is the $L_{q}$-norm of a vector with $q \\in \\{1,2,\\infty\\}$. Note that both k-means clustering and hierarchical\nclustering consider $L_{0}$-norm in the second term, which leads to\na non-convex optimization problem \\citep{Hocking2011,Tan2015}. Therefore, convex\nclustering can be viewed as a convex relaxation of k-means clustering and hierarchical\nclustering, and the convex relaxation ensures that it achieves a unique\nglobal minimizer.\n\nDue to the fused-lasso penalty \\citep{Tibshirani.ea:2005} in the second term of (\\ref{eq:obj-cc}),\nthe above formulation encourages that some of the rows of the solution $\\widehat{\\bfA}$\nare identical. If $\\widehat{A}_{i_{1}\\cdot}=\\widehat{A}_{i_{2}\\cdot}$,\nthen observation $i_{1}$ and observation $i_{2}$ are said to belong\nto the same cluster. The tuning parameter $\\gamma$ in (\\ref{eq:obj-cc})\ncontrols the number of unique rows of $\\widehat{\\bfA}$, that is,\nthe number of estimated clusters. When $\\gamma=0$, $\\widehat{\\bfA}={\\bfX}$,\nand therefore each observation by itself is a cluster. As $\\gamma$\nincreases, some of the rows of $\\widehat{\\bfA}$ become identical,\nwhich demonstrates a fusion process. For sufficiently large $\\gamma$,\nall the rows of $\\widehat{\\bfA}$ will be identical, implying that\nall the observations are estimated to belong to a single cluster.\nCompared to traditional non-convex clustering methods, the\nsolution $\\widehat{\\bfA}$ from convex clustering is unique for\neach given $\\gamma$ since the objective function in (\\ref{eq:obj-cc})\nis strictly convex.\n\nIn recent years, the computational and statistical properties of convex\nclustering have been investigated. In particular, \\citet{Zhu2014} provided conditions for convex clustering\nto recover the true clusters, \\citet{Chi2015} proposed efficient\nand scalable implementations for convex clustering. \\citet{Tan2015}\nstudied several statistical properties of convex clustering. While convex clustering enjoys nice theoretical properties and is\ncomputationally efficient, its performance can be severely deteriorated\nwhen clustering high-dimensional data where the number of features\nbecomes large and many of them may contain no information about the\nclustering structure. Our extensive experimental studies demonstrate\nthat in high-dimensional scenarios the performance of convex clustering\nis unsatisfactory when the uninformative features are included in\nthe clustering. To overcome such a difficulty, a more appropriate convex\nclustering algorithm that can simultaneously perform cluster analysis\nand select informative variables is in demand.\n\nIn this article, we introduce a new clustering method, \\textit{Sparse\nConvex Clustering}, to incorporate the sparsity into convex clustering of high dimensional data. The key idea is to formulate convex clustering\nin a form of regularization, with an adaptive group-lasso penalty\nterm on cluster centers to encourage the sparsity. Despite its simplicity,\nthis regularization operator demands more challenging computational\nand statistical analysis than those in original convex clustering.\nIn particular, computationally, we need to reformulate the sparse convex\nclustering into a few sub-optimization problems and then solve each\nindividual one via a pseudo regression formulation. To prove an unbiased\nestimator for the degrees of freedom of the proposed sparse convex clustering\nmethod, we need to carefully quantify the impact of variable selection\ndue to the group lasso penalty. Our sparse convex clustering method is not only theoretical sound, but also practically promising. The superior performance\nof our procedure is demonstrated in extensive simulated examples and\na real application of hand movement clustering.\n\n\n\\subsection{Related Work}\n\nA related paper on convex clustering is its efficient implementations proposed by \\citet{Chi2015}.  Two efficient algorithms ADMM and AMA are introduced while they are mainly designed for clustering low-dimensional data. In order\nto address high dimensionality, one key ingredient of our sparse convex\nclustering method is a new regularization penalty built upon their\nADMM and AMA algorithms to encourage the sparsity structure of the\nclustering centers. As will be shown in experimental studies, such\nregularization step is able to significantly improve the clustering\naccuracy in high-dimensional clustering problems.\n\nAnother line of research focuses on simultaneous clustering and feature\nselection. Some approaches are model-based clustering methods, such\nas \\citet{Raftery.Dean:2006}, \\citet{Pan.Shen:2007}, \\citet{Wang.Zhu:2008}, \\citet{Xie.ea:2008}, and \\citet{Guo.ea:2010}. In contrast,\nsome approaches are model-free, such as \\citet{Witten.Tibshirani:2010}, \\citet{Sun2012}, and \\citet{Wang.ea:2013}. One common building block of\nthese sparse clustering approaches is the usage of a lasso-type penalty\nfor feature selection. For example, \\citet{Witten.Tibshirani:2010}\ndeveloped a unified framework for feature selection in clustering using the\nlasso penalty \\citep{tibshirani:1996}. \\citet{Sun2012} proposed a sparse k-means using the group-lasso penalty \\citep{Yuan2006}.\nWe refer readers to \\citet{Alelyani.ea:2013} for a thorough overview. In spite of their good numeric performance, these\nsparse clustering procedures still suffer from instabilities due to\nthe non-convex optimization formulations. To overcome it, our sparse\nconvex clustering solves a convex optimization problem and ensures\na unique global solution.\n\n\n\\subsection{Paper Organization}\n\nThe rest of the manuscript is organized as follows. Section \\ref{sec:main}\nintroduces the sparse convex clustering as well as its two efficient algorithms\nand studies its statistical properties. Section \\ref{sec:practical}\ndiscusses practical choices of parameters in the proposed implementations.\nSection \\ref{sec:experiment} evaluates the superior numeric performance\nof the proposed methods through extensive simulations and a real data\napplication. Technical details are provided in Appendix.\n\n\n\\section{Sparse Convex Clustering}\n\n\\label{sec:main}\n\nThis section introduces main results of our sparse convex clustering.\nWe first consider a reformulation of the traditional convex clustering\nand then discuss its sparse extension in Section \\ref{sec:model}.\nThen we provide two efficient algorithms for solving the sparse convex\nclustering in Section \\ref{sec:algorithm} and demonstrate its statistical properties in Section \\ref{sec:thm}.\n\n\n\\subsection{Model}\n\n\\label{sec:model}\n\nTo allow an adaptive penalization, we consider a modification of convex\nclustering (\\ref{eq:obj-cc}),\n\n\n", "index": 3, "text": "\\begin{equation}\n\\min_{\\mathbf{A}\\in \\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{i=1}^{n}||X_{i\\cdot}-A_{i\\cdot}||_{2}^{2}+\\gamma\\sum_{i_{1}<i_{2}}w_{i_{1},i_{2}}||A_{i_{1}\\cdot}-A_{i_{2}\\cdot}||_{q},\\label{eq:obj-cc-w}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{A}\\in\\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{i=1}^{n}||X_{i\\cdot%&#10;}-A_{i\\cdot}||_{2}^{2}+\\gamma\\sum_{i_{1}&lt;i_{2}}w_{i_{1},i_{2}}||A_{i_{1}\\cdot}%&#10;-A_{i_{2}\\cdot}||_{q},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc00</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>X</mi><mrow><mi>i</mi><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><mi>i</mi><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>&lt;</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></munder><mrow><msub><mi>w</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>2</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>q</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nwhere ${\\cal E}=\\{l=(i_{1},i_{2}):1\\leq i_{1}<i_{2}\\leq n\\}$.\n\nFor a given $\\gamma$, let $\\widehat{\\mathbf{A}}=(\\widehat{A}_{1\\cdot},\\cdots,\\widehat{A}_{n.})^{\\trans}=(\\widehat{\\mathbf{a}}_{1},\\ldots,\\widehat{\\mathbf{a}}_{p})$\nbe the solution to (\\ref{eq:obj-cc-w-new}). The clustering structure\nis implied by the observation-level estimates, $\\widehat{A}_{i\\cdot}$,\n$i=1,\\ldots,n$; that is, if $\\widehat{A}_{i_{1}\\cdot}=\\widehat{A}_{i_{2}\\cdot}$,\nthen observations $i_{1}$ and $i_{2}$ are estimated to belong to\nthe same cluster. The feature importance is implied by the feature-level\nestimates, $\\widehat{\\mathbf{a}}_{j}$, $j=1,\\cdots,p$; that is, if the\ncomponents of a feature-level estimate $\\widehat{\\mathbf{a}}_{j}$ are\nidentical, then the corresponding feature $j$ is not informative\nfor clustering. Remind that the feature vectors are centered, then\nfeature $j$ is not informative if and only if $\\|\\widehat{\\mathbf{a}}_{j}\\|_{2}^{2}=\\sum_{i=1}^{n}\\widehat{A}_{ij}^{2}=0$.\n\nIn high-dimensional clustering, it is desired to have a sparse solution\n$\\wh{\\bfA}$ with some of its column vectors being exact $\\bfzero$'s.\nMotivated by the importance of excluding non-informative features, we propose a new sparse convex clustering by incorporating\nan adaptive group-lasso penalty \\citep{Yuan2006,Wang.Leng:2008} into\nthe convex clustering objective function (\\ref{eq:obj-cc-w-new}).\nIn particular, sparse convex clustering solves\n\n\n", "itemtype": "equation", "pos": 10957, "prevtext": "\nwhere the weight $w_{i_{1},i_{2}}\\ge0$. \\citet{Hocking2011} considered a pairwise\naffinity weight $w_{i_{1},i_{2}}=\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$\nand \\citet{Chi2015} suggested $w_{i_{1},i_{2}}=\\iota_{i_{1},i_{2}}^{m}\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$,\nwhere $\\iota_{i_{1},i_{2}}^{m}$ is 1 if observation $i_{2}$ is among\n$i_{1}$'s $m$ nearest neighbors or vice verse, and 0 otherwise.\n\nTo introduce a reformulation of (\\ref{eq:obj-cc-w}), we write the\ndata matrix $\\mathbf{X}$ in feature-level as column vector $\\mathbf{X}=(\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{p})$,\nwhere $\\mathbf{x}_{j}=(X_{1j},\\cdots,X_{nj})^{\\trans}$, $j=1,\\ldots,p$\nand denote $\\mathbf{A}$ in feature-level as column vector $\\mathbf{A}=(\\mathbf{a}_{1},\\cdots,\\mathbf{a}_{p})$.\nWithout loss of generality, we assume the feature vectors are centered,\ni.e., $\\sum_{i=1}^{n}X_{ij}=0$ for each $j=1,\\ldots, p$. Simple algebra implies that (\\ref{eq:obj-cc-w})\ncan be reformulated as\n\n", "index": 5, "text": "\\begin{equation} \\label{eq:obj-cc-w-new}\n\\min_{\\mathbf{A}\\in \\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma\\sum_{l\\in{\\cal E}}w_{l}||A_{i_{1}\\cdot}-A_{i_{2}\\cdot}||_{q},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{A}\\in\\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x%&#10;}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma\\sum_{l\\in{\\cal E}}w_{l}||A_{i_{1}\\cdot}-%&#10;A_{i_{2}\\cdot}||_{q},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc00</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>-</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><mi>\u03b3</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></munder><mrow><msub><mi>w</mi><mi>l</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>2</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>q</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nwhere tuning parameter $\\gamma_{1}$ controls the cluster size and\ntuning parameter $\\gamma_{2}$ controls the number of informative\nfeatures. In the group-lasso penalty, the weight $u_{j}$ plays\nan important role to adaptively penalize the features. Detailed discussions on practical choices of tuning parameters and weights can be found in Section \\ref{sec:practical}.\n\n\n\n\n\\subsection{Algorithms}\n\n\\label{sec:algorithm}\n\nThis subsection discusses two efficient optimization approaches to\nsolve the sparse convex clustering by adopting a similar computational\nstrategy used in \\citet{Chi2015}. Our two approaches are based on\nthe alternating direction method of multipliers (ADMM) algorithm \\citep{Boyd2011,Gabay1976,Glowinski1975}\nand the alternating minimization algorithm (AMA) \\citep{Tseng1991},\nand are referred as sparse ADMM (S-ADMM) and sparse AMA (S-AMA), respectively.\n\nTo implement the S-ADMM and S-AMA algorithms, we rewrite the\nconvex clustering problem in formula (\\ref{eq:obj_constraint}) as\n\\begin{eqnarray*}\n\\min_{\\bfA\\in \\mathbb{R}^{n\\times p}} && \\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}||\\mathbf{v}_{l}||_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}||\\mathbf{a}_{j}||_{2}, \\\\\n{\\rm s.t.} && A_{i_1\\cdot}-A_{i_2\\cdot}-\\bfv_l=\\bfzero.\n\\end{eqnarray*}\nThis is equivalent to minimize the following augmented Lagrangian\nfunction,\n\\begin{eqnarray*}\n\\mathcal{L}_{\\nu}(\\mathbf{A},\\mathbf{V},\\mathbf{\\Lambda}) & = & \\frac{1}{2}\\sum_{j=1}^{p}\\|\\mathbf{x}_{j}-\\mathbf{a}_{j}\\|_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}\\|\\mathbf{v}_{l}\\|_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{i}\\|\\mathbf{a}_{j}\\|_{2}\\\\\n &  & +\\sum_{l\\in\\mathbf{{\\cal E}}}\\langle\\mathbf{\\lambda}_{l},\\mathbf{v}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\rangle+\\frac{\\nu}{2}\\sum_{l\\in{\\cal E}}\\|\\mathbf{v}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\|_{2}^{2},\n\\end{eqnarray*}\nwhere $\\nu$ is a small constant, $\\mathbf{V}=(\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{|{\\cal E}|})$,\nand $\\mathbf{\\Lambda}=(\\mathbf{\\lambda}_{1},\\ldots,\\mathbf{\\lambda}_{|{\\cal E}|})$.\nCompared with the original algorithms proposed in \\citet{Chi2015},\nit becomes challenging to deal with the feature-level and observation-level\nvectors in the new objective function simultaneously.\n\n\n\\subsubsection{S-ADMM}\n\nS-ADMM minimizes the augmented Lagrangian problem by alternatively\nsolving one block of variables at a time. In particular, S-ADMM solves\n\\begin{eqnarray}\n\\mathbf{A}^{m+1} & = & {\\mathop{\\rm argmin}}_{\\mathbf{A}}{\\cal L}_{\\nu}(\\mathbf{A},\\mathbf{V}^{m},\\mathbf{\\Lambda}^{m}),\\nonumber \\\\\n\\mathbf{V}^{m+1} & = & {\\mathop{\\rm argmin}}_{\\mathbf{V}}{\\cal L}_{\\nu}(\\mathbf{A}^{m+1},\\mathbf{V},\\mathbf{\\Lambda}^{m}),\\\\\n\\bLambda^{m+1} &=& {\\mathop{\\rm argmin}}_{\\bLambda} \\calL_{\\nu} (\\bfA^{m+1},\\bfV^{m+1},\\bLambda). \\nonumber\n\n\\end{eqnarray}\n\n\nNext we discuss the detailed updating implementations for $\\mathbf{A},\\mathbf{V}$\nand $\\mathbf{\\Lambda}$ in three steps. A summary of the S-ADMM algorithm\nis shown in Algorithm \\ref{algADMM}.\n\n\\textbf{Step 1: update $\\mathbf{A}$.} Denote $\\wt{\\bfv}_{l}=\\bfv_{l}+\\frac{1}{\\nu}\\blambda_{l}$.\nUpdating $\\bfA$ is equivalent to minimizing\n\\begin{eqnarray}\nf(\\bfA)=\\frac{1}{2}\\sum_{j=1}^{p}\\|\\bfx_{j}-\\bfa_{j}\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\|\\wt{\\bfv}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2}.\\label{eqn:update_A}\n\\end{eqnarray}\n\n\nThis optimization problem is challenging because the objective function involves both rows and columns\nof the matrix $\\bfA$. To tackle this difficulty, the following key lemma associates $(\\ref{eqn:update_A})$\nwith a group-lasso regression problem which can be efficiently\nsolved via standard packages.\n\n\\begin{lemma} \\label{thm:equivalent}\nLet $\\bfI_{n}$ be an $n\\times n$ identity\nmatrix, $\\bfone_{n}$ be an $n$-dimensional\nvector with each component being 1, and $\\bfe_{i}$ be an $n$-dimensional\nvector with each component being 0 but its $i$-th component being\n1. Define $\\bfN^{-1}=(1+n\\nu)^{-1/2}[\\bfI_{n}+n^{-1}(\\sqrt{1+n\\nu}-1)\\bfone_{n}\\bfone_{n}\\trans]$\nand denote $\\bfy_{j}=\\bfN^{-1}[\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})]$\nwith $\\wt{v}_{jl}$ the $j$-th element of $\\wt{\\bfv}_{l}$. Then,\nminimizing $(\\ref{eqn:update_A})$ is equivalent to\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}, \\textrm{~for~each~} j=1,\\ldots,p.\n\\end{eqnarray*}\n\\end{lemma}\n\nThe proof of Lemma \\ref{thm:equivalent} is discussed in Appendix.\nThe key ingredient in the proof is a newly established property of\na permutation matrix, i.e., Proposition \\ref{prop1}. Based on this\nproperty, we are able to solve the minimization of $f(\\bfA)$ by $p$\nseparate sub-optimization problems. This together with the property\nof group-lasso penalty leads to desirable results.\n\n\\textbf{Step 2: update $\\bfV$.} For any $\\sigma>0$ and norm $\\Omega(\\cdot)$,\nwe define a proximal map,\n\\begin{eqnarray*}\n\\textrm{prox}_{\\sigma\\Omega}(\\bfu)={\\mathop{\\rm argmin}}_{\\bfv}\\left[\\sigma\\Omega(\\bfv)+\\frac{1}{2}\\|\\bfu-\\bfv\\|_{2}^{2}\\right].\n\\end{eqnarray*}\nIn S-ADMM, $\\Omega(\\cdot)$ is a $q$-norm $\\|\\cdot\\|_{q}$ with $q=1,2$,\nor $\\infty$, and $\\sigma=\\gamma_{1}w_{l}/\\nu$. We refer the readers to Table 1 of \\citet{Chi2015} for the explicit formulations\nof the proximal map of $q$-norm for $q=1,2$ and $\\infty$. Because vectors ${\\bfv}_{l}$ are\nseparable, they can be solved via proximal maps, that is\n\\begin{eqnarray*}\n\\bfv_{l} & = & {\\mathop{\\rm argmin}}_{\\bfv_{l}}\\frac{1}{2}\\|\\bfv_{l}-(A_{i_{1}\\cdot}-A_{i_{2}\\cdot}-\\nu^{-1}\\blambda_{l})\\|_{2}^{2}+\\frac{\\gamma_{1}w_{l}}{\\nu}\\|\\bfv_{l}\\|_{q}\\\\\n & = & \\textrm{prox}_{\\sigma_{l}\\|\\cdot\\|_{q}}(A_{i_{1}\\cdot}-A_{i_{2}\\cdot}-\\nu^{-1}\\blambda_{l}).\n\\end{eqnarray*}\n\n\n\\textbf{Step 3: update $\\bLambda$.} Finally, $\\blambda_{l}$ can\nbe updated by $\\blambda_{l}=\\blambda_{l}+\\nu(\\bfv_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot})$.\n\n\\begin{algorithm}[H]\n\\protect\\caption{\\quad{}S-ADMM \\label{algADMM}}\n\n\\begin{enumerate}\n\\item Initialize $\\mathbf{V}^{0}$ and $\\mathbf{\\Lambda}^{0}$. For $m=1,2,\\ldots$\n\\item For $j=1,\\ldots,p$, do\n\\begin{eqnarray*}\n\\wt{\\bfv}_{l}^{m-1} & = & \\bfv_{l}^{m-1}+\\frac{1}{\\nu}\\blambda_{l}^{m-1}, l\\in\\calE \\\\\n\\bfy_{j}^{m-1} & = & \\bfN^{-1}\\left(\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{lj}^{m-1}(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\right),\\\\\n\\bfa_{j}^{m} & = & {\\mathop{\\rm argmin}}_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}^{m-1}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n\\begin{eqnarray*}\n\\bfv_{l}^{m}=\\textrm{prox}_{\\sigma_{l}\\|\\cdot\\|_{q}}(A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m}-\\nu^{-1}\\blambda_{l}^{m-1}).\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n\\begin{eqnarray*}\n\\blambda_{l}^{m}=\\blambda_{l}^{m-1}+\\nu(\\bfv_{l}^{m}-A_{i_{1}\\cdot}^{m}+A_{i_{2}\\cdot}^{m}).\n\\end{eqnarray*}\n\n\\item Repeat Steps 2-4 until convergence. \\end{enumerate}\n\\end{algorithm}\n\n\n\n\\subsubsection{S-AMA}\n\nTo increase the computational efficiency, we introduce another algorithm S-AMA for implementing sparse convex clustering. S-AMA is different\nfrom S-ADMM in the update of $\\bfA$. In particular, S-AMA solves\n$\\bfA$ by treating $\\nu=0$, i.e., $\\bfA^{m+1}={\\mathop{\\rm argmin}}_{\\bfA}\\calL_{0}(\\bfA,\\bfV^{m},\\bLambda^{m})$.\nWhen $\\nu=0$, we have $\\mathbf{N}=\\mathbf{I}_{n}$ and $\\bfy_{j}=\\bfx_{j}$.\nAccording to Lemma \\ref{thm:equivalent}, updating $\\bfA$ requires to solve $p$\ngroup-lasso problems:\n\n", "itemtype": "equation", "pos": 12625, "prevtext": "\nwhere ${\\cal E}=\\{l=(i_{1},i_{2}):1\\leq i_{1}<i_{2}\\leq n\\}$.\n\nFor a given $\\gamma$, let $\\widehat{\\mathbf{A}}=(\\widehat{A}_{1\\cdot},\\cdots,\\widehat{A}_{n.})^{\\trans}=(\\widehat{\\mathbf{a}}_{1},\\ldots,\\widehat{\\mathbf{a}}_{p})$\nbe the solution to (\\ref{eq:obj-cc-w-new}). The clustering structure\nis implied by the observation-level estimates, $\\widehat{A}_{i\\cdot}$,\n$i=1,\\ldots,n$; that is, if $\\widehat{A}_{i_{1}\\cdot}=\\widehat{A}_{i_{2}\\cdot}$,\nthen observations $i_{1}$ and $i_{2}$ are estimated to belong to\nthe same cluster. The feature importance is implied by the feature-level\nestimates, $\\widehat{\\mathbf{a}}_{j}$, $j=1,\\cdots,p$; that is, if the\ncomponents of a feature-level estimate $\\widehat{\\mathbf{a}}_{j}$ are\nidentical, then the corresponding feature $j$ is not informative\nfor clustering. Remind that the feature vectors are centered, then\nfeature $j$ is not informative if and only if $\\|\\widehat{\\mathbf{a}}_{j}\\|_{2}^{2}=\\sum_{i=1}^{n}\\widehat{A}_{ij}^{2}=0$.\n\nIn high-dimensional clustering, it is desired to have a sparse solution\n$\\wh{\\bfA}$ with some of its column vectors being exact $\\bfzero$'s.\nMotivated by the importance of excluding non-informative features, we propose a new sparse convex clustering by incorporating\nan adaptive group-lasso penalty \\citep{Yuan2006,Wang.Leng:2008} into\nthe convex clustering objective function (\\ref{eq:obj-cc-w-new}).\nIn particular, sparse convex clustering solves\n\n\n", "index": 7, "text": "\\begin{equation}\\label{eq:obj_constraint}\n\\min_{\\mathbf{A}\\in \\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}||A_{i_{1}\\cdot}-A_{i_{2}\\cdot}||_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}||\\mathbf{a}_{j}||_{2},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\mathbf{A}\\in\\mathbb{R}^{n\\times p}}\\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x%&#10;}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}||A_{i_{1}%&#10;\\cdot}-A_{i_{2}\\cdot}||_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}||\\mathbf{a}_{j}||_{2},\" display=\"block\"><mrow><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>\ud835\udc00</mi><mo>\u2208</mo><msup><mi>\u211d</mi><mrow><mi>n</mi><mo>\u00d7</mo><mi>p</mi></mrow></msup></mrow></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>-</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>l</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2130</mi></mrow></munder><mrow><msub><mi>w</mi><mi>l</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub><mo>-</mo><msub><mi>A</mi><mrow><msub><mi>i</mi><mn>2</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>q</mi></msub></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mrow><mo fence=\"true\">||</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub><mo fence=\"true\">||</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nBy Karush-Kuhn-Tucker (KKT) conditions of the group lasso problem \\citep{Yuan2006}, the solution\nto (\\ref{eq:g1}) has a closed form as\n\\begin{eqnarray*}\n\\wh\\bfa_{j}=\\left(1-\\frac{\\gamma_{2}u_{j}}{\\|\\bfz_{j}\\|_{2}}\\right)_{+}\\bfz_{j},\n\\end{eqnarray*}\nwhere $\\bfz_{j}=\\bfx_{j}+\\sum_{l\\in\\calE}\\lambda_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$\nand $(z)_{+}=\\max\\{0,z\\}$. See the detailed derivations in Appendix.\nThe above formula significantly reduces the computational cost by solving\n$p$ group-lasso problem analytically in each iteration. Note that\nthe above update of $\\bfA$ is independent of $\\bfV$, which indicates\nthat S-AMA algorithm does not need to compute the update of $\\bfV$.\nTherefore S-AMA is much more efficient than S-ADMM algorithm.\n\nNext, we discuss the update of $\\bLambda$. Define $\\calP_{B}(\\bfz)$\nas a projection onto $B=\\{\\bfy:\\|\\bfy\\|_{\\dag}\\leq1\\}$ of the norm\n$\\|\\cdot\\|_{\\dag}$, where $\\|\\cdot\\|_{\\dag}$ is the dual norm of\n$\\|\\cdot\\|_{q}$, which defines the fusion penalty. We show in\nAppendix that the update of $\\bLambda$ reduces to $\\blambda_{l}^{m}= \\calP_{C_{l}}[\\blambda_{l}^{m-1}-\\nu(A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m})]$\nwith $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\nThe S-AMA algorithm is summarized in Algorithm \\ref{algAMA}.\n\n\\begin{algorithm}[H]\n\\protect\\caption{\\quad{}S-AMA \\label{algAMA}}\n\n\\begin{enumerate}\n\\item Initialize $\\mathbf{\\Lambda}{}^{0}$. For $m=1,2,\\ldots$\n\\item For $j=1,\\ldots,p$, do\n\\begin{eqnarray*}\n\\bfz_{j}^{m} & = & \\bfx_{j}+\\sum_{l \\in\\calE}\\lambda_{lj}^{m-1}(\\bfe_{i_{1}}-\\bfe_{i_{2}}),\\\\\n\\bfa_{j}^{m} & = & \\left(1-\\frac{\\gamma_{2}u_{i}}{\\|\\bfz_{i}^{m}\\|_{2}}\\right)_{+}\\bfz_{j}^{m}.\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n", "itemtype": "equation", "pos": 20339, "prevtext": "\nwhere tuning parameter $\\gamma_{1}$ controls the cluster size and\ntuning parameter $\\gamma_{2}$ controls the number of informative\nfeatures. In the group-lasso penalty, the weight $u_{j}$ plays\nan important role to adaptively penalize the features. Detailed discussions on practical choices of tuning parameters and weights can be found in Section \\ref{sec:practical}.\n\n\n\n\n\\subsection{Algorithms}\n\n\\label{sec:algorithm}\n\nThis subsection discusses two efficient optimization approaches to\nsolve the sparse convex clustering by adopting a similar computational\nstrategy used in \\citet{Chi2015}. Our two approaches are based on\nthe alternating direction method of multipliers (ADMM) algorithm \\citep{Boyd2011,Gabay1976,Glowinski1975}\nand the alternating minimization algorithm (AMA) \\citep{Tseng1991},\nand are referred as sparse ADMM (S-ADMM) and sparse AMA (S-AMA), respectively.\n\nTo implement the S-ADMM and S-AMA algorithms, we rewrite the\nconvex clustering problem in formula (\\ref{eq:obj_constraint}) as\n\\begin{eqnarray*}\n\\min_{\\bfA\\in \\mathbb{R}^{n\\times p}} && \\frac{1}{2}\\sum_{j=1}^{p}||\\mathbf{x}_{j}-\\mathbf{a}_{j}||_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}||\\mathbf{v}_{l}||_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}||\\mathbf{a}_{j}||_{2}, \\\\\n{\\rm s.t.} && A_{i_1\\cdot}-A_{i_2\\cdot}-\\bfv_l=\\bfzero.\n\\end{eqnarray*}\nThis is equivalent to minimize the following augmented Lagrangian\nfunction,\n\\begin{eqnarray*}\n\\mathcal{L}_{\\nu}(\\mathbf{A},\\mathbf{V},\\mathbf{\\Lambda}) & = & \\frac{1}{2}\\sum_{j=1}^{p}\\|\\mathbf{x}_{j}-\\mathbf{a}_{j}\\|_{2}^{2}+\\gamma_{1}\\sum_{l\\in{\\cal E}}w_{l}\\|\\mathbf{v}_{l}\\|_{q}+\\gamma_{2}\\sum_{j=1}^{p}u_{i}\\|\\mathbf{a}_{j}\\|_{2}\\\\\n &  & +\\sum_{l\\in\\mathbf{{\\cal E}}}\\langle\\mathbf{\\lambda}_{l},\\mathbf{v}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\rangle+\\frac{\\nu}{2}\\sum_{l\\in{\\cal E}}\\|\\mathbf{v}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\|_{2}^{2},\n\\end{eqnarray*}\nwhere $\\nu$ is a small constant, $\\mathbf{V}=(\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{|{\\cal E}|})$,\nand $\\mathbf{\\Lambda}=(\\mathbf{\\lambda}_{1},\\ldots,\\mathbf{\\lambda}_{|{\\cal E}|})$.\nCompared with the original algorithms proposed in \\citet{Chi2015},\nit becomes challenging to deal with the feature-level and observation-level\nvectors in the new objective function simultaneously.\n\n\n\\subsubsection{S-ADMM}\n\nS-ADMM minimizes the augmented Lagrangian problem by alternatively\nsolving one block of variables at a time. In particular, S-ADMM solves\n\\begin{eqnarray}\n\\mathbf{A}^{m+1} & = & {\\mathop{\\rm argmin}}_{\\mathbf{A}}{\\cal L}_{\\nu}(\\mathbf{A},\\mathbf{V}^{m},\\mathbf{\\Lambda}^{m}),\\nonumber \\\\\n\\mathbf{V}^{m+1} & = & {\\mathop{\\rm argmin}}_{\\mathbf{V}}{\\cal L}_{\\nu}(\\mathbf{A}^{m+1},\\mathbf{V},\\mathbf{\\Lambda}^{m}),\\\\\n\\bLambda^{m+1} &=& {\\mathop{\\rm argmin}}_{\\bLambda} \\calL_{\\nu} (\\bfA^{m+1},\\bfV^{m+1},\\bLambda). \\nonumber\n\n\\end{eqnarray}\n\n\nNext we discuss the detailed updating implementations for $\\mathbf{A},\\mathbf{V}$\nand $\\mathbf{\\Lambda}$ in three steps. A summary of the S-ADMM algorithm\nis shown in Algorithm \\ref{algADMM}.\n\n\\textbf{Step 1: update $\\mathbf{A}$.} Denote $\\wt{\\bfv}_{l}=\\bfv_{l}+\\frac{1}{\\nu}\\blambda_{l}$.\nUpdating $\\bfA$ is equivalent to minimizing\n\\begin{eqnarray}\nf(\\bfA)=\\frac{1}{2}\\sum_{j=1}^{p}\\|\\bfx_{j}-\\bfa_{j}\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\|\\wt{\\bfv}_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2}.\\label{eqn:update_A}\n\\end{eqnarray}\n\n\nThis optimization problem is challenging because the objective function involves both rows and columns\nof the matrix $\\bfA$. To tackle this difficulty, the following key lemma associates $(\\ref{eqn:update_A})$\nwith a group-lasso regression problem which can be efficiently\nsolved via standard packages.\n\n\\begin{lemma} \\label{thm:equivalent}\nLet $\\bfI_{n}$ be an $n\\times n$ identity\nmatrix, $\\bfone_{n}$ be an $n$-dimensional\nvector with each component being 1, and $\\bfe_{i}$ be an $n$-dimensional\nvector with each component being 0 but its $i$-th component being\n1. Define $\\bfN^{-1}=(1+n\\nu)^{-1/2}[\\bfI_{n}+n^{-1}(\\sqrt{1+n\\nu}-1)\\bfone_{n}\\bfone_{n}\\trans]$\nand denote $\\bfy_{j}=\\bfN^{-1}[\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})]$\nwith $\\wt{v}_{jl}$ the $j$-th element of $\\wt{\\bfv}_{l}$. Then,\nminimizing $(\\ref{eqn:update_A})$ is equivalent to\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}, \\textrm{~for~each~} j=1,\\ldots,p.\n\\end{eqnarray*}\n\\end{lemma}\n\nThe proof of Lemma \\ref{thm:equivalent} is discussed in Appendix.\nThe key ingredient in the proof is a newly established property of\na permutation matrix, i.e., Proposition \\ref{prop1}. Based on this\nproperty, we are able to solve the minimization of $f(\\bfA)$ by $p$\nseparate sub-optimization problems. This together with the property\nof group-lasso penalty leads to desirable results.\n\n\\textbf{Step 2: update $\\bfV$.} For any $\\sigma>0$ and norm $\\Omega(\\cdot)$,\nwe define a proximal map,\n\\begin{eqnarray*}\n\\textrm{prox}_{\\sigma\\Omega}(\\bfu)={\\mathop{\\rm argmin}}_{\\bfv}\\left[\\sigma\\Omega(\\bfv)+\\frac{1}{2}\\|\\bfu-\\bfv\\|_{2}^{2}\\right].\n\\end{eqnarray*}\nIn S-ADMM, $\\Omega(\\cdot)$ is a $q$-norm $\\|\\cdot\\|_{q}$ with $q=1,2$,\nor $\\infty$, and $\\sigma=\\gamma_{1}w_{l}/\\nu$. We refer the readers to Table 1 of \\citet{Chi2015} for the explicit formulations\nof the proximal map of $q$-norm for $q=1,2$ and $\\infty$. Because vectors ${\\bfv}_{l}$ are\nseparable, they can be solved via proximal maps, that is\n\\begin{eqnarray*}\n\\bfv_{l} & = & {\\mathop{\\rm argmin}}_{\\bfv_{l}}\\frac{1}{2}\\|\\bfv_{l}-(A_{i_{1}\\cdot}-A_{i_{2}\\cdot}-\\nu^{-1}\\blambda_{l})\\|_{2}^{2}+\\frac{\\gamma_{1}w_{l}}{\\nu}\\|\\bfv_{l}\\|_{q}\\\\\n & = & \\textrm{prox}_{\\sigma_{l}\\|\\cdot\\|_{q}}(A_{i_{1}\\cdot}-A_{i_{2}\\cdot}-\\nu^{-1}\\blambda_{l}).\n\\end{eqnarray*}\n\n\n\\textbf{Step 3: update $\\bLambda$.} Finally, $\\blambda_{l}$ can\nbe updated by $\\blambda_{l}=\\blambda_{l}+\\nu(\\bfv_{l}-A_{i_{1}\\cdot}+A_{i_{2}\\cdot})$.\n\n\\begin{algorithm}[H]\n\\protect\\caption{\\quad{}S-ADMM \\label{algADMM}}\n\n\\begin{enumerate}\n\\item Initialize $\\mathbf{V}^{0}$ and $\\mathbf{\\Lambda}^{0}$. For $m=1,2,\\ldots$\n\\item For $j=1,\\ldots,p$, do\n\\begin{eqnarray*}\n\\wt{\\bfv}_{l}^{m-1} & = & \\bfv_{l}^{m-1}+\\frac{1}{\\nu}\\blambda_{l}^{m-1}, l\\in\\calE \\\\\n\\bfy_{j}^{m-1} & = & \\bfN^{-1}\\left(\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{lj}^{m-1}(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\right),\\\\\n\\bfa_{j}^{m} & = & {\\mathop{\\rm argmin}}_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}^{m-1}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n\\begin{eqnarray*}\n\\bfv_{l}^{m}=\\textrm{prox}_{\\sigma_{l}\\|\\cdot\\|_{q}}(A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m}-\\nu^{-1}\\blambda_{l}^{m-1}).\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n\\begin{eqnarray*}\n\\blambda_{l}^{m}=\\blambda_{l}^{m-1}+\\nu(\\bfv_{l}^{m}-A_{i_{1}\\cdot}^{m}+A_{i_{2}\\cdot}^{m}).\n\\end{eqnarray*}\n\n\\item Repeat Steps 2-4 until convergence. \\end{enumerate}\n\\end{algorithm}\n\n\n\n\\subsubsection{S-AMA}\n\nTo increase the computational efficiency, we introduce another algorithm S-AMA for implementing sparse convex clustering. S-AMA is different\nfrom S-ADMM in the update of $\\bfA$. In particular, S-AMA solves\n$\\bfA$ by treating $\\nu=0$, i.e., $\\bfA^{m+1}={\\mathop{\\rm argmin}}_{\\bfA}\\calL_{0}(\\bfA,\\bfV^{m},\\bLambda^{m})$.\nWhen $\\nu=0$, we have $\\mathbf{N}=\\mathbf{I}_{n}$ and $\\bfy_{j}=\\bfx_{j}$.\nAccording to Lemma \\ref{thm:equivalent}, updating $\\bfA$ requires to solve $p$\ngroup-lasso problems:\n\n", "index": 9, "text": "\\begin{equation}\\label{eq:g1}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\mathbf{x}_{j}-\\mathbf{a}_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\mathbf{a}_{j}\\|_{2}, j=1,\\ldots,p.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\mathbf{x}_{j}-\\mathbf{a}_{j}\\|_{2}^{2}+\\gamma_{2}%&#10;u_{j}\\|\\mathbf{a}_{j}\\|_{2},j=1,\\ldots,p.\" display=\"block\"><mrow><mrow><mrow><mrow><mrow><mrow><mrow><munder><mi>min</mi><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub></munder><mo>\u2061</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>\u2062</mo><msubsup><mrow><mo>\u2225</mo><mrow><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>-</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub></mrow><mo>\u2225</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mrow><mo>\u2225</mo><msub><mi>\ud835\udc1a</mi><mi>j</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub></mrow></mrow><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mi>p</mi></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nwhere $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\n\\item Repeat Steps 2-3 until convergence. \\end{enumerate}\n\\end{algorithm}\n\n\n\n\\subsubsection{Algorithmic Convergence}\n\nThis subsection discusses the convergence of the proposed S-ADMM and\nS-AMA algorithms. \\citet{Chi2015} and the references therein provided sufficient conditions for the convergence\nof the following general optimization problem,\n\\begin{eqnarray}\n\\min_{\\xi,\\zeta}\\ f(\\xi)+g(\\zeta),\\textrm{\\ \\ s.t.\\ \\ }A\\xi+B\\zeta=c.\\label{general_obj}\n\\end{eqnarray}\nThey verified that the ADMM and AMA algorithms for\nconvex clustering, as two special cases of \\eqref{general_obj}, satisfied\nthe sufficient conditions under which the convergence was guaranteed.\n\nThe convergence of our S-ADMM and S-AMA algorithms follows similar\narguments. Note that the only difference between the objective function\nin \\eqref{eq:obj_constraint} and its counterpart in \\citet{Chi2015}\nis a convex penalty term $\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\mathbf{a}_{j}\\|_{2}$.\nDefine the summation of the first and third terms of the objective\nfunction in \\eqref{eq:obj_constraint} as $f(\\cdot)$, and the second\nterm as $g(\\cdot)$. This indicates that the optimization problem\n\\eqref{eq:obj_constraint} is a special case of \\eqref{general_obj}.\nSimple algebra implies that $f(\\cdot)$ is strongly convex. According\nto \\citet{Chi2015}, one can show that, under mild regularization conditions, the convergence\nof S-ADMM is guaranteed for any $\\nu>0$, and the convergence\nof S-AMA algorithm is guaranteed provided that positive\nconstant $\\nu$ is not too large.\n\n\\subsubsection{Computational Consideration}\n\nStep 2 in both Algorithms \\ref{algADMM} and \\ref{algAMA} involves $p$ sub-optimization problems. Therefore, S-ADMM and S-AMA merit from the distributed optimization, and they can handle large-scale problems efficiently. To be specific, Step 2 can be distributed to different processors to obtain estimates of $\\bfa_j$'s which are then gathered to update $\\bfA$. In addition, Steps 3-4 in Algorithm \\ref{algADMM} or Step 3 in Algorithm \\ref{algAMA} can also be distributed to different processors to obtain fast updates.\n\nIt is worth pointing out that the computation of S-AMA is comparable to AMA in \\cite{Chi2015}, while S-ADMM is computationally more expensive than ADMM in \\cite{Chi2015} and S-AMA. This is because Step 2 in S-ADMM does not have a closed-form formula and it requires solving $p$ group-lasso problems assisted by iterations. Our limited experience in numerical studies also confirms the superiority of S-AMA over S-ADMM in terms of the computational cost.\n\n\n\\subsection{Statistical Property}\n\n\\label{sec:thm}\n\nIn this section, we provide unbiased estimators for the degrees of freedoms\nof sparse convex clustering. Degrees of freedom is generally defined in regression problems to\nexplain the amount of flexibility in the model. It is a key component\nfor model selection and statistical hypothesis testing. Note that our sparse convex clustering can be formulated as a penalized\nregression problem for which the degrees of freedom can be established.\nMotivated by \\citet{Tan2015}, we develop unbiased estimators for\nthe degrees of freedom of sparse convex clustering with $q=1$ in\nLemma \\ref{thm:df1} and $q=2$ in Lemma \\ref{thm:df2}. Denote a $p$-dimensional multivariate normal distribution as $\\textrm{MVN}_{p}$.\nFor simplicity, we consider the case with $w_l=1, l \\in \\calE$ in the following theoretical developments.\n\n\\begin{lemma} \\label{thm:df1} Assume $X_{i\\cdot}\\stackrel{iid}{\\sim}\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$, and let $\\widehat{\\bfa} \\buildrel \\Delta \\over = \\textrm{vec}(\\wh{\\bfA})$\nbe the solution to $(\\ref{eq:obj_constraint})$ with $q=1$. Then we\nhave ${\\rm df}\\buildrel \\Delta \\over ={\\rm tr}(\\frac{\\partial\\wh\\bfa}{\\partial\\bfx})$\nis of the form\n\\begin{eqnarray*}\n{\\rm df}_{1} & = & {\\rm tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{2}\\bfP_{1}\\sum_{s\\in\\calB_{12}}\\bigg(\\frac{\\bfD_{s}\\trans\\bfD_{s}} {\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh\\bfa\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\bigg)\\Bigg]^{-1}\\bfP_{1}\\Bigg),\n\\end{eqnarray*}\nwhere $\\bfD_{s}$ and $\\bfP_{1}$ are defined in $(\\ref{eqn:defD})$ and $(\\ref{eqn:defP})$, respectively.\n\\end{lemma}\n\nFollowing a similar proof technique, we provide an unbiased estimator for the degrees of freedom of the sparse convex clustering with $q=2$.\n\n\\begin{lemma} \\label{thm:df2} Assume $X_{i\\cdot}\\stackrel{iid}{\\sim}\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$, and let $\\widehat{\\bfa}$\nbe the solution to $(\\ref{eq:obj_constraint})$ with $q=2$. Therefore,\nthe degrees of freedom is\n\\begin{eqnarray*}\n{\\rm df}_{2} & = & {\\rm tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{1}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{1,\\ldots,|\\calE|\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\\\\n &  & +\\gamma_{2}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{|\\calE|+1,\\ldots,|\\calE|+p\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\Bigg]^{-1}\\bfP_{2}\\Bigg).\n\\end{eqnarray*}\n\\end{lemma}\n\nProofs of Lemmas \\ref{thm:df1}-\\ref{thm:df2} are discussed in Appendix.\n\n\n\\section{Practical Issues}\n\n\\label{sec:practical}\n\nIn Section \\ref{sec:algorithm}, the S-ADMM and S-AMA algorithms rely\non the choice of weights and the tuning parameters $\\gamma_{1}$ and\n$\\gamma_{2}$. In this section, we discuss how to choose these parameters\nin practice.\n\n\n\\subsection{Selection of Weights}\n\\label{sec:weights}\n\nThis subsection introduces practical selections of the weights $w_{i_{1},i_{2}}$,\n$(i_{1},i_{2})\\in\\calE$, in the fused-lasso penalty and the factors\n$u_{j}$, $j=1,\\cdots,p$, in the adaptive group-lasso penalty.\n\nFollowing \\citet{Chi2015}, we choose weights by incorporating the\nm-nearest-neighbors method with Gaussian kernel. To be specific, the\nweight between the sample pair $(i_{1},i_{2})$ is set as $w_{i_{1},i_{2}}=\\iota_{i_{1},i_{2}}^{m}\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$,\nwhere $\\iota_{i_{1},i_{2}}^{m}$ equals 1 if observation $i_{2}$\nis among observation $i_{1}$'s $m$ nearest neighbors or vice versa, and\n0 otherwise. This choice of weights works well for a wide range of\n$\\phi$ when $m$ is small. In our numerical results, $m$ is fixed\nat $5$ and $\\phi$ is fixed at 0.5.\n\nNext we consider the selection of the factor $u_{j}$. As suggested\nby \\citet{zou:2006}, $u_{j}$ can be chosen as $1/\\|\\wh{\\bfa}_{j}^{(0)}\\|_{2}$,\nwhere $\\wh\\bfa_{j}^{(0)}$ is the estimate of $\\bfa_{j}$ in \\eqref{eq:obj_constraint}\nwith $\\gamma_{2}=0$. Such choice of factors penalizes less on informative\nfeatures and penalizes more on uninformative features, and hence leads\nto improved clustering accuracy and variable selection performance\nthan its non-adaptive counterpart.\n\nFinally, in order to ensure that the optimal tuning parameters $\\gamma_{1}$\nand $\\gamma_{2}$ lie in relatively robust intervals regardless of\nfeature dimension and sample size, weights $w_{i_{1},i_{2}}$ and\nfactors $u_{j}$ are re-scaled to sum to $1/\\sqrt{p}$ and $1/\\sqrt{n}$,\nrespectively. Such re-scaling is only for convenience and does not\naffect the final clustering path.\n\n\n\\subsection{Selection of Tuning Parameters}\n\n\\label{sec:tuning}\n\nThis subsection provides a selection method for tuning parameters\n$\\gamma_{1}$ and $\\gamma_{2}$. Remind that $\\gamma_{1}$ controls\nthe number of estimated clusters and $\\gamma_{2}$ controls the number\nof selected informative features.\n\nWe first illustrate via a toy example the effectiveness of tuning\nparameter $\\gamma_{2}$ on variable selection accuracy. In this\nexample, $60$ observations with $p=500$ features are generated from\n4 clusters. Among all the features, only $20$ variables differ between\nclusters. See detailed simulation setup in Section \\ref{sec:simulation}.\nBy fixing $\\wh\\gamma_{1}=2.44$ and varying $\\gamma_{2}$ from $e^{-5.0}$\nto $e^{7.0}$, we plot the path of false negative rate (FNR) and the\npath of false positive rate (FPR) of the final estimator. As shown\nin Figure \\ref{demo-graph}, when $\\gamma_{2}$ is close to zero,\nall features are included, and when $\\gamma_{2}$ increases to some\nranges of intervals, all and only uninformative features are excluded,\ni.e., perfect variable selection performance. This illustrates the\nsensitivity of $\\gamma_{2}$ to the variable selection performance\nof the final estimator. In practice, we aim to estimate a suitable\n$\\gamma_{2}$ that leads to satisfactory variable selection.\n\n\\begin{figure}[!htb]\n\\protect\\caption{Illustration of the effectiveness of $\\gamma_{2}$ on variable selection\naccuracy. The solid curve is the path of false negative rate (FNR),\nand the dashed curve is the path of false positive rate (FPR).}\n\n\n\\centering \\includegraphics[scale=0.5]{simu_4grp_p500_fix_gamma1}\\label{demo-graph}\n\\end{figure}\n\nIn literature, \\citet{Wang:2010} and \\citet{Fang2012} proposed stability\nselection to estimate the tuning parameters in clustering models.\nThe idea behind stability selection is that a good tuning parameter\nshould produce clustering results that are stable with respect to\na small perturbation to the training samples. Stability selection well suits\nthe model selection in cluster analysis because cluster labels are unavailable and the cross-validation method is not applicable in this case.\n\nIn this paper, we propose\nto use stability selection in \\citet{Fang2012} to tune both parameters\n$\\gamma_{1}$ and $\\gamma_{2}$. To be specific, for any given $\\gamma_{1}$\nand $\\gamma_{2}$, based on two sets of bootstrapped samples, two\nclustering results can be produced via \\eqref{eq:obj_constraint}, and\nthen the stability measurement \\citep{Fang2012} can be computed to\nmeasure the agreement between the two clustering results. In order\nto enhance the robustness of the stability selection method, we repeat\nthis procedure $50$ times and then compute the averaged stability\nvalue. Finally, the optimal parameter is selected as the one achieving\nmaximum stability. Our extensive numerical studies suggest that the\nperformance of sparse convex clustering is less sensitive to $\\gamma_{1}$.\nThus, to speed up tuning process, stability path can be computed over\nof a coarse grid of $\\gamma_{1}$ and a fine grid of $\\gamma_{2}$.\n\n\n\n\n\n\\section{Numerical Results}\n\n\\label{sec:experiment}\n\nThis section demonstrates the superior performance of our sparse convex\nclustering in simulated examples in Section \\ref{sec:simulation}\nand a real application of hand movement clustering in Section \\ref{sec:real}.\n\n\n\\subsection{Simulation Studies}\n\n\\label{sec:simulation}\n\nIn this subsection, simulations studies are conducted to evaluate\nthe performance of sparse convex clustering (S-ADMM and S-AMA). They\nare compared to k-means clustering and two convex clustering algorithms: ADMM and AMA \\citep{Chi2015}.\n\nFour simulation settings are considered. Each simulated dataset consists\nof $n=60$ observations with the number of clusters either $K=2$\nor 4, and the number of features either $p=150$ or $500$. In each\nsetting, only the first 20 features are informative and remaining\nfeatures are non-informative. The samples $X_{i\\cdot}\\in\\real^{p},i=1,\\ldots,n$,\nare generated as follows. For each $i$, a cluster label $Z_{i}$\nis uniformly sampled from $\\{1,\\ldots,K\\}$, and then the first 20\ninformative features are generated from $\\textrm{MVN}_p(\\bmu_{K}(Z_{i}),\\bfI_{20})$,\nwhere $\\bmu_{K}(Z_{i})$ is defined as follows.\n\\begin{itemize}\n\\item If $K=2$, $\\bmu_{2}(Z_{i})=\\mu\\bfone_{20}I(Z_{i}=1)-\\mu\\bfone_{20}I(Z_{i}=2)$;\n\\item If $K=4$, $\\bmu_{4}(Z_{i})=(\\mu\\bfone_{10}\\trans,-\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=1)+(-\\mu\\bfone_{10}\\trans,-\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=2)+\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (-\\mu\\bfone_{10}\\trans,\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=3)+(\\mu\\bfone_{10}\\trans,\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=4)$,\n\\end{itemize}\nwhere $\\mu$ controls the\ndistance between cluster centers. Here a large $\\mu$ indicates that\nclusters are well-separated, whereas a small $\\mu$ indicates that\nclusters are overlapped. Finally, the rest $p-20$ noise features\nare generated from $\\calN(0,1)$. \n\nIn summary, four simulation settings are considered. Setting 1: $K=2,p=150$,\nand $\\mu=0.6$; Setting 2: $K=2,p=500$, and $\\mu=0.7$; Setting 3:\n$K=4,p=150$, and $\\mu=0.9$; Setting 4: $K=4,p=500$, and $\\mu=1.2$.\nFor each setting, we run 200 repetitions.\n\nThe RAND index \\citep{Rand1971} is used to measure the agreement\nbetween the estimated clustering result and the underlying true clustering\nassignment. The Rand index ranges between 0 and 1, and a higher value\nindicates better performance. Note that the true cluster labels are known in\nsimulation studies, and thus it is feasible to know how well the candidate methods can\nperform if they are tuned by maximizing the RAND index.\nTo ensure fair comparisons, for each\nrepetition, separate validation samples are generated and used to\nselect an optimal k in k-means, an optimal $\\gamma$ in ADMM and AMA, and optimal $\\gamma_{1}$\nand $\\gamma_{2}$ in S-ADMM and S-AMA. To evaluate the performance\nof variable selection, two measurements are reported: the false negative\nrate (FNR) and the false positive rate (FPR). All the simulation results\nare summarized in Table \\ref{tb_simulation_1}. Due to its relatively\nexpensive computational costs, S-ADMM is not evaluated for Settings\n2 and 4 where $p=500$.\n\nIn all these four simulation settings, the centers are spherical and\nhence k-means always performs well in clustering accuracy, i.e., large\nRAND index. The goals of these simulations are to justify that (1)\nconvex clustering does not perform well when the feature dimension\nis high; (2) sparse convex clustering performs very well when the\nfeature dimension is high; and (3) sparse convex clustering selects\ninformative features with great clustering accuracy. All these claims\nare justified by the results presented in Table \\ref{tb_simulation_1}.\n\n\\begin{table}[!htb]\n\\centering \\protect\\caption{Empirical mean and standard deviation (SD) of the RAND index, false\npositive rate (FPR), and false negative rate (FNR) based on 200 repetitions.\nSetting 1: $K=2,p=150$, and $\\mu=0.6$; Setting 2: $K=2,p=500$,\nand $\\mu=0.7$; Setting 3: $K=4,p=150$, and $\\mu=0.9$; Setting 4:\n$K=4,p=500$, and $\\mu=1.2$. The best RAND in each scenario is shown\nin bold. }\n\n\n\\label{tb_simulation_1} \\vspace{6pt}\n \\centering \n\\begin{tabular}{p{2cm}p{2cm}p{1.2cm}p{1.2cm}p{0.1cm}p{1.2cm}p{1.2cm}p{0.1cm}p{1.2cm}p{1.2cm}}\n\\hline\n\\  & \\  & \\multicolumn{2}{c}{RAND} & \\  & \\multicolumn{2}{c}{FNR} & \\  & \\multicolumn{2}{c}{FPR}\\tabularnewline\n\\hline\n & Algorithm  & mean  & SD  & \\  & mean  & SD  & \\  & mean  & SD \\tabularnewline\n\\hline\n\\hline\nSetting 1  & k-means  & 0.95  & 0.06  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.53  & 0.39  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.66  & 0.40  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-ADMM  & 0.82  & 0.24  & \\  & 0.04  & 0.05  & \\  & 0.25  & 0.16 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.96}  & 0.06  & \\  & 0.03  & 0.07  & \\  & 0.30  & 0.21 \\tabularnewline\n\\hline\nSetting 2  & k-means  & 0.95  & 0.11  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.14  & 0.20  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.08  & 0.21  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.97}  & 0.07  & \\  & 0.07  & 0.09  & \\  & 0.11  & 0.10 \\tabularnewline\n\\hline\nSetting 3  & k-means  & 0.83  & 0.15  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.56  & 0.22  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.47  & 0.21  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-ADMM  & 0.82  & 0.14  & \\  & 0.04  & 0.06  & \\  & 0.25  & 0.24 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.84}  & 0.13  & \\  & 0.02  & 0.04  & \\  & 0.11  & 0.18 \\tabularnewline\n\\hline\nSetting 4  & k-means  & 0.89  & 0.14  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.31  & 0.23  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.31  & 0.20  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.94}  & 0.09  & \\  & 0.01  & 0.02  & \\  & 0.01  & 0.03 \\tabularnewline\n\\hline\n\\end{tabular}\n\\end{table}\n\n\nFirst, convex clustering does not perform well when the feature dimension\nis high, even much worse than k-means. Similar phenomenon was also\nobserved in the simulation studies conducted in \\citet{Tan2015}.\nThis is the motivation for developing sparse convex clustering. Second,\nsparse convex clustering improves convex clustering significantly.\nSparse convex clustering (S-AMA) performs as well as k-means when\n$p=150$, and performs better than k-means when $p=500$. Third, sparse\nconvex clustering selects informative feature with great accuracy,\nthat is, with low FNR and FPR. The feature selection performance of\nsparse convex clustering is very promising for settings where $p=500$.\nCompared with S-ADMM, S-AMA is computationally faster and also delivers slightly better accuracy. Therefore, we recommend\nS-AMA in practice.\n\n\n\\subsection{Real Data Application}\n\n\\label{sec:real}\n\nWe evaluate the performance of sparse convex clustering in LIBRAS\nmovement data from the Machine Learning Repository \\citep{Lichman2013}.\nThe original dataset contains 15 classes with each class referring\nto a hand movement type. Each class contains 24 observations, and\neach observation has 90 features consisting of the coordinates of\nhand movements. We use this dataset without the clustering assignments\nto evaluate each clustering algorithms and then compare the results\nwith the true classes to compute the RAND index. Before cluster analysis,\neach feature is centered. In our S-AMA algorithm, we set $m=5$ and\n$\\phi=1$ for weight $w_{i_{1},i_{2}}$.\n\nNote that some of the original 15 clusters indicate similar hand movements,\nsuch as curved/vertical swing and horizontal/vertical straight-line.\nBy plotting the first two principal components of the 90 features,\none can see that some clusters are severely overlapped. Therefore,\nfor evaluation purpose, six clusters, including vertical swing (labeled as 3),\nanti-clockwise arc (labeled as 4), clockwise arc (labeled as 5), horizontal straight-line (labeled as 7),\nhorizontal wavy (labeled as 11), and\nvertical wavy (labeled as 12) in the original dataset are selected. Figure \\ref{truelabel} displays the\nplot of the first principal component (PC1) against the second principal\ncomponent (PC2) of 90 features for the selected six clusters with\nthe true cluster labels. \n\n\\begin{figure}[!htb]\n\\protect\\caption{The plot of the first principal component against the second principal\ncomponent of 90 features for the selected six clusters with true cluster\nlabels.}\n\n\n\\centering \\includegraphics[scale=0.8]{true_label} \\label{truelabel}\n\\end{figure}\n\n\nWe first display the clustering path of convex clustering (AMA) using\nall 90 features in Figure \\ref{cvxpath}. Clearly, convex clustering\nis only able to distinguish clusters 4 and 5 and treat the rest clusters\nas one class. This phenomenon shows the curse of dimensionality in\nhigh-dimensional clustering and motivates the need to conduct feature\nselection for improved clustering performance.\n\n\\begin{figure}[!htb]\n\\protect\\caption{The clustering path of convex clustering (AMA) using all 90 features\nby plotting the first principal component (PC1) against the second\nprincipal component (PC2).}\n\n\n\\centering \\includegraphics[scale=0.5]{ama_path_1_2}\\label{cvxpath}\n\\end{figure}\n\n\nWe use S-AMA to solve sparse convex clustering. The tuning parameters\nare selected according to the stability selection in Section \\ref{sec:tuning}.\nTable \\ref{tb_realdata_1} reports the number of estimated clusters,\nthe number of selected features, and the RAND index between the estimated\ncluster membership and the true cluster membership for k-means clustering,\nAMA algorithm, and our S-AMA algorithm. Clearly, both convex clustering\n(AMA) and sparse convex clustering (S-AMA) perform better than k-means,\nwhich indicates that the performance of convex clustering or sparse\nconvex clustering is less sensitive to the assumption of spherical\nclustering centers. In addition, by only using $13$ informative features,\nour S-AMA is able to improve the clustering accuracy of convex clustering\n(AMA) by $45\\%$. This indicates the importance of variable selection\nin high-dimensional clustering.\n\n\\begin{table}[!htb]\n\\centering \\protect\\caption{The number of estimated clusters, the number of selected features,\nand the RAND index for k-means, AMA, and our S-AMA algorithm.}\n\n\n\\label{tb_realdata_1} \\vspace{6pt}\n \\centering \n\\begin{tabular}{c|ccc}\n\\hline\nAlgorithm  & \\# of clusters  & \\# of features  & RAND index \\\\\n\\hline\nk-means  & 2  & 90  & 0.06 \\\\\nAMA  & 3  & 90  & 0.31 \\\\\nS-AMA  & 3  & 13  & \\textbf{0.45}\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\nNext we demonstrate the clustering path of sparse convex clustering\n(S-AMA) with only 13 selected features in Figure \\ref{scvxpath}.\nFigure \\ref{scvxpath} displays three big clusters, which is consistent\nwith the number of estimated clusters shown in Table \\ref{tb_realdata_1}.\nAs tuning parameter $\\gamma_{1}$ increases,\nthe clustering path of S-AMA tends to merge clusters 3, 7 and 12 into\none big cluster, merge cluster 4 and 5 into another big cluster, and\nidentify cluster 11 as the third cluster. This finding is displayed in the final clustering\npath of S-AMA executed at the selected $\\gamma_{1}$ and $\\gamma_{2}$\nas shown in Figure \\ref{truebestlabel}. In the plot, the left-panel\ngraph shows the true cluster labels and the right-panel graph shows\nthe three estimated clusters using S-AMA.\n\n\\begin{figure}[!htb]\n\\protect\\caption{The clustering path of sparse convex clustering (S-AMA) using only\n13 selected features by plotting PC1 against PC2. These 13 features\nare selected via stability selection.}\n\n\n\\centering \\includegraphics[scale=0.5]{scvx_stability_best_path_1_2}\\label{scvxpath}\n\\end{figure}\n\n\n\\begin{figure}[!htb]\n\\protect\\caption{The left-panel graph shows the true cluster labels by plotting PC1\nagainst PC2 using only 13 selected features. The right-panel graph\nshows the estimated cluster membership using S-AMA at the selected\ntuning parameters.}\n\n\n\\centering \\includegraphics[width=16cm,height=8cm]{scvx_stability_best_cluster1}\\label{truebestlabel}\n\\end{figure}\n\n\n\\newpage{}\n\n\n\\section*{Appendix}\n\n\\setcounter{equation}{0} \\setcounter{lemma}{0} \\setcounter{proposition}{0}\n\\global\\long\\def\\theequation{A.\\arabic{equation}}\n \\global\\long\\def\\thesubsection{A.\\arabic{subsection}}\n \\global\\long\\def\\thelemma{A.\\arabic{lemma}}\n \\global\\long\\def\\theproposition{A.\\arabic{proposition}}\n\n\n\n\\subsection*{A.1 Proof of Lemma \\ref{thm:equivalent}}\n\nDenote $\\bfa=\\textrm{vec}(\\bfA)$, a vectorization of the matrix\n$\\bfA$. According to the fact that $A_{i_{1}\\cdot}-A_{i_{2}\\cdot}=\\bfA\\trans(\\bfe_{i_{1}}-\\bfe_{i_{2}})$\nand the property of the tensor product $\\textrm{vec}(\\bfR\\bfS\\bfT)=[\\bfT^{T}\\otimes\\bfR]\\textrm{vec}(\\bfS)$,\nsolving the minimization of $f(\\bfA)$ is equivalent to minimize\n\\begin{eqnarray*}\nf(\\bfa)=\\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\|\\bfB_{l}\\bfP\\bfa-\\wt{\\bfv}_{l}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2},\n\\end{eqnarray*}\nwhere $\\bfB_{l}=(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\otimes\\bfI_{p}$\nand $\\bfP$ is a permutation matrix such that $\\textrm{vec}(\\bfA\\trans)=\\bfP\\textrm{vec}(\\bfA)$ ($\\bfP\\trans=\\bfP^{-1}$). Letting\n$\\bfB\\trans=\\left(\\bfB\\trans_{1},\\ldots,\\bfB\\trans_{|\\calE|}\\right),\\wt{\\bfv}\\trans=\\left(\\wt{\\bfv}_{1}\\trans,\\ldots,\\wt{\\bfv}_{|\\calE|}\\trans\\right)$,\nit becomes\n\\begin{eqnarray*}\nf(\\bfa)=\\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\frac{\\nu}{2}\\|\\bfB\\bfP\\bfa-\\wt{\\bfv}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\nTo further simplify the formulae, the following proposition is needed,\nwith proof shown later.\n\\begin{proposition} \\label{prop1} For permutation\nmatrix $\\bfP$ and any $n$-dim vector $\\bfd$, $\\left[\\bfd\\trans\\otimes\\bfI_{p}\\right]\\bfP=\\bfI_{p}\\otimes\\bfd\\trans.$\n\\end{proposition}\nBy Proposition \\ref{prop1}, $\\bfB_{l}\\bfP=\\bfI_{p}\\otimes(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\triangleq\\bfC_{l}$.\nLet $\\bfC\\trans=\\left(\\bfC\\trans_{1},\\ldots,\\bfC\\trans_{|\\calE|}\\right)$,\nthen the second term in $f(\\bfa)$ becomes $\\frac{\\nu}{2}\\|\\bfC\\bfa-\\wt{\\bfv}\\|_{2}^{2}=\\frac{\\nu}{2}\\sum_{j=1}^{p} \\sum_{l\\in\\calE}\\left((\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\bfa_{j}-\\wt{v}_{jl}\\right)_{2}^{2}$.\n\nTherefore, the objective function can be separated to $p$ sub-optimization\nquestions:\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfx_{j}-\\bfa_{j}\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\left((\\bfe_{i_{1}}-\\bfe_{i_2})\\trans\\bfa_{j}-\\wt{v}_{jl}\\right)_{2}^{2} +\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2},\\quad j=1,\\ldots,p.\n\\end{eqnarray*}\nBy some algebra, if $\\calE$ contains all possible edges, it can be\nrewritten as\n\\begin{eqnarray}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\bfa_{j}\\trans\\bfM\\bfa_{j}-\\bfz_{i}\\trans\\bfa_{j} +\\frac{1}{2}\\nu\\wt{\\bfv}_{j\\cdot}\\trans\\wt{\\bfv}_{j\\cdot}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2},\\label{subobj}\n\\end{eqnarray}\nwhere $\\wt{\\bfv}_{j\\cdot}=(\\wt{v}_{j1},\\ldots,\\wt{v}_{j|\\calE|})\\trans$,\n$\\bfM=\\bfI_{n}+\\nu\\sum_{l\\in\\calE}(\\bfe_{i_{1}}-\\bfe_{i_{2}})(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans=(1+n\\nu)\\bfI_{n}-\\nu\\bfone_{n}\\bfone_{n}\\trans$,\nand $\\bfz_{j}=\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$.\nThe KKT conditions of \\eqref{subobj} are\n\\begin{eqnarray*}\n\\forall\\bfa_{j}\\neq\\bfzero,\\quad\\bfM\\bfa_{j}-\\bfz_{i}+\\frac{\\gamma_{2}u_{j}\\bfa_{j}}{\\|\\bfa_{j}\\|_{2}}=\\bfzero;\\quad\\quad\\ \\forall\\bfa_{j}=\\bfzero,\\quad\\|\\bfz_{j}\\|_{2}\\leq\\gamma_{2}u_{j}.\n\\end{eqnarray*}\n\n\nHere are some remarks on the above KKT conditions. $\\bfM$ is positive\ndefinite, thus it can be diagonalized by $\\bfM=\\bfS\\trans\\bPhi\\bfS$,\nwhere $\\bPhi=\\textrm{diag}(\\phi_{1},\\ldots,\\phi_{n})$ and $\\bfS$\nis an orthogonal matrix. It can be verified that $\\phi_{1}=1$ and\n$\\phi_{i}=1+n\\nu,i=2,\\ldots,n$. Then $\\bPhi\\bfS\\bfa_{j}-\\bfS\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\bfS\\bfa_{j}}{\\|\\bfS\\bfa_{j}\\|_{2}}=\\bfzero$.\nLet $\\wt\\bfa_{j}=\\bfS\\bfa_{j}$. One needs to solve $\\bPhi\\wt\\bfa_{j}-\\bfS\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\wt\\bfa_{j}}{\\|\\wt\\bfa_{j}\\|_{2}}=\\bfzero$.\nIf $\\nu=0,\\bPsi=\\bfI$, the solution $\\wt\\bfa_{j}$ shares the same\ndirection. Then an explicit soft-threshold formula can be obtained,\nand this situation under the standard group LASSO problem was discussed\nin \\citet{Yuan2006}. But if $\\nu\\neq0$, a scaling transformation\nis applied to $\\wt\\bfa_{j}$, and there is no explicit solution.\n\nAlternatively, rewrite \\eqref{subobj} so that existing algorithms\ncan be applied. Define\n\\begin{eqnarray*}\n\\bfN=\\sqrt{1+n\\nu}\\bfI_{n}-\\frac{\\sqrt{1+n\\nu}-1}{n}\\bfone_{n}\\bfone_{n}\\trans,\n\\end{eqnarray*}\nwhich performs like a ``design matrix\". It can be verified\nthat $\\bfM=\\bfN\\bfN$ and $\\bfN^{-1}$ has the form defined in Lemma \\ref{thm:equivalent}. Let $\\bfy_{j}=(\\bfN)^{-1}\\bfz_{j}$, which\nperforms like a pseudo outcome in the $j$-th sub-problem. Then \\eqref{subobj}\nis equivalent to\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\nNote that during the whole algorithm, $\\bfN$ and its inverse are\ncalculated only once. This ends the proof of Lemma \\ref{thm:equivalent}.\n\\hfill{}$\\blacksquare$\n\n\\noindent \\textbf{{Proof of Proposition \\ref{prop1}:}} Note that\n$\\bfP=(P_{kl}),1\\leq k,l\\leq np$ here is a unique permutation matrix\nsuch that $P_{kl}=1$ if $k=(i-1)p+j$ and $l=(j-1)n+i,1\\leq i\\leq n,1\\leq j\\leq p$,\nand 0 otherwise. By the definition of $\\bfP$, it is clear that multiplying\na matrix by $\\bfP$ on the right moves its $k$-th column to the $l$-th\ncolumn when $P_{kl}=1$.\n\nConsider the $i$-th element $d_{i}$ of $\\bfd$, then in $\\bfd\\trans\\otimes\\bfI_{p}$,\nits entries at $(j,(i-1)p+j)$ equal $d_{i}$, $j=1,\\ldots,p$. Thus,\nin $(\\bfd\\trans\\otimes\\bfI_{n})\\bfP$, the entry at $(j,(j-1)n+i)$\nequals to $d_{i}$. In $\\bfI_{p}\\otimes\\bfd\\trans$, it is easy to\nsee the entry at $(j,(j-1)n+i)$ equal $d_{i},i=1,\\ldots,n,j=1,\\ldots,p$.\n\n\n\\subsection*{A.2 Update Steps of S-AMA}\n\nBy letting $\\nu=0$ while updating $\\bfA$, the S-ADMM algorithm can\nbe simplified significantly. Noting that $\\bfM=\\bfI_{n}$ and $\\bfz_{j}=\\bfx_{j}+\\sum_{l\\in\\calE}\\lambda_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$,\nwhere $\\lambda_{jl}$ is the $j$-th element of $\\blambda_{l}$, the\nKKT conditions are\n", "itemtype": "equation", "pos": 22218, "prevtext": "\nBy Karush-Kuhn-Tucker (KKT) conditions of the group lasso problem \\citep{Yuan2006}, the solution\nto (\\ref{eq:g1}) has a closed form as\n\\begin{eqnarray*}\n\\wh\\bfa_{j}=\\left(1-\\frac{\\gamma_{2}u_{j}}{\\|\\bfz_{j}\\|_{2}}\\right)_{+}\\bfz_{j},\n\\end{eqnarray*}\nwhere $\\bfz_{j}=\\bfx_{j}+\\sum_{l\\in\\calE}\\lambda_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$\nand $(z)_{+}=\\max\\{0,z\\}$. See the detailed derivations in Appendix.\nThe above formula significantly reduces the computational cost by solving\n$p$ group-lasso problem analytically in each iteration. Note that\nthe above update of $\\bfA$ is independent of $\\bfV$, which indicates\nthat S-AMA algorithm does not need to compute the update of $\\bfV$.\nTherefore S-AMA is much more efficient than S-ADMM algorithm.\n\nNext, we discuss the update of $\\bLambda$. Define $\\calP_{B}(\\bfz)$\nas a projection onto $B=\\{\\bfy:\\|\\bfy\\|_{\\dag}\\leq1\\}$ of the norm\n$\\|\\cdot\\|_{\\dag}$, where $\\|\\cdot\\|_{\\dag}$ is the dual norm of\n$\\|\\cdot\\|_{q}$, which defines the fusion penalty. We show in\nAppendix that the update of $\\bLambda$ reduces to $\\blambda_{l}^{m}= \\calP_{C_{l}}[\\blambda_{l}^{m-1}-\\nu(A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m})]$\nwith $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\nThe S-AMA algorithm is summarized in Algorithm \\ref{algAMA}.\n\n\\begin{algorithm}[H]\n\\protect\\caption{\\quad{}S-AMA \\label{algAMA}}\n\n\\begin{enumerate}\n\\item Initialize $\\mathbf{\\Lambda}{}^{0}$. For $m=1,2,\\ldots$\n\\item For $j=1,\\ldots,p$, do\n\\begin{eqnarray*}\n\\bfz_{j}^{m} & = & \\bfx_{j}+\\sum_{l \\in\\calE}\\lambda_{lj}^{m-1}(\\bfe_{i_{1}}-\\bfe_{i_{2}}),\\\\\n\\bfa_{j}^{m} & = & \\left(1-\\frac{\\gamma_{2}u_{i}}{\\|\\bfz_{i}^{m}\\|_{2}}\\right)_{+}\\bfz_{j}^{m}.\n\\end{eqnarray*}\n\n\\item For $l\\in\\calE$, do\n", "index": 11, "text": "\n\\[\n\\blambda_{l}^{m}=\\calP_{C_{l}}[\\blambda_{l}^{m-1}-\\nu(A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m})],\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\blambda_{l}^{m}=\\calP_{C_{l}}[\\blambda_{l}^{m-1}-\\nu(A_{i_{1}\\cdot}^{m}-A_{i_%&#10;{2}\\cdot}^{m})],\" display=\"block\"><mrow><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\blambda</mtext></merror><mi>l</mi><mi>m</mi></msubsup><mo>=</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calP</mtext></merror><msub><mi>C</mi><mi>l</mi></msub></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\blambda</mtext></merror><mi>l</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>-</mo><mrow><mi>\u03bd</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>A</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow><mi>m</mi></msubsup><mo>-</mo><msubsup><mi>A</mi><mrow><msub><mi>i</mi><mn>2</mn></msub><mo>\u2063</mo><mo>\u22c5</mo></mrow><mi>m</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nThe solutions are $\\wh\\bfa_{j}=\\left(1-\\frac{\\gamma_{2}u_{j}}{\\|\\bfz_{j}\\|_{2}}\\right)_{+}\\bfz_{j}$,\nwhere $(z)_{+}=\\max\\{0,z\\}$. See \\citet{Yuan2006}.\n\nBy applying the projection method, one can update $\\bfv_{l}$ and\n$\\blambda_{l}$ as $\\bfv_{l}^{m+1}=A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{l}^{m}-\\calP_{tB}[A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{l}^{m}]$,\nwhere $t=\\sigma_{l}=\\gamma_{1}w_{l}/\\nu$ and $\\calP_{B}(\\bfz)$ denotes\nprojection onto $B=\\{\\bfy:\\|\\bfy\\|_{\\dag}\\leq1\\}$. The point $\\calP_{B}(\\bfz)$\ncan be characterized by the relations $\\calP_{B}(\\bfz)\\in B$ and\n$\\forall\\bfy\\in B,\\langle\\bfy-\\calP_{B}(\\bfz),\\bfz-\\calP_{B}(\\bfz)\\rangle\\leq0$.\nThen\n\\begin{eqnarray*}\n\\blambda_{l}^{m+1} & = & \\blambda_{l}^{m}+\\nu(\\bfv_{l}^{m+1}-A_{i_{1}\\cdot}^{m+1}+A_{i_{2}\\cdot}^{m+1})\\\\\n & = & -\\nu\\calP_{tB}(A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{1l}^{m}) \\\\\n & = & \\calP_{C_{l}}(\\blambda_{l}^{m}-\\nu\\bfg_{l}^{m+1}),\n\\end{eqnarray*}\nwhere $\\bfg_{l}^{m}=A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m}$ and $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\nNote that there is no need to update $\\bfv_{l}$, and $\\blambda_{l}$\ncan be directly updated.\n\n\n\\subsection*{A.3 Proofs of Lemmas \\ref{thm:df1}-\\ref{thm:df2}: Degrees of freedom}\n\nFollowing the arguments in \\citet{Tan2015}, the number of degrees of freedom\n(df) of \\eqref{eq:obj_constraint}, when $q=1$ or $2$, can be derived\nunder the assumption $w_{l}=1,l\\in\\calE$ and $X_{i\\cdot}\\sim\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$.\n\n\\underline{\\textbf{Case $q=1$:}} Rewrite \\eqref{eq:obj_constraint}\ninto the following formulation:\n\\begin{eqnarray}\n\\min_{\\bfa\\in\\mathbb{R}^{np\\times1}} &  & \\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\gamma_{1}\\sum_{l\\in\\calE}w_{l}\\|\\bfC_{l}\\bfa\\|_{1}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|(\\bfe_{j}^{*T}\\otimes\\bfI_{n})\\bfa\\|_{2},\\label{obj_df_1}\n\\end{eqnarray}\nwhere $\\bfe_{j}^{*}$ is a $p$-dim vector with its $j$-th element\nas 1 and 0 otherwise.\n\nDefine\n\n", "itemtype": "equation", "pos": 50971, "prevtext": "\nwhere $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\n\\item Repeat Steps 2-3 until convergence. \\end{enumerate}\n\\end{algorithm}\n\n\n\n\\subsubsection{Algorithmic Convergence}\n\nThis subsection discusses the convergence of the proposed S-ADMM and\nS-AMA algorithms. \\citet{Chi2015} and the references therein provided sufficient conditions for the convergence\nof the following general optimization problem,\n\\begin{eqnarray}\n\\min_{\\xi,\\zeta}\\ f(\\xi)+g(\\zeta),\\textrm{\\ \\ s.t.\\ \\ }A\\xi+B\\zeta=c.\\label{general_obj}\n\\end{eqnarray}\nThey verified that the ADMM and AMA algorithms for\nconvex clustering, as two special cases of \\eqref{general_obj}, satisfied\nthe sufficient conditions under which the convergence was guaranteed.\n\nThe convergence of our S-ADMM and S-AMA algorithms follows similar\narguments. Note that the only difference between the objective function\nin \\eqref{eq:obj_constraint} and its counterpart in \\citet{Chi2015}\nis a convex penalty term $\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\mathbf{a}_{j}\\|_{2}$.\nDefine the summation of the first and third terms of the objective\nfunction in \\eqref{eq:obj_constraint} as $f(\\cdot)$, and the second\nterm as $g(\\cdot)$. This indicates that the optimization problem\n\\eqref{eq:obj_constraint} is a special case of \\eqref{general_obj}.\nSimple algebra implies that $f(\\cdot)$ is strongly convex. According\nto \\citet{Chi2015}, one can show that, under mild regularization conditions, the convergence\nof S-ADMM is guaranteed for any $\\nu>0$, and the convergence\nof S-AMA algorithm is guaranteed provided that positive\nconstant $\\nu$ is not too large.\n\n\\subsubsection{Computational Consideration}\n\nStep 2 in both Algorithms \\ref{algADMM} and \\ref{algAMA} involves $p$ sub-optimization problems. Therefore, S-ADMM and S-AMA merit from the distributed optimization, and they can handle large-scale problems efficiently. To be specific, Step 2 can be distributed to different processors to obtain estimates of $\\bfa_j$'s which are then gathered to update $\\bfA$. In addition, Steps 3-4 in Algorithm \\ref{algADMM} or Step 3 in Algorithm \\ref{algAMA} can also be distributed to different processors to obtain fast updates.\n\nIt is worth pointing out that the computation of S-AMA is comparable to AMA in \\cite{Chi2015}, while S-ADMM is computationally more expensive than ADMM in \\cite{Chi2015} and S-AMA. This is because Step 2 in S-ADMM does not have a closed-form formula and it requires solving $p$ group-lasso problems assisted by iterations. Our limited experience in numerical studies also confirms the superiority of S-AMA over S-ADMM in terms of the computational cost.\n\n\n\\subsection{Statistical Property}\n\n\\label{sec:thm}\n\nIn this section, we provide unbiased estimators for the degrees of freedoms\nof sparse convex clustering. Degrees of freedom is generally defined in regression problems to\nexplain the amount of flexibility in the model. It is a key component\nfor model selection and statistical hypothesis testing. Note that our sparse convex clustering can be formulated as a penalized\nregression problem for which the degrees of freedom can be established.\nMotivated by \\citet{Tan2015}, we develop unbiased estimators for\nthe degrees of freedom of sparse convex clustering with $q=1$ in\nLemma \\ref{thm:df1} and $q=2$ in Lemma \\ref{thm:df2}. Denote a $p$-dimensional multivariate normal distribution as $\\textrm{MVN}_{p}$.\nFor simplicity, we consider the case with $w_l=1, l \\in \\calE$ in the following theoretical developments.\n\n\\begin{lemma} \\label{thm:df1} Assume $X_{i\\cdot}\\stackrel{iid}{\\sim}\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$, and let $\\widehat{\\bfa} \\buildrel \\Delta \\over = \\textrm{vec}(\\wh{\\bfA})$\nbe the solution to $(\\ref{eq:obj_constraint})$ with $q=1$. Then we\nhave ${\\rm df}\\buildrel \\Delta \\over ={\\rm tr}(\\frac{\\partial\\wh\\bfa}{\\partial\\bfx})$\nis of the form\n\\begin{eqnarray*}\n{\\rm df}_{1} & = & {\\rm tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{2}\\bfP_{1}\\sum_{s\\in\\calB_{12}}\\bigg(\\frac{\\bfD_{s}\\trans\\bfD_{s}} {\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh\\bfa\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\bigg)\\Bigg]^{-1}\\bfP_{1}\\Bigg),\n\\end{eqnarray*}\nwhere $\\bfD_{s}$ and $\\bfP_{1}$ are defined in $(\\ref{eqn:defD})$ and $(\\ref{eqn:defP})$, respectively.\n\\end{lemma}\n\nFollowing a similar proof technique, we provide an unbiased estimator for the degrees of freedom of the sparse convex clustering with $q=2$.\n\n\\begin{lemma} \\label{thm:df2} Assume $X_{i\\cdot}\\stackrel{iid}{\\sim}\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$, and let $\\widehat{\\bfa}$\nbe the solution to $(\\ref{eq:obj_constraint})$ with $q=2$. Therefore,\nthe degrees of freedom is\n\\begin{eqnarray*}\n{\\rm df}_{2} & = & {\\rm tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{1}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{1,\\ldots,|\\calE|\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\\\\n &  & +\\gamma_{2}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{|\\calE|+1,\\ldots,|\\calE|+p\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\Bigg]^{-1}\\bfP_{2}\\Bigg).\n\\end{eqnarray*}\n\\end{lemma}\n\nProofs of Lemmas \\ref{thm:df1}-\\ref{thm:df2} are discussed in Appendix.\n\n\n\\section{Practical Issues}\n\n\\label{sec:practical}\n\nIn Section \\ref{sec:algorithm}, the S-ADMM and S-AMA algorithms rely\non the choice of weights and the tuning parameters $\\gamma_{1}$ and\n$\\gamma_{2}$. In this section, we discuss how to choose these parameters\nin practice.\n\n\n\\subsection{Selection of Weights}\n\\label{sec:weights}\n\nThis subsection introduces practical selections of the weights $w_{i_{1},i_{2}}$,\n$(i_{1},i_{2})\\in\\calE$, in the fused-lasso penalty and the factors\n$u_{j}$, $j=1,\\cdots,p$, in the adaptive group-lasso penalty.\n\nFollowing \\citet{Chi2015}, we choose weights by incorporating the\nm-nearest-neighbors method with Gaussian kernel. To be specific, the\nweight between the sample pair $(i_{1},i_{2})$ is set as $w_{i_{1},i_{2}}=\\iota_{i_{1},i_{2}}^{m}\\exp(-\\phi\\|X_{i_{1}\\cdot}-X_{i_{2}\\cdot}\\|_{2}^{2})$,\nwhere $\\iota_{i_{1},i_{2}}^{m}$ equals 1 if observation $i_{2}$\nis among observation $i_{1}$'s $m$ nearest neighbors or vice versa, and\n0 otherwise. This choice of weights works well for a wide range of\n$\\phi$ when $m$ is small. In our numerical results, $m$ is fixed\nat $5$ and $\\phi$ is fixed at 0.5.\n\nNext we consider the selection of the factor $u_{j}$. As suggested\nby \\citet{zou:2006}, $u_{j}$ can be chosen as $1/\\|\\wh{\\bfa}_{j}^{(0)}\\|_{2}$,\nwhere $\\wh\\bfa_{j}^{(0)}$ is the estimate of $\\bfa_{j}$ in \\eqref{eq:obj_constraint}\nwith $\\gamma_{2}=0$. Such choice of factors penalizes less on informative\nfeatures and penalizes more on uninformative features, and hence leads\nto improved clustering accuracy and variable selection performance\nthan its non-adaptive counterpart.\n\nFinally, in order to ensure that the optimal tuning parameters $\\gamma_{1}$\nand $\\gamma_{2}$ lie in relatively robust intervals regardless of\nfeature dimension and sample size, weights $w_{i_{1},i_{2}}$ and\nfactors $u_{j}$ are re-scaled to sum to $1/\\sqrt{p}$ and $1/\\sqrt{n}$,\nrespectively. Such re-scaling is only for convenience and does not\naffect the final clustering path.\n\n\n\\subsection{Selection of Tuning Parameters}\n\n\\label{sec:tuning}\n\nThis subsection provides a selection method for tuning parameters\n$\\gamma_{1}$ and $\\gamma_{2}$. Remind that $\\gamma_{1}$ controls\nthe number of estimated clusters and $\\gamma_{2}$ controls the number\nof selected informative features.\n\nWe first illustrate via a toy example the effectiveness of tuning\nparameter $\\gamma_{2}$ on variable selection accuracy. In this\nexample, $60$ observations with $p=500$ features are generated from\n4 clusters. Among all the features, only $20$ variables differ between\nclusters. See detailed simulation setup in Section \\ref{sec:simulation}.\nBy fixing $\\wh\\gamma_{1}=2.44$ and varying $\\gamma_{2}$ from $e^{-5.0}$\nto $e^{7.0}$, we plot the path of false negative rate (FNR) and the\npath of false positive rate (FPR) of the final estimator. As shown\nin Figure \\ref{demo-graph}, when $\\gamma_{2}$ is close to zero,\nall features are included, and when $\\gamma_{2}$ increases to some\nranges of intervals, all and only uninformative features are excluded,\ni.e., perfect variable selection performance. This illustrates the\nsensitivity of $\\gamma_{2}$ to the variable selection performance\nof the final estimator. In practice, we aim to estimate a suitable\n$\\gamma_{2}$ that leads to satisfactory variable selection.\n\n\\begin{figure}[!htb]\n\\protect\\caption{Illustration of the effectiveness of $\\gamma_{2}$ on variable selection\naccuracy. The solid curve is the path of false negative rate (FNR),\nand the dashed curve is the path of false positive rate (FPR).}\n\n\n\\centering \\includegraphics[scale=0.5]{simu_4grp_p500_fix_gamma1}\\label{demo-graph}\n\\end{figure}\n\nIn literature, \\citet{Wang:2010} and \\citet{Fang2012} proposed stability\nselection to estimate the tuning parameters in clustering models.\nThe idea behind stability selection is that a good tuning parameter\nshould produce clustering results that are stable with respect to\na small perturbation to the training samples. Stability selection well suits\nthe model selection in cluster analysis because cluster labels are unavailable and the cross-validation method is not applicable in this case.\n\nIn this paper, we propose\nto use stability selection in \\citet{Fang2012} to tune both parameters\n$\\gamma_{1}$ and $\\gamma_{2}$. To be specific, for any given $\\gamma_{1}$\nand $\\gamma_{2}$, based on two sets of bootstrapped samples, two\nclustering results can be produced via \\eqref{eq:obj_constraint}, and\nthen the stability measurement \\citep{Fang2012} can be computed to\nmeasure the agreement between the two clustering results. In order\nto enhance the robustness of the stability selection method, we repeat\nthis procedure $50$ times and then compute the averaged stability\nvalue. Finally, the optimal parameter is selected as the one achieving\nmaximum stability. Our extensive numerical studies suggest that the\nperformance of sparse convex clustering is less sensitive to $\\gamma_{1}$.\nThus, to speed up tuning process, stability path can be computed over\nof a coarse grid of $\\gamma_{1}$ and a fine grid of $\\gamma_{2}$.\n\n\n\n\n\n\\section{Numerical Results}\n\n\\label{sec:experiment}\n\nThis section demonstrates the superior performance of our sparse convex\nclustering in simulated examples in Section \\ref{sec:simulation}\nand a real application of hand movement clustering in Section \\ref{sec:real}.\n\n\n\\subsection{Simulation Studies}\n\n\\label{sec:simulation}\n\nIn this subsection, simulations studies are conducted to evaluate\nthe performance of sparse convex clustering (S-ADMM and S-AMA). They\nare compared to k-means clustering and two convex clustering algorithms: ADMM and AMA \\citep{Chi2015}.\n\nFour simulation settings are considered. Each simulated dataset consists\nof $n=60$ observations with the number of clusters either $K=2$\nor 4, and the number of features either $p=150$ or $500$. In each\nsetting, only the first 20 features are informative and remaining\nfeatures are non-informative. The samples $X_{i\\cdot}\\in\\real^{p},i=1,\\ldots,n$,\nare generated as follows. For each $i$, a cluster label $Z_{i}$\nis uniformly sampled from $\\{1,\\ldots,K\\}$, and then the first 20\ninformative features are generated from $\\textrm{MVN}_p(\\bmu_{K}(Z_{i}),\\bfI_{20})$,\nwhere $\\bmu_{K}(Z_{i})$ is defined as follows.\n\\begin{itemize}\n\\item If $K=2$, $\\bmu_{2}(Z_{i})=\\mu\\bfone_{20}I(Z_{i}=1)-\\mu\\bfone_{20}I(Z_{i}=2)$;\n\\item If $K=4$, $\\bmu_{4}(Z_{i})=(\\mu\\bfone_{10}\\trans,-\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=1)+(-\\mu\\bfone_{10}\\trans,-\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=2)+\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (-\\mu\\bfone_{10}\\trans,\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=3)+(\\mu\\bfone_{10}\\trans,\\mu\\bfone_{10}\\trans)\\trans I(Z_{i}=4)$,\n\\end{itemize}\nwhere $\\mu$ controls the\ndistance between cluster centers. Here a large $\\mu$ indicates that\nclusters are well-separated, whereas a small $\\mu$ indicates that\nclusters are overlapped. Finally, the rest $p-20$ noise features\nare generated from $\\calN(0,1)$. \n\nIn summary, four simulation settings are considered. Setting 1: $K=2,p=150$,\nand $\\mu=0.6$; Setting 2: $K=2,p=500$, and $\\mu=0.7$; Setting 3:\n$K=4,p=150$, and $\\mu=0.9$; Setting 4: $K=4,p=500$, and $\\mu=1.2$.\nFor each setting, we run 200 repetitions.\n\nThe RAND index \\citep{Rand1971} is used to measure the agreement\nbetween the estimated clustering result and the underlying true clustering\nassignment. The Rand index ranges between 0 and 1, and a higher value\nindicates better performance. Note that the true cluster labels are known in\nsimulation studies, and thus it is feasible to know how well the candidate methods can\nperform if they are tuned by maximizing the RAND index.\nTo ensure fair comparisons, for each\nrepetition, separate validation samples are generated and used to\nselect an optimal k in k-means, an optimal $\\gamma$ in ADMM and AMA, and optimal $\\gamma_{1}$\nand $\\gamma_{2}$ in S-ADMM and S-AMA. To evaluate the performance\nof variable selection, two measurements are reported: the false negative\nrate (FNR) and the false positive rate (FPR). All the simulation results\nare summarized in Table \\ref{tb_simulation_1}. Due to its relatively\nexpensive computational costs, S-ADMM is not evaluated for Settings\n2 and 4 where $p=500$.\n\nIn all these four simulation settings, the centers are spherical and\nhence k-means always performs well in clustering accuracy, i.e., large\nRAND index. The goals of these simulations are to justify that (1)\nconvex clustering does not perform well when the feature dimension\nis high; (2) sparse convex clustering performs very well when the\nfeature dimension is high; and (3) sparse convex clustering selects\ninformative features with great clustering accuracy. All these claims\nare justified by the results presented in Table \\ref{tb_simulation_1}.\n\n\\begin{table}[!htb]\n\\centering \\protect\\caption{Empirical mean and standard deviation (SD) of the RAND index, false\npositive rate (FPR), and false negative rate (FNR) based on 200 repetitions.\nSetting 1: $K=2,p=150$, and $\\mu=0.6$; Setting 2: $K=2,p=500$,\nand $\\mu=0.7$; Setting 3: $K=4,p=150$, and $\\mu=0.9$; Setting 4:\n$K=4,p=500$, and $\\mu=1.2$. The best RAND in each scenario is shown\nin bold. }\n\n\n\\label{tb_simulation_1} \\vspace{6pt}\n \\centering \n\\begin{tabular}{p{2cm}p{2cm}p{1.2cm}p{1.2cm}p{0.1cm}p{1.2cm}p{1.2cm}p{0.1cm}p{1.2cm}p{1.2cm}}\n\\hline\n\\  & \\  & \\multicolumn{2}{c}{RAND} & \\  & \\multicolumn{2}{c}{FNR} & \\  & \\multicolumn{2}{c}{FPR}\\tabularnewline\n\\hline\n & Algorithm  & mean  & SD  & \\  & mean  & SD  & \\  & mean  & SD \\tabularnewline\n\\hline\n\\hline\nSetting 1  & k-means  & 0.95  & 0.06  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.53  & 0.39  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.66  & 0.40  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-ADMM  & 0.82  & 0.24  & \\  & 0.04  & 0.05  & \\  & 0.25  & 0.16 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.96}  & 0.06  & \\  & 0.03  & 0.07  & \\  & 0.30  & 0.21 \\tabularnewline\n\\hline\nSetting 2  & k-means  & 0.95  & 0.11  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.14  & 0.20  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.08  & 0.21  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.97}  & 0.07  & \\  & 0.07  & 0.09  & \\  & 0.11  & 0.10 \\tabularnewline\n\\hline\nSetting 3  & k-means  & 0.83  & 0.15  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.56  & 0.22  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.47  & 0.21  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-ADMM  & 0.82  & 0.14  & \\  & 0.04  & 0.06  & \\  & 0.25  & 0.24 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.84}  & 0.13  & \\  & 0.02  & 0.04  & \\  & 0.11  & 0.18 \\tabularnewline\n\\hline\nSetting 4  & k-means  & 0.89  & 0.14  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & ADMM  & 0.31  & 0.23  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & AMA  & 0.31  & 0.20  & \\  & 0.00  & 0.00  & \\  & 1.00  & 0.00 \\tabularnewline\n\\  & S-AMA  & \\textbf{0.94}  & 0.09  & \\  & 0.01  & 0.02  & \\  & 0.01  & 0.03 \\tabularnewline\n\\hline\n\\end{tabular}\n\\end{table}\n\n\nFirst, convex clustering does not perform well when the feature dimension\nis high, even much worse than k-means. Similar phenomenon was also\nobserved in the simulation studies conducted in \\citet{Tan2015}.\nThis is the motivation for developing sparse convex clustering. Second,\nsparse convex clustering improves convex clustering significantly.\nSparse convex clustering (S-AMA) performs as well as k-means when\n$p=150$, and performs better than k-means when $p=500$. Third, sparse\nconvex clustering selects informative feature with great accuracy,\nthat is, with low FNR and FPR. The feature selection performance of\nsparse convex clustering is very promising for settings where $p=500$.\nCompared with S-ADMM, S-AMA is computationally faster and also delivers slightly better accuracy. Therefore, we recommend\nS-AMA in practice.\n\n\n\\subsection{Real Data Application}\n\n\\label{sec:real}\n\nWe evaluate the performance of sparse convex clustering in LIBRAS\nmovement data from the Machine Learning Repository \\citep{Lichman2013}.\nThe original dataset contains 15 classes with each class referring\nto a hand movement type. Each class contains 24 observations, and\neach observation has 90 features consisting of the coordinates of\nhand movements. We use this dataset without the clustering assignments\nto evaluate each clustering algorithms and then compare the results\nwith the true classes to compute the RAND index. Before cluster analysis,\neach feature is centered. In our S-AMA algorithm, we set $m=5$ and\n$\\phi=1$ for weight $w_{i_{1},i_{2}}$.\n\nNote that some of the original 15 clusters indicate similar hand movements,\nsuch as curved/vertical swing and horizontal/vertical straight-line.\nBy plotting the first two principal components of the 90 features,\none can see that some clusters are severely overlapped. Therefore,\nfor evaluation purpose, six clusters, including vertical swing (labeled as 3),\nanti-clockwise arc (labeled as 4), clockwise arc (labeled as 5), horizontal straight-line (labeled as 7),\nhorizontal wavy (labeled as 11), and\nvertical wavy (labeled as 12) in the original dataset are selected. Figure \\ref{truelabel} displays the\nplot of the first principal component (PC1) against the second principal\ncomponent (PC2) of 90 features for the selected six clusters with\nthe true cluster labels. \n\n\\begin{figure}[!htb]\n\\protect\\caption{The plot of the first principal component against the second principal\ncomponent of 90 features for the selected six clusters with true cluster\nlabels.}\n\n\n\\centering \\includegraphics[scale=0.8]{true_label} \\label{truelabel}\n\\end{figure}\n\n\nWe first display the clustering path of convex clustering (AMA) using\nall 90 features in Figure \\ref{cvxpath}. Clearly, convex clustering\nis only able to distinguish clusters 4 and 5 and treat the rest clusters\nas one class. This phenomenon shows the curse of dimensionality in\nhigh-dimensional clustering and motivates the need to conduct feature\nselection for improved clustering performance.\n\n\\begin{figure}[!htb]\n\\protect\\caption{The clustering path of convex clustering (AMA) using all 90 features\nby plotting the first principal component (PC1) against the second\nprincipal component (PC2).}\n\n\n\\centering \\includegraphics[scale=0.5]{ama_path_1_2}\\label{cvxpath}\n\\end{figure}\n\n\nWe use S-AMA to solve sparse convex clustering. The tuning parameters\nare selected according to the stability selection in Section \\ref{sec:tuning}.\nTable \\ref{tb_realdata_1} reports the number of estimated clusters,\nthe number of selected features, and the RAND index between the estimated\ncluster membership and the true cluster membership for k-means clustering,\nAMA algorithm, and our S-AMA algorithm. Clearly, both convex clustering\n(AMA) and sparse convex clustering (S-AMA) perform better than k-means,\nwhich indicates that the performance of convex clustering or sparse\nconvex clustering is less sensitive to the assumption of spherical\nclustering centers. In addition, by only using $13$ informative features,\nour S-AMA is able to improve the clustering accuracy of convex clustering\n(AMA) by $45\\%$. This indicates the importance of variable selection\nin high-dimensional clustering.\n\n\\begin{table}[!htb]\n\\centering \\protect\\caption{The number of estimated clusters, the number of selected features,\nand the RAND index for k-means, AMA, and our S-AMA algorithm.}\n\n\n\\label{tb_realdata_1} \\vspace{6pt}\n \\centering \n\\begin{tabular}{c|ccc}\n\\hline\nAlgorithm  & \\# of clusters  & \\# of features  & RAND index \\\\\n\\hline\nk-means  & 2  & 90  & 0.06 \\\\\nAMA  & 3  & 90  & 0.31 \\\\\nS-AMA  & 3  & 13  & \\textbf{0.45}\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\nNext we demonstrate the clustering path of sparse convex clustering\n(S-AMA) with only 13 selected features in Figure \\ref{scvxpath}.\nFigure \\ref{scvxpath} displays three big clusters, which is consistent\nwith the number of estimated clusters shown in Table \\ref{tb_realdata_1}.\nAs tuning parameter $\\gamma_{1}$ increases,\nthe clustering path of S-AMA tends to merge clusters 3, 7 and 12 into\none big cluster, merge cluster 4 and 5 into another big cluster, and\nidentify cluster 11 as the third cluster. This finding is displayed in the final clustering\npath of S-AMA executed at the selected $\\gamma_{1}$ and $\\gamma_{2}$\nas shown in Figure \\ref{truebestlabel}. In the plot, the left-panel\ngraph shows the true cluster labels and the right-panel graph shows\nthe three estimated clusters using S-AMA.\n\n\\begin{figure}[!htb]\n\\protect\\caption{The clustering path of sparse convex clustering (S-AMA) using only\n13 selected features by plotting PC1 against PC2. These 13 features\nare selected via stability selection.}\n\n\n\\centering \\includegraphics[scale=0.5]{scvx_stability_best_path_1_2}\\label{scvxpath}\n\\end{figure}\n\n\n\\begin{figure}[!htb]\n\\protect\\caption{The left-panel graph shows the true cluster labels by plotting PC1\nagainst PC2 using only 13 selected features. The right-panel graph\nshows the estimated cluster membership using S-AMA at the selected\ntuning parameters.}\n\n\n\\centering \\includegraphics[width=16cm,height=8cm]{scvx_stability_best_cluster1}\\label{truebestlabel}\n\\end{figure}\n\n\n\\newpage{}\n\n\n\\section*{Appendix}\n\n\\setcounter{equation}{0} \\setcounter{lemma}{0} \\setcounter{proposition}{0}\n\\global\\long\\def\\theequation{A.\\arabic{equation}}\n \\global\\long\\def\\thesubsection{A.\\arabic{subsection}}\n \\global\\long\\def\\thelemma{A.\\arabic{lemma}}\n \\global\\long\\def\\theproposition{A.\\arabic{proposition}}\n\n\n\n\\subsection*{A.1 Proof of Lemma \\ref{thm:equivalent}}\n\nDenote $\\bfa=\\textrm{vec}(\\bfA)$, a vectorization of the matrix\n$\\bfA$. According to the fact that $A_{i_{1}\\cdot}-A_{i_{2}\\cdot}=\\bfA\\trans(\\bfe_{i_{1}}-\\bfe_{i_{2}})$\nand the property of the tensor product $\\textrm{vec}(\\bfR\\bfS\\bfT)=[\\bfT^{T}\\otimes\\bfR]\\textrm{vec}(\\bfS)$,\nsolving the minimization of $f(\\bfA)$ is equivalent to minimize\n\\begin{eqnarray*}\nf(\\bfa)=\\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\|\\bfB_{l}\\bfP\\bfa-\\wt{\\bfv}_{l}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2},\n\\end{eqnarray*}\nwhere $\\bfB_{l}=(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\otimes\\bfI_{p}$\nand $\\bfP$ is a permutation matrix such that $\\textrm{vec}(\\bfA\\trans)=\\bfP\\textrm{vec}(\\bfA)$ ($\\bfP\\trans=\\bfP^{-1}$). Letting\n$\\bfB\\trans=\\left(\\bfB\\trans_{1},\\ldots,\\bfB\\trans_{|\\calE|}\\right),\\wt{\\bfv}\\trans=\\left(\\wt{\\bfv}_{1}\\trans,\\ldots,\\wt{\\bfv}_{|\\calE|}\\trans\\right)$,\nit becomes\n\\begin{eqnarray*}\nf(\\bfa)=\\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\frac{\\nu}{2}\\|\\bfB\\bfP\\bfa-\\wt{\\bfv}\\|_{2}^{2}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\nTo further simplify the formulae, the following proposition is needed,\nwith proof shown later.\n\\begin{proposition} \\label{prop1} For permutation\nmatrix $\\bfP$ and any $n$-dim vector $\\bfd$, $\\left[\\bfd\\trans\\otimes\\bfI_{p}\\right]\\bfP=\\bfI_{p}\\otimes\\bfd\\trans.$\n\\end{proposition}\nBy Proposition \\ref{prop1}, $\\bfB_{l}\\bfP=\\bfI_{p}\\otimes(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\triangleq\\bfC_{l}$.\nLet $\\bfC\\trans=\\left(\\bfC\\trans_{1},\\ldots,\\bfC\\trans_{|\\calE|}\\right)$,\nthen the second term in $f(\\bfa)$ becomes $\\frac{\\nu}{2}\\|\\bfC\\bfa-\\wt{\\bfv}\\|_{2}^{2}=\\frac{\\nu}{2}\\sum_{j=1}^{p} \\sum_{l\\in\\calE}\\left((\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans\\bfa_{j}-\\wt{v}_{jl}\\right)_{2}^{2}$.\n\nTherefore, the objective function can be separated to $p$ sub-optimization\nquestions:\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfx_{j}-\\bfa_{j}\\|_{2}^{2}+\\frac{\\nu}{2}\\sum_{l\\in\\calE}\\left((\\bfe_{i_{1}}-\\bfe_{i_2})\\trans\\bfa_{j}-\\wt{v}_{jl}\\right)_{2}^{2} +\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2},\\quad j=1,\\ldots,p.\n\\end{eqnarray*}\nBy some algebra, if $\\calE$ contains all possible edges, it can be\nrewritten as\n\\begin{eqnarray}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\bfa_{j}\\trans\\bfM\\bfa_{j}-\\bfz_{i}\\trans\\bfa_{j} +\\frac{1}{2}\\nu\\wt{\\bfv}_{j\\cdot}\\trans\\wt{\\bfv}_{j\\cdot}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2},\\label{subobj}\n\\end{eqnarray}\nwhere $\\wt{\\bfv}_{j\\cdot}=(\\wt{v}_{j1},\\ldots,\\wt{v}_{j|\\calE|})\\trans$,\n$\\bfM=\\bfI_{n}+\\nu\\sum_{l\\in\\calE}(\\bfe_{i_{1}}-\\bfe_{i_{2}})(\\bfe_{i_{1}}-\\bfe_{i_{2}})\\trans=(1+n\\nu)\\bfI_{n}-\\nu\\bfone_{n}\\bfone_{n}\\trans$,\nand $\\bfz_{j}=\\bfx_{j}+\\nu\\sum_{l\\in\\calE}\\wt{v}_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$.\nThe KKT conditions of \\eqref{subobj} are\n\\begin{eqnarray*}\n\\forall\\bfa_{j}\\neq\\bfzero,\\quad\\bfM\\bfa_{j}-\\bfz_{i}+\\frac{\\gamma_{2}u_{j}\\bfa_{j}}{\\|\\bfa_{j}\\|_{2}}=\\bfzero;\\quad\\quad\\ \\forall\\bfa_{j}=\\bfzero,\\quad\\|\\bfz_{j}\\|_{2}\\leq\\gamma_{2}u_{j}.\n\\end{eqnarray*}\n\n\nHere are some remarks on the above KKT conditions. $\\bfM$ is positive\ndefinite, thus it can be diagonalized by $\\bfM=\\bfS\\trans\\bPhi\\bfS$,\nwhere $\\bPhi=\\textrm{diag}(\\phi_{1},\\ldots,\\phi_{n})$ and $\\bfS$\nis an orthogonal matrix. It can be verified that $\\phi_{1}=1$ and\n$\\phi_{i}=1+n\\nu,i=2,\\ldots,n$. Then $\\bPhi\\bfS\\bfa_{j}-\\bfS\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\bfS\\bfa_{j}}{\\|\\bfS\\bfa_{j}\\|_{2}}=\\bfzero$.\nLet $\\wt\\bfa_{j}=\\bfS\\bfa_{j}$. One needs to solve $\\bPhi\\wt\\bfa_{j}-\\bfS\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\wt\\bfa_{j}}{\\|\\wt\\bfa_{j}\\|_{2}}=\\bfzero$.\nIf $\\nu=0,\\bPsi=\\bfI$, the solution $\\wt\\bfa_{j}$ shares the same\ndirection. Then an explicit soft-threshold formula can be obtained,\nand this situation under the standard group LASSO problem was discussed\nin \\citet{Yuan2006}. But if $\\nu\\neq0$, a scaling transformation\nis applied to $\\wt\\bfa_{j}$, and there is no explicit solution.\n\nAlternatively, rewrite \\eqref{subobj} so that existing algorithms\ncan be applied. Define\n\\begin{eqnarray*}\n\\bfN=\\sqrt{1+n\\nu}\\bfI_{n}-\\frac{\\sqrt{1+n\\nu}-1}{n}\\bfone_{n}\\bfone_{n}\\trans,\n\\end{eqnarray*}\nwhich performs like a ``design matrix\". It can be verified\nthat $\\bfM=\\bfN\\bfN$ and $\\bfN^{-1}$ has the form defined in Lemma \\ref{thm:equivalent}. Let $\\bfy_{j}=(\\bfN)^{-1}\\bfz_{j}$, which\nperforms like a pseudo outcome in the $j$-th sub-problem. Then \\eqref{subobj}\nis equivalent to\n\\begin{eqnarray*}\n\\min_{\\bfa_{j}}\\frac{1}{2}\\|\\bfy_{j}-\\bfN\\bfa_{j}\\|_{2}^{2}+\\gamma_{2}u_{j}\\|\\bfa_{j}\\|_{2}.\n\\end{eqnarray*}\nNote that during the whole algorithm, $\\bfN$ and its inverse are\ncalculated only once. This ends the proof of Lemma \\ref{thm:equivalent}.\n\\hfill{}$\\blacksquare$\n\n\\noindent \\textbf{{Proof of Proposition \\ref{prop1}:}} Note that\n$\\bfP=(P_{kl}),1\\leq k,l\\leq np$ here is a unique permutation matrix\nsuch that $P_{kl}=1$ if $k=(i-1)p+j$ and $l=(j-1)n+i,1\\leq i\\leq n,1\\leq j\\leq p$,\nand 0 otherwise. By the definition of $\\bfP$, it is clear that multiplying\na matrix by $\\bfP$ on the right moves its $k$-th column to the $l$-th\ncolumn when $P_{kl}=1$.\n\nConsider the $i$-th element $d_{i}$ of $\\bfd$, then in $\\bfd\\trans\\otimes\\bfI_{p}$,\nits entries at $(j,(i-1)p+j)$ equal $d_{i}$, $j=1,\\ldots,p$. Thus,\nin $(\\bfd\\trans\\otimes\\bfI_{n})\\bfP$, the entry at $(j,(j-1)n+i)$\nequals to $d_{i}$. In $\\bfI_{p}\\otimes\\bfd\\trans$, it is easy to\nsee the entry at $(j,(j-1)n+i)$ equal $d_{i},i=1,\\ldots,n,j=1,\\ldots,p$.\n\n\n\\subsection*{A.2 Update Steps of S-AMA}\n\nBy letting $\\nu=0$ while updating $\\bfA$, the S-ADMM algorithm can\nbe simplified significantly. Noting that $\\bfM=\\bfI_{n}$ and $\\bfz_{j}=\\bfx_{j}+\\sum_{l\\in\\calE}\\lambda_{jl}(\\bfe_{i_{1}}-\\bfe_{i_{2}})$,\nwhere $\\lambda_{jl}$ is the $j$-th element of $\\blambda_{l}$, the\nKKT conditions are\n", "index": 13, "text": "\n\\[\n\\forall\\bfa_{j}\\neq\\bfzero,\\quad\\bfa_{j}-\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\bfa_{j}}{\\|\\bfa_{j}\\|}=\\bfzero;\\quad\\quad\\forall\\bfa_{j}=\\bfzero,\\quad\\|\\bfz_{i}\\|_{2}\\leq\\gamma_{2}u_{j}.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\forall\\bfa_{j}\\neq\\bfzero,\\quad\\bfa_{j}-\\bfz_{j}+\\frac{\\gamma_{2}u_{j}\\bfa_{j%&#10;}}{\\|\\bfa_{j}\\|}=\\bfzero;\\quad\\quad\\forall\\bfa_{j}=\\bfzero,\\quad\\|\\bfz_{i}\\|_{%&#10;2}\\leq\\gamma_{2}u_{j}.\" display=\"block\"><mrow><mrow><mrow><mrow><mo>\u2200</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub></mrow><mo>\u2260</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfzero</mtext></merror></mrow><mo rspace=\"12.5pt\">,</mo><mrow><mrow><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub><mo>-</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfz</mtext></merror><mi>j</mi></msub></mrow><mo>+</mo><mfrac><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>u</mi><mi>j</mi></msub><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub></mrow><mrow><mo>\u2225</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub><mo>\u2225</mo></mrow></mfrac></mrow><mo>=</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfzero</mtext></merror></mrow><mo rspace=\"22.5pt\">;</mo><mrow><mrow><mrow><mo>\u2200</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfa</mtext></merror><mi>j</mi></msub></mrow><mo>=</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfzero</mtext></merror></mrow><mo rspace=\"12.5pt\">,</mo><mrow><msub><mrow><mo>\u2225</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfz</mtext></merror><mi>i</mi></msub><mo>\u2225</mo></mrow><mn>2</mn></msub><mo>\u2264</mo><mrow><msub><mi>\u03b3</mi><mn>2</mn></msub><mo>\u2062</mo><msub><mi>u</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nand let $\\bfD\\trans=(\\bfD_{1}\\trans,\\ldots,\\bfD_{|\\calE|+p}\\trans)$. Then \\eqref{obj_df_1} becomes\n\\begin{eqnarray*}\n\\min_{\\bfa\\in\\mathbb{R}^{np\\times1}} &  & \\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\|\\bfD_{s}\\bfa\\|_{1}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\|\\bfD_{s}\\bfa\\|_{2}.\n\\end{eqnarray*}\nActually, the second term can be written component-wisely into $\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\sum_{j=1}^{p}|\\bfd_{sj}\\trans\\bfa|$,\nwhere $\\bfd_{sj}$ is the vector consisting of the $j$-th row of\n$\\bfD_{s}$.\n\nLet $\\wh{\\calB}_{1}=\\wh{\\calB}_{11}\\bigcup\\wh{\\calB}_{12}$, where\n$\\wh{\\calB}_{11}=\\{(s,j):|\\bfd_{sj}\\trans\\wh\\bfa|\\neq0,s=1,\\ldots,|\\calE|,j=1,\\ldots,p\\}$\nand $\\wh{\\calB}_{12}=\\{s:\\|\\bfD_{s}\\wh{\\bfa}\\|_{2}\\neq0,s=|\\calE|+1,\\ldots,|\\calE|+p\\}$.\nThe derivative of \\eqref{obj_df_1} is obtained as\n\\begin{eqnarray*}\n\\bfx-\\bfa=\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\sum_{j=1}^{p}f_{sj}\\bfd_{sj}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\bfD_{s}\\trans\\bfg_{s},\n\\end{eqnarray*}\nwhere $f_{sj}=\\textrm{sgn}(\\bfd_{sj}\\trans\\wh{\\bfa})$, if $(s,j)\\in\\wh{\\calB}_{11}$\nand $f_{sj}\\in[-1,1]$, if $s\\notin\\wh{\\calB}_{11}$, and $\\bfg_{s}={\\bfD_{s}\\wh\\bfa}/{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}$,\nif $s\\in\\wh{\\calB}_{12}$ and $\\bfg_{s}\\in\\{\\Gamma:\\|\\Gamma\\|_{2}\\leq1\\}$,\nif $s\\notin\\wh{\\calB}_{12}$.\n\nDefine matrix $\\bfD_{-\\wh{\\calB}_{1}}$ by removing the rows\nof $\\bfD$ corresponding to those elements in $\\wh{\\calB}_{1}$, and\n\n", "itemtype": "equation", "pos": 53154, "prevtext": "\nThe solutions are $\\wh\\bfa_{j}=\\left(1-\\frac{\\gamma_{2}u_{j}}{\\|\\bfz_{j}\\|_{2}}\\right)_{+}\\bfz_{j}$,\nwhere $(z)_{+}=\\max\\{0,z\\}$. See \\citet{Yuan2006}.\n\nBy applying the projection method, one can update $\\bfv_{l}$ and\n$\\blambda_{l}$ as $\\bfv_{l}^{m+1}=A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{l}^{m}-\\calP_{tB}[A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{l}^{m}]$,\nwhere $t=\\sigma_{l}=\\gamma_{1}w_{l}/\\nu$ and $\\calP_{B}(\\bfz)$ denotes\nprojection onto $B=\\{\\bfy:\\|\\bfy\\|_{\\dag}\\leq1\\}$. The point $\\calP_{B}(\\bfz)$\ncan be characterized by the relations $\\calP_{B}(\\bfz)\\in B$ and\n$\\forall\\bfy\\in B,\\langle\\bfy-\\calP_{B}(\\bfz),\\bfz-\\calP_{B}(\\bfz)\\rangle\\leq0$.\nThen\n\\begin{eqnarray*}\n\\blambda_{l}^{m+1} & = & \\blambda_{l}^{m}+\\nu(\\bfv_{l}^{m+1}-A_{i_{1}\\cdot}^{m+1}+A_{i_{2}\\cdot}^{m+1})\\\\\n & = & -\\nu\\calP_{tB}(A_{i_{1}\\cdot}^{m+1}-A_{i_{2}\\cdot}^{m+1}-\\nu^{-1}\\blambda_{1l}^{m}) \\\\\n & = & \\calP_{C_{l}}(\\blambda_{l}^{m}-\\nu\\bfg_{l}^{m+1}),\n\\end{eqnarray*}\nwhere $\\bfg_{l}^{m}=A_{i_{1}\\cdot}^{m}-A_{i_{2}\\cdot}^{m}$ and $C_{l}=\\{\\blambda_{l}:\\|\\blambda_{l}\\|_{\\dag}\\leq\\gamma_{1}w_{l}\\}$.\nNote that there is no need to update $\\bfv_{l}$, and $\\blambda_{l}$\ncan be directly updated.\n\n\n\\subsection*{A.3 Proofs of Lemmas \\ref{thm:df1}-\\ref{thm:df2}: Degrees of freedom}\n\nFollowing the arguments in \\citet{Tan2015}, the number of degrees of freedom\n(df) of \\eqref{eq:obj_constraint}, when $q=1$ or $2$, can be derived\nunder the assumption $w_{l}=1,l\\in\\calE$ and $X_{i\\cdot}\\sim\\textrm{MVN}_{p}(\\bmu,\\sigma^{2}\\bfI_{p})$.\n\n\\underline{\\textbf{Case $q=1$:}} Rewrite \\eqref{eq:obj_constraint}\ninto the following formulation:\n\\begin{eqnarray}\n\\min_{\\bfa\\in\\mathbb{R}^{np\\times1}} &  & \\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\gamma_{1}\\sum_{l\\in\\calE}w_{l}\\|\\bfC_{l}\\bfa\\|_{1}+\\gamma_{2}\\sum_{j=1}^{p}u_{j}\\|(\\bfe_{j}^{*T}\\otimes\\bfI_{n})\\bfa\\|_{2},\\label{obj_df_1}\n\\end{eqnarray}\nwhere $\\bfe_{j}^{*}$ is a $p$-dim vector with its $j$-th element\nas 1 and 0 otherwise.\n\nDefine\n\n", "index": 15, "text": "\\begin{equation}\n\\bfD_{j} = \\begin{cases}  w_{j}\\bfC_{j} & \\mbox{if } s=1,\\ldots,|\\calE| \\\\ u_{s-|\\calE|}(\\bfe_{s-|\\calE|}^{*t}\\otimes\\bfI_{n}), & \\mbox{if } s=|\\calE|+1,\\ldots,|\\calE|+p \\end{cases}, \\label{eqn:defD}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\bfD_{j}=\\begin{cases}w_{j}\\bfC_{j}&amp;\\mbox{if }s=1,\\ldots,|\\calE|\\\\&#10;u_{s-|\\calE|}(\\bfe_{s-|\\calE|}^{*t}\\otimes\\bfI_{n}),&amp;\\mbox{if }s=|\\calE|+1,%&#10;\\ldots,|\\calE|+p\\end{cases},\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfD</mtext></merror><mi>j</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfC</mtext></merror><mi>j</mi></msub></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>s</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calE</mtext></merror><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"left\"><mrow><mrow><msub><mi>u</mi><mrow><mi>s</mi><mo>-</mo><mrow><mo stretchy=\"false\">|</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calE</mtext></merror><mo stretchy=\"false\">|</mo></mrow></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfe</mtext></merror><mrow><mi>s</mi><mo>-</mo><mrow><mo stretchy=\"false\">|</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calE</mtext></merror><mo stretchy=\"false\">|</mo></mrow></mrow><mrow><mi/><mo>*</mo><mi>t</mi></mrow></msubsup><mo>\u2297</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfI</mtext></merror><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign=\"left\"><mrow><mrow><mtext>if\u00a0</mtext><mo>\u2062</mo><mi>s</mi></mrow><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">|</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calE</mtext></merror><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant=\"normal\">\u2026</mi><mo>,</mo><mrow><mrow><mo stretchy=\"false\">|</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calE</mtext></merror><mo stretchy=\"false\">|</mo></mrow><mo>+</mo><mi>p</mi></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04586.tex", "nexttext": "\nwhere $(\\bfD)^{+}$ is the Moore-Penrose pseudo-inverse of any matrix\n$\\bfD$. By the property $\\bfD_{-\\wh{\\calB}_{1}}\\wh\\bfa=\\bfzero$,\n\\begin{eqnarray*}\n\\bfP_{1}\\bfx-\\wh\\bfa & = & \\gamma_{1}\\bfP_{1}\\sum_{s=1}^{|\\calE|}\\sum_{j=1}^{p}f_{sj}\\bfd_{sj}+\\gamma_{2}\\bfP_{1}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\bfD_{s}\\trans\\bfg_{s}\\\\\n & = & \\gamma_{1}\\bfP_{1}\\sum_{(s,j)\\in\\calB_{11}}\\textrm{sgn}(\\bfd_{sj}\\trans\\wh{\\bfa})\\bfd_{sj}+\\gamma_{2}\\bfP_{1}\\sum_{s\\in\\calB_{12}}\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}.\n\\end{eqnarray*}\n\n\nBy the property shown by \\citet{Vaiter2012}; i.e., there exists a\nneighborhood around almost every $\\bfx$ such that the solution $\\wh\\bfa$\nis locally constant with respect to $\\bfx$, the derivative of the\nabove equation with respect to $\\bfx$ is\n\\begin{eqnarray*}\n\\bfP_{1}-\\frac{\\partial\\wh\\bfa}{\\partial\\bfx} & = & \\gamma_{2}\\bfP_{1}\\sum_{s\\in\\calB_{12}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh\\bfa\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\frac{\\partial\\wh\\bfa}{\\partial\\bfx}.\n\\end{eqnarray*}\nTherefore, ${\\rm df}\\triangleq\\textrm{tr}(\\frac{\\partial\\wh\\bfa}{\\partial\\bfx})$\nis of the form\n\\begin{eqnarray*}\n\\textrm{df}_{1} & = & \\textrm{tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{2}\\bfP_{1}\\sum_{s\\in\\calB_{12}}\\bigg(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh\\bfa\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\bigg)\\Bigg]^{-1}\\bfP_{1}\\Bigg).\n\\end{eqnarray*}\n\n\n\\underline{\\textbf{Case $q=2$:}} Rewrite \\eqref{eq:obj_constraint}\ninto the following form when $q=2$, the $L_{2}$-norm:\n\\begin{eqnarray}\n\\min_{\\bfa\\in\\mathbb{R}^{np\\times1}} &  & \\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\gamma_{1}\\sum_{j=1}^{|\\calE|}\\|\\bfD_{s}\\bfa\\|_{2}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\|\\bfD_{s}\\bfa\\|_{2}.\\label{new_obj_df_2}\n\\end{eqnarray}\n\n\nLet $\\wh{\\calB}_{2}=\\{s:\\|\\bfD_{s}\\wh{\\bfa}\\|_{2}\\neq0,s=1,\\ldots,|\\calE|+p\\}$.\nThe derivative of \\eqref{new_obj_df_2} is obtained as\n\\begin{eqnarray*}\n\\bfx-\\bfa=\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\bfD_{s}\\trans\\bfg_{j}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\bfD_{s}\\trans\\bfg_{j},\n\\end{eqnarray*}\nwhere $\\bfg_{s}={\\bfD_{s}\\wh\\bfa}/{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}$, if\n$s\\in\\wh{\\calB}_{2}$ and $\\bfg_{s}\\in\\{\\Gamma:\\|\\Gamma\\|_{2}\\leq1\\}$,\nif $s\\notin\\wh{\\calB}_{2}$. Then, define matrix $\\bfD_{-\\wh{\\calB}_{2}}$\nby removing the rows of $\\bfD$ corresponding to those elements in\n$\\wh{\\calB}_{2}$, and $\\bfP_{2}=\\bfI-\\bfD_{-\\wh{\\calB}_{2}}\\trans(\\bfD_{-\\wh{\\calB}_{2}}\\bfD_{-\\wh{\\calB}_{2}}\\trans)^{+}\\bfD_{-\\wh{\\calB}_{2}}$.\nIt follows\n\\begin{eqnarray*}\n\\frac{\\partial\\wh\\bfa}{\\partial\\bfx} & = & \\Bigg[\\bfI+\\gamma_{1}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{1,\\ldots,|\\calE|\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\\\\n &  & +\\gamma_{2}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{|\\calE|+1,\\ldots,|\\calE|+p\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\Bigg]^{-1}\\bfP_{2}.\n\\end{eqnarray*}\nTherefore, the df when $q=2$ is of the form\n\\begin{eqnarray*}\n\\text{df}_{2} & = & \\text{tr}\\Bigg(\\Bigg[\\bfI+\\gamma_{1}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{1,\\ldots,|\\calE|\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\\\\n &  & +\\gamma_{2}\\bfP_{2}\\sum_{s\\in\\wh{\\calB}_{2}\\cap\\{|\\calE|+1,\\ldots,|\\calE|+p\\}}\\left(\\frac{\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}-\\frac{\\bfD_{s}\\trans\\bfD_{s}\\wh\\bfa\\wh{\\bfa}\\trans\\bfD_{s}\\trans\\bfD_{s}}{\\|\\bfD_{s}\\wh\\bfa\\|_{2}^{3}}\\right)\\Bigg]^{-1}\\bfP_{2}\\Bigg).\n\\end{eqnarray*}\n\n\n \\bibliographystyle{apa}\n\\bibliography{refs_clustering}\n\n\n", "itemtype": "equation", "pos": 54806, "prevtext": "\nand let $\\bfD\\trans=(\\bfD_{1}\\trans,\\ldots,\\bfD_{|\\calE|+p}\\trans)$. Then \\eqref{obj_df_1} becomes\n\\begin{eqnarray*}\n\\min_{\\bfa\\in\\mathbb{R}^{np\\times1}} &  & \\frac{1}{2}\\|\\bfx-\\bfa\\|_{2}^{2}+\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\|\\bfD_{s}\\bfa\\|_{1}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\|\\bfD_{s}\\bfa\\|_{2}.\n\\end{eqnarray*}\nActually, the second term can be written component-wisely into $\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\sum_{j=1}^{p}|\\bfd_{sj}\\trans\\bfa|$,\nwhere $\\bfd_{sj}$ is the vector consisting of the $j$-th row of\n$\\bfD_{s}$.\n\nLet $\\wh{\\calB}_{1}=\\wh{\\calB}_{11}\\bigcup\\wh{\\calB}_{12}$, where\n$\\wh{\\calB}_{11}=\\{(s,j):|\\bfd_{sj}\\trans\\wh\\bfa|\\neq0,s=1,\\ldots,|\\calE|,j=1,\\ldots,p\\}$\nand $\\wh{\\calB}_{12}=\\{s:\\|\\bfD_{s}\\wh{\\bfa}\\|_{2}\\neq0,s=|\\calE|+1,\\ldots,|\\calE|+p\\}$.\nThe derivative of \\eqref{obj_df_1} is obtained as\n\\begin{eqnarray*}\n\\bfx-\\bfa=\\gamma_{1}\\sum_{s=1}^{|\\calE|}\\sum_{j=1}^{p}f_{sj}\\bfd_{sj}+\\gamma_{2}\\sum_{s=|\\calE|+1}^{|\\calE|+p}\\bfD_{s}\\trans\\bfg_{s},\n\\end{eqnarray*}\nwhere $f_{sj}=\\textrm{sgn}(\\bfd_{sj}\\trans\\wh{\\bfa})$, if $(s,j)\\in\\wh{\\calB}_{11}$\nand $f_{sj}\\in[-1,1]$, if $s\\notin\\wh{\\calB}_{11}$, and $\\bfg_{s}={\\bfD_{s}\\wh\\bfa}/{\\|\\bfD_{s}\\wh\\bfa\\|_{2}}$,\nif $s\\in\\wh{\\calB}_{12}$ and $\\bfg_{s}\\in\\{\\Gamma:\\|\\Gamma\\|_{2}\\leq1\\}$,\nif $s\\notin\\wh{\\calB}_{12}$.\n\nDefine matrix $\\bfD_{-\\wh{\\calB}_{1}}$ by removing the rows\nof $\\bfD$ corresponding to those elements in $\\wh{\\calB}_{1}$, and\n\n", "index": 17, "text": "\\begin{equation}\n\\bfP_{1}=\\bfI-\\bfD_{-\\wh{\\calB}_{1}}\\trans(\\bfD_{-\\wh{\\calB}_{1}}\\bfD_{-\\wh{\\calB}_{1}}\\trans)^{+}\\bfD_{-\\wh{\\calB}_{1}}, \\label{eqn:defP}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\bfP_{1}=\\bfI-\\bfD_{-\\wh{\\calB}_{1}}\\trans(\\bfD_{-\\wh{\\calB}_{1}}\\bfD_{-\\wh{%&#10;\\calB}_{1}}\\trans)^{+}\\bfD_{-\\wh{\\calB}_{1}},\" display=\"block\"><mrow><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfP</mtext></merror><mn>1</mn></msub><mo>=</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfI</mtext></merror><mo>-</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfD</mtext></merror><mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\wh</mtext></merror><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calB</mtext></merror><mn>1</mn></msub></mrow></mrow></msub><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\trans</mtext></merror><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfD</mtext></merror><mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\wh</mtext></merror><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calB</mtext></merror><mn>1</mn></msub></mrow></mrow></msub><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfD</mtext></merror><mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\wh</mtext></merror><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calB</mtext></merror><mn>1</mn></msub></mrow></mrow></msub><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\trans</mtext></merror></mrow><mo stretchy=\"false\">)</mo></mrow><mo>+</mo></msup><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\bfD</mtext></merror><mrow><mo>-</mo><mrow><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\wh</mtext></merror><mo>\u2062</mo><msub><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\calB</mtext></merror><mn>1</mn></msub></mrow></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}]