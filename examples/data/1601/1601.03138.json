[{"file": "1601.03138.tex", "nexttext": "\nwhere   $T_{\\rm dc}$, $T_{\\rm lt}$, $T_{\\rm exch}$,\n$T_{\\rm icalc}$, and $T_{\\rm misc}$ are the times for\ndomain composition and particle exchange, local tree construction,\nexchange of particles and superparticles for interaction calculation,\ninteraction calculation, and other calculations such as particle\nupdate, respectively. The term $n_{\\rm dc}$ is the interval at which\nthe domain decomposition is performed.\n\nIn the following, we first construct the model for the communication\ntime. Then we construct models for each terms of \nthe right hand side of Eq.~\\ref{eq:totalcost}, and finally we compare the\nmodel with the actual measurement presented in section\n\\ref{sec:total_time}.\n\n\\subsubsection{Communication model}\n\\label{sec:comm_model}\n\nSince what ultimately determines the efficiency of a calculation\nperformed on large-scale parallel machine is the communication\noverhead, it is very important to understand what types of\ncommunication would take what amount of time on actual hardware. In\nthis section, we summarize the characteristics of the communication\nperformance of K computer.\n\nIn FDPS, almost all communications are through the use of collective\ncommunications, such as {\\tt MPI\\_Allreduce}, {\\tt MPI\\_Alltoall}, and\n{\\tt MPI\\_Alltoallv}. However, to measure the performance of these\nroutines for uniform message length is not enough, since the amount of\ndata to be transferred between processes generally depends on the\nphysical distance between domains assigned to those\nprocesses. Therefore, we first present the timing results for simple\npoint-to-point communication, and then for collective communications.\n\nFigure \\ref{fig:pingpong} shows the elapsed time as the function of\nthe message length, for point-to-point communication between\n``neighboring'' processes. In the case of K computer, we used\nthree-dimensional node allocation, so that ``neighboring'' processes\nare actually close to each other in its torus network.\n\nWe can see that the elapsed time can be fitted reasonably well as\n\n", "itemtype": "equation", "pos": 58361, "prevtext": "\n\n\\title{Implementation and performance of FDPS: A Framework Developing\n  Parallel Particle Simulation Codes}\n\n\\author{Masaki \\textsc{Iwasawa}\\altaffilmark{1}}\n\\email{masaki.iwasawa@riken.jp}\n\\altaffiltext{1}{RIKEN Advanced Institute for Computational Science,\n7--1--26 Minatojima-minami-machi, Chuo-ku, Kobe, Hyogo, Japan}\n\n\\author{Ataru \\textsc{Tanikawa}\\altaffilmark{1,2}}\n\\email{tanikawa@ea.c.u-tokyo.ac.jp}\n\\altaffiltext{2}{Department of Earth and Astronomy, College of Arts and Science, The University of\nTokyo, 3--8--1 Komaba, Meguro-ku, Tokyo, Japan}\n  \n\\author{Natsuki \\textsc{Hosono}\\altaffilmark{1}}\n\\email{natsuki.hosono@riken.jp}\n\n\\author{Keigo \\textsc{Nitadori}\\altaffilmark{1}}\n\\email{keigo@riken.jp}\n\n\\author{Takayuki \\textsc{Muranushi}\\altaffilmark{1}}\n\\email{takayuki.muranushi@riken.jp}\n\n\\author{Junichiro \\textsc{Makino}\\altaffilmark{1,3}}\n\\email{makino@mail.jmlab.jp}\n\\altaffiltext{3}{Earth-Life Science Institute, Tokyo Institute of\n  Technology, 2--12--1 Ookayama, Meguro-ku, Tokyo, Japan}\n\n\\KeyWords{Methods: numerical --- Galaxy: evolution --- Cosmology: dark matter --- Planets and satellites: formation}\n\n\\maketitle\n\n\\begin{abstract}\n\n  We have developed FDPS (Framework for Developing Particle\n  Simulator), which enables researchers and programmers to develop\n  high-performance parallel particle simulation codes easily. The\n  basic idea of FDPS is to separate the program code for complex\n  parallelization including domain decomposition, redistribution of\n  particles, and exchange of particle information for interaction\n  calculation between nodes, from actual interaction calculation and\n  orbital integration. FDPS provides the former part and the users\n  write the latter. Thus, a user can implement a high-performance\n  fully parallelized $N$-body code only in 120 lines. In this paper,\n  we present the structure and implementation of FDPS, and describe\n  its performance on three sample applications: disk galaxy\n  simulation, cosmological simulation and Giant impact simulation. All\n  codes show very good parallel efficiency and scalability on K\n  computer and XC30. FDPS lets the researchers concentrate on the\n  implementation of physics and mathematical schemes, without wasting\n  their time on the development and performance tuning of their codes.\n  \n\\end{abstract}\n\n\n\n\n\\section{Introduction}\n\nParticle-based simulations have been widely used in the field of\ncomputational astronomy. In particle-based simulations, a system under\nstudy is regarded as a collection of mutually-interacting particles,\nor a particle system, and its evolution is described by the evolution\n(in many cases, motion) of individual particles. Since particles move\nautomatically as the result of integration of the equation of motion\nof the particle, the particle-based simulations have an advantage for\na system strongly deformed or a system with high density contrast.\nThis is one of the reason for which particle-based simulations are\nwidely used in astronomy. Examples include the cosmological\nsimulations or the planet-formation simulations with gravitational\n$N$-body code, the simulations of star and galaxy formation with the\nSmoothed Particle Hydrodynamics (SPH) code or other particle-based\ncodes, and the simulations of planetesimals formation with the\nDiscrete Element Method (DEM) code.\n\nIn order to improve the resolution and accuracy of particle-based\nsimulations, it is necessary to utilize present-day HPC systems.\nHowever, to develop a calculation code for particle systems which can\nachieve high efficiency on present-day HPC systems is difficult and\ntime-consuming. There are several reasons for this difficulty. In\norder to achieve high efficiency, we need to decompose the\ncomputational domain, assign subdomains to computing nodes, and\nredistribute particles according to their positions. This\ndecomposition should be dynamically changed to guarantee the good load\nbalance. This division of the computational domain means that\ncomputing nodes need to exchange the information of particles to\nevaluate the interactions on their particles. To achieve high\nefficiency, the amount of the exchanged data must be minimized, and\ninteraction calculation should also be efficient, making good use of\ncache memory and SIMD execution units. It is also important to make\nuse of GPGPUs or other types of accelerators, if available.\n\nDomain decomposition and load balance have been discussed in a number\nof works \\citep{1994JCoPh.111..136S, 1996NewA....1..133D,\n2004PASJ...56..521M, 2009PASJ...61.1319I,\nIshiyama:2012:PAN:2388996.2389003}.  The efficient use of SIMD units\nis discussed\nin \\citet{2006NewA...12..169N}, \\citet{2012NewA...17...82T}\nand \\citet{2013NewA...19...74T}, and GPGPUs\nin \\citet{2009NewA...14..630G}, \\citet{hamada2009novel}, \\citet{Hamada:2009:THN:1654059.1654123}, \\citet{Hamada:2010:TAN:1884643.1884644}, \\citet{2012JCoPh.231.2825B}\nand\n\\citet{Bedorf:2014:PGT:2683593.2683600}.\n\nThus, to develop a code which has all of these necessary features for\npresent-day HPC systems has become a big project which requires\nmulti-year effort of a multi-person team. It has become difficult for\nresearchers outside such a team to try any new experiments which\nrequires nontrivial modification of the code. If one wants to develop\na new numerical scheme for particle-based simulation or to apply it to\nnew problem, it is necessary to write his/her own code. However, it is\npractically impossible for a single person, or even for a group of\npeople, to develop a new code which can run efficiently on present-day\nHPC systems in a reasonable time.  This difficulty, in our opinion,\nhas slowed down the evolution of the entire field of computational\nscience for the last two decades. Here we discussed the situation of\nparticle-based codes. The situation of grid-based codes is similar.\n\nIn order to overcome the difficulty described above, we developed FDPS\n(Framework for Developing Particle\nSimulator)\\footnote{https://github.com/FDPS/FDPS}. Our goal is to\nprovide a software framework which enables researchers to develop\nhigh-performance particle simulation codes quickly. The basic idea of\nFDPS is to separate the above-described ``difficult'' part of the\ncode, such as the domain decomposition and exchange of particles, from\nthe description of the physical problem itself. The difficult part is\nimplemented in FDPS as a library. A user of FDPS needs to define the\ndata structure of particles and the interaction function of\nparticles. Then FDPS receives these code, and generates an efficient\ncode to perform domain decomposition and parallelized calculation of\ninteraction. The user program first uses the functions of FDPS to do\nthe interaction calculation, and then updates the physical data of\nparticles, and repeats this loop until the end of simulation.  Thus,\nthe user of FDPS does not have to write complex code for\nparallelization.\n\nFDPS supports particle simulations with arbitrary pairwise\ninteractions. If the physical system includes many-body interactions,\na user can calculate such interactions within his/her code, letting\nFDPS do the parallelization and calculation of pairwise interactions.\nIn FDPS, the separation of parallelization and physical problem is\ndone through the use of abstract data types implemented using C++\ntemplates. Users of FDPS provide actual particle data class and\ninteraction function to the class template defined in FDPS. C++\ncompiler the ``instantiate'' the real code from the template. Thus,\nusers can develop a gravitational $N$-body code, SPH code, other\nparticles-based fluid code, large-scale molecular dynamics code, DEM\ncode, and many other particle-based code using FDPS, and all of them\nwill automatically parallelized with near-optimal load balancing. A\nsimilar approach to provide general-purpose framework is\nin \\citet{1995CoPhC..87..266W}, but it was limited to long-range\ninteraction with $1/r$ potential.\n\nOne might think that, even though it is not impossible to develop\ngeneral-purpose framework for parallel particle-based simulations,\nsuch a framework would be inevitably  slow. Since the goal of FDPS is\nto make efficient use of present-day HPC systems, we designed FDPS so\nthat it can achieve the performance not worse than that of existing\nlarge-scale parallel codes for particle simulations. We will discuss\nthe achieved performance later.\n\nThe structure of this paper is as follows. In section~\\ref{sec:user},\nwe overview how FDPS users implement a simulation code on FDPS.  In\nsection~\\ref{sec:implementation}, we describe parallel algorithm\nimplemented in FDPS. In section~\\ref{sec:performance}, we measure the\nperformance for three astrophysical applications developed using FDPS\nand construt a performance model. Finally, we summarize this study in\nsection~\\ref{sec:conclusion}.\n\n\n\n\\section{How FDPS works}\n\\label{sec:user}\n\nIn this section, we describe how FDPS works in detail.  In\nsection~\\ref{sec:design}, we describe the design concept of FDPS. In\nsection~\\ref{sec:samplecode}, we present an $N$-body simulation code\nas an example, and describe how FDPS does the parallelization and what\na user program should do.\n\n\\subsection{Design and implementation}\n\\label{sec:design}\n\nIn this section, we describe the design and implementation of FDPS. We\nfirst present the abstract view of calculation codes for\nparticle-based simulations on distributed-memory parallel computers,\nand then describe how such abstraction is realized in FDPS.\n\n\\subsubsection{Abstract view of particle-based simulation codes}\n\\label{sec:view}\n\nIn a particle-based simulation code for distributed-memory parallel\ncomputers, which uses spatial decomposition to reduce communication,\ntime integration of the system proceeds in the following steps:\n\\begin{enumerate}\n\\item The entire computational domain is decomposed into subdomains,\n  and usually one subdomain is assigned to one MPI\n  process. Decomposition should achieve minimization of inter-node\n  communication and good load-balance between nodes.\n \\label{proc:decompose}\n\n\\item Particles are redistributed among the processes, so that each\n  process owns particles in its subdomain.\\label{proc:exchange}\n\n\\item Interactions between particles are calculated.  Each process\n  calculates interactions on its particles. To do so, it receives the\n  necessary data from other processes.\n  \\label{proc:interaction}\n\n\\item The data of particles are updated using the calculated\n  interaction. This part is done without inter-process communication.\n   \\label{proc:local}\n\\end{enumerate}\n\nSteps \\ref{proc:decompose}, \\ref{proc:exchange}, and\n\\ref{proc:interaction} involve parallelization and inter-process\ncommunications. FDPS provides library functions to perform these\nparts. Therefore, users of FDPS do not have to write their own code to\nperform parallelization and inter-process communication.\n\nLet us now explain how FDPS provides the libraries to perform steps\n\\ref{proc:decompose}, \\ref{proc:exchange}, and\n\\ref{proc:interaction} for arbitrary systems of particles, in other\nwords, how FDPS provides the abstraction of these steps. \n\nThe particle-based simulation codes for which FDPS can be used is\ndescribed by the initial value problem of the following set of \nordinary differential equations:\n\n\\begin{eqnarray}\n\n  \\frac{d{\\vec{{u}}}_i}{dt} = {\\vec{{g}}}\\left(\\sum_j^N {\\vec{{f}}}\n  ({\\vec{{u}}}_i, {\\vec{{u}}}_j), {\\vec{{u}}}_i\\right), \\label{eq:geq}\n\n\\end{eqnarray}\n  \nwhere $N$ is the number of particles in the system, ${\\vec{{u}}}_i$ is\nthe physical quantity vector of a particle, the function ${\\vec{{f}}}$\nindicates the contribution of particle $j$ to the time derivative of\nphysical quantities of particle $i$, and ${\\vec{{g}}}$ is a function\nwhich converts the sum of the contributions to actual time\nderivative. For example, in the case of gravitational $N$-body\nsimulation, ${\\vec{{f}}}$ is the mutual gravity between particles,\n${\\vec{{g}}}$ would add, for example, the acceleration due to some\nexternal field, and ${\\vec{{u}}}_j$ contains all necessary data of\nparticles such as position, velocity, mass, and other parameters.\n\nHereafter, we call a particle receiving the interaction\n``$i$-particle'', and a particle exerting that interaction\n``$j$-particle''. The actual content of vector ${\\vec{{u}}}_i$, and the\nfunctional forms of ${\\vec{{f}}}$ and ${\\vec{{g}}}$ depends on the\nphysical system and how it is discretized.\n\nEquation (\\ref{eq:geq}) implies that FDPS can only handle pairwise\ninteractions which can be added to obtain the total contributions of\nother particles. If many-body interaction is important, one can still\nuse FDPS to perform parallelization and calculation of pairwise\ninteractions. Many-body interaction, such as angle and torsion of\nbonding force in molecular dynamics simulation, can be implemented in\nthe user code.\n\n\\subsubsection{Design concept of FDPS}\n\\label{sec:concept}\n\nFDPS provides a template library in C++ language, which receives the\nuser-defined particle (vector ${\\vec{{u}}}_i$) and functions to\ncalculate pairwise interaction (function ${\\vec{{f}}}$). The functions\nprovided by this template library perform the steps\n\\ref{proc:decompose}, \\ref{proc:exchange}, and \\ref{proc:interaction}\ndescribed above. The function ${\\vec{{g}}}$, and the actual time\nintegration of ${\\vec{{u}}}_i$ are done entirely in the user\nprogram. Thus, FDPS provides a powerful, and yet very flexible\nframework for the development of calculation codes for particle-based\nsimulations.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig1.eps}\n  \\end{center}\n  \\caption{The basic concept of FDPS. The user program gives the\n    definitions of particle and interaction to FDPS, and calls FDPS\n    APIs.}\n  \\label{fig:concept}\n\\end{figure}\n\nA user of FDPS can develop the simulation code in the following three\nsteps:\n\\begin{enumerate}\n\\item Define the data structure for ${\\vec{{u}}}_i$, as a class in the\n  C++ language.\n\\item Define the function ${\\vec{{f}}}$. It receives arrays of\n  $i$-particles and $j$-particles, and calculates and accumulates\n  ${\\vec{{f}}}$ on $i$-particles.\n\n\\item Write the user program using the data class and functions\n  provided by FDPS. Currently, the user program should also be written\n  in C++.\n\\end{enumerate}\n\nFigure~\\ref{fig:concept} illustrates how the user-defined code and\nFDPS functions interact. The user program gives the definition of\nparticle and particle-particle interaction to FDPS at the compile\ntime. When executed, the user program first does the initialization\n(the setup of MPI communication is done through a single call to an\nFDPS initialization function), and the setup of the initial\ncondition. Then, the main integration loop is executed. In the main\nloop, first the domain decomposition and exchange of particles are\nperformed, and then the calculation of interactions is\nperformed. These are all done through library calls to FDPS\nfunctions. Finally, the time integration of particles using the\ncalculated interaction is performed.\n\nIn the above description, the interaction calculation is done once per\none iteration of the main loop. It is possible to use integration\nschemes which requires multiple evaluations of interaction within a\nsingle timestep, such as Runge-Kutta schemes. One can just call\ninteraction calculation API of FDPS, with ${\\vec{{u}}}_i$ containing\nnecessary intermediate values.\n\nFDPS takes care of parallelization using MPI, and it can also use\nOpenMP parallelization for internal operations and also for\ninteraction calculation. Thus, an FDPS user does not have to worry\nabout these issues. The efficient use of the cache memory and the SIMD\nexecution unit is not directly taken care by the FDPS libraries, but\nhandled through the interface to the user-defined interaction\ncalculation function. The interface is defined so that the interaction\ncalculation is done for multiple $j$-particles and multiple\n$i$-particles. Thus, it performs a large amount of calculation, on a\nsmall amount of data, since the calculation cost is the product of the\nnumbers of $i$- and $j$-particles, and data amount is sum of them.  In\norder to make efficient use of the SIMD execution unit, the innermost\nloop should be written in such a way that can be recognized as the\ncandidate of vectorization by the compiler used. The interface is\ndefined as taking AoS (array of structures) arguments.  Some compilers\nfor some architecture might not be able to generate the code to\nutilize the SIMD unit for AoS data. In such a case, the user should\nwrite the interaction function in which the AoS data is converted\ninternally to SoA (structure of arrays) data, and converted back to\nthe AoS form after the interaction is actually calculated.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig2.eps}\n  \\end{center}\n  \\caption{Long-range interaction (left) and short-range interaction\n    (right). Gray, red, and blue points are $i$-particle,\n    $j$-particle, and superparticle, respectively.}\n  \\label{fig:forcetype}\n\\end{figure}\n\nOne might think that the definition of the system given in\nequation~(\\ref{eq:geq})  implies that  FDPS has to calculate $N^2$\ninteractions. If that were the case, FDPS would only be useful for\nsmall problems. We implemented $O(N)$ and $O(N\\log N)$ calculation\nalgorithms, for short-range and long-range interactions.\n\nWithin FDPS, the difference between long-range and short-range\ninteractions is slightly different from the physical one of infinite\nand finite effective ranges. When we can and want to apply\nmultipole-like expansion to the contribution from distant particles,\nwe regard that interaction as long-range. The example of the\nlong-range interaction is gravity and Coulomb interaction in the open\nboundary. When periodic boundary is used, they are usually evaluated\nusing $\\mathrm{P^3M}$ or PME method \\citep{hockney1988computer}, in\nwhich the long-range part is evaluated using FFT, and only the\ninteraction with long-range cutoff is evaluated directly.  Even in\nthis case, we can still apply multipole interaction as used in TreePM\nmethod \\citep{1995ApJS...98..355X, 2000ApJS..128..561B,\n  2002JApA...23..185B, 2004NewA....9..111D, 2005MNRAS.364.1105S,\n  2005PASJ...57..849Y, 2009PASJ...61.1319I,\n  Ishiyama:2012:PAN:2388996.2389003}, and in this case the interaction\nis treated as long-range in FDPS.\n\nFor long-range interactions, FDPS uses standard Barnes-Hut tree\nalgorithm \\citep{1986Natur.324..446B, 1990JCoPh..87..161B} parallelized\nfor distributed-memory machines and optimized for cache memory and\nSIMD units \\citep{2009PASJ...61.1319I,\n  Ishiyama:2012:PAN:2388996.2389003}. For short-range interactions,\ninteraction list, or ``neighbor list'' for a group of $i$ particles is\nconstructed using a tree-based search, and that list is used for the\nactual interaction calculation.  Figure~\\ref{fig:forcetype}\nillustrates the long-range and short-range interactions and how they\nare calculated in FDPS\n\nFor the long-range interaction, a multipole-like interaction is\nused. Thus, equation~(\\ref{eq:geq}) is modified to \n\\begin{eqnarray}\n  \\frac{d{\\vec{{u}}}_i}{dt} = {\\vec{{g}}}\\left( \\sum_j^{N_{\\mathrm{J},i}}\n  {\\vec{{f}}}({\\vec{{u}}}_i,{\\vec{{u}}}_j) + \\sum_{j'}^{N_{\\mathrm{S},i}}\n  {\\vec{{f'}}}({\\vec{{u}}}_i,{\\vec{{u'}}}_{j'}), {\\vec{{u}}}_i\n  \\right), \\label{eq:geqL}\n\\end{eqnarray}\nwhere $N_{\\mathrm{J},i}$ and $N_{\\mathrm{S},i}$ are, the number of\n$j$-particles and superparticles for which we apply multipole-like\nexpansion, the vector ${\\vec{{u'}}}_{j'}$ is the physical quantity\nvector of a superparticle, and the function ${\\vec{{f'}}}$ indicates the\ninteraction exerted on particle $i$ by the superparticle $j'$. In\nsimulations with a large number of particles $N$, $N_{\\mathrm{J},i}$\nand $N_{\\mathrm{S},i}$ are many orders of magnitude smaller than $N$.\nThe user should also specify how superparticles are constructed from\nordinary particles, and also from superparticles in the lower level of\nthe tree. For $1/r$ potential, which is the typical usage of the\nlong-range interaction, FDPS provides the default way of construction\nof superparticles up to the quadrupole moment.\n\nIn the case of the short-range interaction, the calculation of\ncontribution of distant particles is suppressed. Thus,\nequation~(\\ref{eq:geq}) is modified to\n\\begin{eqnarray}\n  \\frac{d{\\vec{{u}}}_i}{dt} = {\\vec{{g}}}\\left(\\sum_j^{N_{\\mathrm{J},i}}\n  {\\vec{{f}}}({\\vec{{u}}}_i,{\\vec{{u}}}_j), {\\vec{{u}}}_i\n  \\right), \\label{eq:geqS}\n\\end{eqnarray}\nAs in the case of the long-range force, $N_{\\mathrm{J},i}$ is much\nsmaller than $N$, and usually independent of $N$.\n\n\n\n\n\n\n\n\n\n\\subsection{A working example --- gravitational \\textit{N}-body problem}\n\\label{sec:samplecode}\n\nIn this section, we present an complete working example of the\nsimulation code written using FDPS, to illustrate how a user actually\nuses FDPS. As the target problem, we use the gravitational $N$-body\nproblem with an open boundary.  Within the terminology of FDPS, the\ninteraction between particles in the gravitational $N$-body problem is\nof the ``long-range'' type. Therefore, we need to specify the function\nto calculate interactions for both the ordinary particles and\nsuperparticles. For the sake of brevity, we use the center-of-mass\napproximation for superparticles, which means we can actually use the\nsame function for both types of particles.\n\nThe physical quantity vector ${\\vec{{u}}}_i$ and interaction functions\n${\\vec{{f}}}$, ${\\vec{{f'}}}$, and ${\\vec{{g}}}$ for the gravitational\n$N$-body problem is now given by:\n\\begin{eqnarray}\n  {\\vec{{u}}}_i &= ({\\vec{{r}}}_i,\n  {\\vec{{v}}}_i,m_i) \\label{eq:PhysicalVectorNbody} \\\\\n\n  {\\vec{{f}}} ({\\vec{{u}}}_i, {\\vec{{u}}}_j) &= \\frac{Gm_j \\left(\n    {\\vec{{r}}}_j - {\\vec{{r}}}_i \\right)}{ \\left( |{\\vec{{r}}}_j -\n    {\\vec{{r}}}_i|^2 + \\epsilon_i^2\n    \\right)^{3/2}} \\label{eq:ParticleParticleNbody} \\\\\n\n  {\\vec{{f'}}} ({\\vec{{u}}}_i, {\\vec{{u'}}}_j) &= \\frac{Gm_j' \\left(\n    {\\vec{{r}}}_j - {\\vec{{r'}}}_i \\right)}{ \\left( |{\\vec{{r}}}_j -\n    {\\vec{{r'}}}_i|^2 + \\epsilon_i^2\n    \\right)^{3/2}} \\label{eq:ParticleSuperparticleNbody} \\\\\n\n  {\\vec{{g}}}({\\vec{{F}}},{\\vec{{u}}}_i)  &= ({\\vec{{v}}}_i,{\\vec{{F}}},0),\n\\label{eq:ConversionNbody}\n\\end{eqnarray}\nwhere $m_i$, ${\\vec{{r}}}_i$, ${\\vec{{v}}}_i$, and $\\epsilon_i$ are, the\nmass, position, velocity, and gravitational softening of particle $i$,\n$m_j'$ and ${\\vec{{r'}}}_j$ are, the mass and position of a\nsuperparticle $j$, and $G$ is the gravitational constant.  Note that\nthe shapes of the functions ${\\vec{{f}}}$ and ${\\vec{{f'}}}$ are the same.\n\nThe following list shows the complete one which can be actually\ncompiled and run, not only on a single-core machine but also\nmassively-parallel, distributed-memory machines such as the full-node\nconfiguration of the K computer. The total number of lines is only\n117.\n\n\\input{sample_src.tex}\n\nNow let us explain how this sample code works. This code consists of\nfour parts: The declaration to use FDPS (lines 1 and 2), the\ndefinition of the particle (the vector ${\\vec{{u}}}_i$) (lines 4 to 35),\nthe definition of the gravitational force (the functions ${\\vec{{f}}}$\nand ${\\vec{{f'}}}$) (lines 37 to 61), and the actual user program,\ncomprising a user-defined main routine and user-defined functions from\nwhich library functions of FDPS are called (lines 63 to line 117). In\nthe following, we explain them step by step.\n\nIn order to declare to use FDPS, the only thing the user program need\nto do is to include the header file ``particle\\_simulator.hpp''. This\nfile and other source library files of FDPS should be in the include\npath of the compiler. Everything in the standard FDPS library is\nprovided as the header source library, since they are implemented as\ntemplate libraries which need to receive particle class and\ninteraction functions. Everything in FDPS is provided in the namespace\n``PS''. Therefore in this sample program, we declare it as the default\nnamespace to simplify the code. (We do not omit the namespace ``PS''\nof FDPS functions and class templates in the main routine to indicate\nthat they are provided by FDPS.)\n\nBefore going to the 2nd parts, let us list the data types and classes\ndefined in FDPS. \\texttt{F32/F64} are data types of 32-bit and 64-bit\nfloating points. \\texttt{S32} is a data type of 32-bit signed integer.\n\\texttt{F64vec} is a class of a vector consisting of three 64-bit\nfloating points. This class provides several operators, such as the\naddition, subtraction and the inner product indicated by ``$*$''.\nUsers need not use these data types in their own program, but some of\nthe functions which users should define should return the values in\nthese data types.\n\nIn the 2nd part, we define the data structure for particles, {\\it\ni.e.} the vector ${\\vec{{u}}}_i$, as a class \\texttt{Nbody}. This class\nhas the following member variables: \\texttt{mass}\n($m_i$), \\texttt{eps} ($\\epsilon_i$), \\texttt{pos}\n(${\\vec{{r}}}_i$), \\texttt{vel} (${\\vec{{v}}}_i$), and \\texttt{acc}\n($d{\\vec{{v}}}_i/dt$). Although the member variable \\texttt{acc} does\nnot appear in equation~(\\ref{eq:PhysicalVectorNbody}) --\n(\\ref{eq:ConversionNbody}), we need this variable to store the result\nof the gravitational force calculation. A particle class for FDPS must\nprovide public member functions \\texttt{getPos}, \\texttt{copyFromFP},\n\\texttt{copyFromForce} and \\texttt{clear}, in these names, so that the internal functions\nof FDPS can access the data within the particle class.  For the name\nof the particle class itself and the names of the member variables, a\nuser can use whatever names allowed by the C++ syntax.  The member\nfunctions \\texttt{predict} and \\texttt{correct} are used in the\nuser-defined part of the code to integrate the orbits of particles.\nNote that since the interaction used here is of $1/r$ type, the\ndefinition and construction method of the superparticle are given as\nthe default in FDPS and not shown here.\n\nIn the 3rd part, the interaction functions ${\\vec{{f}}}$ and\n${\\vec{{f'}}}$ are defined. Since the shapes of the functions\n${\\vec{{f}}}$ and ${\\vec{{f'}}}$ are the same, we give one as a template\nfunction.  The interaction function used in FDPS should have the\nfollowing five arguments. The first argument \\texttt{ip} is the\npointer to the array of variables of particle\nclass \\texttt{Nbody}. This argument specifies $i$-particles which\nreceive the interaction. The second argument \\texttt{ni} is the number\nof $i$-particles. The third argument \\texttt{jp} is the pointer to the\narray of variable of a template data type \\texttt{TPJ}. This argument\nspecifies $j$-particles or superparticles which exert the\ninteraction. The fourth argument \\texttt{nj} is the number of\n$j$-particles or super-particles. The fifth argument \\texttt{force} is\nthe pointer to the array of variables of a user-defined class to which\nthe calculated interactions on $i$-particles can be stored. In this\nexample, we used the particle class itself, but this can be another\nclass or a simple array.\n\nThe interaction function is defined as a function object. Thus, it is\ndeclared as a \\texttt{struct}, with the only member\nfunction \\texttt{operator ()}. FDPS can also accept a function pointer\nfor the interaction function. In this example, the interaction is\ncalculated through a simple double loop. In order to make full\nadvantage of SIMD units in modern processors, architecture-dependent\ntuning may be necessary, but only to this single function.\n\nIn the 4th part, we give the main routine and functions called from\nthe main routine. In the following, we describe the main routine in\ndetail, and briefly discuss other functions. The main routine consists\nof the following seven steps:\n\\begin{enumerate}\n\\item Initialize FDPS (line 92). \\label{proc:init}\n\\item Set simulation time and timestep (lines 93 to 95). \\label{proc:literal}\n\\item Create and initialize objects of FDPS classes (lines 96 to 102). \\label{proc:construct}\n\\item Read in particle data from a file (line 103). \\label{proc:input}\n\\item Calculate the gravitational forces of all the particles at the\n  initial time (lines 104 to 106). \\label{proc:calcinteraction}\n\\item Integrate the orbits of all the particles with Leap-Frog method\n  (lines 107 to 114). \\label{proc:integration}\n\\item Finish the use of  FDPS (line 115). \\label{proc:fin}\n\\end{enumerate}\n\nIn the following, we describe  steps~\\ref{proc:init},\n\\ref{proc:construct}, \\ref{proc:input}, \\ref{proc:calcinteraction},\nand \\ref{proc:fin}, and skip steps~\\ref{proc:literal}\nand \\ref{proc:integration}.  In step~\\ref{proc:literal}, we do not\ncall FDPS libraries.  Although we call FDPS libraries in\nstep~\\ref{proc:integration}, the usage is the same as in\nstep~\\ref{proc:calcinteraction}.\n\nIn step~\\ref{proc:init}, the FDPS function \\texttt{Initialize} is\ncalled. In this function, MPI and OpenMP libraries are initialized. If\nneither of them are used, this function does nothing.  All functions\nof FDPS must be called between this function and the\nfunction \\texttt{Finalize}.\n\nIn step~\\ref{proc:construct}, we create and initialize three objects\nof the FDPS classes:\n\\begin{itemize}\n\\item \\texttt{dinfo}: An object of class \\texttt{DomainInfo}. It is\n  used for domain decomposition.\n\\item \\texttt{ptcl}: An object of class template \\texttt{ParticleSystem}.\nIt takes the user-defined particle class (in this\nexample, \\texttt{Nbody}) as the template argument. From the user\nprogram, this object looks as an array of $i$-particles.\n\\item \\texttt{grav}: An object of a data type \\texttt{Monopole} defined in\na class template \\texttt{TreeForForceLong}. This object is used for\nthe calculation of long-range interaction using the tree algorithm.\nIt receives three user-defined classes template arguments: the class\nto store the calculated interaction, the class for $i$-particles and\nthe class for $j$-particles. In this example, all three are the same\nas the original class of particles.  It is possible to define classes\nwith minimal data for these purposes and use them here, in order to\noptimize the cache usage. The data type \\texttt{Monopole} indicates\nthat the center-of-mass approximation is used for superparticles.\n\\end{itemize}\n\nIn step~\\ref{proc:input}, the data of particles are read from a file\ninto the object \\texttt{ptcl}, using the FDPS\nfunction \\texttt{readParticleAscii}. In the function, a member\nfunction of class \\texttt{Nbody}, \\texttt{readAscii}, is called. Note\nthat the user can write and use his/her own I/O functions. In this case,\n\\texttt{readParticleAscii} is unnecessary.\n\nIn step~\\ref{proc:calcinteraction}, the forces on all particles are\ncalculated through the function \\texttt{calcGravAllAndWriteBack},\nwhich is defined in lines 79 to 89. In this function,\nsteps~\\ref{proc:decompose}, \\ref{proc:exchange}, and\n\\ref{proc:interaction} in section~\\ref{sec:view} are performed. In\nother words, all of the actual work of FDPS libraries to calculate\ninteraction between particles take place here. For\nstep~\\ref{proc:decompose}, \\texttt{decomposeDomainAll}, a member\nfunction of class \\texttt{DomainInfo} is called. This function takes\nthe object\n\\texttt{ptcl} as an argument to use the positions of particles to\ndetermine the domain decomposition.  Step~\\ref{proc:exchange} is\nperformed in \\texttt{exchangeParticle}, a member function of\nclass \\texttt{ParticleSystem}. This function takes the\nobject \\texttt{dinfo} as an argument and redistributes particles among\nMPI processes.  Step~\\ref{proc:interaction} is performed\nin \\texttt{calcForceAllAndWriteBack}, a member function of\nclass \\texttt{TreeForForceLong}. This function takes the user-defined\nfunction object \\texttt{CalcGrav} as the first and second arguments,\nand calculates particle-particle and particle-superparticle\ninteractions using them.\n\nIn step~\\ref{proc:fin}, the FDPS function \\texttt{Finalize} is\ncalled. It calls the \\texttt{MPI\\_finalize} function.\n\nIn this section, we have described in detail how a user program\nwritten using FDPS looks like. As we stated earlier, this program can\nbe compiled with or without parallelization using MPI and/or OpenMP,\nwithout any change in the user program. The executable parallelized\nwith MPI is generated by using an appropriate compiler with MPI\nsupport and a compile-time flag.  Thus, a user need not worry about\ncomplicated bookkeeping necessary for parallelization using MPI.\n\nIn the next section, we describe how FDPS provides a generic\nframework which takes care of parallelization\nand bookkeeping for particle-based simulations. \n\n\n\n\n\n\n\n\n\n\n\\section{Implementation}\n\\label{sec:implementation}\n\nIn this section, we describe how the operations discussed in the\nprevious section are implemented in FDPS. In\nsection~\\ref{sec:decomposition} we describe the domain decomposition\nand particle exchange, and in section~\\ref{sec:calculation}, the\ncalculation of interactions.\n\n\\subsection{Domain decomposition and particle exchange}\n\\label{sec:decomposition}\n\nIn this section, we describe how the domain decomposition and the\nexchange of particles are implemented in FDPS. We used the\nmultisection method\n\\citep{2004PASJ...56..521M} with the so-called sampling\nmethod \\citep{Blackston:1997:HPE:509593.509597}. The multisection\nmethod is a generalization of ORB (Orthogonal Recursive Bisection). In\nORB, as its name suggests, bisection is applied to each coordinate\naxis recursively. In multisection method, division in one coordinate\nis not to two domains but to an arbitrary number of domains. Since one\ndimension can be divided to more than two sections, it is not\nnecessary to apply divisions many times. So we apply divisions only\nonce to each coordinate axis. A practical advantage of this method is\nthat the number of processors is not limited to powers of two.\n\nFigure~\\ref{fig:decomposition} illustrates the result of the\nmultisection method with $(n_x, n_y, n_z)=(7,6,1)$. We can see that\nthe size and shape of subdomains shows large variation. By allowing\nthis variation, FDPS achieves quite good load balance and high\nscalability. Note that $n=n_x n_y n_z$ is the number of MPI\nprocesses. By default, values of $n_x$, $n_y$, and $n_z$ are chosen so\nthat they are integers close to $n^{1/3}$. For figure\n~\\ref{fig:decomposition}, we force the numbers used to make\ntwo-dimensional decomposition.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig3.eps}\n  \\end{center}\n  \\caption{Example of the domain decomposition. The division is $7\n    \\times 6$ in 2-dimension.}\n  \\label{fig:decomposition}\n\\end{figure}\n\n\nIn the sampling method, first each process performs random sampling of\nparticles under it, and sends them to the process with rank 0\n(``root'' process hereafter). Then the root process calculates the\ndivision so that sample particles are equally divided over all\nprocesses, and broadcasts the geometry of domains to all other\nprocesses. In order to achieve good load balance, sampling frequency\nshould be changed according to the calculation cost per particle\n\\citep{2009PASJ...61.1319I}.\n\nThe sampling method works fine, if the number of particles per process\nis significantly larger than the number of process. This is, however,\nnot the case for runs with a large number of nodes. For example, K\ncomputer has more than 80,000 nodes, and in this paper we report the\nresult of runs with all nodes. Even with the number of particles per\nprocess same as the number of nodes, the total number of particles\nbecomes 6.4 billion, and we do want to run simulations with smaller\nnumber of particles.\n\nWhen the number of particles per process is not much larger than the\nnumber of processes, the total number of sample particles which the\nroot process need to handle exceeds the number of particles per\nprocess itself, and thus calculation time of domain decomposition in\nthe root process becomes visible.\n\nIn order to reduce the calculation time, we also parallelize domain\ndecomposition, currently in the direction of x axis only. The basic\nidea here is that each node sends the sample particles not to the root\nprocess of the all MPI processes but to the processies with index\n$(i,0,0)$. Then prorcesses  $(i,0,0)$ sort the sample particles\nexchange the number of sample particles they\nreceived. Using these two pieces of information, each of $(i,0,0)$\nprocesses can determine all domain boundaries inside its current\ndomain in the x direction. Thus, they can determine which sample\nparticles should be sent to where. After exchange of the sample\nparticles, each of $(i,0,0)$ processes can determine the\ndecompositions in y and z directions.\n\nA naive implementation of the above algorithm requires ``global''\nsorting of sample particles over all of $(i,0,0)$ processes. In order\nto simplify this part, before each process sends the sample particles\nto  $(i,0,0)$ processes, they exchange their samples with other\nprocesses with the same location in y and z process coordinates, so\nthat they have sample particles in the current domain decomposition in\nthe x direction. As a result, particles sent to $(i,0,0)$ processes\nare already sorted at the level of domains decomposition in x\ndirection, and we need only the  sorting within each of $(i,0,0)$\nprocesses to obtain the globally sorted particles.\n\nThus, our implementation of parallelized domain decomposition\nis as follows:\n\n\\begin{enumerate}\n\\item\nEach process samples particles randomly from its own particles. In\norder to achieve the optimal load balance, the sampling rate of\nparticles is changed so that it is proportional to the CPU time per\nparticle spent on that process \\citep{2009PASJ...61.1319I}. FDPS\nprovides several options including this optimal\nbalance. \\label{prcoc:sampling}\n\n\n\\item\nEach process exchanges the sample particles according to the current\ndomain boundary in the x direction with other processes with the same\ny and z indexes, so that they have sample particles in the current\ndomain decomposition in the x direction.\n\\label{proc:commx}\n\n\n\\item\nEach process with index $(i,y,z)$ sends the sample particles to the\nprocess with index $(i,0,0)$, in other words, the root processes in\neach of y-z plane, collects subsamples.\n\\label{proc:gatherx}\n\n\n\\item\nEach process with index $(i,0,0)$ sorts the sample particles in the x\ndirection. Now, the sample particles are sorted globally in the x\ndirection.\n\n\n\\item\nEach process with index $(i,0,0)$ sends the number of the sample\nparticles to other processes with the same y and z indexes, and\ndetermines the global rank of the sample particles.\n\n\n\\item\nEach process with index $(i,0,0)$ determines the x coordinate of new\ndomains by dividing all sample particles into $n_x$ subsets with equal\nnumber of sample particles and sends them to other processes with the\nsame y and z coordinates.\n\\label{proc:determinex}\n\n\n\\item\nEach process with index $(i,0,0)$ exchanges sample particles with\nother processes with the same y and z indexes, so that they have the\nsample particles in new domain in the x direction.\n\n\n\\item\nEach process with index $(i,0,0)$ determines the y and z coordinates\nof new domains.\n\\label{proc:detyz}\n\n\n\\item\nEach process with index $(i,0,0)$ broadcasts the geometries of new\ndomains to all other processes.\n\\label{proc:broadcasting}\n\n\\end{enumerate}\n\nIt is also possible to parallelize to determine subdomains in\nstep \\ref{proc:detyz}, but even for the full-node runs on K computer\nwe found the current parallelization is sufficient.\n\nFor exchange of particles and also for the exchange of data necessary\nfor interaction calculation, we use {\\tt MPI\\_Alltoall} to exchange\nthe length of the data and {\\tt MPI\\_Alltoallv} to actually exchange\nthe data. At least on K computer, we found that the performance of\nvendor-provided {\\tt MPI\\_Alltoall(v)} is not optimal for short\nmessages. We implemented a hand-crafted version in which the messages\nsent to the same relay points are combined in order to reduce the\ntotal number of messages.\n\n\nAfter the domain decomposition is done and the result is broadcasted\nto all processes, they exchange particles so that each of them has\nparticles in its domain. Since each process has the complete\ninformation of the domain decomposition, this part is pretty\nstraightforward to implement. Each process looks at each of its\nparticles, and determines if that particle is still in its domain.  If\nnot, the process determines to which process that particle should be\nsent. After the destinations of all particles are determined, each\nprocess sends them out, using \\texttt{MPI\\_Alltoallv} function.\n\n\n\n\n\n\n\\subsection{Interaction calculation}\n\\label{sec:calculation}\n\nIn this section, we describe the implementation of the calculation of\ninteractions in FDPS. Conceptually, it consists of the following two\nsteps. In the first step, each process determines, for each of other\nprocesses, which of its particles and superparticles are required by\nthat process for interaction calculation, and sends them to it. In the\nsecond step, each process calculates the interactions onto\n$i$-particles by calling user-defined interaction fucntion.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig4.eps}\n  \\end{center}\n  \\caption{Illustration of communication among processes during the\n    interaction calculation.}\n  \\label{fig:exchangeLET}\n\\end{figure}\n\nFor both steps, the octree structure is used, both for long- and\nshort-range interactions.\n\nIn the first step, each process constructs the tree structure for its\nlocal particles, and uses it to determine what data should be sent to\nother processes. For the long-range interactions, this part is done\nthrough the usual tree traversal \\citep{1986Natur.324..446B,\n  1990JCoPh..87..161B}. For the short-range interactions, tree\ntraversal is also used. A cube in a tree need to be subdivided if it\nis within the cutoff length from anywhere in the domain of the process\nto which the data will be sent.  The current implementation of FDPS\ncan handle four different types of the cutoff length for the\n``short-range'' interaction: fixed, $j$-dependent, $i$-dependent and\nsymmetric.  For $i$-dependent and symmetric cutoffs, FDPS does the\ntree traversal twice.  Figure~\\ref{fig:exchangeLET} illustrates what\ndata are sent, for both long- and short-range interactions.\n\nAfter a process receives all data it requires, it reconstructs the\ntree structure which contains all information necessary to calculate\ninteractions on its particles.\n\nThe interaction calculation is performed using this new tree. The\nprocedure is the same as described in detail in the literature\n\\citep{1990JCoPh..87..161B, 1991PASJ...43..859M}, except for the\nfollowing two differences.  First, this part is fully multithreaded\nusing OpenMP, to achieve very good parallel performance. Second, for\nthe interaction calculation the user-provided functions are used, to\nachieve the flexibility and high performance at the same time.\n\n\n\n\n\n\n\\section{Performance of applications developed using FDPS}\n\\label{sec:performance}\n\nIn this section, we present the performance of three astrophysical\napplications developped using FDPS. One is the pure gravity code with\nopen boundary applied to disk galaxy simulation. The second one is\nagain pure gravity application but with periodic boundary applied to\ncosmological simulation. The third one is gravity + SPH calculation\napplied to the Giant impact simulation.  For the performance\nmeasurement, we used two systems. One is K computer of RIKEN AICS, and\nthe other is Cray XC30 of CfCA, National Astronomical Observatory of\nJapan. K computer consists of 82,944 Fujitsu SPARC64 VIIIfx\nprocessors, each with eight cores. The theoretical peak performance of\none core is 16 Gflops, for both of single- and double-precision\noperations. Cray XC30 of CfCA consists of 1060 nodes, or 2120 Intel\nXeon E5-2690v3 processors (12 cores, 2.6GHz). The theoretical peak\nperformance of one core is 83.2 and 41.6 Gflops for single- and\ndouble-precision operations, respectively.  In all runs on K computer,\nwe use the hybrid MPI-OpenMP mode of FDPS, in which one MPI process is\nassigned to one node. On the other hand, for XC30, we use the flat MPI\nmode of FDPS.  The source code is the same except for that for the\ninteraction calculation functions. The interaction calculation part\nwas written to take full advantage of the SIMD instruction set of the\ntarget architecture, and thus written specifically for K computer\n(HPC-ACE instruction set) and Xeon E5 v3 (AVX2 instruction set).\n\nIn section  \\ref{sec:measuredperformance}, we report the measured\nperformance, and in \\ref{sec:performancemodel} we present the\nperformance model.\n\n\\subsection{Measured Performance}\n\\label{sec:measuredperformance}\n\\subsubsection{Disk galaxy simulation}\n\\label{sec:diskgalaxy}\nIn this section, we discuss the performance and scalability of a\ngravitational $N$-body simulation code implemented using FDPS. The\ncode is essentially the same as the sample code described in\nsection~\\ref{sec:samplecode}, except for the following two differences\nin the user code for the calculation of the interaction. First, here,\nwe used the expansion up to the quadrupole moment, instead of the\nmonopole-only one used in the sample code, to improve the\naccuracy. Second, we used the highly optimized kernel developed using\nSIMD builtin functions, instead of the simple one in the sample code.\n\nWe apply this code for the simulation of the Milky Way-like galaxy,\nwhich consists of a bulge, a disk, and a dark matter halo. For\nexamples of recent large-scale simulations,\nsee \\citet{2011ApJ...730..109F, Bedorf:2014:PGT:2683593.2683600}.\n\nThe initial condition is the Milky Way model, the same as that\nin \\citet{Bedorf:2014:PGT:2683593.2683600}. The mass of the bulge is\n$4.6 \\times 10^9 M_\\odot$ (solar mass), and it has a\nspherically-symmetric density profile of the Hernquist\nmodel \\citep{1990ApJ...356..359H} with the half-mass radius of\n$0.5$~kpc. The disk is an axisymmetric exponential disk with the scale\nradius of $3$~kpc, the scale height of $200$~pc and the mass\n$5.0 \\times 10^{10}M_\\odot$. The dark halo has an Navarro-Frenk-White\n(NFW) density profile \\citep{1996ApJ...462..563N} with the half-mass\nradius of $40$~kpc and the mass of $6.0 \\times 10^{11} M_\\odot$. In\norder to realize the Milky Way model, we used\nGalacticICS \\citep{2005ApJ...631..838W}. For all simulations in this\nsection, we adopt $\\theta=0.4$ for the opening angle for the tree\nalgorithm and we set the average number of particles sampled for the\ndomain decomposition to 500.\n\nFigure~\\ref{fig:evolutiondisk} illustrates the time evolution of the\nbulge and disk in the run with $512$ nodes on the K computer. The disk\nis initially axisymmetric. We can see that spiral structure develops\n(0.5 and 1 Gyrs) and a central bar follows the spiral (1Gyrs and\nlater). As the bar grows, the two-arm structure becomes more visible\n(3Gyrs).\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig5.eps}\n  \\end{center}\n  \\caption{\n  Face-on surface density maps of the bulge and disk.\n  }\n  \\label{fig:evolutiondisk}\n\\end{figure}\n\nFigure~\\ref{fig:disk_weak} shows the measured weak-scaling\nperformance. We fixed the number of particles per core to 266,367 and\nmeasured the performance for the number of cores in the range of 4096\nto 663,552 on the K computer, and in the range of 32 to 2048 on\nXC30. We can see that the measured efficiency and scalability are both\nvery good. The efficiency is more than 50\\% for the entire range of\ncores on the K computer. The efficiency of XC30 is a bit worse than\nthat of the K computer. This difference comes from the difference of\ntwo processors. The Fujitsu processor showed higher efficiency. On the\nother hand, The Intel processor has 5.2times higher peak performance\nper core. We can see that the time for domain decomposition increase\nas we increase the number of cores. The slope is around 2/3 as can be\nexpected from our current algorithm discussed in\nsection \\ref{sec:decomposition}.\n\nFigure~\\ref{fig:disk_strong} shows the measured strong-scaling\nperformance. We fixed the total number of particles to $550$ million\nand measured the performance for 512 to 32768 cores on K computer and\n256 to 2048 cores on XC30. We can also see the measured efficiency and\nscalability are both very good, for the strong-scaling performance.\n\n\\citet{Bedorf:2014:PGT:2683593.2683600}\nreported the wallclock time of 4 seconds for their 27-billion particle\nsimulation on the Titan system with 2048 NVIDIA Tesla K20X, with the\ntheoretical peak performance of 8PF (single precision, since the\nsingle precision was used for the interaction calculation). This\ncorresponds to 0.8 billion particles per second per petaflops. Our\ncode on K computer requires 15 seconds on 16384 nodes (2PF theoretical\npeak), resulting in 1 billion particles per second per\npetaflops. Therefore, we can conclude that our FDPS code achieved the\nperformance slightly better than one of the best codes specialized to\ngravitational $N$-body problem.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig6.eps}\n  \\end{center}\n  \\caption{\n  \n    Weak-scaling performance of the gravitational $N$ body code. The\n    speed of the floating-point operation (top) and wallclock time per\n    one timestep (bottom) are plotted as functions of the number of\n    cores. Open and filled symbols indicate the performances of K\n    computer and cray XC30, respectively. In the top panel, the solid\n    line indicates 50 \\% of the theoretical peak performance of K\n    computer and the dotted line indicates 35 \\% of the theoretical\n    peak performance of XC30. In the bottom panel, time spent for the\n    interaction calculation (diamond), the domain decomposition\n    (square) the exchange particles (triangle) are also shown.\n    \n    } \\label{fig:disk_weak}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig7.eps}\n  \\end{center}\n  \\caption{\n  \nThe same figure as figure \\ref{fig:disk_weak} but for the\nstrong-scaling performance for 550 million particles\n\n    }\n  \\label{fig:disk_strong}\n\\end{figure}\n\n\n\n\n\n\n\n\\subsubsection{Cosmological simulation}\n\n\nIn this section, we discuss the performance of a cosmological\nsimulation code implemented using FDPS. We implemented TreePM (Tree\nParticle-Mesh) method and measured the performance on XC30. Our TreePM\ncode is based on the code developed by K. Yoshikawa. The Particle-Mesh\npart of the code was developed by\n\\citet{Ishiyama:2012:PAN:2388996.2389003} and this code is included in\nthe FDPS package as an external module.\n\nWe initially place particles uniformly in a cube and gave them zero\nvelocity. For the calculation of the tree force , we used a monopole\nonly kernel with cutoff. The cutoff length of the force is three times\nlarger than the width of the mesh. We set $\\theta$ to 0.5. For the\ncalculation of the mesh force, the mass density is assigned to each of\nthe grid points, using the triangular shaped cloud scheme and the\ndensity profile we used is the S2 profile \\citep{hockney1988computer}.\n\nFigures \\ref{fig:cosmo_weak} and \\ref{fig:cosmo_strong} show the weak\nand strong scaling performance, respectively. For the weak-scaling\nmeasurement, we fixed the number of particles per process to 5.73\nmillion and measured the performance for the number of cores in the\nrange of 192 to 12000 on XC30. For the strong-scaling measurements, we\nfixed the total number of particles to $2048^3$ and measured the\nperformance for the number of cores in the range of 1536 to 12000 on\nXC30. We can see that the time for the calculation of the tree force\nis dominant and both of the weak and strong scalings are good except\nfor the very large number of cores (12000) for the strong scaling\nmeasurement. One reason is that the scalability of the calculation of\nthe mesh force is not very good. Another reason is that the time for\nthe domain decomposition grows linearly for large number of cores,\nbecause we did not use parallelized domain decomposition, here. The\nefficiency is 7 \\% of the theoretical peak performance. It is rather\nlow compared to that for the disk galaxy simulations in\nsection \\ref{sec:diskgalaxy}. The main reason is that we use a lookup\ntable for the force calculation. If we evaluate the force without the\nlookup table, the nominal efficiency would be much better, but the\ntotal time would be longer.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig8.eps}\n  \\end{center}\n  \\caption{\n\n    Weak-scaling performance of the TreePM code. The speed of the\n    floating-point operation (top) and wallclock time per one timestep\n    (bottom) are plotted as functions of the number of cores. In the\n    top panel, the solid line indicates 7 \\% of the theoretical peak\n    performance of XC30. In the bottom panel, time spent for the\n    Particle-Particle interaction calculation (diamond), the\n    Particle-Mesh interaction (pentagon), the domain decomposition\n    (square) and the exchange particles (triangle) are also shown.\n    \n  }\n  \\label{fig:cosmo_weak}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig9.eps}\n  \\end{center}\n  \\caption{\n\n    The same as figure \\ref{fig:cosmo_weak} but for the strong-scaling\n    performance. In this case, the number of particles is $2048^3$.\n\n  }\n  \\label{fig:cosmo_strong}\n\\end{figure}\n\n\n\n\\subsubsection{Giant impact simulation}\n\n\\label{sec:sph}\n\nIn this section, we discuss the performance of an SPH simulation code\nwith self-gravity implemented using FDPS. The test problem used is the\nsimulation of Giant Impact (GI). The giant impact\nhypothesis \\citep{1975Icar...24..504H, 1976LPI.....7..120C} is one of\nthe most popular scenarios for the formation of the Moon. The\nhypothesis is as follows. About 5 billion years ago, a Mars-sized\nobject (hereafter, the impactor) collided with the proto-Earth\n(hereafter, the target). A large amount of debris was scattered, which\nfirst formed the debris disk and eventually the Moon. Many researchers\nhave performed simulations of GI, using the SPH method\n\\citep{1986Icar...66..515B, 2013Icar..222..200C, 2014NatGe...7..564A}.\n\nFor the gravity, we used monopole-only kernel with $\\theta=0.5$. We\nadopt the standard SPH scheme\n\\citep{1992ARA&A..30..543M, 2009NewAR..53...78R, 2010ARA&A..48..391S}\nfor the hydro part. Artificial viscosity is used to handle shocks\n\\citep{1997JCoPh.136..298M}, and \nthe standard Balsara switch is used to reduce the shear viscosity\n\\citep{1995JCoPh.121..357B}. In all simulations, we set the cutoff radius\nto be 4.2 times larger than the local mean inter-particle distance. In\nother words, each particle interact with about 300 particles.\n\nAssuming that the target and impactor consist of granite, we adopt\nequation of state of granite \\citep{1986Icar...66..515B} for the\nparticles. The initial conditions, such as the orbital parameters of\nthe two objects, are the same as those\nin \\citet{1986Icar...66..515B}.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig10.eps}\n  \\end{center}\n  \\caption{Temperature maps of the target and impactor in the run with\n  $9.9$ million particles at four different epochs. }\n  \\label{fig:evolutionGI}\n\\end{figure}\n\nFigure~\\ref{fig:evolutionGI} shows the time evolution of the target\nand impactor for a run with 9.9 million particles. We can see that the\nshocks are formed just after the moment of impact in both the target\nand impactor (t=2050sec). The shock propagates in the target, while\nthe impactor is completely disrupted (t=2847sec) and debris is\nejected. A part of the debris falls back to the target, while the rest\nwill eventually form the disk and the Moon. So far, the resolution\nused in the published papers have been much lower. We plan to use this\ncode to improve the accuracy of the GI simulations.\n\nFigures \\ref{fig:gi_weak} and \\ref{fig:gi_strong} show the measured\nweak and strong scaling performance. For the weak-scaling measurement,\nwe fixed the number of particles per core to 20,000 and measured the\nperformance for the number of cores in the range of 256 to 131072 on\nthe K computer. On the other hand, for the strong-scaling measurement,\nwe fixed the total number of particles to $39$ million and measured\nthe performance for the number of cores in the range of 512 to 16384\non K computer. We can see that the performance is good even for very\nlarge number of cores. The efficiency is about 40 \\% of the\ntheoretical peak performance. The hydro part consumes more time than\nthe gravity part does, mainly because the particle-particle\ninteraction is more complicated.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig11.eps}\n  \\end{center}\n  \\caption{\n\n    Weak-scaling performance of the SPH code. The speed of the\n    floating-point operation (top) and wallclock time per one timestep\n    (bottom) are plotted as functions of the number of cores. In the\n    top panel, the solid line indicates 40 \\% of the theoretical peak\n    performance of K computer. In the bottom panel, time spent for the\n    hydrodynamics calculation (cross), the gravity calculation\n    (diamond), the domain decomposition (square) the exchange particles\n    (triangle) are also shown.\n\n}\n  \\label{fig:gi_weak}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig12.eps}\n  \\end{center}\n  \\caption{\n\nThe same as figure \\ref{fig:gi_weak} but for the strong-scaling\nperformance for $39$ million particles.\n\n}\n  \\label{fig:gi_strong}\n\\end{figure}\n\n\n\n\n\n\n\n\\subsection{Performance model}\n\\label{sec:performancemodel}\n\nHere, we present the performance model of applications implemented\nusing FDPS. As described  in section \\ref{sec:user}, the calculation\nof a typical application written using FDPS proceeds in the following\nsteps\n\n\\begin{enumerate}\n  \\item Update the domain decomposition and exchange particles\n    accordingly (not in every timestep).\n  \\item  Construct the local tree structure and exchange particles and\n    superparticles necessary for interaction calculation\n  \\item Construct the ``global'' tree\n  \\item Perform the interaction calculation\n  \\item Update the physical quantities of particles using the\n    calculated interactions\n\\end{enumerate}\n\nIn the case of complex applications which require more than one\ninteraction calculations, each of the above steps, except for the\ndomain decomposition, may be executed more than one time per one\ntimestep.\n\nFor a simple application, thus, the total wallclock time per one timestep should\nbe expressed as\n\n", "index": 1, "text": "\\begin{equation}\n  \\label{eq:totalcost}\n  T_{\\rm step} =  T_{\\rm dc}/n_{\\rm dc}\n               + T_{\\rm lt}\n               + T_{\\rm exch}\n               + T_{\\rm icalc}\n               + T_{\\rm misc},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"T_{\\rm step}=T_{\\rm dc}/n_{\\rm dc}+T_{\\rm lt}+T_{\\rm exch}+T_{\\rm icalc}+T_{%&#10;\\rm misc},\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>step</mi></msub><mo>=</mo><mrow><mrow><msub><mi>T</mi><mi>dc</mi></msub><mo>/</mo><msub><mi>n</mi><mi>dc</mi></msub></mrow><mo>+</mo><msub><mi>T</mi><mi>lt</mi></msub><mo>+</mo><msub><mi>T</mi><mi>exch</mi></msub><mo>+</mo><msub><mi>T</mi><mi>icalc</mi></msub><mo>+</mo><msub><mi>T</mi><mi>misc</mi></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nwhere $T_{\\rm p2p,startup}$ is the startup time which is independent\nof the message length and $T_{\\rm p2p,word}$ is the time to transfer\none byte of message. Here, $n_{\\rm word}$ is the length of the message\nin units of bytes. On K computer, $T_{\\rm p2p,startup}$ is 0.0101 ms\nand $T_{\\rm p2p,word}$ is $2.11 \\times 10^{-7}$ ms. For short message,\nthere is a rather big discrepancy between the fitting curve and\nmeasured points, because for short messages K computer used several\ndifferent algorithms.\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig13.eps}\n  \\end{center}\n  \\caption{\n  \n  Elapsed time for point-to-point communication as a function of size\n  of message measured on K computer.\n\n}\n  \\label{fig:pingpong}\n\\end{figure}\n\n\n\nFigure \\ref{fig:group_comm_a2a} shows the elapsed times for {\\tt\nMPI\\_Alltoallv}. The number of process $n_p$ is 32 to 2048. They are\nagain fitted by the simple form\n\n", "itemtype": "equation", "pos": 60593, "prevtext": "\nwhere   $T_{\\rm dc}$, $T_{\\rm lt}$, $T_{\\rm exch}$,\n$T_{\\rm icalc}$, and $T_{\\rm misc}$ are the times for\ndomain composition and particle exchange, local tree construction,\nexchange of particles and superparticles for interaction calculation,\ninteraction calculation, and other calculations such as particle\nupdate, respectively. The term $n_{\\rm dc}$ is the interval at which\nthe domain decomposition is performed.\n\nIn the following, we first construct the model for the communication\ntime. Then we construct models for each terms of \nthe right hand side of Eq.~\\ref{eq:totalcost}, and finally we compare the\nmodel with the actual measurement presented in section\n\\ref{sec:total_time}.\n\n\\subsubsection{Communication model}\n\\label{sec:comm_model}\n\nSince what ultimately determines the efficiency of a calculation\nperformed on large-scale parallel machine is the communication\noverhead, it is very important to understand what types of\ncommunication would take what amount of time on actual hardware. In\nthis section, we summarize the characteristics of the communication\nperformance of K computer.\n\nIn FDPS, almost all communications are through the use of collective\ncommunications, such as {\\tt MPI\\_Allreduce}, {\\tt MPI\\_Alltoall}, and\n{\\tt MPI\\_Alltoallv}. However, to measure the performance of these\nroutines for uniform message length is not enough, since the amount of\ndata to be transferred between processes generally depends on the\nphysical distance between domains assigned to those\nprocesses. Therefore, we first present the timing results for simple\npoint-to-point communication, and then for collective communications.\n\nFigure \\ref{fig:pingpong} shows the elapsed time as the function of\nthe message length, for point-to-point communication between\n``neighboring'' processes. In the case of K computer, we used\nthree-dimensional node allocation, so that ``neighboring'' processes\nare actually close to each other in its torus network.\n\nWe can see that the elapsed time can be fitted reasonably well as\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:Tp2p}\n  T_{\\rm p2p} = T_{\\rm p2p,startup} + n_{\\rm word}T_{\\rm p2p,word},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"T_{\\rm p2p}=T_{\\rm p2p,startup}+n_{\\rm word}T_{\\rm p2p,word},\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>p2p</mi></msub><mo>=</mo><mrow><msub><mi>T</mi><mrow><mi>p2p</mi><mo>,</mo><mi>startup</mi></mrow></msub><mo>+</mo><mrow><msub><mi>n</mi><mi>word</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mrow><mi>p2p</mi><mo>,</mo><mi>word</mi></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nwhere $T_{alltoallv,\\rm startup}$ is the startup time and\n$T_{alltoallv,\\rm word}$ is the time to transfer one byte of message.\nWe list these values in table \\ref{table:alltoallv}.\n\n\\begin{table}\n\\caption{Time coefficients in equation (\\ref{eq:wtime_group_comm})}\n\\begin{tabular}{|l|l|l|l|} \\hline\n                  & $n_p=32$ & $n_p=256$ & $n_p=2048$ \\\\ \\hline\n$T_{\\rm alltoallv,startup}$ [ms] & 0.103 & 0.460 & 2.87 \\\\\n$T_{\\rm alltoallv,word}$ [ms/byte] & $8.25\\times 10^{-6}$  & $9.13\\times 10^{-5}$ & $1.32\\times 10^{-3}$  \\\\ \\hline\n\\end{tabular}\n\\label{table:alltoallv}\n\\end{table}\n\nThe coefficients themselves in equation (\\ref{eq:wtime_group_comm})\ndepend on the number of MPI processes $n_p$, as shown in\nfigures \\ref{fig:group_comm_a2a_2}. They are modeled as\n\\begin{eqnarray}\n\\label{eq:alltoallv}\n  T_{\\rm alltoallv,startup} &=& \\tau_{\\rm alltoallv,startup} n_p \\\\\n  T_{\\rm alltoallv,word} &=& \\tau_{\\rm alltoallv,word} n_p^{4/3}.\n\\end{eqnarray}\nHere we assume that the speed to transfer message using {\\tt\nMPI\\_Alltoallv} is limited to the bisection bandwidth of the system.\nUnder this assumption, $T_{\\rm alltoallv,word}$ should be proportional\nto $n_p^{4/3}$.  To estimate $\\tau_{\\rm alltoallv,startup}$ and\n$\\tau_{\\rm alltoallv,word}$, we use measurements for message sizes of\n8 byte and 32k byte. In K computer, we found that $\\tau_{\\rm\nalltoallv,startup}$ is $0.00166$ ms and $\\tau_{\\rm alltoallv,word}$ is\n$1.11 \\times 10^{-7}$ ms per byte. If {\\tt MPI\\_Alltoallv} is limited\nto the bisection bandwidth in K computer, $\\tau_{\\rm alltoallv,word}$\nwould be $5 \\times 10^{-8}$ ms per byte. We can see that the actual\nperformance of {\\tt MPI\\_Alltoallv} on K computer is quite good.\n\n\n\\begin{figure}\n  \\begin{center} \\includegraphics[width=8cm]{fig14.eps}\n  \\end{center}\n  \\caption{\n  \n    Elapsed time of {\\tt MPI\\_Alltoallv} as a function of message size\n    measured on K computer.\n\n}\n\\label{fig:group_comm_a2a}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n  \\includegraphics[width=8cm]{fig15.eps}\n  \\end{center}\n  \\caption{\n  \n    Elapsed time of {\\tt MPI\\_Alltoallv} to send messeage of 8 byte\n    (circles) and 32k byte (squares) as a function of the number of\n    processes measured on K computer. Solid and dashed curves indicate\n    the results for the message size of 8 byte and 32k byte,\n    respectively.\n    \n} \\label{fig:group_comm_a2a_2}\n\\end{figure}\n\n\\subsubsection{Domain decomposition}\n\nFor the hierarchical domain decomposition method described in\nsection \\ref{sec:decomposition}, the calculation time is expressed as\n\n\n", "itemtype": "equation", "pos": 61634, "prevtext": "\nwhere $T_{\\rm p2p,startup}$ is the startup time which is independent\nof the message length and $T_{\\rm p2p,word}$ is the time to transfer\none byte of message. Here, $n_{\\rm word}$ is the length of the message\nin units of bytes. On K computer, $T_{\\rm p2p,startup}$ is 0.0101 ms\nand $T_{\\rm p2p,word}$ is $2.11 \\times 10^{-7}$ ms. For short message,\nthere is a rather big discrepancy between the fitting curve and\nmeasured points, because for short messages K computer used several\ndifferent algorithms.\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig13.eps}\n  \\end{center}\n  \\caption{\n  \n  Elapsed time for point-to-point communication as a function of size\n  of message measured on K computer.\n\n}\n  \\label{fig:pingpong}\n\\end{figure}\n\n\n\nFigure \\ref{fig:group_comm_a2a} shows the elapsed times for {\\tt\nMPI\\_Alltoallv}. The number of process $n_p$ is 32 to 2048. They are\nagain fitted by the simple form\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:wtime_group_comm}\n  T_{alltoallv} = T_{alltoallv,\\rm startup} + n_{\\rm word}T_{alltoallv,\\rm word}, \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"T_{alltoallv}=T_{alltoallv,\\rm startup}+n_{\\rm word}T_{alltoallv,\\rm word},\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mrow><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><msub><mi>T</mi><mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>,</mo><mi>startup</mi></mrow></msub><mo>+</mo><mrow><msub><mi>n</mi><mi>word</mi></msub><mo>\u2062</mo><msub><mi>T</mi><mrow><mrow><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>o</mi><mo>\u2062</mo><mi>a</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>l</mi><mo>\u2062</mo><mi>v</mi></mrow><mo>,</mo><mi>word</mi></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nwhere $T_{\\rm dc,gather}$ is the time for the $(i,0,0)$ process to\ncollect sample particles, $T_{\\rm dc,sort}$ is the time to sort sample\nparticles on the $(i,0,0)$ process, $T_{\\rm dc,exch}$ is the time to\nexchange particles after the new domains are determined, and $T_{\\rm\ndc,misc}$ is the time for remaining procedures such as initial\nexchange of samples in x direction, exchange of sample particles and\ndomain boundaries in x direction, and broadcasting of the domain\nboundaries in y-z planes.\n\nOn machines we so far tested, $T_{\\rm dc,gather}$ and $T_{\\rm\n  dc,misc}$ are much smaller than $T_{\\rm dc,sort}$ and $T_{\\rm\n  dc,exch}$. Therefore we consider these two terms only.\n\nFirst, we consider the time to sort sample particles. Since we use the\nquick sort, the term $T_{\\rm dc,sort}$ is expressed as\n\\begin{eqnarray}\nT_{\\rm dc,sort} &=& \\tau_{\\rm qsort} \\left[ 2n_{\\rm smp} n_y n_z {\\rm log} (n_{\\rm smp} n_y n_z) + n_y n_{\\rm smp} n_z {\\rm log} (n_{\\rm smp} n_z) \\right] \\\\ \n             &\\sim& \\tau_{\\rm dc,qsort} n_{\\rm smp}n_p^{2/3},\n\\end{eqnarray}\nwhere $n_{\\rm smp}$ is the average number of sample particles per\nprocess, and $n_x$, $n_y$ and $n_z$ are the numbers of processes in x,\ny and z direction. Here, $\\tau_{\\rm dc,sort} \\sim {\\rm log}(n_{\\rm\nsmp}^3n_p^{5/3})\\tau_{\\rm qsort}$. The first term expresses that time\nto sort samples in y-z planes with respect to x and y direction. The\nsecond term expresses that time to sort samples respect to z\ndirection.\n\nIn order to model $T_{\\rm dc,exch}$, we need to model the number of\nparticles which move from one domain to another. This number would\ndepend on various factors, in particular the nature of the system we\nconsider. For example, if we are calculating the early phase of the\ncosmological structure formation, particles do not move much in a\nsingle timestep, and thus the number of particles moved between\ndomains is small. On the other hand, if we are calculating single\nvirialized self-gravitating system, particles do move a relatively\nlarge distances (comparable to average interparticle distance) in a\nsingle timestep. In this case, if one process contain $n$ particles,\nhalf of particles in the ``surface'' of the domain might migrate in\nand out the domain. Thus, $O(n^{2/3})$ particles could be exchanged in\nthis case.\n\nFigures \\ref{fig:domain_decomposition_sort}, \\ref{fig:domain_decomposition_exch}\nand \\ref{fig:domain_decomposition} show the elapsed time for sorting\nsamples, exchanging samples, and domain decomposition, for the case of\ndisk galaxy simulations, in the case of $n_{\\rm smp}=500$ and $n \\sim\n5.3 \\times 10^5$. We also plot the fitting curves modeled as\n\\begin{eqnarray}\n\\label{eq:ddfit}\n  T_{\\rm dc} &\\sim& T_{\\rm dc,sort} + T_{\\rm dc,exch} \\\\\n             &=& \\tau_{\\rm dc,sort} n_{\\rm smp} n_p^{2/3}\n                 + \\tau_{\\rm dc,exch} \\sigma \\Delta t / \\left<r\\right> n^{2/3} b_p,\n\\end{eqnarray}\nwhere $\\tau_{\\rm dc,sort}$ and $\\tau_{\\rm dc,exch}$ are the execution\ntime for sorting one particle and for exchanging one particle\nrespectively, $\\sigma$ is the typical velocity of particles, $\\Delta\nt$ is the timestep and $\\left<r\\right>$ is the average interparticle\ndistance. For simplicity we ignore weak log term in $T_{\\rm dc,sort}$\n. On K computer, $\\tau_{\\rm dc,sort} = 2.67\\times 10^{-7}$ second and\n$\\tau_{\\rm dc,exch} = 1.42\\times 10^{-7}$ second per byte. Note that\n$\\tau_{\\rm dc,exch} \\sim 672 T_{\\rm p2p,word}$.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig16.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc,sort}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition_sort}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig17.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc,exch}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition_exch}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig18.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition}\n\\end{figure}\n\nThe analysis above, however, indicates that $T_{\\rm dc,exch}$ is, even\nwhen it is relatively large, still much smaller than $T_{\\rm exch}$,\nthe time to exchange particles and superparticles for interaction\ncalculation (see section \\ref{sec:exchnage_list}).\n\n\\subsubsection{Tree construction}\n\nTheoretically, the cost of tree construction is $O(n{\\rm log}n)$, and\nof the same order as the interaction calculations itself. However, in\nour current implementation, the interaction calculation is much more\nexpensive, independent of target architecture and the type of the\ninteraction. Thus we ignore the time for the tree constructions.\n\n\n\\subsubsection{Exchange of particles and superparticles}\n\\label{sec:exchnage_list}\n\nFor the exchange of particles and superparticles, in the current\nimplementation of FDPS, first each node constructs the list of\nparticles and superparticles (hereafter the exchange list) to be sent\nto all other nodes, and then data are exchanged through a single call\nto {\\tt MPI\\_Alltoallv}. The way the exchange list is constructed\ndepends on the force calculation mode. In the case of long-range\nforces, usual tree traversal with a fixed opening angle $\\theta$ is\nperformed. For the short-range forces, the procedure used depends on\nthe subtypes of the interaction. In the case of fixed or $j$-dependent\ncutoff, the exchange list for a node can be constructed by a single\ntraversal of the local tree. On the other hand, for $i$-dependent or\nsymmetric cutoff, first each node constructs the $j$-dependent\nexchange lists and sends them to all other nodes. Each nodes then\nconstructs the $i$-dependent exchange lists and sends then again.\n\nThe time for the construction and exchange of exchange list is thus\ngiven by\n\n", "itemtype": "equation", "pos": 64329, "prevtext": "\nwhere $T_{alltoallv,\\rm startup}$ is the startup time and\n$T_{alltoallv,\\rm word}$ is the time to transfer one byte of message.\nWe list these values in table \\ref{table:alltoallv}.\n\n\\begin{table}\n\\caption{Time coefficients in equation (\\ref{eq:wtime_group_comm})}\n\\begin{tabular}{|l|l|l|l|} \\hline\n                  & $n_p=32$ & $n_p=256$ & $n_p=2048$ \\\\ \\hline\n$T_{\\rm alltoallv,startup}$ [ms] & 0.103 & 0.460 & 2.87 \\\\\n$T_{\\rm alltoallv,word}$ [ms/byte] & $8.25\\times 10^{-6}$  & $9.13\\times 10^{-5}$ & $1.32\\times 10^{-3}$  \\\\ \\hline\n\\end{tabular}\n\\label{table:alltoallv}\n\\end{table}\n\nThe coefficients themselves in equation (\\ref{eq:wtime_group_comm})\ndepend on the number of MPI processes $n_p$, as shown in\nfigures \\ref{fig:group_comm_a2a_2}. They are modeled as\n\\begin{eqnarray}\n\\label{eq:alltoallv}\n  T_{\\rm alltoallv,startup} &=& \\tau_{\\rm alltoallv,startup} n_p \\\\\n  T_{\\rm alltoallv,word} &=& \\tau_{\\rm alltoallv,word} n_p^{4/3}.\n\\end{eqnarray}\nHere we assume that the speed to transfer message using {\\tt\nMPI\\_Alltoallv} is limited to the bisection bandwidth of the system.\nUnder this assumption, $T_{\\rm alltoallv,word}$ should be proportional\nto $n_p^{4/3}$.  To estimate $\\tau_{\\rm alltoallv,startup}$ and\n$\\tau_{\\rm alltoallv,word}$, we use measurements for message sizes of\n8 byte and 32k byte. In K computer, we found that $\\tau_{\\rm\nalltoallv,startup}$ is $0.00166$ ms and $\\tau_{\\rm alltoallv,word}$ is\n$1.11 \\times 10^{-7}$ ms per byte. If {\\tt MPI\\_Alltoallv} is limited\nto the bisection bandwidth in K computer, $\\tau_{\\rm alltoallv,word}$\nwould be $5 \\times 10^{-8}$ ms per byte. We can see that the actual\nperformance of {\\tt MPI\\_Alltoallv} on K computer is quite good.\n\n\n\\begin{figure}\n  \\begin{center} \\includegraphics[width=8cm]{fig14.eps}\n  \\end{center}\n  \\caption{\n  \n    Elapsed time of {\\tt MPI\\_Alltoallv} as a function of message size\n    measured on K computer.\n\n}\n\\label{fig:group_comm_a2a}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n  \\includegraphics[width=8cm]{fig15.eps}\n  \\end{center}\n  \\caption{\n  \n    Elapsed time of {\\tt MPI\\_Alltoallv} to send messeage of 8 byte\n    (circles) and 32k byte (squares) as a function of the number of\n    processes measured on K computer. Solid and dashed curves indicate\n    the results for the message size of 8 byte and 32k byte,\n    respectively.\n    \n} \\label{fig:group_comm_a2a_2}\n\\end{figure}\n\n\\subsubsection{Domain decomposition}\n\nFor the hierarchical domain decomposition method described in\nsection \\ref{sec:decomposition}, the calculation time is expressed as\n\n\n", "index": 7, "text": "\\begin{equation}\n  \\label{eq:dccost}\n  T_{\\rm dc} = T_{\\rm dc,gather}\n            +   T_{\\rm dc,sort}\n            +   T_{\\rm dc,exch}\n            +   T_{\\rm dc,misc},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"T_{\\rm dc}=T_{\\rm dc,gather}+T_{\\rm dc,sort}+T_{\\rm dc,exch}+T_{\\rm dc,misc},\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>dc</mi></msub><mo>=</mo><mrow><msub><mi>T</mi><mrow><mi>dc</mi><mo>,</mo><mi>gather</mi></mrow></msub><mo>+</mo><msub><mi>T</mi><mrow><mi>dc</mi><mo>,</mo><mi>sort</mi></mrow></msub><mo>+</mo><msub><mi>T</mi><mrow><mi>dc</mi><mo>,</mo><mi>exch</mi></mrow></msub><mo>+</mo><msub><mi>T</mi><mrow><mi>dc</mi><mo>,</mo><mi>misc</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\n\nHere, $k_{\\rm type}$ is an coefficient which is unity for fixed and\n$j$-dependent cutoffs and two for other cutoffs. Strictly speaking,\nthe communication cost does not double for $i$-dependent or symmetric\ncutoffs, since we send only particles which was not sent in the first\nstep. However, for simplicity we use $k=2$ for both calculation and\ncommunication.\n\nThe two terms in equation (\\ref{eq:exchangecost}) are then\napproximated as\n\\begin{eqnarray}\n  T_{\\rm exch,const} &=& \\tau_{\\rm exch,const} n_{\\rm exch,list}, \\label{eq:exchange} \\\\\n  T_{\\rm exch,comm}(n_{\\rm msg}) &=&  T_{\\rm alltoallv}\\left( n_{\\rm exch,list}/n_{p} b_p \\right), \\label{eq:exchange_comm}\n\\end{eqnarray}\n\nwhere $n_{\\rm exch,list}$ is the average length of the exchange list\nand $\\tau_{\\rm exch,const}$ is the execution time for constructing one\nexchange list.  Figures \\ref{fig:exchangeLET_nlist-wtime_const}\nand \\ref{fig:exchangeLET_nlist-wtime_exch} show the execution time for\nconstructing and exchanging the exchange list against the average\nlength of the list. Here, $b_p$=48 bytes for both short and long-range\ninteractions. From figure \\ref{fig:exchangeLET_nlist-wtime_const}, we\ncan see that the elapsed time can be fitted well by equation\n(\\ref{eq:exchange}).  Here $\\tau_{\\rm exch,const}$ is $1.12 \\times\n10^{-7}$ second for long-range interaction and $2.31 \\times 10^{-7}$\nsecond for short-range interaction.\n\nFrom figure \\ref{fig:exchangeLET_nlist-wtime_exch}, we can see a large\ndiscrepancy between measured points and the curves predicted from\nequation (\\ref{eq:exchange_comm}). In the measurement of the\nperformance of ${\\tt MPI\\_Alltoallv}$ in section \\ref{sec:comm_model},\nwe used uniform message length across all processes. In actual use in\nexchange particles, the length of the message is not\nuniform. Neighboring processes generally exchange large messages,\nwhile distant processes exchange short message. For such cases,\ntheoretically, communication speed measured in terms of average\nmessage length should be faster. In practice, however, we observed a\nserious degradation of performance. This degradation seems to imply\nthat the current implementation of ${\\tt MPI\\_Alltoallv}$ is\nsuboptimal for non-uniform message size.\n\n\\begin{figure}\n  \\begin{center} \\includegraphics[width=8cm]{fig19.eps} \\end{center} \\caption{\n    Time for the construction of the exchange list plotted against the\n    average length of the list, for the case of $n_p=2048$ and $n \\sim\n    2.7 \\times 10^5, 5.3 \\times 10^5, 1.1 \\times 10^6, 2.1 \\times\n    10^6$. Circles and squares indicate the results for long-range and\n    short-range force, respectively. Solid and dashed curves are model\n    fittings of equation (\\ref{eq:exchange}).\n    } \\label{fig:exchangeLET_nlist-wtime_const}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig20.eps}\n  \\end{center}\n  \\caption{\n\nTime for the communication of the exchange list against the average\nlength of the list per process, for the case of $n_p=2048$ and $n \\sim\n2.7 \\times 10^5, 5.3 \\times 10^5, 1.1 \\times 10^6, 2.1 \\times\n10^6$. Circles and squares indicate the results for long-range and\nshort-range force, respectively. The curve is prediction from\nequation (\\ref{eq:exchange}).\n\n    }\n  \\label{fig:exchangeLET_nlist-wtime_exch}\n\\end{figure}\n\nIn the following, we estimate $n_{\\rm exch,list}$. If we consider a\nrather idealized case, in which all domains are cubes containing $n$\nparticles, the total length of the exchange lists for one domain can\napproximately be given by\n\\begin{eqnarray}\n  n_{\\rm exch,list} &\\sim& \\frac{14n^{2/3}}{\\theta} + \\frac{21\\pi n^{1/3}}{\\theta^2} \\nonumber \\\\\n  &&+ \\frac{28\\pi}{3\\theta^3} {\\rm log_2} \\left\\{ \\frac{\\theta}{2.8}\\left[\\left(nn_p\\right)^{1/3} - n^{1/3}\\right] \\right\\}, \\label{eq:n_list_long}\n\\end{eqnarray}\nfor the case of long-range interactions and\n\n", "itemtype": "equation", "pos": 70564, "prevtext": "\nwhere $T_{\\rm dc,gather}$ is the time for the $(i,0,0)$ process to\ncollect sample particles, $T_{\\rm dc,sort}$ is the time to sort sample\nparticles on the $(i,0,0)$ process, $T_{\\rm dc,exch}$ is the time to\nexchange particles after the new domains are determined, and $T_{\\rm\ndc,misc}$ is the time for remaining procedures such as initial\nexchange of samples in x direction, exchange of sample particles and\ndomain boundaries in x direction, and broadcasting of the domain\nboundaries in y-z planes.\n\nOn machines we so far tested, $T_{\\rm dc,gather}$ and $T_{\\rm\n  dc,misc}$ are much smaller than $T_{\\rm dc,sort}$ and $T_{\\rm\n  dc,exch}$. Therefore we consider these two terms only.\n\nFirst, we consider the time to sort sample particles. Since we use the\nquick sort, the term $T_{\\rm dc,sort}$ is expressed as\n\\begin{eqnarray}\nT_{\\rm dc,sort} &=& \\tau_{\\rm qsort} \\left[ 2n_{\\rm smp} n_y n_z {\\rm log} (n_{\\rm smp} n_y n_z) + n_y n_{\\rm smp} n_z {\\rm log} (n_{\\rm smp} n_z) \\right] \\\\ \n             &\\sim& \\tau_{\\rm dc,qsort} n_{\\rm smp}n_p^{2/3},\n\\end{eqnarray}\nwhere $n_{\\rm smp}$ is the average number of sample particles per\nprocess, and $n_x$, $n_y$ and $n_z$ are the numbers of processes in x,\ny and z direction. Here, $\\tau_{\\rm dc,sort} \\sim {\\rm log}(n_{\\rm\nsmp}^3n_p^{5/3})\\tau_{\\rm qsort}$. The first term expresses that time\nto sort samples in y-z planes with respect to x and y direction. The\nsecond term expresses that time to sort samples respect to z\ndirection.\n\nIn order to model $T_{\\rm dc,exch}$, we need to model the number of\nparticles which move from one domain to another. This number would\ndepend on various factors, in particular the nature of the system we\nconsider. For example, if we are calculating the early phase of the\ncosmological structure formation, particles do not move much in a\nsingle timestep, and thus the number of particles moved between\ndomains is small. On the other hand, if we are calculating single\nvirialized self-gravitating system, particles do move a relatively\nlarge distances (comparable to average interparticle distance) in a\nsingle timestep. In this case, if one process contain $n$ particles,\nhalf of particles in the ``surface'' of the domain might migrate in\nand out the domain. Thus, $O(n^{2/3})$ particles could be exchanged in\nthis case.\n\nFigures \\ref{fig:domain_decomposition_sort}, \\ref{fig:domain_decomposition_exch}\nand \\ref{fig:domain_decomposition} show the elapsed time for sorting\nsamples, exchanging samples, and domain decomposition, for the case of\ndisk galaxy simulations, in the case of $n_{\\rm smp}=500$ and $n \\sim\n5.3 \\times 10^5$. We also plot the fitting curves modeled as\n\\begin{eqnarray}\n\\label{eq:ddfit}\n  T_{\\rm dc} &\\sim& T_{\\rm dc,sort} + T_{\\rm dc,exch} \\\\\n             &=& \\tau_{\\rm dc,sort} n_{\\rm smp} n_p^{2/3}\n                 + \\tau_{\\rm dc,exch} \\sigma \\Delta t / \\left<r\\right> n^{2/3} b_p,\n\\end{eqnarray}\nwhere $\\tau_{\\rm dc,sort}$ and $\\tau_{\\rm dc,exch}$ are the execution\ntime for sorting one particle and for exchanging one particle\nrespectively, $\\sigma$ is the typical velocity of particles, $\\Delta\nt$ is the timestep and $\\left<r\\right>$ is the average interparticle\ndistance. For simplicity we ignore weak log term in $T_{\\rm dc,sort}$\n. On K computer, $\\tau_{\\rm dc,sort} = 2.67\\times 10^{-7}$ second and\n$\\tau_{\\rm dc,exch} = 1.42\\times 10^{-7}$ second per byte. Note that\n$\\tau_{\\rm dc,exch} \\sim 672 T_{\\rm p2p,word}$.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig16.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc,sort}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition_sort}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig17.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc,exch}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition_exch}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig18.eps}\n  \\end{center}\n  \\caption{\n\nMeasured $T_{\\rm dc}$ and its fitted curve as a function of $n_p$, in\nthe case of $n_{\\rm smp}=500$ and $n \\sim 5.3 \\times 10^5$.\n\n    }\n  \\label{fig:domain_decomposition}\n\\end{figure}\n\nThe analysis above, however, indicates that $T_{\\rm dc,exch}$ is, even\nwhen it is relatively large, still much smaller than $T_{\\rm exch}$,\nthe time to exchange particles and superparticles for interaction\ncalculation (see section \\ref{sec:exchnage_list}).\n\n\\subsubsection{Tree construction}\n\nTheoretically, the cost of tree construction is $O(n{\\rm log}n)$, and\nof the same order as the interaction calculations itself. However, in\nour current implementation, the interaction calculation is much more\nexpensive, independent of target architecture and the type of the\ninteraction. Thus we ignore the time for the tree constructions.\n\n\n\\subsubsection{Exchange of particles and superparticles}\n\\label{sec:exchnage_list}\n\nFor the exchange of particles and superparticles, in the current\nimplementation of FDPS, first each node constructs the list of\nparticles and superparticles (hereafter the exchange list) to be sent\nto all other nodes, and then data are exchanged through a single call\nto {\\tt MPI\\_Alltoallv}. The way the exchange list is constructed\ndepends on the force calculation mode. In the case of long-range\nforces, usual tree traversal with a fixed opening angle $\\theta$ is\nperformed. For the short-range forces, the procedure used depends on\nthe subtypes of the interaction. In the case of fixed or $j$-dependent\ncutoff, the exchange list for a node can be constructed by a single\ntraversal of the local tree. On the other hand, for $i$-dependent or\nsymmetric cutoff, first each node constructs the $j$-dependent\nexchange lists and sends them to all other nodes. Each nodes then\nconstructs the $i$-dependent exchange lists and sends then again.\n\nThe time for the construction and exchange of exchange list is thus\ngiven by\n\n", "index": 9, "text": "\\begin{equation}\n  T_{\\rm exch} = k_{\\rm type}(T_{\\rm exch,const}+T_{\\rm exch,comm}).\n  \\label{eq:exchangecost}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"T_{\\rm exch}=k_{\\rm type}(T_{\\rm exch,const}+T_{\\rm exch,comm}).\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>exch</mi></msub><mo>=</mo><mrow><msub><mi>k</mi><mi>type</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>T</mi><mrow><mi>exch</mi><mo>,</mo><mi>const</mi></mrow></msub><mo>+</mo><msub><mi>T</mi><mrow><mi>exch</mi><mo>,</mo><mi>comm</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nfor the case of short-range interactions, where $r_{\\rm cut}$ is the\naverage cutoff length and and $\\left<r\\right>$ is the average\ninterparticle distance. In this section we set $r_{\\rm cut}$ so that\nthe number of particles in the neighbor sphere is to be 100. In other\nwords, $r_{\\rm cut} \\sim 3\\left<r\\right>$.\n\nIn figure \\ref{fig:exchangeLET_nlist-nsend}, we plot the list length\nfor short and long interactions against the average number of\nparticles. The rough estimate of equations (\\ref{eq:n_list_long}) and\n(\\ref{eq:n_list_short}) agree very well with the measurements.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig21.eps}\n  \\end{center}\n  \\caption{\n  \nThe average length of the exchange lists for long-range interaction\n(circles) and for short-range interaction (squares) as a function of\n$n$, in the case of $\\theta=0.4$ and $n_P=2048$. Solid and dashed\ncurves are the predictions of equations (\\ref{eq:n_list_long}) and\n(\\ref{eq:n_list_short}), respectively.\n\n    }\n  \\label{fig:exchangeLET_nlist-nsend}\n\\end{figure}\n\n\n\\subsubsection{Tree traverse and interaction calculation}\n\nThe time for the force calculation is given by\n\n", "itemtype": "equation", "pos": 74548, "prevtext": "\n\nHere, $k_{\\rm type}$ is an coefficient which is unity for fixed and\n$j$-dependent cutoffs and two for other cutoffs. Strictly speaking,\nthe communication cost does not double for $i$-dependent or symmetric\ncutoffs, since we send only particles which was not sent in the first\nstep. However, for simplicity we use $k=2$ for both calculation and\ncommunication.\n\nThe two terms in equation (\\ref{eq:exchangecost}) are then\napproximated as\n\\begin{eqnarray}\n  T_{\\rm exch,const} &=& \\tau_{\\rm exch,const} n_{\\rm exch,list}, \\label{eq:exchange} \\\\\n  T_{\\rm exch,comm}(n_{\\rm msg}) &=&  T_{\\rm alltoallv}\\left( n_{\\rm exch,list}/n_{p} b_p \\right), \\label{eq:exchange_comm}\n\\end{eqnarray}\n\nwhere $n_{\\rm exch,list}$ is the average length of the exchange list\nand $\\tau_{\\rm exch,const}$ is the execution time for constructing one\nexchange list.  Figures \\ref{fig:exchangeLET_nlist-wtime_const}\nand \\ref{fig:exchangeLET_nlist-wtime_exch} show the execution time for\nconstructing and exchanging the exchange list against the average\nlength of the list. Here, $b_p$=48 bytes for both short and long-range\ninteractions. From figure \\ref{fig:exchangeLET_nlist-wtime_const}, we\ncan see that the elapsed time can be fitted well by equation\n(\\ref{eq:exchange}).  Here $\\tau_{\\rm exch,const}$ is $1.12 \\times\n10^{-7}$ second for long-range interaction and $2.31 \\times 10^{-7}$\nsecond for short-range interaction.\n\nFrom figure \\ref{fig:exchangeLET_nlist-wtime_exch}, we can see a large\ndiscrepancy between measured points and the curves predicted from\nequation (\\ref{eq:exchange_comm}). In the measurement of the\nperformance of ${\\tt MPI\\_Alltoallv}$ in section \\ref{sec:comm_model},\nwe used uniform message length across all processes. In actual use in\nexchange particles, the length of the message is not\nuniform. Neighboring processes generally exchange large messages,\nwhile distant processes exchange short message. For such cases,\ntheoretically, communication speed measured in terms of average\nmessage length should be faster. In practice, however, we observed a\nserious degradation of performance. This degradation seems to imply\nthat the current implementation of ${\\tt MPI\\_Alltoallv}$ is\nsuboptimal for non-uniform message size.\n\n\\begin{figure}\n  \\begin{center} \\includegraphics[width=8cm]{fig19.eps} \\end{center} \\caption{\n    Time for the construction of the exchange list plotted against the\n    average length of the list, for the case of $n_p=2048$ and $n \\sim\n    2.7 \\times 10^5, 5.3 \\times 10^5, 1.1 \\times 10^6, 2.1 \\times\n    10^6$. Circles and squares indicate the results for long-range and\n    short-range force, respectively. Solid and dashed curves are model\n    fittings of equation (\\ref{eq:exchange}).\n    } \\label{fig:exchangeLET_nlist-wtime_const}\n\\end{figure}\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig20.eps}\n  \\end{center}\n  \\caption{\n\nTime for the communication of the exchange list against the average\nlength of the list per process, for the case of $n_p=2048$ and $n \\sim\n2.7 \\times 10^5, 5.3 \\times 10^5, 1.1 \\times 10^6, 2.1 \\times\n10^6$. Circles and squares indicate the results for long-range and\nshort-range force, respectively. The curve is prediction from\nequation (\\ref{eq:exchange}).\n\n    }\n  \\label{fig:exchangeLET_nlist-wtime_exch}\n\\end{figure}\n\nIn the following, we estimate $n_{\\rm exch,list}$. If we consider a\nrather idealized case, in which all domains are cubes containing $n$\nparticles, the total length of the exchange lists for one domain can\napproximately be given by\n\\begin{eqnarray}\n  n_{\\rm exch,list} &\\sim& \\frac{14n^{2/3}}{\\theta} + \\frac{21\\pi n^{1/3}}{\\theta^2} \\nonumber \\\\\n  &&+ \\frac{28\\pi}{3\\theta^3} {\\rm log_2} \\left\\{ \\frac{\\theta}{2.8}\\left[\\left(nn_p\\right)^{1/3} - n^{1/3}\\right] \\right\\}, \\label{eq:n_list_long}\n\\end{eqnarray}\nfor the case of long-range interactions and\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:n_list_short}\n  n_{\\rm exch,list} \\sim \\left( n^{1/3}-1+2\\frac{r_{\\rm cut}}{\\left<r\\right>} \\right)^3-n,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"n_{\\rm exch,list}\\sim\\left(n^{1/3}-1+2\\frac{r_{\\rm cut}}{\\left&lt;r\\right&gt;}\\right%&#10;)^{3}-n,\" display=\"block\"><mrow><mrow><msub><mi>n</mi><mrow><mi>exch</mi><mo>,</mo><mi>list</mi></mrow></msub><mo>\u223c</mo><mrow><msup><mrow><mo>(</mo><mrow><mrow><msup><mi>n</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup><mo>-</mo><mn>1</mn></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mfrac><msub><mi>r</mi><mi>cut</mi></msub><mrow><mo>\u27e8</mo><mi>r</mi><mo>\u27e9</mo></mrow></mfrac></mrow></mrow><mo>)</mo></mrow><mn>3</mn></msup><mo>-</mo><mi>n</mi></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nwhere $T_{\\rm icalc,force}$ and $T_{\\rm icalc,const}$ are the time for\nthe force calculations for all particles and the tree traverses for\nall interaction lists, respectively.\n\n$T_{\\rm icalc,force}$ and $T_{\\rm icalc,const}$ are expressed as\n\\begin{eqnarray}\nT_{\\rm icalc,const} &=& \\tau_{\\rm icalc,const} n n_{\\rm icalc,list} / n_{\\rm grp}, \\label{eq:wtime_force} \\\\\nT_{\\rm icalc,force}  &=& \\tau_{\\rm icalc,force} n n_{\\rm icalc,list},\n\\end{eqnarray}\nwhere $n_{\\rm icalc,list}$ is the average length of the interaction\nlist, $n_{\\rm grp}$ is the number of $i$ particle groups for modified\ntree algorithms by \\citet{1990JCoPh..87..161B}, $\\tau_{\\rm\nicalc,force}$ and $\\tau_{\\rm icalc,const}$ are the time for one force\ncalculation and for constructing one interaction list.  In\nfigure \\ref{fig:interaction_calc_nwalk-wtime} we plot the time for the\nconstruction of the interaction list as a function of $n_{\\rm\ngrp}$. On K computer, $\\tau_{\\rm icalc,const}$ are $3.72\\times\n10^{-8}$ second for the long-range force and $6.59\\times 10^{-8}$\nsecond for the short-range force.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig22.eps}\n  \\end{center}\n  \\caption{\n\nTime for the construction of the interaction list for long-range force\n(circles) and short-range force (squares), for the case of $n \\sim\n5.3 \\times 10^5$ and $\\theta = 0.4$. Solid and dashed curves are fitting\ncurves for long-rage and short range forces of\nequation (\\ref{eq:wtime_force}), respectively.\n\n    }\n  \\label{fig:interaction_calc_nwalk-wtime}\n\\end{figure}\n\nThe length of the interaction list is given by\n\\begin{eqnarray}\n  n_{\\rm icalc,list} &\\sim& n_{\\rm grp} + \\frac{14n_{\\rm grp}^{2/3}}{\\theta} + \\frac{21\\pi n_{\\rm grp}^{1/3}}{\\theta^2} \\nonumber \\\\\n  &&+ \\frac{28\\pi}{3\\theta^3} {\\rm log_2} \\left[ \\frac{\\theta}{2.8}\\left\\{\\left(nn_p\\right)^{1/3} - n_{\\rm grp}^{1/3}\\right\\}\\right] \\label{eq:nlist_long}\n\\end{eqnarray}\nfor the case of long-range interactions and\n\n", "itemtype": "equation", "pos": 75854, "prevtext": "\nfor the case of short-range interactions, where $r_{\\rm cut}$ is the\naverage cutoff length and and $\\left<r\\right>$ is the average\ninterparticle distance. In this section we set $r_{\\rm cut}$ so that\nthe number of particles in the neighbor sphere is to be 100. In other\nwords, $r_{\\rm cut} \\sim 3\\left<r\\right>$.\n\nIn figure \\ref{fig:exchangeLET_nlist-nsend}, we plot the list length\nfor short and long interactions against the average number of\nparticles. The rough estimate of equations (\\ref{eq:n_list_long}) and\n(\\ref{eq:n_list_short}) agree very well with the measurements.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig21.eps}\n  \\end{center}\n  \\caption{\n  \nThe average length of the exchange lists for long-range interaction\n(circles) and for short-range interaction (squares) as a function of\n$n$, in the case of $\\theta=0.4$ and $n_P=2048$. Solid and dashed\ncurves are the predictions of equations (\\ref{eq:n_list_long}) and\n(\\ref{eq:n_list_short}), respectively.\n\n    }\n  \\label{fig:exchangeLET_nlist-nsend}\n\\end{figure}\n\n\n\\subsubsection{Tree traverse and interaction calculation}\n\nThe time for the force calculation is given by\n\n", "index": 13, "text": "\\begin{equation}\nT_{\\rm icalc} = T_{\\rm icalc,force} + T_{\\rm icalc,const},\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"T_{\\rm icalc}=T_{\\rm icalc,force}+T_{\\rm icalc,const},\" display=\"block\"><mrow><mrow><msub><mi>T</mi><mi>icalc</mi></msub><mo>=</mo><mrow><msub><mi>T</mi><mrow><mi>icalc</mi><mo>,</mo><mi>force</mi></mrow></msub><mo>+</mo><msub><mi>T</mi><mrow><mi>icalc</mi><mo>,</mo><mi>const</mi></mrow></msub></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.03138.tex", "nexttext": "\nfor the case of short-range interactions.\n\nIn figure \\ref{fig:interaction_calc_ng-nlist} we plot the length of\nthe interactions lists for long-range force and short-range force. We\ncan see that the length of the interaction lists can be fitted\nreasonably well.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig23.eps}\n  \\end{center}\n  \\caption{\n\nThe average length of the interaction list for long-range force\n(circles) and short-range force (squares), for the case of $n \\sim\n5.3 \\times 10^5$ and $\\theta = 0.4$. Solid and dashed curves are fitting\ncurves for long-range and short range forces of equations\n(\\ref{eq:nlist_long}) and (\\ref{eq:nlist_short}).\n\n    }\n  \\label{fig:interaction_calc_ng-nlist}\n\\end{figure}\n\nIn the following, we discuss the time for the force calculation.  The\ntime for the force calculation for one particle pair $\\tau_{\\rm\nicalc,force}$ has different values for different kinds of\ninteractions.  We plot $\\tau_{\\rm icalc,force}$ against $n_{\\rm\nicalc,list}$ for various $n_{\\rm grp}$ in\nfigure \\ref{fig:calc_force}. We can see that for larger $n_{\\rm grp}$,\n$\\tau_{\\rm icalc,force}$ becomes smaller. However, from equation\n(\\ref{eq:nlist_long}), large $n_{\\rm grp}$ leads to large $n_{\\rm\nicalc,list}$ and the number of interactions becomes larger. Thus there\nis an optimal $n_{\\rm grp}$. In our disk-galaxy simulations in K\ncomputer, the optimal $n_{\\rm grp}$ is a few hundreds, and dependence\non $n_{\\rm grp}$ is weak.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig24.eps}\n  \\end{center}\n  \\caption{\n  \n  Time for the evaluation of one gravity force against $n_{\\rm icalc,\n  list}$ for various $n_{\\rm grp}$.\n  \n    }\n  \\label{fig:calc_force}\n\\end{figure}\n\n\n\n\n\\subsubsection{Total time}\n\\label{sec:total_time}\n\nNow we can predict the total time of the calculation using the above\ndiscussions. The total time per one timestep is given by\n\\begin{eqnarray}\n  T_{\\rm step} &\\sim& T_{\\rm dc,sort}/n_{\\rm dc} \\nonumber\n                  + k_{\\rm type}\\left( T_{\\rm exch,const}+T_{\\rm exch,comm} \\right) \\nonumber \\\\\n                  &&+ T_{\\rm icalc,force} + T_{\\rm icalc,const} \\label{eq:totalcost2} \\\\\n             &\\sim& \\tau_{\\rm dc,sort} n_{\\rm smp} n_p^{2/3}/n_{\\rm dc} \\nonumber \\\\\n                &&+ k_{\\rm type}\\left( \\tau_{\\rm exch,const} n_{\\rm exch,list} + \\tau_{\\rm alltoallv,startup}n_p \\right. \\nonumber \\\\\n                &&+ \\left. \\tau_{\\rm alltoallv,word}n_{\\rm exch,list}b_pn_p^{1/3} \\right) \\nonumber \\\\\n                && + \\tau_{\\rm icalc,force} n n_{\\rm icalc,list} \\nonumber \\\\\n                && + \\tau_{\\rm icalc,const} n n_{\\rm icalc,list} / n_{\\rm grp}.   \\label{eq:totalcost3}\n\\end{eqnarray}\nThe time coefficients in equation (\\ref{eq:totalcost3}) for K computer\nare summarized in table \\ref{table:time_coefficients}. In this section\nwe use $n_{\\rm dc}=1$.\n\n\\begin{table}\n\\caption{Time coefficients in equation \\ref{eq:totalcost3} for K computer.\n$\\tau_{\\rm icalc,force}$ is the value for gravity.  }\n\\begin{tabular}{|l|l|} \\hline\n$\\tau_{\\rm alltoallv,startup}$ [s] & $1.66 \\times 10^{-6}$ \\\\ \n$\\tau_{\\rm alltoallv,word}$ [s/byte] & $1.11 \\times 10^{-10}$\\\\\n$\\tau_{\\rm dc,sort}$ [s] & $2.67\\times 10^{-7}$ \\\\ \n$\\tau_{\\rm exch,const}$ [s] & $1.12 \\times 10^{-7}$ \\\\\n$\\tau_{\\rm icalc,const}$ [s] & $3.72\\times 10^{-8}$ \\\\\n$\\tau_{\\rm icalc,force}$ [s] & $3.05\\times 10^{-10}$ \\\\ \\hline\n\\end{tabular}\n\\label{table:time_coefficients}\n\\end{table}\n\nTo see if the predicted time by equation (\\ref{eq:totalcost3}) is\nreasonable, we compare the predicted time and the time obtained from\nthe disk galaxy simulation with the total number of particles ($N$) of\n550 million and $\\theta = 0.4$. In our simulations, we use up to the\nquadrupole moment. On the other hand, we assume the monopole moment\nonly in equation (\\ref{eq:totalcost3}). Thus we have to correct the\ntime for the force calculation in equation (\\ref{eq:totalcost3}). In\nour simulations, the cost of the force calculation of the quadrupole\nmoment is two times higher than that of the monopole moment and about\n75 \\% of particles in the interactions list are superparticles.  Thus\nthe cost of the force calculation in the simulation is 75 \\% higher\nthan the prediction. We apply this correction to equation\n(\\ref{eq:totalcost3}). In\nfigure \\ref{fig:performance_model_strong_breakdown_comp}, we plot the\nbreakdown of the predicted time with the correction and the obtained\ntime from the disk galaxy simulations. We can see that our predicted\ntimes agree with the measurements very well.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig25.eps}\n  \\end{center}\n  \\caption{\n  \n  Breakdown of the total time of the calculation per one timestep\n  against $n_p$, for the case of $N=5.5\\times 10^8$ , $n_{\\rm\n  smp}=500$, $\\theta=0.4$ and $n_{\\rm grp}=130$.\n  \n}\n  \\label{fig:performance_model_strong_breakdown_comp}\n\\end{figure}\n\nIn the following, we analyze the performance of the gravitational many\nbody simulations for various hypothetical computers. In\nfigure \\ref{fig:performance_model_strong_breakdown}, we plot the\nbreakdown of the calculation time predicted using equation\n(\\ref{eq:totalcost3}) for the cases of 1 billion and 10 million\nparticles against $n_p$. For the case of 1 billion particles, we can\nsee that the slope of $T_{\\rm step}$ becomes shallower for\n$n_p \\gtrsim 10000$ and increases for $n_p \\gtrsim 30000$, because\n$T_{\\rm dc,sort}$ dominates. Note that $T_{\\rm exch,comm}$ also has\nthe minimum value. The reason is as follows. For small $n_p$, $T_{\\rm\nalltoallv,word}$ is dominant in $T_{\\rm exch,comm}$ and it decrease as\n$n_p$ increases, because the length of $n_{\\rm exch,list}$ becomes\nsmaller. For large $n_p$, $T_{\\rm alltoallv,startup}$ becomes dominant\nas it increases linearly.  We can see the same tendency for the case\nof 10 million particles. However, the optimal $n_p$, at which $T_{\\rm\nstep}$ is the minimum, is smaller than that for 1 billion particles,\nbecause $T_{\\rm dc,sort}$ is independent of $N$.\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig26.eps}\n  \\end{center}\n  \\caption{\n  \n  Breakdown of the total time of the calculation per one timestep\n  against $n_p$, for the case of $N=10^9$ (top panel) and $=10^7$\n  (bottom panel), $n_{\\rm smp}=500$, $\\theta=0.4$ and $n_{\\rm\n  grp}=300$.\n  \n}\n  \\label{fig:performance_model_strong_breakdown}\n\\end{figure}\n\nIn figure \\ref{fig:performance_model_strong_breakdown_x10}, we plot\nthe breakdown of the predicted calculation time for a hypothetical\ncomputer which has the floating-point operation performance ten times\nfaster than that of K computer (hereafter X10). In other words,\n$\\tau_{\\rm alltoallv,startup}$ and $\\tau_{\\rm alltoallv,word}$ are the\nsame as those of K computer, but $\\tau_{\\rm dc,sort}$, $\\tau_{\\rm\nexch,const}$, $\\tau_{\\rm icalc,const}$ and $\\tau_{\\rm icalc,force}$\nare ten times smaller than those of K computer. We can see that the\noptimal $n_p$ is shifted to smaller $n_p$ for both cases of $N$ of 1\nbillion and 10 million, because $T_{\\rm exch,comm}$ is unchanged.\nHowever, the shortest time per timestep is improved by about a factor\nof five.  If the network performance is also improved by a factor of\nten, we would get the same factor of ten improvement for the shortest\ntime per timestep. In other words, by reducing the network performance\nby a factor of ten, we suffer only a factor of two degradation of the\nshortest time.\n\n\n\n\n\\begin{figure}\n  \\begin{center} \\includegraphics[width=8cm]{fig27.eps} \\end{center} \\caption{\n  \nThe same as figure \\ref{fig:performance_model_strong_breakdown}, but\nfor the floating-point operation performance ten times faster than K\ncomputer.\n  \n}\n  \\label{fig:performance_model_strong_breakdown_x10}\n\\end{figure}\n\nIn figure \\ref{fig:performance_model_strong_predict}, we plot\npredicted $T_{\\rm step}$ for three hypothetical computers and K\ncomputer. Two of four computers are the same computer models we used\nabove. Another is a computer with the floating-point operation\nperformance hundred times faster than K computer (hereafter X100).\nThe last one is a computer of which the performance of the force\ncalculation is ten times faster than K computer (hereafter ACL). In\nother words, only $\\tau_{\\rm icalc,force}$ is ten times smaller than\nthat of K computer.  This computer is roughly mimicking a computer\nwith an accelerator such as, GPU \\citep{hamada2009novel,\nHamada:2010:TAN:1884643.1884644}, GRAPE \\citep{1990Natur.345...33S,\n2003PASJ...55.1163M} and PEZY-SC. Here we use the optimal $n_{\\rm\ngrp}$, at which $T_{\\rm step}$ is minimum, for each computers. For the\ncase of $N=10^9$, the optimal $n_{\\rm grp} \\sim 300$ for K computer\nand X10, $\\sim 400$ for X100 and $\\sim 1600$ for ACL. For the case of\n$N=10^{12}$, the optimal $n_{\\rm grp}$ for K, X10, X100 is the same as\nthose for $N=10^9$, but $\\sim 1800$ for ACL. The optimal value of\n$n_{\\rm grp}$ for ACL is larger than those of any other computers,\nbecause large $n_{\\rm grp}$ reduces the cost of the construction of\nthe interaction list.\n\nFrom figure \\ref{fig:performance_model_strong_predict}, we can see\nthat for small $n_p$, X10 and X100 are ten and hundred times faster\nthan K computer, respectively. However, for the case of $N=10^9$,\n$T_{\\rm step}$ of the values of X10 and X100 increase for $n_p \\gtrsim\n15000$ and $\\gtrsim 7000$, because the $T_{\\rm exch,comm}$ becomes the\nbottleneck. ACL shows a similar performance to that of X10 up to\noptimal $n_p$, because the force calculation is dominant in the total\ncalculation time. On the other hand, for large $n_p$, the performance\nof ACL is almost the same as that of K computer, because ACL has the\nsame bottleneck as K computer has, which is the communication of the\nexchange list.  On the other hand, for the case of $N=10^{12}$,\n$T_{\\rm step}$ is scaled up to $n_p \\sim 10^5$ for all computers. This\nis because for larger $N$ simulation, the costs of the force\ncalculation and the construction of the interaction list are\nrelatively higher than the communication of the exchange list. Thus\nthe optimal $n_p$ is sifted to larger value if we use larger $N$.\n\n\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig28.eps}\n  \\end{center}\n  \\caption{\n  \nPredicted total calculation time for three hypothetical computers and\nK computer as a function of $n_p$, for the case of $n_{\\rm smp}=500$,\n$\\theta=0.4$.  Top and bottom panels indicate the results of the case\nfor $N=10^{12}$ and $N=10^{9}$, respectively.\n  \n}\n  \\label{fig:performance_model_strong_predict}\n\\end{figure}\n\nFrom figures \\ref{fig:performance_model_strong_breakdown}\nand \\ref{fig:performance_model_strong_breakdown_x10}, we can see that\nfor large $n_p$, performance will be limited by $T_{\\rm dc,sort}$ and\n$T_{\\rm exch,comm}$. Therefor, if we can reduce then them further, we\ncan improve the efficiency of the calculation with large $n_p$. It is\npossible to reduce the time for sort by applying the algorithm used in\nx direction to y direction as well or setting $n_{\\rm dc}$ to more\nthan unity. It is more difficult to reduce $T{\\rm exch,comm}$, since\nwe are using system-provided {\\tt MPI\\_Alltoallv}.\n\n\n\n\n\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe have developed a framework for large-scale parallel particle-based\nsimulations, FDPS.  Users of FDPS need not care about complex\nimplementations of domain decomposition, exchange of particles,\ncommunication of data for the interaction calculation, or optimization\nfor multi-core processors.  Using FDPS, particle simulation codes\nwhich achieve high performance and high scalability on massively\nparallel distributed-memory machines can be easily developed for a\nvariety of problems.  As we have shown in\nsection~\\ref{sec:samplecode}, a parallel $N$-body simulation code can\nbe written in less than 120 lines.  Example implementations of\ngravitational $N$-body simulation and SPH simulation showed excellent\nscalability and performance. We hope FDPS will help researchers to\nconcentrate on their research, by removing the burden of complex code\ndevelopment for parallization and architecture-dependent tuning.\n\n\n\n\n\n\\bigskip\n\nWe thank M. Fujii for providing initial conditions of spiral\nsimulations, T. Ishiyama for providing his Particle Mesh code,\nK. Yoshikawa for providing his TreePM code and Y. Maruyama for being\nthe first user of FDPS.  We are grateful to M. Tsubouchi for her help\nin managing the FDPS development team. This research used\ncomputational resources of the K computer provided by the RIKEN\nAdvanced Institute for Computational Science through the HPCI System\nResearch project (Project ID:ra000008). Numerical computations were in\npart carried out on Cray XC30 at Center for Computational\nAstrophysics, National Astronomical Observatory of Japan.\n\n\\begin{thebibliography}{}\n\n\\bibitem[Asphaug \\& Reufer(2014)]{2014NatGe...7..564A}\n {{Asphaug}, E. and {Reufer}, A.}\\ 2014, Natge, 7, 564\n  \n\\bibitem[Bagla(2002)]{2002JApA...23..185B}\n  {Bagla}, J.~S.\\ 2002, JApA, 23, 185\n\\bibitem[{Balsara}(1995)]{1995JCoPh.121..357B}\n{Balsara}, D.~S.\\ 1995, JCoPh, 121, 357\n\\bibitem[Barnes \\& Hut(1986)]{1986Natur.324..446B}\n  {Barnes}, J. and {Hut}, P.\\ 1986, Nature, 324, 446\n\\bibitem[Barnes(1990)]{1990JCoPh..87..161B}\n  {Barnes}, J.\\ 1990, JCoPh, 87, 161\n\\bibitem[{B{\\'e}dorf} et al.(2012)]{2012JCoPh.231.2825B}\n  {B{\\'e}dorf}, J. and {Gaburov}, E. and {Portegies Zwart}, S.\\ 2012, JCoPh, 231, 2825\n\\bibitem[B{\\'e}dorf et al.(2015)]{Bedorf:2014:PGT:2683593.2683600}\nB{\\'e}dorf, J., Gaburov, E., Fujii, M., Nitadori, K., Ishiyama, T., and Portegies Zwart, S.\\ 2014, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE Press, 54  \n\\bibitem[Benz et al.(1986)]{1986Icar...66..515B}\n  {{Benz}, W. and {Slattery}, W.~L. and {Cameron}, A.~G.~W.}\\ 1986, Icar, 66, 515\n\\bibitem[Blackston \\& Suel(1997)]{Blackston:1997:HPE:509593.509597}\n  Blackston, D., and Suel, T.\\ 1997, Proceedings of the 1997 ACM/IEEE\n  Conference on Supercomputing, ACM, 1  \n\\bibitem[Bode et al.(2000)]{2000ApJS..128..561B}\n  {Bode}, P. and {Ostriker}, J.~P. and {Xu}, G.\\ 2000, ApJS, 128, 561\n\n\\bibitem[Cameron \\& Ward(1976)]{1976LPI.....7..120C}\n{{Cameron}, A.~G.~W. and {Ward}, W.~R.}\\ 1976, Lunar and Planetary Science Conference, 7, 120\n\\bibitem[Canup et al.(2013)]{2013Icar..222..200C}\n{{Canup}, R.~M. and {Barr}, A.~C. and {Crawford}, D.~A.}\\ 2013, Icar, 222, 200\n\n\\bibitem[Dehnen \\& Aly(2012)]{2012MNRAS.425.1068D}\n{{Dehnen}, W. and {Aly}, H.}\\ 2012, MNRAS, 425, 1068\n\\bibitem[Dubinski(1996)]{1996NewA....1..133D}\n  {Dubinski}, J.\\ 1996, 1, 133,\n\\bibitem[Dubinski et al.(2004)]{2004NewA....9..111D}\n  Dubinski, J., Kim, J., Park, C., Humble, R.\\ 2004, NewA, 9, 111\n\n\\bibitem[Fujii et al.(2011)]{2011ApJ...730..109F}\n{Fujii}, M.~S., {Baba}, J., {Saitoh}, T.~R., {Makino}, J., {Kokubo}, E., and {Wada}, K.\\ 2011, ApJ, 730, 109\n  \n\\bibitem[Gaburov et al.(2009)]{2009NewA...14..630G} {Gaburov}, E. and\n  {Harfst}, S. and {Portegies Zwart}, S.\\ 2009, NewA, 14, 630\n\n\\bibitem[Hamada et al.(2009a)]{Hamada:2009:THN:1654059.1654123}\n  Hamada, T., Narumi, T., Yokota, R., Yasuoka, K., Nitadori, K., and Taiji, M.\\ 2009,\n  Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, 62, 1\n\\bibitem[Hamada et al.(2009b)]{hamada2009novel}\nHamada, T., Nitadori, K., Benkrid, K., Ohno, Y., Morimoto, G., Masada, T., Shibata, Y., Oguri, K. and Taiji, M.\\ 2009, Computer Science-Research and Development, 24, 21\n\\bibitem[Hamada et al.(2010)]{Hamada:2010:TAN:1884643.1884644}\n  Hamada, T., and Nitadori, K.\\ 2010, Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE Computer Society, 1\n\\bibitem[{Hartmann} \\& {Davis}(1975)]{1975Icar...24..504H}\n  {{Hartmann}, W.~K. and {Davis}, D.~R.}\\ 1975, Icar, 24, 504\n\\bibitem[{Hernquist}(1990)]{1990ApJ...356..359H}\n{Hernquist}, L.\\ 1990, ApJ, 356, 359\n\\bibitem[Hockney \\& Eastwood(1988)]{hockney1988computer} Hockney,\n  R.W. and Eastwood, J.W.\\ 1988, Computer Simulation Using Particles,\n  CRC Press\n  \n\\bibitem[Ishiyama et al.(2009)]{2009PASJ...61.1319I}\n  {Ishiyama}, T. and {Fukushige}, T. and {Makino}, J.\\ 2009, PASJ, 61, 1319\n\\bibitem[Ishiyama et al.(2012)]{Ishiyama:2012:PAN:2388996.2389003}\n  Ishiyama, T., Nitadori, K., and Makino, J.\\ 2012, SC '12, 5, 1\n\n\\bibitem[Navarro et al.(1996)]{1996ApJ...462..563N}\n{{Navarro}, J.~F. and {Frenk}, C.~S. and {White}, S.~D.~M.}\\ 1996, ApJ, 462, 563\n\\bibitem[Nitadori et al.(2006)]{2006NewA...12..169N}\n  {Nitadori}, K. and {Makino}, J. and {Hut}, P.\\ 2006, NewA, 12, 169\n\n\n\\bibitem[{Makino}(1991)]{1991PASJ...43..859M}\n  {Makino}, J.\\ 1991, PASJ, 43, 859\n\\bibitem[Makino et al.(2003)]{2003PASJ...55.1163M} Makino, J.,\n  Fukushige, T., Koga, M., and Namura, K.\\ 2003, \\pasj, 55, 1163\n\\bibitem[Makino(2004)]{2004PASJ...56..521M}\n  {Makino}, J.\\ 2004, PASJ, 56, 521  \n\\bibitem[Monaghan(1992)]{1992ARA&A..30..543M}\n{Monaghan}, J.~J.\\ 1992, ARA\\&A, 30, 543\n\\bibitem[{Monaghan}(1997)]{1997JCoPh.136..298M}\n{Monaghan}, J.~J.\\ 1997, JCoPh, 136, 298\n\n  \n\n\\bibitem[{Rosswog}(2009)]{2009NewAR..53...78R}\n{Rosswog}, S.\\ 2009, NewAR, 53, 78\n  \n\\bibitem[Salmon \\& Warren(1994)]{1994JCoPh.111..136S}\n  {Salmon}, J.~K. and {Warren}, M.~S.\\ 1994, 111, 136,\n\\bibitem[Schuessler \\& Schmitt(1981)]{1981A&A....97..373S}\n{Schuessler}, I. and {Schmitt}, D.\\ 1981, A\\&A, 97, 3735  \n\\bibitem[Springel(2005)]{2005MNRAS.364.1105S}\n  {Springel}, V.\\ 2005, NMRAS, 364, 1105\n\\bibitem[{Springel}(2010)]{2010ARA&A..48..391S}\n{Springel}, V.\\ 2010, ARA\\&A, 48, 391\n\\bibitem[Sugimoto et al.(1990)]{1990Natur.345...33S} Sugimoto, D.,\n  Chikada, Y., Makino, J., et al.\\ 1990, \\nat, 345, 33\n  \n  \n\\bibitem[Tanikawa et al.(2012)]{2012NewA...17...82T}\n  {Tanikawa}, A. and {Yoshikawa}, K. and {Okamoto}, T. and {Nitadori}, K.\\ 2012, NewA, 17, 82\n\\bibitem[Tanikawa et al.(2013)]{2013NewA...19...74T}\n  {Tanikawa}, A. and {Yoshikawa}, K. and {Nitadori}, K. and {Okamoto}, T.\\ 2013, NewA, 19, 74\n\\bibitem[Teodoro et al.(2014)]{2014LPI....45.2703T}\n  {{Teodoro}, L.~F.~A. and {Warren}, M.~S. and {Fryer},\n    C. and {Eke}, V. and {Zahnle}, K.}\\ 2014, Lunar and Planetary Science Conference, 45, 2703\n\n  \n\\bibitem[{Wadsley} et al.(2004)]{2004NewA....9..137W}\n  {Wadsley}, J.~W. and {Stadel}, J. and {Quinn}, T.\\ 2004, NewA, 9, 137\n\\bibitem[Warren \\& Salmon(1995)]{1995CoPhC..87..266W}\n{Warren}, M.~S., and {Salmon}, J.~K.\\ 1995, CoPhC, 87, 266  \n\\bibitem[Widrow \\& Dubinski(2005)]{2005ApJ...631..838W}\n{{Widrow}, L.~M. and {Dubinski}, J.}\\ 2005, ApJ, 631, 838\n\n\\bibitem[Xu(1995)]{1995ApJS...98..355X}\n  {Xu}, G.\\ 1995, ApJS, 98, 355\n\n\\bibitem[Yoshikawa \\& Fukushige(2005)]{2005PASJ...57..849Y}\n{Yoshikawa}, K. and {Fukushige}, T.\\ 2005, PASJ, 57, 849\n  \n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 77904, "prevtext": "\nwhere $T_{\\rm icalc,force}$ and $T_{\\rm icalc,const}$ are the time for\nthe force calculations for all particles and the tree traverses for\nall interaction lists, respectively.\n\n$T_{\\rm icalc,force}$ and $T_{\\rm icalc,const}$ are expressed as\n\\begin{eqnarray}\nT_{\\rm icalc,const} &=& \\tau_{\\rm icalc,const} n n_{\\rm icalc,list} / n_{\\rm grp}, \\label{eq:wtime_force} \\\\\nT_{\\rm icalc,force}  &=& \\tau_{\\rm icalc,force} n n_{\\rm icalc,list},\n\\end{eqnarray}\nwhere $n_{\\rm icalc,list}$ is the average length of the interaction\nlist, $n_{\\rm grp}$ is the number of $i$ particle groups for modified\ntree algorithms by \\citet{1990JCoPh..87..161B}, $\\tau_{\\rm\nicalc,force}$ and $\\tau_{\\rm icalc,const}$ are the time for one force\ncalculation and for constructing one interaction list.  In\nfigure \\ref{fig:interaction_calc_nwalk-wtime} we plot the time for the\nconstruction of the interaction list as a function of $n_{\\rm\ngrp}$. On K computer, $\\tau_{\\rm icalc,const}$ are $3.72\\times\n10^{-8}$ second for the long-range force and $6.59\\times 10^{-8}$\nsecond for the short-range force.\n\n\\begin{figure}\n  \\begin{center}\n    \\includegraphics[width=8cm]{fig22.eps}\n  \\end{center}\n  \\caption{\n\nTime for the construction of the interaction list for long-range force\n(circles) and short-range force (squares), for the case of $n \\sim\n5.3 \\times 10^5$ and $\\theta = 0.4$. Solid and dashed curves are fitting\ncurves for long-rage and short range forces of\nequation (\\ref{eq:wtime_force}), respectively.\n\n    }\n  \\label{fig:interaction_calc_nwalk-wtime}\n\\end{figure}\n\nThe length of the interaction list is given by\n\\begin{eqnarray}\n  n_{\\rm icalc,list} &\\sim& n_{\\rm grp} + \\frac{14n_{\\rm grp}^{2/3}}{\\theta} + \\frac{21\\pi n_{\\rm grp}^{1/3}}{\\theta^2} \\nonumber \\\\\n  &&+ \\frac{28\\pi}{3\\theta^3} {\\rm log_2} \\left[ \\frac{\\theta}{2.8}\\left\\{\\left(nn_p\\right)^{1/3} - n_{\\rm grp}^{1/3}\\right\\}\\right] \\label{eq:nlist_long}\n\\end{eqnarray}\nfor the case of long-range interactions and\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eq:nlist_short}\n  n_{\\rm icalc,list} \\sim \\left( n_{\\rm grp}^{1/3}-1+2\\frac{r_{\\rm cut}}{\\left<r\\right>} \\right)^3,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"n_{\\rm icalc,list}\\sim\\left(n_{\\rm grp}^{1/3}-1+2\\frac{r_{\\rm cut}}{\\left&lt;r%&#10;\\right&gt;}\\right)^{3},\" display=\"block\"><mrow><mrow><msub><mi>n</mi><mrow><mi>icalc</mi><mo>,</mo><mi>list</mi></mrow></msub><mo>\u223c</mo><msup><mrow><mo>(</mo><mrow><mrow><msubsup><mi>n</mi><mi>grp</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msubsup><mo>-</mo><mn>1</mn></mrow><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mfrac><msub><mi>r</mi><mi>cut</mi></msub><mrow><mo>\u27e8</mo><mi>r</mi><mo>\u27e9</mo></mrow></mfrac></mrow></mrow><mo>)</mo></mrow><mn>3</mn></msup></mrow><mo>,</mo></mrow></math>", "type": "latex"}]