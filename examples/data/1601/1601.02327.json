[{"file": "1601.02327.tex", "nexttext": "\nwhere the predicted ratings\n\n", "itemtype": "equation", "pos": 9224, "prevtext": "\n\\maketitle\n\n\\begin{abstract}\n  Recommender systems (RSs) provide an effective way of alleviating the information overload problem by selecting personalized choices. Online social networks and user-generated content provide diverse sources for recommendation beyond ratings, which present opportunities as well as challenges for traditional RSs. Although  {\\em social matrix factorization} (Social MF) can integrate ratings with social relations and {\\em topic matrix factorization} can integrate ratings with item reviews, both of them ignore some useful information. In this paper, we investigate the effective data fusion by combining the two approaches, in two steps. First, we extend Social MF to exploit the graph structure of neighbors. Second, we propose a novel framework {\\mbox{MR3}} to jointly model these three types of information effectively for rating prediction by aligning latent factors and hidden topics. We achieve more accurate rating prediction on two real-life datasets. Furthermore, we measure the contribution of each data source to the proposed framework.\n\\end{abstract}\n\n\\section{Introduction}\n\nFor all the benefits of the information abundance and communication technology, the ``information overload'' is one of the digital-age dilemmas we are confronted with. Recommender systems (RSs) are instrumental in tackling this problem as they help determine which information to offer to individual consumers and allow users to quickly find the personalized information that fits their needs ~\\cite{CF92,linden03:amazon,koren09:MF}. RSs are nowadays ubiquitous in various domains and e-commerce platforms, such as recommendation of books at Amazon, musics at Last.fm, movies at Netflix and references at CiteULike.\n\nSocial networking and knowledge sharing sites like Twitter and Epinions are popular platforms for users to connect to each other, to participate in online activities, and to generate shared opinions. Social relations and item contents provide independent and diverse sources for recommendation beyond explicit rating information~\\cite{ganu09:review,HFT,SoRec,LOCABAL}, which present both opportunities and challenges for traditional RSs.\n\n{\\em Collaborative filtering} (CF) approaches are extensively investigated in research community and widely used in industry. They are based on the naive intuition that if \\mbox{users} rated items similarly in the past, then they are likely to rate other items similarly in the future~\\cite{CF92,sarwar01:itemCF}. Latent factors CF, which learns a latent vector of preferences for each user and a latent vector of attributes for each item, gains popularity and becomes the standard model for recommender due to its accuracy and scalability~\\cite{CFSVD98,koren09:MF}. CF models, however, suffer from data sparsity and the imbalance of ratings; they perform poorly on cold users and cold items for which there are no or few data.\n\nTo overcome these weaknesses, additional sources of information are integrated into RSs. One research thread, which we call {\\em social matrix factorization} (Social MF), is to combine ratings with social relations~\\cite{SoRec,SoReg,tranMF,LOCABAL,TrustSVD}. Extensive studies have found higher likelihood of establishing social ties among people having similar characteristics, namely the theory of homophily~\\cite{homophily,socialmedia}. Given that interpersonal similarity and effective communication condition, homophilous ties become effective means of social influence~\\cite{influence,localInfluence}. Social MF methods factorize rating matrix and social matrix simultaneously.\n\nAnother research thread, which we call {\\em topic matrix factorization} (Topic MF), is to integrate ratings with item contents or reviews text~\\cite{CTR,RMR}. Reviews justify the rating of a user, and ratings are associated with item attributes hidden in reviews~\\cite{jakob09review,ganu09:review}. Topic MF methods combine latent factors in ratings with latent topics in item reviews~\\cite{HFT,TopicMF}. Nevertheless, both Social MF and Topic MF ignore some useful information, either item reviews or social relations.\n\nThere is a tendency towards hybrid methods~\\cite{CFCBFdemograph99,CTRSoRec,CCTRSoRec}. These methods all consider diverse sources for recommendation, however, the first two methods are belonging to one-class CF~\\cite{OneCF} and hence the dimensions discovered are not necessarily correlated with rating; while the last two methods adopt two components which are not effective~\\cite{HFT,LOCABAL}. Hence, it is still a challenge to find an effective way to integrate multiple data sources for recommendation.\n\nIn this paper, we investigate the effectiveness of fusing social relations and review texts to rating prediction in a novel way, inspired by the complementarity of the two independent sources for recommendation. The core idea is the alignment between latent factors found by Social MF and topics found by Topic MF. Our main contributions are outlined as follows.\n\\begin{itemize}\n\\item {Providing a principled way to exploit ratings and social relations tightly for recommendation, where the tightness means exploiting the graph structure of neighbors;}\n\\item {Proposing an effective framework MR3 to jointly model ratings, the social network, and item reviews for rating prediction, where the effectiveness means adopting two effective components in some sense;}\n\\item {Evaluating the proposed model extensively on two real-world datasets to understand its performance.}\n\\end{itemize}\n\nThe organization of this paper is as follows. Problem setting and notations are given in Section \\ref{paper:Setting}. In Section \\ref{paper:MR3}, we present the two components and details of the proposed framework. In Section \\ref{paper:Exp}, we give empirical results on real-life datasets. Concluding remarks with a discussion of some future work are in the final section.\n\n\\section{Problem Statement and Notation}\\label{paper:Setting}\nSuppose there are $I$ users $\\mathcal{U}=\\{u_1,...,u_I\\}$ and $J$ items $\\mathcal{V}=\\{v_1,...,v_J\\}$. Let $R \\in \\mathbb{R}^{I \\times J}$ denote the rating matrix, where $R_{i,j}$ is the rating of user $i$ on item $j$, and we mark a zero if it is unknown. The task of rating prediction is to predict missing ratings from the observed data. Latent factors CF methods like {\\em probabilistic matrix factorization} (PMF)~\\cite{PMF} exploit ratings for recommender.\n\nUsers connect to others in a social network. We use $T \\in \\mathbb{R}^{I \\times I}$ to indicate the user-user social relations; $T_{i,k}$ = 1 if user $i$ has a relation to user $k$ or zero otherwise. Social MF methods like {\\em social recommendation} (SoRec)~\\cite{SoRec} and {\\em local and global} (LOCABAL)~\\cite{LOCABAL} integrate social relations for recommender.\n\nItems have content information, e.g., reviews commented by users. The observed data $d_{i,j}$ is the review of item $j$ written by user $i$, often along with a rating score $R_{i,j}$. Topic MF methods like {\\em collaborative topic regression} (CTR)~\\cite{CTR} and {\\em hidden factors and topics} (HFT)~\\cite{HFT} integrate item content for recommender.\n\nBoth Social MF and Topic MF ignore some useful data sources, either item reviews or social relations. Notations used in this paper are described in Table~\\ref{table:notation}.\n\\begin{table}\n\\centering\n\\begin{tabular}{l l}\n \\Xhline{2\\arrayrulewidth}\n Symbols & Meanings \\\\\n \\hline\n $F$        & dimensionality of latent factors/topics \\\\\n $R_{i,j}$  & rating of item $j$ by user $i$  \\\\\n $U_i$      & $F$-dimensional features for user $i$\\\\\n $V_j$      & $F$-dimensional features for item $j$\\\\\n $W_{i,j}$  & weight on the rating of item $j$ given by user $i$\\\\\n $T_{i,k}$  & social relation between user $i$ and $k$ \\\\\n $C_{i,k}$  & social strength between user $i$ and $k$ \\\\\n $S_{i,k}$  & social rating similarity between user $i$ and  $k$ \\\\\n $H$        & $F \\times F$-dimensional social correlation matrix\\\\\n $d_{i,j}$  & review (`document') of item $j$ by user $i$\\\\\n $w_{d,n}$; $z_{d,n}$ & the $n^{\\mathrm{th}}$ word in doc $d$; corresponding topic\\\\\n $\\theta_j$ & $F$-dimensional topic distribution for item $j$ \\\\\n $\\phi_f$   & word distribution for topic $f$ \\\\\n \\Xhline{2\\arrayrulewidth}\n\\end{tabular}\n\\caption{Notations}\n\\label{table:notation}\n\\end{table}\n\n\\section{The Proposed Framework}\\label{paper:MR3}\n\n\\subsection{Matrix Factorization: A Basic Model}\nRating scores are the explicit user feedback and matrix factorization (MF) is a state-of-the-art recommender method to exploit this rating information. MF techniques have gained popularity and become the standard recommender approaches due to their accuracy and scalability~\\cite{koren09:MF}. They have probabilistic interpretation with Gaussian noise and are very flexible to add side data sources for recommender such as reviews content and social relations introduced in the following subsections. We adopt MF as a basic part of the proposed framework.\n\nMF based RSs are mainly to find the latent user-specific matrix $U=[U_1,...,U_I] \\in \\mathbb{R}^{F \\times I}$ and item-specific matrix $V=[V_1,...,V_J] \\in \\mathbb{R}^{F \\times J}$, where $F$ is the number of latent factors, obtained by solving the following problem\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eq:rating}\n\\min_{U,V} \\sum\\nolimits_{R_{i,j} \\neq 0} {(R_{i,j} - \\hat R_{i,j})}^2 + \\lambda ({\\ensuremath{\\lVert{{U}}\\rVert}}_F^2 + {\\ensuremath{\\lVert{{V}}\\rVert}}_F^2),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\min_{U,V}\\sum\\nolimits_{R_{i,j}\\neq 0}{(R_{i,j}-\\hat{R}_{i,j})}^{2}+\\lambda({%&#10;\\lVert{{U}}\\rVert}_{F}^{2}+{\\lVert{{V}}\\rVert}_{F}^{2}),\" display=\"block\"><mrow><mrow><mrow><munder><mi>min</mi><mrow><mi>U</mi><mo>,</mo><mi>V</mi></mrow></munder><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></msub><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mrow><mo fence=\"true\" stretchy=\"false\">\u2225</mo><mi>U</mi><mo fence=\"true\" stretchy=\"false\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mrow><mo fence=\"true\" stretchy=\"false\">\u2225</mo><mi>V</mi><mo fence=\"true\" stretchy=\"false\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\nand regularization parameter $\\lambda$ controls over-fitting. The rating mean is captured by $\\mu$; $b_i$ and $b_j$ are rating biases of $u_i$ and of $v_j$. The $F$-dimensional feature vectors $U_i$ and $V_j$ represent preferences for user $i$ and characteristics for item $j$, respectively. The dot products $U_i^{\\textrm T} V_j$ capture the interaction or match degree between users and items.\n\n\\subsection{Topic MF: Integrating Rating with Review}\nItem reviews generated by users provide implicit feedback for recommender beyond explicit ratings~\\cite{ganu09:review,TopicMF}. Reviews explain the ratings of users, thus help to understand the rating behavior of users, and alleviate the cold-item problem. On the one hand, item characteristics (i.e., factors) are latent in ratings, and can be found by MF introduced in Eq.(\\ref{eq:rating}); on the other hand, item properties (i.e., topics) are hidden in reviews, and can be found by topic models like {\\em latent Dirichlet allocation} (LDA)~\\cite{LDA}. Together, these intuitions were sharpened into the HFT model~\\cite{HFT}.\n\nThe HFT model combines ratings with reviews by minimizing the following problem\n\n", "itemtype": "equation", "pos": 9462, "prevtext": "\nwhere the predicted ratings\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eq:pred}\n\\hat R_{i,j} = \\mu + b_i + b_j + U_i^{\\textrm T} V_j,\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\hat{R}_{i,j}=\\mu+b_{i}+b_{j}+U_{i}^{\\textrm{T}}V_{j},\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>\u03bc</mi><mo>+</mo><msub><mi>b</mi><mi>i</mi></msub><mo>+</mo><msub><mi>b</mi><mi>j</mi></msub><mo>+</mo><mrow><msubsup><mi>U</mi><mi>i</mi><mtext>T</mtext></msubsup><mo>\u2062</mo><msub><mi>V</mi><mi>j</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\nwhere the LDA parameters $\\theta$ and $\\phi$ denote the topic and word distributions, respectively; $w_{d,n}$ and $z_{d,n}$ are the $n^{\\mathrm{th}}$ word occurring in doc $d$ and the corresponding topic; and $\\lambda$ controls the contribution from reviews content. Summation in the second term is over all documents and each word within.\n\nThe goals to achieve are both modeling ratings accurately and generating reviews likely. The trick of fusing ratings and reviews is the transformation\n\\begin{dmath}\n\\label{eq:tran}\n\\theta_{j,f} = \\frac {\\exp (\\kappa V_{j,f})} {\\sum_f \\exp (\\kappa V_{j,f})},\n\\end{dmath}\nwhere the parameter $\\kappa$ is introduced to control the `peakiness' of the transform and the summation is with respect to the $F$ latent topics/factors. The above function transforms the real-valued parameters $V_j \\in \\mathbb{R}^F$ associated with ratings to the probabilistic ones $\\theta_j \\in \\Delta^F$ associated with reviews. The fusing trick works because if an item exhibits a certain property, it corresponds to some topic being commented by users. We adopt HFT as a component of the proposed framework.~\\footnote{As the same with HFT, we aggregate all reviews of a particular item as a `doc'; so the item index $j$ is corresponding to doc index $j$.}\n\n\n\\subsection{Social MF: Integrating Rating with Relation}\\label{paper:smf}\nSocial relations among users provide additional information for recommender~\\cite{trustRS,tranMF}. On the one hand, social correlation theories~\\cite{socialmedia} including homophily and social influence indicate that the rating behavior of users is correlated with their social factors hidden in the social network, besides their preference factors hidden in the rating matrix. On the other hand, the reputation of a user in the social network reveals her rating confidence, and a consideration from a global perspective can alleviate the rating noise to some extent. Together, these ideas were formulated in LOCABAL~\\cite{LOCABAL}.\n\nThe LOCABAL model combines ratings with social relations to achieve the goals of modeling ratings accurately and capturing local social context by solving the problem\n\\begin{dmath}\n\\label{eq:locabal}\n\\min_{U,V,H} \\sum\\nolimits_{R_{i,j} \\neq 0} W_{i,j} {(R_{i,j} - \\hat R_{i,j})}^2 + \\lambda \\sum\\nolimits_{T_{i,k} \\neq 0} {(S_{i,k} - U_i^{\\mathrm{T}} H U_k)}^2 + \\lambda \\Omega (\\Theta),\n\\end{dmath}\nwhere the rating weight $W_{i,j} = 1/(1+\\log r_i)$ is computed from the PageRank score $r_i$ of user $i$ in the social network, representing the global perspective of social context; $S_{i,k}$ is the cosine similarity between rating vectors of user $i$ and $k$; $H \\in \\mathbb{R}^{F \\times F}$ is the social correlation matrix, capturing the user preference correlation; $\\lambda$ controls the contribution from social relations; and the regularization term is given by\n\n", "itemtype": "equation", "pos": 10725, "prevtext": "\nand regularization parameter $\\lambda$ controls over-fitting. The rating mean is captured by $\\mu$; $b_i$ and $b_j$ are rating biases of $u_i$ and of $v_j$. The $F$-dimensional feature vectors $U_i$ and $V_j$ represent preferences for user $i$ and characteristics for item $j$, respectively. The dot products $U_i^{\\textrm T} V_j$ capture the interaction or match degree between users and items.\n\n\\subsection{Topic MF: Integrating Rating with Review}\nItem reviews generated by users provide implicit feedback for recommender beyond explicit ratings~\\cite{ganu09:review,TopicMF}. Reviews explain the ratings of users, thus help to understand the rating behavior of users, and alleviate the cold-item problem. On the one hand, item characteristics (i.e., factors) are latent in ratings, and can be found by MF introduced in Eq.(\\ref{eq:rating}); on the other hand, item properties (i.e., topics) are hidden in reviews, and can be found by topic models like {\\em latent Dirichlet allocation} (LDA)~\\cite{LDA}. Together, these intuitions were sharpened into the HFT model~\\cite{HFT}.\n\nThe HFT model combines ratings with reviews by minimizing the following problem\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eq:rev}\n\\sum_{R_{i,j} \\neq 0} {(R_{i,j} - \\hat R_{i,j})}^2 - \\lambda \\sum_{d=1}^J \\sum_{n \\in N_d} \\log \\theta_{z_{d,n}} \\phi_{z_{d,n},w_{d,n}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\sum_{R_{i,j}\\neq 0}{(R_{i,j}-\\hat{R}_{i,j})}^{2}-\\lambda\\sum_{d=1}^{J}\\sum_{n%&#10;\\in N_{d}}\\log\\theta_{z_{d,n}}\\phi_{z_{d,n},w_{d,n}}\" display=\"block\"><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></munder><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo>-</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>\u2208</mo><msub><mi>N</mi><mi>d</mi></msub></mrow></munder><mrow><mi>log</mi><mo>\u2061</mo><mrow><msub><mi>\u03b8</mi><msub><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub></msub><mo>\u2062</mo><msub><mi>\u03d5</mi><mrow><msub><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub></mrow></msub></mrow></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\n\\noindent\n\\textbf{eSMF.\\quad} While LOCABAL succeeded in integrating ratings with social relations for recommender from local and global perspectives, it can be further improved by exploiting the graph structure of neighbors. Graph structure of neighbors captures social influence locality~\\cite{localInfluence}, in other words, user behaviors are mainly influenced by direct friends in their ego networks. We employ the trust values used in SoRec~\\cite{SoRec} to exploit this structure, and propose the {\\em extended Social MF} (eSMF) model:\n\\begin{dmath}\n\\label{eq:esmf}\n\\min_{U,V,H} \\sum\\nolimits_{R_{i,j} \\neq 0} W_{i,j} {(R_{i,j} - \\hat R_{i,j})}^2 + \\lambda \\sum\\nolimits_{T_{i,k} \\neq 0} C_{i,k} {(S_{i,k} - U_i^{\\mathrm{T}} H U_k)}^2 + \\lambda \\Omega (\\Theta).\n\\end{dmath}\nThe trust values\n\\begin{dmath}\nC_{ik} = \\sqrt{d^-_{u_k} / (d^+_{u_i} + d^-_{u_k})},\n\\end{dmath}\nwhere the outdegree $d^+_{u_i}$ represents the number of users whom $u_i$ trusts, while the indegree $d^-_{u_k}$ denotes the number of users who trust $u_k$.\n\n\\subsection{MR3: A Model of Rating, Review and Relation}{\\label{paper:mr3}}\n\\begin{figure}\n\\centering\n\\includegraphics[height=4cm,width=3.1in]{depend-graph.pdf}\n\\caption{ {\\em Relationship among matrices of parameters and data.} Shaded nodes are data ($R$: rating matrix, $S$: social rating similarity, and $D$: doc-term matrix of reviews); Others are parameters ($U$: matrix of latent user factors, $V$: matrix of latent item factors, $H$: social correlation matrix, $\\theta$: doc-topic distributions, and $\\phi$: topic-word distributions). Parameters $V$ and $\\theta$ are coupled by Eq.(\\ref{eq:tran}). The double connections between $U$ and $S$ are indicated by the term $(S - U^{\\textrm T}HU)$ in Eq.(\\ref{eq:esmf}).}\n\\label{fig:depend}\n\\end{figure}\n\nSo far, we have described solutions to integrating ratings with reviews (see Eq.(\\ref{eq:rev})) and to integrating ratings with social relations (see Eq.(\\ref{eq:esmf})) based on MF respectively. By aligning latent factors and topics, we propose an effective framework {\\mbox{MR3}} to jointly model ratings with social relations and reviews. MR3 connects Social MF and Topic MF by minimizing the following problem\n\n", "itemtype": "equation", "pos": 13763, "prevtext": "\nwhere the LDA parameters $\\theta$ and $\\phi$ denote the topic and word distributions, respectively; $w_{d,n}$ and $z_{d,n}$ are the $n^{\\mathrm{th}}$ word occurring in doc $d$ and the corresponding topic; and $\\lambda$ controls the contribution from reviews content. Summation in the second term is over all documents and each word within.\n\nThe goals to achieve are both modeling ratings accurately and generating reviews likely. The trick of fusing ratings and reviews is the transformation\n\\begin{dmath}\n\\label{eq:tran}\n\\theta_{j,f} = \\frac {\\exp (\\kappa V_{j,f})} {\\sum_f \\exp (\\kappa V_{j,f})},\n\\end{dmath}\nwhere the parameter $\\kappa$ is introduced to control the `peakiness' of the transform and the summation is with respect to the $F$ latent topics/factors. The above function transforms the real-valued parameters $V_j \\in \\mathbb{R}^F$ associated with ratings to the probabilistic ones $\\theta_j \\in \\Delta^F$ associated with reviews. The fusing trick works because if an item exhibits a certain property, it corresponds to some topic being commented by users. We adopt HFT as a component of the proposed framework.~\\footnote{As the same with HFT, we aggregate all reviews of a particular item as a `doc'; so the item index $j$ is corresponding to doc index $j$.}\n\n\n\\subsection{Social MF: Integrating Rating with Relation}\\label{paper:smf}\nSocial relations among users provide additional information for recommender~\\cite{trustRS,tranMF}. On the one hand, social correlation theories~\\cite{socialmedia} including homophily and social influence indicate that the rating behavior of users is correlated with their social factors hidden in the social network, besides their preference factors hidden in the rating matrix. On the other hand, the reputation of a user in the social network reveals her rating confidence, and a consideration from a global perspective can alleviate the rating noise to some extent. Together, these ideas were formulated in LOCABAL~\\cite{LOCABAL}.\n\nThe LOCABAL model combines ratings with social relations to achieve the goals of modeling ratings accurately and capturing local social context by solving the problem\n\\begin{dmath}\n\\label{eq:locabal}\n\\min_{U,V,H} \\sum\\nolimits_{R_{i,j} \\neq 0} W_{i,j} {(R_{i,j} - \\hat R_{i,j})}^2 + \\lambda \\sum\\nolimits_{T_{i,k} \\neq 0} {(S_{i,k} - U_i^{\\mathrm{T}} H U_k)}^2 + \\lambda \\Omega (\\Theta),\n\\end{dmath}\nwhere the rating weight $W_{i,j} = 1/(1+\\log r_i)$ is computed from the PageRank score $r_i$ of user $i$ in the social network, representing the global perspective of social context; $S_{i,k}$ is the cosine similarity between rating vectors of user $i$ and $k$; $H \\in \\mathbb{R}^{F \\times F}$ is the social correlation matrix, capturing the user preference correlation; $\\lambda$ controls the contribution from social relations; and the regularization term is given by\n\n", "index": 7, "text": "\\begin{equation}\n\\Omega (\\Theta)={\\ensuremath{\\lVert{{U}}\\rVert}}_F^2  + {\\ensuremath{\\lVert{{V}}\\rVert}}_F^2 + {\\ensuremath{\\lVert{{H}}\\rVert}}_F^2.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\Omega(\\Theta)={\\lVert{{U}}\\rVert}_{F}^{2}+{\\lVert{{V}}\\rVert}_{F}^{2}+{\\lVert%&#10;{{H}}\\rVert}_{F}^{2}.\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mrow><mo fence=\"true\" stretchy=\"false\">\u2225</mo><mi>U</mi><mo fence=\"true\" stretchy=\"false\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mrow><mo fence=\"true\" stretchy=\"false\">\u2225</mo><mi>V</mi><mo fence=\"true\" stretchy=\"false\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mrow><mo fence=\"true\" stretchy=\"false\">\u2225</mo><mi>H</mi><mo fence=\"true\" stretchy=\"false\">\u2225</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\nwhere parameters $\\Theta = \\{U,V,H\\}$ are associated with ratings and social relations, parameters $\\Phi =\\{\\theta,\\phi\\}$ associated with reviews text; and $\\lambda_{\\mathrm{rel}}$ and $\\lambda_{\\mathrm{rev}}$ are introduced to balance results from social relations and reviews, respectively.\n\nBefore we delve into the learning algorithm, a brief discussion on Eq.(\\ref{eq:mr3}) is in order. On the right hand, the first term is the rating squared-error weighted by user reputation in the social network; the second term is the negative log likelihood of item reviews corpus; the third term is local social context factorization weighted by trust values among users; the last term is Frobenius norm penalty of parameters to control over-fitting. The connection between ratings and social relations is the shared user latent feature space $U$; ratings and reviews are linked through the transformation involving $V$ and $\\theta$ in Eq.(\\ref{eq:tran}). The dependencies among these data and parameter matrices are depicted in Figure \\ref{fig:depend}.\n\n\\noindent\n\\textbf{Learning.\\quad} Our objective is to search\n\\begin{dmath}\n\\label{eq:mr3f}\n{\\operatorname{arg\\,min}}_{\\Theta,\\Phi,z,\\kappa} \\mathcal{L}(\\Theta,\\Phi,z,\\kappa).\n\\end{dmath}\nObserve that parameters $\\Theta$ and $\\Phi$ are coupled (see above paragraph, Eq.(\\ref{eq:tran}), or Figure \\ref{fig:depend}). The former can be found by gradient descent and the latter by Gibbs sampling; so, we design a procedure alternating between following two steps:\n\\begin{subequations}\n\\label{eq:2step}\n\n", "itemtype": "equation", "pos": 16134, "prevtext": "\n\n\\noindent\n\\textbf{eSMF.\\quad} While LOCABAL succeeded in integrating ratings with social relations for recommender from local and global perspectives, it can be further improved by exploiting the graph structure of neighbors. Graph structure of neighbors captures social influence locality~\\cite{localInfluence}, in other words, user behaviors are mainly influenced by direct friends in their ego networks. We employ the trust values used in SoRec~\\cite{SoRec} to exploit this structure, and propose the {\\em extended Social MF} (eSMF) model:\n\\begin{dmath}\n\\label{eq:esmf}\n\\min_{U,V,H} \\sum\\nolimits_{R_{i,j} \\neq 0} W_{i,j} {(R_{i,j} - \\hat R_{i,j})}^2 + \\lambda \\sum\\nolimits_{T_{i,k} \\neq 0} C_{i,k} {(S_{i,k} - U_i^{\\mathrm{T}} H U_k)}^2 + \\lambda \\Omega (\\Theta).\n\\end{dmath}\nThe trust values\n\\begin{dmath}\nC_{ik} = \\sqrt{d^-_{u_k} / (d^+_{u_i} + d^-_{u_k})},\n\\end{dmath}\nwhere the outdegree $d^+_{u_i}$ represents the number of users whom $u_i$ trusts, while the indegree $d^-_{u_k}$ denotes the number of users who trust $u_k$.\n\n\\subsection{MR3: A Model of Rating, Review and Relation}{\\label{paper:mr3}}\n\\begin{figure}\n\\centering\n\\includegraphics[height=4cm,width=3.1in]{depend-graph.pdf}\n\\caption{ {\\em Relationship among matrices of parameters and data.} Shaded nodes are data ($R$: rating matrix, $S$: social rating similarity, and $D$: doc-term matrix of reviews); Others are parameters ($U$: matrix of latent user factors, $V$: matrix of latent item factors, $H$: social correlation matrix, $\\theta$: doc-topic distributions, and $\\phi$: topic-word distributions). Parameters $V$ and $\\theta$ are coupled by Eq.(\\ref{eq:tran}). The double connections between $U$ and $S$ are indicated by the term $(S - U^{\\textrm T}HU)$ in Eq.(\\ref{eq:esmf}).}\n\\label{fig:depend}\n\\end{figure}\n\nSo far, we have described solutions to integrating ratings with reviews (see Eq.(\\ref{eq:rev})) and to integrating ratings with social relations (see Eq.(\\ref{eq:esmf})) based on MF respectively. By aligning latent factors and topics, we propose an effective framework {\\mbox{MR3}} to jointly model ratings with social relations and reviews. MR3 connects Social MF and Topic MF by minimizing the following problem\n\n", "index": 9, "text": "\\begin{multline}\n\\label{eq:mr3}\n\\mathcal{L}(\\Theta,\\Phi,z,\\kappa) \\triangleq \\sum\\nolimits_{R_{i,j} \\neq 0} W_{i,j} {(R_{i,j} - \\hat R_{i,j})}^2 \\\\\n- \\lambda_{\\mathrm{rev}} \\sum\\nolimits_{d=1}^J \\sum\\nolimits_{n \\in N_d} (\\log \\theta_{z_{d,n}} + \\log \\phi_{z_{d,n},w_{d,n}}) \\\\\n+ \\lambda_{\\mathrm{rel}} \\sum\\nolimits_{T_{i,k} \\neq 0} C_{i,k} {(S_{i,k} - U_i^{\\textrm T} H U_k)}^2 + \\lambda \\Omega (\\Theta) ,\n\\end{multline}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\mathcal{L}(\\Theta,\\Phi,z,\\kappa)\\triangleq\\sum\\nolimits_{R_{i,j}%&#10;\\neq 0}W_{i,j}{(R_{i,j}-\\hat{R}_{i,j})}^{2}\\\\&#10;\\displaystyle-\\lambda_{\\mathrm{rev}}\\sum\\nolimits_{d=1}^{J}\\sum\\nolimits_{n\\in&#10;N%&#10;_{d}}(\\log\\theta_{z_{d,n}}+\\log\\phi_{z_{d,n},w_{d,n}})\\\\&#10;\\displaystyle+\\lambda_{\\mathrm{rel}}\\sum\\nolimits_{T_{i,k}\\neq 0}C_{i,k}{(S_{i%&#10;,k}-U_{i}^{\\textrm{T}}HU_{k})}^{2}+\\lambda\\Omega(\\Theta),\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>,</mo><mi>z</mi><mo>,</mo><mi>\u03ba</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u225c</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></msub><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>rev</mi></msub><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></msubsup><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>n</mi><mo>\u2208</mo><msub><mi>N</mi><mi>d</mi></msub></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>\u03b8</mi><msub><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub></msub></mrow><mo>+</mo><mrow><mi>log</mi><mo>\u2061</mo><msub><mi>\u03d5</mi><mrow><msub><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>rel</mi></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><msub><mi>T</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></msub><mrow><msub><mi>C</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>S</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>-</mo><mrow><msubsup><mi>U</mi><mi>i</mi><mtext>T</mtext></msubsup><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><msub><mi>U</mi><mi>k</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><mi mathvariant=\"normal\">\u03a9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 18105, "prevtext": "\nwhere parameters $\\Theta = \\{U,V,H\\}$ are associated with ratings and social relations, parameters $\\Phi =\\{\\theta,\\phi\\}$ associated with reviews text; and $\\lambda_{\\mathrm{rel}}$ and $\\lambda_{\\mathrm{rev}}$ are introduced to balance results from social relations and reviews, respectively.\n\nBefore we delve into the learning algorithm, a brief discussion on Eq.(\\ref{eq:mr3}) is in order. On the right hand, the first term is the rating squared-error weighted by user reputation in the social network; the second term is the negative log likelihood of item reviews corpus; the third term is local social context factorization weighted by trust values among users; the last term is Frobenius norm penalty of parameters to control over-fitting. The connection between ratings and social relations is the shared user latent feature space $U$; ratings and reviews are linked through the transformation involving $V$ and $\\theta$ in Eq.(\\ref{eq:tran}). The dependencies among these data and parameter matrices are depicted in Figure \\ref{fig:depend}.\n\n\\noindent\n\\textbf{Learning.\\quad} Our objective is to search\n\\begin{dmath}\n\\label{eq:mr3f}\n{\\operatorname{arg\\,min}}_{\\Theta,\\Phi,z,\\kappa} \\mathcal{L}(\\Theta,\\Phi,z,\\kappa).\n\\end{dmath}\nObserve that parameters $\\Theta$ and $\\Phi$ are coupled (see above paragraph, Eq.(\\ref{eq:tran}), or Figure \\ref{fig:depend}). The former can be found by gradient descent and the latter by Gibbs sampling; so, we design a procedure alternating between following two steps:\n\\begin{subequations}\n\\label{eq:2step}\n\n", "index": 11, "text": "\\begin{equation}\n\\label{eq:step1}\n \\mbox{update } \\Theta^{\\mathrm{new}}, \\Phi^{\\mathrm{new}},\\kappa^{\\mathrm{new}} = {\\operatorname{arg\\,min}}_{\\Theta,\\Phi,\\kappa} \\mathcal{L}(\\Theta,\\Phi,\\kappa,z^{\\mathrm{old}});\n \\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mbox{update }\\Theta^{\\mathrm{new}},\\Phi^{\\mathrm{new}},\\kappa^{\\mathrm{new}}=%&#10;{\\operatorname{arg\\,min}}_{\\Theta,\\Phi,\\kappa}\\mathcal{L}(\\Theta,\\Phi,\\kappa,z%&#10;^{\\mathrm{old}});\" display=\"block\"><mrow><mrow><mrow><mrow><mtext>update\u00a0</mtext><mo>\u2062</mo><msup><mi mathvariant=\"normal\">\u0398</mi><mi>new</mi></msup></mrow><mo>,</mo><msup><mi mathvariant=\"normal\">\u03a6</mi><mi>new</mi></msup><mo>,</mo><msup><mi>\u03ba</mi><mi>new</mi></msup></mrow><mo>=</mo><mrow><mrow><msub><mrow><mpadded width=\"+1.7pt\"><mi>arg</mi></mpadded><mo>\u2062</mo><mi>min</mi></mrow><mrow><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>,</mo><mi>\u03ba</mi></mrow></msub><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u0398</mi><mo>,</mo><mi mathvariant=\"normal\">\u03a6</mi><mo>,</mo><mi>\u03ba</mi><mo>,</mo><msup><mi>z</mi><mi>old</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>;</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\\end{subequations}\n\nFor the first step Eq.(\\ref{eq:step1}), topic assignments $z_{d,n}$ for each word in reviews corpus are fixed; then we update the terms {$\\Theta,\\Phi$, and $\\kappa$} by gradient descent (GD). Recall that $\\theta$ and $V$ depend on each other; we fit only $V$ and then determine $\\theta$ by Eq.(\\ref{eq:tran}). This is the same as that in the standard gradient-based MF for recommender except that we have to compute more gradients, which will be given later separately.\n\nFor the second step Eq.(\\ref{eq:step2}), parameters associated with reviews corpus $\\theta$ and $\\phi$ are fixed; then we sample topic assignments $z_{d,n}$ by iterating through all docs $d$ and each word within, setting $z_{d,n} = f$ with probability proportion to $\\theta_{d,f} \\phi_{f,w_{d,n}}$. This is similar to updating $z$ via LDA except that topic proportions $\\theta$ are not sampled from a Dirichlet prior, but instead are determined in the first step.\n\nFinally, the two steps are repeated until a local optimum is reached. In practice, we sample topic assignments every 5 GD iterations/epoches and this is called a pass; usually it is enough to run 50 passes to find a local minima.\n\n\\noindent\n\\textbf{Gradients.\\quad} We now give gradients used in Eq.(\\ref{eq:step1}). (Gradients of biases are omitted; rating mean is not fitted because ratings are centered.) More notations are required here~\\cite{Gibbs}. For each item $j$ (i.e. doc $j$): 1) $M_j$ is an $F$-dimensional count vector, in which each component is the number of times each topic occurs for it; 2) $m_j$ is the number of words in it; and 3) $z_j = \\sum\\nolimits_f \\exp{(\\kappa V_{jf})}$ is a normalizer. For each word $w$: 1) $M_w$ is an $F$-dimensional count vector, in which each component is the number of times it has been assigned to each topic; 2) $m_f$ is the number of times topic $f$ occurs; and 3)$z_f = \\sum\\nolimits_w \\exp{(\\psi_{fw})}$ is a normalizer. Note that $\\phi_f$ is a stochastic vector, so we optimize the corresponding unnormalized vector $\\psi_f$ and then get $\\phi_{fw} = \\exp{(\\psi_{fw})}/z_f$.\n\n", "itemtype": "equation", "pos": 18335, "prevtext": "\n\n", "index": 13, "text": "\\begin{equation}\n\\label{eq:step2}\n \\mbox{sample }\\; z_{d,n}^{\\mathrm{new}} \\mbox{ with probability } p(z_{d,n}^{\\mathrm{new}} = f) = \\phi_{f,w_{d,n}}^{\\mathrm{new}}.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\mbox{sample }\\;z_{d,n}^{\\mathrm{new}}\\mbox{ with probability }p(z_{d,n}^{%&#10;\\mathrm{new}}=f)=\\phi_{f,w_{d,n}}^{\\mathrm{new}}.\" display=\"block\"><mrow><mpadded width=\"+2.8pt\"><mtext>sample\u00a0</mtext></mpadded><msubsup><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow><mi>new</mi></msubsup><mtext>\u00a0with probability\u00a0</mtext><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>z</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow><mi>new</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msubsup><mi>\u03d5</mi><mrow><mi>f</mi><mo>,</mo><msub><mi>w</mi><mrow><mi>d</mi><mo>,</mo><mi>n</mi></mrow></msub></mrow><mi>new</mi></msubsup><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\\begin{dmath}\n\\label{eq:grad-V}\n\\frac {\\partial\\mathcal{L}} {\\partial V_j} = 2 \\sum\\nolimits_{i:R_{i,j} \\neq 0} W_{i,j}(\\hat R_{i,j} - R_{i,j})U_i \\\\\n - \\lambda_{\\mathrm{rev}} \\kappa \\Big(M_j - \\frac {m_j} {z_j} \\exp{(\\kappa V_j})\\Big) + 2 \\lambda V_j.\n\\end{dmath}\n\\begin{dmath}\n\\label{eq:grad-H}\n\\frac 1 2 \\frac {\\partial\\mathcal{L}} {\\partial H} = \\lambda_{\\mathrm{rel}} \\sum_{T_{i,k} \\neq 0} C_{i,k}(U_i^{\\mathrm T}HU_k - S_{i,k})U_i U_{k}^{\\mathrm{T}} + \\lambda H .\n\\end{dmath}\n\n", "itemtype": "equation", "pos": 20605, "prevtext": "\n\\end{subequations}\n\nFor the first step Eq.(\\ref{eq:step1}), topic assignments $z_{d,n}$ for each word in reviews corpus are fixed; then we update the terms {$\\Theta,\\Phi$, and $\\kappa$} by gradient descent (GD). Recall that $\\theta$ and $V$ depend on each other; we fit only $V$ and then determine $\\theta$ by Eq.(\\ref{eq:tran}). This is the same as that in the standard gradient-based MF for recommender except that we have to compute more gradients, which will be given later separately.\n\nFor the second step Eq.(\\ref{eq:step2}), parameters associated with reviews corpus $\\theta$ and $\\phi$ are fixed; then we sample topic assignments $z_{d,n}$ by iterating through all docs $d$ and each word within, setting $z_{d,n} = f$ with probability proportion to $\\theta_{d,f} \\phi_{f,w_{d,n}}$. This is similar to updating $z$ via LDA except that topic proportions $\\theta$ are not sampled from a Dirichlet prior, but instead are determined in the first step.\n\nFinally, the two steps are repeated until a local optimum is reached. In practice, we sample topic assignments every 5 GD iterations/epoches and this is called a pass; usually it is enough to run 50 passes to find a local minima.\n\n\\noindent\n\\textbf{Gradients.\\quad} We now give gradients used in Eq.(\\ref{eq:step1}). (Gradients of biases are omitted; rating mean is not fitted because ratings are centered.) More notations are required here~\\cite{Gibbs}. For each item $j$ (i.e. doc $j$): 1) $M_j$ is an $F$-dimensional count vector, in which each component is the number of times each topic occurs for it; 2) $m_j$ is the number of words in it; and 3) $z_j = \\sum\\nolimits_f \\exp{(\\kappa V_{jf})}$ is a normalizer. For each word $w$: 1) $M_w$ is an $F$-dimensional count vector, in which each component is the number of times it has been assigned to each topic; 2) $m_f$ is the number of times topic $f$ occurs; and 3)$z_f = \\sum\\nolimits_w \\exp{(\\psi_{fw})}$ is a normalizer. Note that $\\phi_f$ is a stochastic vector, so we optimize the corresponding unnormalized vector $\\psi_f$ and then get $\\phi_{fw} = \\exp{(\\psi_{fw})}/z_f$.\n\n", "index": 15, "text": "\\begin{multline}\n\\label{eq:grad-U}\n \\frac 1 2 \\frac {\\partial\\mathcal{L}} {\\partial U_i} = \\sum\\nolimits_{j:R_{i,j} \\neq 0} W_{i,j}(\\hat R_{i,j} - R_{i,j})V_j + \\lambda U_i \\\\\n + \\lambda_{\\mathrm{rel}} \\sum\\nolimits_{k:T_{k,i} \\neq 0} C_{i,k}(U_k^{\\mathrm T}HU_i - S_{i,k})H^{\\mathrm T}U_k \\\\\n + \\lambda_{\\mathrm{rel}} \\sum\\nolimits_{k:T_{i,k} \\neq 0} C_{k,i}(U_i^{\\mathrm T}HU_k - S_{i,k})HU_k .\n\\end{multline}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2}\\frac{\\partial\\mathcal{L}}{\\partial U_{i}}=\\sum%&#10;\\nolimits_{j:R_{i,j}\\neq 0}W_{i,j}(\\hat{R}_{i,j}-R_{i,j})V_{j}+\\lambda U_{i}\\\\&#10;\\displaystyle+\\lambda_{\\mathrm{rel}}\\sum\\nolimits_{k:T_{k,i}\\neq 0}C_{i,k}(U_{%&#10;k}^{\\mathrm{T}}HU_{i}-S_{i,k})H^{\\mathrm{T}}U_{k}\\\\&#10;\\displaystyle+\\lambda_{\\mathrm{rel}}\\sum\\nolimits_{k:T_{i,k}\\neq 0}C_{k,i}(U_{%&#10;i}^{\\mathrm{T}}HU_{k}-S_{i,k})HU_{k}.\" display=\"block\"><mtable displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd columnalign=\"left\"><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mfrac></mrow><mo>=</mo><mrow><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>:</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></mrow></msub><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>V</mi><mi>j</mi></msub></mrow></mrow><mo>+</mo><mrow><mi>\u03bb</mi><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>rel</mi></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>:</mo><mrow><msub><mi>T</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></mrow></msub><mrow><msub><mi>C</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>U</mi><mi>k</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><msub><mi>U</mi><mi>i</mi></msub></mrow><mo>-</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mi>H</mi><mi mathvariant=\"normal\">T</mi></msup><mo>\u2062</mo><msub><mi>U</mi><mi>k</mi></msub></mrow></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign=\"right\"><mrow><mrow><mo>+</mo><mrow><msub><mi>\u03bb</mi><mi>rel</mi></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>k</mi><mo>:</mo><mrow><msub><mi>T</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>\u2260</mo><mn>0</mn></mrow></mrow></msub><mrow><msub><mi>C</mi><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msubsup><mi>U</mi><mi>i</mi><mi mathvariant=\"normal\">T</mi></msubsup><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><msub><mi>U</mi><mi>k</mi></msub></mrow><mo>-</mo><msub><mi>S</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><msub><mi>U</mi><mi>k</mi></msub></mrow></mrow></mrow></mrow><mo>.</mo></mrow></mtd></mtr></mtable></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\n", "itemtype": "equation", "pos": 21499, "prevtext": "\n\\begin{dmath}\n\\label{eq:grad-V}\n\\frac {\\partial\\mathcal{L}} {\\partial V_j} = 2 \\sum\\nolimits_{i:R_{i,j} \\neq 0} W_{i,j}(\\hat R_{i,j} - R_{i,j})U_i \\\\\n - \\lambda_{\\mathrm{rev}} \\kappa \\Big(M_j - \\frac {m_j} {z_j} \\exp{(\\kappa V_j})\\Big) + 2 \\lambda V_j.\n\\end{dmath}\n\\begin{dmath}\n\\label{eq:grad-H}\n\\frac 1 2 \\frac {\\partial\\mathcal{L}} {\\partial H} = \\lambda_{\\mathrm{rel}} \\sum_{T_{i,k} \\neq 0} C_{i,k}(U_i^{\\mathrm T}HU_k - S_{i,k})U_i U_{k}^{\\mathrm{T}} + \\lambda H .\n\\end{dmath}\n\n", "index": 17, "text": "\\begin{equation}\n\\label{eq:grad-phi}\n\\frac {\\partial\\mathcal{L}} {\\partial \\psi_{fw}} = - \\lambda_{\\mathrm{rev}} \\Big(M_{fw} - \\frac {m_f} {z_f} \\exp{(\\psi_{fw}})\\Big).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\mathcal{L}}{\\partial\\psi_{fw}}=-\\lambda_{\\mathrm{rev}}\\Big{(}M_%&#10;{fw}-\\frac{m_{f}}{z_{f}}\\exp{(\\psi_{fw}})\\Big{)}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msub><mi>\u03c8</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>w</mi></mrow></msub></mrow></mfrac><mo>=</mo><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>rev</mi></msub><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msub><mi>M</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo>-</mo><mrow><mfrac><msub><mi>m</mi><mi>f</mi></msub><msub><mi>z</mi><mi>f</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\u03c8</mi><mrow><mi>f</mi><mo>\u2062</mo><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\n\n\n\\section{Experiments}\\label{paper:Exp}\nIn this section, we first evaluate our proposed eSMF component to show the benefit of exploiting the graph structure of neighbors. Then we demonstrate the effectiveness of our proposed MR3 model compared with the individual components. Finally we analyze the contribution of each component of data source to the proposed model, followed by sensitivity of MR3 to hyperparameters.\n\\subsection{Datasets and Metric}\nWe evaluate our models on two datasets: Epinions and Ciao.\\footnote{ \\url{http://www.public.asu.edu/~jtang20/} } They are both knowledge sharing and review sites, in which users can rate items, connect to others, and give reviews on products. We remove stop words\\footnote{\\url{http://www.ranks.nl/stopwords}} and then select top $L$ = 8000 frequent words as vocabulary; we remove users and items that occur only once or twice. The items indexed in the rating matrix are aligned to documents in the doc-term matrix, that is, we aggregate all reviews of a particular item as a `doc'. Statistics of datasets are given in Table~\\ref{table:data}. We see that the rating matrices of both datasets are very sparse, and the average length of documents is short on Epinions.\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{l c c}\n \\Xhline{2\\arrayrulewidth}\n    Statistics          & Epinions  & Ciao \\\\\n \\hline \\hline\n \\# of Users            & 49,454    & 7,340\\\\\n \\# of Items            & 74,154    & 22,472\\\\\n \\# of Ratings/Reviews  & 790,940   & 183,974\\\\\n \\# of Social Relations & 434,680   & 112,942\\\\\n \\# of Words            & 2,246,837 & 28,874,000\\\\\n Rating Density         & 0.00022   & 0.0011\\\\\n Social Density         & 0.00018   & 0.0021\\\\\n Ave. Words Per Item    & 30.3      & 1284.9\\\\\n \\Xhline{2\\arrayrulewidth}\n\\end{tabular}\n\\caption{Statistics of the Two Datasets}\n\\label{table:data}\n\\end{table}\n\nWe randomly select $x$\\% as the training set and report the prediction performance on the remaining 1 - $x$\\% testing set. The metric {\\em root-mean-square error} (RMSE) for rating prediction task is defined as\n\n", "itemtype": "equation", "pos": 21683, "prevtext": "\n\n", "index": 19, "text": "\\begin{equation}\n\\label{eq:grad-kappa}\n\\frac {\\partial\\mathcal{L}} {\\partial \\kappa} = - \\lambda_{\\mathrm{rev}} \\sum\\nolimits_{j,f} V_{jf} \\Big(M_{jf} - \\frac{m_j}{z_j} \\exp{(\\kappa V_{jf}})\\Big).\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial\\mathcal{L}}{\\partial\\kappa}=-\\lambda_{\\mathrm{rev}}\\sum%&#10;\\nolimits_{j,f}V_{jf}\\Big{(}M_{jf}-\\frac{m_{j}}{z_{j}}\\exp{(\\kappa V_{jf}})%&#10;\\Big{)}.\" display=\"block\"><mrow><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><mi>\u03ba</mi></mrow></mfrac><mo>=</mo><mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><mi>rev</mi></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><msub><mi>V</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>f</mi></mrow></msub><mo>\u2062</mo><mrow><mo maxsize=\"160%\" minsize=\"160%\">(</mo><mrow><msub><mi>M</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>f</mi></mrow></msub><mo>-</mo><mrow><mfrac><msub><mi>m</mi><mi>j</mi></msub><msub><mi>z</mi><mi>j</mi></msub></mfrac><mo>\u2062</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\u03ba</mi><mo>\u2062</mo><msub><mi>V</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>f</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo maxsize=\"160%\" minsize=\"160%\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02327.tex", "nexttext": "\nwhere $\\mathcal{T}$ and $|\\mathcal{T}|$ is the test set and its cardinality. A smaller RMSE means a better prediction performance.\n\n\\subsection{Comparing Social MF Methods}\n\\begin{figure}[h]\n\\centering\n\\subfigure{ \\includegraphics[height=3.2cm,width=1.6in]{eSMF.pdf} }\n\\subfigure{ \\includegraphics[height=3.2cm,width=1.6in]{eSMF-Ciao.pdf} }\n\\caption{ {\\em Comparisons of eSMF and LOCABAL on two datasets.} Left: Epinions; Right: Ciao.}\n\\label{fig:eSMF}\n\\end{figure}\n\nWe first compare the eSMF method introduced in Subsec~\\ref{paper:smf} with LOCABAL~\\cite{LOCABAL}, a recent Social MF method. The motivation for the comparison is two-fold: 1) to demonstrate that exploiting ratings and social relations more tightly can further improve the performance of social RSs; 2) to form a nice component of the framework {\\mbox{MR3}}, which we will evaluate in the following subsection.\n\nWe use grid search to determine hyperparameters and report the best RMSE on the testing set over 50 passes (the same routing for comparing MR3 below). For both eSMF and LOCABAL, the number of latent factors $F = 10$, norm penalty $\\lambda = 0.5$, learning rate = 0.0007, momentum = 0.8, and $\\lambda_{\\mathrm{rel}} = 0.1$. Parameters $\\Theta= \\{U,V,H\\}$ are randomly initialized from $\\mathcal{N}(0,0.01)$. The results are given in Figure~\\ref{fig:eSMF}, with varying percentage of the training set = \\{20, 30, 40, 50, 60, 70, 80, 90, 99\\} and we have the following observation:\n\\begin{itemize}\n\\item {Exploiting ratings and social relations tightly can further improve recommender performance in terms of RMSE on both datasets. For example, eSMF obtains 1.18\\%, 0.89\\%, and 0.72\\% relative improvement on Epinions with 20\\%, 50\\%, and 70\\% as the training set respectively.}\n\\end{itemize}\n\n\\subsection{Comparing Different Recommender Systems}\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{ccccccc|ccc}\n\\hline \\hline\n\\multicolumn{1}{c}{\\multirow{2}{*}{Datasets}} & \\multirow{2}{*}{Training} & \\multicolumn{5}{c|}{Methods}   & \\multicolumn{3}{c}{Improvement of MR3 vs. } \\\\ \\cline{3-7} \\cline{8-10}\n\\multicolumn{1}{c}{}                          &        & Mean   & PMF    & HFT    & LOCABAL & MR3    & PMF          & HFT          & LOCABAL      \\\\\n\\hline \\hline\n\\multirow{4}{*}{Epinions} & 20\\% & 1.2265 & 1.2001 & 1.1857 & 1.1222  & 1.1051 & 8.60\\% & 7.29\\% & 1.55\\%\\\\\n                          & 50\\% & 1.2239 & 1.1604 & 1.1323 & 1.1055  & 1.0809 & 7.35\\% & 4.76\\% & 2.28\\%\\\\\n                          & 80\\% & 1.2225 & 1.1502 & 1.0960 & 1.0892  & 1.0648 & 8.02\\% & 2.93\\% & 2.29\\%\\\\\n                          & 90\\% & 1.2187 & 1.1484 & 1.0867 & 1.0840  & 1.0634 & 7.99\\% & 2.19\\% & 1.94\\%\\\\\n\\hline \\hline\n\\multirow{4}{*}{Ciao}   & 20\\% & 1.1095 & 1.0877 & 1.0439 & 1.0287  & 1.0142 & 7.25\\% & 2.93\\% & 1.43\\%\\\\\n                        & 50\\% & 1.0964 & 1.0536 & 1.0379 & 0.9930  & 0.9740 & 8.17\\% & 6.56\\% & 1.95\\%\\\\\n                        & 80\\% & 1.0899 & 1.0418 & 0.9958 & 0.9709  & 0.9521 & 9.42\\% & 4.59\\% & 1.97\\%\\\\\n                        & 90\\% & 1.0841 & 1.0391 & 0.9644 & 0.9587  & 0.9451 & 9.95\\% & 2.04\\% & 1.44\\%\\\\\n\\hline \\hline\n{Average} & & & & & & & 8.34\\% & 4.16\\% & 1.86\\% \\\\\n\\hline \\hline\n\\end{tabular}\n\\captionsetup{justification=centering}\n\\caption{RMSE Comparisons of Different Methods ($F = 10$)}\n\\label{table:mr3}\n\\end{table*}\n\nIn this subsection, we compare the proposed framework MR3 introduced in Subsec~\\ref{paper:mr3} with the following baselines:\n\n\\textbf{Mean.\\quad} This method predicts the rating always using the average, i.e. $\\mu$ in Eq.(\\ref{eq:pred}), across all training ratings. This is the best constant predictor in terms of RMSE.\n\n\\textbf{PMF.\\quad} This method performs matrix factorization on rating matrix as shown in Eq.(\\ref{eq:rating})~\\cite{PMF}. It only uses the rating source.\n\n\\textbf{HFT.\\quad} This method combines latent factors in ratings with hidden topics in reviews as shown in Eq.(\\ref{eq:rev})~\\cite{HFT}. It only uses ratings and reviews.\n\n\\textbf{LOCABAL.\\quad} This method is based on matrix factorization and exploits local and global social context as shown in Eq.(\\ref{eq:locabal})~\\cite{LOCABAL}. It only uses ratings and relations.\n\nWe use the source code PMF\\footnote{\\url{http://www.cs.toronto.edu/~rsalakhu/}} and HFT\\footnote{\\url{http://cseweb.ucsd.edu/~jmcauley/}}. For all methods, we set the number of latent factors $F = 10$, norm penalty $\\lambda = 0.5$, learning rate = 0.0007, momentum = 0.8. For HFT, $\\lambda_{\\mathrm{rev}} = 0.1$; for MR3, $\\lambda_{\\mathrm{rel}} = 0.001$ and $\\lambda_{\\mathrm{rev}} = 0.05$. More details about the sensitivity to parameters of MR3 will be discussed later. The results of the comparison are summarized in Table~\\ref{table:mr3} and we have the following observations.\n\\begin{itemize}\n\\item {Exploiting social relations and reviews beyond ratings can significantly improve recommender performance in terms of RMSE on both datasets. For example, HFT and LOCABAL obtain 4.95\\% and 5.60\\% relative improvement compared with PMF on Epinions with 80\\% as the training set respectively.}\n\\item {Our proposed framework MR3 always achieves the best result. Compared with HFT and LOCABAL, MR3 averagely gains 0.0466 and 0.0217 absolute RMSE improvement on Epinions and 0.0392 and 0.0165 on Ciao respectively. The main reason is that MR3 jointly models all three types of information. The contribution from each data source to MR3 is discussed in the following subsection.}\n\\end{itemize}\n\n\\subsection{Impact of Social Relations and Reviews}\n\\begin{figure}[h]\n\\centering\n\\subfigure{ \\includegraphics[height=3.4cm,width=1.6in]{MR3_Component.pdf} }\n\\subfigure{ \\includegraphics[height=3.4cm,width=1.6in]{MR3_Component-Ciao.pdf} }\n\\caption{ {\\em Predictive performance of MR3 compared with its three components.} Left: Epinions; Right: Ciao.}\n\\label{fig:component}\n\\end{figure}\n\nWe have shown the effectiveness of integrating ratings with social relations and reviews in our proposed framework MR3. We now investigate the contribution of each data source to the MR3 by eliminating the impact of social relations and reviews from it in turn:\n\n\\textbf{MR3$\\backslash$content:\\quad} Eliminating the impact of reviews by setting $\\lambda_{\\mathrm{rev}} = 0$ in Eq.(\\ref{eq:mr3}), which is equivalent to eSMF as shown in Eq.(\\ref{eq:esmf}).\n\n\\textbf{MR3$\\backslash$social:\\quad} Eliminating the impact of social relations by setting $\\lambda_{\\mathrm{rel}} = 0$ in Eq.(\\ref{eq:mr3}), which is equivalent to HFT as shown in Eq.(\\ref{eq:rev}).\n\n\\textbf{MR3$\\backslash$content$\\backslash$social:\\quad} Eliminating the impact of both reviews and social relations by setting $\\lambda_{\\mathrm{rev}} = 0$ and $\\lambda_{\\mathrm{rel}} = 0$ in Eq.(\\ref{eq:mr3}), which is equivalent to PMF as shown in Eq.(\\ref{eq:rating}).\n\nThe predictive results of MR3 and its three components are shown in Figure~\\ref{fig:component}. The performance degrades when either social relations or reviews are eliminated. In detail, {\\em \\mbox{MR3$\\backslash$content}}, {\\em \\mbox {MR3$\\backslash$social}}, and {\\em \\mbox{MR3$\\backslash$content$\\backslash$social}} averagely reduce 1.19\\%, 4.29\\%, and 7.99\\% relative RMSE performance on Epinions respectively, suggesting that both reviews and social relations contain essential information for recommender.\n\n\\subsection{Sensitivity to Parameters: $F$, $\\lambda_{\\mathrm{rel}}$ and $\\lambda_{\\mathrm{rev}}$}\nThe framework MR3 has three important hyperparameter: 1) the number of latent factors $F$; 2) the $\\lambda_{\\mathrm{rev}}$ that controls the contribution from reviews; and 3) the $\\lambda_{\\mathrm{rel}}$ that controls the contribution from social relations. We investigate the sensitivity of MR3 to these parameters by varying one of them while fixing the other two.\n\nFirst, we fix $\\lambda_{\\mathrm{rel}} = 0.001$ and $\\lambda_{\\mathrm{rev}} = 0.05$, and vary the number of latent factors $F = \\{5, 10, 15, 20, 30, 50, 70, 100\\}$ with 20\\%, 50\\%, 80\\% as the training set respectively. As shown in Figure~\\ref{fig:mr3k}, MR3 is relatively stable and not sensitive to $F$, so we choose the reasonable value 10 as default.\n\n\\begin{figure}[!h]\n\\centering\n\\subfigure{\\includegraphics[height=3.4cm,width=1.66in]{MR3-K.pdf}}\n\\subfigure{\\includegraphics[height=3.4cm,width=1.66in]{MR3-K-Ciao.pdf}}\n\\caption{ {\\em Predictive performance of MR3 by varying the number of latent factors $F$.} Fixing $\\lambda_{\\mathrm{rel}} = 0.001$ and $\\lambda_{\\mathrm{rev}} = 0.05$. Left: Epinions; Right: Ciao. }\n\\label{fig:mr3k}\n\\end{figure}\n\nNext, we fix $F = 10$ and study how the reviews associated hyperparameter $\\lambda_{\\mathrm{rev}}$ and the social relations associated one $\\lambda_{\\mathrm{rel}}$ affect the whole performance of MR3.  As shown in Figure~\\ref{fig:mr3relu}, we have some observations: 1) the prediction performance degrades when either $\\lambda_{\\mathrm{rel}} = 0$ or $\\lambda_{\\mathrm{rev}} = 0$; (RMSE is 1.1502 when both are zero.) 2) MR3 is relatively stable and not sensitive to $\\lambda_{\\mathrm{rel}} $ and $\\lambda_{\\mathrm{rev}}$ when they are small (e.g., from 0.0001 to 0.1), so we choose the reasonable values 0.001 and 0.05 for them respectively.\n\n\\section{Conclusion and Future Work}\n\\begin{figure}[h]\n\\centering\n\\includegraphics[height=5cm,width=2.5in]{MR3-two-regus-together.pdf}\n\\caption{ {\\em Predictive performance of MR3 by varying $\\lambda_{\\mathrm{rel}}$ and $\\lambda_{\\mathrm{rev}}$.} Both vary in \\{0, 0.001, 0.005, 0.01, 0.05, 0.1\\}. RMSE is 1.1502 when both are zero. Fixing $F$ = 10. Percent of training set = 80. Dataset: Epinions. }\n\\label{fig:mr3relu}\n\\end{figure}\nHeterogenous recommending information sources beyond explicit ratings like social relations and item reviews present both opportunities and challenges for conventional recommender systems. We investigate how to fuse these three kinds of information tightly and effectively for recommendation. A unified framework \\mbox{MR3} by aligning latent factors and topics is proposed to perform social matrix factorization and topic matrix factorization simultaneously for effective rating prediction. Empirical results on real-world datasets show that our proposed model leads to improved predictive performance. Further experiments are designed to see the impact of each of the data sources.\n\nThe proposed model has some limitations which provide interesting directions for future work. Typically, the number of hidden topics in reviews is less than that of latent factors in ratings; therefore the assumption that these two are equal in the current model is inappropriate~\\cite{JMARS}. As inclination of users, popularity of items, and structure of the social network constantly change, integrating temporal dynamics into MR3 is worth channeling. Integrating Implicit feedback should further improve the performance~\\cite{SVDPP}. Recently, deep neural networks (i.e., deep learning) have been used to learn better representation of both items' characteristics and content for recommendation~\\cite{dl4rs}, so the issue of integrating them into MR3 framework as a nicer component is also interesting.\n\n\\newpage\n\\section*{Acknowledgement}\nWe thank Jiliang Tang for providing datasets. The work was supported by NSFC (61472183, 61333014) and 863 program(2015AA015406).\n\n\n\\bibliographystyle{named}\n\\bibliography{ijcai15}\n\n\n", "itemtype": "equation", "pos": 23960, "prevtext": "\n\n\n\\section{Experiments}\\label{paper:Exp}\nIn this section, we first evaluate our proposed eSMF component to show the benefit of exploiting the graph structure of neighbors. Then we demonstrate the effectiveness of our proposed MR3 model compared with the individual components. Finally we analyze the contribution of each component of data source to the proposed model, followed by sensitivity of MR3 to hyperparameters.\n\\subsection{Datasets and Metric}\nWe evaluate our models on two datasets: Epinions and Ciao.\\footnote{ \\url{http://www.public.asu.edu/~jtang20/} } They are both knowledge sharing and review sites, in which users can rate items, connect to others, and give reviews on products. We remove stop words\\footnote{\\url{http://www.ranks.nl/stopwords}} and then select top $L$ = 8000 frequent words as vocabulary; we remove users and items that occur only once or twice. The items indexed in the rating matrix are aligned to documents in the doc-term matrix, that is, we aggregate all reviews of a particular item as a `doc'. Statistics of datasets are given in Table~\\ref{table:data}. We see that the rating matrices of both datasets are very sparse, and the average length of documents is short on Epinions.\n\\begin{table}[h]\n\\centering\n\\begin{tabular}{l c c}\n \\Xhline{2\\arrayrulewidth}\n    Statistics          & Epinions  & Ciao \\\\\n \\hline \\hline\n \\# of Users            & 49,454    & 7,340\\\\\n \\# of Items            & 74,154    & 22,472\\\\\n \\# of Ratings/Reviews  & 790,940   & 183,974\\\\\n \\# of Social Relations & 434,680   & 112,942\\\\\n \\# of Words            & 2,246,837 & 28,874,000\\\\\n Rating Density         & 0.00022   & 0.0011\\\\\n Social Density         & 0.00018   & 0.0021\\\\\n Ave. Words Per Item    & 30.3      & 1284.9\\\\\n \\Xhline{2\\arrayrulewidth}\n\\end{tabular}\n\\caption{Statistics of the Two Datasets}\n\\label{table:data}\n\\end{table}\n\nWe randomly select $x$\\% as the training set and report the prediction performance on the remaining 1 - $x$\\% testing set. The metric {\\em root-mean-square error} (RMSE) for rating prediction task is defined as\n\n", "index": 21, "text": "\\begin{equation}\nRMSE_{\\mathcal{T}} = \\sqrt { {\\sum\\nolimits_{(u_i,v_j) \\in \\mathcal{T}} (R_{i,j} - \\hat R_{i,j})^2} \\Big{/} {|\\mathcal{T}|} }\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"RMSE_{\\mathcal{T}}=\\sqrt{{\\sum\\nolimits_{(u_{i},v_{j})\\in\\mathcal{T}}(R_{i,j}-%&#10;\\hat{R}_{i,j})^{2}}\\Big{/}{|\\mathcal{T}|}}\" display=\"block\"><mrow><mrow><mi>R</mi><mo>\u2062</mo><mi>M</mi><mo>\u2062</mo><mi>S</mi><mo>\u2062</mo><msub><mi>E</mi><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></msub></mrow><mo>=</mo><msqrt><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi></mrow></msub><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mover accent=\"true\"><mi>R</mi><mo stretchy=\"false\">^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo mathsize=\"160%\" stretchy=\"false\">/</mo><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcaf</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow></msqrt></mrow></math>", "type": "latex"}]