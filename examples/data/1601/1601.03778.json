[{"file": "1601.03778.tex", "nexttext": "\n\n\nThus we interpret $Pr(y_{s,o}^{p} = 1)$ as the probability that a vertex (or subject) $s$ in the knowledge graph $G$ is in a relationship\nof given type $p$ with another vertex (or the object) $o$. \n\n\n\\section{Methods}\n\nIn this section, we describe our model, namely Latent Feature Embedding Model\nwith Bayesian Personalized Ranking (BPR) based optimization technique that we\npropose for the task of link prediction in a knowledge graph. In our link\nprediction setting, for a given predicate $p$, we first construct its bipartite\nsubgraph $G_p(S_p, O_p)$. Then we learn the optimal low dimensional embeddings for its\ncorresponding subject and object entities $s_p \\in S_p$, $o_p \\in O_p$\nby maximizing a ranking based distance function. The learning process\nrelies on Stochastic Gradient Descent (SGD). The SGD based optimization\ntechnique iteratively updates the low dimensional representation of $s_p$ and\n$o_p$ until convergence. Then the learned model is used for ranking the\nunobserved triple facts in descending order such that triple facts with higher score\nvalues have a higher probability of being correct.\n\n\\subsection{Latent Feature Based Embedding Model}~\\\\\nFor each predicate $p$, the model maps both its corresponding subject and object entites $s_{p}$ and $o_{p}$\ninto low-dimensional continuous vector spaces, say $U_{s}^{p} \\in {\\rm I\\!R}^{1 \\times K}$ and $V_{o}^{p} \\in {\\rm I\\!R}^{1 \\times K}$ respectively. We measure the compatibility between \nsubject $s_{p}$ and object $o_{p}$ as dot product of its corresponding latent vectors which is given as below:\n\n\n", "itemtype": "equation", "pos": 16948, "prevtext": "\n\n\n\\title{Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs}\n\n\n\\author{Baichuan Zhang\\thanks{Department of Computer and Information Science,\n                Indiana University - Purdue University Indianapolis, USA, bz3@umail.iu.edu. The work was conducted during author's internship at Pacific Northwest National Laboratory} \\\\\n\t\\and \n\tSutanay Choudhury\\thanks{Pacific Northwest National Laboratory, Richland, WA, USA, \\{Sutanay.Choudhury,Khushbu.Agarwal,Sumit.Purohit\\}@pnnl.gov} \\\\\n\t\\and \n\n\tMohammad Al Hasan\\thanks{Department of Computer and Information Science,\n                Indiana University - Purdue University Indianapolis, USA, \\{alhasan,xning\\}@cs.iupui.edu} \\\\\n\t\\and \n\tXia Ning\\footnotemark[3]\n\t\\and\n\tKhushbu Agarwal\\footnotemark[2]\n\t\\and\n\tSumit Purohit\\footnotemark[2]\n\t\\and\n\tPaola Pesantez Cabrera\\thanks{Department of Computer Science, Washington State University, Pullman, WA, USA, p.pesantezcabrera@email.wsu.edu} \t\n} \n\n\n\n\\date{}\n\n\\maketitle\n\n\\begin{abstract} \n\nLink prediction, or predicting the likelihood of a link in a knowledge graph\nbased on its existing state is a key research task. It differs from a\ntraditional link prediction task in that the links in a knowledge graph are\ncategorized into different predicates and the link prediction performance of\ndifferent predicates in a knowledge graph generally varies widely. In this\nwork, we propose a latent feature embedding based link prediction model which\nconsiders the prediction task for each predicate disjointly. To learn the model\nparameters it utilizes a Bayesian personalized ranking based optimization\ntechnique.  Experimental results on large-scale knowledge bases such as YAGO2\nshow that our link prediction approach achieves substantially higher\nperformance than several state-of-art approaches.  We also show that for a\ngiven predicate the topological properties of the knowledge graph induced by\nthe given predicate edges are key indicators of the link prediction performance of\nthat predicate in the knowledge graph.\n\n\\end{abstract}\n\n\n\n\n\n\n\n\\section{Introduction}\n\nA knowledge graph is a repository of information about entities, where entities\ncan be any thing of interest such as people, location, organization or even\nscientific topics, concepts, etc. An entity is frequently characterized by its\nassociation with other entities. As an example, capturing the knowledge about a\ncompany involves listing its products, location and key individuals.\nSimilarly, knowledge about a person involves her name, date and place of birth,\naffiliation with organizations, etc.  Resource Description Framework (RDF) is a\nfrequent choice for capturing the interactions between two entities.  A RDF\ndataset is equivalent to a heterogeneous graph, where each vertex and edge can\nbelong to different classes.  The class information captures taxonomic\nhierarchies between the type of various entities and relations. As an example,\na knowledge graph may identify Kobe Bryant as a basketball player, while its\nontology will indicate that a basketball player is a particular type\nof athlete.  Thus, one will be able to query for famous athletes in the United\nStates and find Kobe Bryant.  \n\nThe past few years have seen a surge in research on knowledge representations\nand algorithms for building knowledge graphs. For example, Google Knowledge\nVault~\\cite{Dong.Lao.ea:14}, and IBM Watson~\\cite{Fan.David.ea:10} are\ncomprehensive knowledge bases which are built in order to answer questions from\nthe general population. As evident from these works,\nit requires multitude of\nefforts to build a domain specific knowledge graph, which are, triple\nextraction from nature language text, entity and relationship\nmapping~\\cite{Yao.McCallum.ea:13}, \nand link prediction~\\cite{Nickel.Tresp.ea:15}.  Specifically, triples extracted\nfrom the text data sources using state of the art techniques such as OpenIE\n\\cite{etzioni2008open} and semantic role labeling \\cite{collobert2011natural}\nare extremely noisy, and simply adding noisy triple facts into knowledge graph\ndestroys its purpose. So computational methods must be devised for deciding\nwhich of the extracted triples are worthy of insertion into a knowledge graph.\nThere are several considerations for this decision making: (1) trustworthiness\nof the data sources; (2) a belief value reported by a natural language\nprocessing engine expressing its confidence in the correctness of parsing; and (3)\nprior knowledge of subjects and objects. This particular work is motivated by the third factor. \n\n\\iffalse\nWe are interested in streaming data sources in natural language text and continuous updating a knowledge graph based on \nnewer facts that become available over time.  Towards that, we extract triples from the text data sources using \nstate of the art techniques such as OpenIE \\cite{etzioni2008open} and semantic role labeling \\cite{collobert2011natural}.  \nAfter passing these triples through a filtering phase we need to decide whether every triple should be added to the knowledge graph.  \nWe have a number of choices.  A triple (or a fact) may be already present in the knowledge graph.  \nUnder such circumstances we may choose to update any related metadata.  \nFor other facts that are not present in knowledge graph, we need to decide whether it should be added to the knowledge graph.  \nThis is an extremely challenging problem.\n\\fi\n\nLink prediction in knowledge graph is simply a machine learning approach for\nutilizing prior knowledge of subjects and objects as available in the knowledge\ngraph for estimating the confidence of a candidate triple.  Consider the\nfollowing example: given a social media post ``I wish Tom Cruise was the\npresident of United States\", a natural language processing engine will extract\na triple (``Tom Cruise\", ``president of\", ``United States\").  On the other\nhand, a web crawler may find the fact that ``Tom Cruise is president of\nDowntown Medical\", resulting in the triple (``Tom Cruise\", ``president of\",\n``Downtown Medical\").  Although we generally do not have any information about\nthe trustworthiness of the sources, our prior knowledge of the entities\nmentioned in this triples will enable us to decide that the first of the above\ntriples is possibly wrong. Link prediction provides a principles approach for\nsuch a decision-making. Also note that, once we decide to add a triple to the\nknowledge graph, it is important to have a confidence value associated with it.  \n\nAs we use a machine learning approach to compute the confidence of triple\nfacts, it is important that we quantitatively understand the degree of accuracy\nof our prediction \\cite{tan2014trust}. It is important, because for the same\nknowledge graph the prediction accuracy level varies from predicate to\npredicate. As an example, predicting one's school or workplace can be a much\nharder task than predicting one's liking for a local restaurant. Therefore,\ngiven two predicates ``worksAt\" and ``likes\", we expect to see widely varying\naccuracy levels.  Also, the average accuracy levels vary widely from one\nknowledge graph to another.   The desire to obtain a quantitative grasp on\nprediction accuracy is complicated by a number of reasons: 1) Knowledge graphs\nconstructed from web text or using machine reading approaches can have a very\nlarge number of predicates that make manual verification\ndifficult~\\cite{Dong.Lao.ea:14}; 2) Creation of predicates, or the resultant\ngraph structure is strongly shaped by the ontology, and the conversion process\nused to generate RDF statements from a logical record in the data.  Therefore,\nsame data source can be represented in very different models and this leads to\ndifferent accuracy levels for the same predicate.  3) The effectiveness of\nknowledge graphs have inspired their construction from every imaginable data\nsource: product inventories (at retailers such as Wal-mart), online social\nnetworks (such as Facebook), and web pages (Google's Knowledge Vault). As we\nmove from one data source to another, it is critical to understand what\naccuracy levels we can expect from a given predicate.\n\nIn this paper, we use a link prediction~\\footnote{We use link prediction and\nlink recommendation interchangeably.} approach for computing the confidence of\na triple from the prior knowledge about its subject and object. Many works exist\nfor link prediction~\\cite{Hasan.Chaoji.ea:06} in social network\nanalysis~\\cite{Chen.Hero:15}, but they differ from the link prediction in\nknowledge graph; for earlier, all the links are semantically similar, but for\nthe latter based on the predicates the semantic of the links differs widely.\nSo, existing link prediction methods are not very suitable for this task. We\nbuild our link prediction method by borrowing solutions from recommender system\nresearch which accept a user-item matrix and for a  given user-item pair, they\nreturn a score indicating the likelihood of the user purchasing the item.\nLikewise, for a given predicate, we consider the set of subjects and objects as\na user-item matrix and produce a real-valued score to measure the confidence of\nthe given triple. For training the model we use Bayesian personalized\nranking (BPR) based embedding model~\\cite{Rendle.Gantner.ea:09}, which has\nbeen a major work  in the recommendation system. In addition, we also study the\nperformance of our proposed link prediction algorithm in terms of topological\nproperties of knowledge graph and present a linear regression model to reason\nabout its expected level of accuracy for each predicate.\n\nOur contributions in this work are outlined below:\n\n\\begin{enumerate}\n\n\\item We implement a Link Prediction approach for estimating confidence for triples in a Knowledge Graph.  \nSpecifically, we borrow from successful approaches in the recommender systems domain, adopt the algorithms for knowledge graphs and perform a thorough evaluation on a prominent benchmark dataset.\n\n\\item We propose a Latent Feature Embedding based link recommendation model for prediction task and utilize Bayesian Personalized Ranking based optimization technique for learning models for each predicate (Section 4). Our experiments on the well known YAGO2 knowledge graph (Section 5) show that the BPR approach outperforms other competing approaches for a significant set of predicates  (Figure 1).\n\n\\item We apply a linear regression model to quantitatively analyze the correlation between the prediction accuracy for each predicate \nand the topological structure of the induced subgraph of the original Knowledge Graph. Our studies show that metrics such as clustering coefficient or average degree \ncan be used to reason about the expected level of prediction accuracy (Section 5.3, Figure 2).\n\n\\end{enumerate}\n\n\\section{Related Work}\n\nThere is a large body of work on link prediction in knowledge graph.  In terms\nof methodology, factorization based and related latent variable\nmodels~\\cite{Nickel.Tresp.ea:11,Chang.Yang.ea:14, Drumond.Rendle.ea:12,\nRodolphe.Antoine.ea:12, Yao.McCallum.ea:13}, graphical\nmodel~\\cite{Jiang.Dou.ea:12}, and graph feature based\nmethod~\\cite{Lao.Cohen.ea:11, Lao.Cohen:10} are considered. \n\nThere exists large number of works which focus on factorization based models.\nThe common thread among the factorization methods is that they explain the\ntriples via latent features of entities. ~\\cite{Bro.97} presents a tensor based\nmodel that decomposes each entity and predicate in knowledge graphs as a low\ndimensional vector.  However, such a method fails to consider the symmetry\nproperty of the tensor.  In order to solve this issue,\n~\\cite{Nickel.Tresp.ea:11} proposes a relational latent feature model, RESCAL,\nan efficient approach which uses a tensor factorization model that takes the\ninherent structure of relational data into account. By leveraging relational\ndomain knowledge about entity type information, ~\\cite{Chang.Yang.ea:14}\nproposes a tensor decomposition approach for relation extraction in knowledge\nbase which is highly efficient in terms of time complexity.  In addition,\nvarious other latent variable models, such as neural network based\nmethods~\\cite{Dong.Lao.ea:14,Socher.Chen.ea:13}, have been explored for link\nprediction task. However, the major drawback of neural network based models is\ntheir complexity and computational cost in model training and parameter tuning.\nMany of these models require tuning large number of parameters, thus finding\nthe right combination of these parameters is often considered more of an art\nthan science. \n\nRecently graphical models, such as Probabilistic Relational\nModels~\\cite{Friedman.Getoor.ea:99}, Relational Markov\nNetwork~\\cite{Taskar.Abbeel.ea:02}, Markov Logic\nNetwork~\\cite{Jiang.Dou.ea:12,Richardson.Domingos:06} have also been used for\nlink prediction in knowledge graph. For instance,\n~\\cite{Richardson.Domingos:06} proposes a Markov Logic Network (MLN) based\napproach, which is a template language for defining potential functions on\nknowledge graph by logical formula.  Despite its utility for modeling knowledge\ngraph, issues such as rule learning difficulty, tractability problem, and\nparameter estimation pose implementation challenge for MLNs. \n\n\nGraph feature based approaches assume that the existence of an edge can be\npredicted by extracting features from the observed edges in the graph.  Lao and\nCohen ~\\cite{Lao.Cohen.ea:11, Lao.Cohen:10} propose Path Ranking Algorithm\n(PRA) to perform random walk on the graph and compute the probability of each\npath.  The main idea of PRA is to use these path probabilities as supervised\nfeatures for each entity pair, and use any favorable classification model, such\nas logistic regression and SVM, to predict the probability of missing edge\nbetween an entity pair in a knowledge graph. \n\nIt has been demonstrated~\\cite{Bordes.Gabrilovich:14} that no single approach\nemerges as a clear winner. Instead, the merits of factorization models and\ngraph feature models are often complementary with each other. Thus combining\nthe advantages of different approaches for learning knowledge graph is a\npromising option.  For instance,~\\cite{Nickel.Jiang.ea:14} proposes to use\nadditive model, which is a linear combination between RESCAL and PRA.  The\ncombination results in not only decrease the training time but also increase the\naccuracy. ~\\cite{Jiang.Huang.ea:12} combines a latent feature model with an\nadditive term to learn from latent and neighborhood-based information on\nmulti-relational data. ~\\cite{Dong.Lao.ea:14} fuses the outputs of PRA and\nneural network model as features for training a binary classifier.   Our work\nstrongly aligns with this combination approach. In this work, we build matrix\nfactorization based techniques that have been proved successful for recommender systems and plan to\nincorporate graph based features in future work.\n\n\n\\section{Background and Problem Statement}\n\n\\iffalse\n\\textsc{Definition 1:} We define the knowledge graph as a collection of triple facts $G = (S, P, O)$, where $s \\in S$ and $o \\in O$ are the set of subject and object entities and $p \\in P$ is the set of predicates or relations between them.  $G(s, p, o) = 1$ if there is a direct link of type $p$ from $s$ to $o$, and $G(s, p, o) = 0$ otherwise.  \n\\fi\n\n\\begin{definition}{We define the knowledge graph as a collection of triple facts $G = (S, P, O)$, where $s \\in S$ and $o \\in O$ are the set of subject and object entities and $p \\in P$ is the set of predicates or relations between them.  $G(s, p, o) = 1$ if there is a direct link of type $p$ from $s$ to $o$, and $G(s, p, o) = 0$ otherwise.}\\end{definition}\n\nEach triple fact in knowledge graph is a statement interpreted as ``A relationship p holds between entities \n$s$ and $o$\". For instance, the statement ``Kobe Bryant is a player of LA Lakers\" can be expressed by the following triple fact\n(``Kobe Bryant\", ``playsFor\", ``LA Lakers\"). \n\n\\iffalse\n\\textsc{Definition 2:} For each relation $p \\in P$, we define $G_{p}(S_{p}, O_{p})$ as a bipartite subgraph of $G$, where the corresponding \nset of entities $s_{p} \\in S_{p}$, $o_{p} \\in O_{p}$ are connected by relation $p$, namely $G_{p}(s_{p}, o_{p}) = 1$.\n\\fi\n\n\\begin{definition}{For each relation $p \\in P$, we define $G_{p}(S_{p}, O_{p})$ as a bipartite subgraph of $G$, where the corresponding \nset of entities $s_{p} \\in S_{p}$, $o_{p} \\in O_{p}$ are connected by relation $p$, namely $G_{p}(s_{p}, o_{p}) = 1$.}\\end{definition}\n\n\\noindent\\textbf{Problem Statement:} For every predicate $p \\in P$ and given an entity pair $(s, o)$ in $G_p$, our goal is to learn a link recommendation model $M_p$ \nsuch that $x_{s,o} = M_p(s, o)$ is a real-valued score.\n\n\n\n\nDue to the fact that the produced real-valued score is not normalized, we compute the probability $Pr(y_{s,o}^{p} = 1)$, where $y_{s, o}^{p}$\nis a binary random variable that is true iff $G_{p}(s, o) = 1$. We estimate this probability $Pr$ using the logistic function as follows:\n\n", "index": 1, "text": "\\begin{equation}\nPr(y_{s,o}^{p} = 1) = \\frac{1}{1 + exp(-x_{s,o})}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"Pr(y_{s,o}^{p}=1)=\\frac{1}{1+exp(-x_{s,o})}\" display=\"block\"><mrow><mi>P</mi><mi>r</mi><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>y</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mrow><mi>e</mi><mo>\u2062</mo><mi>x</mi><mo>\u2062</mo><mi>p</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>,</mo><mi>o</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nwhere $U^{p} \\in {\\rm I\\!R}^{\\mid S \\mid \\times K}$, $V^{p} \\in {\\rm\nI\\!R}^{\\mid O \\mid \\times K}$, and $b^{p} \\in {\\rm I\\!R}^{\\mid O \\mid \\times\n1}$.  $\\left| S \\right|$ and $\\left| O \\right|$ denote the size of subject and\nobject associated with predicate $p$ respectively.  $K$ is the number of latent\ndimensions and $b_{o}^{p} \\in {\\rm I\\!R}$ is a bias term associated with object\n$o$. Given predicate $p$, the higher the score of $x_{s_{p},o_{p}}$, the more\nsimilar the entities $s_{p}$ and $o_{p}$ in the embedded low dimensional space,\nand the higher the confidence to include this triple fact into knowledge\nbase.\n\n\\subsection{Bayesian Personalized Ranking}~\\\\\nIn collaborative filtering, positive-only data is known as implicit feedback/binary feedback. For example, in the eCommerce platform,\nsome users only buy but do not rate items. Motivated by~\\cite{Rendle.Gantner.ea:09}, we employ Bayesian Personalized Ranking (BPR) based\napproach for model learning. Specifically, in recommender system domain, given user-item matrix, BPR based approach assigns the preference of user for purchased \nitem with higher score than un-purchased item. Likewise, under this context, we assign observed triple facts higher score\nthan unobserved triple facts in knowledge base. We assume that unobserved facts are not necessarily negative, rather they are ``less preferable\" than the observed ones. \n\nFor our task, in each predicate $p$, we denote the observed subject/object entity pair as $(s_p, o^{+}_p)$ and unobserved one\nas $(s_p, o^{-}_p)$. The observed facts in our case are the existing link between $s_p$ and $o_p$ given $G_p$ and unobserved ones are the \nmissing link between them. Given this fact, BPR maximizes the following ranking based distance function:\n\n\n", "itemtype": "equation", "pos": 18609, "prevtext": "\n\n\nThus we interpret $Pr(y_{s,o}^{p} = 1)$ as the probability that a vertex (or subject) $s$ in the knowledge graph $G$ is in a relationship\nof given type $p$ with another vertex (or the object) $o$. \n\n\n\\section{Methods}\n\nIn this section, we describe our model, namely Latent Feature Embedding Model\nwith Bayesian Personalized Ranking (BPR) based optimization technique that we\npropose for the task of link prediction in a knowledge graph. In our link\nprediction setting, for a given predicate $p$, we first construct its bipartite\nsubgraph $G_p(S_p, O_p)$. Then we learn the optimal low dimensional embeddings for its\ncorresponding subject and object entities $s_p \\in S_p$, $o_p \\in O_p$\nby maximizing a ranking based distance function. The learning process\nrelies on Stochastic Gradient Descent (SGD). The SGD based optimization\ntechnique iteratively updates the low dimensional representation of $s_p$ and\n$o_p$ until convergence. Then the learned model is used for ranking the\nunobserved triple facts in descending order such that triple facts with higher score\nvalues have a higher probability of being correct.\n\n\\subsection{Latent Feature Based Embedding Model}~\\\\\nFor each predicate $p$, the model maps both its corresponding subject and object entites $s_{p}$ and $o_{p}$\ninto low-dimensional continuous vector spaces, say $U_{s}^{p} \\in {\\rm I\\!R}^{1 \\times K}$ and $V_{o}^{p} \\in {\\rm I\\!R}^{1 \\times K}$ respectively. We measure the compatibility between \nsubject $s_{p}$ and object $o_{p}$ as dot product of its corresponding latent vectors which is given as below:\n\n\n", "index": 3, "text": "\\begin{equation}\nx_{s_{p},o_{p}} = (U_{s}^{p})(V_{o}^{p})^{T} + b_{o}^{p}\n\\label{eq:latent}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"x_{s_{p},o_{p}}=(U_{s}^{p})(V_{o}^{p})^{T}+b_{o}^{p}\" display=\"block\"><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msub><mi>o</mi><mi>p</mi></msub></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>V</mi><mi>o</mi><mi>p</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup></mrow><mo>+</mo><msubsup><mi>b</mi><mi>o</mi><mi>p</mi></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nwhere $D_{p}$ is a set of samples generated from the training data for predicate $p$, $G_{p}(s_p, o^{+}_p) = 1$ and $G_{p}(s_p, o^{-}_p) = 0$. \nAnd $x_{s_p,o^{+}_p}$ and $x_{s_p,o^{-}_p}$ are the predicted scores of \nsubject $s_p$ on objects $o^{+}_p$ and $o^{-}_p$ respectively. We use the proposed\nlatent feature based embedding model shown in Equation~\\ref{eq:latent} to compute $x_{s_p, o^{+}_p}$ and $x_{s_p, o^{-}_p}$ respectively. \nThe last term in Equation~\\ref{eq:objbpr} is a $l_{2}$-norm regularization term used for \nmodel parameters $\\Theta_{p} = \\{U^{p}, V^{p}, b^{p}\\}$ to avoid overfitting in the learning process. \nIn addition, the logistic function $\\sigma(.)$ in Equation~\\ref{eq:objbpr} is defined as $\\sigma(x) = \\frac{1}{1+e^{-x}}$.\n\n\n\n\n\nNotice that the Equation~\\ref{eq:objbpr} is differentiable, thus we employ the widely used SGD to maximize the objective.\nIn particular, at each iteration, for given predicate $p$, we sample one observed entity pair $(s_p, o^{+}_p)$ and one unobserved one\n$(s_p, o^{-}_p)$ using uniform sampling technique. Then we iteratively update the model parameters $\\Theta_{p}$ based on the sampled pairs. \nSpecifically, for each training instance, we compute the derivative and update the corresponding parameters $\\Theta_{p}$ by walking along the ascending gradient direction. \n\nFor each predicate $p$, given a training triple $(s_p, o^{+}_p, o^{-}_p)$, the gradient of BPR objective in Equation~\\ref{eq:objbpr} with respect to\n$U_{s}^{p}$, $V_{o^{+}}^{p}$, $V_{o^{-}}^{p}$, $b_{o^{+}}^{p}$, $b_{o^{-}}^{p}$ can be computed as follows:\n\n\\begin{eqnarray}\n\\frac{\\partial BPR}{\\partial U_{s}^{p}} &=& \\frac{\\partial \\ln\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})}{\\partial U_{s}^{p}} - 2\\lambda^{p}_{s}U_{s}^{p} \\nonumber \\\\ \n&=& \\scalebox{0.98} {$\\frac{\\partial \\ln\\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial \\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})} \\times \\frac{\\partial\\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}$} \\nonumber\\\\\n&& \\scalebox{0.98} {$\\times \\frac{\\partial(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial U_{s}^{p}} - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n&=& \\frac{1}{\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})} \\times \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p}) \\nonumber\\\\\n&&\\scalebox{0.90} {$\\big(1-\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times(V_{o^{+}}^{p} - V_{o^{-}}^{p}) - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n&=&\\scalebox{0.90} {$\\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)(V_{o^{+}}^{p} - V_{o^{-}}^{p}) - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n\\label{eq:update1}\n\\end{eqnarray}\n\nWe obtain the following using similar chain rule derivation.\n\n\n", "itemtype": "equation", "pos": 20483, "prevtext": "\n\nwhere $U^{p} \\in {\\rm I\\!R}^{\\mid S \\mid \\times K}$, $V^{p} \\in {\\rm\nI\\!R}^{\\mid O \\mid \\times K}$, and $b^{p} \\in {\\rm I\\!R}^{\\mid O \\mid \\times\n1}$.  $\\left| S \\right|$ and $\\left| O \\right|$ denote the size of subject and\nobject associated with predicate $p$ respectively.  $K$ is the number of latent\ndimensions and $b_{o}^{p} \\in {\\rm I\\!R}$ is a bias term associated with object\n$o$. Given predicate $p$, the higher the score of $x_{s_{p},o_{p}}$, the more\nsimilar the entities $s_{p}$ and $o_{p}$ in the embedded low dimensional space,\nand the higher the confidence to include this triple fact into knowledge\nbase.\n\n\\subsection{Bayesian Personalized Ranking}~\\\\\nIn collaborative filtering, positive-only data is known as implicit feedback/binary feedback. For example, in the eCommerce platform,\nsome users only buy but do not rate items. Motivated by~\\cite{Rendle.Gantner.ea:09}, we employ Bayesian Personalized Ranking (BPR) based\napproach for model learning. Specifically, in recommender system domain, given user-item matrix, BPR based approach assigns the preference of user for purchased \nitem with higher score than un-purchased item. Likewise, under this context, we assign observed triple facts higher score\nthan unobserved triple facts in knowledge base. We assume that unobserved facts are not necessarily negative, rather they are ``less preferable\" than the observed ones. \n\nFor our task, in each predicate $p$, we denote the observed subject/object entity pair as $(s_p, o^{+}_p)$ and unobserved one\nas $(s_p, o^{-}_p)$. The observed facts in our case are the existing link between $s_p$ and $o_p$ given $G_p$ and unobserved ones are the \nmissing link between them. Given this fact, BPR maximizes the following ranking based distance function:\n\n\n", "index": 5, "text": "\\begin{equation}\n\n\\scalebox{0.88} {$BPR = \\underset{\\Theta_{p}}{\\text{max}} \\\\ \\sum_{(s_p,o^{+}_p,o^{-}_p) \\in D_{p}} \\ln\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p}) - \\lambda_{\\Theta_{p}}\\mid\\mid\\Theta_{p}\\mid\\mid^{2}$}\n\\label{eq:objbpr}\n\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1.m1\" class=\"ltx_Math\" alttext=\"BPR=\\underset{\\Theta_{p}}{\\text{max}}\\\\&#10;\\sum_{(s_{p},o^{+}_{p},o^{-}_{p})\\in D_{p}}\\ln\\sigma(x_{s_{p},o^{+}_{p}}-x_{s_%&#10;{p},o^{-}_{p}})-\\lambda_{\\Theta_{p}}\\mid\\mid\\Theta_{p}\\mid\\mid^{2}\" display=\"inline\"><mrow><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mrow><mrow><munder accentunder=\"true\"><mtext>max</mtext><msub><mi mathvariant=\"normal\">\u0398</mi><mi>p</mi></msub></munder><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><mrow><mrow><mi>ln</mi><mo>\u2061</mo><mi>\u03c3</mi></mrow><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup></mrow></msub><mo>-</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>-</mo><mrow><msub><mi>\u03bb</mi><msub><mi mathvariant=\"normal\">\u0398</mi><mi>p</mi></msub></msub><mo>\u2062</mo><msup><mrow><mo>\u2223</mo><mrow><mo>\u2223</mo><msub><mi mathvariant=\"normal\">\u0398</mi><mi>p</mi></msub><mo>\u2223</mo></mrow><mo>\u2223</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 23433, "prevtext": "\n\nwhere $D_{p}$ is a set of samples generated from the training data for predicate $p$, $G_{p}(s_p, o^{+}_p) = 1$ and $G_{p}(s_p, o^{-}_p) = 0$. \nAnd $x_{s_p,o^{+}_p}$ and $x_{s_p,o^{-}_p}$ are the predicted scores of \nsubject $s_p$ on objects $o^{+}_p$ and $o^{-}_p$ respectively. We use the proposed\nlatent feature based embedding model shown in Equation~\\ref{eq:latent} to compute $x_{s_p, o^{+}_p}$ and $x_{s_p, o^{-}_p}$ respectively. \nThe last term in Equation~\\ref{eq:objbpr} is a $l_{2}$-norm regularization term used for \nmodel parameters $\\Theta_{p} = \\{U^{p}, V^{p}, b^{p}\\}$ to avoid overfitting in the learning process. \nIn addition, the logistic function $\\sigma(.)$ in Equation~\\ref{eq:objbpr} is defined as $\\sigma(x) = \\frac{1}{1+e^{-x}}$.\n\n\n\n\n\nNotice that the Equation~\\ref{eq:objbpr} is differentiable, thus we employ the widely used SGD to maximize the objective.\nIn particular, at each iteration, for given predicate $p$, we sample one observed entity pair $(s_p, o^{+}_p)$ and one unobserved one\n$(s_p, o^{-}_p)$ using uniform sampling technique. Then we iteratively update the model parameters $\\Theta_{p}$ based on the sampled pairs. \nSpecifically, for each training instance, we compute the derivative and update the corresponding parameters $\\Theta_{p}$ by walking along the ascending gradient direction. \n\nFor each predicate $p$, given a training triple $(s_p, o^{+}_p, o^{-}_p)$, the gradient of BPR objective in Equation~\\ref{eq:objbpr} with respect to\n$U_{s}^{p}$, $V_{o^{+}}^{p}$, $V_{o^{-}}^{p}$, $b_{o^{+}}^{p}$, $b_{o^{-}}^{p}$ can be computed as follows:\n\n\\begin{eqnarray}\n\\frac{\\partial BPR}{\\partial U_{s}^{p}} &=& \\frac{\\partial \\ln\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})}{\\partial U_{s}^{p}} - 2\\lambda^{p}_{s}U_{s}^{p} \\nonumber \\\\ \n&=& \\scalebox{0.98} {$\\frac{\\partial \\ln\\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial \\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})} \\times \\frac{\\partial\\sigma(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}$} \\nonumber\\\\\n&& \\scalebox{0.98} {$\\times \\frac{\\partial(x_{s_p,o^{+}_p} - x_{s_p, o^{-}_p})}{\\partial U_{s}^{p}} - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n&=& \\frac{1}{\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})} \\times \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p}) \\nonumber\\\\\n&&\\scalebox{0.90} {$\\big(1-\\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times(V_{o^{+}}^{p} - V_{o^{-}}^{p}) - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n&=&\\scalebox{0.90} {$\\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)(V_{o^{+}}^{p} - V_{o^{-}}^{p}) - 2\\lambda^{p}_{s}U_{s}^{p}$} \\nonumber \\\\\n\\label{eq:update1}\n\\end{eqnarray}\n\nWe obtain the following using similar chain rule derivation.\n\n\n", "index": 7, "text": "\\begin{equation}\n\\frac{\\partial BPR}{\\partial V_{o^{+}}^{p}} = \\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times U_{s}^{p} - 2\\lambda^{p}_{o^{+}}V_{o^{+}}^{p}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial BPR}{\\partial V_{o^{+}}^{p}}=\\big{(}1-\\sigma(x_{s_{p},o^{+}_{p}%&#10;}-x_{s_{p},o^{-}_{p}})\\big{)}\\times U_{s}^{p}-2\\lambda^{p}_{o^{+}}V_{o^{+}}^{p}\" display=\"block\"><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mfrac><mo>=</mo><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup></mrow></msub><mo>-</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u00d7</mo><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03bb</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>\u2062</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 23620, "prevtext": "\n\n\n", "index": 9, "text": "\\begin{equation}\n\\frac{\\partial BPR}{\\partial V_{o^{-}}^{p}} = \\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times (-U_{s}^{p}) - 2\\lambda^{p}_{o^{-}}V_{o^{-}}^{p}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial BPR}{\\partial V_{o^{-}}^{p}}=\\big{(}1-\\sigma(x_{s_{p},o^{+}_{p}%&#10;}-x_{s_{p},o^{-}_{p}})\\big{)}\\times(-U_{s}^{p})-2\\lambda^{p}_{o^{-}}V_{o^{-}}^%&#10;{p}\" display=\"block\"><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mfrac><mo>=</mo><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup></mrow></msub><mo>-</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03bb</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>\u2062</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 23810, "prevtext": "\n\n\n", "index": 11, "text": "\\begin{equation}\n\\frac{\\partial BPR}{\\partial b_{o^{+}}^{p}} = \\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times 1 - 2\\lambda^{p}_{o^{+}}b_{o^{+}}^{p}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial BPR}{\\partial b_{o^{+}}^{p}}=\\big{(}1-\\sigma(x_{s_{p},o^{+}_{p}%&#10;}-x_{s_{p},o^{-}_{p}})\\big{)}\\times 1-2\\lambda^{p}_{o^{+}}b_{o^{+}}^{p}\" display=\"block\"><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mfrac><mo>=</mo><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup></mrow></msub><mo>-</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u00d7</mo><mn>1</mn></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03bb</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>\u2062</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nNext, the parameters are updated as follows:\n\n\n", "itemtype": "equation", "pos": 23989, "prevtext": "\n\n\n", "index": 13, "text": "\\begin{equation}\n\\frac{\\partial BPR}{\\partial b_{o^{-}}^{p}} = \\big(1 - \\sigma(x_{s_p,o^{+}_p} - x_{s_p,o^{-}_p})\\big)\\times (-1) - 2\\lambda^{p}_{o^{-}}b_{o^{-}}^{p}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"\\frac{\\partial BPR}{\\partial b_{o^{-}}^{p}}=\\big{(}1-\\sigma(x_{s_{p},o^{+}_{p}%&#10;}-x_{s_{p},o^{-}_{p}})\\big{)}\\times(-1)-2\\lambda^{p}_{o^{-}}b_{o^{-}}^{p}\" display=\"block\"><mrow><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mfrac><mo>=</mo><mrow><mrow><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>+</mo></msubsup></mrow></msub><mo>-</mo><msub><mi>x</mi><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>,</mo><msubsup><mi>o</mi><mi>p</mi><mo>-</mo></msubsup></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo>\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03bb</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>\u2062</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 24217, "prevtext": "\n\nNext, the parameters are updated as follows:\n\n\n", "index": 15, "text": "\\begin{equation}\nU_{s}^{p} = U_{s}^{p} + \\alpha\\times\\frac{\\partial BPR}{\\partial U_{s}^{p}} \n\\label{eq:sgd1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"U_{s}^{p}=U_{s}^{p}+\\alpha\\times\\frac{\\partial BPR}{\\partial U_{s}^{p}}\" display=\"block\"><mrow><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup><mo>=</mo><mrow><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u00d7</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>U</mi><mi>s</mi><mi>p</mi></msubsup></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 24343, "prevtext": "\n\n\n", "index": 17, "text": "\\begin{equation}\nV_{o^{+}}^{p} = V_{o^{+}}^{p} + \\alpha\\times\\frac{\\partial BPR}{\\partial V_{o^{+}}^{p}} \n\\label{eq:sgd2}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"V_{o^{+}}^{p}=V_{o^{+}}^{p}+\\alpha\\times\\frac{\\partial BPR}{\\partial V_{o^{+}}%&#10;^{p}}\" display=\"block\"><mrow><msubsup><mi>V</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>=</mo><mrow><msubsup><mi>V</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u00d7</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 24481, "prevtext": "\n\n\n", "index": 19, "text": "\\begin{equation}\nV_{o^{-}}^{p} = V_{o^{-}}^{p} + \\alpha\\times\\frac{\\partial BPR}{\\partial V_{o^{-}}^{p}} \n\\label{eq:sgd3}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"V_{o^{-}}^{p}=V_{o^{-}}^{p}+\\alpha\\times\\frac{\\partial BPR}{\\partial V_{o^{-}}%&#10;^{p}}\" display=\"block\"><mrow><msubsup><mi>V</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>=</mo><mrow><msubsup><mi>V</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u00d7</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>V</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\n\n", "itemtype": "equation", "pos": 24619, "prevtext": "\n\n\n", "index": 21, "text": "\\begin{equation}\nb_{o^{+}}^{p} = b_{o^{+}}^{p} + \\alpha\\times\\frac{\\partial BPR}{\\partial b_{o^{+}}^{p}} \n\\label{eq:sgd4}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"b_{o^{+}}^{p}=b_{o^{+}}^{p}+\\alpha\\times\\frac{\\partial BPR}{\\partial b_{o^{+}}%&#10;^{p}}\" display=\"block\"><mrow><msubsup><mi>b</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>=</mo><mrow><msubsup><mi>b</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u00d7</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>+</mo></msup><mi>p</mi></msubsup></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nwhere $\\alpha$ is the learning rate.\n\n\\begin{algorithm}\n\\caption{Bayesian Personalized Ranking Based Latent Feature Embedding Model}\n\\label{alg:1}\n\\begin{algorithmic}[1]\n\\REQUIRE latent dimension $K$, $G$, target predicate $p$\n\\ENSURE $U^{p}$, $V^{p}$, $b^{p}$\n\\STATE Given target predicate $p$ and entire knowledge graph $G$, construct its bipartite subgraph, $G_{p}$ \n\\STATE $m$ = number of subject entities in $G_{p}$\n\\STATE $n$ = number of object entities in $G_{p}$ \n\\STATE Generate a set of training samples $D_{p} = \\{(s_p, o^{+}_{p}, o^{-}_{p})\\}$ using uniform sampling technique\n\\STATE Initialize $U^{p}$ as size $m \\times K$ matrix with $0$ mean and standard deviation $0.1$\n\\STATE Initialize $V^{p}$ as size $n \\times K$ matrix with $0$ mean and stardard deviation $0.1$\n\\STATE Initialize $b^{p}$ as size $n \\times 1$ column vector with $0$ mean and stardard deviation $0.1$\n\\FORALL{$(s_p, o^{+}_{p}, o^{-}_{p}) \\in D_{p}$}\n  \\STATE Update $U_{s}^{p}$ based on Equation~\\ref{eq:sgd1}\n  \\STATE Update $V_{o^{+}}^{p}$ based on Equation~\\ref{eq:sgd2}\n  \\STATE Update $V_{o^{-}}^{p}$ based on Equation~\\ref{eq:sgd3}\n  \\STATE Update $b_{o^{+}}^{p}$ based on Equation~\\ref{eq:sgd4}\n  \\STATE Update $b_{o^{-}}^{p}$ based on Equation~\\ref{eq:sgd5}\n\\ENDFOR\n\\STATE \\textbf{return} $U^{p}$, $V^{p}$, $b^{p}$\n\\end{algorithmic}  \n\\end{algorithm}\n\n\\subsection{Pseudo-code and Complexity Analysis}~\\\\\nThe pseudo-code of our proposed link prediction model is described in Algorithm~\\ref{alg:1}.\nIt takes the knowledge graph $G$ and a specific target predicate $p$ as input and generates the low\ndimensional latent matrices $U^{p}$, $V^{p}$, $b^{p}$ as output. Line 1 constucts the bipartite subgraph\nof predicate $p$, $G_{p}$ given entire knowledge graph $G$. Line 2-3 compute the number of subject and object entities\nas $m$ and $n$ in resultant bipartite subgraph $G_{p}$ respectively. Line 4 generates a collection of triple samples using \nuniform sampling technique. Line 5-7 initialize the matrices $U^{p}$, $V^{p}$, $b^{p}$ using \nGaussian distribution with $0$ mean and $0.1$ standard deviation, assuming all the entries in $U^{p}$, $V^{p}$ and $b^{p}$ are independent.\nLine 8-14 update corresponding rows of matrices $U^{p}$, $V^{p}$, $b^{p}$ based on the sampled instance $(s_p, o^{+}_{p}, o^{-}_{p})$ in each iteration. \nAs the sample generation step in line 4 is prior to the model parameter learning, thus the convergence criteria of Algorithm~\\ref{alg:1} is to iterate\nover all the sampled triples in $D_{p}$.  \n\nGiven the constructed $G_{p}$ as input, the time complexity of the update rules shown in \nEquations~\\ref{eq:sgd1}~\\ref{eq:sgd2}~\\ref{eq:sgd3}~\\ref{eq:sgd4}~\\ref{eq:sgd5} is $\\mathcal{O}(cK)$, where $K$ is the number of latent features. \nThe total computational complexity of Algorithm~\\ref{alg:1} is then $\\mathcal{O}(\\left|D_{p}\\right|\\cdot cK)$, \nwhere $\\left| D_{p}\\right|$ is the total size of pre-sampled triples shown in line 4 of Algorithm~\\ref{alg:1}.\n\n\\section{Experiments and Results}\nThis section presents our experimental analysis of the Algorithm~\\ref{alg:1} for thirteen unique predicates in the well known YAGO2 knowledge graph~\\cite{Hoffart.Suchanek.ea:11}.\n\nWe construct a model for each predicate and describe our evaluation strategies, \nincluding performance metrics and selection of state-of-the-art methods for benchmarking in section 5.1. We aim to answer two questions through our experiments:\n\n\\begin{figure*}\n\\centering\n\\subfigure[HR Comparison among different link recommendation methods]{\\label{fig:a}\\includegraphics[width=56mm]{HR_yago_K_50}}\n\\subfigure[ARHR Comparison among different link recommendation methods]{\\label{fig:b}\\includegraphics[width=56mm]{ARHR_yago_K_50}}        \n\\subfigure[AUC Comparison among different link recommendation methods]{\\label{fig:c}\\includegraphics[width=56mm]{AUC_yago_K_50}}\n\\caption{Link Recommendation Comparison on YAGO2 Relations}\\label{fig:yagoprediction}\n\\end{figure*}\n\n\\begin{enumerate}\n\n\\item How does our approach compare with related work for link recommendation in knowledge graph?\n\n\\item For a predicate $p$, can we reason about the link prediction model performance $M_p$ in terms of the structural metrics of the bipartite graph $G_p$?\n\n\\end{enumerate}\n\n\\begin{table}[h!]\n\\centering\n\\scalebox{0.75}{\n\\begin{tabular}{*{4}{c}}\n\\toprule\nRelation & \\# Subjects & \\# Objects & \\# of Facts in YAGO2 \\\\ \n\\midrule\nImport & 142 & 62 & 391 \\\\\nExport & 140 & 176 & 579 \\\\\nisInterestedIn & 358 & 213 & 464 \\\\\nhasOfficialLanguage & 583 & 214 & 964 \\\\\ndealsWith & 131 & 124 & 945 \\\\\nhappenedIn & 7121 & 5526 & 12500 \\\\\nparticipatedIn & 2330 & 7043 & 16809 \\\\\nisConnectedTo & 2835 & 4391 & 33581 \\\\\nhasChild & 10758 & 12800 & 17320 \\\\\ninfluence & 8056 & 9153 & 25819 \\\\\nwroteMusicFor & 5109 & 21487 & 24271 \\\\\nedited & 549 & 5673 & 5946 \\\\\nowns & 8330 & 24422 & 26536 \\\\\n\\bottomrule\n\\end{tabular}} \n\\caption{Statistics of Various Relations in YAGO2 Dataset}\n\\vspace{-0.10in}\n\\label{tab:yagosta}\n\\end{table}\n\nTable~\\ref{tab:yagosta} shows the statistic of various YAGO2 relations used in our experiments. \\# Subjects and \\# Objects represent the number of subject and object entities associated with its corresponding predicate. The last column shown in Table~\\ref{tab:yagosta} shows the number of facts for each relation in YAGO2. We run all the experiments on a 2.1 GHz Machine with 4GB memory running Linux operating system. The algorithms are implemented in Python language along with NumPy and SciPy libraries for linear algebra operations. The software is available online for download~\\footnote{\\url{https://sites.google.com/site/baichuanzhangpurdue}}.\n\n\\begin{figure*}\n\\centering\n\\subfigure[Graph Density and HR]{\\label{fig:a}\\includegraphics[width=56mm]{density_HR}}\n\\subfigure[Graph Density and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{density_ARHR}}        \n\\subfigure[Graph Density and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{density_AUC}}\n\\subfigure[Graph Average Degree and HR]{\\label{fig:a}\\includegraphics[width=56mm]{degree_HR}}\n\\subfigure[Graph Average Degree and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{degree_ARHR}}        \n\\subfigure[Graph Average Degree and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{degree_AUC}}\n\\subfigure[Clustering Coefficient and HR]{\\label{fig:a}\\includegraphics[width=56mm]{cc_HR}}\n\\subfigure[Clustering Coefficient and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{cc_ARHR}}        \n\\subfigure[Clustering Coefficient and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{cc_AUC}}\n\n\n\n\\caption{Quantitative Analysis Between Graph Topology and Link Recommendation Model Performance}\\label{fig:topology}\n\\end{figure*}\n\n\n\n\\subsection{Experimental Setting}~\\\\\nFor our experiment, in order to demonstrate the performance of our proposed link prediction model, we use the YAGO2 dataset\nand several evaluation metrics for all compared algorithms. Particularly, for each relation, we split the data into a training part, \nused for model training, and a test part, used for model evaluation. We apply 5-time leave one out evaluation strategy, where for\neach subject, we randomly remove one fact (one subject-object pair) and place it into test set $S_{test}$ and remaining in the training\nset $S_{train}$. For every subject, the training model will generate a size-N ranked list of recommended objects for \nrecommendation task. The evaluation is conducted by comparing the recommendation list of each subject and the\nobject entity of that subject in the test set. Grid search is applied to find regularization parameters, and we set the values of parameters used in section 4.2 as \n$\\lambda_s = \\lambda_{o^{+}} = \\lambda_{o^{-}} = 0.005$. For other model parameters, we fix learning rate\n$\\alpha = 0.2$, and number of latent factors $K = 50$ respectively. \n\nFor parameter in model evaluation, we set $N = 10$. \n\nIn order to illustrate the merit of our proposed approach, we compare our model with the following methods\nfor link prediction in a knowledge graph. Since the problem we solve in this paper is similar to the one-class\nitem recommendation~\\cite{Rendle.Gantner.ea:09} in recommender system domain, we consider the following state-of-the-art one-class\nrecommendation methods as baseline approaches for comparison. \n\n\\begin{enumerate}\n\n\\item \\textbf{Random (Rand):} For each relation, this method randomly selects subject-object entity pair for link recommendation task.\n\n\\item \\textbf{Most Popular (MP):} For each predicate in knowledge base, this method presents a non-personalized ranked object list \nbased on how often object entities are connected among all subject entities. \n\n\\item \\textbf{MF:} The matrix factorization method is proposed by~\\cite{Koren.Bell.ea:09}, which uses a point-wise strategy for solving \nthe one-class item recommendation problem. \n\n\\end{enumerate} \n\nDuring the model evaluation stage, we use three popular metrics, namely Hit Rate (HR), Average Reciprocal Hit-Rank (ARHR), and\nArea Under Curve (AUC), to measure the link recommendation quality of our proposed approach in comparison to baseline methods.\nHR is defined as follows:\n\n\n", "itemtype": "equation", "pos": 24757, "prevtext": "\n\n\n", "index": 23, "text": "\\begin{equation}\nb_{o^{-}}^{p} = b_{o^{-}}^{p} + \\alpha\\times\\frac{\\partial BPR}{\\partial b_{o^{-}}^{p}} \n\\label{eq:sgd5}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"b_{o^{-}}^{p}=b_{o^{-}}^{p}+\\alpha\\times\\frac{\\partial BPR}{\\partial b_{o^{-}}%&#10;^{p}}\" display=\"block\"><mrow><msubsup><mi>b</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>=</mo><mrow><msubsup><mi>b</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup><mo>+</mo><mrow><mi>\u03b1</mi><mo>\u00d7</mo><mfrac><mrow><mo>\u2202</mo><mo>\u2061</mo><mrow><mi>B</mi><mo>\u2062</mo><mi>P</mi><mo>\u2062</mo><mi>R</mi></mrow></mrow><mrow><mo>\u2202</mo><mo>\u2061</mo><msubsup><mi>b</mi><msup><mi>o</mi><mo>-</mo></msup><mi>p</mi></msubsup></mrow></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nwhere $\\#subjects$ is the total number of subject entities in test set, and $\\#hits$ is the number of subjects\nwhose object entity in the test set is recommended in the size-N recommendation list. The second evaluation metric,\nARHR, considering the ranking of the recommended object for each subject entity in knowledge graph, is defined  as\nbelow:\n\n\n", "itemtype": "equation", "pos": 34013, "prevtext": "\n\nwhere $\\alpha$ is the learning rate.\n\n\\begin{algorithm}\n\\caption{Bayesian Personalized Ranking Based Latent Feature Embedding Model}\n\\label{alg:1}\n\\begin{algorithmic}[1]\n\\REQUIRE latent dimension $K$, $G$, target predicate $p$\n\\ENSURE $U^{p}$, $V^{p}$, $b^{p}$\n\\STATE Given target predicate $p$ and entire knowledge graph $G$, construct its bipartite subgraph, $G_{p}$ \n\\STATE $m$ = number of subject entities in $G_{p}$\n\\STATE $n$ = number of object entities in $G_{p}$ \n\\STATE Generate a set of training samples $D_{p} = \\{(s_p, o^{+}_{p}, o^{-}_{p})\\}$ using uniform sampling technique\n\\STATE Initialize $U^{p}$ as size $m \\times K$ matrix with $0$ mean and standard deviation $0.1$\n\\STATE Initialize $V^{p}$ as size $n \\times K$ matrix with $0$ mean and stardard deviation $0.1$\n\\STATE Initialize $b^{p}$ as size $n \\times 1$ column vector with $0$ mean and stardard deviation $0.1$\n\\FORALL{$(s_p, o^{+}_{p}, o^{-}_{p}) \\in D_{p}$}\n  \\STATE Update $U_{s}^{p}$ based on Equation~\\ref{eq:sgd1}\n  \\STATE Update $V_{o^{+}}^{p}$ based on Equation~\\ref{eq:sgd2}\n  \\STATE Update $V_{o^{-}}^{p}$ based on Equation~\\ref{eq:sgd3}\n  \\STATE Update $b_{o^{+}}^{p}$ based on Equation~\\ref{eq:sgd4}\n  \\STATE Update $b_{o^{-}}^{p}$ based on Equation~\\ref{eq:sgd5}\n\\ENDFOR\n\\STATE \\textbf{return} $U^{p}$, $V^{p}$, $b^{p}$\n\\end{algorithmic}  \n\\end{algorithm}\n\n\\subsection{Pseudo-code and Complexity Analysis}~\\\\\nThe pseudo-code of our proposed link prediction model is described in Algorithm~\\ref{alg:1}.\nIt takes the knowledge graph $G$ and a specific target predicate $p$ as input and generates the low\ndimensional latent matrices $U^{p}$, $V^{p}$, $b^{p}$ as output. Line 1 constucts the bipartite subgraph\nof predicate $p$, $G_{p}$ given entire knowledge graph $G$. Line 2-3 compute the number of subject and object entities\nas $m$ and $n$ in resultant bipartite subgraph $G_{p}$ respectively. Line 4 generates a collection of triple samples using \nuniform sampling technique. Line 5-7 initialize the matrices $U^{p}$, $V^{p}$, $b^{p}$ using \nGaussian distribution with $0$ mean and $0.1$ standard deviation, assuming all the entries in $U^{p}$, $V^{p}$ and $b^{p}$ are independent.\nLine 8-14 update corresponding rows of matrices $U^{p}$, $V^{p}$, $b^{p}$ based on the sampled instance $(s_p, o^{+}_{p}, o^{-}_{p})$ in each iteration. \nAs the sample generation step in line 4 is prior to the model parameter learning, thus the convergence criteria of Algorithm~\\ref{alg:1} is to iterate\nover all the sampled triples in $D_{p}$.  \n\nGiven the constructed $G_{p}$ as input, the time complexity of the update rules shown in \nEquations~\\ref{eq:sgd1}~\\ref{eq:sgd2}~\\ref{eq:sgd3}~\\ref{eq:sgd4}~\\ref{eq:sgd5} is $\\mathcal{O}(cK)$, where $K$ is the number of latent features. \nThe total computational complexity of Algorithm~\\ref{alg:1} is then $\\mathcal{O}(\\left|D_{p}\\right|\\cdot cK)$, \nwhere $\\left| D_{p}\\right|$ is the total size of pre-sampled triples shown in line 4 of Algorithm~\\ref{alg:1}.\n\n\\section{Experiments and Results}\nThis section presents our experimental analysis of the Algorithm~\\ref{alg:1} for thirteen unique predicates in the well known YAGO2 knowledge graph~\\cite{Hoffart.Suchanek.ea:11}.\n\nWe construct a model for each predicate and describe our evaluation strategies, \nincluding performance metrics and selection of state-of-the-art methods for benchmarking in section 5.1. We aim to answer two questions through our experiments:\n\n\\begin{figure*}\n\\centering\n\\subfigure[HR Comparison among different link recommendation methods]{\\label{fig:a}\\includegraphics[width=56mm]{HR_yago_K_50}}\n\\subfigure[ARHR Comparison among different link recommendation methods]{\\label{fig:b}\\includegraphics[width=56mm]{ARHR_yago_K_50}}        \n\\subfigure[AUC Comparison among different link recommendation methods]{\\label{fig:c}\\includegraphics[width=56mm]{AUC_yago_K_50}}\n\\caption{Link Recommendation Comparison on YAGO2 Relations}\\label{fig:yagoprediction}\n\\end{figure*}\n\n\\begin{enumerate}\n\n\\item How does our approach compare with related work for link recommendation in knowledge graph?\n\n\\item For a predicate $p$, can we reason about the link prediction model performance $M_p$ in terms of the structural metrics of the bipartite graph $G_p$?\n\n\\end{enumerate}\n\n\\begin{table}[h!]\n\\centering\n\\scalebox{0.75}{\n\\begin{tabular}{*{4}{c}}\n\\toprule\nRelation & \\# Subjects & \\# Objects & \\# of Facts in YAGO2 \\\\ \n\\midrule\nImport & 142 & 62 & 391 \\\\\nExport & 140 & 176 & 579 \\\\\nisInterestedIn & 358 & 213 & 464 \\\\\nhasOfficialLanguage & 583 & 214 & 964 \\\\\ndealsWith & 131 & 124 & 945 \\\\\nhappenedIn & 7121 & 5526 & 12500 \\\\\nparticipatedIn & 2330 & 7043 & 16809 \\\\\nisConnectedTo & 2835 & 4391 & 33581 \\\\\nhasChild & 10758 & 12800 & 17320 \\\\\ninfluence & 8056 & 9153 & 25819 \\\\\nwroteMusicFor & 5109 & 21487 & 24271 \\\\\nedited & 549 & 5673 & 5946 \\\\\nowns & 8330 & 24422 & 26536 \\\\\n\\bottomrule\n\\end{tabular}} \n\\caption{Statistics of Various Relations in YAGO2 Dataset}\n\\vspace{-0.10in}\n\\label{tab:yagosta}\n\\end{table}\n\nTable~\\ref{tab:yagosta} shows the statistic of various YAGO2 relations used in our experiments. \\# Subjects and \\# Objects represent the number of subject and object entities associated with its corresponding predicate. The last column shown in Table~\\ref{tab:yagosta} shows the number of facts for each relation in YAGO2. We run all the experiments on a 2.1 GHz Machine with 4GB memory running Linux operating system. The algorithms are implemented in Python language along with NumPy and SciPy libraries for linear algebra operations. The software is available online for download~\\footnote{\\url{https://sites.google.com/site/baichuanzhangpurdue}}.\n\n\\begin{figure*}\n\\centering\n\\subfigure[Graph Density and HR]{\\label{fig:a}\\includegraphics[width=56mm]{density_HR}}\n\\subfigure[Graph Density and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{density_ARHR}}        \n\\subfigure[Graph Density and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{density_AUC}}\n\\subfigure[Graph Average Degree and HR]{\\label{fig:a}\\includegraphics[width=56mm]{degree_HR}}\n\\subfigure[Graph Average Degree and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{degree_ARHR}}        \n\\subfigure[Graph Average Degree and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{degree_AUC}}\n\\subfigure[Clustering Coefficient and HR]{\\label{fig:a}\\includegraphics[width=56mm]{cc_HR}}\n\\subfigure[Clustering Coefficient and ARHR]{\\label{fig:b}\\includegraphics[width=56mm]{cc_ARHR}}        \n\\subfigure[Clustering Coefficient and AUC]{\\label{fig:c}\\includegraphics[width=56mm]{cc_AUC}}\n\n\n\n\\caption{Quantitative Analysis Between Graph Topology and Link Recommendation Model Performance}\\label{fig:topology}\n\\end{figure*}\n\n\n\n\\subsection{Experimental Setting}~\\\\\nFor our experiment, in order to demonstrate the performance of our proposed link prediction model, we use the YAGO2 dataset\nand several evaluation metrics for all compared algorithms. Particularly, for each relation, we split the data into a training part, \nused for model training, and a test part, used for model evaluation. We apply 5-time leave one out evaluation strategy, where for\neach subject, we randomly remove one fact (one subject-object pair) and place it into test set $S_{test}$ and remaining in the training\nset $S_{train}$. For every subject, the training model will generate a size-N ranked list of recommended objects for \nrecommendation task. The evaluation is conducted by comparing the recommendation list of each subject and the\nobject entity of that subject in the test set. Grid search is applied to find regularization parameters, and we set the values of parameters used in section 4.2 as \n$\\lambda_s = \\lambda_{o^{+}} = \\lambda_{o^{-}} = 0.005$. For other model parameters, we fix learning rate\n$\\alpha = 0.2$, and number of latent factors $K = 50$ respectively. \n\nFor parameter in model evaluation, we set $N = 10$. \n\nIn order to illustrate the merit of our proposed approach, we compare our model with the following methods\nfor link prediction in a knowledge graph. Since the problem we solve in this paper is similar to the one-class\nitem recommendation~\\cite{Rendle.Gantner.ea:09} in recommender system domain, we consider the following state-of-the-art one-class\nrecommendation methods as baseline approaches for comparison. \n\n\\begin{enumerate}\n\n\\item \\textbf{Random (Rand):} For each relation, this method randomly selects subject-object entity pair for link recommendation task.\n\n\\item \\textbf{Most Popular (MP):} For each predicate in knowledge base, this method presents a non-personalized ranked object list \nbased on how often object entities are connected among all subject entities. \n\n\\item \\textbf{MF:} The matrix factorization method is proposed by~\\cite{Koren.Bell.ea:09}, which uses a point-wise strategy for solving \nthe one-class item recommendation problem. \n\n\\end{enumerate} \n\nDuring the model evaluation stage, we use three popular metrics, namely Hit Rate (HR), Average Reciprocal Hit-Rank (ARHR), and\nArea Under Curve (AUC), to measure the link recommendation quality of our proposed approach in comparison to baseline methods.\nHR is defined as follows:\n\n\n", "index": 25, "text": "\\begin{equation}\nHR = \\frac{\\#hits}{\\#subjects}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"HR=\\frac{\\#hits}{\\#subjects}\" display=\"block\"><mrow><mrow><mi>H</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": " \n\nwhere if an object of a subject is recommended for connection in knowledge graph which we name as hit under this scenario,\n$p_i$ is the position of the object in the ranked recommendation list. As we can see, ARHR is a weighted version of HR and it\ncaptures the importance of recommended object in the recommendation list.\n\nThe last metric, AUC is defined as follows:\n\n\n", "itemtype": "equation", "pos": -1, "prevtext": "\n\nwhere $\\#subjects$ is the total number of subject entities in test set, and $\\#hits$ is the number of subjects\nwhose object entity in the test set is recommended in the size-N recommendation list. The second evaluation metric,\nARHR, considering the ranking of the recommended object for each subject entity in knowledge graph, is defined  as\nbelow:\n\n\n", "index": 27, "text": "\\begin{equation}\nARHR = \\frac{1}{\\#subjects}\\sum_{i=1}^{\\#hits}\\frac{1}{p_i}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"ARHR=\\frac{1}{\\#subjects}\\sum_{i=1}^{\\#hits}\\frac{1}{p_{i}}\" display=\"block\"><mrow><mrow><mi>A</mi><mo>\u2062</mo><mi>R</mi><mo>\u2062</mo><mi>H</mi><mo>\u2062</mo><mi>R</mi></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow></mfrac><mo>\u2062</mo><mrow><munderover><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>h</mi><mo>\u2062</mo><mi>i</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow></munderover><mfrac><mn>1</mn><msub><mi>p</mi><mi>i</mi></msub></mfrac></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.03778.tex", "nexttext": "\n\nWhere $E(s) = \\{(o^{+}, o^{-}) |(s,o^{+})\\in S_{test} \\cap (s,o^{-}) \\not \\in (S_{test} \\cup S_{train}) \\}$, and\n$\\delta()$ is the indicator function.\n\nFor all of three metrics, higher values indicate better model performance. Specifically, the trivial AUC of a random predictor\nis 0.5 and the best value of AUC is 1.\n\n\\subsection{YAGO2 Relation Prediction Performance}~\\\\\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure~\\ref{fig:yagoprediction} shows the average link prediction performance for YAGO2 relations using various methods.\nOur proposed latent feature embedding approach shows overall improvement compared with other algorithms on most\nof relations in YAGO2. For instance, for all the YAGO2 predicates used in the experiment, our proposed  model consistently outperforms\nMF based method, which demonstrates the empirical experience that pairwise ranking based method achieves much better performance than pointwise \nregression based method given implicit feedback for link recommendation task. Compared with Popularity based recommendation method MP, our method obtains\nbetter performance for most predicates. For example, predicates such as ``participate\",``connect\",``hasChild\", \nand ``influence\", our proposed model achieves more than 10 times better performance in terms of both HR and ARHR. However, for several predicates\nsuch as ``import\", ``export\", and ``language\", MP based method performs the best among all the competing methods. The good performance of MP\nis owing to the semantic meaning of specific predicate. For instance, ``import\" represents Country/Product relation in YAGO2, which indicates the types \nof its subject and object entities are geographic region and commodity respectively. For such a predicate, most popular\nobject entities such as food, cloth, fuel are linked to most of the countries, which helps MP based method obtain good link recommendation performance.\n\n\n\n\\subsection{Analysis and Discussion}~\\\\\n\n\n\nFigure~\\ref{fig:yagoprediction} shows that the link prediction model performance widely varies from predicate to predicate in the YAGO2 knowledge base. \nFor example, the HR of predicate ``dealsWith\" is significantly better than ``own\". Thus it is critical that we quantitatively understand the model performance\nacross various relations in a knowledge graph.  Recall from the \\textbf{Problem Statement} that given a predicate $p$, our model $M_p$ only accounts for the bipartite subgraph $G_p$.\nMotivated by~\\cite{Liben.Kleinberg:03}, we study the impact of resultant graph structure of $G_p$ on the performance of $M_p$.\n\nFor each predicate $p$, we compute several graph topology metrics on its bipartite subgraph $G_p$ \nsuch as graph density, graph average degree, and clustering coefficient. \nFigure~\\ref{fig:topology} shows the quantitative analysis between graph structure and \nlink prediction model performance of each predicate. In each subfigure, x-axis represents the computed graph topology metric value of each predicate and\ny-axis denotes our proposed link prediction model performance in terms of HR, ARHR, and AUC.  Each cross point shown in blue represents one specific\nYAGO2 predicate used in our experiments. Then we developed a linear regression model to understand the correlation \nbetween link prediction model performance and each graph metric. For each linear regression curve shown in red color,\nwe also report its slope, intercept, and correlation coefficient (rvalue) to capture the association trend. \n\nFrom Figure~\\ref{fig:topology}, both graph density and graph average degree show strong positive correlation signal with \nproposed link prediction model as demonstrated by rvalue. As our approach is inspired by collaborative filtering for recommender systems that accept a user-item matrix as input, for resultant graph of each predicate, \nhigher graph density indicates higher matrix density in user-item matrix, which naturally leads to better recommendation performance in \nrecommender system domain. Similar explanation can be adapted to graph average degree. For the clustering\ncoefficient, it shows strong negative correlation signal with link prediction model performance. For instance, in terms of AUC, the rvalue is around\n$-0.69$. As clustering coefficient (cc) is the number of closed triples over the total number of triples in graph, smaller value of cc indicates\nlower fraction of closed triples in the graph. Based on the transitivity property of a social graph, which states the friends of your friend have high likelihood\nto be friends themselves~\\cite{Zhang.Saha.ea:14, Saha.Zhang.ea:15}, it is relatively easier for link prediction model to predict (i.e.,hit) such link \nwith open triple property in the graph, which leads to better link prediction performance. \n\n\n\n\n\n\n\\section{Conclusion and Future Work}\nInspired by the success of collaborative filtering algorithms for recommender systems, we propose a latent feature based embedding model for the task of link prediction in a knowledge graph.  Our proposed method provides a measure of ``confidence\" for adding a triple into the knowledge graph. We evaluate our implementation on the well known YAGO2 knowledge graph. The experiments show that our Bayesian Personalized Ranking based latent feature embedding approach achieves better performance compared with\ntwo state-of-art recommender system models: Most Popular and Matrix Factorization. \nWe also develop a linear regression model to quantitatively study the correlation between the performance of link prediction model itself \nand various topological metrics of the graph from which the models are constructed. The regression analysis shows strong correlation between the link prediction performance and graph topological features, such as graph density, average degree and clustering coefficient.\n\nFor a given predicate, we build link prediction models solely based on the bipartite subgraph of the original knowledge graph.  \nHowever, as real-world experience suggests, the existence of a relation between two entities can also be predicted from the presence of other relations, \neither direct or through common neighbors. As an example, the knowledge of where someone studies and who they are friends with is useful \nto predict possible workplaces. Incorporating such intuition as ``social signals\" into our current model will be the prime candidate for an immediate future work.\nAnother future work would be to update the knowledge graph based on the newer facts that become available over time in streaming data sources.\n\n\\section*{Acknowledgement}\nThis work was supported by the Analysis In Motion Initiative at Pacific Northwest National Laboratory, which is operated by Battelle Memorial Institute, \nand by Mohammad Al Hasan's NSF CAREER Award (IIS-1149851).\n\n\\bibliographystyle{abbrv}\n\\bibliography{link_prediction}\n\n\\balance\n\n", "itemtype": "equation", "pos": 34890, "prevtext": " \n\nwhere if an object of a subject is recommended for connection in knowledge graph which we name as hit under this scenario,\n$p_i$ is the position of the object in the ranked recommendation list. As we can see, ARHR is a weighted version of HR and it\ncaptures the importance of recommended object in the recommendation list.\n\nThe last metric, AUC is defined as follows:\n\n\n", "index": 29, "text": "\\begin{equation}\n\\scalebox{0.85} {$AUC = \\frac{1}{\\#subjects}\\sum_{s \\in subjects} \\frac{1}{\\mid E(s) \\mid}\\sum_{(o^{+},o^{-}) \\in E(s)} \\delta(x_{s,o^{+}} > x_{s,o^{-}})$}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1.m1\" class=\"ltx_Math\" alttext=\"AUC=\\frac{1}{\\#subjects}\\sum_{s\\in subjects}\\frac{1}{\\mid E(s)\\mid}\\sum_{(o^{+%&#10;},o^{-})\\in E(s)}\\delta(x_{s,o^{+}}&gt;x_{s,o^{-}})\" display=\"inline\"><mrow><mi>A</mi><mi>U</mi><mi>C</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant=\"normal\">#</mi><mo>\u2062</mo><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow></mfrac><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mi>s</mi><mo>\u2208</mo><mrow><mi>s</mi><mo>\u2062</mo><mi>u</mi><mo>\u2062</mo><mi>b</mi><mo>\u2062</mo><mi>j</mi><mo>\u2062</mo><mi>e</mi><mo>\u2062</mo><mi>c</mi><mo>\u2062</mo><mi>t</mi><mo>\u2062</mo><mi>s</mi></mrow></mrow></msub><mfrac><mn>1</mn><mrow><mo>\u2223</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo></mrow></mfrac><msub><mo largeop=\"true\" symmetric=\"true\">\u2211</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msup><mi>o</mi><mo>+</mo></msup><mo>,</mo><msup><mi>o</mi><mo>-</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u2208</mo><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mi>\u03b4</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>,</mo><msup><mi>o</mi><mo>+</mo></msup></mrow></msub><mo>&gt;</mo><msub><mi>x</mi><mrow><mi>s</mi><mo>,</mo><msup><mi>o</mi><mo>-</mo></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}]