[{"file": "1601.01085.tex", "nexttext": "\nwhere ${\\bm{{h}}}^{\\rightarrow}_i$ and ${\\bm{{h}}}^{\\leftarrow}_i$ are the RNN hidden states.  \n\nThe left-to-right RNN\nfunction is defined as \n\n", "itemtype": "equation", "pos": 4420, "prevtext": "\n\n\\maketitle\n\n\\begin{abstract}\nNeural encoder-decoder models of machine translation have achieved impressive results,\nrivalling traditional translation models. However their modelling formulation\nis overly simplistic, and omits several key inductive biases built into\ntraditional models. In this paper we extend the attentional neural \ntranslation model to include structural biases from word based alignment models, \nincluding positional bias, Markov conditioning, fertility and agreement over translation directions. \nWe show improvements over a baseline attentional model and standard phrase-based model \nover several language pairs, evaluating on difficult languages in a low resource setting.\n\\end{abstract}\n\n\\section{Introduction}\nRecently, models of end-to-end machine translation based\non neural network classification have been shown to produce excellent\ntranslations, rivalling or in some cases surpassing traditional statistical machine\ntranslation systems \\cite{kalchbrenner13emnlp,sutskever2014sequence,bahdanau2015neural}.\nThis is despite the neural approaches using an overall simpler model, with fewer assumptions\nabout the learning and prediction problem.\n\nBroadly, neural approaches are based around the notion of an\n\\emph{encoder-decoder}\n\\cite{sutskever2014sequence}, in which the source language is \\emph{encoded}\ninto a distributed representation, followed by a \\emph{decoding}\nstep which generates the target translation. \n\nWe focus on the \\emph{attentional model} of translation \\cite{bahdanau2015neural}  \nwhich uses a dynamic representation of the source sentence \nwhile allowing the decoder to \\emph{attend} to different parts of\nthe source as it generates the target sentence.\n\nThe attentional model raises intriguing opportunities, given the correspondence between the notions of attention \nand alignment in traditional word-based  machine translation models \\cite{brown93}.\n\n\nIn this paper we map modelling biases from word based translation models into the attentional model,\nsuch that known linguistic elements of translation can be better captured. \n\n\n\nWe incorporate \\emph{absolute positional bias} whereby word order \ntends to be similar between the source sentence and its translation \n(e.g., IBM Model 2 and  \\cite{dyer-chahuneau-smith:2013:NAACL-HLT}), \n \\emph{fertility} whereby each instance of a source word type tends \nto be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), \n\\emph{relative position bias} whereby prior preferences for monotonic \nalignments/attention can be encouraged  (e.g., IBM Model 4, 5 and HMM-based Alignment \\cite{vogel96}),\nand \\emph{alignment consistency} whereby the attention in \\emph{both} translation \ndirections are encourged to agree  (e.g. symmetrization heuristics \\cite{och03} \nor joint modelling \\cite{liang2006alignment,ganchev-gracca-taskar:2008:ACLMain}). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe provide an empirical analysis of incorporating the above structural biases into the attentional model, \nconsidering low resource translation scenario over four language-pairs. \n\nOur results demonstrate consistent improvements over vanila encoder-decoder and attentional model in \nterms of the perplexity and BLEU score, e.g.\\@ up to 3.5 BLEU points   \nwhen re-ranking the candiate translations generated by a state-of-the-art phrase based model.\n\\ifnaaclfinal\\else\\footnote{The source code will be released on publication}\\fi\n\n\n\n\n\n\n\n\n\n\n\\section{The attentional model of translation}\n\\label{sec:sentence-encdec}\n\n\n\n\nWe start by reviewing the attentional model of translation \\cite{bahdanau2015neural},\nas illustrated in Fig.~\\ref{fig:sentence-attentional}, before presenting our extensions in \\S\\ref{sec:sent-alignment-structure}.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{attentional_from_presentation}\n\\caption{Attentional model of translation\n  \\protect\\cite{bahdanau2015neural}. The encoder is shown below the decoder,\n  and the edges connecting the two corresponding to the attention\n  mechanism. Heavy edges denote a higher attention weight, and these\n  values are also displayed in matrix form, with one row for each target word. }\n\\label{fig:sentence-attentional}\n\\end{figure}\n\n\n\n\\paragraph{Encoder} The encoding of the source sentence is formulated\nusing a pair of RNNs (denoted \\emph{bi-RNN}) one operating\nleft-to-right over the input sequence and another operating\nright-to-left,\n\n", "index": 1, "text": "\\begin{align*}\n{\\bm{{h}}}^{\\rightarrow}_i &= \\text{RNN}({\\bm{{h}}}^{\\rightarrow}_{i-1}, {\\bm{{{r}}}}^{(\\text{{s}})}_{s_i}) \\\\\n{\\bm{{h}}}^{\\leftarrow}_i &= \\text{RNN}({\\bm{{h}}}^{\\rightarrow}_{i+1}, {\\bm{{{r}}}}^{(\\text{{s}})}_{s_i}) \n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{h}}}^{\\rightarrow}_{i}\" display=\"inline\"><msubsup><mi>\ud835\udc89</mi><mi>i</mi><mo>\u2192</mo></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{RNN}({\\bm{{h}}}^{\\rightarrow}_{i-1},{\\bm{{{r}}}}^{(\\text{{%&#10;s}})}_{s_{i}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>RNN</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>\u2192</mo></msubsup><mo>,</mo><msubsup><mi>\ud835\udc93</mi><msub><mi>s</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mtext>s</mtext><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{h}}}^{\\leftarrow}_{i}\" display=\"inline\"><msubsup><mi>\ud835\udc89</mi><mi>i</mi><mo>\u2190</mo></msubsup></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\text{RNN}({\\bm{{h}}}^{\\rightarrow}_{i+1},{\\bm{{{r}}}}^{(\\text{{%&#10;s}})}_{s_{i}})\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mtext>RNN</mtext><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>\u2192</mo></msubsup><mo>,</mo><msubsup><mi>\ud835\udc93</mi><msub><mi>s</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mtext>s</mtext><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nwhere ${\\bm{{h}}}_0^{\\rightarrow} {\\in \\mathbb{R}^{{H}}}$ is a learned parameter vector, as are\n${\\mathbf{{{R}}}}^{(\\text{{s}})} {\\in \\mathbb{R}^{{V_S} \\times {E}}}$,\n${\\bm{{W}}}_{si}^{\\rightarrow} {\\in \\mathbb{R}^{{H} \\times {E}}}$,  ${\\bm{{W}}}_{sh}^{\\rightarrow} {\\in \\mathbb{R}^{{H} \\times {H}}}$ and ${\\bm{{b}}}_{s}^{\\rightarrow} {\\in \\mathbb{R}^{{H}}}$, with\n$H$ the number of hidden units,  $V_S$ the size of the source\nvocabulary and $E$ the word embedding\ndimensionality.\\footnote{Similarly, ${\\bm{{h}}}_0^{\\leftarrow} {\\in \\mathbb{R}^{{H}}}, {\\bm{{W}}}_{si}^{\\leftarrow} {\\in \\mathbb{R}^{{H} \\times {E}}}, {\\bm{{W}}}_{sh}^{\\leftarrow} {\\in \\mathbb{R}^{{H} \\times {H}}}, {\\bm{{b}}}_{s}^{\\leftarrow} {\\in \\mathbb{R}^{{H}}}$ are the parameters of the right-to-left RNN. Note that we use a long short term memory unit \\cite{Hochreiter1997} in place of the RNN, shown here for simplicity of exposition.}\n\nEach source word is then\n represented as a pair of hidden states, one from each RNN, \n${\\bm{{e}}}_i  = \\left[ \\begin{array}{c} {\\bm{{h}}}^{\\rightarrow}_i \\\\ {\\bm{{h}}}^{\\leftarrow}_i \\end{array} \\right]$.\nThis encodes not only the word but also its left and right context,\nwhich can provide important evidence for its translation.\n\n\n\n\nA crucial question is how this dynamic sized matrix \n${\\mathbf{{E}}} = \\left[ {\\bm{{e}}}_1, {\\bm{{e}}}_2, \\ldots, {\\bm{{e}}}_I\\right] {\\in \\mathbb{R}^{{I} \\times {H}}}$ \ncan be used in the decoder to generate the target sentence.\nAs with Sutskever's encoder-decoder, the target sentence is created\nleft-to-right using a RNN, while the encoded source is used to bias\nthe process as an auxiliary input. The mechanism for this\nbias is by attentional vectors, i.e. vectors of scores\nover each source sentence location, which are used to aggregate the\ndynamic source encoding into a fixed length vector.\n\n\n\\paragraph{Decoder} The decoder operates as a standard RNN over the  translation\n${\\bm{{t}}}$, formulated as follows\n{\\small\n\n", "itemtype": "equation", "pos": 4810, "prevtext": "\nwhere ${\\bm{{h}}}^{\\rightarrow}_i$ and ${\\bm{{h}}}^{\\leftarrow}_i$ are the RNN hidden states.  \n\nThe left-to-right RNN\nfunction is defined as \n\n", "index": 3, "text": "\\begin{align}\n{\\bm{{h}}}^{\\rightarrow}_i = \\tanh\\left( {\\bm{{W}}}_{si}^{\\rightarrow} {\\bm{{{r}}}}^{(\\text{{s}})}_{s_i} +  {\\bm{{W}}}_{sh}^{\\rightarrow} {\\bm{{h}}}^{\\rightarrow}_{i-1} +  {\\bm{{b}}}_{s}^{\\rightarrow} \\right)\n\\label{eq:sent-rnn-encoder}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{h}}}^{\\rightarrow}_{i}=\\tanh\\left({\\bm{{W}}}_{si}^{%&#10;\\rightarrow}{\\bm{{{r}}}}^{(\\text{{s}})}_{s_{i}}+{\\bm{{W}}}_{sh}^{\\rightarrow}{%&#10;\\bm{{h}}}^{\\rightarrow}_{i-1}+{\\bm{{b}}}_{s}^{\\rightarrow}\\right)\" display=\"inline\"><mrow><msubsup><mi>\ud835\udc89</mi><mi>i</mi><mo>\u2192</mo></msubsup><mo>=</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>i</mi></mrow><mo>\u2192</mo></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc93</mi><msub><mi>s</mi><mi>i</mi></msub><mrow><mo stretchy=\"false\">(</mo><mtext>s</mtext><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msubsup><mi>\ud835\udc7e</mi><mrow><mi>s</mi><mo>\u2062</mo><mi>h</mi></mrow><mo>\u2192</mo></msubsup><mo>\u2062</mo><msubsup><mi>\ud835\udc89</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mo>\u2192</mo></msubsup></mrow><mo>+</mo><msubsup><mi>\ud835\udc83</mi><mi>s</mi><mo>\u2192</mo></msubsup></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\n}\nwhere the decoder RNN is defined analogously to\nEq~\\ref{eq:sent-rnn-encoder} but with an additional input, the source attention component\n${\\bm{{c}}}_j {\\in \\mathbb{R}^{{2H}}}$ and weighting matrix ${\\mathbf{{{W}}}}^{(\\text{{ta}})} {\\in \\mathbb{R}^{{H} \\times {2H}}}$.\nThe hidden state of the recurrence is then passed through a single\nhidden layer\\footnote{In \\newcite{bahdanau2015neural} they use a max-out\n  layer for this final step, however we found this to be a needless\n  complication, and instead use a standard hidden layer with tanh activation.}\n (Eq~\\ref{eq:sent-am-hidden-out}) in combination with the\n source attention and target word \nusing weighting matrices\n${\\mathbf{{{W}}}}^{(\\text{{uc}})} {\\in \\mathbb{R}^{{H} \\times {2H}}}$ and\n${\\mathbf{{{W}}}}^{(\\text{{ui}})} {\\in \\mathbb{R}^{{H} \\times {E}}}$.\nIn Eq~\\ref{eq:sent-am-out} this vector is transformed to be\ntarget vocabulary sized, using weight matrix \n${\\mathbf{{{W}}}}^{(\\text{{ou}})} {\\in \\mathbb{R}^{{V_T} \\times {H}}}$ and bias ${\\bm{{{b}}}}^{(\\text{{to}})} {\\in \\mathbb{R}^{{V_T}}}$, after which a $\\operatorname{softmax}$ is taken, and the resulting normalised\nvector used as the parameters of a Categorical distribution in generating the next target word.\n\nThe presentation above assumes a simple RNN is used to define the\nrecurrence over hidden states, however we can easily use alternative\nformulations of recurrent networks including multiple-layer RNNs,\ngated recurrent units (GRU; \\newcite{ChoGRU2014}), or long short-term memory (LSTM; \\newcite{Hochreiter1997}) units.\nThese more advanced methods allow for more efficient learning of more\ncomplex concepts, particularly long distance effects. Empirically we found\nLSTMs to be the best performing, and therefore use these units herein.\n\n\nThe last key detail is the attentional component ${\\bm{{c}}}_j$ in Eqs~\\ref{eq:sent-am-decoder} and~\\ref{eq:sent-am-hidden-out}, which is defined as follows\n\n", "itemtype": "equation", "pos": 7044, "prevtext": "\nwhere ${\\bm{{h}}}_0^{\\rightarrow} {\\in \\mathbb{R}^{{H}}}$ is a learned parameter vector, as are\n${\\mathbf{{{R}}}}^{(\\text{{s}})} {\\in \\mathbb{R}^{{V_S} \\times {E}}}$,\n${\\bm{{W}}}_{si}^{\\rightarrow} {\\in \\mathbb{R}^{{H} \\times {E}}}$,  ${\\bm{{W}}}_{sh}^{\\rightarrow} {\\in \\mathbb{R}^{{H} \\times {H}}}$ and ${\\bm{{b}}}_{s}^{\\rightarrow} {\\in \\mathbb{R}^{{H}}}$, with\n$H$ the number of hidden units,  $V_S$ the size of the source\nvocabulary and $E$ the word embedding\ndimensionality.\\footnote{Similarly, ${\\bm{{h}}}_0^{\\leftarrow} {\\in \\mathbb{R}^{{H}}}, {\\bm{{W}}}_{si}^{\\leftarrow} {\\in \\mathbb{R}^{{H} \\times {E}}}, {\\bm{{W}}}_{sh}^{\\leftarrow} {\\in \\mathbb{R}^{{H} \\times {H}}}, {\\bm{{b}}}_{s}^{\\leftarrow} {\\in \\mathbb{R}^{{H}}}$ are the parameters of the right-to-left RNN. Note that we use a long short term memory unit \\cite{Hochreiter1997} in place of the RNN, shown here for simplicity of exposition.}\n\nEach source word is then\n represented as a pair of hidden states, one from each RNN, \n${\\bm{{e}}}_i  = \\left[ \\begin{array}{c} {\\bm{{h}}}^{\\rightarrow}_i \\\\ {\\bm{{h}}}^{\\leftarrow}_i \\end{array} \\right]$.\nThis encodes not only the word but also its left and right context,\nwhich can provide important evidence for its translation.\n\n\n\n\nA crucial question is how this dynamic sized matrix \n${\\mathbf{{E}}} = \\left[ {\\bm{{e}}}_1, {\\bm{{e}}}_2, \\ldots, {\\bm{{e}}}_I\\right] {\\in \\mathbb{R}^{{I} \\times {H}}}$ \ncan be used in the decoder to generate the target sentence.\nAs with Sutskever's encoder-decoder, the target sentence is created\nleft-to-right using a RNN, while the encoded source is used to bias\nthe process as an auxiliary input. The mechanism for this\nbias is by attentional vectors, i.e. vectors of scores\nover each source sentence location, which are used to aggregate the\ndynamic source encoding into a fixed length vector.\n\n\n\\paragraph{Decoder} The decoder operates as a standard RNN over the  translation\n${\\bm{{t}}}$, formulated as follows\n{\\small\n\n", "index": 5, "text": "\\begin{align}\n{\\bm{{g}}}_{j} & = \\tanh \\left( {\\mathbf{{{W}}}}^{(\\text{{th}})} {\\bm{{g}}}_{j-1} + {\\mathbf{{{W}}}}^{(\\text{{ti}})} {\\bm{{{r}}}}^{(\\text{{t}})}_{t_{j-1}} + {\\mathbf{{{W}}}}^{(\\text{{ta}})} {\\bm{{c}}}_j \\right) \n\\label{eq:sent-am-decoder} \\\\\n{\\bm{{u}}}_j & = \\tanh \\left( {\\bm{{g}}}_j +  {\\mathbf{{{W}}}}^{(\\text{{uc}})} {\\bm{{c}}}_j + {\\mathbf{{{W}}}}^{(\\text{{ui}})} {\\bm{{{r}}}}^{(\\text{{t}})}_{t_{j-1}}  \\right) \n\\label{eq:sent-am-hidden-out} \\\\\nt_{j} & \\sim {\\operatorname{softmax}} \\left( {\\mathbf{{{W}}}}^{(\\text{{ou}})} {\\bm{{u}}}_j + {\\bm{{{b}}}}^{(\\text{{to}})}  \\right)\n\\label{eq:sent-am-out}\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{g}}}_{j}\" display=\"inline\"><msub><mi>\ud835\udc88</mi><mi>j</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\tanh\\left({\\mathbf{{{W}}}}^{(\\text{{th}})}{\\bm{{g}}}_{j-1}+{%&#10;\\mathbf{{{W}}}}^{(\\text{{ti}})}{\\bm{{{r}}}}^{(\\text{{t}})}_{t_{j-1}}+{\\mathbf{%&#10;{{W}}}}^{(\\text{{ta}})}{\\bm{{c}}}_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>th</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc88</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ti</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc93</mi><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mtext>t</mtext><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ta</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>j</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{u}}}_{j}\" display=\"inline\"><msub><mi>\ud835\udc96</mi><mi>j</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\tanh\\left({\\bm{{g}}}_{j}+{\\mathbf{{{W}}}}^{(\\text{{uc}})}{\\bm{{%&#10;c}}}_{j}+{\\mathbf{{{W}}}}^{(\\text{{ui}})}{\\bm{{{r}}}}^{(\\text{{t}})}_{t_{j-1}}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><msub><mi>\ud835\udc88</mi><mi>j</mi></msub><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>uc</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc84</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ui</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc93</mi><msub><mi>t</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mrow><mo stretchy=\"false\">(</mo><mtext>t</mtext><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle t_{j}\" display=\"inline\"><msub><mi>t</mi><mi>j</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sim{\\operatorname{softmax}}\\left({\\mathbf{{{W}}}}^{(\\text{{ou}})%&#10;}{\\bm{{u}}}_{j}+{\\bm{{{b}}}}^{(\\text{{to}})}\\right)\" display=\"inline\"><mrow><mi/><mo>\u223c</mo><mrow><mo>softmax</mo><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ou</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc96</mi><mi>j</mi></msub></mrow><mo>+</mo><msup><mi>\ud835\udc83</mi><mrow><mo stretchy=\"false\">(</mo><mtext>to</mtext><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nwith the scalars $f_{ji}$ denoting the compatibility between the target hidden state\n${\\bm{{g}}}_{j-1}$ and the source encoding ${\\bm{{e}}}_i$. This is defined as a neural\nnetwork with one hidden layer of size $A$ and a single output, parameterised by\n${\\mathbf{{{W}}}}^{(\\text{{ae}})} {\\in \\mathbb{R}^{{A} \\times {2H}}}$,\n${\\mathbf{{{W}}}}^{(\\text{{ah}})} {\\in \\mathbb{R}^{{A} \\times {H}}}$ and\n${\\bm{{v}}} {\\in \\mathbb{R}^{{A}}}$.\nThe $\\operatorname{softmax}$ then normalises the scalar compatibility values such that\nfor a given target word $j$, the values of $\\alpha_j$ can be\ninterpreted as alignment probabilities to each source\nlocation. Finally, these alignments are used to to reweight\nthe source components $E$ to produce a fixed length context \nrepresentation.\n\nTraining of this model is done by minimising the cross-entropy of the target sentence,\nmeasured word-by-word as for a language model. \n\n\n\n\nWe use standard stochastic gradient\noptimisation using the back-propagation technique for computation of\npartial derivatives according to the chain rule.\n\n\n\n\n\n\n\n\n\n\\section{Incorporating Structural Biases}\n\\label{sec:sent-alignment-structure}\n\nThe attentional model, as described above, provides a powerful and\nelegant model of translation in which alignments between source and\ntarget words are learned through the implicit conditioning context afforded\nby the attention mechanism. Despite its elegance, the attentional model omits several key \ncomponents of a traditional alignment models such as the IBM models\n\\cite{brown93} and Vogel's hidden Markov Model \\cite{vogel96} as\nimplemented in the GIZA++ toolkit \\cite{och03}. Combining the strengths\nof this highly successful body of research into a neural model of\nmachine translation holds potential to further improve modelling\naccuracy of neural techniques. \nBelow we outline methods for incorporating these factors as structural biases into the attentional model.\n\n\n\\subsection{Position bias}\n\n\nFirst we consider position bias, based on the observation that a word at a given relative position \n  in the source tends to align to a word at a similiar relative position in the target, $\\frac{i}{I} \\approx \\frac{j}{J}$ (IBM 2). Related, alignments tend to occur near the diagonal \\cite{dyer-chahuneau-smith:2013:NAACL-HLT}, when considering the alignments as a binary $I \\times J$ matrix (illustrated in Figure~\\ref{fig:sentence-attentional}), where the cell at $(i,j)$ denotes whether an alignment exists between source word $i$  and target word $j$. \n\nWe include a position bias through redefining the pre-normalised attention scalars $f_{ji}$ in Eq~\\ref{eq:sent-fji} as:\n\n", "itemtype": "equation", "pos": 9605, "prevtext": "\n}\nwhere the decoder RNN is defined analogously to\nEq~\\ref{eq:sent-rnn-encoder} but with an additional input, the source attention component\n${\\bm{{c}}}_j {\\in \\mathbb{R}^{{2H}}}$ and weighting matrix ${\\mathbf{{{W}}}}^{(\\text{{ta}})} {\\in \\mathbb{R}^{{H} \\times {2H}}}$.\nThe hidden state of the recurrence is then passed through a single\nhidden layer\\footnote{In \\newcite{bahdanau2015neural} they use a max-out\n  layer for this final step, however we found this to be a needless\n  complication, and instead use a standard hidden layer with tanh activation.}\n (Eq~\\ref{eq:sent-am-hidden-out}) in combination with the\n source attention and target word \nusing weighting matrices\n${\\mathbf{{{W}}}}^{(\\text{{uc}})} {\\in \\mathbb{R}^{{H} \\times {2H}}}$ and\n${\\mathbf{{{W}}}}^{(\\text{{ui}})} {\\in \\mathbb{R}^{{H} \\times {E}}}$.\nIn Eq~\\ref{eq:sent-am-out} this vector is transformed to be\ntarget vocabulary sized, using weight matrix \n${\\mathbf{{{W}}}}^{(\\text{{ou}})} {\\in \\mathbb{R}^{{V_T} \\times {H}}}$ and bias ${\\bm{{{b}}}}^{(\\text{{to}})} {\\in \\mathbb{R}^{{V_T}}}$, after which a $\\operatorname{softmax}$ is taken, and the resulting normalised\nvector used as the parameters of a Categorical distribution in generating the next target word.\n\nThe presentation above assumes a simple RNN is used to define the\nrecurrence over hidden states, however we can easily use alternative\nformulations of recurrent networks including multiple-layer RNNs,\ngated recurrent units (GRU; \\newcite{ChoGRU2014}), or long short-term memory (LSTM; \\newcite{Hochreiter1997}) units.\nThese more advanced methods allow for more efficient learning of more\ncomplex concepts, particularly long distance effects. Empirically we found\nLSTMs to be the best performing, and therefore use these units herein.\n\n\nThe last key detail is the attentional component ${\\bm{{c}}}_j$ in Eqs~\\ref{eq:sent-am-decoder} and~\\ref{eq:sent-am-hidden-out}, which is defined as follows\n\n", "index": 7, "text": "\\begin{align}\nf_{ji} & = {\\bm{{v}}}^\\top \\tanh \\left( {\\mathbf{{{W}}}}^{(\\text{{ae}})} {\\bm{{e}}}_i + {\\mathbf{{{W}}}}^{(\\text{{ah}})} {\\bm{{g}}}_{j-1}\\right) \\label{eq:sent-fji} \\\\\n{\\bm{{\\alpha}}}_j & = {\\operatorname{softmax}} \\left( {\\bm{{f}}}_j \\right) \\nonumber \\\\ \n{\\bm{{c}}}_j &= \\sum_i \\alpha_{ji} {\\bm{{e}}}_i \\nonumber\n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{ji}\" display=\"inline\"><msub><mi>f</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\bm{{v}}}^{\\top}\\tanh\\left({\\mathbf{{{W}}}}^{(\\text{{ae}})}{\\bm%&#10;{{e}}}_{i}+{\\mathbf{{{W}}}}^{(\\text{{ah}})}{\\bm{{g}}}_{j-1}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><msup><mi>\ud835\udc97</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ae</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc86</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ah</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc88</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{\\alpha}}}_{j}\" display=\"inline\"><msub><mi>\ud835\udf36</mi><mi>j</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={\\operatorname{softmax}}\\left({\\bm{{f}}}_{j}\\right)\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mo>softmax</mo><mo>\u2061</mo><mrow><mo>(</mo><msub><mi>\ud835\udc87</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{{c}}}_{j}\" display=\"inline\"><msub><mi>\ud835\udc84</mi><mi>j</mi></msub></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{i}\\alpha_{ji}{\\bm{{e}}}_{i}\" display=\"inline\"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><mrow><msub><mi>\u03b1</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc86</mi><mi>i</mi></msub></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nwhere the new component in the input is a simple feature function of\nthe positions in the source and target sentences and the source length, \n", "itemtype": "equation", "pos": 12585, "prevtext": "\nwith the scalars $f_{ji}$ denoting the compatibility between the target hidden state\n${\\bm{{g}}}_{j-1}$ and the source encoding ${\\bm{{e}}}_i$. This is defined as a neural\nnetwork with one hidden layer of size $A$ and a single output, parameterised by\n${\\mathbf{{{W}}}}^{(\\text{{ae}})} {\\in \\mathbb{R}^{{A} \\times {2H}}}$,\n${\\mathbf{{{W}}}}^{(\\text{{ah}})} {\\in \\mathbb{R}^{{A} \\times {H}}}$ and\n${\\bm{{v}}} {\\in \\mathbb{R}^{{A}}}$.\nThe $\\operatorname{softmax}$ then normalises the scalar compatibility values such that\nfor a given target word $j$, the values of $\\alpha_j$ can be\ninterpreted as alignment probabilities to each source\nlocation. Finally, these alignments are used to to reweight\nthe source components $E$ to produce a fixed length context \nrepresentation.\n\nTraining of this model is done by minimising the cross-entropy of the target sentence,\nmeasured word-by-word as for a language model. \n\n\n\n\nWe use standard stochastic gradient\noptimisation using the back-propagation technique for computation of\npartial derivatives according to the chain rule.\n\n\n\n\n\n\n\n\n\n\\section{Incorporating Structural Biases}\n\\label{sec:sent-alignment-structure}\n\nThe attentional model, as described above, provides a powerful and\nelegant model of translation in which alignments between source and\ntarget words are learned through the implicit conditioning context afforded\nby the attention mechanism. Despite its elegance, the attentional model omits several key \ncomponents of a traditional alignment models such as the IBM models\n\\cite{brown93} and Vogel's hidden Markov Model \\cite{vogel96} as\nimplemented in the GIZA++ toolkit \\cite{och03}. Combining the strengths\nof this highly successful body of research into a neural model of\nmachine translation holds potential to further improve modelling\naccuracy of neural techniques. \nBelow we outline methods for incorporating these factors as structural biases into the attentional model.\n\n\n\\subsection{Position bias}\n\n\nFirst we consider position bias, based on the observation that a word at a given relative position \n  in the source tends to align to a word at a similiar relative position in the target, $\\frac{i}{I} \\approx \\frac{j}{J}$ (IBM 2). Related, alignments tend to occur near the diagonal \\cite{dyer-chahuneau-smith:2013:NAACL-HLT}, when considering the alignments as a binary $I \\times J$ matrix (illustrated in Figure~\\ref{fig:sentence-attentional}), where the cell at $(i,j)$ denotes whether an alignment exists between source word $i$  and target word $j$. \n\nWe include a position bias through redefining the pre-normalised attention scalars $f_{ji}$ in Eq~\\ref{eq:sent-fji} as:\n\n", "index": 9, "text": "\\begin{align}\nf_{ji}  = {\\bm{{v}}}^\\top \\tanh \\big( &{\\mathbf{{{W}}}}^{(\\text{{ae}})} {\\bm{{e}}}_i + {\\mathbf{{{W}}}}^{(\\text{{ah}})} {\\bm{{g}}}_{j-1} + \\nonumber\\\\\n&  {\\mathbf{{{W}}}}^{(\\text{{ap}})} \\psi(j, i, I) \\big) \\label{eq:sent-fji-positional} \n\\end{align}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle f_{ji}={\\bm{{v}}}^{\\top}\\tanh\\big{(}\" display=\"inline\"><mrow><msub><mi>f</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><msup><mi>\ud835\udc97</mi><mo>\u22a4</mo></msup><mi>tanh</mi><mo maxsize=\"120%\" minsize=\"120%\">(</mo></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{{W}}}}^{(\\text{{ae}})}{\\bm{{e}}}_{i}+{\\mathbf{{{W}}}}^{%&#10;(\\text{{ah}})}{\\bm{{g}}}_{j-1}+\" display=\"inline\"><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ae</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc86</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ah</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\ud835\udc88</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{{{W}}}}^{(\\text{{ap}})}\\psi(j,i,I)\\big{)}\" display=\"inline\"><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>ap</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mi>\u03c8</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nand ${\\mathbf{{{W}}}}^{(\\text{{ap}})} {\\in \\mathbb{R}^{{A} \\times {3}}}$. We exclude the target length $J$ as this is unknown during\ndecoding, as a partial translation can have several (infinite)\ndifferent lengths. The use of the $\\log(1 + \\cdot)$ function is to \navoid numerical instabilities from widely varying sentence lengths.\nThe non-linearity in Eq~\\ref{eq:sent-fji-positional} allows for\ncomplex functions of these inputs to be learned, such as relative\npositions and approximate distance from the diagonal, as well as their\ninteractions with the other inputs (e.g., to learn that some words are\nexceptional cases where a diagonal bias should not apply).\n\n\\subsection{Markov condition}\n\n\n\n\n\nThe HMM model of translation \\cite{vogel96} is based on a Markov \ncondition over alignment random variables, to allow the model to\nlearn local effects such as when $i \\gets j$ is aligned then it is likely\nthat $i+1 \\gets j+1$ or $i \\gets j+1$. These correspond to\nlocal diagonal alignments or one-to-many alignments, respectively.\nIn general, there are many correlations between the alignments of\na word and the word immediately to its left.\n\nMarkov conditioning can also be incorporated in \na similar manner to positional bias, by augmenting the attentional input from\nEqs~\\ref{eq:sent-fji}~and~\\ref{eq:sent-fji-positional} to include:\n\n", "itemtype": "equation", "pos": 12991, "prevtext": "\nwhere the new component in the input is a simple feature function of\nthe positions in the source and target sentences and the source length, \n", "index": 11, "text": "\n\\[ \\psi(j, i, I) = \\bigg[ \\log(1+j), \\log(1+i), \\log(1+I) \\bigg]^{\\top}  \\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\psi(j,i,I)=\\bigg{[}\\log(1+j),\\log(1+i),\\log(1+I)\\bigg{]}^{\\top}\" display=\"block\"><mrow><mrow><mi>\u03c8</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>i</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>log</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mi>I</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mo>\u22a4</mo></msup></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nwhere  $\\ldots$ abbreviates the ${\\bm{{e}}}_i$, ${\\bm{{g}}}_{j-1}$ and $\\psi$ components from \nEq~\\ref{eq:sent-fji-positional}, and \n $\\xi_1({\\bm{{\\alpha}}}_{j-1})$ provides a fixed dimensional representation of\nthe attention state for the preceding word. \n\nIt is not immediately obvious how to incorporate the previous attention vector\nas ${\\bm{{\\alpha}}}$ is dynamically sized to match the\nsource sentence length, thus using it directly would not\ngeneralise over sentences of different lengths. For this reason, we\nmake a simplification by just considering local moves offset by $\\pm k$ positions, that is, \n\n\n", "itemtype": "equation", "pos": 14404, "prevtext": "\nand ${\\mathbf{{{W}}}}^{(\\text{{ap}})} {\\in \\mathbb{R}^{{A} \\times {3}}}$. We exclude the target length $J$ as this is unknown during\ndecoding, as a partial translation can have several (infinite)\ndifferent lengths. The use of the $\\log(1 + \\cdot)$ function is to \navoid numerical instabilities from widely varying sentence lengths.\nThe non-linearity in Eq~\\ref{eq:sent-fji-positional} allows for\ncomplex functions of these inputs to be learned, such as relative\npositions and approximate distance from the diagonal, as well as their\ninteractions with the other inputs (e.g., to learn that some words are\nexceptional cases where a diagonal bias should not apply).\n\n\\subsection{Markov condition}\n\n\n\n\n\nThe HMM model of translation \\cite{vogel96} is based on a Markov \ncondition over alignment random variables, to allow the model to\nlearn local effects such as when $i \\gets j$ is aligned then it is likely\nthat $i+1 \\gets j+1$ or $i \\gets j+1$. These correspond to\nlocal diagonal alignments or one-to-many alignments, respectively.\nIn general, there are many correlations between the alignments of\na word and the word immediately to its left.\n\nMarkov conditioning can also be incorporated in \na similar manner to positional bias, by augmenting the attentional input from\nEqs~\\ref{eq:sent-fji}~and~\\ref{eq:sent-fji-positional} to include:\n\n", "index": 13, "text": "\\begin{equation}\nf_{ji}  = {\\bm{{v}}}^\\top \\tanh \\left( \\ldots + {\\mathbf{{{W}}}}^{(\\text{{am}})} \\xi_1({\\bm{{\\alpha}}}_{j-1};i)\\right) \\label{eq:sent-fji-markov} \n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"f_{ji}={\\bm{{v}}}^{\\top}\\tanh\\left(\\ldots+{\\mathbf{{{W}}}}^{(\\text{{am}})}\\xi_%&#10;{1}({\\bm{{\\alpha}}}_{j-1};i)\\right)\" display=\"block\"><mrow><msub><mi>f</mi><mrow><mi>j</mi><mo>\u2062</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msup><mi>\ud835\udc97</mi><mo>\u22a4</mo></msup><mo>\u2062</mo><mrow><mi>tanh</mi><mo>\u2061</mo><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">\u2026</mi><mo>+</mo><mrow><msup><mi>\ud835\udc16</mi><mrow><mo stretchy=\"false\">(</mo><mtext>am</mtext><mo stretchy=\"false\">)</mo></mrow></msup><mo>\u2062</mo><msub><mi>\u03be</mi><mn>1</mn></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\n\nwith ${\\mathbf{{{W}}}}^{(\\text{{am}})} {\\in \\mathbb{R}^{{A} \\times {(2k+1)}}}$. \n\nOur approach is likely to capture the most important alignments patterns forming the backbone\nof the alignment HMM, namely monotone, 1-to-many, and local inversions.\n\n\n\n\n\n\n\n\n\n\\subsection{Fertility}\n\nFertility is the propensity for a word to be translated as a consistent\nnumber of words in the other language, e.g., \\emph{Iseseisvusdeklaratsioon}\ntranslates as \\emph{(the) Declaration of Independence}.\nFertility is a central component in the IBM models\n3--5 \\cite{brown93}. Incorporating fertility into the\nattentional model is a little more involved, and we present two techniques for\ndoing so. \n\n\\paragraph{Local fertility}\nFirst we consider a feature-based technique, which includes the following  features\n\n{\\small\n\n", "itemtype": "equation", "pos": 15194, "prevtext": "\nwhere  $\\ldots$ abbreviates the ${\\bm{{e}}}_i$, ${\\bm{{g}}}_{j-1}$ and $\\psi$ components from \nEq~\\ref{eq:sent-fji-positional}, and \n $\\xi_1({\\bm{{\\alpha}}}_{j-1})$ provides a fixed dimensional representation of\nthe attention state for the preceding word. \n\nIt is not immediately obvious how to incorporate the previous attention vector\nas ${\\bm{{\\alpha}}}$ is dynamically sized to match the\nsource sentence length, thus using it directly would not\ngeneralise over sentences of different lengths. For this reason, we\nmake a simplification by just considering local moves offset by $\\pm k$ positions, that is, \n\n\n", "index": 15, "text": "\\begin{equation}\n \\xi_1({\\bm{{\\alpha}}}_{j-1}; i) = \\bigg[ \\alpha_{j-1,i-k}, .., \\alpha_{j-1,i},   .., \\alpha_{j-1,i+k} \\bigg ]^\\top \\nonumber\n\\label{eq:sent-xi}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex7.m1\" class=\"ltx_Math\" alttext=\"\\xi_{1}({\\bm{{\\alpha}}}_{j-1};i)=\\bigg{[}\\alpha_{j-1,i-k},..,\\alpha_{j-1,i},..%&#10;,\\alpha_{j-1,i+k}\\bigg{]}^{\\top}\" display=\"block\"><mrow><msub><mi>\u03be</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>;</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mrow><mo maxsize=\"210%\" minsize=\"210%\">[</mo><msub><mi>\u03b1</mi><mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>\u03b1</mi><mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>\u03b1</mi><mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>i</mi><mo>+</mo><mi>k</mi></mrow></mrow></msub><mo maxsize=\"210%\" minsize=\"210%\">]</mo></mrow><mo>\u22a4</mo></msup></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\n}\nand the corresponding feature weights, i.e., ${\\mathbf{{{W}}}}^{(\\text{{af}})} {\\in \\mathbb{R}^{{A} \\times {(2k+1)}}}$. These sums\nrepresent the total alignment score for the surrounding source words, similar \nto fertility in a traditional latent variable model, which is the\nsum over binary alignment random variables. \nA word which already has several alignments can be excluded from participating in more alignments,\nthus combating the garbage collection problem. Conversely words that\ntend to need high fertility  \ncan be learned through the interactions between these features and the\nword and context embeddings in Eq~\\ref{eq:sent-fji-markov}.\n\n\\paragraph{Global fertility}\nA second, more explicit, technique for incorporating fertility is to include \nthis as a modelling constraint. Initially we considered a soft constraint based \non the approach in \\cite{icml2015_xuc15}, where an \nimage captioning model was biased to attend to every pixel in the image \nexactly once. In our setting, the same idea can be applied through adding a\nregularisation term to the training objective of the form $ \\sum_i \\left(1 - \\sum_j \\alpha_{j,i} \\right)^2 $.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever this method is overly restrictive: enforcing that every word is used exactly once \nis not appropriate in translation where some words are likely to be dropped (e.g., determiners and other function words), while others\nmight need to be translated several times to produce a phrase in the target language.\\footnote{\nModern decoders \\cite{koehn2003statistical} often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility in contiguous translation chunks.\n\n\n}\nFor this reason we develop an alternative method, based around a contextual fertility model,\n\n$p(f_i | {\\bm{{s}}}, i) = \\mathcal{N}\\left( \\mu(e_i), \\sigma^2(e_i) \\right) $\n\nwhich scores the fertility of source word $i$, defined as $f_i = \\sum_j \\alpha_{j,i}$, using a normal\ndistribution\\footnote{The normal distribution is deficient, as it has support for all scalar values, despite $f_i$ being bounded above and below ($0 \\le f_i \\le J$). This could be corrected by using a truncated normal, or various other choices of distribution.} parameterised by $\\mu$ and $\\sigma^2$, both positive scalar valued non-linear functions \nof the source word encoding $e_i$. This is incorporated into the training objective as an additional\nadditive term, $\\sum_i \\log p(f_i | {\\bm{{s}}}, i)$, for each training sentence.\n\nThis formulation allows for greater consistency in translation, \nthrough e.g., learning which words tend to be omitted from translation, or translate as several words.\nCompared to the fertility model in IBM 3--5 \\cite{brown93}, ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions.\n\n\\subsection{Bilingual Symmetry}\n\\label{sec:sent-alignment-symmetry}\n\nSo far we have considered a conditional model of the target given the\nsource, modelling $p({\\bm{{t}}} | {\\bm{{s}}})$. However it is well established for\nlatent variable translation models that the alignments improve if  $p({\\bm{{s}}} | {\\bm{{t}}})$ is also modelled\nand the inferences of both directional models are combined -- evidenced\nby the symmetrisation heuristics used in most decoders \\cite{koehn2005iwslt},\nand also by explicit joint agreement training objectives \\cite{liang2006alignment,ganchev-gracca-taskar:2008:ACLMain}.\nThe rationale is that both models make somewhat independent errors, so an ensemble\nstands to gain from variance reduction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{biattentional_from_presentation}\n\\caption{Symmetric training with trace bonus, computed as matrix multiplication,  $-\\operatorname{tr}({\\bm{{\\alpha}}}^{s\\leftarrow t} {\\bm{{\\alpha}}}^{s\\rightarrow t ~ \\top})$. Dark shading indicates higher values. }\n\\label{fig:btrace}\n\\end{figure}\n\nWe propose a method for joint training of two directional models as\npictured in Figure~\\ref{fig:btrace}.  Training twinned models involves optimising\n\n\\mbox{$\\mathcal{L}  = -\\log p({\\bm{{t}}} | {\\bm{{s}}}) - \\log p({\\bm{{s}}} | {\\bm{{t}}}) + \\gamma B $} \n\n\nwhere, as before, we consider only a single sentence pair, for simplicity of notation.\nThis corresponds to a pseudo-likelihood objective, with the $B$ linking\nthe two models.\\footnote{We could share some parameters, e.g., the word\n  embedding matrices, however we found this didn't make much difference\n  versus using disjoint parameter sets. We set $\\gamma=1$ herein.} \nThe $B$ component considers the alignment (attention) matrices,\n${\\bm{{\\alpha}}}^{s\\rightarrow t} {\\in \\mathbb{R}^{{J} \\times {I}}}$ and ${\\bm{{\\alpha}}}^{t \\leftarrow s} {\\in \\mathbb{R}^{{I} \\times {J}}}$, and attempts to make these close to one another for both\ntranslation directions (see Fig.~\\ref{fig:btrace}). To achieve this,\nwe use a `trace bonus', inspired by \\cite{levinboim15}, formulated as\n\n", "itemtype": "equation", "pos": 16174, "prevtext": "\n\nwith ${\\mathbf{{{W}}}}^{(\\text{{am}})} {\\in \\mathbb{R}^{{A} \\times {(2k+1)}}}$. \n\nOur approach is likely to capture the most important alignments patterns forming the backbone\nof the alignment HMM, namely monotone, 1-to-many, and local inversions.\n\n\n\n\n\n\n\n\n\n\\subsection{Fertility}\n\nFertility is the propensity for a word to be translated as a consistent\nnumber of words in the other language, e.g., \\emph{Iseseisvusdeklaratsioon}\ntranslates as \\emph{(the) Declaration of Independence}.\nFertility is a central component in the IBM models\n3--5 \\cite{brown93}. Incorporating fertility into the\nattentional model is a little more involved, and we present two techniques for\ndoing so. \n\n\\paragraph{Local fertility}\nFirst we consider a feature-based technique, which includes the following  features\n\n{\\small\n\n", "index": 17, "text": "\\begin{equation}\n \\xi_2({\\bm{{\\alpha}}}_{<j}; i) = \\left[ \\sum_{j'<j} \\alpha_{j',i-k},.., \\sum_{j'<j} \\alpha_{j',i},  .., \\sum_{j'<j} \\alpha_{j',i+1} \\right ]^\\top \\nonumber\n\\label{eq:sent-xi-fertility}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\xi_{2}({\\bm{{\\alpha}}}_{&lt;j};i)=\\left[\\sum_{j^{\\prime}&lt;j}\\alpha_{j^{\\prime},i-%&#10;k},..,\\sum_{j^{\\prime}&lt;j}\\alpha_{j^{\\prime},i},..,\\sum_{j^{\\prime}&lt;j}\\alpha_{j%&#10;^{\\prime},i+1}\\right]^{\\top}\" display=\"block\"><mrow><msub><mi>\u03be</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udf36</mi><mrow><mi/><mo>&lt;</mo><mi>j</mi></mrow></msub><mo>;</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><msup><mrow><mo>[</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>&lt;</mo><mi>j</mi></mrow></munder><msub><mi>\u03b1</mi><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>,</mo><mrow><mi>i</mi><mo>-</mo><mi>k</mi></mrow></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>,</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>&lt;</mo><mi>j</mi></mrow></munder><msub><mi>\u03b1</mi><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>,</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>&lt;</mo><mi>j</mi></mrow></munder><msub><mi>\u03b1</mi><mrow><msup><mi>j</mi><mo>\u2032</mo></msup><mo>,</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><mo>]</mo></mrow><mo>\u22a4</mo></msup></mrow></math>", "type": "latex"}, {"file": "1601.01085.tex", "nexttext": "\nAs the alignment cells are normalised using the $\\operatorname{softmax}$ and thus take\nvalues in [0,1],  the trace term is bounded above by $\\min(I,J)$ which\n\noccurs when the two alignment matrices are transposes of each other, representing perfect one-to-one alignments in both directions\n\n\n\n\n\n\n\n\n\n\n\\section{Experiments}\n\n\\begin{table}\n\\centering\n\\begin{tabular}{|c||S[table-format = 4.0]S[table-format = 4.0]|S[table-format = 3.2]S[table-format = 3.2]|}\n\\hline\n\\textbf{lang-pair} &  \\multicolumn{2}{c|}{\\textbf{\\# tokens (K)}}  & \\multicolumn{2}{c|}{\\textbf{\\# types (K)}} \\\\\n\\hline \\hline\n\nZh-En &  422 & 454 & 3.44 & 3.12 \\\\\n\nRu-En &  1639 & 1809 & 145 & 65 \\\\\n\nEt-En &  1411 & 1857 & 90 & 25 \\\\\n\nRo-En &  1782 & 1806 & 39 & 24 \\\\\n\\hline\n\\end{tabular}\n\\caption{Statistics of the training sets, showing in each cell the count for the source language (left) and target language (right).}\n\n\\label{datasets_tab}\n\\end{table}\n\n\n\\paragraph{Datasets.} We conducted our experiments with four language pairs, translating between English $\\leftrightarrow$ Romanian, Estonian, Russian and Chinese. \nThese languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian).\nWe focus on a (simulated) low resource setting, where only a limited amount of training data is available.\nThis serves to demonstrate the robustness and generalisation of our model on sparse data -- something that \nhas not yet been established for neural models with millions of parameters with vast potential for over-fitting.\n\nTable \\ref{datasets_tab} shows the statistics of the training sets.\\footnote{For all datasets words were thresholded for training frequency $\\ge 5$, with uncommon training and unseen testing words replaced by an {\\textlangle unk\\textrangle\\xspace} symbol.}\n\nFor Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. \nWe used `devset1\\_2' and `devset\\_3' as the development and test sets, respectively, and in both cases used only the first reference for evaluation.  \n\nFor other language pairs, the data come from the Europarl corpus \\cite{koehn2005epc}, where we used 100K sentence pairs for training, and 3K for development and 2K for testing.\\footnote{The first 100K sentence pairs were used for training, while the development and test were drawn from the last 100K sentence pairs, taking the first 2K for testing and the last 3K for development.}\n\n\n\\paragraph{Models and Baselines.} We have implemented our neural translation model with linguistic features \nin C++ using the CNN library.\\footnote{\\url{https://github.com/clab/cnn/}} \nWe compared our proposed model against our implementations of \nthe attentional model \\cite{bahdanau2015neural} and\nencoder-decoder architecture \\cite{sutskever2014sequence}. \nAs the baseline, we used a state-of-the-art phrase-based statistical machine translation model \nbuilt using Moses \\cite{koehn2007moses} with the standard\nfeatures: relative-frequency and lexical translation model probabilities in both directions;\ndistortion model; language model and word count. \n\n\n\n\n\n\n\n\n\n\nWe used KenLM  \\cite{Heafield-kenlm}  to create  3-gram\nlanguage models with Kneser-Ney smoothing on the target side of the bilingual training corpora.\n\n\\paragraph{Evaluation Measures.} \n\nFollowing previous work \\cite{kalchbrenner13emnlp,sutskever2014sequence,bahdanau2015neural,neubig15wat}, \nwe evaluated all neural models using test set perplexities and in a re-ranking setting, using BLEU \\cite{Papineni:2002:BMA:1073083.1073135} measure.\nFor re-ranking, we generated 100-best translations using the baseline phrase-based model, \nto which we added log probability features from our neural models alongside  \nthe features of the underlying phrase-based model.\n\n\\subsection{Analysis of Alignment Biases}\n\n\\begin{table}\n\\centering\n\n\\sisetup{\nround-mode = places,\nround-precision = 2,\ndetect-weight,\ntable-format = 2.2\n}\n\\begin{tabular}{|l||S|S[round-precision = 1,table-format=2.1]|}\n\\hline \n\\bf configuration & \n{ \\bf test} & {\\bf \\#param (M)} \\\\\n\\hline \\hline\n  \n  Sutskever encdec & 5.34871 & 8.687665 \\\\\n\n\\hline\n\n Attentional & 4.76879 & 14.977841 \\\\\n +align  & 4.56259 & 14.980145 \\\\\n +align+glofer &  5.20311 & 15.505971 \\\\\n +align+glofer-pre & 4.31061 & 15.505971 \\\\\n +align+sym & 4.4404 & 30.121885 \\\\\n +align+sym+glofer-pre & 4.432 & 31.173537 \\\\\n\\hline\n\\end{tabular}\n\\caption{Perplexity results for attentional model variants evaluated on BTEC zh$\\rightarrow$en, and number of model parameters (in millions).}\n\\label{tab:sent-btec-pplx}\n\\end{table}\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{plots-ro-en}\n\n\\caption{Perplexity with training epochs on ro-en translation, comparing several model variants.}\n\\label{fig:ro-en-abelation}\n\\end{figure}\n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=\\textwidth]{attention_figure_crop}\n\\caption{Example development sentence, showing the inferred attention matrix for various models for Et~$\\leftrightarrow$~En. Rows correspond to the translation direction and columns correspond to different models: attentional, with\nalignment features (+align), global fertility (+glofer), and symmetric joint training (+sym). Darker shades denote higher values and white denotes zero.}\n\\label{fig:agreement}\n\\end{figure*}\n\nWe start by investigating the effect of various linguistic constraints, \ndescribed in Section \\ref{sec:sent-alignment-structure}, on the attentional model.\n\nTable~\\ref{tab:sent-btec-pplx} presents the perplexity of trained models\n\n\nfor Chinese$\\rightarrow$English translation.\n\n\n\nFor comparison, we report the results of an encoder-decoder-based  neural translation  model \\cite{sutskever2014sequence} as the baseline. \n\n\n\nAll other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, \nusing 512 embedding, 512 hidden, and 256 alignment dimensions.\n\nFor each model, we also report the number of its parameters. \n\nModels are trained using stochastic gradient, allowing up to 20 epochs. \nFor each model the best perplexity on the held-out development set is reported, which was achieved in 5-10 epochs for most cases.\n\nAs expected, the vanilla attentional model greatly improves over encoder-decoder (perplexity of 4.77 vs.\\@ 5.35), \nclearly making good use of the additional context.\n\n\n\nAdding the combined positional bias, local fertility, and Markov structure (denoted by +align) \nfurther decreases the perplexity to 4.56. \n\nAdding the global fertility (+glofer) is detrimental, however, increases perplexity to 5.20. \n\n\n\nInterestingly, global fertility does helps to reduce the perplexity (to 4.31) when using with pre-training setting (+align+glofer-pre). In this case, it is refining an already excellent model from which reliable global fertility estimates can be obtained.\nThis finding is consistent with the other languages, see Figure~\\ref{fig:ro-en-abelation} which shows typical learning curves of different variants of the attentional model. \nNote that when global fertility is added to the vanilla attentional model with alignment features, it significantly slows down training as it limits exploration in early training iterations, however it does bring a sizeable win when used to fine-tune a pre-trained model.\n\n\n\n\n\n\n\nFinally, the bilingual symmetry also helps to reduce the perplexity scores when used with the alignment features, however, does not combine well with global fertility (+align+sym+glofer-pre).\nThis is perhaps an unsurprising result as both methods impose a often-times similar regularising effect over the attention matrix. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure \\ref{fig:agreement} illustrates the different attention matrices inferred by the various model variants. \nNote the difference between the base attentional model and its variant with alignment features (`+align'), \nwhere more weight is assigned to diagonal and 1-to-many alignments. \n\nGlobal fertility pushes more attention to the sentinel symbols {\\textlangle s\\textrangle\\xspace} and {\\textlangle /s\\textrangle\\xspace}. \nDeterminers and prepositions in English show much lower fertility than nouns, \nwhile Estonian nouns have even higher fertility.\nThis accords with Estonian morphology, wherein nouns are inflected with rich case \nmarking, e.g., \\emph{n\\~{o}ukoguga} has the cogitative \\emph{-ga} suffix, meaning `with', \nand thus translates as several English words (\\emph{with the council}).\nThe right-most column corresponds to joint symmetric training, with many more \nconfident attention values especially for consistent 1-to-many alignments \n(\\emph{difficult} in English and \\emph{raskeid} in Estonian, an adjective in partitive case meaning \\emph{some difficult}).\n\n\n\n\n\n\n\\subsection{Full Results}\n\n\n\nThe perplexity results of the neural models for the two translation \ndirections across the four language pairs are presented in Table~\\ref{res:perplexity}.a \nand~\\ref{res:perplexity}.b. \n\nIn all cases, our work achieves lower perplexities compared to \nthe vanilla attentional model and the encoder-decoder architecture, \nowing to the linguistic constraints. \n\n\nTable \\ref{res:bleu:rerank} presents the BLEU scores for the re-ranking setting for the \ntranslating into English from our four languages.\n\nWe compare re-ranking settings using the log probabilities produced by our model as additional features \nvs.\\@ using log probabilities from the vanilla attentional model and the encoder-decoder.  \n\nThe re-rankers based on our model are significantly better than the rest \nfor Chinese and Estonian, and on par with the other for Russian and Romanian$\\rightarrow$English.\n\nIn all cases our model has performance at least 1 BLEU point better than the baseline phrase-based system.\n\nIt is worth noting that for Chinese-English, our re-ranker leads to an increase of almost 3 points in the BLEU score using an \\emph{ensemble} of neural models with different \nconfigurations.\\footnote{We use the outputs of 6--12 models trained in both directions, using different alignment and fertility options, and using a smaller dimensionality than earlier (100 embedding, 100 hidden and 50 attention dimensions).}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}\n\\centering\n\\begin{tabular}{c}\n\n{\\small\n\\sisetup{\nround-mode = figures,\nround-precision = 3,\ndetect-weight,\ntable-format = 2.2\n}\n\\robustify\\bfseries\n\\begin{tabular}{|l||S[detect-weight,table-format = 3.3]SSS|}\n\\hline\nLang. Pair & {Zh-En} & {Ru-En} & {Et-En} & {Ro-En} \\\\\n\\hline \\hline\nEnc-Dec & 5.34871 & 61.9471 & 18.2496 & 10.2685 \\\\\nAttentional & 4.76879 & 41.7227 & 12.7635 & 6.62272 \\\\\nOur Work & \\bfseries 4.31061 & \\bfseries 39.8725 & \\bfseries 11.82 & \\bfseries 5.89019 \\\\\n\\hline\n\\end{tabular} \n} \\\\\n(a) \\\\\n\n{\\small\n\\sisetup{\nround-mode = figures,\nround-precision = 3,\ndetect-weight,\ntable-format = 2.2\n}\n\\robustify\\bfseries\n\\begin{tabular}{|l||SSSS|}\n\\hline\nLang. Pair & {En-Zh} & {En-Ru} & {En-Et} & {En-Ro} \\\\\n\\hline \\hline\nEnc-Dec & 8.5965 & 67.31 & 31.4038 & 11.5043 \\\\\nAttentional & 7.48505 & 43.0271 & 19.4009 & 7.29634 \\\\\nOur Work & \\bfseries 6.24009 & \\bfseries 40.6302 & \\bfseries 16.9558 & \\bfseries 6.34599 \\\\\n\\hline\n\\end{tabular} \n} \\\\\n(b) \\\\\n\n\\end{tabular}\n\\caption{Perplexity on the test sets for the two translation directions. Our work includes: bidirectional LSTM attentional model combined with positional bias, Markov, local fertility, and global fertility (pre-trained setting).}\n\\label{res:perplexity}\n\\end{table}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\small\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}\n\\centering\n\\sisetup{\nround-mode = figures,\nround-precision = 4,\ndetect-weight,\ntable-format = 2.2\n}\n\\robustify\\bfseries\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{|l||SSSS|}\n\\hline\nLang. Pair & {Zh-En} & {Ru-En} & {Et-En} & {Ro-En} \\\\\n\\hline \\hline\nPhrase-based  & 40.63 & 18.7 & 31.99&  45.21\\\\\n\n\\hline\n\n Enc-Dec  & 40.41 & 18.83 & 32.20 & 45.36 \\\\\n\n Attentional   & 41.16 \\textsuperscript{$\\clubsuit$}& \\bfseries 19.79 & 32.78 & 46.83 \\\\\n\n\n\n\n\n\n\n\n\nOur Work  & \\bfseries 44.14 \\textsuperscript{$\\clubsuit\\spadesuit$} & 19.73 & \\bfseries 33.26 \\textsuperscript{$\\spadesuit$} & \\bfseries 46.88 \\\\\n\\hline\n\\end{tabular}}\n\\caption{BLEU scores  on the test sets for re-ranking.\n\\textbf{bold:} Best performance, $^{\\spadesuit}$: Significantly better than Attentional,  $^{\\clubsuit}$: Using ensemble of models. \n\n} \n\\label{res:bleu:rerank}\n\\end{table}\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Related Work}\n\n\n\n\n\\newcite{kalchbrenner13emnlp}\nwere the first to propose a full neural model of translation, using a convolutional network as the source encoder, followed by an RNN decoder to generate the target translation.\n\nThis was extended in \\newcite{sutskever2014sequence}, who replaced the source encoder with an RNN using a Long Short-Term Memory (LSTM), and \\newcite{bahdanau2015neural} who introduced the notion of ``attention'' to the model, whereby the source context can dynamically change during the decoding process to attend to the most relevant parts of the source sentence\n\n\\newcite{luong-pham-manning:2015:EMNLP} refined the attention mechanism to be more local, by \nconstraining attention to a text span, whose words' representations are averaged.\n\nTo leverage the attention history, \\cite{luong-pham-manning:2015:EMNLP}  made use\nof the attention vector of the previous position when generating the attention vector for the next position, similar in spirit to our method for incorporating alignment structural biases.\n\nConcurrent with our work, \\newcite{chengetal2015} proposed a similar agreement-based\njoint training for bidirectional attention-based neural machine translation, and showed significant improvement in the BLEU score for the large data French$\\leftrightarrow$English translation.\n\n\n\\section{Conclusion}\n\nWe have shown that the attentional model of translation does not capture many \nwell known properties of traditional word-based translation models, and proposed\nseveral ways of imposing these as structural biases on the model. We show \nimprovements across several challenging language pairs in a low-resource setting,\nboth in perplexity and re-ranking evaluations. In future work we intend to\ninvestigate the model performance on larger datasets, and incorporate further linguistic information such as morphological representations.\n\n\\ifnaaclfinal\n\\subsection*{Acknowledgements}\n\nThe work reported here was started at JSALT 2015 in UW, Seattle, and was supported by JHU via grants from NSF (IIS), DARPA (LORELEI), Google, Microsoft, Amazon, Mitsubishi Electric, and MERL. Dr Cohn was supported by the ARC (Future Fellowship).\n\\fi\n\n\n\n\n\n\n\n\n\\bibliographystyle{naaclhlt2016}\n\\bibliography{cite-strings,sentence,cite-definitions}\n\n\n", "itemtype": "equation", "pos": 21578, "prevtext": "\n}\nand the corresponding feature weights, i.e., ${\\mathbf{{{W}}}}^{(\\text{{af}})} {\\in \\mathbb{R}^{{A} \\times {(2k+1)}}}$. These sums\nrepresent the total alignment score for the surrounding source words, similar \nto fertility in a traditional latent variable model, which is the\nsum over binary alignment random variables. \nA word which already has several alignments can be excluded from participating in more alignments,\nthus combating the garbage collection problem. Conversely words that\ntend to need high fertility  \ncan be learned through the interactions between these features and the\nword and context embeddings in Eq~\\ref{eq:sent-fji-markov}.\n\n\\paragraph{Global fertility}\nA second, more explicit, technique for incorporating fertility is to include \nthis as a modelling constraint. Initially we considered a soft constraint based \non the approach in \\cite{icml2015_xuc15}, where an \nimage captioning model was biased to attend to every pixel in the image \nexactly once. In our setting, the same idea can be applied through adding a\nregularisation term to the training objective of the form $ \\sum_i \\left(1 - \\sum_j \\alpha_{j,i} \\right)^2 $.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever this method is overly restrictive: enforcing that every word is used exactly once \nis not appropriate in translation where some words are likely to be dropped (e.g., determiners and other function words), while others\nmight need to be translated several times to produce a phrase in the target language.\\footnote{\nModern decoders \\cite{koehn2003statistical} often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility in contiguous translation chunks.\n\n\n}\nFor this reason we develop an alternative method, based around a contextual fertility model,\n\n$p(f_i | {\\bm{{s}}}, i) = \\mathcal{N}\\left( \\mu(e_i), \\sigma^2(e_i) \\right) $\n\nwhich scores the fertility of source word $i$, defined as $f_i = \\sum_j \\alpha_{j,i}$, using a normal\ndistribution\\footnote{The normal distribution is deficient, as it has support for all scalar values, despite $f_i$ being bounded above and below ($0 \\le f_i \\le J$). This could be corrected by using a truncated normal, or various other choices of distribution.} parameterised by $\\mu$ and $\\sigma^2$, both positive scalar valued non-linear functions \nof the source word encoding $e_i$. This is incorporated into the training objective as an additional\nadditive term, $\\sum_i \\log p(f_i | {\\bm{{s}}}, i)$, for each training sentence.\n\nThis formulation allows for greater consistency in translation, \nthrough e.g., learning which words tend to be omitted from translation, or translate as several words.\nCompared to the fertility model in IBM 3--5 \\cite{brown93}, ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions.\n\n\\subsection{Bilingual Symmetry}\n\\label{sec:sent-alignment-symmetry}\n\nSo far we have considered a conditional model of the target given the\nsource, modelling $p({\\bm{{t}}} | {\\bm{{s}}})$. However it is well established for\nlatent variable translation models that the alignments improve if  $p({\\bm{{s}}} | {\\bm{{t}}})$ is also modelled\nand the inferences of both directional models are combined -- evidenced\nby the symmetrisation heuristics used in most decoders \\cite{koehn2005iwslt},\nand also by explicit joint agreement training objectives \\cite{liang2006alignment,ganchev-gracca-taskar:2008:ACLMain}.\nThe rationale is that both models make somewhat independent errors, so an ensemble\nstands to gain from variance reduction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{biattentional_from_presentation}\n\\caption{Symmetric training with trace bonus, computed as matrix multiplication,  $-\\operatorname{tr}({\\bm{{\\alpha}}}^{s\\leftarrow t} {\\bm{{\\alpha}}}^{s\\rightarrow t ~ \\top})$. Dark shading indicates higher values. }\n\\label{fig:btrace}\n\\end{figure}\n\nWe propose a method for joint training of two directional models as\npictured in Figure~\\ref{fig:btrace}.  Training twinned models involves optimising\n\n\\mbox{$\\mathcal{L}  = -\\log p({\\bm{{t}}} | {\\bm{{s}}}) - \\log p({\\bm{{s}}} | {\\bm{{t}}}) + \\gamma B $} \n\n\nwhere, as before, we consider only a single sentence pair, for simplicity of notation.\nThis corresponds to a pseudo-likelihood objective, with the $B$ linking\nthe two models.\\footnote{We could share some parameters, e.g., the word\n  embedding matrices, however we found this didn't make much difference\n  versus using disjoint parameter sets. We set $\\gamma=1$ herein.} \nThe $B$ component considers the alignment (attention) matrices,\n${\\bm{{\\alpha}}}^{s\\rightarrow t} {\\in \\mathbb{R}^{{J} \\times {I}}}$ and ${\\bm{{\\alpha}}}^{t \\leftarrow s} {\\in \\mathbb{R}^{{I} \\times {J}}}$, and attempts to make these close to one another for both\ntranslation directions (see Fig.~\\ref{fig:btrace}). To achieve this,\nwe use a `trace bonus', inspired by \\cite{levinboim15}, formulated as\n\n", "index": 19, "text": "\\begin{align*}\nB = -\\operatorname{tr}({\\bm{{\\alpha}}}^{s\\leftarrow t~\\top} {\\bm{{\\alpha}}}^{s\\rightarrow t}) & = \\sum_j \\sum_i \\alpha^{s\\leftarrow t}_{i,j} \\alpha^{s\\rightarrow t}_{j,i}\\, .\n\n\\end{align*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle B=-\\operatorname{tr}({\\bm{{\\alpha}}}^{s\\leftarrow t~{}\\top}{\\bm{%&#10;{\\alpha}}}^{s\\rightarrow t})\" display=\"inline\"><mrow><mi>B</mi><mo>=</mo><mrow><mo>-</mo><mrow><mo>tr</mo><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udf36</mi><mrow><mi>s</mi><mo>\u2190</mo><mrow><mpadded width=\"+3.3pt\"><mi>t</mi></mpadded><mo>\u22a4</mo></mrow></mrow></msup><mo>\u2062</mo><msup><mi>\ud835\udf36</mi><mrow><mi>s</mi><mo>\u2192</mo><mi>t</mi></mrow></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></math>\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle=\\sum_{j}\\sum_{i}\\alpha^{s\\leftarrow t}_{i,j}\\alpha^{s\\rightarrow&#10;t%&#10;}_{j,i}\\,.\\par&#10;\" display=\"inline\"><mrow><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>j</mi></munder></mstyle><mrow><mstyle displaystyle=\"true\"><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder></mstyle><mrow><msubsup><mi>\u03b1</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mrow><mi>s</mi><mo>\u2190</mo><mi>t</mi></mrow></msubsup><mo>\u2062</mo><mpadded width=\"+1.7pt\"><msubsup><mi>\u03b1</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow><mrow><mi>s</mi><mo>\u2192</mo><mi>t</mi></mrow></msubsup></mpadded></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}]