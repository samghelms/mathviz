[{"file": "1601.04756.tex", "nexttext": "\nwhere $x_{i}$ is a minority instance, $x^{'}$ is the randomly chosen k-nearest neighbor, and $\\delta$ is a random number between 0 and 1.\n\n\\subsection{Proposed Sampling Techniques}\nThe five new sampling techniques proposed in this paper are based on the abovementioned sampling techniques. \n\\subsubsection{SMOTERandRep}\nThe first technique is called SMOTERandRep, a short name for SMOTE Random Replacement. This technique is based on the idea of SMOTE because it also oversamples the minority class and create synthetic instances. Given an instance $x_{i}$ of a minority class and its k-nearest neighbors, it creates a new instance by copying the features of $x_{i}$ and randomly replacing $\\log_{2}a$ attribute values with the values from the randomly selected k-nearest neighbors where $a$ is the total number of attributes and k value is also equal to $\\log_{2}a$. \n\\subsubsection{Lax Random Undersampling}\nThe second technique is called Lax Random Undersampling (LRU). This technique is based on the idea of RU and the difference is instead of aiming for having an equal number of instances for the minority class and majority class, it randomly delete instances from the majority class until the number of instances in this class is equal to the number of instances of the minority class with the most number of instances.\n \\subsubsection{Lax Random Oversampling}\n The third technique is called Lax Random Oversampling (LRO) and as its name suggests, is based on the idea of RO. This technique differs with RO because instead of having an equal number of instances for the minority class and majority class, it randomly replicates the minority instances until the minority class with the most number of instances is replicated once. \n \\subsubsection{Combined-LROU}\n The fourth and fifth techniques are simply the combination of the second and third techniques (LRO and LRU) and that is why it is called Combined-LROU (C-LROU) and Combined-LRUO (C-LRUO). The difference of the two techniques is the order of combining LRO and LRU, for example, C-LROU first applies LRO on the original data set, then LRU is applied on the resulting data set. \n \\subsubsection{Combined-LRUO}\n As for C-LRUO, it first applies LRU on the original data set and after that LRO is applied on the resulting data set.\n\n\\section{Methodology}\n\\label{sec:methodology}\n\nThe student data used in this study for the UPLB classification problem was gathered from the Office of the University Registrar (OUR) of UPLB and from five different colleges of the university. The collected data identified 13 possible classification of an instance of a student and these are shiftee (SHC), transfer to another UP campus (TUP), transfer to another school (TAS), scholastic delinquency warning (SDW), scholastic delinquency probationary (SDP), scholastic delinquency dismissal (SDD), scholastic delinquency permanent disqualification (SPQ), readmission of disqualified and dismissed students (RAM), absence without leave (AWL), leave of absecence (LOA), honorable dismissal (HDM), scholastic delinquency warning and probationary (SWP), and continuing student (COS). A total of 2297 instances of students were collected and the distribution of the data per class is shown in Figure~1.\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{origdata.eps}}\n\\caption{Distribution of the original students' data on 13 classes.}\n  \\label{fig:origdata}\n\\end{figure*}\n\nWaikato Environment for Knowledge Analysis (WEKA) is an open source (available under GNU General Public License) and cross-platform (written in Java) data mining software for data mining tasks~\\citep{hall-2009}. WEKA collects and automates state-of-the-art data mining algorithms thus, its implementation of stackingC ensemble learning algorithm as well as the implementations of other algorithms (i.e., SVM, MP, CART, C4.5, and k-NN) was used in this study. WEKA's implementation of SMOTE technique was also used in this study. The evaluation metrics used to measure the performance of the classifier were F-measure and G-mean together with stratified tenfold cross-validation as the validation method. Both metrics are widely used metric; the F-measure metric shows the classifier's performance for every class while G-mean measures the overall performance of a classifier~\\citep{wang-2012}. The maximum value for both metrics is one. WEKA has also provided the functionality to compute the F-measure value of a generated classifier and it was also used in this study. The G-mean metric, on the other hand, can be easily computed using recall value performance of the classifier for every class (also available in WEKA) and the formula is\n\n", "itemtype": "equation", "pos": 7252, "prevtext": "\n\n\n\\conferenceinfo{PCSC 2016}{March 16-18, 2016, Puerto Princesa City, Palawan.}\n\\CopyrightYear{2016} \n\\crdata{x-xxxxx-xx}  \n\n\n\\title{Improved Sampling Techniques for\\\\Learning an Imbalanced Data Set}\n\n\\numberofauthors{1}\n\\author{\n\\alignauthor\nMaureen Lyndel C. Lauron and Jaderick P. Pabico\\\\\n       \\affaddr{Institute of Computer Science}\\\\\n       \\affaddr{University of the Philippines Los Ba\\~nos}\\\\\n       \\affaddr{College 4031, Laguna}\n}\n\n\\maketitle\n\\begin{abstract}\nThis paper presents the performance of a classifier built using the stackingC algorithm in nine different data sets. Each data set is generated using a sampling technique applied on the original imbalanced data set. Five new sampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random Oversampling, Lax Random Undersampling, Combined-Lax Random Oversampling Undersampling, and Combined-Lax Random Undersampling Oversampling) that were based on the three sampling techniques (i.e., Random Undersampling, Random Oversampling, and Synthetic Minority Oversampling Technique) usually used as solutions in imbalance learning. The metrics used to evaluate the classifier's performance were F-measure and G-mean. F-measure determines the performance of the classifier for every class, while G-mean measures the overall performance of the classifier. The results using F-measure showed that for the data without a sampling technique, the classifier's performance is good only for the majority class. It also showed that among the eight sampling techniques, RU and LRU have the worst performance while other techniques (i.e., RO, C-LRUO and C-LROU) performed well only on some classes. The best performing techniques in all data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measure values between 0.5 and 0.65.  The results using G-mean showed that the oversampling technique that attained the highest G-mean value is LRO (0.86), next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83). Combining the result of the two metrics (F-measure and G-mean), only the three sampling techniques are considered as good performing (i.e., LRO, SMOTE, and SMOTERandRep)\n\\end{abstract}\n\\category{H.2.8}{Database Management}{Database Application}[Data Mining]\n\\terms{Imbalance Learning, Sampling Techniques}\n\\keywords{Classification, Ensemble Learning, stackingC}\n\n\\section{Introduction}\nIn data mining, to have an extreme imbalanced data set is inevitable such as the case of the classification problem in this study. It is natural for the data set to have a skewed distribution because there are far more continuing students (COS) than students that go on leave of absence (LOA). The class with the most number of instances (i.e., COS) is called the majority class while the class with the small number of instances (i.e., LOA) is called the minority class. In the case this study, there are twelve minority classes (refer to Figure 1) thus the sampling techniques were used as solutions to the multi-class imbalance learning problem. \n\t\nThe problem with an extreme imbalance data set is the skewed distribution of the data that makes the learning algorithms ineffective, especially in predicting minority classes. This problem has drawn enough attention from both industry and academia, thus many state-of-the-art solutions now exists such as sampling techniques, cost sensitive methods, and kernel-based methods~\\citep{he-2009}. Even if some empirical studies have shown that cost sensitive and kernel-based methods are superior than sampling techniques, sampling techniques still also dominates the current researches for imbalanced learning because of its flexibility for any learning algorithm. Meanwhile, the two techniques can be used only with a specific learning algorithm, for example, kernel-based methods is implemented with support vector machines and some cost sensitive methods with adaptive boosting~\\citep{he-2009}.\n\t\nThere are many machine learning algorithms developed already for the classification task but each algorithm typically suits some classification problems better than others; and this is why there is no universal data mining method for all problem types~\\citep{fayyad-1996}. With the claim that all classification algorithms are equally alike with each having its own advantages and disadvantages, a question on whether a new algorithm that mixes some of the advantages of the algorithms performs better than individual algorithms was investigated. This new algorithm composed of different learning algorithms is called ensemble learning and it was verified using multiple data sets that an ensemble learner has a better performance than an individual learning algorithm~\\citep{lauron-2016}. The ensemble learning algorithm used in building a classifier in this study is stackingC and the combined learning algorithms were support vector machines (SVM), k-nearest neighbor (k-NN), C4.5, classification and regression trees (CART), and multilayer perceptron (MP). This combination of learning algorithms was used because it was found to be the near optimal combination of learning algorithms for stackingC ensemble classifier~\\citep{lauron-2016}.\n\t\nThe classification problem of this study concerns the classification of the incoming freshmen student of the University of the Philippines Los Ba\\~{n}os (UPLB) if whether they will shift course, transfer to another school or UP campus, go on absence without leave (AWOL), be a continuing student, and so forth using their available data such as their high school (HS) grades, HS type, UP College Admission Test result, family income, address, etc.\n\n\n\\section{Sampling Techniques}\n\\label{sec:samptech}\n\n\\looseness-1Sampling techniques as a solution to the imbalance learning problem simply modifies an imbalanced data set so that it will have a somewhat balanced distribution. Some sampling techniques commonly used in imbalance learning are random undersampling (RU), random oversampling (RO), and synthetic minority oversampling technique (SMOTE)~\\citep{he-2009}. \n\\subsection{Sampling Techniques in the Literature}\n\\subsubsection{Random Undersampling}\nThe mechanics of RU as its name implies, randomly removes instances from all classes except from the class with the least number of instances (most minority class) until the number of instances for all the classes is equal. \n\\subsubsection{Random Oversampling}\nWhile undersampling removes data from the original data, RO adds data by randomly replicating the instances of the twelve minority classes until the number of instances for all the classes are equal. \n\\subsubsection{Synthetic Minority Oversampling Technique}\nThe SMOTE just like RO also adds new data but in a different manner~\\citep{chawla-2002}. SMOTE creates artificial data based on feature space similarities between existing instances of the minority class. This technique uses the k-nearest neighbor algorithm to find the k-nearest neighbors of an instance $x_{i}$ in the minority class. To create a synthetic instance $x_\\mathrm{new}$, a random instance $x^{'}$ is chosen from the k-nearest neighbors, then the corresponding feature vector difference is multiplied with a random number between [0,1] and finally this vector is added to $x_{i}$ \n\n", "index": 1, "text": "\\begin{equation}\nx_\\mathrm{new} = x_{i} + (x^{'}- x_{i}) \\times \\delta\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"x_{\\mathrm{new}}=x_{i}+(x^{{}^{\\prime}}-x_{i})\\times\\delta\" display=\"block\"><mrow><msub><mi>x</mi><mi>new</mi></msub><mo>=</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>x</mi><msup><mi/><mo>\u2032</mo></msup></msup><mo>-</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u00d7</mo><mi>\u03b4</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04756.tex", "nexttext": "\nwhere $c$ is the total number of classes and  $rc_{i}$is the recall value of the class.\n\n\nAfter applying the eight sampling techniques on the original student data, the new data distributions per sampling technique is shown in Figures~2--6. The distribution of the data for both SMOTE and SMOTERandRep is the same as shown in Figure~2.  \n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{smotesdata}}\n\\caption{Distribution of the students' data on 13 classes after applying either SMOTE or SMOTERandRep sampling technique.}\n  \\label{fig:smotesdata}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{rodata}}\n\\caption{Distribution of the students' data on 13 classes after applying RO.}\n  \\label{fig:rodata}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{rudata}}\n\\caption{Distribution of the students' data on 13 classes after applying RU.}\n  \\label{fig:rudata}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{lrodata}}\n\\caption{Distribution of the students' data on 13 classes after applying LRO.}\n  \\label{fig:lrodata}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{lrudata}}\n\\caption{Distribution of the students' data on 13 classes after applying LRU.}\n  \\label{fig:lrudata}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{clroudata}}\n\\caption{Distribution of the students' data on 13 classes after applying C-LROU.}\n  \\label{fig:clroudata}\n\\end{figure*}\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{clruodata}}\n\\caption{Distribution of the students' data on 13 classes after applying C-LRUO.}\n  \\label{fig:clruodata}\n\\end{figure*}\n\n\\section{Results}\n\\label{sec:result}\n\nAs expected, the classifier's performance using the original data is good for the majority class (COS) alone as shown in Figure 9. Among the eight sampling techniques, RU and LRU showed the worst performance for all the classes while other techniques (i.e., RO, C-LRUO, and C-LROU) performed well only on some of the classes. The best performing sampling techniques for all the classes were SMOTE, SMOTERandRep, and LRO having the lowest F-measure class values (SDW) of 0.65, 0.52, and 0.61 respectively. The result implies that sampling techniques indeed improves the performance of the classifier and that oversampling techniques are preferable than undersampling techniques.\n\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=18cm]{fmeasuer}}\n\\caption{Classifier's performance over nine data sets using F-measure metric.}\n  \\label{fig:fmeasure}\n\\end{figure*}\n\nAs for the overall performance of the classifier, the lowest G-mean values were of the original data, LRU, and RU as shown in Figure 10. The top sampling techniques with the highest G-mean values were LRO (0.86), C-LROU (0.85), SMOTE (0.84), and SMOTERandRep (0.83). The G-mean results are consistent with the F-measure results except for C-LROU. This still means that the C-LROU technique cannot be considered as a sampling technique with a good performance because even if it has the highest F-measure values on all classes (except the majority class), it has jeopardized the performance of the classifier on the majority class. \n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{gmean}}\n\\caption{Classifier's performance over nine data sets using G-mean metric.}\n  \\label{fig:gmean}\n\\end{figure*}\n\n\\section{Conclusion}\n\\label{sec:conc}\nThe results showed that indeed sampling techniques improved the performance of the classifier for all minority classes. Between the two techniques (RO and RU), the results also showed that RO is preferable. The reason of this is obvious, since RU removes instances of the majority class, as the classifier is generated, it missed important concepts about the majority class resulting to a better performance on some minority classes but jeopardized performance on the majority class. The result of this study also showed that LRO, SMOTE, SMOTERandRep were the best performing techniques. To further verify this result, it is suggested that the next iterations of this study make use of larger multi-class imbalanced data set and use hold and wait validation method, instead of stratified tenfold cross-validation.\n\\bibliographystyle{plainnat}\n\\bibliography{improved-sampling-techniques}\n\n\\balancecolumns \n\n\n", "itemtype": "equation", "pos": 12007, "prevtext": "\nwhere $x_{i}$ is a minority instance, $x^{'}$ is the randomly chosen k-nearest neighbor, and $\\delta$ is a random number between 0 and 1.\n\n\\subsection{Proposed Sampling Techniques}\nThe five new sampling techniques proposed in this paper are based on the abovementioned sampling techniques. \n\\subsubsection{SMOTERandRep}\nThe first technique is called SMOTERandRep, a short name for SMOTE Random Replacement. This technique is based on the idea of SMOTE because it also oversamples the minority class and create synthetic instances. Given an instance $x_{i}$ of a minority class and its k-nearest neighbors, it creates a new instance by copying the features of $x_{i}$ and randomly replacing $\\log_{2}a$ attribute values with the values from the randomly selected k-nearest neighbors where $a$ is the total number of attributes and k value is also equal to $\\log_{2}a$. \n\\subsubsection{Lax Random Undersampling}\nThe second technique is called Lax Random Undersampling (LRU). This technique is based on the idea of RU and the difference is instead of aiming for having an equal number of instances for the minority class and majority class, it randomly delete instances from the majority class until the number of instances in this class is equal to the number of instances of the minority class with the most number of instances.\n \\subsubsection{Lax Random Oversampling}\n The third technique is called Lax Random Oversampling (LRO) and as its name suggests, is based on the idea of RO. This technique differs with RO because instead of having an equal number of instances for the minority class and majority class, it randomly replicates the minority instances until the minority class with the most number of instances is replicated once. \n \\subsubsection{Combined-LROU}\n The fourth and fifth techniques are simply the combination of the second and third techniques (LRO and LRU) and that is why it is called Combined-LROU (C-LROU) and Combined-LRUO (C-LRUO). The difference of the two techniques is the order of combining LRO and LRU, for example, C-LROU first applies LRO on the original data set, then LRU is applied on the resulting data set. \n \\subsubsection{Combined-LRUO}\n As for C-LRUO, it first applies LRU on the original data set and after that LRO is applied on the resulting data set.\n\n\\section{Methodology}\n\\label{sec:methodology}\n\nThe student data used in this study for the UPLB classification problem was gathered from the Office of the University Registrar (OUR) of UPLB and from five different colleges of the university. The collected data identified 13 possible classification of an instance of a student and these are shiftee (SHC), transfer to another UP campus (TUP), transfer to another school (TAS), scholastic delinquency warning (SDW), scholastic delinquency probationary (SDP), scholastic delinquency dismissal (SDD), scholastic delinquency permanent disqualification (SPQ), readmission of disqualified and dismissed students (RAM), absence without leave (AWL), leave of absecence (LOA), honorable dismissal (HDM), scholastic delinquency warning and probationary (SWP), and continuing student (COS). A total of 2297 instances of students were collected and the distribution of the data per class is shown in Figure~1.\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=14cm]{origdata.eps}}\n\\caption{Distribution of the original students' data on 13 classes.}\n  \\label{fig:origdata}\n\\end{figure*}\n\nWaikato Environment for Knowledge Analysis (WEKA) is an open source (available under GNU General Public License) and cross-platform (written in Java) data mining software for data mining tasks~\\citep{hall-2009}. WEKA collects and automates state-of-the-art data mining algorithms thus, its implementation of stackingC ensemble learning algorithm as well as the implementations of other algorithms (i.e., SVM, MP, CART, C4.5, and k-NN) was used in this study. WEKA's implementation of SMOTE technique was also used in this study. The evaluation metrics used to measure the performance of the classifier were F-measure and G-mean together with stratified tenfold cross-validation as the validation method. Both metrics are widely used metric; the F-measure metric shows the classifier's performance for every class while G-mean measures the overall performance of a classifier~\\citep{wang-2012}. The maximum value for both metrics is one. WEKA has also provided the functionality to compute the F-measure value of a generated classifier and it was also used in this study. The G-mean metric, on the other hand, can be easily computed using recall value performance of the classifier for every class (also available in WEKA) and the formula is\n\n", "index": 3, "text": "\\begin{equation}\n\t\\mathrm{G\\text{-}mean} = \\left(\\Pi_{i=1}^{c}rc_{i}\\right)^{\\frac{1}{c}}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{G\\text{-}mean}=\\left(\\Pi_{i=1}^{c}rc_{i}\\right)^{\\frac{1}{c}}\" display=\"block\"><mrow><mrow><mrow><mi mathvariant=\"normal\">G</mi><mo>\u2062</mo><merror class=\"ltx_ERROR undefined undefined\"><mtext>\\text</mtext></merror></mrow><mo>-</mo><mi>mean</mi></mrow><mo>=</mo><msup><mrow><mo>(</mo><mrow><msubsup><mi mathvariant=\"normal\">\u03a0</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></msubsup><mo>\u2062</mo><mi>r</mi><mo>\u2062</mo><msub><mi>c</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mfrac><mn>1</mn><mi>c</mi></mfrac></msup></mrow></math>", "type": "latex"}]