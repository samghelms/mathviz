[{"file": "1601.00665.tex", "nexttext": "\n\\noindent\nbe surprising?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I do not think so.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} And what about the outcome where all tosses but the last one came up heads?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Yes, it would be surprising.  I would suspect cheating.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} But why? Both outcomes have exactly the same probability, $2^{-41}$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I begin to see your point. The second outcome is special.  It is surprising even though it has not been predicted. I guess it is surprising by its very nature.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Yes. But what makes the second outcome special?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} The particularly simple specification?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} That is exactly it. The particularly simple specification makes the outcome surprising and independent from the execution of the trial.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But how do you measure the simplicity of specifications? There are 41 characters, including blanks, in the phrase ``all tosses but the last one came up heads'', and 41 binary symbols in \\ref{equ:ht}. Since the Latin alphabet (with the blank symbols) is richer than the binary alphabet $\\{H,T\\}$, one can reasonably argue that the specification \\ref{equ:ht} is simpler, much simpler.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} This is a good question.  The whole next section is devoted to it.\n\n\n\\section{Random Events and their Specification \\\\ Complexity}\n\\label{sec:complexity}\n\n\\subsection{Algorithmic Information Theory}\n\\label{sub:ait1}\n\nThe introductory examples illustrate the possibility that some presumably random events may be not random at all.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} What does it mean that an event is random?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Classical probability theory does not address the question but algorithmic information theory (AIT) does. The basic ideas of AIT were discovered in the 1960s independently by Ray Solomonoff \\cite{Solomonoff:1964}, Andrei N. Kolmogorov \\cite{Kolmogorov:1965} and Gregory J. Chaitin \\cite{Chaitin:1966}. The history of these discoveries is described in \\S1.13 of the book \\cite{Li-Vitanyi:1997} by Li and Vit\\'{a}nyi that we use as our main reference on the subject. Chaitin's book  \\cite{Chaitin:1987} on AIT is available online.\n\nA key notion of AIT is the Kolmogorov (or information) complexity of strings.  Intuitively, the Kolmogorov complexity $C(s)$ of a string $s$ is the length of a shortest program that outputs $s$. The larger $C(s)$ is, comparative to the length of $s$, the more random $s$ is.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Programs in what programming language?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Programs for a universal Turing machine. AIT was influenced by the computation theory of the time. Traditionally, in AIT, one restricts attention to Turing machines with the binary alphabet, and the universality of a Turing machine $U$ means that $U$ faithfully simulates any other Turing machine $T$ on any input $x$ given $T$ (in one form or another) and given exactly that same input $x$. View a universal Turing machine $U$ as a programming language, so that programs are binary strings.  The Kolmogorov complexity $C_U(s)$ of a binary string $s$ is the length of a shortest $U$ program that outputs $s$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But this depends on the choice of a universal Turing machine $U$.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} It does. But, by the Invariance Theorem \\cite[\\S2.1]{Li-Vitanyi:1997}, for any two universal Turing machines $U_1$ and $U_2$, there is a constant $k$ such that\n$\n C_U(s) \\leq C_V(s) + k\n$\nfor all binary strings $s$.  In that sense, the dependence on the choice of universal Turing machine $U$ is limited.\n\nThere is also a conditional version $C_U(s|t)$ of Kolmogorov complexity, that is the complexity of string $s$ given a string $t$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I wonder how would one use Kolmogorov complexity to show that the suspicious outcomes of the introductory examples are not random.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Unfortunately Kolmogorov complexity does not seem to work well for our purposes.\nWhatever universal Turing machine $U$ is fixed, the function $C_U(s)$ is not computable \\cite[Theorem~2.3.2]{Li-Vitanyi:1997}. And the machine $U$ does not know anything about the scenarios. Consider the lottery scenario for example. Intuitively the event of Donna winning the lottery should have smaller description complexity than the event of some stranger to John winning the lottery. But this is most probably not the case, precisely because the machine $U$ does not know anything about the scenario.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Maybe one can use the conditional version $C_U(s|t_0)$ of Kolmogorov complexity where $t_0$ is a particular string that describes the given scenario. I suspect that the function $f(s) = C_U(s|t_0)$ is still uncomputable. But maybe one can approximate it.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Maybe. But it seems to us that there are simpler and more natural ways to deal with scenarios like those in our introductory examples.\n\n\\subsection{Description Complexity}\n\\label{sub:dc}\n\nWe assume that the reader is familiar with the basics of first-order logic though we recall some\nnotions. A (one-sorted) relational structure $A$ consists of\n\\begin{itemize}\n\\item a nonempty set, the \\emph{base set} of structure $A$, whose elements are called the\nelements of $A$;\n\\item several relations over the base set including equality, the \\emph{basic relations} of\nstructure $A$; each basic relation has its own arity (the number of arguments);\n\\item several distinguished elements of $A$ known as \\emph{constants}.\n\\end{itemize}\nThe ever present equality is a \\emph{logic relation}, in contrast to other basic relations. The names of the basic non-logic relations and constants form the \\emph{vocabulary} of $A$. Equality is typically omitted when a structure is described. For example, a directed graph is typically described as a relational structure with one binary relation and no constants.\n\nA multi-sorted relational structure is defined similarly except that the base set is split into several nonempty subsets called \\emph{sorts}. Each argument position $i$ of every basic relation $R$ is assigned a particular sort $S_i$; the \\emph{type} of $R$ is the direct product $S_1\\times\\cdots\\times S_r$ where $r$ is the arity of $R$. Equality, the only logic relation, is an exception. Its type can be described as $\\bigcup_S (S\\times S)$ where $S$ ranges over the sorts. The vocabulary of a multi-sorted relational structure contains the names of sorts,  relations and constants. Besides, the vocabulary indicates the types of basic relations, individual variables and constants.\n\n\\begin{example}\\label{ex:2}\nHere is a structure related to the lottery scenario. It has two sorts. One sort, called Person, consists of people, namely all lottery ticket owners as well as John, the lottery organizer. The other sort, called Ticket, consists of all the lottery tickets that have been sold. The structure has a binary relation Owns of type Person $\\times$ Ticket, with the obvious interpretation. It also has a constant John that denotes the lottery organizer.\n\\end{example}\n\nIf $A$ is a relational structure, $S$ is a sort of $A$ and $X\\subseteq S$, we say that $X$ is \\emph{definable} in $A$ if there is a first order formula ${\\varphi}(x)$ with a single free variable $x$ of type $S$ such that $X$ is the set of elements $a$ of sort $S$\nsatisfying the proposition ${\\varphi}(a)$ in $A$, that is if\n\n", "itemtype": "equation", "pos": 22084, "prevtext": "\n\\title{Impugning Randomness, Convincingly}\n\\author{Yuri Gurevich\n\\thanks{Microsoft Research, Redmond, WA, U.S.A. \\url{gurevich@microsoft.com}}\n\\and\nGrant Olney Passmore\n\\thanks{Clare Hall, University of Cambridge and LFCS, University of Edinburgh, UK.\n\\url{grant.passmore@cl.cam.ac.uk}}\n\\date{May 2011}}\n\\maketitle\n\n\\mbox{}\\vspace{1in}\n\\begin{abstract}\\noindent\nJohn organized a state lottery and his wife won the main prize. You may feel that the event of her winning wasn't particularly random, but how would you argue that in a fair court of law? Traditional probability theory does not even have the notion of random events. Algorithmic information theory does, but it is not applicable to real-world scenarios like the lottery one. We attempt to rectify that.\n\\end{abstract}\n\n\\newpage\n\\tableofcontents\n\n\\newpage\n\\section{Introduction}\n\\label{sec:intro}\n\nTo motivate our study, we begin with four examples. In each of the four cases, a probabilistic\ntrial produces a suspicious outcome, and the question arises whether the outcome is the result of\npure chance.  The first case is a thought experiment inspired by a remark of Leonid Levin in\narticle \\cite{Levin:1984}.  The second and third cases are also thought experiments, arguably\nmore realistic.  The fourth case is real.\n\n\\subsection{A Lottery Case}\n\\label{sub:lottery1}\n\nJohn and Donna live in Georgia, a state of about 10,000,000 inhabitants.  John is Donna's husband\nand the president of the Georgia State Lottery. Anybody may enter into the lottery by buying as\nmany tickets as (s)he wishes. Every ticket is uniquely numbered.  A winner is chosen at random by\nthe selection of a ticket number.  This year Donna happens to win.  Over 10,000,000 tickets were\npurchased in total, spread among about 4,000,000 people.  Donna purchased three. John bought\nnone.\n\nSoon after this win is announced, the local media begins to echo claims of corruption against\nJohn. How could it be that of all of the about 4,000,000 participants, the president's wife won?\nSurely something must be amiss. John, faced with allegations of unfairness, argues as follows:\n\n\\begin{quote}\nSomeone had to win the lottery.\n\nThe process of choosing the winner was fair.\nAlmost every ticket owner (the only exception being a handful of people who bought many tickets)\nhad a small chance of winning. If a stranger to me who also bought a small number of tickets had\nwon, no one would be crying foul. But, such a stranger would have roughly the same small\nprobability of winning as Donna did. Given that someone had to win, nothing strange has happened.\nIn particular, there are no grounds to claim the lottery was rigged.\n\\end{quote}\n\n\\noindent\nWould you believe him?\n\n\\subsection{A Jury Case}\n\\label{sub:jury1}\n\nThomas seemed to be a common criminal but there was something uncommon about his case. At least\nthe prosecutor thought so. As prospective jurors were questioned, she realized that some of them\nwere unusually informed. She investigated. It turned out that seven out of 50 prospective jurors\nbelonged to a Facebook group of about 100 people that discussed Thomas's case. This was the only\nFacebook group that discussed the case.\n\nProspective jurors had been chosen at random from a population of about one million adults\navailable for the purpose. Can it be mere chance that so many prospective jurors belong to the\none and relatively small Facebook group that have discussed the case?\n\n\\subsection{A Stalking Case}\n\\label{sub:stalking1}\n\nAlice and Bob are a couple living in New York City. They don't have a real kitchen and usually\ndine out.  Chris is Alice's unstable ex-boyfriend, and  Alice believes that Chris is stalking\nher. Too often she has seen him at restaurants.  Alice has wanted to obtain a restraining order\nbut Bob has argued that they didn't have enough evidence to convince the authorities. After all,\nChris and Alice used to live together and may naturally frequent the same restaurants.  So Bob\nsuggested an experiment: ``There are at least 100 reasonable restaurants within walking distance\nfrom our place. For the next ten nights, let's pick a restaurant at random except that it should\nbe a new restaurant each time''.  They performed the proposed experiment. In 5 out of the 10\ncases, Chris showed up. Is this evidence sufficient for Alice to obtain a restraining order?\n\n\n\\subsection{The Case of the Man with the Golden Arm}\n\\label{sub:caputo1}\n\nThe story appeared in New York Times on July 23, 1985, on page B1. We learned of it from the book\n\\cite{Dembski:1998}.\n\n\\begin{quote}\nTRENTON, July 22 --- The New Jersey Supreme Court today caught up with the ``man with the golden\narm,'' Nicholas Caputo, the Essex County Clerk and a Democrat who has conducted drawings for\ndecades that have given Democrats the top ballot line in the county 40 times out of 41 times.\n\\end{quote}\n\n\\noindent\nThe court felt that something was wrong.\n\n\\begin{quote}\n The court suggested --- but did not order --- changes in the way Mr. Caputo conducts the\ndrawings to stem ``further loss of public confidence in the integrity of the electoral\nprocess.''\n\\end{quote}\n\n\\noindent\nCaputo wasn't punished. A question arises whether the circumstantial evidence was sufficient to\njustify punishing him.\n\n\n\\subsection{Overview}\n\\label{sub:overview}\n\nThe four cases above have something in common. In each case, there is a strong suspicion that a\npresumably random event is not random at all. But how can one justify the suspicion?\n\nThe purpose of this paper is to build a practical framework in which the desired chance-elimination arguments can be formalized and defended. We start, in \\S\\ref{sec:bridge}, with a classical principle, often called Cournot's principle, according to which it is a practical certainty that an event with very small probability will not happen. We expound Cournot's principle. In particular, we make explicit that the event of interest is supposed to be specified in advance. Then we generalize Cournot's principle to a more liberal principle, called the bridge principle, that requires only that the event of interest be specified independently from the execution of the probabilistic trial in question. At the end of \\S\\ref{sec:bridge}, we address the question how an after-the-fact specification can be independent. The inspiration comes from algorithmic information theory, and the intuitive idea is this: some specifications are so succinct that they could have been naturally written ahead of time (and maybe have been written in similar cases in the past).\n\n\\S\\ref{sec:complexity} is auxiliary. First we recall some basic notions of algorithmic information theory, in particular the notion of Kolmogorov complexity (or information complexity) of events. In algorithmic information theory, events are represented by binary strings. The Kolmogorov complexity of an event is the length of a shortest program for a fixed universal Turing machine that outputs the string presentation of the event. This approach does not work for our purposes. In each probabilistic case in question, we need specifications formulated in terms pertinent to the case and abstracted from irrelevant information. To this end we use logic, and the rest of the section is devoted to logic. We recall and illustrate logic structures and the notion of logical definability. Then we introduce and discuss the notion of the description complexity of events.\n\nIn \\S\\ref{sec:impugn}, we explain how we intend to impugn randomness.\nIn \\S\\ref{sec:examples} we illustrate our approach on the cases described above.\nThere are many discussions with our old friend Quisani throughout the article. The final discussion is in \\S\\ref{sec:final}.\n\n\\subsection*{Acknowledgments}\n\nWe thank Andreas Blass, Leonid Levin, Glenn Shafer and Vladimir Vovk for\ndiscussions that influenced our thinking, and we thank Cristian Calude, Guido de Caso, Alexander Shen and Paul Vit\\'{a}nyi for very useful comments on a draft of this paper.\n\n\n\\section{Bridging Probabilities and the Physical World}\n\\label{sec:bridge}\n\nWe explicate and broaden the well known principle according to which it is a practical certainty\nthat an event with very small probability will not happen.\n\n\\subsection{Cournot's Principle}\n\\label{sub:cournot}\n\nProbability theory is applied widely and successfully. But what makes\nthis mathematical theory relevant to the physical world?  In The Art of\nConjecturing, published posthumously in 1713\n\\cite{Bernoulli:1713}, Jakob Bernoulli related\nmathematical probabilities to practical (or ``moral'') certainty:\n\n\\begin{quote}\n  Something is morally certain if its probability is so close to certainty that the shortfall is\nimperceptible. Something is morally impossible if its probability is no more than the amount by\nwhich moral certainty falls short of complete certainty. Because it is only rarely possible to\nobtain full certainty, necessity and custom demand that what is merely morally certain be taken\nas certain. It would therefore be useful if fixed limits were set for moral certainty by the\nauthority of the magistracy --- if it were determined, that is to say, whether 99/100 certainty\nis sufficient or 999/1000 is required.\n\\end{quote}\n\n\\noindent\nIn other words, it is a practical certainty that an event with very small probability will not\nhappen.\nAntoine Cournot seems to be the first to suggest, in the book \\cite{Cournot:1843}, that this\nprinciple is the only way of connecting mathematical probabilities to the world. Accordingly the\nprinciple is often ascribed to Cournot.\n\n\\medskip\\noindent\n{\\bf Cournot's Principle}\\quad\nIt is a practical certainty that an event with very small probability will not happen.\n\\medskip\n\nThe principle was supported by many heavyweights of probability theory and statistics, in particular \\'{E}mile Borel \\cite{Borel:1943}, Ronald A. Fisher \\cite{Fisher:1925}, Andrei N. Kolmogorov \\cite{Kolmogorov:1933} and Paul L\\'{e}vy \\cite{Levy:1925}. Borel called Cournot's principle The Single Law of Chance.\n\nMore information on Cournot's principle is found in the Shafer and Vovk book\n\\cite{Shafer-Vovk:2001} and Shafer's lecture \\cite{Shafer:2006} that is available online.\n\n\n\\subsection{How Small is Sufficiently Small?}\n\\label{sub:small}\n\n\\smallskip\\noindent\\textbf{Quisani:\\ } How small is sufficiently small? I presume that there is some cut-off point, a threshold value, the least upper bound for the sufficiently small values.\n\n\\smallskip\\noindent\\textbf{Authors:\\ } Yes, that's the idea. Let us quote \\'{E}mile Borel in this connection. In his 1943 book \\cite{Borel:1943} for the non-scientist he wrote the following.\n\n\\begin{quote}\nWhen we stated The Single Law of Chance, ``events whose probability is sufficiently small never occur,'' we did not conceal the lack of precision of the statement.  There are cases where no doubt is possible; such is that of the complete works of Goethe being reproduced by a typist who does not know German and is typing at random.  Between this somewhat extreme case and ones in which the probabilities are very small but nevertheless such that the occurrence of the corresponding event is not incredible, there are many intermediate cases.  We shall attempt to determine as precisely as possible which values of probability must be regarded as negligible under certain circumstances.\n\\end{quote}\n\n\\noindent\nNote ``under certain circumstances''. Different application areas may use different threshold values.  You may want to reject the null hypothesis ``by preponderance of evidence'' or ``beyond a reasonable doubt''. If the courts of law were to use probabilistic thresholds (tailored to specialized circumstances), these distinct judicial criteria would give rise to distinct threshold values. Different criteria and different threshold values may be used in testing scientific hypotheses.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I suppose much experience is needed to just propose a reasonable threshold for a fixed application area. And new developments, new technologies may require that the accepted value be revised. I am thinking of the use of DNA evidence in courts. A number of convictions have been overturned. If the courts used thresholds, some of them should have been adjusted down. I can imagine also the necessity to adjust a threshold value up. Think of cases when people have not been convicted but later confessed to crimes.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} The issue of appropriate threshold values has been much discussed, especially in connection with statistical hypothesis testing. In fact there are many discussions, in various applications domains, e.g. clinical trials \\cite{Davis-Hardy:1990,Friedman:1996,Meinert:1986,Moye:1998}, psychology \\cite{Cohen:1994,Fraley:2003}. Statistical hypothesis testing is often misunderstood and abused \\cite{Batanero:2000,Cohen:1994,Fraley:2003,Morrison+1:1970}. In the rest of the paper, we will avoid the issue of appropriate threshold values.\n\n\\begin{proviso}\nGiven a probabilistic trial, we will always assume the existence of an agreed and current probability threshold for the application domain of the trial.\n\\end{proviso}\n\n\\subsection{Cournot's Principle Expounded}\n\\label{sub:expound}\n\nOur formulation of Cournot's principle is rather common. It is also aphoristic. As stated, Cournot's principle does not hold water: events of very small probability happen all the time in the physical world. Several aspects of the principle are implicit. One aspect, for example, is that the probabilistic experiment in question is performed only once. (This particular aspect is made explicit in \\cite{Kolmogorov:1933}.) In this subsection we explicate Cournot's principle. Later we will broaden it and call it the bridge principle to emphasize that it bridges between probability theory and the physical world.\n\nAs in probability theory, a trial $T$ is a real or imaginary experiment with a well defined set $\\Omega_T$ of possible outcomes and with events of $T$ as subsets of $\\Omega_T$. In general (when $\\Omega_T$ is uncountable) there may be some subsets of $\\Omega_T$ that are not events. The case of most interest to us is when $\\Omega_T$ is finite; in that case every subset of $\\Omega_T$ is an event.\n\nWe introduce a few nonstandard terms that are useful for our purposes. An \\emph{executed trial} is a trial together with a particular execution of the trial; the execution results in a particular outcome called the \\emph{actual outcome} of the executed trial. An event $E$ \\emph{happens} or \\emph{occurs} at  the executed trial if and only if $E$ contains the actual outcome.\n\nA \\emph{probabilistic trial} $(T,{\\mathcal{F}})$ is a trial $T$ together with a hypothesis, called the \\emph{null hypothesis}, that the probability distribution that governs the trial $T$ belongs to ${\\mathcal{F}}$. The probability distributions of ${\\mathcal{F}}$ are the \\emph{innate probability distributions} of the probabilistic trial $(T,{\\mathcal{F}})$. (We are not going to define what it means for a trial $T$ to be governed by a probability distribution ${\\mathtt{P}}$; the connection to the physical world is given by the expounded Cournot principle below.) An \\emph{executed probabilistic trial} is a probabilistic trial $(T,{\\mathcal{F}})$ together with a particular execution of $T$ (that produces the actual outcome of the executed probabilistic trial).\n\nAn event $E$ of a probabilistic trial $(T,{\\mathcal{F}})$ is \\emph{negligible} if, for every innate probability distribution ${\\mathtt{P}}$, the probability ${\\mathtt{P}}(E)$ is less than the current probability threshold in the application area of the trial.\n\n\\begin{example}\\label{ex:1}\nView the lottery of \\S\\ref{sub:lottery1} as an executed probabilistic trial with possible outcomes of the form ``$o$ wins the lottery'' where $o$ ranges over the lottery ticket owners. The null hypothesis says that the trial is governed by the probability distribution where the probability of outcome ``$o$ wins the lottery'' is proportional to the number of lottery tickets that $o$ owns. The actual outcome is ``Donna wins the lottery''. Since Donna bought three tickets, the event ``the winner bought three tickets'' occurs during the execution.\n\\end{example}\n\nA \\emph{probabilistic scenario} $(T,{\\mathcal{F}},E)$ is a probabilistic trial $(T,{\\mathcal{F}})$ together with an event $E\\subseteq\\Omega_T$ called the \\emph{focal event} of the probabilistic scenario. An \\emph{executed probabilistic scenario} is a probabilistic scenario $(T,{\\mathcal{F}},E)$ together with a particular execution of $T$ (that produces the actual outcome of the executed probabilistic scenario).\n\n\\paragraph{Cournot's Principle Expounded}\nConsider a probabilistic scenario with a negligible focal event. If the focal event is specified\nbefore the execution of the trial then it is practically certain that the focal event will not\nhappen upon the execution.\n\\medskip\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} What is the point in fixing an event and execution?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} If too many small-probability events are specified then it may become likely that at least one\nof them happens even if we restrict attention to one execution of the trial. Similarly any event\nof  positive probability becomes likely to happen if the trial is  executed too many times.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Given an informal description of a trial, it may be not obvious what the possible outcomes\nare. Consider the lottery case. The way the story is told in \\S\\ref{sub:lottery1}, every outcome\nis associated with the winning person. This is natural. But it is also natural, maybe even more\nnatural, to associate outcomes with the winning tickets.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} You are right. An informal description of a trial may be somewhat vague about precisely what possible outcomes are. But the definition of a probabilistic trial requires that the set of possible outcomes be indicated explicitly. In the lottery case, there are indeed these two natural ways to view possible outcomes. It does not really matter which way to go. We picked the first way because it is a tiny bit more convenient for our purposes.\n\n\n\n\n\\subsection{Cournot's Principle and Statistical Hypothesis Testing}\n\\label{sub:fisher}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} You started with the problem of connecting probabilities to the physical world. But do you\nknow the true probabilities of real world events? I think not. All you have is a mathematical\nmodel. It surely is at best an approximation of reality. For example, people speak about tossing\na fair coin but no real coin is perfectly fair and no tossing is perfect. More importantly, a\nreal world trial may be rigged, so that your mathematical model may be far from reality. I would\nnot be surprised if some magicians can produce any desired sequence of heads and tails by\nrepeatedly tossing a coin.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Turn Cournot's principle upside down. Consider an executed probabilistic scenario with a\nnegligible focal event specified before executing the trial. If the focal event occurs during the\nexecution of the trial then reject the null hypothesis.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Is this related to statistical hypothesis testing?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} This is, as far as we understand, the basic idea of Ronald A. Fisher's method of statistical hypothesis testing  \\cite{Fisher:1925,Fisher:1926,Fisher:1956}. The term ``null hypothesis'' is borrowed from Fisher. In Fisher's approach, the focal event is specified by means of the $p$-value of some statistics.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Hmm, I don't know anything about $p$-values. What is your best reference on Fisher's\napproach?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} The Cox and Hinkley book \\cite{Cox-Hinkley:1974}.\n\n\n\\subsection{The Bridge Principle}\n\\label{sub:bridge1}\n\nNow we are ready to broaden Cournot's principle. We use the definitions of \\S\\ref{sub:expound}.\n\n\\medskip\\noindent\n{\\bf The Bridge Principle}\nConsider a probabilistic scenario with a negligible focal event. If the focal event is specified\n\\emph{independently} of the  execution of the trial then it is practically certain that the focal\nevent does not happen upon the execution.\n\\medskip\n\nWe will use the bridge principle as a ground for rejection of (or at least as a significant argument against) the null hypothesis.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} How can an after-the-fact specification be independent? I think that I understand the intent of prior specifications.  They are predictions.  If I give Grant a deck of cards and, without examining the cards, he draws the king of spades, there is no surprise. But if he announces in advance that he is going to draw the king of spades and then indeed he does that, then there is a surprise. I'd think that Grant is a magician. But after-the-fact ``predictions'' do not make sense to me.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Suppose that Grant did not announce a card in advance.  Instead Yuri announces the card, after the fact and apparently without any  communication with Grant. Is there an element of surprise?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Yes, I suppose there is. I would suspect that Grant is a magician or that there was some communication between you two. I guess the point is not that the focal event is specified in advance but that it is specified --- possibly after the fact --- without any knowledge about the outcome of the trial, not even partial knowledge.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Let us give a name to the principle that you propose:\n\n\\medskip\\noindent\n{\\bf The Narrow Bridge Principle} Consider a probabilistic scenario with a negligible focal event. If the focal event is specified without any information about the actual outcome of the trial then it is practically certain that the focal event does not happen upon the execution.\n\\medskip\n\n\\noindent\nUnfortunately the narrow bridge principle is too narrow for our purposes. To illustrate the broader principle, consider a trial that consists of tossing a fair coin 41 times. Would the outcome\n\n\n", "index": 1, "text": "\\begin{equation}\\label{equ:ht}\n\\mathrm{HTTT THTH HTHT HTHH TTHT THHH TTTH HTHH TTTT THHH H}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{HTTTTHTHHTHTHTHHTTHTTHHHTTTHHTHHTTTTTHHHH}\" display=\"block\"><mi>HTTTTHTHHTHTHTHHTTHTTHHHTTTHHTHHTTTTTHHHH</mi></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\n\nThe formula ${\\varphi}(x)$ is a \\emph{definition} of $X$ in $A$.\n\nThe description complexity of $X$ in $A$ is the length of the shortest\ndefinition of $X$ in $A$.\n\n\\begin{example}\\label{ex:3}\nThe event ``the winner owns just one ticket'' consists of the outcomes ``p wins the lottery'' where $p$ ranges over the people owning exactly one\nticket. The event is thus definable by formula\n\n", "itemtype": "equation", "pos": 29819, "prevtext": "\n\\noindent\nbe surprising?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I do not think so.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} And what about the outcome where all tosses but the last one came up heads?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Yes, it would be surprising.  I would suspect cheating.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} But why? Both outcomes have exactly the same probability, $2^{-41}$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I begin to see your point. The second outcome is special.  It is surprising even though it has not been predicted. I guess it is surprising by its very nature.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Yes. But what makes the second outcome special?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} The particularly simple specification?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} That is exactly it. The particularly simple specification makes the outcome surprising and independent from the execution of the trial.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But how do you measure the simplicity of specifications? There are 41 characters, including blanks, in the phrase ``all tosses but the last one came up heads'', and 41 binary symbols in \\ref{equ:ht}. Since the Latin alphabet (with the blank symbols) is richer than the binary alphabet $\\{H,T\\}$, one can reasonably argue that the specification \\ref{equ:ht} is simpler, much simpler.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} This is a good question.  The whole next section is devoted to it.\n\n\n\\section{Random Events and their Specification \\\\ Complexity}\n\\label{sec:complexity}\n\n\\subsection{Algorithmic Information Theory}\n\\label{sub:ait1}\n\nThe introductory examples illustrate the possibility that some presumably random events may be not random at all.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} What does it mean that an event is random?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Classical probability theory does not address the question but algorithmic information theory (AIT) does. The basic ideas of AIT were discovered in the 1960s independently by Ray Solomonoff \\cite{Solomonoff:1964}, Andrei N. Kolmogorov \\cite{Kolmogorov:1965} and Gregory J. Chaitin \\cite{Chaitin:1966}. The history of these discoveries is described in \\S1.13 of the book \\cite{Li-Vitanyi:1997} by Li and Vit\\'{a}nyi that we use as our main reference on the subject. Chaitin's book  \\cite{Chaitin:1987} on AIT is available online.\n\nA key notion of AIT is the Kolmogorov (or information) complexity of strings.  Intuitively, the Kolmogorov complexity $C(s)$ of a string $s$ is the length of a shortest program that outputs $s$. The larger $C(s)$ is, comparative to the length of $s$, the more random $s$ is.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Programs in what programming language?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Programs for a universal Turing machine. AIT was influenced by the computation theory of the time. Traditionally, in AIT, one restricts attention to Turing machines with the binary alphabet, and the universality of a Turing machine $U$ means that $U$ faithfully simulates any other Turing machine $T$ on any input $x$ given $T$ (in one form or another) and given exactly that same input $x$. View a universal Turing machine $U$ as a programming language, so that programs are binary strings.  The Kolmogorov complexity $C_U(s)$ of a binary string $s$ is the length of a shortest $U$ program that outputs $s$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But this depends on the choice of a universal Turing machine $U$.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} It does. But, by the Invariance Theorem \\cite[\\S2.1]{Li-Vitanyi:1997}, for any two universal Turing machines $U_1$ and $U_2$, there is a constant $k$ such that\n$\n C_U(s) \\leq C_V(s) + k\n$\nfor all binary strings $s$.  In that sense, the dependence on the choice of universal Turing machine $U$ is limited.\n\nThere is also a conditional version $C_U(s|t)$ of Kolmogorov complexity, that is the complexity of string $s$ given a string $t$.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I wonder how would one use Kolmogorov complexity to show that the suspicious outcomes of the introductory examples are not random.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Unfortunately Kolmogorov complexity does not seem to work well for our purposes.\nWhatever universal Turing machine $U$ is fixed, the function $C_U(s)$ is not computable \\cite[Theorem~2.3.2]{Li-Vitanyi:1997}. And the machine $U$ does not know anything about the scenarios. Consider the lottery scenario for example. Intuitively the event of Donna winning the lottery should have smaller description complexity than the event of some stranger to John winning the lottery. But this is most probably not the case, precisely because the machine $U$ does not know anything about the scenario.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Maybe one can use the conditional version $C_U(s|t_0)$ of Kolmogorov complexity where $t_0$ is a particular string that describes the given scenario. I suspect that the function $f(s) = C_U(s|t_0)$ is still uncomputable. But maybe one can approximate it.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Maybe. But it seems to us that there are simpler and more natural ways to deal with scenarios like those in our introductory examples.\n\n\\subsection{Description Complexity}\n\\label{sub:dc}\n\nWe assume that the reader is familiar with the basics of first-order logic though we recall some\nnotions. A (one-sorted) relational structure $A$ consists of\n\\begin{itemize}\n\\item a nonempty set, the \\emph{base set} of structure $A$, whose elements are called the\nelements of $A$;\n\\item several relations over the base set including equality, the \\emph{basic relations} of\nstructure $A$; each basic relation has its own arity (the number of arguments);\n\\item several distinguished elements of $A$ known as \\emph{constants}.\n\\end{itemize}\nThe ever present equality is a \\emph{logic relation}, in contrast to other basic relations. The names of the basic non-logic relations and constants form the \\emph{vocabulary} of $A$. Equality is typically omitted when a structure is described. For example, a directed graph is typically described as a relational structure with one binary relation and no constants.\n\nA multi-sorted relational structure is defined similarly except that the base set is split into several nonempty subsets called \\emph{sorts}. Each argument position $i$ of every basic relation $R$ is assigned a particular sort $S_i$; the \\emph{type} of $R$ is the direct product $S_1\\times\\cdots\\times S_r$ where $r$ is the arity of $R$. Equality, the only logic relation, is an exception. Its type can be described as $\\bigcup_S (S\\times S)$ where $S$ ranges over the sorts. The vocabulary of a multi-sorted relational structure contains the names of sorts,  relations and constants. Besides, the vocabulary indicates the types of basic relations, individual variables and constants.\n\n\\begin{example}\\label{ex:2}\nHere is a structure related to the lottery scenario. It has two sorts. One sort, called Person, consists of people, namely all lottery ticket owners as well as John, the lottery organizer. The other sort, called Ticket, consists of all the lottery tickets that have been sold. The structure has a binary relation Owns of type Person $\\times$ Ticket, with the obvious interpretation. It also has a constant John that denotes the lottery organizer.\n\\end{example}\n\nIf $A$ is a relational structure, $S$ is a sort of $A$ and $X\\subseteq S$, we say that $X$ is \\emph{definable} in $A$ if there is a first order formula ${\\varphi}(x)$ with a single free variable $x$ of type $S$ such that $X$ is the set of elements $a$ of sort $S$\nsatisfying the proposition ${\\varphi}(a)$ in $A$, that is if\n\n", "index": 3, "text": "$$\n X = \\{a:\\ A\\models{\\varphi}(a)\\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"X=\\{a:\\ A\\models{\\varphi}(a)\\}.\" display=\"block\"><mrow><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>a</mi><mo rspace=\"7.5pt\">:</mo><mrow><mi>A</mi><mo>\u22a7</mo><mrow><mi>\u03c6</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\nin the structure of Example~\\ref{ex:2}.\n\\end{example}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} How do you measure the length of a formula?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} View the names of relations, variables and constants as single symbols, and count the number of symbols in the formula. Recall that the vocabulary specifies the type of every variable and every constant.\n\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} As far as the lottery case is concerned, the structure of Example~\\ref{ex:2} is poor. For example, it does not distinguish between people that own the same amount of tickets. In particular, it does not distinguish between Donna and anybody who owns exactly three tickets. You can extend it with a constant for Donna. If you want that the structure reflects a broader suspicion that John may cheat, you can add a constant for every person such that there is a reasonable suspicion that John will make him a winner. Much depends of course on what is known about John. For example, you can add a constant for every close relative and every close friend of John.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Alteratively we may introduce binary relations CloseRelative$(p,q)$ and CloseFriend$(p,q)$.\n\n\\begin{example}\\label{ex:4}\nExtend the structure of Example~\\ref{ex:2} with binary relations\nCloseRelative$(p,q)$ and CloseFriend$(p,q)$ of type\nPerson $\\times$ Person, with the obvious interpretations: $q$ is a close relative of $p$, and $q$\nis a close friend of $p$ respectively; in either case $q$ is distinct from $p$.\n\\end{example}\n\n\\subsection{Alternatives}\n\\label{sub:alt}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} There is something simplistic about Example~\\ref{ex:4}. Both relations seem to play equal roles. In reality, one of them may be more important. For example, John may be more willing to make a close relative, rather than a close friend, to win. People often put different weights on different relations. For a recent example see \\cite{Richardson-Domingos:2006}. You should do the same.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} You are right of course but, for the time being, we keep things simple.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} And why do you use relational first-order logic? There are many logics in the literature.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} In this first paper on the issue, it is beneficial for us to use relational first-order logic as our specification logic. It the best known and most popular logic, and it works reasonably well.  As (and if) the subject develops, it may be discovered that the best specification logic for one application domain may not be the best for another. At this point, our experience is very limited.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} First-order logic isn't the best logic for all purposes.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} It is not. And there are two distinct issues here. One issue is expressivity. If, for example, you need recursion, first-order logic may be not for you. It lacks recursion. The other issue is succinctness. It is possible to increase the succinctness of relational first-order specifications without increasing the expressive power of the logic. For example, one may want to use function symbols. One very modest extension of relational first-order logic which is nevertheless useful in making specifications shorter is to introduce quantifiers $\\dot\\exists x{\\varphi}$ (note the dot over $\\exists$) saying that there exists $x$ different from all free variables of the formula ${\\varphi}$ under quantification. If $y_1,\\dots,y_k$ are the free variables of ${\\varphi}$  then $\\dot{\\exists}x{\\varphi}$ is equivalent to\n\n", "itemtype": "equation", "pos": 30246, "prevtext": "\n\nThe formula ${\\varphi}(x)$ is a \\emph{definition} of $X$ in $A$.\n\nThe description complexity of $X$ in $A$ is the length of the shortest\ndefinition of $X$ in $A$.\n\n\\begin{example}\\label{ex:3}\nThe event ``the winner owns just one ticket'' consists of the outcomes ``p wins the lottery'' where $p$ ranges over the people owning exactly one\nticket. The event is thus definable by formula\n\n", "index": 5, "text": "$$\n \\exists t_1({\\mathrm{Owns}}(p,t_1) \\land (\\forall t_2 ({\\mathrm{Owns}}(p,t_2) \\to t_1 = t_2))).\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\exists t_{1}({\\mathrm{Owns}}(p,t_{1})\\land(\\forall t_{2}({\\mathrm{Owns}}(p,t_%&#10;{2})\\to t_{1}=t_{2}))).\" display=\"block\"><mrow><mo>\u2203</mo><msub><mi>t</mi><mn>1</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>Owns</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2227</mo><mrow><mo stretchy=\"false\">(</mo><mo>\u2200</mo><msub><mi>t</mi><mn>2</mn></msub><mrow><mo stretchy=\"false\">(</mo><mi>Owns</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2192</mo><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\nbut it is shorter. It would be natural of course to introduce the $\\dot{\\exists}$ quantifier together with its dual $\\dot{\\forall}$ quantifier.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Instead of logic, one can use computation models, especially restricted computation models, e.g. finite state automata, for specification.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Yes, you are right. Note though that, for every common computation model, there is a logic with equivalent expressivity. For example, in the case of finite automata over strings, it is existential second-order logic \\cite{Buchi:1960}.\n\n\n\\section{Impugning Randomness}\n\\label{sec:impugn}\n\nNow we are ready to explain our method of impugning the null hypothesis in executed probabilistic scenarios with suspicious outcomes. Given\n\\begin{itemize}\n\\item a trial such that some of its outcomes arouse suspicion  and\n\\item a null hypothesis about the probability distribution that governs the trial,\n\\end{itemize}\none has several tasks to do.\n\n\\medskip\\noindent\n{\\bf 1: Background Information}\\quad\nAnalyze the probabilistic trial and establish what background information is relevant.\n\n\\medskip\\noindent\n{\\bf 2: Logic Model}\\quad\nModel the trial and relevant background information as a logic structure.\n\n\\medskip\\noindent\n{\\bf 3: Focal Event}\nPropose a focal event that is\n\\begin{itemize}\n\\item negligible under the null hypothesis and\n\\item has a short description in the logic model.\n\\end{itemize}\n\n\\medskip\\noindent\nBy the bridge principle, the focal event is not supposed to happen, under the null hypothesis, during the execution of the trial. If the focal event contains the actual outcome of the trial, then the focal event has happened. This gives us a reason to reject the null hypothesis.\n\n\\subsubsection*{What background information is relevant?}\n\nRelevant background information reflects various ways that suspicious outcomes occur. In this\nconnection historical data is important. In the lottery case, for example, it is relevant that\nsome lottery organizers have been known to cheat.\n\n\n\\subsubsection*{What does the Model Builder Know about the Actual Outcome?}\n\nThe less the model builder knows about the actual outcome the better. Ideally the model builder has no information about the actual outcome, so that we can use the narrow bridge principle. We may not have a model builder with no information about the actual outcome; it may even happen that the actual outcome has been advertised so widely that everybody knows it. In the absence of blissfully unaware model builder, we should try to put ourselves into his/her shoes.\n\n\\subsubsection*{The Desired Logic Model}\n\nOne may be lucky to find an existing logic model that has been successfully used in similar\nscenarios. If not, construct the most natural and frugal model you can.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} ``Natural'' is a positive word. Surely it is beneficial that the desired model is natural.\nBut why should the model be frugal?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} If the model is too rich (like in the case of classical algorithmic information theory), too\nmany events have short specifications. Imagine for example that, in the lottery case, the model\nallows you to specify shortly various people that have nothing to do with the lottery organizer.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But how do you know that those people have nothing to do with the lottery organizer? Maybe one\nof them is a secret lover of the lottery organizer.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Indeed, the background information deemed relevant may be deficient. But, at the model\nbuilding stage, we want to reflect only the background information deemed relevant.\n\n\\subsubsection*{The Desired Focal Event}\n\nThe desired focal event contains the suspicious outcomes of the trial.\n\n\n\\section{Examples}\n\\label{sec:examples}\n\nWe return to the four cases of \\S\\ref{sec:intro}.\n\n\\subsection{The Case of Lottery}\n\\label{sub:lottery2}\n\n\\paragraph{Trial} For an informal description of the trial see \\S\\ref{sub:lottery1}. Recall that\nJohn is the lottery organizer, and Donna is his wife. As mentioned in Example~\\ref{ex:1}, we view\nthe lottery as a trial with potential outcomes of the form ``o wins the lottery'' where o ranges\nover the lottery ticket owners.\n\n\\paragraph{Null Hypothesis}\n\nThere is only one innate probability distribution ${\\mathtt{P}}$, and the probability ${\\mathtt{P}}(x)$ of any person\n$x$ to win is proportional to the number of lottery tickets that $x$ owns.\n\n\\paragraph{Background Information} We assume that the following is known about John, the lottery organizer. He is a family man, with a few close friends that he has known for a long time. He bought no lottery tickets.\n\n\\paragraph{Actual Outcome} ``Donna wins the lottery''.\n\n\\paragraph{Logic Model} Our model is a simplification of the structure of Example~\\ref{ex:4}. We don't need the sort Ticket introduced originally in Example~\\ref{ex:2} to illustrate the notion of multi-sorted model. And we do not need the full extent of relations CloseRelative and CloseFriend, only the sections of them related to John. Our model is one-sorted. The one sort, called Person, consists of people, namely all lottery ticket owners as well as John, the lottery organizer. The structure has one constant and two unary relations. The constant is John; it denotes the lottery organizer. The two unary relations are ${\\mathrm{CloseRelativeOfJohn}}(p)$ and ${\\mathrm{CloseFriendOfJohn}}(p)$, both of type Person. The interpretations of the two relations are obvious: $p$ is a close relative of John, and $p$ is a close friend of respectively; in both cases $p$ is distinct from John.\n\n\\paragraph{Focal Event:} The winner is a close relative or friend of John, the lottery organizer, in other words, the winner belongs to the set\n\n", "itemtype": "equation", "pos": 33956, "prevtext": "\nin the structure of Example~\\ref{ex:2}.\n\\end{example}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} How do you measure the length of a formula?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} View the names of relations, variables and constants as single symbols, and count the number of symbols in the formula. Recall that the vocabulary specifies the type of every variable and every constant.\n\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} As far as the lottery case is concerned, the structure of Example~\\ref{ex:2} is poor. For example, it does not distinguish between people that own the same amount of tickets. In particular, it does not distinguish between Donna and anybody who owns exactly three tickets. You can extend it with a constant for Donna. If you want that the structure reflects a broader suspicion that John may cheat, you can add a constant for every person such that there is a reasonable suspicion that John will make him a winner. Much depends of course on what is known about John. For example, you can add a constant for every close relative and every close friend of John.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Alteratively we may introduce binary relations CloseRelative$(p,q)$ and CloseFriend$(p,q)$.\n\n\\begin{example}\\label{ex:4}\nExtend the structure of Example~\\ref{ex:2} with binary relations\nCloseRelative$(p,q)$ and CloseFriend$(p,q)$ of type\nPerson $\\times$ Person, with the obvious interpretations: $q$ is a close relative of $p$, and $q$\nis a close friend of $p$ respectively; in either case $q$ is distinct from $p$.\n\\end{example}\n\n\\subsection{Alternatives}\n\\label{sub:alt}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} There is something simplistic about Example~\\ref{ex:4}. Both relations seem to play equal roles. In reality, one of them may be more important. For example, John may be more willing to make a close relative, rather than a close friend, to win. People often put different weights on different relations. For a recent example see \\cite{Richardson-Domingos:2006}. You should do the same.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} You are right of course but, for the time being, we keep things simple.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} And why do you use relational first-order logic? There are many logics in the literature.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} In this first paper on the issue, it is beneficial for us to use relational first-order logic as our specification logic. It the best known and most popular logic, and it works reasonably well.  As (and if) the subject develops, it may be discovered that the best specification logic for one application domain may not be the best for another. At this point, our experience is very limited.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} First-order logic isn't the best logic for all purposes.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} It is not. And there are two distinct issues here. One issue is expressivity. If, for example, you need recursion, first-order logic may be not for you. It lacks recursion. The other issue is succinctness. It is possible to increase the succinctness of relational first-order specifications without increasing the expressive power of the logic. For example, one may want to use function symbols. One very modest extension of relational first-order logic which is nevertheless useful in making specifications shorter is to introduce quantifiers $\\dot\\exists x{\\varphi}$ (note the dot over $\\exists$) saying that there exists $x$ different from all free variables of the formula ${\\varphi}$ under quantification. If $y_1,\\dots,y_k$ are the free variables of ${\\varphi}$  then $\\dot{\\exists}x{\\varphi}$ is equivalent to\n\n", "index": 7, "text": "$$\n \\exists x (x\\neq y_1 \\land \\cdots \\land x\\neq y_k \\land {\\varphi})\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex3.m1\" class=\"ltx_Math\" alttext=\"\\exists x(x\\neq y_{1}\\land\\cdots\\land x\\neq y_{k}\\land{\\varphi})\" display=\"block\"><mrow><mo>\u2203</mo><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>\u2260</mo><msub><mi>y</mi><mn>1</mn></msub><mo>\u2227</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2227</mo><mi>x</mi><mo>\u2260</mo><msub><mi>y</mi><mi>k</mi></msub><mo>\u2227</mo><mi>\u03c6</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\n\n\n\\subsection{The Case of Jury Selection}\n\\label{sub:jury2}\n\n\\paragraph{Trial}\n\nFor an informal description of the trial see \\S\\ref{sub:jury1}. The trial in question selects a\npool of 50 prospective jurors from about a 1,000,000 people available for the purpose.\n\n\\paragraph{Null Hypothesis}\n\nThere is only one innate probability distribution, and the one innate probability distribution is\nuniform so that all possible pools of 50 prospective jurors are equally probable.\n\n\\paragraph{Background Information}\n\nThere is a unique Facebook group of about 100 people that discusses the criminal case.\n\n\\paragraph{Actual Outcome} A pool with seven prospective jurors from the Facebook group.\n\n\\paragraph{Logic Model}\n\nThe model has two sorts and one relation.\n\\begin{itemize}\n\\item Sort Pool consists of all possible pools of 50 prospective jurors.\n\\item Sort Member consists of the members of the Facebook group that discussed Thomas's\ncase.\n\\item The relation ${\\mathrm{In}}(m,p)$ of type Member $\\times$ Pool holds if and only if member $m$ of the Facebook group belongs to\npool $p$.\n\\end{itemize}\n\n\\paragraph{Focal Event}\n\n\n", "itemtype": "equation", "pos": 39831, "prevtext": "\nbut it is shorter. It would be natural of course to introduce the $\\dot{\\exists}$ quantifier together with its dual $\\dot{\\forall}$ quantifier.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Instead of logic, one can use computation models, especially restricted computation models, e.g. finite state automata, for specification.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Yes, you are right. Note though that, for every common computation model, there is a logic with equivalent expressivity. For example, in the case of finite automata over strings, it is existential second-order logic \\cite{Buchi:1960}.\n\n\n\\section{Impugning Randomness}\n\\label{sec:impugn}\n\nNow we are ready to explain our method of impugning the null hypothesis in executed probabilistic scenarios with suspicious outcomes. Given\n\\begin{itemize}\n\\item a trial such that some of its outcomes arouse suspicion  and\n\\item a null hypothesis about the probability distribution that governs the trial,\n\\end{itemize}\none has several tasks to do.\n\n\\medskip\\noindent\n{\\bf 1: Background Information}\\quad\nAnalyze the probabilistic trial and establish what background information is relevant.\n\n\\medskip\\noindent\n{\\bf 2: Logic Model}\\quad\nModel the trial and relevant background information as a logic structure.\n\n\\medskip\\noindent\n{\\bf 3: Focal Event}\nPropose a focal event that is\n\\begin{itemize}\n\\item negligible under the null hypothesis and\n\\item has a short description in the logic model.\n\\end{itemize}\n\n\\medskip\\noindent\nBy the bridge principle, the focal event is not supposed to happen, under the null hypothesis, during the execution of the trial. If the focal event contains the actual outcome of the trial, then the focal event has happened. This gives us a reason to reject the null hypothesis.\n\n\\subsubsection*{What background information is relevant?}\n\nRelevant background information reflects various ways that suspicious outcomes occur. In this\nconnection historical data is important. In the lottery case, for example, it is relevant that\nsome lottery organizers have been known to cheat.\n\n\n\\subsubsection*{What does the Model Builder Know about the Actual Outcome?}\n\nThe less the model builder knows about the actual outcome the better. Ideally the model builder has no information about the actual outcome, so that we can use the narrow bridge principle. We may not have a model builder with no information about the actual outcome; it may even happen that the actual outcome has been advertised so widely that everybody knows it. In the absence of blissfully unaware model builder, we should try to put ourselves into his/her shoes.\n\n\\subsubsection*{The Desired Logic Model}\n\nOne may be lucky to find an existing logic model that has been successfully used in similar\nscenarios. If not, construct the most natural and frugal model you can.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} ``Natural'' is a positive word. Surely it is beneficial that the desired model is natural.\nBut why should the model be frugal?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} If the model is too rich (like in the case of classical algorithmic information theory), too\nmany events have short specifications. Imagine for example that, in the lottery case, the model\nallows you to specify shortly various people that have nothing to do with the lottery organizer.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} But how do you know that those people have nothing to do with the lottery organizer? Maybe one\nof them is a secret lover of the lottery organizer.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Indeed, the background information deemed relevant may be deficient. But, at the model\nbuilding stage, we want to reflect only the background information deemed relevant.\n\n\\subsubsection*{The Desired Focal Event}\n\nThe desired focal event contains the suspicious outcomes of the trial.\n\n\n\\section{Examples}\n\\label{sec:examples}\n\nWe return to the four cases of \\S\\ref{sec:intro}.\n\n\\subsection{The Case of Lottery}\n\\label{sub:lottery2}\n\n\\paragraph{Trial} For an informal description of the trial see \\S\\ref{sub:lottery1}. Recall that\nJohn is the lottery organizer, and Donna is his wife. As mentioned in Example~\\ref{ex:1}, we view\nthe lottery as a trial with potential outcomes of the form ``o wins the lottery'' where o ranges\nover the lottery ticket owners.\n\n\\paragraph{Null Hypothesis}\n\nThere is only one innate probability distribution ${\\mathtt{P}}$, and the probability ${\\mathtt{P}}(x)$ of any person\n$x$ to win is proportional to the number of lottery tickets that $x$ owns.\n\n\\paragraph{Background Information} We assume that the following is known about John, the lottery organizer. He is a family man, with a few close friends that he has known for a long time. He bought no lottery tickets.\n\n\\paragraph{Actual Outcome} ``Donna wins the lottery''.\n\n\\paragraph{Logic Model} Our model is a simplification of the structure of Example~\\ref{ex:4}. We don't need the sort Ticket introduced originally in Example~\\ref{ex:2} to illustrate the notion of multi-sorted model. And we do not need the full extent of relations CloseRelative and CloseFriend, only the sections of them related to John. Our model is one-sorted. The one sort, called Person, consists of people, namely all lottery ticket owners as well as John, the lottery organizer. The structure has one constant and two unary relations. The constant is John; it denotes the lottery organizer. The two unary relations are ${\\mathrm{CloseRelativeOfJohn}}(p)$ and ${\\mathrm{CloseFriendOfJohn}}(p)$, both of type Person. The interpretations of the two relations are obvious: $p$ is a close relative of John, and $p$ is a close friend of respectively; in both cases $p$ is distinct from John.\n\n\\paragraph{Focal Event:} The winner is a close relative or friend of John, the lottery organizer, in other words, the winner belongs to the set\n\n", "index": 9, "text": "$$\n \\{x:\\ {\\mathrm{CloseRelativeOfJohn}}(x) \\lor {\\mathrm{CloseFriendOfJohn}}(x) \\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\{x:\\ {\\mathrm{CloseRelativeOfJohn}}(x)\\lor{\\mathrm{CloseFriendOfJohn}}(x)\\}.\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">{</mo><mi>x</mi><mo rspace=\"7.5pt\">:</mo><mrow><mrow><mi>CloseRelativeOfJohn</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2228</mo><mrow><mi>CloseFriendOfJohn</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} If the null hypothesis is impugned then some rules have been violated. Who is the guilty\nparty? In the lottery case, it was clear more or less that John was the guilty party. In this\ncase, it is not obvious who the guilty party is.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} We do not pretend to know the guilty party. Our only goal is to impugn randomness. There may\nbe more than one guilty party as far as we know. Their actions may or may not have been\ncoordinated.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} As far as I see, plenty of randomness might have remained. You did not impugn all randomness.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} You are right. Let us express our goal more precisely: it is to impugn the null hypothesis, no\nmore no less.\n\n\n\n\\subsection{A Stalking Case}\n\\label{sub:stalking2}\n\n\\paragraph{Trial}\nAn informal description of the case is given in \\S\\ref{sub:stalking1}. The 10 nights of the trial\nmay be represented by numbers $1,\\dots,10$.\nThe outcomes of the trial may be represented by functions $f$ from $\\{1, \\ldots, 10\\}$ to\n$\\{0,1\\}$ where the meaning of $f(n)=1$ (resp. $f(n)=0$) is that Alice and Bob met (resp. did not\nmet) Chris at the restaurant on night $n$.\n\n\\paragraph{Null Hypothesis}\n\nIntentionally, the null hypothesis says that Chris does not stalk Alice. Formally, the null\nhypothesis says that a probability distribution ${\\mathtt{P}}$ on the outcomes is innate if and only if it\nsatisfies the following two requirements for every outcome $f$.\n\\begin{enumerate}\n\\item Events $f(n_1)=1$ and $f(n_2)=1$ are independent for any nights $n_1\\neq n_2$.\n\\item ${\\mathtt{P}}(f(n)=1) \\leq 1/100$ for every night $n$.\n\\end{enumerate}\n\\noindent\nRequirement 2 says that ${\\mathtt{P}}(f(n)=1)$ is less than (rather than equal to) $1/100$ rather than\n${\\mathtt{P}}(f(n)=1) = 1/100$ because, at night $n$, Chris may not to be present at all in any of the 100\nrestaurants at the time when Alice and Bob dine. If he is in one of the 100 restaurants when\nAlice and Bob dine then ${\\mathtt{P}}(f(n)=1) = 1/100$.\n\n\\paragraph{Background Information} Chris is suspected of stalking Alice in restaurants.\n\n\\paragraph{Actual Outcome} Five times out of ten times Alice and Bob meet Chris at the chosen\nrestaurant.\n\n\\paragraph{Logic Model}\nThere are three sorts of elements and one relation.\n\\begin{itemize}\n \\item Sort Night consists of numbers $1,\\dots,10$.\n \\item Sort Outcome consists of all possible functions from Night to $\\{0,1\\}$.\n \\item Relation $R(f,n)$ holds if and only if, on night $n$, Alice and Bob meet Chris at the\nchosen restaurant.\n\\end{itemize}\n\n\\noindent\n{\\bf Focal Event}\\quad\n $\\{f:\\ \\exists n_1 \\exists n_2\n \\big( n_1\\neq n_2 \\land R(f,n_1) \\land R(f,n_2) \\big) \\}$.\n\\medskip\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} The focal event is that there are two distinct nights when Chris dines at the restaurant where\nAlice and Bob dine. What if the current probability threshold is lower than you presume, and the\nfocal event turns out to be non-negligible?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} In this particular case, it is natural to consider the focal event that there are three\ndistinct nights when Chris dines at the restaurant where Alice and Bob dine.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} It is rather expensive to say that there are $k$ distinct elements; the description complexity is $O(k^2)$. Now I see why you mentioned those dotted existential quantifiers $\\dot{\\exists}$ in \\S\\ref{sub:alt}.\n\n\n\n\\subsection{The case of Nicholas Caputo}\n\\label{sub:caputo2}\n\n\\paragraph{Trial} An informal description of the case is given in \\S\\ref{sub:caputo1}. The $41$ elections may be represented by numbers $1,\\dots,41$. The possible outcomes of the trial can be seen as functions $f$ from $\\{1,\\dots,41\\}$ to \\{D,N\\} where the letters $D$, $N$ indicate whether the top ballot line went to a Democrat or not.\n\n\\paragraph{Null Hypothesis}\nIntentionally the null hypothesis is that the drawings were fair.\nFormally, the null hypothesis says that there is a unique innate probability distribution ${\\mathtt{P}}$ on\nthe outcomes, and that ${\\mathtt{P}}$ satisfies the following two requirements for every outcome $f$.\n\\begin{enumerate}\n\\item Events $f(e_1)=D$ and $f(e_2)=D$ are independent for any elections $e_1\\neq e_2$.\n\\item For every election $e$, ${\\mathtt{P}}(f(e)=D) = r_e$ where $r_e$ is the fraction of Democrats on\nthe ballot.\n\\end{enumerate}\n\\noindent\nWe assume that every $r_e\\geq 0.4$.\n\n\\paragraph{Background Information} The county clerk, who conducted the drawings, was a Democrat.\n\n\\paragraph{Actual Outcome} $40$ times out of $41$ times the top ballot line went to Democrats.\n\n\\paragraph{Logic Model} The model has three sorts, two constants and one relation.\n\\begin{itemize}\n\\item Sort Election consists of numbers $1,\\ldots,41$ representing the $41$ elections.\n\\item Sort Party consists of two elements. The elements of Party will be called parties.\n\\item Sort Outcome consists of all $f$ from Election to Party.\n\\item The constants $D$ and $N$ of type Party denote distinct elements of type Party.\n\\item The relation $R(f,e,p)$ of type Outcome $\\times$ Election $\\times$ Party holds if and only if, according to outcome $f$, the top ballot line went to party $p$ at elections $e$.\n\\end{itemize}\n\n\\paragraph{Focal Event:}\n\n", "itemtype": "equation", "pos": 41041, "prevtext": "\n\n\n\\subsection{The Case of Jury Selection}\n\\label{sub:jury2}\n\n\\paragraph{Trial}\n\nFor an informal description of the trial see \\S\\ref{sub:jury1}. The trial in question selects a\npool of 50 prospective jurors from about a 1,000,000 people available for the purpose.\n\n\\paragraph{Null Hypothesis}\n\nThere is only one innate probability distribution, and the one innate probability distribution is\nuniform so that all possible pools of 50 prospective jurors are equally probable.\n\n\\paragraph{Background Information}\n\nThere is a unique Facebook group of about 100 people that discusses the criminal case.\n\n\\paragraph{Actual Outcome} A pool with seven prospective jurors from the Facebook group.\n\n\\paragraph{Logic Model}\n\nThe model has two sorts and one relation.\n\\begin{itemize}\n\\item Sort Pool consists of all possible pools of 50 prospective jurors.\n\\item Sort Member consists of the members of the Facebook group that discussed Thomas's\ncase.\n\\item The relation ${\\mathrm{In}}(m,p)$ of type Member $\\times$ Pool holds if and only if member $m$ of the Facebook group belongs to\npool $p$.\n\\end{itemize}\n\n\\paragraph{Focal Event}\n\n\n", "index": 11, "text": "$$\n \\{p:\\ \\exists m_1 \\exists m_2\n \\big(m_1\\neq m_2\\ \\land\\ {\\mathrm{In}}(m_1,p)\\ \\land\\ {\\mathrm{In}}(m_2,p) \\big)\\}\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\{p:\\ \\exists m_{1}\\exists m_{2}\\big{(}m_{1}\\neq m_{2}\\ \\land\\ {\\mathrm{In}}(m%&#10;_{1},p)\\ \\land\\ {\\mathrm{In}}(m_{2},p)\\big{)}\\}\" display=\"block\"><mrow><mo stretchy=\"false\">{</mo><mi>p</mi><mo rspace=\"7.5pt\">:</mo><mo>\u2203</mo><msub><mi>m</mi><mn>1</mn></msub><mo>\u2203</mo><msub><mi>m</mi><mn>2</mn></msub><mrow><mo maxsize=\"120%\" minsize=\"120%\">(</mo><msub><mi>m</mi><mn>1</mn></msub><mo>\u2260</mo><mpadded width=\"+5pt\"><msub><mi>m</mi><mn>2</mn></msub></mpadded><mo rspace=\"7.5pt\">\u2227</mo><mi>In</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mn>1</mn></msub><mo>,</mo><mi>p</mi><mo rspace=\"7.5pt\" stretchy=\"false\">)</mo></mrow><mo rspace=\"7.5pt\">\u2227</mo><mi>In</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mn>2</mn></msub><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><mo maxsize=\"120%\" minsize=\"120%\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></math>", "type": "latex"}, {"file": "1601.00665.tex", "nexttext": "\n\n\n\\section{Related Work}\n\\label{sec:related}\n\nOur paper touches upon diverse areas of science. We restrict attention to a few key issues: Cournot's principle, algebraic information complexity, and social network analysis.\n\n\\subsection{Cournot's Principle}\n\\label{sub:bridge2}\n\nThe idea that specified events of small probability do not happen seems to be fundamental to our human experience. And it has been much discussed, applied and misapplied. We don't --- and couldn't --- survey here the ocean of related literature. In \\S\\ref{sec:bridge} we gave already quite a number of references in support of Cournot's principle. On the topic of misapplication of Cournot's principle, let us now turn to the work of William Dembski. Dembski is an intelligent design theorist who has written at least two books, that are influential in creationist circles, on applications of ``The Law of Small Probability'' to proving intelligent design \\cite{Dembski:1998,Dembski:2006}.\n\nWe single out Dembski because it is the only approach that we know which is, at least on the surface, similar to ours. Both approaches generalize Cournot's principle and speak of independent specifications. And both approaches use the information complexity of an event as a basis to argue that it was implicitly specified. We discovered Dembski's books rather late, when this paper was in an advanced stage, and our first impression, mostly from the introductory part of book \\cite{Dembski:1998}, was that he ate our lunch so to speak. But then we realized how different the two approaches really were. And then we found good mathematical examinations of the fundamental flaws of Dembski's work: \\cite{Wein:2002} and \\cite{Bradley:2009}.\n\nOur approach is much more narrow. In each of our scenarios, there is a particular trial $T$ with well defined set $\\Omega_T$ of possible outcomes, a fixed family ${\\mathcal{F}}$ of probability distributions --- the innate probability distributions --- on $\\Omega_T$, and a particular event --- the focal event --- of sufficiently  small probability with respect to every innate probability distribution. The null conjecture is that the trial is governed by one of the innate probability distributions. Here events are subsets of $\\Omega_T$, the trial is supposed to be executed only once, and the focal event is supposed to be specified independently from the actual outcome. By impugning randomness we mean impugning the null hypothesis.\n\nDembski's introductory examples look similar. In fact we borrowed one of his examples, about ``the man with a golden arm''. But Dembski applies his theory to vastly broader scenarios where an event may be e.g. the emergence of life. And he wants to impugn any chance whatsoever. That seems hopeless to us.\n\nConsider the emergence of life case for example. What would the probabilistic trial be in that case? If one takes the creationist point of view then there is no probabilistic trial. Let's take the mainstream scientific point of view, the one that Dembski intends to impugn. It is not clear at all what the trial is, when it starts and when it is finished, what the possible outcomes are, and what probability distributions need to be rejected.\n\nThe most liberal part of our approach is the definition of independent specification. But even in\nthat aspect, our approach is super narrow comparative to Dembski's.\n\nThere are other issues with Dembski's work; see \\cite{Wein:2002,\n  Bradley:2009}.\n\n\\subsection{Algorithmic Information Theory}\n\\label{sub:ait2}\n\nThe idea of basing the intrinsic randomness of an event upon its description in a fixed language is fundamental to algorithmic information theory (in short AIT) \\cite{Chaitin:1987,Li-Vitanyi:1997} originated by Ray Solomonoff \\cite{Solomonoff:1964}, Andrei N. Kolmogorov \\cite{Kolmogorov:1965} and Gregory J. Chaitin \\cite{Chaitin:1966}.\n\nIn \\S\\ref{sub:ait1}, we sketched the basic ideas of the theory. In the classical AIT, the theoretical power is gained by basing the information complexity measure on universal Turing machines. This becomes an impediment to practical applications; the classical information complexity of (the string representation of) events is not computable. For practical applications, it is thus natural to look at restricted variants of AIT which ``impoverish'' the event description language even though the classical theorems of AIT may no longer hold.\n\nThe influential Lempel-Ziv compression theory of strings \\cite{Lempel-Ziv:1977,Lempel-Ziv:1978} can be viewed as such a restriction of AIT. However Lempel and Ziv developed their theory without any direct connection with AIT. One recent and even more restrictive theory \\cite{Calude+2:2011} was inspired by AIT: ``we develop a version of Algorithmic Information Theory (AIT)\nby replacing Turing machines with finite transducers''.\n\nOne useful application of AIT to real-world phenomena has been through the Universal Similarity Metric and its uses in genetics and bioinformatics \\cite{Krasnogor:2004, Li:2001, Ferragina:2007, Gilbert:2007}, plagiarism detection \\cite{ChenLi:2004} and even analysis of music \\cite{Cilibrasi:2004}. In \\cite{Cilibrasi:2007}, the authors combine a restricted variant of Kolmogorov complexity with results obtained from Google searches to derive a metric for the similarity of the meaning of words and phrases.  In doing so, they are able to automatically distinguish between colors and numbers, perform rudimentary automatic English to Spanish translation, and even distinguish works of art by properties of the artists. In such lines of research, practitioners often replace the Kolmogorov complexity measure with measures based on string-compression algorithms  \\cite{Welch:1984,bzip2,ppm} more efficient than the original Lempel-Ziv algorithms.\n\nIn cognitive science, the simplicity theory of Chater, Vit\\'{a}nyi, Dessalles and Schmidhuber offers an explanation as to why human beings tend to find certain events ``interesting'' \\cite{Chater:2008}. The explanation correlates the interest of an event with its simplicity (i.e., the lowness of its Kolmogorov complexity) .\n\nOur logic-based definition of description complexity in \\S\\ref{sub:dc} fits this mold of restricted algorithmic information theories. We note, however, that the logic approach is rather general and can handle the classical information complexity and its restricted versions and even its more powerful (e.g. hyper-arithmetical) versions.\n\n\\subsection{Social Network Analysis}\n\\label{sub:social}\n\nThe idea of modeling real-world scenarios using relational\nstructures dates back at least to the 1950s\n\\cite{Merton:1957}.  The primary scientific\ndevelopers of this idea were for many years sociologists and social\nanthropologists working in the field of social network analysis.\nAs a field of mathematical sociology, social network analysis has put forth a network-theory\noriented view of social relationships and used it to quantitatively analyze social phenomena.\n\nEven in the days before massive social network data was available, social network analysts obtained fascinating results. For example, in a 1973 paper ``The Strength of Weak Ties'', Mark Granovetter put forth the idea that most jobs in the United States are found through ``weak ties'', that is acquaintances the job seeker knows only slightly. Granovetter obtained his relational data by interviewing only dozens of people, yet his conclusions held up experimentally and are widely influential today in sociology.\nWith the advent of large-scale digitized repositories of relational social network data such as Facebook (according to a recent estimate, more than 40\\% of the US\npopulation have Facebook accounts \\cite{Wells:2010}), the applicability of social network analysis techniques grew tremendously.\nThe relational algebra of social network analysis tends to be simple.  Typically, analysis is done with rudimentary graph theory: members of a population (called actors) are nodes in a graph and the relationships of interest between actors are modeled as edges. Multiple binary relations are combined into composite relations so that core social network analysis calculations are done over a single graph's adjacency matrix\n\\cite{Wasserman:1994}.\n\nIn the case that our models are graphs, there is much machinery of social network analysis which could be of use to us.  For instance, social network analysts have developed robust and scalable methods for determining the central nodes of interest of social networks, based upon things like weighted connectivity.  We can imagine this being useful for impugning randomness. For instance, if one does not know which members of the population should be distinguished and named by constant symbols, the very structure of a social network may force certain nodes upfront.  There are many other techniques from social network analysis (and available high-performance software) which have the potential to be useful for our goals.\n\n\\section{Final Discussion}\n\\label{sec:final}\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I have been thinking about algorithmic information theory and its applications, and I also did some reading, e.g. \\cite{Vitanyi+2:1997,Shen:2009}. In general your logic approach appeals to me but I have some reservations.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Let's start with the positive part. What do you like about the logic approach?\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} The situation at hand is described directly and rather naturally. I also like that some outcomes and events are not definable at all. Consider for example the lottery-related model in \\S\\ref{sub:lottery2}. Unless John, the lottery organizer, has a single close relative or a single close friend, no particular outcome is definable in the model. And the model does not distinguish at all between any two persons outside the circle that contains John, his close relatives and his close friends. This simplicity may be naive but it is certainly appealing.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Indistinguishability is important. It is rather surprising in a sense that, in the application of probability theory, so often one is able to compute or at least approximate probabilities. Think about it. The number of possible outcomes of a trial may be rather large and may even be infinite. And these are just outcomes; most events contain multiple outcomes. A probabilistic measure on the event space is a function from the events to real numbers between 0 and 1 that satisfies some slight conditions.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Most probability measures are useless I guess. Which of them are useful?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Those few that allow us feasible --- though possibly approximate --- computations of the probabilities of interesting events. Typically useful measures heavily exploit the symmetries inherent in the trial and the independence of various parts of the trial.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} I think I see the connection to indistinguishability. But let me go to my reservations. It is basically about the annoying freedom in fixing the probability threshold, in choosing the appropriate logic, in figuring out what background information is relevant and what should the focal event be, in constructing the logical model, and in deciding whether a proposed logical specification of the focal event is short enough.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} The ``annoying freedom'' is inherent in the impugning-randomness problem.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Kolmogorov complexity is objective, due to the Invariance Theorem mentioned in \\S\\ref{sub:ait1}.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} It is objective only to a point.  Recall that the Invariance Theorem involves an unspecified additive constant. So Kolmogorov complexity also suffers from the nagging question ``is it short enough''. Besides, one may be interested in the length of a shortest specification of a given string in first-order arithmetic or Zermelo-Fraenkel set theory for example. The resulting specification complexity measures are rather objective. They are undecidable of course, but so is Kolmogorov complexity.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} So how do you intend to deal with the annoying freedom?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} We believe that the annoying-freedom problem cannot be solved by theorists. It can be solved, better and better, by experimentation, trial and error, accumulation of historical records, standardization, etc.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} Allow me one other question before we finish. You mentioned in \\S\\ref{sub:fisher} that, in Fisher's approach, the focal event is specified by means of the $p$-value of some statistics. ``In statistical significance testing'', says Wikipedia \\cite{p-value}, ``the $p$-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true''. Note the closure under the at-least-as-extreme values. If a focal event is not specified by means of a $p$-value, is there any kind of closure that the focal event should satisfy?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} Yes, in our examples, the focal event contained not only the actually observed outcome but also other suspicious outcomes. In fact, the focal-event approach is rather flexible. Consider the lottery scenario for example. The actual outcome --- that Donna won the lottery --- may be judged to be the most suspicious and, from that point of view, the most extreme, so that there are no other outcomes at least as extreme. But the focal event contains additional suspicious outcomes.\n\n\n\\begin{thebibliography}{99}\n\n\\bibitem{Batanero:2000}\nCarmen Batanero.\n\\newblock Controversies around the role of statistical tests in\n  experimental research.\n\\newblock {\\em Mathematical Thinking and Learning}, 2((1-2)): 75--98,\n  2000.\n\n\\bibitem{Bernoulli:1713}\nJakob Bernoulli.\n\\newblock {\\em Ars Conjectandi}.\n\\newblock Thurneysen Brothers, Basel, 1713.\n\n\\bibitem{Borel:1943}\n\\'{E}mile Borel.\n\\newblock {\\em Les Probabilit\\'{e}s et la Vie}.\n\\newblock Presses Universitaires de France, Paris, 1943.\n\\newblock English: {\\em Probabilities and Life}, Dover, 1962.\n\n\\bibitem{Bradley:2009}\nJames Bradley.\n\\newblock {Why Dembski's Design Inference Doesn't Work}.\n\\newblock The Biologos Foundation, 2009.\n\n\\bibitem{Buchi:1960}\nJ.~Richard B\\\"{u}chi.\n\\newblock {W}eak second-order arithmetic and finite automata.\n\\newblock {\\em Zeitschrift f\\\"{u}r Mathematische Logik und Grundlagen\n  der Mathematik}, 6: 66--92, 1960.\n\n\\bibitem{bzip2}\nbzip2.\n\\newblock \\url{http://bzip.org/}\n\n\\bibitem{Calude+2:2011}\nCristian S. Calude, Kai Salomaa and Tania K. Roblot.\n\\newblock Finite State Complexity.\n\\newblock {\\em Theoretical Computer Science}, to appear.\n\n\\bibitem{Chaitin:1966}\nGregory~J. Chaitin.\n\\newblock On the length of programs for computing finite binary sequences.\n\\newblock {\\em Journal of ACM}, 13, 547--569, 1966.\n\n\\bibitem{Chaitin:1987}\nGregory~J. Chaitin.\n\\newblock {\\em Algorithmic Information Theory}.\n\\newblock Cambridge University Press, New York, NY, USA, 1987.\n\n\\bibitem{Chater:2008}\nNick Chater.\n\\newblock {The search for simplicity: a fundamental cognitive principle?}\n\\newblock {\\em The Quarterly Journal of Experimental Psychology Section A},\n  52(2): 273--302, 1999.\n\n\\bibitem{ChenLi:2004}\nXin Chen, B.~Francia, Ming Li, B.~McKinnon, and A.~Seker.\n\\newblock Shared information and program plagiarism detection.\n\\newblock {\\em IEEE Transactions on Information Theory},\n50(7): 1545 -- 1551, 2004.\n\n\\bibitem{Cilibrasi:2007}\nRudi Cilibrasi and Paul Vit\\'{a}nyi.\n\\newblock The Google Similarity Distance.\n\\newblock {\\em IEEE Transactions on Knowledge and Data Engineering},\n19(3): 370--383, March, 2007.\n\n\\bibitem{Cilibrasi:2004}\nRudi Cilibrasi, Paul Vit\\'{a}nyi, and Ronald De~Wolf.\n\\newblock Algorithmic clustering of music based on string compression.\n\\newblock {\\em Comput. Music J.}, 28: 49--67, December 2004.\n\n\\bibitem{Cohen:1994}\nJacob Cohen.\n\\newblock The Earth Is Round (p < .05).\n\\newblock {\\em American Psychologist} 49(12): 997--1003, 1994.\n\n\\bibitem{Cournot:1843}\nAntoine Augustin Cournot.\n\\newblock {\\em Exposition de la Th\\'{e}orie des Chances et des Probabilit\\'{e}s}.\n\\newblock Hachette, Paris, 1843.\n\n\\bibitem{Cox-Hinkley:1974}\nD.~R. Cox and D.~V Hinkley.\n\\newblock {\\em Theoretical Statistics}.\n\\newblock Chapman and Hall London, 1974.\n\n\\bibitem{Davis-Hardy:1990}\nB.R. Davis and R.J. Hardy,\n\\newblock Upper bounds for type I and type II error rates in conditional power calculations\n\\newblock {\\em Communications in Statistics}, 19(10): 3571--3584, 1990.\n\n\\bibitem{Dembski:1998}\nWilliam~A. Dembski.\n\\newblock {\\em The Design Inference: Eliminating Chance Through Small\n  Probabilities}.\n\\newblock Cambridge University Press, 1998.\n\n\\bibitem{Dembski:2006}\nWilliam~A. Dembski.\n\\newblock {\\em No Free Lunch: Why Specified Complexity Cannot Be Purchased\n  Without Intelligence}.\n\\newblock Rowman and Littlefield, 2006.\n\n\\bibitem{Ferragina:2007}\nPaolo Ferragina, Raffaele Giancarlo, Valentina Greco, Giovanni Manzini, and\n  Gabriel Valiente.\n\\newblock Compression-based classification of biological sequences and\n  structures via the universal similarity metric: experimental assessment.\n\\newblock {\\em BMC Bioinformatics}, 8(1): 252, 2007.\n\n\\bibitem{Fisher:1925}\nRonald~A. Fisher.\n\\newblock {\\em Statistical Methods for Research Workers}.\n\\newblock Oliver and Boyd, Edinburgh, 1925.\n\n\\bibitem{Fisher:1926}\nRonald~A. Fisher.\n\\newblock {\\em Statistical Methods and Scientific Inference}.\n\\newblock Hafner, 1926.\n\n\\bibitem{Fisher:1956}\nRonald~A. Fisher.\n\\newblock The arrangement of field experiments.\n\\newblock {\\em Journal of the Ministry of Agriculture of Great Britain},\n33: 503--513, 1956.\n\n\\bibitem{Fraley:2003}\nR. Chris Fraley.\n\\newblock The Statistical Significance Testing Controversy: A Critical Analysis.\n\\newblock \\url{http://www.uic.edu/classes/psych/psych548/fraley/}.\n\n\\bibitem{Friedman:1996}\nL. Friedman, C. Furberg and D. DeMets.\n\\newblock {\\em Fundamentals of Clinical Trials}, 3rd edition. Mosby, 1996.\n\n\\bibitem{Gilbert:2007}\nD.~Gilbert, F.~Rossello, G.~Valiente, and M.~Veeramalai.\n\\newblock Alignment-free comparison of tops strings.\n\\newblock {\\em London Algorithmics and Stringology 2006}, 8: 177--197, 2007.\n\n\\bibitem{Granovetter:1973}\nMark~S. Granovetter.\n\\newblock The strength of weak ties.\n\\newblock {\\em The American Journal of Sociology}, 78(6): 1360--1380, 1973.\n\n\\bibitem{Vitanyi+2:1997}\nWalter Kirchherr, Ming Li and Paul Vit\\'{a}nyi.\n\\newblock The miraculous universal distribution.\n\\newblock {\\em The Mathematical Intelligencer}, 19(4): 7--15, 1997.\n\n\\bibitem{Kolmogorov:1933}\nAndrei~N. Kolmogorov.\n\\newblock {\\em Grundbegriffe der Wahrscheinlichkeitsrechnung}.\n\\newblock Springer, Berlin, 1933.\n\n\\bibitem{Kolmogorov:1965}\nAndrei~N. Kolmogorov.\n\\newblock Three approaches to the quantitative definition of information.\n\\newblock {\\em Problems Inform. Transmission}, 1(1): 1--7, 1965.\n\n\\bibitem{Krasnogor:2004}\nN.~Krasnogor and D.~A. Pelta.\n\\newblock Measuring the similarity of protein structures by means of the\n  universal similarity metric.\n\\newblock {\\em Bioinformatics}, 20: 1015--1021, May 2004.\n\n\\bibitem{Levin:1984}\nLeonid A. Levin.\n\\newblock Randomness Conservation Inequalities; Information and Independence in\nMathematical Theories.\n\\newblock Information and Control 61: 15--37, 1984.\n\n\\bibitem{Levy:1925}\nPaul L\\'{e}vy.\n\\newblock {\\em Calcul de probabilit\\'{e}s}.\n\\newblock Gauthier-Villars, Paris, 1925\n\n\\bibitem{Li:2001}\nMing Li, Jonathan~H. Badger, Xin Chen, Sam Kwong, Paul Kearney, and Hao\\-yong Zhang.\n\\newblock An information-based sequence distance and its application to whole mitochondrial genome phylogeny.\n\\newblock {\\em Bioinformatics}, 17(2): 149--154, 2001.\n\n\\bibitem{Li-Vitanyi:1997}\nMing Li and Paul Vit\\'{a}nyi.\n\\newblock {\\em An Introduction to Kolmogorov Complexity and its Applications}.\n\\newblock 2nd edition, Springer, 1997.\n\n\\bibitem{Meinert:1986}\nC.L. Meinert.\n\\newblock {\\em Clinical Trials Design, Conduct, and Analysis}.\n\\newblock Oxford University Press, 1986.\n\n\\bibitem{Merton:1957}\nRobert~King Merton.\n\\newblock {\\em Social Theory and Social Structure}.\n\\newblock Free Press, (Glencoe, Il.), 1957.\n\n\\bibitem{Morrison+1:1970}\nDenton E. Morrison and Ramon E. Henkel, editors.\n\\newblock {\\em The Significance Test Controversy}.\n\\newblock Aldine. Oxford, England, 1970.\n\n\\bibitem{Moye:1998}\nL.A. Moye.\n\\newblock P-value interpretation and alpha allocation in clinical trials.\n\\newblock {\\em Annals of Epidemiology}, 8(6): 351--357, 1998.\n\n\\bibitem{Richardson-Domingos:2006}\nMatthew Richardson and Pedro Domingos.\n\\newblock Markov logic networks.\n\\newblock {\\em Machine Learning}, 62(1-2): 107--136, 2006.\n\n\\bibitem{Shafer:2006}\nGlenn Shafer.\n\\newblock Why did Cournot's Principle disappear?\n\\newblock Lecture at \\'{E}cole des Hautes \\'{E}tudes en Sciences Sociales, May  2006.\n\\newblock \\url{http://www.glennshafer.com/assets/downloads/disappear.pdf}.\n\n\\bibitem{Shafer-Vovk:2001}\nGlenn Shafer and Vladimir Vovk.\n\\newblock Probability and Finance: It's Only a Game.\n\\newblock John Wiley and Sons, 2001.\n\n\\bibitem{Shen:2009}\nAlexander Shen.\n\\newblock Algorithmic Information Theory and Foundations of Probability.\n\\newblock {\\em arXiv} 0906.4411, 2009.\n\n\\bibitem{Solomonoff:1964}\nRay Solomonoff.\n\\newblock A formal theory of inductive inference, part 1 and part 2.\n\\newblock {\\em Information and Control}, 7(1): 1--22, and 7(2): 224--254, 1964.\n\n\\bibitem{Wasserman:1994}\nS.~Wasserman and K.~Faust.\n\\newblock {\\em Social Network Analysis: Methods and Applications}.\n\\newblock Cambridge Univ Pr, 1994.\n\n\\bibitem{Wein:2002}\nRichard Wein.\n\\newblock Not a Free Lunch But a Box of Chocolates: A critique of William Dembski's book ``No Free Lunch''.\n\\newblock \\url{http://www.talkorigins.org/design/faqs/nfl}.\n\n\\bibitem{Welch:1984}\nTerry Welch.\n\\newblock A Technique for High-Performance Data Compression.\n\\newblock {\\em IEEE Computer}, 17(6): 8-\u009619, 1984.\n\n\\bibitem{Wells:2010}\nRoy Wells.\n\\newblock 41.6\\% the US Population Has a Facebook Account.\n\\newblock  Posted August 8, 2010 at\n  \\url{http://socialmediatoday.com/index.php?q=roywells1/158020/416-us-population-h\nas-facebook-account}.\n\n\\bibitem{ppm}\nWikipedia.\n\\newblock Prediction by partial matching.\n\\newblock \\url{http://en.wikipedia.org/wiki/PPM_compression_algorithm}.\n\n\\bibitem{p-value}\nWikipedia.\n\\newblock P-value.\n\\newblock \\url{http://en.wikipedia.org/wiki/P-value}.\n\n\\bibitem{Lempel-Ziv:1977}\nJacob Ziv and Abraham Lempel.\n\\newblock A universal algorithm for sequential data compression.\n\\newblock {\\em IEEE Transactions on Information Theory}, 23(3): 337--343, 1977.\n\n\n\\bibitem{Lempel-Ziv:1978}\nJacob Ziv and Abraham Lempel.\n\\newblock Compression of individual sequences via variable-rate coding.\n\\newblock {\\em IEEE Transactions on Information Theory}, 24(5): 530--536, 1978.\n\n\n\\end{thebibliography}\n\n\n", "itemtype": "equation", "pos": 46456, "prevtext": "\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} If the null hypothesis is impugned then some rules have been violated. Who is the guilty\nparty? In the lottery case, it was clear more or less that John was the guilty party. In this\ncase, it is not obvious who the guilty party is.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} We do not pretend to know the guilty party. Our only goal is to impugn randomness. There may\nbe more than one guilty party as far as we know. Their actions may or may not have been\ncoordinated.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} As far as I see, plenty of randomness might have remained. You did not impugn all randomness.\n\n{\\smallskip\\noindent\\textbf{A:\\ }} You are right. Let us express our goal more precisely: it is to impugn the null hypothesis, no\nmore no less.\n\n\n\n\\subsection{A Stalking Case}\n\\label{sub:stalking2}\n\n\\paragraph{Trial}\nAn informal description of the case is given in \\S\\ref{sub:stalking1}. The 10 nights of the trial\nmay be represented by numbers $1,\\dots,10$.\nThe outcomes of the trial may be represented by functions $f$ from $\\{1, \\ldots, 10\\}$ to\n$\\{0,1\\}$ where the meaning of $f(n)=1$ (resp. $f(n)=0$) is that Alice and Bob met (resp. did not\nmet) Chris at the restaurant on night $n$.\n\n\\paragraph{Null Hypothesis}\n\nIntentionally, the null hypothesis says that Chris does not stalk Alice. Formally, the null\nhypothesis says that a probability distribution ${\\mathtt{P}}$ on the outcomes is innate if and only if it\nsatisfies the following two requirements for every outcome $f$.\n\\begin{enumerate}\n\\item Events $f(n_1)=1$ and $f(n_2)=1$ are independent for any nights $n_1\\neq n_2$.\n\\item ${\\mathtt{P}}(f(n)=1) \\leq 1/100$ for every night $n$.\n\\end{enumerate}\n\\noindent\nRequirement 2 says that ${\\mathtt{P}}(f(n)=1)$ is less than (rather than equal to) $1/100$ rather than\n${\\mathtt{P}}(f(n)=1) = 1/100$ because, at night $n$, Chris may not to be present at all in any of the 100\nrestaurants at the time when Alice and Bob dine. If he is in one of the 100 restaurants when\nAlice and Bob dine then ${\\mathtt{P}}(f(n)=1) = 1/100$.\n\n\\paragraph{Background Information} Chris is suspected of stalking Alice in restaurants.\n\n\\paragraph{Actual Outcome} Five times out of ten times Alice and Bob meet Chris at the chosen\nrestaurant.\n\n\\paragraph{Logic Model}\nThere are three sorts of elements and one relation.\n\\begin{itemize}\n \\item Sort Night consists of numbers $1,\\dots,10$.\n \\item Sort Outcome consists of all possible functions from Night to $\\{0,1\\}$.\n \\item Relation $R(f,n)$ holds if and only if, on night $n$, Alice and Bob meet Chris at the\nchosen restaurant.\n\\end{itemize}\n\n\\noindent\n{\\bf Focal Event}\\quad\n $\\{f:\\ \\exists n_1 \\exists n_2\n \\big( n_1\\neq n_2 \\land R(f,n_1) \\land R(f,n_2) \\big) \\}$.\n\\medskip\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} The focal event is that there are two distinct nights when Chris dines at the restaurant where\nAlice and Bob dine. What if the current probability threshold is lower than you presume, and the\nfocal event turns out to be non-negligible?\n\n{\\smallskip\\noindent\\textbf{A:\\ }} In this particular case, it is natural to consider the focal event that there are three\ndistinct nights when Chris dines at the restaurant where Alice and Bob dine.\n\n{\\smallskip\\noindent\\textbf{Q:\\ }} It is rather expensive to say that there are $k$ distinct elements; the description complexity is $O(k^2)$. Now I see why you mentioned those dotted existential quantifiers $\\dot{\\exists}$ in \\S\\ref{sub:alt}.\n\n\n\n\\subsection{The case of Nicholas Caputo}\n\\label{sub:caputo2}\n\n\\paragraph{Trial} An informal description of the case is given in \\S\\ref{sub:caputo1}. The $41$ elections may be represented by numbers $1,\\dots,41$. The possible outcomes of the trial can be seen as functions $f$ from $\\{1,\\dots,41\\}$ to \\{D,N\\} where the letters $D$, $N$ indicate whether the top ballot line went to a Democrat or not.\n\n\\paragraph{Null Hypothesis}\nIntentionally the null hypothesis is that the drawings were fair.\nFormally, the null hypothesis says that there is a unique innate probability distribution ${\\mathtt{P}}$ on\nthe outcomes, and that ${\\mathtt{P}}$ satisfies the following two requirements for every outcome $f$.\n\\begin{enumerate}\n\\item Events $f(e_1)=D$ and $f(e_2)=D$ are independent for any elections $e_1\\neq e_2$.\n\\item For every election $e$, ${\\mathtt{P}}(f(e)=D) = r_e$ where $r_e$ is the fraction of Democrats on\nthe ballot.\n\\end{enumerate}\n\\noindent\nWe assume that every $r_e\\geq 0.4$.\n\n\\paragraph{Background Information} The county clerk, who conducted the drawings, was a Democrat.\n\n\\paragraph{Actual Outcome} $40$ times out of $41$ times the top ballot line went to Democrats.\n\n\\paragraph{Logic Model} The model has three sorts, two constants and one relation.\n\\begin{itemize}\n\\item Sort Election consists of numbers $1,\\ldots,41$ representing the $41$ elections.\n\\item Sort Party consists of two elements. The elements of Party will be called parties.\n\\item Sort Outcome consists of all $f$ from Election to Party.\n\\item The constants $D$ and $N$ of type Party denote distinct elements of type Party.\n\\item The relation $R(f,e,p)$ of type Outcome $\\times$ Election $\\times$ Party holds if and only if, according to outcome $f$, the top ballot line went to party $p$ at elections $e$.\n\\end{itemize}\n\n\\paragraph{Focal Event:}\n\n", "index": 13, "text": "$$\n \\{f:\\ \\forall i\\forall j ((R(f,i,N) \\land R(f,j,N))\\to i=j)\\}.\n$$\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex6.m1\" class=\"ltx_Math\" alttext=\"\\{f:\\ \\forall i\\forall j((R(f,i,N)\\land R(f,j,N))\\to i=j)\\}.\" display=\"block\"><mrow><mrow><mo stretchy=\"false\">{</mo><mi>f</mi><mo rspace=\"7.5pt\">:</mo><mo>\u2200</mo><mi>i</mi><mo>\u2200</mo><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">(</mo><mi>R</mi><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u2227</mo><mi>R</mi><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2192</mo><mi>i</mi><mo>=</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}]