[{"file": "1601.02147.tex", "nexttext": "\n\nHere $F(x) = \\int f(x,y) \\mu_x(dy)$ is the averaged vector field, with\n$\\mu_x(dy)$ being the ergodic invariant measure of the fast variables\n$Y_x$ with a frozen $x$ variable. This averaging principle is akin to\nthe law of large number (LLN) in the present context and it suggests\nto simulate the evolution of the slow variables using~\\eqref{eq:2}\nrather than~\\eqref{e:fs_intro} when $\\eps$ is small. This requires to\nestimate $F(x)$, which typically has to be done on-the-fly given the\ncurrent value of the slow variables. To this end, note that if Euler's\nmethod with time step $\\Delta t$ is used as integrator for the slow\nvariables in~\\eqref{e:fs_intro}, we can approximate\n$X^\\eps(n\\Delta t)$ by $x_n$ satisfying the recurrence\n\n\\begin{equ}\n  \\label{e:scheme1}\n  x^\\eps_{n+1} = x^\\eps_{n} + \\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x_n^\\eps,Y^\\eps_{x_n^\\eps}(s))ds \\;,\n\\end{equ}\n\nwhere $Y^\\eps_{x}$ denotes the solution to the second equation\nin~\\eqref{e:fs_intro} with $X^\\eps$ kept fixed at the value~$x$.  If\n$\\eps $ is small enough that $\\Delta t / \\eps$ is larger than the\nmixing time of~$Y_x^\\eps$, the Birkhoff integral in~\\eqref{eq:B} is in\nfact close to the averaged coefficient in~\\eqref{eq:2}, in the sense\nthat\n\n\\begin{equ}\n  \\label{eq:B}\n  F(x) \\approx \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x,Y^\\eps_{x}(s))ds\\;.\n\\end{equ}\n\nTherefore \\eqref{e:scheme1} can also be thought of as an integrator\nfor the averaged equation~\\eqref{eq:2}. In fact, when $\\eps$ is small,\none can obtain a good approximation of $F(x)$ using only a fraction of\nthe macro time step. In particular, we expect that\n\n\\begin{equ}\n  \\label{e:intro_ergodic_approx}\n  \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x,Y^\\eps_{x}(s))ds \\approx \\frac{\\lambda}{\\Delta\n    t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x,Y^\\eps_{x}(s))ds =: F_n(x)\n\\end{equ}\n\nwith $\\lambda\\ge 1$ provided that $\\Delta t / (\\eps \\lambda)$ remains\nlarger than the mixing time of $Y^\\eps_x$. This observation is at the\ncore of HMM-type methods -- in essence, they amount to replacing\n\\eqref{e:scheme1} by\n\n\\begin{equ}\n  \\label{e:scheme2}\n  x_{n+1} = x_{n} + \\Delta t\\, F_n(x_n) \\;.\n\\end{equ}\n\nSince the number of computations required to compute the effective\nvector field $F_n(x)$ is reduced by a factor~$\\lambda$, this is also\nthe speed-up factor for an HMM-type method.\n\nFrom the argument above, it is apparent that there is another,\nequivalent way to think about HMM-type methods, as was first pointed\nout in \\cite{fatkullin04} (see\nalso~\\cite{vanden2007hmm,weinan2009general,ariel2012multiscale,\n  ariel2013multiscale}.  Indeed, the integral defining $F_n(x)$\nin~\\eqref{e:intro_ergodic_approx} can be recast into an integral on\nthe full interval $[n\\Delta t,(n+1)\\Delta t]$ by a change of\nintegration variables, which amount to rescaling the internal clock of\nthe variables $Y^\\eps_x$. In other words, HMM-type methods can also be\nthought of as approximating the fast-slow system in~\\eqref{e:fs_intro}\nby\n\n\\begin{equs}\n  \\label{e:cp_intro}\n  \\frac{d{\\widetilde{X}}^\\eps}{dt} &= f({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^\\eps) \\\\\n  \\frac{d{\\widetilde{Y}}^\\eps}{dt} &= \\frac{1}{\\eps\\lambda} g({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^\\eps) \\;.\n \\end{equs} \n \n If $\\eps\\ll 1$, we can reasonably replace $\\eps$ with $\\eps \\lambda$,\n provided that this product still remains small -- in particular, the\n evolution of the slow variables in~\\eqref{e:cp_intro} is still\n captured by the limiting equation~\\eqref{eq:2}. Hence HMM-type\n methods are akin to artificial compressibility \\cite{chorin67} in\n fluid simulations and Car-Parrinello methods \\cite{car85} in\n molecular dynamics.\n\n\n The approximations in~\\eqref{e:intro_ergodic_approx} or\n \\eqref{e:cp_intro} are perfectly reasonable if we are only interested\n in staying faithful to the averaged equation~\\eqref{eq:2} -- that is\n to say, HMM-type approximations will have the correct law of large\n numbers (LLN) behavior. However, the fluctuations about that average\n will be enhanced by a factor of $\\lambda$. This is quite clear from\n the interpretation \\eqref{e:cp_intro}, since in the original model\n \\eqref{e:fs_intro}, the local fluctuations about the average are of\n order $\\sqrt{\\eps}$ and in \\eqref{e:cp_intro} they are of order\n $\\sqrt{\\eps \\lambda}$. The large fluctuations about the average\n caused by rare events are similarly inflated by a factor of\n $\\lambda$. This can be an issue, for example in metastable fast-slow\n systems, where the large fluctuations about the average determine the\n waiting times for transitions between metastable states. In\n particular we shall see that an HMM-type scheme drastically decreases\n these waiting times due to the enhanced fluctuations.\n\n In this article we propose a simple modification of HMM which\n corrects the problem of enhanced fluctuations. The key idea is\n to replace the approximation \\eqref{e:intro_ergodic_approx} with\n\n\\begin{equ}\n  \\label{e:intro_ergodic_approx2}\n  \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x,Y^\\eps_{x}(s))ds\n  \\approx \n  \\sum_{j=1}^\\lambda\\frac{1}{\\Delta\n    t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} \n  f(x,Y^{\\eps,j}_{x}(s))ds\\;, \n\\end{equ}\n\nwhere each $Y^{\\eps,j}_x$ is an independent copy of $Y^\\eps_x$. By\ncomparing \\eqref{e:intro_ergodic_approx} with\n\\eqref{e:intro_ergodic_approx2}, we see that the first approximation\nis \\emph{essentially} replacing a sum of $\\lambda$ weakly correlated\nrandom variables with one random variable, multiplied by\n$\\lambda$. This introduces correlations that should not be there and\nin particular results in enhanced fluctuations. In\n\\eqref{e:intro_ergodic_approx2}, we instead replace the sum of\n$\\lambda$ weakly correlated random variables with a sum of $\\lambda$\nindependent random variables. This is a much more reasonable approximation to\nmake, since these random variables are becoming less and less\ncorrelated as $\\eps$ gets smaller. Since the terms appearing on the\nright hand side are independent of each other, they can be computed in\nparallel. Thus if one has $\\lambda$ CPUs available, then the real time\nof the computations is identical to HMM. For this reason, we call the\nmodification the parallelized HMM (PHMM). Note that, in analogy\nto~\\eqref{e:cp_intro}, one can interpret PHMM as approximating\n\\eqref{e:fs_intro} by the system\n\n  \\begin{equs}\n    \\label{e:cp_intro_p}\n  \\frac{d{\\widetilde{X}}^\\eps}{dt} &= \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^{\\eps,j}) \\\\\n  \\frac{d{\\widetilde{Y}}^{\\eps,j}}{dt} &= \\frac{1}{\\eps\\lambda}\n  g({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^{\\eps,j}) \\quad \\text{for\n    $j=1,\\dots,\\lambda$}\\;.\n \\end{equs}  \n\n It is clear that this approximation will be as good as\n \\eqref{e:cp_intro} in term of the LLN, but in contrast with\n \\eqref{e:cp_intro}, we will show below that it captures the\n fluctuations about the average correctly, both in terms of small\n Gaussian fluctuations and large fluctuations describing rare\n events. A similar observation in the context of numerical\n homogenization was made in~\\cite{bal11,bal14}.\n \n\n The outline of the remainder of this article is as follows. In\n Section~\\ref{s:fs} we recall the averaging principle for stochastic\n fast-slow systems and describe how to characterize the fluctuations\n about this average, including local Gaussian fluctuations and large\n deviation principles. In Section \\ref{s:hmm} we recall the HMM-type\n methods. In Section \\ref{s:hmm_fluctuations} we show that they lead to\n enhanced fluctuations. In Section \\ref{s:phmm} we introduce the PHMM\n modification and in Section \\ref{s:phmm_fluctuations} show that this\n approximation yields the correct fluctuations, both in terms of local\n Gaussian fluctuations and large deviations. In Section \\ref{s:num} we\n test PHMM for a variety of simple models and conclude in Section\n \\ref{s:discussion} with a discussion.\n \n \n\\section{Average and fluctuations in fast-slow systems}\n\\label{s:fs}\n\nFor simplicity we will from here on assume that the fast variables are\nstochastic. This assumption is convenient, but not necessary, since\nall the averaging and fluctuation properties stated below are known to\nhold for large classes of fast-slow systems with deterministically\nchaotic fast variables \\cite{kifer92, dolgopyat04,\n  kelly15a,kelly15b}. The fast-slow systems we investigate are given\nby\n  \\begin{equs}\\label{e:fs}\n \\frac{dX^\\eps}{dt} &= f(X^\\eps,Y^\\eps) \\\\\n dY^\\eps &= \\frac{1}{\\eps} g(X^\\eps,Y^\\eps) dt + \\frac{1}{\\sqrt{\\eps}} \\sigma(X^\\eps,Y^\\eps) dW \\;\\;,\n \\end{equs}\n where $f : \\reals^d \\times \\reals^e \\to \\reals^d$,\n $g : \\reals^d \\times \\reals^e \\to \\reals^e$,\n $\\sigma: \\reals^d \\times \\reals^e \\to \\reals^e\\times \\reals^e$, and\n $W$ is a standard Wiener process in $\\reals^e$.  We assume that for\n every $x \\in \\reals^d$, the Markov process described by the SDE\n\n \n", "itemtype": "equation", "pos": 1446, "prevtext": "\n\n\\maketitle\n\n\n\n\n\\begin{abstract}\n  How heterogeneous multiscale methods (HMM) handle fluctuations\n  acting on the slow variables in fast-slow systems is\n  investigated. In particular, it is shown via analysis of central\n  limit theorems (CLT) and large deviation principles (LDP) that the\n  standard version of HMM artificially amplifies these fluctuations. A\n  simple modification of HMM, termed parallel HMM, is introduced and\n  is shown to remedy this problem, capturing fluctuations correctly\n  both at the level of the CLT and the LDP. Similar type of arguments\n  can also be used to justify that the $\\tau$-leaping method used in\n  the context of Gillespie's stochastic simulation algorithm for\n  Markov jump processes also captures the right CLT and LDP for these\n  processes.\n\\end{abstract}\n\n\n\\large\n\n \n \n \\section{Introduction}\n \\label{s:intro}\n The heterogeneous multiscale methods (HMM)\n \\cite{engquist03,vanden01,\n   weinan2007heterogeneous,abdulle2012heterogeneous} provide an\n efficient strategy for integrating fast-slow systems of the type\n\n\\begin{equs}\n    \\label{e:fs_intro}\n    \\frac{dX^\\eps}{dt} &= f(X^\\eps,Y^\\eps) \\\\\n    \\frac{dY^\\eps}{dt} &= \\frac{1}{\\eps} g(X^\\eps,Y^\\eps) \\;.\n\\end{equs}\n\nThe method relies on an averaging principle that holds under some\nassumption of ergodicity and states that as $\\eps\\to0$ the slow\nvariables $X^\\eps$ can be uniformly approximated by the solution to\nthe following averaged equation\n\n \n", "index": 1, "text": "\\begin{equation}\n   \\label{eq:2}\n   \\dot{{\\bar{X}}} = F({\\bar{X}})\\;.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\dot{{\\bar{X}}}=F({\\bar{X}})\\;.\" display=\"block\"><mrow><mrow><mover accent=\"true\"><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">\u00af</mo></mover><mo>\u02d9</mo></mover><mo>=</mo><mrow><mi>F</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>X</mi><mo stretchy=\"false\">\u00af</mo></mover><mo rspace=\"5.3pt\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.02147.tex", "nexttext": "\nis ergodic, with invariant measure $\\mu_x$, and has sufficient mixing\nproperties. For full details on the necessary mixing properties, see\nfor instance \\cite{fw12}.\n\n\n\nIn this section we briefly recall the averaging principle for\nstochastic fast-slow systems and discuss two results that characterize\nthe fluctuations about the average, the central limit theorem (CLT)\nand the large deviations principle (LDP).\n\n\\subsection{Averaging principle}\nAs $\\eps\\to 0$, each realization of $X^\\eps$, with initial condition\n$X^\\eps(0) = x$, tends towards a trajectory of a deterministic system\n\n\\begin{equ}\n  \\label{e:fs_avg}\n  \\frac{d{\\bar{X}}}{dt} = F({\\bar{X}})  \\;, \\quad {\\bar{X}}(0) = x\\;,\n\\end{equ}\n\nwhere $F(x) = \\int f(x,y) \\mu_x(dy)$ and $\\mu_x$ is the invariant\nmeasure corresponding to the Markov process\n$dY_x = b(x,Y_x)dt + \\sigma(x,Y_x) dW$. The convergence is in an\nalmost sure and uniform sense:\n\\begin{equ}\n\\lim_{\\eps \\to 0}\\sup_{t \\leq T} |X^\\eps(t) - {\\bar{X}}(t)|  = 0\n\\end{equ}\nfor every fixed $T>0$, every choice of initial condition $x$ and\nalmost surely every initial condition $Y^\\eps(0)$ (a.s. with respect\nto $\\mu_x$) as well as every realization of the Brownian paths driving\nthe fast variables. Details of this convergence result in the setting\nabove are given in (for instance) \\cite[Chapter 7.2]{fw12}.\n\n\n\\subsection{Small fluctuations -- CLT}\n\\label{s:fs_clt}\n\nThe \\emph{small} fluctuations of $X^\\eps$ about the averaged system\n${\\bar{X}}$ can be understood by characterizing the limiting behavior of\n\\begin{equ}\nZ^\\eps : = \\frac{X^\\eps - {\\bar{X}}}{\\sqrt{\\eps}}\\;,\n\\end{equ}\nas $\\eps\\to 0$. It can be shown that the process $Z^\\eps$ converges in\ndistribution (on the space of continuous functions\n$C([0,T]; \\reals^d)$ endowed with the sup-norm topology) to a process\n$Z$ defined by the SDE\n\\begin{equ}\\label{e:fs_clt}\ndZ = B_0({\\bar{X}}) Z dt + \\eta ({\\bar{X}}) dV \\;, \\quad  Z(0) = 0\\;,\n\\end{equ} \nHere ${\\bar{X}}$ solves the averaged system in~\\eqref{e:fs_avg}, $V$ is a\nstandard Wiener process, $B_0 := B_1 + B_2$ with\n\\begin{equs}\nB_1 (x) &= \\int \\nabla_x f(x,y) \\mu_x(dy )\\\\\nB_2(x) &= \\int_0^\\infty d\\tau \\int \\mu_x(dy) \\nabla_y \n{\\mathbf{E}}_y \\bigg( {\\tilde{f}}(x,Y_x(\\tau))\\bigg)  \\nabla_x b(x,y)    \n\\end{equs}\nand \n\\begin{equ}\n\\eta(x)\\eta^T(x) = \\int_0^\\infty d\\tau\\, {\\mathbf{E}} {\\tilde{f}}(x, Y_x(0))\\otimes \n{\\tilde{f}} (x,Y_x(\\tau) \\;,\n\\end{equ} \nwhere ${\\tilde{f}}(x,y) = f(x,y) - F(x)$, ${\\mathbf{E}}_y$ denotes expectation over\nrealizations of $Y_x$ with $Y_x(0) = y$, and ${\\mathbf{E}}$ denotes expectation\nover realization of $Y_x$ with $Y_x(0) \\sim \\mu_x$. We include next a\nformal argument deriving this limit, as it will prove useful when\nanalyzing the multiscale approximation methods. We will replicate the\nargument given in \\cite{bouchet15}; a more complete and rigorous\nargument can be found in \\cite[Chapter 7.3]{fw12}.\n\n\nFirst, we write a system of equation for the triple\n$({\\bar{X}}, Z^\\eps, Y^\\eps)$ in the following approximated form, which\nuses nothing more than Taylor expansions of the original system\nin~\\eqref{e:fs_intro}:\n\\begin{equs}\n  \\frac{d{\\bar{X}}}{dt} &= F({\\bar{X}}) \\\\\n  \\frac{dZ^\\eps}{dt} & = \\frac{1}{\\sqrt{\\eps}} {\\tilde{f}}({\\bar{X}}, Y^\\eps) \n  + \\nabla_x f ({\\bar{X}}, Y^\\eps)  + O(\\sqrt{\\eps}) \\\\\n  dY^\\eps &= \\frac{1}{\\eps} b({\\bar{X}},Y^\\eps) dt + \\frac{1}{\\sqrt{\\eps}}\n  \\nabla_x b({\\bar{X}}, Y^\\eps) Z^\\eps dt + \\frac{1}{\\sqrt{\\eps} }\n  \\sigma({\\bar{X}}, Y^\\eps) dW + O(1)\\;.\n\\end{equs}\nWe now proceed with a classical perturbation expansion on the\ngenerator of the triple $({\\bar{X}},Z^\\eps,Y^\\eps)$. In particular we have\n${\\mathcal{L}}_\\eps = \\frac{1}{\\eps}{\\mathcal{L}}_{0} + \\frac{1}{\\sqrt{\\eps}}{\\mathcal{L}}_1 + {\\mathcal{L}}_2\n+ \\dots$ where\n\\begin{equs}\n{\\mathcal{L}}_0  &=  b(x,y)\\cdot\\nabla_y + a(x,y) : \\nabla_y^2\\\\\n{\\mathcal{L}}_1 &=  {\\tilde{f}}(x,y) \\cdot\\nabla_z + (\\nabla_x b(x,y) z) \\cdot \\nabla_y \\\\\n{\\mathcal{L}}_2 &= F(x) \\cdot \\nabla_x + (\\nabla_x f(x,y)z) \\cdot\\nabla_z\n\\end{equs}\nand $a=\\sigma\\sigma^T$. Let\n$u_\\eps(x,z,y,t) = {\\mathbf{E}}_{(x,z,y)} \\vphi({\\bar{X}}(t), Z^\\eps(t),Y^\\eps(t))$\nand introduce the ansatz\n$u_\\eps = u_0 + \\sqrt{\\eps} u_1 + \\eps u_2 + \\dots$. By substituting\n$u_\\eps$ into $\\del_t u_\\eps = {\\mathcal{L}}_\\eps u_\\eps$ and equating powers of\n$\\eps$ we obtain\n\\begin{equs}\nO(\\eps^{-1}) &: {\\mathcal{L}}_0 u_0 = 0 \\\\    \nO(\\eps^{-1/2}) &: {\\mathcal{L}}_0 u_1 = -{\\mathcal{L}}_1 u_0 \\\\\nO(\\eps^{-1}) &: \\del_t u_0 = {\\mathcal{L}}_2 u_0 + {\\mathcal{L}}_1 u_1 + {\\mathcal{L}}_0 u_2\\;.\n\\end{equs}\nFrom the $O(\\eps^{-1})$ identity, we obtain $u_0 = u_0 (x,z,t)$,\nconfirming that the leading order term is independent of $y$. By the\nFredholm alternative, the $O(\\eps^{-1/2})$ identity has a solution\n$u_1$ which has the Feynman-Kac representation\n\\begin{equ}\n  u_1(x,y,z)  = \\int_0^\\infty d\\tau \\, {\\mathbf{E}}_y \\left( {\\tilde{f}} (x,\n    Y_x(\\tau) ) \\right) \n  \\cdot\\nabla_z u_0 (x,z)\\;,\n\\end{equ}\nwhere $Y_x$ denotes the Markov process generated by ${\\mathcal{L}}_0$, i.e. the\nsolution of~\\eqref{eq:1}. Finally, if we average the $O(1)$ identity\nagainst the invariant measure corresponding to ${\\mathcal{L}}_0$, we obtain\n\\begin{equs}\n\\del_t u_0  &= F(x)\\nabla_x u_0 + \n\\int \\mu_x (dy) (\\nabla_x f(x,y)z) \\cdot\\nabla_z u_0  \\\\  \n& + \\int \\mu_x(dy) \\int_0^\\infty d\\tau {\\tilde{f}}(x,y) \\otimes \n{\\mathbf{E}}_y{\\tilde{f}}(x,Y_x(\\tau)) : \\nabla^2_z u_0 \\\\\n& + \\int \\mu_x (dy) (\\nabla_x b(x,y) z) \\int_0^\\infty d\\tau  \\,\\nabla_y \n{\\mathbf{E}}_y{\\tilde{f}}(x,Y_x(\\tau))  \\nabla_z u_0 \\;.\n\\end{equs}\nClearly, this is the forward Kolmogorov equation for the Markov\nprocess $({\\bar{X}}, Z)$ defined by\n\\begin{equs}\n\\frac{d{\\bar{X}}}{dt} &= F({\\bar{X}}) \\\\\nd Z &= B_0({\\bar{X}}) Z dt + \\eta ({\\bar{X}} ) dV\n\\end{equs}\nwith $B_0$ and $\\eta$ defined as above. \n\n\\subsection{Large fluctuations -- LDP}\n\\label{s:fs_ldp}\n\nA large deviation principle (LDP) for the fast-slow system\n\\eqref{e:fs} quantifies probabilities of $O(1)$ fluctuations of\n$X^\\eps$ away from the averaged trajectory ${\\bar{X}}$. The probability of\nsuch events vanishes exponentially quickly and as a consequence are\nnot accounted for by the CLT fluctuations, hence an LDP accounts for\nthe \\emph{rare events}.\n\nWe say that the slow variables $X^\\eps$ satisfy a \\emph{large\n  deviation principle} (LDP) with action functional ${\\mathcal{S}}_{[0,T]}$ if\nfor any set\n$\\Gamma \\subset \\{ \\gamma \\in C([0,T], \\reals^d) : \\gamma(0) = x \\}$\nwe have\n\\begin{equs}\\label{e:fs_ldp_def}\n-\\inf_{\\gamma \\in \\mathring{\\Gamma}} {\\mathcal{S}}_{[0,T]}(\\gamma) &\\leq \\liminf_{\\eps \\to 0} \\eps \\log {\\mathbf{P}} \\left( X^\\eps \\in \\Gamma \\right)\\\\ &\\leq \\limsup_{\\eps \\to 0} \\eps \\log {\\mathbf{P}}\\left( X^\\eps \\in \\Gamma \\right) \\leq -\\inf_{\\gamma \\in \\bar{\\Gamma}} {\\mathcal{S}}_{[0,T]} (\\gamma)\\;,\n\\end{equs}\nwhere $\\mathring{\\Gamma}$ and $\\bar{\\Gamma}$ denote the interior and\nclosure of $\\Gamma$ respectively.\n \nAn LDP also determines many important features of $O(1)$ fluctuations\nthat occur on large time scales, such as the probability of transition\nfrom one metastable set to another. For example, suppose that $X^\\eps$\nis known to satisfy an LDP with action functional ${\\mathcal{S}}_{[0,T]}$. Let\n$D$ be an open domain in $\\reals^d$ with smooth boundary $\\del D$\nand let $x^* \\in D$ be an asymptotically stable equilibrium for the\naveraged system $\\dot{{\\bar{X}}} = F({\\bar{X}})$. When $\\eps \\ll 1$, we expect\nthat a trajectory of $X^\\eps$ that starts in $D$ will tend towards the\nequilibrium $x^*$ and exhibit $O(\\sqrt{\\eps})$ fluctuations about the\nequilibrium -- these fluctuations are described by the CLT. On very\nlarge time scales, these small fluctuations have a chance to `pile up'\ninto an $O(1)$ fluctuation, producing behavior of the trajectory that\nwould be considered impossible for the averaged system. Such\nfluctuations are not accurately described by the CLT and requires the\nLDP instead. For example, the asymptotic behaviour of escape time from\nthe domain $D$,\n\\begin{equ}\n\\tau^\\eps = \\inf \\{t > 0 : X^\\eps(t) \\notin D \\}\\;,\n\\end{equ}\ncan be quantified in terms of the \\emph{quasi-potential} defined by\n\\begin{equ}\\label{e:quasi_def}\n{\\mathcal{V}} (x,y) = \\inf_{T > 0} \\inf_{\\gamma(0)=x , \\gamma(T) = y} {\\mathcal{S}}_{[0,T]} (\\gamma)\n\\end{equ}\nUnder natural conditions, it can be shown that for any $x\\in D$\n\\begin{equ}\n\\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_x \\tau^\\eps = \\inf_{y \\in \\del D} {\\mathcal{V}} (x^*, y) \\;.\n\\end{equ}\nHence the time it takes to pass from the neighborhood of one\nequilibrium to another may be quantified using the LDP. Details on the\nescape time of fast-slow systems can be found in \\cite[Chapter\n7.6]{fw12}.\n\nLDPs for fast-slow systems of the type \\eqref{e:fs} are well\nunderstood \\cite[Chapter 7.4]{fw12}. First define the Hamiltonian\n${\\pazocal{H}} : \\reals^d \\times \\reals^d \\to \\reals$ by\n\\begin{equ}\\label{e:fs_ldp_ham}\n{\\pazocal{H}} (x,\\theta) = \\lim_{T\\to \\infty} \\frac{1}{T} \\log {\\mathbf{E}}_y \\exp \\bigg( \\theta \\cdot \\int_0^T f(x,Y_x(s)) ds   \\bigg) \\;,\n\\end{equ} \nwhere $Y_{x}$ denotes the Markov process governed by $dY_x = b(x,Y_x) dt + \\sigma(x,Y_x) dW$. Let ${\\mathcal{L}} : \\reals^d \\times \\reals^d \\to \\reals$ be the Legendre transform of ${\\pazocal{H}}$:\n\\begin{equ}\\label{e:fs_ldp_legendre}\n{\\mathcal{L}} (x, \\beta) = \\sup_{\\theta} \\left( \\theta \\cdot \\beta - {\\pazocal{H}} (x, \\theta)    \\right) \\;.\n\\end{equ}\nThen the action functional is given by\n\\begin{equ}\\label{e:action_legendre}\n{\\mathcal{S}}_{[0,T]} (\\gamma) = \\int_0^T {\\mathcal{L}} (\\gamma(s), \\dot{\\gamma}(s)) ds\\;.\n\\end{equ} \nIt can also be shown that the function\n$u(t,x) = \\inf_{\\gamma(0) = x }{\\mathcal{S}}_{[0,t]} (\\gamma)$ satisfies the\nHamilton-Jacobi equation\n\\begin{equ}\\label{e:fs_ldp_hj}\n\\del_t u(t,x) = {\\pazocal{H}} (x ,\\nabla u(t,x)) \\;.\n\\end{equ}\nDonsker-Varadhan theory tells us that the connection between\nHamilton-Jacobi equations and LDPs is in fact much deeper. Firstly,\n\\emph{Varadhan's Lemma} states that if a process $X^\\eps$ is known to\nsatisfy an LDP with some associated Hamiltonian ${\\pazocal{H}}$, then for any\n$\\vphi : \\reals^d \\to \\reals$ we have the generalized Laplace\nmethod-type result\n\\begin{equ}\\label{e:fs_ldp_varadhan}\n\\lim_{\\eps \\to 0 } \\eps \\log {\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (X^\\eps (t))   \\right) = S_t \\vphi (x)\n\\end{equ}\nwhere $S_t$ is the semigroup associated with the Hamilton-Jacobi\nequation $\\del_t u = {\\pazocal{H}} (x,\\nabla u)$. Conversely, if it is known\nthat \\eqref{e:fs_ldp_varadhan} holds for all $(x,t)$ and a suitable\nclass of $\\vphi$, then the inverse Varadhan's lemma states that\n$X^\\eps$ satisfies an LDP with action functional given by\n\\eqref{e:fs_ldp_legendre}, \\eqref{e:action_legendre}. Hence we can use\n\\eqref{e:fs_ldp_varadhan} to determine the action functional for a\ngiven process.\n\nIn the next few sections, we will exploit both sides of Varadhan's\nlemma when investigating the large fluctuations of the HMM and related\nschemes. More complete discussions on Varadhan's Lemma can be found in\n\\cite[Chapters 4.3, 4.4]{dz09}.\n   \n\n\n \\section{HMM for fast-slow systems}\n\\label{s:hmm}\nWhen applied to the stochastic fast-slow system \\eqref{e:fs}, HMM-type\nschemes rely on the fact that the slow $X^\\eps$ variables, and the\ncoefficients that govern them, converge to a set of reduced variables\nas $\\eps$ tends to zero. We will describe a simplest version of the\nmethod below, which is more convenient to deal with mathematically. \n\nBefore proceeding, we digress briefly on notation. When referring to\ncontinuous time variables we will always use upper case symbols\n($X^\\eps,Y^\\eps$ etc) and when referring to discrete time\napproximations we will always use lower case symbols ($x^\\eps_n$,\n$y^\\eps_n$ etc). We will also encounter continuous time variables\nwhose definition depends on the integer $n$ for which we have\n$t \\in [n\\Delta t, (n+1)\\Delta t)$. We will see below that such\ncontinuous time variables are used to define discrete time\napproximations. In this situation we will use upper case symbols with\na subscript $n$ (eg. $X^\\eps_n$).\n\n\nLet us now describe a `high-level' version of HMM.  Fix a step size\n$\\Delta t$ and define the intervals\n$I_{n, \\Delta t}: = [n\\Delta t, (n+1)\\Delta t)$. On each interval\n$I_{n,\\Delta t}$ we update $x^\\eps_n \\approx X^\\eps(n\\Delta t)$ to\n$x^\\eps_{n+1} \\approx X^\\eps((n+1)\\Delta t)$ via an iteration of the\nfollowing two steps:\n \\begin{enumerate}\n \\item (Micro step) Integrate the fast variables over the interval\n   $I_{n,\\Delta t}$, with the slow variable frozen at\n   $X^\\eps = x^\\eps_n$. That is, the fast variables are approximated\n   by\n\\begin{equ}\\label{e:micro}\nY^\\eps_{n}(t) = Y^\\eps_n(n\\Delta t) + \\frac{1}{\\eps} \n\\int_{n\\Delta t}^t g(x^\\eps_n,Y_{n}^\\eps (s))ds \n+  {\\frac{1}{\\sqrt{\\eps}}}\\int_{n\\Delta t}^t \\sigma (x^\\eps_n, Y^{\\eps}_n(s)) dW(s) \n\\end{equ}\nfor $n\\Delta t \\leq t \\leq (n+ 1/\\lambda)\\Delta t $ with some\n$\\lambda \\geq 1$ (that is, we do not necessarily integrate the\n$Y_n^\\eps$ variables over the whole time window). Due to the\nergodicity of $Y_x$, the initialization of $Y^\\eps_n$ is not crucial\nto the performance of the algorithm. It is however convenient to use\n$Y^\\eps_{n+1}(0) = Y^\\eps_n((n+ 1/\\lambda)\\Delta t)$, since this\nreinitialization leads to the interpretation of the HMM scheme given\nin~\\eqref{e:hmm_cp} below.\n\n\\item (Macro step) Use the time series from the micro step to update\n  $x^\\eps_n$ to $x^\\eps_{n+1}$ via\n\\begin{equ}\n  \\label{e:macro}\n  x^{\\eps}_{n+1} = x^\\eps_n + \\lambda \\int_{n\\Delta\n    t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n,Y^\\eps_n(s)) ds\\;.\n\\end{equ}\n\nNote that we do not require $Y^\\eps_n$ over the whole $\\Delta t$ time\nstep, but only a fraction of the step large enough for $Y^\\eps_n$ to\nmix. Indeed, if $\\eps$ is small enough, we have the approximate equality\n\n\\begin{equ}\n\\frac{\\lambda}{ \\Delta t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta\n  t} \nf(x^\\eps_n,Y^\\eps_n(s)) ds \\approx\n\\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n,Y^\\eps_n(s))\nds \n\\end{equ}\nsince both sides are close the the ergodic mean\n$\\int f(x^\\eps_n , y) d\\mu_{x^\\eps_n}(y )$. \n\\end{enumerate} \nClearly, the efficiency of the methods comes from the fact that we do\nnot need to compute the fast variables on the whole time interval\n$I_{n,\\Delta t}$ but only a $1/\\lambda$ fraction of it. Hence\n$\\lambda$ should be considered the \\emph{speed-up factor} of HMM.\n\nAs already stated, the algorithm above  is a high-level version,\nin that one must do further approximations to make the method\nimplementable. For example, one typically must specify some\napproximation scheme to integrate \\eqref{e:micro}, for instance with\nEuler-Maruyama we compute the time series by\n\\begin{equ}\\label{e:micro_euler}\ny^\\eps_{n,m+1} = y^\\eps_{n,m} + \\frac{\\delta t}{\\eps}\ng(x^\\eps_n,y^\\eps_{n,m}) \n+ \\sqrt{\\frac{\\delta t}{\\eps}} \\sigma (x^\\eps_n, y^\\eps_{n,m}) \\xi_{n,m} \\;,\n\\end{equ}\nwhere $0 \\leq m \\leq M$ is the index within the micro step,\n$\\xi_{n,m}$ are i.i.d. standard Gaussians and the micro-scale step\nsize $\\delta t$ is much smaller than the macro-scale step size\n$\\Delta t$. In the macro step, we would similarly have\n\\begin{equ}\\label{e:macro_euler}\nx^\\eps_{n+1} = x^\\eps_n + \\Delta t  \\, F_n(x_n^\\eps) \n\\end{equ}\nwhere $F_n(x) = \\frac{1}{M} \\sum_{m=1}^M f(x, y^\\eps_{n,m})$ and\n$ M = \\Delta t / (\\delta t \\lambda)$.\n\nThe following observation, which is taken from \\cite{fatkullin04},\nwill allow us to easily describe the average and fluctuations of the\nabove method. On each interval $I_{n,\\Delta t}$, the high-level HMM\nscheme described above is equivalently given by\n$x^\\eps_{n+1} = X^\\eps_n((n+1)\\Delta t)$, where $X^\\eps_n$ solves the\nsystem\n\\begin{equs}\\label{e:hmm_cp}\n\\frac{dX^\\eps_n}{dt} &= f(x^\\eps_n,{\\widetilde{Y}}^\\eps_n) \\\\\nd{\\widetilde{Y}}^\\eps_n &= \\frac{1}{\\eps \\lambda} b(x^\\eps_n,{\\widetilde{Y}}^\\eps_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda}}\\sigma (x^\\eps_n,{\\widetilde{Y}}^\\eps_n) dB\\;,\n\\end{equs}\ndefined on the interval $n\\Delta t \\leq t \\leq (n+1)\\Delta t$, with\nthe initial condition $X^\\eps_n(n\\Delta t) = x^\\eps_n$. This can be\nchecked by a simple rescaling of time. It is clear that the efficiency\nof HMM essentially comes from saying that the fast-slow system is not\ndrastically changed if one replaces $\\eps$ with the slightly larger,\nbut still very small $\\eps \\lambda$. \n\n\\section{Average and fluctuations in HMM methods}\n\\label{s:hmm_fluctuations}\n\nIn this section we investigate whether the limit theorems discussed\nin Section \\ref{s:fs}, i.e. the averaging principle, the CLT\nfluctuations and the LDP fluctuations, are also valid in the HMM\napproximation a fast-slow system. We will see that the averaging\nprinciple is the only property that holds, and that both types of\nfluctuations are \\emph{inflated} by the HMM method.\n\n\\subsection{Averaging}\\label{s:hmm_avg}\nBy construction, HMM-type schemes capture the correct averaging\nprinciple. More precisely, if we take $\\eps \\to 0$ then the sequence\n$x^\\eps_n$ converges to some ${\\bar{x}}_n$, where ${\\bar{x}}_n$ is a numerical\napproximation of the true averaged system ${\\bar{X}}$. If this numerical\napproximation is well-posed, the limits $\\eps \\to 0$ and\n$\\Delta t \\to 0$ commute with one another. Hence the HMM approximation\n$x^\\eps_n$ is consistent, in that it features approximately the same\naveraging behavior as the original fast-slow system.\n\nWe will argue the claim by induction. Suppose that for some $n\\geq 0$ we know that $\\lim_{\\eps \\to 0} x^\\eps_n = {\\bar{x}}_n$  (the $n=0$ claim is trivial, since they are both simply the initial condition). Then, using the representation \\eqref{e:hmm_cp} we know that $x^\\eps_{n+1} = X^\\eps_n((n+1)\\Delta t)$ where $X^\\eps_n(n\\Delta t) = x^\\eps_n$. Since \\eqref{e:hmm_cp} is a fast-slow system of the form \\eqref{e:fs} we can apply the averaging principle from Section \\ref{s:fs}. In particular it follows that $X^\\eps_n \\to {\\bar{X}}_n$ uniformly (and almost surely) on $I_{n,\\Delta t}$, where ${\\bar{X}}_n$ satisfies the averaged ODE\n\\begin{equ}\n\\frac{d{\\bar{X}}_n}{dt} = \\int f({\\bar{x}}_n , y) \\mu_{ {\\bar{x}}_n} (dy ) = F({\\bar{x}}_n)\\;.\n\\end{equ}  \nSince the right hand side is a constant, it follows that $x^\\eps_{n+1} \\to {\\bar{x}}_{n+1}$ as $\\eps \\to 0$, where\n\\begin{equ}\n{\\bar{x}}_{n+1} = {\\bar{x}}_n + F( {\\bar{x}}_n) \\Delta t \\;.\n\\end{equ} \nThis is nothing more than the Euler approximation of the true averaged variables ${\\bar{X}}$, which completes the induction and hence the claim. \n\\par\nIntroducing an integrator in to the micro-step will make things more complicated, as the invariant measures appearing will be those of the discretized fast variables. In \\cite{mattingly02} it is shown that discretizations of SDEs often do not possess the ergodic properties of the original system. For those situations where no such issues arise, rigorous arguments concerning this scenario, including rates of convergence for the schemes, are given in \\cite{liu05}.  \n\n\n\\subsection{Small fluctuations}\\label{s:hmm_clt}\nFor HMM-type methods, the CLT fluctuations about the average become inflated by a factor of $\\sqrt{\\lambda}$. That is, if we define\n\\begin{equ}\nz^\\eps_{n+1} = \\frac{x^\\eps_{n+1} - {\\bar{x}}_{n+1}}{\\sqrt{\\eps}} \\;,\n\\end{equ}\nthen as $\\eps\\to 0$, the fluctuations described by $z^\\eps_{n+1}$ are not consistent with \\eqref{e:fs_clt}, but rather with the SDE \n\\begin{equ}\\label{e:hmm_clt_z}\ndZ = B({\\bar{X}}) Z dt + \\sqrt{\\lambda} \\eta({\\bar{X}}) dV\\;, \\quad Z(0) = 0\n\\end{equ} \nwhere ${\\bar{X}}$ satisfies the correct averaged system. \n\\par\nAs above, by consistency we mean that when we take $\\eps\\to 0$, the sequence $\\{z^\\eps_n\\}_{n\\geq 0}$ converges to some well-posed discretization of the SDE \\eqref{e:hmm_clt_z}. Since $Z(0) = 0$, it is easy to see that the solution to this equation is simply $\\sqrt{\\lambda}$ times the solution of \\eqref{e:fs_clt}. Hence the fluctuations of the HMM-type scheme are inflated by a factor of $\\sqrt{\\lambda}$.  \n\\par\nIt is convenient to look instead at the rescaled fluctuations \n\\begin{equ}\n{\\hat{z}}^\\eps_n = z^\\eps_n / \\sqrt{\\lambda} =  \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps \\lambda}}\\;,\n\\end{equ}\nsince this allows us to reproduce the argument from Section \\ref{s:fs_clt}, with $\\eps ' = \\eps \\lambda$ playing the role of $\\eps$. We will again argue by induction, assuming for some $n\\geq 0$ that ${\\hat{z}}^\\eps_n \\to {\\hat{z}}_n$ as $\\eps \\to 0$ (the $n=0$ case is trivial). \n\\par\nThe rescaled fluctuations are given by ${\\hat{z}}^\\eps_{n+1} = Z^\\eps_n((n+1)\\Delta t)$ where $Z^\\eps_n (t) = (X^\\eps_n(t) - {\\bar{X}}_n(t)) / \\sqrt{\\eps \\lambda}$ and $X^\\eps_n(t)$ is governed by the system \\eqref{e:hmm_cp} with initial condition $X^\\eps_n(n\\Delta t) = x^\\eps_n$ and ${\\bar{X}}_n$ satisfies\n\\begin{equ}\n\\frac{d{\\bar{X}}_n}{dt} = F({\\bar{x}}_n) \n\\end{equ}\nwith initial condition ${\\bar{X}}_n (n\\Delta t) = {\\bar{x}}_n$. We can then obtain the reduced equations for the pair $(X_n^\\eps, Z^\\eps_n)$ by arguing exactly as in Section \\ref{s:fs}. Indeed, the triple $({\\bar{X}}_n, Z^\\eps_n, {\\widetilde{Y}}^\\eps_n)$ is governed by the system \n\\begin{equs}\n\\frac{d{\\bar{X}}_n}{dt} &= F({\\bar{x}}_n)\\\\\n\\frac{d {\\widehat{Z}}^\\eps_n}{dt} & = \\frac{1}{\\sqrt{\\eps \\lambda }} {\\tilde{f}} ({\\bar{x}}_n, {\\widetilde{Y}}^\\eps_n) + \\nabla_x f ({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) {\\hat{z}}_n + O(\\sqrt{\\eps \\lambda })\t   \\\\\nd{\\widetilde{Y}}^\\eps_n & = \\frac{1}{\\eps \\lambda} b({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda }} \\nabla_x b({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) {\\hat{z}}_n dt + \\frac{1}{\\sqrt{\\eps \\lambda }}\\sigma ({\\bar{x}}^\\eps_n, {\\widetilde{Y}}^\\eps_n) dW + O(1)\n\\end{equs}\nFrom here on we can carry out the calculation precisely as in Section \\ref{s:fs_clt}, with the added convenience of the vector fields no longer depending on $x$ as a variable. In doing so we obtain ${\\widehat{Z}}^\\eps_n \\to {\\widehat{Z}}_n$ (in distribution) as $\\eps \\to 0$, where\n\\begin{equ}\nd{\\widehat{Z}}_n =  B_0({\\bar{x}}_n) {\\hat{z}}_n dt + \\eta({\\bar{x}}_n) dV\\;,\n\\end{equ}   \nwith the initial condition defined recursively by ${\\widehat{Z}}_n (n\\Delta t) ={\\hat{z}}_n$. Using the fact that ${\\hat{z}}_{n+1} = {\\widehat{Z}}_n((n+1)\\Delta t)$, we obtain\n\\begin{equ}\n{\\hat{z}}_{n+1} = {\\hat{z}}_n + B_0 ({\\bar{x}}_n) {\\hat{z}}_n \\Delta t + \\eta({\\bar{x}}_n) \\sqrt{\\Delta t} \\xi_n\n\\end{equ} \nwhere $\\xi_n$ are iid standard Gaussians. Hence we obtain the\nEuler-Maruyama scheme for the correct CLT \\eqref{e:fs_clt}. However,\nsince ${\\hat{z}}^\\eps_n$ describes the rescaled fluctuations, we see that\nthe true fluctuations $z^\\eps_n$ of HMM are consistent with the\ninflated \\eqref{e:hmm_clt_z}.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Large fluctuations}\n\\label{s:hmm_ldp}\n\nAs with the CLT, the LDP of the HMM scheme is not consistent with the true LDP of the fast-slow system, but rather a rescaled version of the true LDP. In particular, define $u_{\\lambda,\\Delta t}$ by   \n\\begin{equ}\nu_{\\lambda , \\Delta t}(t,x) = \\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_{x} \\exp \\bigg(\\frac{1}{\\eps} \\vphi( x^\\eps_{n+1} ) \\bigg)\n\\end{equ}\nfor $t \\in I_{n,\\Delta t}$. If the $O(1)$ fluctuations of HMM were consistent with those of the fast-slow system, we would expect $u_{\\lambda,\\Delta}$ to converge to the solution of \\eqref{e:fs_ldp_hj} as $\\Delta t \\to 0$. Instead, we find that  as $\\Delta t\\to 0$,  $u_{\\lambda,\\Delta t}(t,x)$ converges to the solution to the Hamilton-Jacobi equation\n\\begin{equ}\\label{e:hmm_ldp}\n\\del_t u_\\lambda  = \\frac{1}{\\lambda} {\\pazocal{H}} (x ,\\lambda \\nabla u_\\lambda)\\;\\quad\\; \\quad u_\\lambda(0,x) = \\vphi(x) \\;.\n\\end{equ}\nIn light of the discussion in Section \\ref{s:fs_ldp}, the reverse Varadhan lemma suggests that the HMM scheme is consistent with the wrong LDP. Before proving this claim, we first discuss some implications.  \n\\par\n{{The rescaled Hamilton-Jacobi equation implies that the action functional for HMM will be a rescaled version of that for the true fast-slow system. \n\n\n\n\n\n\n\n\nIndeed, it is easy to see that the Langrangian corresponding to HMM simplifies to \n\\begin{equ}\n\\widehat{{\\mathcal{L}}}(x,\\beta) := \\sup_{\\theta} \\left( \\theta \\cdot \\beta - \\frac{1}{\\lambda} {\\pazocal{H}}(x,\\lambda\\theta) \\right) = \\frac{1}{\\lambda} {\\mathcal{L}}(x,\\beta)\\;,\n\\end{equ}\nwhere ${\\mathcal{L}}$ is the Lagrangian for the true fast-slow system. Thus, the action of the HMM approximation is given by $\\widehat{{\\mathcal{S}}}_{[0,T]} = \\lambda^{-1} {\\mathcal{S}}_{[0,T]}$ where ${\\mathcal{S}}$ is the action of the true fast-slow system. \\par\nIn particular, it follows immediately from the definition that the HMM approximation has quasi-potential $\\widehat{{\\mathcal{V}}}(x,y) = \\lambda^{-1} {\\mathcal{V}}(x,y)$, where ${\\mathcal{V}}$ is the true quasi-potential. As a consequence, the escape times for the HMM scheme will be\ndrastically faster than those of the fast-slow system. In the\nterminology of Section \\ref{s:fs_ldp}, if we let\n$\\tau^{\\eps,\\Delta t}$ be the escape time for the HMM scheme then for\n$\\eps, \\Delta t \\ll 1$ we expect\n\\begin{equ}\\label{e:hmm_ldp_escape}\n  {\\mathbf{E}} \\tau^{\\eps,\\Delta t} \\asymp\\exp\\Big( \\frac{1}{\\eps \\lambda}{\\mathcal{V}}\n  (x^*, \\del D) \\Big) \\;.\n\\end{equ} \nwhere $\\asymp$ log-asymptotic equality.  Thus, the log-expected\nescape times are decreasing proportionally with $\\lambda$. On the other hand, since the HMM action is a multiple of the true action, the minimizers will be unchanged by the HMM approximation. Hence the large deviation transition pathways will be unchanged by the HMM approximation. }}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo justify the claim for $u_{\\lambda,\\Delta t}$ \\eqref{e:hmm_ldp}, we first introduce\nsome notation. Let $S_{t}^{(\\alpha)}$ be the semigroup associated with\nthe Hamilton-Jacobi equation\n\\begin{equ}\\label{e:hmm_hj_alpha}\n\\del_t v(t,x) = {\\pazocal{H}} (\\alpha, \\nabla v(t,x))\\;,\n\\end{equ}\nnotice that this is the same as the true Hamilton-Jacobi equation\n\\eqref{e:fs_ldp_hj} but with the first argument of the Hamiltonian now\nfrozen as a parameter $\\alpha$. The necessity of the parameter\n$\\alpha$ is due to the fact that in the system for\n$(X^\\eps_n, Y^\\eps_n)$, the $x$ variable in the fast process is frozen\nto its value at the left end point of the interval, and hence is\ntreated as a parameter on each interval.\n\n\n\n\nWe also introduce the operator\n$S_{t} \\psi (x) = S^{(\\alpha)}_{t} \\psi (x) |_{\\alpha = x}$ and also\n$S_{\\lambda, t} = \\lambda^{-1} S_{ t} (\\lambda \\cdot)$. In this\nnotation, it is simple to show that\n\\begin{equ}\\label{e:hmm_ldp_Qn}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_n)  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\lambda, \\Delta t})^n \\vphi(x) \\right)\\;.\n\\end{equ}\nWe will verify \\eqref{e:hmm_ldp_Qn} by induction, starting with the\n$n=1$ case. Since, on the interval $I_{0,\\Delta t}$, the pair\n$(X^\\eps_0, {\\widetilde{Y}}^\\eps_0)$ is a fast-slow system of the form\n\\eqref{e:fs} with $\\eps$ replaced by $\\eps \\lambda$, it follows from\nSection \\ref{s:fs_ldp} that $X^\\eps_0$ satisfies an LDP with action\nfunctional derived from the Hamiltonian-Jacobi equation\n\\eqref{e:hmm_hj_alpha}, with the parameter $\\alpha$ set to the value\nof $X^\\eps_0$ at the left endpoint, which is $X^\\eps_0(0) = x$. Hence,\nit follows from Varadhan's lemma that for any suitable\n$\\psi : \\reals^d \\to \\reals$\n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1} \\psi (X^\\eps_0(\\Delta t))\\right) \\asymp \\exp \\left( (\\eps \\lambda)^{-1} S_{\\Delta t}^{(\\alpha)} \\psi(x)|_{\\alpha = x}  \\right) \\;.\n\\end{equ}\nHence, since $x_1^\\eps = X^\\eps_0(\\Delta t)$ with $X^\\eps_0(0) = x$, we have\n\\begin{equs}\n{\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_1) \\right) &= {\\mathbf{E}}_x \\exp \\left((\\eps\\lambda)^{-1} \\lambda \\vphi (X^\\eps_1(\\Delta t))\\right)\\\\ &\\asymp \\exp \\left((\\eps \\lambda)^{-1} S^{(\\alpha)}_{\\Delta t}(\\lambda \\vphi)(x)|_{\\alpha = x} \\right) = \\exp \\left(\\eps^{-1} S_{\\lambda, \\Delta t}\\vphi(x) \\right)\n\\end{equs}\nas claimed. Now, suppose \\eqref{e:hmm_ldp_Qn} holds for all $k$ with $n \\geq k \\geq 1$, then \n\\begin{equ}\\label{e:hmm_lpd_Q1a}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right)  = {\\mathbf{E}}_x {\\mathbf{E}}_{x_1^\\eps} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) \\;.\n\\end{equ} \n\nBy the inductive hypothesis, we have that\n\\begin{equ}\\label{e:hmm_lpd_Q1b}\n{\\mathbf{E}}_{x_1^\\eps} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right)\\;.\n\\end{equ}\nApplying \\eqref{e:hmm_lpd_Q1b} under the expectation in\n\\eqref{e:hmm_lpd_Q1a} (see Remark~\\ref{rmk:under_exp}) we see that\n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) = {\\mathbf{E}}_x {\\mathbf{E}}_{x^\\eps_1} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right)  \\asymp {\\mathbf{E}}_x \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right)\\;.\n\\end{equ}\nNow applying the inductive hypothesis with $n=1$ and $\\psi (\\cdot) = (S_{\\lambda,\\Delta t})^n \\vphi(\\cdot) $\n\\begin{equ}\n{\\mathbf{E}}_x \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right) \\asymp \\exp\\left(\\eps^{-1} S_{\\lambda,\\Delta t} (S_{\\lambda,\\Delta t})^n \\vphi(x) \\right)\\;,\n\\end{equ}\nwhich completes the induction.\n\\par\nBy definition, we therefore have $u_{\\lambda,\\Delta t}(t,x) = (S_{\\lambda,\\Delta t})^n \\vphi (x)$ when $t \\in I_{n,\\Delta t}$. All that remains is to argue that $u_{\\lambda,\\Delta t}$ converges to the solution of \\eqref{e:hmm_ldp} as $\\Delta t \\to 0$. But this can be seen from the expansion of the semigroup \n\\begin{equs}\n\\frac{u_{\\lambda,\\Delta t}(t + \\Delta t,x) - u_{\\lambda,\\Delta t}(t,x)}{\\Delta t} &= \n\\frac{(S_{\\lambda,\\Delta t})^{n+1}\\vphi (x)  - (S_{\\lambda,\\Delta t})^{n}\\vphi (x)}{\\Delta t}\\label{e:hmm_ldp_expansion} \\\\  &= \\frac{S_{\\lambda,\\Delta t} (S_\\Delta t)^n \\vphi (x) - (S_{\\lambda,\\Delta t})^n \\vphi (x)}{\\Delta t}\\\\ &= \\lambda^{-1} {\\pazocal{H}} (\\alpha , \\lambda \\nabla (S_{\\lambda,\\Delta t} )^n \\vphi(x) )|_{\\alpha = x} + O(\\Delta t) \\\\ \n& = \\lambda^{-1} {\\pazocal{H}} (x , \\lambda \\nabla u_{\\lambda,\\Delta t}(t,x))) + O(\\Delta t)\n\\end{equs}\nwhich yields the desired limiting equation. \n\n \\begin{rmk}\n\\label{rmk:under_exp}\nRegarding the operation of taking the log-asymptotic result inside the\nexpectation, one can find such calculations done rigorously in (for\ninstance) \\cite[Lemma 4.3]{fw12}.\n \\end{rmk}\n\n\\begin{rmk}\n\\label{rmk:rescaling}\nFrom the discussion above, it appears that the mean transition time\ncan be estimated from HMM upon exponential rescaling, see\n\\eqref{e:hmm_ldp_escape}. This is true, but only at the level of the\n(rough) log-asymptotic estimate of this time. How to rescale the\nprefactor is by no means obvious. As we will see below PHMM avoids\nthis issue altogether since it does not necessitate any rescaling.\n \\end{rmk}\n\n \n \\section{Parallelized HMM}\n\\label{s:phmm}\n \n There is a simple variant of the above HMM-type scheme which captures\n the correct average behavior and  fluctuations, both at the level of the CLT\n and LDP. In a usual HMM type method, the key approximation is given\n by\n\\begin{equ}\\label{e:hmm_approx}\n\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds \\approx \\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds\\;,\n\\end{equ}\nwhich only requires computation of the fast variables on the interval $[n\\Delta t , (n+1/\\lambda)\\Delta t]$. This approximation is effective at replicating averages, but poor at replicating fluctuations. Indeed,  for each $j$, the time series $Y^\\eps_{n}$ on the interval $[(n+j/\\lambda)\\Delta t ,(n+(j+1)/\\lambda)\\Delta t]$ is replaced with an identical copy of the time series from the interval $[n \\Delta t ,(n+1/\\lambda)\\Delta t]$. This introduces strong correlations between random variables that should be essentially independent. Parallelized HMM avoids this issue by employing the approximation\n\\begin{equ}\n\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds \\approx \\sum_{j=1}^\\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n , Y^{\\eps,j}_n(s))ds\\;,\n\\end{equ}\nwhere $Y^{\\eps,j}_n$ are for each $j$ independent copies of the time series computed in \\eqref{e:hmm_approx}. Due to their independence, each copy of the fast variables can be computed in parallel, hence we refer to the method as parallel HMM (PHMM). The method is summarized below. \n \n  \\begin{enumerate}\n  \\item (Micro step) On the interval $I_{n,\\Delta t}$, simulate\n    $\\lambda$ independent copies of the of the fast-variables, each\n    copy simulated precisely as in the usual HMM. That is, let\n\\begin{equ}\\label{e:microp}\nY^{\\eps,j}_n = Y^{\\eps,j}_n(n\\Delta t) + \\frac{1}{\\eps} \\int_{n\\Delta t}^t g(x^\\eps_n,Y^{\\eps,j}_n(s))ds +  {\\frac{1}{\\sqrt{\\eps}}}\\int_{n\\Delta t}^t \\sigma (x^\\eps_n, Y^{\\eps,j}_n(s)) dW_j(s) \n\\end{equ}\nfor $j=1,\\dots,\\lambda$ with $W_j$ independent Brownian motions. As\nwith ordinary HMM, we will not require the time series of the whole\ninterval $I_{n,\\Delta t}$ but only over the subset\n$[n\\Delta t, (n + 1/\\lambda)\\Delta t )$.\n\\item (Macro step) Use the time series from the micro step to update\n  $x^\\eps_n$ to $x^\\eps_{n+1}$ by\n\\begin{equ}\\label{e:macrop}\nx^\\eps_{n+1} = x^\\eps_n + \\sum_{j=1}^\\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n,Y^{\\eps,j}_n(s)) ds\\;. \n\\end{equ}\n\\end{enumerate} \n\nAs with the HMM-type method, it will be convenient to write PHMM as a\nfast-slow system (when restricted to an interval $I_{n,\\Delta\n  t}$).\nAkin to \\eqref{e:hmm_cp}, it is easy to verify that the parallel HMM\nscheme is described by the system\n\\begin{equs}\\label{e:phmm_cp}\n\\frac{dX^\\eps_{n}}{dt} &= \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) \\\\\nd{\\widetilde{Y}}^\\eps_{n,j} &= \\frac{1}{\\eps \\lambda} b(x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda}}\\sigma (x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) dW_j\\;,\n\\end{equs}\nfor $j=1,\\dots,\\lambda$ with the initial condition $X^\\eps_{n}(n\\Delta t) = x^\\eps_n$. \n\n\\section{Average and fluctuations in parallelized HMM}\n\\label{s:phmm_fluctuations}\n\nIn this section we check that the averaged behavior and the\nfluctuations in the PHMM method are consistent with those in the original fast\nslow system.\n\\subsection{Averaging}\nProceeding exactly as in Section \\ref{s:hmm_avg}, it follows that as $\\eps \\to 0$ the PHMM scheme $x^\\eps_{n+1}$ converges to ${\\bar{x}}_{n+1} = {{\\bar{X}}}_{n} ((n+1)\\Delta t) $ where\n\\begin{equ}\\label{e:phmm_avg}\n\\frac{d{\\bar{X}}_{n}}{d t} = \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda F ({\\bar{x}}_n) =  F({\\bar{x}}_n)\n\\end{equ}\nwith initial condition ${\\bar{X}}_{n} (n \\Delta t) = {\\bar{x}}_n$. Hence, we\nare in the exact same situation as with ordinary HMM, so the averaged\nbehavior is consistent with that of the original fast slow system.\n\n\n\\subsection{Small fluctuations}\\label{s:phmm_clt}\nWe now show that the fluctuations\n\\begin{equ}\nz^\\eps_{n} = \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps}}\n\\end{equ}\nare consistent with the correct CLT fluctuations, described by \\eqref{e:fs_clt}. As in Section \\ref{s:hmm_clt}, we instead look at the rescaled fluctuations\n\\begin{equ}\n{\\hat{z}}^\\eps_{n} = \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps \\lambda }}\\;.\n\\end{equ}\nIn particular we will show that these rescaled fluctuations are consistent with \n\\begin{equ}\\label{e:phmm_clt_a}\nd{\\widehat{Z}} = B_0({\\bar{X}}){\\widehat{Z}} dt + \\lambda^{-1/2} \\eta({\\bar{X}}) dV\\;.\n\\end{equ}\nThe claim for $z^{\\eps}$ will follows immediately from the claim for ${\\hat{z}}^\\eps$. \n\\par\nWe have that ${\\hat{z}}^\\eps_{n+1} = {\\widehat{Z}}^\\eps_{n} ((n+1)\\Delta t)$ where\n\\begin{equ}\n{\\widehat{Z}}^\\eps_{n}(t)=  \\frac{X^\\eps_{n}(t) - {\\bar{X}}_{n}(t)}{\\sqrt{\\eps \\lambda}} \n\\end{equ} \nwith $X^\\eps_{n}$ given by the system \\eqref{e:phmm_cp} and ${\\bar{X}}_{n}$ given by the averaged equation \\eqref{e:phmm_avg}. As in Section \\ref{s:hmm_clt}, we derive a system for the triple $({\\bar{X}}_{n}, {\\widehat{Z}}^\\eps_{n}, {\\widetilde{Y}}^\\eps_{n})$, where now the fast process has $\\lambda$ independent components ${\\widetilde{Y}}^\\eps_{n} = ({\\widetilde{Y}}^{\\eps,1}_{n}, \\dots,{\\widetilde{Y}}^{\\eps,\\lambda}_{n})$:\n\\begin{equs}\\label{e:phmm_clt_triple}\n\\frac{d{\\bar{X}}_{n}}{dt} &= F({\\bar{x}}_n)\\\\\n\\frac{d {\\widehat{Z}}^\\eps_{n}}{dt} & = \\frac{1}{\\sqrt{\\eps \\lambda }} \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda {\\tilde{f}} ({\\bar{x}}_n, {\\widetilde{Y}}^{\\eps,j}_{n}) + \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\nabla_x f ({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) {\\hat{z}}_n + O(\\sqrt{\\eps \\lambda })\t   \\\\\nd{\\widetilde{Y}}^{\\eps,j}_{n} & = \\frac{1}{\\eps \\lambda} b({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) dt + \\frac{1}{\\sqrt{\\eps \\lambda }} \\nabla_x b({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) {\\hat{z}}_n dt + \\frac{1}{\\sqrt{\\eps \\lambda }}\\sigma ({\\bar{x}}_n, {\\widetilde{Y}}^{\\eps,j}_{n}) dW_j + O(1)\\;.\n\\end{equs}\n With a modicum added difficulty, we can now argue as in Section \\ref{s:fs_clt} with $\\eps ' = \\eps \\lambda$ playing the role of $\\eps$. The invariant measure $\\mu_x^\\lambda (dy)$ associated with the generator of $Y^\\eps_n$ is now the product measure\n\\begin{equ}\n\\mu_x^\\lambda (dy_1,\\dots,dy_\\lambda) = \\mu_x(dy_1) \\dots \\mu_x (dy_\\lambda)\n\\end{equ}\nwhere $\\mu_x$ is the invariant measure associated with ${\\mathcal{L}}_0$ from Section \\ref{s:fs_clt}. This product structure simplifies the seemingly complicated expressions arising in the perturbation expansion of \\eqref{e:phmm_clt_triple}. In the setting of Section \\ref{s:fs_clt} we have that $u_0 = u_0 (x,z,t)$ and \n\\begin{equs}\\label{e:phmm_u1}\nu_1 (x,z,y,t)  = (-{\\mathcal{L}}_0^{(1)} - \\dots - {\\mathcal{L}}_0^{(\\lambda)})^{-1} {\\mathcal{L}}_1 u_0 (x,z,y,t)\\;,\n\\end{equs}\nwhere ${\\mathcal{L}}^{(j)}_0 = b({\\bar{x}}_n , y_j) \\nabla_{y_j} + \\frac{1}{2} \\sigma \\sigma^T ({\\bar{x}}_n , y_j ) : \\nabla_{y_j}^2$\n\n\nSince \n\\begin{equ}\n{\\mathcal{L}}_1 u_0 (x,z,y,t) = \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \n{\\tilde{f}} ({\\bar{x}}_n,y_j) \\cdot \\nabla_z u_0 (x,z,t)\\;,\n\\end{equ}\nthe Feynman-Kac representation of \\eqref{e:phmm_u1} yields\n\\begin{equ}\nu_1(x,z,y,t) = \\frac{1}{\\lambda} \\sum_{j=1}^{\\lambda} \n\\int_0^\\infty d\\tau {\\mathbf{E}}_{y_j} {\\tilde{f}} ({\\bar{x}}_n, Y_{{\\bar{x}}_n,j}(\\tau)) \n\\cdot \\nabla_z u_0(x,z,t)\\;.\n\\end{equ}\nThe equation for $u_0$ is now given by\n\\begin{equs}\n  \\label{e:phmm_u0_big}\n  \\del_t u_0  &= F({\\bar{x}}_n)\\nabla_x u_0 + \\int \\mu_{{\\bar{x}}_n} (dy_1)\\dots \\mu_{{\\bar{x}}_n}( dy_\\lambda) (\\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\nabla_x f({\\bar{x}}_n,y_j){\\hat{z}}_n) \\nabla_z u_0  \\\\  \n&\\quad  + \\int \\mu_{{\\bar{x}}_n} (dy_1)\\dots \\mu_{{\\bar{x}}_n}( dy_\\lambda) \\\\\n& \\qquad \\times \\Biggl( \\int_0^\\infty d\\tau \\left(\\frac{1}{\\lambda} \\sum_{j=1}^\\lambda {\\tilde{f}}({\\bar{x}}_n,y_j) \\right) \\otimes \\left( \\frac{1}{\\lambda}\\sum_{k=1}^\\lambda {\\mathbf{E}}_y{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))\\right) : \\nabla^2_z u_0 \\\\\n& \\qquad + \\sum_{j=1}^\\lambda (\\nabla_x b({\\bar{x}}_n,y_j) {\\hat{z}}_n) \\int_0^\\infty d\\tau \\, \\nabla_{y_j} \\frac{1}{\\lambda}\\sum_{k=1}^\\lambda{\\mathbf{E}}_{y_k}{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\Biggr)\\;.\n\\end{equs}\nBy expanding the product measure, the second term on the right hand side of \\eqref{e:phmm_u0_big} becomes \n\\begin{equ}\n  \\begin{aligned}\n    & \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\int \\mu_{{\\bar{x}}_n}( dy_j)\n    (\\nabla_x f({\\bar{x}}_n,y_j){\\hat{z}}_n) \\cdot\\nabla_z u_0\\\\\n    & = \\int\n    \\mu_{{\\bar{x}}_n}( dy_1) (\\nabla_x f({\\bar{x}}_n,y_1){\\hat{z}}_n) \\cdot\\nabla_z u_0\n    = (B_1({\\bar{x}}_n) {\\hat{z}}_n ) \\cdot\\nabla_z u_0 \\;,\n  \\end{aligned}\n\\end{equ}\n Likewise, using the independence of $Y^j_{x}$ for distinct $j$, the third term becomes \n\\begin{equs}\n\\frac{1}{\\lambda^2} \\sum_{j,k=1}^\\lambda & \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^k_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 \\\\\n& = \\frac{1}{\\lambda^2} \\sum_{j=1}^\\lambda  \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 \\\\\n & = \\frac{1}{\\lambda} \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^1_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^1_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 = \\frac{1}{\\lambda} \\eta({\\bar{x}}_n) \\eta({\\bar{x}}_n)^T : \\nabla_z^2 u_0 \\;.\n\\end{equs}\nwhere the expectation is taken over realizations of $Y^j_{x}$ with $Y^j_{x}(0) \\sim \\mu_x$. Finally, since the $\\nabla_{y_j} {\\mathbf{E}}_{y_k}$ term vanishes on the off-diagonal, the last term in \\eqref{e:phmm_u0_big} reduces to\n\\begin{equs}\n\\frac{1}{\\lambda}  \\sum_{j,k=1}^\\lambda & \\int_0^\\infty d\\tau \\int\n\\mu_{{\\bar{x}}_n} (dy_j)\\mu_{{\\bar{x}}_n} (dy_k)  (\\nabla_x b({\\bar{x}}_n,y_j)\n{\\hat{z}}_n)  \\cdot \\nabla_{y_j}\n{\\mathbf{E}}_{y_k}{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))  \\cdot \\nabla_z u_0 \\\\ \n&= \\frac{1}{\\lambda}  \\sum_{j=1}^\\lambda  \\int_0^\\infty d\\tau \\int \\mu_{{\\bar{x}}_n} (dy_j)\\mu_{{\\bar{x}}_n} (dy_k)  (\\nabla_x b({\\bar{x}}_n,y_j) {\\hat{z}}_n)  \\nabla_{y_j} {\\mathbf{E}}_{y_j}{\\tilde{f}}({\\bar{x}}_n,Y^j_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\\\ \n& = \\int_0^\\infty d\\tau \\int \\mu_{{\\bar{x}}_n} (dy_1)\\mu_{{\\bar{x}}_n} (dy_k)\n(\\nabla_x b(x,y_1) {\\hat{z}}_n)  \\nabla_{y_1}\n{\\mathbf{E}}_{y_1}{\\tilde{f}}({\\bar{x}}_n,Y^1_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\\\\n&= (B_2 ({\\bar{x}}_n ) {\\hat{z}}_n )  \\cdot \\nabla_{z} u_0 \\;.\n\\end{equs}\nIt follows immediately that the reduced equation for the pair\n$({\\bar{X}}_n, \\hat Z^\\eps_n)$ is \n\\begin{equs}\n\\frac{d{\\bar{X}}_{n}}{dt} &= F({\\bar{x}}_n) \\\\\nd{\\widehat{Z}}_n &= B_0({\\bar{x}}_n) {\\widehat{Z}}_n dt + \\lambda^{-1/2} \\eta ({\\bar{x}}_n) dV\\;,\n\\end{equs}\nwith initial conditions ${\\widehat{Z}}_n(n\\Delta t) = {\\hat{z}}_{n}$ and ${\\bar{X}}_n(n\\Delta t) = {\\bar{x}}_n$.  Hence we see that ${\\hat{z}}_{n+1}$ is described by \n\\begin{equ}\n{\\hat{z}}_{n+1} = {\\hat{z}}_n + B({\\bar{x}}_n) {\\hat{z}}_n \\Delta t +\n\\lambda^{-1/2}\\eta({\\bar{x}}_n) \\sqrt{\\Delta t} \\, \\xi_n\n\\end{equ}\nwhich is the Euler-Maruyama scheme for \\eqref{e:phmm_clt_a}.\n\n\n\n\n\n\n\\subsection{Large fluctuations}\n\\label{s:phmm_ldp}\n\nIn this section we show that the LDP for PHMM is consistent with the\ntrue LDP from Section \\ref{s:fs_ldp}. In particular, let\n\\begin{equ}\nu_{\\lambda,\\Delta t} (t,x) = \\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_{n}) \\right)\n\\end{equ}\nfor $t \\in I_{n,\\Delta t}$, where $x^\\eps_n$ is the PHMM approximation. We will argue that $u_{\\lambda,\\Delta t}(t,x) \\to u(t,x)$ as $\\Delta t \\to 0$, where $u$ solves the correct Hamilton-Jacobi equation \\eqref{e:fs_ldp_hj}.   \n\\par\nThe argument is a slight modification of that given in Section \\ref{s:hmm_ldp}. Before proceeding, we recall the notation $S^{(\\alpha)}_{\\Delta t}$ for the semigroup associated with the Hamilton-Jacobi equation\n\\begin{equ}\\label{e:phmm_ldp_hj}\n\\del_t u (t,x) = {\\pazocal{H}} (\\alpha , \\nabla u (t,x)) \\;,\n\\end{equ}\nwhere ${\\pazocal{H}}$ is the Hamiltonian defined by \\eqref{e:fs_ldp_ham}. We also define the operator $S_{\\Delta t} \\vphi(x) = S^{(\\alpha)}_{\\Delta t} \\vphi (x)|_{\\alpha = x}$. \n\nAs in Section \\ref{s:hmm_ldp}, the claim follows from the asymptotic statement\n\\begin{equ}\\label{e:phmm_ldp_dv}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_n)  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\Delta t})^n \\vphi(x) \\right)\\;, \\quad \\eps \\to 0\\;.\n\\end{equ}\nGiven \\eqref{e:phmm_ldp_dv}, by an identical argument to that started in Equation \\eqref{e:hmm_ldp_expansion}, it follows from \\eqref{e:phmm_ldp_dv} that $u_{\\lambda,\\Delta t} $ is indeed a numerical approximation of the solution to \\eqref{e:phmm_ldp_hj} and hence $u_{\\lambda,\\Delta t} \\to u$ as $\\Delta t \\to 0$.   \n\\par\nWe will verify \\eqref{e:phmm_ldp_dv} by induction, starting with the $n=1$ case. Since $(X^\\eps_, {\\widetilde{Y}}^\\eps_{0,1},\\dots,{\\widetilde{Y}}^\\eps_{0,\\lambda})$ is a fast-slow system of the form \\eqref{e:fs} with $\\eps$ replaced by $\\eps \\lambda$, it follows from Section \\ref{s:fs_ldp} (Varadhan's lemma) that \n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1} \\psi (X^\\eps_1(\\Delta t))\\right) \\asymp \\exp \\left( (\\eps \\lambda)^{-1} {\\widehat{S}}_{\\Delta t}^{(\\alpha)} \\psi(x)|_{\\alpha = x}  \\right) \\;,\n\\end{equ}\nwhere ${\\widehat{S}}_{\\Delta t}^{(\\alpha)}$ is the semigroup associated with $\\del_t v(t,x) = {\\widehat{\\pazocal{H}}} (\\alpha , \\nabla v(t,x))$ and\n\\begin{equ}\n{\\widehat{\\pazocal{H}}} (\\alpha,\\theta) = \\lim_{T\\to \\infty } T^{-1} \\log {\\mathbf{E}} \\exp \\left(\\theta \\cdot \\int_0^T d\\tau \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(\\alpha, Y^j_{\\alpha}(\\tau)) \\right)\\;.\n\\end{equ}\nHence we have\n\\begin{equ}\\label{e:phmm_ldp_semi}\n  \\begin{aligned}\n    {\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_1)\\right) &= {\\mathbf{E}}_x \\exp\n    \\left((\\eps \\lambda)^{-1} \\lambda\\vphi (X^\\eps_0(\\Delta t))\\right)\\\\\n    &\\asymp {\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1}\n      {\\widehat{S}}^{(\\alpha)}_{\\Delta t} (\\lambda \\vphi) (x)|_{\\alpha = x}\n    \\right)\n  \\end{aligned}\n\\end{equ}\nBut since $Y^j_{\\alpha}$ are iid for distinct $j$, the Hamiltonian ${\\widehat{\\pazocal{H}}}$ reduces to\n\\begin{equs}\n\\lim_{T\\to \\infty } T^{-1} &\\log {\\mathbf{E}} \\exp \\left(\\theta \\cdot \\int_0^T d\\tau \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(\\alpha, Y^j_{\\alpha}(\\tau)) \\right)\\\\ \n&= \\lambda \\lim_{T\\to \\infty } T^{-1} \\log {\\mathbf{E}} \\exp \\left(\\frac{\\theta}{\\lambda} \\cdot \\int_0^T d\\tau  f(\\alpha, Y^1_{\\alpha}(\\tau)) \\right) = \\lambda {\\pazocal{H}} (\\alpha , \\frac{\\theta}{\\lambda} )\\;.\n\\end{equs}\nIt follows that\n\\begin{equ}\n\\del_t \\left(\\lambda^{-1}{\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi)\\right) = \\lambda^{-1} {\\widehat{\\pazocal{H}}} (\\alpha , \\nabla ({\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi))) = {\\pazocal{H}} (\\alpha , \\lambda^{-1}\\nabla ({\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi)) )\n\\end{equ}\nand hence $\\lambda^{-1} {\\widehat{S}}^{(\\alpha)}_{\\Delta t} (\\lambda \\vphi) = S_{\\Delta t}^{(\\alpha)}\\vphi$. Combining this with \\eqref{e:phmm_ldp_semi} completes the claim for $n=1$. The proof of the inductive step for arbitrary $n\\geq 1$ follows identically to Section \\ref{s:hmm_ldp}. \n\n\n\n\\section{Numerical evidence}\n\\label{s:num}\n\nIn this section, we investigate the performance of the standard HMM\nand PHMM methods for systems with well understood fluctuations and\nmetastability properties. These simple experiments confirm that HMM\namplifies fluctuations, which can drastically change the system's metastable\nbehavior, and that the PHMM succeeds in avoiding these problems. In\nSection \\ref{s:num_clt} we investigate simple CLT fluctuations for a\nsimple quadratic potential systems, in Section \\ref{s:num_ldp} we look\nat large deviation fluctuations for a quartic double-well\npotential. Finally in Section \\ref{s:num_nondiff} we look at\nfluctuations for a non-diffusive double well potential, which has\nlarge deviation properties that cannot be captured by a so-called\n`small noise' diffusion.\n\n\n\\subsection{Small fluctuations} \n\\label{s:num_clt}\n\nWe examine the small CLT-type fluctuations by looking the following\nfast-slow system\n\n\\begin{equs}\n\\frac{dX}{dt} &= Y - X \\\\\ndY &= \\frac{\\theta}{\\eps} (\\mu X - Y) dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\n\nIt is simple to check that the averaged system is given by\n\\begin{displaymath}\n  \\frac{d{\\bar{X}}}{dt} = (\\mu - 1) {\\bar{X}}.\n\\end{displaymath}\nHence for $\\mu < 1$ the\naveraged system is a gradient flow in a quadratic potential centered\nat the origin.\n\nWe will first illustrate that the HMM-type method described in Section\n\\ref{s:hmm} inflates the $O(\\sqrt{\\eps})$ fluctuations about the\naverage by a factor of $\\sqrt{\\lambda}$. In Figure \\ref{fig:clt_hist}\nwe plot histograms of the slow variable $X$ for different speed-up\nfactors $\\lambda$. It is clear that the spread of the invariant\ndistribution is increasing with $\\lambda$. The profile remains\nGaussian but the variance is greatly inflated. In Figure\n\\ref{fig:clt_var} we plot the variance of the stationary time series\nfor $X$ as a function of $\\lambda$. The blue line is computed using\nHMM and the red line is computed using PHMM. As predicted by the\ntheory in Section \\ref{s:hmm_clt}, in the case of HMM the variance is\nincreasing linearly with $\\lambda$ and in the case of PHMM the\nvariance is approximately constant. Note that in this example, the CLT\ncaptures the large deviations as well. \n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.5\\textwidth]{clt_hist}\n\\caption{Histogram of $X$ variables. Parameters used are\n  $\\eps = 10^{-2}$, $\\delta t = 0.1$, $\\theta= 1$, $\\mu=0.5$,\n  $\\sigma= 5$, $T = 10^4$. }\n\\label{fig:clt_hist}\n\\end{center}\n\\end{figure}\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.8\\textwidth]{clt_var}\n\\caption{Comparing the stationary variance of HMM and PHMM as a function of $\\lambda$. Once again, we use parameters $\\eps = 10^{-2}$, $\\delta t = 0.1$, $\\theta= 1$, $\\mu=0.5$,  $\\sigma= 5$, $T = 10^4$.}\n\\label{fig:clt_var}\n\\end{center}\n\\end{figure}\n\n\n\\subsection{Large fluctuations}\n\\label{s:num_ldp}\n\nTo investigate the affect of parallelization on $O(1)$ deviations not\ncaptured by the CLT, we will look at a fast-slow system which exhibits\nmetastability. Hence it is natural to take\n\\begin{equs}\\label{e:num_fs2}\n\\frac{dX}{dt} &= Y - X^3 \\\\\ndY &= \\frac{\\theta}{\\eps} (\\mu X - Y) dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\nIt is simple to check that the averaged system is\n\\begin{displaymath}\n  \\frac{d{\\bar{X}}}{dt} = \\mu {\\bar{X}} - {\\bar{X}}^3.\n\\end{displaymath}\nHence for any $\\mu > 0$ the\naveraged system is a gradient flow in a symmetric double well\npotential, with stable equilibria at $\\pm \\sqrt{\\mu}$ and a saddle\npoint at the origin. The large fluctuations of the fast-slow system\ncan be investigated by looking at the first passage time for\ntransitions from a neighborhood of one stable equilibrium to the other.\n\nIn Figure \\ref{fig:ldp_mfpt} we compare the mean first passage time\nfor HMM and PHMM as a function of $\\lambda$. Even for $\\lambda = 2$,\nthe distinction between the two methods is vast, with the mean first\npassage time for HMM rapidly dropping off and for PHMM staying\napproximately constant.\n\nIn Figure \\ref{fig:ldp_hist} we compare respectively the stationary\ndistributions of the true fast-slow system, HMM ($\\lambda =5$) and\nPHMM ($\\lambda=5$). In the case of HMM, the energy barrier separating\nthe two metastable states is now overpopulated, which explains the\nrapid fall in mean first passage time. In the case of PHMM, the\nhistogram is indistinguishable from the true stationary distribution\n(with the exception of a slight asymmetry).\n\n\n\nIn Figure \\ref{fig:ldp_fpt_cdf} we plot the cumulative distributions\nfunction (CDF) for the first passage time, comparing that of the true\nfast-slow system, with HMM ($\\lambda=5$) and PHMM ($\\lambda=5$). We\nsee that the HMM first passage times are supported on a much faster\ntime scale than that of the true fast-slow system. In contrast, the\nCDF of PHMM is almost indistinguishable from that of the true\nfast-slow system. Hence PHMM is not just replicating the mean first\npassage time, but also the entire distribution of first passage times.\n\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.8\\textwidth]{hmm_phmm_mfpt}\n\\caption{The mean first passage time as a function of the speed-up\n  factor $\\lambda$, for HMM (red dotted) and PHMM (blue dotted). We\n  include the LDP predicted curve for the mean first passage time of HMM, as\n  discussed in Section~\\ref{s:hmm_ldp}. $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\n\\label{fig:ldp_mfpt}\n\\end{center}\n\\end{figure}\n\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{hist_1_5_5}\n\\caption{Histogram of $X$ variables. $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\n\\label{fig:ldp_hist}\n\\end{center}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\textwidth]{fpt_cdf_quarticslow.eps}\n\\caption{Cumulative\n  distribution functions for first passage times of the true model\n  (red) for \\eqref{e:num_fs2}, HMM with $\\lambda = 5$ (green) and PHMM\n  with $\\lambda = 5$ (blue). The parameters used are $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\\label{fig:ldp_fpt_cdf}\n\\end{figure}\n\n\n\\subsection{Asymmetric, non-diffusive fluctuations}\n\\label{s:num_nondiff}\n\nWe now compare HMM and PHMM for a multiscale model that also displays\nmetastability, but in which the large fluctuations cannot be\ncharacterized by a `small noise' Ito diffusion. In particular, the\nHamiltonian describing the LDP of the system is non-quadratic, as\nopposed the the previous systems. The system has been used\n\\cite{bouchet15} to illustrate the ineffectiveness of diffusion-type\napproximations for fast-slow systems. The fast-slow system is given by\n\\begin{equs}\\label{e:num_fs3}\n\\frac{dX}{dt} &= Y^2 - \\nu X\\\\\ndY &=  - \\frac{1}{\\eps} \\gamma(X) Y dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\n\nwhere $\\gamma(x) = x^4/10 - x^2 + 3$ .  The averaged equation for this\nsystem reads\n\n\\begin{displaymath}\n  \\frac{d\\bar X}{dt} = \\frac{\\sigma^2}{2\\gamma(\\bar X)} - \\nu \\bar X\n\\end{displaymath}\nFor $\\nu = 1$ and $\\sigma = \\sqrt{3}$, this averaged equation\npossesses two stable fixed points at $x\\approx 0.555$ and\n$x\\approx=2.459$ and one unstable fixed point at $x\\approx 2.459$. The\nthe rates of transition between these stable fixed points is captured\nby the LDP. By an elementary calculation \\cite{bouchet15}, the\nHamiltonian of this LDP is found to be non-quadratic and given by\n\n\\begin{displaymath}\n  {\\pazocal{H}} (x,\\theta) = - \\nu x \\theta +\\frac12\\left(\\gamma(x) -\n    \\sqrt{\\gamma^2(x)-2\\sigma^2 \\theta}\\right)\n\\end{displaymath}\n\nThe quasi-potential associated with this Hamiltonian satisfies\n$0= {\\pazocal{H}}(x,{\\mathcal{V}}')$, i.e.\n\n\\begin{displaymath}\n  {\\mathcal{V}}'(x) = \\frac{\\nu x \\gamma(x) - \\tfrac12 \\sigma^2}{\\nu^2x^2}\\;,\n\\end{displaymath}\nand is displayed in Figure \\ref{fig:num_quasi}. Whilst there is a\nsignificant barrier corresponding to left-to-right transitions, there\nis almost no barrier corresponding to right-to-left transitions.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{QP1}\\caption{The quasi-potential\n    ${\\mathcal{V}}(x)$ (red curve) and the one obtained from a quadratic approximation of\n    the Hamiltonian (orange curve). Also shown in blue is the\n    coefficient at the right-hand side of the reduced equation.\\label{fig:num_quasi}}\n\\end{figure}\n\nIn Figure \\ref{fig:ldp_fpt_cdf2} we plot CDFs of the first passage\ntimes: due to the asymmetry we plot separately the transitions from\nthe left-to-right and right-to-left.  For left-to-right transitions,\nthe HMM procedure drastically speeds up transitions because it \nenhances fluctuations: as is the case with the previous experiment,\nthe HMM transitions are supported on a timescale several orders of\nmagnitude faster than those of the true fast slow system. The PHMM\nmethod does not experience this problem and the distribution of first\npassage times agrees quite well with the true model.  For\nright-to-left transitions, PHMM shows similarly good agreement with\nthe true fast-slow system, but in contrast HMM is not too far off\neither. This can be accounted for by the `flatness' of the right\npotential well, meaning that increasing the amplitude of fluctuations\nwill only decrease the escape time by a linear multiplicative\nfactor. We note that the noise appearing in the CDF plots is due to\nthe scarcity of transitions occurring in the model \\eqref{e:num_fs3}.\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\textwidth]{fpt_cdf_quadraticslow_v1.eps}\n\\caption{Cumulative\n  distribution functions for first passage times of the true model for\n  \\eqref{e:num_fs3} (red), HMM with $\\lambda = 5$ (green) and PHMM\n  with $\\lambda = 5$ (blue). Left-to-right transitions on the left,\n  right-to-left transition on the right.  The parameters used are\n  $\\eps = 0.05$, $\\delta t = 0.5$, $\\nu= 1$, $\\sigma= \\sqrt{3}$,\n  $T = 1\\times 10^7$}\\label{fig:ldp_fpt_cdf2}\n\\end{figure}\n \n\n\\section{Discussion}\n\\label{s:discussion}\n\nWe have investigated HMM methods for fast-slow systems, in particular\ntheir ability (or lack thereof) to capture fluctuations, both small\n(CLT) and large (LDP). We found, both theoretically (Section\n\\ref{s:hmm_fluctuations}) and numerically (Section \\ref{s:num}), that\nthe amplitude of fluctuations is enhanced by an HMM-type method. In\nparticular with an HMM speed up factor~$\\lambda$, in the CLT the\nvariance of Gaussian fluctuations about the average is increased by a\nfactor~$\\lambda$ as well. In the LDP, the quasi-potential is decreased\nby a factor~$\\lambda$, leading to the first passage times being\nsupported on a time scale $\\lambda$ orders of magnitude smaller than\nin the true fast slow system. This inability to correctly capture\nfluctuations about the average suggests that HMM can be a poor\napproximation of fast-slow systems, particularly when metastable\nbehavior is important. {As noted in Section\n  \\ref{s:hmm_ldp}, although the fluctuations of HMM are enhanced, the\n  large deviation transition \\emph{pathways} remain faithful to the\n  true model. Thus we stress that HMM is a reliable method of finding\n  transition pathways in metastable systems, but not for simulating\n  their dynamics.}\n\nWe have introduced a simple modification of HMM, called parallel HMM\n(PHMM), which avoids these fluctuation issues. In particular, the PHMM\nmethod yields fluctuations that are consistent with the true fast slow\nsystem for any speed up factor $\\lambda$ (provided that we still have\n$\\eps \\lambda \\ll 1$), as was shown both theoretically (Section\n\\ref{s:phmm_fluctuations}) and numerically (Section \\ref{s:num}). The\nHMM method relies on computing one short burst of the fast variables,\nand inferring the statistical behavior of the fast-variables by\nextrapolating this short burst over a large time window. PHMM on the\nother hand computes an ensemble of $\\lambda$ short bursts, and infers\nthe statistics of the fast variables using the ensemble. Since the\nensemble members are independent, they can be computed in\nparallel. Hence if one has $\\lambda$ CPUs available, then the real\ncomputational time required in PHMM is identical to that in HMM. \n\nInterestingly, one can draw connections between the parallel method\nintroduced here and the tau-leaping method used in stochastic chemical\nkinetics \\cite{gillespie00}. The tau-leaping method is an\napproximation used to speed up simulation of stochastic fast-slow\nsystems of the type\n\\begin{equ}\\label{e:tau_leap}\nX^\\eps(t) = X^\\eps(0) + \\sum_{k=1}^m \\eps {\\pazocal{N}}_k \n\\left(  \\eps^{-1}\\int_0^t a_k(X^\\eps(s))ds  \\right)\\nu_k\\;,\n\\end{equ}\nwhere ${\\pazocal{N}}_k$ are independent unit rate Poisson processes, $\\nu_k$ are\nvectors in $\\reals^d$ and $a_k : \\reals^d \\to \\reals$. The system\n\\eqref{e:tau_leap} can be solved exactly by the stochastic simulation\nalgorithm (SSA), but when $\\eps$ is small this can be extremely\nexpensive, due to the Poisson clocks being reset each time a jump\noccurs. The tau-leaping procedure avoids this issue by chopping the\nsimulation window into sub-intervals of size $\\tau$ and on each\nsubinterval fixing the Poisson clocks to their value at the left\nendpoint. The speed-up is a result of the fact that one can simulate\nthe Poisson jumps in parallel, since their clocks are fixed over the\n$\\tau$ interval.\n\n\n\n\n\n\n\nAs a consequence of this analogy, one can check (using calculations\nsimilar to those found above) that the tau-leaping method also\ncaptures the fluctuations correctly, both at the level of the CLT and\nthat of the LDP. The former observation was made\nin~\\cite{anderson2011error}; to the best of our knowledge, the second\none is new.\n\n\nAs a final note, we stress that there are non-dissipative fast-slow\nsystems for which the PHMM will not be effective at capturing their\nlong time scale behavior, including metastability. These are system\nfor which the CLT and LDP hold on $O(1)$ timescale, but they either\ncannot be extended to longer time-scale (in the case of the CLT) or\nleads to trivial prediction on these time scales (in the case of the\nLDP). To clarify this point, take for example the fast-slow\nLangevin system\n\\begin{equs}\n  \\label{e:fs_hamiltonian}\n  \\dot{q}_1 &= p_1 \\quad \\dot{p}_1 = q_1 - q_1^3 + (q_2 - q_1) \\;, \\\\\n  \\dot{q}_2 &= \\eps^{-1} p_2 \\quad \\dot{p}_2 = \\eps^{-1}(q_1 - q_2) \n- \\eps^{-1} \\gamma p_2 + \\sqrt{2 \\eps^{-1}\\beta^{-1}\\gamma} \\eta \\;.\n\\end{equs}\n\nwhere $\\gamma>0$ and $\\beta>0$ are parameters.  For any value of\n$\\eps$, $\\gamma$, this system is invariant with respect to the Gibbs\nmeasure with Hamiltonian\n\\begin{equ}\n  H(q_1,q_2,p_1,p_2) = \\frac{1}{2}p_1^2 + \\frac{1}{2}p_2^2 \n  + \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 + \\frac{1}{2}(q_1-q_2)^2 \\;.\n\\end{equ}\nAs $\\eps\\to 0$, it is easy to check that the slow variables\n$(q_1,q_2)$ converge to the averaged system\n\\begin{equ}\n  \\label{e:hamilt_avg}\n  \\Dot{\\bar q}_1 = \\bar p_1 \\qquad \\Dot{\\bar p}_1 = - G'(\\bar q_1)\n\\end{equ}\nwhere the averaged vector field is the gradient of the free energy\n\\begin{equ}\n  G(q_1) = \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 + \\frac{1}{2}q_1^2 \n= - \\beta^{-1}\\log \\int \\exp (\\beta U(q_1,q_2))dq_2 \\;,\n\\end{equ}\nwith\n$U (q_1,q_2) = \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 +\n\\frac{1}{2}(q_1-q_2)^2$.\nLikewise, if we introduce \n\n\\begin{displaymath}\n  \\eta_1= \\frac{q_1-\\bar q_1}{\\sqrt{\\eps}}, \\qquad \n  \\zeta_1= \\frac{p_1-\\bar p_1}{\\sqrt{\\eps}},\n\\end{displaymath}\n\nthe CLT indicates that the evolution of these variables are captured\nby \n\n\n", "itemtype": "equation", "pos": 10355, "prevtext": "\n\nHere $F(x) = \\int f(x,y) \\mu_x(dy)$ is the averaged vector field, with\n$\\mu_x(dy)$ being the ergodic invariant measure of the fast variables\n$Y_x$ with a frozen $x$ variable. This averaging principle is akin to\nthe law of large number (LLN) in the present context and it suggests\nto simulate the evolution of the slow variables using~\\eqref{eq:2}\nrather than~\\eqref{e:fs_intro} when $\\eps$ is small. This requires to\nestimate $F(x)$, which typically has to be done on-the-fly given the\ncurrent value of the slow variables. To this end, note that if Euler's\nmethod with time step $\\Delta t$ is used as integrator for the slow\nvariables in~\\eqref{e:fs_intro}, we can approximate\n$X^\\eps(n\\Delta t)$ by $x_n$ satisfying the recurrence\n\n\\begin{equ}\n  \\label{e:scheme1}\n  x^\\eps_{n+1} = x^\\eps_{n} + \\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x_n^\\eps,Y^\\eps_{x_n^\\eps}(s))ds \\;,\n\\end{equ}\n\nwhere $Y^\\eps_{x}$ denotes the solution to the second equation\nin~\\eqref{e:fs_intro} with $X^\\eps$ kept fixed at the value~$x$.  If\n$\\eps $ is small enough that $\\Delta t / \\eps$ is larger than the\nmixing time of~$Y_x^\\eps$, the Birkhoff integral in~\\eqref{eq:B} is in\nfact close to the averaged coefficient in~\\eqref{eq:2}, in the sense\nthat\n\n\\begin{equ}\n  \\label{eq:B}\n  F(x) \\approx \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x,Y^\\eps_{x}(s))ds\\;.\n\\end{equ}\n\nTherefore \\eqref{e:scheme1} can also be thought of as an integrator\nfor the averaged equation~\\eqref{eq:2}. In fact, when $\\eps$ is small,\none can obtain a good approximation of $F(x)$ using only a fraction of\nthe macro time step. In particular, we expect that\n\n\\begin{equ}\n  \\label{e:intro_ergodic_approx}\n  \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t}\n  f(x,Y^\\eps_{x}(s))ds \\approx \\frac{\\lambda}{\\Delta\n    t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x,Y^\\eps_{x}(s))ds =: F_n(x)\n\\end{equ}\n\nwith $\\lambda\\ge 1$ provided that $\\Delta t / (\\eps \\lambda)$ remains\nlarger than the mixing time of $Y^\\eps_x$. This observation is at the\ncore of HMM-type methods -- in essence, they amount to replacing\n\\eqref{e:scheme1} by\n\n\\begin{equ}\n  \\label{e:scheme2}\n  x_{n+1} = x_{n} + \\Delta t\\, F_n(x_n) \\;.\n\\end{equ}\n\nSince the number of computations required to compute the effective\nvector field $F_n(x)$ is reduced by a factor~$\\lambda$, this is also\nthe speed-up factor for an HMM-type method.\n\nFrom the argument above, it is apparent that there is another,\nequivalent way to think about HMM-type methods, as was first pointed\nout in \\cite{fatkullin04} (see\nalso~\\cite{vanden2007hmm,weinan2009general,ariel2012multiscale,\n  ariel2013multiscale}.  Indeed, the integral defining $F_n(x)$\nin~\\eqref{e:intro_ergodic_approx} can be recast into an integral on\nthe full interval $[n\\Delta t,(n+1)\\Delta t]$ by a change of\nintegration variables, which amount to rescaling the internal clock of\nthe variables $Y^\\eps_x$. In other words, HMM-type methods can also be\nthought of as approximating the fast-slow system in~\\eqref{e:fs_intro}\nby\n\n\\begin{equs}\n  \\label{e:cp_intro}\n  \\frac{d{\\widetilde{X}}^\\eps}{dt} &= f({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^\\eps) \\\\\n  \\frac{d{\\widetilde{Y}}^\\eps}{dt} &= \\frac{1}{\\eps\\lambda} g({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^\\eps) \\;.\n \\end{equs} \n \n If $\\eps\\ll 1$, we can reasonably replace $\\eps$ with $\\eps \\lambda$,\n provided that this product still remains small -- in particular, the\n evolution of the slow variables in~\\eqref{e:cp_intro} is still\n captured by the limiting equation~\\eqref{eq:2}. Hence HMM-type\n methods are akin to artificial compressibility \\cite{chorin67} in\n fluid simulations and Car-Parrinello methods \\cite{car85} in\n molecular dynamics.\n\n\n The approximations in~\\eqref{e:intro_ergodic_approx} or\n \\eqref{e:cp_intro} are perfectly reasonable if we are only interested\n in staying faithful to the averaged equation~\\eqref{eq:2} -- that is\n to say, HMM-type approximations will have the correct law of large\n numbers (LLN) behavior. However, the fluctuations about that average\n will be enhanced by a factor of $\\lambda$. This is quite clear from\n the interpretation \\eqref{e:cp_intro}, since in the original model\n \\eqref{e:fs_intro}, the local fluctuations about the average are of\n order $\\sqrt{\\eps}$ and in \\eqref{e:cp_intro} they are of order\n $\\sqrt{\\eps \\lambda}$. The large fluctuations about the average\n caused by rare events are similarly inflated by a factor of\n $\\lambda$. This can be an issue, for example in metastable fast-slow\n systems, where the large fluctuations about the average determine the\n waiting times for transitions between metastable states. In\n particular we shall see that an HMM-type scheme drastically decreases\n these waiting times due to the enhanced fluctuations.\n\n In this article we propose a simple modification of HMM which\n corrects the problem of enhanced fluctuations. The key idea is\n to replace the approximation \\eqref{e:intro_ergodic_approx} with\n\n\\begin{equ}\n  \\label{e:intro_ergodic_approx2}\n  \\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x,Y^\\eps_{x}(s))ds\n  \\approx \n  \\sum_{j=1}^\\lambda\\frac{1}{\\Delta\n    t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} \n  f(x,Y^{\\eps,j}_{x}(s))ds\\;, \n\\end{equ}\n\nwhere each $Y^{\\eps,j}_x$ is an independent copy of $Y^\\eps_x$. By\ncomparing \\eqref{e:intro_ergodic_approx} with\n\\eqref{e:intro_ergodic_approx2}, we see that the first approximation\nis \\emph{essentially} replacing a sum of $\\lambda$ weakly correlated\nrandom variables with one random variable, multiplied by\n$\\lambda$. This introduces correlations that should not be there and\nin particular results in enhanced fluctuations. In\n\\eqref{e:intro_ergodic_approx2}, we instead replace the sum of\n$\\lambda$ weakly correlated random variables with a sum of $\\lambda$\nindependent random variables. This is a much more reasonable approximation to\nmake, since these random variables are becoming less and less\ncorrelated as $\\eps$ gets smaller. Since the terms appearing on the\nright hand side are independent of each other, they can be computed in\nparallel. Thus if one has $\\lambda$ CPUs available, then the real time\nof the computations is identical to HMM. For this reason, we call the\nmodification the parallelized HMM (PHMM). Note that, in analogy\nto~\\eqref{e:cp_intro}, one can interpret PHMM as approximating\n\\eqref{e:fs_intro} by the system\n\n  \\begin{equs}\n    \\label{e:cp_intro_p}\n  \\frac{d{\\widetilde{X}}^\\eps}{dt} &= \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^{\\eps,j}) \\\\\n  \\frac{d{\\widetilde{Y}}^{\\eps,j}}{dt} &= \\frac{1}{\\eps\\lambda}\n  g({\\widetilde{X}}^\\eps,{\\widetilde{Y}}^{\\eps,j}) \\quad \\text{for\n    $j=1,\\dots,\\lambda$}\\;.\n \\end{equs}  \n\n It is clear that this approximation will be as good as\n \\eqref{e:cp_intro} in term of the LLN, but in contrast with\n \\eqref{e:cp_intro}, we will show below that it captures the\n fluctuations about the average correctly, both in terms of small\n Gaussian fluctuations and large fluctuations describing rare\n events. A similar observation in the context of numerical\n homogenization was made in~\\cite{bal11,bal14}.\n \n\n The outline of the remainder of this article is as follows. In\n Section~\\ref{s:fs} we recall the averaging principle for stochastic\n fast-slow systems and describe how to characterize the fluctuations\n about this average, including local Gaussian fluctuations and large\n deviation principles. In Section \\ref{s:hmm} we recall the HMM-type\n methods. In Section \\ref{s:hmm_fluctuations} we show that they lead to\n enhanced fluctuations. In Section \\ref{s:phmm} we introduce the PHMM\n modification and in Section \\ref{s:phmm_fluctuations} show that this\n approximation yields the correct fluctuations, both in terms of local\n Gaussian fluctuations and large deviations. In Section \\ref{s:num} we\n test PHMM for a variety of simple models and conclude in Section\n \\ref{s:discussion} with a discussion.\n \n \n\\section{Average and fluctuations in fast-slow systems}\n\\label{s:fs}\n\nFor simplicity we will from here on assume that the fast variables are\nstochastic. This assumption is convenient, but not necessary, since\nall the averaging and fluctuation properties stated below are known to\nhold for large classes of fast-slow systems with deterministically\nchaotic fast variables \\cite{kifer92, dolgopyat04,\n  kelly15a,kelly15b}. The fast-slow systems we investigate are given\nby\n  \\begin{equs}\\label{e:fs}\n \\frac{dX^\\eps}{dt} &= f(X^\\eps,Y^\\eps) \\\\\n dY^\\eps &= \\frac{1}{\\eps} g(X^\\eps,Y^\\eps) dt + \\frac{1}{\\sqrt{\\eps}} \\sigma(X^\\eps,Y^\\eps) dW \\;\\;,\n \\end{equs}\n where $f : \\reals^d \\times \\reals^e \\to \\reals^d$,\n $g : \\reals^d \\times \\reals^e \\to \\reals^e$,\n $\\sigma: \\reals^d \\times \\reals^e \\to \\reals^e\\times \\reals^e$, and\n $W$ is a standard Wiener process in $\\reals^e$.  We assume that for\n every $x \\in \\reals^d$, the Markov process described by the SDE\n\n \n", "index": 3, "text": "\\begin{equation}\n  dY_x = b(x,Y_x) dt + \\sigma(x,Y_x)dW\n  \\label{eq:1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"dY_{x}=b(x,Y_{x})dt+\\sigma(x,Y_{x})dW\" display=\"block\"><mrow><mrow><mi>d</mi><mo>\u2062</mo><msub><mi>Y</mi><mi>x</mi></msub></mrow><mo>=</mo><mrow><mrow><mi>b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>Y</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>t</mi></mrow><mo>+</mo><mrow><mi>\u03c3</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>Y</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>W</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02147.tex", "nexttext": "\n\nand we can also derive an LDP for \\eqref{e:fs_hamiltonian} with\naction\n\n\n", "itemtype": "equation", "pos": 71895, "prevtext": "\nis ergodic, with invariant measure $\\mu_x$, and has sufficient mixing\nproperties. For full details on the necessary mixing properties, see\nfor instance \\cite{fw12}.\n\n\n\nIn this section we briefly recall the averaging principle for\nstochastic fast-slow systems and discuss two results that characterize\nthe fluctuations about the average, the central limit theorem (CLT)\nand the large deviations principle (LDP).\n\n\\subsection{Averaging principle}\nAs $\\eps\\to 0$, each realization of $X^\\eps$, with initial condition\n$X^\\eps(0) = x$, tends towards a trajectory of a deterministic system\n\n\\begin{equ}\n  \\label{e:fs_avg}\n  \\frac{d{\\bar{X}}}{dt} = F({\\bar{X}})  \\;, \\quad {\\bar{X}}(0) = x\\;,\n\\end{equ}\n\nwhere $F(x) = \\int f(x,y) \\mu_x(dy)$ and $\\mu_x$ is the invariant\nmeasure corresponding to the Markov process\n$dY_x = b(x,Y_x)dt + \\sigma(x,Y_x) dW$. The convergence is in an\nalmost sure and uniform sense:\n\\begin{equ}\n\\lim_{\\eps \\to 0}\\sup_{t \\leq T} |X^\\eps(t) - {\\bar{X}}(t)|  = 0\n\\end{equ}\nfor every fixed $T>0$, every choice of initial condition $x$ and\nalmost surely every initial condition $Y^\\eps(0)$ (a.s. with respect\nto $\\mu_x$) as well as every realization of the Brownian paths driving\nthe fast variables. Details of this convergence result in the setting\nabove are given in (for instance) \\cite[Chapter 7.2]{fw12}.\n\n\n\\subsection{Small fluctuations -- CLT}\n\\label{s:fs_clt}\n\nThe \\emph{small} fluctuations of $X^\\eps$ about the averaged system\n${\\bar{X}}$ can be understood by characterizing the limiting behavior of\n\\begin{equ}\nZ^\\eps : = \\frac{X^\\eps - {\\bar{X}}}{\\sqrt{\\eps}}\\;,\n\\end{equ}\nas $\\eps\\to 0$. It can be shown that the process $Z^\\eps$ converges in\ndistribution (on the space of continuous functions\n$C([0,T]; \\reals^d)$ endowed with the sup-norm topology) to a process\n$Z$ defined by the SDE\n\\begin{equ}\\label{e:fs_clt}\ndZ = B_0({\\bar{X}}) Z dt + \\eta ({\\bar{X}}) dV \\;, \\quad  Z(0) = 0\\;,\n\\end{equ} \nHere ${\\bar{X}}$ solves the averaged system in~\\eqref{e:fs_avg}, $V$ is a\nstandard Wiener process, $B_0 := B_1 + B_2$ with\n\\begin{equs}\nB_1 (x) &= \\int \\nabla_x f(x,y) \\mu_x(dy )\\\\\nB_2(x) &= \\int_0^\\infty d\\tau \\int \\mu_x(dy) \\nabla_y \n{\\mathbf{E}}_y \\bigg( {\\tilde{f}}(x,Y_x(\\tau))\\bigg)  \\nabla_x b(x,y)    \n\\end{equs}\nand \n\\begin{equ}\n\\eta(x)\\eta^T(x) = \\int_0^\\infty d\\tau\\, {\\mathbf{E}} {\\tilde{f}}(x, Y_x(0))\\otimes \n{\\tilde{f}} (x,Y_x(\\tau) \\;,\n\\end{equ} \nwhere ${\\tilde{f}}(x,y) = f(x,y) - F(x)$, ${\\mathbf{E}}_y$ denotes expectation over\nrealizations of $Y_x$ with $Y_x(0) = y$, and ${\\mathbf{E}}$ denotes expectation\nover realization of $Y_x$ with $Y_x(0) \\sim \\mu_x$. We include next a\nformal argument deriving this limit, as it will prove useful when\nanalyzing the multiscale approximation methods. We will replicate the\nargument given in \\cite{bouchet15}; a more complete and rigorous\nargument can be found in \\cite[Chapter 7.3]{fw12}.\n\n\nFirst, we write a system of equation for the triple\n$({\\bar{X}}, Z^\\eps, Y^\\eps)$ in the following approximated form, which\nuses nothing more than Taylor expansions of the original system\nin~\\eqref{e:fs_intro}:\n\\begin{equs}\n  \\frac{d{\\bar{X}}}{dt} &= F({\\bar{X}}) \\\\\n  \\frac{dZ^\\eps}{dt} & = \\frac{1}{\\sqrt{\\eps}} {\\tilde{f}}({\\bar{X}}, Y^\\eps) \n  + \\nabla_x f ({\\bar{X}}, Y^\\eps)  + O(\\sqrt{\\eps}) \\\\\n  dY^\\eps &= \\frac{1}{\\eps} b({\\bar{X}},Y^\\eps) dt + \\frac{1}{\\sqrt{\\eps}}\n  \\nabla_x b({\\bar{X}}, Y^\\eps) Z^\\eps dt + \\frac{1}{\\sqrt{\\eps} }\n  \\sigma({\\bar{X}}, Y^\\eps) dW + O(1)\\;.\n\\end{equs}\nWe now proceed with a classical perturbation expansion on the\ngenerator of the triple $({\\bar{X}},Z^\\eps,Y^\\eps)$. In particular we have\n${\\mathcal{L}}_\\eps = \\frac{1}{\\eps}{\\mathcal{L}}_{0} + \\frac{1}{\\sqrt{\\eps}}{\\mathcal{L}}_1 + {\\mathcal{L}}_2\n+ \\dots$ where\n\\begin{equs}\n{\\mathcal{L}}_0  &=  b(x,y)\\cdot\\nabla_y + a(x,y) : \\nabla_y^2\\\\\n{\\mathcal{L}}_1 &=  {\\tilde{f}}(x,y) \\cdot\\nabla_z + (\\nabla_x b(x,y) z) \\cdot \\nabla_y \\\\\n{\\mathcal{L}}_2 &= F(x) \\cdot \\nabla_x + (\\nabla_x f(x,y)z) \\cdot\\nabla_z\n\\end{equs}\nand $a=\\sigma\\sigma^T$. Let\n$u_\\eps(x,z,y,t) = {\\mathbf{E}}_{(x,z,y)} \\vphi({\\bar{X}}(t), Z^\\eps(t),Y^\\eps(t))$\nand introduce the ansatz\n$u_\\eps = u_0 + \\sqrt{\\eps} u_1 + \\eps u_2 + \\dots$. By substituting\n$u_\\eps$ into $\\del_t u_\\eps = {\\mathcal{L}}_\\eps u_\\eps$ and equating powers of\n$\\eps$ we obtain\n\\begin{equs}\nO(\\eps^{-1}) &: {\\mathcal{L}}_0 u_0 = 0 \\\\    \nO(\\eps^{-1/2}) &: {\\mathcal{L}}_0 u_1 = -{\\mathcal{L}}_1 u_0 \\\\\nO(\\eps^{-1}) &: \\del_t u_0 = {\\mathcal{L}}_2 u_0 + {\\mathcal{L}}_1 u_1 + {\\mathcal{L}}_0 u_2\\;.\n\\end{equs}\nFrom the $O(\\eps^{-1})$ identity, we obtain $u_0 = u_0 (x,z,t)$,\nconfirming that the leading order term is independent of $y$. By the\nFredholm alternative, the $O(\\eps^{-1/2})$ identity has a solution\n$u_1$ which has the Feynman-Kac representation\n\\begin{equ}\n  u_1(x,y,z)  = \\int_0^\\infty d\\tau \\, {\\mathbf{E}}_y \\left( {\\tilde{f}} (x,\n    Y_x(\\tau) ) \\right) \n  \\cdot\\nabla_z u_0 (x,z)\\;,\n\\end{equ}\nwhere $Y_x$ denotes the Markov process generated by ${\\mathcal{L}}_0$, i.e. the\nsolution of~\\eqref{eq:1}. Finally, if we average the $O(1)$ identity\nagainst the invariant measure corresponding to ${\\mathcal{L}}_0$, we obtain\n\\begin{equs}\n\\del_t u_0  &= F(x)\\nabla_x u_0 + \n\\int \\mu_x (dy) (\\nabla_x f(x,y)z) \\cdot\\nabla_z u_0  \\\\  \n& + \\int \\mu_x(dy) \\int_0^\\infty d\\tau {\\tilde{f}}(x,y) \\otimes \n{\\mathbf{E}}_y{\\tilde{f}}(x,Y_x(\\tau)) : \\nabla^2_z u_0 \\\\\n& + \\int \\mu_x (dy) (\\nabla_x b(x,y) z) \\int_0^\\infty d\\tau  \\,\\nabla_y \n{\\mathbf{E}}_y{\\tilde{f}}(x,Y_x(\\tau))  \\nabla_z u_0 \\;.\n\\end{equs}\nClearly, this is the forward Kolmogorov equation for the Markov\nprocess $({\\bar{X}}, Z)$ defined by\n\\begin{equs}\n\\frac{d{\\bar{X}}}{dt} &= F({\\bar{X}}) \\\\\nd Z &= B_0({\\bar{X}}) Z dt + \\eta ({\\bar{X}} ) dV\n\\end{equs}\nwith $B_0$ and $\\eta$ defined as above. \n\n\\subsection{Large fluctuations -- LDP}\n\\label{s:fs_ldp}\n\nA large deviation principle (LDP) for the fast-slow system\n\\eqref{e:fs} quantifies probabilities of $O(1)$ fluctuations of\n$X^\\eps$ away from the averaged trajectory ${\\bar{X}}$. The probability of\nsuch events vanishes exponentially quickly and as a consequence are\nnot accounted for by the CLT fluctuations, hence an LDP accounts for\nthe \\emph{rare events}.\n\nWe say that the slow variables $X^\\eps$ satisfy a \\emph{large\n  deviation principle} (LDP) with action functional ${\\mathcal{S}}_{[0,T]}$ if\nfor any set\n$\\Gamma \\subset \\{ \\gamma \\in C([0,T], \\reals^d) : \\gamma(0) = x \\}$\nwe have\n\\begin{equs}\\label{e:fs_ldp_def}\n-\\inf_{\\gamma \\in \\mathring{\\Gamma}} {\\mathcal{S}}_{[0,T]}(\\gamma) &\\leq \\liminf_{\\eps \\to 0} \\eps \\log {\\mathbf{P}} \\left( X^\\eps \\in \\Gamma \\right)\\\\ &\\leq \\limsup_{\\eps \\to 0} \\eps \\log {\\mathbf{P}}\\left( X^\\eps \\in \\Gamma \\right) \\leq -\\inf_{\\gamma \\in \\bar{\\Gamma}} {\\mathcal{S}}_{[0,T]} (\\gamma)\\;,\n\\end{equs}\nwhere $\\mathring{\\Gamma}$ and $\\bar{\\Gamma}$ denote the interior and\nclosure of $\\Gamma$ respectively.\n \nAn LDP also determines many important features of $O(1)$ fluctuations\nthat occur on large time scales, such as the probability of transition\nfrom one metastable set to another. For example, suppose that $X^\\eps$\nis known to satisfy an LDP with action functional ${\\mathcal{S}}_{[0,T]}$. Let\n$D$ be an open domain in $\\reals^d$ with smooth boundary $\\del D$\nand let $x^* \\in D$ be an asymptotically stable equilibrium for the\naveraged system $\\dot{{\\bar{X}}} = F({\\bar{X}})$. When $\\eps \\ll 1$, we expect\nthat a trajectory of $X^\\eps$ that starts in $D$ will tend towards the\nequilibrium $x^*$ and exhibit $O(\\sqrt{\\eps})$ fluctuations about the\nequilibrium -- these fluctuations are described by the CLT. On very\nlarge time scales, these small fluctuations have a chance to `pile up'\ninto an $O(1)$ fluctuation, producing behavior of the trajectory that\nwould be considered impossible for the averaged system. Such\nfluctuations are not accurately described by the CLT and requires the\nLDP instead. For example, the asymptotic behaviour of escape time from\nthe domain $D$,\n\\begin{equ}\n\\tau^\\eps = \\inf \\{t > 0 : X^\\eps(t) \\notin D \\}\\;,\n\\end{equ}\ncan be quantified in terms of the \\emph{quasi-potential} defined by\n\\begin{equ}\\label{e:quasi_def}\n{\\mathcal{V}} (x,y) = \\inf_{T > 0} \\inf_{\\gamma(0)=x , \\gamma(T) = y} {\\mathcal{S}}_{[0,T]} (\\gamma)\n\\end{equ}\nUnder natural conditions, it can be shown that for any $x\\in D$\n\\begin{equ}\n\\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_x \\tau^\\eps = \\inf_{y \\in \\del D} {\\mathcal{V}} (x^*, y) \\;.\n\\end{equ}\nHence the time it takes to pass from the neighborhood of one\nequilibrium to another may be quantified using the LDP. Details on the\nescape time of fast-slow systems can be found in \\cite[Chapter\n7.6]{fw12}.\n\nLDPs for fast-slow systems of the type \\eqref{e:fs} are well\nunderstood \\cite[Chapter 7.4]{fw12}. First define the Hamiltonian\n${\\pazocal{H}} : \\reals^d \\times \\reals^d \\to \\reals$ by\n\\begin{equ}\\label{e:fs_ldp_ham}\n{\\pazocal{H}} (x,\\theta) = \\lim_{T\\to \\infty} \\frac{1}{T} \\log {\\mathbf{E}}_y \\exp \\bigg( \\theta \\cdot \\int_0^T f(x,Y_x(s)) ds   \\bigg) \\;,\n\\end{equ} \nwhere $Y_{x}$ denotes the Markov process governed by $dY_x = b(x,Y_x) dt + \\sigma(x,Y_x) dW$. Let ${\\mathcal{L}} : \\reals^d \\times \\reals^d \\to \\reals$ be the Legendre transform of ${\\pazocal{H}}$:\n\\begin{equ}\\label{e:fs_ldp_legendre}\n{\\mathcal{L}} (x, \\beta) = \\sup_{\\theta} \\left( \\theta \\cdot \\beta - {\\pazocal{H}} (x, \\theta)    \\right) \\;.\n\\end{equ}\nThen the action functional is given by\n\\begin{equ}\\label{e:action_legendre}\n{\\mathcal{S}}_{[0,T]} (\\gamma) = \\int_0^T {\\mathcal{L}} (\\gamma(s), \\dot{\\gamma}(s)) ds\\;.\n\\end{equ} \nIt can also be shown that the function\n$u(t,x) = \\inf_{\\gamma(0) = x }{\\mathcal{S}}_{[0,t]} (\\gamma)$ satisfies the\nHamilton-Jacobi equation\n\\begin{equ}\\label{e:fs_ldp_hj}\n\\del_t u(t,x) = {\\pazocal{H}} (x ,\\nabla u(t,x)) \\;.\n\\end{equ}\nDonsker-Varadhan theory tells us that the connection between\nHamilton-Jacobi equations and LDPs is in fact much deeper. Firstly,\n\\emph{Varadhan's Lemma} states that if a process $X^\\eps$ is known to\nsatisfy an LDP with some associated Hamiltonian ${\\pazocal{H}}$, then for any\n$\\vphi : \\reals^d \\to \\reals$ we have the generalized Laplace\nmethod-type result\n\\begin{equ}\\label{e:fs_ldp_varadhan}\n\\lim_{\\eps \\to 0 } \\eps \\log {\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (X^\\eps (t))   \\right) = S_t \\vphi (x)\n\\end{equ}\nwhere $S_t$ is the semigroup associated with the Hamilton-Jacobi\nequation $\\del_t u = {\\pazocal{H}} (x,\\nabla u)$. Conversely, if it is known\nthat \\eqref{e:fs_ldp_varadhan} holds for all $(x,t)$ and a suitable\nclass of $\\vphi$, then the inverse Varadhan's lemma states that\n$X^\\eps$ satisfies an LDP with action functional given by\n\\eqref{e:fs_ldp_legendre}, \\eqref{e:action_legendre}. Hence we can use\n\\eqref{e:fs_ldp_varadhan} to determine the action functional for a\ngiven process.\n\nIn the next few sections, we will exploit both sides of Varadhan's\nlemma when investigating the large fluctuations of the HMM and related\nschemes. More complete discussions on Varadhan's Lemma can be found in\n\\cite[Chapters 4.3, 4.4]{dz09}.\n   \n\n\n \\section{HMM for fast-slow systems}\n\\label{s:hmm}\nWhen applied to the stochastic fast-slow system \\eqref{e:fs}, HMM-type\nschemes rely on the fact that the slow $X^\\eps$ variables, and the\ncoefficients that govern them, converge to a set of reduced variables\nas $\\eps$ tends to zero. We will describe a simplest version of the\nmethod below, which is more convenient to deal with mathematically. \n\nBefore proceeding, we digress briefly on notation. When referring to\ncontinuous time variables we will always use upper case symbols\n($X^\\eps,Y^\\eps$ etc) and when referring to discrete time\napproximations we will always use lower case symbols ($x^\\eps_n$,\n$y^\\eps_n$ etc). We will also encounter continuous time variables\nwhose definition depends on the integer $n$ for which we have\n$t \\in [n\\Delta t, (n+1)\\Delta t)$. We will see below that such\ncontinuous time variables are used to define discrete time\napproximations. In this situation we will use upper case symbols with\na subscript $n$ (eg. $X^\\eps_n$).\n\n\nLet us now describe a `high-level' version of HMM.  Fix a step size\n$\\Delta t$ and define the intervals\n$I_{n, \\Delta t}: = [n\\Delta t, (n+1)\\Delta t)$. On each interval\n$I_{n,\\Delta t}$ we update $x^\\eps_n \\approx X^\\eps(n\\Delta t)$ to\n$x^\\eps_{n+1} \\approx X^\\eps((n+1)\\Delta t)$ via an iteration of the\nfollowing two steps:\n \\begin{enumerate}\n \\item (Micro step) Integrate the fast variables over the interval\n   $I_{n,\\Delta t}$, with the slow variable frozen at\n   $X^\\eps = x^\\eps_n$. That is, the fast variables are approximated\n   by\n\\begin{equ}\\label{e:micro}\nY^\\eps_{n}(t) = Y^\\eps_n(n\\Delta t) + \\frac{1}{\\eps} \n\\int_{n\\Delta t}^t g(x^\\eps_n,Y_{n}^\\eps (s))ds \n+  {\\frac{1}{\\sqrt{\\eps}}}\\int_{n\\Delta t}^t \\sigma (x^\\eps_n, Y^{\\eps}_n(s)) dW(s) \n\\end{equ}\nfor $n\\Delta t \\leq t \\leq (n+ 1/\\lambda)\\Delta t $ with some\n$\\lambda \\geq 1$ (that is, we do not necessarily integrate the\n$Y_n^\\eps$ variables over the whole time window). Due to the\nergodicity of $Y_x$, the initialization of $Y^\\eps_n$ is not crucial\nto the performance of the algorithm. It is however convenient to use\n$Y^\\eps_{n+1}(0) = Y^\\eps_n((n+ 1/\\lambda)\\Delta t)$, since this\nreinitialization leads to the interpretation of the HMM scheme given\nin~\\eqref{e:hmm_cp} below.\n\n\\item (Macro step) Use the time series from the micro step to update\n  $x^\\eps_n$ to $x^\\eps_{n+1}$ via\n\\begin{equ}\n  \\label{e:macro}\n  x^{\\eps}_{n+1} = x^\\eps_n + \\lambda \\int_{n\\Delta\n    t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n,Y^\\eps_n(s)) ds\\;.\n\\end{equ}\n\nNote that we do not require $Y^\\eps_n$ over the whole $\\Delta t$ time\nstep, but only a fraction of the step large enough for $Y^\\eps_n$ to\nmix. Indeed, if $\\eps$ is small enough, we have the approximate equality\n\n\\begin{equ}\n\\frac{\\lambda}{ \\Delta t}\\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta\n  t} \nf(x^\\eps_n,Y^\\eps_n(s)) ds \\approx\n\\frac{1}{\\Delta t}\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n,Y^\\eps_n(s))\nds \n\\end{equ}\nsince both sides are close the the ergodic mean\n$\\int f(x^\\eps_n , y) d\\mu_{x^\\eps_n}(y )$. \n\\end{enumerate} \nClearly, the efficiency of the methods comes from the fact that we do\nnot need to compute the fast variables on the whole time interval\n$I_{n,\\Delta t}$ but only a $1/\\lambda$ fraction of it. Hence\n$\\lambda$ should be considered the \\emph{speed-up factor} of HMM.\n\nAs already stated, the algorithm above  is a high-level version,\nin that one must do further approximations to make the method\nimplementable. For example, one typically must specify some\napproximation scheme to integrate \\eqref{e:micro}, for instance with\nEuler-Maruyama we compute the time series by\n\\begin{equ}\\label{e:micro_euler}\ny^\\eps_{n,m+1} = y^\\eps_{n,m} + \\frac{\\delta t}{\\eps}\ng(x^\\eps_n,y^\\eps_{n,m}) \n+ \\sqrt{\\frac{\\delta t}{\\eps}} \\sigma (x^\\eps_n, y^\\eps_{n,m}) \\xi_{n,m} \\;,\n\\end{equ}\nwhere $0 \\leq m \\leq M$ is the index within the micro step,\n$\\xi_{n,m}$ are i.i.d. standard Gaussians and the micro-scale step\nsize $\\delta t$ is much smaller than the macro-scale step size\n$\\Delta t$. In the macro step, we would similarly have\n\\begin{equ}\\label{e:macro_euler}\nx^\\eps_{n+1} = x^\\eps_n + \\Delta t  \\, F_n(x_n^\\eps) \n\\end{equ}\nwhere $F_n(x) = \\frac{1}{M} \\sum_{m=1}^M f(x, y^\\eps_{n,m})$ and\n$ M = \\Delta t / (\\delta t \\lambda)$.\n\nThe following observation, which is taken from \\cite{fatkullin04},\nwill allow us to easily describe the average and fluctuations of the\nabove method. On each interval $I_{n,\\Delta t}$, the high-level HMM\nscheme described above is equivalently given by\n$x^\\eps_{n+1} = X^\\eps_n((n+1)\\Delta t)$, where $X^\\eps_n$ solves the\nsystem\n\\begin{equs}\\label{e:hmm_cp}\n\\frac{dX^\\eps_n}{dt} &= f(x^\\eps_n,{\\widetilde{Y}}^\\eps_n) \\\\\nd{\\widetilde{Y}}^\\eps_n &= \\frac{1}{\\eps \\lambda} b(x^\\eps_n,{\\widetilde{Y}}^\\eps_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda}}\\sigma (x^\\eps_n,{\\widetilde{Y}}^\\eps_n) dB\\;,\n\\end{equs}\ndefined on the interval $n\\Delta t \\leq t \\leq (n+1)\\Delta t$, with\nthe initial condition $X^\\eps_n(n\\Delta t) = x^\\eps_n$. This can be\nchecked by a simple rescaling of time. It is clear that the efficiency\nof HMM essentially comes from saying that the fast-slow system is not\ndrastically changed if one replaces $\\eps$ with the slightly larger,\nbut still very small $\\eps \\lambda$. \n\n\\section{Average and fluctuations in HMM methods}\n\\label{s:hmm_fluctuations}\n\nIn this section we investigate whether the limit theorems discussed\nin Section \\ref{s:fs}, i.e. the averaging principle, the CLT\nfluctuations and the LDP fluctuations, are also valid in the HMM\napproximation a fast-slow system. We will see that the averaging\nprinciple is the only property that holds, and that both types of\nfluctuations are \\emph{inflated} by the HMM method.\n\n\\subsection{Averaging}\\label{s:hmm_avg}\nBy construction, HMM-type schemes capture the correct averaging\nprinciple. More precisely, if we take $\\eps \\to 0$ then the sequence\n$x^\\eps_n$ converges to some ${\\bar{x}}_n$, where ${\\bar{x}}_n$ is a numerical\napproximation of the true averaged system ${\\bar{X}}$. If this numerical\napproximation is well-posed, the limits $\\eps \\to 0$ and\n$\\Delta t \\to 0$ commute with one another. Hence the HMM approximation\n$x^\\eps_n$ is consistent, in that it features approximately the same\naveraging behavior as the original fast-slow system.\n\nWe will argue the claim by induction. Suppose that for some $n\\geq 0$ we know that $\\lim_{\\eps \\to 0} x^\\eps_n = {\\bar{x}}_n$  (the $n=0$ claim is trivial, since they are both simply the initial condition). Then, using the representation \\eqref{e:hmm_cp} we know that $x^\\eps_{n+1} = X^\\eps_n((n+1)\\Delta t)$ where $X^\\eps_n(n\\Delta t) = x^\\eps_n$. Since \\eqref{e:hmm_cp} is a fast-slow system of the form \\eqref{e:fs} we can apply the averaging principle from Section \\ref{s:fs}. In particular it follows that $X^\\eps_n \\to {\\bar{X}}_n$ uniformly (and almost surely) on $I_{n,\\Delta t}$, where ${\\bar{X}}_n$ satisfies the averaged ODE\n\\begin{equ}\n\\frac{d{\\bar{X}}_n}{dt} = \\int f({\\bar{x}}_n , y) \\mu_{ {\\bar{x}}_n} (dy ) = F({\\bar{x}}_n)\\;.\n\\end{equ}  \nSince the right hand side is a constant, it follows that $x^\\eps_{n+1} \\to {\\bar{x}}_{n+1}$ as $\\eps \\to 0$, where\n\\begin{equ}\n{\\bar{x}}_{n+1} = {\\bar{x}}_n + F( {\\bar{x}}_n) \\Delta t \\;.\n\\end{equ} \nThis is nothing more than the Euler approximation of the true averaged variables ${\\bar{X}}$, which completes the induction and hence the claim. \n\\par\nIntroducing an integrator in to the micro-step will make things more complicated, as the invariant measures appearing will be those of the discretized fast variables. In \\cite{mattingly02} it is shown that discretizations of SDEs often do not possess the ergodic properties of the original system. For those situations where no such issues arise, rigorous arguments concerning this scenario, including rates of convergence for the schemes, are given in \\cite{liu05}.  \n\n\n\\subsection{Small fluctuations}\\label{s:hmm_clt}\nFor HMM-type methods, the CLT fluctuations about the average become inflated by a factor of $\\sqrt{\\lambda}$. That is, if we define\n\\begin{equ}\nz^\\eps_{n+1} = \\frac{x^\\eps_{n+1} - {\\bar{x}}_{n+1}}{\\sqrt{\\eps}} \\;,\n\\end{equ}\nthen as $\\eps\\to 0$, the fluctuations described by $z^\\eps_{n+1}$ are not consistent with \\eqref{e:fs_clt}, but rather with the SDE \n\\begin{equ}\\label{e:hmm_clt_z}\ndZ = B({\\bar{X}}) Z dt + \\sqrt{\\lambda} \\eta({\\bar{X}}) dV\\;, \\quad Z(0) = 0\n\\end{equ} \nwhere ${\\bar{X}}$ satisfies the correct averaged system. \n\\par\nAs above, by consistency we mean that when we take $\\eps\\to 0$, the sequence $\\{z^\\eps_n\\}_{n\\geq 0}$ converges to some well-posed discretization of the SDE \\eqref{e:hmm_clt_z}. Since $Z(0) = 0$, it is easy to see that the solution to this equation is simply $\\sqrt{\\lambda}$ times the solution of \\eqref{e:fs_clt}. Hence the fluctuations of the HMM-type scheme are inflated by a factor of $\\sqrt{\\lambda}$.  \n\\par\nIt is convenient to look instead at the rescaled fluctuations \n\\begin{equ}\n{\\hat{z}}^\\eps_n = z^\\eps_n / \\sqrt{\\lambda} =  \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps \\lambda}}\\;,\n\\end{equ}\nsince this allows us to reproduce the argument from Section \\ref{s:fs_clt}, with $\\eps ' = \\eps \\lambda$ playing the role of $\\eps$. We will again argue by induction, assuming for some $n\\geq 0$ that ${\\hat{z}}^\\eps_n \\to {\\hat{z}}_n$ as $\\eps \\to 0$ (the $n=0$ case is trivial). \n\\par\nThe rescaled fluctuations are given by ${\\hat{z}}^\\eps_{n+1} = Z^\\eps_n((n+1)\\Delta t)$ where $Z^\\eps_n (t) = (X^\\eps_n(t) - {\\bar{X}}_n(t)) / \\sqrt{\\eps \\lambda}$ and $X^\\eps_n(t)$ is governed by the system \\eqref{e:hmm_cp} with initial condition $X^\\eps_n(n\\Delta t) = x^\\eps_n$ and ${\\bar{X}}_n$ satisfies\n\\begin{equ}\n\\frac{d{\\bar{X}}_n}{dt} = F({\\bar{x}}_n) \n\\end{equ}\nwith initial condition ${\\bar{X}}_n (n\\Delta t) = {\\bar{x}}_n$. We can then obtain the reduced equations for the pair $(X_n^\\eps, Z^\\eps_n)$ by arguing exactly as in Section \\ref{s:fs}. Indeed, the triple $({\\bar{X}}_n, Z^\\eps_n, {\\widetilde{Y}}^\\eps_n)$ is governed by the system \n\\begin{equs}\n\\frac{d{\\bar{X}}_n}{dt} &= F({\\bar{x}}_n)\\\\\n\\frac{d {\\widehat{Z}}^\\eps_n}{dt} & = \\frac{1}{\\sqrt{\\eps \\lambda }} {\\tilde{f}} ({\\bar{x}}_n, {\\widetilde{Y}}^\\eps_n) + \\nabla_x f ({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) {\\hat{z}}_n + O(\\sqrt{\\eps \\lambda })\t   \\\\\nd{\\widetilde{Y}}^\\eps_n & = \\frac{1}{\\eps \\lambda} b({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda }} \\nabla_x b({\\bar{x}}_n , {\\widetilde{Y}}^\\eps_n) {\\hat{z}}_n dt + \\frac{1}{\\sqrt{\\eps \\lambda }}\\sigma ({\\bar{x}}^\\eps_n, {\\widetilde{Y}}^\\eps_n) dW + O(1)\n\\end{equs}\nFrom here on we can carry out the calculation precisely as in Section \\ref{s:fs_clt}, with the added convenience of the vector fields no longer depending on $x$ as a variable. In doing so we obtain ${\\widehat{Z}}^\\eps_n \\to {\\widehat{Z}}_n$ (in distribution) as $\\eps \\to 0$, where\n\\begin{equ}\nd{\\widehat{Z}}_n =  B_0({\\bar{x}}_n) {\\hat{z}}_n dt + \\eta({\\bar{x}}_n) dV\\;,\n\\end{equ}   \nwith the initial condition defined recursively by ${\\widehat{Z}}_n (n\\Delta t) ={\\hat{z}}_n$. Using the fact that ${\\hat{z}}_{n+1} = {\\widehat{Z}}_n((n+1)\\Delta t)$, we obtain\n\\begin{equ}\n{\\hat{z}}_{n+1} = {\\hat{z}}_n + B_0 ({\\bar{x}}_n) {\\hat{z}}_n \\Delta t + \\eta({\\bar{x}}_n) \\sqrt{\\Delta t} \\xi_n\n\\end{equ} \nwhere $\\xi_n$ are iid standard Gaussians. Hence we obtain the\nEuler-Maruyama scheme for the correct CLT \\eqref{e:fs_clt}. However,\nsince ${\\hat{z}}^\\eps_n$ describes the rescaled fluctuations, we see that\nthe true fluctuations $z^\\eps_n$ of HMM are consistent with the\ninflated \\eqref{e:hmm_clt_z}.\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Large fluctuations}\n\\label{s:hmm_ldp}\n\nAs with the CLT, the LDP of the HMM scheme is not consistent with the true LDP of the fast-slow system, but rather a rescaled version of the true LDP. In particular, define $u_{\\lambda,\\Delta t}$ by   \n\\begin{equ}\nu_{\\lambda , \\Delta t}(t,x) = \\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_{x} \\exp \\bigg(\\frac{1}{\\eps} \\vphi( x^\\eps_{n+1} ) \\bigg)\n\\end{equ}\nfor $t \\in I_{n,\\Delta t}$. If the $O(1)$ fluctuations of HMM were consistent with those of the fast-slow system, we would expect $u_{\\lambda,\\Delta}$ to converge to the solution of \\eqref{e:fs_ldp_hj} as $\\Delta t \\to 0$. Instead, we find that  as $\\Delta t\\to 0$,  $u_{\\lambda,\\Delta t}(t,x)$ converges to the solution to the Hamilton-Jacobi equation\n\\begin{equ}\\label{e:hmm_ldp}\n\\del_t u_\\lambda  = \\frac{1}{\\lambda} {\\pazocal{H}} (x ,\\lambda \\nabla u_\\lambda)\\;\\quad\\; \\quad u_\\lambda(0,x) = \\vphi(x) \\;.\n\\end{equ}\nIn light of the discussion in Section \\ref{s:fs_ldp}, the reverse Varadhan lemma suggests that the HMM scheme is consistent with the wrong LDP. Before proving this claim, we first discuss some implications.  \n\\par\n{{The rescaled Hamilton-Jacobi equation implies that the action functional for HMM will be a rescaled version of that for the true fast-slow system. \n\n\n\n\n\n\n\n\nIndeed, it is easy to see that the Langrangian corresponding to HMM simplifies to \n\\begin{equ}\n\\widehat{{\\mathcal{L}}}(x,\\beta) := \\sup_{\\theta} \\left( \\theta \\cdot \\beta - \\frac{1}{\\lambda} {\\pazocal{H}}(x,\\lambda\\theta) \\right) = \\frac{1}{\\lambda} {\\mathcal{L}}(x,\\beta)\\;,\n\\end{equ}\nwhere ${\\mathcal{L}}$ is the Lagrangian for the true fast-slow system. Thus, the action of the HMM approximation is given by $\\widehat{{\\mathcal{S}}}_{[0,T]} = \\lambda^{-1} {\\mathcal{S}}_{[0,T]}$ where ${\\mathcal{S}}$ is the action of the true fast-slow system. \\par\nIn particular, it follows immediately from the definition that the HMM approximation has quasi-potential $\\widehat{{\\mathcal{V}}}(x,y) = \\lambda^{-1} {\\mathcal{V}}(x,y)$, where ${\\mathcal{V}}$ is the true quasi-potential. As a consequence, the escape times for the HMM scheme will be\ndrastically faster than those of the fast-slow system. In the\nterminology of Section \\ref{s:fs_ldp}, if we let\n$\\tau^{\\eps,\\Delta t}$ be the escape time for the HMM scheme then for\n$\\eps, \\Delta t \\ll 1$ we expect\n\\begin{equ}\\label{e:hmm_ldp_escape}\n  {\\mathbf{E}} \\tau^{\\eps,\\Delta t} \\asymp\\exp\\Big( \\frac{1}{\\eps \\lambda}{\\mathcal{V}}\n  (x^*, \\del D) \\Big) \\;.\n\\end{equ} \nwhere $\\asymp$ log-asymptotic equality.  Thus, the log-expected\nescape times are decreasing proportionally with $\\lambda$. On the other hand, since the HMM action is a multiple of the true action, the minimizers will be unchanged by the HMM approximation. Hence the large deviation transition pathways will be unchanged by the HMM approximation. }}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo justify the claim for $u_{\\lambda,\\Delta t}$ \\eqref{e:hmm_ldp}, we first introduce\nsome notation. Let $S_{t}^{(\\alpha)}$ be the semigroup associated with\nthe Hamilton-Jacobi equation\n\\begin{equ}\\label{e:hmm_hj_alpha}\n\\del_t v(t,x) = {\\pazocal{H}} (\\alpha, \\nabla v(t,x))\\;,\n\\end{equ}\nnotice that this is the same as the true Hamilton-Jacobi equation\n\\eqref{e:fs_ldp_hj} but with the first argument of the Hamiltonian now\nfrozen as a parameter $\\alpha$. The necessity of the parameter\n$\\alpha$ is due to the fact that in the system for\n$(X^\\eps_n, Y^\\eps_n)$, the $x$ variable in the fast process is frozen\nto its value at the left end point of the interval, and hence is\ntreated as a parameter on each interval.\n\n\n\n\nWe also introduce the operator\n$S_{t} \\psi (x) = S^{(\\alpha)}_{t} \\psi (x) |_{\\alpha = x}$ and also\n$S_{\\lambda, t} = \\lambda^{-1} S_{ t} (\\lambda \\cdot)$. In this\nnotation, it is simple to show that\n\\begin{equ}\\label{e:hmm_ldp_Qn}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_n)  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\lambda, \\Delta t})^n \\vphi(x) \\right)\\;.\n\\end{equ}\nWe will verify \\eqref{e:hmm_ldp_Qn} by induction, starting with the\n$n=1$ case. Since, on the interval $I_{0,\\Delta t}$, the pair\n$(X^\\eps_0, {\\widetilde{Y}}^\\eps_0)$ is a fast-slow system of the form\n\\eqref{e:fs} with $\\eps$ replaced by $\\eps \\lambda$, it follows from\nSection \\ref{s:fs_ldp} that $X^\\eps_0$ satisfies an LDP with action\nfunctional derived from the Hamiltonian-Jacobi equation\n\\eqref{e:hmm_hj_alpha}, with the parameter $\\alpha$ set to the value\nof $X^\\eps_0$ at the left endpoint, which is $X^\\eps_0(0) = x$. Hence,\nit follows from Varadhan's lemma that for any suitable\n$\\psi : \\reals^d \\to \\reals$\n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1} \\psi (X^\\eps_0(\\Delta t))\\right) \\asymp \\exp \\left( (\\eps \\lambda)^{-1} S_{\\Delta t}^{(\\alpha)} \\psi(x)|_{\\alpha = x}  \\right) \\;.\n\\end{equ}\nHence, since $x_1^\\eps = X^\\eps_0(\\Delta t)$ with $X^\\eps_0(0) = x$, we have\n\\begin{equs}\n{\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_1) \\right) &= {\\mathbf{E}}_x \\exp \\left((\\eps\\lambda)^{-1} \\lambda \\vphi (X^\\eps_1(\\Delta t))\\right)\\\\ &\\asymp \\exp \\left((\\eps \\lambda)^{-1} S^{(\\alpha)}_{\\Delta t}(\\lambda \\vphi)(x)|_{\\alpha = x} \\right) = \\exp \\left(\\eps^{-1} S_{\\lambda, \\Delta t}\\vphi(x) \\right)\n\\end{equs}\nas claimed. Now, suppose \\eqref{e:hmm_ldp_Qn} holds for all $k$ with $n \\geq k \\geq 1$, then \n\\begin{equ}\\label{e:hmm_lpd_Q1a}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right)  = {\\mathbf{E}}_x {\\mathbf{E}}_{x_1^\\eps} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) \\;.\n\\end{equ} \n\nBy the inductive hypothesis, we have that\n\\begin{equ}\\label{e:hmm_lpd_Q1b}\n{\\mathbf{E}}_{x_1^\\eps} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right)\\;.\n\\end{equ}\nApplying \\eqref{e:hmm_lpd_Q1b} under the expectation in\n\\eqref{e:hmm_lpd_Q1a} (see Remark~\\ref{rmk:under_exp}) we see that\n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right) = {\\mathbf{E}}_x {\\mathbf{E}}_{x^\\eps_1} \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_{n+1})  \\right)  \\asymp {\\mathbf{E}}_x \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right)\\;.\n\\end{equ}\nNow applying the inductive hypothesis with $n=1$ and $\\psi (\\cdot) = (S_{\\lambda,\\Delta t})^n \\vphi(\\cdot) $\n\\begin{equ}\n{\\mathbf{E}}_x \\exp\\left(\\eps^{-1} (S_{\\lambda,\\Delta t})^n \\vphi(x^\\eps_1) \\right) \\asymp \\exp\\left(\\eps^{-1} S_{\\lambda,\\Delta t} (S_{\\lambda,\\Delta t})^n \\vphi(x) \\right)\\;,\n\\end{equ}\nwhich completes the induction.\n\\par\nBy definition, we therefore have $u_{\\lambda,\\Delta t}(t,x) = (S_{\\lambda,\\Delta t})^n \\vphi (x)$ when $t \\in I_{n,\\Delta t}$. All that remains is to argue that $u_{\\lambda,\\Delta t}$ converges to the solution of \\eqref{e:hmm_ldp} as $\\Delta t \\to 0$. But this can be seen from the expansion of the semigroup \n\\begin{equs}\n\\frac{u_{\\lambda,\\Delta t}(t + \\Delta t,x) - u_{\\lambda,\\Delta t}(t,x)}{\\Delta t} &= \n\\frac{(S_{\\lambda,\\Delta t})^{n+1}\\vphi (x)  - (S_{\\lambda,\\Delta t})^{n}\\vphi (x)}{\\Delta t}\\label{e:hmm_ldp_expansion} \\\\  &= \\frac{S_{\\lambda,\\Delta t} (S_\\Delta t)^n \\vphi (x) - (S_{\\lambda,\\Delta t})^n \\vphi (x)}{\\Delta t}\\\\ &= \\lambda^{-1} {\\pazocal{H}} (\\alpha , \\lambda \\nabla (S_{\\lambda,\\Delta t} )^n \\vphi(x) )|_{\\alpha = x} + O(\\Delta t) \\\\ \n& = \\lambda^{-1} {\\pazocal{H}} (x , \\lambda \\nabla u_{\\lambda,\\Delta t}(t,x))) + O(\\Delta t)\n\\end{equs}\nwhich yields the desired limiting equation. \n\n \\begin{rmk}\n\\label{rmk:under_exp}\nRegarding the operation of taking the log-asymptotic result inside the\nexpectation, one can find such calculations done rigorously in (for\ninstance) \\cite[Lemma 4.3]{fw12}.\n \\end{rmk}\n\n\\begin{rmk}\n\\label{rmk:rescaling}\nFrom the discussion above, it appears that the mean transition time\ncan be estimated from HMM upon exponential rescaling, see\n\\eqref{e:hmm_ldp_escape}. This is true, but only at the level of the\n(rough) log-asymptotic estimate of this time. How to rescale the\nprefactor is by no means obvious. As we will see below PHMM avoids\nthis issue altogether since it does not necessitate any rescaling.\n \\end{rmk}\n\n \n \\section{Parallelized HMM}\n\\label{s:phmm}\n \n There is a simple variant of the above HMM-type scheme which captures\n the correct average behavior and  fluctuations, both at the level of the CLT\n and LDP. In a usual HMM type method, the key approximation is given\n by\n\\begin{equ}\\label{e:hmm_approx}\n\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds \\approx \\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds\\;,\n\\end{equ}\nwhich only requires computation of the fast variables on the interval $[n\\Delta t , (n+1/\\lambda)\\Delta t]$. This approximation is effective at replicating averages, but poor at replicating fluctuations. Indeed,  for each $j$, the time series $Y^\\eps_{n}$ on the interval $[(n+j/\\lambda)\\Delta t ,(n+(j+1)/\\lambda)\\Delta t]$ is replaced with an identical copy of the time series from the interval $[n \\Delta t ,(n+1/\\lambda)\\Delta t]$. This introduces strong correlations between random variables that should be essentially independent. Parallelized HMM avoids this issue by employing the approximation\n\\begin{equ}\n\\int_{n\\Delta t}^{(n+1)\\Delta t} f(x^\\eps_n , Y^\\eps_{n}(s))ds \\approx \\sum_{j=1}^\\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n , Y^{\\eps,j}_n(s))ds\\;,\n\\end{equ}\nwhere $Y^{\\eps,j}_n$ are for each $j$ independent copies of the time series computed in \\eqref{e:hmm_approx}. Due to their independence, each copy of the fast variables can be computed in parallel, hence we refer to the method as parallel HMM (PHMM). The method is summarized below. \n \n  \\begin{enumerate}\n  \\item (Micro step) On the interval $I_{n,\\Delta t}$, simulate\n    $\\lambda$ independent copies of the of the fast-variables, each\n    copy simulated precisely as in the usual HMM. That is, let\n\\begin{equ}\\label{e:microp}\nY^{\\eps,j}_n = Y^{\\eps,j}_n(n\\Delta t) + \\frac{1}{\\eps} \\int_{n\\Delta t}^t g(x^\\eps_n,Y^{\\eps,j}_n(s))ds +  {\\frac{1}{\\sqrt{\\eps}}}\\int_{n\\Delta t}^t \\sigma (x^\\eps_n, Y^{\\eps,j}_n(s)) dW_j(s) \n\\end{equ}\nfor $j=1,\\dots,\\lambda$ with $W_j$ independent Brownian motions. As\nwith ordinary HMM, we will not require the time series of the whole\ninterval $I_{n,\\Delta t}$ but only over the subset\n$[n\\Delta t, (n + 1/\\lambda)\\Delta t )$.\n\\item (Macro step) Use the time series from the micro step to update\n  $x^\\eps_n$ to $x^\\eps_{n+1}$ by\n\\begin{equ}\\label{e:macrop}\nx^\\eps_{n+1} = x^\\eps_n + \\sum_{j=1}^\\lambda \\int_{n\\Delta t}^{(n+1/\\lambda)\\Delta t} f(x^\\eps_n,Y^{\\eps,j}_n(s)) ds\\;. \n\\end{equ}\n\\end{enumerate} \n\nAs with the HMM-type method, it will be convenient to write PHMM as a\nfast-slow system (when restricted to an interval $I_{n,\\Delta\n  t}$).\nAkin to \\eqref{e:hmm_cp}, it is easy to verify that the parallel HMM\nscheme is described by the system\n\\begin{equs}\\label{e:phmm_cp}\n\\frac{dX^\\eps_{n}}{dt} &= \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) \\\\\nd{\\widetilde{Y}}^\\eps_{n,j} &= \\frac{1}{\\eps \\lambda} b(x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) dt + \\frac{1}{\\sqrt{\\eps \\lambda}}\\sigma (x^\\eps_n,{\\widetilde{Y}}^{\\eps,j}_n) dW_j\\;,\n\\end{equs}\nfor $j=1,\\dots,\\lambda$ with the initial condition $X^\\eps_{n}(n\\Delta t) = x^\\eps_n$. \n\n\\section{Average and fluctuations in parallelized HMM}\n\\label{s:phmm_fluctuations}\n\nIn this section we check that the averaged behavior and the\nfluctuations in the PHMM method are consistent with those in the original fast\nslow system.\n\\subsection{Averaging}\nProceeding exactly as in Section \\ref{s:hmm_avg}, it follows that as $\\eps \\to 0$ the PHMM scheme $x^\\eps_{n+1}$ converges to ${\\bar{x}}_{n+1} = {{\\bar{X}}}_{n} ((n+1)\\Delta t) $ where\n\\begin{equ}\\label{e:phmm_avg}\n\\frac{d{\\bar{X}}_{n}}{d t} = \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda F ({\\bar{x}}_n) =  F({\\bar{x}}_n)\n\\end{equ}\nwith initial condition ${\\bar{X}}_{n} (n \\Delta t) = {\\bar{x}}_n$. Hence, we\nare in the exact same situation as with ordinary HMM, so the averaged\nbehavior is consistent with that of the original fast slow system.\n\n\n\\subsection{Small fluctuations}\\label{s:phmm_clt}\nWe now show that the fluctuations\n\\begin{equ}\nz^\\eps_{n} = \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps}}\n\\end{equ}\nare consistent with the correct CLT fluctuations, described by \\eqref{e:fs_clt}. As in Section \\ref{s:hmm_clt}, we instead look at the rescaled fluctuations\n\\begin{equ}\n{\\hat{z}}^\\eps_{n} = \\frac{x^\\eps_n - {\\bar{x}}_n}{\\sqrt{\\eps \\lambda }}\\;.\n\\end{equ}\nIn particular we will show that these rescaled fluctuations are consistent with \n\\begin{equ}\\label{e:phmm_clt_a}\nd{\\widehat{Z}} = B_0({\\bar{X}}){\\widehat{Z}} dt + \\lambda^{-1/2} \\eta({\\bar{X}}) dV\\;.\n\\end{equ}\nThe claim for $z^{\\eps}$ will follows immediately from the claim for ${\\hat{z}}^\\eps$. \n\\par\nWe have that ${\\hat{z}}^\\eps_{n+1} = {\\widehat{Z}}^\\eps_{n} ((n+1)\\Delta t)$ where\n\\begin{equ}\n{\\widehat{Z}}^\\eps_{n}(t)=  \\frac{X^\\eps_{n}(t) - {\\bar{X}}_{n}(t)}{\\sqrt{\\eps \\lambda}} \n\\end{equ} \nwith $X^\\eps_{n}$ given by the system \\eqref{e:phmm_cp} and ${\\bar{X}}_{n}$ given by the averaged equation \\eqref{e:phmm_avg}. As in Section \\ref{s:hmm_clt}, we derive a system for the triple $({\\bar{X}}_{n}, {\\widehat{Z}}^\\eps_{n}, {\\widetilde{Y}}^\\eps_{n})$, where now the fast process has $\\lambda$ independent components ${\\widetilde{Y}}^\\eps_{n} = ({\\widetilde{Y}}^{\\eps,1}_{n}, \\dots,{\\widetilde{Y}}^{\\eps,\\lambda}_{n})$:\n\\begin{equs}\\label{e:phmm_clt_triple}\n\\frac{d{\\bar{X}}_{n}}{dt} &= F({\\bar{x}}_n)\\\\\n\\frac{d {\\widehat{Z}}^\\eps_{n}}{dt} & = \\frac{1}{\\sqrt{\\eps \\lambda }} \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda {\\tilde{f}} ({\\bar{x}}_n, {\\widetilde{Y}}^{\\eps,j}_{n}) + \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\nabla_x f ({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) {\\hat{z}}_n + O(\\sqrt{\\eps \\lambda })\t   \\\\\nd{\\widetilde{Y}}^{\\eps,j}_{n} & = \\frac{1}{\\eps \\lambda} b({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) dt + \\frac{1}{\\sqrt{\\eps \\lambda }} \\nabla_x b({\\bar{x}}_n , {\\widetilde{Y}}^{\\eps,j}_{n}) {\\hat{z}}_n dt + \\frac{1}{\\sqrt{\\eps \\lambda }}\\sigma ({\\bar{x}}_n, {\\widetilde{Y}}^{\\eps,j}_{n}) dW_j + O(1)\\;.\n\\end{equs}\n With a modicum added difficulty, we can now argue as in Section \\ref{s:fs_clt} with $\\eps ' = \\eps \\lambda$ playing the role of $\\eps$. The invariant measure $\\mu_x^\\lambda (dy)$ associated with the generator of $Y^\\eps_n$ is now the product measure\n\\begin{equ}\n\\mu_x^\\lambda (dy_1,\\dots,dy_\\lambda) = \\mu_x(dy_1) \\dots \\mu_x (dy_\\lambda)\n\\end{equ}\nwhere $\\mu_x$ is the invariant measure associated with ${\\mathcal{L}}_0$ from Section \\ref{s:fs_clt}. This product structure simplifies the seemingly complicated expressions arising in the perturbation expansion of \\eqref{e:phmm_clt_triple}. In the setting of Section \\ref{s:fs_clt} we have that $u_0 = u_0 (x,z,t)$ and \n\\begin{equs}\\label{e:phmm_u1}\nu_1 (x,z,y,t)  = (-{\\mathcal{L}}_0^{(1)} - \\dots - {\\mathcal{L}}_0^{(\\lambda)})^{-1} {\\mathcal{L}}_1 u_0 (x,z,y,t)\\;,\n\\end{equs}\nwhere ${\\mathcal{L}}^{(j)}_0 = b({\\bar{x}}_n , y_j) \\nabla_{y_j} + \\frac{1}{2} \\sigma \\sigma^T ({\\bar{x}}_n , y_j ) : \\nabla_{y_j}^2$\n\n\nSince \n\\begin{equ}\n{\\mathcal{L}}_1 u_0 (x,z,y,t) = \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \n{\\tilde{f}} ({\\bar{x}}_n,y_j) \\cdot \\nabla_z u_0 (x,z,t)\\;,\n\\end{equ}\nthe Feynman-Kac representation of \\eqref{e:phmm_u1} yields\n\\begin{equ}\nu_1(x,z,y,t) = \\frac{1}{\\lambda} \\sum_{j=1}^{\\lambda} \n\\int_0^\\infty d\\tau {\\mathbf{E}}_{y_j} {\\tilde{f}} ({\\bar{x}}_n, Y_{{\\bar{x}}_n,j}(\\tau)) \n\\cdot \\nabla_z u_0(x,z,t)\\;.\n\\end{equ}\nThe equation for $u_0$ is now given by\n\\begin{equs}\n  \\label{e:phmm_u0_big}\n  \\del_t u_0  &= F({\\bar{x}}_n)\\nabla_x u_0 + \\int \\mu_{{\\bar{x}}_n} (dy_1)\\dots \\mu_{{\\bar{x}}_n}( dy_\\lambda) (\\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\nabla_x f({\\bar{x}}_n,y_j){\\hat{z}}_n) \\nabla_z u_0  \\\\  \n&\\quad  + \\int \\mu_{{\\bar{x}}_n} (dy_1)\\dots \\mu_{{\\bar{x}}_n}( dy_\\lambda) \\\\\n& \\qquad \\times \\Biggl( \\int_0^\\infty d\\tau \\left(\\frac{1}{\\lambda} \\sum_{j=1}^\\lambda {\\tilde{f}}({\\bar{x}}_n,y_j) \\right) \\otimes \\left( \\frac{1}{\\lambda}\\sum_{k=1}^\\lambda {\\mathbf{E}}_y{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))\\right) : \\nabla^2_z u_0 \\\\\n& \\qquad + \\sum_{j=1}^\\lambda (\\nabla_x b({\\bar{x}}_n,y_j) {\\hat{z}}_n) \\int_0^\\infty d\\tau \\, \\nabla_{y_j} \\frac{1}{\\lambda}\\sum_{k=1}^\\lambda{\\mathbf{E}}_{y_k}{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\Biggr)\\;.\n\\end{equs}\nBy expanding the product measure, the second term on the right hand side of \\eqref{e:phmm_u0_big} becomes \n\\begin{equ}\n  \\begin{aligned}\n    & \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda \\int \\mu_{{\\bar{x}}_n}( dy_j)\n    (\\nabla_x f({\\bar{x}}_n,y_j){\\hat{z}}_n) \\cdot\\nabla_z u_0\\\\\n    & = \\int\n    \\mu_{{\\bar{x}}_n}( dy_1) (\\nabla_x f({\\bar{x}}_n,y_1){\\hat{z}}_n) \\cdot\\nabla_z u_0\n    = (B_1({\\bar{x}}_n) {\\hat{z}}_n ) \\cdot\\nabla_z u_0 \\;,\n  \\end{aligned}\n\\end{equ}\n Likewise, using the independence of $Y^j_{x}$ for distinct $j$, the third term becomes \n\\begin{equs}\n\\frac{1}{\\lambda^2} \\sum_{j,k=1}^\\lambda & \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^k_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 \\\\\n& = \\frac{1}{\\lambda^2} \\sum_{j=1}^\\lambda  \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^j_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 \\\\\n & = \\frac{1}{\\lambda} \\int_0^\\infty d\\tau {\\mathbf{E}} {\\tilde{f}} ({\\bar{x}}_n, Y^1_{{\\bar{x}}_n}(0))\\otimes {\\tilde{f}} ({\\bar{x}}_n, Y^1_{{\\bar{x}}_n}(\\tau)) : \\nabla_z^2 u_0 = \\frac{1}{\\lambda} \\eta({\\bar{x}}_n) \\eta({\\bar{x}}_n)^T : \\nabla_z^2 u_0 \\;.\n\\end{equs}\nwhere the expectation is taken over realizations of $Y^j_{x}$ with $Y^j_{x}(0) \\sim \\mu_x$. Finally, since the $\\nabla_{y_j} {\\mathbf{E}}_{y_k}$ term vanishes on the off-diagonal, the last term in \\eqref{e:phmm_u0_big} reduces to\n\\begin{equs}\n\\frac{1}{\\lambda}  \\sum_{j,k=1}^\\lambda & \\int_0^\\infty d\\tau \\int\n\\mu_{{\\bar{x}}_n} (dy_j)\\mu_{{\\bar{x}}_n} (dy_k)  (\\nabla_x b({\\bar{x}}_n,y_j)\n{\\hat{z}}_n)  \\cdot \\nabla_{y_j}\n{\\mathbf{E}}_{y_k}{\\tilde{f}}({\\bar{x}}_n,Y^k_{{\\bar{x}}_n}(\\tau))  \\cdot \\nabla_z u_0 \\\\ \n&= \\frac{1}{\\lambda}  \\sum_{j=1}^\\lambda  \\int_0^\\infty d\\tau \\int \\mu_{{\\bar{x}}_n} (dy_j)\\mu_{{\\bar{x}}_n} (dy_k)  (\\nabla_x b({\\bar{x}}_n,y_j) {\\hat{z}}_n)  \\nabla_{y_j} {\\mathbf{E}}_{y_j}{\\tilde{f}}({\\bar{x}}_n,Y^j_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\\\ \n& = \\int_0^\\infty d\\tau \\int \\mu_{{\\bar{x}}_n} (dy_1)\\mu_{{\\bar{x}}_n} (dy_k)\n(\\nabla_x b(x,y_1) {\\hat{z}}_n)  \\nabla_{y_1}\n{\\mathbf{E}}_{y_1}{\\tilde{f}}({\\bar{x}}_n,Y^1_{{\\bar{x}}_n}(\\tau))  \\nabla_z u_0 \\\\\n&= (B_2 ({\\bar{x}}_n ) {\\hat{z}}_n )  \\cdot \\nabla_{z} u_0 \\;.\n\\end{equs}\nIt follows immediately that the reduced equation for the pair\n$({\\bar{X}}_n, \\hat Z^\\eps_n)$ is \n\\begin{equs}\n\\frac{d{\\bar{X}}_{n}}{dt} &= F({\\bar{x}}_n) \\\\\nd{\\widehat{Z}}_n &= B_0({\\bar{x}}_n) {\\widehat{Z}}_n dt + \\lambda^{-1/2} \\eta ({\\bar{x}}_n) dV\\;,\n\\end{equs}\nwith initial conditions ${\\widehat{Z}}_n(n\\Delta t) = {\\hat{z}}_{n}$ and ${\\bar{X}}_n(n\\Delta t) = {\\bar{x}}_n$.  Hence we see that ${\\hat{z}}_{n+1}$ is described by \n\\begin{equ}\n{\\hat{z}}_{n+1} = {\\hat{z}}_n + B({\\bar{x}}_n) {\\hat{z}}_n \\Delta t +\n\\lambda^{-1/2}\\eta({\\bar{x}}_n) \\sqrt{\\Delta t} \\, \\xi_n\n\\end{equ}\nwhich is the Euler-Maruyama scheme for \\eqref{e:phmm_clt_a}.\n\n\n\n\n\n\n\\subsection{Large fluctuations}\n\\label{s:phmm_ldp}\n\nIn this section we show that the LDP for PHMM is consistent with the\ntrue LDP from Section \\ref{s:fs_ldp}. In particular, let\n\\begin{equ}\nu_{\\lambda,\\Delta t} (t,x) = \\lim_{\\eps \\to 0} \\eps \\log {\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_{n}) \\right)\n\\end{equ}\nfor $t \\in I_{n,\\Delta t}$, where $x^\\eps_n$ is the PHMM approximation. We will argue that $u_{\\lambda,\\Delta t}(t,x) \\to u(t,x)$ as $\\Delta t \\to 0$, where $u$ solves the correct Hamilton-Jacobi equation \\eqref{e:fs_ldp_hj}.   \n\\par\nThe argument is a slight modification of that given in Section \\ref{s:hmm_ldp}. Before proceeding, we recall the notation $S^{(\\alpha)}_{\\Delta t}$ for the semigroup associated with the Hamilton-Jacobi equation\n\\begin{equ}\\label{e:phmm_ldp_hj}\n\\del_t u (t,x) = {\\pazocal{H}} (\\alpha , \\nabla u (t,x)) \\;,\n\\end{equ}\nwhere ${\\pazocal{H}}$ is the Hamiltonian defined by \\eqref{e:fs_ldp_ham}. We also define the operator $S_{\\Delta t} \\vphi(x) = S^{(\\alpha)}_{\\Delta t} \\vphi (x)|_{\\alpha = x}$. \n\nAs in Section \\ref{s:hmm_ldp}, the claim follows from the asymptotic statement\n\\begin{equ}\\label{e:phmm_ldp_dv}\n{\\mathbf{E}}_x \\exp \\left( \\eps^{-1} \\vphi (x^\\eps_n)  \\right) \\asymp \\exp\\left(\\eps^{-1} (S_{\\Delta t})^n \\vphi(x) \\right)\\;, \\quad \\eps \\to 0\\;.\n\\end{equ}\nGiven \\eqref{e:phmm_ldp_dv}, by an identical argument to that started in Equation \\eqref{e:hmm_ldp_expansion}, it follows from \\eqref{e:phmm_ldp_dv} that $u_{\\lambda,\\Delta t} $ is indeed a numerical approximation of the solution to \\eqref{e:phmm_ldp_hj} and hence $u_{\\lambda,\\Delta t} \\to u$ as $\\Delta t \\to 0$.   \n\\par\nWe will verify \\eqref{e:phmm_ldp_dv} by induction, starting with the $n=1$ case. Since $(X^\\eps_, {\\widetilde{Y}}^\\eps_{0,1},\\dots,{\\widetilde{Y}}^\\eps_{0,\\lambda})$ is a fast-slow system of the form \\eqref{e:fs} with $\\eps$ replaced by $\\eps \\lambda$, it follows from Section \\ref{s:fs_ldp} (Varadhan's lemma) that \n\\begin{equ}\n{\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1} \\psi (X^\\eps_1(\\Delta t))\\right) \\asymp \\exp \\left( (\\eps \\lambda)^{-1} {\\widehat{S}}_{\\Delta t}^{(\\alpha)} \\psi(x)|_{\\alpha = x}  \\right) \\;,\n\\end{equ}\nwhere ${\\widehat{S}}_{\\Delta t}^{(\\alpha)}$ is the semigroup associated with $\\del_t v(t,x) = {\\widehat{\\pazocal{H}}} (\\alpha , \\nabla v(t,x))$ and\n\\begin{equ}\n{\\widehat{\\pazocal{H}}} (\\alpha,\\theta) = \\lim_{T\\to \\infty } T^{-1} \\log {\\mathbf{E}} \\exp \\left(\\theta \\cdot \\int_0^T d\\tau \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(\\alpha, Y^j_{\\alpha}(\\tau)) \\right)\\;.\n\\end{equ}\nHence we have\n\\begin{equ}\\label{e:phmm_ldp_semi}\n  \\begin{aligned}\n    {\\mathbf{E}}_x \\exp \\left(\\eps^{-1} \\vphi (x^\\eps_1)\\right) &= {\\mathbf{E}}_x \\exp\n    \\left((\\eps \\lambda)^{-1} \\lambda\\vphi (X^\\eps_0(\\Delta t))\\right)\\\\\n    &\\asymp {\\mathbf{E}}_x \\exp \\left((\\eps \\lambda)^{-1}\n      {\\widehat{S}}^{(\\alpha)}_{\\Delta t} (\\lambda \\vphi) (x)|_{\\alpha = x}\n    \\right)\n  \\end{aligned}\n\\end{equ}\nBut since $Y^j_{\\alpha}$ are iid for distinct $j$, the Hamiltonian ${\\widehat{\\pazocal{H}}}$ reduces to\n\\begin{equs}\n\\lim_{T\\to \\infty } T^{-1} &\\log {\\mathbf{E}} \\exp \\left(\\theta \\cdot \\int_0^T d\\tau \\frac{1}{\\lambda}\\sum_{j=1}^\\lambda f(\\alpha, Y^j_{\\alpha}(\\tau)) \\right)\\\\ \n&= \\lambda \\lim_{T\\to \\infty } T^{-1} \\log {\\mathbf{E}} \\exp \\left(\\frac{\\theta}{\\lambda} \\cdot \\int_0^T d\\tau  f(\\alpha, Y^1_{\\alpha}(\\tau)) \\right) = \\lambda {\\pazocal{H}} (\\alpha , \\frac{\\theta}{\\lambda} )\\;.\n\\end{equs}\nIt follows that\n\\begin{equ}\n\\del_t \\left(\\lambda^{-1}{\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi)\\right) = \\lambda^{-1} {\\widehat{\\pazocal{H}}} (\\alpha , \\nabla ({\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi))) = {\\pazocal{H}} (\\alpha , \\lambda^{-1}\\nabla ({\\widehat{S}}_t^{(\\alpha)} (\\lambda \\vphi)) )\n\\end{equ}\nand hence $\\lambda^{-1} {\\widehat{S}}^{(\\alpha)}_{\\Delta t} (\\lambda \\vphi) = S_{\\Delta t}^{(\\alpha)}\\vphi$. Combining this with \\eqref{e:phmm_ldp_semi} completes the claim for $n=1$. The proof of the inductive step for arbitrary $n\\geq 1$ follows identically to Section \\ref{s:hmm_ldp}. \n\n\n\n\\section{Numerical evidence}\n\\label{s:num}\n\nIn this section, we investigate the performance of the standard HMM\nand PHMM methods for systems with well understood fluctuations and\nmetastability properties. These simple experiments confirm that HMM\namplifies fluctuations, which can drastically change the system's metastable\nbehavior, and that the PHMM succeeds in avoiding these problems. In\nSection \\ref{s:num_clt} we investigate simple CLT fluctuations for a\nsimple quadratic potential systems, in Section \\ref{s:num_ldp} we look\nat large deviation fluctuations for a quartic double-well\npotential. Finally in Section \\ref{s:num_nondiff} we look at\nfluctuations for a non-diffusive double well potential, which has\nlarge deviation properties that cannot be captured by a so-called\n`small noise' diffusion.\n\n\n\\subsection{Small fluctuations} \n\\label{s:num_clt}\n\nWe examine the small CLT-type fluctuations by looking the following\nfast-slow system\n\n\\begin{equs}\n\\frac{dX}{dt} &= Y - X \\\\\ndY &= \\frac{\\theta}{\\eps} (\\mu X - Y) dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\n\nIt is simple to check that the averaged system is given by\n\\begin{displaymath}\n  \\frac{d{\\bar{X}}}{dt} = (\\mu - 1) {\\bar{X}}.\n\\end{displaymath}\nHence for $\\mu < 1$ the\naveraged system is a gradient flow in a quadratic potential centered\nat the origin.\n\nWe will first illustrate that the HMM-type method described in Section\n\\ref{s:hmm} inflates the $O(\\sqrt{\\eps})$ fluctuations about the\naverage by a factor of $\\sqrt{\\lambda}$. In Figure \\ref{fig:clt_hist}\nwe plot histograms of the slow variable $X$ for different speed-up\nfactors $\\lambda$. It is clear that the spread of the invariant\ndistribution is increasing with $\\lambda$. The profile remains\nGaussian but the variance is greatly inflated. In Figure\n\\ref{fig:clt_var} we plot the variance of the stationary time series\nfor $X$ as a function of $\\lambda$. The blue line is computed using\nHMM and the red line is computed using PHMM. As predicted by the\ntheory in Section \\ref{s:hmm_clt}, in the case of HMM the variance is\nincreasing linearly with $\\lambda$ and in the case of PHMM the\nvariance is approximately constant. Note that in this example, the CLT\ncaptures the large deviations as well. \n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.5\\textwidth]{clt_hist}\n\\caption{Histogram of $X$ variables. Parameters used are\n  $\\eps = 10^{-2}$, $\\delta t = 0.1$, $\\theta= 1$, $\\mu=0.5$,\n  $\\sigma= 5$, $T = 10^4$. }\n\\label{fig:clt_hist}\n\\end{center}\n\\end{figure}\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.8\\textwidth]{clt_var}\n\\caption{Comparing the stationary variance of HMM and PHMM as a function of $\\lambda$. Once again, we use parameters $\\eps = 10^{-2}$, $\\delta t = 0.1$, $\\theta= 1$, $\\mu=0.5$,  $\\sigma= 5$, $T = 10^4$.}\n\\label{fig:clt_var}\n\\end{center}\n\\end{figure}\n\n\n\\subsection{Large fluctuations}\n\\label{s:num_ldp}\n\nTo investigate the affect of parallelization on $O(1)$ deviations not\ncaptured by the CLT, we will look at a fast-slow system which exhibits\nmetastability. Hence it is natural to take\n\\begin{equs}\\label{e:num_fs2}\n\\frac{dX}{dt} &= Y - X^3 \\\\\ndY &= \\frac{\\theta}{\\eps} (\\mu X - Y) dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\nIt is simple to check that the averaged system is\n\\begin{displaymath}\n  \\frac{d{\\bar{X}}}{dt} = \\mu {\\bar{X}} - {\\bar{X}}^3.\n\\end{displaymath}\nHence for any $\\mu > 0$ the\naveraged system is a gradient flow in a symmetric double well\npotential, with stable equilibria at $\\pm \\sqrt{\\mu}$ and a saddle\npoint at the origin. The large fluctuations of the fast-slow system\ncan be investigated by looking at the first passage time for\ntransitions from a neighborhood of one stable equilibrium to the other.\n\nIn Figure \\ref{fig:ldp_mfpt} we compare the mean first passage time\nfor HMM and PHMM as a function of $\\lambda$. Even for $\\lambda = 2$,\nthe distinction between the two methods is vast, with the mean first\npassage time for HMM rapidly dropping off and for PHMM staying\napproximately constant.\n\nIn Figure \\ref{fig:ldp_hist} we compare respectively the stationary\ndistributions of the true fast-slow system, HMM ($\\lambda =5$) and\nPHMM ($\\lambda=5$). In the case of HMM, the energy barrier separating\nthe two metastable states is now overpopulated, which explains the\nrapid fall in mean first passage time. In the case of PHMM, the\nhistogram is indistinguishable from the true stationary distribution\n(with the exception of a slight asymmetry).\n\n\n\nIn Figure \\ref{fig:ldp_fpt_cdf} we plot the cumulative distributions\nfunction (CDF) for the first passage time, comparing that of the true\nfast-slow system, with HMM ($\\lambda=5$) and PHMM ($\\lambda=5$). We\nsee that the HMM first passage times are supported on a much faster\ntime scale than that of the true fast-slow system. In contrast, the\nCDF of PHMM is almost indistinguishable from that of the true\nfast-slow system. Hence PHMM is not just replicating the mean first\npassage time, but also the entire distribution of first passage times.\n\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=0.8\\textwidth]{hmm_phmm_mfpt}\n\\caption{The mean first passage time as a function of the speed-up\n  factor $\\lambda$, for HMM (red dotted) and PHMM (blue dotted). We\n  include the LDP predicted curve for the mean first passage time of HMM, as\n  discussed in Section~\\ref{s:hmm_ldp}. $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\n\\label{fig:ldp_mfpt}\n\\end{center}\n\\end{figure}\n\n\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{hist_1_5_5}\n\\caption{Histogram of $X$ variables. $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\n\\label{fig:ldp_hist}\n\\end{center}\n\\end{figure}\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.8\\textwidth]{fpt_cdf_quarticslow.eps}\n\\caption{Cumulative\n  distribution functions for first passage times of the true model\n  (red) for \\eqref{e:num_fs2}, HMM with $\\lambda = 5$ (green) and PHMM\n  with $\\lambda = 5$ (blue). The parameters used are $\\eps = 10^{-3}$,\n  $\\delta t = 0.05$, $\\theta= 1$, $\\mu=1$, $\\sigma= 15$,\n  $T = 5\\times 10^4$}\\label{fig:ldp_fpt_cdf}\n\\end{figure}\n\n\n\\subsection{Asymmetric, non-diffusive fluctuations}\n\\label{s:num_nondiff}\n\nWe now compare HMM and PHMM for a multiscale model that also displays\nmetastability, but in which the large fluctuations cannot be\ncharacterized by a `small noise' Ito diffusion. In particular, the\nHamiltonian describing the LDP of the system is non-quadratic, as\nopposed the the previous systems. The system has been used\n\\cite{bouchet15} to illustrate the ineffectiveness of diffusion-type\napproximations for fast-slow systems. The fast-slow system is given by\n\\begin{equs}\\label{e:num_fs3}\n\\frac{dX}{dt} &= Y^2 - \\nu X\\\\\ndY &=  - \\frac{1}{\\eps} \\gamma(X) Y dt + \\frac{\\sigma}{\\sqrt{\\eps}} dW\\;.\n\\end{equs}\n\nwhere $\\gamma(x) = x^4/10 - x^2 + 3$ .  The averaged equation for this\nsystem reads\n\n\\begin{displaymath}\n  \\frac{d\\bar X}{dt} = \\frac{\\sigma^2}{2\\gamma(\\bar X)} - \\nu \\bar X\n\\end{displaymath}\nFor $\\nu = 1$ and $\\sigma = \\sqrt{3}$, this averaged equation\npossesses two stable fixed points at $x\\approx 0.555$ and\n$x\\approx=2.459$ and one unstable fixed point at $x\\approx 2.459$. The\nthe rates of transition between these stable fixed points is captured\nby the LDP. By an elementary calculation \\cite{bouchet15}, the\nHamiltonian of this LDP is found to be non-quadratic and given by\n\n\\begin{displaymath}\n  {\\pazocal{H}} (x,\\theta) = - \\nu x \\theta +\\frac12\\left(\\gamma(x) -\n    \\sqrt{\\gamma^2(x)-2\\sigma^2 \\theta}\\right)\n\\end{displaymath}\n\nThe quasi-potential associated with this Hamiltonian satisfies\n$0= {\\pazocal{H}}(x,{\\mathcal{V}}')$, i.e.\n\n\\begin{displaymath}\n  {\\mathcal{V}}'(x) = \\frac{\\nu x \\gamma(x) - \\tfrac12 \\sigma^2}{\\nu^2x^2}\\;,\n\\end{displaymath}\nand is displayed in Figure \\ref{fig:num_quasi}. Whilst there is a\nsignificant barrier corresponding to left-to-right transitions, there\nis almost no barrier corresponding to right-to-left transitions.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\textwidth]{QP1}\\caption{The quasi-potential\n    ${\\mathcal{V}}(x)$ (red curve) and the one obtained from a quadratic approximation of\n    the Hamiltonian (orange curve). Also shown in blue is the\n    coefficient at the right-hand side of the reduced equation.\\label{fig:num_quasi}}\n\\end{figure}\n\nIn Figure \\ref{fig:ldp_fpt_cdf2} we plot CDFs of the first passage\ntimes: due to the asymmetry we plot separately the transitions from\nthe left-to-right and right-to-left.  For left-to-right transitions,\nthe HMM procedure drastically speeds up transitions because it \nenhances fluctuations: as is the case with the previous experiment,\nthe HMM transitions are supported on a timescale several orders of\nmagnitude faster than those of the true fast slow system. The PHMM\nmethod does not experience this problem and the distribution of first\npassage times agrees quite well with the true model.  For\nright-to-left transitions, PHMM shows similarly good agreement with\nthe true fast-slow system, but in contrast HMM is not too far off\neither. This can be accounted for by the `flatness' of the right\npotential well, meaning that increasing the amplitude of fluctuations\nwill only decrease the escape time by a linear multiplicative\nfactor. We note that the noise appearing in the CDF plots is due to\nthe scarcity of transitions occurring in the model \\eqref{e:num_fs3}.\n\n\n\n\n\n\n\n\n\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\textwidth]{fpt_cdf_quadraticslow_v1.eps}\n\\caption{Cumulative\n  distribution functions for first passage times of the true model for\n  \\eqref{e:num_fs3} (red), HMM with $\\lambda = 5$ (green) and PHMM\n  with $\\lambda = 5$ (blue). Left-to-right transitions on the left,\n  right-to-left transition on the right.  The parameters used are\n  $\\eps = 0.05$, $\\delta t = 0.5$, $\\nu= 1$, $\\sigma= \\sqrt{3}$,\n  $T = 1\\times 10^7$}\\label{fig:ldp_fpt_cdf2}\n\\end{figure}\n \n\n\\section{Discussion}\n\\label{s:discussion}\n\nWe have investigated HMM methods for fast-slow systems, in particular\ntheir ability (or lack thereof) to capture fluctuations, both small\n(CLT) and large (LDP). We found, both theoretically (Section\n\\ref{s:hmm_fluctuations}) and numerically (Section \\ref{s:num}), that\nthe amplitude of fluctuations is enhanced by an HMM-type method. In\nparticular with an HMM speed up factor~$\\lambda$, in the CLT the\nvariance of Gaussian fluctuations about the average is increased by a\nfactor~$\\lambda$ as well. In the LDP, the quasi-potential is decreased\nby a factor~$\\lambda$, leading to the first passage times being\nsupported on a time scale $\\lambda$ orders of magnitude smaller than\nin the true fast slow system. This inability to correctly capture\nfluctuations about the average suggests that HMM can be a poor\napproximation of fast-slow systems, particularly when metastable\nbehavior is important. {As noted in Section\n  \\ref{s:hmm_ldp}, although the fluctuations of HMM are enhanced, the\n  large deviation transition \\emph{pathways} remain faithful to the\n  true model. Thus we stress that HMM is a reliable method of finding\n  transition pathways in metastable systems, but not for simulating\n  their dynamics.}\n\nWe have introduced a simple modification of HMM, called parallel HMM\n(PHMM), which avoids these fluctuation issues. In particular, the PHMM\nmethod yields fluctuations that are consistent with the true fast slow\nsystem for any speed up factor $\\lambda$ (provided that we still have\n$\\eps \\lambda \\ll 1$), as was shown both theoretically (Section\n\\ref{s:phmm_fluctuations}) and numerically (Section \\ref{s:num}). The\nHMM method relies on computing one short burst of the fast variables,\nand inferring the statistical behavior of the fast-variables by\nextrapolating this short burst over a large time window. PHMM on the\nother hand computes an ensemble of $\\lambda$ short bursts, and infers\nthe statistics of the fast variables using the ensemble. Since the\nensemble members are independent, they can be computed in\nparallel. Hence if one has $\\lambda$ CPUs available, then the real\ncomputational time required in PHMM is identical to that in HMM. \n\nInterestingly, one can draw connections between the parallel method\nintroduced here and the tau-leaping method used in stochastic chemical\nkinetics \\cite{gillespie00}. The tau-leaping method is an\napproximation used to speed up simulation of stochastic fast-slow\nsystems of the type\n\\begin{equ}\\label{e:tau_leap}\nX^\\eps(t) = X^\\eps(0) + \\sum_{k=1}^m \\eps {\\pazocal{N}}_k \n\\left(  \\eps^{-1}\\int_0^t a_k(X^\\eps(s))ds  \\right)\\nu_k\\;,\n\\end{equ}\nwhere ${\\pazocal{N}}_k$ are independent unit rate Poisson processes, $\\nu_k$ are\nvectors in $\\reals^d$ and $a_k : \\reals^d \\to \\reals$. The system\n\\eqref{e:tau_leap} can be solved exactly by the stochastic simulation\nalgorithm (SSA), but when $\\eps$ is small this can be extremely\nexpensive, due to the Poisson clocks being reset each time a jump\noccurs. The tau-leaping procedure avoids this issue by chopping the\nsimulation window into sub-intervals of size $\\tau$ and on each\nsubinterval fixing the Poisson clocks to their value at the left\nendpoint. The speed-up is a result of the fact that one can simulate\nthe Poisson jumps in parallel, since their clocks are fixed over the\n$\\tau$ interval.\n\n\n\n\n\n\n\nAs a consequence of this analogy, one can check (using calculations\nsimilar to those found above) that the tau-leaping method also\ncaptures the fluctuations correctly, both at the level of the CLT and\nthat of the LDP. The former observation was made\nin~\\cite{anderson2011error}; to the best of our knowledge, the second\none is new.\n\n\nAs a final note, we stress that there are non-dissipative fast-slow\nsystems for which the PHMM will not be effective at capturing their\nlong time scale behavior, including metastability. These are system\nfor which the CLT and LDP hold on $O(1)$ timescale, but they either\ncannot be extended to longer time-scale (in the case of the CLT) or\nleads to trivial prediction on these time scales (in the case of the\nLDP). To clarify this point, take for example the fast-slow\nLangevin system\n\\begin{equs}\n  \\label{e:fs_hamiltonian}\n  \\dot{q}_1 &= p_1 \\quad \\dot{p}_1 = q_1 - q_1^3 + (q_2 - q_1) \\;, \\\\\n  \\dot{q}_2 &= \\eps^{-1} p_2 \\quad \\dot{p}_2 = \\eps^{-1}(q_1 - q_2) \n- \\eps^{-1} \\gamma p_2 + \\sqrt{2 \\eps^{-1}\\beta^{-1}\\gamma} \\eta \\;.\n\\end{equs}\n\nwhere $\\gamma>0$ and $\\beta>0$ are parameters.  For any value of\n$\\eps$, $\\gamma$, this system is invariant with respect to the Gibbs\nmeasure with Hamiltonian\n\\begin{equ}\n  H(q_1,q_2,p_1,p_2) = \\frac{1}{2}p_1^2 + \\frac{1}{2}p_2^2 \n  + \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 + \\frac{1}{2}(q_1-q_2)^2 \\;.\n\\end{equ}\nAs $\\eps\\to 0$, it is easy to check that the slow variables\n$(q_1,q_2)$ converge to the averaged system\n\\begin{equ}\n  \\label{e:hamilt_avg}\n  \\Dot{\\bar q}_1 = \\bar p_1 \\qquad \\Dot{\\bar p}_1 = - G'(\\bar q_1)\n\\end{equ}\nwhere the averaged vector field is the gradient of the free energy\n\\begin{equ}\n  G(q_1) = \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 + \\frac{1}{2}q_1^2 \n= - \\beta^{-1}\\log \\int \\exp (\\beta U(q_1,q_2))dq_2 \\;,\n\\end{equ}\nwith\n$U (q_1,q_2) = \\frac{1}{4}q_1^4 - \\frac{1}{2}q_1^2 +\n\\frac{1}{2}(q_1-q_2)^2$.\nLikewise, if we introduce \n\n\\begin{displaymath}\n  \\eta_1= \\frac{q_1-\\bar q_1}{\\sqrt{\\eps}}, \\qquad \n  \\zeta_1= \\frac{p_1-\\bar p_1}{\\sqrt{\\eps}},\n\\end{displaymath}\n\nthe CLT indicates that the evolution of these variables are captured\nby \n\n\n", "index": 5, "text": "\\begin{equation}\n  \\label{eq:3}\n  \\dot \\eta_1 = \\zeta_1, \\qquad d\\zeta_1 = \\sqrt{2\\beta^{-1}\\gamma}\\, dB\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"\\dot{\\eta}_{1}=\\zeta_{1},\\qquad d\\zeta_{1}=\\sqrt{2\\beta^{-1}\\gamma}\\,dB\" display=\"block\"><mrow><mrow><msub><mover accent=\"true\"><mi>\u03b7</mi><mo>\u02d9</mo></mover><mn>1</mn></msub><mo>=</mo><msub><mi>\u03b6</mi><mn>1</mn></msub></mrow><mo rspace=\"22.5pt\">,</mo><mrow><mrow><mi>d</mi><mo>\u2062</mo><msub><mi>\u03b6</mi><mn>1</mn></msub></mrow><mo>=</mo><mrow><mpadded width=\"+1.7pt\"><msqrt><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\u03b2</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>\u2062</mo><mi>\u03b3</mi></mrow></msqrt></mpadded><mo>\u2062</mo><mi>d</mi><mo>\u2062</mo><mi>B</mi></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.02147.tex", "nexttext": "\n\nHowever, neither~\\eqref{eq:3} nor~\\eqref{eq:4} capture the long\ntime behavior of the solution to~\\eqref{e:fs_hamiltonian}. The problem\nstems from the fact that the averaged equation in~\\eqref{e:hamilt_avg}\nis Hamiltonian, hence non-dissipative. As a result, fluctuations\naccumulate as time goes on. Eventually, the CLT stops being valid, and\nthe LDP becomes trivial -- in particular, it is easy to see that the\nquasi-potential associated with the action in~\\eqref{eq:4} is\nflat. For examples of this type, other techniques will have to be\nemployed to describe their long time behavior including, possibly,\ntheir metastability (which, in the case of~\\eqref{e:fs_hamiltonian} is\ncontrolled by how small~$\\beta^{-1}$ is, rather than~$\\eps$). These\nquestions will be investigated elsewhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\\bibliographystyle{./Martin}\n\\bibliography{./hmm_fluctuations}\n\n\n", "itemtype": "equation", "pos": 72088, "prevtext": "\n\nand we can also derive an LDP for \\eqref{e:fs_hamiltonian} with\naction\n\n\n", "index": 7, "text": "\\begin{equation}\n  \\label{eq:4}\n  {\\mathcal{S}}_{[0,T]} (q_1) = \\frac{\\beta}{4\\gamma} \\int_0^T |\\ddot q_1-\n    q_1+q_1^3|^2 dt\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"{\\mathcal{S}}_{[0,T]}(q_{1})=\\frac{\\beta}{4\\gamma}\\int_{0}^{T}|\\ddot{q}_{1}-q_%&#10;{1}+q_{1}^{3}|^{2}dt\" display=\"block\"><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy=\"false\">]</mo></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mi>\u03b2</mi><mrow><mn>4</mn><mo>\u2062</mo><mi>\u03b3</mi></mrow></mfrac><mo>\u2062</mo><mrow><msubsup><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mn>0</mn><mi>T</mi></msubsup><mrow><msup><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mover accent=\"true\"><mi>q</mi><mo>\u00a8</mo></mover><mn>1</mn></msub><mo>-</mo><msub><mi>q</mi><mn>1</mn></msub></mrow><mo>+</mo><msubsup><mi>q</mi><mn>1</mn><mn>3</mn></msubsup></mrow><mo stretchy=\"false\">|</mo></mrow><mn>2</mn></msup><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><mi>t</mi></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}]