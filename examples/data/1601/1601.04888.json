[{"file": "1601.04888.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 17453, "prevtext": "\n\n\n\n\\title{A Closed-Form Solution to Tensor Voting: Theory and Applications}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{Tai-Pang~Wu,~Sai-Kit~Yeung,~Jiaya~Jia,~Chi-Keung~Tang,~\\and\nG\\'{e}rard Medioni\n\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem T.-P.~Wu  is with the Enterprise and Consumer Electronics Group,\nHong Kong Applied Science and Technology Research Institute (ASTRI), Hong Kong. E-mail: tpwu@astri.org.\n\\IEEEcompsocthanksitem S.-K.~Yeung and C.-K.~Tang are with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong.\nE-mail: \\{saikit,cktang\\}@cse.ust.hk.\n\\IEEEcompsocthanksitem J. Jia is with the Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong.\nE-mail: leojia@cse.cuhk.edu.hk \n\\IEEEcompsocthanksitem G.~Medioni is with the Department of Computer Science,\nUniversity of Southern California, USA. E-mail: medioni@usc.edu.\n\n\n\n}\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\markboth{Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence}\n{Wu \\MakeLowercase{\\textit{et al.}}: Tensor Voting Revisited: A Closed-Form Solution for Robust Parameter Estimation via Expectation-Maximization}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\n\nWe prove a closed-form solution to tensor voting ({\\bf CFTV}):\ngiven a point set in any dimensions, our closed-form solution provides\nan exact, continuous and efficient algorithm for computing a\nstructure-aware tensor that simultaneously achieves salient structure\ndetection and outlier attenuation. Using CFTV, we prove the\nconvergence of tensor voting on a Markov random field (MRF), thus\ntermed as {\\bf MRFTV}, where the structure-aware tensor at each \ninput site reaches a stationary state upon convergence in \nstructure propagation.\n\nWe then embed structure-aware tensor into expectation maximization (EM)\nfor optimizing a single linear structure to achieve efficient and robust\nparameter estimation. Specifically, our {\\bf EMTV} algorithm optimizes\nboth the tensor and fitting parameters and does\nnot require random sampling consensus typically used in existing\nrobust statistical\ntechniques.  We performed quantitative evaluation on its accuracy and robustness,\nshowing that EMTV performs better than the original TV\nand other state-of-the-art techniques\nin fundamental matrix estimation for multiview stereo matching.\nThe extensions of CFTV and EMTV for extracting multiple and nonlinear structures are underway.\nAn addendum is included in this arXiv version.\n\n\n\n\n\n\n\n\n\n\\end{abstract}\n\n\n\n\n\n\n\n\n\\begin{IEEEkeywords}\nTensor voting,\nclosed-form solution,\nstructure inference,\nparameter estimation,\nmultiview stereo.\n\\end{IEEEkeywords}\n}\n\n\n\\maketitle\n\n\n\n\n\n\n\n\n\\IEEEdisplaynotcompsoctitleabstractindextext\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Introduction}\n\\label{sec:intro}\n\\IEEEPARstart{T}{his} paper reinvents tensor voting~\\cite{book_with_names}\nfor robust computer vision, by proving a closed-form solution to computing \nan {\\em exact\nstructure-aware tensor} after {\\em data communication} in a feature space\nof any dimensions, where the goal is salient structure inference from \nnoisy and corrupted data.\n\nTo infer structures from noisy data corrupted by outliers, in tensor\nvoting, input points communicate among themselves subject to proximity and\ncontinuity constraints. Consequently, each point is\naware of its structure saliency via a structure-aware tensor.\nStructure refers to surfaces,\ncurves, or junctions if the feature space is three dimensional where\na structure-aware tensor can be visualized as an ellipsoid: if\na point belongs to a smooth surface, the resulting ellipsoid\nafter data communication resembles a stick pointing along the surface normal;\nif a point lies on a curve the tensor resembles a plate,\nwhere the curve tangent is perpendicular to the plate tensor;\nif it is a point junction where surfaces intersect, the tensor will be like\na ball. An outlier is characterized by a set of inconsistent votes it receives\nafter data communication.\n\nWe develop in this paper a closed-form solution to tensor voting (CFTV),\nwhich is applicable to the special as well as general theory of tensor\nvoting.  This paper focuses on the {\\em special} theory,  where\nthe above data communication is data driven without using constraints\nother than proximity and continuity. The special theory, sometimes\ncoined as ``first voting pass,'' is applied to process raw input data\nto detect structures and outliers. In addition to structure detection\nand outlier attenuation, in the {\\em general} theory of tensor voting,\ntensor votes are propagated along preferred directions to achieve data\ncommunication when such directions are available, typically after\nthe first pass, such that useful tensor votes are reinforced whereas\nirrelevant ones are suppressed. \n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{fig01.png}\n\\end{center}\n\\caption{\\small\nInlier/outlier and tensor inverse illustration.\n(a) The normal votes received\nat a surface point cast by points in ${\\mathbf{x}}$'s neighborhood.\nThree salient outliers are present.\n(b)\nFor a non-surface point,\nthere is no preference to any normals.\n(c) The structure-aware tensor induced by the normal observations in (a),\nwhich is represented by a $d$-D ellipsoid, where $d \\ge 2$.\nThe orange curve (dashed curve) represents the variance produced\nalong all possible directions.\n(d) The structure-aware tensor collected after collecting the \nreceived votes in (b).\n(e) and (f) correspond to the inverse of (c) and (d), respectively.\n}\n\\label{fig:inlier_outlier_illustration}\n\\vspace{-0.2in}\n\\end{figure*}\n\nExpressing tensor voting in a single and compact equation,\nor a closed-form solution, offers many advantages:\nnot only an exact and efficient solution can be achieved with\nless implementation effort for salient structure detection\nand outlier attenuation, formal and useful mathematical\noperations such as differential calculus can be applied\nwhich is otherwise impossible using the original tensor\nvoting procedure. Notably, we can prove the convergence\nof tensor voting on Markov random fields (MRFTV) where a \nstructure-aware tensor at each input site achieves a \nstationary state upon convergence. \n\nUsing CFTV, \n\n\n\nwe contribute a mathematical derivation based on expectation \nmaximization (EM) that\napplies the exact tensor solution for extracting the most salient\nlinear structure, despite that the input data is highly corrupted.\nOur algorithm is called EMTV, which optimizes both the tensor and\nfitting parameters upon convergence and does not require random\nsampling consensus typical of existing robust statistical techniques.\nThe extension to extract salient multiple and nonlinear structures is\nunderway.  \n\n\n\n\n\n\n\n\nWhile the mathematical derivation may seem involved, our main\nresults for CFTV, MRFTV and\nEMTV, that is, \nEqns~(\\ref{eqn:cal_S}), (\\ref{eqn:cal_S_inv}), \n(\\ref{eqn:energy_update_rule}), (\\ref{eqn:sor_update_rule}),\n(\\ref{eqn:e_step_update}), and (\\ref{eqn:m_step_update}).\nhave rigorous mathematical foundations, are applicable\nto any dimensions, produce more robust and accurate results as demonstrated\nin our qualitative and quantitative evaluation using challenging\nsynthetic and real data, but on the other hand are easier to implement.\nThe source codes accompanying this paper are available in the\nsupplemental material.\n\n\\section{Related Work}\n\\label{sec:related}\nWhile this paper is mainly concerned with tensor voting, we provide\na concise review on robust estimation and expectation maximization.\n\n\n\n\\noindent {\\bf Robust estimators.}\nRobust techniques are widely used and an excellent review\nof the theoretical foundations of robust\nmethods in the context of computer vision can be found\nin~\\cite{meer_bookchapter}.\n\nThe Hough transform~\\cite{hough_reference} is a robust voting-based\ntechnique operating in a parameter space capable of extracting multiple\nmodels from noisy data.\n\n\n\n\nStatistical Hough transform~\\cite{dahyot_pami2009} can be used\nfor high dimensional spaces with sparse observations.\nMean shift~\\cite{meanshift} has been widely used since its introduction\nto computer vision for robust feature space analysis. The Adaptive mean\nshift~\\cite{adaptive_meanshift} with variable bandwidth in high dimensions\nwas introduced in texture classification and has since been applied to other\nvision tasks. Another popular robust method in computer vision\nis in the class of random sampling consensus (RANSAC) procedures\n\\cite{fischler_acm81} which have spawned a lot of follow-up work\n(e.g., optimal randomized RANSAC~\\cite{chum_pami08}).\n\nLike RANSAC~\\cite{fischler_acm81}, robust estimators\nincluding the LMedS~\\cite{rousseeuw84} and the M-estimator~\\cite{huber81}\nadopted a statistical approach. The\nLMedS, RANSAC and the Hough transform can be expressed\nas M-estimators with auxiliary scale~\\cite{meer_bookchapter}.\nThe choice of scales\nand parameters related to the noise level are major issues.\nExisting works on robust scale estimation use random\nsampling~\\cite{subbarao_meer06}, or operate on\ndifferent assumptions (e.g., more than 50\\% of the data should be\ninliers~\\cite{rousseeuw_1987}; inliers have a Gaussian\ndistribution~\\cite{lee_1998}). Among them, the Adaptive Scale Sample\nConsensus (ASSC) estimator~\\cite{wang_and_suter} has shown the best\nperformance where the estimation process requires no free parameter as input.\nRather than using a Gaussian distribution to model inliers, the authors of \n\\cite{wang_and_suter} proposed to use a two-step scale estimator (TSSE) to refine the model scale:\nfirst, a non-Gaussian distribution is used to model inliers where\nlocal peaks of density are found by mean shift~\\cite{meanshift};\nsecond, the scale parameter is estimated by a median scale estimator with\nthe estimated peaks and valleys. \n\n\nOn the other hand, the projection-based M-estimator (pbM)~\\cite{chen_meer_iccv03}, \nan improvement made on the M-estimator, uses a Parzen window for scale\nestimation, so the scale parameter is automatically found by\nsearching for the normal direction (projection direction)\nthat maximizes the sharpest peak of the density. This\ndoes not require an input scale from the user. While these\nrecent methods can tolerate more outliers, most of them still\nrely on or are based on RANSAC and a number of random\nsampling trials is required to achieve the desired robustness.\n\nTo reject outliers, a multi-pass method using $L_\\infty$-norms was\nproposed to successively detect outliers which are characterized by\nmaximum errors~\\cite{sim_hartley_cvpr06}.\n\n\n\\noindent {\\bf Expectation Maximization.}\n\n\n\n\n\n\nEM has been used in handling missing\ndata and identifying outliers in robust computer\nvision~\\cite{computer_vision_book},\nand its convergence properties were studied~\\cite{embook}.\nIn essence, EM consists of two steps~\\cite{bilmes97gentle,embook}:\n\\begin{enumerate}\n\\item {\\bf E-Step}. Computing an expected value for the complete data set using \nincomplete data and the current estimates of the parameters.\n\\item {\\bf M-Step}. Maximizing the complete data log-likelihood using the \nexpected value computed in the E-step.\n\\end{enumerate}\n\nEM is a powerful inference algorithm, but it is also well-known from \n\\cite{computer_vision_book} that:\n1) initialization is an issue because EM can get stuck in poor local minima,\nand 2) treatment of data points with small expected weights requires great\ncare. They should not be regarded as negligible, as their aggregate effect can\nbe quite significant.  In this paper we initialize EMTV using structure-aware\ntensors obtained by CFTV. As we will demonstrate, such initialization not \nonly allows the EMTV algorithm to converge quickly (typically within 20\niterations) but also produces accurate and robust solution in parameter\nestimation and outlier rejection. \n\n\n\n\\if 0\n\n\\section{Exactness and Optimality}\n\\label{sec:optimality}\n\nFocusing on the special tensor voting theory, the problem is \nto compute a structure-aware tensor at each input site. \n\nRather than seeking an exact solution to the above problem, pioneering\nas well as recent works on tensor communication used a discrete voting algorithm\nas an approximation. Such algorithms have been criticized for lack of a\nmathematical foundation. Concerns were even raised on the ``optimality''\nof such voting procedure. Note, for example, in the conclusion\nof~\\cite{meer_bookchapter} that tensor voting was not discussed\nbecause it is ``somewhat ad-hoc'' and does not seem to have a\n``common statistical framework with solid theoretical (mathematical)\nfoundations $\\dots$ of robust estimators''.  Despite works such as\n\\cite{franken_eccv06,jmlr} on improving the speed and accuracy,\nthey either are discrete approximations or require a complex voting\nprocedure without precise mathematical underpinnings.\n\nIn our opinion, tensor voting is different from optimization\nbecause it is {\\em not} defined by an objective function.\nTensor voting is concerned with data or tensor communication and therefore\nit does not make sense to impose the concept of ``optimality''\non tensor voting as was done in other optimization problems.\n\n\n\nBut then, does it mean that tensor voting is at odds with optimization\ntechniques widely used in computer vision? Previously, the answer was\n{\\em yes} because of the complex discrete voting procedure using discrete\nvoting fields which produce approximate solutions to tensor voting.\nIn contrast, our answer is {\\em no}: by embedding the closed-form\nsolution to tensor voting in an objective function, the resulting\nfunction can be differentiated and thus the notion of optimality can\nbe unambiguously defined: in the EMTV algorithm for parameter estimation,\nit makes sense to characterize the optimality of the estimated parameters\nbecause an objective function is defined. In the sequel, we will prove \nthe convergence of tensor voting in outlier rejection, that is, \nall tensors will achieve a stationary state no matter how many \ntimes tensor voting is applied after convergence.\n\nIn the following, we first introduce the closed-form solution to tensor\nvoting (CFTV), which contributes an exact solution for computing\nstructure-aware tensors. Then, based on the unique properties of\nCFTV, we derive a novel EM algorithm based on tensor voting, EMTV,\nfor robust parameter estimation where the optimality of the estimated\nparameters can be precisely measured using an objective function.\n\n\\fi\n\n\\section{Data Communication}\n\\label{sec:smooth_connection}\nIn the tensor voting framework, a data point, or voter, communicates with\nanother data point, or vote receiver,  subject to proximity and continuity\nconstraints, resulting in a tensor vote cast from the voter to the vote\nreceiver (Fig.~\\ref{fig:inlier_outlier_illustration}). In the following, we use ${\\mathbf{n}}$ to denote a unit voting stick tensor,\n${\\bf v}$  to denote a stick tensor vote received. Stick tensor vote \nmay not be unit vectors when they are multiplied by vote strength.\nThese stick tensors are building elements of a structure-aware tensor vote.\n\n\n\n\nHere, we first define a decay function $\\eta$ to encode the proximity\nand smoothness constraints (Eqns~\\ref{eqn:neighbor_weighting} and \\ref{eqn:neighbor_weighting_Gaussian}).\nWhile similar in effect to the decay function used in the original\ntensor voting, and also to the one used in~\\cite{franken_eccv06}\nwhere a vote attenuation function is\ndefined to decouple proximity and curvature terms, our modified\nfunction, which also differs from that in~\\cite{jmlr},\nenables a closed-form solution for tensor voting without resorting\nto precomputed discrete voting fields.\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.6\\linewidth]{fig02.png}\n\\end{center}\n\\caption{\\small \n(a)  The normal vote ${\\mathbf v}_i$ received at ${\\mathbf{x}}_i$ using an arc of the\nosculating circle between ${\\mathbf{x}}_i$ and ${\\mathbf{x}}_j$, assuming the normal voter\nat ${\\mathbf{x}}_j$ is ${\\mathbf{n}}_j$, where ${\\mathbf{n}}_j$, ${\\mathbf r}_{ij}$, and ${\\mathbf v}_i$\nare unit vectors in this illustration.\n(b) Plot of Eqn.~(\\ref{eqn:neighbor_weighting}) in 2D.\n}\n\\label{fig:osculating_arc}\n\\label{fig:weighted_color_plot}\n\\vspace{-0.2in}\n\\end{figure}\nRefer to Fig.~\\ref{fig:osculating_arc}.\nConsider two points ${\\mathbf{x}}_i \\in \\Bbb{R}^d$ and ${\\mathbf{x}}_j \\in \\Bbb{R}^d$\n(where $d > 1$ is the dimension) that are connected by some smooth\nstructure in the feature/solution space.\nSuppose that the unit normal ${\\mathbf{n}}_j$ at ${\\mathbf{x}}_j$ is known.\nWe want to generate at ${\\mathbf{x}}_i$ a normal (vote) ${\\mathbf v}_i$\nso that we can calculate $\\mathbf{K}_i \\in \\Bbb{R}^d \\times \\Bbb{R}^d$, where $\\mathbf{K}_i$ is\nthe structure-aware tensor at ${\\mathbf{x}}_i$ in the presence of\n ${\\mathbf n}_j$ at ${\\mathbf{x}}_j$. In tensor voting, a \nstructure-aware tensor is a second-order symmetric tensor \nwhich can be visualized as an ellipsoid.\n\nWhile many possibilities exist, the unit\ndirection ${\\mathbf v}_i$ can be derived by fitting an arc of the\nosculating circle between the two points.  Such an arc keeps\nthe curvature constant along the hypothesized connection, thus\nencoding the smoothness constraint.\n$\\mathbf{K}_i$ is then given by ${\\mathbf v}_i {\\mathbf v}_i^T$ multiplied by\n$\\eta({\\mathbf{x}}_i, {\\mathbf{x}}_j, {\\mathbf{n}}_j)$ defined as:\n\n", "index": 1, "text": "\\begin{equation}\n\\label{eqn:neighbor_weighting}\n\\eta( {\\mathbf{x}}_i, {\\mathbf{x}}_j, {\\mathbf{n}}_j ) = c_{ij} ( 1 - ( \\mathbf{r}_{ij}^T {\\mathbf{n}}_j )^2  )\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E1.m1\" class=\"ltx_Math\" alttext=\"\\eta({\\mathbf{x}}_{i},{\\mathbf{x}}_{j},{\\mathbf{n}}_{j})=c_{ij}(1-(\\mathbf{r}_%&#10;{ij}^{T}{\\mathbf{n}}_{j})^{2})\" display=\"block\"><mrow><mrow><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>,</mo><msub><mi>\ud835\udc27</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc2b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc27</mi><mi>j</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nis an exponential function using Euclidean distance for attenuating \nthe strength based on proximity.\n$\\sigma_d$ is the size of local neighborhood (or the scale parameter,\nthe only free parameter in tensor voting).\n\nIn Eqn.~(\\ref{eqn:neighbor_weighting}), \n${\\mathbf r}_{ij} \\in \\Bbb{R}^d$ is a \nunit vector at ${\\mathbf{x}}_j$ pointing to ${\\mathbf{x}}_i$, and\n$1 - ( {\\mathbf r}_{ij}^T {\\mathbf{n}}_j )^2$ is a squared-sine\nfunction\\footnote{\n$\\sin^2 \\rho = 1 - \\cos^2 \\rho$, where $\\cos^2 \\rho= ({\\mathbf\nr}_{ij}^T {\\mathbf{n}}_j)^2$ and $\\rho$ is the angle between\n${\\mathbf r }_{ij}$ and ${\\mathbf{n}}_j$.} for attenuating the\ncontribution according to curvature. Similar to the original\ntensor voting framework, Eqn.~(\\ref{eqn:neighbor_weighting}) favors\nnearby neighbors that produce small-curvature connections,\nthus encoding the smoothness constraint. A plot of the 2D version of\nEqn.~(\\ref{eqn:neighbor_weighting}) is shown in\nFig.~\\ref{fig:weighted_color_plot}(b), where ${\\mathbf{x}}_j$ is located at\nthe center of the image and ${\\mathbf n}_j$ is aligned with the blue\nline. The higher the intensity, the higher the value\nEqn.~(\\ref{eqn:neighbor_weighting}) produces at a given pixel location.\n\nNext, consider the general case where the normal $\\mathbf{n}_j$ at \n${\\mathbf{x}}_j$ is unavailable. Here, let ${\\mathbf{K}}_j$ at ${\\mathbf{x}}_j$ be any \nsecond-order symmetric tensor, which is typically initialized as \nan identity matrix if no normal information is available. \n\nTo compute $\\mathbf{K}_i$ given ${\\mathbf{K}}_j$, we consider equivalently \nthe set of all possible unit normals $\\{ {\\mathbf{n}}_{\\theta j} \\}$ associated with \nthe corresponding length $\\{ \\tau_{\\theta j} \\}$ which make up \n${\\mathbf{K}}_j$ at ${\\mathbf{x}}_j$, where  $\\{ {\\mathbf{n}}_{\\theta j} \\}$ and $\\{ \\tau_{\\theta j} \\}$ \nare respectively indexed by all possible directions $\\theta$. \nEach $\\tau_{\\theta j} {\\mathbf{n}}_{\\theta j}$ postulates a normal vote \n${\\bf v}_{\\theta}({\\mathbf{x}}_i,{\\mathbf{x}}_j)$ at ${\\mathbf{x}}_i$ under the same\nsmoothness constraint prescribed by the corresponding arc of the \nosculating circle as illustrated in Fig.~\\ref{fig:general}.\n\nLet ${\\mathbf{S}}_{ij}$ be the second-order symmetric {\\bf tensor vote} obtained\nat ${\\mathbf{x}}_i$ due to this complete set of normals at ${\\mathbf{x}}_j$ defined above. \nWe have\n\n", "itemtype": "equation", "pos": 17634, "prevtext": "\nwhere\n\n", "index": 3, "text": "\\begin{equation}\n\\label{eqn:neighbor_weighting_Gaussian}\nc_{ij} = \\exp( - \\frac{||{\\mathbf{x}}_i - {\\mathbf{x}}_j ||^2 }{ \\sigma_d })\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E2.m1\" class=\"ltx_Math\" alttext=\"c_{ij}=\\exp(-\\frac{||{\\mathbf{x}}_{i}-{\\mathbf{x}}_{j}||^{2}}{\\sigma_{d}})\" display=\"block\"><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>exp</mi><mo>\u2061</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>-</mo><mfrac><msup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mn>2</mn></msup><msub><mi>\u03c3</mi><mi>d</mi></msub></mfrac></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere\n\n", "itemtype": "equation", "pos": 20134, "prevtext": "\nis an exponential function using Euclidean distance for attenuating \nthe strength based on proximity.\n$\\sigma_d$ is the size of local neighborhood (or the scale parameter,\nthe only free parameter in tensor voting).\n\nIn Eqn.~(\\ref{eqn:neighbor_weighting}), \n${\\mathbf r}_{ij} \\in \\Bbb{R}^d$ is a \nunit vector at ${\\mathbf{x}}_j$ pointing to ${\\mathbf{x}}_i$, and\n$1 - ( {\\mathbf r}_{ij}^T {\\mathbf{n}}_j )^2$ is a squared-sine\nfunction\\footnote{\n$\\sin^2 \\rho = 1 - \\cos^2 \\rho$, where $\\cos^2 \\rho= ({\\mathbf\nr}_{ij}^T {\\mathbf{n}}_j)^2$ and $\\rho$ is the angle between\n${\\mathbf r }_{ij}$ and ${\\mathbf{n}}_j$.} for attenuating the\ncontribution according to curvature. Similar to the original\ntensor voting framework, Eqn.~(\\ref{eqn:neighbor_weighting}) favors\nnearby neighbors that produce small-curvature connections,\nthus encoding the smoothness constraint. A plot of the 2D version of\nEqn.~(\\ref{eqn:neighbor_weighting}) is shown in\nFig.~\\ref{fig:weighted_color_plot}(b), where ${\\mathbf{x}}_j$ is located at\nthe center of the image and ${\\mathbf n}_j$ is aligned with the blue\nline. The higher the intensity, the higher the value\nEqn.~(\\ref{eqn:neighbor_weighting}) produces at a given pixel location.\n\nNext, consider the general case where the normal $\\mathbf{n}_j$ at \n${\\mathbf{x}}_j$ is unavailable. Here, let ${\\mathbf{K}}_j$ at ${\\mathbf{x}}_j$ be any \nsecond-order symmetric tensor, which is typically initialized as \nan identity matrix if no normal information is available. \n\nTo compute $\\mathbf{K}_i$ given ${\\mathbf{K}}_j$, we consider equivalently \nthe set of all possible unit normals $\\{ {\\mathbf{n}}_{\\theta j} \\}$ associated with \nthe corresponding length $\\{ \\tau_{\\theta j} \\}$ which make up \n${\\mathbf{K}}_j$ at ${\\mathbf{x}}_j$, where  $\\{ {\\mathbf{n}}_{\\theta j} \\}$ and $\\{ \\tau_{\\theta j} \\}$ \nare respectively indexed by all possible directions $\\theta$. \nEach $\\tau_{\\theta j} {\\mathbf{n}}_{\\theta j}$ postulates a normal vote \n${\\bf v}_{\\theta}({\\mathbf{x}}_i,{\\mathbf{x}}_j)$ at ${\\mathbf{x}}_i$ under the same\nsmoothness constraint prescribed by the corresponding arc of the \nosculating circle as illustrated in Fig.~\\ref{fig:general}.\n\nLet ${\\mathbf{S}}_{ij}$ be the second-order symmetric {\\bf tensor vote} obtained\nat ${\\mathbf{x}}_i$ due to this complete set of normals at ${\\mathbf{x}}_j$ defined above. \nWe have\n\n", "index": 5, "text": "\\begin{equation}\n\\label{eqn:S_integration}\n{\\mathbf{S}}_{ij} = \\int_{ \\mathbf{N}_{\\theta j} \\in \\nu } {\\bf v}_{\\theta}({\\mathbf{x}}_i, {\\mathbf{x}}_j )\n{\\bf v}_{\\theta}({\\mathbf{x}}_i, {\\mathbf{x}}_j)^T \\eta({\\mathbf{x}}_i, {\\mathbf{x}}_j, {\\mathbf{n}}_{\\theta j}) d\\mathbf{N}_{\\theta j}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=\\int_{\\mathbf{N}_{\\theta j}\\in\\nu}{\\bf v}_{\\theta}({\\mathbf{%&#10;x}}_{i},{\\mathbf{x}}_{j}){\\bf v}_{\\theta}({\\mathbf{x}}_{i},{\\mathbf{x}}_{j})^{%&#10;T}\\eta({\\mathbf{x}}_{i},{\\mathbf{x}}_{j},{\\mathbf{n}}_{\\theta j})d\\mathbf{N}_{%&#10;\\theta j}\" display=\"block\"><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>\ud835\udc0d</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2208</mo><mi>\u03bd</mi></mrow></msub><mrow><msub><mi>\ud835\udc2f</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\ud835\udc2f</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo>\u2062</mo><mi>\u03b7</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo>,</mo><msub><mi>\ud835\udc27</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\ud835\udc0d</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nand $\\nu$ is the space containing all possible $\\mathbf{N}_{\\theta\nj}$. For example, if $\\nu$ is 2D, the complete set of unit normals\n${\\mathbf{n}}_{\\theta}$ describes a unit circle. If $\\nu$ is 3D, the complete\nset of unit normals ${\\mathbf{n}}_{\\theta}$ describes a unit sphere\\footnote{\nThe domain of integration $\\nu$ represents the space of stick tensors given \nby ${\\mathbf{n}}_{\\theta j}$. Note that $d > 1$;  alternatively, it can be \nunderstood by expressing $\\mathbf{N}_{\\theta j}$ using polar coordinates; \nand thus in $N$ dimensions, $\\theta = (\\phi_1, \\phi_2, \\cdots, \\phi_{n-1})$. \n\nIt follows naturally we do not use $\\theta$  to define the integration\ndomain, because rather than simply writing\n\n$ \\int_{ \\mathbf{N}_{\\theta j} \\in \\nu } \\cdots d  \\mathbf{N}_{\\theta j} $,\n   it would have been\n\n$  \\int_{\\phi_1} \\int_{\\phi_2}  \\cdots \\int_{\\phi_{n-1}}  \\cdots  d\\phi_{n-1} d\\phi_{n-2} \\cdots d\\phi_1 $\nmaking the derivation of the proof of Theorem 1 more complicated.\n}.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.3\\linewidth]{fig03.png} \n\\end{center}\n\\caption{\\small\nIllustration of Eqn. (\\ref{eqn:v}). Normal vote ${\\mathbf v}_i = {\\mathbf v}_\\theta( {\\mathbf{x}}_i, {\\mathbf{x}}_j)$ \nreceived at ${\\mathbf{x}}_i$ using the arc of the osculating circle between ${\\mathbf{x}}_i$ and ${\\mathbf{x}}_j$, \nconsidering one of the normal voters at ${\\mathbf{x}}_j$ is ${\\mathbf{n}}_{\\theta j}$. Here,\n${\\mathbf{n}}_{\\theta j}$ and ${\\mathbf r}$ are unit vectors.\n}\n\\label{fig:general}\n\\vspace{-0.2in}\n\\end{figure}\n\n\nIn a typical tensor voting implementation, Eqn.~(\\ref{eqn:S_integration}) is\nprecomputed as discrete voting fields\n(e.g., plate and ball voting fields in 3D tensor voting~\\cite{book_with_names}):\nthe integration is implemented by rotating and summing the\ncontributions using matrix addition.\nAlthough precomputed once, such discrete approximations\ninvolve uniform and dense sampling of tensor votes\n${\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ in higher dimensions where\nthe number of dimensions depends on the problem.\nIn the following section, we will prove a closed-form solution to\nEqn.~(\\ref{eqn:S_integration}), which provides an efficient and\nexact solution to computing ${\\mathbf{K}}$ without resorting to discrete\nand dense sampling.\n\n\n\n\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{fig04.png} \n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small (a) {\\bf 3D ball voting field}. A slice generated using the closed-form solution Eqn~(\\ref{eqn:cal_S}), which has similar tensor orientations (but different tensor strengths) as the ball voting field in~\\cite{book_with_names}. \n(b) {\\bf 3D plate voting field}. Left: a cut of the voting \nfield (direction of $\\hat{e}_3$\nnormal to the page). Right: a cut of the same voting field,\nshowing the $(\\lambda_2 - \\lambda_3) \\hat{e}_3$ component\n(i.e., component parallel to the tangent direction). The field is generated by\nusing Eqn~(\\ref{eqn:cal_S}) showing similar tensor orientations as the\nplate voting field in~\\cite{book_with_names}. \n(c) {\\bf 3D stick voting field}. A slice after zeroing out votes lying in\nthe 45-degree zone as done in~\\cite{book_with_names}. The stick tensor\norientations shown in the figure are identical to those in the 3D stick voting field in\n\\cite{book_with_names}.\n(d) Vote computation using\nthe closed-form solution in one single step by Eqn~(\\ref{eqn:cal_S}).\n\n}\n\\label{fig:illus}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\n\\section{Closed-Form Solution}\n\n\\begin{theorem}\n\\emph{(Closed-Form Solution to Tensor Voting)}\n\\label{thm:cftv}\nThe tensor vote at ${\\mathbf{x}}_i$ induced by ${\\mathbf{K}}_j$ located at ${\\mathbf{x}}_j$ is given by\nthe following closed-form solution:\n\n", "itemtype": "equation", "pos": 20443, "prevtext": "\nwhere\n\n", "index": 7, "text": "\\begin{equation}\n\\mathbf{N}_{\\theta j} = {\\mathbf{n}}_{\\theta j} {\\mathbf{n}}_{\\theta j}^T\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E4.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{N}_{\\theta j}={\\mathbf{n}}_{\\theta j}{\\mathbf{n}}_{\\theta j}^{T}\" display=\"block\"><mrow><msub><mi>\ud835\udc0d</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>\ud835\udc27</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mrow><mi>\u03b8</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>T</mi></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere ${\\mathbf{K}}_j$ is a second-order symmetric tensor,\n${\\mathbf{R}}_{ij} ={\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$,\n${\\mathbf{R}}'_{ij} = ({\\mathbf I} - \\frac{1}{2} {\\mathbf r}_{ij} {\\mathbf\nr}_{ij}^T) {\\mathbf{R}}_{ij}$, \n${\\mathbf I}$ is an identity,\n${\\mathbf r}_{ij}$ is a unit vector pointing from\n${\\mathbf{x}}_j$ to ${\\mathbf{x}}_i$ and $c_{ij} = \\exp( - \\frac{||{\\mathbf{x}}_i - {\\mathbf{x}}_j ||^2 }{ \\sigma_d })$\nwith $\\sigma_d$ as the scale parameter.\n\\end{theorem}\n\n\n\\begin{proof}\n\nFor simplicity of notation, set ${\\mathbf r} = {\\mathbf\nr}_{ij}$, ${\\mathbf{n}}_{\\theta} = {\\mathbf{n}}_{\\theta j}$ and ${\\mathbf N}_{\\theta}\n= {\\mathbf N}_{\\theta j}$. Now, using the above-mentioned\nosculating arc connection,\n${\\bf v}_{\\theta}({\\mathbf{x}}_i, {\\mathbf{x}}_j)$ can be expressed as\n\n", "itemtype": "equation", "pos": 24268, "prevtext": "\nand $\\nu$ is the space containing all possible $\\mathbf{N}_{\\theta\nj}$. For example, if $\\nu$ is 2D, the complete set of unit normals\n${\\mathbf{n}}_{\\theta}$ describes a unit circle. If $\\nu$ is 3D, the complete\nset of unit normals ${\\mathbf{n}}_{\\theta}$ describes a unit sphere\\footnote{\nThe domain of integration $\\nu$ represents the space of stick tensors given \nby ${\\mathbf{n}}_{\\theta j}$. Note that $d > 1$;  alternatively, it can be \nunderstood by expressing $\\mathbf{N}_{\\theta j}$ using polar coordinates; \nand thus in $N$ dimensions, $\\theta = (\\phi_1, \\phi_2, \\cdots, \\phi_{n-1})$. \n\nIt follows naturally we do not use $\\theta$  to define the integration\ndomain, because rather than simply writing\n\n$ \\int_{ \\mathbf{N}_{\\theta j} \\in \\nu } \\cdots d  \\mathbf{N}_{\\theta j} $,\n   it would have been\n\n$  \\int_{\\phi_1} \\int_{\\phi_2}  \\cdots \\int_{\\phi_{n-1}}  \\cdots  d\\phi_{n-1} d\\phi_{n-2} \\cdots d\\phi_1 $\nmaking the derivation of the proof of Theorem 1 more complicated.\n}.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.3\\linewidth]{fig03.png} \n\\end{center}\n\\caption{\\small\nIllustration of Eqn. (\\ref{eqn:v}). Normal vote ${\\mathbf v}_i = {\\mathbf v}_\\theta( {\\mathbf{x}}_i, {\\mathbf{x}}_j)$ \nreceived at ${\\mathbf{x}}_i$ using the arc of the osculating circle between ${\\mathbf{x}}_i$ and ${\\mathbf{x}}_j$, \nconsidering one of the normal voters at ${\\mathbf{x}}_j$ is ${\\mathbf{n}}_{\\theta j}$. Here,\n${\\mathbf{n}}_{\\theta j}$ and ${\\mathbf r}$ are unit vectors.\n}\n\\label{fig:general}\n\\vspace{-0.2in}\n\\end{figure}\n\n\nIn a typical tensor voting implementation, Eqn.~(\\ref{eqn:S_integration}) is\nprecomputed as discrete voting fields\n(e.g., plate and ball voting fields in 3D tensor voting~\\cite{book_with_names}):\nthe integration is implemented by rotating and summing the\ncontributions using matrix addition.\nAlthough precomputed once, such discrete approximations\ninvolve uniform and dense sampling of tensor votes\n${\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ in higher dimensions where\nthe number of dimensions depends on the problem.\nIn the following section, we will prove a closed-form solution to\nEqn.~(\\ref{eqn:S_integration}), which provides an efficient and\nexact solution to computing ${\\mathbf{K}}$ without resorting to discrete\nand dense sampling.\n\n\n\n\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{fig04.png} \n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small (a) {\\bf 3D ball voting field}. A slice generated using the closed-form solution Eqn~(\\ref{eqn:cal_S}), which has similar tensor orientations (but different tensor strengths) as the ball voting field in~\\cite{book_with_names}. \n(b) {\\bf 3D plate voting field}. Left: a cut of the voting \nfield (direction of $\\hat{e}_3$\nnormal to the page). Right: a cut of the same voting field,\nshowing the $(\\lambda_2 - \\lambda_3) \\hat{e}_3$ component\n(i.e., component parallel to the tangent direction). The field is generated by\nusing Eqn~(\\ref{eqn:cal_S}) showing similar tensor orientations as the\nplate voting field in~\\cite{book_with_names}. \n(c) {\\bf 3D stick voting field}. A slice after zeroing out votes lying in\nthe 45-degree zone as done in~\\cite{book_with_names}. The stick tensor\norientations shown in the figure are identical to those in the 3D stick voting field in\n\\cite{book_with_names}.\n(d) Vote computation using\nthe closed-form solution in one single step by Eqn~(\\ref{eqn:cal_S}).\n\n}\n\\label{fig:illus}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\n\\section{Closed-Form Solution}\n\n\\begin{theorem}\n\\emph{(Closed-Form Solution to Tensor Voting)}\n\\label{thm:cftv}\nThe tensor vote at ${\\mathbf{x}}_i$ induced by ${\\mathbf{K}}_j$ located at ${\\mathbf{x}}_j$ is given by\nthe following closed-form solution:\n\n", "index": 9, "text": "\\begin{equation*}\n{\\mathbf{S}}_{ij} = c_{ij} {\\mathbf{R}}_{ij} {\\mathbf{K}}_j {\\mathbf{R}}'_{ij}\n\\end{equation*}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex1.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=c_{ij}{\\mathbf{R}}_{ij}{\\mathbf{K}}_{j}{\\mathbf{R}}^{\\prime}%&#10;_{ij}\" display=\"block\"><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc0a</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nRecall that ${\\mathbf{n}}_{\\theta}$ is the unit normal at ${\\mathbf{x}}_j$ with\ndirection $\\theta$, and that $\\tau_{\\theta}$ is the length\nassociated with the normal.  This vector subtraction equation is \nshown in Fig.~\\ref{fig:general} where the roles of \n${\\bf v}_{\\theta}, {\\mathbf{n}}_{\\theta},  \\mathbf{r}$, and $\\tau_{\\theta}$ \nare illustrated.\n\nLet\n\n", "itemtype": "equation", "pos": 25190, "prevtext": "\nwhere ${\\mathbf{K}}_j$ is a second-order symmetric tensor,\n${\\mathbf{R}}_{ij} ={\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$,\n${\\mathbf{R}}'_{ij} = ({\\mathbf I} - \\frac{1}{2} {\\mathbf r}_{ij} {\\mathbf\nr}_{ij}^T) {\\mathbf{R}}_{ij}$, \n${\\mathbf I}$ is an identity,\n${\\mathbf r}_{ij}$ is a unit vector pointing from\n${\\mathbf{x}}_j$ to ${\\mathbf{x}}_i$ and $c_{ij} = \\exp( - \\frac{||{\\mathbf{x}}_i - {\\mathbf{x}}_j ||^2 }{ \\sigma_d })$\nwith $\\sigma_d$ as the scale parameter.\n\\end{theorem}\n\n\n\\begin{proof}\n\nFor simplicity of notation, set ${\\mathbf r} = {\\mathbf\nr}_{ij}$, ${\\mathbf{n}}_{\\theta} = {\\mathbf{n}}_{\\theta j}$ and ${\\mathbf N}_{\\theta}\n= {\\mathbf N}_{\\theta j}$. Now, using the above-mentioned\nosculating arc connection,\n${\\bf v}_{\\theta}({\\mathbf{x}}_i, {\\mathbf{x}}_j)$ can be expressed as\n\n", "index": 11, "text": "\\begin{equation}\n{\\bf v}_{\\theta}({\\mathbf{x}}_i, {\\mathbf{x}}_j) = ( {\\mathbf{n}}_{\\theta} - 2 \\mathbf{r}\n(\\mathbf{r}^T {\\mathbf{n}}_{\\theta}) ) \\tau_{\\theta}\n\\label{eqn:v}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E5.m1\" class=\"ltx_Math\" alttext=\"{\\bf v}_{\\theta}({\\mathbf{x}}_{i},{\\mathbf{x}}_{j})=({\\mathbf{n}}_{\\theta}-2%&#10;\\mathbf{r}(\\mathbf{r}^{T}{\\mathbf{n}}_{\\theta}))\\tau_{\\theta}\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc2f</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>\ud835\udc31</mi><mi>i</mi></msub><mo>,</mo><msub><mi>\ud835\udc31</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>\ud835\udc2b</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>\ud835\udc2b</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msub><mi>\u03c4</mi><mi>\u03b8</mi></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $\\mathbf{I}$ is an identity, we can rewrite\nEqn.~(\\ref{eqn:S_integration}) into the following\nform:\n\n", "itemtype": "equation", "pos": 25735, "prevtext": "\nRecall that ${\\mathbf{n}}_{\\theta}$ is the unit normal at ${\\mathbf{x}}_j$ with\ndirection $\\theta$, and that $\\tau_{\\theta}$ is the length\nassociated with the normal.  This vector subtraction equation is \nshown in Fig.~\\ref{fig:general} where the roles of \n${\\bf v}_{\\theta}, {\\mathbf{n}}_{\\theta},  \\mathbf{r}$, and $\\tau_{\\theta}$ \nare illustrated.\n\nLet\n\n", "index": 13, "text": "\\begin{equation}\n\\label{eqn:closed_derived_start} \\mathbf{R} = ( \\mathbf{I} - 2\n\\mathbf{r} \\mathbf{r}^T ),\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E6.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{R}=(\\mathbf{I}-2\\mathbf{r}\\mathbf{r}^{T}),\" display=\"block\"><mrow><mrow><mi>\ud835\udc11</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><mrow><mn>2</mn><mo>\u2062</mo><msup><mi>\ud835\udc2b\ud835\udc2b</mi><mi>T</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\n\n\nFollowing the derivation:\n\\begin{eqnarray}\n\\nonumber {\\mathbf{S}}_{ij} &=& c_{ij} \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{R} {\\mathbf{n}}_{\\theta} ( 1 - ({\\mathbf{n}}_{\\theta}^T \\mathbf{r})^2  )  {\\mathbf{n}}_{\\theta}^T \\mathbf{R}^T d\\mathbf{N}_{\\theta} \\\\\n\\nonumber  &=& c_{ij} \\mathbf{R} \\left( \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 {\\mathbf{n}}_{\\theta} ( 1 - {\\mathbf{n}}_{\\theta}^T \\mathbf{r} \\mathbf{r}^T {\\mathbf{n}}_{\\theta} )  {\\mathbf{n}}_{\\theta}^T d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\\\\\n\\nonumber &=& c_{ij} \\mathbf{R} \\left( \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} - \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T \\\\\n\\label{eqn:cf_inter} &=& c_{ij} \\mathbf{R} \\left(  \\mathbf{K}_j -\n\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2\n\\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}\nd\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\n\\end{eqnarray}\nThe integration can be solved by integration by parts. Let\n$f(\\theta) = \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}$, $f'(\\theta) =\n\\tau_{\\theta}^2 \\mathbf{I}$, $g(\\theta) = \\frac{1}{2} \\mathbf{r}\n\\mathbf{r}^T \\mathbf{N}_{\\theta}^2 $ and $g'(\\theta) = \\mathbf{r}\n\\mathbf{r}^T \\mathbf{N}_{\\theta} $, and note that $\n\\mathbf{N}_{\\theta}^q = \\mathbf{N}_{\\theta}$ for all $q \\in\n\\Bbb{Z}^+$ \n(see this footnote\\footnote{The derivation is as follows $\\mathbf{N}_{\\theta}^q = {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T \\cdots  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta} \\cdot 1 \\cdot 1 \\cdots 1 \\cdot  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T = \\mathbf{N}_{\\theta}$.}), \nand ${\\mathbf K}_j$, \nin the most general form, can be\nexpressed as a generic tensor $\\int_{\\mathbf{N}_{\\theta} \\in \\nu}\n\\tau_{\\theta}^2 \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta}$. So we\nhave\n\\begin{eqnarray}\n\\nonumber & &\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ f(\\theta) g(\\theta) \\right]_{\\mathbf{N}_{\\theta} \\in \\nu} - \\int_{\\mathbf{N}_{\\theta} \\in \\nu} f'(\\theta) g(\\theta) d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ \\frac{1}{2} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2\n\\right]_{\\mathbf{N}_{\\theta} \\in \\nu}\n- \\frac{1}{2} \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\\\\n\n\n\n\n\n\n\n\n\n\\nonumber &=& \\frac{1}{2} \\int_{{\\mathbf N}_{\\theta} \\in \\nu} \\left( \\tau_{\\theta}^2 \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf N}_{\\theta}] {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2\n+ \\tau_{\\theta}^2 {\\mathbf N}_{\\theta} \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2] \\right) d{\\mathbf N}_{\\theta} \\\\\n\\nonumber & & - \\frac{1}{2} {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j.\n\\end{eqnarray}\n\nThe explanation for the last equality above is given in this footnote\\footnote{\nHere, we rewrite the first term by the product rule for derivative\nand the fundamental theorem of calculus and then express part of the\nsecond term by a generic tensor. We obtain:\n", "itemtype": "equation", "pos": 25963, "prevtext": "\nwhere $\\mathbf{I}$ is an identity, we can rewrite\nEqn.~(\\ref{eqn:S_integration}) into the following\nform:\n\n", "index": 15, "text": "\\begin{equation}\n\\label{eqn:closed_derive_start} {\\mathbf{S}}_{ij} = c_{ij}\n\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{R}\n{\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T \\mathbf{R}^T ( 1 - ({\\mathbf{n}}_{\\theta}^T\n\\mathbf{r})^2  ) d\\mathbf{N}_{\\theta} .\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E7.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=c_{ij}\\int_{\\mathbf{N}_{\\theta}\\in\\nu}\\tau_{\\theta}^{2}%&#10;\\mathbf{R}{\\mathbf{n}}_{\\theta}{\\mathbf{n}}_{\\theta}^{T}\\mathbf{R}^{T}(1-({%&#10;\\mathbf{n}}_{\\theta}^{T}\\mathbf{r})^{2})d\\mathbf{N}_{\\theta}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub><mo>\u2208</mo><mi>\u03bd</mi></mrow></msub><mrow><msubsup><mi>\u03c4</mi><mi>\u03b8</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc11\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup><mo>\u2062</mo><msup><mi>\ud835\udc11</mi><mi>T</mi></msup><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc2b</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n}.\n\nFinally, we apply the fact that $\\mathbf{N}_{\\theta}^q = \\mathbf{N}_{\\theta}$ (for all $q \\in\n\\Bbb{Z}^+$) to convert $\\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2]$\ninto $\\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}]$. We obtain\n\\begin{eqnarray}\n\\nonumber & & \\frac{1}{2} \\int_{{\\mathbf N}_{\\theta} \\in \\nu}\n\\left( \\tau_{\\theta}^2 {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2 +\n\\tau_{\\theta}^2 {\\mathbf N}_{\\theta}\n{\\mathbf r} {\\mathbf r}^T \\right) d{\\mathbf N}_{\\theta} - \\frac{1}{2} {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j \\\\\n\\nonumber &=& \\frac{1}{2} \\left( {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j + {\\mathbf K}_j {\\mathbf r} {\\mathbf r}^T - {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j  \\right) \\\\\n\\label{eqn:int_by_part_result} &=& \\frac{1}{2} {\\mathbf K}_j \\mathbf{r} \\mathbf{r}^T.\n\\end{eqnarray}\nBy substituting Eqn. (\\ref{eqn:int_by_part_result}) back to Eqn. (\\ref{eqn:cf_inter}),\nwe obtain the result as follows:\n\n", "itemtype": "equation", "pos": 29532, "prevtext": "\n\n\n\nFollowing the derivation:\n\\begin{eqnarray}\n\\nonumber {\\mathbf{S}}_{ij} &=& c_{ij} \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{R} {\\mathbf{n}}_{\\theta} ( 1 - ({\\mathbf{n}}_{\\theta}^T \\mathbf{r})^2  )  {\\mathbf{n}}_{\\theta}^T \\mathbf{R}^T d\\mathbf{N}_{\\theta} \\\\\n\\nonumber  &=& c_{ij} \\mathbf{R} \\left( \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 {\\mathbf{n}}_{\\theta} ( 1 - {\\mathbf{n}}_{\\theta}^T \\mathbf{r} \\mathbf{r}^T {\\mathbf{n}}_{\\theta} )  {\\mathbf{n}}_{\\theta}^T d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\\\\\n\\nonumber &=& c_{ij} \\mathbf{R} \\left( \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} - \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T \\\\\n\\label{eqn:cf_inter} &=& c_{ij} \\mathbf{R} \\left(  \\mathbf{K}_j -\n\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2\n\\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}\nd\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\n\\end{eqnarray}\nThe integration can be solved by integration by parts. Let\n$f(\\theta) = \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}$, $f'(\\theta) =\n\\tau_{\\theta}^2 \\mathbf{I}$, $g(\\theta) = \\frac{1}{2} \\mathbf{r}\n\\mathbf{r}^T \\mathbf{N}_{\\theta}^2 $ and $g'(\\theta) = \\mathbf{r}\n\\mathbf{r}^T \\mathbf{N}_{\\theta} $, and note that $\n\\mathbf{N}_{\\theta}^q = \\mathbf{N}_{\\theta}$ for all $q \\in\n\\Bbb{Z}^+$ \n(see this footnote\\footnote{The derivation is as follows $\\mathbf{N}_{\\theta}^q = {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T \\cdots  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta} \\cdot 1 \\cdot 1 \\cdots 1 \\cdot  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T = \\mathbf{N}_{\\theta}$.}), \nand ${\\mathbf K}_j$, \nin the most general form, can be\nexpressed as a generic tensor $\\int_{\\mathbf{N}_{\\theta} \\in \\nu}\n\\tau_{\\theta}^2 \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta}$. So we\nhave\n\\begin{eqnarray}\n\\nonumber & &\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ f(\\theta) g(\\theta) \\right]_{\\mathbf{N}_{\\theta} \\in \\nu} - \\int_{\\mathbf{N}_{\\theta} \\in \\nu} f'(\\theta) g(\\theta) d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ \\frac{1}{2} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2\n\\right]_{\\mathbf{N}_{\\theta} \\in \\nu}\n- \\frac{1}{2} \\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\\\\n\n\n\n\n\n\n\n\n\n\\nonumber &=& \\frac{1}{2} \\int_{{\\mathbf N}_{\\theta} \\in \\nu} \\left( \\tau_{\\theta}^2 \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf N}_{\\theta}] {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2\n+ \\tau_{\\theta}^2 {\\mathbf N}_{\\theta} \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2] \\right) d{\\mathbf N}_{\\theta} \\\\\n\\nonumber & & - \\frac{1}{2} {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j.\n\\end{eqnarray}\n\nThe explanation for the last equality above is given in this footnote\\footnote{\nHere, we rewrite the first term by the product rule for derivative\nand the fundamental theorem of calculus and then express part of the\nsecond term by a generic tensor. We obtain:\n", "index": 17, "text": "\n\\[\n\\frac{1}{2} \\int_{{\\mathbf N}_{\\theta} \\in \\nu}\n\\frac{d}{d{\\mathbf N}_{\\theta}}[ \\tau_{\\theta}^2 {\\mathbf N}_{\\theta}\n{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2 ] d{\\mathbf N}_{\\theta} - \\frac{1}{2} {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j.\n\\]\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\int_{{\\mathbf{N}}_{\\theta}\\in\\nu}\\frac{d}{d{\\mathbf{N}}_{\\theta}}[%&#10;\\tau_{\\theta}^{2}{\\mathbf{N}}_{\\theta}{\\mathbf{r}}{\\mathbf{r}}^{T}{\\mathbf{N}}%&#10;_{\\theta}^{2}]d{\\mathbf{N}}_{\\theta}-\\frac{1}{2}{\\mathbf{r}}{\\mathbf{r}}^{T}{%&#10;\\mathbf{K}}_{j}.\" display=\"block\"><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><mrow><msub><mo largeop=\"true\" symmetric=\"true\">\u222b</mo><mrow><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub><mo>\u2208</mo><mi>\u03bd</mi></mrow></msub><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mo>\u2062</mo><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub></mrow></mfrac><mo>\u2062</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mi>\u03c4</mi><mi>\u03b8</mi><mn>2</mn></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msup><mi>\ud835\udc2b\ud835\udc2b</mi><mi>T</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc0d</mi><mi>\u03b8</mi><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow><mo>\u2062</mo><mrow><mo>\ud835\udc51</mo><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub></mrow></mrow></mrow></mrow><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mi>\ud835\udc2b\ud835\udc2b</mi><mi>T</mi></msup><mo>\u2062</mo><msub><mi>\ud835\udc0a</mi><mi>j</mi></msub></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nReplace $\\mathbf{r}$ by ${\\mathbf r}_{ij}$ such that ${\\mathbf{R}}_{ij} =\n{\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$ and let\n${\\mathbf{R}}'_{ij} = ({\\mathbf I} - \\frac{1}{2} {\\mathbf r}_{ij} {\\mathbf\nr}_{ij}^T) {\\mathbf{R}}_{ij}$, we obtain\n\n", "itemtype": "equation", "pos": 30781, "prevtext": "\n}.\n\nFinally, we apply the fact that $\\mathbf{N}_{\\theta}^q = \\mathbf{N}_{\\theta}$ (for all $q \\in\n\\Bbb{Z}^+$) to convert $\\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2]$\ninto $\\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}]$. We obtain\n\\begin{eqnarray}\n\\nonumber & & \\frac{1}{2} \\int_{{\\mathbf N}_{\\theta} \\in \\nu}\n\\left( \\tau_{\\theta}^2 {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2 +\n\\tau_{\\theta}^2 {\\mathbf N}_{\\theta}\n{\\mathbf r} {\\mathbf r}^T \\right) d{\\mathbf N}_{\\theta} - \\frac{1}{2} {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j \\\\\n\\nonumber &=& \\frac{1}{2} \\left( {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j + {\\mathbf K}_j {\\mathbf r} {\\mathbf r}^T - {\\mathbf r} {\\mathbf r}^T {\\mathbf K}_j  \\right) \\\\\n\\label{eqn:int_by_part_result} &=& \\frac{1}{2} {\\mathbf K}_j \\mathbf{r} \\mathbf{r}^T.\n\\end{eqnarray}\nBy substituting Eqn. (\\ref{eqn:int_by_part_result}) back to Eqn. (\\ref{eqn:cf_inter}),\nwe obtain the result as follows:\n\n", "index": 19, "text": "\\begin{equation}\n{\\mathbf{S}}_{ij}= c_{ij} {\\mathbf{R}}  {\\mathbf{K}}_j \\left(  \\mathbf{I} - \\frac{1}{2}  \\mathbf{r} \\mathbf{r}^T \\right) {\\mathbf{R}}^T. \\\\\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E8.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=c_{ij}{\\mathbf{R}}{\\mathbf{K}}_{j}\\left(\\mathbf{I}-\\frac{1}{%&#10;2}\\mathbf{r}\\mathbf{r}^{T}\\right){\\mathbf{R}}^{T}.\\\\&#10;\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc11\ud835\udc0a</mi><mi>j</mi></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><mi>\ud835\udc08</mi><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msup><mi>\ud835\udc2b\ud835\udc2b</mi><mi>T</mi></msup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msup><mi>\ud835\udc11</mi><mi>T</mi></msup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\\end{proof}\n\nA {\\em structure-aware tensor} ${\\mathbf{K}}_i = \\sum_j {\\mathbf{S}}_{ij}$ can thus be assigned at\neach site ${\\mathbf{x}}_i$. This tensor sum considers both geometric proximity\nand smoothness constraints in the presence of neighbors ${\\mathbf{x}}_j$ under\nthe chosen scale of analysis. Note also that Eqn. (\\ref{eqn:cal_S}) is an\nexact equivalent of Eqn. (\\ref{eqn:S_integration}),\nor (\\ref{eqn:closed_derive_start}), that is, the first principle.\nSince the first principle produces a positive semi-definite matrix, Eqn.\n(\\ref{eqn:cal_S}) still produces a positive semi-definite matrix. \n\nIn tensor voting,\neigen-decomposition is applied to a structure-aware tensor. In\nthree dimensions, the eigensystem has eigenvalues\n$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3 \\ge 0$ with the corresponding\neigenvectors $\\hat{e}_1, \\hat{e}_2$, and $\\hat{e}_3$. $\\lambda_1 - \\lambda_2$\ndenotes surface saliency with normal direction indicated by $\\hat{e}_1$;\n$\\lambda_2 - \\lambda_3$ denotes curve saliency with tangent direction\nindicated by $\\hat{e}_3$; junction saliency is indicated by\n$\\lambda_3$.\n\nWhile it may be difficult to observe any geometric intuition directly\nfrom this closed-form solution,  the geometric meaning of the\nclosed-form solution has been described by Eqn. (\\ref{eqn:S_integration})\n(or (\\ref{eqn:closed_derive_start}), the first principle),\nsince Eqn. (\\ref{eqn:cal_S}) is equivalent to Eqn.~(\\ref{eqn:S_integration}). Note that our solution is different from, for instance,~\\cite{jmlr},\nwhere the $N$-D formulation is approached from a more geometric point of view.\n\n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.55\\linewidth]{fig05.png}\n\\end{center}\n\\caption{\\small Close-ups of ball voting fields generated using the original tensor voting\nframework (left) and CFTV (right).}\n\\label{fig:ball_cmp}\n\\vspace{-0.2in}\n\\end{figure}\n\nAs will be shown in the next section on EMTV, the inverse of ${\\mathbf{K}}_{j}$ is used.\nIn case of a perfect stick tensor, which can be equivalently represented\nas a rank-1 matrix, does not have an inverse. Similar in spirit where a\nGaussian function can be interpreted as an impulse function associated\nwith a spread representing uncertainty, a similar statistical approach is \nadopted here in characterizing our tensor inverse. Specifically,\nthe uncertainty is incorporated using a ball tensor, \nwhere $\\epsilon {\\mathbf I}$ is added to ${\\mathbf{K}}_{j}$, \n$\\epsilon$ is a small positive constant (0.001) and ${\\mathbf I}$ an identity matrix.\nFig.~\\ref{fig:inlier_outlier_illustration} shows a tensor \nand its inverse for some selected cases.\nThe following corollary regarding the inverse of ${\\mathbf{S}}_{ij}$ is useful:\n\\begin{corollary}\nLet ${\\mathbf{R}}{''}_{ij} = {\\mathbf{R}}_{ij} ({\\mathbf I} +\n{\\mathbf r}_{ij} {\\mathbf r}_{ij}^T)$ and also note that ${\\mathbf\nR}_{ij}^{-1} = {\\mathbf R}_{ij}$, the corresponding inverse of ${\\mathbf{S}}_{ij}$\nis:\n\n\n", "itemtype": "equation", "pos": 31204, "prevtext": "\nReplace $\\mathbf{r}$ by ${\\mathbf r}_{ij}$ such that ${\\mathbf{R}}_{ij} =\n{\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$ and let\n${\\mathbf{R}}'_{ij} = ({\\mathbf I} - \\frac{1}{2} {\\mathbf r}_{ij} {\\mathbf\nr}_{ij}^T) {\\mathbf{R}}_{ij}$, we obtain\n\n", "index": 21, "text": "\\begin{equation}\n{\\mathbf{S}}_{ij} = c_{ij} {\\mathbf{R}}_{ij} {\\mathbf{K}}_j {\\mathbf{R}}'_{ij}.\n\\label{eqn:cal_S}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E9.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=c_{ij}{\\mathbf{R}}_{ij}{\\mathbf{K}}_{j}{\\mathbf{R}}^{\\prime}%&#10;_{ij}.\" display=\"block\"><mrow><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc0a</mi><mi>j</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup></mrow></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\\end{corollary}\n\n\\begin{proof}\nThis corollary  can simply be proved by applying inverse\nto Eqn (\\ref{eqn:cal_S}).\n\\end{proof}\n\nNote the initial ${\\mathbf{K}}_j$ can be either derived when input \ndirection is available, or simply assigned as an\nidentity matrix otherwise.\n\n\n\\subsection{Examples}\n\nUsing Eqn~(\\ref{eqn:cal_S}), given any input ${\\mathbf{K}}_j$ at site $j$\nwhich is a second-order symmetric tensor, the output\ntensor ${\\mathbf{S}}_{ij}$ can be computed directly. Note that ${\\mathbf{R}}_{ij}$\nis a $d \\times d$ matrix of the same dimensionality $d$ as ${\\mathbf{K}}_j$.\nTo verify our closed-form solution, we perform the following \nto compare with the voting fields used by the original\ntensor voting framework (Fig.~\\ref{fig:illus}):\n\n\n\\begin{itemize}\n\\item[(a)] Set ${\\mathbf{K}}$ to be an identity (ball tensor) in Eqn~(\\ref{eqn:cal_S})\nand compute all votes ${\\mathbf{S}}$ in a neighhorhood. This procedure generates the\nball voting field, Fig.~\\ref{fig:illus}(a).\n\\item[(b)] Set ${\\mathbf{K}}$ to be a plate tensor\n$\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]$\nin Eqn~(\\ref{eqn:cal_S}) and compute all votes ${\\mathbf{S}}$ in a neighborhood.\nThis procedure generates the plate voting field, Fig.~\\ref{fig:illus}(b).\n\\item[(c)] Set ${\\mathbf{K}}$ to be a stick tensor\n$\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]$\nin Eqn~(\\ref{eqn:cal_S}) and compute all votes ${\\mathbf{S}}$ in a neighborhood.\nThis procedure generates the stick voting field, Fig.~\\ref{fig:illus}(c).\n\\item[(d)] Set ${\\mathbf{K}}$ to be {\\em any} generic second-order tensor in\nEqn~(\\ref{eqn:cal_S}) to compute a tensor vote ${\\mathbf{S}}$ at a given site.\nWe do not need a voting field, or the somewhat complex procedure described\nin~\\cite{jmlr}. In one single step using the closed-form solution\nEqn~(\\ref{eqn:cal_S}), we obtain ${\\mathbf{S}}$ as shown in Fig.~\\ref{fig:illus}(d).\n\\end{itemize}\n\n\nNote that the stick voting field generation is the same as the closed-form \nsolution given by the arc of an osculating circle. On the other hand, since the\nclosed-form solution does not remove votes lying beyond the 45-degree zone \nas done in the original framework, it is useful\nto compare the ball voting field generated using the CFTV and the original \nframework. Fig.~\\ref{fig:ball_cmp} shows the close-ups of the ball voting fields \ngenerated using the original framework and CFTV. As anticipated, the tensor \norientations are almost the same (with the maximum angular deviation \nat $4.531^\\circ$), while the tensor strength is different due to the use of \ndifferent decay functions. The new computation results in perfect vote \norientations which are radial, and the angular discrepancies are due \nto the discrete approximations in the original solution.\n\nWhile the above illustrates the usage of Eqn~(\\ref{eqn:cal_S})\nin three dimensions, the equation applies to any dimensions $d$. All\nof the ${\\mathbf{S}}$'s returned by Eqn~(\\ref{eqn:cal_S}) are second-order symmetric\ntensors and can be decomposed using eigen-decomposition.\nThe implementation of Eqn~(\\ref{eqn:cal_S}) is a matter\nof a few lines of C++ code.\n\n\n\n\n\n\nOur ``voting without voting fields'' method is uniform to \nany input tensors ${\\mathbf{K}}_j$ that are second-order symmetric tensor \nin its closed-form expressed by Eqn~(\\ref{eqn:cal_S}), \nwhere formal mathematical operation\ncan be applied on this compact equation, which is otherwise\ndifficult on the algorithmic procedure described in previous tensor\nvoting papers.\nNotably, using the closed-form solution,\nwe are now able to prove mathematically the convergence of tensor voting\nin the next section.\n\n\n\n\n\n\n\n\\vspace{-0.1in}\n\n\\subsection{Time Complexity}\nAkin to the original tensor voting formalism, each site (input or\nnon-input) communicates with each other on an Markov random field \n(MRF) in a broad sense, where the number of edges depends on the scale\nof analysis, parameterized by $\\sigma_d$ in Eqn\n(\\ref{eqn:neighbor_weighting_Gaussian}). In our implementation,\nwe use an efficient data structure such as ANN tree~\\cite{ann} to\naccess a constant number of neighbors ${\\mathbf{x}}_j$ of each ${\\mathbf{x}}_i$.\nIt should be noted that under a large scale of analysis where\nthe number of neighbors is sufficiently large, similar number\nof neighbors are accessed in ours and the original tensor voting\nimplementation.\n\nThe speed of accessing nearest neighbors can be greatly increased\n(polylogarithmic) by using ANN thus making efficient the computation of a\nstructure-aware tensor.  Note that the running time\nfor this implementation of closed-from solution is $O(d^3)$, while\nthe running time for the original tensor voting (TV) is $O( u^{d-1} )$,\nwhere $d$ is the dimension of the space and $u$ is the number\nof sampling directions for a given dimension. Because of this,\na typical TV implementation precomputes and stores the dense tensor fields.\nFor example, when $d = 3$ and $u = 180$ for high accuracy, our\nmethod requires 27 operation units, while  a typical TV implementation\nrequires 32400 operation units.\nGiven 1980 points and the same number of neighbors,\nthe time to compute a structure-aware tensor using our method\nis about $0.0001$ second; it takes about $0.1$ second for\na typical TV implementation to output the corresponding tensor.\nThe measurement was performed on a computer running on a core \nduo 2GHz CPU with 2GB RAM.\n\nNote that the asymptotic running time for the improved TV in~\\cite{jmlr} is\n$O( d \\gamma^2 )$ since it applies Gramm-Schmidt process to perform\ncomponent decomposition, where $\\gamma$ is the number of linearly independent\nset of the tensors.\nIn most of the cases, $\\gamma = d$. So, the running time for our method is\ncomparable to~\\cite{jmlr}. However, their approach does not have a\nprecise mathematical solution.\n\n\\section{MRFTV}\n\nWe have proved CFTV for the special theory of tensor voting, or the\n``first voting pass'' for structure inference. Conventionally, tensor\nvoting was done in two passes, where the second pass was used for\nstructure propagation in the preferred direction after disabling the ball\ncomponent in the structure-aware tensor. What happens if more tensor\nvoting passes are applied? This has never been answered properly.\n\nIn this section we provide a convergence proof for tensor voting based \non CFTV: the structure-aware tensor obtained at each site achieves \na stationary state upon convergence. Our convergence proof makes \nuse of Markov random fields (MRF), thus termed as MRFTV. \n\nIt should be noted that the original tensor voting formulation is also\nconstructed on an MRF according to the broad definition, since random\nvariables (that is, the tensors after voting) are defined on the nodes\nof an undirected graph in which each node is connected to all neighbors\nwithin a fixed distance.  On the other hand, without CFTV, it was\npreviously difficult to write down an objective function and to prove\nthe convergence.  One caveat to note in the following \nis that we do not disable the ball component in each iteration, which will be\naddressed in the future in developing the general theory of tensor voting\nin structure propagation.\nAs we will demonstrate, MRFTV does not smooth out important features \n(Fig.~\\ref{fig:L}) and still possesses high outlier rejection ability \n(Fig.~\\ref{fig:filtering}).\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.98\\linewidth]{fig06.png} \n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small Convergence of MRF-TV. From left to right: input points,\nresult after 2 passes, result after convergence (10 iterations). Visually,\nthis simple example distinguishes tensor voting from smoothing as the\nsharp orientation discontinuity is preserved upon convergence. \n}\n\\label{fig:L}\n\\vspace{-0.1in}\n\\end{figure}\n\nRecall in MRF, a Markov network is a graph consisting of two types of nodes --\na set of hidden variables ${\\mathbf{E}}$ and a set of observed variables ${\\mathbf{O}}$, where\nthe edges of the graph are described by the\nfollowing posterior probability $\\text{P} ({\\mathbf{E}}|{\\mathbf{O}})$ with\nstandard Bayesian framework:\n\n", "itemtype": "equation", "pos": 34263, "prevtext": "\n\\end{proof}\n\nA {\\em structure-aware tensor} ${\\mathbf{K}}_i = \\sum_j {\\mathbf{S}}_{ij}$ can thus be assigned at\neach site ${\\mathbf{x}}_i$. This tensor sum considers both geometric proximity\nand smoothness constraints in the presence of neighbors ${\\mathbf{x}}_j$ under\nthe chosen scale of analysis. Note also that Eqn. (\\ref{eqn:cal_S}) is an\nexact equivalent of Eqn. (\\ref{eqn:S_integration}),\nor (\\ref{eqn:closed_derive_start}), that is, the first principle.\nSince the first principle produces a positive semi-definite matrix, Eqn.\n(\\ref{eqn:cal_S}) still produces a positive semi-definite matrix. \n\nIn tensor voting,\neigen-decomposition is applied to a structure-aware tensor. In\nthree dimensions, the eigensystem has eigenvalues\n$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3 \\ge 0$ with the corresponding\neigenvectors $\\hat{e}_1, \\hat{e}_2$, and $\\hat{e}_3$. $\\lambda_1 - \\lambda_2$\ndenotes surface saliency with normal direction indicated by $\\hat{e}_1$;\n$\\lambda_2 - \\lambda_3$ denotes curve saliency with tangent direction\nindicated by $\\hat{e}_3$; junction saliency is indicated by\n$\\lambda_3$.\n\nWhile it may be difficult to observe any geometric intuition directly\nfrom this closed-form solution,  the geometric meaning of the\nclosed-form solution has been described by Eqn. (\\ref{eqn:S_integration})\n(or (\\ref{eqn:closed_derive_start}), the first principle),\nsince Eqn. (\\ref{eqn:cal_S}) is equivalent to Eqn.~(\\ref{eqn:S_integration}). Note that our solution is different from, for instance,~\\cite{jmlr},\nwhere the $N$-D formulation is approached from a more geometric point of view.\n\n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.55\\linewidth]{fig05.png}\n\\end{center}\n\\caption{\\small Close-ups of ball voting fields generated using the original tensor voting\nframework (left) and CFTV (right).}\n\\label{fig:ball_cmp}\n\\vspace{-0.2in}\n\\end{figure}\n\nAs will be shown in the next section on EMTV, the inverse of ${\\mathbf{K}}_{j}$ is used.\nIn case of a perfect stick tensor, which can be equivalently represented\nas a rank-1 matrix, does not have an inverse. Similar in spirit where a\nGaussian function can be interpreted as an impulse function associated\nwith a spread representing uncertainty, a similar statistical approach is \nadopted here in characterizing our tensor inverse. Specifically,\nthe uncertainty is incorporated using a ball tensor, \nwhere $\\epsilon {\\mathbf I}$ is added to ${\\mathbf{K}}_{j}$, \n$\\epsilon$ is a small positive constant (0.001) and ${\\mathbf I}$ an identity matrix.\nFig.~\\ref{fig:inlier_outlier_illustration} shows a tensor \nand its inverse for some selected cases.\nThe following corollary regarding the inverse of ${\\mathbf{S}}_{ij}$ is useful:\n\\begin{corollary}\nLet ${\\mathbf{R}}{''}_{ij} = {\\mathbf{R}}_{ij} ({\\mathbf I} +\n{\\mathbf r}_{ij} {\\mathbf r}_{ij}^T)$ and also note that ${\\mathbf\nR}_{ij}^{-1} = {\\mathbf R}_{ij}$, the corresponding inverse of ${\\mathbf{S}}_{ij}$\nis:\n\n\n", "index": 23, "text": "\\begin{equation}\n\\label{eqn:cal_S_inv} {\\mathbf{S}}_{ij}' = c_{ij}^{-1}\n{\\mathbf{R}}{''}_{ij} {\\mathbf{K}}_j^{-1} {\\mathbf{R}}_{ij}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E10.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}^{\\prime}=c_{ij}^{-1}{\\mathbf{R}}{{}^{\\prime\\prime}}_{ij}{%&#10;\\mathbf{K}}_{j}^{-1}{\\mathbf{R}}_{ij}\" display=\"block\"><mrow><msubsup><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup><mo>=</mo><mrow><msubsup><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mi>\ud835\udc11</mi><mo>\u2062</mo><mmultiscripts><mi>\ud835\udc0a</mi><mi>j</mi><mrow><mo>-</mo><mn>1</mn></mrow><mprescripts/><none/><mi>\u2032\u2032</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><none/></mmultiscripts><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\nBy letting ${\\mathbf{E}} = \\{ {\\mathbf{K}}_i | i = 1,2,\\cdots,N \\}$ and\n${\\mathbf{O}} = \\{ \\tilde{{\\mathbf{K}}}_i | i = 1,2, \\cdots, N\\}$, where $N$ is total number of\npoints and $\\tilde{{\\mathbf{K}}}_i$ is the known tensor at ${\\mathbf{x}}_i$, and suppose that\ninliers follow Gaussian distribution, we obtain the \nlikelihood $\\text{P} ({\\mathbf{O}}|{\\mathbf{E}})$ and the prior $\\text{P}({\\mathbf{E}})$ as follows:\n\\begin{eqnarray}\n\\label{mrf1}\n\\text{P} ({\\mathbf{O}}|{\\mathbf{E}}) &\\propto& \\prod_i \\text{p} (\\tilde{{\\mathbf{K}}}_i | {\\mathbf{K}}_i) = \\prod_i e^{ - \\frac{ || {\\mathbf{K}}_i - \\tilde{{\\mathbf{K}}}_i ||_F^2 }{ \\sigma_h} } \\\\\n\\text{P}({\\mathbf{E}}) &\\propto& \\prod_i \\prod_{j \\in {\\cal N} (i)} \\text{p} ({\\mathbf{S}}_{ij}|{\\mathbf{K}}_i ) \\\\\n&=& \\prod_i \\prod_{j \\in {\\cal N} (i)} e^{- \\frac{ || {\\mathbf{K}}_i - {\\mathbf{S}}_{ij} ||_F^2 }{\\sigma_s} }\n\\label{mrf2}\n\\end{eqnarray}\nwhere $||\\cdot ||_F$ is the Frobenius norm,\n$\\tilde{{\\mathbf{K}}}_i$ is the known tensor\nat ${\\mathbf{x}}_i$, ${\\cal N} (i)$ is the set of neighbor corresponds to ${\\mathbf{x}}_i$ and\n$\\sigma_h$ and $\\sigma_s$ are two constants respectively. \n\nNote that we use the Frobenius norm to encode tensor orientation\nconsistency as well as to reflect the necessary vote saliency \ninformation including distance and continuity\nattenuation. For example, suppose we have a unit stick\ntensor at ${\\mathbf{x}}_i$ and a stick vote (received at ${\\mathbf{x}}_i$), which\nis parallel to it but with magnitude equal to 0.8. In another scenario\n${\\mathbf{x}}_i$ receives from a voter farther away a stick vote with\nthe same orientation but magnitude being equal to 0.2.\nThe Frobenius norm reflects the difference in saliency despite \nthe perfect orientation consistency in both cases.  Notwithstanding, it \nis arguable that Frobenius norm may not be the perfect solution\nto encode orientation consistency constraint in the pertinent \nequations, while this current form works acceptably well in our \nexperiments in practice.\n\nBy taking the logarithm of Eqn~(\\ref{eqn:bayesian}), we obtain \nthe following energy function:\n\n", "itemtype": "equation", "pos": 42542, "prevtext": "\n\\end{corollary}\n\n\\begin{proof}\nThis corollary  can simply be proved by applying inverse\nto Eqn (\\ref{eqn:cal_S}).\n\\end{proof}\n\nNote the initial ${\\mathbf{K}}_j$ can be either derived when input \ndirection is available, or simply assigned as an\nidentity matrix otherwise.\n\n\n\\subsection{Examples}\n\nUsing Eqn~(\\ref{eqn:cal_S}), given any input ${\\mathbf{K}}_j$ at site $j$\nwhich is a second-order symmetric tensor, the output\ntensor ${\\mathbf{S}}_{ij}$ can be computed directly. Note that ${\\mathbf{R}}_{ij}$\nis a $d \\times d$ matrix of the same dimensionality $d$ as ${\\mathbf{K}}_j$.\nTo verify our closed-form solution, we perform the following \nto compare with the voting fields used by the original\ntensor voting framework (Fig.~\\ref{fig:illus}):\n\n\n\\begin{itemize}\n\\item[(a)] Set ${\\mathbf{K}}$ to be an identity (ball tensor) in Eqn~(\\ref{eqn:cal_S})\nand compute all votes ${\\mathbf{S}}$ in a neighhorhood. This procedure generates the\nball voting field, Fig.~\\ref{fig:illus}(a).\n\\item[(b)] Set ${\\mathbf{K}}$ to be a plate tensor\n$\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]$\nin Eqn~(\\ref{eqn:cal_S}) and compute all votes ${\\mathbf{S}}$ in a neighborhood.\nThis procedure generates the plate voting field, Fig.~\\ref{fig:illus}(b).\n\\item[(c)] Set ${\\mathbf{K}}$ to be a stick tensor\n$\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{array}\\right]$\nin Eqn~(\\ref{eqn:cal_S}) and compute all votes ${\\mathbf{S}}$ in a neighborhood.\nThis procedure generates the stick voting field, Fig.~\\ref{fig:illus}(c).\n\\item[(d)] Set ${\\mathbf{K}}$ to be {\\em any} generic second-order tensor in\nEqn~(\\ref{eqn:cal_S}) to compute a tensor vote ${\\mathbf{S}}$ at a given site.\nWe do not need a voting field, or the somewhat complex procedure described\nin~\\cite{jmlr}. In one single step using the closed-form solution\nEqn~(\\ref{eqn:cal_S}), we obtain ${\\mathbf{S}}$ as shown in Fig.~\\ref{fig:illus}(d).\n\\end{itemize}\n\n\nNote that the stick voting field generation is the same as the closed-form \nsolution given by the arc of an osculating circle. On the other hand, since the\nclosed-form solution does not remove votes lying beyond the 45-degree zone \nas done in the original framework, it is useful\nto compare the ball voting field generated using the CFTV and the original \nframework. Fig.~\\ref{fig:ball_cmp} shows the close-ups of the ball voting fields \ngenerated using the original framework and CFTV. As anticipated, the tensor \norientations are almost the same (with the maximum angular deviation \nat $4.531^\\circ$), while the tensor strength is different due to the use of \ndifferent decay functions. The new computation results in perfect vote \norientations which are radial, and the angular discrepancies are due \nto the discrete approximations in the original solution.\n\nWhile the above illustrates the usage of Eqn~(\\ref{eqn:cal_S})\nin three dimensions, the equation applies to any dimensions $d$. All\nof the ${\\mathbf{S}}$'s returned by Eqn~(\\ref{eqn:cal_S}) are second-order symmetric\ntensors and can be decomposed using eigen-decomposition.\nThe implementation of Eqn~(\\ref{eqn:cal_S}) is a matter\nof a few lines of C++ code.\n\n\n\n\n\n\nOur ``voting without voting fields'' method is uniform to \nany input tensors ${\\mathbf{K}}_j$ that are second-order symmetric tensor \nin its closed-form expressed by Eqn~(\\ref{eqn:cal_S}), \nwhere formal mathematical operation\ncan be applied on this compact equation, which is otherwise\ndifficult on the algorithmic procedure described in previous tensor\nvoting papers.\nNotably, using the closed-form solution,\nwe are now able to prove mathematically the convergence of tensor voting\nin the next section.\n\n\n\n\n\n\n\n\\vspace{-0.1in}\n\n\\subsection{Time Complexity}\nAkin to the original tensor voting formalism, each site (input or\nnon-input) communicates with each other on an Markov random field \n(MRF) in a broad sense, where the number of edges depends on the scale\nof analysis, parameterized by $\\sigma_d$ in Eqn\n(\\ref{eqn:neighbor_weighting_Gaussian}). In our implementation,\nwe use an efficient data structure such as ANN tree~\\cite{ann} to\naccess a constant number of neighbors ${\\mathbf{x}}_j$ of each ${\\mathbf{x}}_i$.\nIt should be noted that under a large scale of analysis where\nthe number of neighbors is sufficiently large, similar number\nof neighbors are accessed in ours and the original tensor voting\nimplementation.\n\nThe speed of accessing nearest neighbors can be greatly increased\n(polylogarithmic) by using ANN thus making efficient the computation of a\nstructure-aware tensor.  Note that the running time\nfor this implementation of closed-from solution is $O(d^3)$, while\nthe running time for the original tensor voting (TV) is $O( u^{d-1} )$,\nwhere $d$ is the dimension of the space and $u$ is the number\nof sampling directions for a given dimension. Because of this,\na typical TV implementation precomputes and stores the dense tensor fields.\nFor example, when $d = 3$ and $u = 180$ for high accuracy, our\nmethod requires 27 operation units, while  a typical TV implementation\nrequires 32400 operation units.\nGiven 1980 points and the same number of neighbors,\nthe time to compute a structure-aware tensor using our method\nis about $0.0001$ second; it takes about $0.1$ second for\na typical TV implementation to output the corresponding tensor.\nThe measurement was performed on a computer running on a core \nduo 2GHz CPU with 2GB RAM.\n\nNote that the asymptotic running time for the improved TV in~\\cite{jmlr} is\n$O( d \\gamma^2 )$ since it applies Gramm-Schmidt process to perform\ncomponent decomposition, where $\\gamma$ is the number of linearly independent\nset of the tensors.\nIn most of the cases, $\\gamma = d$. So, the running time for our method is\ncomparable to~\\cite{jmlr}. However, their approach does not have a\nprecise mathematical solution.\n\n\\section{MRFTV}\n\nWe have proved CFTV for the special theory of tensor voting, or the\n``first voting pass'' for structure inference. Conventionally, tensor\nvoting was done in two passes, where the second pass was used for\nstructure propagation in the preferred direction after disabling the ball\ncomponent in the structure-aware tensor. What happens if more tensor\nvoting passes are applied? This has never been answered properly.\n\nIn this section we provide a convergence proof for tensor voting based \non CFTV: the structure-aware tensor obtained at each site achieves \na stationary state upon convergence. Our convergence proof makes \nuse of Markov random fields (MRF), thus termed as MRFTV. \n\nIt should be noted that the original tensor voting formulation is also\nconstructed on an MRF according to the broad definition, since random\nvariables (that is, the tensors after voting) are defined on the nodes\nof an undirected graph in which each node is connected to all neighbors\nwithin a fixed distance.  On the other hand, without CFTV, it was\npreviously difficult to write down an objective function and to prove\nthe convergence.  One caveat to note in the following \nis that we do not disable the ball component in each iteration, which will be\naddressed in the future in developing the general theory of tensor voting\nin structure propagation.\nAs we will demonstrate, MRFTV does not smooth out important features \n(Fig.~\\ref{fig:L}) and still possesses high outlier rejection ability \n(Fig.~\\ref{fig:filtering}).\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.98\\linewidth]{fig06.png} \n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small Convergence of MRF-TV. From left to right: input points,\nresult after 2 passes, result after convergence (10 iterations). Visually,\nthis simple example distinguishes tensor voting from smoothing as the\nsharp orientation discontinuity is preserved upon convergence. \n}\n\\label{fig:L}\n\\vspace{-0.1in}\n\\end{figure}\n\nRecall in MRF, a Markov network is a graph consisting of two types of nodes --\na set of hidden variables ${\\mathbf{E}}$ and a set of observed variables ${\\mathbf{O}}$, where\nthe edges of the graph are described by the\nfollowing posterior probability $\\text{P} ({\\mathbf{E}}|{\\mathbf{O}})$ with\nstandard Bayesian framework:\n\n", "index": 25, "text": "\\begin{equation}\n\\text{P} ({\\mathbf{E}}|{\\mathbf{O}}) \\propto \\text{P} ({\\mathbf{O}}|{\\mathbf{E}}) \\text{P}({\\mathbf{E}})\n\\label{eqn:bayesian}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E11.m1\" class=\"ltx_Math\" alttext=\"\\text{P}({\\mathbf{E}}|{\\mathbf{O}})\\propto\\text{P}({\\mathbf{O}}|{\\mathbf{E}})%&#10;\\text{P}({\\mathbf{E}})\" display=\"block\"><mrow><mtext>P</mtext><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc0e</mi><mo stretchy=\"false\">)</mo></mrow><mo>\u221d</mo><mtext>P</mtext><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc0e</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc04</mi><mo stretchy=\"false\">)</mo></mrow><mtext>P</mtext><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $g = \\frac{\\sigma_h}{\\sigma_s}$. Theoretically, this {\\em quadratic}\nenergy function can be directly solved by Singular Value\nDecomposition (SVD).  Since $N$ can be large thus making direct\nSVD impractical, we adopt an iterative approach: by taking\nthe partial derivative of Eqn~(\\ref{eqn:energy_function})\n(w.r.t. to ${\\mathbf{K}}_i$) the following update rule is obtained:\n\n", "itemtype": "equation", "pos": 44799, "prevtext": "\n\nBy letting ${\\mathbf{E}} = \\{ {\\mathbf{K}}_i | i = 1,2,\\cdots,N \\}$ and\n${\\mathbf{O}} = \\{ \\tilde{{\\mathbf{K}}}_i | i = 1,2, \\cdots, N\\}$, where $N$ is total number of\npoints and $\\tilde{{\\mathbf{K}}}_i$ is the known tensor at ${\\mathbf{x}}_i$, and suppose that\ninliers follow Gaussian distribution, we obtain the \nlikelihood $\\text{P} ({\\mathbf{O}}|{\\mathbf{E}})$ and the prior $\\text{P}({\\mathbf{E}})$ as follows:\n\\begin{eqnarray}\n\\label{mrf1}\n\\text{P} ({\\mathbf{O}}|{\\mathbf{E}}) &\\propto& \\prod_i \\text{p} (\\tilde{{\\mathbf{K}}}_i | {\\mathbf{K}}_i) = \\prod_i e^{ - \\frac{ || {\\mathbf{K}}_i - \\tilde{{\\mathbf{K}}}_i ||_F^2 }{ \\sigma_h} } \\\\\n\\text{P}({\\mathbf{E}}) &\\propto& \\prod_i \\prod_{j \\in {\\cal N} (i)} \\text{p} ({\\mathbf{S}}_{ij}|{\\mathbf{K}}_i ) \\\\\n&=& \\prod_i \\prod_{j \\in {\\cal N} (i)} e^{- \\frac{ || {\\mathbf{K}}_i - {\\mathbf{S}}_{ij} ||_F^2 }{\\sigma_s} }\n\\label{mrf2}\n\\end{eqnarray}\nwhere $||\\cdot ||_F$ is the Frobenius norm,\n$\\tilde{{\\mathbf{K}}}_i$ is the known tensor\nat ${\\mathbf{x}}_i$, ${\\cal N} (i)$ is the set of neighbor corresponds to ${\\mathbf{x}}_i$ and\n$\\sigma_h$ and $\\sigma_s$ are two constants respectively. \n\nNote that we use the Frobenius norm to encode tensor orientation\nconsistency as well as to reflect the necessary vote saliency \ninformation including distance and continuity\nattenuation. For example, suppose we have a unit stick\ntensor at ${\\mathbf{x}}_i$ and a stick vote (received at ${\\mathbf{x}}_i$), which\nis parallel to it but with magnitude equal to 0.8. In another scenario\n${\\mathbf{x}}_i$ receives from a voter farther away a stick vote with\nthe same orientation but magnitude being equal to 0.2.\nThe Frobenius norm reflects the difference in saliency despite \nthe perfect orientation consistency in both cases.  Notwithstanding, it \nis arguable that Frobenius norm may not be the perfect solution\nto encode orientation consistency constraint in the pertinent \nequations, while this current form works acceptably well in our \nexperiments in practice.\n\nBy taking the logarithm of Eqn~(\\ref{eqn:bayesian}), we obtain \nthe following energy function:\n\n", "index": 27, "text": "\\begin{equation}\nE({\\mathbf{E}}) = \\sum_i || {\\mathbf{K}}_i - \\tilde{{\\mathbf{K}}}_i ||_F^2 +\ng \\sum_i \\sum_{j \\in {\\cal N} (i)} || {\\mathbf{K}}_i - {\\mathbf{S}}_{ij} ||_F^2\n\\label{eqn:energy_function}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E12.m1\" class=\"ltx_Math\" alttext=\"E({\\mathbf{E}})=\\sum_{i}||{\\mathbf{K}}_{i}-\\tilde{{\\mathbf{K}}}_{i}||_{F}^{2}+%&#10;g\\sum_{i}\\sum_{j\\in{\\cal N}(i)}||{\\mathbf{K}}_{i}-{\\mathbf{S}}_{ij}||_{F}^{2}\" display=\"block\"><mrow><mrow><mi>E</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc04</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc0a</mi><mi>i</mi></msub><mo>-</mo><msub><mover accent=\"true\"><mi>\ud835\udc0a</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mi>i</mi></munder><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msub><mi>\ud835\udc0a</mi><mi>i</mi></msub><mo>-</mo><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhich is a Gauss-Seidel solution.\nWhen successive over-relaxation (SOR) is employed, the update rule becomes:\n\n", "itemtype": "equation", "pos": 45397, "prevtext": "\nwhere $g = \\frac{\\sigma_h}{\\sigma_s}$. Theoretically, this {\\em quadratic}\nenergy function can be directly solved by Singular Value\nDecomposition (SVD).  Since $N$ can be large thus making direct\nSVD impractical, we adopt an iterative approach: by taking\nthe partial derivative of Eqn~(\\ref{eqn:energy_function})\n(w.r.t. to ${\\mathbf{K}}_i$) the following update rule is obtained:\n\n", "index": 29, "text": "\\begin{equation}\n\\label{eqn:energy_update_rule}\n{\\mathbf{K}}_i^* = (\\tilde{{\\mathbf{K}}}_i + 2 g \\sum_{j \\in {\\cal N} (i)} {\\mathbf{S}}_{ij}) ({\\mathbf{I}} + g \\sum_{j \\in {\\cal N} (i)} ({\\mathbf{I}} + c_{ij}^2 {{\\mathbf{R}}_{ij}'}^2) )^{-1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E13.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{K}}_{i}^{*}=(\\tilde{{\\mathbf{K}}}_{i}+2g\\sum_{j\\in{\\cal N}(i)}{%&#10;\\mathbf{S}}_{ij})({\\mathbf{I}}+g\\sum_{j\\in{\\cal N}(i)}({\\mathbf{I}}+c_{ij}^{2}%&#10;{{\\mathbf{R}}_{ij}^{\\prime}}^{2}))^{-1}\" display=\"block\"><mrow><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mover accent=\"true\"><mi>\ud835\udc0a</mi><mo stretchy=\"false\">~</mo></mover><mi>i</mi></msub><mo>+</mo><mrow><mn>2</mn><mo>\u2062</mo><mi>g</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>+</mo><mrow><mi>g</mi><mo>\u2062</mo><mrow><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca9</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mo stretchy=\"false\">(</mo><mrow><mi>\ud835\udc08</mi><mo>+</mo><mrow><msubsup><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo>\u2062</mo><mmultiscripts><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo><none/><mn>2</mn></mmultiscripts></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $1 < q < 2$ is the SOR weight and $m$ is the iteration number.\nAfter each iteration, we normalize ${\\mathbf{K}}_i$ such that the eigenvalues \nof the corresponding eigensystem are within the range $(0, 1]$. \n\nThe above proof on convergence of MRF-TV \nshows that structure-aware tensors\nachieve stationary states after a finite number Gauss-Seidel \niterations in the above formulation.\nIt also dispels a common pitfall that tensor voting is similar in\neffect to smoothing. Using the same scale of analysis (that is, in\n(\\ref{eqn:neighbor_weighting_Gaussian})) and same $\\sigma_h$, $\\sigma_s$\nin each iteration, tensor saliency and orientation will both\nconverge.  We observe that the converged tensor orientation is in\nfact similar to that obtained after two voting passes using the\noriginal framework, where the orientations at curve junctions\nare not smoothed out. See Fig.~\\ref{fig:L} for an example where sharp\norientation discontinuity is not smoothed out when tensor voting\nconverges. Here, $\\lambda_1$ of each structure-aware tensor is not\nnormalized to 1 for visualizing its structure saliency after convergence. \nTable~\\ref{tab:conv} summarizes the quantitative comparison with the\nground-truth orientation.\n\n\\if 0\n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{cccc}\n\\hline\n& \\multicolumn{3}{c}{Tensor orientation angular error (in deg)} \\\\ \n& min & max & avg \\\\ \\hline\n2 passes & 0.000000 & 0.115430 & 0.027264 \\\\\nconverged & 0.000000 & 0.045051 & 0.010683 \\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\caption{\\small Comparison with ground truth for the example in Fig.~\\ref{fig:L}. }\n\\label{tab:conv}\n\\vspace{-0.2in}\n\\end{table}\n\\fi\n\n\\begin{table}[t]\n\\begin{center}\n\\includegraphics[width=0.75\\linewidth]{t1.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small Comparison with ground truth for the example in Fig.~\\ref{fig:L}. }\n\\label{tab:conv}\n\\vspace{-0.2in}\n\\end{table}\n\n\\section{EMTV}\n\n\n\n\n\nPreviously, while tensor voting was capable of rejecting outliers, it\nfell short of producing accurate parameter estimation, explaining the use\nof RANSAC in the final parameter estimation step after outlier\nrejection~\\cite{4dpami}.\n\nThis section describes the EMTV algorithm for optimizing\na) the structure-aware tensor ${\\mathbf{K}}$ at each input site, and b)\nthe parameters of a single plane ${\\mathbf{h}}$ of {\\em any} dimensionality\ncontaining the inliers.\nThis algorithm will be applied to stereo matching.\n\n\n\n\n\n\n\n\n\n\n\nWe first formulate the three constraints to be used in EMTV.\nThese constraints are not mutually exclusive, where knowing the\nvalues satisfying one constraint will help computing the values of the others. However, in\nour case, they are all unknowns, so EM is particularly suitable for\ntheir optimization, since the expectation calculation and parameter\nestimation are solved alternately.\n\n\n\\vspace{-0.15in}\n\n\\subsection{Constraints}\n\\label{sec:cases}\n\n\n\n\n\n\n\n\n\n\n\n\\noindent{\\bf Data constraint } Suppose we have a set of clean\ndata. One necessary objective is to minimize\nthe following for all ${\\mathbf{x}}_i \\in \\Bbb{R}^d$ with $d > 1$:\n\n", "itemtype": "equation", "pos": 45764, "prevtext": "\nwhich is a Gauss-Seidel solution.\nWhen successive over-relaxation (SOR) is employed, the update rule becomes:\n\n", "index": 31, "text": "\\begin{equation}\n\\label{eqn:sor_update_rule}\n{\\mathbf{K}}_i^{(m+1)} = (1 - q) {\\mathbf{K}}_i^{(m)} + q {\\mathbf{K}}_i^*\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E14.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{K}}_{i}^{(m+1)}=(1-q){\\mathbf{K}}_{i}^{(m)}+q{\\mathbf{K}}_{i}^{*}\" display=\"block\"><mrow><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>+</mo><mrow><mi>q</mi><mo>\u2062</mo><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mo>*</mo></msubsup></mrow></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere ${\\mathbf{h}} \\in \\Bbb{R}^d$ is a unit vector representing \nthe plane (or the model) to be estimated.\\footnote{\nNote that, in some cases, the underlying model is represented in this\nform ${\\mathbf{x}}_i^T {\\mathbf{h}} - z_i$ where we can re-arrange it into\nthe form given by Eqn.~(\\ref{eqn:data}). \n\n\nFor example, expand ${\\mathbf{x}}_i^T {\\mathbf{h}} - z_i$ into $ax_i + by_i + 1 z_i = 0$, \nwhich can written in the form of (\\ref{eqn:data}).\n}\n\n\n\nThis is a typical data term that measures the faithfulness of the input data to the fitting plane.\n\n\\noindent{\\bf Orientation consistency  } The plane being estimated is defined by\nthe vector ${\\mathbf{h}}$. Since the tensor ${\\mathbf{K}}_i \\in \\Bbb{R}^d \\times \\Bbb{R}^d$\nencodes structure awareness, if\n${\\mathbf{x}}_i$ is an inlier, the orientation information encoded by ${\\mathbf{K}}_i$ and ${\\mathbf{h}}$ have to be\nconsistent. That is, the variance ${\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}}$ produced by ${\\mathbf{h}}$\nshould be minimal.\nOtherwise, ${\\mathbf{x}}_i$ might be generated by other models even if it minimizes\nEqn~(\\ref{eqn:data}).\n\n\n\nMathematically, we minimize:\n\n", "itemtype": "equation", "pos": 48961, "prevtext": "\nwhere $1 < q < 2$ is the SOR weight and $m$ is the iteration number.\nAfter each iteration, we normalize ${\\mathbf{K}}_i$ such that the eigenvalues \nof the corresponding eigensystem are within the range $(0, 1]$. \n\nThe above proof on convergence of MRF-TV \nshows that structure-aware tensors\nachieve stationary states after a finite number Gauss-Seidel \niterations in the above formulation.\nIt also dispels a common pitfall that tensor voting is similar in\neffect to smoothing. Using the same scale of analysis (that is, in\n(\\ref{eqn:neighbor_weighting_Gaussian})) and same $\\sigma_h$, $\\sigma_s$\nin each iteration, tensor saliency and orientation will both\nconverge.  We observe that the converged tensor orientation is in\nfact similar to that obtained after two voting passes using the\noriginal framework, where the orientations at curve junctions\nare not smoothed out. See Fig.~\\ref{fig:L} for an example where sharp\norientation discontinuity is not smoothed out when tensor voting\nconverges. Here, $\\lambda_1$ of each structure-aware tensor is not\nnormalized to 1 for visualizing its structure saliency after convergence. \nTable~\\ref{tab:conv} summarizes the quantitative comparison with the\nground-truth orientation.\n\n\\if 0\n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{cccc}\n\\hline\n& \\multicolumn{3}{c}{Tensor orientation angular error (in deg)} \\\\ \n& min & max & avg \\\\ \\hline\n2 passes & 0.000000 & 0.115430 & 0.027264 \\\\\nconverged & 0.000000 & 0.045051 & 0.010683 \\\\ \\hline\n\\end{tabular}\n\\end{center}\n\\caption{\\small Comparison with ground truth for the example in Fig.~\\ref{fig:L}. }\n\\label{tab:conv}\n\\vspace{-0.2in}\n\\end{table}\n\\fi\n\n\\begin{table}[t]\n\\begin{center}\n\\includegraphics[width=0.75\\linewidth]{t1.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small Comparison with ground truth for the example in Fig.~\\ref{fig:L}. }\n\\label{tab:conv}\n\\vspace{-0.2in}\n\\end{table}\n\n\\section{EMTV}\n\n\n\n\n\nPreviously, while tensor voting was capable of rejecting outliers, it\nfell short of producing accurate parameter estimation, explaining the use\nof RANSAC in the final parameter estimation step after outlier\nrejection~\\cite{4dpami}.\n\nThis section describes the EMTV algorithm for optimizing\na) the structure-aware tensor ${\\mathbf{K}}$ at each input site, and b)\nthe parameters of a single plane ${\\mathbf{h}}$ of {\\em any} dimensionality\ncontaining the inliers.\nThis algorithm will be applied to stereo matching.\n\n\n\n\n\n\n\n\n\n\n\nWe first formulate the three constraints to be used in EMTV.\nThese constraints are not mutually exclusive, where knowing the\nvalues satisfying one constraint will help computing the values of the others. However, in\nour case, they are all unknowns, so EM is particularly suitable for\ntheir optimization, since the expectation calculation and parameter\nestimation are solved alternately.\n\n\n\\vspace{-0.15in}\n\n\\subsection{Constraints}\n\\label{sec:cases}\n\n\n\n\n\n\n\n\n\n\n\n\\noindent{\\bf Data constraint } Suppose we have a set of clean\ndata. One necessary objective is to minimize\nthe following for all ${\\mathbf{x}}_i \\in \\Bbb{R}^d$ with $d > 1$:\n\n", "index": 33, "text": "\\begin{equation}\n\\label{eqn:data}\n|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E15.m1\" class=\"ltx_Math\" alttext=\"||{\\mathbf{x}}_{i}^{T}{\\mathbf{h}}||\" display=\"block\"><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>\ud835\udc31</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi></mrow><mo fence=\"true\">||</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\n\n\n\n\\noindent{\\bf Neighborhood consistency  } While the estimated ${\\mathbf{K}}_i$ helps\nto indicate inlier/outlier information, ${\\mathbf{K}}_i$ has to be consistent with\nthe local structure imposed by its neighbors (when they are known).\nIf ${\\mathbf{K}}_i$ is consistent with ${\\mathbf{h}}$ but not the local neighborhood,\neither ${\\mathbf{h}}$ or ${\\mathbf{K}}_i$ is wrong. In practice, we minimize the\nfollowing Frobenius norm as in Eqns (\\ref{mrf1})--(\\ref{mrf2}):\n\n", "itemtype": "equation", "pos": 50190, "prevtext": "\nwhere ${\\mathbf{h}} \\in \\Bbb{R}^d$ is a unit vector representing \nthe plane (or the model) to be estimated.\\footnote{\nNote that, in some cases, the underlying model is represented in this\nform ${\\mathbf{x}}_i^T {\\mathbf{h}} - z_i$ where we can re-arrange it into\nthe form given by Eqn.~(\\ref{eqn:data}). \n\n\nFor example, expand ${\\mathbf{x}}_i^T {\\mathbf{h}} - z_i$ into $ax_i + by_i + 1 z_i = 0$, \nwhich can written in the form of (\\ref{eqn:data}).\n}\n\n\n\nThis is a typical data term that measures the faithfulness of the input data to the fitting plane.\n\n\\noindent{\\bf Orientation consistency  } The plane being estimated is defined by\nthe vector ${\\mathbf{h}}$. Since the tensor ${\\mathbf{K}}_i \\in \\Bbb{R}^d \\times \\Bbb{R}^d$\nencodes structure awareness, if\n${\\mathbf{x}}_i$ is an inlier, the orientation information encoded by ${\\mathbf{K}}_i$ and ${\\mathbf{h}}$ have to be\nconsistent. That is, the variance ${\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}}$ produced by ${\\mathbf{h}}$\nshould be minimal.\nOtherwise, ${\\mathbf{x}}_i$ might be generated by other models even if it minimizes\nEqn~(\\ref{eqn:data}).\n\n\n\nMathematically, we minimize:\n\n", "index": 35, "text": "\\begin{equation}\n\\label{eqn:orientation_consistency}\n|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E16.m1\" class=\"ltx_Math\" alttext=\"||{\\mathbf{h}}^{T}{\\mathbf{K}}_{i}^{-1}{\\mathbf{h}}||.\" display=\"block\"><mrow><mrow><mo fence=\"true\">||</mo><mrow><msup><mi>\ud835\udc21</mi><mi>T</mi></msup><mo>\u2062</mo><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi></mrow><mo fence=\"true\">||</mo></mrow><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nIn the spirit of MRF, ${\\mathbf{S}}'_{ij}$ encodes the tensor\ninformation within ${\\mathbf{x}}_i$'s neighborhood, thus a natural choice for\ndefining the term for measuring neighborhood orientation consistency.\nThis is also useful, as we will see, to the M-step of EMTV which makes\nthe MRF assumption.\n\n\nThe above three constraints will interact with each other in\nthe proposed EM algorithm.\n\n\\subsection{Objective Function}\nDefine ${\\mathbf{O}} = \\{ o_i = {\\mathbf{x}}_i | i = 1, \\cdots , N \\}$ to be the set of\nobservations. Our goal is to optimize ${\\mathbf{h}}$ and ${\\mathbf{K}}_i^{-1}$ given\n${\\mathbf{O}}$. Mathematically, we solve the objective function:\n\n", "itemtype": "equation", "pos": 50784, "prevtext": "\n\n\n\n\n\\noindent{\\bf Neighborhood consistency  } While the estimated ${\\mathbf{K}}_i$ helps\nto indicate inlier/outlier information, ${\\mathbf{K}}_i$ has to be consistent with\nthe local structure imposed by its neighbors (when they are known).\nIf ${\\mathbf{K}}_i$ is consistent with ${\\mathbf{h}}$ but not the local neighborhood,\neither ${\\mathbf{h}}$ or ${\\mathbf{K}}_i$ is wrong. In practice, we minimize the\nfollowing Frobenius norm as in Eqns (\\ref{mrf1})--(\\ref{mrf2}):\n\n", "index": 37, "text": "\\begin{equation}\n\\label{eqn:neighborhood_consistency}\n|| {\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij} ||_F.\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E17.m1\" class=\"ltx_Math\" alttext=\"||{\\mathbf{K}}_{i}^{-1}-{\\mathbf{S}}^{\\prime}_{ij}||_{F}.\" display=\"block\"><mrow><msub><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>-</mo><msubsup><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi></msub><mo>.</mo></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $P( {\\mathbf{O}}, {\\mathbf{R}} | \\Lambda )$ is the complete-data likelihood to\nbe maximized, ${\\mathbf{R}} = \\{ r_i \\}$ is a set of hidden states indicating\nif observation $o_i$ is an outlier ($r_i = 0$) or inlier ($r_i = 1$), and\n$\\Lambda = \\{ \\{ {\\mathbf{K}}_i^{-1} \\}, {\\mathbf{h}}, \\alpha, \\sigma, \\sigma_1, \\sigma_2 \\}$\nis a set of parameters to be estimated. $\\alpha$, $\\sigma$, $\\sigma_1$ and\n$\\sigma_2$ are parameters imposed by some distributions, which will\nbe explained shortly by using an equation to be introduced\\footnote{See the M-step in\nEqn~(\\ref{eqn:m_step_update}).}. Our EM algorithm estimates an optimal\n$\\Lambda^*$ by finding the value of the complete-data log likelihood\nwith respect to ${\\mathbf{R}}$ given ${\\mathbf{O}}$ and the current estimated\nparameters $\\Lambda'$:\n\n", "itemtype": "equation", "pos": 51565, "prevtext": "\nIn the spirit of MRF, ${\\mathbf{S}}'_{ij}$ encodes the tensor\ninformation within ${\\mathbf{x}}_i$'s neighborhood, thus a natural choice for\ndefining the term for measuring neighborhood orientation consistency.\nThis is also useful, as we will see, to the M-step of EMTV which makes\nthe MRF assumption.\n\n\nThe above three constraints will interact with each other in\nthe proposed EM algorithm.\n\n\\subsection{Objective Function}\nDefine ${\\mathbf{O}} = \\{ o_i = {\\mathbf{x}}_i | i = 1, \\cdots , N \\}$ to be the set of\nobservations. Our goal is to optimize ${\\mathbf{h}}$ and ${\\mathbf{K}}_i^{-1}$ given\n${\\mathbf{O}}$. Mathematically, we solve the objective function:\n\n", "index": 39, "text": "\\begin{equation}\n\\label{eqn:em_objective} \\Lambda^* = \\arg \\max_\\Lambda P( {\\mathbf{O}}, {\\mathbf{R}} |\n\\Lambda )\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E18.m1\" class=\"ltx_Math\" alttext=\"\\Lambda^{*}=\\arg\\max_{\\Lambda}P({\\mathbf{O}},{\\mathbf{R}}|\\Lambda)\" display=\"block\"><mrow><msup><mi mathvariant=\"normal\">\u039b</mi><mo>*</mo></msup><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi mathvariant=\"normal\">\u039b</mi></munder><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc0e</mi><mo>,</mo><mi>\ud835\udc11</mi><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $\\psi$ is a space containing all possible configurations of\n${\\mathbf{R}}$ of size $N$. Although EM does not guarantee a global optimal\nsolution theoretically, because CFTV provides good initialization,\nwe will demonstrate empirically that reasonable results can be obtained.\n\n\n\\vspace{-0.1in}\n\n\\subsection{ Expectation (E-Step) }\n\\label{sec:e_step} In this section, the marginal distribution $p(r_i\n| o_i, \\Lambda')$ will be defined so that we can maximize the\nparameters in the next step (M-Step) given the current parameters.\n\nIf $r_i = 1$, the observation $o_i$ is an inlier and therefore\nminimizes the first two conditions (Eqns~\\ref{eqn:data}\nand~\\ref{eqn:orientation_consistency}) in\nSection~\\ref{sec:cases}, that is, the data and orientation\nconstraints. In both cases, we assume that inliers follow a Gaussian\ndistribution which explains the use of ${\\mathbf{K}}_i^{-1}$ instead\nof ${\\mathbf{K}}_i$.\\footnote{Although a linear structure is being optimized here, \nthe inliers together may describe a structure that does not necessarily \nfollow any particular model. Each inlier may not exactly lie on this \nstructure where the misalignment follows the Gaussian distribution.}\nWe model ${\\rm p}( o_i | r_i, \\Lambda' )$ as\n\\begin{eqnarray}\n\\propto \\left\\{\n\\begin{array}{ll}\n  \\exp(-\\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } ) \\exp(-\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ),\n& \\hbox{if $r_i = 1$;} \\\\\n    \\frac{1}{C}, & \\hbox{if $r_i = 0$.}\n\\label{eqn:o_i_prob}\n\\end{array}\n\\right.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\nWe assume that outliers follow uniform distribution where $C$ \nis a constant that models the distribution. Let $C_m$ be the \nmaximum dimension of the bounding box of the input. In practice, \n$C_m \\leq C \\leq 2 C_m$ produces similar results. \n\nSince we have no prior information on a point being an inlier \nor outlier, we may assume that the mixture probability of the \nobservations $p(r_i = 1) = p(r_i = 0)$ equals to a constant \n$\\alpha = 0.5$ such that we have no bias to either category \n(inlier/outlier). \n\n\nFor generality in the following\nwe will include $\\alpha$ in the derivation.\n\nDefine $w_i = p(r_i | o_i, \\Lambda')$ to be the probability of $o_i$\nbeing an inlier. Then\n\n\\if 0\n\\begin{eqnarray}\n\\nonumber w_i &= & {\\rm p}(r_i =1 | o_i, \\Lambda') = \n\\frac{ {\\rm p}( o_i, r_i = 1 | \\Lambda' ) } { {\\rm p}( o_i | \\Lambda') } \\\\\n& = & \\frac{ \\alpha ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } ) \\exp( -\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) } { \\alpha ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 }  ) \n\\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || }{ 2 \\sigma_1^2 } ) \n+ \\frac{ 1 - \\alpha } {C} }\n\\label{eqn:e_step_update} \n\\end{eqnarray}\n\\fi\n\n\\begin{eqnarray}\n\\nonumber w_i &= & {\\rm p}(r_i =1 | o_i, \\Lambda') =\n\\frac{ {\\rm p}( o_i, r_i = 1 | \\Lambda' ) } { {\\rm p}( o_i | \\Lambda') } \\\\\n& = & \\frac{ \\alpha \\beta ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } )    \\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) } { \\alpha \\beta ~   \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 }  )   \\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || }{ 2 \\sigma_1^2 } )\n+ \\frac{ 1 - \\alpha } {C} }\n\\label{eqn:e_step_update}\n\\end{eqnarray}\nwhere $\\beta = \\frac{1}{2 \\sigma \\sigma_1 \\pi}$ is the normalization term.\n\n\n\\vspace{-0.1in}\n\n\\subsection{ Maximization (M-Step) }\n\\label{sec:m_step} In the M-Step, we maximize\nEqn.~(\\ref{eqn:q_function}) using $w_i$ obtained from the E-Step.\nSince neighborhood information is considered, we model $P({\\mathbf{O}}, {\\mathbf{R}} |\n\\Lambda )$ as a MRF:\n\n", "itemtype": "equation", "pos": 52495, "prevtext": "\nwhere $P( {\\mathbf{O}}, {\\mathbf{R}} | \\Lambda )$ is the complete-data likelihood to\nbe maximized, ${\\mathbf{R}} = \\{ r_i \\}$ is a set of hidden states indicating\nif observation $o_i$ is an outlier ($r_i = 0$) or inlier ($r_i = 1$), and\n$\\Lambda = \\{ \\{ {\\mathbf{K}}_i^{-1} \\}, {\\mathbf{h}}, \\alpha, \\sigma, \\sigma_1, \\sigma_2 \\}$\nis a set of parameters to be estimated. $\\alpha$, $\\sigma$, $\\sigma_1$ and\n$\\sigma_2$ are parameters imposed by some distributions, which will\nbe explained shortly by using an equation to be introduced\\footnote{See the M-step in\nEqn~(\\ref{eqn:m_step_update}).}. Our EM algorithm estimates an optimal\n$\\Lambda^*$ by finding the value of the complete-data log likelihood\nwith respect to ${\\mathbf{R}}$ given ${\\mathbf{O}}$ and the current estimated\nparameters $\\Lambda'$:\n\n", "index": 41, "text": "\\begin{equation}\n\\label{eqn:q_function} Q( \\Lambda, \\Lambda' ) = \\sum_{{\\mathbf{R}} \\in \\psi}\n\\log P({\\mathbf{O}}, {\\mathbf{R}} | \\Lambda ) P({\\mathbf{R}} | {\\mathbf{O}}, \\Lambda')\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E19.m1\" class=\"ltx_Math\" alttext=\"Q(\\Lambda,\\Lambda^{\\prime})=\\sum_{{\\mathbf{R}}\\in\\psi}\\log P({\\mathbf{O}},{%&#10;\\mathbf{R}}|\\Lambda)P({\\mathbf{R}}|{\\mathbf{O}},\\Lambda^{\\prime})\" display=\"block\"><mrow><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">\u039b</mi><mo>,</mo><msup><mi mathvariant=\"normal\">\u039b</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u2211</mo><mrow><mi>\ud835\udc11</mi><mo>\u2208</mo><mi>\u03c8</mi></mrow></munder><mi>log</mi><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc0e</mi><mo>,</mo><mi>\ud835\udc11</mi><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc11</mi><mo stretchy=\"false\">|</mo><mi>\ud835\udc0e</mi><mo>,</mo><msup><mi mathvariant=\"normal\">\u039b</mi><mo>\u2032</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere ${\\cal G}(i)$ is the set of neighbors of $i$. In theory,\n${\\cal G}(i)$ contains all the input points except $i$, since\n$c_{ij}$ in Eqn.~(\\ref{eqn:neighbor_weighting_Gaussian}) is always\nnon-zero (because of the long tail of the Gaussian distribution). In\npractice, we can prune away the points in ${\\cal G}(i)$ where the\nvalues of $c_{ij}$ are negligible. This can greatly reduce the size\nof the neighborhood. Again, using ANN tree~\\cite{ann}, the speed of\nsearching for nearest neighbors can be greatly increased.\n\nLet us examine the two terms in Eqn.~(\\ref{eqn:mrf}).\n$p(o_i | r_i, \\Lambda)$ has been\ndefined in Eqn.~(\\ref{eqn:o_i_prob}). We define $p(r_i | r_j,\n\\Lambda)$ here. Using the third condition mentioned in\nEqn.~(\\ref{eqn:neighborhood_consistency}), we have:\n\n", "itemtype": "equation", "pos": 56460, "prevtext": "\nwhere $\\psi$ is a space containing all possible configurations of\n${\\mathbf{R}}$ of size $N$. Although EM does not guarantee a global optimal\nsolution theoretically, because CFTV provides good initialization,\nwe will demonstrate empirically that reasonable results can be obtained.\n\n\n\\vspace{-0.1in}\n\n\\subsection{ Expectation (E-Step) }\n\\label{sec:e_step} In this section, the marginal distribution $p(r_i\n| o_i, \\Lambda')$ will be defined so that we can maximize the\nparameters in the next step (M-Step) given the current parameters.\n\nIf $r_i = 1$, the observation $o_i$ is an inlier and therefore\nminimizes the first two conditions (Eqns~\\ref{eqn:data}\nand~\\ref{eqn:orientation_consistency}) in\nSection~\\ref{sec:cases}, that is, the data and orientation\nconstraints. In both cases, we assume that inliers follow a Gaussian\ndistribution which explains the use of ${\\mathbf{K}}_i^{-1}$ instead\nof ${\\mathbf{K}}_i$.\\footnote{Although a linear structure is being optimized here, \nthe inliers together may describe a structure that does not necessarily \nfollow any particular model. Each inlier may not exactly lie on this \nstructure where the misalignment follows the Gaussian distribution.}\nWe model ${\\rm p}( o_i | r_i, \\Lambda' )$ as\n\\begin{eqnarray}\n\\propto \\left\\{\n\\begin{array}{ll}\n  \\exp(-\\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } ) \\exp(-\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ),\n& \\hbox{if $r_i = 1$;} \\\\\n    \\frac{1}{C}, & \\hbox{if $r_i = 0$.}\n\\label{eqn:o_i_prob}\n\\end{array}\n\\right.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\nWe assume that outliers follow uniform distribution where $C$ \nis a constant that models the distribution. Let $C_m$ be the \nmaximum dimension of the bounding box of the input. In practice, \n$C_m \\leq C \\leq 2 C_m$ produces similar results. \n\nSince we have no prior information on a point being an inlier \nor outlier, we may assume that the mixture probability of the \nobservations $p(r_i = 1) = p(r_i = 0)$ equals to a constant \n$\\alpha = 0.5$ such that we have no bias to either category \n(inlier/outlier). \n\n\nFor generality in the following\nwe will include $\\alpha$ in the derivation.\n\nDefine $w_i = p(r_i | o_i, \\Lambda')$ to be the probability of $o_i$\nbeing an inlier. Then\n\n\\if 0\n\\begin{eqnarray}\n\\nonumber w_i &= & {\\rm p}(r_i =1 | o_i, \\Lambda') = \n\\frac{ {\\rm p}( o_i, r_i = 1 | \\Lambda' ) } { {\\rm p}( o_i | \\Lambda') } \\\\\n& = & \\frac{ \\alpha ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } ) \\exp( -\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) } { \\alpha ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 }  ) \n\\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || }{ 2 \\sigma_1^2 } ) \n+ \\frac{ 1 - \\alpha } {C} }\n\\label{eqn:e_step_update} \n\\end{eqnarray}\n\\fi\n\n\\begin{eqnarray}\n\\nonumber w_i &= & {\\rm p}(r_i =1 | o_i, \\Lambda') =\n\\frac{ {\\rm p}( o_i, r_i = 1 | \\Lambda' ) } { {\\rm p}( o_i | \\Lambda') } \\\\\n& = & \\frac{ \\alpha \\beta ~ \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 } )    \\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) } { \\alpha \\beta ~   \\exp( - \\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 }{ 2 \\sigma^2 }  )   \\exp( - \\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || }{ 2 \\sigma_1^2 } )\n+ \\frac{ 1 - \\alpha } {C} }\n\\label{eqn:e_step_update}\n\\end{eqnarray}\nwhere $\\beta = \\frac{1}{2 \\sigma \\sigma_1 \\pi}$ is the normalization term.\n\n\n\\vspace{-0.1in}\n\n\\subsection{ Maximization (M-Step) }\n\\label{sec:m_step} In the M-Step, we maximize\nEqn.~(\\ref{eqn:q_function}) using $w_i$ obtained from the E-Step.\nSince neighborhood information is considered, we model $P({\\mathbf{O}}, {\\mathbf{R}} |\n\\Lambda )$ as a MRF:\n\n", "index": 43, "text": "\\begin{equation}\n\\label{eqn:mrf} P({\\mathbf{O}}, {\\mathbf{R}} | \\Lambda ) = \\prod_i \\prod_{ j \\in {\\cal\nG}(i)} p(r_i | r_j, \\Lambda) p(o_i | r_i, \\Lambda)\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E20.m1\" class=\"ltx_Math\" alttext=\"P({\\mathbf{O}},{\\mathbf{R}}|\\Lambda)=\\prod_{i}\\prod_{j\\in{\\cal G}(i)}p(r_{i}|r%&#10;_{j},\\Lambda)p(o_{i}|r_{i},\\Lambda)\" display=\"block\"><mrow><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>\ud835\udc0e</mi><mo>,</mo><mi>\ud835\udc11</mi><mo stretchy=\"false\">|</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mi>i</mi></munder><munder><mo largeop=\"true\" movablelimits=\"false\" symmetric=\"true\">\u220f</mo><mrow><mi>j</mi><mo>\u2208</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udca2</mi><mo>\u2062</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>r</mi><mi>j</mi></msub><mo>,</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>r</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\n\nWe are now ready to expand Eqn.~(\\ref{eqn:q_function}). Since $r_i$\ncan only assume two values (0 or 1), we can rewrite $Q(\\Lambda,\n\\Lambda')$ in Eqn.~(\\ref{eqn:q_function}) into the following form:\n\n\\begin{eqnarray}\n\n\n\n\n\\nonumber \\sum_{t \\in \\{0,1\\}} \\log ( \\prod_i \\prod_{ j \\in {\\cal\nG}(i)} p(r_i = t | r_j, \\Lambda) p(o_i | r_i = t, \\Lambda))\nP({\\mathbf{R}} | {\\mathbf{O}},\\Lambda')\n\n\\end{eqnarray}\nAfter expansion,\n\\begin{eqnarray}\n\\nonumber\nQ(\\Lambda , \\Lambda') &=& \\sum_i \\log( \\alpha \\frac{1}{\\sigma \\sqrt{2 \\pi} } \\exp(-\\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}}||^2 }{ 2 \\sigma^2 } )  ) w_i \\\\\n\\nonumber\n&+& \\sum_i \\log( \\frac{1}{\\sigma_1 \\sqrt{2 \\pi}} \\exp(-\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) ) w_i\\\\    \n\\label{eqn:q_function_expanded} &+& \\sum_i \\log( \\exp(-\\frac{||\n{\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij} ||_F^2}{ 2 \\sigma_2^2} ) ) w_i w_j \\nonumber \\\\\n&+& \\sum_i \\log( \\frac{1 - \\alpha}{C} ) ( 1 - w_i )\n\\end{eqnarray}\n\nTo maximize Eqn.~(\\ref{eqn:q_function_expanded}), we set the first\nderivative of $Q$ with respect to ${\\mathbf{K}}_i^{-1}$, ${\\mathbf{h}}$, $\\alpha$, $\\sigma$,\n$\\sigma_1$ and $\\sigma_2$ to zero respectively to obtain the\nfollowing set of update rules:\n\n\\begin{eqnarray*}\n\\nonumber \\alpha &=& \\frac{1}{N} \\sum_i w_i \\\\\n\\nonumber {\\mathbf{K}}_i^{-1} &=&  \\frac{1}{\\sum_{j \\in {\\cal G}(i)} w_j} ( \\sum_{j \\in {\\cal G}(i)} {\\mathbf{S}}'_{ij} w_j - \\frac{\\sigma_2^2}{ 2 \\sigma_1^2 } {\\mathbf{h}} {\\mathbf{h}}^T w_i )\\\\\n\\nonumber  \\min ||  {\\mathbf{M}} {\\mathbf{h}} || & & \\mbox{subject to $|| {\\mathbf{h}} || = 1$} \\\\\n\n\n\\nonumber \\sigma^2 &=& \\frac{\\sum_i ||{\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 w_i }{ \\sum_i w_i} \n\\end{eqnarray*}\n\\begin{eqnarray}\n\\nonumber \\sigma_1^2 &=& \\frac{ \\sum_i || {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || w_i }{ \\sum_i w_i } \\\\\n\\nonumber \\label{eqn:m_step_update} \\sigma_2^2 &=& \\frac{ \\sum_i \\sum_{j \\in {\\cal G}(i)} || {\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij} ||_F^2 w_i w_j }{ \\sum_i w_i} \\\\\n\\end{eqnarray}\nwhere ${\\mathbf{M}} = \\sum_i {\\mathbf{x}}_i {\\mathbf{x}}_i^T w_i +\n\\frac{\\sigma^2}{\\sigma_1^2} \\sum_i {\\mathbf{K}}_i^{-1} w_i$ and ${\\cal G}(i)$\nis a set of neighbors of $i$.  \n\nEqn.~(\\ref{eqn:m_step_update}) constitutes the set \nof update rules for the M-step.\n\n\n\n\n\n\n\n\n\n\n\nIn each iteration, after the update rules have been\nexecuted, we normalize ${\\mathbf{K}}_i^{-1}$ \nonto the feasible solution space by normalization, that is,\nthe eigenvalues of the corresponding eigensystem are\nwithin the range $(0, 1]$.   \nAlso, ${\\mathbf{S}}'_{ij}$ will be updated\nwith the newly estimated ${\\mathbf{K}}_i^{-1}$.\n\n\n\n\n\n\n\n\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{fig07.png}\n\\caption{\\small\nThe top-left subfigure shows the plot of $\\frac{R}{R+1}$. \nThe four 2D data sets shown here have OI ratios $[1,20,45,80]$ \nrespectively, which correspond to\noutlier percentages $[50\\%,95\\%,98\\%,99\\%]$. Our EMTV can tolerate\nOI ratios $\\le$ 51 in this example. The original input,\nthe estimated line after the first EMTV iteration \nusing CFTV to initialize the algorithm, and the line\nparameters after the first EMTV iteration and final EMTV convergence\nwere shown.  The ground-truth parameter is $[-0.71, 0.71]$.\n}\n\\label{fig:percent_vs_IO}\n\\end{center}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\\subsection{Implementation and Initialization}\n\nIn summary, Eqns~(\\ref{eqn:cal_S_inv}),~(\\ref{eqn:e_step_update})\nand~(\\ref{eqn:m_step_update}) are all the equations needed to\nimplement EMTV and therefore the implementation is straightforward.\n\nNoting that initialization is important to an EM algorithm, to initialize EMTV, we set $\\sigma_1$\nto be a very large value, ${\\mathbf{K}}_i = \\mathbf{I}$\nand $w_i = 1$ for all $i$.  ${\\mathbf{S}}_{ij}'$ is initialized to\nbe the inverse of ${\\mathbf{S}}_{ij}$, computed using the closed-form\nsolution presented in the previous section.\nThese initialization values mean that at the beginning we have no preference for the surface orientation. So all the input points are initially considered as inliers. With such initialization, we execute the first and the second rules in\nEqn.~(\\ref{eqn:m_step_update}) in sequence. Note that when the first rule is being executed, the term involving ${\\mathbf{h}}$ is ignored because of the large $\\sigma_1$, thus we can obtain ${\\mathbf{K}}_i^{-1}$\nfor the second rule. After that, we can start executing the\nalgorithm from the E-step.  This initialization procedure\nis used in all the experiments in the following sections.\nFig.~\\ref{fig:percent_vs_IO} shows the result after the first \nEMTV iteration on an example; note in particular that even though the initialization is \nat times not close to the solution\nour EMTV algorithm can still converge to the desired ground-truth \nsolution. \n\n\n\\section{Experimental Results}\n\\label{sec:results}\nFirst, quantitative comparison\nwill be studied to evaluate EMTV with well-known algorithms:\nRANSAC~\\cite{fischler_acm81},\nASSC~\\cite{wang_and_suter}, and TV~\\cite{book_with_names}.\nIn addition, we also provide the result using the least squares method\nas a baseline comparison.\nSecond, we apply our method to real data\nwith synthetic outliers and/or noise where the\nground truth is available, and perform comparison.\nThird, more experiments on multiview stereo matching on\nreal images are performed.\n\nAs we will show, EMTV performed the best in highly corrupted data, because\nit is designed to seek one linear structure of known type (as opposed to\nmultiple, potentially nonlinear structures of unknown type).  The use\nof orientation constraints, in addition to position constraints, makes\nEMTV superior to the random sampling methods as well.\n\n\n\n\\noindent {\\bf Outlier/inlier (OI) ratio} \\ \nWe will use the {\\em outlier/inlier (OI) ratio} to characterize\nthe outlier level, which is related to the outlier percentage\n$Z \\in [0,1]$\n\n", "itemtype": "equation", "pos": 57408, "prevtext": "\nwhere ${\\cal G}(i)$ is the set of neighbors of $i$. In theory,\n${\\cal G}(i)$ contains all the input points except $i$, since\n$c_{ij}$ in Eqn.~(\\ref{eqn:neighbor_weighting_Gaussian}) is always\nnon-zero (because of the long tail of the Gaussian distribution). In\npractice, we can prune away the points in ${\\cal G}(i)$ where the\nvalues of $c_{ij}$ are negligible. This can greatly reduce the size\nof the neighborhood. Again, using ANN tree~\\cite{ann}, the speed of\nsearching for nearest neighbors can be greatly increased.\n\nLet us examine the two terms in Eqn.~(\\ref{eqn:mrf}).\n$p(o_i | r_i, \\Lambda)$ has been\ndefined in Eqn.~(\\ref{eqn:o_i_prob}). We define $p(r_i | r_j,\n\\Lambda)$ here. Using the third condition mentioned in\nEqn.~(\\ref{eqn:neighborhood_consistency}), we have:\n\n", "index": 45, "text": "\\begin{equation}\np(r_i | r_j, \\Lambda) = \\exp( - \\frac{ ||{\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij}\n||^2_F } {2 \\sigma^2_2})\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E21.m1\" class=\"ltx_Math\" alttext=\"p(r_{i}|r_{j},\\Lambda)=\\exp(-\\frac{||{\\mathbf{K}}_{i}^{-1}-{\\mathbf{S}}^{%&#10;\\prime}_{ij}||^{2}_{F}}{2\\sigma^{2}_{2}})\" display=\"block\"><mrow><mi>p</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy=\"false\">|</mo><msub><mi>r</mi><mi>j</mi></msub><mo>,</mo><mi mathvariant=\"normal\">\u039b</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo>-</mo><mfrac><msubsup><mrow><mo fence=\"true\">||</mo><mrow><msubsup><mi>\ud835\udc0a</mi><mi>i</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>-</mo><msubsup><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mo>\u2032</mo></msubsup></mrow><mo fence=\"true\">||</mo></mrow><mi>F</mi><mn>2</mn></msubsup><mrow><mn>2</mn><mo>\u2062</mo><msubsup><mi>\u03c3</mi><mn>2</mn><mn>2</mn></msubsup></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere $R$ is the OI ratio.\nFig.~\\ref{fig:percent_vs_IO} shows a plot of $Z = \\frac{R}{R+1}$\nindicating that it is much more difficult for a given method to handle\nthe same percentage increase in outliers as the value of $Z$ increases.\nNote the rapid increase in the number of outliers as $Z$ increases from\n50\\% to 99\\%. That is, it is more difficult for a given method \nto tolerate an addition of, say 20\\% outliers, when $Z$ is\nincreased from 70\\% to 90\\% than from 50\\% to 70\\%.\nThus the OI ratio gives more insight in studying an \nalgorithm's performance on severely corrupted data.\n\n\\vspace{-0.1in}\n\\subsection{Robustness}\nWe generate a set of 2D synthetic data\nto evaluate the performance on line fitting, by randomly sampling $44$ points from a line within the range $[-1,-1] \\times [1, 1]$\nwhere the locations of the points are contaminated by Gaussian noise of $0.1$ standard deviation. \nRandom outliers were added to the data with different OI ratios.\n\nThe data set is then partitioned into two:\n\n\\begin{itemize}\n\\item {\\sc Set 1}: OI ratio $\\in [0.1, 1]$ with step size 0.1,\n\\item {\\sc Set 2}: OI ratio $\\in [1,100]$ with step size 1.\n\\end{itemize}\n\nIn other words, the partition is done at 50\\% outliers. \nNote from the plot in Fig.~\\ref{fig:percent_vs_IO}\nthat the number of outliers increases rapidly after 50\\% outliers.  Sample data sets with\ndifferent OI ratios are shown in the top of Fig.~\\ref{fig:percent_vs_IO}. Outliers were\nadded within a bounding circle of radius 2. In particular, the bottom of\nFig.~\\ref{fig:percent_vs_IO} shows the result of the first EMTV\niteration upon initialization using CFTV.\n\n\n\n\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{fig08.png} \n\\end{center}\n\\caption{\\small Error plots for {\\sc Set 1} (OI ratio $=[0.1, 1]$, up to 50\\% outliers)\nand {\\sc Set 2}  (OI ratio $=[1,100]$, $\\ge$ 50\\% outliers). Left:\nfor {\\sc Set 1}, all the tested methods except the least-squares demonstrated reliable results. \nEMTV is deterministic and converges quickly, capable of\ncorrecting Gaussian noise inherent in the inliers and\nrejecting spurious outliers, and resulting in the almost-zero\nerror curve.\nRight: for {\\sc Set 2}, EMTV still has an almost-zero error curve\nup to an OI ratio of 51 ($\\simeq$ 98.1\\% outliers).\nWe ran 100 trials in RANSAC and ASSC and averaged the results.\nThe maximum and minimum errors of RANSAC and ASSC\nare shown below each error plot.\n}\n\\label{fig:plot_small_noise}\n\\label{fig:plot_large_noise}\n\\vspace{-0.1in}\n\\end{figure*}\n\nThe input scale, which is used in RANSAC, TV and EMTV, was estimated automatically\nby TSSE proposed in~\\cite{wang_and_suter}.  Note in principle these scales are\nnot the same, because TSSE estimates the scales of residuals in the normal space. Therefore,\nthe scale estimated by TSSE used in TV and EMTV are only\napproximations. As we will demonstrate below, even with such rough\napproximations, EMTV still performs very well showing that it is not\nsensitive to scale inaccuracy, a nice property of tensor voting\nwhich will be shown in an experiment to be detailed shortly.  \nNote that ASSC~\\cite{wang_and_suter} does not require any input scale.\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{fig09.png}\n\\end{center}\n\\caption{\\small Inputs containing various measurement errors, with OI ratio = 10 and fixed\noutliers location. The estimated models (depicted by the red lines) obtained using EMTV are overlayed on\nthe inputs. Notice the line cluster becomes less salient when s.d. = 0.25. }\n\\label{fig:varying_sd_input}\n\\vspace{-0.2in}\n\\end{figure*}\n\n{\\sc Set 1} -- Refer to the {\\em left} of\nFig.~\\ref{fig:plot_small_noise} which shows the error produced\nby various methods tested on {\\sc Set 1}. The error is measured by\nthe angle between the estimated line and the ground-truth.\nExcept the least squares  method,\nwe observe that all the tested methods (RANSAC, ASSC, TV and\nEMTV) performed very well with OI ratios $\\leq 1$. For RANSAC and\nASSC, all the detected inliers were finally used in parameter\nestimation. Note that the errors measured for RANSAC and ASSC were\nthe average errors in 100 executions\\footnote{We\nexecuted the algorithm 100 times. In each execution, iterative \nrandom sampling was done where the desired probability of\nchoosing at least one sample free from outliers was set to $0.99$\n(default value). \n\n\n}, \n\nFig.~\\ref{fig:plot_small_noise} also shows the\nmaximum and minimum errors of the two methods after running 100\ntrials.  EMTV does not have such maximum\nand minimum error plots because it is deterministic.\n\nObserve that the errors produced by our method are almost zero in {\\sc Set 1}.\nEMTV is deterministic and converges quickly, capable of\ncorrecting Gaussian noise inherent in the inliers and\nrejecting spurious outliers, and resulting in the almost-zero\nerror curve. RANSAC and ASSC have error $< 0.6^\\circ$ , which is\nstill very acceptable.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{fig10.png} \n\\end{center}\n\\caption{\\small Measurement error: standard deviation varies from 0.01 to 0.29 with OI ratio at 10.  }\n\\label{fig:varying_sd_results}\n\\vspace{-0.2in}\n\\end{figure}\n\n{\\sc Set 2 } -- Refer to the {\\em right} of Fig.~\\ref{fig:plot_large_noise} which shows the result for {\\sc Set 2}, from which we can distinguish the performance of the methods. TV breaks down at OI ratios $\\geq 20$. After that, the performance of TV is unpredictable. EMTV breaks down at OI ratios $\\ge$ 51, showing greater robustness than TV in this experiment due to the EM parameter fitting procedure.\n\nThe performance of RANSAC and ASSC were quite stable where the average errors\nare within 4 and 7 degrees over the whole spectrum of OI ratios considered.\nThe maximum and minimum errors are shown in the bottom of\nFig.~\\ref{fig:plot_large_noise}, which shows that they can be very large at\ntimes. EMTV produces almost zero errors with OI ratio $\\leq 51$, but then\nbreaks down with unpredictable performance.\nFrom the experiments on {\\sc Set 1} and {\\sc Set 2} we conclude that\nEMTV is robust up to an OI ratio of 51 ($\\simeq$98.1\\% outliers).\n\n\\noindent {\\bf Insensitivity to choice of scale}.\n\nWe studied the errors produced by EMTV with different scales\n$\\sigma_d$ (Eqn.~(\\ref{eqn:neighbor_weighting_Gaussian})), given\nOI ratio of 10 ($\\simeq$91\\% outliers). Even in the presence of many\noutliers, EMTV broke down only when $\\sigma_d \\simeq 0.7$\n(the ground-truth $\\sigma_d$ is 0.1), which indicates that our method is\nnot sensitive to large deviations of scale. Note that the scale parameter\ncan sometimes be automatically estimated (e.g., by modifying the original TSSE\nto handle tangent space) as was done in the previous experiment.\n\n\\noindent {\\bf Large measurement errors}.\nIn this experiment, we increased the measurement error by increasing the\nstandard deviation (s.d.) from 0.01\nto 0.29, while keeping OI ratio equal to 10 and the location of the\noutliers fixed. Some of the input data sets are depicted\nin Fig.~\\ref{fig:varying_sd_input}, showing that the inliers\nare less salient as the standard deviation (s.d.) increases. A similar experiment was also\nperformed in~\\cite{meer_bookchapter}.\nAgain, we compared our method with\nRANSAC, ASSC and TV.\n\n\n\nAccording to the error plot in the {\\em top} of\nFig.~\\ref{fig:varying_sd_results},\n\n\n\n\n\nTV is very sensitive to the change of s.d.: when the\ns.d. is greater than 0.03, the performance is unpredictable.\nWith increasing s.d., the performance of RANSAC and\nASSC degrade gracefully while ASSC always outperforms\nRANSAC.  The {\\em bottom} of Fig.~\\ref{fig:varying_sd_results}\nshows the corresponding maximum and minimum error\nin 100 executions.\n\nOn the other hand, we observe the performance of EMTV\n(with $\\sigma_d = 0.05$) is extremely steady and accurate\nwhen s.d. $<0.15$. After that, although its error plot\nexhibits some perturbation, the errors produced are\nstill small and the performance is quite stable compared\nwith other methods.\n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{fig11.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em Corridor}. \nRMS error plot of various methods. }\n\\label{fig:epipolar_line}\n\\vspace{-0.15in}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{fig12.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small\n {\\em Teapot}: (a) 4 images (one in enlarged view) from the input image\n set consisting of 30 images captured around the object in a casual manner.\n (b)--(f) show two views of the sparse reconstruction generated by using \n{\\tt KeyMatchFull} (398 points), \n{\\tt linear\\_match} (493 points),\n{\\tt ransac\\_match} (37 points), \n{\\tt assc\\_match} (208 points), and\n{\\tt emtv\\_match} (2152 points). \n The candidate matches returned by SIFT are extremely noisy due to the\n ambiguous patchy patterns.\n On average 17404 trials were run in {\\small {\\tt ransac\\_match}}.\n It is time consuming to run more trials on this noisy and large\n input where an image pair can have as many as 5000 similar matches.\n Similarly for {\\tt assc\\_match} where additional running time is needed\n to estimate the scale parameter in each iteration. On the other hand,\n {\\tt emtv\\_match} does not require any random sampling.\n}\n\\label{fig:teapot}\n\\vspace{-0.1in}\n\\end{figure*}\n\n\n\n\\subsection{Fundamental Matrix Estimation}\n\\label{sec:example2}\n\n\n\n\n\n\n\n\nGiven an image pair with $p \\ge 8$ correspondences\n${\\cal P} = \\{ (\\mathbf{u}_i, \\mathbf{u'}_i) | 8 \\leq i \\leq p \\} $,\nthe goal is to estimate the\n$3 \\times 3$ fundamental matrix $\\mathbf{F}= [f]_{a,b}$,\nwhere $a,b \\in \\{ 1,2,3 \\}$, such that\n\n", "itemtype": "equation", "pos": 63417, "prevtext": "\n\nWe are now ready to expand Eqn.~(\\ref{eqn:q_function}). Since $r_i$\ncan only assume two values (0 or 1), we can rewrite $Q(\\Lambda,\n\\Lambda')$ in Eqn.~(\\ref{eqn:q_function}) into the following form:\n\n\\begin{eqnarray}\n\n\n\n\n\\nonumber \\sum_{t \\in \\{0,1\\}} \\log ( \\prod_i \\prod_{ j \\in {\\cal\nG}(i)} p(r_i = t | r_j, \\Lambda) p(o_i | r_i = t, \\Lambda))\nP({\\mathbf{R}} | {\\mathbf{O}},\\Lambda')\n\n\\end{eqnarray}\nAfter expansion,\n\\begin{eqnarray}\n\\nonumber\nQ(\\Lambda , \\Lambda') &=& \\sum_i \\log( \\alpha \\frac{1}{\\sigma \\sqrt{2 \\pi} } \\exp(-\\frac{|| {\\mathbf{x}}_i^T {\\mathbf{h}}||^2 }{ 2 \\sigma^2 } )  ) w_i \\\\\n\\nonumber\n&+& \\sum_i \\log( \\frac{1}{\\sigma_1 \\sqrt{2 \\pi}} \\exp(-\\frac{|| {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} ||}{ 2 \\sigma_1^2 } ) ) w_i\\\\    \n\\label{eqn:q_function_expanded} &+& \\sum_i \\log( \\exp(-\\frac{||\n{\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij} ||_F^2}{ 2 \\sigma_2^2} ) ) w_i w_j \\nonumber \\\\\n&+& \\sum_i \\log( \\frac{1 - \\alpha}{C} ) ( 1 - w_i )\n\\end{eqnarray}\n\nTo maximize Eqn.~(\\ref{eqn:q_function_expanded}), we set the first\nderivative of $Q$ with respect to ${\\mathbf{K}}_i^{-1}$, ${\\mathbf{h}}$, $\\alpha$, $\\sigma$,\n$\\sigma_1$ and $\\sigma_2$ to zero respectively to obtain the\nfollowing set of update rules:\n\n\\begin{eqnarray*}\n\\nonumber \\alpha &=& \\frac{1}{N} \\sum_i w_i \\\\\n\\nonumber {\\mathbf{K}}_i^{-1} &=&  \\frac{1}{\\sum_{j \\in {\\cal G}(i)} w_j} ( \\sum_{j \\in {\\cal G}(i)} {\\mathbf{S}}'_{ij} w_j - \\frac{\\sigma_2^2}{ 2 \\sigma_1^2 } {\\mathbf{h}} {\\mathbf{h}}^T w_i )\\\\\n\\nonumber  \\min ||  {\\mathbf{M}} {\\mathbf{h}} || & & \\mbox{subject to $|| {\\mathbf{h}} || = 1$} \\\\\n\n\n\\nonumber \\sigma^2 &=& \\frac{\\sum_i ||{\\mathbf{x}}_i^T {\\mathbf{h}} ||^2 w_i }{ \\sum_i w_i} \n\\end{eqnarray*}\n\\begin{eqnarray}\n\\nonumber \\sigma_1^2 &=& \\frac{ \\sum_i || {\\mathbf{h}}^T {\\mathbf{K}}_i^{-1} {\\mathbf{h}} || w_i }{ \\sum_i w_i } \\\\\n\\nonumber \\label{eqn:m_step_update} \\sigma_2^2 &=& \\frac{ \\sum_i \\sum_{j \\in {\\cal G}(i)} || {\\mathbf{K}}_i^{-1} - {\\mathbf{S}}'_{ij} ||_F^2 w_i w_j }{ \\sum_i w_i} \\\\\n\\end{eqnarray}\nwhere ${\\mathbf{M}} = \\sum_i {\\mathbf{x}}_i {\\mathbf{x}}_i^T w_i +\n\\frac{\\sigma^2}{\\sigma_1^2} \\sum_i {\\mathbf{K}}_i^{-1} w_i$ and ${\\cal G}(i)$\nis a set of neighbors of $i$.  \n\nEqn.~(\\ref{eqn:m_step_update}) constitutes the set \nof update rules for the M-step.\n\n\n\n\n\n\n\n\n\n\n\nIn each iteration, after the update rules have been\nexecuted, we normalize ${\\mathbf{K}}_i^{-1}$ \nonto the feasible solution space by normalization, that is,\nthe eigenvalues of the corresponding eigensystem are\nwithin the range $(0, 1]$.   \nAlso, ${\\mathbf{S}}'_{ij}$ will be updated\nwith the newly estimated ${\\mathbf{K}}_i^{-1}$.\n\n\n\n\n\n\n\n\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{fig07.png}\n\\caption{\\small\nThe top-left subfigure shows the plot of $\\frac{R}{R+1}$. \nThe four 2D data sets shown here have OI ratios $[1,20,45,80]$ \nrespectively, which correspond to\noutlier percentages $[50\\%,95\\%,98\\%,99\\%]$. Our EMTV can tolerate\nOI ratios $\\le$ 51 in this example. The original input,\nthe estimated line after the first EMTV iteration \nusing CFTV to initialize the algorithm, and the line\nparameters after the first EMTV iteration and final EMTV convergence\nwere shown.  The ground-truth parameter is $[-0.71, 0.71]$.\n}\n\\label{fig:percent_vs_IO}\n\\end{center}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\\subsection{Implementation and Initialization}\n\nIn summary, Eqns~(\\ref{eqn:cal_S_inv}),~(\\ref{eqn:e_step_update})\nand~(\\ref{eqn:m_step_update}) are all the equations needed to\nimplement EMTV and therefore the implementation is straightforward.\n\nNoting that initialization is important to an EM algorithm, to initialize EMTV, we set $\\sigma_1$\nto be a very large value, ${\\mathbf{K}}_i = \\mathbf{I}$\nand $w_i = 1$ for all $i$.  ${\\mathbf{S}}_{ij}'$ is initialized to\nbe the inverse of ${\\mathbf{S}}_{ij}$, computed using the closed-form\nsolution presented in the previous section.\nThese initialization values mean that at the beginning we have no preference for the surface orientation. So all the input points are initially considered as inliers. With such initialization, we execute the first and the second rules in\nEqn.~(\\ref{eqn:m_step_update}) in sequence. Note that when the first rule is being executed, the term involving ${\\mathbf{h}}$ is ignored because of the large $\\sigma_1$, thus we can obtain ${\\mathbf{K}}_i^{-1}$\nfor the second rule. After that, we can start executing the\nalgorithm from the E-step.  This initialization procedure\nis used in all the experiments in the following sections.\nFig.~\\ref{fig:percent_vs_IO} shows the result after the first \nEMTV iteration on an example; note in particular that even though the initialization is \nat times not close to the solution\nour EMTV algorithm can still converge to the desired ground-truth \nsolution. \n\n\n\\section{Experimental Results}\n\\label{sec:results}\nFirst, quantitative comparison\nwill be studied to evaluate EMTV with well-known algorithms:\nRANSAC~\\cite{fischler_acm81},\nASSC~\\cite{wang_and_suter}, and TV~\\cite{book_with_names}.\nIn addition, we also provide the result using the least squares method\nas a baseline comparison.\nSecond, we apply our method to real data\nwith synthetic outliers and/or noise where the\nground truth is available, and perform comparison.\nThird, more experiments on multiview stereo matching on\nreal images are performed.\n\nAs we will show, EMTV performed the best in highly corrupted data, because\nit is designed to seek one linear structure of known type (as opposed to\nmultiple, potentially nonlinear structures of unknown type).  The use\nof orientation constraints, in addition to position constraints, makes\nEMTV superior to the random sampling methods as well.\n\n\n\n\\noindent {\\bf Outlier/inlier (OI) ratio} \\ \nWe will use the {\\em outlier/inlier (OI) ratio} to characterize\nthe outlier level, which is related to the outlier percentage\n$Z \\in [0,1]$\n\n", "index": 47, "text": "\\begin{equation}\n\\label{eqn:percent_vs_IO}\nZ = \\frac{R}{R + 1}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E22.m1\" class=\"ltx_Math\" alttext=\"Z=\\frac{R}{R+1}\" display=\"block\"><mrow><mi>Z</mi><mo>=</mo><mfrac><mi>R</mi><mrow><mi>R</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nfor all $i$. $\\mathbf{F}$ is of rank 2. Let $\\mathbf{u} = (u,v,1)^T$ and\n$\\mathbf{u'} = (u',v',1)$, Eqn.~(\\ref{eqn:essential}) can be rewritten\ninto:\n\n", "itemtype": "equation", "pos": 73122, "prevtext": "\nwhere $R$ is the OI ratio.\nFig.~\\ref{fig:percent_vs_IO} shows a plot of $Z = \\frac{R}{R+1}$\nindicating that it is much more difficult for a given method to handle\nthe same percentage increase in outliers as the value of $Z$ increases.\nNote the rapid increase in the number of outliers as $Z$ increases from\n50\\% to 99\\%. That is, it is more difficult for a given method \nto tolerate an addition of, say 20\\% outliers, when $Z$ is\nincreased from 70\\% to 90\\% than from 50\\% to 70\\%.\nThus the OI ratio gives more insight in studying an \nalgorithm's performance on severely corrupted data.\n\n\\vspace{-0.1in}\n\\subsection{Robustness}\nWe generate a set of 2D synthetic data\nto evaluate the performance on line fitting, by randomly sampling $44$ points from a line within the range $[-1,-1] \\times [1, 1]$\nwhere the locations of the points are contaminated by Gaussian noise of $0.1$ standard deviation. \nRandom outliers were added to the data with different OI ratios.\n\nThe data set is then partitioned into two:\n\n\\begin{itemize}\n\\item {\\sc Set 1}: OI ratio $\\in [0.1, 1]$ with step size 0.1,\n\\item {\\sc Set 2}: OI ratio $\\in [1,100]$ with step size 1.\n\\end{itemize}\n\nIn other words, the partition is done at 50\\% outliers. \nNote from the plot in Fig.~\\ref{fig:percent_vs_IO}\nthat the number of outliers increases rapidly after 50\\% outliers.  Sample data sets with\ndifferent OI ratios are shown in the top of Fig.~\\ref{fig:percent_vs_IO}. Outliers were\nadded within a bounding circle of radius 2. In particular, the bottom of\nFig.~\\ref{fig:percent_vs_IO} shows the result of the first EMTV\niteration upon initialization using CFTV.\n\n\n\n\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{fig08.png} \n\\end{center}\n\\caption{\\small Error plots for {\\sc Set 1} (OI ratio $=[0.1, 1]$, up to 50\\% outliers)\nand {\\sc Set 2}  (OI ratio $=[1,100]$, $\\ge$ 50\\% outliers). Left:\nfor {\\sc Set 1}, all the tested methods except the least-squares demonstrated reliable results. \nEMTV is deterministic and converges quickly, capable of\ncorrecting Gaussian noise inherent in the inliers and\nrejecting spurious outliers, and resulting in the almost-zero\nerror curve.\nRight: for {\\sc Set 2}, EMTV still has an almost-zero error curve\nup to an OI ratio of 51 ($\\simeq$ 98.1\\% outliers).\nWe ran 100 trials in RANSAC and ASSC and averaged the results.\nThe maximum and minimum errors of RANSAC and ASSC\nare shown below each error plot.\n}\n\\label{fig:plot_small_noise}\n\\label{fig:plot_large_noise}\n\\vspace{-0.1in}\n\\end{figure*}\n\nThe input scale, which is used in RANSAC, TV and EMTV, was estimated automatically\nby TSSE proposed in~\\cite{wang_and_suter}.  Note in principle these scales are\nnot the same, because TSSE estimates the scales of residuals in the normal space. Therefore,\nthe scale estimated by TSSE used in TV and EMTV are only\napproximations. As we will demonstrate below, even with such rough\napproximations, EMTV still performs very well showing that it is not\nsensitive to scale inaccuracy, a nice property of tensor voting\nwhich will be shown in an experiment to be detailed shortly.  \nNote that ASSC~\\cite{wang_and_suter} does not require any input scale.\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{fig09.png}\n\\end{center}\n\\caption{\\small Inputs containing various measurement errors, with OI ratio = 10 and fixed\noutliers location. The estimated models (depicted by the red lines) obtained using EMTV are overlayed on\nthe inputs. Notice the line cluster becomes less salient when s.d. = 0.25. }\n\\label{fig:varying_sd_input}\n\\vspace{-0.2in}\n\\end{figure*}\n\n{\\sc Set 1} -- Refer to the {\\em left} of\nFig.~\\ref{fig:plot_small_noise} which shows the error produced\nby various methods tested on {\\sc Set 1}. The error is measured by\nthe angle between the estimated line and the ground-truth.\nExcept the least squares  method,\nwe observe that all the tested methods (RANSAC, ASSC, TV and\nEMTV) performed very well with OI ratios $\\leq 1$. For RANSAC and\nASSC, all the detected inliers were finally used in parameter\nestimation. Note that the errors measured for RANSAC and ASSC were\nthe average errors in 100 executions\\footnote{We\nexecuted the algorithm 100 times. In each execution, iterative \nrandom sampling was done where the desired probability of\nchoosing at least one sample free from outliers was set to $0.99$\n(default value). \n\n\n}, \n\nFig.~\\ref{fig:plot_small_noise} also shows the\nmaximum and minimum errors of the two methods after running 100\ntrials.  EMTV does not have such maximum\nand minimum error plots because it is deterministic.\n\nObserve that the errors produced by our method are almost zero in {\\sc Set 1}.\nEMTV is deterministic and converges quickly, capable of\ncorrecting Gaussian noise inherent in the inliers and\nrejecting spurious outliers, and resulting in the almost-zero\nerror curve. RANSAC and ASSC have error $< 0.6^\\circ$ , which is\nstill very acceptable.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{fig10.png} \n\\end{center}\n\\caption{\\small Measurement error: standard deviation varies from 0.01 to 0.29 with OI ratio at 10.  }\n\\label{fig:varying_sd_results}\n\\vspace{-0.2in}\n\\end{figure}\n\n{\\sc Set 2 } -- Refer to the {\\em right} of Fig.~\\ref{fig:plot_large_noise} which shows the result for {\\sc Set 2}, from which we can distinguish the performance of the methods. TV breaks down at OI ratios $\\geq 20$. After that, the performance of TV is unpredictable. EMTV breaks down at OI ratios $\\ge$ 51, showing greater robustness than TV in this experiment due to the EM parameter fitting procedure.\n\nThe performance of RANSAC and ASSC were quite stable where the average errors\nare within 4 and 7 degrees over the whole spectrum of OI ratios considered.\nThe maximum and minimum errors are shown in the bottom of\nFig.~\\ref{fig:plot_large_noise}, which shows that they can be very large at\ntimes. EMTV produces almost zero errors with OI ratio $\\leq 51$, but then\nbreaks down with unpredictable performance.\nFrom the experiments on {\\sc Set 1} and {\\sc Set 2} we conclude that\nEMTV is robust up to an OI ratio of 51 ($\\simeq$98.1\\% outliers).\n\n\\noindent {\\bf Insensitivity to choice of scale}.\n\nWe studied the errors produced by EMTV with different scales\n$\\sigma_d$ (Eqn.~(\\ref{eqn:neighbor_weighting_Gaussian})), given\nOI ratio of 10 ($\\simeq$91\\% outliers). Even in the presence of many\noutliers, EMTV broke down only when $\\sigma_d \\simeq 0.7$\n(the ground-truth $\\sigma_d$ is 0.1), which indicates that our method is\nnot sensitive to large deviations of scale. Note that the scale parameter\ncan sometimes be automatically estimated (e.g., by modifying the original TSSE\nto handle tangent space) as was done in the previous experiment.\n\n\\noindent {\\bf Large measurement errors}.\nIn this experiment, we increased the measurement error by increasing the\nstandard deviation (s.d.) from 0.01\nto 0.29, while keeping OI ratio equal to 10 and the location of the\noutliers fixed. Some of the input data sets are depicted\nin Fig.~\\ref{fig:varying_sd_input}, showing that the inliers\nare less salient as the standard deviation (s.d.) increases. A similar experiment was also\nperformed in~\\cite{meer_bookchapter}.\nAgain, we compared our method with\nRANSAC, ASSC and TV.\n\n\n\nAccording to the error plot in the {\\em top} of\nFig.~\\ref{fig:varying_sd_results},\n\n\n\n\n\nTV is very sensitive to the change of s.d.: when the\ns.d. is greater than 0.03, the performance is unpredictable.\nWith increasing s.d., the performance of RANSAC and\nASSC degrade gracefully while ASSC always outperforms\nRANSAC.  The {\\em bottom} of Fig.~\\ref{fig:varying_sd_results}\nshows the corresponding maximum and minimum error\nin 100 executions.\n\nOn the other hand, we observe the performance of EMTV\n(with $\\sigma_d = 0.05$) is extremely steady and accurate\nwhen s.d. $<0.15$. After that, although its error plot\nexhibits some perturbation, the errors produced are\nstill small and the performance is quite stable compared\nwith other methods.\n\n\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{fig11.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em Corridor}. \nRMS error plot of various methods. }\n\\label{fig:epipolar_line}\n\\vspace{-0.15in}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{fig12.png}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small\n {\\em Teapot}: (a) 4 images (one in enlarged view) from the input image\n set consisting of 30 images captured around the object in a casual manner.\n (b)--(f) show two views of the sparse reconstruction generated by using \n{\\tt KeyMatchFull} (398 points), \n{\\tt linear\\_match} (493 points),\n{\\tt ransac\\_match} (37 points), \n{\\tt assc\\_match} (208 points), and\n{\\tt emtv\\_match} (2152 points). \n The candidate matches returned by SIFT are extremely noisy due to the\n ambiguous patchy patterns.\n On average 17404 trials were run in {\\small {\\tt ransac\\_match}}.\n It is time consuming to run more trials on this noisy and large\n input where an image pair can have as many as 5000 similar matches.\n Similarly for {\\tt assc\\_match} where additional running time is needed\n to estimate the scale parameter in each iteration. On the other hand,\n {\\tt emtv\\_match} does not require any random sampling.\n}\n\\label{fig:teapot}\n\\vspace{-0.1in}\n\\end{figure*}\n\n\n\n\\subsection{Fundamental Matrix Estimation}\n\\label{sec:example2}\n\n\n\n\n\n\n\n\nGiven an image pair with $p \\ge 8$ correspondences\n${\\cal P} = \\{ (\\mathbf{u}_i, \\mathbf{u'}_i) | 8 \\leq i \\leq p \\} $,\nthe goal is to estimate the\n$3 \\times 3$ fundamental matrix $\\mathbf{F}= [f]_{a,b}$,\nwhere $a,b \\in \\{ 1,2,3 \\}$, such that\n\n", "index": 49, "text": "\\begin{equation}\n\\label{eqn:essential} \\mathbf{u'}_i^T \\mathbf{F} \\mathbf{u}_i = 0\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E23.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{u^{\\prime}}_{i}^{T}\\mathbf{F}\\mathbf{u}_{i}=0\" display=\"block\"><mrow><mrow><mmultiscripts><mi>\ud835\udc2e</mi><none/><mo>\u2032</mo><mi>i</mi><mi>T</mi></mmultiscripts><mo>\u2062</mo><msub><mi>\ud835\udc05\ud835\udc2e</mi><mi>i</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere\n\\begin{eqnarray}\n\\nonumber \\mathbf{U} &=& (uu', uv', u, vu', vv' v, u', v', 1)^T \\\\\n\\nonumber \\mathbf{v} &=& (f_{11}, f_{21}, f_{31}, f_{12}, f_{22}, f_{32}, f_{13}, f_{23}, f_{33} )^T\n\\end{eqnarray}\n\nNoting that Eqn.~(\\ref{eqn:essential_rearrange}) is a simple plane\nequation, if we can detect and handle noise and outliers in the feature space,\nEqn.~(\\ref{eqn:essential_rearrange}) should enable us to produce\na good estimation.\n\n\n\n\n\n\nFinally, we apply~\\cite{hartley_defence} to obtain \na rank-2 fundamental matrix. Data normalization is similarly\ndone as in~\\cite{hartley_defence} before the optimization.\n\n\n\n\n\n\n\n\nWe evaluate the results by estimating the fundamental matrix\nof the data set {\\em Corridor}, which is available at\nwww.robots.ox.ac.uk/$\\sim$vgg/data.html. The matches\nof feature points (Harris corners) are available. Random outliers\nwere added in the feature space. \n\nFig.~\\ref{fig:epipolar_line} shows the plot of RMS error, which\nis computed by summing up and averaging\n$\\sqrt{\\frac{1}{p}\\sum_i|| \\mathbf{U}_i^T \\hat{\\mathbf{h}}||^2}$\nover all pairs,\nwhere $\\mathbf{U}_i$ is the set of clean data, and $\\hat{\\mathbf{h}}$\nis the 9D vector produced from the rank-2 fundamental matrices\nestimated by various methods. Note that all the images available\nin the {\\em Corridor} data set are used, that is, all $C^{11}_2$\npairs were tested.\n\n\nIt can be observed\nthat RANSAC breaks down at an OI ratio $\\simeq 20$, or 95.23\\% outliers.\nASSC is very stable with RMS error $< 0.15$. TV breaks down at\nan OI ratio $\\simeq 10$. EMTV has negligible RMS error before it\nstarts to break down at an OI ratio $\\simeq 40$. This finding\nechoes that of~\\cite{hartley_defence} that linear solution is sufficient\nwhen outliers are properly handled.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.75\\linewidth]{fig13.png}\n\\end{center}\n\\caption{\\small Results before and after filtering of {\\em Hall 3} (images shown\nin Fig.~\\ref{fig:hall3_recon}). \n\n\nAll salient 3D structures are retained in the filtered\nresult, including the bushes near the left facade and planters near the\nright facade in this top view of the building.}\n\\label{fig:filtering}\n\\vspace{-0.2in}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{fig14.png}\n\\end{center}\n\\vspace{-0.2in}\n\\caption{\\small The {\\em Hall 3} reconstruction: ten of the input images (top) and five views of the quasi-dense 3D reconstruction (bottom).}\n\\label{fig:hall3_recon}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\\subsection{Matching}\n\\label{sec:teapot}\nIn the uncalibrated scenario, EMTV estimates parameter accurately by employing\nCFTV, and effectively discards epipolar geometries induced by wrong matches.\nTypically, camera calibration is performed using nonlinear least-squares\nminimization and bundle adjustment~\\cite{lourakis_ba} which requires good\nmatches as input.  In this experiment, candidate matches\nare generated by comparing the resulting 128D SIFT feature vectors~\\cite{sift}, \nso many matched keypoints are not corresponding.\n\nThe epipolar constraint is enforced in\nthe matching process using EMTV, which returns the fundamental\nmatrix {\\em and} the probability $w_i$ (Eqn~(\\ref{eqn:e_step_update}))\nof a keypoint pair $i$ being an inlier. In the experiment,\nwe assume keypoint pair $i$ is an inlier if $w_i > 0.8$.\n\\if 0\nNote that we did not use any random sampling.\nThe following compares {\\small {\\tt emtv\\_match}} with\n{\\small {\\tt KeyMatchFull}}~\\cite{snavely_ijcv07},\n{\\small {\\tt assc\\_match}}~\\cite{wang_and_suter},\n{\\small {\\tt ransac\\_match}}, and\n{\\small {\\tt linear\\_match}},\nwhere the latter three perform hyper-plane fitting by using\nASSC, RANSAC, and least-squares respectively.\n\n\\begin{figure}[t]\n\\begin{center}\n\\begin{tabular}{@{\\hspace{0mm}}c@{\\hspace{0mm}}c@{\\hspace{0mm}}c}\n\\includegraphics[width=0.30\\linewidth]{figure/LongJing/TeaBoxUnd2.bmp.eps} &\n\\includegraphics[width=0.32\\linewidth]{figure/LongJing/original_tea.bmp.eps} &\n\\includegraphics[width=0.32\\linewidth]{figure/LongJing/EM_tea.bmp.eps} \\\\\n{\\small (a) } & {\\small (b)} & {\\small (c)} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\small {\\em Tea Can}: (a) One of the two input images. (b) Sparse\nreconstruction generated by using {\\small {\\tt KeyMatchFull}}. (c) Sparse\nreconstruction generated by using {\\small {\\tt emtv\\_match}}. }\n\\label{fig:longjing}\n\\vspace{-0.2in}\n\\end{figure}\n\n\\noindent {\\em Tea Can}.  Fig.~\\ref{fig:longjing} shows\nthat, by using our filtered matches, even in the absence\nof any focal length input, our sparse reconstruction of\nthe tea can (the image pair was obtained from~\\cite{zzhang_pami}),\nproduced by the nonlinear least-squares minimization and\nbundle adjustment~\\cite{lourakis_ba}, is denser and contains\nless errors as compared with~\\cite{snavely_ijcv07}, where\nwe can faithfully reconstruct the right-angled container.\n\n\n\\noindent {\\em Teapot}. \n\\fi\nFig.~\\ref{fig:teapot} shows\nour running example {\\em teapot} which contains\nrepetitive patterns across the whole object. Wrong matches\ncan be easily produced by similar patterns on different parts\nof the teapot. This data set contains 30 images captured\nusing a Nikon D70 camera. Automatic configuration was set\nduring the image capture.\nVisually, the result produced using {\\small {\\tt emtv\\_match}}\nis much denser than the results produced with\n{\\small {\\tt KeyMatchFull}}~\\cite{snavely_ijcv07}, \n{\\small {\\tt linear\\_match}},\n{\\small {\\tt assc\\_match}}~\\cite{wang_and_suter}, \nand {\\small {\\tt ransac\\_match}}.\nNote in particular that only {\\small {\\tt emtv\\_match}}\nrecovers the overall geometry of the teapot, whereas\nthe other methods can only recover one side of the teapot.\n\n\n\nThis example is challenging, because the teapot's shape is quite symmetric\nand the patchy patterns look identical everywhere. \nAs was done in~\\cite{snavely_ijcv07},\neach photo was paired respectively with a number of photos with\ncamera poses satisfying certain basic criteria conducive to matching\nor making the numerical process stable\n(e.g. wide-baseline stereo). We can regard this pair-up process\nas one of computing connected components. If the fundamental matrix\nbetween any successive images are incorrectly estimated,\nthe corresponding components will no longer be connected,\nresulting in the situation that only one side or part of the\nobject can be recovered.\n\nSince {\\small {\\tt KeyMatchFull}} and {\\small {\\tt linear\\_match}}\nuse simple distance measure for finding matches, the coverage of\nthe corresponding connected components tend to be small. \nIt is interesting to note that the worst result is produced by using\n{\\small {\\tt ransac\\_match}}. This can be attributed to three reasons:\n(1) the fundamental matrix is of rank 2 which implies that ${\\mathbf{h}}$\nspans a subspace $\\leq$ 8-D rather than a 9-D hyperplane;\n(2) the input matches contain too many outliers for some image pairs;\n(3) it is not feasible to fine tune the scale parameter for\nevery possible image pair and so we used a single value for all of the images.\nA slight improvement could be found from\nASSC. However, it still suffers from problems (1) and (2) and so the\nresult is not very good even compared with\n{\\small {\\tt KeyMatchFull}} and {\\small {\\tt linear\\_match}}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, {\\small {\\tt emtv\\_match}} utilizes the epipolar\ngeometry constraint by computing the fundamental matrix in a data\ndriven manner. Since the outliers are effectively filtered out,\nthe estimated fundamental matrices are sufficiently accurate\nto pair up all of the images into a single connected component. \nThus, the overall 3D geometry can be recovered from all the available views.\n\n\\if 0\n\\begin{figure*}[!ht]\n  \\begin{center}\n  \\begin{tabular}{@{\\hspace{-0.05in}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c}\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth_view1.bmp.eps} &\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth2_pmvs_normal_view0.bmp.eps} &\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth2_tmvs_normal_view0.bmp.eps} &\n  \\includegraphics[width=0.17\\linewidth]{figure/earth/earth2_pmvs_view0.bmp.eps} &\n  \\includegraphics[width=0.17\\linewidth]{figure/earth/earth2_tmvs_view0.bmp.eps} \\\\\n(a) & (b) & (c) & (d) & (e)\n  \\end{tabular}\n  \\end{center}\n  \\caption{\\small {\\em Earth}. (a) an input image (81 in total), (b) and (c): zoom-in\n   views of the normal reconstruction produced by PMVS and TMVS after the propagation\n   step, (d) and (e) show respectively one view of the quasi-dense reconstruction by\n  PMVS and TMVS.}\n  \\label{fig:earth}\n\\vspace{-0.1in}\n\\end{figure*}\n\\fi\n\n\n\n\\subsection{Multiview Stereo Reconstruction}\n\\label{sec:mvs}\nThis section outlines how CFTV and EMTV are applied to \nimprove the match-propagate-filter pipeline in multiview stereo.\nMatch-propagate-filter is a competitive approach to \nmultiview stereo\nreconstruction for computing a (quasi) dense representation. Starting\nfrom a sparse set of initial matches with high confidence, matches\nare propagated using photoconsistency to produce a (quasi) dense\nreconstruction of the target shape. Visibility consistency can be\napplied to remove outliers.\nAmong the existing works using the match-propagate-filter\napproach, patch-based multiview stereo (or PMVS) proposed\nin~\\cite{furukawa_pami09} has produced some best results to date. \n\nWe observe that PMVS had not fully utilized the 3D information\ninherent in the sparse and dense geometry before, during \nand after propagation, as patches do not adequately communicate \namong each other. As noted in~\\cite{furukawa_pami09}, data communication \nshould not be\ndone by smoothing, but the lack of communication will cause\nperturbed surface normals and patch outliers during the propagation \nstage.  In~\\cite{tmvs}, we proposed tensor-based multiview stereo (TMVS) \nand used 3D structure-aware tensors which communicate among each other \nvia CFTV.  We found that such tensor communication not only improves \npropagation in MVS without undesirable\nsmoothing but also benefits the entire match-propagate-filter pipeline\nwithin a unified framework. \n\nWe captured 179 photos around a building which \nwere first calibrated as described in section~\\ref{sec:teapot}.\nAll images were taken on the ground\nlevel not higher than the building, so we have very few samples of the\nrooftop. The building facades are curved and the windows on the building\nlook identical to each other. The patterns on the front and back facade\nlook nearly identical. These ambiguities cause significant challenges\nin the matching stage especially\nfor wide-baseline stereo. TMVS was run to obtain the quasi-dense reconstruction,\nwhere MRFTV was used to filter outliers as shown in Fig.~\\ref{fig:filtering}.\nFig.~\\ref{fig:hall3_recon} shows the 3D reconstruction which is faithful\nto the real building.\nReaders are referred to~\\cite{tmvs} for more detail and experimental\nevaluation of TMVS.  \n\n\n\\if 0\n\n\\begin{figure}[t]\n\\begin{center}\n\\begin{tabular}{\n@{\\hspace{0mm}}c@{\\hspace{0mm}}c\n@{\\hspace{0mm}}c@{\\hspace{0mm}}c\n}\n\\includegraphics[height=1.5in]{figure/tripp/tripp_image.eps} &\n\\includegraphics[height=1.5in]{figure/tripp/tripp_pmvs_view0.bmp.eps} &\n\\includegraphics[height=1.5in]{figure/tripp/tripp_tmvs_view0.bmp.eps} &\n\\end{tabular}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em Tripp} reconstruction from sparse data set:\nthree input images (left) and the quasi-dense 3D reconstruction\nproduced by PMVS (middle) and TMVS (right).}\n\\label{fig:tripp}\n\n\n\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{figure/george/george_image.eps}\n\\begin{tabular}{\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n}\n\\includegraphics[height=1in]{figure/george/george_view0.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view1.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view3.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view2.bmp.eps}\n\\end{tabular}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em George} reconstruction from sparse data set:\nfive input images (top) and four views of the quasi-dense\n3D reconstruction (bottom).}\n\\label{fig:george}\n\\vspace{-0.1in}\n\\end{figure}\n\\fi\n\n\n\n\n\\section{Conclusions}\nA closed-form solution is proved for the special theory of tensor voting\n(CFTV) for computing an exact structure-aware tensor in any dimensions.\nFor structure propagation, we derive a quadratic energy for MRFTV, thus \nproviding a convergence proof for tensor voting which is impossible \nto prove using the original tensor voting procedure. Then, \nwe derive EMTV for optimizing both the tensor and model parameters for \nrobust parameter estimation. We performed quantitative and qualitative \nevaluation using challenging synthetic and real data sets.  \n\n\n\n\nIn the future we will develop a closed-form solution for the general theory of tensor voting, and extend EMTV to extract \nmultiple and nonlinear structures.  We have provided C$++$ source code, \nbut it is straightforward to implement \nEqns~(\\ref{eqn:cal_S}), (\\ref{eqn:cal_S_inv}), \n(\\ref{eqn:e_step_update}), (\\ref{eqn:m_step_update}), \n(\\ref{eqn:energy_update_rule}), and (\\ref{eqn:sor_update_rule}).\nWe demonstrated promising results in multiview stereo, and will \napply our closed-form solution to address important computer \nvision problems.\n\n\n\\ifCLASSOPTIONcompsoc\n\n  \\section*{Acknowledgments}\n\\else\n  \n  \\section*{Acknowledgment}\n\\fi\n\nThe authors would like to thank the Associate Editor and all of the\nanonymous reviewers. Special thanks go to Reviewer 1 for his/her \nhelpful and detailed comments throughout the review cycle. \n\n\n\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{cftv}\n}\n\n\\vspace{-0.5in}\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{pang_pic.png}}] \n{Tai-Pang Wu} \nreceived the PhD degree in computer science from the Hong\nKong University of Science and Technology (HKUST) in 2007. He was\nawarded the Microsoft Fellowship in 2006. After graduation, he has\nbeen employed as a postdoctoral fellow in Microsoft Research Asia\nBeijing (2007--2008) and the Chinese University of Hong Kong\n(2008--2010). He is currently a Senior Research at the Enterprise and \nConsumer Electronics Group of the Hong Kong Applied Science \nand Technology Research Institute (ASTRI). \nHis research interests include computer vision and computer graphics. \nHe is a member of the IEEE and the IEEE Computer\nSociety.\n\\end{IEEEbiography}\n\n\\vspace{-0.5in}\n\\begin{IEEEbiography} \n[{\\includegraphics[width=1\\linewidth]{kit.png}}] \n{Sai-Kit Yeung} received his PhD degree in electronic and computer\nengineering from the Hong Kong University of Science and Technology\n(HKUST) in 2009. He received his BEng degree (First Class Honors) in\ncomputer engineering and MPhil degree in bioengineering from HKUST in\n2003 and 2005 respectively. \n\n\n\nHe is currently an Assistant Professor at the Singapore University \nof Technology and Design (SUTD).\nPrior to joining SUTD, he was a Postdoctoral Scholar in the Department\nof Mathematics, University of California, Los Angeles (UCLA) in 2010.\nHis research interests include computer vision, computer graphics,\ncomputational photography, image/video processing, and medical\nimaging. He is a member of IEEE and IEEE Computer Society.\n\\end{IEEEbiography}\n\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{jia.png}}] \n{Jiaya Jia} \nJiaya Jia received the PhD degree in computer science from Hong Kong\nUniversity of Science and Technology in 2004 and is currently an associate\nprofessor in Department of Computer Science and Engineering at the Chinese\nUniversity of Hong Kong (CUHK). He was a visiting scholar at Microsoft\nResearch Asia from March 2004 to August 2005 and conducted collaborative\nresearch at Adobe Systems in 2007. He leads the research group in CUHK,\nfocusing on computational photography, 3D reconstruction, practical\noptimization, and motion estimation. He serves as an associate editor for\nTPAMI and as an area chair for ICCV 2011. He was on the program committees\nof several major conferences, including ICCV, ECCV, and CVPR, and co-chaired\nthe Workshop on Interactive Computer Vision in conjunction with ICCV 2007.\nHe received the Young Researcher Award 2008 and Research Excellence Award\n2009 from CUHK. He is a senior member of the IEEE.\n\\end{IEEEbiography}\n\n\\vspace{-1.0in}\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{tang_ck.png}}] \n{Chi-Keung Tang}\nreceived the MSc and PhD degrees in\ncomputer science from the University of Southern California\n(USC), Los Angeles, in 1999 and 2000, respectively.\nSince 2000, he has been with the Department of\nComputer Science at the Hong Kong University of Science and\nTechnology (HKUST) where he is currently a professor. He is an \nadjunct researcher at the Visual Computing Group of Microsoft\nResearch Asia.  His research areas are computer vision, computer\ngraphics, and human-computer interaction. He is an associate editor\nof IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), \nand on the editorial board of International Journal of Computer Vision (IJCV).\nHe served as an area chair for ACCV 2006 (Hyderabad), ICCV 2007\n(Rio de Janeiro), ICCV 2009 (Kyoto), ICCV 2011 (Barcelona), \nand as a technical papers committee member for the inaugural \nSIGGRAPH Asia 2008 (Singapore), SIGGRAPH 2011 (Vancouver), and \nSIGGRAPH Asia 2011 (Hong Kong), SIGGRAPH 2012 (Los Angeles).  \nHe is a senior member of the IEEE and the IEEE Computer Society.\n\\end{IEEEbiography}\n\n\\vspace{-1.0in}\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{medioni.png}}] \n{G\\'{e}rard Medioni} \nreceived the Dipl\\^{o}me d'Ing\\'{e}nieur\nfrom the \\'{E}cole Nationale Sup\\'{e}rieure des\nT\\'{e}l\\'{e}communications (ENST), Paris, in 1977 and\nthe MS and PhD degrees from the University of\nSouthern California (USC) in 1980 and 1983,\nrespectively. He has been with USC since then\nand is currently a professor of computer science\nand electrical engineering, a codirector of the\nInstitute for Robotics and Intelligent Systems\n(IRIS), and a codirector of the USC Games\nInstitute. He served as the chairman of the Computer Science\nDepartment from 2001 to 2007. He has made significant contributions\nto the field of computer vision. His research covers a broad spectrum of\nthe field, such as edge detection, stereo and motion analysis, shape\ninference and description, and system integration. He has published\n3 books, more than 50 journal papers, and 150 conference articles. He\nis the holder of eight international patents. He is an associate editor of\nthe Image and Vision Computing Journal, Pattern Recognition and\nImage Analysis Journal, and International Journal of Image and Video\nProcessing. He served as a program cochair of the 1991 IEEE\nComputer Vision and Pattern Recognition (CVPR) Conference and\nthe 1995 IEEE International Symposium on Computer Vision, a general\ncochair of the 1997 IEEE CVPR Conference, a conference cochair of\nthe 1998 International Conference on Pattern Recognition, a general\ncochair of the 2001 IEEE CVPR Conference, a general cochair of the\n2007 IEEE CVPR Conference, and a general cochair of the upcoming\n2009 IEEE CVPR Conference. He is a fellow of the IEEE, IAPR, and\nAAAI and a member of the IEEE Computer Society.\n\\end{IEEEbiography}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSOPTIONcompsoc\n  \n  \n  \n\\else\n  \n  \n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSINFOpdf\n  \n  \n  \n  \n  \n  \n\\else\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hyphenation{op-tical net-works semi-conduc-tor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{\n        Tai-Pang~Wu,~\n\tSai-Kit~Yeung,~\n        Chi-Keung~Tang$^*$\\thanks{Please direct all inquiries to the contact author. E-mail: cktang@cse.ust.hk},\n\n\n        Jiaya Jia~\n        and~Gerard~Medioni\n\\if 0\n\\IEEEcompsocitemizethanks{ \n\\IEEEcompsocthanksitem T.-P.~Wu and C.-K.~Tang are with the Department of Computer Science\nand Engineering, Hong Kong University of Science and Technology, Clear Water Bay,\nHong Kong. Email: \\{pang,cktang\\}@cs.ust.hk. \n\\IEEEcompsocthanksitem S.-K. Yeung is with the Pillar of Information Systems\nTechnology and Design, Singapore University of Technology and Design, Singapore. Email: saikit@sutd.edu.sg.\n\\IEEEcompsocthanksitem J.~Jia is with the Department of Computer Science \nand Engineering, Chinese University of Hong Kong. Email: leojia@cse.cuhk.edu.hk.\n\\IEEEcompsocthanksitem G.~Medioni is with the Department of Computer Science, \nUniversity of Southern California. Email: medioni@iris.usc.edu.\n}\n\\fi\n}\n\n\n\n\\if 0\n\n\\author{Chi-Keung~Tang~\n\\IEEEcompsocitemizethanks{\n\n\n\\IEEEcompsocthanksitem C.-K. Tang is with the Department of Computer Science\nand Engineering, Hong Kong University of Science and Technology, Clear Water Bay,\nHong Kong. Email: cktang@cs.ust.hk.}\n}\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\if 0\n\n\\fi\n\n\n\n\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\\boldmath\nWe respond to the comments paper~\\cite{comment} on the proof to\nthe closed-form solution to tensor voting~\\cite{cftv} or CFTV.\nFirst, the proof is correct and let ${\\mathbf{S}}$ be the resulting tensor\nwhich may be asymmetric.  Second, ${\\mathbf{S}}$ should be interpreted using\nsingular value decomposition (SVD), where the symmetricity of ${\\mathbf{S}}$\nis unimportant, because the corresponding eigensystems\nto the positive semidefinite (PSD) systems, namely, ${\\mathbf{S}} {\\mathbf{S}}^T$ or ${\\mathbf{S}}^T {\\mathbf{S}}$, \nare used in practice.  Finally, we prove a symmetric version of \nCFTV, run extensive simulations and show that the original \ntensor voting, the asymmetric CFTV and symmetric CFTV produce \n{\\em practically the same}\nempirical results in tensor direction except\nin high uncertainty situations due to ball tensors and low saliency.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n}\n\n\n\\maketitle\n\n\n\n\n\n\n\n\n\n\\IEEEdisplaynotcompsoctitleabstractindextext\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\\setcounter{section}{0}\n\\setcounter{theorem}{0}\n\\setcounter{footnote}{0}\n\n\\section*{Addendum}\nA closed-form solution to tensor voting or CFTV was proved in~\\cite{cftv}.  \nWith CFTV, discrete voting field is no longer required where uniform\nsampling, computation and storage efficiency are issues in high dimensional \ninference.  \n\n\n\nWe respond to the comments paper~\\cite{comment} on the proof to\nthe closed-form solution to tensor voting~\\cite{cftv} or CFTV.\nFirst, the proof is correct and let ${\\mathbf{S}}$ be the resulting tensor\nwhich may be asymmetric.  Second, ${\\mathbf{S}}$ should be interpreted using\nsingular value decomposition (SVD), where the symmetricity of ${\\mathbf{S}}$\nis unimportant, because the corresponding eigensystems\nto the positive semidefinite (PSD) systems, namely, ${\\mathbf{S}} {\\mathbf{S}}^T$ or ${\\mathbf{S}}^T {\\mathbf{S}}$, \nare used in practice.  Finally, we prove a symmetric version of \nCFTV, run extensive simulations and show that the original \ntensor voting, the asymmetric CFTV and symmetric CFTV produce \n{\\em practically the same}\nempirical results in tensor direction except\nin high uncertainty situations due to ball tensors and low saliency.\n\nDirty codes for the experimental section to show the practical equivalence \nof the asymmetric CFTV, symmetric CFTV, and original discrete tensor voting \n(that is, Eq.~(2)) in 2D are available\\footnote{Reader may however find it easier \nto implement the closed form solutions on their own and generate the discrete voting \nfields for direct comparison; 2D codes for generating discrete voting fields using \nthe equation right after ``So we have\" and before the new integration by parts \nintroduced in~\\cite{cftv} are available upon request.}.\n\n\\section{Asymmetric CFTV}\nWe reprise here the main result in~\\cite{cftv} in an equivalent form:\n\\begin{theorem}\n\\emph{(Closed-Form Solution to Tensor Voting)} \n\\label{thm:cftv}\nThe tensor vote at ${\\mathbf{x}}_i$ induced by ${\\mathbf{K}}_j$ located at ${\\mathbf{x}}_j$ is given by\nthe following closed-form solution:\n\n", "itemtype": "equation", "pos": 73370, "prevtext": "\nfor all $i$. $\\mathbf{F}$ is of rank 2. Let $\\mathbf{u} = (u,v,1)^T$ and\n$\\mathbf{u'} = (u',v',1)$, Eqn.~(\\ref{eqn:essential}) can be rewritten\ninto:\n\n", "index": 51, "text": "\\begin{equation}\n\\label{eqn:essential_rearrange} \\mathbf{U}_i^T \\mathbf{h} = 0\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E24.m1\" class=\"ltx_Math\" alttext=\"\\mathbf{U}_{i}^{T}\\mathbf{h}=0\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udc14</mi><mi>i</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi>\ud835\udc21</mi></mrow><mo>=</mo><mn>0</mn></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nwhere ${\\mathbf{K}}_j$ is a second-order symmetric tensor,\n${\\mathbf{R}}_{ij} ={\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$,\n${\\mathbf I}$ is an identity, \n${\\mathbf r}_{ij}$ is a unit vector pointing from\n${\\mathbf{x}}_j$ to ${\\mathbf{x}}_i$ and $c_{ij} = \\exp( - \\frac{||{\\mathbf{x}}_i - {\\mathbf{x}}_j ||^2 }{ \\sigma_d })$\nwith $\\sigma_d$ as the scale parameter.\n\\end{theorem}\nTo simplify notation we will drop the subscripts in ${\\mathbf{R}}$ and ${\\mathbf{r}}$, and\nlet ${\\mathbf{T}}= \\frac{1}{2} {\\mathbf{K}}_j {\\mathbf{r}}{\\mathbf{r}}^T$.\n\n\n\n\nIn~\\cite{comment} an example is given: for an input PSD ${\\mathbf{K}}_j = \n\\left[\n\\begin{array}{cc}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{array}\n\\right]$, \nthe output ${\\mathbf{S}}$ computed using Theorem~\\ref{thm:cftv} \n\n\n\n\n\n\n\n\nis asymmetric, much less that ${\\mathbf{S}}$ is a PSD matrix.  It was also \npointed out in~\\cite{comment} potential technical flaws \ninvolving the derivative with respect to a unit stick tensor in \nthe proof to Theorem~\\ref{thm:cftv}, which is related to \nFootnote~3 in~\\cite{cftv}.\n\n\nNote that Theorem~\\ref{thm:cftv} does not guarantee the output \n${\\mathbf{S}}$ is symmetric or PSD. In the C codes accompanying~\\cite{cftv}, which is \navailable in the IEEE digital library, the {\\tt eig\\_sys} function \nbehaves like singular value decomposition {\\tt svd}.  The statements \nand experiments pertinent to ${\\mathbf{S}}$ in~\\cite{cftv} subsequent to Theorem~\\ref{thm:cftv} \nin fact refer to the SVD results on ${\\mathbf{S}}$, and we apologize for not \nmaking this explicitly clear in the paper.\n\nRecall the singular value decomposition and the eigen-decomposition \nare related, namely, the left-singular vectors of ${\\mathbf{S}}$ are eigenvectors \nof ${\\mathbf{S}} {\\mathbf{S}}^T$, the right singular-vectors of ${\\mathbf{S}}$ are eigenvectors of \n${\\mathbf{S}}^T {\\mathbf{S}}$, where the eigenvectors are orthonormal bases.  \n\n\nWe performed sanity check by running extensive simulations in \ndimensions up to 51 and show that \nall eigenvalues of ${\\mathbf{S}} {\\mathbf{S}}^T$ and ${\\mathbf{S}}^T {\\mathbf{S}}$ are nonnegative,\nand that they are PSD.  The symmetricity of ${\\mathbf{S}}$ is unimportant \nin practice. \n\nNonetheless, for theoretical interest we provide in the following an alternative\nproof to Theorem~\\ref{thm:cftv} which produces a symmetric ${\\mathbf{S}}$ and\nserves to dispel the flaws pointed out in~\\cite{comment}.  \nFinally we run simulations to show that the original tensor voting, \nthe asymmetric CFTV in~\\cite{cftv} and the symmetric \nCFTV in the following produce the practically the same results.\n\n\n\n\\section{Symmetric CFTV}\nFrom the first principle, integrating unit stick tensors ${\\mathbf{N}}_{\\theta} = {\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ \nin all directions $\\theta$ with strength $\\tau_\\theta^2$, we obtain Eq.~(8) in~\\cite{cftv}:\n\n\\begin{eqnarray}\n\\label{eqn:cf_inter} {\\mathbf{S}}_{ij} &=& c_{ij} \\mathbf{R} \\left(  \\mathbf{K}_j - \n\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\n\\label{eq:eq8}\n\\end{eqnarray}\nNote the similar form of Eqs~(\\ref{eq:cftv}) and (\\ref{eq:eq8}), and \nthe unconventional integration domain in Eq.~(\\ref{eq:eq8}) \nwhere $\\nu$ represents the space of stick tensors given\nby ${\\mathbf{n}}_{\\theta j}$ as explained in~\\cite{cftv}\\footnote{The suggestion in~\\cite{comment}\nwas our first attempt and as explained in~\\cite{cftv}, it does not have obvious advantage\nwhile making the derivation unnecessarily complicated.}.  Let $\\Omega$ be\nthe integration domain to shorten our notations.\n \nLet ${{\\mathbf{T}}}_{\\mathit{sym}}$ be the integration in Eq.~(\\ref{eqn:cf_inter}).  \n${\\mathbf{T}}_{\\mathit{sym}}$ can be solved by integration by parts.  Here, we repeat \nFootnote~3 in~\\cite{cftv} on the contraction of a sequence of identical unit \nstick tensors when multiplied together.  Let ${\\mathbf{N}}_{\\theta} = {\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ be a unit stick tensor, where ${\\mathbf{n}}_{\\theta}$ is a unit normal at angle $\\theta$, and $q \\ge 1$ be a positive integer, then\n\n", "itemtype": "equation", "pos": 97866, "prevtext": "\nwhere\n\\begin{eqnarray}\n\\nonumber \\mathbf{U} &=& (uu', uv', u, vu', vv' v, u', v', 1)^T \\\\\n\\nonumber \\mathbf{v} &=& (f_{11}, f_{21}, f_{31}, f_{12}, f_{22}, f_{32}, f_{13}, f_{23}, f_{33} )^T\n\\end{eqnarray}\n\nNoting that Eqn.~(\\ref{eqn:essential_rearrange}) is a simple plane\nequation, if we can detect and handle noise and outliers in the feature space,\nEqn.~(\\ref{eqn:essential_rearrange}) should enable us to produce\na good estimation.\n\n\n\n\n\n\nFinally, we apply~\\cite{hartley_defence} to obtain \na rank-2 fundamental matrix. Data normalization is similarly\ndone as in~\\cite{hartley_defence} before the optimization.\n\n\n\n\n\n\n\n\nWe evaluate the results by estimating the fundamental matrix\nof the data set {\\em Corridor}, which is available at\nwww.robots.ox.ac.uk/$\\sim$vgg/data.html. The matches\nof feature points (Harris corners) are available. Random outliers\nwere added in the feature space. \n\nFig.~\\ref{fig:epipolar_line} shows the plot of RMS error, which\nis computed by summing up and averaging\n$\\sqrt{\\frac{1}{p}\\sum_i|| \\mathbf{U}_i^T \\hat{\\mathbf{h}}||^2}$\nover all pairs,\nwhere $\\mathbf{U}_i$ is the set of clean data, and $\\hat{\\mathbf{h}}$\nis the 9D vector produced from the rank-2 fundamental matrices\nestimated by various methods. Note that all the images available\nin the {\\em Corridor} data set are used, that is, all $C^{11}_2$\npairs were tested.\n\n\nIt can be observed\nthat RANSAC breaks down at an OI ratio $\\simeq 20$, or 95.23\\% outliers.\nASSC is very stable with RMS error $< 0.15$. TV breaks down at\nan OI ratio $\\simeq 10$. EMTV has negligible RMS error before it\nstarts to break down at an OI ratio $\\simeq 40$. This finding\nechoes that of~\\cite{hartley_defence} that linear solution is sufficient\nwhen outliers are properly handled.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.75\\linewidth]{fig13.png}\n\\end{center}\n\\caption{\\small Results before and after filtering of {\\em Hall 3} (images shown\nin Fig.~\\ref{fig:hall3_recon}). \n\n\nAll salient 3D structures are retained in the filtered\nresult, including the bushes near the left facade and planters near the\nright facade in this top view of the building.}\n\\label{fig:filtering}\n\\vspace{-0.2in}\n\\end{figure}\n\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{fig14.png}\n\\end{center}\n\\vspace{-0.2in}\n\\caption{\\small The {\\em Hall 3} reconstruction: ten of the input images (top) and five views of the quasi-dense 3D reconstruction (bottom).}\n\\label{fig:hall3_recon}\n\\vspace{-0.2in}\n\\end{figure*}\n\n\\subsection{Matching}\n\\label{sec:teapot}\nIn the uncalibrated scenario, EMTV estimates parameter accurately by employing\nCFTV, and effectively discards epipolar geometries induced by wrong matches.\nTypically, camera calibration is performed using nonlinear least-squares\nminimization and bundle adjustment~\\cite{lourakis_ba} which requires good\nmatches as input.  In this experiment, candidate matches\nare generated by comparing the resulting 128D SIFT feature vectors~\\cite{sift}, \nso many matched keypoints are not corresponding.\n\nThe epipolar constraint is enforced in\nthe matching process using EMTV, which returns the fundamental\nmatrix {\\em and} the probability $w_i$ (Eqn~(\\ref{eqn:e_step_update}))\nof a keypoint pair $i$ being an inlier. In the experiment,\nwe assume keypoint pair $i$ is an inlier if $w_i > 0.8$.\n\\if 0\nNote that we did not use any random sampling.\nThe following compares {\\small {\\tt emtv\\_match}} with\n{\\small {\\tt KeyMatchFull}}~\\cite{snavely_ijcv07},\n{\\small {\\tt assc\\_match}}~\\cite{wang_and_suter},\n{\\small {\\tt ransac\\_match}}, and\n{\\small {\\tt linear\\_match}},\nwhere the latter three perform hyper-plane fitting by using\nASSC, RANSAC, and least-squares respectively.\n\n\\begin{figure}[t]\n\\begin{center}\n\\begin{tabular}{@{\\hspace{0mm}}c@{\\hspace{0mm}}c@{\\hspace{0mm}}c}\n\\includegraphics[width=0.30\\linewidth]{figure/LongJing/TeaBoxUnd2.bmp.eps} &\n\\includegraphics[width=0.32\\linewidth]{figure/LongJing/original_tea.bmp.eps} &\n\\includegraphics[width=0.32\\linewidth]{figure/LongJing/EM_tea.bmp.eps} \\\\\n{\\small (a) } & {\\small (b)} & {\\small (c)} \\\\\n\\end{tabular}\n\\end{center}\n\\caption{\\small {\\em Tea Can}: (a) One of the two input images. (b) Sparse\nreconstruction generated by using {\\small {\\tt KeyMatchFull}}. (c) Sparse\nreconstruction generated by using {\\small {\\tt emtv\\_match}}. }\n\\label{fig:longjing}\n\\vspace{-0.2in}\n\\end{figure}\n\n\\noindent {\\em Tea Can}.  Fig.~\\ref{fig:longjing} shows\nthat, by using our filtered matches, even in the absence\nof any focal length input, our sparse reconstruction of\nthe tea can (the image pair was obtained from~\\cite{zzhang_pami}),\nproduced by the nonlinear least-squares minimization and\nbundle adjustment~\\cite{lourakis_ba}, is denser and contains\nless errors as compared with~\\cite{snavely_ijcv07}, where\nwe can faithfully reconstruct the right-angled container.\n\n\n\\noindent {\\em Teapot}. \n\\fi\nFig.~\\ref{fig:teapot} shows\nour running example {\\em teapot} which contains\nrepetitive patterns across the whole object. Wrong matches\ncan be easily produced by similar patterns on different parts\nof the teapot. This data set contains 30 images captured\nusing a Nikon D70 camera. Automatic configuration was set\nduring the image capture.\nVisually, the result produced using {\\small {\\tt emtv\\_match}}\nis much denser than the results produced with\n{\\small {\\tt KeyMatchFull}}~\\cite{snavely_ijcv07}, \n{\\small {\\tt linear\\_match}},\n{\\small {\\tt assc\\_match}}~\\cite{wang_and_suter}, \nand {\\small {\\tt ransac\\_match}}.\nNote in particular that only {\\small {\\tt emtv\\_match}}\nrecovers the overall geometry of the teapot, whereas\nthe other methods can only recover one side of the teapot.\n\n\n\nThis example is challenging, because the teapot's shape is quite symmetric\nand the patchy patterns look identical everywhere. \nAs was done in~\\cite{snavely_ijcv07},\neach photo was paired respectively with a number of photos with\ncamera poses satisfying certain basic criteria conducive to matching\nor making the numerical process stable\n(e.g. wide-baseline stereo). We can regard this pair-up process\nas one of computing connected components. If the fundamental matrix\nbetween any successive images are incorrectly estimated,\nthe corresponding components will no longer be connected,\nresulting in the situation that only one side or part of the\nobject can be recovered.\n\nSince {\\small {\\tt KeyMatchFull}} and {\\small {\\tt linear\\_match}}\nuse simple distance measure for finding matches, the coverage of\nthe corresponding connected components tend to be small. \nIt is interesting to note that the worst result is produced by using\n{\\small {\\tt ransac\\_match}}. This can be attributed to three reasons:\n(1) the fundamental matrix is of rank 2 which implies that ${\\mathbf{h}}$\nspans a subspace $\\leq$ 8-D rather than a 9-D hyperplane;\n(2) the input matches contain too many outliers for some image pairs;\n(3) it is not feasible to fine tune the scale parameter for\nevery possible image pair and so we used a single value for all of the images.\nA slight improvement could be found from\nASSC. However, it still suffers from problems (1) and (2) and so the\nresult is not very good even compared with\n{\\small {\\tt KeyMatchFull}} and {\\small {\\tt linear\\_match}}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, {\\small {\\tt emtv\\_match}} utilizes the epipolar\ngeometry constraint by computing the fundamental matrix in a data\ndriven manner. Since the outliers are effectively filtered out,\nthe estimated fundamental matrices are sufficiently accurate\nto pair up all of the images into a single connected component. \nThus, the overall 3D geometry can be recovered from all the available views.\n\n\\if 0\n\\begin{figure*}[!ht]\n  \\begin{center}\n  \\begin{tabular}{@{\\hspace{-0.05in}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c@{\\hspace{1mm}}c}\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth_view1.bmp.eps} &\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth2_pmvs_normal_view0.bmp.eps} &\n  \\includegraphics[width=0.14\\linewidth]{figure/earth/earth2_tmvs_normal_view0.bmp.eps} &\n  \\includegraphics[width=0.17\\linewidth]{figure/earth/earth2_pmvs_view0.bmp.eps} &\n  \\includegraphics[width=0.17\\linewidth]{figure/earth/earth2_tmvs_view0.bmp.eps} \\\\\n(a) & (b) & (c) & (d) & (e)\n  \\end{tabular}\n  \\end{center}\n  \\caption{\\small {\\em Earth}. (a) an input image (81 in total), (b) and (c): zoom-in\n   views of the normal reconstruction produced by PMVS and TMVS after the propagation\n   step, (d) and (e) show respectively one view of the quasi-dense reconstruction by\n  PMVS and TMVS.}\n  \\label{fig:earth}\n\\vspace{-0.1in}\n\\end{figure*}\n\\fi\n\n\n\n\\subsection{Multiview Stereo Reconstruction}\n\\label{sec:mvs}\nThis section outlines how CFTV and EMTV are applied to \nimprove the match-propagate-filter pipeline in multiview stereo.\nMatch-propagate-filter is a competitive approach to \nmultiview stereo\nreconstruction for computing a (quasi) dense representation. Starting\nfrom a sparse set of initial matches with high confidence, matches\nare propagated using photoconsistency to produce a (quasi) dense\nreconstruction of the target shape. Visibility consistency can be\napplied to remove outliers.\nAmong the existing works using the match-propagate-filter\napproach, patch-based multiview stereo (or PMVS) proposed\nin~\\cite{furukawa_pami09} has produced some best results to date. \n\nWe observe that PMVS had not fully utilized the 3D information\ninherent in the sparse and dense geometry before, during \nand after propagation, as patches do not adequately communicate \namong each other. As noted in~\\cite{furukawa_pami09}, data communication \nshould not be\ndone by smoothing, but the lack of communication will cause\nperturbed surface normals and patch outliers during the propagation \nstage.  In~\\cite{tmvs}, we proposed tensor-based multiview stereo (TMVS) \nand used 3D structure-aware tensors which communicate among each other \nvia CFTV.  We found that such tensor communication not only improves \npropagation in MVS without undesirable\nsmoothing but also benefits the entire match-propagate-filter pipeline\nwithin a unified framework. \n\nWe captured 179 photos around a building which \nwere first calibrated as described in section~\\ref{sec:teapot}.\nAll images were taken on the ground\nlevel not higher than the building, so we have very few samples of the\nrooftop. The building facades are curved and the windows on the building\nlook identical to each other. The patterns on the front and back facade\nlook nearly identical. These ambiguities cause significant challenges\nin the matching stage especially\nfor wide-baseline stereo. TMVS was run to obtain the quasi-dense reconstruction,\nwhere MRFTV was used to filter outliers as shown in Fig.~\\ref{fig:filtering}.\nFig.~\\ref{fig:hall3_recon} shows the 3D reconstruction which is faithful\nto the real building.\nReaders are referred to~\\cite{tmvs} for more detail and experimental\nevaluation of TMVS.  \n\n\n\\if 0\n\n\\begin{figure}[t]\n\\begin{center}\n\\begin{tabular}{\n@{\\hspace{0mm}}c@{\\hspace{0mm}}c\n@{\\hspace{0mm}}c@{\\hspace{0mm}}c\n}\n\\includegraphics[height=1.5in]{figure/tripp/tripp_image.eps} &\n\\includegraphics[height=1.5in]{figure/tripp/tripp_pmvs_view0.bmp.eps} &\n\\includegraphics[height=1.5in]{figure/tripp/tripp_tmvs_view0.bmp.eps} &\n\\end{tabular}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em Tripp} reconstruction from sparse data set:\nthree input images (left) and the quasi-dense 3D reconstruction\nproduced by PMVS (middle) and TMVS (right).}\n\\label{fig:tripp}\n\n\n\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{figure/george/george_image.eps}\n\\begin{tabular}{\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n@{\\hspace{0.2mm}}c@{\\hspace{0.2mm}}c\n}\n\\includegraphics[height=1in]{figure/george/george_view0.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view1.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view3.bmp.eps} &\n\\includegraphics[height=1in]{figure/george/george_view2.bmp.eps}\n\\end{tabular}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small {\\em George} reconstruction from sparse data set:\nfive input images (top) and four views of the quasi-dense\n3D reconstruction (bottom).}\n\\label{fig:george}\n\\vspace{-0.1in}\n\\end{figure}\n\\fi\n\n\n\n\n\\section{Conclusions}\nA closed-form solution is proved for the special theory of tensor voting\n(CFTV) for computing an exact structure-aware tensor in any dimensions.\nFor structure propagation, we derive a quadratic energy for MRFTV, thus \nproviding a convergence proof for tensor voting which is impossible \nto prove using the original tensor voting procedure. Then, \nwe derive EMTV for optimizing both the tensor and model parameters for \nrobust parameter estimation. We performed quantitative and qualitative \nevaluation using challenging synthetic and real data sets.  \n\n\n\n\nIn the future we will develop a closed-form solution for the general theory of tensor voting, and extend EMTV to extract \nmultiple and nonlinear structures.  We have provided C$++$ source code, \nbut it is straightforward to implement \nEqns~(\\ref{eqn:cal_S}), (\\ref{eqn:cal_S_inv}), \n(\\ref{eqn:e_step_update}), (\\ref{eqn:m_step_update}), \n(\\ref{eqn:energy_update_rule}), and (\\ref{eqn:sor_update_rule}).\nWe demonstrated promising results in multiview stereo, and will \napply our closed-form solution to address important computer \nvision problems.\n\n\n\\ifCLASSOPTIONcompsoc\n\n  \\section*{Acknowledgments}\n\\else\n  \n  \\section*{Acknowledgment}\n\\fi\n\nThe authors would like to thank the Associate Editor and all of the\nanonymous reviewers. Special thanks go to Reviewer 1 for his/her \nhelpful and detailed comments throughout the review cycle. \n\n\n\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\small\n\\bibliographystyle{ieee}\n\\bibliography{cftv}\n}\n\n\\vspace{-0.5in}\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{pang_pic.png}}] \n{Tai-Pang Wu} \nreceived the PhD degree in computer science from the Hong\nKong University of Science and Technology (HKUST) in 2007. He was\nawarded the Microsoft Fellowship in 2006. After graduation, he has\nbeen employed as a postdoctoral fellow in Microsoft Research Asia\nBeijing (2007--2008) and the Chinese University of Hong Kong\n(2008--2010). He is currently a Senior Research at the Enterprise and \nConsumer Electronics Group of the Hong Kong Applied Science \nand Technology Research Institute (ASTRI). \nHis research interests include computer vision and computer graphics. \nHe is a member of the IEEE and the IEEE Computer\nSociety.\n\\end{IEEEbiography}\n\n\\vspace{-0.5in}\n\\begin{IEEEbiography} \n[{\\includegraphics[width=1\\linewidth]{kit.png}}] \n{Sai-Kit Yeung} received his PhD degree in electronic and computer\nengineering from the Hong Kong University of Science and Technology\n(HKUST) in 2009. He received his BEng degree (First Class Honors) in\ncomputer engineering and MPhil degree in bioengineering from HKUST in\n2003 and 2005 respectively. \n\n\n\nHe is currently an Assistant Professor at the Singapore University \nof Technology and Design (SUTD).\nPrior to joining SUTD, he was a Postdoctoral Scholar in the Department\nof Mathematics, University of California, Los Angeles (UCLA) in 2010.\nHis research interests include computer vision, computer graphics,\ncomputational photography, image/video processing, and medical\nimaging. He is a member of IEEE and IEEE Computer Society.\n\\end{IEEEbiography}\n\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{jia.png}}] \n{Jiaya Jia} \nJiaya Jia received the PhD degree in computer science from Hong Kong\nUniversity of Science and Technology in 2004 and is currently an associate\nprofessor in Department of Computer Science and Engineering at the Chinese\nUniversity of Hong Kong (CUHK). He was a visiting scholar at Microsoft\nResearch Asia from March 2004 to August 2005 and conducted collaborative\nresearch at Adobe Systems in 2007. He leads the research group in CUHK,\nfocusing on computational photography, 3D reconstruction, practical\noptimization, and motion estimation. He serves as an associate editor for\nTPAMI and as an area chair for ICCV 2011. He was on the program committees\nof several major conferences, including ICCV, ECCV, and CVPR, and co-chaired\nthe Workshop on Interactive Computer Vision in conjunction with ICCV 2007.\nHe received the Young Researcher Award 2008 and Research Excellence Award\n2009 from CUHK. He is a senior member of the IEEE.\n\\end{IEEEbiography}\n\n\\vspace{-1.0in}\n\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{tang_ck.png}}] \n{Chi-Keung Tang}\nreceived the MSc and PhD degrees in\ncomputer science from the University of Southern California\n(USC), Los Angeles, in 1999 and 2000, respectively.\nSince 2000, he has been with the Department of\nComputer Science at the Hong Kong University of Science and\nTechnology (HKUST) where he is currently a professor. He is an \nadjunct researcher at the Visual Computing Group of Microsoft\nResearch Asia.  His research areas are computer vision, computer\ngraphics, and human-computer interaction. He is an associate editor\nof IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), \nand on the editorial board of International Journal of Computer Vision (IJCV).\nHe served as an area chair for ACCV 2006 (Hyderabad), ICCV 2007\n(Rio de Janeiro), ICCV 2009 (Kyoto), ICCV 2011 (Barcelona), \nand as a technical papers committee member for the inaugural \nSIGGRAPH Asia 2008 (Singapore), SIGGRAPH 2011 (Vancouver), and \nSIGGRAPH Asia 2011 (Hong Kong), SIGGRAPH 2012 (Los Angeles).  \nHe is a senior member of the IEEE and the IEEE Computer Society.\n\\end{IEEEbiography}\n\n\\vspace{-1.0in}\n\\begin{IEEEbiography}\n[{\\includegraphics[width=1\\linewidth]{medioni.png}}] \n{G\\'{e}rard Medioni} \nreceived the Dipl\\^{o}me d'Ing\\'{e}nieur\nfrom the \\'{E}cole Nationale Sup\\'{e}rieure des\nT\\'{e}l\\'{e}communications (ENST), Paris, in 1977 and\nthe MS and PhD degrees from the University of\nSouthern California (USC) in 1980 and 1983,\nrespectively. He has been with USC since then\nand is currently a professor of computer science\nand electrical engineering, a codirector of the\nInstitute for Robotics and Intelligent Systems\n(IRIS), and a codirector of the USC Games\nInstitute. He served as the chairman of the Computer Science\nDepartment from 2001 to 2007. He has made significant contributions\nto the field of computer vision. His research covers a broad spectrum of\nthe field, such as edge detection, stereo and motion analysis, shape\ninference and description, and system integration. He has published\n3 books, more than 50 journal papers, and 150 conference articles. He\nis the holder of eight international patents. He is an associate editor of\nthe Image and Vision Computing Journal, Pattern Recognition and\nImage Analysis Journal, and International Journal of Image and Video\nProcessing. He served as a program cochair of the 1991 IEEE\nComputer Vision and Pattern Recognition (CVPR) Conference and\nthe 1995 IEEE International Symposium on Computer Vision, a general\ncochair of the 1997 IEEE CVPR Conference, a conference cochair of\nthe 1998 International Conference on Pattern Recognition, a general\ncochair of the 2001 IEEE CVPR Conference, a general cochair of the\n2007 IEEE CVPR Conference, and a general cochair of the upcoming\n2009 IEEE CVPR Conference. He is a fellow of the IEEE, IAPR, and\nAAAI and a member of the IEEE Computer Society.\n\\end{IEEEbiography}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSOPTIONcompsoc\n  \n  \n  \n\\else\n  \n  \n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSINFOpdf\n  \n  \n  \n  \n  \n  \n\\else\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hyphenation{op-tical net-works semi-conduc-tor}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\author{\n        Tai-Pang~Wu,~\n\tSai-Kit~Yeung,~\n        Chi-Keung~Tang$^*$\\thanks{Please direct all inquiries to the contact author. E-mail: cktang@cse.ust.hk},\n\n\n        Jiaya Jia~\n        and~Gerard~Medioni\n\\if 0\n\\IEEEcompsocitemizethanks{ \n\\IEEEcompsocthanksitem T.-P.~Wu and C.-K.~Tang are with the Department of Computer Science\nand Engineering, Hong Kong University of Science and Technology, Clear Water Bay,\nHong Kong. Email: \\{pang,cktang\\}@cs.ust.hk. \n\\IEEEcompsocthanksitem S.-K. Yeung is with the Pillar of Information Systems\nTechnology and Design, Singapore University of Technology and Design, Singapore. Email: saikit@sutd.edu.sg.\n\\IEEEcompsocthanksitem J.~Jia is with the Department of Computer Science \nand Engineering, Chinese University of Hong Kong. Email: leojia@cse.cuhk.edu.hk.\n\\IEEEcompsocthanksitem G.~Medioni is with the Department of Computer Science, \nUniversity of Southern California. Email: medioni@iris.usc.edu.\n}\n\\fi\n}\n\n\n\n\\if 0\n\n\\author{Chi-Keung~Tang~\n\\IEEEcompsocitemizethanks{\n\n\n\\IEEEcompsocthanksitem C.-K. Tang is with the Department of Computer Science\nand Engineering, Hong Kong University of Science and Technology, Clear Water Bay,\nHong Kong. Email: cktang@cs.ust.hk.}\n}\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\if 0\n\n\\fi\n\n\n\n\n\n\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\\boldmath\nWe respond to the comments paper~\\cite{comment} on the proof to\nthe closed-form solution to tensor voting~\\cite{cftv} or CFTV.\nFirst, the proof is correct and let ${\\mathbf{S}}$ be the resulting tensor\nwhich may be asymmetric.  Second, ${\\mathbf{S}}$ should be interpreted using\nsingular value decomposition (SVD), where the symmetricity of ${\\mathbf{S}}$\nis unimportant, because the corresponding eigensystems\nto the positive semidefinite (PSD) systems, namely, ${\\mathbf{S}} {\\mathbf{S}}^T$ or ${\\mathbf{S}}^T {\\mathbf{S}}$, \nare used in practice.  Finally, we prove a symmetric version of \nCFTV, run extensive simulations and show that the original \ntensor voting, the asymmetric CFTV and symmetric CFTV produce \n{\\em practically the same}\nempirical results in tensor direction except\nin high uncertainty situations due to ball tensors and low saliency.\n\\end{abstract}\n\n\n\n\n\n\n\n\n\n\n\n\n}\n\n\n\\maketitle\n\n\n\n\n\n\n\n\n\n\\IEEEdisplaynotcompsoctitleabstractindextext\n\n\n\n\n\n\n\n\n\n\n\n\n\\IEEEpeerreviewmaketitle\n\n\n\\setcounter{section}{0}\n\\setcounter{theorem}{0}\n\\setcounter{footnote}{0}\n\n\\section*{Addendum}\nA closed-form solution to tensor voting or CFTV was proved in~\\cite{cftv}.  \nWith CFTV, discrete voting field is no longer required where uniform\nsampling, computation and storage efficiency are issues in high dimensional \ninference.  \n\n\n\nWe respond to the comments paper~\\cite{comment} on the proof to\nthe closed-form solution to tensor voting~\\cite{cftv} or CFTV.\nFirst, the proof is correct and let ${\\mathbf{S}}$ be the resulting tensor\nwhich may be asymmetric.  Second, ${\\mathbf{S}}$ should be interpreted using\nsingular value decomposition (SVD), where the symmetricity of ${\\mathbf{S}}$\nis unimportant, because the corresponding eigensystems\nto the positive semidefinite (PSD) systems, namely, ${\\mathbf{S}} {\\mathbf{S}}^T$ or ${\\mathbf{S}}^T {\\mathbf{S}}$, \nare used in practice.  Finally, we prove a symmetric version of \nCFTV, run extensive simulations and show that the original \ntensor voting, the asymmetric CFTV and symmetric CFTV produce \n{\\em practically the same}\nempirical results in tensor direction except\nin high uncertainty situations due to ball tensors and low saliency.\n\nDirty codes for the experimental section to show the practical equivalence \nof the asymmetric CFTV, symmetric CFTV, and original discrete tensor voting \n(that is, Eq.~(2)) in 2D are available\\footnote{Reader may however find it easier \nto implement the closed form solutions on their own and generate the discrete voting \nfields for direct comparison; 2D codes for generating discrete voting fields using \nthe equation right after ``So we have\" and before the new integration by parts \nintroduced in~\\cite{cftv} are available upon request.}.\n\n\\section{Asymmetric CFTV}\nWe reprise here the main result in~\\cite{cftv} in an equivalent form:\n\\begin{theorem}\n\\emph{(Closed-Form Solution to Tensor Voting)} \n\\label{thm:cftv}\nThe tensor vote at ${\\mathbf{x}}_i$ induced by ${\\mathbf{K}}_j$ located at ${\\mathbf{x}}_j$ is given by\nthe following closed-form solution:\n\n", "index": 53, "text": "\\begin{equation}\n{\\mathbf{S}}_{ij} = c_{ij} {\\mathbf{R}}_{ij} \\left( {\\mathbf{K}}_j - \\frac{1}{2} {\\mathbf{K}}_j {\\mathbf{r}}_{ij}{\\mathbf{r}}_{ij}^T \\right) {\\mathbf{R}}^T_{ij}\n\\label{eq:cftv}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E25.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{S}}_{ij}=c_{ij}{\\mathbf{R}}_{ij}\\left({\\mathbf{K}}_{j}-\\frac{1}{2}{%&#10;\\mathbf{K}}_{j}{\\mathbf{r}}_{ij}{\\mathbf{r}}_{ij}^{T}\\right){\\mathbf{R}}^{T}_{ij}\" display=\"block\"><mrow><msub><mi>\ud835\udc12</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>c</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msub><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><mrow><mo>(</mo><mrow><msub><mi>\ud835\udc0a</mi><mi>j</mi></msub><mo>-</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>\u2062</mo><msub><mi>\ud835\udc0a</mi><mi>j</mi></msub><mo>\u2062</mo><msub><mi>\ud835\udc2b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc2b</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>T</mi></msubsup></mrow></mrow><mo>)</mo></mrow><mo>\u2062</mo><msubsup><mi>\ud835\udc11</mi><mrow><mi>i</mi><mo>\u2062</mo><mi>j</mi></mrow><mi>T</mi></msubsup></mrow></mrow></math>", "type": "latex"}, {"file": "1601.04888.tex", "nexttext": "\nTo preserve symmetry, we leverage Footnote~3 or Eq.~(\\ref{eq:eat}) above but rather than exclusively using contraction, (i.e., ${\\mathbf{N}}_{\\theta}^q = {\\mathbf{N}}_{\\theta}$) as done in~\\cite{cftv}, we use expansion (i.e., ${\\mathbf{N}}_{\\theta} = {\\mathbf{N}}_{\\theta}^q$) in the following:\n\\begin{eqnarray}\n{\\mathbf{T}}_{\\mathit{sym}} \n= \\int_{\\Omega} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta}\n= \\int_{\\Omega} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2 \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2 d\\mathbf{N}_{\\theta} \n\n\\end{eqnarray}\nLet $f(\\theta) = \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2$, then\n$f'(\\theta) = 2 \\tau_{\\theta}^2 {\\mathbf{N}}_\\theta d{\\mathbf{N}}_\\theta = 2 \\tau_{\\theta}^2 {\\mathbf{N}}_\\theta^2 d{\\mathbf{N}}_\\theta$ after\nexpansion.  Similarly, let\n$g(\\theta) = \\frac{1}{2} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2$ and \n$g'(\\theta) = \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d{\\mathbf{N}}_\\theta =\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2 d{\\mathbf{N}}_{\\theta}$ after\nexpansion.  \n\\footnote{This is in disagreement with the claims about the authors' \nEq.~(7) in~\\cite{comment}, which was in fact never used in their \nintention in our derivation in~\\cite{cftv}.}\nNote also ${\\mathbf K}_j$, in the most general form, can be\nexpressed as $\\int_{\\Omega} \n\\tau_{\\theta}^2 \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta}$.  So, we obtain\n\\begin{eqnarray}\n\\nonumber {{\\mathbf{T}}}_{\\mathit{sym}} &=&\\int_{\\Omega} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2 \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2 d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ f(\\theta) g(\\theta) \\right]_{\\Omega} - \\int_{\\Omega} f'(\\theta) g(\\theta) d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=& \\left[ \\frac{1}{2} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2\n\\right]_{\\Omega}\n- \\int_{\\Omega} \\tau_{\\theta}^2 {\\mathbf{N}}_\\theta^2 \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2 d\\mathbf{N}_{\\theta} \\\\\n\\nonumber &=&  \\left[ \\frac{1}{2} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2\n\\right]_{\\Omega}\n- {\\mathbf{T}}_{\\mathit{sym}} \\\\\n\n\n\\nonumber &=& \\frac{1}{4}  \\left[ \\tau_{\\theta}^2 \\mathbf{N}_{\\theta}^2\n\\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta}^2\n\\right]_{\\Omega} \\\\\n\\nonumber &=&  \\frac{1}{4} \\int_{\\Omega} \\left( \\tau_{\\theta}^2 \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf N}_{\\theta}^2] {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2\n+ \\tau_{\\theta}^2 {\\mathbf N}_{\\theta}^2 \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}^2] \\right) d{\\mathbf N}_{\\theta} \\\\\n\\nonumber\n&=&  \\frac{1}{4} \\int_{\\Omega} \\left( \\tau_{\\theta}^2 \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf N}_{\\theta}] {\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta} + \\tau_{\\theta}^2 {\\mathbf N}_{\\theta} \\frac{d}{d{\\mathbf N}_{\\theta}}[{\\mathbf r} {\\mathbf r}^T {\\mathbf N}_{\\theta}] \\right) d{\\mathbf N}_{\\theta} \\\\\n\\\\\n\\label{eq:contract} \n\n\\nonumber &=&  \\frac{1}{4}   \\int_{\\Omega} \\left(  {\\mathbf{r}} {\\mathbf{r}}^T \\tau_\\theta^2 {\\mathbf{N}}_\\theta + \\tau_\\theta^2 {\\mathbf{N}}_\\theta {\\mathbf{r}} {\\mathbf{r}}^T \\right)  d{\\mathbf N}_{\\theta} \\\\\n\\label{eq:symcftv} &=& \\frac{1}{4} \\left( {\\mathbf{r}} {\\mathbf{r}}^T {\\mathbf{K}}_j + {\\mathbf{K}}_j {\\mathbf{r}} {\\mathbf{r}}^T \\right).\n\\end{eqnarray}\nHere, in the derivative with respect to a unit stick tensor ${\\mathbf{N}}_\\theta$ \nalong the tensor direction, the tensor magnitude $\\tau_\\theta^2$ can \nbe legitimately regarded as a constant\\footnote{Here is another disagreement \nwith~\\cite{comment}: the tensor direction and tensor magnitude are two entirely \ndifferent entities.}. \n\nNote that we apply contraction in Eq.~(\\ref{eq:contract}).  Comparing the\npertinent ${\\mathbf{T}}$ in the asymmetric CFTV in Eq.~(\\ref{eq:cftv}) and symmetric CFTV in Eq.~(\\ref{eq:symcftv}):\n\\begin{eqnarray*}\n{\\mathbf{T}}_{\\mathit{asym}} &=& \\frac{1}{2} {\\mathbf{K}}_j {\\mathbf{r}} {\\mathbf{r}}^T \\\\ \\nonumber  \n{\\mathbf{T}}_{\\mathit{sym}} &=& \\frac{1}{4} \\left( {\\mathbf{r}} {\\mathbf{r}}^T {\\mathbf{K}}_j + {\\mathbf{K}}_j {\\mathbf{r}} {\\mathbf{r}}^T \\right)\n\\end{eqnarray*}\nit is interesting to observe how tensor contraction and expansion when applied as\ndescribed can preserve tensor symmetry whereas in~\\cite{cftv}, only tensor \ncontraction was applied. \n\nNotwithstanding, the symmetricity of ${\\mathbf{S}}$ is unimportant in practice as \nsingular value decomposition will be applied to ${\\mathbf{S}}$ before the \neigenvalues and eigenvectors are used.  The following section reports \nthe results of our experiments on the asymmetric and symmetric CFTV \nwhen used in practice. \n\n\\section{Experiments}\n\nIn each of the following simulations, a random input PSD matrix \n${\\mathbf{K}} = {\\mathbf{A}} {\\mathbf{A}}^T + \\epsilon {\\mathbf{I}}$ is generated, where ${\\mathbf{A}}$ is \na random square matrix, ${\\mathbf{I}}$ is an identity, $\\epsilon$ is a \nsmall constant (e.g. $1e^{-2})$. \n\n\\begin{enumerate}\n\\item  (Sanity check)\n$N$D simulations ($N = 2$ to $51$) of $1000$ tensors for each dimension\n\\begin{enumerate}\n\\item The ${\\mathbf{S}}$ produced by symmetric CFTV is indeed symmetric. This is confirmed\nby testing each ${\\mathbf{S}}$ using ${\\mathit norm}({\\mathbf{S}}^{-1} {\\mathbf{S}}^T-{\\mathbf{I}}) = 0$, \nwhere ${\\mathit norm}(\\cdot)$ is the $L_2$ norm of a matrix. \n\\item ${\\mathbf{S}}^T {\\mathbf{S}}$ and ${\\mathbf{S}} {\\mathbf{S}}^T$ are PSD where ${\\mathbf{S}}$ can be produced by asymmetric or\nsymmetric CFTV.  This is validated by checking all eigenvalues being nonnegative.\n\\end{enumerate}\n\n\\item \n2D simulations of more than 1 million tensors show the practical equivalence \nin tensor direction among\n\\begin{enumerate}\n\\item discrete solution to original tensor voting Eq.~(\\ref{eq:eq8}), or Eq.~(8) in~\\cite{cftv},\n\\item ${\\mathbf{S}}^T {\\mathbf{S}}$ and ${\\mathbf{S}} {\\mathbf{S}}^T$ produced by asymmetric CFTV,\n\\item ${\\mathbf{S}}^T {\\mathbf{S}}$ ($ = {\\mathbf{S}} {\\mathbf{S}}^T$ when ${\\mathbf{S}}$ is symmetric) produced by symmetric CFTV,\n\\end{enumerate}\nwhile relative tensor saliency is preserved.\n\\label{eqtest}\n\n\\item \n$N$D simulations ($N = 2$ to $51$) of $1000$ tensors for each dimension \nshow the practical equivalence in tensor direction among\n\\begin{enumerate}\n\\item ${\\mathbf{S}}^T {\\mathbf{S}}$ and ${\\mathbf{S}} {\\mathbf{S}}^T$ produced by asymmetric CFTV,\n\\item ${\\mathbf{S}}^T {\\mathbf{S}}$ ($ = {\\mathbf{S}} {\\mathbf{S}}^T$) produced by symmetric CFTV,\n\\end{enumerate}\nin their largest eigenvectors which encompass the most ``energy\" while the rest\nrepresents uncertainty in orientation each spanning a plane perpendicular to \nthe largest eigenvectors.\n\\label{eqtest3}\n\n\\end{enumerate}\n\nFor simulations in (\\ref{eqtest}), we exclude ball tensors from our tabulation \nfor the obvious reason: any two orthonormal vectors describe the equivalent \nunit ball tensor.  The mean and maximum deviation in tensor\ndirection are respectively 0.9709 and 0.9537 (score for perfect alignment is 1) in terms \nof the dot product among the relevant eigenvectors.  \nThe deviation can be explained by the imperfect uniform sampling for a 2D ellipse used in \ncomputing the discrete tensor voting solution: there is no good way\nfor uniform sampling in $N$ dimensions\\footnote{While uniform sampling on a 2D circle\nis trivial, uniform sampling on a 2D ellipse is not straightforward.  \nFor a 3D {\\em sphere}, recursive subdivision of an icosahedron is \na good approximation but no good approximation for a 3D ellipsoid exists.}. \nWhen\nwe turned off the discrete tensor voting but compared only the asymmetric\nand symmetric CFTV, the mean and maximum deviation in tensor direction \nare respectively improved to 0.9857 and 0.9718.   \n\nFor tensor saliency, we found that while the normalized saliencies are \nnot identical among the four versions of tensor voting~\\footnote{The \ntensor saliency produced by discrete simulation of Eq.~(\\ref{eq:eq8}) \nis proportional to the number of samples, thus we normalize the eigenvalues\nsuch that the smallest eigenvalue is 1.}, their relative order is preserved.  \nThat is, when we sort the eigenvectors according to their corresponding \neigenvalues in the respective implementation of tensor voting, the \nsorted eigenvectors are always empirically identical among the four \ncases.\n\n\nFor simulations in (\\ref{eqtest3}), we did not compare the discrete solution: \nin higher dimensions, uniform sampling of an $N$D ellipsoid is an issue to \ndiscrete tensor voting.  The mean and maximum deviation among the largest\neigenvector of the three versions are respectively 0.9940 and 0.9857. \n \n\\section{Epilogue}\n\nWe revisit the main result in~\\cite{cftv}, and reconfirm the efficacy of the \nclosed-form solution to tensor voting which votes for the \nmost likely connection without discrete voting fields, which is \nparticularly relevant in high dimensional inference where uniform\nsampling and storage of discrete voting fields are issues.  As an aside,\nwe prove the symmetric version of CFTV.\n\nThe application of tensor contraction and expansion given by Eq.~(\\ref{eq:eat}) \nis instrumental to the derivation of the closed-form solution.  Interestingly,\nwhile ${\\mathbf{S}}$ may be asymmetric, pre-multiplying or post-multiplying by itself \nnot only echos the contraction/expansion operation given by Eq.~(\\ref{eq:eat}) but also\nproduces a PSD system that agrees with the original tensor voting result.  \nAs shown above, the inherent flexibility also makes symmetric CFTV possible. \nThus we believe further exploration may lead to useful and interesting \ntheoretical results on tensor voting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\\small\n\n\\begin{thebibliography}{1}\\itemsep=-1pt\n\n\\bibitem{comment}\nE.~Maggiori, P.~Lotito, H.~L. Manterola, and M.~del Fresno.\n\\newblock Comments on ``a closed-form solution to tensor voting: Theory and\n  applications\".\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  36(12), 2014.\n\n\\bibitem{cftv}\nT.-P. Wu, S.-K. Yeung, J.~Jia, C.-K. Tang, and G.~Medioni.\n\\newblock A closed-form solution to tensor voting: Theory and applications.\n\\newblock {\\em IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  34(8):1482--1495, 2012.\n\n\\end{thebibliography}\n\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "itemtype": "equation", "pos": 102215, "prevtext": "\nwhere ${\\mathbf{K}}_j$ is a second-order symmetric tensor,\n${\\mathbf{R}}_{ij} ={\\mathbf I} - 2 {\\mathbf r}_{ij} {\\mathbf r}_{ij}^T$,\n${\\mathbf I}$ is an identity, \n${\\mathbf r}_{ij}$ is a unit vector pointing from\n${\\mathbf{x}}_j$ to ${\\mathbf{x}}_i$ and $c_{ij} = \\exp( - \\frac{||{\\mathbf{x}}_i - {\\mathbf{x}}_j ||^2 }{ \\sigma_d })$\nwith $\\sigma_d$ as the scale parameter.\n\\end{theorem}\nTo simplify notation we will drop the subscripts in ${\\mathbf{R}}$ and ${\\mathbf{r}}$, and\nlet ${\\mathbf{T}}= \\frac{1}{2} {\\mathbf{K}}_j {\\mathbf{r}}{\\mathbf{r}}^T$.\n\n\n\n\nIn~\\cite{comment} an example is given: for an input PSD ${\\mathbf{K}}_j = \n\\left[\n\\begin{array}{cc}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{array}\n\\right]$, \nthe output ${\\mathbf{S}}$ computed using Theorem~\\ref{thm:cftv} \n\n\n\n\n\n\n\n\nis asymmetric, much less that ${\\mathbf{S}}$ is a PSD matrix.  It was also \npointed out in~\\cite{comment} potential technical flaws \ninvolving the derivative with respect to a unit stick tensor in \nthe proof to Theorem~\\ref{thm:cftv}, which is related to \nFootnote~3 in~\\cite{cftv}.\n\n\nNote that Theorem~\\ref{thm:cftv} does not guarantee the output \n${\\mathbf{S}}$ is symmetric or PSD. In the C codes accompanying~\\cite{cftv}, which is \navailable in the IEEE digital library, the {\\tt eig\\_sys} function \nbehaves like singular value decomposition {\\tt svd}.  The statements \nand experiments pertinent to ${\\mathbf{S}}$ in~\\cite{cftv} subsequent to Theorem~\\ref{thm:cftv} \nin fact refer to the SVD results on ${\\mathbf{S}}$, and we apologize for not \nmaking this explicitly clear in the paper.\n\nRecall the singular value decomposition and the eigen-decomposition \nare related, namely, the left-singular vectors of ${\\mathbf{S}}$ are eigenvectors \nof ${\\mathbf{S}} {\\mathbf{S}}^T$, the right singular-vectors of ${\\mathbf{S}}$ are eigenvectors of \n${\\mathbf{S}}^T {\\mathbf{S}}$, where the eigenvectors are orthonormal bases.  \n\n\nWe performed sanity check by running extensive simulations in \ndimensions up to 51 and show that \nall eigenvalues of ${\\mathbf{S}} {\\mathbf{S}}^T$ and ${\\mathbf{S}}^T {\\mathbf{S}}$ are nonnegative,\nand that they are PSD.  The symmetricity of ${\\mathbf{S}}$ is unimportant \nin practice. \n\nNonetheless, for theoretical interest we provide in the following an alternative\nproof to Theorem~\\ref{thm:cftv} which produces a symmetric ${\\mathbf{S}}$ and\nserves to dispel the flaws pointed out in~\\cite{comment}.  \nFinally we run simulations to show that the original tensor voting, \nthe asymmetric CFTV in~\\cite{cftv} and the symmetric \nCFTV in the following produce the practically the same results.\n\n\n\n\\section{Symmetric CFTV}\nFrom the first principle, integrating unit stick tensors ${\\mathbf{N}}_{\\theta} = {\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ \nin all directions $\\theta$ with strength $\\tau_\\theta^2$, we obtain Eq.~(8) in~\\cite{cftv}:\n\n\\begin{eqnarray}\n\\label{eqn:cf_inter} {\\mathbf{S}}_{ij} &=& c_{ij} \\mathbf{R} \\left(  \\mathbf{K}_j - \n\\int_{\\mathbf{N}_{\\theta} \\in \\nu} \\tau_{\\theta}^2 \\mathbf{N}_{\\theta} \\mathbf{r} \\mathbf{r}^T \\mathbf{N}_{\\theta} d\\mathbf{N}_{\\theta} \\right) \\mathbf{R}^T\n\\label{eq:eq8}\n\\end{eqnarray}\nNote the similar form of Eqs~(\\ref{eq:cftv}) and (\\ref{eq:eq8}), and \nthe unconventional integration domain in Eq.~(\\ref{eq:eq8}) \nwhere $\\nu$ represents the space of stick tensors given\nby ${\\mathbf{n}}_{\\theta j}$ as explained in~\\cite{cftv}\\footnote{The suggestion in~\\cite{comment}\nwas our first attempt and as explained in~\\cite{cftv}, it does not have obvious advantage\nwhile making the derivation unnecessarily complicated.}.  Let $\\Omega$ be\nthe integration domain to shorten our notations.\n \nLet ${{\\mathbf{T}}}_{\\mathit{sym}}$ be the integration in Eq.~(\\ref{eqn:cf_inter}).  \n${\\mathbf{T}}_{\\mathit{sym}}$ can be solved by integration by parts.  Here, we repeat \nFootnote~3 in~\\cite{cftv} on the contraction of a sequence of identical unit \nstick tensors when multiplied together.  Let ${\\mathbf{N}}_{\\theta} = {\\mathbf{n}}_{\\theta} {\\mathbf{n}}_{\\theta}^T$ be a unit stick tensor, where ${\\mathbf{n}}_{\\theta}$ is a unit normal at angle $\\theta$, and $q \\ge 1$ be a positive integer, then\n\n", "index": 55, "text": "\\begin{equation}\n{\\mathbf{N}}_{\\theta}^q = {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T \\cdots  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta} \\cdot 1 \\cdot 1 \\cdots 1 \\cdot  {\\mathbf{n}}_{\\theta}^T =  {\\mathbf{n}}_{\\theta}  {\\mathbf{n}}_{\\theta}^T = \\mathbf{N}_{\\theta}.\n\\label{eq:eat}\n\\end{equation}\n", "mathml": "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" id=\"S0.E26.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{N}}_{\\theta}^{q}={\\mathbf{n}}_{\\theta}{\\mathbf{n}}_{\\theta}^{T}{%&#10;\\mathbf{n}}_{\\theta}{\\mathbf{n}}_{\\theta}^{T}\\cdots{\\mathbf{n}}_{\\theta}{%&#10;\\mathbf{n}}_{\\theta}^{T}={\\mathbf{n}}_{\\theta}\\cdot 1\\cdot 1\\cdots 1\\cdot{%&#10;\\mathbf{n}}_{\\theta}^{T}={\\mathbf{n}}_{\\theta}{\\mathbf{n}}_{\\theta}^{T}=%&#10;\\mathbf{N}_{\\theta}.\" display=\"block\"><mrow><mrow><msubsup><mi>\ud835\udc0d</mi><mi>\u03b8</mi><mi>q</mi></msubsup><mo>=</mo><mrow><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup><mo>\u2062</mo><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u22c5</mo><mn>1</mn><mo>\u22c5</mo><mn>1</mn></mrow><mo>\u2062</mo><mi mathvariant=\"normal\">\u22ef</mi><mo>\u2062</mo><mn>1</mn></mrow><mo>\u22c5</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup></mrow><mo>=</mo><mrow><msub><mi>\ud835\udc27</mi><mi>\u03b8</mi></msub><mo>\u2062</mo><msubsup><mi>\ud835\udc27</mi><mi>\u03b8</mi><mi>T</mi></msubsup></mrow><mo>=</mo><msub><mi>\ud835\udc0d</mi><mi>\u03b8</mi></msub></mrow><mo>.</mo></mrow></math>", "type": "latex"}]